Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 177?180,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Semantic Links from a Corpus of
Parallel Temporal and Causal Relations
Steven Bethard
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
steven.bethard@colorado.edu
James H. Martin
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
james.martin@colorado.edu
Abstract
Finding temporal and causal relations is cru-
cial to understanding the semantic structure
of a text. Since existing corpora provide no
parallel temporal and causal annotations, we
annotated 1000 conjoined event pairs, achiev-
ing inter-annotator agreement of 81.2% on
temporal relations and 77.8% on causal re-
lations. We trained machine learning mod-
els using features derived from WordNet and
the Google N-gram corpus, and they out-
performed a variety of baselines, achieving
an F-measure of 49.0 for temporals and 52.4
for causals. Analysis of these models sug-
gests that additional data will improve perfor-
mance, and that temporal information is cru-
cial to causal relation identification.
1 Introduction
Working out how events are tied together temporally
and causally is a crucial component for successful
natural language understanding. Consider the text:
(1) I ate a bad tuna sandwich, got food poisoning
and had to have a shot in my shoulder. wsj 0409
To understand the semantic structure here, a system
must order events along a timeline, recognizing that
getting food poisoning occurred BEFORE having a
shot. The system must also identify when an event
is not independent of the surrounding events, e.g.
got food poisoning was CAUSED by eating a bad
sandwich. Recognizing these temporal and causal
relations is crucial for applications like question an-
swering which must face queries like How did he get
food poisoning? or What was the treatment?
Currently, no existing resource has all the neces-
sary pieces for investigating parallel temporal and
causal phenomena. The TimeBank (Pustejovsky et
al., 2003) links events with BEFORE and AFTER
relations, but includes no causal links. PropBank
(Kingsbury and Palmer, 2002) identifies ARGM-TMP
and ARGM-CAU relations, but arguments may only
be temporal or causal, never both. Thus existing
corpora are missing some crucial pieces for study-
ing temporal-causal interactions. Our research aims
to fill these gaps by building a corpus of parallel
temporal and causal relations and exploring machine
learning approaches to extracting these relations.
2 Related Work
Much recent work on temporal relations revolved
around the TimeBank and TempEval (Verhagen et
al., 2007). These works annotated temporal relations
between events and times, but low inter-annotator
agreement made many TimeBank and TempEval
tasks difficult (Boguraev and Ando, 2005; Verha-
gen et al, 2007). Still, TempEval showed that on a
constrained tense identification task, systems could
achieve accuracies in the 80s, and Bethard and col-
leagues (Bethard et al, 2007) showed that temporal
relations between a verb and a complement clause
could be identified with accuracies of nearly 90%.
Recent work on causal relations has also found
that arbitrary relations in text are difficult to annotate
and give poor system performance (Reitter, 2003).
Girju and colleagues have made progress by select-
ing constrained pairs of events using web search pat-
terns. Both manually generated Cause-Effect pat-
terns (Girju et al, 2007) and patterns based on nouns
177
Full Train Test
Documents 556 344 212
Event pairs 1000 697 303
BEFORE relations 313 232 81
AFTER relations 16 11 5
CAUSAL relations 271 207 64
Table 1: Contents of the corpus and its train/test sections
Task Agreement Kappa F
Temporals 81.2 0.715 71.9
Causals 77.8 0.556 66.5
Table 2: Inter-annotator agreement by task.
linked causally in WordNet (Girju, 2003) were used
to collect examples for annotation, with the result-
ing corpora allowing machine learning models to
achieve performance in the 70s and 80s.
3 Conjoined Events Corpus
Prior work showed that finding temporal and causal
relations is more tractable in carefully selected cor-
pora. Thus we chose a simple construction that
frequently expressed both temporal and causal rela-
tions, and accounted for 10% of all adjacent verbal
events: events conjoined by the word and.
Our temporal annotation guidelines were based
on the guidelines for TimeBank and TempEval, aug-
mented with the guidelines of (Bethard et al, 2008).
Annotators used the labels:
BEFORE The first event fully precedes the second
AFTER The second event fully precedes the first
NO-REL Neither event clearly precedes the other
Our causal annotation guidelines were based on
paraphrasing rather than the intuitive notions of
cause used in prior work (Girju, 2003; Girju et al,
2007). Annotators selected the best paraphrase of
?and? from the following options:
CAUSAL and as a result, and as a consequence,
and enabled by that
NO-REL and independently, and for similar reasons
To build the corpus, we first identified verbs
that represented events by running the system of
(Bethard and Martin, 2006) on the TreeBank. We
then used a set of tree-walking rules to identify con-
joined event pairs. 1000 pairs were annotated by
two annotators and adjudicated by a third. Table 1
S
ADVP
RB
Then
NP
PRP
they
VP
VP CC VP
VBD
took
NP
DT
the
NN
art
PP
TO
to
NP
NNP
Acapulco
and
began
SVBD
VP
TO
to
VP
VB
trade
NP
some of it
PP
for cocaine
Figure 1: Syntactic tree from wsj 0450 with events took
and began highlighted.
and Table 2 give statistics for the resulting corpus1.
The annotators had substantial agreement on tem-
porals (81.2%) and moderate agreement on causals
(77.8%). We also report F-measure agreement, since
BEFORE, AFTER and CAUSAL relations are more in-
teresting than NO-REL. Annotators had F-measure
agreement of 71.9 on temporals and 66.5 causals.
4 Machine Learning Methods
We used our corpus for machine learning experi-
ments where relation identification was viewed as
pair-wise classification. Consider the sentence:
(2) The man who had brought it in for an esti-
mate had [EVENT returned] to collect it and was
[EVENT waiting] in the hall. wsj 0450
A temporal classifier should label returned-waiting
with BEFORE since returned occurred first, and a
causal classifier should label it CAUSAL since this
and can be paraphrased as and as a result.
We identified both syntactic and semantic features
for our task. These will be described using the ex-
ample event pair in Figure 1. Our syntactic features
characterized surrounding surface structures:
? The event words, lemmas and part-of-speech tags,
e.g. took, take, VBD and began, begin, VBD.
? All words, lemmas and part-of-speech tags in the
verb phrases of each event, e.g. took, take, VBD
and began, to, trade, begin, trade, VBD,TO,VB.
? The syntactic paths from the first event to
the common ancestor to the second event, e.g.
VBD>VP, VP and VP<VBD.
1Train: wsj 0416-wsj 0759. Test: wsj 0760-wsj 0971.
verbs.colorado.edu/?bethard/treebank-verb-conj-anns.xml
178
? All words before, between and after the event pair,
e.g. Then, they plus the, art, to, Acapulco, and
plus to, trade, some, of, it, for, cocaine.
Our semantic features encoded surrounding word
meanings. We used WordNet (Fellbaum, 1998) root
synsets (roots) and lexicographer file names (lex-
names) to derive the following features:
? All event roots and lexnames, e.g. take#33,
move#1 . . . body, change . . . for took and be#0,
begin#1 . . . change, communication . . . for began.
? All lexnames before, between and after the event
pair, e.g. all plus artifact, location, etc. plus pos-
session, artifact, etc.
? All roots and lexnames shared by both events, e.g.
took and began were both act#0, be#0 and change,
communication, etc.
? The least common ancestor (LCA) senses shared
by both events, e.g. took and began meet only at
their roots, so the LCA senses are act#0 and be#0.
We also extracted temporal and causal word associ-
ations from the Google N-gram corpus (Brants and
Franz, 2006), using <keyword> <pronoun>
<word> patterns, where before and after were the
keywords for temporals, and because was the key-
word for causals. Word scores were assigned as:
score(w) = log
(
Nkeyword(w)
N(w)
)
where Nkeyword(w) is the number of times the word
appeared in the keyword?s pattern, and N(w) is the
number of times the word was in the corpus. The
following features were derived from these scores:
? Whether the event score was in at least the N th
percentile, e.g. took?s ?6.1 because score placed
it above 84% of the scores, so the feature was true
for N = 70 and N = 80, but false for N = 90.
? Whether the first event score was greater than the
second by at least N , e.g. took and began have
after scores of ?6.3 and ?6.2 so the feature was
true for N = ?1, but false for N = 0 and N = 1.
5 Results
We trained SVMperf classifiers (Joachims, 2005) for
the temporal and causal relation tasks2 using the
2We built multi-class SVMs using the one-vs-rest approach
and used 5-fold cross-validation on the training data to set pa-
rameters. For temporals, C=0.1 (for syntactic-only models),
Temporals Causals
Model P R F1 P R F1
BEFORE 26.7 94.2 41.6 - - -
CAUSAL - - - 21.1 100.0 34.8
1st Event 35.0 24.4 28.8 31.0 20.3 24.5
2nd Event 36.1 30.2 32.9 22.4 17.2 19.5
POS Pair 46.7 8.1 13.9 30.0 4.7 8.1
Syntactic 36.5 53.5 43.4 24.4 79.7 37.4
Semantic 35.8 55.8 43.6 27.2 64.1 38.1
All 43.6 55.8 49.0 27.0 59.4 37.1
All+Tmp - - - 46.9 59.4 52.4
Table 3: Performance of the temporal relation identifica-
tion models: (A)ccuracy, (P)recision, (R)ecall and (F1)-
measure. The null label is NO-REL.
train/test split from Table 1 and the feature sets:
Syntactic The syntactic features from Section 4.
Semantic The semantic features from Section 4.
All Both syntactic and semantic features.
All+Tmp (Causals Only) Syntactic and semantic
features, plus the gold-standard temporal label.
We compared our models against several baselines,
using precision, recall and F-measure since the NO-
REL labels were uninteresting. Two simple baselines
had 0% recall: a lookup table of event word pairs3,
and the majority class (NO-REL) label for causals.
We therefore considered the following baselines:
BEFORE Classify all instances as BEFORE, the ma-
jority class label for temporals.
CAUSAL Classify all instances as CAUSAL.
1st Event Use a lookup table of 1st words and the
labels they were assigned in the training data.
2nd Event As 1st Event, but using 2nd words.
POS Pair As 1st Event, but using part of speech tag
pairs. POS tags encode tense, so this suggests the
performance of a tense-based classifier.
The results on our test data are shown in Table 3. For
temporal relations, the F-measures of all SVM mod-
els exceeded all baselines, with the combination of
syntactic and semantic features performing 5 points
better (43.6% precision and 55.8% recall) than either
feature set individually. This suggests that our syn-
tactic and semantic features encoded complemen-
tary information for the temporal relation task. For
C=1.0 (for all other models), and loss-function=F1 (for all
models). For causals, C=0.1 and loss-function=precision/recall
break even point (for all models).
3Only 3 word pairs from training were seen during testing.
179
Figure 2: Model precisions (dotted lines) and percent of
events in the test data seen during training (solid lines),
given increasing fractions of the training data.
causal relations, all SVM models again exceeded all
baselines, but combining syntactic features with se-
mantic ones gained little. However, knowing about
underlying temporal relations boosted performance
to 46.9% precision and 59.4% recall. This shows
that progress in causal relation identification will re-
quire knowledge of temporal relations.
We examined the effect of corpus size on our
models by training them on increasing fractions of
the training data and evaluating them on the test
data. The precisions of the resulting models are
shown as dotted lines in Figure 2. The models im-
prove steadily, and the causals precision can be seen
to follow the solid curves which show how event
coverage increases with increased training data. A
logarithmic trendline fit to these seen-event curves
suggests that annotating all 5,013 event pairs in the
Penn TreeBank could move event coverage up from
the mid 50s to the mid 80s. Thus annotating addi-
tional data should provide a substantial benefit to our
temporal and causal relation identification systems.
6 Conclusions
Our research fills a gap in existing corpora and NLP
systems, examining parallel temporal and causal re-
lations. We annotated 1000 event pairs conjoined
by the word and, assigning each pair both a tempo-
ral and causal relation. Annotators achieved 81.2%
agreement on temporal relations and 77.8% agree-
ment on causal relations. Using features based on
WordNet and the Google N-gram corpus, we trained
support vector machine models that achieved 49.0
F on temporal relations, and 37.1 F on causal rela-
tions. Providing temporal information to the causal
relations classifier boosted its results to 52.4 F. Fu-
ture work will investigate increasing the size of the
corpus and developing more statistical approaches
like the Google N-gram scores to take advantage of
large-scale resources to characterize word meaning.
Acknowledgments
This research was performed in part under an ap-
pointment to the U.S. Department of Homeland Se-
curity (DHS) Scholarship and Fellowship Program.
References
S. Bethard and J. H. Martin. 2006. Identification of event
mentions and their semantic class. In EMNLP-2006.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007.
Timelines from text: Identification of syntactic tem-
poral relations. In ICSC-2007.
S. Bethard, W. Corvey, S. Klingenstein, and J. H. Martin.
2008. Building a corpus of temporal-causal structure.
In LREC-2008.
B. Boguraev and R. K. Ando. 2005. Timebank-
driven timeml analysis. In Annotating, Extracting
and Reasoning about Time and Events. IBFI, Schloss
Dagstuhl, Germany.
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium, Philadelphia.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney,
and D. Yuret. 2007. Semeval-2007 task 04: Classi-
fication of semantic relations between nominals. In
SemEval-2007.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL Workshop on Multi-
lingual Summarization and Question Answering.
T. Joachims. 2005. A support vector method for multi-
variate performance measures. In ICML-2005.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC-2002.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647?656.
D. Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature sup-
port vector models. LDV-Forum, GLDV-Journal for
Computational Linguistics and Language Technology,
18(1/2):38?52.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. Semeval-2007
task 15: Tempeval temporal relation identification. In
SemEval-2007.
180
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 146?154,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Identification of Event Mentions and their Semantic Class 
 
 
Steven Bethard 
Department of Computer Science 
University of Colorado at Boulder 
430 UCB, Boulder, CO 80309, USA 
steven.bethard@colorado.edu 
James H. Martin 
Department of Computer Science 
University of Colorado at Boulder 
430 UCB, Boulder, CO 80309, USA 
james.martin@colorado.edu 
 
  
 
Abstract 
Complex tasks like question answering 
need to be able to identify events in text 
and the relations among those events. We 
show that this event identification task 
and a related task, identifying the seman-
tic class of these events, can both be for-
mulated as classification problems in a 
word-chunking paradigm. We introduce a 
variety of linguistically motivated fea-
tures for this task and then train a system 
that is able to identify events with a pre-
cision of 82% and a recall of 71%. We 
then show a variety of analyses of this 
model, and their implications for the 
event identification task. 
1 Introduction 
Research in question answering, machine transla-
tion and other fields has shown that being able to 
recognize the important entities in a text is often 
a critical component of these systems. Such en-
tity information gives the machine access to a 
deeper level of semantics than words alone can 
provide, and thus offers advantages for these 
complex tasks. Of course, texts are composed of 
much more than just sets of entities, and archi-
tectures that rely solely on word and entity-based 
techniques are likely to have difficulty with tasks 
that depend more heavily on event and temporal 
relations. Consider a question answering system 
that receives the following questions: 
? Is Anwar al-Sadat still the president of 
Egypt? 
? How did the linking of the Argentinean 
peso to the US dollar in 1991 contribute to 
economic crisis of Argentina in 2003? 
Processing such questions requires not only 
knowing what the important people, places and 
other entities are, but also what kind of events 
they are involved in, the roles they play in those 
events, and the relations among those events. 
Thus, we suggest that identifying such events in 
a text should play an important role in systems 
that attempt to address questions like these. 
Of course, to identify events in texts, we must 
define what exactly it is we mean by ?event?. In 
this work, we adopt a traditional linguistic defini-
tion of an event that divides words into two as-
pectual types: states and events. States describe 
situations that are static or unchanging for their 
duration, while events describe situations that 
involve some internal structure. For example, 
predicates like know and love would be states 
because if we know (or love) someone for a pe-
riod of time, we know (or love) that person at 
each point during the period. Predicates like run 
or deliver a sermon would be events because 
they are built of smaller dissimilar components: 
run includes raising and lowering of legs and 
deliver a sermon includes the various tongue 
movements required to produce words. 
To better explain how we approach the task of 
identifying such events, we first discuss some 
past work on related tasks. Then we briefly dis-
cuss the characteristics of the TimeBank, a cor-
pus containing event-annotated data. Next we 
present our formulation of event identification as 
a classification task and introduce the linguistic 
features that serve as input to the algorithm. Fi-
nally, we show the results of STEP (our ?System 
for Textual Event Parsing?) which applies these 
techniques to the TimeBank data. 
2 Related Efforts 
Such aspectual distinctions have been alive and 
well in the linguistic literature since at least the 
late 60s (Vendler, 1967). However, the use of the 
146
term event in natural language processing work 
has often diverged quite considerably from this 
linguistic notion. In the Topic Detection and 
Tracking (TDT) task, events were sets of docu-
ments that described ?some unique thing that 
happens at some point in time? (Allan et. al., 
1998). In the Message Understanding Confer-
ence (MUC), events were groups of phrases that 
formed a template relating participants, times 
and places to each other (Marsh and Per-
zanowski, 1997). In the work of Filatova and 
Hatzivassiloglou (2003), events consisted of a 
verb and two named-entities occurring together 
frequently across several documents on a topic. 
Several recent efforts have stayed close to the 
linguistic definition of events. One such example 
is the work of Siegel and McKeown (2000) 
which showed that machine learning models 
could be trained to identify some of the tradi-
tional linguistic aspectual distinctions. They 
manually annotated the verbs in a small set of 
texts as either state or event, and then used a va-
riety of linguistically motivated features to train 
machine learning models that were able to make 
the event/state distinction with 93.9% accuracy.  
Another closely related effort was the Evita 
system, developed by Saur? et. al. (2005). This 
work considered a corpus of events called 
TimeBank, whose annotation scheme was moti-
vated largely by the linguistic definitions of 
events. Saur? et. al. showed that a linguistically 
motivated and mainly rule-based algorithm could 
perform well on this task. 
Our work draws from both the Siegel and 
McKeown and Saur? et. al. works. We consider 
the same TimeBank corpus as Saur? et. al., but 
apply a statistical machine learning approach 
akin to that of Siegel and McKeown. We demon-
strate that combining machine learning tech-
niques with linguistically motivated features can 
produce models from the TimeBank data that are 
capable of making a variety of subtle aspectual 
distinctions. 
3 Events in the TimeBank 
TimeBank (Pustejovsky, et. al. 2003b) consists 
of just under 200 documents containing 70,000 
words; it is drawn from news texts from a variety 
of different domains, including newswire and 
transcribed broadcast news. These documents are 
annotated using the TimeML annotation scheme 
(Pustejovsky, et. al. 2003a), which aims to iden-
tify not just times and dates, but events and the 
temporal relations between these events. 
Of interest here are the EVENT annotations, 
of which TimeBank 1.1 has annotated 8312. 
TimeBank annotates a word or phrase as an 
EVENT if it describes a situation that can ?hap-
pen? or ?occur?, or if it describes a ?state? or 
?circumstance? that ?participate[s] in an opposi-
tion structure in a given text? (Pustejovsky, et. al. 
2003b). Note that the TimeBank events are not 
restricted to verbs; nouns and adjectives denote 
events as well. 
The TimeBank definition of event differs in a 
few ways from the traditional linguistic defini-
tion of event. TimeBank EVENTs include not 
only the normal linguistic events, but also some 
linguistic states, depending on the contexts in 
which they occur. For example1, in the sentence 
None of the people on board the airbus survived 
the crash the phrase on board would be consid-
ered to describe an EVENT because that state 
changes in the time span covered by the text. Not 
all linguistic states become TimeBank EVENTs 
in this manner, however. For example, the state 
described by New York is on the east coast holds 
true for a time span much longer than the typical 
newswire document and would therefore not be 
labeled as an EVENT. 
In addition to identifying which words in the 
TimeBank are EVENTs, the TimeBank also pro-
vides a semantic class label for each EVENT. 
The possible labels include OCCURRENCE, 
PERCEPTION, REPORTING, ASPECTUAL, 
STATE, I_STATE, I_ACTION, and MODAL, 
and are described in more detail in (Pustejovsky, 
et. al. 2003a). 
We consider two tasks on this data: 
(1) Identifying which words and phrases are 
EVENTs, and 
(2) Identifying their semantic classes. 
The next section describes how we turn these 
tasks into machine learning problems. 
4 Event Identification as Classification 
We view event identification as a classification 
task using a word-chunking paradigm similar to 
that used by Carreras et. al. (2002). For each 
word in a document, we assign a label indicating 
whether the word is inside or outside of an event. 
We use the standard B-I-O formulation of the 
word-chunking task that augments each class 
label with an indicator of whether the given word 
                                                 
1
 These examples are derived from (Pustejovsky, et. al. 
2003b) 
147
is (B)eginning, (I)nside or (O)utside of a chunk 
(Ramshaw & Marcus, 1995). So, for example, 
under this scheme, sentence (1) would have its 
words labeled as in Table 1. 
(1) The company?s sales force 
[EVENT(I_ACTION) applauded] the 
[EVENT(OCCURRENCE) shake up] 
The two columns of labels in Table 1 show how 
the class labels differ depending on our task. If 
we?re interested only in the simple event identi-
fication task, it?s sufficient to know that ap-
plauded and shake both begin events (and so 
have the label B), up is inside an event (and so 
has the label I), and all other words are outside 
events (and so have the label O). These labels are 
shown in the column labeled Event Label. If in 
addition to identifying events, we also want to 
identify their semantic classes, then we need to 
know that applauded begins an intentional action 
event (B_I_ACTION), shake begins an occur-
rence event (B_OCCURRENCE), up is inside an 
occurrence event (I_OCCURRENCE), and all 
other words are outside of events (O). These la-
bels are shown in the column labeled Event Se-
mantic Class Label. Note that while the eight 
semantic class labels in the TimeBank could po-
tentially introduce as many as 8 ? 2 + 1 = 17 
chunk labels, not all types of events appear as 
multi-word phrases, so we see only 13 of these 
labels in our data. 
5 Classifier Features 
Having cast the problem as a chunking task, our 
next step is to select and represent a useful set of 
features. In our case, since each classification 
instance is a word, our features need to provide 
the information that we deem important for rec-
ognizing whether a word is part of an event or 
not. We consider a number of such features, 
grouped into feature classes for the purposes of 
discussion. 
5.1 Text feature 
This feature is just the textual string for the word. 
5.2 Affix features 
These features attempt to isolate the potentially 
important subsequences of characters in the 
word.  These are intended to identify affixes that 
have a preference for different types of events. 
Affixes: These features identify the first three 
and four characters of the word, and the last three 
and four characters of the word. 
Nominalization suffix: This feature indicates 
which of the suffixes typically associated with 
nominalizations ? ing(s), ion(s), ment(s), and 
nce(s) ? the word ends with. This overlaps with 
the Suffixes feature, but allows the classifier to 
more easily treat nominalizations specially. 
5.3 Morphological features 
These features identify the various morphologi-
cal variants of a word, so that, for example, the 
words resist, resisted and resistance can all be 
identified as the same basic event type. 
Morphological stem: This feature gives the base 
form of the word, so for example, the stem of 
assisted is assist and the stem of investigations is 
investigation.  Stems are identified with a lookup 
table from the University of Pennsylvania of 
around 300,000 words. 
Root verb: This feature gives the verb from 
which the word is derived. For example, assis-
tance is derived from assist and investigation is 
derived from investigate.  Root verbs are identi-
fied with an in-house lookup table of around 
5000 nominalizations. 
5.4 Word class features 
These features attempt to group the words into 
different types of classes.  The intention here is 
to identify correlations between classes of words 
and classes of events, e.g. that events are more 
likely to be expressed as verbs or in verb phrases 
than they are as nouns. 
Part-of-speech: This feature contains the word?s 
part-of-speech based on the Penn Treebank tag 
set. Part-of-speech tags are assigned by the MX-
POST maximum-entropy based part-of-speech 
tagger (Ratnaparkhi, 1996). 
Word Event Label Event Semantic 
Class Label 
The O O 
company O O 
?s O O 
sales O O 
force O O 
applauded B B_I_ACTION 
the O O 
shake B B_OCCURRENCE 
up I I_OCCURRENCE 
. O O 
Table 1: Event chunks for sentence (1) 
148
Syntactic-chunk label: The value of this feature 
is a B-I-O style label indicating what kind of 
syntactic chunk the word is contained in, e.g. 
noun phrase, verb phrase, or prepositional 
phrase. These are assigned using a word-
chunking SVM-based system trained on the 
CoNLL-2000 data2 (which uses the lowest nodes 
of the Penn TreeBank syntactic trees to break 
sentences into base phrases). 
Word cluster: This feature indicates which verb 
or noun cluster the word is a member of. The 
clusters were derived from the co-occurrence 
statistics of verbs and their direct objects, in the 
same manner as Pradhan et. al. (2004). This pro-
duced 128 clusters (half verbs, half nouns) cover-
ing around 100,000 words. 
5.5 Governing features 
These features attempt to include some simple 
dependency information from the surrounding 
words, using the dependency parses produced by 
Minipar3.  These features aim to identify events 
that are expressed as phrases or that require 
knowledge of the surrounding phrase to be iden-
tified. 
Governing light verb: This feature indicates 
which, if any, of the light verbs be, have, get, 
give, make, put, and take governs the word. This 
is intended to capture adjectival predicates such 
as may be ready, and nominal predicates such as 
make an offer, where ready and offer should be 
identified as events. 
Determiner type: This feature indicates the type 
of determiner a noun phrase has. If the noun 
phrase has an explicit determiner, e.g. a, the or 
some, the value of this feature is the determiner 
itself. We use the determiners themselves as fea-
ture values here because they form a small, 
closed class of words. For open-class determiner-
like modifiers, we instead group them into 
classes.  For noun phrases that are explicitly 
quantified, like a million dollars, the value is 
CARDINAL, while for noun phrases modified 
by other possessive noun phrases, like Bush's 
real objectives, the value is GENITIVE. For 
noun phrases without a determiner-like modifier, 
the value is PROPER_NOUN, BARE_PLURAL 
or BARE_SINGULAR, depending on the noun 
type. 
                                                 
2
 http://cnts.uia.ac.be/conll2000/ 
3
 http://www.cs.ualberta.ca/~lindek/minipar.htm 
Subject determiner type: This feature indicates 
for a verb the determiner type (as above) of its 
subject. This is intended to distinguish generic 
sentences like Cats have fur from non-generics 
like The cat has fur.  
5.6 Temporal features 
These features try to identify temporal relations 
between words.  Since the duration of a situation 
is at the core of the TimeBank definition of 
events, features that can get at such information 
are particularly relevant. 
Time chunk label: The value of this feature is a 
B-I-O label indicating whether or not this word is 
contained in a temporal annotation. The temporal 
annotations are produced by a word-chunking 
SVM-based system trained on the temporal ex-
pressions (TIMEX2 annotations) in the TERN 
2004 data4.  In addition to identifying expres-
sions like Monday and this year, the TERN data 
identifies event-containing expressions like the 
time she arrived at her doctor's office. 
Governing temporal: This feature indicates 
which kind of temporal preposition governs the 
word. Since the TimeBank is particularly inter-
ested in which events start or end within the time 
span of the document, we consider prepositions 
likely to indicate such a change of state, includ-
ing after, before, during, following, since, till, 
until and while. 
Modifying temporal: This feature indicates 
which kind of temporal expression modifies the 
word. Temporal expressions are recognized as 
above, and the type of modification is either the 
preposition that joins the temporal annotation to 
the word, or ADVERBIAL for any non-
preposition modification. This is intended to cap-
ture that modifying temporal expressions often 
indicate event times, e.g. He ran the race in an 
hour. 
5.7 Negation feature 
This feature indicates which negative particle, 
e.g. not, never, etc., modifies the word. The idea 
is based Siegel and McKeown?s (2000) findings 
which suggested that in some corpora states oc-
cur more freely with negation than events do. 
5.8 WordNet hypernym features 
These features indicate to which of the WordNet 
noun and verb sub-hierarchies the word belongs. 
                                                 
4
 http://timex2.mitre.org/tern.html 
149
Rather than include all of the thousands of dif-
ferent sub-hierarchies in WordNet, we first se-
lected the most useful candidates by looking at 
the overlap with WordNet and our training data. 
For each hierarchy in WordNet, we considered a 
classifier that labeled all words in that hierarchy 
as events, and all words outside of that hierarchy 
as non-events5. We then evaluated these classifi-
ers on our training data, and selected the ten with 
the highest F-measures. This resulted in selecting 
the following synsets: 
? noun: state 
? noun: psychological feature 
? noun: event 
? verb: think, cogitate, cerebrate 
? verb: move, displace 
? noun: group, grouping 
? verb: act, move 
? noun: act, human action, human activity 
? noun: abstraction 
? noun: entity 
The values of the features were then whether or 
not the word fell into the hierarchy defined by 
each one of these roots. Note that since there are 
no WordNet senses labeled in our data, we ac-
cept a word as falling into one of the above hier-
archies if any of its senses fall into that hierar-
chy. 
6 Classifier Parameters 
The features described in the previous section 
give us a way to provide the learning algorithm 
with the necessary information to make a classi-
fication decision. The next step is to convert our 
training data into sets of features, and feed these 
classification instances to the learning algorithm. 
For the learning task, we use the TinySVM6 sup-
port vector machine (SVM) implementation in 
conjunction with YamCha7 (Kudo & Matsumoto, 
2001), a suite for general-purpose chunking. 
YamCha has a number of parameters that de-
fine how it learns. The first of these is the win-
dow width of the ?sliding window? that it uses. 
                                                 
5
 We also considered the reverse classifiers, which classi-
fied all words in the hierarchy as non-events and all words 
outside the hierarchy as events. 
6
 http://chasen.org/~taku/software/TinySVM/ 
7
 http://chasen.org/~taku/software/yamcha/ 
A sliding window is a way of including some of 
the context when the classification decision is 
made for a word. This is done by including the 
features of preceding and following words in 
addition to the features of the word to be classi-
fied. To illustrate this, we consider our earlier 
example, now augmented with some additional 
features in Table 2. 
To classify up in this scenario, we now look 
not only at its features, but at the features of 
some of the neighboring words. For example, if 
our window width was 1, the feature values we 
would use for classification would be those in the 
outlined box, that is, the features of shake, up 
and the sentence final period. Note that we do 
not include the classification labels for either up 
or the period since neither of these classifications 
is available at the time we try to classify up. Us-
ing such a sliding window allows YamCha to 
include important information, like that up is 
preceded by shake and that shake was identified 
as beginning an event. 
In addition to the window width parameter, 
YamCha also requires values for the following 
three parameters: the penalty for misclassifica-
tion (C), the kernel?s polynomial degree, and the 
method for applying binary classifiers to our 
multi-class problem, either pair-wise or one-vs-
rest. In our experiments, we chose a one-vs-rest 
multi-class scheme to keep training time down, 
and then tried different variations of all the other 
parameters to explore a variety of models. 
7 Baseline Models 
To be able to meaningfully evaluate the models 
we train, we needed to establish a reasonable 
baseline. Because the majority class baseline 
would simply label every word as a non-event, 
we introduce two baseline models that should be 
more reasonable: Memorize and Sim-Evita. 
Word POS Stem Label 
The DT the O 
company NN company O 
?s POS ?s O 
sales NNS sale O 
force NN force O 
applauded VBD applaud B 
The DT the O 
shake NN shake B 
up RP up  
. . .  
Table 2: A window of word features 
150
The Memorize baseline is essentially a lookup 
table ? it memorizes the training data. This sys-
tem assigns to each word the label with which it 
occurred most frequently in the training data, or 
the label O (not an event) if the word never oc-
curred in the training data. 
The Sim-Evita model is our attempt to simu-
late the Evita system (Saur? et. al. 2005). As part 
of its algorithm, Evita includes a check that de-
termines whether or not a word occurs as an 
event in TimeBank. It performs this check even 
when evaluated on TimeBank, and thus though 
Evita reports 74% precision and 87% recall, 
these numbers are artificially inflated because the 
system was trained and tested on the same cor-
pus. Thus we cannot directly compare our results 
to theirs. Instead, we simulate Evita by taking the 
information that it encodes as rules, and encod-
ing this instead as features which we provide to a 
YamCha-based system. 
Saur? et. al. (2005) provides a description of 
Evita?s rules, which, according to the text, are 
based on information from lexical stems, part of 
speech tags, syntactic chunks, weak stative 
predicates, copular verbs, complements of copu-
lar predicates, verbs with bare plural subjects and 
WordNet ancestors. We decided that the follow-
ing features most fully covered the same infor-
mation: 
? Text 
? Morphological stem 
? Part-of-speech 
? Syntactic-chunk label 
? Governing light verb 
? Subject determiner type 
? WordNet hypernyms 
We also decided that since Evita does not con-
sider a word-window around the word to be clas-
sified, we should set our window size parameter 
to zero. 
Because our approximation of Evita uses a 
feature-based statistical machine learning algo-
rithm instead of the rule-based Evita algorithm, it 
cannot predict how well Evita would perform if 
it had not used the same data for training and 
testing. However, it can give us an approxima-
tion of how well a model can perform using in-
formation similar to that of Evita. 
8 Results 
Having decided on our feature space, our learn-
ing model, and the baselines to which we will 
compare, we now describe the results of our 
models on the TimeBank. We selected a strati-
fied sample of 90% of the TimeBank data for a 
training set, and reserved the remaining 10% for 
testing8. 
We consider three evaluation measures: preci-
sion, recall and F-measure. Precision is defined 
as the number of B and I labels our system iden-
tifies correctly, divided by the total number of B 
and I labels our system predicted. Recall is de-
fined as the number of B and I labels our system 
identifies correctly, divided by the total number 
of B and I labels in the TimeBank data. F-
measure is defined as the geometric mean of pre-
cision and recall9. 
To determine the best parameter settings for 
the models, we performed cross-validations on 
our training data, leaving the testing data un-
touched. We divided the training data randomly 
into five equally-sized sections. Then, for each 
set of parameters to be evaluated, we determined 
a cross-validation F-measure by averaging the F-
measures of five runs, each tested on one of the 
training data sections and trained on the remain-
ing training data sections. We selected the pa-
rameters of the model that had the best cross-
validation F-measure on the training data as the 
parameters for the rest of our experiments. For 
the simple event identification model this se-
lected a window width of 2, polynomial degree 
of 3 and C value of 0.1, and for the event and 
class identification model this selected a window 
width of 1, polynomial degree of 1 and C value 
0.1. For the Sim-Evita simple event identification 
model this selected a degree of 2 and C value of 
0.01, and for the Sim-Evita event and class iden-
tification model, this selected a degree of 1 and C 
value of 1.0. 
Having selected the appropriate parameters for 
our learning algorithm, we then trained our SVM 
models on the training data. Table 3 presents the 
results of these models on the test data. Our 
model (named STEP above for ?System for Tex-
                                                 
8
 The testing documents were: 
APW19980219.0476, APW19980418.0210, 
NYT19980206.0466, PRI19980303.2000.2550, 
ea980120.1830.0071, and the wsj_XXXX_orig documents 
numbered 0122, 0157, 0172, 0313, 0348, 0541, 0584, 0667, 
0736, 0791, 0907, 0991 and 1033. 
9
 
RP
RPF
+
??
=
2
 
151
tual Event Parsing?) outperforms both baselines 
on both tasks. For simple event identification, the 
main win over both baselines is an increased re-
call. Our model achieves a recall of 70.6%, about 
5% better than our simulation of Evita, and 
nearly 15% better than the Memorize baseline. 
For event and class identification, the win is 
again in recall, though to a lesser degree. Our 
system achieves a recall of 51.2%, about 5% bet-
ter than Sim-Evita, and 10% better than Memo-
rize. On this task, we also achieve a precision of 
66.7%, about 10% better than the precision of 
Sim-Evita. This indicates that the model trained 
with no context window and using the Evita-like 
feature set was at a distinct disadvantage over the 
model which had access to all of the features. 
Table 4 and Table 5 show the results of our 
systems on various sub-tasks, with the ?%? col-
umn indicating what percent of the events in the 
test data each subtask contained. Table 4 shows 
that in both tasks, we do dramatically better on 
verbs than on nouns, especially as far as recall is 
concerned. This is relatively unsurprising ? not 
only is there more data for verbs (59% of event 
words are verbs, while only 28% are nouns), but 
our models generally do better on words they 
have seen before, and there are many more nouns 
we have not seen than there are verbs. 
Table 5 shows how well we did individually 
on each type of label. For simple event identifi-
cation (the top two rows) we can see that we do 
substantially better on B labels than on I labels, 
as we would expect since 92% of event words 
are labeled B. The label-wise performance for 
the event and class identification (the bottom 
seven rows) is more interesting. Our best per-
formance is actually on Reporting event words, 
even though the data is mainly Occurrence event 
words. One reason for this is that instances of the 
word said make up about 60% of Reporting 
event words in the TimeBank. The word said is 
relatively easy to get right because it comes with 
by far the most training data10, and because it is 
almost always an event: 98% of the time in the 
TimeBank, and 100% of the time in our test data. 
To determine how much each of the feature 
sets contributed to our models we also performed 
a pair of ablation studies. In each ablation study, 
we trained a series of models on successively 
fewer feature sets, removing the least important 
feature set each time. The least important feature 
set was determined by finding out which feature 
set?s removal caused the smallest drop in F-
measure. The result of this process was a list of 
our feature sets, ordered by importance. These 
lists are given for both tasks in Table 6, along 
with the precision, recall and F-measures of the 
various corresponding models.  Each row in 
Table 6 corresponds to a model trained on the 
feature sets named in that row and all the rows 
below it.  Thus, on the top row, no feature sets 
have been removed, and on the bottom row only 
one feature set remains. 
                                                 
10
 The word ?said? has over 600 instances in TimeBank. 
The word with the next most instances has just over 200 
 Event Identification Event and Class Identification 
Model Precision Recall F Precision Recall F  
Memorize 0.806 0.557 0.658 0.640 0.413 0.502  
Sim-Evita 0.812 0.659 0.727 0.571 0.459 0.509  
STEP 0.820 0.706 0.759 0.667 0.512 0.579  
Table 3: Overall results for both tasks 
 Event Identification Event and Class Identification 
 % Precision Recall F % Precision Recall F 
Verbs 59 0.864 0.903 0.883 59 0.714 0.701 0.707 
Nouns 28 0.729 0.432 0.543 28 0.473 0.261 0.337 
Table 4: Results by word class for both tasks 
 % Precision Recall F 
B 92 0.827 0.737 0.779 
I 8 0.679 0.339 0.452 
B Occurrence 44 0.633 0.727 0.677 
B State 14 0.519 0.136 0.215 
B Reporting 11 0.909 0.779 0.839 
B Istate 10 0.737 0.378 0.500 
B Iaction 10 0.480 0.174 0.255 
I State 7 0.818 0.173 0.286 
B Aspectual 3 0.684 0.684 0.684 
Table 5: Results by label 
152
So, for example, in the simple event identifica-
tion task, we see that the Governing, Negation, 
Affix and WordNet features are hurting the clas-
sifier somewhat ? a model trained without these 
features performs at an F-measure of 0.772, more 
than 1% better than a model including these fea-
tures. In contrast, we can see that for the event 
and semantic class identification task, the Word-
Net and Affix features are actually among the 
most important, with only the Word class fea-
tures accompanying them in the top three. These 
ablation results suggest that word class, textual, 
morphological and temporal information is most 
useful for simple event identification, and affix, 
WordNet and negation information is only really 
needed when the semantic class of an event must 
also be identified. 
The last thing we investigated was the effect 
of additional training data. To do so, we trained 
the model on increasing fractions of the training 
data, and measured the classification accuracy on 
the testing data of each of the models thus 
trained. The resulting graph is shown in Figure 1. 
The Majority line indicates the classifier accu-
racy when the classifier always guesses majority 
class, that is, (O)utside of an event. We can see 
from the two learning curves that even with only 
the small amount of data available in the 
TimeBank, our models are already reaching the 
level part of the learning curve at somewhere 
around 20% of the data. This suggests that, 
though additional data may help somewhat in the 
data sparseness problem, substantial further pro-
gress on this task will require new, more descrip-
tive features. 
9 Conclusions 
In this paper, we showed that statistical machine 
learning techniques can be successfully applied 
to the problem of identifying fine-grained events 
in a text. We formulated this task as a statistical 
classification task using a word-chunking para-
digm, where words are labeled as beginning, in-
side or outside of an event. We introduced a va-
riety of relevant linguistically-motivated fea-
tures, and showed that models trained in this way 
could perform quite well on the task, with a pre-
cision of 82% and a recall of 71%. This method 
extended to the task of identifying the semantic 
class of an event with a precision of 67% and a 
recall of 51%. Our analysis of these models indi-
cates that while the simple event identification 
task can be approached with mostly simple text 
and word-class based features, identifying the 
semantic class of an event requires features that 
encode more of the semantic context of the 
words. Finally, our training curves suggest that 
future research in this area should focus primar-
ily on identifying more discriminative features. 
Event Identification  Event and Class Identification 
Feature set Precision Recall F  Feature set Precision Recall F 
Governing 0.820 0.706 0.759  Governing 0.667 0.512 0.579 
Negation 0.824 0.713 0.765  Temporal 0.675 0.513 0.583 
Affix 0.826 0.715 0.766  Negation 0.672 0.510 0.580 
WordNet 0.818 0.723 0.768  Morphological 0.670 0.509 0.579 
Temporal 0.820 0.729 0.772  Text 0.671 0.505 0.576 
Morphological 0.816 0.727 0.769  WordNet 0.679 0.497 0.574 
Text 0.816 0.697 0.752  Word class 0.682 0.474 0.559 
Word class 0.719 0.677 0.697  Affix 0.720 0.421 0.531 
Table 6: Ablations for both tasks. For each task, the least important feature sets appear at the top of the 
table, and most important feature sets appear at the bottom. For each row, the precision, recall and F-
measure indicate the scores of a model trained with only the feature sets named in that row and the 
rows below it. 
0.
85
0.
89
0.
93
0.
97
0 0.2 0.4 0.6 0.8 1
Fraction of Training Data
A
cc
u
ra
cy
 
o
n
 
Te
st
 
D
at
a
Event Event+Semantic Majority
 
Figure 1: Learning Curves 
153
10 Acknowledgments 
This work was partially supported by a DHS fel-
lowship to the first author and by ARDA under 
AQUAINT project MDA908-02-C-0008. Com-
puter time was provided by NSF ARI Grant 
#CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE 
SciDAC grant #DE-FG02-04ER63870, NSF 
sponsorship of the National Center for Atmos-
pheric Research, and a grant from the IBM 
Shared University Research (SUR) program. 
Any opinions, findings, or recommendations are 
those of the authors and do not necessarily reflect 
the views of the sponsors. Particular thanks go to 
Wayne Ward and Martha Palmer for many help-
ful discussions and comments. 
References 
James Allan, Jaime Carbonell, George Dodding-ton, 
Jonathan Yamron and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
In: Proceedings of DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Xavier Carreras, Llu?s M?rquez and Llu?s Padr?. 
Named Entity Extraction using AdaBoost. 2002. In 
Proceedings of CoNNL-2002. 
Elena Filatova and Vasileios Hatzivassiloglou. Do-
main-Independent Detection, Extraction, and La-
beling of Atomic Events. 2003. In the Proceedings 
of Recent Advances in Natural Language Process-
ing Conference, September 2003. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with support vector machines. In Proceedings of 
NAACL 2001. 
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 
evaluation of IE technology: Over-view of results. 
In Proceedings of the Seventh MUC. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chines. In Proceedings of HLT/NAACL 2004. 
James Pustejovsky, Jos? Casta?o, Robert Ingria, 
Roser Saur?, Robert Gaizauskas, Andrea Setzer and 
Graham Katz. TimeML: 2003a. Robust Specifica-
tion of Event and Temporal Expressions in Text. In 
Proceedings of the Fifth International Workshop on 
Computational Semantics (IWCS-5) 
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer, 
Dragomir Radev, Beth Sundheim, David Day, Lisa 
Ferro and Marcia Lazo. 2003b. The TIMEBANK 
Corpus. In Proceedings of Corpus Linguistics 
2003, 647-656. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text Chunking using Transformation-Based Learn-
ing. In Proceedings of the ACL Third Workshop 
on Very Large Corpora. 82-94. 
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of EMNLP 1996. 
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky 2005. Evita: A Robust 
Event Recognizer For QA Systems. In Proceedings 
of HLT-EMNLP 2005. 
Eric V. Siegel and Kathleen R. McKeown. Learning 
Methods to Combine Linguistic Indicators: Im-
proving Aspectual Classification and Revealing 
Linguistic Insights. Computational Linguistics, 
26(4):595 627. 
Zeno Vendler. 1967. Verbs and times. In Linguistics 
and Philosophy. Cornell University Press, Ithaca, 
New 
154
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 129?132,
Prague, June 2007. c?2007 Association for Computational Linguistics
CU-TMP:
Temporal Relation Classification Using Syntactic and Semantic Features
Steven Bethard and James H. Martin
Department of Computer Science
University of Colorado at Boulder
430 UCB, Boulder, CO 80309, USA
{bethard,martin}@colorado.edu
Abstract
We approached the temporal relation identi-
fication tasks of TempEval 2007 as pair-wise
classification tasks. We introduced a va-
riety of syntactically and semantically mo-
tivated features, including temporal-logic-
based features derived from running our
Task B system on the Task A and C data.
We trained support vector machine models
and achieved the second highest accuracies
on the tasks: 61% on Task A, 75% on Task B
and 54% on Task C.
1 Introduction
In recent years, the temporal structure of text has be-
come a popular area of natural language processing
research. Consider a sentence like:
(1) The top commander of a Cambodian resistance
force said Thursday he has sent a team to
recover the remains of a British mine removal
expert kidnapped and presumed killed by
Khmer Rouge guerrillas almost two years ago.
English speakers immediately recognize that kid-
napping came first, then sending, and finally saying,
even though before and after never appeared in the
text. How can machines learn to do the same?
The 2007 TempEval competition tries to address
this question by establishing a common corpus on
which research systems can compete to find tempo-
ral relations (Verhagen et al, 2007). TempEval con-
siders the following types of event-time temporal re-
lations:
Task A Events1and times within the same sentence
Task B Events1 and document times
Task C Matrix verb events in adjacent sentences
In each of these tasks, systems attempt to annotate
pairs with one of the following relations: BEFORE,
BEFORE-OR-OVERLAP, OVERLAP, OVERLAP-OF-
AFTER, AFTER or VAGUE. Competing systems are
instructed to find all temporal relations of these
types in a corpus of newswire documents.
We approach these tasks as pair-wise classifi-
cation problems, where each event/time pair is
assigned one of the TempEval relation classes
(BEFORE, AFTER, etc.). Event/time pairs are en-
coded using syntactically and semantically moti-
vated features, and then used to train support vector
machine (SVM) classifiers.
The remainder of this paper is structured as fol-
lows. Section 2 describes the features used to char-
acterize event/time relations. Section 3 explains how
we used these features to train SVM models for each
task. Section 4 discusses the performance of our
models on the TempEval data, and Section 5 sum-
marizes the lessons learned and future directions.
2 Features
We used a variety of lexical, syntactic and semantic
features to characterize the different types of tempo-
ral relations. In each task, the events and times were
characterized using the features:
word The text of the event or time words
1TempEval only considers events that occurred at least 20
times in the TimeBank (Pustejovsky et al, 2003) corpus for
these tasks
129
Shhhhhhhhhhhhh
QQQ
(((((((((((((
PPhhhhh(((((
IN
For
NPhhhh((((
NPhhh(((
[TIMEthe quarter]
VPXXX
VBN
ended
NPXXX
[TIMESept 30]
NP
Delta
VPhhhhh(((((
VBD
[EVENTposted]
NPhhhhh(((((
net income of $133 million
Figure 1: A syntactic tree. The path between posted and the quarter is VBD-VP-S-PP-NP-NP
pos The parts of speech2of the words, e.g. this cru-
cial moment has the parts of speech DT-JJ-NN.
gov-prep Any prepositions governing the event or
time, e.g. in during the Iran-Iraq war, the
preposition during governs the event war, and
in after ten years, the preposition after governs
the time ten years.
gov-verb The verb that governs the event or time,
e.g. in rejected in peace talks, the verb rejected
governs the event talks, and in withdrawing on
Friday, the verb withdrawing governs the time
Friday. For events that are verbs, this feature is
just the event itself.
gov-verb-pos The part of speech2 of the governing
verb, e.g. withdrawing has the part of speech
VBG.
aux Any auxiliary verbs and adverbs modifying the
governing verb, e.g. in could not come, the
words could and not are considered auxiliaries
for the event come, and in will begin withdraw-
ing on Friday, the words will and begin are con-
sidered auxiliaries for the time Friday.
Events were further characterized using the features
(the last six use gold-standard TempEval markup):
modal Whether or not the event has one of the aux-
iliaries, can, will, shall, may, or any of their
variants (could, would, etc.).
gold-stem The stem, e.g. the stem of fallen is fall.
gold-pos The part-of-speech, e.g. NOUN or VERB.
gold-class The semantic class, e.g. REPORTING.
gold-tense The tense, e.g. PAST or PRESENT.
gold-aspect The aspect, e.g. PERFECTIVE.
gold-polarity The polarity, e.g. POS or NEG.
Times were further characterized using the follow-
ing gold-standard TempEval features:
2From MXPOST (ftp.cis.upenn.edu/pub/adwait/jmx/)
gold-type The type, e.g. DATE or TIME.
gold-value The value, e.g. PAST REF or 1990-09.
gold-func The temporal function, e.g. TRUE.
These gold-standard event and time features are sim-
ilar to those used by Mani and colleagues (2006).
The features above don?t capture much of the dif-
ferences between the tasks, so we introduced some
task-specific features. Task A included the features:
inter-time The count of time expressions between
the event and time, e.g. in Figure 1, there is
one time expression, Sept 30, between the event
posted and the time the quarter.
inter-path The syntactic path between the event
and the time, e.g. in Figure 1 the
path between posted and the quarter is
VBD>VP>S<PP<NP<NP.
inter-path-parts The path, broken into three parts:
the tags from the event to the lowest common
ancestor (LCA), the LCA, and the tags from the
LCA to the time, e.g. in Figure 1 the parts are
VBD>VP, S and PP<NP<NP.
inter-clause The number of clause nodes along the
syntactic path, e.g. in Figure 1 there is one
clause node along the path, the top S node.
Our syntactic features were derived from a syntactic
tree, though Boguraev and Ando (2005) suggest that
some could be derived from finite state grammars.
For Task C we included the following feature:
tense-rules The relation predicted by a set of tense
rules, where past tense events come BEFORE
present tense events, present tense events come
BEFORE future tense events, etc. In the text:
(2) Finally today, we [EVENT learned] that
the space agency has taken a giant leap
forward. Collins will be [EVENT named]
commander of Space Shuttle Columbia.
130
Since learned is in past tense and named is in
future, the relation is (learned BEFORE named).
In preliminary experiments, the Task B system had
the best performance, so we ran this system on the
data for Tasks A and C, and used the output to add
the following feature for both tasks:
task-b-rel The relation predicted by combining the
output of the Task B system with temporal
logic. For example, consider the text:
(3) [TIME 08-15-90 (=1990-08-15)]
Iraq?s Saddam Hussein
[TIME today (=1990-08-15)] sought
peace on another front by promising to
release soldiers captured during the
Iran-Iraq [EVENT war].
If Task B said (war BEFORE 08?15?90)
then since 08?15?90=1990?08?15=today,
the relation (war BEFORE today) must hold.
3 Models
Using the features described in the previous section,
each temporal relation ? an event paired with a time
or another event ? was translated into a set of fea-
ture values. Pairing those feature values with the
TempEval labels (BEFORE, AFTER, etc.) we trained
a statistical classifier for each task. We chose sup-
port vector machines3(SVMs) for our classifiers as
they have shown good performance on a variety of
natural language processing tasks (Kudo and Mat-
sumoto, 2001; Pradhan et al, 2005).
Using cross-validations on the training data, we
performed a simple feature selection where any fea-
ture whose removal improved the cross-validation
F-score was discarded. The resulting features for
each task are listed in Table 1. After feature selec-
tion, we set the SVM free parameters, e.g. the ker-
nel degree and cost of misclassification, by perform-
ing additional cross-validations on the training data,
and selecting the model parameters which yielded
the highest F-score for each task4.
3We used the TinySVM implementation from
http://chasen.org/%7Etaku/software/TinySVM/ and trained
one-vs-rest classifiers.
4We only experimented with polynomial kernels.
Feature Task A Task B Task C
event-word
event-pos X X
event-gov-prep X X
event-gov-verb X X
event-gov-verb-pos X X 2
event-aux X X X
modal X X
gold-stem X X 1
gold-pos X X
gold-class X X X
gold-tense X X X
gold-aspect X X
gold-polarity X X
time-word X
time-pos X
time-gov-prep X
time-gov-verb X
time-gov-verb-pos X
time-aux X
gold-type
gold-value X X
gold-func X
inter-time X
inter-path X
inter-path-parts X
inter-clause X
tense-rules X
task-b-rel X X
Table 1: Features used in each task. An X indicates
that the feature was used for that task. For Task C, 1
indicates that the feature was used only for the first
event and not the second, and 2 indicates the reverse.
Strict Relaxed
Task P R F P R F
A 0.61 0.61 0.61 0.63 0.63 0.63
B 0.75 0.75 0.75 0.76 0.76 0.76
C 0.54 0.54 0.54 0.60 0.60 0.60
Table 2: (P)recision, (R)ecall and (F)-measure of
the models on each task. Precision, recall and F-
measure are all equivalent to classification accuracy.
4 Results
We evaluated our classifers on the TempEval test
data. Because the Task A and C models derived fea-
tures from the Task B temporal relations, we first ran
the Task B classifer over all the data, and then ran the
Task A and Task C classifiers over their individual
data. The resulting temporal relation classifications
were evalutated using the standard TempEval scor-
ing script. Table 2 summarizes these results.
Our models achieved an accuracy of 61% on
Task A, 75% on Task B and 54% on Task C, the
second highest scores on all these tasks. The Temp-
131
Task Feature Removed Model Accuracy
A
- 0.663
time-gov-prep 0.650
gold-value 0.652
polarity 0.655
task-b-rel 0.656
B
- 0.809
event-aux 0.780
gold-stem 0.784
gold-class 0.794
C
- 0.534
event-gov-verb-2 0.522
event-aux-2 0.525
gold-class-1 0.526
gold-class-2 0.527
event-pos-2, task-b-rel 0.529
Table 3: Feature analysis. The ?-? lines show the
accuracy of the model with all features.
Eval scoring script also reported a relaxed measure
where for example, systems could get partial credit
for matching a gold standard label like OVERLAP-
OR-AFTER with OVERLAP or AFTER. Under this
measure, our models achieved an accuracy of 63%
on Task A, 76% on Task B and 60% on Task C, again
the second highest scores in the competition.
We performed a basic feature analysis where, for
each feature in a task, a model was trained with that
feature removed and all other features retained. We
evaluated the performance of the resulting models
using cross-validations on the training data5. Fea-
tures whose removal resulted in the largest drops in
model performance are listed in Table 3.
For Task A, the most important features were the
preposition governing the time and the time?s nor-
malized value. For Task B, the most important fea-
tures were the auxiliaries governing the event, and
the event?s stem. For Task C, the most important
features were the verb and auxiliaries governing the
second event. For both Tasks A and C, the features
based on the Task B relations were one of the top
six features. In general however, no single feature
dominated any one task ? the greatest drop in per-
formance from removing a feature was only 2.9%.
5 Conclusions
TempEval 2007 introduced a common dataset for
work on identifying temporal relations. We framed
5We used cross-validations on the training data to preserve
the validity of the TempEval test data for future research
the TempEval tasks as pair-wise classification prob-
lems where pairs of events and times were assigned
a temporal relation class. We introduced a variety of
syntactic and semantic features, including paths be-
tween constituents in a syntactic tree, and temporal
relations deduced by running our Task B system on
the Task A and C data. Our models achieved an ac-
curacy of 61% on Task A, 75% on Task B and 54%
on Task C. Analysis of these models indicated that
no single feature dominated any given task, and sug-
gested that future work should focus on new features
to better characterize temporal relations.
6 Acknowledgments
This research was performed under an appointment
of the first author to the DHS Scholarship and
Fellowship Program, administered by the ORISE
through an interagency agreement between DOE
and DHS. ORISE is managed by ORAU under DOE
contract number DE-AC05-06OR23100. All opin-
ions expressed in this paper are the author?s and
do not necessarily reflect the policies and views of
DHS, DOE, or ORAU/ORISE.
References
B. Boguraev and R. K. Ando. 2005. Timebank-driven
timeml analysis. In Graham Katz, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, Dagstuhl Sem-
inars. German Research Foundation.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In NAACL.
I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and
J. Pustejovsky. 2006. Machine learning of temporal
relations. In COLING/ACL.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Mar-
tin, and D. Jurafsky. 2005. Support vector learning for
semantic argument classification. Machine Learning,
60(1):11?39.
J. Pustejovsky, P. Hanks, R. Saur, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647?656.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, and
J. Pustejovsky. 2007. Semeval-2007 task 15: Temp-
eval temporal relation identification. In SemEval-
2007: 4th International Workshop on Semantic Evalu-
ations.
132
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building Test Suites for UIMA Components 
 
 
Philip V. Ogren Steven J. Bethard 
Center for Computational Pharmacology Department of Computer Science 
University of Colorado Denver Stanford University 
Denver, CO 80217, USA Stanford, CA 94305, USA 
philip@ogren.info bethard@stanford.edu 
 
 
 
 
 
 
Abstract 
We summarize our experiences building a 
comprehensive suite of tests for a statistical 
natural language processing toolkit, ClearTK. 
We describe some of the challenges we en-
countered, introduce a software project that 
emerged from these efforts, summarize our re-
sulting test suite, and discuss some of the les-
sons learned.  
1 Introduction 
We are actively developing a software toolkit for 
statistical natural processing called ClearTK (Og-
ren et al, 2008) 1, which is built on top of the Un-
structured Information Management Architecture 
(UIMA) (Ferrucci and Lally, 2004). From the be-
ginning of the project, we have built and main-
tained a comprehensive test suite for the ClearTK 
components. This test suite has proved to be inva-
luable as our APIs and implementations have 
evolved and matured. As is common with early-
stage software projects, our code has undergone 
number of significant refactoring changes and such 
changes invariably break code that was previously 
working. We have found that our test suite has 
made it much easier to identify problems intro-
duced by refactoring in addition to preemptively 
discovering bugs that are present in new code. We 
have also observed anecdotally that code that is 
                                                          
1 http://cleartk.googlecode.com 
more thoroughly tested as measured by code cov-
erage has proven to be more reliable and easier to 
maintain. 
While this test suite has been an indispensable 
resource for our project, we have found creating 
tests for our UIMA components to be challenging 
for a number of reasons. In a typical UIMA 
processing pipeline, components created by devel-
opers are instantiated by a UIMA container called 
the Collection Processing Manager (CPM) which 
decides at runtime how to instantiate components 
and what order they should run via configuration 
information provided in descriptor files. This pat-
tern is typical of programming frameworks: the 
developer creates components that satisfy some 
API specification and then these components are 
managed by the framework. This means that the 
developer rarely directly instantiates the compo-
nents that are developed and simple programs con-
sisting of e.g. a main method are uncommon and 
can be awkward to create. This is indeed consistent 
with our experiences with UIMA. While this is 
generally a favorable approach for system devel-
opment and deployment, it presents challenges to 
the developer that wants to isolate specific compo-
nents (or classes that support them) for unit or 
functional testing purposes. 
2 Testing UIMA Components 
UIMA coordinates data generated and consumed 
by different components using a data structure 
called the Common Analysis Structure (CAS). The 
1
CAS represents the current state of analysis that 
has been performed on the data being analyzed. As 
a simple example, a UIMA component that per-
forms tokenization on text would add token anno-
tations to the CAS. A subsequent component such 
as a part-of-speech tagger would read the token 
annotations from the CAS and update them with 
part-of-speech labels. We have found that many of 
our tests involve making assertions on the contents 
of the CAS after a component or series of compo-
nents has been executed for a given set of configu-
ration parameters and input data. As such, the test 
must obtain an instance of a CAS after it has been 
passed through the components relevant to the 
tests.  
For very simple scenarios a single descriptor file 
can be written which specifies all the configuration 
parameters necessary to instantiate a UIMA com-
ponent, create a CAS instance, and process the 
CAS with the component. Creating and processing 
a CAS from such a descriptor file takes 5-10 lines 
of Java code, plus 30-50 lines of XML for the de-
scriptor file. This is not a large overhead if there is 
a single test per component, however, testing a 
variety of parameter settings for each component 
results in a proliferation of descriptor files. These 
descriptor files can be difficult to maintain in an 
evolving codebase because they are tightly coupled 
with the Java components they describe, yet most 
code refactoring tools fail to update the XML de-
scriptor when they modify the Java code. As a re-
sult, the test suite can become unreliable unless 
substantial manual effort is applied to maintain the 
descriptor files. 
Thus, for ease of refactoring and to minimize the 
number of additional files required, it made sense 
to put most of the testing code in Java instead of 
XML. But the UIMA framework does not make it 
easy to instantiate components or create CAS ob-
jects without an XML descriptor, so even for rela-
tively simple scenarios we found ourselves writing 
dozens of lines of setup code before we could even 
start to make assertions about the expected con-
tents of a CAS. Fortunately, much of this code was 
similar across test cases, so as the ClearTK test 
suite grew, we consolidated the common testing 
code. The end result was a number of utility 
classes which allow UIMA components to be in-
stantiated and run over CAS objects in just 5-10 
lines of Java code. We decided that these utilities 
could also ease testing for projects other than 
ClearTK, so we created the UUTUC project, which 
provides our UIMA unit test utility code. 
3 UUTUC  
UUTUC2 provides a number of convenience 
classes for instantiating, running, and testing 
UIMA components without the overhead of the 
typical UIMA processing pipeline and without the 
need to provide XML descriptor files. 
Note that UUTUC cannot isolate components 
entirely from UIMA ? it is still necessary, for ex-
ample, to create AnalysisEngine objects, JCas ob-
jects, Annotation objects, etc. Even if it were 
possible to isolate components entirely from 
UIMA, this would generally be undesirable as it 
would result in testing components in a different 
environment from that of their expected runtime. 
Instead, UUTUC makes it easier to create UIMA 
objects entirely in Java code, without having to 
create the various XML descriptor files that are 
usually required by UIMA.  
Figure 1 provides a complete code listing for a 
test of a UIMA component we wrote that provides 
a simple wrapper around the widely used Snowball 
stemmer3. A complete understanding of this code 
would require detailed UIMA background that is 
outside the scope this paper. In short, however, the 
code creates a UIMA component from the Snow-
ballStemmer class, fills a CAS with text and to-
kens, processes this CAS with the stemmer, and 
checks that the tokens were stemmed as expected. 
Here are some of the highlights of how UUTUC 
made this easier: 
Line 3 uses TypeSystemDescriptionFactory 
to create a TypeSystemDescription from the 
user-defined annotation classes Token and Sen-
tence. Without this factory, a 10 line XML de-
scriptor would have been required. 
Line 5 uses AnalysisEngineFactory to create 
an AnalysisEngine component from the user-
defined annotator class SnowballStemmer and 
the type system description, setting the stemmer 
name parameter to "English". Without this 
factory, a 40-50 line XML descriptor would 
have been required (and near duplicate descrip-
                                                          
2 http://uutuc.googlecode.com ? provided under BSD license 
3 http://snowball.tartarus.org 
2
tor files would have been required for each ad-
ditional parameter setting tested). 
Line 11 uses TokenFactory to set the text of 
the CAS object and to populate it with Token 
and Sentence annotations. Creating these anno-
tations and adding them to the CAS manually 
would have taken about 20 lines of Java code, 
including many character offsets that would 
have to be manually adjusted any time the test 
case was changed. 
While a Python programmer might not be im-
pressed with the brevity of this code, anyone who 
has written Java test code for UIMA components 
will appreciate the simplicity of this test over an 
approach that does not make use of the UUTUC 
utility classes. 
4 Results  
The test suite we created for ClearTK was built 
using UUTUC and JUnit version 44 and consists of 
92 class definitions (i.e. files that end in .java) con-
taining 258 tests (i.e. methods with the marked 
with the annotation @Test). These tests contain a 
total of 1,943 individual assertions. To measure 
code coverage of our unit tests we use EclEmma5, 
a lightweight analysis tool available for the Eclipse 
development environment, which counts the num-
ber of lines that are executed (or not) when a suite 
of unit tests are executed. While this approach pro-
                                                          
4 http://junit.org 
5 http://www.eclemma.org 
vides only a rough approximation of how well the 
unit tests ?cover? the source code, we have found 
anecdotally that code with higher coverage re-
ported by EclEmma proves to be more reliable and 
easier to maintain. Overall, our test suite provides 
74.3% code coverage of ClearTK (5,391 lines cov-
ered out of 7,252) after factoring out automatically 
generated code created by JCasGen.  Much of the 
uncovered code corresponds to the blocks catching 
rare exceptions. While it is important to test that 
code throws exceptions when it is expected to, 
forcing test code to throw all exceptions that are 
explicitly caught can be tedious and sometimes 
technically quite difficult.  
5 Discussion  
We learned several lessons while building our test 
suite. We started writing tests using Groovy, a dy-
namic language for the Java Virtual Machine. The 
hope was to simplify testing by using a less ver-
bose language than Java. While Groovy provides a 
great syntax for creating tests that are much less 
verbose, we found that creating and maintaining 
these unit tests was cumbersome using the Eclipse 
plug-in that was available at the time (Summer 
2007). In particular, refactoring tasks such as 
changing class names or method names would suc-
ceed in the Java code, but the Groovy test code 
would not be updated, a similar problem to that of 
UIMA?s XML descriptor files. We also found that 
Eclipse became less responsive because user ac-
tions would often wait for the Groovy compiler to 
 1 @Test 
 2 public void testSimple() throws UIMAException { 
 3     TypeSystemDescription typeSystemDescription = TypeSystemDescriptionFactory 
 4         .createTypeSystemDescription(Token.class, Sentence.class); 
 5     AnalysisEngine engine = AnalysisEngineFactory.createAnalysisEngine( 
 6         SnowballStemmer.class, typeSystemDescription, 
 7         SnowballStemmer.PARAM_STEMMER_NAME, "English"); 
 8     JCas jCas = engine.newJCas(); 
 9     String text =   "The brown foxes jumped quickly over the lazy dog."; 
10     String tokens = "The brown foxes jumped quickly over the lazy dog ."; 
11     TokenFactory.createTokens(jCas, text, Token.class, Sentence.class, tokens); 
12     engine.process(jCas); 
13     List<String> actual = new ArrayList<String>(); 
14     for (Token token: AnnotationRetrieval.getAnnotations(jCas, Token.class)) { 
15         actual.add(token.getStem()); 
16     } 
17     String expected = "the brown fox jump quick over the lazi dog ."; 
18     Assert.assertEquals(Arrays.asList(expected.split(" ")), actual); 
19 } 
Figure 1: A complete test case using UUTUC. 
3
complete. Additionally, Groovy tests involving 
Java?s Generics would sometimes work on one 
platform (Windows) and fail on another (Linux or 
Mac). For these reasons we abandoned using 
Groovy and converted our tests to Java. It should 
be noted that the authors are novice users of 
Groovy and that Groovy (and the Eclipse Groovy 
plug-in) may have matured significantly in the in-
tervening two years.  
Another challenge we confronted while building 
our test suite was the use of licensed data. For ex-
ample, ClearTK contains a component for reading 
and parsing PennTreebank formatted data. One of 
our tests reads in and parses the entire PennTree-
bank corpus, but since we do not have the rights to 
redistribute the PennTreeBank, we could not in-
clude this test as part of the test suite distributed 
with ClearTK. So as not to lose this valuable test, 
we created a sibling project of ClearTK which is 
not publicly available, but from which we could 
run tests on ClearTK. This sibling project now 
contains all of our unit tests which use data we 
cannot distribute. We are considering making this 
project available separately for those who have 
access to the relevant data sets.  
We have begun to compile a growing list of best 
practices for our test suite. These include: 
Reuse JCas objects. In UIMA, creating a JCas 
object is expensive. Instead of creating a new 
JCas object for each test, a single JCas object 
should be reused for many tests where possible. 
Refer to descriptors by name, not location. 
UIMA allows descriptors to be located by either 
?location? (a file system path) or ?name? (a Ja-
va-style dotted package name). Descriptors re-
ferred to by ?name? can be found in a .jar file, 
while descriptors referred to by ?location? can-
not. This applies to imports of both type system 
descriptions (e.g. in component descriptors) and 
to imports of CAS processors (e.g. in collection 
processing engine descriptors). 
Test loading of descriptor files. As discussed, 
XML descriptor files can become stale in an 
evolving codebase. Simply loading each de-
scriptor in UIMA and verifying that the para-
meters are as expected is often enough to keep 
the descriptor files working if the actual com-
ponent code is being properly checked through 
other tests. 
Test copyright and license statements. We 
found it useful to add unit tests that search 
through our source files (both Java code and 
descriptor files) and verify that appropriate 
copyright and license statements are present. 
Such statements were a requirement of the 
technology transfer office we were working 
with, and were often accidentally omitted when 
new source files were added to ClearTK. Add-
ing a unit test to check for this meant that we 
caught such omissions much earlier. 
As ClearTK has grown in size and complexity its 
test suite has proven many times over to be a vital 
instrument in detecting bugs introduced by extend-
ing or refactoring existing code. We have found 
that the code in UUTUC has greatly decreased the 
burden of maintaining and extending this test suite, 
and so we have made it available for others to use. 
References  
Philip V. Ogren, Philipp G. Wetzler, and Steven Be-
thard. 2008. ClearTK: a UIMA toolkit for statistical 
natural language processing. In UIMA for NLP 
workshop at LREC. 
David Ferrucci and Adam Lally. 2004. UIMA: an archi-
tectural approach to unstructured information 
processing in the corporate research environment. 
Natural Language Engineering, 10(3-4):327?348. 
4
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 9?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Topic Model Analysis of Metaphor Frequency for Psycholinguistic Stimuli
Steven Bethard
Computer Science Department
Stanford University
Stanford, CA 94305
bethard@stanford.edu
Vicky Tzuyin Lai
Department of Linguistics
University of Colorado
295 UCB, Boulder CO 80309
vicky.lai@colorado.edu
James H. Martin
Department of Computer Science
University of Colorado
430 UCB, Boulder CO 80309
james.martin@colorado.edu
Abstract
Psycholinguistic studies of metaphor process-
ing must control their stimuli not just for
word frequency but also for the frequency
with which a term is used metaphorically.
Thus, we consider the task of metaphor fre-
quency estimation, which predicts how often
target words will be used metaphorically. We
develop metaphor classifiers which represent
metaphorical domains through Latent Dirich-
let Allocation, and apply these classifiers to
the target words, aggregating their decisions to
estimate the metaphorical frequencies. Train-
ing on only 400 sentences, our models are able
to achieve 61.3% accuracy on metaphor clas-
sification and 77.8% accuracy on HIGH vs.
LOW metaphorical frequency estimation.
1 Introduction
Psycholinguistic studies of metaphor try to under-
stand metaphorical language comprehension by pre-
senting subjects with linguistic stimuli and observ-
ing their responses. Recent work has observed such
responses at the electrophysiological level, measur-
ing brain electrical activity as the stimuli are read
(Coulson and Petten, 2002; Tartter et al, 2002; Iaki-
mova et al, 2005; Arzouan et al, 2007; Lai et al,
2007). All these studies have attempted to make
comparisons across different types of stimuli (e.g.
literal vs. metaphorical) by holding the frequen-
cies of the target words constant across experimental
conditions. For example, Tartter et al (2002) com-
pared the metaphorical and literal sentences his face
was contorted by an angry cloud and his face was
contorted by an angry frown, where the two sen-
tences end in different words, but where the final
words cloud and frown had similar word frequen-
cies. As another example, Lai et al (2007) com-
pared the metaphorical and literal sentences Their
theories have collapsed and The old building has
collapsed, where the two sentences end in exactly
the same words, so the target word frequencies
across conditions were perfectly matched. In both
designs, controlling for word frequency allowed the
researchers to attribute the differences in experimen-
tal conditions to interesting factors, like figurativity,
rather than simple word frequency.
However, word frequency is not the only type of
frequency relevant to such experiments. In particu-
lar, metaphorical frequency, that is, how inherently
metaphorical one word is as compared to another,
may also play an important role in explaining the
psycholinguistic results. For example, if collapsed
is usually used literally, a greater processing effort
may be observed when a metaphorical instance of
collapsed is presented. Likewise, if collapsed is
usually used metaphorically, greater effort may be
observed when a literal instance is presented. Psy-
cholinguistic studies of metaphor have not, to date,
controlled for such metaphorical frequency because
there were no corpora or algorithms which could
provide the needed metaphorical frequencies.
The present study aims to address this deficiency
by producing models which can automatically esti-
mate how often a word is used metaphorically. We
build these models using only 50 examples each of
a small number of target words (< 10), rather than
requiring 50 or more examples of every target word
9
(100+) in the stimuli, as would be required by stan-
dard corpus linguistics methods. Our approach is
also novel in that it combines metaphor classifica-
tion with statistical topic models. Topic models are
intuitively promising for our task because they pro-
duce topics that seem to translate well to the theory
of conceptual domains, which suggests that, for ex-
ample, conceptual domains such as THEORIES and
BUILDINGS are used to understand Their theories
have collapsed. These topic models also show some
promise for distinguishing conventional metaphors
from novel metaphors.
2 Prior Work
Two types of prior research inform our current
study: corpus analyses investigating metaphor fre-
quency by hand, and machine learning models that
classify text as either literal or metaphorical. The
latter could be used to estimate metaphor frequen-
cies by applying the classifier to a corpus and aggre-
gating the classifications.
2.1 Metaphor Frequency
Researchers have manually estimated several differ-
ent kinds of metaphor frequency. Pollio et al (1990)
looked at overall metaphorical frequency, perform-
ing an exhaustive analysis of a variety of texts, and
concluding that there were about five metaphors for
every 100 words of text. Martin (1994) looked at
the frequency of different types of metaphor, us-
ing a sample of 600 sentences from the Wall Street
Journal (WSJ), and concluded among other things
that the most frequent type of WSJ metaphor was
VALUE is LOCATION, e.g. Spain Fund tumbled
23%. Martin (2006) looked at conditional probabil-
ities of metaphor, for example noting that in 2400
WSJ sentences, the probability of seeing an instance
of a metaphor was greatly increased after a first in-
stance had already been observed. However, none of
these studies provided the metaphorical frequencies
of individual words needed for our research.
Sardinha (2008) performed what is probably clos-
est to the type of analysis we are interested in.
Using a corpus of Portuguese conference calls,
Berber Sardinha identified 432 terms that were used
metaphorically. He then took 100 instances of each
of these terms in a general Brazilian corpus and
manually annotated them as being either literal or
metaphorical. Berber Sardinha found that on aver-
age these terms were used metaphorically 70% of
the time, and provided analysis of the metaphor-
ical frequencies of a number of individual terms.
While it is exactly these kinds of individual term
frequencies that we are after, we cannot use Berber
Sardinha?s data because his corpus was in Por-
tuguese while we are interested in English. This
brings out one of the main drawbacks of the corpus
annotation approach: moving to a new language (or
even a new genre) requires an extensive manual an-
notation project. Our goal is to avoid such costs by
taking advantage of machine learning techniques for
automatically identifying metaphorical text.
2.2 Metaphor Classification
Recent years have seen a rising interest in metaphor
classification systems. Birke and Sarkar (2006) took
a semi-supervised approach, collecting noisy exam-
ples of literal and non-literal sentences from both
WordNet and metaphor dictionaries, and using a
word-based measure of sentence similarity to group
sentences into literal and non-literal clusters. They
evaluated on hand-annotated sentences for 25 target
words and reported an F-score of 0.538, a substantial
improvement over the 0.294 majority class baseline.
Gedigian et al (2006) approached metaphor
identification as supervised classification, annotat-
ing around 4000 WSJ motion words as literal or
metaphorical, and training a maximum entropy clas-
sifier using as features based on named entities,
WordNet and semantic roles. They achieved an ac-
curacy of 95.1%, a decent improvement over the
very high majority class baseline of 93.8%.
Krishnakumaran and Zhu (2007) focused on three
syntactically constrained sub-types of metaphors:
nouns joined by be, nouns following verbs, and
nouns following adjectives. They combined Word-
Net hypernym information with bigram statistics
and a threshold, and evaluated their algorithm on
the Berkeley Master Metaphor List (Lakoff, 1994),
achieving an accuracy of around 46%.
All of these approaches produced models which
could be applied to new text to identify metaphors,
but each has some drawbacks for our task. The
WSJ study of Gedigian et al (2006) found 94% of
their target words to be metaphorical, a vastly differ-
10
Target L M M%
attacked 32 18 36%
born 45 5 10%
budding 16 34 68%
collapsed 10 40 80%
digest 7 43 86%
drifted 16 34 68%
floating 25 25 50%
sank 31 19 38%
spoke 47 3 6%
Total 229 221 49%
Table 1: Metaphorical (M) and literal (L) counts, and
metaphorical percentage (M%), for the annotated verbs.
ent number from the 49% for our target words (see
Section 3). Krishnakumaran and Zhu (2007) con-
sidered only a few different syntactic constructions,
but we need to consider all the ways a metaphor
may be expressed to evaulate overall metaphor fre-
quency. Birke and Sarkar (2006) did consider a va-
riety of target words in unrestricted text, but relied
on large scale language resources like WordNet and
metaphor dictionaries, while we are interested in ap-
proaches that are less resource intensive.
Thus, rather than basing our models on these prior
systems, we develop a novel approach to metaphor
frequency estimation based on using topic models to
operationalize metaphorical domains.
3 Data
The first step in building models of metaphorical
frequency is obtaining data for training and evalu-
ation. In one of the post-hoc analyses of the Lai et
al. (2007) experiment, 50 sentences from the British
National Corpus (BNC, 2007) were gathered for
each of nine of their target words. They annotated
each instance as either literal or metaphorical, and
then used these annotations to calculate metaphori-
cal frequencies for analysis.
This data served as our starting point for exploring
computational approaches to estimating metaphor-
ical frequency. Table 1 shows the nine verbs and
their metaphorical frequencies. Table 2 shows some
examples. Some verbs, such as digest, are almost al-
ways used metaphorically (86% of the time), while
other verbs, such as spoke, are almost always used
L Aye, that?s where I was born and reared.
M VATman threatens our budding entrepreneurs.
M Suddenly all her bravado collapsed.
L This makes it easier for us to digest the wheat.
L Gulls drifted lethargically on the swell.
M My heart sank as I looked around.
Table 2: Examples of sentences with metaphorical (M)
and literal (L) target words.
T# Most frequent words
00 book (4%) write (2%) read (2%) english (2%)
17 record (3%) music (2%) band (2%) play (2%)
42 social (3%) history (2%) culture (1%) society (1%)
58 film (3%) play (2%) theatre (1%) women (1%)
82 dog (9%) rabbit (2%) ferret (1%) pet (1%)
Table 3: Example topics (T#) from the BNC and their
most frequent words. Numbers in parentheses indicate
the percent of the topic each word represents.
literally (94% of the time). Annotation of just 50
instances of each of these nine verbs was time con-
suming, and yet to fully re-analyze the ERP results,
metaphorical frequencies would be needed for all of
the over 100 target words. Thus our goal was to au-
tomate this process.
4 Topic Models
Our approach to estimating metaphorical frequen-
cies was first to classify words in unrestricted text
as literal or metaphorical, and then to aggregate
those decisions to estimate a frequency. Thus, we
first needed to build a model which could iden-
tify metaphorical expressions. Our approach to this
problem was based on the theory of conceptual do-
mains, in which metaphors are seen as taking terms
from one domain (e.g. attacked) and applying them
to another domain (e.g. argument).
To operationalize these domains, we employed
statistical topic models, in particular, Latent Dirich-
let Allocation (LDA) (Blei et al, 2003). Intuitively,
LDA looks at how words co-occur in the documents
of a large corpus, and identifies topics or groups of
words that are semantically similar. For example,
Table 3 shows a few topics from the BNC. These
topics can be thought of as grouping words by their
semantic domains. For example, we might think of
topic 00 as the Book domain and topic 42 as the Soci-
ety domain. Because LDA generates topics that look
11
much like the source and target domains associated
with metaphors, we expect that LDA can provide a
boost to metaphor identification models.
The LDA algorithm is usually presented as a gen-
erative model, that is, as an imagined process that
someone might go through when writing a text. This
generative process looks something like:
1. Decide what topics you want to write about.
2. Pick one of those topics.
3. Think of words used to discuss that topic.
4. Pick one of those words.
5. To generate the next word, go back to 2.
This is a somewhat unrealistic description of the
writing process, but it gets at the idea that the words
in a document are topically coherent. Formally, the
process above can be described as:
1. For each document d select a topic distribution
?d ? Dir(?)
2. Select a topic z ? ?d
3. For each topic select a word distribution
?z ? Dir(?)
4. Select a word w ? ?z
The goal of the LDA learning algorithm then is to
maximize the likelihood of our documents, where
for one document p(d|?, ?) = ?Ni=1 p(wi|?, ?). Es-
timating these probabilities can be done in a few dif-
ferent ways, but in this paper we use Gibbs sampling
as it has been widely implemented and was available
in the LingPipe toolkit (Alias-i, 2008).
Gibbs sampling starts by randomly assigning top-
ics to all words in the corpus. Then the word-topic
distributions and document-topic distributions are
estimated using the following equations:
P (zi|zi?, wi, di, wi?, di?, ?, ?) = ?ij?jd?T
t=1 ?it?td
?ij = Cwordij+??W
k=1 Cwordkj+W?
?jd = Cdocdj+??T
k=1 Cdocdk+T?
Cwordij is the number of times word i was assigned
topic j, Cdocdj is the number of times topic j ap-
pears in document d, W is the total number of
unique words in the corpus, and T is the number
of topics requested. In essence, we count the num-
ber of times that a word is assigned a topic and
the number of times a topic appears in a document,
and we use these numbers to estimate word-topic
and document-topic probabilities. Once topics have
been assigned and distributions have been calcu-
lated, Gibbs sampling repeats the process, this time
selecting a new topic for each word by looking at
the calculated probabilities. The process is repeated
until the distributions become stable or a set number
of iterations is reached.
We ran LDA over the documents in the BNC, ex-
tracting 100 topics after 2000 iterations of Gibbs
sampling. We left the ? and ? parameters at their
LingPipe defaults of 0.1 and 0.01, respectively. Ta-
ble 3 shows some of the resulting topics.
5 Metaphor Frequency
Our primary goal was to use the topics produced by
LDA to help characterize words in terms of their
metaphorical frequency. We approached this prob-
lem by first training metaphor classifiers based on
LDA topics to identify target words in text as lit-
eral or metaphorical. Then we ran these classifiers
over unseen data, and aggregated the individual de-
cisions. The result is an approximate metaphorical
frequency for each word. The following sections de-
tail this process and discuss our preliminary results.
5.1 Metaphor Classification
Our data is composed of 50 sentences for each of
nine target words, with each sentence annotated as
either metaphorical or literal. We treated this as a
classification task, where the classifier took as input
a sentence containing a target word, and produced as
output either LITERAL or METAPHORICAL.
We trained support vector machine (SVM) clas-
sifiers on this data, using LDA topics as features.
For each of the sentences in our data, we used the
LDA topic models to assign topic probability distri-
butions to each of the words in the sentence. We then
summed the topic distributions over all the words in
the sentence to produce a sentence-wide topic dis-
tribution. The result was that for each sentence we
could say something like ?this sentence was com-
posed of 5% topic 00, 2% topic 01, 8% topic 02,
etc.? We used these sentence-level topic probabil-
ity distributions as features for an SVM classifier, in
particular, SVMperf (Joachims, 2005).
We compared this SVM-LDA model against two
baselines. The first was the standard majority class
12
classifier, which simply assigns all instances in the
test data whichever label (metaphorical or literal)
was most comon in the training data.
The second baseline was an SVM based on TF-
IDF features, a well known document classification
model (Joachims, 1998; Sebastiani, 2002; Lewis et
al., 2004). Under this approach, there is a numeric
feature for each of the 3000+ words in the training
data, and each word feature is assigned the weight:
|{w ? doc : w = word}|
|{w ? doc}| ?log
|{d ? docs}|
|{d ? docs : w ? d}|
Essentially, this formula means that the weight in-
creases with the number of times the word occurs
in the document, and decreases with the number of
documents in the corpus that contain that word. The
vectors of TF-IDF features are then normalized to
have Euclidean length 1.0, using the formula:
weight(word) = tf-idf(word)? ?
word?
tf-idf(word?)2
To evaluate our model against both the majority
class and the TF-IDF baselines, we ran nine-fold
cross-validations, where each fold corresponded to
a single target word. Note that this means that we
trained our models on the sentences of eight target
words, and tested on the sentences of the ninth tar-
get word. This is a harder evaluation than a strat-
ified cross-validation where all target words would
have been observed during training. But it is a much
more realistic evaluation for our task, where we want
to learn enough about metaphors from nine target
words that we can automatically classify instances
of the remaining 95.
Table 4 compares the performance of our SVM-
LDA model and the baseline models1. The major-
ity class classifier performs poorly, achieving only
26.4% accuracy2. The TF-IDF based model per-
forms much better, at 50.7% accuracy. However, our
SVM based on LDA features outperforms both base-
line models, achieving 54.9% accuracy.
1For all models, hyper parameters (the cost parameter, the
loss function, etc.) were set using only the training data of each
fold by running an inner eight-fold cross validation.
2This might be initially surprising since our corpus was 49%
metaphorical. Consider, however, that during cross validation,
holding out a more metaphorical target word for testing means
that our training data is more literal, and vice versa.
Model Accuracy
Majority Class 26.4%
SVM + TF-IDF 50.7%
SVM + LDA topics 54.9%
SVM + LDA topics + LDA groups 61.3%
Table 4: Model performance on the literal vs. metaphor-
ical classification task.
Type Most frequent words
CONCRETE book write read english novel
ABSTRACT god church christian jesus spirit
MIXED sleep dream earth theory moon
OTHER many time only number large
Table 5: Examples of annotated topics.
5.2 Annotating Topics
The metaphor classification results showed the ben-
efit of operationalizing metaphor domains as LDA
topics. But metaphors are typically viewed as map-
ping a concrete source domain onto an abstract tar-
get domain, and our LDA topics had no direct notion
of this concrete/abstract distinction. To try to repre-
sent this distinction, we manually annotated3 the 100
LDA topics with one of four labels: CONCRETE,
ABSTRACT, MIXED or OTHER. Table 5 shows ex-
amples of the annotated topics.
We then used the annotated topics to generate new
features for our classifiers. In addition to the original
100 topic probability features, we provided four new
probability features, one for each of our labels, cal-
culated by taking the sum of the probabilities of the
corresponding topics. For example, since topics 07,
13, 37 and 77 were identified as ABSTRACT topics,
the probability of the new ABSTRACT feature was
just the sum of the probabilities of the topic features
07, 13, 37 and 77. The last row of Table 4 shows
the performance of the SVM model trained with the
augmented feature set. This model outperforms all
our other models, achieving an accuracy of 61.3%
on the literal vs. metaphorical distinction.
These results are interesting because they show
that human analysis of LDA topics can add substan-
tial value for machine learning models at a low cost.
Annotating the entire set of 100 topics took under
3All annotation was performed by a single annotator. Future
work will measure inter-annotator agreement.
13
Model Accuracy
Majority Class 0.0%
SVM + TF-IDF 22.2%
SVM + LDA topics 55.6%
SVM + LDA topics + LDA groups 77.8%
Table 6: Model performance on the HIGH vs. LOW
metaphor frequency prediction task.
an hour, and yet provided a 6% gain in model ac-
curacy. The speed of annotation suggests that LDA
topics are conceptually accessible to humans, and
the performance boost suggests that manual group-
ing of LDA topics may be a fruitful area for feature
engineering.
5.3 Predicting Metaphorical Frequencies
Having constructed successful metaphor classifica-
tion models, we return to our question of metaphor-
ical frequency. Given a target word, can we pre-
dict the frequency with which that word will be
used metaphorically? Our models are not accurate
enough that we can expect the frequencies derived
from them to be exact predictions of metaphorical
frequency. But we may be able to distinguish, for
example, words with high metaphorical frequency
from words with low metaphorical frequency.
Thus, we evaluate our models on the binary task
of assigning target words an overall metaporical fre-
quency, either HIGH (? 50%) or LOW (< 50%). We
can perform this evaluation using the same data and
cross validation technique as before, this time exam-
ining each testing fold (which corresponds to a sin-
gle target word) and aggregating the metaphor clas-
sifications to get a metaphorical frequency estimate
of that target. Table 6 shows how the models fared
on this task. The majority class model misclassified
all the words, and the TF-IDF model managed to get
only two of the nine correct. The LDA models per-
formed better, with the model including the grouped
topic features achieving 77.8% accuracy. This sug-
gests that our model may already be good enough
to use for analysis of the original Lai experimental
data. Of course, this evaluation was carried out only
over the nine available target words, so additional
evaluation will be necessary to confirm these trends.
To further analyze our model performance, we
looked at the metaphorical frequency estimates for
Word True Predicted Difference
attacked 36% 24% -12%
born 10% 2% -8%
budding 68% 98% +30%
collapsed 80% 98% +18%
digest 86% 40% -46%
drifted 68% 92% +24%
floating 50% 100% +50%
sank 38% 26% -12%
spoke 6% 62% +56%
Table 7: Model performance on the HIGH vs. LOW
metaphor frequency prediction task.
each target word. Table 7 shows the estimates of
our best model along with the true metaphorical fre-
quencies. The three target words with the largest dif-
ferences between true and predicted accuracies are
spoke, floating and digest, with spoke and floating
predicted to be much more metaphorical than they
actually are, and digest predicted to be much less.
We also performed some analysis of the model er-
rors. In many cases it was difficult to judge why the
model succeeded or failed in identifying a metaphor,
but a couple of things stood out. First, 70% of the
digest instances our model misclassified were Di-
gest (capitalized), e.g. Middle East Economic Di-
gest. Our topic models were trained on all lower-
cased words, so Digest and digest were not distin-
guished. Re-training the models without collaps-
ing the case distinctions might address this prob-
lem. Second, spoke seems to be an inherently harder
term to classify because it co-occurs with so many
other topics. About 40% of the spoke instances oc-
curred as spoke of or spoke about, where speaking
about a metaphorical topic caused spoke to be inter-
preted metaphorically, and speaking about a literal
topic caused spoke to be interpreted literally. Ad-
dressing this problem would probably require some
understanding of argument structure, perhaps akin
to what was done by Gedigian et al (2006).
6 Metaphor Novelty
As a final exploration of topic models for metaphor-
ical domains, we considered metaphorical novelty,
as used in the original Lai experiment. In particular,
we were interested in how LDA topics might reflect
14
Type Stimulus Sentence
LIT Every soldier in the frontline was attacked
CON Every point in my argument was attacked
NOV Every second of our time was attacked
ANOM Every drop of rain was attacked
LIT The old building has collapsed
CON Their theories have collapsed
NOV Their compromises have collapsed
ANOM The apples have collapsed
Table 8: Example stimuli: literal (LIT), conventional
metaphor (CON), novel metaphor (NOV) and anomalous
(ANOM).
more conventional or more novel metaphors. In the
Lai experiment, conventional and novel metaphors
for a particular target word shared the same source
domain (e.g. WAR) but differed in the target domain
(e.g. ARGUMENT vs. TIME). If LDA topics are
a good operationalization of such domains, then it
should be possible use LDA topics to distinguish be-
tween conventional and novel metaphors.
To explore this area, we employed the stimuli
from the Lai experiment, and looked in particular
at the conventional and novel conditions. The Lai
experiment used 104 different target words, so these
data included 104 conventional metaphors and 104
novel metaphors. Novel metaphors were generated
for the Lai experiment by considering a conventional
source-target mapping and selecting a new target
domain. For example, the conventional metaphor
Every point in my argument was attacked maps
the source domain WAR to the target domain AR-
GUMENT, while the novel metaphor Every second
of our time was attacked maps the source domain
WAR to the target domain TIME. Table 8 shows ex-
ample stimulus sentences from the Lai experiment.
Though these experimental stimuli have the draw-
back of being manually constructed, not collected
from a corpus, they have the advantage of being
already annotated with a definition of novelty that
clearly distinguishes the two types of metaphors.
We performed a simple correlational analysis us-
ing the conventional and novel metaphors from the
Lai experiment. We produced topic distributions for
each stimulus, using our topic models trained on the
BNC. We then labeled conventional metaphors as -1
and novel metaphors as +1, and identified the top-
-0.19 like house old shop door look street room
-0.18 darlington programme club said durham hall
-0.15 film play theatre women actor work perform
-0.14 area local plan develop land house rural urban
-0.14 any sale good publish custom product price
Table 9: Top 5 topics correlated with conventionality.
0.20 freud sexual sophie male joanna people female
0.17 doctor leed rory dalek fergus date subject aug
0.13 book write read english novel publish reader
0.11 lorton kirov dougal jed manville vologski celia
0.09 war british france britain french nation europe
Table 10: Top 5 topics correlated with novelty.
ics that correlated best with this distinction. Table 9
shows the most negatively correlated (conventional)
topics and Table 10 shows the most positively corre-
lated (novel) topics.
Though even the best correlations are somewhat
low, there seem to be some trends in this analysis.
Conventional metaphors seem to correspond more
to concrete terms, like house, club, play and sale.
Novel metaphors have less of a coherent theme, in-
cluding terms like freud and sexual as well as names
like Rory, Kirov and Britain. This may reflect a
real distinction in the use of conventional and novel
metaphors, or it may be an artifact of how the exper-
imental stimuli were created. A deeper investigation
into the relations between LDA topics and metaphor
novelty will probably require annotating sentences
from some naturally occuring data.
7 Conclusions
We presented a novel two-phase approach to the task
of metaphorical frequency estimation. First, exam-
ples of a target word were automatically classified
as literal or metaphorical, and then these classifi-
cations were aggregated to estimate how often the
target word was used metaphorically. Our classi-
fiers operationalized metaphorical source and target
domains using topics derived from Latent Dirichlet
Allocation. Support vector machine classifiers took
these topic probability distributions and learned to
classify sentences as literal or metaphorical. These
models achieved 61.3% accuracy on the classifiation
task, and their aggregated classifications produced
an accuracy of 77.8% on the task of distinguishing
15
between target words with high and low metaphori-
cal frequencies.
Future work will perform a larger scale eval-
uation, and will use our model?s metaphorical
frequency estimates to analyze psycholinguistic
data. In particular, we will split the conventional
metaphorical sentences of Lai et al (2007) into
low and high-frequency items. If the low and
high frequency items display significantly differ-
ent brainwave patterns, then this could suggest that
metaphorical frequency of a given word plays a crit-
ical role in metaphor comprehension.
Future work will also explore frequency effects
that consider the sentential context in the stimulus
items. For example, a context like ?Their theories
have ? probably gives a higher expectation of a
metaphorical word filling in the blank than a context
like ?The old building has ?. Having a measure
of how much the words in the preceding context pre-
dict an upcoming metaphor would provide another
useful stimulus control.
References
Alias-i. 2008. LingPipe 3.7.0. http://alias-
i.com/lingpipe/, October.
Yossi Arzouan, Abraham Goldstein, and Miriam Faust.
2007. Brainwaves are stethoscopes: ERP correlates
of novel metaphor comprehension. Brain Research,
1160:69?81, July.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In European Chapter of the ACL
(EACL).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
BNC. 2007. The british national corpus, version 3
(BNC XML edition). Distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium. http://www.natcorp.ox.ac.uk/.
Seana Coulson and Cyma Van Petten. 2002. Conceptual
integration and metaphor: an event-related potential
study. Memory & Cognition, 30(6):958?68, Septem-
ber. PMID: 12450098.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Workshop
On Scalable Natural Language Understanding.
Galina Iakimova, Christine Passerieux, Jean-Paul Lau-
rent, and Marie-Christine Hardy-Bayle. 2005.
ERPs of metaphoric, literal, and incongruous seman-
tic processing in schizophrenia. Psychophysiology,
42(4):380?390.
Thorsten Joachims, 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features, pages 137?142. Springer Berlin / Heidel-
berg.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd international conference on Machine learn-
ing, pages 377?384, Bonn, Germany. ACM.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Workshop on Computational Approaches to Figurative
Language.
Vicky Tzuyin Lai, Tim Curran, and Lise Menn.
2007. The comprehension of conventional and novel
metaphors: An ERP study. In 13th Annual Confer-
ence on Architectures and Mechanisms for Language
Processing, August.
George Lakoff. 1994. Conceptual metaphor WWW
server. http://cogsci.berkeley.edu/lakoff/.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for text
categorization research. J. Mach. Learn. Res., 5:361?
397.
James H. Martin. 1994. MetaBank: a Knowledge-Base
of metaphoric language conventions. Computational
Intelligence, 10(2):134?149.
James H. Martin. 2006. A rational analysis of the con-
text effect on metaphor processing. In Stefan Th. Gries
and Anatol Stefanowitsch, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy. Mouton de
Gruyter.
Howard R. Pollio, Michael K. Smith, and Marilyn R. Pol-
lio. 1990. Figurative language and cognitive psychol-
ogy. Language and Cognitive Processes, 5:141?167.
Tony Berber Sardinha. 2008. Metaphor probabilities
in corpora. In Mara Sofia Zanotto, Lynne Cameron,
and Marilda do Couto Cavalcanti, editors, Confronting
Metaphor in Use, pages 127?147. John Benjamins.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys
(CSUR), 34(1):1?47.
Vivien C. Tartter, Hilary Gomes, Boris Dubrovsky, So-
phie Molholm, and Rosemarie Vala Stewart. 2002.
Novel metaphors appear anomalous at least momen-
tarily: Evidence from N400. Brain and Language,
80(3):488?509, March.
16
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1228?1237, Dublin, Ireland, August 23-29 2014.
Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?
Upendra Sapkota and Thamar Solorio
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
{upendra,solorio}@cis.uab.edu
Manuel Montes-y-G
?
omez
Instituto Nacional de Astrof??sica
Optica y Electr?onica
Puebla, Mexico
mmontesg@ccc.inaoep.mx
Steven Bethard
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
bethard@cis.uab.edu
Paolo Rosso
NLE Lab - PRHLT Research Center
Universitat Polit`ecnica de Val`encia
Valencia, Spain
prosso@dsic.upv.es
Abstract
Most previous research on authorship attribution (AA) assumes that the training and test data
are drawn from same distribution. But in real scenarios, this assumption is too strong. The goal
of this study is to improve the prediction results in cross-topic AA (CTAA), where the training
data comes from one topic but the test data comes from another. Our proposed idea is to build
a predictive model for one topic using documents from all other available topics. In addition
to improving the performance of CTAA, we also make a thorough analysis of the sensitivity to
changes in topic of four most commonly used feature types in AA. We empirically illustrate that
our proposed framework is significantly better than the one trained on a single out-of-domain
topic and is as effective, in some cases, as same-topic setting.
1 Introduction
Authorship Attribution is the problem of identifying who, from a number of given candidate authors,
wrote the given piece of text. The authorship attribution task can be viewed as a multi-class single-label
text classification task where each author indicates a class. However, the purpose of AA is to model each
author?s writing style. AA methods have a wide range of applications, including Forensic Linguistics (spam
filtering (de Vel et al., 2001), verifying the authorship of threatening emails), cybercrimes (identifying
authors of malicious code and defending against pedophiles), and plagiarism detection (Stamatatos, 2011).
The AA methods can be useful in applied areas such as law and journalism where the identification
of the true author of a piece of text (such as a ransom note) may be able to save lives or help prosecute
offenders. One of the outstanding problems in AA studies is the unrealistic assumption that the samples of
both known and unknown authorship are drawn from the same distribution. This assumption considerably
simplifies the AA task but also limits the practical usability of the methods. In practical scenarios usually
the documents under investigation are from a different domain than that of the training documents. We
feel the need to advance the way AA methods are designed so that the bridge between domains will
be minimized to obtain the optimum performance. Therefore, we try to improve the performance of
cross-topic AA (CTAA), one of the dimensions of cross-domain AA (CDAA) where training and test data
come from different topics.
In this paper, we focus on one of the outstanding research questions on AA: Can we reliably predict
the author of a document written in one topic with a predictive model developed using documents from
other topics? We hypothesize that the addition of training data even if it comes from a topic different
than that of the test data improves cross-topic AA performance. To test the hypothesis, we compare
the performance of our proposed model trained on documents from all available out-of-topic data with
two models, one trained on single out-of-topic data and another trained on the same topic (intra-topic)
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1228
data. We also compare the performance of using four widely used features in AA to demonstrate their
discriminative power in intra-topic and cross-topic AA. The contributions of this study are as follows:
? We propose a new method to identify the author of a document on a topic using a predictive model
trained on examples from different topics. The successful results attained indicate that authors
maintain a consistent style across topics.
? This is the first comprehensive study showing empirically which widely used features in AA are
effective for cross-topic AA. We demonstrate that character n-grams are a strong discriminator among
authors in CTAA and that lexical features are less effective in CTAA than they are for intra-topic AA.
? We empirically illustrate that having the same amount of training documents from multiple topics is
significantly better than having documents from a single topic. It shows that topic variety in training
documents improves the performance of CTAA.
? We also demonstrate that across all genres, adding an extra topic to the training data gives a character
n-gram model a greater boost in performance than to a stop-word, a stylistic or a lexical model. This
is true regardless of the topics on which the model is trained.
? Our proposed methodology is simple to implement suggesting that our findings on cross-topic AA
will be generalizable to other classification problems too.
The paper is organized as follows. Section 2 describes two cross-topic datasets while Section 3 describes
the methodology for our experiments. Section 4 describes different features while Section 5 presents
the experimental setup. We present the evaluation and analysis in Sections 6 and 7. In Section 8, we
describe previous studies on cross-topic AA. Finally, Section 9 presents our conclusions and some future
directions.
2 Cross-Topic Datasets
Although several corpora are available for traditional AA, we need datasets containing documents from a
number of authors from different domains (different topics, different genres). We need many topics to be
able to test cross-topic performance, and many genres to ensure that our findings are robust across different
styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus
on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and
cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication
samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six
topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex
Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009).
Using this dataset, it is possible to see how the performance of cross-topic AA changes across different
genres.
Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors
in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments)
about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets.
Corpus #authors #genres #topics
avg avg avg
#docs/author #sentences/doc #words/doc
Dataset 1 21 6 6 36 31.7 600
Dataset 2 13 1 4 64 53 1034
Table 1: Some statistics about dataset 1 and dataset 2.
In dataset 1, the average document length is almost half the average document length in dataset 2, while
the number of authors is almost twice as that in dataset 2. Also, in dataset 1, there is only one document
written by an author on each topic on each genre. However, there are, on average, 16 documents per author
per topic on each genre in dataset 2. Overall, dataset 1 seems more challenging and resembles more a
realistic scenario of forensic investigations where very few short documents per author might be available.
1229
3 Methodology
To answer our research question and test our hypothesis, we designed three training scenarios. First
of all, to demonstrate the complexity of cross-topic tasks, we compare the performance between two
training conditions: Intra-Topic (IT), and Single Cross-Topic (SCT). Once we show that it is important to
solve this CTAA problem, we design one more training condition based on our proposed idea, Multiple
Cross-Topics (MCT) and compare its performance with the IT and the SCT scenarios.
Intra-Topic (IT) In this scenario, all the documents in both the training and test data belong to the same
topic. Although this is a strong assumption that does not hold true in most of the realistic scenarios, we
examine AA under such conditions in order to be able to compare it with our proposed methods.
Single Cross-Topic (SCT) In this setting, the test data consists of documents from a single topic while
the AA model is trained using documents belonging to another topic different than the topic of the test
data, but from the same genre. For example, in dataset 1, for ?Chat? genre, a model could be trained
on a topic ?Gay Marriage? and tested on the topic ?Legalization of Marijuana?. We experiment on all
combinations of test/train topics, i.e., for each test topic, we train separately on each of the remaining
topics.
Multiple Cross-Topics (MCT) Unlike in SCT and IT scenarios, here for each test topic, we train
on documents from all available topics other than the one used for testing. Our assumption is that
authors somehow maintain their unique writeprints across different topics. Therefore, even though the
additional data comes from a topic different than that of the test data, we expect to see improvements in
the performance of cross-topic AA.
In the SCT scenario, since there is a mismatch between the training and test topic, we expect to obtain
experimental results worst than that of the IT scenario. However, we expect that the performance of
cross-topic AA using our proposed MCT scenario will be better than SCT in all the cases.
4 Features
The choice of features depends greatly on the type of classification problem. Previous research has
explored various types of features that can discriminate among the candidate authors. Stylistic features,
character-level and word-level n-grams are the most frequently and successfully used features (Houvardas
and Stamatatos, 2006; Zheng et al., 2006; Frantzeskou et al., 2007; Abbasi and Chen, 2008; Luyckx
and Daelemans, 2011; Koppel et al., 2011). We consider four of the most widely used features. Our
goal behind exploring four different types of features is to understand which features are the best for
cross-topic AA.
Lexical Features. Bag-of-words is one of the commonly used document representations that uses
single-content words as document features. Authorship attribution approaches using a bag-of-words
representation have been found to be effective (Diederich et al., 2003; Kaster et al., 2005; Zhao and
Zobel, 2005; Coyotl-Morales et al., 2006). We call bag-of-words the lexical features since we exclude
stop-words.
Stop-Words. Stop-words carry no or very little semantic meaning of the texts, however, their use
indicates the presence of certain syntactic structures. Although, these words are excluded in the topic-
based text classification tasks due to lack of any semantic information in them, we believe these features
will be effective in cross-domain AA as hinted by previous work (Goldstein-Stewart et al., 2009). Typically,
words such as articles, prepositions, and conjunctions are considered as stop-words. We use a list of stop
words publicly available for download (www.webconfs.com/stop-words.php).
Stylistic Features. Previous research has shown stylistic features to be effective in AA (Stamatatos,
2006; Bhargava et al., 2013). We use 13 stylistic features: number of sentences, number of tokens per
sentence, number of punctuations per sentence, number of emoticons per document, percentage of words
without vowel, percentage of contractions, percentage of total alphabetic characters, percentage of two
consecutive punctuations, percentage of three consecutive punctuations, percentage of upper case words,
1230
total parenthesis count, percentage of sentence initial words with first letter capitalized, and percentage of
words without vowel.
Character n-grams. An n-gram is a sequence of n-contiguous characters. These features capture both
the thematic as well as stylistic information of the texts, and hence have been proven to be very effective
in previous AA studies (Keselj et al., 2003; Peng et al., 2003; Escalante et al., 2011). Since these features
carry stylistic choices of the authors, we believe they will be stable across domains.
5 Experimental Settings
Following the training scenarios discussed previously in Section 3, we performed a set of experiments.
We used 643 predefined stop-words. We considered as lexical features all words that were not stop words,
and were among the 3,500 most frequent words occurring at least twice in the training data. We used
3,500 most frequent character 3-grams occurring at least six times in the training data.
Since dataset 1 is already balanced across authors, we used all the documents from this dataset. However,
dataset 2 was originally imbalanced, therefore we chose at most ten documents per author to avoid a highly
skewed distribution. In order to create a corpus like in the realistic scenarios of forensic investigations
such as tweets, SMS, and emails, we chunked each selected document by sentence boundaries into five
new short documents. This shortening of the documents increases the complexity of the task but enhances
the practical applicability of our methods. We use these chunked versions for evaluating our proposed
method. Splitting the documents in this way has been used in the past to deal with the lack of more
documents per author(Luyckx and Daelemans, 2011; Koppel and Winter, 2014).
We obtained the performance measures using support vector machines (SVMs) implemented in Weka
(Witten and Frank, 2005) with default parameters. We considered using SVMs because preliminary results
showed this algorithm outperformed other reasonable alternatives. We used prediction accuracy as the
performance measure to evaluate different training scenarios. Rather than just comparing the accuracies,
we make most of the decisions based on statistical significance computed using two-tailed t-tests with
95% confidence interval.
All the experiments for cross-topic settings are carried out by controlling the genre. In the IT scenario,
we computed the accuracy on each test topic using stratified 10-fold cross-validation. In the SCT scenario,
for each test topic, prediction accuracy was computed by training separately on each remaining topic and
averaging performances. We computed the accuracy on each test topic in the MCT scenario by withholding
one topic as test topic and training on all other topics. For each training scenario, we computed one single
score for each genre by averaging the accuracies across all test topics belonging to that genre.
6 Experimental Results and Evaluation
In this section, we report results and analysis on different experiments we carried out. We will start
by showing empirically the challenge of cross-topic AA. Then, we will show results of our proposed
approach.
6.1 Is Cross-Topic AA More Difficult than Intra-Topic AA?
Genre
Lexical Features Stop-words Stylistic Features Character n-grams
IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT
Chat 25.71 13.11 96.11
?
19.21 16.54 16.14
?
41.90 27.49 34.39
?
39.21 27.56 42.27
?
Essay 26.58 5.92 348.99
?
16.80 11.77 42.74
?
15.66 14.56 7.02 30.90 13.28 132.68
?
Email 19.80 6.22 218.33
?
16.43 12.67 29.68
?
25.29 24.4 3.52 24.94 14.52 71.76
?
Phone Interview 37.62 10.29 265.6
?
33.49 18.00 86.06
?
33.02 16.16 51.06
?
56.99 25.46 123.84
?
Blog 22.18 6.32 250.95
?
15.37 11.25 36.62
?
13.16 11.31 14.06
?
25.38 12.03 110.97
?
Discussion 23.37 11.64 100.77
?
23.37 16.31 43.29
?
30.99 15.8 49.02
?
40.69 25.28 60.96
?
Table 2: Comparison of AA performance on IT and SCT scenarios on dataset 1. For each feature type, the
IT and SCT columns indicate the accuracy (%) while the IT-SCT column is the relative gain of IT over
SCT. For each genre, bold figures represent the best accuracy. Statistical significance is indicated by
?
in
positive direction and by
[
in negative direction.
1231
First of all, we want to understand if the cross-topic problem is more difficult than the intra-topic
problem of AA. We compared the performance of the IT and the SCT scenarios using four types of
features on various genres of dataset 1 as shown in Table 2. We clearly observed that for each genre, and
for each feature type, the performance of the IT scenario is better than the SCT scenario and the difference
is statistically significant. The only exceptions are ?Email? and ?Essay? genres for stylistic features. This
is a strong indication that irrespective of the type of domain as well as the features considered, cross-topic
AA is much more difficult than intra-topic AA.
6.2 Does Our Proposed Method Improve CTAA Performance?
We target to answer: Can we reliably predict the author of a document written in one topic with a predictive
model developed using documents from multiple other topics? We carry out various experiments and
compare the performance of our proposed MCT scenario with that of IT and SCT scenarios separately.
Although, comparing MCT with only SCT would be enough to answer our research question and test our
hypothesis, we are also interested in gaining more insights about cross-topic AA and understanding how
it compares to IT, the simplest case of AA.
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 25.71 13.11 33.02 28.43
?
151.87
?
Essay 26.58 5.92 12.64 -52.45
[
113.51
?
Email 19.80 6.22 11.87 -40.05
[
90.84
?
Phone Interview 37.62 10.29 20.95 -44.31
[
103.6
?
Blog 22.18 6.32 13.15 -40.71 108.07
?
Discussion 23.37 11.64 25.26 8.09 117.01
?
(a) Lexical Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 19.21 16.54 33.49 74.34
?
102.48
?
Essay 16.80 11.77 22.06 31.31
?
97.08
?
Email 16.43 12.67 24.97 51.98
?
116.06
?
Phone Interview 33.49 18,00 38.89 16.12 115.67
?
Blog 15.37 11.25 20.43 32.92 81.6
?
Discussion 23.37 16.31 32.59 39.45
?
99.82
?
(b) Stop-words
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 41.90 27.49 37.62 -10.21 36.85
?
Essay 15.66 14.56 23.36 49.17
?
60.44
?
Email 25.29 24.4 33.12 30.96
?
35.74
?
Phone Interview 33.02 16.16 23.49 -28.86 45.36
?
Blog 13.16 11.31 15.67 26.29
?
38.55
?
Discussion 30.99 15.8 24.33 -21.49 53.99
?
(c) Stylistic Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 39.21 27.56 57.46 46.54
?
108.49
?
Essay 30.9 13.28 36.66 18.64 176.05
?
Email 24.94 14.52 36.53 46.47
?
151.58
?
Phone Interview 56.99 25.46 56.35 -1.12 121.33
?
Blog 25.38 12.03 33.41 31.64 177.72
?
Discussion 40.69 25.28 49.91 22.66
?
97.43
?
(d) Character n-grams
Table 3: Performance of lexical, stop-words, stylistic, and character n?gram features on dataset 1. The
SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT columns present
the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive
direction and by
[
in negative direction.
MCT-SCT columns on Table 3 illustrate the statistical significance of MCT over SCT in a positive
direction for all the genres. Using any type of feature in any genre, it is possible to significantly improve
the performance of CTAA by training a machine learning algorithm using documents from all available
out-of-domain topics. This serves as evidence to confirm our hypothesis and answer our research question
that documents written in one topic can be reliably predicted with a model developed using documents
from multiple other topics. This indicates that authors maintain a consistent writing style across topics.
In the MCT-IT column in Table 3(a), we can seen that the IT is significantly better than the MCT in
three genres, while the MCT is better than the IT in only one. This is because lexical features directly
capture the choices of authors in a certain thematic area, and hence they yield a good performance in the
intra-topic setting. However, we observed contrasting and interesting patterns using stop-words, stylistic
features, and character n-grams (MCT-IT column of Tables 3(b), 3(c), and 3(d)). MCT was better than IT,
and the difference was significantly better, in 10 genres, while IT performance was significantly better
than MCT in none of the genres. This is a very interesting finding as we observed that the cross-topic AA
problem can be solved as effectively as the intra-topic AA problem using these features and a variety of
topics.
Also using dataset 2, we found that for each type of feature, MCT is better than SCT, and the difference
is statistically significant as shown in Table 4. This is another supporting evidence to our hypothesis. The
small gain of IT over MCT suggests that our proposed approach is competitive even with the IT scenario.
1232
Feature Type IT SCT MCT MCT-IT MCT-SCT
Lexical Features 63.98 21.46 38.62 -39.64
[
79.96
?
Stop-words 45.01 31.66 41.21 -8.44 30.16
?
Stylistic Features 32.85 27.46 32.17 -2.07 17.15
?
Character n-grams 75.08 45.87 64.54 -14.04
[
40.7
?
Table 4: Performance of four types of features on three different training scenarios on dataset 2. For each
feature type, the SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT
columns present the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
6.3 Sensitivity of Features to Changes in Topic
We also want to demonstrate the behavior of four different feature types to changes in topic. We want to
test if lexical features favor intra-topic AA and character n-grams favor cross-topic AA. Unlike lexical
features, character n-grams carry stylistic choices of authors, and hence are expected to be robust across
topics. In Table 2, for each genre, the relative gain of IT over SCT using lexical features is highest
compared to that of stop-words, stylistic features, and character n-grams, thereby indicating that lexical
features are more effective for ITAA than for CTAA. It is also apparent in Table 2 that the gain of
characters n-grams is always better than that of stop-words and stylistic features. While looking at the
performance on the SCT scenario using four features, it is observed that character n-grams give the best
performance, while stop-words and stylistic features give the second best performance, which leaves
lexical features at the bottom. This is because the first three features are topic-independent and hence
were able to better discriminate among authors in cross-topic scenarios than lexical features. However,
overall, character n-grams have the highest discriminative power in both IT and SCT, which confirms
findings of earlier research (Stamatatos, 2013).
In Table 3, character n-grams, when compared to lexical features, stop-words, and stylistic features,
yield the highest average relative gain on MCT over the SCT scenario (138.77%, vs 114.15% for lexical
features, 97.41% for stop-words, 46.55% for stylistic features). Also, comparing the prediction accuracies
of all four features separately in SCT, IT, and MCT scenarios, it is observed that character n-grams score
best in most of the genres on each training scenario. This confirms that character n-grams have higher
discriminative power in cross-topic AA than stop-words, stylistic features and lexical features.
For cross-topic AA, we observed that the accuracy across the board is not high. It is because the CTAA
task is harder than other single domain classification tasks since the topics of the test data are fully disjoint
with the topics of the training data. On top of that, the shorter document length makes it more challenging.
The current system might not be production quality, but our findings will enable better models in the
future that hopefully will be accurate enough to solve CTAA problems more effectively.
6.4 Cross-Topic AA with Varying Number of Training Topics
For traditional AA, it has been shown that around 10,000 word-tokens per author suffice as a ?reliable
minimum for an authorial set? (Burrows, 2007). In our study, we have as few as 600 word-tokens per
author, much less than the minimum size requirement stated by previous research. In this section, we look
at how performance improves with increase in amount of training data by adding additional topics.
To explore this, we experimented by training on documents from all possible combinations of topics. In
dataset 1, there are a total of six topics. Therefore, for each test topic, we experiment separately using
one, two, three, four, and five topics for training. When measuring performance on k training topics, we
gather all possible combinations of training on k of the five topics and then average the performance
across all these combinations. For example, if we use two topics for training, then for each test topic, there
are
(
5
2
)
= 10 possible training combinations that we then average to get a final score. We illustrate the
results in Figure 1 for four genres using four types of features. Irrespective of the genres, topics, and types
of features used, CTAA performance improves gradually with addition of more data. In most genres, this
improvement seems to be almost linear with the number of topics trained on, suggesting that gathering
more out-of-topic data should continue to improve the performance. We also observed that the character
1233
(a) Genre = Discussion (b) Genre = Phone Interview
(c) Genre = Essay (d) Genre = Blog
Figure 1: Effect of training on varying number of topics in CTAA using lexical, stop-words, stylistic, and
character n-gram features on dataset 1.
n-grams are the most effective author discriminator in cross-topic AA.
We performed a deeper analysis of the effect of individual topics, which is shown in Table 5. We took
an initial topic as training data and then paired it with each of the other topics as additional training data
and measured the average performance gain from the addition of the second topic. It is shown that across
all genres, adding a second topic to the training data gives a character n-gram model greatest boost in
performance than to a stop word or a stylistic or a lexical model. This is true regardless of the topics on
which the model is trained. We do not observe negative transfer as in transfer learning (Pan and Yang,
2010) because in cross topic AA authors maintain styles across topics.
Initial Topic
Genre = Chat Genre = Email
Lexical Stop-words Stylistic Character n-grams Lexical Stop-words Stylistic Character n-grams
Sex Discrimination 5.85 5.57 1.67 10.33 2.24 7.29 8.86 9.72
Legalization of Marijuana 7.86 7.76 1.57 12.19 2.91 3.32 5.21 7.39
Catholic Church 6.24 8.76 6.24 14.33 2.41 4.48 3.59 5.22
Privacy Rights 5.9 4.66 1.9 14.05 2.97 6.45 4.6 10.06
War in Iraq 8.1 7.95 3.48 15.57 3.96 7.58 2.99 7.79
Gay Marriage 7.19 5.85 7.19 10.29 2.57 4.31 1.98 6.82
Table 5: Average performance gain from adding an additional topic as training data across different initial
topics on dataset 1. Each value is the average accuracy gain after adding the second topic.
7 Is it Just ?More Data? that is Helping or is ?Diversity? Relevant?
The quantity of training data was not controlled in the experiments presented in Section 6, therefore,
we performed some additional experiments where we did control for this. In Table 6, we present the
comparison of SCT and MCT scenarios using the same amount of training data to understand whether
the performance improvement in the MCT scenario is due to diversity or due to the fact of adding more
data. We use dataset 1 to make this comparison. For the SCT scenario, for each test topic, we averaged
1234
performance over three random samplings, where in each sampling we randomly selected four documents
per author in each training topic. For the MCT scenario, for each test topic, we averaged performance
Lexical Features Stop-words Stylistic Features Character n-grams
Genre SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT
Chat 12.24 13.94 13.89
?
14.37 16.35 13.78
?
26.52 28.52 7.54
?
24.39 25.17 3.2
?
Essay 9.11 11.3 24.04
?
12.43 14.12 13.6
?
21.35 22.93 7.4
?
18.37 19.58 6.59
?
Discussion 9.65 10.52 9.02
?
12.93 13.7 5.96
?
19.57 20.85 6.54
?
19.84 21.48 8.27
?
Email 8.84 9.98 12.9
?
12.48 13.91 11.46
?
20.89 21.92 4.93
?
17.91 20.76 15.91
?
Phone Interview 8.94 10.84 21.25
?
14.65 17.67 20.61
?
19.73 20.94 6.13
?
18.84 26.35 39.86
?
Blog 8.45 9.66 14.32
?
12.78 14.05 9.94
?
18.53 19.62 5.88
?
17.58 19.95 13.48
?
Table 6: Comparison of MCT and SCT scenarios on controlled training data using four types of features
on dataset 1. For each feature type, the SCT and MCT columns indicate the accuracy (%) while the
MCT-SCT columns present the relative gain of MCT over the SCT. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
over three random samplings, where in each sampling we randomly selected four training topics. For
each selection of four training topics, we averaged performance over three random samplings where in
each sampling we randomly selected one document per author in each training topic. Thus, we ended up
with the same number of documents for training both models. Even with the same amount of training
data, training on documents from different topics is better than training on documents from a single topic,
with statistically significant performance gains ranging from 3.2% to 39.86% as shown in Table 6. This
demonstrates that data from a diverse set of topics will still give a boost in performance and is always
significantly better than using data from the same topic.
8 Related Work
The majority of the work in authorship attribution deals with single-domain datasets. However, there
have been a handful of studies that add some cross-topic flavor in the AA task (Mikros and Argiri, 2007;
Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013). Mikros et al. (2007) concluded that
many stylometric variables are actually discriminating topic rather than author and their use in AA should
be done carefully. However, the study was performed on a single corpus containing only two authors
in two topics that raises questions on reliability of their conclusions. Stamatatos (2013) illustrated the
effectiveness of character n-grams in cross-topic AA. It was also shown in that study that avoiding rare
features is effective in both intra-topic and cross-topic AA. However, all these conclusions came from
training an SVM classifier in only one fixed topic. In contrast, in our paper, we draw our conclusions from
all possible training/testing combinations rather than fixing in advance the training topic.
Goldstein-Stewart et al. (2009) also carried out some cross-topic experiments by concatenating the texts
of an author from different genres. This experimental setting results in a corpus where each test document
contains a mix of genres, which is not representative of real world AA problems. Still, to provide some
comparisons to the work of Goldstein-Stewart et al. (2009), we concatenated all the texts in dataset 1
produced by an individual on a single topic, across all genres to produce one document per author on each
topic. We compare our results with those reported in the paper under same training/testing conditions. We
withheld one topic and trained on documents from the other five topics.
Test Topic Lexical Stop-words Stylistic Character n-grams Stop-words + Character n-grams Previous Work
Sex Discrimination 66.67 76.19 33.33 95.24 95.24 95
Catholic Church 76.19 95.24 38.10 95.24 100 95
Gay Marriage 80.95 80.95 42.86 90.48 90.48 95
Legalization of Marijuana 52.38 66.67 33.33 95.24 100 100
Privacy Rights 42.86 52.38 28.57 95.24 90.48 100
War in Iraq 57.14 71.43 38.10 100 100 81
Average 62.7 73.81 35.72 95.24 96.03 94.33
Table 7: Comparing performance of our work with previous work in the same training/testing setting. The
results in the last column were obtained from Goldstein-Stewart et al.(2009). For each test topic, the bold
figure represents the best performance.
1235
The last column of Table 7 presents the results obtained by using the combination of stop-words and 88
Linguistic Inquiry and Word Count (LIWC) features as reported in Goldstein-Stewart et al. (2009). We
observed that the combination of character n-grams and stop-words, on average, performs better than
those reported in the paper. On this fixed training/testing scenario, we see better accuracies, as high as
100%, across the board. This is because, in this experiment, each training sample on average was ? 25
times longer than the training sample in our chunked versions. This illustrates that authorship attribution
of short documents, as in our chunked versions, is a challenging task, but we believe it resembles a more
realistic scenario of forensic investigations.
9 Conclusions and Future Work
In this research, we presented the first comprehensive study with rigorous analysis on cross-topic AA.
Although previous work had hinted some of our findings, it was based on very limited experiments (using
only one fixed topic for training). We investigated CTAA using all possible combinations of topics to draw
more robust and stable conclusions. We first illustrated the difficulty of cross-topic AA by comparing its
performance with intra-topic AA using different types of features. We demonstrated that a framework
trained on documents belonging to thematic areas different than that of the documents under investigation
statistically improves the performance of cross-topic AA. This improves the ability of the model to find the
authors of documents belonging to a new topic not present during the training of the model. By controlling
the training data, we demonstrated that training on diverse topics is better than training on a single topic
confirming that MCT not only benefits from more data but also from a thematic variety. We also showed a
statistical analysis that lexical features are closer to the thematic area and hence were an effective author
discriminator in intra-topic attribution. Similarly, character n-grams prove to be a very powerful feature
especially in a condition where training and test documents come from different thematic areas. Although
intra-topic AA is easier than cross-topic AA, our proposed model for CTAA achieves performance close
or in some cases, better than that of an intra-topic AA model. Another interesting conclusion of our study
is that addition of more training data from any topic, no matter how distant or close it is with the topic of
documents under investigation, improves the performance of CTAA for all types of features. We believe
that our contribution to cross-topic AA will be generalizable to other classification problems too.
In the future, we plan to explore the cross-genre problem of AA that is critical for tasks like linking
user accounts across emails, blogs, and other social media. Our proposed CTAA approach can be directly
applied to the cross-genre problem but we may discover different feature behavior in this scenario. We
also plan to explore domain adaptation and transfer learning techniques to solve CDAA problems.
Acknowledgements
This research was partially supported by ONR grant N00014-12-1-0217, NSF award 1254108, and NSF
award 1350360. It was also supported in part by the CONACYT grant 134186 and the WIQ-EI IRSES
project (grant no. 269180) within the FP 7 Marie Curie.
References
A. Abbasi and H. Chen. 2008. Writeprints: A stylometric approach to identity-level identification and similarity
detection in cyberspace. ACM Trans. Inf. Syst., 26(2):7:1?7:29, April.
M. Bhargava, P. Mehndiratta, and K. Asawa. 2013. Stylometric analysis for authorship attribution on twitter. In
Big Data Analytics, volume 8302 of Lecture Notes in Computer Science, pages 37?47. Springer International
Publishing.
J. Burrows. 2007. All the way through: Testing for authorship in different frequency strata. Literary & Linguistic
Computing, 22:27 ? 47.
R. Mar??a Coyotl-Morales, L. Villase?nor Pineda, M. Montes-y G?omez, and P. Rosso. 2006. Authorship attribution
using word sequences. In Proceedings of the 11th Iberoamerican Conference on Progress in Pattern Recogni-
tion, Image Analysis and Applications, CIARP?06, pages 844?853, Berlin, Heidelberg. Springer-Verlag.
1236
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001. Multi-topic e-mail authorship attribution forensics.
In Proceedings of the Workshop on Data Mining for Security Applications, 8th ACM Conference on Computer
Security.
J. Diederich, J. Kindermann, E. Leopold, and G. Paass. 2003. Authorship attribution with support vector machines.
Applied Intelligence, 19:109?123, May.
H. J. Escalante, T. Solorio, and M. Montes-y G?omez. 2011. Local histograms of character n-grams for author-
ship attribution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 288?298, Portland, Oregon, USA, June. Association for Computational
Linguistics.
G. Frantzeskou, E. Stamatatos, S. Gritzalis, and C. E. Chaski. 2007. Identifying authorship by byte-level n-grams:
The source code author profile (SCAP) method. Journal of Digital Evidence, 6(1).
J. Goldstein-Stewart, R. Winder, and R. Evans Sabin. 2009. Person identification from text and speech genre
samples. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, EACL ?09, pages 336?344, Stroudsburg, PA, USA. Association for Computational Linguistics.
J. Houvardas and E. Stamatatos. 2006. N-gram feature selection for authorship identification. In J. Euzenat and
J. Domingue, editors, AIMSA 2006, volume 4183 of LNAI, pages 77?86.
A. Kaster, S. Siersdorfer, and G. Weikum. 2005. Combining text and linguistic document representations for
authorship attribution. In SIGIR Workshop: Stylistic Analysis of Text for Information Access, pages 27?35.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-gram based author profiles for authorship attribution. In
Proceedings of the Pacific Association for Computational Linguistics, pages 255?264.
M. Koppel and Y. Winter. 2014. Determining if two documents are written by the same author. Journal of the
Association for Information Science and Technology, 65(1):178?187.
M. Koppel, J. Schler, and S. Argamon. 2011. Authorship attribution in the wild. Language Resources and
Evaluation, 45:83?94.
K. Luyckx and W. Daelemans. 2011. The effect of author set size and data size in authorship attribution. Literary
and Linguistic Computing, 26(1):35?55.
G. K. Mikros and E. K. Argiri. 2007. Investigating topic influence in authorship attribution. In Proceedings of
the International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection,
pages 29?35.
S. Jialin Pan and Q. Yang. 2010. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng.,
22(10):1345?1359, October.
F. Peng, D. Schuurmans, V. Keselj, and S. Wang. 2003. Language independent authorship attribution using char-
acter level language models. In 10th Conference of the European Chapter of the Association for Computational
Linguistics, EACL, pages 267?274.
A. I. Schein, J. F. Caver, R. J. Honaker, and C. H. Martell. 2010. Author attribution evaluation with novel topic
cross-validation. In The 2010 International Conference on Knowledge Discovery and Information Retrieval,
Valencia, Spain, October.
E. Stamatatos. 2006. Authorship attribution based on feature set subspacing ensembles. International Journal on
Artificial Intelligence tools, 15(5):823?838.
E. Stamatatos. 2011. Plagiarism detection using stopword n-grams. Journal of the American Society for Informa-
tion Science and Technology, 62(12):2512?2527.
E. Stamatatos. 2013. On the robustness of authorship attribution based on character n-gram features. Journal of
Law & Policy, 21(2):421 ? 439.
I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan
Kauffmann, 2nd edition.
Y. Zhao and J. Zobel. 2005. Effective and scalable authorship attribution using function words. In Proceedings of
2nd Asian Information Retrieval Symposium, volume 3689 of LNCS, pages 174?189, Jeju Island, Korea.
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A framework for authorship identification of online messages:
Writing-style features and classification techniques. J. Am. Soc. Inf. Sci. Technol., 57(3):378?393, February.
1237
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 821?826,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A synchronous context free grammar for time normalization
Steven Bethard
University of Alabama at Birmingham
Birmingham, Alabama, USA
bethard@cis.uab.edu
Abstract
We present an approach to time normalization
(e.g. the day before yesterday?2013-04-12)
based on a synchronous context free grammar.
Synchronous rules map the source language
to formally defined operators for manipulat-
ing times (FINDENCLOSED, STARTATENDOF,
etc.). Time expressions are then parsed using
an extended CYK+ algorithm, and converted
to a normalized form by applying the opera-
tors recursively. For evaluation, a small set
of synchronous rules for English time expres-
sions were developed. Our model outperforms
HeidelTime, the best time normalization sys-
tem in TempEval 2013, on four different time
normalization corpora.
1 Introduction
Time normalization is the task of converting a natural
language expression of time into a formal representa-
tion of a time on a timeline. For example, the expres-
sion the day before yesterday would be normalized
to the formal representation 2013-04-12 (assuming
that today is 2013-04-14) in the ISO-TimeML rep-
resentation language (Pustejovsky et al, 2010). Time
normalization is a crucial part of almost any infor-
mation extraction task that needs to place entities or
events along a timeline. And research into methods
for time normalization has been growing since the
ACE1 and TempEval (Verhagen et al, 2010; UzZa-
man et al, 2013) challenges began to include time
normalization as a shared task.
1http://www.itl.nist.gov/iad/mig/tests/ace/
Most prior work on time normalization has taken
a rule-based, string-to-string translation approach.
That is, each word in a time expression is looked up
in a normalization lexicon, and then rules map this
sequence of lexical entries directly to the normalized
form. HeidelTime (Stro?tgen and Gertz, 2012), which
had the highest performance in TempEval 2010 and
2013, and TIMEN (Llorens et al, 2012), which re-
ported slightly higher performance in its own experi-
ments, both follow this approach. A drawback of this
approach though is that there is no nesting of rules:
for example, in HeidelTime the rules for yesterday
and the day before yesterday are completely separate,
despite the compositional nature of the latter.
A notable exception to the string-to-string ap-
proach is the work of (Angeli et al, 2012). They de-
fine a target grammar of typed pre-terminals, such as
YESTERDAY (a SEQUENCE) or DAY (a DURATION),
and compositional operations, such as SHIFTLEFT
(a (RANGE, DURATION) ? RANGE). They apply
an expectation-maximization approach to learn how
words align to elements of the target grammar, and
achieve performance close to that of the rule-based
systems. However, their grammar does not allow
for non-binary or partially lexicalized rules (e.g. SE-
QUENCE ? DURATION before SEQUENCE would
be impossible), and some of their primitive elements
could naturally be expressed using other primitives
(e.g. YESTERDAY as SHIFTLEFT(TODAY, 1 DAY)).
We present a synchronous grammar for time nor-
malization that addresses these shortcomings. We
first define a grammar of formal operations over tem-
poral elements. We then develop synchronous rules
that map time expression words to temporal opera-
821
Source: English
[TIMESPAN]1
[UNIT]2
[NIL]4
the
week
of [TIMESPAN]3
[FIELD]5
[FIELD:MONTH]6
March
[FIELD:DAY]7
[INT:1-31]8
6
Target: Time Operators
[TIMESPAN]1
FINDENCLOSING [TIMESPAN]3
FINDEARLIER PRESENT [FIELD]5
[FIELD:MONTH]6
MONTHOFYEAR 3
[FIELD:DAY]7
DAYOFMONTH [INT:1-31]8
6
[UNIT]2
WEEKS
Figure 1: The synchronous parse from the source language the week of March 6 to the target formal time representation
FINDENCLOSING(FINDEARLIER(PRESENT, MONTHOFYEAR?3, DAYOFMONTH?6), WEEKS). Subscripts on
non-terminals indicate the alignment between the source and target parses.
tors, and perform normalization by parsing with an
extended CYK+ parsing algorithm. We evaluate this
approach to time normalization on the TimeBank,
AQUAINT, Timen and TempEval 2013 corpora.
2 Synchronous grammars
Our time grammar is based on the synchronous con-
text free grammar formalism. Synchronous gram-
mars allow two trees, one in the source language
and one in the target language, to be constructed si-
multaneously. A synchronous context free grammar
has rules of the form X ? (S,T, A), where X is a
non-terminal, S is the sequence of terminals and non-
terminals that X expands to in the source language,
T is the sequence of terminals and non-terminals
that X expands to in the target language, and A is
the alignment between the non-terminals of S and T
(which must be the same).
For time normalization, the source side is the nat-
ural language text, and the target side is a formal
grammar of temporal operators. Figure 1 shows a
synchronous parse of the week of March 6 2. The left
side is the source side (an English expression), the
right side is the target side (a temporal operator ex-
pression), and the alignment is shown via subscripts.
2Figure 1 corresponds to an interpretation along the lines of
the week of the last March 6. The full grammar developed in this
article would also produce an interpretation corresponding to the
week of the next March 6, since the phrase is ambiguous.
3 Target time grammar
The right side of Figure 1 shows an example of
our target formal representation: FINDENCLOSING(
FINDEARLIER(PRESENT, MONTHOFYEAR?3,
DAYOFMONTH?6), WEEKS). Each terminal in
the parse is either a numeric value or an opera-
tor like FINDENCLOSING, WEEKS or MONTHOF-
YEAR. Each non-terminal combines terminals or
non-terminals to create a [TIMESPAN], [PERIOD],
[FIELD], [UNIT] or [INT]. The list of rules allowed
by our target grammar (the right-hand side of our
synchronous grammar) is given in Table 1.
Each of the target operators defines a procedure for
creating a temporal object from others. For example,
FINDENCLOSING takes a [TIMESPAN] and a [UNIT]
and expands the start and end of the time span to fill
a period of one unit. This could be used, for exam-
ple, to define today as FINDENCLOSING(PRESENT,
DAYS), where the PRESENT, which is instantaneous,
is expanded out to the enclosing day. Note that we
define things like today and yesterday in terms of
primitive operations, rather than making them primi-
tives themselves as in (Angeli et al, 2012).
The left side of Figure 1 shows the synchronous
parse of the source language. Note that each of the
non-terminals is aligned (shown as a subscript) with
a non-terminal in the target parse3, while terminals
are not aligned and may freely appear or disappear
3We actually allow a slightly asynchronous grammar, where
a non-terminal may be used 0 or more times on the target side.
822
[INT] ? integer
[UNIT] ? unit
[FIELD] ? field [INT]
[FIELD] ? [FIELD]*
[PERIOD] ? SIMPLE [INT] [UNIT]
[PERIOD] ? FRACTIONAL [INT] [INT] [UNIT]
[PERIOD] ? UNSPECIFIED [UNIT]
[PERIOD] ? WITHMODIFIER [PERIOD] modifier
[TIMESPAN] ? PAST
[TIMESPAN] ? PRESENT
[TIMESPAN] ? FUTURE
[TIMESPAN] ? FINDEARLIER [TIMESPAN] [FIELD]
[TIMESPAN] ? FINDLATER [TIMESPAN] [FIELD]
[TIMESPAN] ? FINDENCLOSING [TIMESPAN] [UNIT]
[TIMESPAN] ? FINDENCLOSED [TIMESPAN] [FIELD]
[TIMESPAN] ? STARTATENDOF [TIMESPAN] [PERIOD]
[TIMESPAN] ? ENDATSTARTOF [TIMESPAN] [PERIOD]
[TIMESPAN] ? MOVEEARLIER [TIMESPAN] [PERIOD]
[TIMESPAN] ? MOVELATER [TIMESPAN] [PERIOD]
[TIMESPAN] ? WITHMODIFIER [TIMESPAN] modifier
Table 1: Rules allowed by the target time grammar.
A ?unit? is any java.time.temporal.TemporalUnit,
e.g. SECONDS, WEEKS or DECADES. A ?field? is any
java.time.temporal.TemporalField, e.g. HOUR-
OFAMPM, DAYOFMONTH or CENTURY. A ?modifier?
is any of the TIMEX3 ?mod? values defined in TimeML.
from the source to the target. Each non-terminal thus
corresponds to a synchronous grammar rule that de-
scribes how a source expression should be translated
into the target time grammar. For example the root
nodes correspond to an application of the following
full synchronous rule:
[TIMESPAN]?
source: [UNIT] of [TIMESPAN]
target: FINDENCLOSING [TIMESPAN] [UNIT]
4 Parsing algorithm
Parsing with a synchronous context free grammar is
much the same as parsing with just the source side of
the grammar. Only a small amount of bookkeeping is
necessary to allow the generation of the target parse
once the source parse is complete. We can therefore
apply standard parsing algorithms to this task.
However, we have some additional grammar re-
quirements. As shown in Figure 1, we allow rules
that expand into more than two terminals or non-
terminals, the mixing of terminals and non-terminals
in a production, a special [NIL] non-terminal for the
ignoring of words, and a special [INT] non-terminal
that can match ranges of integers and does not re-
quire all possible integers to be manually listed in
the grammar. This means that we can?t directly use
CYK parsing or even CYK+ parsing (Chappelier and
Rajman, 1998), which allows rules that expand into
more than two terminals or non-terminals, but does
not meet our other requirements.
Algorithm 1 shows our extended version of CYK+
parsing. As with standard CYK+ parsing, two charts
are filled, one for rules that have been completed (C)
and one for rules that have been only partially ad-
vanced (P ). All parses covering 1 terminal are com-
pleted first, then these are used to complete parses
covering 2 terminals, etc. until all parses covering all
terminals are complete.
Our extensions to the standard CYK+ parsing are
as follows. To handle integers, we modify the ini-
tialization to generate new rules on the fly for any
numeric terminals that fit the range of an [INT:X-Y]
non-terminal in the grammar (starts at line 5). To
allow mixing of terminals and non-terminals, we ex-
tend the initialization step to also produce partial
parses (line 17), and extend the parse advancement
step to allow advancing rules with terminals (starting
at line 23). Finally, to handle [NIL] rules, which con-
sume tokens but are not included in the final parse,
we add a step where rules are allowed to advance,
unchanged, past a [NIL] rule (starting at line 35).
5 Parsing example
As an example, consider parsing the week of March
6 with the following source side grammar:
[NIL] ? the
[UNIT] ? week
[MONTH] ? March
[DAY] ? [INT:1-31]
[FIELD] ? [MONTH][DAY]
[TIMESPAN] ? [FIELD]
[TIMESPAN] ? [UNIT] of [TIMESPAN]
First the algorithm handles the numeric special case,
completing an [INT] parse for the token 6 at index 4:
C(1,4) ?= [INT:1-31] ? 6
Then it completes parses based on just the terminals:
C(1,0) ?= [NIL] ? the
C(1,1) ?= [UNIT] ? week
C(1,3) ?= [MONTH] ? March
Next, the algorithm starts working on parses that
span 1 token. It can start two partial parses, using the
[UNIT] at C(1,1), and using the [MONTH] at C(1,3):
P(1,1) ?= [TIMESPAN] ? [UNIT] ? of [TIMESPAN]
P(1,3) ?= [FIELD] ? [MONTH] ? [DAY]
823
Algorithm 1 CYK+ parsing, extended for partially
lexicalized rules, [Nil] rules and numbers
Require: G a set of rules, w a sequence of tokens
1: function PARSE(G,w)
2: C ? a new |w|+ 1 by |w| matrix
3: P ? a new |w|+ 1 by |w| matrix
4: // Generate rules on the fly for numeric tokens
5: for i? 0 . . . (|w| ? 1) do
6: if ISNUMBER(wi) then
7: for all [INT:x-y] ? non-terminals of G do
8: if x ? TONUMBER(wi) ? y then
9: C(1,i) ?= [INT:x-y]? wi
10: // Start any rules that begin with terminals
11: for i? 0 . . . (|w| ? 1) do
12: for all X? ?? ? G do
13: if ?j | ? = wi:j ? ?ISTERMINAL(?0) then
14: if ? =  then
15: C(|wi:j |,i) ?= X? wi:j?
16: else
17: P(|wi:j |,i) ?= (|wi:j |,X? wi:j?)
18: for n? 1 . . . |w|; i? 0 . . . (|w| ? n) do
19: // Find all parses of size n starting at i
20: form? 1 . . . n do
21: for all (p,X? ?) ? P(m,i) do
22: // Advance partial parses using terminals
23: if wi+m:i+n = ?p:p+n?m then
24: if ?p+n?m:|?| =  then
25: C(n,i) ?= X? ?
26: else
27: P(n,i) ?= (p+ n?m,X? ?)
28: // Advance partial parses using completes
29: for all ?p ? ? ? C(n?m,i+m) do
30: if |?| = p+ 1 then
31: C(n,i) ?= X? ?
32: else
33: P(n,i) ?= (p+ 1,X? ?)
34: // Advance complete parses past [Nil] parses
35: for all X? ? ? C(m,i) do
36: for all Y? ? ? C(n?m,i+m) do
37: if X 6= Nil ? Y = Nil then
38: C(n,i) ?= X? ?
39: else if X = Nil ? Y 6= Nil then
40: C(n,i) ?= Y? ?
41: // Start any rules that begin with a complete parse
42: for all X? ? ? C(n,i) do
43: for all Y? X? ? C(n,i) do
44: if ? =  then
45: C(n,i) ?= Y? X?
46: else
47: P(n,i) ?= (1,Y? X?)
48: return C(|w|,0)
(The ? is the visual equivalent of the first element in
the partial parse tuples of Algorithm 1, which marks
parsing progress.) And given the [INT:1-31] atC(1,4)
the algorithm can make a complete size 1 parse:
C(1,4) ?= [DAY] ? [INT:1-31]
The algorithm then moves on to create parses that
span 2 tokens. The special handling of [NIL] allows
the [UNIT] at C(1,1) to absorb the [NIL] at C(1,0):
C(2,0) ?= [UNIT] ? week
This [UNIT] then allows the start of a partial parse:
P(2,0) ?= [TIMESPAN] ? [UNIT] ? of [TIMESPAN]
The partial parse at P(1,1) can be advanced using of
at position 2, creating another 2 token partial parse:
P(2,1) ?= [TIMESPAN] ? [UNIT] of ? [TIMESPAN])
The partial parse at P(1,3) can be advanced using the
[DAY] at C(1,4), completing the 2 token parse:
C(2,3) ?= [FIELD] ? [MONTH][DAY]
This [FIELD] allows completion of a 2 token parse:
C(2,3) ?= [TIMESPAN] ? [FIELD]
The algorithm then moves on to 3 token parses. Only
one is possible: the partial parse at P(2,0) can be
advanced using the of at position 2, yielding:
P(3,0) ?= [TIMESPAN] ? [UNIT] of ? [TIMESPAN]
The algorithm moves on to 4 token parses, finding
that the partial parse at P(2,1) can be advanced using
the [TIMESPAN] at C(2,3), completing the parse:
C(4,1) ?= [TIMESPAN] ? [UNIT] of [TIMESPAN]
Finally, the algorithm moves on to 5 token parses,
where (1) the special handling of [NIL] allows the
partial parse at C(4,1) to consume the [NIL] at C(1,0)
and (2) the partial parse at P(3,0) can be advanced
using the [TIMESPAN] at C(2,3). Both of these yield:
C(5,0) ?= [TIMESPAN] ? [UNIT] of [TIMESPAN]
The complete parses in C(5,0) are then determinis-
tically translated into target side parses using the
alignments in the rules of the synchronous grammar.
6 Evaluation
Using our synchronous grammar formalism for time
normalization, we manually developed a grammar
for English time expressions. Following the lead of
TIMEN and HeidelTime, we developed our grammar
by inspecting examples from the AQUAINT4 and
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2002T31
824
N TIMEN HeidelTime SCFG
AQUAINT 652 69.5 74.7 76.5
TimeBank 1426 67.7 80.9 84.9
Timen 214 67.8 49.1 56.5
TempEval2013 158 74.1 78.5 81.6
Table 2: Performance of TIMEN, HeidelTime and our
synchronous context free grammar (SCFG) on each evalu-
ation corpus. (N is the number of time expressions.)
TimeBank (Pustejovsky et al, 2003) corpora. The
resulting grammar has 354 rules, 192 of which are
only lexical, e.g., [UNIT]? (seconds, SECONDS).
Our grammar produces multiple parses when the
input is ambiguous. For example, the expression
Monday could mean either the previous Monday or
the following Monday, and the expression the day
could refer either to a period of one day, or to a spe-
cific day in time, e.g. 2013-04-14. For such expres-
sions, our grammar produces both parses. To choose
between the two, we employ a very simple set of
heuristics: (1) prefer [TIMESPAN] to [PERIOD], (2)
prefer an earlier [TIMESPAN] to a later one and (3)
prefer a [TIMESPAN] with QUARTERS granularity
if the anchor time is also in QUARTERS (this is a
common rule in TimeBank annotations).
We evaluate on the AQUAINT corpus, the Time-
Bank corpus, the Timen corpus (Llorens et al, 2012)
and the TempEval 2013 test set (UzZaman et al,
2013)5. We compare to two6 state-of-the-art systems:
TIMEN and HeidelTime. Table 2 shows the results.
Our synchronous grammar approach outperformed
HeidelTime on all corpora, both on the training cor-
pora (AQUAINT and TimeBank) and on the test cor-
pora (Timen and TempEval 2013). Both our model
and HeidelTime outperformed TIMEN on all corpora
except for the Timen corpus.
To better understand the issues in the Timen cor-
pus, we manually inspected the 33 time expressions
that TIMEN normalized correctly and our approach
5We evaluate normalization accuracy over all time expres-
sions, not the F1 of both finding and normalizing expressions, so
the numbers here are not directly comparable to those reported
by the TempEval 2013 evaluation.
6Though its performance was slightly lower than HeidelTime,
we also intended to compare to the (Angeli et al, 2012) system.
Its authors graciously helped us get the code running, but to date
all models we were able to train performed substantially worse
than their reported results, so we do not compare to them here.
normalized incorrectly. 4 errors were places where
our heuristic was wrong (e.g. we chose the earlier,
not the later Sept. 22). 6 errors were coverage prob-
lems of our grammar, e.g. not handling season, every
time or long ago. 2 errors were actually human an-
notation errors (several years ago was annotated as
PASTREF and daily was annotated as XXXX-XX-
XX, while the guidelines say these should be PXY
and P1D respectively). The remaining 21 errors were
from two new normalization forms not present at all
in the training data: 19 instances of THH:MM:SS
(times were always YYYY-MM-DDTHH:MM:SS
in the training data) and 2 instances of BCYYYY
(years were always YYYY in the training data).
7 Discussion
Our synchronous grammar approach to time normal-
ization, which handles recursive structures better than
existing string-to-string approaches and handles a
wider variety of grammars than existing parsing ap-
proaches, outperforms the HeidelTime system on
four evaluation corpora and outperforms the TIMEN
system on three of the four corpora.
Our time normalization code and models are
freely available. The source code and English
grammar are hosted at https://github.com/
bethard/timenorm, and official releases are pub-
lished to Maven Central (group=info.bethard,
artifact=timenorm).
In future work, we plan to replace the heuristic
for selecting between ambiguous parses with a more
principled approach. It would be a simple extension
to support a probabilistic grammar, as in (Angeli et
al., 2012). But given an expression like Monday, it
would still be impossible to decide whether it refers to
the future or the past, since the surrounding context,
e.g. tense of the governing verb, is needed for such a
judgment. A more promising approach would be to
train a classifier that selects between the ambiguous
parses based on features of the surrounding context.
Acknowledgements
The project described was supported in part by Grant
Number R01LM010090 from the National Library Of
Medicine. The content is solely the responsibility of the
authors and does not necessarily represent the official
views of the National Library of Medicine or the National
Institutes of Health.
825
References
[Angeli et al2012] Gabor Angeli, Christopher Manning,
and Daniel Jurafsky. 2012. Parsing time: Learning to
interpret time expressions. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 446?455, Montre?al, Canada,
June. Association for Computational Linguistics.
[Chappelier and Rajman1998] Jean-Ce?dric Chappelier
and Martin Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137. Citeseer.
[Llorens et al2012] Hector Llorens, Leon Derczynski,
Robert Gaizauskas, and Estela Saquete. 2012. TIMEN:
An open temporal expression normalisation resource.
In Proceedings of the Eight International Conference
on Language Resources and Evaluation (LREC?12),
Istanbul, Turkey, May. European Language Resources
Association (ELRA).
[Pustejovsky et al2003] James Pustejovsky, Patrick
Hanks, Roser Sauri, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth Sundheim,
David Day, Lisa Ferro, and Marcia Lazo. 2003. The
TIMEBANK corpus. In Corpus Linguistics, pages
647?656, Lancaster, UK.
[Pustejovsky et al2010] James Pustejovsky, Kiyong Lee,
Harry Bunt, and Laurent Romary. 2010. ISO-TimeML:
An international standard for semantic annotation. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
[Stro?tgen and Gertz2012] Jannik Stro?tgen and Michael
Gertz. 2012. Multilingual and cross-domain temporal
tagging. Language Resources and Evaluation.
[UzZaman et al2013] Naushad UzZaman, Hector Llorens,
Leon Derczynski, James Allen, Marc Verhagen, and
James Pustejovsky. 2013. Semeval-2013 task 1:
Tempeval-3: Evaluating time expressions, events, and
temporal relations. In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
[Verhagen et al2010] Marc Verhagen, Roser Sauri, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, page
5762, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
826
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336?344,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Skip N-grams and Ranking Functions for Predicting Script Events
Bram Jans
KU Leuven
Leuven, Belgium
bram.jans@gmail.com
Steven Bethard
University of Colorado Boulder
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ivan Vulic?
KU Leuven
Leuven, Belgium
ivan.vulic@cs.kuleuven.be
Marie Francine Moens
KU Leuven
Leuven, Belgium
sien.moens@cs.kuleuven.be
Abstract
In this paper, we extend current state-of-the-
art research on unsupervised acquisition of
scripts, that is, stereotypical and frequently
observed sequences of events. We design,
evaluate and compare different methods for
constructing models for script event predic-
tion: given a partial chain of events in a
script, predict other events that are likely
to belong to the script. Our work aims
to answer key questions about how best
to (1) identify representative event chains
from a source text, (2) gather statistics from
the event chains, and (3) choose ranking
functions for predicting new script events.
We make several contributions, introducing
skip-grams for collecting event statistics, de-
signing improved methods for ranking event
predictions, defining a more reliable evalu-
ation metric for measuring predictiveness,
and providing a systematic analysis of the
various event prediction models.
1 Introduction
There has been recent interest in automatically ac-
quiring world knowledge in the form of scripts
(Schank and Abelson, 1977), that is, frequently
recurring situations that have a stereotypical se-
quence of events, such as a visit to a restaurant.
All of the techniques so far proposed for this task
share a common sub-task: given an event or partial
chain of events, predict other events that belong
to the same script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Chambers and Ju-
rafsky, 2011; Manshadi et al 2008; McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010; Regneri
et al 2010). Such a model can then serve as input
to a system that identifies the order of the events
within that script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009) or that generates
a story using the selected events (McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010).
In this article, we analyze and compare tech-
niques for constructing models that, given a partial
chain of events, predict other events that belong to
the script. In particular, we consider the following
questions:
? How should representative chains of events
be selected from the source text?
? Given an event chain, how should statistics
be gathered from it?
? Given event n-gram statistics, which ranking
function best predicts the events for a script?
In the process of answering these questions, this
article makes several contributions to the field of
script and narrative event chain understanding:
? We explore for the first time the use of skip-
grams for collecting narrative event statistics,
and show that this approach performs better
than classic n-gram statistics.
? We propose a new method for ranking events
given a partial script, and show that it per-
forms substantially better than ranking meth-
ods from prior work.
? We propose a new evaluation procedure (us-
ing Recall@N) for the cloze test, and advo-
cate its usage instead of average rank used
previously in the literature.
? We provide a systematic analysis of the in-
teractions between the choices made when
constructing an event prediction model.
336
Section 2 gives an overview of the prior work
related to this task. Section 3 lists and briefly de-
scribes different approaches that try to provide
answers to the three questions posed in this intro-
duction, while Section 4 presents the results of our
experiments and reports on our findings. Finally,
Section 5 provides a conclusive discussion along
with ideas for future work.
2 Prior Work
Our work is primarily inspired by the work of
Chambers and Jurafsky, which combined a depen-
dency parser with coreference resolution to col-
lect event script statistics and predict script events
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). For each document in their training
corpus, they used coreference resolution to iden-
tify all the entities, and a dependency parser to
identify all verbs that had an entity as either a sub-
ject or object. They defined an event as a verb plus
a dependency type (either subject or object), and
collected for each entity, the chain of events that
it participated in. They then calculated pointwise
mutual information (PMI) statistics over all the
pairs of events that occurred in the event chains in
their corpus. To predict a new script event given
a partial chain of events, they selected the event
with the highest sum of PMIs with all the events
in the partial chain.
The work of McIntyre and Lapata followed in
this same paradigm, (McIntyre and Lapata, 2009;
McIntyre and Lapata, 2010), collecting chains of
events by looking at entities and the sequence of
verbs for which they were a subject or object. They
also calculated statistics over the collected event
chains, though they considered both event bigram
and event trigram counts. Rather than predicting
an event for a script however, they used these sim-
ple counts to predict the next event that should be
generated for a children?s story.
Manshadi and colleagues were concerned about
the scalability of running parsers and coreference
over a large collection of story blogs, and so used
a simplified version of event chains ? just the main
verb of each sentence (Manshadi et al 2008).
Rather than rely on an ad-hoc summation of PMIs,
they apply language modeling techniques (specifi-
cally, a smoothed 5-gram model) over the sequence
of events in the collected chains. However, they
only tested these language models on sequencing
tasks (e.g. is the real sequence better than a ran-
dom sequence?) rather than on prediction tasks
(e.g. which event should follow these events?).
In the current article, we attempt to shed some
light on these previous works by comparing differ-
ent ways of collecting and using event chains.
3 Methods
Models that predict script events typically have
three stages. First, a large corpus is processed to
find event chains in each of the documents. Next,
statistics over these event chains are gathered and
stored. Finally, the gathered statistics are used to
create a model that takes as input a partial script
and produces as output a ranked list of events for
that script. The following sections give more de-
tails about each of these stages and identify the
decisions that must be made in each step, and an
overview of the whole process with an example
source text is displayed in Figure 1.
3.1 Identifying Event Chains
Event chains are typically defined as a sequence
of actions performed by some actor. Formally, an
event chain C for some actor a, is a partially or-
dered set of events (v, d) where each v is a verb
that has the actor a as its dependency d. Following
prior work (Chambers and Jurafsky, 2008; Cham-
bers and Jurafsky, 2009; McIntyre and Lapata,
2009; McIntyre and Lapata, 2010), these event
chains are identified by running a coreference sys-
tem and a dependency parser. Then for each en-
tity identified by the coreference system, all verbs
that have a mention of that entity as one of their
dependencies are collected1. The event chain is
then the sequence of (verb, dependency-type) tu-
ples. For example, given the sentence A Crow
was sitting on a branch of a tree when a Fox ob-
served her, the event chain for the Crow would be
(sitting, SUBJECT), (observed, OBJECT).
Once event chains have been identified, the most
appropriate event chains for training the model
must be selected. The goal of this process is to
select the subset of the event chains identified by
the coreference system and the dependency parser
that look to be the most reliable. Both the coref-
erence system and the dependency parser make
some errors, so not all event chains are necessarily
useful for training a model. The three strategies
we consider for this selection process are:
1Also following prior work, we consider only the depen-
dencies subject and object.
337
John woke up. He opened his eyes and yawned. Then he crossed the room and walked to the door.There he saw Mary. Mary smiled and kissed him. Then they both blushed.JOHN(woke, SUBJ)(opened, SUBJ)(yawned, SUBJ)(crossed, SUBJ)(walked, SUBJ)(saw, SUBJ)(kissed, OBJ)(blushed, SUBJ) MARY(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)(blushed, SUBJ)all chains, long chains,the longest chain all chains 1. Identifying event chains... [(saw, OBJ), (smiled, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(smiled, SUBJ), (blushed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(saw, OBJ), (blushed, SUBJ)]...[(kissed, SUBJ), (blushed, SUBJ)]regular bigrams 2-skip bigrams1-skip bigrams 2. Gathering event chain statistics(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)_________ (missing event)constructing a partial script (cloze test)1. (looked, OBJ)2. (gave, SUBJ)3. (saw, SUBJ)... 1. (kissed, OBJ)2. (looked, OBJ)3. (waited, SUBJ)... 1. (blushed, SUBJ)2. (kissed, OBJ)3. (smiled, SUBJ)C&J PMIOrdered PMIBigram prob. 3. Predicting script events
Figure 1: An overview of the whole linear work flow showing the three key steps ? identifying event chains,
collecting statistics out of the chains and predicting a missing event in a script. The figure also displays how a
partial script for evaluation (Section 4.3) is constructed. We show the whole process for Mary?s event chain only,
but the same steps are followed for John?s event chain.
? Select all event chains, that is, all sequences
of two or more events linked by common
actors. This strategy will produce the largest
number of event chains to train a model from,
but it may produce noisier training data as
the very short chains included by this strategy
may be less likely to represent real scripts.
? Select all long event chains consisting of 5
or more events. This strategy will produce a
smaller number of event chains, but as they
are longer, they may be more likely to repre-
sent scripts.
? Select only the longest event chain. This
strategy will produce the smallest number of
event chains from a corpus. However, they
may be of higher quality, since this strategy
looks for the key actor in each story, and only
uses the events that are tied together by that
key actor. Since this is the single actor that
played the largest role in the story, its actions
may be the most likely to represent a real
script.
3.2 Gathering Event Chain Statistics
Once event chains have been collected from the
corpus, the statistics necessary for constructing
the event prediction model must be gathered. Fol-
lowing prior work (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Manshadi et al
2008; McIntyre and Lapata, 2009; McIntyre and
Lapata, 2010), we focus on gathering statistics
about the n-grams of events that occur in the
collected event chains. Specifically, we look at
strategies for collecting bigram statistics, the most
common type of statistics gathered in prior work.
We consider three strategies for collecting bigram
statistics:
? Regular bigrams. We find all pairs of
events that are adjacent in an event chain
and collect the number of times each event
pair was observed. For example, given the
chain of events (saw, SUBJ), (kissed, OBJ),
(blushed, SUBJ), we would extract the two
event bigrams: ((saw, SUBJ), (kissed, OBJ))
338
and ((kissed, OBJ), (blushed, SUBJ)). In addi-
tion to the event pair counts, we also collect
the number of times each event was observed
individually, to allow for various conditional
probability calculations. This strategy fol-
lows the classic approach for most language
models.
? 1-skip bigrams. We collect pairs of events
that occur with 0 or 1 events intervening be-
tween them. For example, given the chain
(saw, SUBJ), (kissed, OBJ), (blushed, SUBJ),
we would extract three bigrams: the two regu-
lar bigrams ((saw, SUBJ), (kissed, OBJ)) and
((kissed, OBJ), (blushed, SUBJ)), plus the 1-
skip-bigram, ((saw, SUBJ), (blushed, SUBJ)).
This approach to collecting n-gram statistics
is sometimes called skip-gram modeling, and
it can reduce data sparsity by extracting more
event pairs per chain (Guthrie et al 2006).
It has not previously been applied in the task
of predicting script events, but it may be
quite appropriate to this task because in most
scripts it is possible to skip some events in
the sequence.
? 2-skip bigrams. We collect pairs of events
that occur with 0, 1 or 2 intervening events,
similar to what was done in the 1-skip bi-
grams strategy. This will extract even more
pairs of events from each chain, but it is pos-
sible the statistics over these pairs of events
will be noisier.
3.3 Predicting Script Events
Once statistics over event chains have been col-
lected, it is possible to construct the model for
predicting script events. The input of this model
will be a partial script c of n events, where c =
c1c2 . . . cn = (v1, d1), (v2, d2), . . . , (vn, dn), and
the output of this model will be a ranked list of
events where the highest ranked events are the ones
most likely to belong to the event sequence in the
script. Thus, the key issue for this model is to de-
fine the function f for ranking events. We consider
three such ranking functions:
? Chambers & Jurafsky PMI. Chambers and
Jurafsky (2008) define their event ranking
function based on pointwise mutual infor-
mation. Given a partial script c as defined
above, they consider each event e = (v?, d?)
collected from their corpus, and score it as
the sum of the pointwise mutual informations
between the event e and each of the events in
the script:
f(e, c) =
n?
i
log
P (ci, e)
P (ci)P (e)
Chambers and Jurafsky?s description of this
score suggests that it is unordered, such that
P (a, b) = P (b, a). Thus the probabilities
must be defined as:
P (e1, e2) =
C(e1, e2) + C(e2, e1)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
where C(e1, e2) is the number of times that
the ordered event pair (e1, e2) was counted in
the training data, and C(e) is the number of
times that the event e was counted.
? Ordered PMI. A variation on the approach
of Chambers and Jurafsky is to have a score
that takes the order of the events in the chain
into account. In this scenario, we assume that
in addition to the partial script of events, we
are given an insertion point, m, where the
new event should be added. The score is then
defined as:
f(e, c) =
m?
k=1
log
P (ck, e)
P (ck)P (e)
+
n?
k=m+1
log
P (e, ck)
P (e)P (ck)
where the probabilities are defined as:
P (e1, e2) =
C(e1, e2)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
This approach uses pointwise mutual infor-
mation but also models the event chain in the
order it was observed.
? Bigram probabilities. Finally, a natural
ranking function, which has not been applied
to the script event prediction task (but has
339
been applied to related tasks (Manshadi et
al., 2008)) is to use the bigram probabilities
of language modeling rather than pointwise
mutual information scores. Again, given an
insertion point m for the event in the script,
we define the score as:
f(e, c) =
m?
k=1
logP (e|ck) +
n?
k=m+1
logP (ck|e)
where the conditional probability is defined
as2:
P (e1|e2) =
C(e1, e2)
C(e2)
This approach scores an event based on the
probability that it was observed following all
the events before it in the chain and preceding
all the events after it in the chain. This ap-
proach most directly models the event chain
in the order it was observed.
4 Experiments
Our experiments aimed to answer three questions:
Which event chains are worth keeping? How
should event bigram counts be collected? And
which ranking method is best for predicting script
events? To answer these questions we use two
corpora, the Reuters Corpus and the Andrew Lang
Fairy Tale Corpus, to evaluate our three differ-
ent chain selection methods, {all chains, long
chains, the longest chain}, our three different bi-
gram counting methods, {regular bigrams, 1-skip
bigrams, 2-skip bigrams}, and our three different
ranking methods, {Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}.
4.1 Corpora
We consider two corpora for evaluation:
? Reuters Corpus, Volume 1 3 (Lewis et
al., 2004) ? a large collection of 806, 791
news stories written in English concerning
a number of different topics such as politics,
2Note that predicted bigram probabilities are calculated
in this way for both classic language modeling and skip-gram
modeling. In skip-gram modeling, skips in the n-grams are
only used to increase the size of the training data; prediction
is performed exactly as in classic language modeling.
3http://trec.nist.gov/data/reuters/reuters.html
economics, sports, etc., strongly varying in
length, topics and narrative structure.
? Andrew Lang Fairy Tale Corpus 4 ? a
small collection of 437 children stories with
an average length of 125 sentences, and used
previously for story generation by McIntyre
and Lapata (2009).
In general, the Reuters Corpus is much larger and
allows us to see how well script events can be
predicted when a lot of data is available, while the
Andrew Lang Fairy Tale Corpus is much smaller,
but has a more straightforward narrative structure
that may make identifying scripts simpler.
4.2 Corpus Processing
Constructing a model for predicting script events
requires a corpus that has been parsed with a de-
pendency parser, and whose entities have been
identified via a coreference system. We there-
fore processed our corpora by (1) filtering out
non-narrative articles, (2) applying a dependency
parser, (3) applying a coreference resolution sys-
tem and (4) identifying event chains via entities
and dependencies.
First, articles that had no narrative content were
removed from the corpora. In the Reuters Corpus,
we removed all files solely listing stock exchange
values, interest rates, etc., as well as all articles
that were simply summaries of headlines from dif-
ferent countries or cities. After removing these
files, the Reuters corpus was reduced to 788, 245
files. Removing files from the Fairy Tale corpus
was not necessary ? all 437 stories were retained.
We then applied the Stanford Parser (Klein and
Manning, 2003) to identify the dependency struc-
ture of each sentence in each article in the corpus.
This parser produces a constitutent-based syntactic
parse tree for each sentence, and then converts this
tree to a collapsed dependency structure via a set
of tree patterns.
Next we applied the OpenNLP coreference en-
gine5 to identify the entities in each article, and the
noun phrases that were mentions of each entity.
Finally, to identify the event chains, we took
each of the entities proposed by the coreference
system, walked through each of the noun phrases
associated with that entity, retrieved any subject
4http://www.mythfolklore.net/andrewlang/
5http://incubator.apache.org/opennlp/
340
or object dependencies that linked a verb to that
noun phrase, and created an event chain from the
sequence of (verb, dependency-type) tuples in the
order that they appeared in the text.
4.3 Evaluation Metrics
We follow the approach of Chambers and Jurafsky
(2008), evaluating our models for predicting script
events in a narrative cloze task. The narrative
cloze task is inspired by the classic psychological
cloze task in which subjects are given a sentence
with a word missing and asked to fill in the blank
(Taylor, 1953). Similarly, in the narrative cloze
task, the system is given a sequence of events from
a script where one event is missing, and asked
to predict the missing event. The difficulty of a
cloze task depends a lot on the context around
the missing item ? in some cases it may be quite
predictable, but in many cases there is no single
correct answer, though some answers are more
probable than others. Thus, performing well on a
cloze task is more about ranking the missing event
highly, and not about proposing a single ?correct?
event.
In this way, narrative cloze is like perplexity
in a language model. However, where perplexity
measures how good the model is at predicting a
script event given the previous events in the script,
narrative cloze measures how good the model is
at predicting what is missing between events in
the script. Thus narrative cloze is somewhat more
appropriate to our task, and at the same time sim-
plifies comparisons to prior work.
Rather than manually constructing a set of
scripts on which to run the cloze test, we follow
Chambers and Jurafsky in reserving a section of
our parsed corpora for testing, and then using the
event chains from that section as the scripts for
which the system must predict events. Given an
event chain of length n, we run n cloze tests, with
a different one of the n events removed each time
to create a partial script from the remaining n? 1
events (see Figure 1). Given a partial script as
input, an accurate event prediction model should
rank the missing event highly in the guess list that
it generates as output.
We consider two approaches to evaluating the
guess lists produced in response to narrative cloze
tests. Both are defined in terms of a test collection
C, consisting of |C| partial scripts, where for each
partial script c with missing event e, ranksys(c) is
the rank of e in the system?s guess list for c.
? Average rank. The average rank of the miss-
ing event across all of the partial scripts:
1
|C|
?
c?C
ranksys(c)
This is the evaluation metric used by Cham-
bers and Jurafsky (2008).
? Recall@N. The fraction of partial scripts
where the missing event is ranked N or less6
in the guess list.
1
|C|
|{c : c ? C ? ranksys(c) ? N}|
In our experiments we use N = 50, but re-
sults are roughly similar for lower and higher
values of N .
Recall@N has not been used before for evaluat-
ing models that predict script events, however we
suggest that it is a more reliable metric than Av-
erage rank. When calculating the average rank,
the length of the guess lists will have a significant
influence on results. For instance, if a small model
is trained with only a small vocabulary of events,
its guess lists will usually be shorter than a larger
model, but if both models predict the missing event
at the bottom of the list, the larger model will get
penalized more. Recall@N does not have this is-
sue ? it is not influenced by length of the guess
lists.
An alternative evaluation metric would have
been mean average precision (MAP), a metric
commonly used to evaluate information retrieval.
Mean average precision reduces to mean recipro-
cal rank (MRR) when there?s only a single answer
as in the case of narrative cloze, and would have
scored the ranked lists as:
1
|C|
?
c?C
1
ranksys(c)
Note that mean reciprocal rank has the same issues
with guess list length that average rank does. Thus,
since it does not aid us in comparing to prior work,
and it has the same deficiencies as average rank,
we do not report MRR in this article.
6Rank 1 is the event that the system predicts is most prob-
able, so we want the missing event to have the smallest rank
possible.
341
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 502 0.5179
long chains 549 0.4951
the longest chain 546 0.4984
Table 1: Chain selection methods for the Reuters corpus
- comparison of average ranks and Recall@50.
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 1650 0.3376
long chains 452 0.3461
the longest chain 1534 0.3376
Table 2: Chain selection methods for the Fairy Tale
corpus - comparison of average ranks and Recall@50.
4.4 Results
We considered all 27 combinations of our chain
selection methods, bigram counting methods, and
ranking methods: {all chains, long chains, the
longest chain}x{regular bigrams, 1-skip bigrams,
2-skip bigrams}x{Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}. The best among
these 27 combinations for the Reuters corpus was
{all chains}x{2-skip bigrams}x{bigram probabil-
ities} achieving an average rank of 502 and a Re-
call@50 of 0.5179.
Since viewing all the combinations at once
would be confusing, instead the following sec-
tions investigate each decision (selection, counting,
ranking) one at a time. While one decision is var-
ied across its three choices, the other decisions are
held to their values in the best model above.
4.4.1 Identifying Event Chains
We first try to answer the question: How should
representative chains of events be selected from
the source text? Tables 1 and 2 show perfor-
mance when we vary the strategy for selecting
event chains, while fixing the counting method to
2-skip bigrams, and fixing the ranking method to
bigram probabilities.
For the Reuters collection, we see that using all
chains gives a lower average rank and a higher
Recall@50 than either of the strategies that select
a subset of the event chains. The explanation is
probably simple: using all chains produces more
than 700,000 bigrams from the Reuters corpus,
while using only the long chains produces only
around 300,000. So more data is better data for
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 789 0.4886
1-skip bigrams 630 0.4951
2-skip bigrams 502 0.5179
Table 3: Event bigram selection methods for the
Reuters corpus - comparison of average ranks and Re-
call@50.
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 2363 0.3227
1-skip bigrams 1690 0.3418
2-skip bigrams 1650 0.3376
Table 4: Event bigram selection methods for the Fairy
Tales corpus - comparison of average ranks and Re-
call@50.
predicting script events.
For the Fairy Tale collection, long chains gives
the lowest average rank and highest Recall@50. In
this collection, there is apparently some benefit to
filtering the shorter event chains, probably because
the collection is small enough that the noise in-
troduced from dependency and coreference errors
plays a larger role.
4.4.2 Gathering Event Chain Statistics
We next try to answer the question: Given an
event chain, how should statistics be gathered from
it? Tables 3 and 4 show performance when we vary
the strategy for counting event pairs, while fixing
the selecting method to all chains, and fixing the
ranking method to bigram probabilities.
For the Reuters corpus, 2-skip bigrams achieves
the lowest average rank and the highest Recall@50.
For the Fairy Tale corpus, 1-skip bigrams and 2-
skip bigrams perform similarly, and both have
lower average rank and higher Recall@50 than
regular bigrams.
Skip-grams probably outperform regular n-
grams on both of these corpora because the skip-
grams provide many more event pairs over which
to calculate statistics: in the Reuters corpus, regu-
lar bigrams extracts 737,103 bigrams, while 2-skip
bigrams extracts 1,201,185 bigrams. Though skip-
grams have not been applied to predicting script
events before, it seems that they are a good fit,
and better capture statistics about narrative event
chains than regular n-grams do.
342
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 2052 0.1954
ordered PMI 3584 0.1694
bigram prob. 502 0.5179
Table 5: Ranking methods for the Reuters corpus -
comparison of average ranks and Recall@50.
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 1455 0.1975
ordered PMI 2460 0.0467
bigram prob. 1650 0.3376
Table 6: Ranking methods for the Fairy Tale corpus -
comparison of average ranks and Recall@50.
4.4.3 Predicting Script Events
Finally, we try to answer the question: Given
event n-gram statistics, which ranking function
best predicts the events for a script? Tables 5 and
6 show performance when we vary the strategy for
ranking event predictions, while fixing the selec-
tion method to all chains, and fixing the counting
method to 2-skip bigrams.
For both Reuters and the Fairy Tale corpus, Re-
call@50 identifies bigram probabilities as the best
ranking function by far. On the Reuters corpus
the Chambers & Jurafsky PMI ranking method
achieves Recall@50 of only 0.1954, while bigram
probabilities ranking method achieves 0.5179. The
gap is also quite large on the Fairy Tales corpus:
0.1975 vs. 0.3376.
On the Reuters corpus, average rank also identi-
fies bigram probabilities as the best ranking func-
tion, yet for the Fairy Tales corpus, Chambers &
Jurafsky PMI and bigram probabilities have simi-
lar average ranks. This inconsistency is probably
due to the flaws in the average rank evaluation
measure that were discussed in Section 4.3 ? the
measure is overly sensitive to the length of the
guess list, particularly when the missing event is
ranked lower, as it is likely to be when training on
a smaller corpus like the Fairy Tales corpus.
5 Discussion
Our experiments have led us to several important
conclusions. First, we have introduced skip-grams
and proved their utility for acquiring script knowl-
edge ? our models that employ skip bigrams score
consistently higher on event prediction. By follow-
ing the intuition that events do not have to appear
strictly one after another to be closely semantically
related, skip-grams decrease data sparsity and in-
crease the size of the training data.
Second, our novel bigram probabilities ranking
function outperforms the other ranking methods.
In particular, it outperforms the state-of-the-art
pointwise mutual information method introduced
by Chambers and Jurafsky (2008), and it does so
by a large margin, more than doubling the Re-
call@50 on the Reuters corpus. The key insight
here is that, when modeling events in a script, a
language-model-like approach better fits the task
than a mutual information approach.
Third, we have discussed why Recall@N is a
better and more consistent evaluation metric than
Average rank. However, both evaluation metrics
suffer from the strictness of the narrative cloze test,
which accepts only one event being the correct
event, while it is sometimes very difficult, even
for humans, to predict the missing events, and
sometimes more solutions are possible and equally
correct. In future research, our goal is to design
a better evaluation framework which is more suit-
able for this task, where credit can be given for
proposed script events that are appropriate but not
identical to the ones observed in a text.
Fourth, we have observed some differences in
results between the Reuters and the Fairy Tale
corpora. The results for Reuters are consistently
better (higher Recall@50, lower average rank), al-
though fairy tales contain a plainer narrative struc-
ture, which should be more appropriate to our task.
This again leads us to the conclusion that more
data (even with more noise as in Reuters) leads to
a greater coverage of events, better overall models
and, consequently, to more accurate predictions.
Still, the Reuters corpus seems to be far from a
perfect corpus for research in the automatic acqui-
sition of scripts, since only a small portion of the
corpus contains true narratives. Future work must
therefore gather a large corpus of true narratives,
like fairy tales and children?s stories, whose sim-
ple plot structures should provide better learning
material, both for models predicting script events,
and for related tasks like automatic storytelling
(McIntyre and Lapata, 2009).
One of the limitations of the work presented
here is that it takes a fairly linear, n-gram-based ap-
proach to characterizing story structure. We think
such an approach is useful because it forms a natu-
343
ral baseline for the task (as it does in many other
tasks such as named entity tagging and language
modeling). However, story structure is seldom
strictly linear, and future work should consider
models based on grammatical or discourse links
that can capture the more complex nature of script
events and story structure.
Acknowledgments
We would like to thank the anonymous reviewers
for their constructive comments. This research
was carried out as a master thesis in the frame-
work of the TERENCE European project (EU FP7-
257410).
References
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 602?610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 976?986.
David Guthrie, Ben Allison, W. Liu, Louise Guthrie,
and Yorick Wilks. 2006. A closer look at skip-gram
modelling. In Proceedings of the Fifth international
Conference on Language Resources and Evaluation
(LREC), pages 1222?1225.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for
text categorization research. Journal of Machine
Learning Research, 5:361?397.
Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proceed-
ings of the Twenty-First International Florida Artifi-
cial Intelligence Research Society Conference.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 217?225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1562?1572.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979?988.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: an inquiry into
human knowledge structures. Lawrence Erlbaum
Associates.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readibility. Journalism Quarterly,
30:415?433.
344
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271?276,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Model-Portability Experiments for Textual Temporal Analysis 
Oleksandr Kolomiyets, Steven Bethard and Marie-Francine Moens Department of Computer Science Katholieke Universiteit Leuven Celestijnenlaan 200A, Heverlee, 3001, Belgium {oleksandr.kolomiyets, steven.bethard, sien.moens}@cs.kuleuven.be  
 
 
Abstract 
We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substan-tial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alne never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.  
1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a cru-cial first step for applications like question answer-ing that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time 
normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al, 2010). Many researchers com-peted in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al, 2005; Ahn et al, 2007; Poveda et al, 2007; Str?tgen and Gertz 2010; Llorens et al, 2010), and achieving F1 measures as high as 0.86 for recog-nizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire.  Thus we know little about how well time expression recognition systems generalize to other sorts of text. We there-fore take a state-of-the-art time recognizer and eva-luate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expres-sions than are available explicitly in the newswire training data. We therefore introduce a semi-supervised approach for expanding the training data, where we take words from temporal expres-sions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via Word-Net, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised mod-el on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 
271
2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Sur-deanu et al, 2006). The most relevant research to our work here is that of (Poveda et al, 2009), which investigated a semi-supervised approach to time expression rec-ognition. They begin by selecting 100 time expres-sions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding to-kens, parts-of-speech, syntactic chunks etc.) and then search for new seeds given their patterns. The patterns resulting from this iterative process achieve F1 scores of up to 0.604 on the test half of the Automatic Content Extraction corpus. Our approach is quite different from that of (Po-veda et al, 2009) ? we use our training corpus for learning a supervised model rather than for se-lecting high precision seeds, we generate addi-tional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010).  3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: ? The head word of the phrase ? The part-of-speech tag of the head word ? All tokens and part-of-speech tags in the phrase as a bag of words 
? The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30  ? The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 ? The concatenated string of the syntactic types of the children of the phrase in the parse tree ? The depth in the parse tree  3.2 Lexical Resources for Bootstrapping Sparsity of annotated corpora is the biggest chal-lenge for any supervised machine learning tech-nique and especially for porting the trained models onto other domains. To overcome this problem we hypothesize that knowledge of semantically similar words, like temporal triggers, could be found by associating words that do not occur in the training set to similar words that do occur in the training set. Furthermore, we would like to learn these similarities automatically to be independent of knowledge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) ? a language model that learns from an unlabeled corpus how to pro-vide a weighted set of synonyms for words in con-text. The LWLM model is trained on the Reuters news article corpus of 80 million words.  WordNet (Miller, 1995) is another resource for synonyms widely used in research and applications of natural language processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense. For each synset in WordNet there is a collection of other ?sister? syn-sets, called coordinate terms, which are topologi-cally located under the same hypernym.  3.3 Bootstrapping Strategies Having a list of synonyms for each token in the sentence, we can replace one of the original tokens by its synonym while still mostly preserving the sentence semantics. We choose to replace just the headword, under the assumption that since tempo-ral trigger words usually occur at the headword position, adding alternative synonyms for the headword should allow our model to learn tempo-ral triggers that did not appear in the training data.  
272
We designed the following bootstrapping strate-gies for generating new temporal expressions: ? LWLM: the phrasal head is replaced by one of the LWLM synonyms. ? WordNet 1st Sense: Synonyms and coordinate terms for the most common sense of the phrasal head are selected and used for generat-ing new examples of time expressions. ? WordNet Pseudo-Lesk: The synset for the phrasal head is selected as having the largest intersection between the synset?s words and the LWLM synonyms. Then, synonyms and coordinate terms are used for generating new examples of time expressions. ? LWLM+WordNet: The intersection of the LWLM synonyms and the WordNet synset found by pseudo-Lesk are used. In this way for every annotated time expression we generate n new examples (n?[1,10]) and use them for training bootstrapped classification models.  4 Experimental Setup The tested model is trained on the official Tem-pEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens. For testing the portability of the model to other domains we anno-tated two small target domain document collec-tions with TIMEX3 tags. The first corpus is 12 Reuters news articles from the Reuters corpus 
(Lewis et al, 2004), containing 2960 total tokens and 240 annotated TIMEX3 tokens (inter-annotator agreement 0.909 F1-score). The second corpus is the Wikipedia article for Barak Obama (http://en.wikipedia.org/wiki/Obama), containing 7029 total tokens and 512 annotated TIMEX3 to-kens (inter-annotator agreement 0.901 F1-score). The basic TempEval model is evaluated on the source domain (TempEval 2010 evaluation set ? 9599 tokens in total and 269 TIMEX3 annotated tokens) and target domain data (Reuters and Wikipedia) using the TempEval 2010 evaluation metrics. Since porting the model onto other do-mains usually causes a performance drop, our ex-periments are focused on improving the results by employing different bootstrapping strategies1. 5 Results The recognition performance of the model is re-ported in Table 1 (column ?Basic TempEval Mod-el?) for the source and the target domains. The basic TempEval model itself achieves F1-score of 0.834 on the official TempEval 2010 evaluation corpus and has a potential rank 8 among 15 par-ticipated systems. The top seven TempEval-2 sys-tems achieved F1-score between 0.83 and 0.86.                                                            1 The annotated datasets are available at http://www.cs.kuleuven.be/groups/liir/software.php 
Bootstrapped Models   Basic TempEval Model LWLM WordNet 1st Sense WordNet Pseudo-Lesk LWLM+ WordNet # Syn 0 1 1 1 2 P 0.916 0.865 0.881 0.894 0.857 R 0.770 0.807 0.773 0.781 0.830 TempEval 2010 F1 0.834 0.835 0.824 0.833 0.829 # Syn 0 5 7 6 4 P 0.896 0.841 0.820 0.839 0.860 R 0.679 0.812 0.721 0.717 0.742 Reuters F1 0.773 0.826 0.767 0.773 0.796 # Syn 0 3 1 6 5 P 0.959 0.924 0.922 0.909 0.913 R 0.770 0.830 0.781 0.820 0.844 Wikipedia F1 0.859 0.874 0.858 0.862 0.877 Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters and Wikipedia) domains. Bootstrapped models were asked to generate between one and ten additional train-ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem-pEval Model are marked in bold. 
273
However, this model does not port well to the Reuters corpus (0.773 vs. 0.834 F1-score). For the Wikipedia-based corpus, the basic TempEval mod-el actually performs a little better than on the source domain (0.859 vs. 0.834 F1-score). Four bootstrapping strategies were proposed and evaluated. Table 1 shows the maximum F1 score achieved by each of these strategies, along with the number of generated synonyms (between one and ten) at which this maximum was achieved. None of the bootstrapped models outperformed the basic TempEval model on the TempEval 2010 evalua-tion data, and the WordNet 1st Sense strategy and the WordNet Pseudo-Lesk strategy never outper-formed the basic TempEval model on any corpus. However, for the Reuters and Wikipedia cor-pora, the LWLM and LWLM+WordNet bootstrap-ping strategies outperformed the basic TempEval model. The LWLM strategy gives a large boost to model performance on the Reuters corpus from 0.773 up to 0.826 (a 23.3% error reduction) when using the first 5 synonyms. This puts performance on Reuters near performance on the TempEval domain from which the model was trained (0.834). This suggests that the (Reuters-trained) LWLM is finding exactly the right kinds of synonyms: those that were not originally present in the TempEval data but are present in the Reuters test data. On the Wikipedia corpus, the LWLM bootstrapping strat-egy results in a moderate boost, from 0.859 up to 0.874 (a 10.6% error reduction) when using the first three synonyms. Figure 1 shows that using more synonyms with this strategy drops perform-
ance on the Wikipedia corpus back down to the level of the basic TempEval model. The LWLM+WordNet strategy gives a moderate boost on the Reuters corpus from 0.773 up to 0.796 (a 10.1% error reduction) when four synonyms are used. Figure 2 shows that using six or more syno-nyms drops this performance back to just above the basic TempEval model. On the Wikipedia corpus, the LWLM+WordNet strategy results in a moder-ate boost, from 0.859 up to 0.877 (a 12.8% error reduction), with five synonyms. Using additional synonyms results in a small decline in perform-ance, though even with ten synonyms, the per-formance is better than the basic TempEval model. In general, the LWLM strategy gives the best performance, while the LWLM+WordNet strategy is less sensitive to the exact number of synonyms used when expanding the training data. 6 TempEval Error Analysis We were curious why synonym-based boot-strapping did not improve performance on the source-domain TempEval 2010 data. An error analysis suggested that some time expressions might have been left unannotated by the human annotators. Two of the authors re-annotated the TempEval evaluation data, finding inter-annotator agreement of 0.912 F1-score with each other, but only 0.868 and 0.887 F1-score with the TempEval annotators, primarily due to unannotated time ex-pressions such as 23-year, a few days and third-quarter. 
 Figure 1: F1 score of the LWLM bootstrapping strat-egy, generating from zero to ten additional training examples per instance. 
 Figure 2: F1 score of the LWLM+WordNet bootstrap-ping strategy, generating from zero to ten additional training examples per instance. 
274
Using this re-annotated TempEval 2010 data2, we re-evaluated the proposed bootstrapping tech-niques. Figure 3 and Figure 4 compare perform-ance on the original TempEval data to performance on the re-annotated version. We now see the same trends for the TempEval data as were observed for the Reuters and Wikipedia corpora: using a small number of synonyms from the LWLM to generate new training examples leads to performance gains. The LWLM bootstrapping model using the first synonym achieves 0.861 F1 score, a 22.8% error reduction over the baseline of 0.820 F1 score. 7 Discussion and Conclusions We have presented model-portability experiments on time expression recognition with a number of bootstrapping strategies. These bootstrapping strat-egies generate additional training examples by substituting temporal expression words with poten-tial synonyms from two sources: WordNet and the Latent Word Language Model (LWLM). Bootstrapping with LWLM synonyms provides a large boost for Reuters data and TempEval data and a decent boost for Wikipedia data when the top few synonyms are used. Additional synonyms do not help, probably because they are too newswire-specific: both the contexts from the TempEval training data and the synonyms from the Reuters-trained LWLM come from newswire text, so the 
                                                           2 Available at http://www.cs.kuleuven.be/groups/liir/software.php 
lower synonyms are probably more domain-specific. Intersecting the synonyms generated by the LWLM and by WordNet moderates the LWLM, making the bootstrapping strategy less sensitive to the exact number of synonyms used. However, while the intersected model performs as well as the LWLM model on Wikipedia, the gains over the non-bootstrapped model on Reuters and TempEval data are smaller. Overall, our results show that when porting time expression recognition models to other domains, a performance drop can be avoided by synonym-based bootstrapping. Future work will focus on using synonym-based expansion in the contexts (not just the time expressions headwords), and on incorporating contextual information and syntactic transformations. Acknowledgments This work has been funded by the Flemish gov-ernment as a part of the project AMASS++ (Ad-vanced Multimedia Alignment and Structured Summarization) (Grant: IWT-SBO-060051). References David Ahn, Joris van Rantwijk, and Maarten de Rijke. 2007. A Cascaded Machine Learning Approach to Interpreting Temporal Expressions. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Lin-guistics (NAACL-HLT 2007). 
 Figure 3: F1 score of the LWLM bootstrapping strat-egy, comparing performance on the original TempEval data to the re-annotated version. 
 Figure 4: F1 score of the LWLM+WordNet bootstrap-ping strategy, comparing performance on the original TempEval data to the re-annotated version. 
275
Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceed-ings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 100?110, College Park, MD. ACL. Koen Deschacht and Marie-Francine Moens. 2009. Us-ing the Latent Words Language Model for Semi-Supervised Semantic Role Labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In Proceedings of the 16th Conference on Computa-tional Linguistics, pp. 466?471. Kadri Hacioglu, Ying Chen, and Benjamin Douglas 2005. Automatic Time Expression Labeling for Eng-lish and Chinese Text. In Gelbukh, A. (ed.) CICLing 2005. LNCS, vol. 3406, pp. 548?559. Springer, Hei-delberg. Oleksandr Kolomiyets, Marie-Francine Moens. 2010. KUL: Recognition and Normalization of Temporal Expressions. In Proceedings of SemEval-2 5th Work-shop on Semantic Evaluation. pp. 325-328. Uppsala, Sweden. ACL. David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. Machine Learning Re-search. 5: 361-397 Inderjeet Mani, and George Wilson. 2000. Robust Tem-poral Processing of News. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pp. 69-76, Morristown, NJ. ACL. George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41. Matteo Negri, and Luca Marseglia. 2004. Recognition and Normalization of Time Expressions: ITC-irst at TERN 2004. Technical Report, ITC-irst, Trento. Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval 2. In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, pp. 284?291, Uppsala, Sweden. ACL. Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2007. A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English. In Proceedings of the International Sym-posium on Temporal Representation and Reasoning, pp. 141-149. 
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2009. An Analysis of Bootstrapping for the Recognition of Temporal Expressions. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pp. 49-57, Stroudsburg, PA, USA. ACL. Jannik Str?tgen and Michael Gertz. 2010. HeidelTime: High Quality Rule-Based Extraction and Normaliza-tion of Temporal Expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 321?324, Uppsala, Sweden. ACL.  Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Informa-tion Extraction Patterns. In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006). ACL. Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 57?62, Upp-sala, Sweden. ACL. 	 ?David Yarowsky. 1995. Unsupervised word sense dis-ambiguation rivaling supervised methods. In Pro-ceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189?196, Cambridge, MA. ACL.   
276
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 88?97,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting Narrative Timelines as Temporal Dependency Structures
Oleksandr Kolomiyets
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Oleksandr.Kolomiyets@
cs.kuleuven.be
Steven Bethard
University of Colorado
Campus Box 594
Boulder, CO 80309, USA
Steven.Bethard@
colorado.edu
Marie-Francine Moens
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Sien.Moens@
cs.kuleuven.be
Abstract
We propose a new approach to characterizing
the timeline of a text: temporal dependency
structures, where all the events of a narrative
are linked via partial ordering relations like BE-
FORE, AFTER, OVERLAP and IDENTITY. We
annotate a corpus of children?s stories with tem-
poral dependency trees, achieving agreement
(Krippendorff?s Alpha) of 0.856 on the event
words, 0.822 on the links between events, and
of 0.700 on the ordering relation labels. We
compare two parsing models for temporal de-
pendency structures, and show that a determin-
istic non-projective dependency parser outper-
forms a graph-based maximum spanning tree
parser, achieving labeled attachment accuracy
of 0.647 and labeled tree edit distance of 0.596.
Our analysis of the dependency parser errors
gives some insights into future research direc-
tions.
1 Introduction
There has been much recent interest in identifying
events, times and their relations along the timeline,
from event and time ordering problems in the Temp-
Eval shared tasks (Verhagen et al, 2007; Verhagen
et al, 2010), to identifying time arguments of event
structures in the Automated Content Extraction pro-
gram (Linguistic Data Consortium, 2005; Gupta and
Ji, 2009), to timestamping event intervals in the
Knowledge Base Population shared task (Artiles et
al., 2011; Amigo? et al, 2011).
However, to date, this research has produced frag-
mented document timelines, because only specific
types of temporal relations in specific contexts have
been targeted. For example, the TempEval tasks only
looked at relations between events in the same or ad-
jacent sentences (Verhagen et al, 2007; Verhagen et
al., 2010), and the Automated Content Extraction pro-
gram only looked at time arguments for specific types
of events, like being born or transferring money.
In this article, we propose an approach to temporal
information extraction that identifies a single con-
nected timeline for a text. The temporal language
in a text often fails to specify a total ordering over
all the events, so we annotate the timelines as tem-
poral dependency structures, where each event is a
node in the dependency tree, and each edge between
nodes represents a temporal ordering relation such
as BEFORE, AFTER, OVERLAP or IDENTITY. We
construct an evaluation corpus by annotating such
temporal dependency trees over a set of children?s
stories. We then demonstrate how to train a time-
line extraction system based on dependency parsing
techniques instead of the pair-wise classification ap-
proaches typical of prior work.
The main contributions of this article are:
? We propose a new approach to characterizing
temporal structure via dependency trees.
? We produce an annotated corpus of temporal
dependency trees in children?s stories.
? We design a non-projective dependency parser
for inferring timelines from text.
The following sections first review some relevant
prior work, then describe the corpus annotation and
the dependency parsing algorithm, and finally present
our evaluation results.
88
2 Related Work
Much prior work on the annotation of temporal in-
formation has constructed corpora with incomplete
timelines. The TimeBank (Pustejovsky et al, 2003b;
Pustejovsky et al, 2003a) provided a corpus anno-
tated for all events and times, but temporal relations
were only annotated when the relation was judged to
be salient by the annotator. In the TempEval compe-
titions (Verhagen et al, 2007; Verhagen et al, 2010),
annotated texts were provided for a few different
event and time configurations, for example, an event
and a time in the same sentence, or two main-clause
events from adjacent sentences. Bethard et al (2007)
proposed to annotate temporal relations one syntactic
construction at a time, producing an initial corpus of
only verbal events linked to events in subordinated
clauses. One notable exception to this pattern of
incomplete timelines is the work of Bramsen et al
(2006) where temporal structures were annotated as
directed acyclic graphs. However they worked on a
much coarser granularity, annotating not the order-
ing between individual events, but between multi-
sentence segments of text.
In part because of the structure of the available
training corpora, most existing temporal informa-
tion extraction models formulate temporal linking
as a pair-wise classification task, where each pair
of events and/or times is examined and classified as
having a temporal relation or not. Early work on the
TimeBank took this approach (Boguraev and Ando,
2005), classifying relations between all events and
times within 64 tokens of each other. Most of the top-
performing systems in the TempEval competitions
also took this pair-wise classification approach for
both event-time and event-event temporal relations
(Bethard and Martin, 2007; Cheng et al, 2007; UzZa-
man and Allen, 2010; Llorens et al, 2010). Systems
have also tried to take advantage of more global in-
formation to ensure that the pair-wise classifications
satisfy temporal logic transitivity constraints, using
frameworks such as integer linear programming and
Markov logic networks (Bramsen et al, 2006; Cham-
bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz-
Zaman and Allen, 2010). Yet the basic approach is
still centered around pair-wise classifications, not the
complete temporal structure of a document.
Our work builds upon this prior research, both
improving the annotation approach to generate the
fully connected timeline of a story, and improving
the models for timeline extraction using dependency
parsing techniques. We use the annotation scheme
introduced in more detail in Bethard et. al. (2012),
which proposes to annotate temporal relations as de-
pendency links between head events and dependent
events. This annotation scheme addresses the issues
of incoherent and incomplete annotations by guaran-
teeing that all events in a plot are connected along
a single timeline. These connected timelines allow
us to design new models for timeline extraction in
which we jointly infer the temporal structure of the
text and the labeled temporal relations. We employ
methods from syntactic dependency parsing, adapt-
ing them to our task by including features typical of
temporal relation labeling models.
3 Corpus Annotation
The corpus of stories for children was drawn from the
fables collection of (McIntyre and Lapata, 2009)1 and
annotated as described in (Bethard et al, 2012). In
this section we illustrate the main annotation princi-
ples for coherent temporal annotation. As an example
story, consider:
Two Travellers were on the road together,
when a Bear suddenly appeared on the
scene. Before he observed them, one made
for a tree at the side of the road, and
climbed up into the branches and hid there.
The other was not so nimble as his compan-
ion; and, as he could not escape, he threw
himself on the ground and pretended to be
dead. . . [37.txt]
Figure 1 shows the temporal dependency structure
that we expect our annotators to identify in this story.
The annotators were provided with guidelines both
for which kinds of words should be identified as
events, and for which kinds of events should be
linked by temporal relations. For identifying event
words, the standard TimeML guidelines for anno-
tating events (Pustejovsky et al, 2003a) were aug-
mented with two additional guidelines:
1Data available at http://homepages.inf.ed.ac.
uk/s0233364/McIntyreLapata09/
89
Figure 1: Event timeline for the story of the Travellers and the Bear. Nodes are events and edges are temporal relations.
Edges denote temporal relations signaled by linguistic cues in the text. Temporal relations that can be inferred via
transitivity are not shown.
? Skip negated, modal or hypothetical events (e.g.
could not escape, dead in pretended to be dead).
? For phrasal events, select the single word that
best paraphrases the meaning (e.g. in used to
snap the event should be snap, in kept perfectly
still the event should be still).
For identifying the temporal dependencies (i.e. the
ordering relations between event words), the anno-
tators were instructed to link each event in the story
to a single nearby event, similar to what has been
observed in reading comprehension studies (Johnson-
Laird, 1980; Brewer and Lichtenstein, 1982). When
there were several reasonable nearby events to choose
from, the annotators were instructed to choose the
temporal relation that was easiest to infer from the
text (e.g. preferring relations with explicit cue words
like before). A set of six temporal relations was used:
BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDEN-
TITY or OVERLAP.
Two annotators annotated temporal dependency
structures in the first 100 fables of the McIntyre-
Lapata collection and measured inter-annotator agree-
ment by Krippendorff?s Alpha for nominal data (Krip-
pendorff, 2004; Hayes and Krippendorff, 2007). For
the resulting annotated corpus annotators achieved
Alpha of 0.856 on the event words, 0.822 on the links
between events, and of 0.700 on the ordering rela-
tion labels. Thus, we concluded that the temporal
dependency annotation paradigm was reliable, and
the resulting corpus of 100 fables2 could be used to
2Available from http://www.bethard.info/data/
fables-100-temporal-dependency.xml
train a temporal dependency parsing model.
4 Parsing Models
We consider two different approaches to learning a
temporal dependency parser: a shift-reduce model
(Nivre, 2008) and a graph-based model (McDonald
et al, 2005). Both models take as input a sequence
of event words and produce as output a tree structure
where the events are linked via temporal relations.
Formally, a parsing model is a function (W ? ?)
where W = w1w2 . . . wn is a sequence of event
words, and pi ? ? is a dependency tree pi = (V,E)
where:
? V = W ? {Root}, that is, the vertex set of the
graph is the set of words in W plus an artificial
root node.
? E = {(wh, r, wd) : wh ? V,wd ? V, r ? R =
{BEFORE, AFTER, INCLUDES, IS INCLUDED,
IDENTITY, OVERLAP}}, that is, in the edge set
of the graph, each edge is a link between a de-
pendent word and its head word, labeled with a
temporal relation.
? (wh, r, wd) ? E =? wd 6= Root, that is, the
artificial root node has no head.
? (wh, r, wd) ? E =? ((w?h, r
?, wd) ? E =?
wh = w?h? r = r
?), that is, for every node there
is at most one head and one relation label.
? E contains no (non-empty) subset of arcs
(wh, ri, wi), (wi, rj , wj), . . . , (wk, rl, wh), that
is, there are no cycles in the graph.
90
SHIFT Move all of L2 and the head of Q onto L1
([a1 . . . ai], [b1 . . . bj ], [wkwk+1 . . .], E) ? ([a1 . . . aib1 . . . bjwk], [], [wk+1 . . .], E)
NO-ARC Move the head of L1 to the head of L2
([a1 . . . aiai+1], [b1 . . . bj ], Q,E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], Q,E)
LEFT-ARC Create a relation where the head of L1 depends on the head of Q
Not applicable if ai+1 is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (wk, r, ai+1)
RIGHT-ARC Create a relation where the head of Q depends on the head of L1
Not applicable if wk is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (ai+1, r, wk)
Table 1: Transition system for Covington-style shift-reduce dependency parsers.
4.1 Shift-Reduce Parsing Model
Shift-reduce dependency parsers start with an input
queue of unlinked words, and link them into a tree
by repeatedly choosing and performing actions like
shifting a node to a stack, or popping two nodes from
the stack and linking them. Shift-reduce parsers are
typically defined in terms of configurations and a tran-
sition system, where the configurations describe the
current internal state of the parser, and the transition
system describes how to get from one state to another.
Formally, a deterministic shift-reduce dependency
parser is defined as (C, T,CF , INIT, TREE) where:
? C is the set of possible parser configurations ci
? T ? (C ? C) is the set of transitions ti from
one configuration cj to another cj+1 allowed by
the parser
? INIT ? (W ? C) is a function from the input
words to an initial parser configuration
? CF ? C are the set of final parser configura-
tions cF where the parser is allowed to terminate
? TREE ? (CF ? ?) is a function that extracts a
dependency tree pi from a final parser state cF
Given this formalism and an oracle o ? (C ? T ),
which can choose a transition given the current con-
figuration of the parser, dependency parsing can be
accomplished by Algorithm 1. For temporal depen-
dency parsing, we adopt the Covington set of transi-
tions (Covington, 2001) as it allows for parsing the
non-projective trees, which may also contain ?cross-
ing? edges, that occasionally occur in our annotated
corpus. Our parser is therefore defined as:
Algorithm 1 Deterministic parsing with an oracle.
c? INIT(W )
while c /? CF do
t? o(c)
c? t(c)
end while
return TREE(c)
? c = (L1, L2, Q,E) is a parser configuration,
where L1 and L2 are lists for temporary storage,
Q is the queue of input words, and E is the set
of identified edges of the dependency tree.
? T = {SHIFT,NO-ARC,LEFT-ARC,RIGHT-ARC}
is the set of transitions described in Table 1.
? INIT(W ) = ([Root], [], [w1, w2, . . . , wn], ?)
puts all input words on the queue and the ar-
tificial root on L1.
? CF = {(L1, L2, Q,E) ? C : L1 = {W ?
{Root}}, L2 = Q = ?} accepts final states
where the input words have been moved off of
the queue and lists and into the edges in E.
? TREE((L1, L2, Q,E)) = (W ?{Root}, E) ex-
tracts the final dependency tree.
The oracle o is typically defined as a machine learn-
ing classifier, which characterizes a parser configu-
ration c in terms of a set of features. For temporal
dependency parsing, we learn a Support Vector Ma-
chine classifier (Yamada and Matsumoto, 2003) using
the features described in Section 5.
4.2 Graph-Based Parsing Model
One shortcoming of the shift-reduce dependency
parsing approach is that each transition decision
91
Figure 2: A setting for the graph-based parsing model: an initial dense graph G (left) with edge scores SCORE(e). The
resulting dependency tree as a spanning tree with the highest score over the edges (right).
made by the model is final, and cannot be revisited to
search for more globally optimal trees. Graph-based
models are an alternative dependency parsing model,
which assembles a graph with weighted edges be-
tween all pairs of words, and selects the tree-shaped
subset of this graph that gives the highest total score
(Fig. 2). Formally, a graph-based parser follows
Algorithm 2, where:
? W ? = W ? {Root}
? SCORE ? ((W ??R?W ) ? <) is a function
for scoring edges
? SPANNINGTREE is a function for selecting a
subset of edges that is a tree that spans over all
the nodes of the graph.
Algorithm 2 Graph-based dependency parsing
E ? {(e, SCORE(e)) : e ? (W ??R?W ))}
G? (W ?, E)
return SPANNINGTREE(G)
The SPANNINGTREE function is usually defined
using one of the efficient search techniques for find-
ing a maximum spanning tree. For temporal depen-
dency parsing, we use the Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967) which
solves this problem by iteratively selecting the edge
with the highest weight and removing edges that
would create cycles. The result is the globally op-
timal maximum spanning tree for the graph (Geor-
giadis, 2003).
The SCORE function is typically defined as a ma-
chine learning model that scores an edge based on a
set of features. For temporal dependency parsing, we
learn a model to predict edge scores via the Margin
Infused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003; Crammer et al, 2006) using the set of
features defined in Section 5.
5 Feature Design
The proposed parsing algorithms both rely on ma-
chine learning methods. The shift-reduce parser
(SRP) trains a machine learning classifier as the or-
acle o ? (C ? T ) to predict a transition t from a
parser configuration c = (L1, L2, Q,E), using node
features such as the heads of L1, L2 and Q, and
edge features from the already predicted temporal
relations in E. The graph-based maximum spanning
tree (MST) parser trains a machine learning model
to predict SCORE(e) for an edge e = (wi, rj , wk),
using features of the nodes wi and wk. The full set
of features proposed for both parsing models, de-
rived from the state-of-the-art systems for temporal
relation labeling, is presented in Table 2. Note that
both models share features that look at the nodes,
while only the shift-reduce parser has features for
previously classified edges.
6 Evaluations
Evaluations were performed using 10-fold cross-
validation on the fables annotated in Section 3. The
corpus contains 100 fables, a total of 14,279 tokens
and a total of 1136 annotated temporal relations. As
92
Feature SRP MST
Word
?? ??
Lemma
?? ??
Part of speech (POS) tag
?? ??
Suffixes
?? ??
Syntactically governing verb
?? ??
Governing verb lemma
?? ??
Governing verb POS tag
?? ??
Governing verb POS suffixes
?? ??
Prepositional phrase occurrence
?? ??
Dominated by auxiliary verb?
?? ??
Dominated by modal verb?
?? ??
Temporal signal word is nearby?
?? ??
Head word lemma
?? ??
Temporal relation labels of ai and its
leftmost and rightmost dependents
?
Temporal relation labels of ai?1?s
leftmost and rightmost dependents
?
Temporal relation labels of b1 and its
leftmost and rightmost dependents
?
Table 2: Features for the shift-reduce parser (SRP) and the
graph-based maximum spanning tree (MST) parser. The?? features are extracted from the heads of L1, L2 and Q
for SRP and from each node of the edge for MST.
only 40 instances of OVERLAP relations were an-
notated when neither INCLUDES nor IS INCLUDED
label matched, for evaluation purposes all instances
of these relations were merged into the temporally
coarse OVERLAP relation. Thus, the total number of
OVERLAP relations in the corpus grew from 40 to
258 annotations in total.
To evaluate the parsing models (SRP and MST)
we proposed two baselines. Both are based on the
assumption of linear temporal structures of narratives
as the temporal ordering process that was evidenced
by studies in human text rewriting (Hickmann, 2003).
The proposed baselines are:
? LinearSeq: A model that assumes all events
occur in the order they are written, adding links
between each pair of adjacent events, and label-
ing all links with the relation BEFORE.
? ClassifySeq: A model that links each pair of
adjacent events, but trains a pair-wise classifier
to predict the relation label for each pair. The
classifier is a support vector machine trained us-
ing the same features as the MST parser. This is
an approximation of prior work, where the pairs
of events to classify with a temporal relation
were given as an input to the system. (Note that
Section 6.2 will show that for our corpus, apply-
ing the model only to adjacent pairs of events
is quite competitive for just getting the basic
unlabeled link structure right.)
The Shift-Reduce parser (SRP; Section 4.1) and the
graph-based, maximum spanning tree parser (MST;
Section 4.2) are compared to these baselines.
6.1 Evaluation Criteria and Metrics
Model performance was evaluated using standard
evaluation criteria for parser evaluations:
Unlabeled Attachment Score (UAS) The fraction
of events whose head events were correctly predicted.
This measures whether the correct pairs of events
were linked, but not if they were linked by the correct
relations.
Labeled Attachment Score (LAS) The fraction
of events whose head events were correctly pre-
dicted with the correct relations. This measures both
whether the correct pairs of events were linked and
whether their temporal ordering is correct.
Tree Edit Distance In addition to the UAS and
LAS the tree edit distance score has been recently in-
troduced for evaluating dependency structures (Tsar-
faty et al, 2011). The tree edit distance score
for a tree pi is based on the following operations
? ? ? : ? = {DELETE, INSERT, RELABEL}:
? ? =DELETE delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
? ? =INSERT insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
? ? =RELABEL change the label of node v in pi
Any two trees pi1 and pi2 can be turned one into an-
other by a sequence of edit operations {?1, ..., ?n}.
93
UAS LAS UTEDS LTEDS
LinearSeq 0.830 0.581 0.689 0.549
ClassifySeq 0.830 0.581 0.689 0.549
MST 0.837 0.614? 0.710 0.571
SRP 0.830 0.647?? 0.712 0.596?
Table 3: Performance levels of temporal structure pars-
ing methods. A ? indicates that the model outperforms
LinearSeq and ClassifiedSeq at p < 0.01 and a ? indicates
that the model outperforms MST at p < 0.05.
Taking the shortest such sequence, the tree edit dis-
tance is calculated as the sum of the edit operation
costs divided by the size of the tree (i.e. the number
of words in the sentence). For temporal dependency
trees, we assume each operation costs 1.0. The fi-
nal score subtracts the edit distance from 1 so that
a perfect tree has score 1.0. The labeled tree edit
distance score (LTEDS) calculates sequences over
the tree with all its labeled temporal relations, while
the unlabeled tree edit distance score (UTEDS) treats
all edges as if they had the same label.
6.2 Results
Table 3 shows the results of the evaluation. The
unlabeled attachment score for the LinearSeq base-
line was 0.830, suggesting that annotators were most
often linking adjacent events. At the same time,
the labeled attachment score was 0.581, indicating
that even in fables, the stories are not simply linear,
that is, there are many relations other than BEFORE.
The ClassifySeq baseline performs identically to the
LinearSeq baseline, which shows that the simple pair-
wise classifier was unable to learn anything beyond
predicting all relations as BEFORE.
In terms of labeled attachment score, both de-
pendency parsing models outperformed the base-
line models ? the maximum spanning tree parser
achieved 0.614 LAS, and the shift-reduce parser
achieved 0.647 LAS. The shift-reduce parser also
outperformed the baseline models in terms of labeled
tree edit distance, achieving 0.596 LTEDS vs. the
baseline 0.549 LTEDS. These results indicate that de-
pendency parsing models are a good fit to our whole-
story timeline extraction task.
Finally, in comparing the two different depen-
dency parsing models, we observe that the shift-
reduce parser outperforms the maximum spanning
Error Type Num. %
OVERLAP? BEFORE 24 43.7
Attach to further head 18 32.7
Attach to nearer head 6 11.0
Other types of errors 7 12.6
Total 55 100
Table 4: Error distribution from the analysis of 55 errors
of the Shift-Reduce parsing model.
tree parser in terms of labeled attachment score
(0.647 vs. 0.614). It has been argued that graph-
based models like the maximum spanning tree parser
should be able to produce more globally consistent
and correct dependency trees, yet we do not observe
that here. A likely explanation for this phenomenon
is that the shift-reduce parsing model allows for fea-
tures describing previous parse decisions (similar to
the incremental nature of human parse decisions),
while the joint nature of the maximum spanning tree
parser does not.
6.3 Error Analysis
To better understand the errors our model is still mak-
ing, we examined two folds (55 errors in total in
20% of the evaluation data) and identified the major
categories of errors:
? OVERLAP? BEFORE: The model predicts the
correct head, but predicts its label as BEFORE,
while the correct label is OVERLAP.
? Attach to further head: The model predicts
the wrong head, and predicts as the head an
event that is further away than the true head.
? Attach to nearer head: The model predicts the
wrong head, and predicts as the head an event
that is closer than the true head.
Table 4 shows the distribution of the errors over these
categories. The two most common types of errors,
OVERLAP ? BEFORE and Attach to further head,
account for 76.4% of all the errors.
The most common type of error is predicting
a BEFORE relation when the correct answer is an
OVERLAP relation. Figure 3 shows an example of
such an error, where the model predicts that the
Spendthrift stood before he saw, while the anno-
tator indicates that the seeing happened during the
94
Figure 3: An OVERLAP ? BEFORE parser error. True
links are solid lines; the parser error is the dotted line.
Figure 4: Parser errors attaching to further away heads.
True links are solid lines; parser errors are dotted lines.
time in which he was standing. An analysis of these
OVERLAP? BEFORE errors suggests that they occur
in scenarios like this one, where the duration of one
event is significantly longer than the duration of an-
other, but there are no direct cues for these duration
differences. We also observe these types of errors
when one event has many sub-events, and therefore
the duration of the main event typically includes the
durations of all the sub-events. It might be possible
to address these kinds of errors by incorporating auto-
matically extracted event duration information (Pan
et al, 2006; Gusev et al, 2011).
The second most common error type of the model
is the prediction of a head event that is further away
than the head identified by the annotators. Figure 4
gives an example of such an error, where the model
predicts that the gathering includes the smarting, in-
stead of that the gathering includes the stung. The
second error in the figure is also of the same type.
In 65% of the cases where this type of error occurs,
it occurs after the parser had already made a label
classification error such as BEFORE ? OVERLAP.
So these errors may be in part due to the sequen-
tial nature of shift-reduce parsing, where early errors
propagate and cause later errors.
7 Discussion and Conclusions
In this article, we have presented an approach to tem-
poral information extraction that represents the time-
line of a story as a temporal dependency tree. We
have constructed an evaluation corpus where such
temporal dependencies have been annotated over a
set of 100 children?s stories. We have introduced two
dependency parsing techniques for extracting story
timelines and have shown that both outperform a rule-
based baseline and a prior-work-inspired pair-wise
classification baseline. Comparing the two depen-
dency parsing models, we have found that a shift-
reduce parser, which more closely mirrors the incre-
mental processing of our human annotators, outper-
forms a graph-based maximum spanning tree parser.
Our error analysis of the shift-reduce parser revealed
that being able to estimate differences in event dura-
tions may play a key role in improving parse quality.
We have focused on children?s stories in this study,
in part because they typically have simpler temporal
structures (though not so simple that our rule-based
baseline could parse them accurately). In most of our
fables, there were only one or two characters with at
most one or two simultaneous sequences of actions.
In other domains, the timeline of a text is likely to
be more complex. For example, in clinical records,
descriptions of patients may jump back and forth
between the patient history, the current examination,
and procedures that have not yet happened.
In future work, we plan to investigate how to best
apply the dependency structure approach to such
domains. One approach might be to first group
events into their narrative containers (Pustejovsky
and Stubbs, 2011), for example, grouping together all
events linked to the time of a patient?s examination.
Then within each narrative container, our dependency
parsing approach could be applied. Another approach
might be to join the individual timeline trees into a
document-wide tree via discourse relations or rela-
tions to the document creation time. Work on how
humans incrementally process such timelines in text
may help to decide which of these approaches holds
the most promise.
Acknowledgements
We would like to thank the anonymous reviewers
for their constructive comments. This research was
partially funded by the TERENCE project (EU FP7-
257410) and the PARIS project (IWT SBO 110067).
95
References
[Amigo? et al2011] Enrique Amigo?, Javier Artiles, Qi Li,
and Heng Ji. 2011. An evaluation framework for aggre-
gated temporal information extraction. In SIGIR-2011
Workshop on Entity-Oriented Search.
[Artiles et al2011] Javier Artiles, Qi Li, Taylor Cas-
sidy, Suzanne Tamang, and Heng Ji. 2011.
CUNY BLENDER TAC-KBP2011 temporal slot fill-
ing system description. In Text Analytics Conference
(TAC2011).
[Bethard and Martin2007] Steven Bethard and James H.
Martin. 2007. CU-TMP: Temporal relation classifica-
tion using syntactic and semantic features. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 129?132, Prague,
Czech Republic, June. ACL.
[Bethard et al2007] Steven Bethard, James H. Martin, and
Sara Klingenstein. 2007. Finding temporal structure in
text: Machine learning of syntactic temporal relations.
International Journal of Semantic Computing (IJSC),
1(4):441?458, 12.
[Bethard et al2012] Steven Bethard, Oleksandr
Kolomiyets, and Marie-Francine Moens. 2012.
Annotating narrative timelines as temporal dependency
structures. In Proceedings of the International
Conference on Linguistic Resources and Evaluation,
Istanbul, Turkey, May. ELRA.
[Boguraev and Ando2005] Branimir Boguraev and
Rie Kubota Ando. 2005. TimeBank-driven TimeML
analysis. In Annotating, Extracting and Reasoning
about Time and Events. Springer.
[Bramsen et al2006] P. Bramsen, P. Deshpande, Y.K. Lee,
and R. Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 189?
198. ACL.
[Brewer and Lichtenstein1982] William F. Brewer and Ed-
ward H. Lichtenstein. 1982. Stories are to entertain: A
structural-affect theory of stories. Journal of Pragmat-
ics, 6(5-6):473 ? 486.
[Chambers and Jurafsky2008] N. Chambers and D. Juraf-
sky. 2008. Jointly combining implicit constraints im-
proves temporal ordering. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706. ACL.
[Cheng et al2007] Yuchang Cheng, Masayuki Asahara,
and Yuji Matsumoto. 2007. NAIST.Japan: Tempo-
ral relation identification using dependency parsed tree.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 245?248,
Prague, Czech Republic, June. ACL.
[Chu and Liu1965] Y. J. Chu and T.H. Liu. 1965. On
the shortest arborescence of a directed graph. Science
Sinica, pages 1396?1400.
[Covington2001] M.A. Covington. 2001. A fundamental
algorithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
[Crammer and Singer2003] K. Crammer and Y. Singer.
2003. Ultraconservative online algorithms for multi-
class problems. Journal of Machine Learning Research,
3:951?991.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
[Edmonds1967] J. Edmonds. 1967. Optimum branchings.
Journal of Research of the National Bureau of Stan-
dards, pages 233?240.
[Georgiadis2003] L. Georgiadis. 2003. Arborescence op-
timization problems solvable by Edmonds? algorithm.
Theoretical Computer Science, 301(1-3):427?437.
[Gupta and Ji2009] Prashant Gupta and Heng Ji. 2009.
Predicting unknown time arguments based on cross-
event propagation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ?09, pages
369?372, Stroudsburg, PA, USA. ACL.
[Gusev et al2011] Andrey Gusev, Nathanael Chambers,
Divye Raj Khilnani, Pranav Khaitan, Steven Bethard,
and Dan Jurafsky. 2011. Using query patterns to learn
the duration of events. In Proceedings of the Interna-
tional Conference on Computational Semantics, pages
145?154.
[Hayes and Krippendorff2007] A.F. Hayes and K. Krip-
pendorff. 2007. Answering the call for a standard
reliability measure for coding data. Communication
Methods and Measures, 1(1):77?89.
[Hickmann2003] Maya Hickmann. 2003. Children?s Dis-
course: Person, Space and Time Across Languages.
Cambridge University Press, Cambridge, UK.
[Johnson-Laird1980] P.N. Johnson-Laird. 1980. Men-
tal models in cognitive science. Cognitive Science,
4(1):71?115.
[Krippendorff2004] K. Krippendorff. 2004. Content anal-
ysis: An introduction to its methodology. Sage Publica-
tions, Inc.
[Linguistic Data Consortium2005] Linguistic Data Con-
sortium. 2005. ACE (Automatic Content Extraction)
English annotation guidelines for events version 5.4.3
2005.07.01.
[Llorens et al2010] Hector Llorens, Estela Saquete, and
Borja Navarro. 2010. TIPSem (English and Spanish):
Evaluating CRFs and semantic roles in TempEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291, Uppsala, Sweden,
July. ACL.
96
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
[McIntyre and Lapata2009] N. McIntyre and M. Lapata.
2009. Learning to tell tales: A data-driven approach to
story generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
217?225. ACL.
[Nivre2008] J. Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computational
Linguistics, 34(4):513?553.
[Pan et al2006] Feng Pan, Rutu Mulkar, and Jerry R.
Hobbs. 2006. Learning event durations from event
descriptions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 393?400, Sydney, Australia, July. ACL.
[Pustejovsky and Stubbs2011] J. Pustejovsky and
A. Stubbs. 2011. Increasing informativeness in
temporal annotation. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 152?160. ACL.
[Pustejovsky et al2003a] James Pustejovsky, Jose?
Castan?o, Robert Ingria, Roser Saury?, Robert
Gaizauskas, Andrea Setzer, and Graham Katz. 2003a.
TimeML: Robust specification of event and temporal
expressions in text. In Proceedings of the Fifth
International Workshop on Computational Semantics
(IWCS-5), Tilburg.
[Pustejovsky et al2003b] James Pustejovsky, Patrick
Hanks, Roser Saury?, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth Sundheim,
David Day, Lisa Ferro, and Marcia Lazo. 2003b.
The TimeBank corpus. In Proceedings of Corpus
Linguistics, pages 647?656.
[Tsarfaty et al2011] R. Tsarfaty, J. Nivre, and E. Ander-
sson. 2011. Evaluating dependency parsing: Robust
and heuristics-free cross-annotation evaluation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 385?396. ACL.
[UzZaman and Allen2010] Naushad UzZaman and James
Allen. 2010. TRIPS and TRIOS system for TempEval-
2: Extracting temporal information from text. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 276?283, Uppsala, Sweden, July.
ACL.
[Verhagen et al2007] Marc Verhagen, Robert Gaizauskas,
Frank Schilder, Graham Katz, and James Pustejovsky.
2007. SemEval2007 Task 15: TempEval temporal rela-
tion identification. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.
[Verhagen et al2010] Marc Verhagen, Roser Saur??, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 Task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 57?62, Stroudsburg, PA, USA. ACL.
[Yamada and Matsumoto2003] H. Yamada and Y. Mat-
sumoto. 2003. Statistical dependency analysis with
support vector machines. In Proceedings of IWPT.
[Yoshikawa et al2009] K. Yoshikawa, S. Riedel, M. Asa-
hara, and Y. Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
405?413. ACL.
97
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Descending-Path Convolution Kernel for Syntactic Structures
Chen Lin
1
, Timothy Miller
1
, Alvin Kho
1
, Steven Bethard
2
,
Dmitriy Dligach
1
, Sameer Pradhan
1
and Guergana Savova
1
,
1
Children?s Hospital Boston Informatics Program and Harvard Medical School
{firstname.lastname}@childrens.harvard.edu
2
Department of Computer and Information Sciences, University of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
1 Introduction
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson?s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al, 2007).
2 Background
2.1 Syntax-based Tree Kernels
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
81
a)
NP
DT
a
NN
cat
NP
DT
a
NN
dog
b)
NP
DT
a
NN
cat
NP
DT
a
JJ
fat
NN
cat
c)
S
ADVP
RB
here
NP
PRP
she
VP
VBZ
comes
S
NP
PRP
she
VP
VBZ
comes
ADVP
RB
here
Figure 1: Three example tree pairs.
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching ? substructure (ii) ? while also generating
some fragments that violate grammatical intuitions.
Zhang et al (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP ? DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al?s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP ? DT NN ). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al, 2007) and the route kernel
(RtT) (Aiolli et al, 2009). SpT uses a q-gram
? a sequence of connected vertices of length q ?
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a?b?c is
different from a?b?c. RtT uses a set of routes as
basic structures, which observes grammar rules by
NP
DT
a
NN
cat
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
recording the index of a neighbor node.
2.2 Temporal Relation Extraction
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al, 2003) and the AQUAINT
TimeML corpus
1
. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data
2
(Styler et al, 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
3 Methods
Here we decribe the Descending Path Kernel
(DPK).
1
http://www.timeml.org
2
http://thyme.healthnlp.org
82
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T , dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v ? ? ? ? ? dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, ? ? ? , n, and path
i
(T ) is the frequency
of the i-th descending path in T . We represent T as
a vector of frequencies of all its descending paths:
?(T ) = (path
1
(T ), ? ? ? , path
n
(T )).
The similarity between any two trees T
1
and T
2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T
1
, T
2
) = ??(T
1
),?(T
2
)?.
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP?DT JJ NN versus NP?DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T
1
and T
2
:
K(T
1
, T
2
) = ??(T
1
),?(T
2
)?
=
?
i
path
i
(T
1
) ? path
i
(T
2
)
=
?
n
1
?N
1
?
n
2
?N
2
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
=
?
n
1
?N
1
n
2
?N
2
C(n
1
, n
2
)
(1)
where N
1
and N
2
are the sets of nodes in T
1
and
T
2
respectively, i indexes the set of possible paths,
I
path
i
(n) is an indicator function that is 1 iff the
descending path
i
is rooted at node n or 0 other-
wise. C(n
1
, n
2
) counts the number of common
descending paths rooted at nodes n
1
and n
2
:
C(n
1
, n
2
) =
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
C(n
1
, n
2
) can be computed in polynomial time by
the following recursive rules:
Rule 1: If n
1
and n
2
have different labels (e.g.,
?DT? versus ?NN?), then C(n1, n2) = 0;
Rule 2: Else if n
1
and n
2
have the same labels
and are both pre-terminals (POS tags), then
C(n
1
, n
2
) = 1 +
{
1 if term(n
1
) = term(n
2
)
0 otherwise.
where term(n) is the terminal symbol under n;
Rule 3: Else if n
1
and n
2
have the same labels
and they are not both pre-terminals, then:
C(n
1
, n
2
) = 1 +
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter ? to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
C(n
1
, n
2
) = 1 +
{
? if term(n
1
) = term(n
2
)
0 otherwise.
and Rule 3 becomes:
C(n
1
, n
2
) = 1 + ?
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n
1
and n
2
, and the number of
matching nodes below n
1
and n
2
.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
3.1 Relationship with other kernels
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
83
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O
(
?|N
1
||N
2
|
)
b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O
(
?
2
|N
1
||N
2
|
)
b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O
(
?
3
|N
1
||N
2
|
)
b 36 15 0.65
c 127 34 0.42
Table 1: Comparison of the worst case computa-
tional complexicity (? - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK? SST? PTK. In Table 1, given
? = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O
(
|N
1
||N
2
|
)
since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of ? (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of ?
2
to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of ?
3
. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering ? in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP? NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP? DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
4 Evaluation
We applied DPK to two published temporal relation
extraction systems: (Miller et al, 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
4.1 Narrative Container Discovery
The task here as described by Miller et al (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
K
C
(s
1
, s
2
) = ?
P
?
p=1
K
T
(t
p
1
, t
p
2
)+K
F
(f
1
, f
2
) (2)
where s
i
is an instance object composed of flat fea-
tures f
i
and a syntactic tree t
i
. A syntactic tree t
i
84
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel K
T
, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
? -weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel K
F
(linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
4.2 Cleartk-TimeML
We tested one sub-task from TempEval-2013 ?
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn?t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as K
F
to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
4.3 Results and Discussion
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
Features K
T
P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
5 Conclusion
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
6 Acknowledgements
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
85
References
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17?24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10?14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185?193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292?299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18?26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53?68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143?154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200?207.
86
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501?506,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
An Annotation Framework for Dense Event Ordering
Taylor Cassidy
IBM Research
taylor.cassidy.ctr@mail.mil
Bill McDowell
Carnegie Mellon University
forkunited@gmail.com
Nathanael Chambers
US Naval Academy
nchamber@usna.edu
Steven Bethard
Univ. of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Today?s event ordering research is heav-
ily dependent on annotated corpora. Cur-
rent corpora influence shared evaluations
and drive algorithm development. Partly
due to this dependence, most research fo-
cuses on partial orderings of a document?s
events. For instance, the TempEval com-
petitions and the TimeBank only annotate
small portions of the event graph, focusing
on the most salient events or on specific
types of event pairs (e.g., only events in the
same sentence). Deeper temporal reason-
ers struggle with this sparsity because the
entire temporal picture is not represented.
This paper proposes a new annotation pro-
cess with a mechanism to force annotators
to label connected graphs. It generates 10
times more relations per document than the
TimeBank, and our TimeBank-Dense cor-
pus is larger than all current corpora. We
hope this process and its dense corpus en-
courages research on new global models
with deeper reasoning.
1 Introduction
The TimeBank Corpus (Pustejovsky et al, 2003)
ushered in a wave of data-driven event ordering
research. It provided for a common dataset of re-
lations between events and time expressions that
allowed the community to compare approaches.
Later corpora and competitions have based their
tasks on the TimeBank setup. This paper ad-
dresses one of its shortcomings: sparse annotation.
We describe a new annotation framework (and a
TimeBank-Dense corpus) that we believe is needed
to fulfill the data needs of deeper reasoners.
The TimeBank includes a small subset of all
possible relations in its documents. The annota-
tors were instructed to label relations critical to the
document?s understanding. The result is a sparse la-
beling that leaves much of the document unlabeled.
The TempEval contests have largely followed suit
and focused on specific types of event pairs. For
instance, TempEval (Verhagen et al, 2007) only
labeled relations between events that syntactically
dominated each other. This paper is the first attempt
to annotate a document?s entire temporal graph.
A consequence of focusing on all relations is a
shift from the traditional classification task, where
the system is given a pair of events and asked only
to label the type of relation, to an identification task,
where the system must determine for itself which
events in the document to pair up. For example, in
TempEval-1 and 2 (Verhagen et al, 2007; Verha-
gen et al, 2010), systems were given event pairs
in specific syntactic positions: events and times in
the same noun phrase, main events in consecutive
sentences, etc. We now aim for a shift in the com-
munity wherein all pairs are considered candidates
for temporal ordering, allowing researchers to ask
questions such as: how must algorithms adapt to
label the complete graph of pairs, and if the more
difficult and ambiguous event pairs are included,
how must feature-based learners change?
We are not the first to propose these questions,
but this paper is the first to directly propose the
means by which they can be addressed. The stated
goal of TempEval-3 (UzZaman et al, 2013) was to
focus on relation identification instead of classifica-
tion, but the training and evaluation data followed
the TimeBank approach where only a subset of
event pairs were labeled. As a result, many systems
focused on classification, with the top system clas-
sifying pairs in only three syntactic constructions
501
There were four or five people inside, 
and they just started firing 
 
Ms. Sanders was hit several times and 
was  pronounced dead at the scene. 
 
The other customers fled, and the 
police said it did not appear that anyone 
else was injured. 
There were four or five people inside, 
and they just started firing 
 
Ms. Sanders was hit several times and 
was pronounced dead at the scene. 
 
The other customers fled, and the 
police said it did not appear that anyone 
else was injured. 
Current Systems & Evaluations This Proposal 
Figure 1: A TimeBank annotated document is on the left, and this paper?s TimeBank-Dense annotation is
on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations.
(Bethard, 2013). We describe the first annotation
framework that forces annotators to annotate all
pairs
1
. With this new process, we created a dense
ordering of document events that can properly eval-
uate both relation identification and relation anno-
tation. Figure 1 illustrates one document before
and after our new annotations.
2 Previous Annotation Work
The majority of corpora and competitions for event
ordering contain sparse annotations. Annotators for
the original TimeBank (Pustejovsky et al, 2003)
only annotated relations judged to be salient by
the annotator. Subsequent TempEval competitions
(Verhagen et al, 2007; Verhagen et al, 2010; Uz-
Zaman et al, 2013) mostly relied on the TimeBank,
but also aimed to improve coverage by annotating
relations between all events and times in the same
sentence. However, event tokens that were men-
tioned fewer than 20 times were excluded and only
one TempEval task considered relations between
events in different sentences. In practical terms, the
resulting evaluations remained sparse.
A major dilemma underlying these sparse tasks
is that the unlabeled event/time pairs are ambigu-
ous. Each unlabeled pair holds 3 possibilities:
1. The annotator looked at the pair of events and
decided that no temporal relation exists.
2. The annotator did not look at the pair of
events, so a relation may or may not exist.
3. The annotator failed to look at the pair of
events, so a single relation may exist.
Training and evaluation of temporal reasoners is
hampered by this ambiguity. To combat this, our
1
As discussed below, all pairs in a given window size.
Events Times Rels R
TimeBank 7935 1414 6418 0.7
Bramsen 2006 627 ? 615 1.0
TempEval-07 6832 1249 5790 0.7
TempEval-10 5688 2117 4907 0.6
TempEval-13 11145 2078 11098 0.8
Kolomiyets-12 1233 ? 1139 0.9
Do 2012
2
324 232 3132 5.6
This work 1729 289 12715 6.3
Table 1: Events, times, relations and the ratio of
relations to events + times (R) in various corpora.
annotation adopts the VAGUE relation introduced
by TempEval 2007, and our approach forces anno-
tators to use it. This is the only work that includes
such a mechanism.
This paper is not the first to look into more dense
annotations. Bramsen et al (2006) annotated multi-
sentence segments of text to build directed acyclic
graphs. Kolomiyets et al (2012) annotated ?tem-
poral dependency structures?, though they only
focused on relations between pairs of events. Do
et al (2012) produced the densest annotation, but
?the annotator was not required to annotate all pairs
of event mentions, but as many as possible?. The
current paper takes a different tack to annotation
by requiring annotators to label every possible pair
of events/times in a given window. Thus this work
is the first annotation effort that can guarantee its
event/time graph to be strongly connected.
Table 1 compares the size and density of our
corpus to others. Ours is the densest and it contains
the largest number of temporal relations.
2
Do et al (2012) reports 6264 relations, but this includes
both the relations and their inverses. We thus halve the count
502
3 A Framework for Dense Annotation
Frameworks for annotating text typically have two
independent facets: (1) the practical means of how
to label the text, and (2) the higher-level rules about
when something should be labeled. The first is
often accomplished through a markup language,
and we follow prior work in adopting TimeML here.
The second facet is the focus of this paper: when
should an annotator label an ordering relation?
Our proposal starts with documents that have al-
ready been annotated with events, time expressions,
and document creation times (DCT). The following
sentence serves as our motivating example:
Police confirmed Friday that the body
found along a highway in San Juan be-
longed to Jorge Hernandez.
This sentence is represented by a 4 node graph (3
events and 1 time). In a completely annotated graph
it would have 6 edges (relations) connecting the
nodes. In the TimeBank, from which this sentence
is drawn, only 3 of the 6 edges are labeled.
The impact of these annotation decisions (i.e.,
when to annotate a relation) can be significant. In
this example, a learner must somehow deal with
the 3 unlabeled edges. One option is to assume that
they are vague or ambiguous. However, all 6 edges
have clear well-defined ordering relations:
belonged BEFORE confirmed
belonged BEFORE found
found BEFORE confirmed
belonged BEFORE Friday
confirmed IS INCLUDED IN Friday
found IS INCLUDED IN Friday
3
Learning algorithms handle these unlabeled
edges by making incorrect assumptions, or by ig-
noring large parts of the temporal graph. Sev-
eral models with rich temporal reasoners have
been published, but since they require more con-
nected graphs, improvement over pairwise classi-
fiers have been minimal (Chambers and Jurafsky,
2008; Yoshikawa et al, 2009). This paper thus
proposes an annotation process that builds denser
graphs with formal properties that learners can rely
on, such as locally complete subgraphs.
3.1 Ensuring Dense Graphs
While the ideal goal is to create a complete graph,
the time it would take to hand-label n(n ? 1)/2
for accurate comparison to other corpora.
3
Revealed by the previous sentence (not shown here).
edges is prohibitive. We approximate completeness
by creating locally complete graphs over neigh-
boring sentences. The resulting event graph for a
document is strongly connected, but not complete.
Specifically, the following edge types are included:
1. Event-Event, Event-Time, and Time-Time
pairs in the same sentence
2. Event-Event, Event-Time, and Time-Time
pairs between the current and next sentence
3. Event-DCT pairs for every event in the text
4. Time-DCT pairs for every time expression in
the text
Our process requires annotators to annotate the
above edge types, enforced via an annotation tool.
We describe the relation set and this tool next.
3.1.1 Temporal Relations
The TimeBank corpus uses 14 relations based on
the Allen interval relations. The TempEval contests
have used a small set of relations (TempEval-1) and
the larger set of 14 relations (TempEval-3). Pub-
lished work has mirrored this trend, and different
groups focus on different aspects of the semantics.
We chose a middle ground between coarse and
fine-grained distinctions for annotation, settling on
6 relations: before, after, includes, is included, si-
multaneous, and vague. We do not adopt a more
fine-grained set because we annotate pairs that are
far more ambiguous than those considered in previ-
ous efforts. Decisions between relations like before
and immediately before can complicate an already
difficult task. The added benefit of a corpus (or
working system) that makes fine-grained distinc-
tions is also not clear. We lean toward higher an-
notator agreement with relations that have greater
separation between their semantics
4
.
3.1.2 Enforcing Annotation
Imposing the above rules on annotators requires
automated assistance. We built a new tool that
reads TimeML formatted text, and computes the
set of required edges. Annotators are prompted to
assign a label for each edge, and skipping edges is
prohibited.
5
The tool is unique in that it includes
a transitive reasoner that infers relations based on
the annotator?s latest annotations. For example,
4
For instance, a relation like starts is a special case of in-
cludes if events are viewed as open intervals, and immediately
before is a special case of before. We avoid this overlap and
only use includes and before
5
Note that annotators are presented with pairs in order from
document start to finish, starting with the first two events.
503
if event e
1
IS INCLUDED in t
1
, and t
1
BEFORE
e
2
, the tool automatically labels e
1
BEFORE e
2
.
The transitivity inference is run after each input
label, and the human annotator cannot override
the inferences. This prohibits the annotator from
entering edges that break transitivity. As a result,
several properties are ensured through this process:
the graph (1) is a strongly connected graph, (2) is
consistent with no contradictions, and (3) has all
required edges labeled. These 3 properties are new
to all current ordering corpora.
3.2 Annotation Guidelines
Since the annotation tool frees the annotators from
the decision of when to label an edge, the focus is
now what to label each edge. This section describes
the guidelines for dense annotation.
The 80% confidence rule: The decision to label
an edge as VAGUE instead of a defined temporal
relation is critical. We adopted an 80% rule that in-
structed annotators to choose a specific non-vague
relation if they are 80% confident that it was the
writer?s intent that a reader infer that relation. By
not requiring 100% confidence, we allow for alter-
native interpretations that conflict with the chosen
edge label as long as that alternative is sufficiently
unlikely. In practice, annotators had different inter-
pretations of what constitutes 80% certainty, and
this generated much discussion. We mitigated these
disagreements with the following rule.
Majority annotator agreement: An edge?s la-
bel is the relation that received a majority of an-
notator votes, otherwise it is marked VAGUE. If a
document has 2 annotators, both have to agree on
the relation or it is labeled VAGUE. A document
with 3 annotators requires 2 to agree. This agree-
ment rule acts as a check to our 80% confidence
rule, backing off to VAGUE when decisions are un-
certain (arguably, this is the definition of VAGUE).
We also encouraged consistent labelings with
guidelines inspired by Bethard and Martin (2008).
Modal and conditional events: interpreted with
a possible worlds analysis. The core event was
treated as having occurred, whether or not the text
implied that it had occurred. For example,
They [EVENT expect] him to [EVENT
cut] costs throughout the organization.
This event pair is ordered (expect before cut) since
the expectation occurs before the cutting (in the
possible world where the cutting occurs). Negated
events and hypotheticals are treated similarly. One
assumes the event does occur, and all other events
are ordered accordingly. Negated states like ?is not
anticipating? are interpreted as though the antici-
pation occurs, and surrounding events are ordered
with regard to its presumed temporal span.
Aspectual Events: annotated as IS INCLUDED
in their event arguments. For instance, events that
describe the manner in which another event is per-
formed are considered encompassed by the broader
event. Consider the following example:
The move may [EVENT help] [EVENT
prevent] Martin Ackerman from making
a run at the computer-services concern.
This event pair is assigned the relation (help IS IN-
CLUDED in prevent) because the help event is not
meaningful on its own. It describes the proportion
of the preventing accounted for by the move. In
TimeBank, the intentional action class is used in-
stead of the aspectual class in this case, but we still
consider it covered by this guideline.
Events that attribute a property: to a person
or event are interpreted to end when the entity ends.
For instance, ?the talk is nonsense? evokes a non-
sense event with an end point that coincides with
the end of the talk.
Time Expressions: the words now and today
were given ?long now? interpretations if the words
could be replaced with nowadays and not change
the meaning of their sentences. The time?s dura-
tion starts sometime in the past and INCLUDES the
DCT. If nowadays is not suitable, then the now was
INCLUDED IN the DCT.
Generic Events: can be ordered with respect to
each other, but must be VAGUE with respect to
nearby non-generic events.
4 TimeBank-Dense: corpus statistics
We chose a subset of TimeBank documents for our
new corpus: TimeBank-Dense. This provided an
initial labeling of events and time expressions. Us-
ing the tool described above, we annotated 36 ran-
dom documents with at least two annotators each.
These 36 were annotated with 4 times as many
relations as the entire 183 document TimeBank.
The four authors of this paper were the four an-
notators. All four annotated the same initial docu-
ment, conflicts and disagreements were discussed,
504
Annotated Relation Count
BEFORE 2590 INCLUDES 836
AFTER 2104 INCLUDED IN 1060
SIMULTAN. 215 VAGUE 5910
Total Relations: 12715
Table 2: Relation counts in TimeBank-Dense.
and guidelines were updated accordingly. The rest
of the documents were then annotated indepen-
dently. Document annotation was not random, but
we mixed pairs of authors where time constraints al-
lowed. Table 2 shows the relation counts in the final
corpus, and Table 3 gives the annotator agreement.
We show precision (holding one annotation as gold)
and kappa computed on the 4 types of pairs from
section 3.1. Micro-averaged precision was 65.1%,
compared to TimeBank?s 77%. Kappa ranged from
.56-.64, a slight drop from TimeBank?s .71.
The vague relation makes up 46% of the rela-
tions. This is the first empirical count of how many
temporal relations in news articles are truly vague.
Our lower agreement is likely due to the more
difficult task. Table 5 breaks down the individual
disagreements. The most frequent pertained to the
VAGUE relation. Practically speaking, VAGUE was
applied to the final graph if either annotator chose
it. This seems appropriate since a disagreement be-
tween annotators implies that the relation is vague.
The following example illustrates the difficulty
of labeling edges with a VAGUE relation:
No one was hurt, but firefighters or-
dered the evacuation of nearby homes
and said they?ll monitor the ground.
Both annotators chose VAGUE to label ordered and
said because the order is unclear. However, they
disagreed on evacuation with monitor. One chose
VAGUE, but the other chose IS INCLUDED. There is
a valid interpretation where a monitoring process
has already begun, and continues after the evacua-
tion. This interpretation reached 80% confidence
for one annotator, but not the other. In the face of
such a disagreement, the pair is labeled VAGUE.
How often do these disagreements occur? Ta-
ble 4 shows the 3 sources: (1) mutual vague: anno-
tators agree it is vague, (2) partial vague: one anno-
tator chooses vague, but the other does not, and (3)
no vague: annotators choose conflicting non-vague
relations. Only 17% of these disagreements are due
to hard conflicts (no vague). The released corpus
includes these 3 fine-grained VAGUE relations.
Annotators # Links Prec Kappa
A and B 9282 .65 .56
A and D 1605 .72 .63
B and D 279 .70 .64
C and D 1549 .65 .57
Table 3: Agreement between different annotators.
# Vague
Mutual VAGUE 1657 (28%)
Partial VAGUE 3234 (55%)
No VAGUE 1019 (17%)
Table 4: VAGUE relation origins. Partial vague:
one annotator does not choose vague. No vague:
neither annotator chooses vague.
b a i ii s v
b 1776 22 88 37 21 192
a 17 1444 32 102 9 155
i 71 34 642 45 23 191
ii 81 76 40 826 31 230
s 12 8 25 28 147 29
v 500 441 289 356 64 1197
Table 5: Relation agreement between the two main
annotators. Most disagreements involved VAGUE.
5 Conclusion
We described our annotation framework that pro-
duces corpora with formal guarantees about the an-
notated graph?s structure. Both the annotation tool
and the new TimeBank-Dense corpus are publicly
available.
6
This is the first corpus with guarantees
of connectedness, consistency, and a semantics for
unlabeled edges. We hope to encourage a shift in
the temporal ordering community to consider the
entire document when making local decisions. Fur-
ther work is needed to handle difficult pairs with
the VAGUE relation. We look forward to evaluating
new algorithms on this dense corpus.
Acknowledgments
This work was supported, in part, by the Johns
Hopkins Human Language Technology Center of
Excellence. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors. We also give thanks
to Benjamin Van Durme for assistance and insight.
6
http://www.usna.edu/Users/cs/nchamber/caevo/
505
References
Steven Bethard, William J Corvey, Sara Klingenstein,
and James H Martin. 2008. Building a corpus of
temporal-causal structure. In LREC.
Steven Bethard. 2013. Cleartk-timeml: A minimal-
ist approach to tempeval 2013. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 10?14, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
P. Bramsen, P. Deshpande, Y.K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In Proceedings of
the 2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 189?198. ACL.
N. Chambers and D. Jurafsky. 2008. Jointly com-
bining implicit constraints improves temporal order-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
698?706. ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 677?687, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 88?97, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
75?80. Association for Computational Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62. As-
sociation for Computational Linguistics.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal rela-
tions with Markov Logic. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
405?413. ACL.
506
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Stanford CoreNLP Natural Language Processing Toolkit
Christopher D. Manning
Linguistics & Computer Science
Stanford University
manning@stanford.edu
Mihai Surdeanu
SISTA
University of Arizona
msurdeanu@email.arizona.edu
John Bauer
Dept of Computer Science
Stanford University
horatio@stanford.edu
Jenny Finkel
Prismatic Inc.
jrfinkel@gmail.com
Steven J. Bethard
Computer and Information Sciences
U. of Alabama at Birmingham
bethard@cis.uab.edu
David McClosky
IBM Research
dmcclosky@us.ibm.com
Abstract
We describe the design and use of the
Stanford CoreNLP toolkit, an extensible
pipeline that provides core natural lan-
guage analysis. This toolkit is quite widely
used, both in the research NLP community
and also among commercial and govern-
ment users of open source NLP technol-
ogy. We suggest that this follows from
a simple, approachable design, straight-
forward interfaces, the inclusion of ro-
bust and good quality analysis compo-
nents, and not requiring use of a large
amount of associated baggage.
1 Introduction
This paper describe the design and development of
Stanford CoreNLP, a Java (or at least JVM-based)
annotation pipeline framework, which provides
most of the common core natural language pro-
cessing (NLP) steps, from tokenization through to
coreference resolution. We describe the original
design of the system and its strengths (section 2),
simple usage patterns (section 3), the set of pro-
vided annotators and how properties control them
(section 4), and how to add additional annotators
(section 5), before concluding with some higher-
level remarks and additional appendices. While
there are several good natural language analysis
toolkits, Stanford CoreNLP is one of the most
used, and a central theme is trying to identify the
attributes that contributed to its success.
2 Original Design and Development
Our pipeline system was initially designed for in-
ternal use. Previously, when combining multiple
natural language analysis components, each with
their own ad hoc APIs, we had tied them together
with custom glue code. The initial version of the
Tokeniza)on*
Sentence*Spli0ng*
Part4of4speech*Tagging*
Morphological*Analysis*
Named*En)ty*Recogni)on*
Syntac)c*Parsing*
Other*Annotators*
Coreference*Resolu)on**
Raw*text*
Execu)
on*Flow
* Annota)on*Object*
Annotated*text*
(tokenize)*
(ssplit)*
(pos)*
(lemma)*
(ner)*
(parse)*
(dcoref)*
(gender, sentiment)!
Figure 1: Overall system architecture: Raw text
is put into an Annotation object and then a se-
quence of Annotators add information in an analy-
sis pipeline. The resulting Annotation, containing
all the analysis information added by the Annota-
tors, can be output in XML or plain text forms.
annotation pipeline was developed in 2006 in or-
der to replace this jumble with something better.
A uniform interface was provided for an Annota-
tor that adds some kind of analysis information to
some text. An Annotator does this by taking in an
Annotation object to which it can add extra infor-
mation. An Annotation is stored as a typesafe het-
erogeneous map, following the ideas for this data
type presented by Bloch (2008). This basic archi-
tecture has proven quite successful, and is still the
basis of the system described here. It is illustrated
in figure 1. The motivations were:
? To be able to quickly and painlessly get linguis-
tic annotations for a text.
? To hide variations across components behind a
common API.
? To have a minimal conceptual footprint, so the
system is easy to learn.
? To provide a lightweight framework, using plain
Java objects (rather than something of heav-
ier weight, such as XML or UIMA?s Common
Analysis System (CAS) objects).
55
In 2009, initially as part of a multi-site grant
project, the system was extended to be more easily
usable by a broader range of users. We provided
a command-line interface and the ability to write
out an Annotation in various formats, including
XML. Further work led to the system being re-
leased as free open source software in 2010.
On the one hand, from an architectural perspec-
tive, Stanford CoreNLP does not attempt to do ev-
erything. It is nothing more than a straightforward
pipeline architecture. It provides only a Java API.
1
It does not attempt to provide multiple machine
scale-out (though it does provide multi-threaded
processing on a single machine). It provides a sim-
ple concrete API. But these requirements satisfy
a large percentage of potential users, and the re-
sulting simplicity makes it easier for users to get
started with the framework. That is, the primary
advantage of Stanford CoreNLP over larger frame-
works like UIMA (Ferrucci and Lally, 2004) or
GATE (Cunningham et al., 2002) is that users do
not have to learn UIMA or GATE before they can
get started; they only need to know a little Java.
In practice, this is a large and important differ-
entiator. If more complex scenarios are required,
such as multiple machine scale-out, they can nor-
mally be achieved by running the analysis pipeline
within a system that focuses on distributed work-
flows (such as Hadoop or Spark). Other systems
attempt to provide more, such as the UIUC Cu-
rator (Clarke et al., 2012), which includes inter-
machine client-server communication for process-
ing and the caching of natural language analyses.
But this functionality comes at a cost. The system
is complex to install and complex to understand.
Moreover, in practice, an organization may well
be committed to a scale-out solution which is dif-
ferent from that provided by the natural language
analysis toolkit. For example, they may be using
Kryo or Google?s protobuf for binary serialization
rather than Apache Thrift which underlies Cura-
tor. In this case, the user is better served by a fairly
small and self-contained natural language analysis
system, rather than something which comes with
a lot of baggage for all sorts of purposes, most of
which they are not using.
On the other hand, most users benefit greatly
from the provision of a set of stable, robust, high
1
Nevertheless, it can call an analysis component written in
other languages via an appropriate wrapper Annotator, and
in turn, it has been wrapped by many people to provide Stan-
ford CoreNLP bindings for other languages.
quality linguistic analysis components, which can
be easily invoked for common scenarios. While
the builder of a larger system may have made over-
all design choices, such as how to handle scale-
out, they are unlikely to be an NLP expert, and
are hence looking for NLP components that just
work. This is a huge advantage that Stanford
CoreNLP and GATE have over the empty tool-
box of an Apache UIMA download, something
addressed in part by the development of well-
integrated component packages for UIMA, such
as ClearTK (Bethard et al., 2014), DKPro Core
(Gurevych et al., 2007), and JCoRe (Hahn et al.,
2008). However, the solution provided by these
packages remains harder to learn, more complex
and heavier weight for users than the pipeline de-
scribed here.
These attributes echo what Patricio (2009) ar-
gued made Hibernate successful, including: (i) do
one thing well, (ii) avoid over-design, and (iii)
up and running in ten minutes or less! Indeed,
the design and success of Stanford CoreNLP also
reflects several other of the factors that Patricio
highlights, including (iv) avoid standardism, (v)
documentation, and (vi) developer responsiveness.
While there are many factors that contribute to the
uptake of a project, and it is hard to show causal-
ity, we believe that some of these attributes ac-
count for the fact that Stanford CoreNLP is one of
the more used NLP toolkits. While we certainly
have not done a perfect job, compared to much
academic software, Stanford CoreNLP has gained
from attributes such as clear open source licens-
ing, a modicum of attention to documentation, and
attempting to answer user questions.
3 Elementary Usage
A key design goal was to make it very simple to
set up and run processing pipelines, from either
the API or the command-line. Using the API, run-
ning a pipeline can be as easy as figure 2. Or,
at the command-line, doing linguistic processing
for a file can be as easy as figure 3. Real life is
rarely this simple, but the ability to get started us-
ing the product with minimal configuration code
gives new users a very good initial experience.
Figure 4 gives a more realistic (and complete)
example of use, showing several key properties of
the system. An annotation pipeline can be applied
to any text, such as a paragraph or whole story
rather than just a single sentence. The behavior of
56
Annotator pipeline = new StanfordCoreNLP();
Annotation annotation = new Annotation(
"Can you parse my sentence?");
pipeline.annotate(annotation);
Figure 2: Minimal code for an analysis pipeline.
export StanfordCoreNLP_HOME /where/installed
java -Xmx2g -cp $StanfordCoreNLP_HOME/*
edu.stanford.nlp.StanfordCoreNLP
-file input.txt
Figure 3: Minimal command-line invocation.
import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;
public class StanfordCoreNlpExample {
public static void main(String[] args) throws IOException {
PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");
Properties props = new Properties();
props.setProperty("annotators",
"tokenize, ssplit, pos, lemma, ner, parse");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(
"This is a short sentence. And this is another.");
pipeline.annotate(annotation);
pipeline.xmlPrint(annotation, xmlOut);
// An Annotation is a Map and you can get and use the
// various analyses individually. For instance, this
// gets the parse tree of the 1st sentence in the text.
List<CoreMap> sentences = annotation.get(
CoreAnnotations.SentencesAnnotation.class);
if (sentences != null && sentences.size() > 0) {
CoreMap sentence = sentences.get(0);
Tree tree = sentence.get(TreeAnnotation.class);
PrintWriter out = new PrintWriter(System.out);
out.println("The first sentence parsed is:");
tree.pennPrint(out);
}
}
}
Figure 4: A simple, complete example program.
annotators in a pipeline is controlled by standard
Java properties in a Properties object. The most
basic property to specify is what annotators to run,
in what order, as shown here. But as discussed be-
low, most annotators have their own properties to
allow further customization of their usage. If none
are specified, reasonable defaults are used. Run-
ning the pipeline is as simple as in the first exam-
ple, but then we show two possibilities for access-
ing the results. First, we convert the Annotation
object to XML and write it to a file. Second, we
show code that gets a particular type of informa-
tion out of an Annotation and then prints it.
Our presentation shows only usage in Java, but
the Stanford CoreNLP pipeline has been wrapped
by others so that it can be accessed easily from
many languages, including Python, Ruby, Perl,
Scala, Clojure, Javascript (node.js), and .NET lan-
guages, including C# and F#.
4 Provided annotators
The annotators provided with StanfordCoreNLP
can work with any character encoding, making use
of Java?s good Unicode support, but the system
defaults to UTF-8 encoding. The annotators also
support processing in various human languages,
providing that suitable underlying models or re-
sources are available for the different languages.
The system comes packaged with models for En-
glish. Separate model packages provide support
for Chinese and for case-insensitive processing of
English. Support for other languages is less com-
plete, but many of the Annotators also support
models for French, German, and Arabic (see ap-
pendix B), and building models for further lan-
guages is possible using the underlying tools. In
this section, we outline the provided annotators,
focusing on the English versions. It should be
noted that some of the models underlying annota-
tors are trained from annotated corpora using su-
pervised machine learning, while others are rule-
based components, which nevertheless often re-
quire some language resources of their own.
tokenize Tokenizes the text into a sequence of to-
kens. The English component provides a PTB-
style tokenizer, extended to reasonably handle
noisy and web text. The corresponding com-
ponents for Chinese and Arabic provide word
and clitic segmentation. The tokenizer saves the
character offsets of each token in the input text.
cleanxml Removes most or all XML tags from
the document.
ssplit Splits a sequence of tokens into sentences.
truecase Determines the likely true case of tokens
in text (that is, their likely case in well-edited
text), where this information was lost, e.g., for
all upper case text. This is implemented with
a discriminative model using a CRF sequence
tagger (Finkel et al., 2005).
pos Labels tokens with their part-of-speech (POS)
tag, using a maximum entropy POS tagger
(Toutanova et al., 2003).
lemma Generates the lemmas (base forms) for all
tokens in the annotation.
gender Adds likely gender information to names.
ner Recognizes named (PERSON, LOCATION,
ORGANIZATION, MISC) and numerical
(MONEY, NUMBER, DATE, TIME, DU-
RATION, SET) entities. With the default
57
annotators, named entities are recognized
using a combination of CRF sequence taggers
trained on various corpora (Finkel et al., 2005),
while numerical entities are recognized using
two rule-based systems, one for money and
numbers, and a separate state-of-the-art system
for processing temporal expressions (Chang
and Manning, 2012).
regexner Implements a simple, rule-based NER
over token sequences building on Java regular
expressions. The goal of this Annotator is to
provide a simple framework to allow a user to
incorporate NE labels that are not annotated in
traditional NL corpora. For example, a default
list of regular expressions that we distribute
in the models file recognizes ideologies (IDE-
OLOGY), nationalities (NATIONALITY), reli-
gions (RELIGION), and titles (TITLE).
parse Provides full syntactic analysis, including
both constituent and dependency representa-
tion, based on a probabilistic parser (Klein and
Manning, 2003; de Marneffe et al., 2006).
sentiment Sentiment analysis with a composi-
tional model over trees using deep learning
(Socher et al., 2013). Nodes of a binarized tree
of each sentence, including, in particular, the
root node of each sentence, are given a senti-
ment score.
dcoref Implements mention detection and both
pronominal and nominal coreference resolution
(Lee et al., 2013). The entire coreference graph
of a text (with head words of mentions as nodes)
is provided in the Annotation.
Most of these annotators have various options
which can be controlled by properties. These can
either be added to the Properties object when cre-
ating an annotation pipeline via the API, or spec-
ified either by command-line flags or through a
properties file when running the system from the
command-line. As a simple example, input to the
system may already be tokenized and presented
one-sentence-per-line. In this case, we wish the
tokenization and sentence splitting to just work by
using the whitespace, rather than trying to do any-
thing more creative (be it right or wrong). This can
be accomplished by adding two properties, either
to a properties file:
tokenize.whitespace: true
ssplit.eolonly: true
in code:
/** Simple annotator for locations stored in a gazetteer. */
package org.foo;
public class GazetteerLocationAnnotator implements Annotator {
// this is the only method an Annotator must implement
public void annotate(Annotation annotation) {
// traverse all sentences in this document
for (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {
// loop over all tokens in sentence (the text already tokenized)
List<CoreLabel> toks = sentence.get(TokensAnnotation.class);
for (int start = 0; start < toks.size(); start++) {
// assumes that the gazetteer returns the token index
// after the match or -1 otherwise
int end = Gazetteer.isLocation(toks, start);
if (end > start) {
for (int i = start; i < end; i ++) {
toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");
}
}
}
}
}
}
Figure 5: An example of a simple custom anno-
tator. The annotator marks the words of possibly
multi-word locations that are in a gazetteer.
props.setProperty("tokenize.whitespace", "true");
props.setProperty("ssplit.eolonly", "true");
or via command-line flags:
-tokenize.whitespace -ssplit.eolonly
We do not attempt to describe all the properties
understood by each annotator here; they are avail-
able in the documentation for Stanford CoreNLP.
However, we note that they follow the pattern of
being x.y, where x is the name of the annotator
that they apply to.
5 Adding annotators
While most users work with the provided annota-
tors, it is quite easy to add additional custom an-
notators to the system. We illustrate here both how
to write an Annotator in code and how to load it
into the Stanford CoreNLP system. An Annotator
is a class that implements three methods: a sin-
gle method for analysis, and two that describe the
dependencies between analysis steps:
public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
The information in an Annotation is updated in
place (usually in a non-destructive manner, by
adding new keys and values to the Annotation).
The code for a simple Annotator that marks loca-
tions contained in a gazetteer is shown in figure 5.
2
Similar code can be used to write a wrapper Anno-
tator, which calls some pre-existing analysis com-
ponent, and adds its results to the Annotation.
2
The functionality of this annotator is already provided by
the regexner annotator, but it serves as a simple example.
58
While building an analysis pipeline, Stanford
CoreNLP can add additional annotators to the
pipeline which are loaded using reflection. To pro-
vide a new Annotator, the user extends the class
edu.stanford.nlp.pipeline.Annotator
and provides a constructor with the signature
(String, Properties). Then, the user adds
the property
customAnnotatorClass.FOO: BAR
to the properties used to create the pipeline. If
FOO is then added to the list of annotators, the
class BAR will be loaded to instantiate it. The
Properties object is also passed to the constructor,
so that annotator-specific behavior can be initial-
ized from the Properties object. For instance, for
the example above, the properties file lines might
be:
customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotator
annotators: tokenize,ssplit,locgaz
locgaz.maxLength: 5
6 Conclusion
In this paper, we have presented the design
and usage of the Stanford CoreNLP system, an
annotation-based NLP processing pipeline. We
have in particular tried to emphasize the proper-
ties that we feel have made it successful. Rather
than trying to provide the largest and most engi-
neered kitchen sink, the goal has been to make it
as easy as possible for users to get started using
the framework, and to keep the framework small,
so it is easily comprehensible, and can easily be
used as a component within the much larger sys-
tem that a user may be developing. The broad us-
age of this system, and of other systems such as
NLTK (Bird et al., 2009), which emphasize acces-
sibility to beginning users, suggests the merits of
this approach.
A Pointers
Website: http://nlp.stanford.edu/software/
corenlp.shtml
Github: https://github.com/stanfordnlp/CoreNLP
Maven: http://mvnrepository.com/artifact/edu.
stanford.nlp/stanford-corenlp
License: GPL v2+
Stanford CoreNLP keeps the models for ma-
chine learning components and miscellaneous
other data files in a separate models jar file. If you
are using Maven, you need to make sure that you
list the dependency on this models file as well as
the code jar file. You can do that with code like the
following in your pom.xml. Note the extra depen-
dency with a classifier element at the bottom.
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
</dependency>
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
<classifier>models</classifier>
</dependency>
B Human language support
We summarize the analysis components supported
for different human languages in early 2014.
Annotator Ara- Chi- Eng- Fre- Ger-
bic nese lish nch man
Tokenize X X X X X
Sent. split X X X X X
Truecase X
POS X X X X X
Lemma X
Gender X
NER X X X
RegexNER X X X X X
Parse X X X X X
Dep. Parse X X
Sentiment X
Coref. X
C Getting the sentiment of sentences
We show a command-line for sentiment analysis.
$ cat sentiment.txt
I liked it.
It was a fantastic experience.
The plot move rather slowly.
$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators
tokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/
english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/
englishPCFG.ser.gz ... done [1.4 sec].
Adding annotator sentiment
Ready to process: 1 files, skipped 0, total 1
Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt ... writing to /Users/manning/Software/
stanford-corenlp-full-2014-01-04/sentiment.txt.xml {
Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt [0.583 seconds]
} [1.219 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.0 sec.
59
MorphaAnnotator: 0.0 sec.
ParserAnnotator: 0.4 sec.
SentimentAnnotator: 0.1 sec.
TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec.
Pipeline setup: 3.0 sec.
Total time for StanfordCoreNLP pipeline: 4.2 sec.
$ grep sentiment sentiment.txt.xml
<sentence id="1" sentimentValue="3" sentiment="Positive">
<sentence id="2" sentimentValue="4" sentiment="Verypositive">
<sentence id="3" sentimentValue="1" sentiment="Negative">
D Use within UIMA
The main part of using Stanford CoreNLP within
the UIMA framework (Ferrucci and Lally, 2004)
is mapping between CoreNLP annotations, which
are regular Java classes, and UIMA annotations,
which are declared via XML type descriptors
(from which UIMA-specific Java classes are gen-
erated). A wrapper for CoreNLP will typically de-
fine a subclass of JCasAnnotator ImplBase whose
process method: (i) extracts UIMA annotations
from the CAS, (ii) converts UIMA annotations to
CoreNLP annotations, (iii) runs CoreNLP on the
input annotations, (iv) converts the CoreNLP out-
put annotations into UIMA annotations, and (v)
saves the UIMA annotations to the CAS.
To illustrate part of this process, the ClearTK
(Bethard et al., 2014) wrapper converts CoreNLP
token annotations to UIMA annotations and saves
them to the CAS with the following code:
int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);
int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);
String pos = tokenAnn.get(PartOfSpeechAnnotation.class);
String lemma = tokenAnn.get(LemmaAnnotation.class);
Token token = new Token(jCas, begin, end);
token.setPos(pos);
token.setLemma(lemma);
token.addToIndexes();
where Token is a UIMA type, declared as:
<typeSystemDescription>
<name>Token</name>
<types>
<typeDescription>
<name>org.cleartk.token.type.Token</name>
<supertypeName>uima.tcas.Annotation</supertypeName>
<features>
<featureDescription>
<name>pos</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>lemma</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
</types>
</typeSystemDescription>
References
Steven Bethard, Philip Ogren, and Lee Becker. 2014.
ClearTK 2.0: Design patterns for machine learning
in UIMA. In LREC 2014.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Joshua Bloch. 2008. Effective Java. Addison Wesley,
Upper Saddle River, NJ, 2nd edition.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC 2012.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
learned to stop worrying and love NLP pipelines).
In LREC 2012.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In ACL 2002.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, pages 449?454.
David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL 43, pages 363?370.
I. Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,
M. Weimer, and T. Zesch. 2007. Darmstadt knowl-
edge processing repository based on UIMA. In
First Workshop on Unstructured Information Man-
agement Architecture at GLDV 2007, T?ubingen.
U. Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,
Poprat M, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the Julie lab UIMA component
registry. In LREC 2008.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, Advances in
Neural Information Processing Systems, volume 15,
pages 3?10. MIT Press.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Anthony Patricio. 2009. Why this project is success-
ful? https://community.jboss.org/wiki/
WhyThisProjectIsSuccessful.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP 2013, pages 1631?1642.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL 3, pages 252?259.
60
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Back to Basics for Monolingual Alignment: Exploiting Word Similarity and
Contextual Evidence
Md Arafat Sultan?, Steven Bethard? and Tamara Sumner?
?Institute of Cognitive Science and Department of Computer Science
University of Colorado Boulder
?Department of Computer and Information Sciences
University of Alabama at Birmingham
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
Abstract
We present a simple, easy-to-replicate monolin-
gual aligner that demonstrates state-of-the-art
performance while relying on almost no su-
pervision and a very small number of external
resources. Based on the hypothesis that words
with similar meanings represent potential pairs
for alignment if located in similar contexts, we
propose a system that operates by finding such
pairs. In two intrinsic evaluations on alignment
test data, our system achieves F1 scores of 88?
92%, demonstrating 1?3% absolute improve-
ment over the previous best system. Moreover,
in two extrinsic evaluations our aligner out-
performs existing aligners, and even a naive
application of the aligner approaches state-of-
the-art performance in each extrinsic task.
1 Introduction
Monolingual alignment is the task of discovering and
aligning similar semantic units in a pair of sentences
expressed in a natural language. Such alignments pro-
vide valuable information regarding how and to what
extent the two sentences are related. Consequently,
alignment is a central component of a number of
important tasks involving text comparison: textual
entailment recognition, textual similarity identifica-
tion, paraphrase detection, question answering and
text summarization, to name a few.
The high utility of monolingual alignment has
spawned significant research on the topic in the re-
cent past. Major efforts that have treated alignment
as a standalone problem (MacCartney et al., 2008;
Thadani and McKeown, 2011; Yao et al., 2013a) are
primarily supervised, thanks to the manually aligned
corpus with training and test sets from Microsoft Re-
search (Brockett, 2007). Primary concerns of such
work include both quality and speed, due to the fact
that alignment is frequently a component of larger
NLP tasks.
Driven by similar motivations, we seek to devise a
lightweight, easy-to-construct aligner that produces
high-quality output and is applicable to various end
tasks. Amid a variety of problem formulations and
ingenious approaches to alignment, we take a step
back and examine closely the effectiveness of two
frequently made assumptions: 1) Related semantic
units in two sentences must be similar or related
in their meaning, and 2) Commonalities in their se-
mantic contexts in the respective sentences provide
additional evidence of their relatedness (MacCartney
et al., 2008; Thadani and McKeown, 2011; Yao et al.,
2013a; Yao et al., 2013b). Alignment, based solely
on these two assumptions, reduces to finding the best
combination of pairs of similar semantic units in sim-
ilar contexts.
Exploiting existing resources to identify similarity
of semantic units, we search for robust techniques
to identify contextual commonalities. Dependency
trees are a commonly used structure for this purpose.
While they remain a central part of our aligner, we
expand the horizons of dependency-based alignment
beyond exact matching by systematically exploiting
the notion of ?type equivalence? with a small hand-
crafted set of equivalent dependency types. In addi-
tion, we augment dependency-based alignment with
surface-level text analysis.
While phrasal alignments are important and have
219
Transactions of the Association for Computational Linguistics, 2 (2014) 219?230. Action Editor: Alexander Koller.
Submitted 11/2013; Revised 1/2014; Published 5/2014. c?2014 Association for Computational Linguistics.
been investigated in multiple studies, we focus pri-
marily on word alignments (which have been shown
to form the vast majority of alignments (? 95%)
in multiple human-annotated corpora (Yao et al.,
2013b)), keeping the framework flexible enough to
allow incorporation of phrasal alignments in future.
Evaluation of our aligner on the benchmark dataset
reported in (Brockett, 2007) shows an F1 score of
91.7%: a 3.1% absolute improvement over the previ-
ous best system (Yao et al., 2013a), corresponding
to a 27.2% error reduction. It shows superior perfor-
mance also on the dataset reported in (Thadani et
al., 2012). Additionally, we present results of two
extrinsic evaluations, namely textual similarity iden-
tification and paraphrase detection. Our aligner not
only outperforms existing aligners in each task, but
also approaches top systems for the extrinsic tasks.
2 Background
Monolingual alignment has been applied to various
NLP tasks including textual entailment recognition
(Hickl et al., 2006; Hickl and Bensley, 2007), para-
phrase identification (Das and Smith, 2009; Madnani
et al., 2012), and textual similarity assessment (Ba?r
et al., 2012; Han et al., 2013) ? in some cases ex-
plicitly, i.e., as a separate module. But many such
systems resort to simplistic and/or ad-hoc strategies
for alignment and in most such work, the alignment
modules were not separately evaluated on alignment
benchmarks, making their direct assessment difficult.
With the introduction of the MSR alignment cor-
pus (Brockett, 2007) developed from the second
Recognizing Textual Entailment challenge data (Bar-
Haim et al., 2006), direct evaluation and comparison
of aligners became possible. The first aligner trained
and evaluated on the corpus was a phrasal aligner
called MANLI (MacCartney et al., 2008). It repre-
sents alignments as sets of different edit operations
(where a sequence of edits turns one input sentence
into the other) and finds an optimal set of edits via
a simulated annealing search. Weights of different
edit features are learned from the training set of the
MSR alignment corpus using a perceptron learning
algorithm. MANLI incorporates only shallow fea-
tures characterizing contextual similarity: relative
positions of the two phrases being aligned (or not) in
the two sentences and boolean features representing
whether or not the preceding and following tokens of
the two phrases are similar.
Thadani and McKeown (2011) substituted
MANLI?s simulated annealing-based decoding with
integer linear programming, and achieved a consider-
able speed-up. More importantly for our discussion,
they found contextual evidence in the form of syn-
tactic constraints useful in better aligning stop words.
Thadani et al. (2012) further extended the model by
adding features characterizing dependency arc edits,
effectively bringing stronger influence of contextual
similarity into alignment decisions. Again the perfor-
mance improved consequently.
The most successful aligner to date both in terms
of accuracy and speed, called JacanaAlign, was de-
veloped by Yao et al. (2013a). In contrast to the
earlier systems, JacanaAlign is a word aligner that
formulates alignment as a sequence labeling prob-
lem. Each word in the source sentence is labeled
with the corresponding target word index if an align-
ment is found. It employs a conditional random field
to assign the labels and uses a feature set similar to
MANLI?s in terms of the information they encode
(with some extensions). Contextual features include
only semantic match of the left and the right neigh-
bors of the two words and their POS tags. Even
though JacanaAlign outperformed the MANLI en-
hancements despite having less contextual features,
it is difficult to compare the role of context in the
two models because of the large paradigmatic dispar-
ity. An extension of JacanaAlign was proposed for
phrasal alignments in (Yao et al., 2013b), but the
contextual features remained largely unchanged.
Noticeable in all the above systems is the use of
contextual evidence as a feature for alignment, but
in our opinion, not to an extent sufficient to harness
its full potential. Even though deeper dependency-
based modeling of contextual commonalities can be
found in some other studies (Kouylekov and Magnini,
2005; Chambers et al., 2007; Chang et al., 2010; Yao
et al., 2013c), we believe there is further scope for
systematic exploitation of contextual evidence for
alignment, which we aim to do in this work.
On the contrary, word semantic similarity has been
a central component of most aligners; various mea-
sures of word similarity have been utilized, including
string similarity, resource-based similarity (derived
from one or more lexical resources like WordNet)
220
Align
identical
word
sequences
Align
named
entities
Align
content
words
Align
stop
words
Figure 1: System overview
and distributional similarity (computed from word
co-occurrence statistics in large corpora). An impor-
tant trade-off between precision, coverage and speed
exists here and aligners commonly rely on only a
subset of these measures (Thadani and McKeown,
2011; Yao et al., 2013a). We use the Paraphrase
Database (PPDB) (Ganitkevitch et al., 2013), which
is a large resource of lexical and phrasal paraphrases
constructed using bilingual pivoting (Bannard and
Callison-Burch, 2005) over large parallel corpora.
3 System
Our system operates as a pipeline of alignment mod-
ules that differ in the types of word pairs they align.
Figure 1 is a simplistic representation of the pipeline.
Each module makes use of contextual evidence to
make alignment decisions. In addition, the last two
modules are informed by a word semantic similarity
algorithm. Because of their phrasal nature, we treat
named entities separately from other content words.
The rationale behind the order in which the modules
are arranged is discussed later in this section (3.3.5).
Before discussing each alignment module in de-
tail, we describe the system components that identify
word and contextual similarity.
3.1 Word Similarity
The ability to correctly identify semantic similarity
between words is crucial to our aligner, since con-
textual evidence is important only for similar words.
Instead of treating word similarity as a continuous
variable, we define three levels of similarity.
The first level is an exact word or lemma match
which is represented by a similarity score of 1. The
second level represents semantic similarity between
two terms which are not identical. To identify such
word pairs, we employ the Paraphrase Database
(PPDB)1. We use the largest (XXXL) of the PPDB?s
lexical paraphrase packages and treat all pairs iden-
tically by ignoring the accompanying statistics. We
1http://paraphrase.org
customize the resource by removing pairs of identi-
cal words or lemmas and adding lemmatized forms
of the remaining pairs. For now, we use the term
ppdbSim to refer to the similarity of each word pair
in this modified version of PPDB (which is a value in
(0, 1)) and later explain how we determine it (Section
3.3.5). Finally, any pair of different words which is
absent in PPDB is assigned a zero similarity score.
3.2 Extracting Contextual Evidence
Our alignment modules collect contextual evidence
from two complementary sources: syntactic depen-
dencies and words occurring within a small textual
vicinity of the two words to be aligned. The applica-
tion of each kind assumes a common principle of min-
imal evidence. Formally, given two input sentences
S and T , we consider two words s ? S and t ? T to
form a candidate pair for alignment if ?rs ? S and
?rt ? T such that:
1. (s, t) ? <Sim and (rs, rt) ? <Sim, where
<Sim is a binary relation indicating sufficient
semantic relatedness between the members of
each pair (? ppdbSim in our case).
2. (s, rs) ? <C1 and (t, rt) ? <C2 , such that
<C1 ? <C2 ; where <C1 and <C2 are binary re-
lations representing specific types of contextual
relationships between two words in a sentence
(e.g., an nsubj dependency between a verb and
a noun). The symbol ? represents equivalence
between two relationships, including identical-
ity.
Note that the minimal-evidence assumption holds
a single piece of contextual evidence as sufficient
support for a potential alignment; but as we discuss
later in this section, an evidence for word pair (s, t)
(where s ? S and t ? S) may not lead to an align-
ment if there exists a competing pair (s?, t) or (s, t?)
with more evidence (where s? ? S and t? ? T ).
In the rest of this section, we elaborate the different
forms of contextual relationships we exploit along
with the notion of equivalence between relationships.
3.2.1 Syntactic Dependencies
Dependencies can be important sources of con-
textual evidence. Two nsubj children rs and rt of
two verbs s ? S and t ? T , for example, pro-
vide evidence for not only an (s, t) alignment, but
221
S: He wrote a book .
nsubj
dobj
det
T : I read the book he wrote .
nsubj
dobj
det
rcmod
nsubj
Figure 2: Equivalent dependency types: dobj and rcmod
also an (rs, rt) alignment if (s, t) ? <Sim and
(rs, rt) ? <Sim. (We adopt the Stanford typed de-
pendencies (de Marneffe and Manning, 2008).)
Moreover, dependency types can exhibit equiva-
lence; consider the two sentences in Figure 2. The
dobj dependency in S is equivalent to the rcmod
dependency in T (dobj ? rcmod, following our ear-
lier notation) since they represent the same semantic
relation between identical word pairs in the two sen-
tences. To be able to use such evidence for alignment,
we need to go beyond exact matching of dependen-
cies and develop a mapping among dependency types
that encodes such equivalence. Note also that the
parent-child roles are opposite for the two depen-
dency types in the above example, a scenario that
such a mapping must accommodate.
The four possible such scenarios regarding parent-
child orientations are shown in Figure 3. If (s, t) ?
<Sim and (rs, rt) ? <Sim (represented by bidirec-
tional arrows), then each orientation represents a set
of possible ways in which the S and T dependen-
cies (unidirectional arrows) can provide evidence of
similarity between the contexts of s in S and t in T .
Each such set comprises equivalent dependency type
pairs for that orientation. In the example of Figure 2,
(dobj, rcmod) is such a pair for orientation (c), given
s = t = ?wrote? and rs = rt = ?book?.
We apply the notion of dependency type equiva-
lence to intra-category alignment of content words
in four major lexical categories: verbs, nouns,
adjectives and adverbs (the Stanford POS tag-
ger (Toutanova et al., 2003) is used to identify the
categories). Table 1 shows dependency type equiva-
lences for each lexical category of s and t.
The ??? sign on column 5 of some rows repre-
sents a duplication of the column 4 content of the
s
rs
t
rt
rs
s
rt
t
s
rs
t
rt
s
rs
t
rt
(a) (b) (c) (d)
Figure 3: Parent-child orientations in dependencies
same row. For each row, columns 4 and 5 show two
sets of dependency types; each member of the first
is equivalent to each member of the second for the
current orientation (column 1) and lexical categories
of the associated words (columns 2 and 3). For exam-
ple, row 2 represents the fact that an agent relation
(between s and rs; s is the parent) is equivalent to an
nsubj relation (between t and rt; t is the parent).
Note that the equivalences are fundamentally re-
dundant across different orientations. For example,
row 2 (which is presented as an instance of ori-
entation (a)) can also be presented as an instance
of orientation (b) with POS(s)=POS(t)=noun and
POS(rs)=POS(rt)=verb. We avoid such redundancy
for compactness. For the same reason, the equiva-
lence of dobj and rcmod in Figure 2 is shown in the
table only as an instance of orientation (c) and not as
an instance of orientation (d) (in general, this is why
orientations (b) and (d) are absent in the table).
We present dependency-based contextual evidence
extraction in Algorithm 1. (The Stanford dependency
parser (de Marneffe et al., 2006) is used to extract the
dependencies.) Given a word pair (si, tj) from the in-
put sentences S and T , it collects contextual evidence
(as indexes of rsi and rtj with a positive similarity)
for each matching row in Table 1. An exact match
of the two dependencies is also considered a piece
of evidence. Note that Table 1 only considers con-
tent word pairs (si, tj) such that POS(si)=POS(tj),
but as 90% of all content word alignments in the
MSR alignment dev set are within the same lexical
category, this is a reasonable set to start with.
3.2.2 Textual Neighborhood
While equivalent dependencies can provide strong
contextual evidence, they can not ensure high recall
because, a) the ability to accurately extract depen-
222
Orientation POS(s, t) POS(rs, rt) S Dependency Types T Dependency Types
s
rs
t
rt
verb
verb {purpcl, xcomp} ??
noun
{agent, nsubj, xsubj} ??
{dobj, nsubjpass, rel} ??
{tmod, prep in, prep at, prep on} ??
{iobj, prep to} ??
noun
verb {infmod, partmod, rcmod} ??
(a) noun {pos, nn, prep of, prep in, prep at, prep for} ??adjective {amod, rcmod} ??
s
rs
t
rt verb verb
{conj and} ??
{conj or} ??
{conj nor} ??
noun {dobj, nsubjpass, rel} {infmod, partmod, rcmod}
adjective {acomp} {cop, csubj}
noun noun
{conj and} ??
{conj or} ??
{conj nor} ??
adjective {amod, rcmod} {nsubj}
adjective adjective
{conj and} ??
{conj or} ??
(c) {conj nor} ??
adverb adverb
{conj and} ??
{conj or} ??
{conj nor} ??
Table 1: Equivalent dependency structures
Algorithm 1: depContext(S, T, i, j, EQ)
Input:
1. S, T : Sentences to be aligned
2. i: Index of a word in S
3. j: Index of a word in T
4. EQ: Dependency type equivalences (Table 1)
Output: context = {(k, l)}: pairs of word indexes
context? {(k, l) : wordSim(sk, tl) > 01
? (i, k, ?s) ? dependencies(S)2
? (j, l, ?t) ? dependencies(T )3
? POS(si) = POS(tj) ? POS(sk) = POS(tl)4
? (?s = ?t5
? (POS(si), POS(sk), ?s, ?t) ? EQ))}6
dencies is limited by the accuracy of the parser, and
b) we investigate equivalence types for only inter-
lexical-category alignment in this study. Therefore
we apply an additional model of word context: the
textual neighborhood of s in S and t in T .
Extraction of contextual evidence for content
words from textual neighborhood is described in Al-
gorithm 2. Like the dependency-based module, it
accumulates evidence for each (si, tj) pair by in-
specting multiple pairs of neighboring words. But in-
stead of aligning only words within a lexical category,
Algorithm 2: textContext(S, T, i, j, STOP)
Input:
1. S, T : Sentences to be aligned
2. i: Index of a word in S
3. j: Index of a word in T
4. STOP: A set of stop words
Output: context = {(k, l)}: pairs of word indexes
Ci ? {k : k ? [i? 3, i+ 3] ? k 6= i ? sk 6? STOP}1
Cj ? {l : l ? [j ? 3, j + 3] ? l 6= j ? tl 6? STOP}2
context? Ci ? Cj3
this module also performs inter-category alignment,
considering content words within a (3, 3) window
of si and tj as neighbors. We implement relational
equivalence (?) here by holding any two positions
within the window equally contributive and mutually
comparable as sources of contextual evidence.
3.3 The Alignment Algorithm
We now describe each alignment module in the
pipeline and their sequence of operation.
3.3.1 Identical Word Sequences
The presence of a common word sequence in S
and T is indicative of an (a) identical, and (b) con-
223
textually similar word in the other sentence for each
word in the sequence. We observe the results of
aligning identical words in such sequences of length
n containing at least one content word. This simple
heuristic demonstrates a high precision (? 97%) on
the MSR alignment dev set for n ? 2, and therefore
we consider membership in such sequences as the
simplest form of contextual evidence in our system
and align all identical word sequence pairs in S and
T containing at least one content word. From this
point on, we will refer to this module as wsAlign.
3.3.2 Named Entities
We align named entities separately to enable the
alignment of full and partial mentions (and acronyms)
of the same entity. We use the Stanford Named Entity
Recognizer (Finkel et al., 2005) to identify named
entities in S and T . After aligning the exact term
matches, any unmatched term of a partial mention
is aligned to all terms in the full mention. The mod-
ule recognizes only first-letter acronyms and aligns
an acronym to all terms in the full mention of the
corresponding name.
Since named entities are instances of nouns, named
entity alignment is also informed by contextual ev-
idence (which we discuss in the next section), but
happens before alignment of other generic content
words. Parents (or children) of a named entity are
simply the parents (or children) of its head word. We
will refer to this module as a method named neAlign
from this point on.
3.3.3 Content Words
Extraction of contextual evidence for promising
content word pairs has already been discussed in
Section 3.2, covering both dependency-based context
and textual context.
Algorithm 3 (cwDepAlign) describes the
dependency-based alignment process. For each
potentially alignable pair (si, tj), the dependency-
based context is extracted as described in Algorithm
1, and context similarity is calculated as the sum
of the word similarities of the (sk, tl) context word
pairs (lines 2-7). (The wordSim method returns a
similarity score in {0, ppdbSim, 1}.) The alignment
score of the (si, tj) pair is then a weighted sum
of word and contextual similarity (lines 8-12).
(We discuss how the weights are set in Section
Algorithm 3: cwDepAlign(S, T,EQ,AE , w, STOP)
Input:
1. S, T : Sentences to be aligned
2. EQ: Dependency type equivalences (Table 1)
3. AE : Already aligned word pair indexes
4. w: Weight of word similarity relative to contex-
tual similarity
5. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
?? ?; ?? ? ?; ?? ?1
for si ? S, tj ? T do2
if si 6? STOP ? ??tl : (i, l) ? AE3
? tj 6? STOP ? ??sk : (k, j) ? AE4
? wordSim(si, tj) > 0 then5
context? depContext(S, T, i, j, EQ)6
contextSim?
?
(k,l)?context
wordSim(sk, tl)
7
if contextSim > 0 then8
?? ? ? {(i, j)}9
??(i, j)? context10
?(i, j)? w ? wordSim(si, tj)11
+(1? w) ? contextSim12
Linearize and sort ? in decreasing order of ?(i, j)13
A? ?14
for (i, j) ? ? do15
if ??l : (i, l) ? A16
???k : (k, j) ? A then17
A? A ? {(i, j)}18
for (k, l) ? ??(i, j) do19
if ??q : (k, q) ? A ?AE20
???p : (p, l) ? A ?AE then21
A? A ? {(k, l)}22
3.3.5.) The module then aligns (si, tj) pairs with
non-zero evidence in decreasing order of this score
(lines 13-18). In addition, it aligns all the pairs
that contributed contextual evidence for the (si, tj)
alignment (lines 19-22). Note that we implement a
one-to-one alignment whereby a word gets aligned
at most once within the module.
Algorithm 4 (cwTextAlign) presents alignment
based on similarities in the textual neighborhood. For
each potentially alignable pair (si, tj), Algorithm 2
is used to extract the context, which is a set of neigh-
boring content word pairs (lines 2-7). The contextual
similarity is the sum of the similarities of these pairs
224
Algorithm 4: cwTextAlign(S, T,AE , w, STOP)
Input:
1. S, T : Sentences to be aligned
2. AE : Existing alignments by word indexes
3. w: Weight of word similarity relative to contex-
tual similarity
4. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
?? ?; ?? ?1
for si ? S, tj ? T do2
if si 6? STOP ? ??tl : (i, l) ? AE3
? tj 6? STOP ? ??sk : (k, j) ? AE4
? wordSim(si, tj) > 0 then5
?? ? ? {(i, j)}6
context? textContext(S, T, i, j, STOP)7
contextSim?
?
(k,l)?context
wordSim(sk, tl)
8
?(i, j)? w ? wordSim(si, tj)9
+ (1? w) ? contextSim10
Linearize and sort ? in decreasing order of ?(i, j)11
A? ?12
for (i, j) ? ? do13
if ??l : (i, l) ? A14
???k : (k, j) ? A then15
A? A ? {(i, j)}16
(line 8), and the alignment score is a weighted sum of
word similarity and contextual similarity (lines 9, 10).
The alignment score is then used to make one-to-one
word alignment decisions (lines 11-16). Considering
textual neighbors as weaker sources of evidence, we
do not align the neighbors.
Note that in cwTextAlign we also align semanti-
cally similar content word pairs (si, tj) with no con-
textual similarities if no pairs (sk, tj) or (si, tl) exist
with a higher alignment score. This is a consequence
of our observation of the MSR alignment dev data,
where we find that more often than not content words
are inherently sufficiently meaningful to be aligned
even in the absence of contextual evidence when
there are no competing pairs.
The content word alignment module is thus itself
a pipeline of cwDepAlign and cwTextAlign.
3.3.4 Stop Words
We follow the contextual evidence-based approach
to align stop words as well, some of which get aligned
Algorithm 5: align(S, T,EQ,w, STOP)
Input:
1. S, T : Sentences to be aligned
2. EQ: Dependency type equivalences (Table 1)
3. w: Weight of word similarity relative to contex-
tual similarity
4. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
A? wsAlign(S, T )1
A? A ? neAlign(S, T,EQ,A,w)2
A? A ? cwDepAlign(S, T,EQ,A,w, STOP)3
A? A ? cwTextAlign(S, T,A,w, STOP)4
A? A ? swDepAlign(S, T,A,w, STOP)5
A? A ? swTextAlign(S, T,A,w, STOP)6
as part of word sequence alignment as discussed in
Section 3.3.1, and neighbor alignment as discussed
in Section 3.3.3. For the rest we utilize dependen-
cies and textual neighborhoods as before, with three
adjustments.
Firstly, since stop word alignment is the last mod-
ule in our pipeline, rather than considering all se-
mantically similar word pairs for contextual similar-
ity, we consider only aligned pairs. Secondly, since
many stop words (e.g. determiners, modals) typi-
cally demonstrate little variation in the dependencies
they engage in, we ignore type equivalences for stop
words and implement only exact matching of depen-
dencies. (Stop words in general can participate in
equivalent dependencies, but we leave constructing
a corresponding mapping for future work.) Finally,
for textual neighborhood, we separately check align-
ments of the left and the right neighbors and aggre-
gate the evidences to determine alignment ? again
due to the primarily syntactic nature of interaction of
stop words with their neighbors.
Thus stop words are also aligned in a sequence of
dependency and textual neighborhood-based align-
ments. We assume two corresponding modules
named swDepAlign and swTextAlign, respectively.
3.3.5 The Algorithm
Our full alignment pipeline is shown as the method
align in Algorithm 5. Note that the strict order of the
alignment modules limits the scope of downstream
modules since each such module discards any word
that has already been aligned by an earlier module
225
(this is accomplished via the variable A; the corre-
sponding parameter in Algorithms 3 and 4 is AE).
The rationales behind the specific order of the mod-
ules can now be explained: (1) wsAlign is a module
with relatively higher precision, (2) it is convenient to
align named entities before other content words to en-
able alignment of entity mentions of different lengths,
(3) dependency-based evidence was observed to be
more reliable (i.e. of higher precision) than textual
evidence in the MSR alignment dev set, and (4) stop
word alignments are dependent on existing content
word alignments.
The aligner assumes two free parameters:
ppdbSim and w (in Algorithms 3 and 4). To
determine their values, we exhaustively search
through the two-dimensional space (ppdbSim,w)
for ppdbSim,w ? {0.1, ..., 0.9, 1} and the combina-
tion (0.9, 0.9) yields the best F1 score for the MSR
alignment dev set. We use these values for the aligner
in all of its subsequent applications.
4 Evaluation
We evaluate the performance of our aligner both in-
trinsically and extrinsically on multiple corpora.
4.1 Intrinsic Evaluation
The MSR alignment dataset2 (Brockett, 2007) was
designed to train and directly evaluate automated
aligners. Three annotators individually aligned words
and phrases in 1600 pairs of premise and hypothe-
sis sentences from the RTE2 challenge data (divided
into dev and test sets, each consisting of 800 sen-
tences). The dataset has subsequently been used to
assess several top performing aligners (MacCartney
et al., 2008; Thadani and McKeown, 2011; Yao et
al., 2013a; Yao et al., 2013b). We use the test set for
evaluation in the same manner as these studies: (a)
we apply majority rule to select from the three sets
of annotations for each sentence and discard three-
way disagreements, (b) we evaluate only on the sure
links (word pairs that annotators mentioned should
certainly be aligned, as opposed to possible links).
We test the generalizability of the aligner by eval-
uating it, unchanged (i.e. with identical parameter
values), on a second alignment corpus: the Edin-
2http://www.cs.biu.ac.il/ nlp/files/RTE 2006 Aligned.zip
System P% R% F1% E%
MS
R
MacCartney et al. (2008) 85.4 85.3 85.3 21.3
Thadani & McKeown (2011) 89.5 86.2 87.8 33.0
Yao et al. (2013a) 93.7 84.0 88.6 35.3
Yao et al. (2013b) 92.1 82.8 86.8 29.1
This Work 93.7 89.8 91.7 43.8
ED
B+
+ Yao et al. (2013a) 91.3 82.0 86.4 15.0
Yao et al. (2013b) 90.4 81.9 85.9 13.7
This Work 93.5 82.5 87.6 18.3
Table 2: Results of intrinsic evaluation on two datasets
burgh++3 (Thadani et al., 2012) corpus. The test set
consists of 306 pairs; each pair is aligned by at most
two annotators and we adopt the random selection
policy described in (Thadani et al., 2012) to resolve
disagreements.
Table 2 shows the results. For each corpus, it
shows precision (% alignments that matched with
gold annotations), recall (% gold alignments discov-
ered by the aligner), F1 score and the percentage
of sentences that received the exact gold alignments
(denoted by E) from the aligner.
On the MSR test set, our aligner shows a 3.1%
improvement in F1 score over the previous best sys-
tem (Yao et al., 2013a) with a 27.2% error reduction.
Importantly, it demonstrates a considerable increase
in recall without a loss of precision. TheE score also
increases as a consequence.
On the Edinburgh++ test set, our system achieves a
1.2% increase in F1 score (an error reduction of 8.8%)
over the previous best system (Yao et al., 2013a),
with improvements in both precision and recall. This
is a remarkable result that demonstrates the general
applicability of the aligner, as no parameter tuning
took place.
4.1.1 Ablation Test
We perform a set of ablation tests to assess the
importance of the aligner?s individual components.
Each row of Table 3 beginning with (-) shows a fea-
ture excluded from the aligner and two associated
sets of metrics, showing the performance of the re-
sulting algorithm on the two alignment corpora.
Without a word similarity module, recall drops
as would be expected. Without contextual evidence
(word sequences, dependencies and textual neigh-
bors) precision drops considerably and recall also
falls. Without dependencies, the aligner still gives
3http://www.ling.ohio-state.edu/?scott/#edinburgh-plusplus
226
MSR EDB++
Feature P% R% F1% P% R% F1%
Original 93.7 89.8 91.7 93.5 82.5 87.6
(-) Word Similarity 95.2 86.3 90.5 95.1 77.3 85.3
(-) Contextual Evidence 81.3 86.0 83.6 86.4 80.6 83.4
(-) Dependencies 94.2 88.8 91.4 93.8 81.3 87.1
(-) Text Neighborhood 85.5 90.4 87.9 90.4 84.3 87.2
(-) Stop Words 94.2 88.3 91.2 92.2 80.0 85.7
Table 3: Ablation test results
state-of-the-art results, which points to the possibility
of a very fast yet high-performance aligner. Without
evidence from textual neighbors, however, the preci-
sion of the aligner suffers badly. Textual neighbors
find alignments across different lexical categories,
a type of alignment that is currently not supported
by our dependency equivalences. Extending the set
of dependency equivalences might alleviate this is-
sue. Finally, without stop words (i.e. while aligning
content words only) recall drops just a little for each
corpus, which is expected as content words form the
vast majority of non-identical word alignments.
4.2 Extrinsic Evaluation
We extrinsically evaluate our system on textual simi-
larity identification and paraphrase detection. Here
we discuss each task and the results of evaluation.
4.2.1 Semantic Textual Similarity
Given two short input text fragments (commonly
sentences), the goal of this task is to identify the
degree to which the two fragments are semantically
similar. The *SEM 2013 STS task (Agirre et al.,
2013) assessed a number of STS systems on four test
datasets by comparing their output scores to human
annotations. Pearson correlation coefficient with hu-
man annotations was computed individually for each
test set and a weighted sum of the correlations was
used as the final evaluation metric (the weight for
each dataset was proportional to its size).
We apply our aligner to the task by aligning each
sentence pair and taking the proportion of content
words aligned in the two sentences (by normalizing
with the harmonic mean of their number of content
words) as a proxy of their semantic similarity. Only
three of the four STS 2013 datasets are freely avail-
able4 (headlines, OnWN, and FNWN), which we use
for our experiments (leaving out the SMT dataset).
4http://ixa2.si.ehu.es/sts/
System Correl.% Rank
Han et al. (2013) 73.7 1 (original)
JacanaAlign 46.2 66
This Work 67.2 7
Table 4: Extrinsic evaluation on STS 2013 data
These three sets contain 1500 annotated sentence
pairs in total.
Table 4 shows the results. The first row shows the
performance of the top system in the task. With a
direct application of our aligner (no parameter tun-
ing), our STS algorithm acheives a 67.15% weighted
correlation, which would earn it the 7th rank among
90 participating systems. Considering the fact that
alignment is one of many components of STS, this
result is truly promising.
For comparison, we also evaluate the previous best
aligner named JacanaAlign (Yao et al., 2013a) on
STS 2013 data (the JacanaAlign public release5 is
used, which is a version of the original aligner with
extra lexical resources). We apply three different val-
ues derived from its output as proxies of semantic
similarity: a) aligned content word proportion, b) the
Viterbi decoding score, and c) the normalized decod-
ing score. Of the three, (b) gives the best results,
which we show in row 2 of Table 4. Our aligner
outperforms JacanaAlign by a large margin.
4.2.2 Paraphrase Identification
The goal of paraphrase identification is to decide if
two sentences have the same meaning. The output is
a yes/no decision instead of a real-valued similarity
score as in STS. We use the MSR paraphrase cor-
pus6 (4076 dev pairs, 1725 test pairs) (Dolan et al.,
2004) to evaluate our aligner and compare with other
aligners. Following earlier work (MacCartney et al.,
2008; Yao et al., 2013b), we use a normalized align-
ment score of the two sentences to make a decision
based on a threshold which we set using the dev set.
Alignments with a higher-than-threshold score are
taken to be paraphrases and the rest non-paraphrases.
Again, this is an oversimplified application of the
aligner, even more so than in STS, since a small
change in linguistic properties of two sentences
(e.g. polarity or modality) can turn them into non-
5https://code.google.com/p/jacana/
6http://research.microsoft.com/en-us/downloads/607d14d9-
20cd-47e3-85bc-a2f65cd28042/
227
System Acc.% P% R% F1%
Madnani et al. (2012) 77.4 79.0 89.9 84.1
Yao et al. (2013a) 70.0 72.6 88.1 79.6
Yao et al. (2013b) 68.1 68.6 95.8 79.9
This Work 73.4 76.6 86.4 81.2
Table 5: Extrinsic evaluation on MSR paraphrase data
paraphrases despite having a high degree of align-
ment. So the aligner was not expected to demonstrate
state-of-the-art performance, but still it gets close as
shown in Table 5. The first column shows the accu-
racy of each system in classifying the input sentences
into one of two classes: true (paraphrases) and false
(non-paraphrases). The rest of the columns show the
performance of the system for the true class in terms
of precision, recall, and F1 score. Italicized numbers
represent scores that were not reported by the authors
of the corresponding papers, but have been recon-
structed from the reported data (and hence are likely
to have small precision errors).
The first row shows the best performance by any
system on the test set to the best of our knowledge.
The next two rows show the performances of two
state-of-the-art aligners (performances of both sys-
tems were reported in (Yao et al., 2013b)). The
last row shows the performance of our aligner. Al-
though it does worse than the best paraphrase system,
it outperforms the other aligners.
5 Discussion
Our experiments reveal that a word aligner based on
simple measures of lexical and contextual similar-
ity can demonstrate state-of-the-art accuracy. How-
ever, as alignment is frequently a component of larger
tasks, high accuracy alone is not always sufficient.
Other dimensions of an aligner?s usability include
speed, consumption of computing resources, replica-
bility, and generalizability to different applications.
Our design goals include achieving a balance among
such multifarious and conflicting goals.
A speed advantage of our aligner stems from for-
mulating the problem as one-to-one word alignment
and thus avoiding an expensive decoding phase. The
presence of multiple phases is offset by discarding
already aligned words in subsequent phases. The
use of PPDB as the only (hashable) word similarity
resource helps in reducing latency as well as space
requirements. As shown in Section 4.1.1, further
speedup could be achieved with only a small perfor-
mance degradation by considering only the textual
neighborhood as source of contextual evidence.
However, the two major goals that we believe the
aligner achieves to the greatest extent are replicabil-
ity and generalizability. The easy replicability of
the aligner stems from its use of only basic and fre-
quently used NLP modules (a lemmatizer, a POS
tagger, an NER module, and a dependency parser: all
available as part of the Stanford CoreNLP suite7; we
use a Python wrapper8) and a single word similarity
resource (PPDB).
We experimentally show that the aligner can be
successfully applied to different alignment datasets
as well as multiple end tasks. We believe a design
characteristic that enhances the generalizability of
the aligner is its minimal dependence on the MSR
alignment training data, which originates from a tex-
tual entailment corpus having unique properties such
as disparities in the lengths of the input sentences
and a directional nature of their relationship (i.e.,
the premise implying the hypothesis, but not vice
versa). A related potential reason is the symmetry
of the aligner?s output (caused by its assumption of
no directionality) ? the fact that it outputs the same
set of alignments regardless of the order of the input
sentences, in contrast to most existing aligners.
Major limitations of the aligner include the inabil-
ity to align phrases, including multiword expressions.
It is incapable of capturing and exploiting long dis-
tance dependencies among words (e.g. coreferences).
No word similarity resource is perfect and PPDB is
no exception, therefore certain word alignments also
remain undetected.
6 Conclusions
We show how contextual evidence can be used to
construct a monolingual word aligner with certain de-
sired properties, including state-of-the-art accuracy,
easy replicability, and high generalizability. Some
potential avenues for future work include: allow-
ing phrase-level alignment via phrasal similarity re-
sources (e.g. the phrasal paraphrases of PPDB), in-
cluding other sources of similarity such as vector
space models or WordNet relations, expanding the set
7http://nlp.stanford.edu/downloads/corenlp.shtml
8https://github.com/dasmith/stanford-corenlp-python
228
of dependency equivalences and/or using semantic
role equivalences, and formulating our alignment al-
gorithm as objective optimization rather than greedy
search.
The aligner is available for download at
https://github.com/ma-sultan/
monolingual-word-aligner.
Acknowledgments
This material is based in part upon work supported by
the National Science Foundation under Grant Num-
bers EHR/0835393 and EHR/0835381. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics. Association for Computational
Linguistics, 32-43.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics. Association for Computa-
tional Linguistics, 597-604.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics. Association for
Computational Linguistics, 435-440.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of The Second
PASCAL Recognising Textual Entailment Challenge.
Chris Brockett. 2007. Aligning the RTE 2006 Cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Nathanael Chambers, Daniel Cer, Trond Grenager, David
Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine
de Marneffe, Daniel Ramage, Eric Yeh, and Christopher
D Manning. 2007. Learning alignments and leverag-
ing natural logic. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing As-
sociation for Computational Linguistics, 165-170.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative Learning over Con-
strained Latent Representations. In Proceedings of the
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Association for Computational Linguistics, 429-437.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase Iden-
tication as Probabilistic Quasi-Synchronous Recogni-
tion. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Association for Computational Linguistics,
468-476.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation. 449-454.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical Report, Stanford University.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised Construction of Large Paraphrase Corpora:
Exploiting Massively Parallel News Sources. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics. Association for Computational
Linguistics, 350-356.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating Non-local Information into
Information Extraction Systems by Gibbs Sampling. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, 363-370.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics. Association for Computational
Linguistics, 758-764.
Lushan Han, Abhay Kashyap, Tim Finin, James Mayeld,
and Jonathan Weese. 2013. UMBC EBIQUITY-CORE:
Semantic Textual Similarity Systems. In Proceedings
of the Second Joint Conference on Lexical and Compu-
tational Semantics, Volume 1. Association for Compu-
tational Linguistics, 44-52.
Andrew Hickl and Jeremy Bensley. 2007. A Discourse
Commitment-Based Framework for Recognizing Tex-
tual Entailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing. As-
sociation for Computational Linguistics, 171-176.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCCs GROUNDHOG
229
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the PASCAL Challenges
Workshop: Recognising Textual Entailment Challenge
17-20.
Bill MacCartney, Michel Galley, and Christopher D. Man-
ning. 2008. A Phrase-Based Alignment Model for Nat-
ural Language Inference. In Proceedings of the 2008
Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics,
802-811.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics for
Paraphrase Identification. In Proceedings of 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, 182-190.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and Syntactically-Informed Decoding for Monolingual
Phrase-Based Alignment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies. Associa-
tion for Computational Linguistics, 254-259.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A Joint Phrasal and Dependency Model for Paraphrase
Alignment. In Proceedings of COLING 2012: Posters.
1229-1238.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003 Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics, 173-180.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013a. A Lightweight and High Per-
formance Monolingual Word Aligner. In Proceedings
of the 51st Annual Meeting of the Association for Com-
putational Linguistics. Association for Computational
Linguistics, 702-707.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013b. Semi-Markov Phrase-based
Monolingual Alignment. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics,
590-600.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013c. Answer Extraction as Se-
quence Tagging with Tree Edit Distance. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics. Association for Computational Linguistics, 858-
867.
230
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 365?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 3: Spatial Role Labeling
Parisa Kordjamshidi
Katholieke Universiteit Leuven
parisa.kordjamshidi@
cs.kuleuven.be
Steven Bethard
University of Colorado
steven.bethard@
colorado.edu
Marie-Francine Moens
Katholieke Universiteit Leuven
sien.moens@
cs.kuleuven.be
Abstract
This SemEval2012 shared task is based on a
recently introduced spatial annotation scheme
called Spatial Role Labeling. The Spatial Role
Labeling task concerns the extraction of main
components of the spatial semantics from nat-
ural language: trajectors, landmarks and spa-
tial indicators. In addition to these major
components, the links between them and the
general-type of spatial relationships includ-
ing region, direction and distance are targeted.
The annotated dataset contains about 1213
sentences which describe 612 images of the
CLEF IAPR TC-12 Image Benchmark. We
have one participant system with two runs.
The participant?s runs are compared to the sys-
tem in (Kordjamshidi et al, 2011c) which is
provided by task organizers.
1 Introduction
One of the essential functions of natural language is
to talk about spatial relationships between objects.
The sentence ?Give me the book on AI on the big
table behind the wall.? expresses information about
the spatial configuration of the objects (book, table,
wall) in some space. Particularly, it explains the re-
gion occupied by the book with respect to the table
and the direction (orientation) of the table with re-
spect to the wall. Understanding such spatial utter-
ances is a problem in many areas, including robotics,
navigation, traffic management, and query answer-
ing systems (Tappan, 2004).
Linguistic constructs can express highly complex,
relational structures of objects, spatial relations be-
tween them, and patterns of motion through space
relative to some reference point. Compared to nat-
ural language, formal spatial models focus on one
particular spatial aspect such as orientation, topol-
ogy or distance and specify its underlying spatial
logic in detail (Hois and Kutz, 2008). These for-
mal models enable spatial reasoning that is difficult
to perform on natural language expressions.
Learning how to map natural language spatial in-
formation onto a formal representation is a challeng-
ing problem. The complexity of spatial semantics
from the cognitive-linguistic point of view on the
one hand, the diversity of formal spatial represen-
tation models in different applications on the other
hand and the gap between the specification level of
the two sides has led to the present situation that no
well-defined framework for automatic spatial infor-
mation extraction exists that can handle all of these
aspects.
In a previous paper (Kordjamshidi et al, 2010b),
we introduced the task of spatial role labeling
(SpRL) and proposed an annotation scheme that is
language-independent and practically facilitates the
application of machine learning techniques. Our
framework consists of a set of spatial roles based
on the theory of holistic spatial semantics (Zlat-
evl, 2007) with the intent of covering the main as-
pects of spatial concepts at a course level, includ-
ing both static and dynamic spatial semantics. This
shared task is defined on the basis of that annota-
tion scheme. Since this is the first shared task on the
spatial information and this particular data, we pro-
posed a simplified version of the original scheme.
The intention of this simplification was to make this
practice feasible in the given timeframe. However,
365
the current task is very challenging particularly for
learning the spatial links and relations.
The core problem of SpRL is: i) the identification
of the words that play a role in describing spatial
concepts, and ii) the classification of the relational
role that these words play in the spatial configura-
tion.
For example, consider again the sentence ?Give
me the book on AI on the big table behind the wall.?.
The phrase headed by the token book is referring
to a trajector object. The trajector (TR) is an en-
tity whose location is described in the sentence. The
phrase headed by the token table is referring to the
role of a landmark (LM). The landmark is a refer-
ence object for describing the location of a trajector.
These two spatial entities are related by the spatial
expression on denoted as spatial indicator (SP). The
spatial indicator (often a preposition in English, but
sometimes a verb, noun, adjective, or adverb) indi-
cates the existence of spatial information in the sen-
tence and establishes the type of a spatial relation.
The spatial relations that can be extracted from the
whole sentence are <onSP bookTR tableLM> and
<behindSP tableTR wallLM>. One could also use
spatial reasoning to infer that the statement <behind
book wall> holds, however, such inferred relations
are not considered in this task. Although the spa-
tial indicators are mostly prepositions, the reverse
may not hold- for example, the first preposition
on only states the topic of the book, so <on book
AI> is not a spatial relation. For each of the true
spatial relations, a general type is assigned. The
<onSP bookTR tableLM> relation expresses a kind
of topological relationship between the two objects
and we assign it a general type named region. The
<behindSP tableTR wallLM> relation expresses di-
rectional information and we assign it a general type
named direction.
In general we assume two main abstraction layers
for the extraction of spatial information (Bateman,
2010; Kordjamshidi et al, 2010a; Kordjamshidi et
al., 2011a): (a) a linguistic layer, corresponding to
the annotation scheme described above, which starts
with unrestricted natural language and predicts the
existence of spatial information at the sentence level
by identifying the words that play a particular spa-
tial role as well as their spatial relationship; (b) a
formal layer, in which the spatial roles are mapped
onto a spatial calculus model (Galton, 2009). For
example, the linguistic layer recognizes that the spa-
tial relation (on) holds between book and table, and
the formal layer maps this to a specific, formal spa-
tial representation, e.g., a logical representation like
AboveExternallyConnected(book, table) or a
formal qualitative spatial representation like EC (ex-
ternally connected) in the RCC model (Regional
Connection Calculus) (Cohn and Renz, 2008).
In this shared task we focus on the first (linguistic)
level which is a necessary step for mapping natural
language to any formal spatial calculus. The main
roles that are considered here are trajector, land-
mark, spatial indicator, their links and the general
type of their spatial relation. The general type of a
relation can be direction, region or distance.
2 Motivation and related work
Spatial role labeling is a key task for applications
that are required to answer questions or reason about
spatial relationships between entities. Examples in-
clude systems that perform text-to-scene conversion,
generation of textual descriptions from visual data,
robot navigation tasks, giving directional instruc-
tions, and geographical information systems (GIS).
Recent research trends (Ross et al, 2010; Hois et
al., 2011; Tellex et al, 2011) indicate an increasing
interest in the area of extracting spatial information
from language and mapping it to a formal spatial
representation. Although cognitive-linguistic stud-
ies have investigated this problem extensively, the
computational aspect of making this bridge between
language and formal spatial representation (Hois
and Kutz, 2008) is still in its elementary stages. The
possession of a practical and appropriate annotation
scheme along with data is the first requirement. To
obtain this one has to investigate and schematize
both linguistic and spatial ontologies. This process
needs to cover the necessary information and seman-
tics on the one hand, and to maintain the practical
feasibility of the automatic annotation of unobserved
data on the other hand.
In recent research on spatial information and nat-
ural language, several annotation schemes have been
proposed such as ACE, GUM, GML, KML, TRML
which are briefly described and compared to Spa-
tialML scheme in (MITRE Corporation, 2010). But
366
to our knowledge, the main obstacles for employing
machine learning in this context and the very limited
usage of this effective approach have been (a) the
lack of an agreement on a unique semantic model
for spatial information; (b) the diversity of formal
spatial relations; and consequently (c) the lack of
annotated data on which machine learning can be
employed to learn and extract the spatial relations.
The most systematic work in this area includes the
SpatialML (Mani et al, 2008) scheme which fo-
cuses on geographical information, and the work of
(Pustejovsky and Moszkowicz, 2009) in which the
pivot of the spatial information is the spatial verb.
The most recent and active work is the ISO-Space
scheme (Pustejovsky et al, 2011) which is based
on the above two schemes. The ideas behind ISO-
Space are closely related to our annotation scheme
in (Kordjamshidi et al, 2010b), however it consid-
ers more detailed and fine-grained spatial and lin-
guistic elements which makes the preparation of the
data for machine learning more difficult.
Spatial information is directly related to the part
of the language that can be visualized. Thus, the
extraction of spatial information is useful for mul-
timodal environments. One advantage of our pro-
posed scheme is that it considers this dimension. Be-
cause it abstracts the spatial elements that could be
aligned with the objects in images/videos and used
for annotation of audio-visual descriptions (Butko et
al., 2011). This is useful in the multimodal environ-
ments where, for example, natural language instruc-
tions are given to a robot for finding the way or ob-
jects.
Not much work exists on using annotations for
learning models to extract spatial information. Our
previous work (Kordjamshidi et al, 2011c) is a first
step in this direction and provides a domain indepen-
dent linguistic and spatial analysis to this problem.
This shared task invites interested research groups
for a similar effort. The idea behind this task is
firstly to motivate the application of different ma-
chine learning approaches, secondly to investigate
effective features for this task, and thirdly to reveal
the practical problems in the annotation schemes and
the annotated concepts. This will help to enrich the
data and the annotation in parallel with the machine
learning practice.
3 Annotation scheme
As mentioned in the introduction, the annotation of
the data set is according to the general spatial role
labeling scheme (Kordjamshidi et al, 2010b). The
below example presents the annotated elements in
this scheme.
A womanTR and a childTR are
walkingMOTION overSP the squareLM .
General-type: region
Specific type: RCC
Spatial value: PP (proper part)
Dynamic
Path: middle
Frame of reference: ?
According to this scheme the main spatial roles are,
Trajector (TR). The entity, i.e., person, object or
event whose location is described, which can
be static or dynamic; (also called: local/figure
object, locatum). In the above example woman
and child are two trajectors.
Landmark (LM). The reference entity in relation
to which the location or the motion of the tra-
jector is specified. (also called: reference ob-
ject or relatum). square is the landmark in the
above example.
Spatial indicator (SP). The element that defines
constraints on spatial properties such as the lo-
cation of the trajector with respect to the land-
mark. The spatial indicator determines the type
of spatial relation. The preposition over is an-
notated as the spatial indicator in the current
example.
Moreover, the links between the three roles are an-
notated as a spatial Relation. Since each spatial
relation is defined with three arguments we call
it a spatial triplet. Each triplet indicates a re-
lation between the three above mentioned spatial
roles. The sentence contains two spatial relations
of <overSP womanTR squareLM> and <overSP
childTR squareLM>, with the same spatial at-
tributes listed below the example. In spatial infor-
mation theory the relations and properties are usu-
ally grouped into the domains of topological, direc-
tional, and distance relations and also shape (Stock,
367
1997). Accordingly, we propose a mapping between
the extracted spatial triplets to the coarse-grained
type of spatial relationships including region, direc-
tion or distance. We call these types as general-
type of the spatial relations and briefly describe
these below:
Region. refers to a region of space which is always
defined in relation to a landmark, e.g. the inte-
rior or exterior, e.g. ?the flower is in the vase?.
Direction. denotes a direction along the axes pro-
vided by the different frames of reference, in
case the trajector of motion is not characterized
in terms of its relation to the region of a land-
mark, e.g. ?the vase is on the left?.
Distance. states information about the spatial dis-
tance of the objects and could be a qualitative
expression such as close, far or quantitative
such as 12 km, e.g. ?the kids are close to the
blackboard?.
The general-type of the relation in the example is
annotated as region.
After extraction of these relations a next fine-
grained step will be to map each general spatial re-
lationship to an appropriate spatial calculi represen-
tation. This step is not intended for this task and
the additional tags in the scheme will be consid-
ered in the future shared tasks. For example Re-
gion Connection Calculus RCC-8 (Cohn and Renz,
2008) representation reflects region-based topolog-
ical relations. Topological or region-based spatial
information has been researched in depth in the area
of qualitative spatial representation and reasoning.
We assume that the trajectors and landmarks can of-
ten be interpreted as spatial regions and, as a conse-
quence, their relation can be annotated with a spe-
cific RCC-8 relation. The RCC type in the above
example is specifically annotated as the PP (proper
part). Similarly, the direction and distance relations
are mapped to more specific formal representations.
Two additional annotations are about motion
verbs and dynamism. Dynamic spatial information
are associated with spatial movements and spatial
changes. In dynamic spatial relations mostly mo-
tion verbs are involved. Motion verbs carry spatial
information and influence the spatial semantics. In
the above example the spatial indicator over is re-
lated to a motion verb walking. Hence the spatial
relation is dynamic and walking is annotated as the
motion. In contrast to the dynamic spatial relations,
the static ones explain a static spatial configuration
such as the example of the previous section <onSP
bookTR tableLM> .
In the case of dynamic spatial information a path
is associated with the location of the trajector. In our
scheme the path is characterized by the three values
of beginning, middle, end and zero. The frame of
reference can be intrinsic, relative or absolute and is
typically relevant for directional relations. For more
details about the scheme, see (Kordjamshidi et al,
2010b).
4 Tasks
The SemEval-2012 shared task is defined in three
parts.
? The first part considers labeling the spatial
indicators and trajector(s) / landmark(s). In
other words at this step we consider the
extraction of the individual roles that are
tagged with TRAJECTOR, LANDMARK and
SPATIAL INDICATOR.
? The second part is a kind of relation prediction
task and the goal is to extract triples contain-
ing (spatial-indicator, trajector, landmark). The
prediction of the tag of RELATION with its three
arguments of SP, TR, LM at the same time is
considered.
? The third part concerns the classification of the
type of the spatial relation. At the most coarse-
grained level this includes labeling the spatial
relations i.e. the triplets of (spatial indicator,
trajector, landmark) with region, direction, and
distance labels. This means the general-type
of the RELATION should be predicted. The
general-type is an attribute of the RELATION
tag, see the example represented in XML for-
mat in section 5.1.
5 Preparation of the dataset
The annotated corpus that we used for this shared
task is a subset of IAPR TC-12 image Bench-
mark (Grubinger et al, 2006). It contains 613 text
368
files that include 1213 sentences in total. This is an
extension of the dataset used in (Kordjamshidi et
al., 2011c). The original corpus was available free
of charge and without copyright restrictions. The
corpus contains images taken by tourists with de-
scriptions in different languages. The texts describe
objects, and their absolute and relative positions in
the image. This makes the corpus a rich resource for
spatial information. However the descriptions are
not always limited to spatial information. Therefore
they are less domain-specific and contain free expla-
nations about the images. Table 1 shows the detailed
statistics of this data. The average length of the sen-
tences in this data is about 15 words including punc-
tuation marks with a standard deviation of 8.
The spatial roles are assigned both to phrases and
their headwords, but only the headwords are eval-
uated for this task. The spatial relations indicate a
triplet of these roles. The general-type is assigned to
each triplet of spatial indicator, trajector and land-
mark.
At the starting point two annotators including one
task-organizer and another non-expert annotator, an-
notated 325 sentences for the spatial roles and rela-
tions. The purpose was to realize the disagreement
points and prepare a set of instructions in a way to
achieve highest-possible agreement. From the first
effort an inter-annotator agreement (Carletta, 1996)
of 0.89 for Cohen?s kappa was obtained. We contin-
ued with the a third annotator for the remaining 888
sentences. The annotator had an explanatory session
and received a set of instructions and annotated ex-
amples to decrease the ambiguity in the annotations.
To avoid complexity only the relations that are di-
rectly expressed in the sentence are annotated and
spatial reasoning was avoided during the annota-
tions. Sometimes the trajectors and landmarks or
both are implicit, meaning that there is no word in
the sentence to represent them. For example in the
sentence Come over here, the trajector you is only
implicitly present. To be consistent with the number
of arguments in spatial relations, in these cases we
use the term undefined for the implicit roles. There-
fore, the spatial relation in the above example is
<overSP undefinedTR hereLM>.
5.1 Data format
The data is released in XML format. The original
textual files are split into sentences. Each sentence
is placed in a <SENTENCE/> tag and assigned an
identifier. This tag contains all the other tags which
describe the content and spatial relations of one sen-
tence.
The content of the sentence is placed in the
<CONTENT/> tag. The words in each sentence
are assigned identifiers depending on their specific
roles. Trajectors, landmarks and spatial indicators
are identified by <TRAJECTOR/>, <LANDMARK/>
and <SPATIAL INDICATOR/> tags, respectively.
Each of these XML elements has an ?ID? attribute
that identifies a related word by its index. The ?ID?
prefixed by either ?TW?, ?LW? or ?SW?, respec-
tively for the mentioned roles. For example, a tra-
jector with ID=?TW2? corresponds to the word at
index 2 in the sentence. Indexes start at 0. Com-
mas, parentheses and apostrophes are also counted
as tokens.
Spatial relations are assigned identifiers too, and
relate the role-playing words to each other. Spa-
tial relations are identified by the <RELATION/>
tag. The spatial indicator, trajector and land-
mark for the relation are identified by the ?SP?,
?TR? and ?LM? attributes, respectively. The val-
ues of these attributes correspond to the ?ID? at-
tributes in the <TRAJECTOR/>, <LANDMARK/>
and <SPATIAL INDICATOR/> elements. If a tra-
jector or landmark is implicit, then the index of
?TR? or ?LM? attribute will be set to a dummy
index. This dummy index is equal to the in-
dex of the last word in the sentence plus one.
In this case, the value of TRAJECTOR or LAND-
MARK is set to ?undefined?. The coarse-grained
spatial type of the relation is indicated by the
?GENERAL TYPE? attribute and gets one value
in {REGION, DIRECTION, DISTANCE}. In the
original data set there are cases annotated with
multiple spatial types. This is due to the ambi-
guity and/or under-specificity of natural language
compared to formal spatial representations (Kord-
jamshidi et al, 2010a). In this task the general-
type with a higher priority by the annotator is pro-
vided. Here, by the high priority type, we mean the
general type which has been the most informative
369
Spatial Roles Relations General Types
Sentences TR LM SP Spatial triplets Region Direction Distance
1213 1593 1408 1464 1715 1036 644 35
Table 1: Number of annotated components in the data set.
and relevant type for a relation, from the annotator?s
point of view. This task considers labeling words
rather than phrases for all spatial roles. However, in
the XML file for spatial indicators often the whole
phrase is tagged. In these cases, the index of the
indicator refers to one word which is typically the
spatial preposition of the phrase. For evaluation only
the indexed words are compared and should be pre-
dicted correctly.
Below is one example copied from the data. For
more examples and details about the general anno-
tation scheme see (Kordjamshidi et al, 2010b).
<SENTENCE ID=?S11?>
<CONTENT >
there are red umbrellas in a park on the right .
</CONTENT>
<TRAJECTOR ID=?TW3?>
umbrellas
</TRAJECTOR>
<LANDMARK ID=?LW6?>
park
</LANDMARK>
<SPATIAL INDICATOR ID=?SW4?>
in
</SPATIAL INDICATOR>
<RELATION ID=?R0? SP=?SW4? TR=?TW3?
LM=?LW6? GENERAL TYPE=?REGION?/>
<SPATIAL INDICATOR ID=?SW7?>
on the right
</SPATIAL INDICATOR>
<RELATION ID=?R1? SP=?SW7? TR=?TW3?
LM=?LW6? GENERAL TYPE=?DIRECTION?/>
</SENTENCE>
The dataset, both train and test, also the 10-fold
splits are made available in the LIIR research group
webpage of KU Leuven.1
6 Evaluation methodology
According to the usual setting of the shared tasks
our evaluation setting was based on splitting the data
set into a training and a testing set. Each set con-
tained about 50% of the whole data. The test set re-
1http://www.cs.kuleuven.be/groups/liir/software/
SpRL Data/
leased without the ground-truth labels. However, af-
ter the systems submission deadline the ground-truth
test was released. Hence the participant group per-
formed an additional 10-fold cross validation eval-
uation too. We report the results of both evaluation
settings.
Prediction of each component including TRAJEC-
TORs, LANDMARKs and SPATIAL-INDICATORs is
evaluated on the test set using their individual spatial
element XML tags. The evaluation metrics of pre-
cision, recall and F1-measure are used, which are
defined as:
recall = TPTP+FN (1)
precision = TPTP+FP (2)
F1 = 2?recall?precision(recall+precision) , (3)
where:
TP = the number of system-produced
XML tags that match an annotated XML
tag,
FP = the number of system-produced
XML tags that do not match an annotated
tag,
FN = the number of annotated XML tags
that do not match a system-produced tag.
For the roles evaluation two XML tags match
when they have exactly same identifier. In fact,
when the identifiers are the same then the role and
the word index are the same. In addition, systems
are evaluated on how well they are able to retrieve
triplets of (trajector, spatial-indicator, landmark), in
terms of precision, recall and F1-measure. The TP,
FP, FN are counted in a similar way but two RELA-
TION tags match if the combination of their TR, LM
and SP is exactly the same. In other words a true pre-
diction requires all the three elements are correctly
predicted at the same time.
The last evaluation is on how well the systems are
able to retrieve the relations and their general type
370
i.e {region, direction, distance} at the same time.
To evaluate the GENERAL-TYPE similarly the RELA-
TION tag is checked. For a true prediction, an exact
match between the ground-truth and all the elements
of the predicted RELATION tag including TR, LM,SP
and GENERAL-TYPE is required.
7 Systems and results
One system with two runs was submitted from the
University of Texas Dallas. The two runs (Roberts
and Harabagiu, 2012), UTDSPRL-SUPERVISED1
and UTDSPRL-SUPERVISED2 are based on the
joint classification of the spatial triplets in a bi-
nary classification setting. To produce the candi-
date (indicator, trajector, landmark) triples, in the
first stage heuristic rules targeting a high recall are
used. Then a binary support vector machine clas-
sifier is employed to predict whether a triple is a
spatial relation or not. Both runs start with a large
number of manually engineered features, and use
floating forward feature selection to select the most
important ones. The difference between the two
runs of UTDSPRL-SUPERVISED1 and UTDSPRL-
SUPERVISED2 is their feature set. Particularly, in
UTDSPRL-SUPERVISED1 a joint feature based on
the conjunctions (e.g. and, but) is considered before
running feature selection but this feature is removed
in UTDSPRL-SUPERVISED2.
The submitted runs are compared to a previous
system from the task organizers (Kordjamshidi et
al., 2011c) which is evaluated on the current data
with the same settings. This system, KUL-SKIP-
CHAIN-CRF, uses a skip chain conditional random
field (CRF) model (Sutton and MacCallum, 2006)
to annotate the sentence as a sequence. It considers
the long distance dependencies between the prepo-
sitions and nouns in the sentence.
The type and structure of the features used in the
UTD and KUL systems are different. In the UTD
system, the classifier works on triples and the fea-
tures are of two main types: (a) argument-specific
features about the trajector, landmark, or indicator
e.g., the landmark?s hypernyms, or the indicator?s
first token; and (b) joint features that consider two
or more of the arguments, e.g. the dependency path
between indicator and landmark. For more detail,
see (Roberts and Harabagiu, 2012). In the KUL sys-
Label Precsion Recall F1
TRAJECTOR 0.731 0.621 0.672
LANDMARK 0.871 0.645 0.741
SPATIAL-INDICATOR 0.928 0.712 0.806
RELATION 0.567 0.500 0.531
GENERAL-TYPE 0.561 0.494 0.526
Table 2: UTDSPRL-SUPERVISED1: The University
of Texas-Dallas system with a larger number of fea-
tures,test/train one split.
Label Precsion Recall F1
TRAJECTOR 0.782 0.646 0.707
LANDMARK 0.894 0.680 0.772
SPATIAL-INDICATOR 0.940 0.732 0.823
RELATION 0.610 0.540 0.573
GENERAL-TYPE 0.603 0.534 0.566
Table 3: UTDSPRL-SUPERVISED2: The University of
Texas-Dallas system with a smaller number of features,
test/train one split.
Label Precsion Recall F1
TRAJECTOR 0.697 0.603 0.646
LANDMARK 0.773 0.740 0.756
SPATIAL-INDICATOR 0.913 0.887 0.900
RELATION 0.487 0.512 0.500
Table 4: KUL-SKIP-CHAIN-CRF: The organizers? sys-
tem (Kordjamshidi et al, 2011c)- test/train one split.
tem, the classifier works on all tokens in a sentence,
and a number of linguistically motivated local and
pairwise features over candidate words and preposi-
tions are used. To consider long distance dependen-
cies a template, called a preposition template, is used
in the general CRF framework. Loopy belief prop-
agation is used for inference. Mallet2 and GRMM:3
implementations are employed there.
Tables 2, 3 and 4 show the results of the three
runs in the standard setting of the shared task us-
ing the train/test split. In this evaluation setting the
UTDSPRL-SUPERVISED2 run achieves the highest
performance on the test set, with F1 of 0.573 for
the full triplet identification task, and an F1 of 0.566
for additionally classifying the triplet?s general-type
2http://mallet.cs.umass.edu/download.php
3http://mallet.cs.umass.edu/grmm/index.php
371
System Precsion Recall F1
KUL-SKIP-CHAIN-CRF 0.745 0.773 0.758
UTDSPRL-SUPERVISED2 0.773 0.679 0.723
Table 5: The RELATION extraction of KUL-SKIP-CHAIN-CRF (Kordjamshidi et al, 2011c) vs. UTDSPRL-
SUPERVISED2 evaluated with 10-fold cross validation
correctly. It also consistently outperforms both the
UTDSPRL-SUPERVISED1 run and the KUL-SKIP-
CHAIN-CRF system on each of the individual trajec-
tor, landmark and spatial-indicator extraction.
The dataset was relatively small, so we released
the test data and the two systems were addition-
ally evaluated using 10-fold cross validation. The
results of this cross-validation are shown in Ta-
ble 5. The UTDSPRL-SUPERVISED2 run achieves
a higher precision, while the KUL-SKIP-CHAIN-
CRF system achieves a higher recall. It should be
mentioned the 10-fold splits used by KUL and UTD
are not the same. This implies that the results with
exactly the same cross-folds may vary slightly from
these reported in Table 5.
Using 10-fold cross validation, we also evaluated
the classification of the general-type of a relation
given the manually annotated positive triplets. The
UTDSPRL-SUPERVISED2 system achieved F1=
0.974, and similar experiments using SMO-SVM in
(Kordjamshidi et al, 2011b; Kordjamshidi et al,
2011a) achieved F1= 0.973. Thus it appears that
identifying the general-type of a relation is a rela-
tively easy task on this data.
Discussion. Since the feature sets of the two sys-
tems are different and given the evaluation results
in the two evaluation settings, it is difficult to assert
which model is better in general. Obviously using
joint features potentially inputs richer information to
the model. However, it can increase the sparsity in
one hand and overfitting on the training data on the
other hand. Another problem is that finding heuris-
tics for high recall that are sufficiently general to be
used in every domain is not an easy task. By increas-
ing the number of candidates the dataset imbalance
will increase dramatically. This can cause a lower
performance of a joint model based on a binary clas-
sification setting when applied on different data sets.
It seems that this task might require a more elabo-
rated structured output prediction model which can
consider the joint features and alleviate the problem
of huge negatives in that framework while consider-
ing the correlations between the output components.
8 Conclusion
The SemEval-2012 spatial role labeling task is a
starting point to formally consider the extraction of
spatial semantics from the language. The aim is
to consider this task as a standalone linguistic task
which is important for many applications. Our first
practice on this task and the current submitted sys-
tem to SemEval 2012 clarify the type of the features
and the machine learning approaches appropriate for
it. The proposed features and models help to per-
form this task automatically in a reasonable accu-
racy. Although the spatial scheme is domain inde-
pendent, the achieved accuracy is dependent on the
domain of the used data for training a model. Our
future plan is to extend the data for the next work-
shops and to cover more semantic aspects of spatial
information particularly for mapping to formal spa-
tial representation models and spatial calculus.
Acknowledgments
Special thanks to Martijn Van Otterlo for his great
cooperation from the initiation phase and in the
growth of this task. Many thanks to Sabine
Drebusch for her kind and open cooperation in an-
notating the very first dataset. Thanks to Tigist Kas-
sahun for her help in annotating the current dataset.
Thanks the participant team of the University of
Texas Dallas and their useful feedback on the an-
notated data.
References
J. A. Bateman. 2010. Language and space: a two-level
semantic approach based on principles of ontological
engineering. International Journal of Speech Technol-
ogy, 13(1):29?48.
372
T. Butko, C. Nadeu, and A. Moreno. 2011. A multi-
lingual corpus for rich audio-visual scenedescription
in a meeting-room environment. In ICMI workshop
on multimodal corpora for machine learning: Taking
Stock and Roadmapping the Future.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
A. G. Cohn and J. Renz. 2008. Qualitative spatial repre-
sentation and reasoning. In Handbook of Knowledge
Representation, volume 3 of Foundations of Artificial
Intelligence, pages 551 ? 596. Elsevier.
A. Galton. 2009. Spatial and temporal knowledge rep-
resentation. Journal of Earth Science Informatics,
2(3):169?187.
M. Grubinger, P. Clough, Henning Mu?ller, and Thomas
Deselaers. 2006. The IAPR benchmark: A new evalu-
ation resource for visual information systems. In In-
ternational Conference on Language Resources and
Evaluation (LREC).
J. Hois and O. Kutz. 2008. Natural language meets spa-
tial calculi. In Christian Freksa, Nora S. Newcombe,
Peter Ga?rdenfors, and Stefan Wo?lfl, editors, Spatial
Cognition, volume 5248 of Lecture Notes in Computer
Science, pages 266?282. Springer.
J. Hois, R. J. Ross, J. D. Kelleher, and J. A. Bateman.
2011. Computational models of spatial language in-
terpretation and generation. In COSLI-2011.
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010a. From language towards formal spatial calculi.
In Workshop on Computational Models of Spatial Lan-
guage Interpretation (CoSLI 2010, at Spatial Cogni-
tion 2010).
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010b. Spatial role labeling: Task definition and anno-
tation scheme. In Proceedings of the Seventh confer-
ence on International Language Resources and Eval-
uation (LREC?10).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.-F.
Moens. 2011a. Machine learning for interpretation of
spatial natural language in terms of qsr. Poster Presen-
tation at the 10th International Conference on Spatial
Information Theory (COSIT?11).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.F.
Moens. 2011b. Learning to interpret spatial natural
language in terms of qualitative spatial relations. Rep-
resenting space in cognition: Interrelations of behav-
ior, language, and formal models. Series Explorations
in Language and Space, Oxford University Press, sub-
mitted.
P. Kordjamshidi, M. Van Otterlo, and M.F. Moens.
2011c. Spatial role labeling: Towards extraction of
spatial relations from natural language. ACM Trans.
Speech Lang. Process., 8:1?36, December.
I. Mani, J. Hitzeman, J. Richer, D. Harris, R. Quimby, and
B. Wellner. 2008. SpatialML: Annotation scheme,
corpora, and tools. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08). European Language Re-
sources Association (ELRA).
MITRE Corporation. 2010. SpatialML: Annotation
scheme for marking spatial expression in natural lan-
guage. Technical Report Version 3.0.1, The MITRE
Corporation.
J. Pustejovsky and J.L. Moszkowicz. 2009. Integrat-
ing motion predicate classes with spatial and tempo-
ral annotations. In CoLing 2008: Companion volume
Posters and Demonstrations, pages 95?98.
J. Pustejovsky, J. Moszkowicz, and M. Verhagen. 2011.
Iso-space: The annotation of spatial information in
language. In Proceedings of ISA-6: ACL-ISO Inter-
national Workshop on Semantic Annotation.
K. Roberts and S.M. Harabagiu. 2012. Utd-sprl: A joint
approach to spatial role labeling. In Submitted to this
workshop of SemEval-2012.
R. Ross, J. Hois, and J. Kelleher. 2010. Computational
models of spatial language interpretation. In COSLI-
2010.
O. Stock, editor. 1997. Spatial and Temporal Reasoning.
Kluwer.
C. Sutton and A. MacCallum. 2006. Introduction to con-
ditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statis-
tical Relational Learning. MIT Press.
D. A. Tappan. 2004. Knowledge-Based Spatial Rea-
soning for Automated Scene Generation from Text De-
scriptions. Ph.D. thesis.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, and
N. Roy A. G. Banerjee, S. Teller. 2011. Understand-
ing natural language commands for robotic naviga-
tion and mobile manipulation. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
San Francisco, CA.
J. Zlatevl. 2007. Spatial semantics. In Hubert Cuyck-
ens and Dirk Geeraerts (eds.) The Oxford Handbook
of Cognitive Linguistics, Chapter 13, pages 318?350.
373
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 176?180, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DLS@CU-CORE: A Simple Machine Learning Model of Semantic    
Textual Similarity 
Md. Arafat Sultan, Steven Bethard, Tamara Sumner 
Institute of Cognitive Science and Department of Computer Science  
University of Colorado, Boulder, CO 80309 
{arafat.sultan, steven.bethard, sumner}@colorado.edu 
   
Abstract 
We present a system submitted in the Semantic 
Textual Similarity (STS) task at the Second 
Joint Conference on Lexical and Computa-
tional Semantics (*SEM 2013). Given two 
short text fragments, the goal of the system is 
to determine their semantic similarity. Our sys-
tem makes use of three different measures of 
text similarity: word n-gram overlap, character 
n-gram overlap and semantic overlap. Using 
these measures as features, it trains a support 
vector regression model on SemEval STS 2012 
data. This model is then applied on the STS 
2013 data to compute textual similarities. Two 
different selections of training data result in 
very different performance levels: while a cor-
relation of 0.4135 with gold standards was ob-
served in the official evaluation (ranked 63rd 
among all systems) for one selection, the other 
resulted in a correlation of 0.5352 (that would 
rank 21st).  
1 Introduction 
Automatically identifying the semantic similarity 
between two short text fragments (e.g. sentences) is 
an important research problem having many im-
portant applications in natural language processing, 
information retrieval, and digital education. Exam-
ples include automatic text summarization, question 
answering, essay grading, among others.  
   However, despite having important applications, 
semantic similarity identification at the level of 
short text fragments is a relatively recent area of in-
vestigation. The problem was formally brought to 
attention and the first solutions were proposed in 
2006 with the works reported in (Mihalcea et al, 
2006) and (Li et al, 2006). Work prior to these fo-
cused primarily on large documents (or individual 
words) (Mihalcea et al, 2006). But the sentence-
level granularity of the problem is characterized by 
factors like high specificity and low topicality of the 
expressed information, and potentially small lexical 
overlap even between very similar texts, asking for 
an approach different from those that were designed 
for larger texts. 
Since its inception, the problem has seen a large 
number of solutions in a relatively small amount of 
time. The central idea behind most solutions is the 
identification and alignment of semantically similar 
or related words across the two sentences, and the 
aggregation of these similarities to generate an over-
all similarity score (Mihalcea et al, 2006; Islam and 
Inkpen, 2008; ?ari? et al, 2012). 
The Semantic Textual Similarity task (STS) or-
ganized as part of the Semantic Evaluation Exer-
cises (see (Agirre et al, 2012) for a description of 
STS 2012) provides a common platform for evalua-
tion of such systems via comparison with human-
annotated similarity scores over a large dataset.  
In this paper, we present a system which was 
submitted in STS 2013. Our system is based on very 
simple measures of lexical and character-level over-
lap, semantic overlap between the two sentences 
based on word relatedness measures, and surface 
features like the sentences? lengths. These measures 
are used as features for a support vector regression 
model that we train with annotated data from 
SemEval STS 2012. Finally, the trained model is ap-
plied on the STS 2013 test pairs. 
Our approach is inspired by the success of simi-
lar systems in STS 2012: systems that combine mul-
tiple measures of similarity using a machine learn-
ing model to generate an overall score (B?r et al, 
2012; ?ari? et al, 2012). We wanted to investigate 
how a minimal system of this kind, making use of 
very few external resources, performs on a large da-
taset. Our experiments reveal that the performance 
of such a system depends highly on the training 
data. While training on one dataset yielded a best 
176
correlation (among our three runs, described later in 
this document) of only 0.4135 with the gold scores, 
training on another dataset showed a considerably 
higher correlation of 0.5352.  
2 Computation of Text Similarity: System 
Overview 
In this section, we present a high-level description 
of our system. More details on extraction of some of 
the measures of similarity are provided in Section 3. 
Given two input sentences ?1 and ?2, our algo-
rithm can be described as follows: 
1. Compute semantic overlap (8 features): 
a. Lemmatize ?1 and ?2 using a memory-
based lemmatizer1 and remove all stop 
words. 
b. Compute the degree to which the concepts 
in ?1 are covered by semantically similar 
concepts in ?2 and vice versa (see Section 3 
for details). The result of this step is two dif-
ferent ?degree of containment? values (?1 in 
?2 and vice versa). 
c. Compute the minimum, maximum, arith-
metic mean and harmonic mean of the two 
values to use as features in the machine 
learning model. 
d. Repeat steps 1a through 1c for a weighted 
version of semantic overlap where each 
word in the first sentence is assigned a 
weight which is proportional to its specific-
ity in a selected corpus (see Section 3). 
2. Compute word ?-gram overlap (16 features): 
a. Extract ?-grams (for ? = 1, 2, 3, 4) of all 
words in ?1 and ?2 for four different setups 
characterized by the four different value 
combinations of the two following varia-
bles: lemmatization (on and off), stop-
WordsRemoved (on and off). 
b. Compute the four measures (min, max, 
arithmetic and harmonic mean) for each 
value of n. 
3. Compute character ?-gram overlap (16 fea-
tures):  
a. Repeat   all steps in 2 above for character ?-
grams (? = 2, 3, 4, 5). 
                                                          
1 http://www.clips.ua.ac.be/pages/MBSP#lemmatizer 
2 http://conceptnet5.media.mit.edu/data/5.1/as-
soc/c/en/cat? filter=/c/en/dog&limit=1 
4. Compute sentence length features (2 features): 
a. Compute the lengths of ?1 and ?2; and the 
minimum and maximum of the two values. 
b. Include the ratio of the maximum to the min-
imum and the difference between the maxi-
mum and minimum in the feature set. 
5. Train a support vector regression model on the 
features extracted in steps 1 through 4 above us-
ing data from SemEval 2012 STS (see Section 
4 for specifics on the dataset). We used the 
LibSVM implementation of SVR in WEKA. 
6. Apply the model on STS 2013 test data. 
3 Semantic Overlap Measures 
In this section, we describe the computation of the 
two sets of semantic overlap measures mentioned in 
step 1 of the algorithm in Section 2. 
We compute semantic overlap between two sen-
tences by first computing the semantic relatedness 
among their constituent words. Automatically com-
puting the semantic relatedness between words is a 
well-studied problem and many solutions to the 
problem have been proposed. We compute word re-
latedness in two forms: semantic relatedness and 
string similarity. For semantic relatedness, we uti-
lize two web services. The first one concerns a re-
source named ConceptNet (Liu and Singh, 2004), 
which holds a large amount of common sense 
knowledge concerning relationships between real-
world entities. It provides a web service2 that gener-
ates word relatedness scores based on these relation-
ships. We will use the term ?????(?1, ?2) to de-
note the relatedness of the two words ?1 and ?2 as 
generated by ConceptNet. 
We also used the web service3 provided by an-
other resource named Wikipedia Miner (Milne and 
Witten, 2013). While ConceptNet successfully cap-
tures common sense knowledge about words and 
concepts, Wikipedia Miner specializes in identify-
ing relationships between scientific concepts pow-
ered by Wikipedia's vast repository of scientific in-
formation (for example, Einstein and relativity). We 
will use the term ?????(?1, ?2) to denote the re-
latedness of the two words ?1 and ?2 as generated 
by Wikipedia Miner. Using two systems enabled us 
3 http://wikipedia-miner.cms.waikato.ac.nz/ser-
vices/compare?  term1=cat&term2=dog 
177
to increase the coverage of our word similarity com-
putation algorithm.  
Each of these web services return a score in the 
range [0, 1] where 0 represents no relatedness and 1 
represents complete similarity. A manual inspection 
of both services indicates that in almost all cases 
where the services? word similarity scores deviate 
from what would be the human-perceived similar-
ity, they generate lower scores (i.e. lower than the 
human-perceived score). This is why we take the 
maximum of the two services? similarity scores for 
any given word pair as their semantic relatedness: 
??????(?1, ?2) 
= max?{?????(?1, ?2),?????(?1, ?2)} 
We also compute the string similarity between 
the two words by taking a weighted combination of 
the normalized lengths of their longest common 
substring, subsequence and prefix (normalization is 
done for each of the three by dividing its length with 
the length of the smaller word). We will refer to the 
string similarity between words ?1 and ?2 as 
?????????(??1, ?2). This idea is taken from (Islam 
and Inkpen, 2008); the rationale is to be able to find 
the similarity between (1) words that have the same 
lemma but the lemmatizer failed to lemmatize at 
least one of the two surface forms successfully, and 
(2) words at least one of which has been misspelled. 
We take the maximum of the string similarity and 
the semantic relatedness between two words as the 
final measure of their similarity: 
???(?1, ?2) 
= max?{??????(?1, ?2), ?????????(?1, ?2)} 
At the sentence level, our first set of semantic 
overlap measures (step 1b) is an unweighted meas-
ure that treats all content words equally. More spe-
cifically, after the preprocessing in step 1a of the al-
gorithm, we compute the degree of semantic cover-
age of concepts expressed by individual content 
words in ?1 by ?2 using the following equation: 
?????(?1, ?2) =
? [max
???2
{???(?, ?)}]???1
|?1|
 
                                                          
4 http://googleresearch.blogspot.com/2006/08/all-our-n-
gram-are-belong-to-you.html 
where ???(?, ?) is the similarity between the two 
lemmas ? and ?. 
We also compute a weighted version of semantic 
coverage (step 1d in the algorithm) by incorporating 
the specificity of each word (measured by its infor-
mation content) as shown in the equation below:  
????(?1, ?2) =
? [max
???2
{??(?). ???(?, ?)}]???1
|?1|
 
where ??(?) stands for the information content of 
the word ?. Less common words (across a selected 
corpus) have high information content: 
??(?) = ln
? ?(??)????
?(?)
 
where C is the set of all words in the chosen corpus 
and f(w) is the frequency of the word w in the cor-
pus. We have used the Google Unigram Corpus4 to 
assign the required frequencies to these words. 
4 Evaluation 
The STS 2013 test data consists of four datasets: 
two datasets consisting of gloss pairs (OnWN: 561 
pairs and FNWN: 189 pairs), a dataset of machine 
translation evaluation pairs (SMT: 750 pairs) and a 
dataset consisting of news headlines (headlines: 750 
pairs). For each dataset, the output of a system is 
evaluated via comparison with human-annotated 
similarity scores and measured using the Pearson 
Correlation Coefficient. Then a weighted sum of the 
correlations for all datasets are taken to be the final 
score, where each dataset?s weight is the proportion 
of sentence pairs in that dataset. 
We computed the similarity scores using three 
different feature sets (for our three runs) for the sup-
port vector regression model: 
1. All features mentioned in Section 2. This set of 
features were used in our run 1. 
2. All features except word ?-gram overlap (ex-
periments on STS 2012 test data revealed that 
using word n-grams actually lowers the perfor-
mance of our model, hence this decision). These 
are the features that were used in our run 2. 
3. Only character ?-gram and length features (just 
to test the performance of the model without 
178
any semantic features). Our run 3 was based on 
these features. 
We trained the support vector regression model 
on two different training datasets, both drawn from 
STS 2012 data: 
1. In the first setup, we chose the training datasets 
from STS 2012 that we considered the most 
similar to the test dataset. The only exception 
was the FNWN dataset, for which we selected 
the all the datasets from 2012 because no single 
dataset from STS 2012 seemed to have similar-
ity with this dataset. For the OnWN test dataset, 
we selected the OnWN dataset from STS 2012. 
For both headlines and SMT, we selected SMT-
news and SMTeuroparl from STS 2012. The ra-
tionale behind this selection was to train the ma-
chine learning model on a distribution similar to 
the test data. 
2. In the second setup, we aggregated all datasets 
(train and test) from STS 2012 and used this 
combined dataset to train the three models that 
were later applied on each STS 2013 test data. 
Here the rationale is to train on as much data as 
possible. 
Table 1 shows the results for the first setup. This 
is the performance of the set of scores which we ac-
tually submitted in STS 2013. The first four col-
umns show the correlations of our system with the 
gold standard for all runs. The rightmost column 
shows the overall weighted correlations. As we can 
see, run 1 with all the features demonstrated the best 
performance among the three runs. There was a con-
siderable drop in performance in run 3 which did not 
utilize any semantic similarity measure. 
Table 1. Results for manually selected training data 
Run headlines OnWN FNWN SMT Total 
1 .4921 .3769 .4647 .3492 .4135 
2 .4669 .4165 .3859 .3411 .4056 
3 .3867 .2386 .3726 .3337 .3309 
As evident from the table, evaluation results did 
not indicate a particularly promising system. Our 
best system ranked 63rd among the 90 systems eval-
uated in STS 2013. We further investigated to find 
out the reason: is the set of our features insufficient 
to capture text semantic similarity, or were the train-
ing data inappropriate for their corresponding test 
data? This is why we experimented with the second 
setup discussed above. Following are the results:  
Table 2. Results for combined training data 
Run headlines OnWN FNWN SMT Total 
1 .6854 .5981 .4647 .3518 .5339 
2 .7141 .5953 .3859 .349 .5352 
3 .6998 .4826 .3726 .3365 .4971 
As we can see in Table 2, the correlations for all 
feature sets improved by more than 10% for each 
run. In this case, the best system with correlation 
0.5352 would rank 21st among all systems in STS 
2013. These results indicate that the primary reason 
behind the system?s previous bad performance (Ta-
ble 1) was the selection of an inappropriate dataset. 
Although it was not clear in the beginning which of 
the two options would be the better, this second ex-
periment reveals that selecting the largest possible 
dataset to train is the better choice for this dataset. 
5 Conclusions 
In this paper, we have shown how simple measures 
of text similarity using minimal external resources 
can be used in a machine learning setup to compute 
semantic similarity between short text fragments. 
One important finding is that more training data, 
even when drawn from annotations on different 
sources of text and thus potentially having different 
feature value distributions, improve the accuracy of 
the model in the task. Possible future expansion in-
cludes use of more robust concept alignment strate-
gies using semantic role labeling, inclusion of struc-
tural similarities of the sentences (e.g. word order, 
syntax) in the feature set, incorporating word sense 
disambiguation and more robust strategies of con-
cept weighting into the process, among others. 
References 
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: a pilot on se-
mantic textual similarity. In Proceedings of the First 
Joint Conference on Lexical and Computational Se-
mantics. ACL, Stroudsburg, PA, USA, 385-393. 
Daniel B?r, Chris Biemann, Iryna Gurevych, and Torsten 
Zesch. 2012. UKP: computing semantic textual simi-
larity by combining multiple content similarity 
measures. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics. ACL, 
Stroudsburg, PA, USA, 435-440. 
Aminul Islam and Diana Inkpen. 2008. Semantic text 
similarity using corpus-based word similarity and 
string similarity.  ACM Trans. Knowl. Discov. Data 2, 
2, Article 10 (July 2008), 25 pages. 
179
Yuhua Li, David Mclean, Zuhair A. Bandar, James D. 
O?Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE 
Transactions on Knowledge and Data Engineering, 
vol.18, no.8, 1138-1150. 
Hugo Liu and Push Singh. 2004. ConceptNet ? a prac-
tical commonsense reasoning tool-kit. BT Technology 
Journal 22, 4 (October 2004), 211-226. 
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 
2006. Corpus-based and knowledge-based measures 
of text semantic similarity. In Proceedings of the 21st 
national conference on Artificial intelligence - Volume 
1 (AAAI'06), Anthony Cohn (Ed.), Vol. 1. AAAI 
Press 775-780. 
David Milne and Ian H. Witten. 2013. An open-source 
toolkit for mining Wikipedia. Artif. Intell. 194 (Janu-
ary 2013), 222-239. 
Frane ?ari?, Goran Glava?, Mladen Karan, Jan ?najder, 
and Bojana Dalbelo Ba?i?.?ari?. 2012. TakeLab: sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the First Joint Conference on Lexical and 
Computational Semantics. ACL, Stroudsburg, PA, 
USA, 441-448.  
180
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 10?14, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ClearTK-TimeML: A minimalist approach to TempEval 2013
Steven Bethard
Center for Computational Language and Education Research
University of Colorado Boulder
Boulder, Colorado 80309-0594, USA
steven.bethard@colorado.edu
Abstract
The ClearTK-TimeML submission to Temp-
Eval 2013 competed in all English tasks: identi-
fying events, identifying times, and identifying
temporal relations. The system is a pipeline of
machine-learning models, each with a small set
of features from a simple morpho-syntactic an-
notation pipeline, and where temporal relations
are only predicted for a small set of syntac-
tic constructions and relation types. ClearTK-
TimeML ranked 1st for temporal relation F1,
time extent strict F1 and event tense accuracy.
1 Introduction
The TempEval shared tasks (Verhagen et al, 2007;
Verhagen et al, 2010; UzZaman et al, 2013) have
been one of the key venues for researchers to com-
pare methods for temporal information extraction. In
TempEval 2013, systems are asked to identify events,
times and temporal relations in unstructured text.
This paper describes the ClearTK-TimeML system
submitted to TempEval 2013. This system is based
off of the ClearTK framework for machine learning
(Ogren et al, 2008)1, and decomposes TempEval
2013 into a series of sub-tasks, each of which is for-
mulated as a machine-learning classification problem.
The goals of the ClearTK-TimeML approach were:
? To use a small set of simple features that can be
derived from either tokens, part-of-speech tags or
syntactic constituency parses.
? To restrict temporal relation classification to a sub-
set of constructions and relation types for which
the models are most confident.
1http://cleartk.googlecode.com/
Thus, each classifier in the ClearTK-TimeML
pipeline uses only the features shared by success-
ful models in previous work (Bethard and Martin,
2006; Bethard and Martin, 2007; Llorens et al, 2010;
UzZaman and Allen, 2010) that can be derived from
a simple morpho-syntactic annotation pipeline2. And
each of the temporal relation classifiers is restricted
to a particular syntactic construction and to a partic-
ular set of temporal relation labels. The following
sections describe the models, classifiers and datasets
behind the ClearTK-TimeML approach.
2 Time models
Time extent identification was modeled as a BIO
token-chunking task, where each token in the text
is classified as being at the B(eginning) of, I(nside)
of, or O(utside) of a time expression. The following
features were used to characterize tokens:
? The token?s text
? The token?s stem
? The token?s part-of-speech
? The unicode character categories for each character
of the token, with repeats merged (e.g. Dec28
would be ?LuLlNd?)
? The temporal type of each alphanumeric sub-token,
derived from a 58-word gazetteer of time words
? All of the above features for the preceding 3 and
following 3 tokens
Time type identification was modeled as a multi-
class classification task, where each time is classified
2 OpenNLP sentence segmenter, ClearTK PennTreebank-
Tokenizer, Apache Lucene Snowball stemmer, OpenNLP part-
of-speech tagger, and OpenNLP constituency parser
10
as DATE, TIME, DURATION or SET. The following
features were used to characterize times:
? The text of all tokens in the time expression
? The text of the last token in the time expression
? The unicode character categories for each character
of the token, with repeats merged
? The temporal type of each alphanumeric sub-token,
derived from a 58-word gazetteer of time words
Time value identification was not modeled by the
system. Instead, the TimeN time normalization sys-
tem (Llorens et al, 2012) was used.
3 Event models
Event extent identification, like time extent identi-
fication, was modeled as BIO token chunking. The
following features were used to characterize tokens:
? The token?s text
? The token?s stem
? The token?s part-of-speech
? The syntactic category of the token?s parent in the
constituency tree
? The text of the first sibling of the token in the
constituency tree
? The text of the preceding 3 and following 3 tokens
Event aspect identification was modeled as a multi-
class classification task, where each event is classi-
fied as PROGRESSIVE, PERFECTIVE, PERFECTIVE-
PROGRESSIVE or NONE. The following features
were used to characterize events:
? The part-of-speech tags of all tokens in the event
? The text of any verbs in the preceding 3 tokens
Event class identification was modeled as a multi-
class classification task, where each event is classi-
fied as OCCURRENCE, PERCEPTION, REPORTING,
ASPECTUAL, STATE, I-STATE or I-ACTION. The
following features were used to characterize events:
? The stems of all tokens in the event
? The part-of-speech tags of all tokens in the event
Event modality identification was modeled as a
multi-class classification task, where each event is
classified as one of WOULD, COULD, CAN, etc. The
following features were used to characterize events:
? The text of any prepositions, adverbs or modal
verbs in the preceding 3 tokens
Event polarity identification was modeled as a bi-
nary classification task, where each event is classified
as POS or NEG. The following features were used to
characterize events:
? The text of any adverbs in the preceding 3 tokens
Event tense identification was modeled as a multi-
class classification task, where each event is clas-
sified as FUTURE, INFINITIVE, PAST, PASTPART,
PRESENT, PRESPART or NONE. The following fea-
tures were used to characterize events:
? The last two characters of the event
? The part-of-speech tags of all tokens in the event
? The text of any prepositions, verbs or modal verbs
in the preceding 3 tokens
4 Temporal relation models
Three different models, described below, were trained
for temporal relation identification. All models fol-
lowed a multi-class classification approach, pairing
an event and a time or an event and an event, and
trying to predict a temporal relation type (BEFORE,
AFTER, INCLUDES, etc.) or NORELATION if there
was no temporal relation between the pair.
While the training and evaluation data allowed
for 14 possible relation types, each of the temporal
relation models was restricted to a subset of relations,
with all other relations mapped to the NORELATION
type. The subset of relations for each model was
selected by inspecting the confusion matrix of the
model?s errors on the training data, and removing
relations that were frequently confused and whose
removal improved performance on the training data.
Event to document creation time relations were
classified by considering (event, time) pairs where
each event in the text was paired with the document
creation time. The classifier was restricted to the rela-
tions BEFORE, AFTER and INCLUDES. The follow-
ing features were used to characterize such relations:
? The event?s aspect (as classified above)
? The event?s class (as classified above)
? The event?s modality (as classified above)
? The event?s polarity (as classified above)
? The event?s tense (as classified above)
? The text of the event, only if the event was identi-
fied as having class ASPECTUAL
11
Event to same sentence time relations were clas-
sified by considering (event, time) pairs where the
syntactic path from event to time matched a regu-
lar expression of syntactic categories and up/down
movements through the tree: ?((NP|PP|ADVP)?)*
((VP|SBAR|S)?)* (S|SBAR|VP|NP) (?(VP|SBAR|S))*
(?(NP|PP|ADVP))*$. The classifier relations were re-
stricted to INCLUDES and IS-INCLUDED. The follow-
ing features were used to characterize such relations:
? The event?s class (as classified above)
? The event?s tense (as classified above)
? The text of any prepositions or verbs in the 5 tokens
following the event
? The time?s type (as classified above)
? The text of all tokens in the time expression
? The text of any prepositions or verbs in the 5 tokens
preceding the time expression
Event to same sentence event relations were clas-
sified by considering (event, event) pairs where
the syntactic path from one event to the other
matched ?((VP?|ADJP?|NP?)? (VP|ADJP|S|SBAR)
(?(S|SBAR|PP))* ((?VP|?ADJP)*|(?NP)*)$. The classi-
fier relations were restricted to BEFORE and AFTER.
The following features were used to characterize such
relations:
? The aspect (as classified above) for each event
? The class (as classified above) for each event
? The tense (as classified above) for each event
? The text of the first child of the grandparent of the
event in the constituency tree, for each event
? The path through the syntactic constituency tree
from one event to the other
? The tokens appearing between the two events
5 Classifiers
The above models described the translation from
TempEval tasks to classification problems and clas-
sifier features. For BIO token-chunking problems,
Mallet3 conditional random fields and LIBLINEAR4
support vector machines and logistic regression were
applied. For the other problems, LIBLINEAR, Mal-
let MaxEnt and OpenNLP MaxEnt5 were applied.
All classifiers have hyper-parameters that must be
3http://mallet.cs.umass.edu/
4http://www.csie.ntu.edu.tw/?cjlin/liblinear/
5http://opennlp.apache.org/
tuned during training ? LIBLINEAR has the classi-
fier type and the cost parameter, Mallet CRF has the
iteration count and the Gaussian prior variance, etc.6
The best classifier for each training data set was
selected via a grid search over classifiers and param-
eter settings. The grid of parameters was manually
selected to provide several reasonable values for each
classifier parameter. Each (classifier, parameters)
point on the grid was evaluated with a 2-fold cross
validation on the training data, and the best perform-
ing (classifier, parameters) was selected as the final
model to run on the TempEval 2013 test set.
6 Data sets
The classifiers were trained using the following
sources of training data:
TB The TimeBank event, time and relation annota-
tions, as provided by the TempEval organizers.
AQ The AQUAINT event, time and relation annota-
tions, as provided by the TempEval organizers.
SLV The ?Silver? event, time and relation annota-
tions, from the TempEval organizers? system.
BMK The verb-clause temporal relation annotations
of (Bethard et al, 2007). These relations are
added on top of the original relations.
PM The temporal relations inferred via closure on
the TimeBank and AQUAINT data by Philippe
Muller7. These relations replace the original
ones, except in files where no relations were
inferred (because of temporal inconsistencies).
7 Results
Table 1 shows the performance of the ClearTK-
TimeML models across the different tasks when
trained on different sets of training data. The ?Data?
column of each row indicates both the training data
sources (as in Section 6), and whether the events and
times were predicted by the models (?system?) or
taken from the annotators (?human?). Performance
is reported in terms of strict precision (P), Recall (R)
and F1 for event extents, time extents and temporal
relations, and in terms of Accuracy (A) on the cor-
rectly identified extents for event and time attributes.
6For BIO token-chunking tasks, LIBLINEAR also had a pa-
rameter for how many previous classifications to use as features.
7https://groups.google.com/d/topic/tempeval/
LJNQKwYHgL8
12
Data Event Time Relation
annotation events extent class tense aspect extent value type type
sources & times F1 P R A A A F1 P R A A F1 P R
TB+BMK system 77.3 81.9 73.3 84.6 80.4 91.0 82.7 85.9 79.7 71.7 93.3 31.0 34.1 28.4
TB system 77.3 81.9 73.3 84.6 80.4 91.0 82.7 85.9 79.7 71.7 93.3 29.8 34.5 26.2
TB+AQ system 78.8 81.4 76.4 86.1 78.2 90.9 77.0 83.2 71.7 69.9 92.9 28.6 30.9 26.6
TB+AQ+PM system 78.8 81.4 76.4 86.1 78.2 90.9 77.0 83.2 71.7 69.9 92.9 28.5 29.7 27.3
*TB+AQ+SLV system 80.5 82.1 78.9 88.4 71.6 91.2 80.0 91.6 71.0 73.6 91.5 27.8 26.5 29.3
Highest in TempEval 81.1 82.0 80.8 89.2 80.4 91.8 82.7 91.4 80.4 86.0 93.7 31.0 34.5 34.4
TB+BMK human - - - - - - - - - - - 36.3 37.3 35.2
TB human - - - - - - - - - - - 35.2 37.6 33.0
TB+AQ human - - - - - - - - - - - 34.1 33.3 35.0
TB+AQ+PM human - - - - - - - - - - - 35.9 35.2 36.6
*TB+AQ+SLV human - - - - - - - - - - - 37.7 34.9 41.0
Highest in TempEval - - - - - - - - - - - 36.3 37.6 65.6
Table 1: Performance across different training data. Systems marked with * were tested after the official evaluation.
Scores in bold are at least as high as the highest in TempEval.
Training on the AQUAINT (AQ) data in addition to
the TimeBank (TB) hurt times and relations. Adding
the AQUAINT data caused a -2.7 drop in extent preci-
sion, a -8.0 drop in extent recall, a -1.8 drop in value
accuracy and a -0.4 drop in type accuracy, and a -3.6
to -4.3 drop in relation recall.
Training on the ?Silver? (SLV) data in addition
to TB+AQ data gave mixed results. There were big
gains for time extent precision (+8.4), time value ac-
curacy (+3.7), event extent recall (+2.5) and event
class accuracy (+2.3), but a big drop for event tense
accuracy (-6.6). Relation recall improved (+2.7 with
system events and times, +6.0 with manual) but pre-
cision varied (-4.4 with system, +1.6 with manual).
Adding verb-clause relations (BMK) and closure-
inferred relations (PM) increased recall but low-
ered precision. With system-annotated events and
times, the change was +2.2/-0.4 (recall/precision)
for verb-clause relations, and +0.7/-1.2 for closure-
inferred relations. With manually-annotated events
and times, the change was +2.2/-0.3 for verb-clause
relations, and (the one exception where recall im-
proved) +1.5/+1.9 for closure-inferred relations.
8 Discussion
Overall, the ClearTK-TimeML ranked 1st in relation
F1, time extent strict F1 and event tense accuracy.
Analysis across the different ClearTK-TimeML
runs showed that including annotations from the
AQUAINT corpus hurt model performance across
a variety of tasks. A manual inspection of the
AQUAINT corpus revealed many annotation errors,
suggesting that the drop may be the result of attempt-
ing to learn from inconsistent training data. The
AQUAINT corpus may thus have to be partially re-
annotated to be useful as a training corpus.
Analysis also showed that adding more relation
annotations increased recall, typically at the cost of
precision, even though the added annotations were
highly accurate: (Bethard et al, 2007) reported agree-
ment of 90%, and temporal closure relations were
100% deterministic from the already-annotated re-
lations. One would expect that adding such high-
quality relations would only improve performance.
But not all temporal relations were annotated by the
TempEval 2013 annotators, so the system could be
marked wrong for a finding a true temporal relation
that was not noticed by the annotators. Further analy-
sis is necessary to investigate this hypothesis.
Acknowledgements
Thanks to Philippe Muller for providing the closure-
inferred relations. The project described was supported in
part by Grant Number R01LM010090 from the National
Library Of Medicine. The content is solely the responsi-
bility of the authors and does not necessarily represent the
official views of the National Library Of Medicine or the
National Institutes of Health.
13
References
[Bethard and Martin2006] Steven Bethard and James H.
Martin. 2006. Identification of event mentions and
their semantic class. In Empirical Methods in Natural
Language Processing (EMNLP), page 146154. (Accep-
tance rate 31%).
[Bethard and Martin2007] Steven Bethard and James H.
Martin. 2007. CU-TMP: temporal relation classifica-
tion using syntactic and semantic features. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations, pages 129?132, Prague, Czech Republic.
Association for Computational Linguistics.
[Bethard et al2007] Steven Bethard, James H. Martin, and
Sara Klingenstein. 2007. Finding temporal structure
in text: Machine learning of syntactic temporal rela-
tions. International Journal of Semantic Computing,
01(04):441.
[Llorens et al2010] Hector Llorens, Estela Saquete, and
Borja Navarro. 2010. TIPSem (English and Spanish):
Evaluating CRFs and semantic roles in TempEval-2.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, page 284291, Uppsala, Sweden,
July. Association for Computational Linguistics.
[Llorens et al2012] Hector Llorens, Leon Derczynski,
Robert Gaizauskas, and Estela Saquete. 2012. TIMEN:
an open temporal expression normalisation resource.
In Proceedings of the Eight International Conference
on Language Resources and Evaluation (LREC?12),
Istanbul, Turkey, May. European Language Resources
Association (ELRA).
[Ogren et al2008] Philip V. Ogren, Philipp G. Wetzler,
and Steven Bethard. 2008. ClearTK: A UIMA toolkit
for statistical natural language processing. In Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP workshop at Language Resources and
Evaluation Conference (LREC), 5.
[UzZaman and Allen2010] Naushad UzZaman and James
Allen. 2010. TRIPS and TRIOS system for TempEval-
2: extracting temporal information from text. In Pro-
ceedings of the 5th International Workshop on Semantic
Evaluation, page 276283, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
[UzZaman et al2013] Naushad UzZaman, Hector Llorens,
James F. Allen, Leon Derczynski, Marc Verhagen,
and James Pustejovsky. 2013. SemEval-2013 task
1: TempEval-3 evaluating time expressions, events, and
temporal relations. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Conference
on Lexical and Computational Semantcis (*SEM 2013).
Association for Computational Linguistics, June.
[Verhagen et al2007] Marc Verhagen, Robert Gaizauskas,
Frank Schilder, Mark Hepple, Graham Katz, and James
Pustejovsky. 2007. SemEval-2007 task 15: TempEval
temporal relation identification. In Proceedings of the
4th International Workshop on Semantic Evaluations,
pages 75?80, Prague, Czech Republic. Association for
Computational Linguistics.
[Verhagen et al2010] Marc Verhagen, Roser Sauri, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, page
5762, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
14
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 255?262, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 3: Spatial Role Labeling
Oleksandr Kolomiyets?, Parisa Kordjamshidi?,
Steven Bethard? and Marie-Francine Moens?
?KU Leuven, Celestijnenlaan 200A, Heverlee 3001, Belgium
?University of Colorado, Campus Box 594 Boulder, Colorado, USA
Abstract
Many NLP applications require information
about locations of objects referenced in text,
or relations between them in space. For ex-
ample, the phrase a book on the desk contains
information about the location of the object
book, as trajector, with respect to another ob-
ject desk, as landmark. Spatial Role Label-
ing (SpRL) is an evaluation task in the infor-
mation extraction domain which sets a goal
to automatically process text and identify ob-
jects of spatial scenes and relations between
them. This paper describes the task in Se-
mantic Evaluations 2013, annotation schema,
corpora, participants, methods and results ob-
tained by the participants.
1 Introduction
Spatial Role Labeling at SemEval-2013 is the sec-
ond iteration of the task, which was initially in-
troduced at SemEval-2012 (Kordjamshidi et al,
2012a). The second iteration extends the previous
work with an additional training corpus, which con-
tains besides ?static? spatial relations, annotated mo-
tions. Motion detection is a novel task for annotating
trajectors (objects, which are moving), landmarks
(spatial context in which the motion is performed),
motion indicators (lexical triggers which signals tra-
jector?s motion), paths (a path along which the mo-
tion is performed), directions (absolute or relative
directions of trajector?s motion) and distances (a
distance as a product of motion). For annotating
motions the existing annotation scheme has been
adapted with additional markables which are, all to-
gether, described below.
2 Spatial Annotation Schema
In this Section we describe the annotation format of
spatial markables in text, and annotation guidelines
for the annotators.
2.1 Spatial Annotation Format
Building upon the previous work, we used the no-
tions of trajectors, landmarks and spatial indicators
as introduced by Kordjamshidi et al (2010). In ad-
dition, we further expanded the set of spatial roles
labels with motion indicators, paths, directions and
distances to capture fine-grained spatial semantics of
static spatial relations (as the ones which do not in-
volve motions), and to accommodate dynamic spa-
tial relations (the ones which do involve motions).
2.1.1 Static Spatial Relations and their Roles
Static spatial relations are defined as relations be-
tween still objects, whereas one object plays a cen-
tral role in the spatial scene, which is called tra-
jector, and the second one plays a secondary role,
and it is called landmark. In language, a spatial re-
lation between two objects is usually implemented
by a preposition (in, on, at, etc.) or a prepositional
phrase (on top of, inside of, etc.).
A static spatial relation is defined as a tuple that
contains a trajector, a landmark and a spatial indica-
tor. In the annotation schema, these annotations are
defined as follows:
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase that denotes a central
object of a spatial scene. For example:
? [Trajector a lake] in the forest
255
? [Trajector a flag] on top of the building
Landmark: Landmark is a spatial role label as-
signed to a word or a phrase that denotes a secondary
object of a spatial scene, to which a possible spatial
relation (as between two objects in space) can be es-
tablished. For example:
? a lake in [Landmark the forest]
? a flag on top of [Landmark the building]
Spatial Indicator: Spatial Indicator is a spatial
role label assigned to a word or a phrase that sig-
nals a spatial relation between objects (trajectors and
landmarks) of a spatial scene. For example:
? a lake [Sp indicator in] the forest
? a flag] [Sp indicator on top of ] the building
Spatial Relation: Spatial Relation is a relation
that holds between spatial markables in text as, e.g.,
between a trajector and a landmark and triggered by
a spatial indicator. In spatial information theory the
relations and properties are usually grouped into the
domains of topological, directional, and distance re-
lations and also shape (Stock, 1998). Three semantic
classes for spatial relations were proposed:
? Region. This type refers to a region of space
which is always defined in relation to a land-
mark, e.g., the interior or exterior. For exam-
ple:
a lake in the forest =? ?Region, [Sp indicator
in], [Trajector a lake], [Landmark the forest]?
? Direction. This relation type denotes a direc-
tion along the axes provided by the different
frames of reference, in case the trajector of mo-
tion is not characterized in terms of its relation
to the region of a landmark. For example:
a flag on top of the building =? ?Direction,
[Sp indicator on top of ], [Trajector a flag],
[Landmark the building]?
? Distance. Type Distance states information
about the spatial distance of the objects and
could be a qualitative expression, such as close,
far or quantitative, such as 12 km. For example:
the kids are close to the blackboard =?
?Distance, [Distance close], [Trajector the kids],
[Landmark the blackboard]?
2.1.2 Dynamic Spatial Relations
In addition to static spatial relations and their
roles, SpRL-2013 introduces new spatial roles to
capture dynamic spatial relations which involve
motions. Let us demonstrate this with the following
example:
(1) In Brazil coming from the North-East I
stepped into the small forest and followed down a
dried creek.
The text above describes a motion, and the reader
can identify a number of concepts which are pecu-
liar for motions: there is an object whose location
is changing, the motion is performed in a specific
spatial context, with a specific direction, and with a
number of locations related to the object?s motion.
There has been an enormous effort in formalizing
and annotating motions in natural language. While
annotating motions was out of scope for the previ-
ous SpRL task and SpatialML (Mani et al, 2010),
the most recent work on the Dynamic Interval Tem-
poral Logic (DITL) (Pustejovsky and Moszkowicz,
2011) presents a framework for modeling motions
as a change of state, which adapts linguistic back-
ground considering path constructions and manner-
of-motion constructions. On this basis the Spa-
tiotemporal Markup Language (STML) has been in-
troduced for annotating motions in natural language.
In STML, a motion is treated as a change of location
over time, while differentiating between a number
of spatial configurations along the path. Being well-
defined for the formal representations of motion and
reasoning, in which representations either take ex-
plicit reference to temporal frames or reify a spatial
object for a path, all the previous work seems to be
difficult to apply in practice when annotating mo-
tions in natural language. It can be attributed to pos-
sible vague descriptions of path in natural language
when neither clear temporal event ordering, nor dis-
tinction between the start, end or intermediate path
point can be made.
In SpRL-2013, we simplify the previously intro-
duced notion of path in order to provide practical
motion annotations. For dynamic spatial relations
we introduce the following roles:
256
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase which denotes an object
which moves, starts, interrupts, resumes a motion, or
is forcibly involved in a motion. For example:
? ... coming from the North-East [Trajector I]
stepped into ...
Motion Indicator: Motion indicator is a spatial
role label assigned to a word or a phrase which sig-
nals a motion of the trajector along a path. In Exam-
ple (1), a number of motion indicators can be identi-
fied:
? ... [Motion coming] from the North-East I
[Motion stepped into] ... and [Motion followed
down] ...
Path: Path is a spatial role label assigned to a word
or phrase that denotes the path of the motion as the
trajector is moving along, starting in, arriving in or
traversing it. In SpRL-2013, as opposite to STML,
the notion of path does not have the temporal dimen-
sion, thus whenever the motion is performed along a
path, for which either a start, an intermediate, an end
path point, or an entire path can be identified in text,
they are labeled as path. In Example (1), a number
of path labels can be identified:
? ... coming [Path from the North-East] I stepped
into [Path the small forest] and followed down
[Path a dried creek].
Landmark: The notion of path should not be con-
fused with landmarks. For spatial annotations, land-
mark has been introduced as a spatial role label for
a secondary object of the spatial scene. Being of
great importance for static spatial relations, in dy-
namic spatial relations, landmarks are used to cap-
ture a spatial context of a motion as for example:
? In [Landmark Brazil] coming from the North-
East ...
Distance: In contrast to the previous SpRL anno-
tation standard, in which distances and directions
have been uniformly treated as signals, in SpRL-
2013 if the motion is performed for a certain dis-
tance, and such a distance is mentioned in text, the
corresponding textual span is labeled as distance.
Distance is a spatial role label assigned to a word
or a phrase that denotes an absolute or relative dis-
tance of motion, or the distance between a trajector
and a landmark in case of a static spatial scene. For
example:
? [Distance 25 km]
? [Distance about 100 m]
? [Distance not far away]
? [Distance 25 min by car]
Direction: Additionally, if the motion is per-
formed in a certain (absolute or relative) direction,
and such a direction is mentioned in text, the corre-
sponding textual span is annotated as direction. Di-
rection is a spatial role label assigned to a word or
a phrase that denotes an absolute or relative direc-
tion of motion, or a spatial arrangement between a
trajector and a landmark. For example:
? [Direction the North-West]
? [Direction northwards]
? [Direction west]
? [Direction the left-hand side]
Spatial Relation: Similarly to static spatial rela-
tions, dynamic spatial relations are annotated by re-
lations that hold between a number of spatial roles.
The major difference to static spatial relations is the
mandatory motion indicator1. For example:
? In Brazil coming from the North-East I ...
=? ?Direction, [Sp indicator In], [Trajector I],
[Landmark Brazil], [Motion coming],[Path from
the North-East]?
? ... I stepped into the small forest and ...
=? ?Direction, [Trajector I], [Motion stepped
into],[Path the small forest]?
? ... I [...] and followed down a dried creek.
=? ?Direction, [Trajector I], [Motion followed
down],[Path a dried creek]?
1All dynamic spatial relations were annotated with type Di-
rection.
257
Corpus Files Sent. TR LM SI MI Path Dir Dis Relation
IAPR TC-12
Training 1 600 716 661 670 - - - - 765
Evaluation 1 613 872 743 796 - - - - 940
Confluence
Project
Training 95 1422 1701 1037 879 1039 945 223 307 2105
Evaluation 22 367 497 316 247 305 240 37 87 598
Table 1: Corpus statistics for SpRL-2013 with respect to annotated spatial roles (trajectors (TR), landmarks (LM),
spatial indicators (SI), motion indicators (MI), paths (Path), directions (Dir) and distances (Dis)) and spatial relations.
3 Corpora
The data for the shared task comprises two different
corpora.
3.1 IAPR TC-12 Image Benchmark Corpus
The first corpus is a subset of the IAPR TC-12 image
benchmark corpus (Grubinger et al, 2006). It con-
tains 613 text files that include 1213 sentences in to-
tal, and represents an extension of the dataset previ-
ously used in (Kordjamshidi et al, 2011). The orig-
inal corpus was available free of charge and without
copyright restrictions. The corpus contains images
taken by tourists with descriptions in different lan-
guages. The texts describe objects, and their abso-
lute and relative positions in the image. This makes
the corpus a rich resource for spatial information,
however, the descriptions are not always limited to
spatial information. Therefore, they are less domain-
specific and contain free explanations about the im-
ages. For training we released 600 sentences (about
50% of the corpus), and used remaining 613 sen-
tences for evaluations.
3.2 Confluence Project Corpus
The second corpus comes from the Confluence
project that targets the description of locations sit-
uated at each of the latitude and longitude inte-
ger degree intersection in the world. This corpus
contains user-generated content produced by, some-
times, non-native English speakers. We gathered the
content by keeping the original orthography and for-
mating. In addition, we stored the URLs of the de-
scriptions and extracted the coordinates of the de-
scribed confluence point, which might be interest-
ing for further research. In total, the entire corpus
contains 117 files with 1789 sentences (about 40,000
tokens). For training we released 95 annotated files
with 1422 sentences, 2105 annotated relations in to-
tal. For evaluation we used 22 annotated files with
367 sentences. The statistics on both corpora are
provided in Table 1.
3.3 Data Format
One important change to the data was made in
SpRL-2013. In contrast to SpRL-2012, where spa-
tial roles were annotated over ?head words? whose
indexes were part of unique identifiers, in SpRL-
2013 we switched to span-based annotations. More-
over, in order to provide a single data format for
the task, we transformed SpRL-2012 data into span-
based annotations, in course of which, we identified
a number of annotation errors and made further im-
provements for about 50 annotations.
For annotating the Confluence Project corpus we
used a freely available annotation tool MAE created
by Amber Stubbs (Stubbs, 2011). The resulting data
format uses the same annotation tags as in SpRL-
2012, but each role annotation refers to a character
offset in the original text2. Spatial relations are com-
posed of references to annotations by their unique
identifiers. Similarly to SpRL-2012, we allowed
annotators to provide non-consuming annotations,
where entity mentions, for which spatial roles can
be identified, are omitted in text but necessary for a
spatial relation triggered by either a spatial indicator
or a motion indicator. Two spatial roles are eligible
for non-consuming annotations: trajectors and land-
marks.
4 Tasks Descriptions
For the sake of consistency with SpRL-2012, in
SpRL-2013 we proposed the following tasks:
2Due to paper length constraints we omit the BNF specifica-
tions for spatial roles and relations. For further data format in-
formation we refer the reader to the task description web page:
www.cs.york.ac.uk/semeval-2013/task3/
258
? Task A: Identification of markable spans for
three types of spatial annotations such as tra-
jector, landmark and spatial indicator.
? Task B: Identification of tuples (triplets) that
connect trajectors, landmarks and spatial indi-
cators identified in Task A into spatial relations.
That is, identification of spatial relations with
three markables connected, and without se-
mantic relation classification.
? Task C: Identification of markable spans for all
spatial annotations such as trajector, landmark,
spatial indicator, motion indicator, path, direc-
tion and distance.
? Task D: Identification of n-tuples that connect
spatial markables identified in Task C into spa-
tial relations. That is, identification of spatial
relations with as many participating mark-
ables as possible, and without semantic rela-
tion classification.
? Task E: Semantic classification of spatial rela-
tions identified in Task D.
5 Evaluation Criteria and Metrics
System outputs were evaluated against the gold
annotations, which had to conform to the role?s
Backus-Naur form. For Tasks A and C, the system
annotations are spatial roles: spans of text associated
with spatial role types. A system annotation of a
role is considered correct if it has a minimal overlap
of one character with a gold annotation and matches
the role type of the gold annotation. For Tasks B and
D, the system annotations are spatial relation tuples
(of length 3 in task B, of length 3 to 5 in Task D) of
references to markable annotations. A system anno-
tation of a spatial relation tuple is considered correct
if it is of the same length as the gold annotation, and
if each spatial role in the system tuple matches each
role in the gold tuple. A spatial role estimated by a
system is considered correct if it matches a gold ref-
erence when having the same character offsets and
markable types (strict evaluation settings). In ad-
dition we introduced relaxed evaluation settings, in
which a minimal overlap of one character between
a system and a gold markable references is required
for a positive match under condition that the roles
match. For Task E, the system annotations are spa-
tial relation tuples of length 3 to 5, along with re-
lation type labels. A system annotation of a spatial
relation is considered correct if the spatial relation
tuple is correct under the evaluation of Task D and
the relation type of the system relation is the same
as the relation type of the gold relation.
Systems were evaluated for each of the tasks in
terms of precision (P), recall (R) and F1-score which
are defined as follows:
Precision =
tp
tp + fp
(1)
Recall =
tp
tp + fn
(2)
where tp is the number of true positives (the num-
ber of instances that are correctly found), fp is the
number of false positives (number of instances that
are predicted by the system but not a true instance),
and fn is the number of false negatives (missing re-
sults).
F1 = 2 ?
Precision ?Recall
Precision + Recall
(3)
6 System Description and Evaluation
Results
UNITOR. The UNITOR-HMM-TK system ad-
dressed Tasks A,B and C (Bastianelli et al, 2013).
In Tasks A and C, roles are labeled by a sequence-
based classifier: each word in a sentence is classi-
fied with respect to the possible spatial roles. An
approach based on the SVM-HMM learning algo-
rithm, formulated in (Tsochantaridis et al, 2006),
was used. It is in line with other methods based
on sequence-based classifier for Spatial Role La-
beling, such as Conditional Random Fields (Kord-
jamshidi et al, 2011), and the same SVM-HMM
learning algorithm (Kordjamshidi et al, 2012b).
UNITOR?s labeling approach has been inspired by
the work in (Croce et al, 2012), where an SVM-
HMM learning algorithm has been applied to the
classical FrameNet-based Semantic Role Labeling.
The main contribution of the proposed approach is
the adoption of shallow grammatical features instead
of the full syntax of the sentence, in order to avoid
over-fitting on the training data. Moreover, lexical
information has been generalized through the use
259
Run Task Evaluation Label P R F1-score
UNITOR.Run1.1
Task A relaxed
TR 0.684 0.681 0.682
LM 0.741 0.835 0.785
SI 0.967 0.889 0.926
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run1.2
Task A relaxed
TR 0.682 0.493 0.572
LM 0.801 0.560 0.659
SI 0.968 0.585 0.729
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run2.1
Task A relaxed
TR 0.565 0.317 0.406
LM 0.661 0.476 0.554
SI 0.612 0.481 0.538
Task C relaxed
TR 0.565 0.317 0.406
LM 0.662 0.476 0.554
SI 0.609 0.479 0.536
MI 0.892 0.294 0.443
Path 0.775 0.295 0.427
Dir 0.312 0.229 0.264
Dis 0.946 0.331 0.490
Table 2: Results of UNITOR for SpRL-2013 tasks (Task A, B and C).
of Word Space ? a Distributional Model of Lexi-
cal Semantics derived from the unsupevised anal-
ysis of an unlabeled large-scale corpus (Sahlgren,
2006). Similarly to the approaches demonstrated
in SpRL-2012, the proposed approach first classi-
fies spatial and motion indicators, then, using these
outcomes further spatial roles are determined. For
classifying indicators, the classifier makes use of
lexical and grammatical features like lemmas, part-
of-speech tags and lexical context representations.
The remaining spatial roles are estimated by another
classifier additionally employing the lemma of the
indicator, distance and relative position to the indi-
cator, and the number of tokens composing the indi-
cator as features.
In Task B, all roles found in a sentence for Task A
are combined to generate candidate relations, which
are verified by a Support Vector Machine (SVM)
classifier. As the entire sentence is informative
to determine the proper conjunction of all roles, a
Smoothed Partial Tree Kernel (SPTK) within the
classifier that enhances both syntactic and lexical in-
formation of the examples was applied (Croce et al,
2011). This is a convolution kernel that measures the
similarity between syntactic structures, which are
partially similar and whose nodes can be different,
but are, nevertheless, semantically related. Each ex-
ample is represented as a tree-structure which is di-
rectly derived from the sentence dependency parse,
and thus allows for avoiding manual feature engi-
neering as in contrast to the work of Roberts and
Harabagiu (2012). In the end, the similarity score
between lexical nodes is measured by the Word
Space model.
UNITOR submitted two runs for the IAPR TC-
12 Image benchmark corpus (we refer to them
as to UNITOR.Run1.1 and UNITOR.Run1.2) and
one run for the Confluence Project corpus (UN-
ITOR.Run2.1), based on the models individually
trained on the different corpora. The difference
between UNITOR.Run1.1 and UNITOR.Run1.2 is
that for UNITOR.Run1.1 the results are obtained for
all spatial roles (also the ones that have no spatial
relation), and UNITOR.Run1.2 only provided the
roles for which also spatial relations were identified.
The results are presented in Table 2.
260
Although, not directly comparable to the results in
SpRL-2012, one may observe some common trends.
First, similarly to the previous findings, the perfor-
mance for recognition of landmarks and spatial in-
dicators (Task A) on the IAPR TC-12 Image bench-
mark corpus is better than trajectors (F1-scores of
0.785, 0.926 and 0.682 respectively), and spatial in-
dicators is the ?easiest? spatial role to recognize (F1-
score of 0.926).
In contrast, spatial role labeling on the Confluence
Project corpus performs worse than on the IAPR
TC-12 Image benchmark corpus (with F1-scores of
0.406, 0.538 and 0.554 for trajectors, spatial indica-
tors and landmarks respectively). Interestingly, the
performance for landmarks is generally higher than
for trajectors, which is in line with previous findings
in SpRL-2012. The performance drop on the new
corpus can be attributed to more complex text and
descriptions, whereas multiple roles can be identi-
fied for the same span (for example, a path which
spans over trajectors, landmarks and spatial indica-
tors). For the new spatial roles of motion indicators,
paths, directions and distances, the performance lev-
els are overall higher than for trajectors with an ex-
ception of directions. Yet, the precision levels for
new roles is much higher than the recall (0.892 vs.
0.294 for motion indicators, 0.775 vs. 0.295 for
paths and 0.946 vs. 0.331 for distances). Directions
turned out to be the most difficult role to classify
(0.312, 0.229 and 0.264 for P , R and F1-score re-
spectively).
7 Conclusion
In this paper we described an evaluation task on Spa-
tial Role Labeling in the context of Semantic Evalu-
ations 2013. The task sets a goal to automatically
process text and identify objects of spatial scenes
and relations between them. Building largely upon
the previous evaluation campaign, SpRL-2012, in
SpRL-2013 we introduced additional spatial roles
and relations for capturing motions in text. In ad-
dition, a new annotated corpus for spatial roles (in-
cluding annotated motions) was produced and re-
leased to the participants. It comprises a set of 117
files with about 40,000 tokens in total.
With the registered number of 10 participants and
the final number of submissions (only one) we can
conclude that spatial role labeling is an interesting
task within the research community, however some-
times underestimated in its complexity. Our further
steps in promoting spatial role labeling will be a de-
tailed description of the annotation scheme and an-
notation guidelines, analysis of the corpora and ob-
tained results.
Acknowledgments
The presented research was supporter by the PARIS
project (IWT - SBO 110067), TERENCE (EU FP7?
257410) and MUSE (EU FP7?296703).
References
Emanuele Bastianelli, Danilo Croce, Roberto Basili, and
Daniele Nardi. 2013. UNITOR-HMM-TK: Struc-
tured Kernel-based learning for Spatial Role Labeling.
In Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013). Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-
tianelli. 2012. Structured Learning for Semantic Role
Labeling. Intelligenza Artificiale, 6(2):163?176.
Michael Grubinger, Paul Clough, Henning Mu?ller, and
Thomas Deselaers. 2006. The IAPR TC-12 Bench-
mark: A New Evaluation Resource for Visual Informa-
tion Systems. In International Workshop OntoImage,
pages 13?23.
Parisa Kordjamshidi, Marie-Francine Moens, and Mar-
tijn van Otterlo. 2010. Spatial Role Labeling: Task
Definition and Annotation Scheme. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation (LREC?10), pages 413?420.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial Role Labeling: To-
wards Extraction of Spatial Relations from Natural
Language. ACM Transactions on Speech and Lan-
guage Processing (TSLP), 8(3):4.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012a. Semeval-2012 Task 3: Spa-
tial Role Labeling. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, pages
365?373. Association for Computational Linguistics.
Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
261
2012b. Relational Learning for Spatial Relation Ex-
traction from Natural Language. In Inductive Logic
Programming, pages 204?220. Springer.
Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-
man, Rob Quimby, Justin Richer, Ben Wellner, Scott
Mardis, and Seamus Clancy. 2010. SpatialML: Anno-
tation Scheme, Resources, and Evaluation. Language
Resources and Evaluation, 44(3):263?280.
James Pustejovsky and Jessica L Moszkowicz. 2011.
The Qualitative Spatial Dynamics of Motion in Lan-
guage. Spatial Cognition & Computation, 11(1):15?
44.
Kirk Roberts and Sanda M Harabagiu. 2012. UTD-
SpRL: A Joint Approach to Spatial Role Labeling. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 419?424. Association for
Computational Linguistics.
Magnus Sahlgren. 2006. The Word-space Model. Ph.D.
thesis, Stockholm University.
Oliviero Stock. 1998. Spatial and Temporal Reasoning.
Springer-Verlag New York Incorporated.
Amber Stubbs. 2011. MAE and MAI: Lightweight An-
notation and Adjudication Tools. In Proceedings of
the 5th Linguistic Annotation Workshop, LAW V ?11,
pages 129?133, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, Yasemin Altun, and Yoram Singer. 2006. Large
Margin Methods for Structured and Interdependent
Output Variables. Journal of Machine Learning Re-
search, 6(2):1453.
262
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 603?607, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CU : Computational Assessment of Short Free Text Answers - A Tool for
Evaluating Students? Understanding
Ifeyinwa Okoye
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
okoye@colorado.edu
Steven Bethard
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
bethard@colorado.edu
Tamara Sumner
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
sumner@colorado.edu
Abstract
Assessing student understanding by evaluat-
ing their free text answers to posed questions
is a very important task. However, manually,
it is time-consuming and computationally, it is
difficult. This paper details our shallow NLP
approach to computationally assessing student
free text answers when a reference answer is
provided. For four out of the five test sets, our
system achieved an overall accuracy above the
median and mean.
1 Introduction
Assessing student understanding is one of the holy
grails of education (Redecker et al, 2012). If we
(teachers, tutors, intelligent tutors, potential employ-
ers, parents and school administrators) know what
and how much a student knows, then we know what
the student still needs to learn. And then, can ef-
ficiently and effectively educate the student. How-
ever, the task of assessing what exactly a student un-
derstands about a particular topic can be expensive,
difficult and subjective.
Using multiple choice questionnaires is one of
the most prevalent forms of assessing student under-
standing because it is easy and fast, both manually
and computationally. However there has been a lot
of pushback from educators about the validity of re-
sults gotten from multiple choice questionnaires.
Assessing student understanding by evaluating
student free text answers either written or spoken is
one of the preferred alternatives to multiple choice
questionnaires. As an assessment tool, free text an-
swers can illuminate what and how much a student
knows since the student is forced to recall terms and
make connections between those terms rather than
just picking one out of several options. However,
assessing free text answers manually is tedious, ex-
pensive and time-consuming, hence the search for a
computational option.
There are three main issues that can limit the com-
putational approach and corresponding performance
when assessing free text answers: (1) the unit of
assessment, (2) the reference and (3) the level of
assessment. The unit of assessment can be words,
facets, phrases, sentences, short answers or essays.
The reference is the correct answer and what is
being compared to the student answer. Most re-
searchers generate the reference manually (Noorbe-
hbahani and Kardan, 2011; Graesser et al, 2004) but
some have focused on automatically generating the
reference (Ahmad, 2009). The level of assessment
can be coarse with 2 categories such as correct and
incorrect or more finer-grained with up to 19 cate-
gories as in (Ahmad, 2009). In general, the finer-
grained assessments are more difficult to assess.
2 The Student Response Analysis Task
The student response analysis task was posed as fol-
lows: Given a question, a known correct/reference
answer and a 1 or 2 sentence student answer, classify
the student answer into two, three or five categories.
The two categories were correct and incorrect; the
three categories were correct, contradictory and in-
correct; while the five categories were correct, par-
tially correct but incomplete, contradictory, irrele-
vant and not in the domain (Dzikovska et al, 2013).
We chose to work on the 2-way response task only
603
because for our application, we need to simply know
if a student answer is correct or incorrect. Our ap-
plication is an interactive essay-based personalized
learning environment (Bethard et al, 2012).
The overarching goal of our application is to cre-
ate a scalable online service that recommends re-
sources to users based on the their conceptual under-
standing expressed in an essay or short answer form.
Our application automatically constructs a domain
knowledge base from digital library resources and
identifies the core concepts in the domain knowl-
edge base.It detects flaws and gaps in users? sci-
ence knowledge and recommends digital library re-
sources to address users? misconceptions and knowl-
edge gaps. The gaps are detected by identifying the
core concepts which the user has not discussed. The
flaws (incorrect understanding/misconceptions) are
currently being identified by a process of (1) seg-
menting a student essay into sentences, (2) align-
ing the student sentence to a sentence in the domain
knowledge base and (3) using the system we devel-
oped for the student response analysis task to deter-
mine if the student sentence is correct or incorrect.
The development of our misconception detection
algorithm has been limited by the alignment task.
However, with the data set from the student response
analysis task containing correct alignments, we hope
to be able to use it to make improvements to our
misconception detection algorithm. We discuss our
current misconception detection system below.
3 System Description
Our system mainly exploits shallow NLP tech-
niques, in particular text overlap, to see how much
we can gain from using a simple system and how
much more some more semantic features could add
to the simple system. Although we have access to
the question which a 1-2 sentence student answer
corresponds to, we chose not to use that in our sys-
tem because in our application we do not have ac-
cess to that information. We were trying to build a
system that would work in our current essay-based
application.
Some of the student answers in the dataset have a
particular reference answer which they match. How-
ever, we do not make use of this information in our
system either. We assume that for a particular ques-
tion, all the corresponding reference answers can be
used to determine the correctness of any of the stu-
dent answers.
3.1 Features
The features we use are:
1. CosineSimilarity : This is the average cosine
similarity (Jurafsky and James, 2000) between
a student answer vector and all the correspond-
ing reference answer vectors. The vectors are
based on word counts. The words were low-
ercased and included stopwords and punctua-
tions.
2. CosineSimilarityNormalized : This is the av-
erage cosine similarity between a student an-
swer vector and all the corresponding reference
answer vectors, with the word counts within
the vectors divided by the word counts in Gi-
gaword, a background corpus. We divided
the raw counts by the counts in Gigaword to
ensure that punctuations, stopwords and other
non-discriminatory words do not artificially in-
crease the cosine similarity.
3. UnigramRefStudent : This is the average un-
igram coverage of the reference answers by a
student answer. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number of
unigrams in the reference answer that are con-
tained in the student answer and divide it by the
number of unigrams in the reference answer.
The value we get for this feature, is the aver-
age over all the reference answers.
4. UnigramStudentRef : This is the average uni-
gram coverage of the student answer by the ref-
erence answers. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number
of unigrams in the student answer that are con-
tained in the reference answer and divide it by
the number of unigrams in the student answer.
The value we get for this feature, is the average
over all the reference answers.
604
5. BigramRefStudent : This is similar to the Un-
igramRefStudent feature, but using bigrams.
6. BigramStudentRef : This is similar to the Un-
igramStudentRef feature, but using bigrams.
7. LemmaRefStudent : This is similar to the Un-
igramRefStudent feature, but in this case, the
lemmas are used in place of words.
8. LemmaStudentRef : This is similar to the Un-
igramStudentRef feature, but in this case, the
lemmas are used in place of words.
9. UnigramPosRefStudent : This is similar to
the UnigramRefStudent feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
10. UnigramPosStudentRef : This is similar to
the UnigramStudentRef feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
11. BigramPosRefStudent : This is similar to the
BigramRefStudent feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
12. BigramPosStudentRef : This is similar to the
BigramStudentRef feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
3.2 Implementation
We used the ClearTK (Ogren et al, 2008) toolkit
within Eclipse to extract features from the student
and reference sentences. We trained a LibSVM
(Chang and Lin, 2011) binary classifier to classify a
feature vector into two classes, correct or incorrect.
We used the default parameters for LibSVM except
for the cost parameter, for which we tried different
values. However, the default value of 1 gave us the
best result on the training set. Our two runs/systems
are essentially the same system but with a cost pa-
rameter of 1 and 10.
4 Results
The Student Response Analysis Task overall re-
sult can be found in the Task description paper
(Dzikovska et al, 2013). The CU system achieved
a ranking of above the mean and median for four
of the five different test sets. We perfomed below
the mean and median on the sciEntsBank unseen an-
swers. The accuracy result for the test data is shown
in Table 4. The results on our training data and
a breakdown of the contribution of each feature is
shown in Table 5. In Table 5 ALL refers to all the
features while ALL-CosineSimilarity is all the fea-
tures excluding the CosineSimilarity feature.
Sys
tem
beetle
un-
seen
an-
swers
beetle
un-
seen
ques-
tions
sciEnts
Bank
un-
seen
an-
swers
sciEnts
Bank
un-
seen
ques-
tions
sciEnts
Bank
un-
seen
do-
mains
CU
run
1
0.786 0.718 0.656 0.674 0.693
CU
run
2
0.784 0.717 0.654 0.671 0.691
Table 1: Overall Accuracy results for CU system on the
test Data
5 Discussion
As can be seen from Table 4 and further elaborated
on in (Dzikovska et al, 2013), there were two main
datasets, Beetle and SciEntsBank. The Beetle data
set has multiple reference answer per question while
the SciEntsBank has one reference answer per ques-
tion. Our system did better on the beetle data set
than the SciEntsBank data set, both during devel-
opment and on the final test sets. This leads us to
believe that our system will do well when there are
multiple reference answers rather than just one.
We analyzed the training data to understand
where our system was failing and what we could do
to make it better. We tried removing stopwords be-
fore constructing the feature vectors but that made
the results worse. Here are two examples where re-
moving the stopwords will make it impossible to as-
certain the validity of the student answer:
? It was connected. becomes connected
605
? It will work because that is closing the switch.
becomes work closing switch
Because the student answers are free text and use
pronouns in place of the nouns that were in the ques-
tion, the stop words are important to provide context.
Feature Type Beetle
& sci-
Ents
Bank
1 ALL 0.703
2 ALL - CosineSimilarity 0.702
3 ALL - CosineSimilari-
tyNormalized
0.700
4 ALL - UnigramRefStudent 0.702
5 ALL - UnigramStudentRef 0.701
6 ALL - BigramRefStudent 0.702
7 ALL - BigramStudentRef 0.699
8 ALL - LemmaRefStudent 0.701
9 ALL - LemmaStudentRef 0.700
10 ALL - UnigramPosRefStu-
dent
0.703
11 ALL - UnigramPosStuden-
tRef
0.703
12 ALL - BigramPosRefStu-
dent
0.702
13 ALL - BigramPosStuden-
tRef
0.702
Table 2: Accuracy results for 5X cross validation on the
training data
Currently, we are working on extracting and
adding several features that we did not use for the
task due to time constraints, to see if they improve
our result. Some of the things we are working on
are:
1. Resolving Coreference
We will use the current state-of-art coreference
system and assume that the question precedes
the student answer in a paragraph when resolv-
ing coreference.
2. Compare main predicates
The question is how to assign a value to the se-
mantic similarity between the main predicates.
If the predicates are separate and connect, then
there should be a way to indicate that the men-
tion of one of them in the reference, precludes
the validity of the student answer being correct
if it mentions the other. However, we also have
to take negation into account here. not sepa-
rated and connected should be marked as very
similar if not equal. We plan to include the al-
gorithm from the best system in the semantic
similarity task to our current system.
3. Compare main subject and object from a
syntactic parse or the numbered arguments
in semantic role label arguments
We have to resolve coreference for this to work
well. And again, we run into the problem of
how to assign a semantic similarity value to two
words that might not share the same synset in
ontologies such as Wordnet.
4. Optimize parameters and explore other clas-
sifiers Throughout developing and testing our
system, we used only the LibSVM classifier
and only optimized the cost parameter. How-
ever, there might be a different classifier or
set of options that can model the data better.
We hope to run through most of the classifiers
available and see if using a different one, with
different options improves our accuracy.
6 Conclusion
We have shown that there is value in using shallow
NLP features to judge the validity of free answer text
when the reference answers are given. However,
looking at the sentences that our system labeled as
correct and the gold standard incorrect or vice versa,
it is clear that we have to delve into more seman-
tic features if we want our system to be more accu-
rate. We hope to keep working on this task in sub-
sequent years to ensure continuous improvements in
systems that can assess student knowledge by eval-
uating free answer texts. Such systems will be able
to give students the formative feedback they need
to help them learn better. In addition, such systems
will provide teachers, intelligent tutors and adminis-
trators with feedback about student knowledge, so as
to help them adapt their curriculum, teaching and tu-
toring methods to better serve students? knowledge
needs.
606
References
Faisal Ahmad. 2009. Generating conceptually personal-
ized interactions for educational digital libraries using
concept maps. Ph.D. thesis, University of Colorado at
Boulder.
Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H
Martin, Md Arafat Sultan, and Tamara Sumner. 2012.
Identifying science concepts and student misconcep-
tions in an interactive essay writing tutor. In Proceed-
ings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 12?21. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Arthur Graesser, Shulan Lu, George Jackson, Heather
Mitchell, Mathew Ventura, Andrew Olney, and Max
Louwerse. 2004. AutoTutor: A tutor with dialogue
in natural language. Behavior Research Methods,
36:180?192.
Daniel Jurafsky and H James. 2000. Speech and lan-
guage processing an introduction to natural language
processing, computational linguistics, and speech.
F Noorbehbahani and AA Kardan. 2011. The automatic
assessment of free text answers using a modified bleu
algorithm. Computers & Education, 56(2):337?345.
Philip V Ogren, Philipp G Wetzler, and Steven J Bethard.
2008. Cleartk: A uima toolkit for statistical natu-
ral language processing. Towards Enhanced Inter-
operability for Large HLT Systems: UIMA for NLP,
page 32.
Christine Redecker, Yves Punie, and Anusca Ferrari.
2012. eassessment for 21st century learning and skills.
In 21st Century Learning for 21st Century Skills,
pages 292?305. Springer.
607
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241?246,
Dublin, Ireland, August 23-24, 2014.
DLS@CU: Sentence Similarity from Word Alignment
Md Arafat Sultan
?
, Steven Bethard
?
, Tamara Sumner
?
?
Institute of Cognitive Science and Department of Computer Science
University of Colorado Boulder
?
Department of Computer and Information Sciences
University of Alabama at Birmingham
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
Abstract
We present an algorithm for computing
the semantic similarity between two sen-
tences. It adopts the hypothesis that se-
mantic similarity is a monotonically in-
creasing function of the degree to which
(1) the two sentences contain similar se-
mantic units, and (2) such units occur in
similar semantic contexts. With a simplis-
tic operationalization of the notion of se-
mantic units with individual words, we ex-
perimentally show that this hypothesis can
lead to state-of-the-art results for sentence-
level semantic similarity. At the Sem-
Eval 2014 STS task (task 10), our system
demonstrated the best performance (mea-
sured by correlation with human annota-
tions) among 38 system runs.
1 Introduction
Semantic textual similarity (STS), in the context
of short text fragments, has drawn considerable
attention in recent times. Its application spans a
multitude of areas, including natural language pro-
cessing, information retrieval and digital learning.
Examples of tasks that benefit from STS include
text summarization, machine translation, question
answering, short answer scoring, and so on.
The annual series of SemEval STS tasks (Agirre
et al., 2012; Agirre et al., 2013; Agirre et al., 2014)
is an important platform where STS systems are
evaluated on common data and evaluation criteria.
In this article, we describe an STS system which
participated and outperformed all other systems at
SemEval 2014.
The algorithm is a straightforward application
of the monolingual word aligner presented in (Sul-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tan et al., 2014). This aligner aligns related words
in two sentences based on the following properties
of the words:
1. They are semantically similar.
2. They occur in similar semantic contexts in
the respective sentences.
The output of the word aligner for a sentence
pair can be used to predict the pair?s semantic sim-
ilarity by taking the proportion of their aligned
content words. Intuitively, the more semantic
components in the sentences we can meaningfully
align, the higher their semantic similarity should
be. In experiments on STS 2013 data reported
by Sultan et al. (2014), this approach was found
highly effective. We also adopt this hypothesis of
semantic compositionality for STS 2014.
We implement an STS algorithm that is only
slightly different from the algorithm in (Sultan et
al., 2014). The approach remains equally success-
ful on STS 2014 data.
2 Background
We focus on two relevant topics in this section:
the state of the art of STS research, and the word
aligner presented in (Sultan et al., 2014).
2.1 Semantic Textual Similarity
Since the inception of textual similarity research
for short text, perhaps with the studies reported
by Mihalcea et al. (2006) and Li et al. (2006),
the topic has spawned significant research inter-
est. The majority of systems have been reported
as part of the SemEval 2012 and *SEM 2013 STS
tasks (Agirre et al., 2012; Agirre et al., 2013).
Here we confine our discussion to systems that
participated in these tasks.
With designated training data for several test
sets, supervised systems were the most successful
in STS 2012 (B?ar et al., 2012;
?
Sari?c et al., 2012;
241
Jimenez et al., 2012). Such systems typically ap-
ply a regression algorithm on a large number of
STS features (e.g., string similarity, syntactic sim-
ilarity and word or phrase-level semantic similar-
ity) to generate a final similarity score. This ap-
proach continued to do well in 2013 (Han et al.,
2013; Wu et al., 2013; Shareghi and Bergler, 2013)
even without domain-specific training data, but the
best results were demonstrated by an unsupervised
system (Han et al., 2013). This has important im-
plications for STS since extraction of each feature
adds to the latency of a supervised system. STS
systems are typically important in the context of
a larger system rather than on their own, so high
latency is an obvious drawback for such systems.
We present an STS system that has simplicity,
high accuracy and speed as its design goals, can
be deployed without any supervision, operates in
a linguistically principled manner with purely se-
mantic sentence properties, and was the top sys-
tem at SemEval STS 2014.
2.2 The Sultan et al. (2014) Aligner
The word aligner presented in (Sultan et al., 2014)
has been used unchanged in this work and plays a
central role in our STS algorithm. We give only an
overview here; for the full details, see the original
article.
We will denote the sentences being aligned (and
are subsequently input to the STS algorithm) as
S
(1)
and S
(2)
. We describe only content word
alignment here; stop words are not used in our STS
computation.
The aligner first identifies word pairs w
(1)
i
?
S
(1)
and w
(2)
j
? S
(2)
such that:
1. w
(1)
i
and w
(2)
j
have non-zero semantic simi-
larity, sim
Wij
. The calculation of sim
Wij
is
described in Section 2.2.1.
2. The semantic contexts of w
(1)
i
and w
(2)
j
have
some similarity, sim
Cij
. We define the se-
mantic context of a word w in a sentence
S as a set of words in S, and the seman-
tic context of the word pair (w
(1)
i
, w
(2)
j
), de-
noted by context
ij
, as the Cartesian product
of the context of w
(1)
i
in S
(1)
and the con-
text of w
(2)
j
in S
(2)
. We define several types
of context (i.e., several selections of words)
and describe the corresponding calculations
of sim
Cij
in Section 2.2.2.
3. There are no competing pairs scoring higher
Align
identical
word
sequences
Align
named
entities
Align
content
words
using
depen-
dencies
Align
content
words
using sur-
rounding
words
Figure 1: The alignment pipeline.
than (w
(1)
i
, w
(2)
j
) under f(sim
W
, sim
C
) =
0.9 ? sim
W
+ 0.1 ? sim
C
. That is,
there are no pairs (w
(1)
k
, w
(2)
j
) such that
f(sim
Wkj
, sim
Ckj
) > f(sim
Wij
, sim
Cij
),
and there are no pairs (w
(1)
i
, w
(2)
l
) such that
f(sim
Wil
, sim
Cil
) > f(sim
Wij
, sim
Cij
).
The weights 0.9 and 0.1 were derived empiri-
cally via a grid search in the range [0, 1] (with
a step size of 0.1) to maximize alignment per-
formance on the training set of the (Brockett,
2007) alignment corpus. This set contains
800 human-aligned sentence pairs collected
from a textual entailment corpus (Bar-Haim
et al., 2006).
The aligner then performs one-to-one word align-
ments in decreasing order of the f value.
This alignment process is applied in four steps
as shown in Figure 1; each step applies the above
process to a particular type of context: identi-
cal words, dependencies and surrounding content
words. Additionally, named entities are aligned in
a separate step (details in Section 2.2.2).
Words that are aligned by an earlier module of
the pipeline are not allowed to be re-aligned by
downstream modules.
2.2.1 Word Similarity
Word similarity (sim
W
) is computed as follows:
1. If the two words or their lemmas are identi-
cal, then sim
W
= 1.
2. If the two words are present as a pair
in the lexical XXXL corpus of the Para-
phrase Database
1
(PPDB) (Ganitkevitch et
al., 2013), then sim
W
= 0.9.
2
For this
step, PPDB was augmented with lemmatized
forms of the already existing word pairs.
3
1
PPDB is a large database of lexical, phrasal and syntactic
paraphrases.
2
Again, the value 0.9 was derived empirically via a grid
search in [0, 1] (step size = 0.1) to maximize alignment per-
formance on the (Brockett, 2007) training data.
3
The Python NLTK WordNetLemmatizer was used to
lemmatize the original PPDB words.
242
3. For any other word pair, sim
W
= 0.
2.2.2 Contextual Similarity
Contextual similarity (sim
C
) for a word pair
(w
(1)
i
, w
(2)
j
) is computed as the sum of the word
similarities for each pair of words in the context of
(w
(1)
i
, w
(2)
j
). That is:
sim
Cij
=
?
(w
(1)
k
,w
(2)
l
) ? context
ij
sim
Wkl
Each of the stages in Figure 1 employs a specific
type of context.
Identical Word Sequences. Contextual sim-
ilarity for identical word sequences (a word se-
quence W which is present in both S
(1)
and S
(2)
and contains at least one content word) defines the
context by pairing up each word in the instance of
W in S
(1)
with its occurrence in the instance of
W in S
(2)
. All such sequences with length ? 2
are aligned; longer sequences are aligned before
shorter ones. This simple step was found to be of
very high precision in (Sultan et al., 2014) and re-
duces the overall computational cost of alignment.
Named Entities. Named entities are a special
case in the alignment pipeline. Even though the
context for a named entity is defined in the same
way as it is defined for any other content word
(as described below), named entities are aligned
in a separate step before other content words be-
cause they have special properties such as corefer-
ring mentions of different lengths (e.g. Smith and
John Smith, BBC and British Broadcasting Cor-
poration). The head word of the named entity is
used in dependency calculations.
Dependencies. Dependency-based contex-
tual similarity defines the context for the pair
(w
(1)
i
, w
(2)
j
) using the syntactic dependencies of
w
(1)
i
and w
(2)
j
. The context is the set of all word
pairs (w
(1)
k
, w
(2)
l
) such that:
? w
(1)
k
is a dependency of w
(1)
i
,
? w
(2)
l
is a dependency of w
(2)
j
,
? w
(1)
i
and w
(2)
j
have the same lexical category,
? w
(1)
k
and w
(2)
l
have the same lexical category,
and,
? The two dependencies are either identical or
semantically ?equivalent? according to the
equivalence table provided by Sultan et al.
S
(1)
: He wrote a book .
nsubj
dobj
det
S
(2)
: I read the book he wrote .
nsubj
dobj
det
rcmod
nsubj
Figure 2: Example of dependency equivalence.
(2014). We explain semantic equivalence of
dependencies using an example below.
Equivalence of Dependency Structures. Con-
sider S
(1)
and S
(2)
in Figure 2. Note that w
(1)
2
=
w
(2)
6
= ?wrote? and w
(1)
4
= w
(2)
4
= ?book? in
this pair. Now, each of the two following typed
dependencies: dobj(w
(1)
2
, w
(1)
4
) in S
(1)
and rc-
mod(w
(2)
4
, w
(2)
6
) in S
(2)
, represents the relation
?thing that was written? between the verb ?wrote?
and its argument ?book?. Thus, to summarize,
an instance of contextual evidence for a possible
alignment between the pair (w
(1)
2
, w
(2)
6
) (?wrote?)
lies in the pair (w
(1)
4
, w
(2)
4
) (?book?) and the equiv-
alence of the two dependency types dobj and rc-
mod.
The equivalence table of Sultan et al. (2014) is
a list of all such possible equivalences among dif-
ferent dependency types (given that w
(1)
i
has the
same lexical category as w
(2)
j
and w
(1)
k
has the
same lexical category as w
(2)
l
).
If there are no word pairs with identical or
equivalent dependencies as defined above, i.e. if
sim
Cij
= 0, then w
(1)
i
and w
(2)
j
will not be
aligned by this module.
Surrounding Content Words. Surrounding-
word-based contextual similarity defines the con-
text of a word in a sentence as a fixed window of
3 words to its left and 3 words to its right. Only
content words in the window are considered. (As
explained in the beginning of this section, the con-
text of the pair (w
(1)
i
, w
(2)
j
) is then the Cartesian
product of the context of w
(1)
i
in S
(1)
and w
(2)
j
in
S
(2)
.) Note that w
(1)
i
and w
(2)
j
can be of different
lexical categories here.
A content word can often be surrounded by
stop words which provide almost no information
about its semantic context. The chosen window
size is assumed, on average, to effectively make
243
Data Set Source of Text # of Pairs
deft-forum discussion forums 450
deft-news news articles 300
headlines news headlines 750
images image descriptions 750
OnWN word sense definitions 750
tweet-news news articles and tweets 750
Table 1: Test sets for SemEval STS 2014.
sufficient contextual information available while
avoiding the inclusion of contextually unrelated
words. But further experiments are necessary to
determine the best span in the context of align-
ment.
Unlike dependency-based alignment, even if
there are no similar words in the context, i.e. if
sim
Cij
= 0, w
(1)
i
may still be aligned to w
(2)
j
if
sim
Wij
> 0 and no alignments for w
(1)
i
or w
(2)
j
have been found by earlier stages of the pipeline.
2.2.3 The Alignment Sequence
The rationale behind the specific sequence of
alignment steps (Figure 1) was explained in (Sul-
tan et al., 2014): (1) Identical word sequence
alignment was found to be the step with the
highest precision in experiments on the (Brock-
ett, 2007) training data, (2) It is convenient to
align named entities before other content words
to enable alignment of entity mentions of differ-
ent lengths, (3) Dependency-based evidence was
observed to be more reliable (i.e. of higher preci-
sion) than textual evidence on the (Brockett, 2007)
training data.
3 Method
Our STS score is a function of the proportions of
aligned content words in the two input sentences.
The proportion of content words in S
(1)
that are
aligned to some word in S
(2)
is:
prop
(1)
Al
=
|{i : [?j : (i, j) ? Al] and w
(1)
i
? C}|
|{i : w
(1)
i
? C}|
where C is the set of all content words in En-
glish and Al are the predicted word alignments. A
word alignment is a pair of indices (i, j) indicating
that word w
(1)
i
is aligned to w
(2)
j
. The proportion
of aligned content words in S
(2)
, prop
(2)
Al
, can be
computed in a similar way.
We posit that a simple yet sensible way to obtain
an STS estimate for S
(1)
and S
(2)
is to take a mean
Data Set Run 1 Run 2
deft-forum 0.4828 0.4828
deft-news 0.7657 0.7657
headlines 0.7646 0.7646
images 0.8214 0.8214
OnWN 0.7227 0.8589
tweet-news 0.7639 0.7639
Weighted Mean 0.7337 0.7610
Table 2: Results of evaluation on SemEval STS
2014 data. Each value on columns 2 and 3 is the
correlation between system output and human an-
notations for the corresponding data set. The last
row shows the value of the final evaluation metric.
of prop
(1)
Al
and prop
(2)
Al
. Our two submitted runs
use the harmonic mean:
sim(S
(1)
, S
(2)
) =
2? prop
(1)
Al
? prop
(2)
Al
prop
(1)
Al
+ prop
(2)
Al
It is a more conservative estimate than the arith-
metic mean, and penalizes sentence pairs with a
large disparity between the values of prop
(1)
Al
and
prop
(2)
Al
. Experiments on STS 2012 and 2013 data
revealed the harmonic mean of the two propor-
tions to be a better STS estimate than the arith-
metic mean.
4 Data
STS systems at SemEval 2014 were evaluated on
six data sets. Each test set consists of a number
of sentence pairs; each pair has a human-assigned
similarity score in the range [0, 5] which increases
with similarity. Every score is the mean of five
scores crowdsourced using the Amazon Mechan-
ical Turk. The sentences were collected from a
variety of sources. In Table 1, we provide a brief
description of each test set.
5 Evaluation
We submitted the results of two system runs at
SemEval 2014 based on the idea presented in Sec-
tion 3. The two runs were identical, except for the
fact that for the OnWN test set, we specified the
following words as additional stop words during
run 2 (but not during run 1): something, someone,
somebody, act, activity, some, state.
4
For both
4
OnWN has many sentence pairs where each sentence
is of the form ?the act/activity/state of verb+ing some-
thing/somebody?. The selected words act merely as fillers
in such pairs and consequently do not typically contribute to
the similarity scores.
244
Data Set Run 1 Run 2
FNWN 0.4686 0.4686
headlines 0.7797 0.7797
OnWN 0.6083 0.8197
SMT 0.3837 0.3837
Weighted Mean 0.5788 0.6315
Table 3: Results of evaluation on *SEM STS 2013
data.
runs, the tweet-news sentences were preprocessed
by separating the hashtag from the word for each
hashtagged word.
Table 2 shows the performance of each run.
Rows 1 through 6 show the Pearson correlation
coefficients between the system scores and human
annotations for all test sets. The last row shows
the value of the final evaluation metric, which is a
weighted sum of all correlations in rows 1?6. The
weight assigned to a data set is proportional to its
number of pairs. Our run 1 ranked 7th and run 2
ranked 1st among 38 submitted system runs.
An important implication of these results is the
fact that knowledge of domain-specific stop words
can be beneficial for an STS system. Even though
we imparted this knowledge to our system during
run 2 via a manually constructed set of additional
stop words, simple measures like TF-IDF can be
used to automate the process.
5.1 Performance on STS 2012 and 2013 Data
We applied our algorithm on the 2012 and 2013
STS test sets to examine its general utility. Note
that the STS 2013 setup was similar to STS 2014
with no domain-dependent training data, whereas
several of the 2012 test sets had designated train-
ing data.
Over all the 2013 test sets, our two runs demon-
strated weighted correlations of 0.5788 (rank: 4)
and 0.6315 (rank: 1), respectively. Table 3 shows
performances on individual test sets. (Descrip-
tions of the test sets can be found in (Agirre et
al., 2013).) Again, run 2 outperformed run 1 on
OnWN by a large margin.
On the 2012 test sets, however, the performance
was worse (relative to other systems), with respec-
tive weighted correlations of 0.6476 (rank: 8) and
0.6423 (rank: 9). Table 4 shows performances on
individual test sets. (Descriptions of the test sets
can be found in (Agirre et al., 2012).)
This performance drop seems to be an obvious
consequence of the fact that our algorithm was
not trained on domain-specific data: on STS 2013
Data Set Run 1 Run 2
MSRpar 0.6413 0.6413
MSRvid 0.8200 0.8200
OnWN 0.7227 0.7004
SMTeuroparl 0.4267 0.4267
SMTnews 0.4486 0.4486
Weighted Mean 0.6476 0.6423
Table 4: Results of evaluation on SemEval STS
2012 data.
data, the top two STS 2012 systems, with respec-
tive weighted correlations of 0.5652 and 0.5221
(Agirre et al., 2013), were outperformed by our
system by a large margin.
In contrast to the other two years, our run 1
outperformed run 2 on the 2012 OnWN test set
by a very small margin. A closer inspection
revealed that the previously mentioned sentence
structure ?the act/activity/state of verb+ing some-
thing/somebody? is much less common in this set,
and as a result, our additional stop words tend to
play more salient semantic roles in this set than in
the other two OnWN sets (i.e. they act relatively
more as content words than stop words). The drop
in correlation with human annotations is a con-
sequence of this role reversal. This result again
shows the importance of a proper selection of stop
words for STS and also points to the challenges
associated with making such a selection.
6 Conclusions and Future Work
We show that alignment of related words in two
sentences, if carried out in a principled and accu-
rate manner, can yield state-of-the-art results for
sentence-level semantic similarity. Our system has
the desired quality of being both accurate and fast.
Evaluation on test data from different STS years
demonstrates its general applicability as well.
The idea of STS from alignment is worth inves-
tigating with larger semantic units (i.e. phrases)
in the two sentences. Another possible research
direction is to investigate whether the alignment
proportions observed for the two sentences can be
used as features to improve performance in a su-
pervised setup (even though this scenario is ar-
guably less common in practice because of un-
availability of domain or situation-specific train-
ing data).
Acknowledgments
This material is based in part upon work supported
by the National Science Foundation under Grant
245
Numbers EHR/0835393 and EHR/0835381. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views
of the National Science Foundation.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, Volume 2: Proceedings
of the Sixth International Workshop on Semantic
Evaluation, SemEval ?12, pages 385-393, Montreal,
Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics, *SEM ?13, pages 32-43, Atlanta,
Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The Second PASCAL Recognising
Textual Entailment Challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation, held in
conjunction with the 1st Joint Conference on Lexical
and Computational Semantics, SemEval ?12, pages
435-440, Montreal, Canada.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL-HLT ?13,
pages 758-764, Atlanta, Georgia, USA.
Lushan Han, Abhay Kashyap, Tim Finin, James
Mayfield, and Jonathan Weese. 2013. UMBC
EBIQUITY-CORE: Semantic Textual Similarity
Systems. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics,
*SEM ?13, pages 44-52, Atlanta, Georgia, USA.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality: a parameterized sim-
ilarity function for text comparison. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation, held in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
SemEval ?12, pages 449-453, Montreal, Canada.
Yuhua Li, David Mclean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engi-
neering, vol.18, no.8. 1138-1150.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st national conference on Artificial in-
telligence, AAAI ?06, pages 775-780, Boston, Mas-
sachusetts, USA.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. TakeLab: sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation, held in conjunction with the 1st
Joint Conference on Lexical and Computational Se-
mantics, SemEval ?12, pages 441-448, Montreal,
Canada.
Ehsan Shareghi and Sabine Bergler. 2013. CLaC-
CORE: Exhaustive Feature Combination for Mea-
suring Textual Similarity. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics, *SEM ?13, pages 202-206, At-
lanta, Georgia, USA.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to Basics for Monolingual Align-
ment: Exploiting Word Similarity and Contextual
Evidence. Transactions of the Association for Com-
putational Linguistics, 2 (May), pages 219-230.
Stephen Wu, Dongqing Zhu, Ben Carterette, and Hong-
fang Liu. 2013. MayoClinicNLP-CORE: Semantic
representations for textual similarity. In Proceed-
ings of the Second Joint Conference on Lexical and
Computational Semantics, *SEM ?13, pages 148-
154, Atlanta, Georgia, USA.
246
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 122?130,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing and language studies: the new generation of linguistic data
Robert Munroa Steven Bethardb Victor Kupermana Vicky Tzuyin Laic
Robin Melnicka Christopher Pottsa Tyler Schnoebelena Harry Tilya
aDepartment of Linguistics, Stanford University
bDepartment of Computer Science, Stanford University
cDepartment of Linguistics, University of Colorado
{rmunro,bethard,vickup,rmelnick,cgpotts,tylers,hjt}
@stanford.edu
vicky.lai@colorado.edu
Abstract
We present a compendium of recent and cur-
rent projects that utilize crowdsourcing tech-
nologies for language studies, finding that the
quality is comparable to controlled labora-
tory experiments, and in some cases superior.
While crowdsourcing has primarily been used
for annotation in recent language studies, the
results here demonstrate that far richer data
may be generated in a range of linguistic dis-
ciplines from semantics to psycholinguistics.
For these, we report a number of successful
methods for evaluating data quality in the ab-
sence of a ?correct? response for any given
data point.
1 Introduction
Crowdsourcing?s greatest contribution to language
studies might be the ability to generate new kinds
of data, especially within experimental paradigms.
The speed and cost benefits for annotation are cer-
tainly impressive (Snow et al, 2008; Callison-
Burch, 2009; Hsueh et al, 2009) but we hope to
show that some of the greatest gains are in the very
nature of the phenomena that we can now study.
For psycholinguistic experiments in particular, we
are not so much utilizing ?artificial artificial? intelli-
gence as the plain intelligence and linguistic intu-
itions of each crowdsourced worker ? the ?voices
in the crowd?, so to speak. In many experiments
we are studying gradient phenomena where there
are no right answers. Even when there is binary
response we are often interested in the distribution
of responses over many speakers rather than spe-
cific data points. This differentiates experimentation
from more common means of determining the qual-
ity of crowdsourced results as there is no gold stan-
dard against which to evaluate the quality or ?cor-
rectness? of each individual response.
The purpose of this paper is therefore two-fold.
We summarize seven current projects that are utiliz-
ing crowdsourcing technologies, all of them some-
what novel to the NLP community but with potential
for future research in computational linguistics. For
each, we also discuss methods for evaluating quality,
finding the crowdsourced results to often be indistin-
guishable from controlled laboratory experiments.
In Section 2 we present the results from seman-
tic transparency experiments showing near-perfect
interworker reliability and a strong correlation be-
tween crowdsourced data and lab results. Ex-
tending to audio data, we show in Section 3
that crowdsourced subjects were statistically in-
distinguishable from a lab control group in seg-
mentation tasks. Section 4 shows that labora-
tory results from simple Cloze tasks can be repro-
duced with crowdsourcing. In Section 5 we offer
strong evidence that crowdsourcing can also repli-
cate limited-population, controlled-condition lab re-
sults for grammaticality judgments. In Section 6 we
use crowdsourcing to support corpus studies with a
precision not possible with even very large corpora.
Moving to the brain itself, Section 7 demonstrates
that ERP brainwave analysis can be enhanced by
crowdsourced analysis of experimental stimuli. Fi-
nally, in Section 8 we outline simple heuristics for
ensuring that microtasking workers are applying the
linguistic attentiveness required to undertake more
complex tasks.
122
2 Transparency of phrasal verbs
Phrasal verbs are those verbs that spread their mean-
ing out across both a verb and a particle, as in ?lift
up?. Semantic transparency is a measure of how
strongly the phrasal verb entails the component verb.
For example, to what extent does ?lifting up? entail
?lifting?? We can see the variation between phrasal
verbs when we compare the transparency of ?lift up?
to the opacity of ?give up?.
We conducted five experiments around seman-
tic transparency, with results showing that crowd-
sourced results correlate well with each other and
against lab data (? up to 0.9). Interrater reliability is
also very high: ? = 0.823, which Landis and Koch
(1977) would call ?almost perfect agreement.?
The crowdsourced results reported here represent
judgments by 215 people. Two experiments were
performed using Stanford University undergradu-
ates. The first involved a questionnaire asking par-
ticipants to rate the semantic transparency of 96
phrasal verbs. The second experiment consisted of
a paper questionnaire with the phrasal verbs in con-
text. That is, the first group of ?StudentLong? par-
ticipants rated the similarity of ?cool? to ?cool down?
on a scale 1-7:
cool cool down
The ?StudentContext? participants performed the
same basic task but saw each verb/phrasal verb pair
with an example of the phrasal verb in context.
With Mechanical Turk, we had three conditions:
TurkLong: A replication of the first questionnaire
and its 96 questions.
TurkShort: The 96-questions were randomized into
batches of 6. Thus, some participants ended up giv-
ing responses to all phrasal verbs, while others only
gave 6, 12, 18, etc responses.
TurkContext: A variation of the ?StudentContext?
task ? participants were given examples of the
phrasal verbs, though as with ?TurkShort?, they were
only asked to rate 6 phrasal verbs at a time.
What we find is a split into relatively high and low
correlations, as Figure 1 shows. All Mechanical
Turk tests correlate very well with one another (all
? > 0.7), although the tasks and raters are differ-
ent. The correlation between the student participants
who were given sentence contexts and the workers
TurkLong
2 3 4 5 6 2 3 4 5 6
2.0
3.5
5.0
2
3
4
5
6
r = 0.92
p = 0
rs = 0.92
p = 0
TurkShort
r = 0.74
p = 0
rs = 0.73
p = 0
r = 0.77
p = 0
rs = 0.75
p = 0
TurkContext
3
4
5
6
2
3
4
5
6 r = 0.68
p = 0
rs = 0.67
p = 0
r = 0.7
p = 0
rs = 0.67
p = 0
r = 0.9
p = 0
rs = 0.9
p = 0
StudentContext
2.0 3.5 5.0
r = 0.46
p = 0
rs = 0.46
p = 0
r = 0.48
p = 0
rs = 0.48
p = 0
3 4 5 6
r = 0.46
p = 0
rs = 0.45
p = 0
r = 0.41
p = 0
rs = 0.44
p = 0
2.5 3.5 4.5 5.5
2.5
3.5
4.5
5.5StudentLong
Figure 1: Panels at the diagonal report histograms of dis-
tributions of ratings across populations of participants;
panels above the diagonal plot the locally weighted scat-
terplot smoothing Lowess functions for a pair of corre-
lated variables; panels below the diagonal report correla-
tion coefficients (the r value is Pearson?s r, the rs value
is Spearman?s ?) and respective ? values.
who saw context is especially high (0.9). All corre-
lations with StudentLong are relatively low, but this
is actually true for StudentLong vs. StudentContext,
too (? = 0.44), even though both groups are Stan-
ford undergraduates.
Intra-class correlation coefficients (ICC) measure
the agreement among participants, and these are
high for all groups except StudentLong. Just among
StudentLong participants, the ICC consistency is
only 0.0934 and their ICC agreement is 0.0854.
Once we drop StudentLong, we see that all of the
remaining tests have high consistency (average of
0.78 for ICC consistency, 0.74 for ICC agreement).
For example, if we combine TurkContext and Stu-
dentContext, ICC consistency is 0.899 and ICC
agreement of 0.900. Cohen?s kappa measurement
also measures how well raters agree, weeding out
chance agreements. Again, StudentLong is an out-
lier. Together, TurkContext / StudentContext gets a
weighted kappa score of 0.823 ? the overall average
(excepting StudentLong) is ? = 0.700.
More details about the results in this section can
be found in Schnoebelen and Kuperman (submit-
ted).
123
3 Segmentation of an audio speech stream
The ability of browsers to present multimedia re-
sources makes it feasible to use crowdsourcing tech-
niques to generate data using spoken as well as writ-
ten stimuli. In this section we report an MTurk repli-
cation of a classic psycholinguistic result that relies
on audio presentation of speech. We developed a
web-based interface that allows us to collect data
in a statistical word segmentation paradigm. The
core is a Flash applet developed using Adobe Flex
which presents audio stimuli and collects participant
responses (Frank et al, submitted).
Human children possess a remarkable ability to
learn the words and structures of languages they are
exposed to without explicit instruction. One partic-
ularly remarkable aspect is that unlike many written
languages, spoken language lacks spaces between
words: from spoken input, children learn not only
the mapping between meanings and words but also
what the words themselves are, with no direct infor-
mation about where one ends and the next begins.
Research in statistical word segmentation has shown
that both infants and adults use statistical properties
of speech in an unknown language to infer a proba-
ble vocabulary. In one classic study, Saffran, New-
port & Aslin (1996) showed that after a few minutes
of exposure to a language made by randomly con-
catenating copies of invented words, adult partici-
pants could discriminate those words from syllable
sequences that also occurred in the input but crossed
a word boundary. We replicated this study showing
that cheap and readily accessible data from crowd-
sourced workers compares well to data from partic-
ipants recorded in person in the lab.
Participants heard 75 sentences from one of 16 ar-
tificially constructed languages. Each language con-
tained 2 two-syllable, 2 three-syllable, and 2 four
syllable words, with syllables drawn from a possi-
ble set of 18. Each sentence consisted of four words
sampled without replacement from this set and con-
catenated. Sentences were rendered as audio by
the MBROLA synthesizer (Dutoit et al, 1996) at a
constant pitch of 100Hz with 25ms consonants and
225ms vowels. Between each sentence, participants
were required to click a ?next? button to continue,
preventing workers from leaving their computer dur-
ing this training phase. To ensure workers could ac-
Figure 2: Per-subject correct responses for lab and MTurk
participants. Bars show group means, and the dashed line
indicates the chance baseline.
tually hear the stimuli, they were first asked to enter
an English word presented auditorily.
Workers then completed ten test trials in which
they heard one word from the language and one non-
word made by concatenating all but the first syllable
of one word with the first syllable of another. If the
words ?bapu? and ?gudi? had been presented adja-
cently, the string ?pugu? would have been heard, de-
spite not being a word of the language. Both were
also displayed orthographically, and the worker was
instructed to click on the one which had appeared in
the previously heard language.
The language materials described above were
taken from a Saffran et al (1996) replication re-
ported as Experiment 2 in Frank, Goldwater, Grif-
fiths & Tenenbaum (under review). We compared
the results from lab participants reported in that ar-
ticle to data from MTurk workers using the applet
described above. Each response was marked ?cor-
rect? if the participant chose the word rather than the
nonword. 12 lab subjects achieved 71% correct re-
sponses, while 24 MTurk workers were only slightly
lower at 66%. The MTurk results proved signif-
icantly different from a ?random clicking? base-
line of 50% (t(23) = 5.92, p = 4.95 ? 10?06)
but not significantly different from the lab subjects
(Welch two-sample t-test for unequal sample sizes,
t(21.21) = ?.92, p = .37). Per-subject means for
the lab and MTurk data are plotted in Figure 2.
124
4 Contextual predictability
As psycholinguists build models of sentence pro-
cessing (e.g., from eye tracking studies), they need
to understand the effect of the available sentence
context. One way to gauge this is the Cloze task pro-
posed in Taylor (1953): participants are presented
with a sentence fragment and asked to provide the
upcoming word. Researchers do this for every word
in every stimulus and use the percentage of ?correct?
guesses as input into their statistical and computa-
tional models.
Rather than running such norming studies on un-
dergraduates in lab settings (as is typical), our results
suggest that psycholinguists will be able to crowd-
source these tasks, saving time and money without
sacrificing reliability (Schnoebelen and Kuperman,
submitted).
Our results are taken from 488 Americans, rang-
ing from age 16-80 (mean: 34.49, median: 32,
mode: 27) with about 25% each from the East and
Midwest, 31% from the South, the rest from the
West and Alaska. They represent a range of educa-
tion levels, though the majority had been to college:
about 33.8% had bachelor?s degrees, another 28.1%
had some college but without a degree.
By contrast, the lab data was gathered from 20
participants, all undergraduates at the University of
Massachusetts at Amherst in the mid-1990?s (Re-
ichle et al, 1998). Both populations provided judg-
ments on 488 words in 48 sentences. In general,
crowdsourcing gave more diverse responses, as we
would expect from a more diverse population.
The correlation between lab and crowdsourced
data by Spearman?s rank correlation is 0.823 (? <
0.0001), but we can be even more conservative by
eliminating the 124 words that had predictability
scores of 0 across both groups. By and large, the
lab participants and the workers are consistent in
which words they fail to predict. Even when we
eliminate these shared zeros, the correlation is still
high between the two data sets: weighted ? = 0.759
(? < 0.0001).
5 Judgment studies of fine-grained
probabilistic grammatical knowledge
Moving to syntax, we demonstrate here that gram-
maticality judgments from lab studies can also be
Figure 3: Mean ?that?-inclusion ratings plotted against
corresponding corpus-model predictions. The solid line
would represent perfect alignment between judgments
and corpus model. Non-parametric Lowess smoothers il-
lustrate the significant correlation between lab and crowd
population results.
reproduced through crowdsourcing.
Corpus studies of spontaneous speech suggest
that grammaticality is gradient (Wasow, 2008), and
models of English complement clause (CC) and rel-
ative clause (RC) ?that?-optionality have as their
most significant factor the predictability of embed-
ding, given verb (CC) and head noun (RC) lemma
(Jaeger, 2006; Jaeger, in press). Establishing that
these highly gradient factors are similarly involved
in judgments could provide evidence that such fine-
grained probabilistic knowledge is part of linguistic
competence.
We undertook six such judgment experiments:
two baseline studies with lab populations then four
additional crowdsourced trials via MTurk.
Experiment 1, a lab trial (26 participants, 30
items), began with the models of RC-reduction de-
veloped in Jaeger (2006). Corpus tokens were
binned by relative model-predicted probability of
?that?-omission. Six tokens were extracted at ran-
dom from each of five bins (0??<20% likelihood of
?that?-inclusion; 20??<40%; and so on). In a gra-
dient scoring paradigm with 100 points distributed
between available options (Bresnan, 2007) partici-
125
pants rated how likely each choice ? with or without
?that? ? was as the continuation of a segment of dis-
course. As hypothesized, mean participant ratings
significantly correlate with corpus model predictions
(r = 0.614, ? = 0.0003).
Experiment 2 (29 participants) replicated Exper-
iment 1 to address concerns that subjects might be
?over-thinking? the process. We used a timed forced-
choice paradigm where participants had from 5 to 24
seconds (varied as a linear function of token length)
to choose between the reduced/unreduced RC stim-
uli. These results correlate even more closely with
predictions (r = 0.838, ? < 0.0001).
Experiments 3 and 4 replicated 1 and 2 on MTurk
(1200 tasks each). Results were filtered by volun-
teered demographics to select the same subject pro-
file as the lab experiments. Response-time outliers
were also excluded to avoid fast-click-through and
distracted-worker data. Combined, these steps elim-
inated 384 (32.0%) and 378 (31.5%) tasks, respec-
tively, with 89 and 66 unique participants remaining.
While crowdsourced measures might be expected to
yield lower correlations due to such unbalanced data
sets, the results remain significant in both trials (r =
0.562, ? = 0.0009; r = 0.364, ? = 0.0285), offer-
ing strong evidence that crowdsourcing can replicate
limited-population, controlled-condition lab results,
and of the robustness of the alignment between pro-
duction and judgment models. Figure 3 compares
lab and crowd population results in the 100-point
task (Experiments 1 and 3).
Experiments 5 and 6 (1600 hits each) employed
the same paradigms via MTurk to investigate ?that?-
mentioning in CCs, where predictability of embed-
ding is an even stronger factor in the corpus model.
Filtering reduced the data by 590 (36.9%) and 863
(53.9%) hits. As with the first four experiments,
each of these trials produced significant correlations
(r = 0.433, ? = 0.0107; r = 0.500, ? = 0.0034; re-
spectively). Finally, mixed-effect binary logistic re-
gression models ? with verb lemma and test subject
ID as random effects ? were fitted to these judgment
data. As in the corpus-derived models, predictability
of embedding remains the most significant factor in
all experimental models.
The results across both lab and crowdsourced
studies suggest that speakers consider the same fac-
tors in judgment as in production, offering evidence
Figure 4: Odds ratio of a Nominal Agent being embed-
ded within a Sentential Agent or non-Agent, relative to
random chance. (? < 0.001 for all)
that competence grammar includes access to prob-
ability distributions. Meanwhile, the strong cor-
relations across populations offer encouraging evi-
dence in support of using the latter in psycholinguis-
tic judgment research.
6 Confirming corpus trends
Crowdsourcing can also be used to establish the va-
lidity of corpus trends found in otherwise skewed
data. The experiments in this section were mo-
tivated by the NomBank corpus of nominal pred-
icate/arguments (Meyers et al, 2004) where we
found that an Agent semantic role was much more
likely to be embedded within a sentential Agent. For
example, (1) is more likely than (2) to receive the
Agent interpretation for the ?the police?, but both
have same potential range of meanings:
(1) ?The investigation of the police took 3 weeks to
complete?
(2) ?It took 3 weeks to complete the investigation of
the police?
While the trend is significant (? < 0.001), the
corpus is not representative speech.
First, there are no minimal pairs of sentences in
NomBank like (1) and (2) that have the same poten-
tial range of meanings. Second, the s-genitive (?the
police?s investigation?) is inherently more Agen-
tive than the of-genitive (?the investigation of the
police?) and it is also more compact. Sentential
subjects tend to be lighter than objects, and more
likely to realize Agents, so the resulting correlation
could be indirect. Finally, if we sampled only the
predicates/arguments in NomBank that are frequent
in different sentential positions, we are limited to:
126
?earning, product, profit, trading, loss, share, rate,
sale, price?. This purely financial terminology is not
representative of a typical acquisition environment ?
no child should be exposed to only such language ?
so it is difficult to draw broad conclusions about the
cognitive viability of this correlation, even within
English. It is because of factors like these that cor-
pus linguistics has been somewhat of a ?poor cousin?
to theoretical linguistics.
Therefore, two sets of experiments were under-
taken to confirm that the trend is not epiphenomenal,
one testing comprehension and one testing produc-
tion.
The first tested thousands of workers? interpre-
tations of sentences like those in (1) and (2), over
a number of predicate/argument pairs (?shooting of
the hunters?, ?destruction of the army? etc). Work-
ers were asked their interpretation of the most likely
meaning. For example, does (1) mean: ?a: the po-
lice were doing the investigation? or ?b: the po-
lice are being investigated?. To control for errors
or click-throughs, two plainly incorrect options were
included. We estimate the erroneous response rate at
about 0.4% ? less than many lab studies.
For the second set of experiments, workers were
asked to reword an unambiguous sentence using a
given phrase. For example, rewording the following
using ?the investigation of the police?:
(3) ?Following the shooting of a commuter in Oak-
land last week, a reporter has uncovered new evi-
dence while investigating the police involved.?
We then (manually) recorded whether the required
phrase was in a sentential Agent or non-Agent posi-
tion.
Figure 4 gives the results from the corpus analy-
sis and both experiments. The results clearly show
a significant trend for all, and that the NomBank
trend falls between the comprehension and produc-
tion tasks, which would be expected for this highly
edited register. It therefore supports the validity of
the corpus results.
The phenomena likely exists to aid comprehen-
sion, as the cognitive realization of just one role
needs to be activated at a given moment. Despite
the near-ubiquity of ?Agent? in studies of semantic
roles, we do not yet have a clear theory of this lin-
guistic entity, or even firm evidence of its existence
Figure 5: Distribution of metaphorical frequencies.
(Parikh, 2010). This study therefore goes some way
towards illuminating this. More broadly, the experi-
ments in this section support the wider use of crowd-
sourcing as a tool for language cognition research in
conjunction with more traditional corpus studies.
7 Post-hoc metaphorical frequency
analysis of electrophysiological responses
Beyond reproducing laboratory and corpus studies,
crowdsourcing also offers the opportunity to newly
analyze data drawn from many other experimental
stimuli. In this section, we demonstrate that crowd-
sourced workers can help us better understand ERP
brainwave data by looking at how frequently words
are used metaphorically.
Recent work in event related potentials (ERP) has
suggested that even conventional metaphors, such as
?All my ideas were attacked? require additional pro-
cessing effort in the brain as compared to literal sen-
tences like ?All the soldiers were attacked? (Lai et
al., 2009). This study in particular observed an N400
effect where negative waves 400 milliseconds after
the presentation of the target words (e.g. attacked)
were larger when the word was used metaphorically
than when used literally.
The proposed explanation for this effect is that
metaphors really do demand more from the brain
than literal sentences. However, N400 effects are
also observed when subjects encounter something
that is semantically inappropriate or unexpected.
While the Lai experiment controlled for overall
word frequency, it might be possible to explain away
these N400 effects if it turned out that in the real
127
world the target words were almost always used
literally, so that seeing them used metaphorically
would be semantically incongruous.
To test this alternative hypothesis, we gathered
sense frequency distributions for each of the target
words ? the hypothesis predicts that these should
be skewed towards literal senses. For each of the
104 target words, we selected 50 random sentences
from the American National Corpus (ANC), fill-
ing in with British National Corpus sentences when
there were too few in the ANC. We gave the sen-
tences to crowdsourced workers and asked them to
label each target word as being used literally or
metaphorically. Each task contained one sentence
for each of the 104 target words, with the order of
words and the literal/metaphorical buttons random-
ized. Each sentence was annotated 5 times.
To encourage native speakers of English, we had
the MTurk service require that our workers be within
the United States, and posted the text ?Please ac-
cept this HIT only if you are a native speaker of En-
glish? in bold at the top of each HIT. We also used
Javascript to force workers to spend at least 2 sec-
onds on each sentence and we rejected results from
workers that had chance level (50%) agreement with
the other workers.
Though our tasks produced words annotated with
literal and metaphorical tags, we were less inter-
ested in the individual annotations (though agree-
ment was decent at 73%) and more interested in the
overall pattern for each target word. Some words,
like fruit, were almost always used literally (92%),
while other words, like hurdle were almost always
used metaphorically (91%) .
Overall, the target words had a mean metaphor-
ical frequency of 53%, indicating that their literal
and metaphorical senses were used in nearly equal
proportions. Figure 5 shows that the metaphorical
frequencies follow roughly a bell-curved distribu-
tion1, which is especially interesting given that the
target words were hand-selected for the Lai experi-
ment and not drawn randomly from a corpus. We did
not observe any skew towards literal senses as the
alternative hypothesis would have predicted. This
suggests that the findings of Lai, Curran, and Menn
1A Shapiro-Wilk test fails to reject the null hypothesis of a
normal distribution (p=0.09).
Item type correct incorrect
?easy? 60 2
?promise? 59 3
stacked genitive 55 7
Table 1: Response data for three control items, with the
goal of identifying workers who lack the requisite atten-
tiveness. All show high attentiveness. The difference be-
tween the ?easy? and ?stacked genitive? is trending but not
significant (? = 0.0835), indicating that any of these may
be used.
(2009) cannot be dismissed based on a sense fre-
quency argument.
We also took advantage of the collected sense fre-
quency distributions to re-analyze data from the Lai
experiment. We split the target words into a high bin
(average 72% metaphorical) and a low bin (average
33% metaphorical), matching the number of items
and average word log-frequency per bin. Looking at
the average ERPs (brain waves) over time for each
bin revealed that when subjects were reading novel
metaphors, there was a significant difference (p =
.01) at about 200ms (P200) between the ERPs for
the highly literal words and the ERPs for the highly
metaphorical words. Thus, not only does metaphori-
cal frequency influence figurative language process-
ing, but it does so much earlier than semantic effects
are usually observed (e.g. N400 effects at 400ms)2.
8 Screening for linguistic attentiveness
For annotation tasks, crowdsourcing is most suc-
cessful when the tasks are designed to be as simple
as possible, but in experimental work we don?t al-
ways want to target the shallowest knowledge of the
workers, so here we seek to discover just how atten-
tive the workers really are.
When running psycholinguistics experiments in
the lab, the experimenters generally have the chance
to interact with participants. It is not uncommon
for prospective subjects to be visibly exhausted, dis-
tracted, or inebriated, or not fluent in the given lan-
guage to a requisite level of competence. When
these participants turn up as outliers in the experi-
mental data, it is easy enough to see why ? they
fell asleep, couldn?t understand the instructions, etc.
2These results are consistent with recent findings that irony
frequency may also produce P200 effects (Regel et al, 2010).
128
With crowdsourcing we lose the chance to have
these brief but valuable encounters, and so anoma-
lous response data are harder to interpret.
We present two simple experiments for measuring
linguistic attentiveness, which can be used as one
component of a language study or to broadly evalu-
ate the linguistic competency of the workers. Taking
well-known constructions from the literature, we se-
lected constructions that: (a) exist in most (perhaps
all) dialects of English; (b) involve high frequency
lexical items; and (c) tend to be acquired relatively
late by first-language learners.
We have found two constructions from Carol
Chomsky?s (1969) work on first-language acquisi-
tion to be particularly useful:
(4) John is easy to see.
(5) John is eager to see.
Example (4) is accurately paraphrased as ?It is easy
to see John?, where John is the object of ?see?,
whereas (5) is accurately paraphrased as ?John is ea-
ger for John to see?, where John is the subject of
?see?. A similar shift happens with ?promise?:
(6) Bozo told Donald to sing.
(7) Bozo promised Donald to sing.
We presented workers with a multiple-choice ques-
tion that contained both subject and object para-
phrases as options.
In similar experiments, we adapted examples
from Roeper (2007), who looked at stacked prenom-
inal possessive constructions:
(8) John?s sister?s friend?s car.
These are cross-linguistically rare and challenging
even for native speakers. As above, the workers
were asked to choose between paraphrases.
Workers who provide accurate judgments are
likely to have a level of English competence and de-
votion to the task that suffices for many language
experiments. The results from one short audio study
are given in Table 1. They indicate a high degree of
attentiveness; as a group, our subjects performed at
the near-perfect levels we expect for fluent adults.
We predict that adding tasks like these to experi-
ments will not only screen for attentiveness, but also
prompt for greater attention from an otherwise dis-
tracted worker, improving results at both ends.
9 Conclusions
While crowdsourcing was first used by linguists for
annotation, we hope that the results here demon-
strate the potential for far richer studies. In a
range of linguistic disciplines from semantics to
psycholinguistics it enables systematic, large-scale
judgment studies that are more affordable and con-
venient than expensive, time-consuming lab-based
studies. With crowdsourcing technologies, linguists
have a reliable new tool for experimentally investi-
gating language processing and linguistic theory.
Here, we have reproduced many ?classic? large-
scale lab studies with a relative ease. We can en-
vision many more ways that crowdsourcing might
come to shape new methodologies for language
studies. The affordability and agility brings experi-
mental linguistics closer to corpus linguistics, allow-
ing the quick generation of targeted corpora. Multi-
ple iterations that were previously possible only over
many years and several grants (and therefore never
attempted) are now possible in a matter of days. This
could launch whole new multi-tiered experimental
designs, or at the very least allow ?rapid prototyp-
ing? of experiments for later lab-based verification.
Crowdsourcing also brings psycholinguistics
much closer to computational linguistics. The
two fields have always shared empirical data-driven
methodologies and computer-aided methods. We
now share a work-space too. Historically, NLP has
necessarily drawn corpora from the parts of linguis-
tic theory that have stayed still long enough to sup-
port time-consuming annotation projects. The re-
sults here have implications for such tasks, includ-
ing parsing, word-sense disambiguation and seman-
tic role labeling, but the most static parts of a field
are rarely the most exciting. We therefore predict
that crowdsourcing will also lead to an expanded,
more dynamic NLP repertoire.
Finally, for the past half-century theoretical lin-
guistics has relied heavily on ?introspective? corpus
generation, as the rare edge cases often tell us the
most about the boundaries of a given language. Now
that we can quickly and confidently generate empir-
ical results to evaluate hypotheses drawn from intu-
itions about the most infrequent linguistic phenom-
ena, the need for this particular fallback has dimin-
ished ? the stimuli are abundant.
129
Acknowledgements
We owe thanks to many people, especially within
the Department of Linguistics at Stanford, which has
quickly become a hive of activitiy for crowdsourced
linguistic research. In particular, we thank Tom Wa-
sow for his guidance in Section 5, Chris Manning for
his guidance in Section 6, and Florian T. Jaeger for
providing the corpus-derived base models in Section
5 (Jaeger, 2006). We also thank Michael C. Frank
for providing the design, materials, and lab data used
to evaluate the methods in Section 3. Several of the
projects reported here were supported by Stanford
Graduate Fellowships.
References
Joan Bresnan. 2007. Is syntactic knowledge probabilis-
tic? Experiments with the English dative alternation.
In Sam Featherston and Wolfgang Sternefeld, editors,
Roots: Linguistics in search of its evidential base,
pages 75?96. Mouton de Gruyter, Berlin.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 286?295.
Carol Chomsky. 1969. The Acquisition of Syntax in Chil-
dren from 5 to 10. MIT Press, Cambridge, MA.
Thierry Dutoit, Vincent Pagel, Nicolas Pierret, Franois
Bataille, and Olivier van der Vrecken. 1996. The
MBROLA project: Towards a set of high quality
speech synthesizers free of use for non commercial
purposes. In Fourth International Conference on Spo-
ken Language Processing, pages 75?96.
Michael Frank, Harry Tily, Inbal Arnon, and Sharon
Goldwater. submitted. Beyond transitional probabili-
ties: Human learners impose a parsimony bias in sta-
tistical word segmentation.
Michael Frank, Sharon Goldwater, Thomas Griffiths, and
Joshua Tenenbaum. under review. Modeling human
performance in statistical word segmentation.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35.
Florian Jaeger. 2006. Redundancy and syntactic reduc-
tion in spontaneous speech. Ph.D. thesis, Stanford
University, Stanford, CA.
Florian Jaeger. in press. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology.
Vicky Tzuyin Lai, Tim Curran, and Lise Menn. 2009.
Comprehending conventional and novel metaphors:
An ERP study. Brain Research, 1284:145?155, Au-
gust.
Richard Landis and Gary Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, , and Ralph
Grishman. 2004. Annotating noun argument structure
for NomBank. In Proceedings of LREC-2004.
Prashant Parikh. 2010. Language and Equilibrium. MIT
Press, Cambridge, MA.
Stefanie Regel, Seana Coulson, and Thomas C. Gunter.
2010. The communicative style of a speaker can af-
fect language comprehension? ERP evidence from the
comprehension of irony. Brain Research, 1311:121?
135.
Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher,
and Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125?157.
Tom Roeper. 2007. The Prism of Grammar: How Child
Language Illuminates Humanism. MIT Press, Cam-
bridge, MA.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Word segmentation: The role of distribu-
tional cues. Journal of memory and language, 35:606?
621.
Tyler Schnoebelen and Victor Kuperman. submitted. Us-
ing Amazon Mechanical Turk for linguistic research:
Fast, cheap, easy, and reliable.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew T. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263.
Wilson Taylor. 1953. Cloze procedure: A new tool for
measuring readability. Journalism Quarterly, 30:415?
433.
Tom Wasow. 2008. Gradient data and gradient gram-
mars. In Proceedings of the 43rd Annual Meeting of
the Chicago Linguistics Society, pages 255?271.
130
Using Query Patterns to Learn the Duration of Events
Andrey Gusev Nathanael Chambers Pranav Khaitan Divye Khilnani
Steven Bethard Dan Jurafsky
Department of Computer Science, Stanford University
{agusev,nc,pranavkh,divyeraj,bethard,jurafsky}@cs.stanford.edu
Abstract
We present the first approach to learning the durations of events without annotated training data,
employing web query patterns to infer duration distributions. For example, we learn that ?war?
lasts years or decades, while ?look? lasts seconds or minutes. Learning aspectual information is an
important goal for computational semantics and duration information may help enable rich document
understanding. We first describe and improve a supervised baseline that relies on event duration
annotations. We then show how web queries for linguistic patterns can help learn the duration of
events without labeled data, producing fine-grained duration judgments that surpass the supervised
system. We evaluate on the TimeBank duration corpus, and also investigate how an event?s participants
(arguments) effect its duration using a corpus collected through Amazon?s Mechanical Turk. We make
available a new database of events and their duration distributions for use in research involving the
temporal and aspectual properties of events.
1 Introduction
Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language
understanding. For example, knowing whether a nominal is a person or organization and whether a person
is male or female substantially improves coreference resolution, even when such knowledge is gathered
through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing
algorithms and resources for such semantic knowledge have focused primarily on static properties of
nominals (e.g. gender or entity type), not dynamic properties of verbs and events.
This paper shows how to learn one such property: the typical duration of events. Since an event?s
duration is highly dependent on context, our algorithm models this aspectual property as a distribution
over durations rather than a single mean duration. For example, a ?war? typically lasts years, sometimes
months, but almost never seconds, while ?look? typically lasts seconds or minutes, but rarely years or
decades. Our approach uses web queries to model an event?s typical distribution in the real world.
Learning such rich aspectual properties of events is an important area for computational semantics,
and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way
that gender has benefited nominal coreference systems. Event durations are also key to building event
timelines and other deeper temporal understandings of a text (Verhagen et al, 2007; Pustejovsky and
Verhagen, 2009).
The contributions of this work are:
? Demonstrating how to acquire event duration distributions by querying the web with patterns.
? Showing that a system that predicts event durations based only on our web count distributions can
outperform a supervised system that requires manually annotated training data.
? Making available an event duration lexicon with duration distributions for common English events.
We first review previous work and describe our re-implementation and augmentation of the latest
supervised system for predicting event durations. Next, we present our approach to learning event
distributions based on web counts. We then evaluate both of these models on an existing annotated corpus
of event durations and make comparisons to durations we collected using Amazon?s Mechanical Turk.
Finally, we present a generated database of event durations.
145
2 Previous Work
Early work on extracting event properties focused on linguistic aspect, for example, automatically
distinguishing culminated events that have an end point from non-culminated events that do not (Siegel
and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed
by Pan et al (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al,
2003) with duration lower and upper bounds. They then trained support vector machines on their annotated
corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes,
hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and
WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is
also a good baseline. We replicate their work and also add new features as described below.
Our approach to the duration problem is inspired by the standard use of web patterns for the acquisition
of relational lexical knowledge. Hearst (1998) first observed that a phrase like ?. . . algae, such as
Gelidium. . . ? indicates that ?Gelidium? is a type of ?algae?, and so hypernym-hyponym relations can
be identified by querying a text collection with patterns like ?such <noun> as <noun>? and ?<noun> ,
including <noun>?. A wide variety of pattern-based work followed, including the application of the idea
in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like ?to
<verb> and then <verb>? (Chklovski and Pantel, 2004).
More recent work has learned nominal gender and animacy by matching patterns like ?<noun> *
himself? and ?<noun> and her? to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like
?John Joseph?, which were observed often with masculine pronouns and never with feminine or neuter
pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can
predict person names as well as a fully supervised named entity recognition system.
Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to
the task of estimating event durations. One difference from previous work is the distributional nature of
the extracted knowledge. In the time domain, unlike in most previous relation-extraction domains, there is
rarely a single correct answer: ?war? may last months, years or decades, though years is the most likely.
Our goal is thus to produce a distribution over durations rather than a single mean duration.
3 Duration Prediction Tasks
In both our supervised and unsupervised models, we consider two types of event duration predictions: a
coarse-grained task in which we only want to know whether the event lasts more or less than a day, and a
fine-grained task in which we want to know whether the event lasts seconds, minutes, hours, days, weeks,
months or years. These two duration prediction tasks were originally suggested by Pan et al (2006), based
on their annotation of a subset of newspaper articles in the Timebank corpus (Pustejovsky et al, 2003).
Events were annotated with a minimum and maximum duration like the following:
? 5 minutes ? 1 hour: A Brooklyn woman who was watching her clothes dry in a laundromat.
? 1 week ? 3 months: Eileen Collins will be named commander of the Space Shuttle mission.
? 3 days ? 2 months: President Clinton says he is committed to a possible strike against Iraq. . .
Pan et al suggested the coarse-grained binary classification task because they found that the mean event
durations from their annotations were distributed bimodally across the corpus, roughly split into short
events (less than a day) and long events (more than a day). The fine-grained classification task provides
additional information beyond this simple two way distinction.
For both tasks, we must convert the minimum/maximum duration annotations into single labels. We
follow Pan et al (2006) and take the arithmetic mean of the minimum and maximum durations in seconds.
For example, in the first event above, 5 minutes would be converted into 300 seconds, 1 hour would be
converted into 3600 seconds, the resulting mean would be 1950 seconds, and therefore this event would
be labeled less-than-a-day for the coarse-grained task, and minutes for the fine-grained task. These labels
can then be used directly to train and evaluate our models.
146
4 Supervised Approach
Before describing our query-based approach, we describe our baseline, a replication and extension of the
supervised system from Pan et al (2006). We first briefly describe their features, which are shared across
the coarse and fine-grained tasks, and then suggest new features.
4.1 Pan et. al. Features
The Pan et al (2006) system included the following features which we also replicate:
Event Properties: The event token, lemma and part of speech (POS) tag.
Bag of Words: The n tokens to the left and right of the event word. However, because Pan et al
found that n = 0 performed best, we omit this feature.
Subject and Object: The head word of the syntactic subject and object of the event, along with their
lemmas and POS tags. Subjects and objects provide important context. For example, ?saw Europe? lasts
for weeks or months while ?saw the goal? lasts only seconds.
Hypernyms: WordNet hypernyms for the event, its subject and its object. Starting from the first
synset of each lemma, three hypernyms were extracted from the WordNet hierarchy. Hypernyms can help
cluster similar events together. For example, the event plan had three hypernym ancestors as features:
idea, content and cognition.
4.2 New Features
We present results for our implementation of the Pan et al (2006) system in Section 8. However, we also
implemented additional features.
Event Attributes: Timebank annotates individual events with four attributes: the event word?s tense
(past, present, future, none), aspect (e.g., progressive), modality (e.g., could, would, can, etc.), and event
class (occurrence, aspectual, state, etc.). We use each of these as a feature in our classifier. The aspect and
tense of the event, in particular, are well known indicators of the temporal shape of events (Vendler, 1976).
Named Entity Classes: Pan et al found the subject and object of the events to be useful features,
helping to identify the particular sense of the event. We used a named entity recognizer to add more
information about the subjects and objects, labeling them as persons, organizations, locations, or other.
Typed Dependencies: We coded aspects of the subcategorization frame of a predicate, such as
transitivity, or the presence of prepositional objects or adverbial modifiers, by adding a binary feature
for each typed dependency1 seen with a verb or noun. We experimented with including the head of the
argument itself, but results were best when only the dependency type was included.
Reporting Verbs: Many of the events in Timebank are reporting verbs (say, report, reply, etc.). We
used a list of reporting verbs to identify these events with a binary feature.
4.3 Classifier
Both the Pan et al feature set and our extended feature set were used to train supervised classifiers for the
two event duration prediction tasks. We experimented with naive bayes, logistic regression, maximum
entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model
performed best in cross-validations on the training data.
5 Unsupervised Approach
While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available
training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news
articles (Pan et al, 2006), and labeling further data is quite expensive. This motivates our desire to find an
1We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003).
147
approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the
Web to search for duration-specific patterns. This section describes our web-based approach to learning
event durations.
5.1 Web Query Patterns
Temporal properties of events are often described explicitly in language-specific constructions which can
help us infer an event?s duration. Consider the following two sentences from our corpus:
? Many spend hours surfing the Internet.
? The answer is coming up in a few minutes.
These sentences explicitly describe the duration of the events. In the first, the dominating clause spend
hours tells us how long surfing the Internet lasts (hours, not seconds), and in the second, the preposition
attachment serves a similar role. These examples are very rare in the corpus, but as can be seen, are
extremely informative when present. We developed several such informative patterns, and searched the
Web to find instances of them being used with our target events.
For each pattern described below, we use Yahoo! to search for the patterns occurring with our events.
We collect the total hit counts and use them as indicators of duration. The Yahoo! search API returns two
numbers for a query: totalhits and deephits. The former excludes duplicate pages and limits the number
of documents per domain while the latter includes all duplicates. We take the sum of these two numbers
as our count (this worked better than either of the two individually on the training data and provides
a balance between the benefits of each estimate) and normalize the results as described in Section 5.2.
Queries are submitted as complete phrases with quotation marks, so the results only include exact phrase
matches. This greatly reduces the number of hits, but results in more precise distributions.
5.1.1 Coarse-Grained Patterns
The coarse grained task is a binary decision: less than a day or more than a day. We can model this
task directly by looking for constructions that can only be used with events that take less than a day.
The adverb yesterday fills this role nicely; an event modified by yesterday strongly implies that it took
place within a single day?s time. For example, ?shares closed at $18 yesterday? implies that the closing
happened in less than a day. We thus consider the following two query patterns:
? <eventpast> yesterday
? <eventpastp> yesterday
where <eventpast> is the past tense (preterite) form of the event (e.g., ran), and <eventpastp> is the past
progressive form of the event (e.g., was running).
5.1.2 Fine-Grained Patterns
For the fine-grained task, we need patterns that can identify when an event falls into any of the various
buckets: seconds, minutes, hours, etc. Thus, our fine-grained patterns are parameterized both by the event
and by the bucket of interest. We use the following patterns inspired in part by Dowty (1979):
1. <eventpast> for * <bucket>
2. <eventpastp> for * <bucket>
3. spent * <bucket> <eventger>
where <eventpast> and <eventpastp> are defined as above, <eventger> is the gerund form of the event (e.g.,
running), and the wildcard ?*? can match any single token2.
The following three patterns ultimately did not improve the system?s performance on the training data:
4. <eventpast> in * <bucket>
5. takes * <bucket> to <event>
6. <eventpast> last <bucket>
Pattern 4 returned a lot of hits, but had low precision as it picked up many non-durative expressions.
Pattern 5 was very precise but typically returned few hits, and pattern 6 worked for, e.g., last week, but did
not work for shorter durations. All reported systems use patterns 1-3 and do not include 4-6.
2We experimented with varying numbers of wildcards but found little difference in performance on the training data.
148
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(a) ?was saying for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(b) ?for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(c) (a) counts divided by (b) counts
Figure 1: Normalizing the distribution for the pattern ?was saying for <bucket>?.
We also tried adding subjects and/or objects to the patterns when they were present for an event.
However, we found that the benefit of the extra context was outweighed by the significantly fewer hits that
resulted. We implemented several backoff approaches that removed the subject and object from the query,
however, the counts from these backoff approaches were less reliable than just using the base event.
5.2 Predicting Durations from Patterns
To predict the duration of an event from the above patterns, we first insert the event into each pattern
template and query the web to see how often the filled template occurs. These counts form a distribution
over each of the bins of interest, e.g., in the fine-grained task we have counts for seconds, minutes, hours,
etc. We discard pattern distributions with very low total counts, and normalize the remaining pattern
distributions based on the frequency with which the pattern occurs in general. Finally, we uniformly
merge the distributions from all patterns, and use the resulting distribution to select a duration label for
the event. The following sections detail this process.
5.2.1 Coarse-Grained Prediction
For the coarse-grained task of less than a day vs. more than a day, we collect counts using the two
yesterday patterns described above. We then normalize these counts by the count of the event?s occurrence
in general. For example, given the event run, we query for ?ran yesterday? and divide by the count of
?ran?. This gives us the probability of seeing yesterday given that we saw ran. We average the probabilities
from the two yesterday patterns, and classify an event as lasting less than a day if its average probability
exceeds a threshold t. We optimized t to our training set (t = .002). This basically says that if an event
occurs with yesterday more than 0.2% of the time, we will assume that the event lasts less than a day.
5.2.2 Fine-Grained Prediction
As with the coarse-grained task, our fine-grained approach begins by collecting counts using the three
fine-grained patterns discussed above. Since each fine-grained pattern has both an <event> and a <bucket>
slot to be filled, for a single event and a single pattern, we end up making 8 queries to cover each of the 8
buckets: seconds, minutes, hours, days, weeks, months, years and decades. After these queries, we have a
pattern-specific distribution of counts over the various buckets, a coarse measure of the types of durations
that might be appropriate to this event. Figure 1(a) shows an example of such a distribution.
As can be seen in Figure 1(a), this initial distribution can be skewed in various ways ? in this case,
years is given far too much mass. This is because in addition to the single event interpretation of words
like ?saying?, there are iterative or habitual interpretations (Moens and Steedman, 1988; Frawley, 1992).
Iterative events occur repeatedly over a period of time, e.g., ?he?s been saying for years that. . . ? The two
interpretations are apparent in the raw distributions of smile and run in Figure 2. The large peak at years
for run shows that it is common to say someone ?was running for years.? Conversely, it is less common to
say someone ?was smiling for years,? so the distribution for smile is less biased towards years.
149
        

	
A
B
C
D 
	AB
CDBEA	FB
Figure 2: Two double peaked distributions.
Coverage of Fine-Grained Query Patterns
Number of Patterns Total Events Precision
At least one 1359 (81.7%) 57.3
At least two 1142 (68.6%) 58.6
All three 428 (25.7%) 65.7
Figure 3: The number of events that match n fine-
grained patterns and the pattern precision on these
events. The training set consists of 1664 events.
While the problem of distinguishing single events from iterative events is out of the scope of this paper
(though an interesting avenue for future research), we can partially address the problem by recognizing
that some buckets are simply more frequent in text than others. For example, Figure 1(b) shows that it is
by far more common to see ?for <bucket>? filled with years than with any other duration unit. Thus, for
each bucket, we divide the counts collected with the event patterns by the counts we get for the pattern
without the event3. Essentially, this gives us for each bucket the probability of the event given that bucket.
Figure 1(c) shows that the resulting normalized distribution fits our intution of how long ?saying? should
last much better than the raw counts: seconds and minutes have much more of the mass now.
After normalizing an event?s counts for each pattern, we combine the distributions from the three
different patterns if their hit counts pass certain confidence thresholds. The total hit count for each pattern
must exceed a minimum threshold tmin = 100 and not exceed a maximum threshold tmax = 100, 000
(both thresholds were optimized on the training data). The former avoids building distributions from a
sparse number of hits, and the latter avoids classifying generic and polysemous events like ?to make? that
return a large number of hits. We found such events to produce generic distributions that do not help in
classification. If all three patterns pass our confidence thresholds, we merge the pattern distributions by
summing them bucket-wise together and renormalizing the resulting distribution to sum to 1. Merging the
patterns mitigates the noise from any single pattern.
To predict the event?s duration, we then select the bucket with the highest smoothed score:
score(bi) = bi?1 + bi + bi+1
where bi is a duration bucket and 0 < i < 9. We define b0 = b9 = 0. In other words, the score of the
minute bucket is the sum of three buckets: second, minute and hour. This parallels the smoothing of the
evaluation metric introduced by (Pan et al, 2006) which we also adopt for evaluation in Section 7.
In the case that fewer than three of our patterns matched, we backoff to the majority class (months for
fine-grained, and more-than-a-day for coarse-grained). We experimented with only requiring one or two
patterns to match, but found the best results on training when requiring all three. Figure 3 shows the large
jump in precision when all three are required. The evaluation is discussed in Section 7.
5.2.3 Coarse-Grained Prediction via Fine-Grained Prediction
We can also use the distributions collected from the fine-grained task to predict coarse-grained labels. We
use the above approach and return less than a day if the selected fine-grained bucket was seconds, minutes
or hours, and more than a day otherwise. We also tried summing over the duration buckets: p(seconds) +
p(minutes) + p(hours) for less than day and p(days) + p(weeks) + p(months) + p(years) + p(decades) for
more than a day, but the simpler approach outperformed these summations in training.
3We also explored normalizing not by the global distribution on the Web, but by the average of the distributions of all the
events in our dataset. However, on the training data, using the global distribution performed better.
150
6 Datasets
6.1 Timebank Duration
As described in Section 3, Pan et al (2006) labeled 58 documents with event durations. We follow their
method of isolating the 10 WSJ articles as a separate test set which we call TestWSJ (147 events). For
the remaining 48 documents, they split the 2132 event instances into a Train and Test set with 1705 and
427 events respectively. Their split was conducted over the bag of events, so their train and test sets may
include events that came from the same document. Their particular split was unavailable.
We instead use a document-split that divides the two sets into bins of documents. Each document?s
entire set of events is assigned to either the training set or the test set, so we do not mix events across
sets. Since documents often repeat mentions of events, this split is more conservative by not mixing test
mentions with the training set. Train, Test, and TestWSJ contain 1664 events (714 unique verbs), 471 events
(274 unique), and 147 events (84 unique) respectively. For each base verb, we created queries as described
in Section 5.1.2. The train/test split is available at http://cs.stanford.edu/people/agusev/durations/.
6.2 Mechanical Turk Dataset
We also collected event durations from Amazon?s Mechanical Turk (MTurk), an online marketplace from
Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts
of money. Prior work has shown that human judgments from MTurk can often be as reliable as trained
annotators (Snow et al, 2008) or subjects in controlled lab studies (Munro et al, 2010), particularly when
judgments are aggregated over many MTurk workers (?Turkers?). Our motivation for using Turkers is to
better analyze system errors. For example, if we give humans an event in isolation (no sentence context),
how well can they guess the durations assigned by the Pan et. al. annotators? This measures how big the
gap is between a system that looks only at the event, and a system that integrates all available context.
To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a
superset of the events annotated by Pan et al (2006)) and asked them to decide whether the event was most
likely to take seconds, minutes, hours, days, weeks, months, years or decades. We had events annotated
in two different contexts: in isolation, where only the event itself was given (e.g., ?allocated?), and in
subject-object context, where a minimal phrase including the event and its subject and object was given
(e.g., ?the mayor allocated funds?). In both types of tasks, we asked 10 Turkers to label each event,
and they were paid $0.0025 for each annotation ($0.05 for a block of 20 events). To filter out obvious
spammers, we added a test item randomly to each block, e.g., adding the event ?minutes? and rejecting
work from Turkers who labeled this anything other than the duration minutes.
The resulting annotations give duration distributions for each of our events. For example, when
presented the event ?remodeling?, 1 Turker responded with days, 6 with weeks, 2 with months and 1
with years. These annotations suggest that we generally expect ?remodeling? to take weeks, but it may
sometimes take more or less. To produce a single fine-grained label from these distributions, we take the
duration bin with the largest number of Turker annotations, e.g. for ?remodeling?, we would produce the
label weeks. To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained
label was seconds, minutes or hours and more-than-a-day otherwise.
7 Experiment Setup
As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by
converting each to seconds using ISO standards and calculating the arithmetic mean. If the mean is
? 86400 seconds, it is considered less-than-a-day for the coarse-grained task. The fine-grained buckets
are similarly calculated, e.g., X is labeled days if 86400 < X ? 604800. The Pan et al (2006) evaluation
does not include a decades bucket, but our system still uses ?decades? in its queries.
We optimized all parameters of both the supervised and unsupervised systems on the training set, only
running on test after selecting our best performing model. We compare to the majority class as a baseline,
151
Coarse-Grained
Test TestWSJ
Supervised, Pan 73.3 73.5
Supervised, all 73.0 74.8
Fine-Grained
Test TestWSJ
Supervised, Pan 62.2 61.9
Supervised, all 62.4 66.0
Figure 4: Accuracies of the supervised maximum entropy classifiers with two different feature sets.
Coarse-Grained
Test TestWSJ
Majority class 62.4 57.1
Supervised, all 73.0* 74.8*
Web counts, yesterday 70.7* 74.8*
Web counts, buckets 72.4* 73.5*
Fine-Grained
Test TestWSJ
Majority class 59.2 52.4
Supervised, all 62.4 66.0?
Web counts, buckets 66.5* 68.7*
Figure 5: System accuracy compared against supervised and majority class. * indicates statistical
significance (McNemar?s Test, two-tailed) against majority class at the p < 0.01 level, ? at p < 0.05
tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.
To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement
matching as in Pan et al (2006) on the fine-grained task. In this approximate agreement, a guess is
considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if
minutes, hours or days is the gold class). Pan et al use this approach since human labeling agreement is
low (44.4%) on the exact agreement fine-grained task.
8 Results
Figure 4 compares the performance of our two supervised models; the reimplementation of Pan et al
(2006) (Supervised, Pan), and our improved model with new features (Supervised, all). The new model
performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial
news articles in the TestWSJ test. On the latter, the new model improves over Pan et al by 1.3% absolute
on the coarse-grained task, and by 4.1% absolute on the fine-grained task. We report results from the
maximum entropy model as it slightly outperformed the naive bayes and support vector machine models4.
We compare these supervised results against our web-based unsupervised systems in Figure 5. For the
coarse-grained task, we have two web count systems described in Section 5: one based on the yesterday
patterns (Web counts, yesterday), and one based on first gathering the fine-grained bucket counts and
then converting those to coarse-grained labels (Web counts, buckets). Generally, these models perform
within 1-2% of the supervised model on the coarse-grained task, though the yesterday-based classifier
exactly matches the supervised system?s performance on the TestWSJ data. The supervised system?s
higher results are not statistically significant against our web-based systems.
For the fine-grained task, Figure 5 compares our web counts algorithm based on duration distributions
(Section 5) to the baseline and supervised systems. Our web counts approach outperforms the best
supervised system by 4.1% absolute on the Test set and by 2.7% absolute on the out-of-domain TestWSJ.
To get an idea of how much the subject/object context could help predict event duration if integrated
perfectly, we evaluated the Mechanical Turk annotations against the Pan et. al. annotated dataset using
approximate agreement as described in Section 7. Figure 6 gives the performance of the Turkers given
two types of context: just the event itself (Event only), and the event plus its subject and/or object (Event
and args). Turkers performed below the majority class baseline when given only the event, but generally
above the baseline when given the subject and object, improving up to 20% over the event-only condition.
Figure 7 shows examples of events with different learned durations.
4This differs from Pan et al who found support vector machines to be the best classifier.
152
Mechanical Turk Accuracy
Coarse Fine
Test WSJ Test WSJ
Majority class 62.4 57.1 59.2 52.4
Event only 52.0 49.4 42.1 43.8
Event and args 65.0 70.1 56.7 59.9
Figure 6: Accuracy of Mechanical Turkers
against Pan et. al. annotations.
Learned Examples
talk to tourism leaders minutes
driving hours
shut down the supply route days
travel weeks
the downturn across Asia months
build a museum years
Figure 7: Examples of web query durations.
9 Discussion
Our novel approach to learning event durations showed 4.1% and 2.7% absolute gains over a state-of-the-
art supervised classifier. Although the gain is not statistically significant, these results nonetheless suggest
that we are learning as much about event durations from the web counts as we are currently able to learn
with our improvements to Pan et al?s (2006) supervised system. This is encouraging because it indicates
that we may not need extensive manual annotations to acquire event durations. Further, our final query
system achieves these results with only the event word, and without considering the subject, object or
other types of context.
Despite the fact that we saw little gains in performance when including subjects and objects in our
query patterns, the Mechanical Turk evaluation suggests that more information may still be gleaned from
the additional context. Giving Turkers the subject and object improved their label accuracy by 10-20%
absolute. This suggests that finding a way to include subjects and objects in the web queries, for example
by using thesauri to generate related queries, is a valuable line of research for future work.
Finally, these MTurk experiments suggest that classifying events for duration out of context is a
difficult task. Pan et al (2006) reported 0.88 annotator agreement on the coarse-grained task when given
the entire document context. Out of context, given just the event word, our Turkers only achieved 52%
and 49% accuracy. Not surprisingly, the task is more difficult without the document. Our system, however,
was also only given the event word, but it was able achieve over 70% in accuracy. This suggests that rich
language understanding is often needed to correctly label an event for duration, but in the absence of such
understanding, modeling the duration by web counts appears to be a practical and useful alternative.
10 A Database of Event Durations
Given the strong performance of our model on duration classification, we are releasing a database of
events and their normalized duration distributions, as predicted by our bucket-based fine-grained model.
We extracted the 1000 most frequent verbs from a newspaper corpus (the NYT portion of Gigaword
Graff (2002)) with the 10 most frequent grammatical objects of each verb. These 10, 000 events and their
duration distributions are available at http://cs.stanford.edu/people/agusev/durations/.
Acknowledgements
Thanks to Chris Manning and the anonymous reviewers for insightful comments and feedback. This
research draws on data provided by Yahoo!, Inc., through its Yahoo! Search Services offering. We
gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those
of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.
153
References
Bergsma, S. (2005). Automatic acquisition of gender information for anaphora resolution. In Advances
in Artificial Intelligence, Volume 3501 of Lecture Notes in Computer Science, pp. 342?353. Springer
Berlin / Heidelberg.
Chen, Z. and H. Ji (2009). Graph-based event coreference resolution. In Proceedings of the Workshop on
Graph-based Methods for Natural Language Processing (TextGraphs-4), Singapore, pp. 54?57. ACL.
Chklovski, T. and P. Pantel (2004). Verbocean: Mining the web for fine-grained semantic verb relations.
In D. Lin and D. Wu (Eds.), Proceedings of EMNLP 2004, Barcelona, Spain, pp. 33?40.
Dowty, D. R. (1979). Word Meaning and Montague Grammar. Kluwer Academic Publishers.
Frawley, W. (1992). Linguistic Semantics. Routledge.
Graff, D. (2002). English Gigaword. Linguistic Data Consortium.
Haghighi, A. and D. Klein (2009). Simple coreference resolution with rich syntactic and semantic features.
In Proceedings of EMNLP-2009, Singapore, pp. 1152?1161.
Hearst, M. A. (1998). Automated discovery of wordnet relations. In WordNet: An Electronic Lexical
Database. MIT Press.
Ji, H. and D. Lin (2009). Gender and animacy knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proceedings of the Pacific Asia Conference on Language,
Information and Computation.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Moens, M. and M. Steedman (1988). Temporal ontology in natural language. Computational Linguis-
tics 2(14), 15?21.
Munro, R., S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, and H. Tily (2010).
Crowdsourcing and language studies: the new generation of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk,
Los Angeles, pp. 122?130.
Pan, F., R. Mulkar, and J. Hobbs (2006). Learning event durations from event descriptions. In Proceedings
of COLING-ACL.
Pustejovsky, J., P. Hanks, R. Sauri, A. See, D. Day, L. Ferro, R. Gaizauskas, M. Lazo, A. Setzer, and
B. Sundheim (2003). The timebank corpus. Corpus Linguistics, 647?656.
Pustejovsky, J. and M. Verhagen (2009). Semeval-2010 task 13: Evaluating events, time expressions, and
temporal relations (tempeval-2). In Proceedings of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009), Boulder, Colorado, pp. 112?116.
Siegel, E. V. and K. R. McKeown (2000). Learning methods to combine linguistic indicators: improving
aspectual classification and revealing linguistic insights. Computational Linguistics 26(4), 595?628.
Snow, R., B. O?Connor, D. Jurafsky, and A. Ng (2008). Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, Hawaii.
Vendler, Z. (1976). Verbs and times. Linguistics in Philosophy, 97?121.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky (2007). Semeval-2007
task 15: Tempeval temporal relation identification. In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pp. 75?80.
154
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 12?21,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying science concepts and student misconceptions
in an interactive essay writing tutor
Steven Bethard
University of Colorado
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ifeyinwa Okoye
University of Colorado
Boulder, Colorado, USA
ifeyinwa.okoye@colorado.edu
Md. Arafat Sultan
University of Colorado
Boulder, Colorado, USA
arafat.sultan@colorado.edu
Haojie Hang
University of Colorado
Boulder, Colorado, USA
haojie.hang@colorado.edu
James H. Martin
University of Colorado
Boulder, Colorado, USA
james.martin@colorado.edu
Tamara Sumner
University of Colorado
Boulder, Colorado, USA
tamara.sumner@colorado.edu
Abstract
We present initial steps towards an interac-
tive essay writing tutor that improves science
knowledge by analyzing student essays for mis-
conceptions and recommending science web-
pages that help correct those misconceptions.
We describe the five components in this sys-
tem: identifying core science concepts, deter-
mining appropriate pedagogical sequences for
the science concepts, identifying student mis-
conceptions in essays, aligning student miscon-
ceptions to science concepts, and recommend-
ing webpages to address misconceptions. We
provide initial models and evaluations of the
models for each component.
1 Introduction
Students come to class with a variety of misconcep-
tions present in their science knowledge. For ex-
ample, science assessments developed by the Amer-
ican Association for the Advancement of Science
(AAAS)1 showed that 49% of American 6th-8th
graders believe that the Earth?s tectonic plates are
only feet thick (while in fact they are miles thick)
and that 48% of American 6th-8th graders believe
that atoms of a solid are not moving (while in fact
all atoms are in constant motion). A key challenge
for interactive tutoring systems is thus to identify and
correct such student misconceptions.
In this article, we develop an interactive essay writ-
ing tutor that tries to address these challenges. The
tutor first examines a set of science webpages to iden-
tify key concepts (Section 4) and attempts to order
1http://assessment.aaas.org/
the science concepts in a pedagogically appropriate
learning path (Section 5). Then the tutor examines a
student essay and identifies misconception sentences
(Section 6) and aligns these misconceptions to the
true science concepts (Section 7). Finally, the tutor
suggests science webpages that can help the student
address each of the misconceptions (Section 8).
The key contributions of this work are:
? Demonstrating that a summarization approach
can identify core science concepts
? Showing how a learning path model can be boot-
strapped from webpages with grade metadata
? Developing models for misconception identifi-
cation based on textual entailment techniques
? Presenting an information retrieval approach to
aligning misconceptions to science concepts
? Designing a system that recommends webpages
to address student misconceptions
2 Related work
Interactive tutoring systems have been designed for
a variety of domains and applications. Dialog-based
tutoring systems, such as Why2-Atlas (VanLehn et
al., 2002), AutoTutor (Graesser et al, 2004) and
MetaTutor (Azevedo et al, 2008), interact with stu-
dents via questions and answers. Student knowledge
is judged by comparing student responses to knowl-
edge bases of domain concepts and misconceptions.
These knowledge bases are typically manually cu-
rated, and a new knowledge base must be constructed
for each new domain where the tutor is to be used.
12
Essay-based tutoring systems, such as Summary
Street (Wade-Stein and Kintsch, 2004) or CLICK
(de la Chica et al, 2008b), interact with students who
are writing a summary or essay. They compare what
the student has written to domain knowledge in the
form of textbooks or webpages. They typically do not
require a knowledge base to be manually constructed,
instead using natural language processing techniques
to compare the student?s essay to the information in
the textbooks or webpages.
The current work is inspired by these essay-based
tutoring systems, where interaction revolves around
essay writing. However, where Summary Street re-
lies primarily upon measuring how much of a text-
book a student essay has ?covered?, we aim to give
more detailed assessments that pinpoint specific stu-
dent misconceptions. CLICK targets a similar goal
to ours, but assumes that accurate knowledge maps
can be generated for both the domain knowledge and
for each student essay. Our approach does not re-
quire the automatic generation of knowledge maps,
instead working directly with the sentences in the
student essays and the webpages of science domain
knowledge.
3 System overview
Our system is composed of five key components.
First, a core concept identifier examines domain
knowledge (webpages) and identifies key concepts
(sentences) that describe the most important pieces
of knowledge in the domain. Second, a concept se-
quencer assigns a pedagogically appropriate order in
which a student should learn the identified core con-
cepts. Third, a misconception identifier examines the
student essay and identifies sentences that describe
misconceptions the student has about the domain.
Fourth, a misconception-concept aligner finds a core
concept that can be used to correct each misconcep-
tion. Finally, a recommender takes all the informa-
tion about core concepts and student misconceptions,
decides what order to address the misconceptions in,
and identifies a set of resources (webpages) for the
student to read.
To assemble this system, we draw on a variety of
existing datasets (and some data collection of our
own). For example, we use data from an annotation
study of concept coreness to evaluate our model for
identifying domain concepts, and we use data from
science assessments of the American Association for
the Advancement of Science to train and evaluate our
model for identifying misconceptions. We use this
disparate data to establish baseline models for each of
the tutor?s components. In the near future, this base-
line tutoring system will be used to collect student
essays and other data that will allow us to develop
more sophisticated model for each component.
4 Identifying core concepts
This first module aims at automatically identifying a
set of core concepts in a given set of digital library
resources or webpages. Core concepts in a subject
domain are critical ideas necessary to support deep
science learning and transfer in that domain. From
a digital learning perspective, availability of such
concepts helps in providing pedagogical feedback
to learners to support robust learning and also in
prioritizing instructional intervention (e.g., deciding
the order in which to treat student misconceptions).
A concept can be materialized using different levels
of linguistic expressions (e.g. phrases, sentences or
paragraphs), but for this work, we focus only on
individual sentences as expressions of concepts.
We used COGENT (de la Chica et al, 2008a), a
multi-document summarization system to extract con-
cepts (i.e. sentences) from a given set of resources.
In the following two subsections, we describe the
COGENT system, discuss how we used it for core
concept extraction and report the results of its evalu-
ation of effectiveness.
4.1 Model
COGENT is a text summarizer that builds on MEAD
(Radev et al, 2004), a multidocument summarization
and evaluation platform . MEAD was originally de-
veloped to summarize news articles. COGENT aims
to generate pedagogically useful summaries from
educational resources.
COGENT extends MEAD by incorporating new
features in the summarization process. MEAD uses
a set of generic (i.e. domain-independent) features to
evaluate each sentence in the given set of documents.
These features include the length of the sentence, the
distance from the sentence to the beginning of the
document, etc. Individual scores of a sentence along
13
these dimensions are combined to assign a total score
to the sentence. After removing redundant sentences,
MEAD then generates a summary using the sentences
that had the highest scores. A user-specified parame-
ter determines the number of sentences included in
the summary.
COGENT extends this framework by incorporat-
ing new domain-general and domain-specific features
in the sentence scoring process. The domain-general
features include a document structure feature, which
takes into account a sentence?s level in terms of
HTML headings, and a content word density fea-
ture, which computes the ratio of content words to
function words. The domain-specific features include
an educational standards feature, which uses a TF-
IDF based textual similarity score between a sentence
and nationally recognized educational goals from the
American Association for the Advancement of Sci-
ence (AAAS) Benchmarks (Project2061., 1993) and
the associated National Science Education Standards
(NRC, 1996), and a gazetteer feature, which scores
sentences highly that mention many unique names
from a gazetteer of named entities.
While in the past, COGENT was used primarily
as a summarization system, in the current work, we
evaluate its utility as a means of identifying core
concepts. That is, are the top sentences selected
by COGENT also the sentences describing the key
science concepts in the domain?
4.2 Evaluation
We evaluate the core concept extraction module by
assessing the extracted concepts against human ex-
pert annotations. We ran an annotation study where
two human experts assigned ?coreness? ratings to
a selected set of sentences collected from digital
resources in three science domains: Plate Tecton-
ics, Weather and Climate, and Biological Evolution.
These experts had been recruited based on their train-
ing and expertise in the selected subject domains.
First, a set of digital resources was selected from
the Digital Library for Earth System Education
(DLESE) 2 across the three subject domains. Then
COGENT was used to extract the top 5% sentences
for each domain. The experts then annotated each
extracted sentence with its coreness rating on a scale
2http://www.dlese.org
Extraction %
0.5% 1.0% 2.5% 5.0%
Plate Tectonics 3.33 3.27 3.00 2.81
Weather and Climate 3.13 2.97 3.07 2.99
Biological Evolution 2.00 2.13 2.46 2.25
Table 1: Average coreness of sentences extracted at differ-
ent percentages in each domain
of 1 to 4, 4 being the highest. Human annotation is
a time-consuming process and this is why we had
to limit the number of extracted sentences to a mod-
erate 5% (which is still more than 400 sentences).
17% of the sentences were double annotated and the
inter-rater reliability, measured by Spearman?s rho,
was 0.38. These expert ratings of sentences form the
basis of our evaluation.
Table 1 shows the average coreness assigned by the
experts to sentences extracted by COGENT in each
domain, for different extraction percentages. For ex-
ample, if COGENT is used to extract the top 1% of
sentences from all the Plate Tectonics resources, then
the average of their coreness ratings (as assigned by
the experts) is 3.27, representing a high level of core-
ness. This is essentially a measure of the precision
of COGENT at 1% extraction. Note that we cannot
calculate a measure of recall without asking experts
to annotate all of the domain sentences, a time con-
suming task which was outside of the scope of this
study.
The performance of COGENT was the best in the
Plate Tectonics domain since the domain-aware fea-
tures (e.g. the gazetteer features) used to train CO-
GENT were selected from this domain. In the ?near
domain? of Weather and Climate, the performance is
still good, but performance falls in the ?far domain?
of Biological Evolution, because of the significant
differences between the training domain and the test
domain. In the two latter domains, the performance
of COGENT was also inconsistent in that with an
increase in the extraction percentage, the average
coreness increased in some cases and decreased in
others. This inconsistency and overall degradation
in performance in the two latter domains are indica-
tive of the importance of introducing domain-aware
features into COGENT.
It is evident from the values in Table 1 that the
core concepts extraction module does a decent job,
14
especially when trained with appropriate domain-
aware features.
5 Sequencing core concepts
The goal of this next component is to take a set of
core science concepts (sentences), as produced by
the preceding module, and predict an appropriate se-
quence in which those concepts should be learned by
the student. Some concepts serve as building blocks
for other concepts, and thus it is essential to learn the
basic concepts first (and address any misconceptions
associated with them) before moving on to other con-
cepts that depend on the basic concepts. For example,
a student must first understand the concept of tectonic
plates before they can understand the concept of a
convergent plate boundary. The sequence of core
concepts that results from this module will serve as
input for the later module that prioritizes a student?s
misconceptions.
There may exist several different but reasonable
concept sequences (also known as learning paths) ?
the goal of this component is to recommend at least
one of these. As a first step, we focus on generating
a single concept sequence that represents a general
path through the learning goals, much like textbooks
and curriculums do.
5.1 Models
Our model for concept sequencing is a pair-wise
ordering model, that takes two concepts c1 and c2,
and predicts whether c1 should come before or after
c2 in the recommended learning path. Formally,
SEQUENCE(c1, c2) =
{
0 if c1 < c2
1 if c1 ? c2
To generate a complete ordering of concepts, we
construct a precedence table from these pair-wise
judgments and generate a path that is consistent with
these judgments.
We learn the SEQUENCE model as a supervised
classifier, where a feature vector is extracted for each
of the two concepts and the two feature vectors, con-
catenated, serve as the input to the classifier. For each
word in each concept, we include the following two
features:
? local word count - the number of times the
word appeared in this concept
? global word count - the log of the ratio between
the number of times the word occurred in the
concept and the number of times it occurred in
a background corpus, Gigaword (Graff, 2002)
These features are motivated by the work of Tanaka-
ishii et al(2010) that showed that local and global
word count features were sufficient to build a pair-
wise readability classifier that achieved 90% accu-
racy.
For the supervised classifier, we consider naive
Bayes, decision trees, and support vector machines.
5.2 Evaluation
To evaluate our concept sequencing model, we gath-
ered learning paths from experts in high school earth
science. Using the model from Section 4, we selected
30 core concepts for the domain of plate tectonics.
We asked two earth science experts to each come up
with two learning paths for these core concepts, with
the first path following an evidence or research based
and second path following a traditional learning path.
An evidence or research based learning path, is
a pedagogy where students are encouraged to use
the scientific method to learn about a phenomena, i.e
they gather information by observing the phenomena,
form a hypothesis, perform experiment, collect and
analyze data and then interpret the data and draw
conclusions that hopefully align with the current un-
derstanding about the phenomena. A teacher that
uses this learning path acts as a guide on the side. A
traditional learning path on the other hand, is the ped-
agogy where teachers are simply trying to pass on the
correct information to students rather than letting the
students discover the information themselves. In a
classroom environment, a teacher using this learning
path would be seen as the classical sage on stage.
We used the learning paths collected from the ex-
perts to form two test sets, one for the evidence-based
pedagogy, and one for the traditional pedagogy. For
each pedagogy, we asked which of all the possible
pair-wise orderings our experts agreed upon. For ex-
ample, if the first expert said that A < B < C and
the second expert said that A < C < B, then both
experts agreed that A < B and A < C, while they
disagreed on whether B < C or C < B. Note that
we evaluate pair-wise orderings here, not a complete
ranking of the concepts, because the experts did not
15
Pedagogy Pairs (%) c1 < c2 c1 ? c2
Evidence 637 (68%) 48.5% 51.5%
Traditional 613 (70%) 48.5% 51.5%
Table 2: Test sets for sequencing concepts. The Pairs
column shows how many pairs the experts agreed upon
(out of a total of 30 ? 29 = 870 pairs).
produce a total ordering of the concepts, only a par-
tial tree-like ordering. The experts put the concepts
in levels, with concepts in the same level having no
precedence relationship, while a concept in a lower
level preceded a concept in a higher level.
For our test sets, we selected only the pairs on
which both experts agreed. Table 2 shows that experts
agreed on 68-70% of the pair-wise orderings. Table
2 also shows the percentage of each type of pair-wise
ordering (c1 < c2 vs. c1 ? c2) present in the data.
Note that even though all concepts are paired with all
other concepts, because the experts do not produce
complete orderings, the number of agreements for
each type of ordering may not be the same. Consider
the case where expert E1 says that concepts A and
B are on the same level (i.e., A = B) and expert E2
says that concept A is in a lower level than concept
B (i.e., A < B). Then for the pair (A,B), they
disagree on the relation (E1 says A ? B while E2
says A < B) but for the pair (B,A) they agree on
the relation (they both say B ? A). As a result, the
c1 ? c2 class is slightly larger than the c1 < c2 class.
Since these data sets were small, we reserved them
for testing, and trained our pair-wise classification
model using a proxy task: ordering sentences by
grade. In this task, the model is given two sentences
s1 and s2, one written for middle school and written
for high school, and asked to decide whether s1 < s2
(i.e. s1 is the middle school sentence) or s2 < s1
(i.e. s2 is the middle school sentence). We expect
that a model for ordering sentences by grade should
also be a reasonable model for ordering concepts
for a pedagogical learning path. And importantly,
getting grade ordering data automatically is easy: the
Digital Library for Earth System Education (DLESE)
contains a variety of earth science resources with
metadata about the grade level they were written for.
To construct the training data, we searched the
DLESE website for text resources that contained
the words earthquake or plate tectonics. We col-
Baseline NaiveBayes SVM
Evidence 51.5% 60.8% 53.3%
Traditional 51.5% 56.6% 49.7%
Table 3: Accuracy result from Naive Bayes and SVM for
classifying the core concepts
lected 10 such resources for each of the two grade
cohorts, middle school (we allowed anything K-8)
and high school (we allowed anything 9+). We down-
loaded the webpage for each resource, and used CO-
GENT to extract the 20 most important sentences
from each. This resulted in 200 sentences for each
of the two grade cohorts. To create pairs of grade-
ordered sentences, we paired up middle and high
school concepts both ways: middle school first (i.e.
SEQUENCE(cm, ch) = 0) and high school first (i.e.
SEQUENCE(ch, cm) = 1). This resulted in 40,000
grade-ordered sentence pairs for training.
We then used this proxy-task training data to
train our models. We extracted 1702 unique non-
stopwords from the training data, resulting in 3404
features per concept, and 6808 features per con-
cept pair (i.e. per classification instance). On the
grade-ordering task, we evaluated three models using
WEKA3, a naive Bayes model, a decision tree (J48)
model, and a support vector machine (SVM) model.
Using a stratified 50/50 split of the training data, we
found that the naive Bayes and SVM models both
achieved an accuracy of 80.2%, while the decision
tree achieved only 62%. So, we selected the naive
Bayes and SVM models for our real task, concept
sequencing.
Table 3 shows the performance of the two models
on the expert judgments of concept sequencing. We
find that the naive Bayes model produces more expert-
like concept sequences than would be generated by
chance and also outperforms the SVM model on the
concept sequencing task. For the final output of the
module, we combine the pair-wise judgments into a
complete concept sequence, breaking any ties in the
pair-wise judgments by preferring the order of the
concepts in the output of the core concept identifier.
3http://www.cs.waikato.ac.nz/ml/weka/
16
6 Identifying student misconceptions
The previous components have focused on analyzing
the background knowledge ? finding core concepts
in the domain and selecting an appropriate learning
sequence for these concepts. The current component
focuses on the student essay, using the collected back-
ground knowledge to help analyze the essay and give
feedback.
Given a student essay, the goal of this component
is to identify which sentences in the essay are most
likely to be misconceptions. The task of misconcep-
tion identification is closely related to the task of
textual entailment (Dagan et al, 2006), in which the
goal is to predict if a hypothesis sentence, H, can be
reasonably concluded given another sentence, T. In
misconception identification, the goal is to predict if
a student sentence can be concluded from any com-
bination of the sentences in the domain knowledge,
similar to a textual entailment task with a single H
but many Ts. A student sentence that can not be
concluded from the domain knowledge is likely a
misconception.
6.1 Models
We developed two models for identifying student
misconceptions, inspired by work in textual entail-
ment that showed that a model that simply counts the
words in H that appeared in T, after expanding the
words in T using WordNet, achieves state-of-the-art
performance (Shnarch et al, 2011)4.
The Coverage model scores a student sentence
by counting the number of its words that are also in
some domain sentence. Low-scoring sentences are
likely misconceptions. Formally:
SCORE(s) =
|s ? d|
|s|
d =
?
s??D
EXPAND(s?)
where s is a student sentence (a list of words), D is
the set of domain sentences, and EXPAND performs
lexical expansion on the words of a sentence.
The Retrieval model indexes the domain sen-
tences with an information retrieval system (we use
4The paper also proposes a more elaborate probabilistic
model, but shows that the ?lexical coverage? model we adopt
here is quite competitive both with their probabilistic model and
with the top-performing systems of RTE5 and RTE6.
Lucene5), and scores a student sentence by querying
the index and summing the scores. Formally:
SCORE(s) =
?
s??D
SCORElucene(s, EXPAND(s
?))
where s, D and EXPAND are defined as before, and
SCORElucene is a cosine over TF-IDF vectors6.
For both the Coverage and Retrieval models, we
consider the following lexical expansion techniques
for defining the EXPAND function:
? tokens ? words in the sentence (no expansion)
? tokens, synsets ? words in the sentence, plus
all lemmas of all WordNet synsets of each word
? tokens, synsetsexpanded ? words in the sentence,
plus all lemmas of all WordNet synsets of each
word, plus all lemmas of derived forms, hy-
ponyms or meroynms of the WordNet synsets
? tokens, synsetsexpanded?4 ? words in the sen-
tence, plus all lemmas of all WordNet synsets of
each word, plus all lemmas of WordNet synsets
reachable by a path of no more than 4 links
through derived forms, hyponyms or meroynms
6.2 Evaluation
We evaluate the quality of our misconception identi-
fication models using data collected from the Amer-
ican Association for the Advancement of Science?s
Project 2061 Science Assessment Website7. This
website identifies the main ideas in various topics
under Life Science, Physical Science and Earth Sci-
ence, and for each idea provides several sentences
of description along with its individual concepts and
common student misconceptions.
We used 3 topics (17 ideas, averaging 6.2 descrip-
tion sentences, 7.1 concept sentences and 9.9 miscon-
ception sentences each) as a development set:
CE Cells
AM Atoms, Molecules, and States of Matter
PT Plate Tectonics
We used 11 topics (64 ideas, averaging 5.9 descrip-
tion sentences, 9.4 concept sentences and 8.6 miscon-
ception sentences each) as the test set:
5http://lucene.apache.org
6See org.apache.lucene.search.Similarity javadoc for details.
7http://assessment.aaas.org/
17
Model MAP P@1
Randomly ordered 0.607 0.607
Coverage - tokens 0.647 0.471
Coverage - tokens, synsets 0.633 0.529
Coverage - tokens, synsetsexpanded 0.650 0.471
Coverage - tokens, synsetsexpanded?4 0.690 0.706
Retrieval - tokens 0.665 0.529
Retrieval - tokens, synsets 0.641 0.471
Retrieval - tokens, synsetsexpanded 0.650 0.529
Retrieval - tokens, synsetsexpanded?4 0.684 0.647
Table 4: Development set results for identifying miscon-
ceptions.
EN Evolution and Natural Selection
BF Human Body Systems
IE Interdependence in Ecosystems
ME Matter and Energy in Living Systems
RH Reproduction, Genes, and Heredity
EG Energy: Forms, Transformation, Transfer. . .
FM Force and Motion
SC Substances, Chemical Reactions. . .
WC Weather and Climate: Basic Elements
CL Weather and Climate: Seasonal Differences
WE Weathering, Erosion, and Deposition
For the evaluation, we provide all of the idea?s de-
scription sentences as the domain knowledge, and
combine all of an idea?s concepts and misconcep-
tions into a ?student essay?8. We then ask the system
to rank the sentences in the essay, placing miscon-
ceptions above true concepts. Accuracy at placing
misconceptions at the top of the ranked list is then
measured using mean average precision (MAP) and
precision at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Table 4
shows that on the development set, while all models
outperformed the random ordering baseline?s MAP
(0.607), only models with lexical expansion from
4-link WordNet chains outperformed the baseline?s
P@1 (0.607). The Coverage and Retrieval models us-
ing this expansion technique had comparable MAPs
8These ?student essays? are a naive approximation of real
essays, but the sentences are at least drawn from real student er-
rors. In the future, we hope to create an evaluation corpus where
real student essays have been annotated for misconceptions.
Model MAP P@1
Randomly ordered 0.487 0.487
Coverage - tokens, synsetsexpanded?4 0.603 0.578
Retrieval - tokens, synsetsexpanded?4 0.644 0.625
Table 5: Test set results for identifying misconceptions.
(0.690 vs. 0.684), but the Coverage model had a
higher P@1 (0.706 vs. 0.647). These top two mis-
conception identification models were evaluated on
the test set. Table 5 shows that both models again
outperformed the random ordering baseline, and the
Retrieval model outperformed the Coverage model
(0.644 vs. 0.603 MAP, 0.625 vs. 0.578 P@1).
7 Aligning misconceptions to concepts
The goal of this component is to take the miscon-
ception sentences identified in a student essay and
align them to the core science concepts identified for
the domain. For example, a student misconception
like Earth?s plates cannot bend would be aligned to
a science concept like Mountains form when plate
material slowly bends over time.
7.1 Models
The model for misconception-concept alignment
takes a similar approach to that of the Retrieval
model for misconception identification. The align-
ment model applies lexical expansion to each word
in a core science concept, indexes the expanded con-
cepts with an information retrieval system, and scores
each concept for its relevance to a student misconcep-
tion by querying the index with the misconception
and returning the index?s score for that concept. For-
mally:
SCORE(c) = SCORElucene(m, EXPAND(c))
where m is the query misconception, c is the science
concept, and EXPAND and SCORElucene are defined
as in the Retrieval model for misconception identi-
fication. The concept with the highest score is the
concept that best aligns to the student misconception
according to the model.
For lexical expansion, we consider the same defini-
tions of EXPAND as for misconception identification:
tokens; tokens, synsets; tokens, synsetsexpanded;
and tokens, synsetsexpanded?4.
18
Model MAP P@1
Randomly ordered 0.276 0.276
Alignment - Tokens 0.731 0.639
Alignment - Tokens, synsets 0.813 0.734
Alignment - tokens, synsetsexpanded 0.790 0.698
Alignment - Tokens, synsetsexpanded?4 0.762 0.639
Table 6: Development set results for aligning concepts to
misconceptions.
7.2 Evaluation
We again leverage the AAAS Science Assessments to
evaluate the misconception-concept alignment mod-
els. In addition to identifying key science ideas, and
the concepts and common misconceptions within
each idea, the AAAS Science Assessments provide
links between the misconceptions and the concepts.
Usually there is a single concept to which each mis-
conception is aligned, but the AAAS data aligns as
many as 16 concepts to a misconception in some
cases.
For the evaluation, we give the system one miscon-
ception from an idea, and the list of all concepts from
that idea, and ask the system to rank the concepts9.
If the system performs well, the concepts that are
aligned to the misconception should be ranked above
the other concepts. Accuracy at placing the aligned
concepts at the top of the ranked list is then measured
using mean average precision (MAP) and precision
at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Ta-
ble 6 shows that on the development set, all models
outperformed the random ordering baseline. Lexi-
cal expansion with tokens and synsets achieved the
highest performance, 0.813 MAP and 0.734 P@1.
This model was evaluated on the test set, and Table 7
shows that the model again outperformed the random
ordering baseline, achieving 0.704 MAP and 0.611
P@1. Overall, these are promising results ? given a
student misconception, the model?s first choice for a
concept to address the misconception is helpful more
than 60% of the time.
9As discussed in Section 6.2, there are on average 9.4 con-
cepts per item. This is not too far off from the 10-20 core con-
cepts we typically expect the tutor to extract for each domain.
Model MAP P@1
Randomly ordered 0.259 0.259
Alignment - Tokens, synsets 0.704 0.611
Table 7: Test set results for aligning concepts to miscon-
ceptions.
8 Recommending resources
The goal of this component is to take a set of student
misconceptions, the core science concepts to which
each misconception is aligned, and the pedagogical
ordering of the core science concepts, and recom-
mend digital resources (webpages) to address the
most important of the misconceptions. For example,
a student that believes that water evaporates into the
air only when the air is very warm might be directed
to websites about evaporation and condensation. The
recommended resources are intended to help the stu-
dent quickly locate the concept knowledge necessary
to correct each of their misconceptions.
8.1 Models
The intuition behind our model is simple: sentences
from recommended resources should contain the
same or lexically related terminology as both the
misconception sentences and their aligned concepts.
As a first approach to this problem, we focus on the
overlap between recommended sentences and the
misconception sentences, and use an information re-
trieval approach to build a resource recommender.
First, the user gives the model a set of domain
knowledge webpages, and we use an information re-
trieval system (Lucene) to index each sentence from
each of the webpages. (Note that we index all sen-
tences, not just core concept sentences.) Given a
student misconception, we query the index and iden-
tify the source URL for each sentence that is returned.
We then return the list of the recommended URLs,
keeping only the first instance of each URL if dupli-
cates exist. Formally:
SCORE(url) = max
s?url
SCORElucene(m, s)
where url is a domain resource, s is a sentence from a
domain resource and m is the student misconception.
URLs are ranked by score and the top k URLs are
returned as recommendations.
19
8.2 Evaluation
As a preliminary evaluation of the resource recom-
mendation model, we obtained student misconcep-
tion sentences that had been aligned to concepts in
a knowledge map of plate tectonics (Ahmad, 2009).
The concepts in the knowledge map were originally
drawn from 37 domain webpages, thus each concept
could serve as a link between a student misconcep-
tion and a recommended webpage. For evaluation,
we took all 11 misconceptions for a single student,
where each misconception had been aligned through
the concepts to on average 3.4 URLs. For each mis-
conception, we asked the recommender model to
rank the 37 domain URLs in order of their relevance
to the student misconception.
We expect the final interactive essay writing sys-
tem to return up to k = 5 resources for each mis-
conception, so we evaluated the performance of the
recommender model in terms of precision at five
(P@5). That is, of the top five URLs recommended
by the system, how many were also recommended
by the experts? Averaging over the 11 student mis-
conception queries, the current model achieves P@5
of 32%, an acceptable initial baseline as randomly
recommending resources would achieve only P@5
of 9%.
9 Discussion
In this article, we have presented our initial steps
towards an interactive essay writing system that can
help students identify and remedy misconceptions in
their science knowledge. The system relies on tech-
niques drawn from a variety of areas of natural lan-
guage processing research, including multi-document
summarization, textual entailment and information
retrieval. Each component has been evaluated inde-
pendently and demonstrated promising initial perfor-
mance.
A variety of challenges remain for this effort. The
core concept identification system performs well on
the plate tectonics domain that it was originally de-
veloped for, but poorer on more distant domains,
suggesting the need for more domain-independent
features. The model for sequencing science concepts
pedagogically uses only the most basic of word-based
features, and could potentially benefit from features
drawn from other research areas such as text readabil-
ity. The misconception identification and alignment
models perform well on the AAAS science assess-
ments but have not yet been evaluated on real student
essays, which may require moving from lexical cover-
age models to more sophisticated entailment models.
Finally, the recommender model considers only in-
formation about the misconception sentence (not the
aligned core concept nor the pedagogical ordering of
concepts) and recommends entire resources instead
of directing students to specifically relevant sentences
or paragraphs.
Perhaps the most important challenge for this work
will be moving from evaluating the components in-
dependently to a whole-system evaluation in the con-
text of a real essay writing task. We are currently
designing a study to gather data on students using the
system, from which we hope to derive information
about which components are most reliable or useful
to the students. This information will help guide our
research to focus on improving the components that
yield the greatest benefits to the students.
References
[Ahmad2009] Faisal Ahmad. 2009. Generating conceptu-
ally personalized interactions for educational digital
libraries using concept maps. Ph.D. thesis, University
of Colorado at Boulder.
[Azevedo et al2008] Roger Azevedo, Amy Witherspoon,
Arthur Graesser, Danielle McNamara, Vasile Rus,
Zhiqiang Cai, Mihai Lintean, and Emily Siler. 2008.
MetaTutor: An adaptive hypermedia system for train-
ing and fostering self-regulated learning about complex
science topics. In Meeting of Society for Computers in
Psychology, November.
[Dagan et al2006] Ido Dagan, Oren Glickman, and Ber-
nardo Magnini. 2006. The PASCAL recognising
textual entailment challenge. In Joaquin Quin?onero
Candela, Ido Dagan, Bernardo Magnini, and Florence
d?Alche? Buc, editors, Machine Learning Challenges.
Evaluating Predictive Uncertainty, Visual Object Clas-
sification, and Recognising Tectual Entailment, volume
3944 of Lecture Notes in Computer Science, pages 177?
190. Springer Berlin / Heidelberg.
[de la Chica et al2008a] Sebastian de la Chica, Faisal Ah-
mad, James H. Martin, and Tamara Sumner. 2008a.
Pedagogically useful extractive summaries for science
education. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 177?184, Stroudsburg, PA, USA.
Association for Computational Linguistics.
20
[de la Chica et al2008b] Sebastian de la Chica, Faisal Ah-
mad, Tamara Sumner, James H. Martin, and Kirsten
Butcher. 2008b. Computational foundations for person-
alizing instruction with digital libraries. International
Journal on Digital Libraries, 9(1):3?18, July.
[Graesser et al2004] Arthur Graesser, Shulan Lu, George
Jackson, Heather Mitchell, Mathew Ventura, Andrew
Olney, and Max Louwerse. 2004. AutoTutor: A tutor
with dialogue in natural language. Behavior Research
Methods, 36:180?192.
[Graff2002] David Graff. 2002. English Gigaword. Lin-
guistic Data Consortium.
[NRC1996] National Research Council NRC. 1996.
National Science Education Standards. National
Academy Press, Washington DC.
[Project2061.1993] Project2061. 1993. Benchmarks for
Science Literacy. Oxford University Press, New York,
United States.
[Radev et al2004] Dragomir R. Radev, Hongyan Jing,
Ma?gorzata Stys?, and Daniel Tam. 2004. Centroid-
based summarization of multiple documents. Inf. Pro-
cess. Manage., 40(6):919?938, November.
[Shnarch et al2011] Eyal Shnarch, Jacob Goldberger, and
Ido Dagan. 2011. A probabilistic modeling frame-
work for lexical entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 558?563, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
[Tanaka-Ishii et al2010] K. Tanaka-Ishii, S. Tezuka, and
H. Terada. 2010. Sorting texts by readability. Compu-
tational Linguistics, 36(2):203?227.
[VanLehn et al2002] Kurt VanLehn, Pamela Jordan, Car-
olyn Rose?, Dumisizwe Bhembe, Michael Bo?ttner, Andy
Gaydos, Maxim Makatchev, Umarani Pappuswamy,
Michael Ringenberg, Antonio Roque, Stephanie Siler,
and Ramesh Srivastava. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay writ-
ing. In Stefano Cerri, Guy Gouarde`res, and Fa?bio
Paraguac?u, editors, Intelligent Tutoring Systems, vol-
ume 2363 of Lecture Notes in Computer Science, pages
158?167. Springer Berlin / Heidelberg.
[Wade-Stein and Kintsch2004] David Wade-Stein and
Eileen Kintsch. 2004. Summary Street: Interactive
computer support for writing. Cognition and Instruc-
tion, 22(3):333?362.
21
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering Narrative Containers in Clinical Text
Timothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,
Sameer Pradhan1, Chen Lin1, and Guergana K. Savova1
1 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical School
firstname.lastname@childrens.harvard.edu
2 Center for Computational Language and Education Research, University of Colorado Boulder
steven.bethard@colorado.edu
Abstract
The clinical narrative contains a great deal
of valuable information that is only under-
standable in a temporal context. Events,
time expressions, and temporal relations
convey information about the time course
of a patient?s clinical record that must be
understood for many applications of inter-
est. In this paper, we focus on extracting
information about how time expressions
and events are related by narrative con-
tainers. We use support vector machines
with composite kernels, which allows for
integrating standard feature kernels with
tree kernels for representing structured
features such as constituency trees. Our
experiments show that using tree kernels
in addition to standard feature kernels im-
proves F1 classification for this task.
1 Introduction
Clinical narratives are a rich source of unstruc-
tured information that hold great potential for im-
pacting clinical research and clinical care. These
narratives consist of unstructured natural language
descriptions of various stages of clinical care,
which makes them information dense but chal-
lenging to use computationally. Information ex-
tracted from these narratives is already being used
for clinical research tasks such as automatic phe-
notype classification for collecting disease cohorts
retrospectively (Ananthakrishnan et al, 2013),
which can in turn be used for a variety of studies,
including pharmacogenomics (Lin et al, 2012;
Wilke et al, 2011). Future applications may use
information extracted from the clinical narrative at
the point of care to assist physicians in decision-
making in a real time fashion.
One of the most interesting and challenging as-
pects of clinical text is the pervasiveness of tempo-
rally grounded information. This includes a num-
ber of clinical concepts which are events with fi-
nite time spans (e.g., surgery or x-ray), time ex-
pressions (December, postoperatively), and links
that relate events to times or other events. For ex-
ample, surgery last May relates the time last May
with the event surgery via the CONTAINS relation,
while Vicodin after surgery relates the medication
event Vicodin with the procedure event surgery via
the AFTER relation. There are many potential ap-
plications of clinical information extraction that
are only possible with an understanding of the or-
dering and duration of the events in a clinical en-
counter.
In this work we focus on extracting a particu-
lar temporal relation, CONTAINS, that holds be-
tween a time expression and an event expression.
This level of representation is based on the compu-
tational discourse model of narrative containers
(Pustejovsky and Stubbs, 2011), which are time
expressions or events which are central to a sec-
tion of a text, usually manifested by being rela-
tive hubs of temporal relation links. We argue that
containment relations are useful as an intermediate
level of granularity between full temporal relation
extraction and ?coarse? temporal bins (Raghavan
et al, 2012) like before admission, on admission,
and after admission. Correctly extracting CON-
TAINS relations will, for example, allow for more
accurate placement of events on a timeline, to
the resolution possible by the number of time ex-
pressions in the document. We suspect that this
finer grained information will also be more useful
for downstream applications like coreference, for
which coarse information was found to be useful.
The approach we develop is a supervised machine
18
learning approach in which pairs of time expres-
sions and events are classified as CONTAINS or
not. The specific approach is a support vector ma-
chine using both standard feature kernels and tree
kernels, a novel approach to this problem in this
domain that has shown promise on other relation
extraction tasks.
This work makes use of a new corpus we devel-
oped as part of the THYME1 project (Temporal
History of Your Medical Events) focusing on tem-
poral events and relations in clinical text. This cor-
pus consists of clinical and pathology notes on col-
orectal cancer from Mayo Clinic. Gold standard
annotations include Penn Treebank-style phrase
structure in addition to clinically relevant temporal
annotations like clinical events, temporal expres-
sions, and various temporal relations.
2 Background and Related Work
2.1 Annotation Methodology
The THYME annotation guidelines2 detail the ex-
tension of TimeML (Pustejovsky et al, 2003b)
to the annotations of events, temporal expres-
sions and temporal relations in the clinical do-
main. In summary, an EVENT is anything that is
relevant to the clinical timeline. Temporal expres-
sions (TIMEX3s) in the clinical domain are simi-
lar to those in the general domain with two excep-
tions. First, TimeML sets and frequencies occur
much more often in the clinical domain, especially
with regard to medications and treatments (Clar-
itin 30mg twice daily). The second deviation is a
new type of TIMEX3 ? PREPOSTEXP which covers
temporally complex terms like preoperative, post-
operative, and intraoperative.
EVENTs and TIMEX3s are ordered on a timeline
through temporal TLINKs which range from fairly
coarse (the relation to document time creation) to
fairly granular (the explicit pairwise TLINKs be-
tween EVENTs and/or TIMEX3s). Of note for this
work, the CONTAINS relation between a TIMEX3
and an EVENT means that the span of the EVENT
is completely within the span of the TIMEX3. The
interannotator agreement F1-score for CONTAINS
for the set of documents used here was 0.60.
2.2 Narrative Containers
One relatively new concept for marking temporal
relations is that of narrative containers, as in Puste-
1http://clear.colorado.edu/TemporalWiki
2Annotation guidelines are posted on the THYME wiki.
jovsky and Stubbs (2011). Narrative containers
are time spans which are central to the discourse
and often subsume multiple events and time ex-
pressions. They are often anchored by a time ex-
pression, though more abstract events may also act
as anchors. Using the narrative container frame-
work significantly reduces the number of explicit
TLINK annotations yet retains a relevant degree of
granularity enabling inferencing.
Consider the following clinical text example
with DocTime of February 8.
The patient recovered well after her ini-
tial first surgery on December 16th to
remove the adenocarcinoma, although
on the evening of January 3rd she was
admitted with a fever and treated with
antibiotics.
There are three narrative containers in this snip-
pet ? (1) the broad period leading up to the docu-
ment creation time which includes the events of re-
covered and adenocarcinoma, (2) December 16th,
which includes the events of surgery and remove,
and (3) January 3rd, which includes the events of
admitted, fever, and treated.
Using only the relation to the document creation
time would provide too coarse of a timeline result-
ing in collapsing the three narrative containers (the
coarse time bins of Raghavan et al (2012) would
collapse all events into the before admission cat-
egory). On the other hand, marking explicit links
between every pair of events and temporal expres-
sions would be tedious and redundant. In this ex-
ample, there is no need to explicitly mark that, for
instance, fever was AFTER surgery, because we
know that the fever happened on January 3rd and
that the surgery happened on December 16th, and
that January 3rd is AFTER December 16th. With
the grouping of EVENTs in this way, we can infer
the links between them and reduce annotator ef-
fort. Narrative containers strike the right balance
between parsimony and expressiveness.
2.3 Related Work
Of course, the possibility of annotating temporal
containment relations was allowed by even the ear-
liest versions of the TimeML specification using
TLINKs with the relation type INCLUDES. How-
ever, TimeML is a specification not a guideline,
and as such, the way in which temporal relations
have been annotated has varied widely and no
19
corpus has previously been annotated with narra-
tive containers in mind. In the TimeBank corpus
(Pustejovsky et al, 2003a), annotators annotated
only a sparse, mostly disconnected graph of the
temporal relations that seemed salient to them. In
TempEval 2007 and 2010 (Verhagen et al, 2007;
Verhagen et al, 2010), annotators annotated only
relations in specific constructions ? e.g. all pairs
of events and times in a sentence ? and used a re-
stricted set of relation types that excluded the IN-
CLUDES relation. TempEval 2013 (UzZaman et
al., 2013) allowed INCLUDES relations, but again
only in particular constructions or when the rela-
tion seemed salient to the annotators. The 2012
i2b2 Challenge3, which provided TimeML anno-
tations on clinical data, annotated the INCLUDES
relation, but merged it with other relations for the
evaluation due to low inter-annotator agreement.
Since no narrative container-annotated corpora
exist, there are also no existing models for extract-
ing narrative container relations. However, we
can draw on the various methods applied to re-
lated temporal relation tasks. Most relevant is the
work on linking events to timestamps. This was
one of the subtasks in TempEval 2007 and 2010,
and systems used a variety of features including
words, part-of-speech tags, and the syntactic path
between the event and the time (Bethard and Mar-
tin, 2007; Llorens et al, 2010). Syntactic path
features were also used in the 2012 i2b2 Chal-
lenge, where they provided gains especially for
intra-sentential temporal links (Xu et al, 2013).
Recent research has also looked to syntac-
tic tree kernels for temporal relation extraction.
Mirroshandel et al (2009) used a path-enclosed
tree (i.e., selecting only the sub-tree containing
the event and time), and used various weighting
scheme variants of this approach on the Time-
Bank (Pustejovsky et al, 2003a) and Opinion4
corpora. Hovy et al (2012) used a flat tree struc-
ture for each event-time pair, including only token-
based information (words, part of speech tags) be-
tween the event and time, and found that adding
such tree kernels on top of a baseline set of fea-
tures improved event-time linking performance on
the TempEval 2007 and Machine Reading cor-
pora (Strassel et al, 2010). While Mirroshandel et
al. saw improvements using a representation with
syntactic structure, Hovy et al used the flat tree
3http://i2b2.org/NLP/TemporalRelations
4Also known as the AQUAINT TimeML corpus ?
http://www.timeml.org
structure because they found that ?using a full-
parse syntactic tree as input representation did not
help performance.? Thus, it remains an open ques-
tion exactly where and when syntactic tree kernels
will help temporal relation extraction.
3 Methods
Inspired by this prior work, we treat the narrative
container extraction task as a within-sentence rela-
tion extraction task between time and event men-
tions. For each sentence, this approach iterates
over every gold standard annotated EVENT, pair-
ing it with each TIMEX3 in the sentence, and uses
a supervised machine learning algorithm to clas-
sify each pair as related by the CONTAINS relation
or not. Training examples are generated in the
same way, with pairs corresponding to annotated
links marked as positive examples and all others
marked as negative. We investigate a variety of
features for the classifier as well as a variety of
tree kernel combinations.
This straightforward approach does not address
all relation pairs, setting aside event-event rela-
tions and inter-sentential relations, which are both
likely to require different approaches.
3.1 SVM with Tree Kernels
The machine learning approach we use is support
vector machine (SVM) with standard feature ker-
nels, tree kernels, and composite kernels that com-
bine the two. SVMs are used extensively for clas-
sification tasks in natural language processing, due
to robust performance and widely available soft-
ware packages. We take advantage of the ability
in SVMs to represent structured features such as
trees using convolution kernels (Collins and Duffy,
2001), also known as tree kernels. This kernel
computes similarity between two tree structures
by computing the number of common sub-trees,
with a weight parameter to discount the influence
of larger structural similarities. The specific for-
malism we use is sometimes called a subset tree
kernel (Moschitti, 2006), which checks for simi-
larity on subtrees of all sizes, as long as each sub-
tree has its production rule completely expanded.
A useful property of kernels is that a linear com-
bination of two kernels is guaranteed to be a ker-
nel (Cristianini and Shawe-Taylor, 2000). In ad-
dition, the product of two kernels is also a ker-
nel. This means that it is simple to combine tradi-
tional feature-based kernels used in SVMs (linear,
20
polynomial, radial basis function) with tree ker-
nels representing structural information. This ap-
proach of using composite kernels has been widely
used in the task of relation extraction where syn-
tactic information is presumed to be useful, but is
hard to represent as traditional numeric features.
We investigate a few different composite ker-
nels here, including a linear combination:
KC(o1, o2) = ? ?KT (t1, t2) +KF (f1, f2) (1)
where a composite kernel KC operates on objects
oj composed of features fj and tree tj , by adding
a tree kernel KT weighted by ? to a feature kernel
KF . We also use a composite kernel that takes the
product of kernels:
KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)
Sometimes it is beneficial to make use of multi-
ple syntactic ?views? of the same instance. Below
we will describe many different tree representa-
tions, and the tree kernel framework allows them
to all be used simultaneously, by simply summing
the similarities of the different representations and
taking the combined sum as the tree kernel value:
KT ({t
1
1, t
2
1 . . . , t
N
1 }, {t
1
2, t
2
2, . . . , t
N
2 }) =
N?
i=1
KT (t
i
1, t
i
2) (3)
where i indexes the N different tree views. In all
kernel combinations we compute the normalized
version of both the feature and tree kernels so that
they can be combined on an even footing.
The actual implementations we use for train-
ing are the SVM-LIGHT-TK package (Mos-
chitti, 2006), which is a tree kernel extension to
SVMlight (Joachims, 1999). At test time, we
use the SVM-LIGHT-TK bindings of the ClearTK
toolkit (Ogren et al, 2009) in a module built on
top of Apache cTAKES (Savova et al, 2010), to
take advantage of the pre-processing stages.
3.2 Flat Features
The flat features developed for the standard fea-
ture kernel include the text of each argument as a
whole, the tokens of each argument represented as
a bag of words, the first and last word of each ar-
gument, and the preceding and following words of
each argument as bags of words. The token con-
text between arguments is also represented using
the text span as a whole, the first and last words,
the set of words represented as a bag of words, and
the distance between the arguments. In addition,
part of speech (POS) tag features are extracted for
each mention, with separate bag of POS tag fea-
tures for each argument. The POS features are
generated by the cTAKES POS tagger.
We also include semantic features of each argu-
ment. For event mentions, we include a feature
marking the contextual modality, which can take
on the possible values Actual, Hedged, Hypothet-
ical, or Generic, which is part of the gold stan-
dard annotations. This feature was included as it
was presumed that actual events are more likely
to have definite time spans, and thus be related
to times, than hypothetical or generic mentions of
events. For time mentions we include a feature for
the time class, with possible values of Date, Time,
Duration, Quantifier, Set, or Prepostexp. The time
class feature was used as it was hypothesized that
dates and times are more likely to contain events
than sets (e.g., once a month).
3.3 Tree Kernel Representations
We leverage existing tree kernel representations
for this work, using some directly and others as
starting point to a domain-specific representation.
First, we take advantage of the (relatively) flat
structured tree kernel representations of Hovy et
al. (2012). This representation uses lexical items
such as POS tags rather than constituent struc-
ture, but places them into an ordered tree struc-
ture, which allows tree kernels to use them as a
bag of items while also taking advantage of order-
ing structure when it is useful. Figure 1 shows an
example tree for an event-time pair for which a re-
lation exists, where the lexical information used is
POS tag information for each term (the represen-
tation that Hovy et al found most useful). We also
used a version of this representation where the sur-
face form is used instead of the POS tag.
While Hovy et al showed positive results using
this representation over just standard features, it is
still somewhat constrained in its ability to repre-
sent long distance relations. This is because the
subset tree kernel compares only complete rule
productions, and with long distance relations a flat
tree structure will have a production that is too big
to learn. Alternatively, tree kernel representations
can be based on constituent structure, as is com-
mon in the relation extraction literature. This will
21
BOP
Event-Actual
TOK
VBN
TOK
TO
TOK
VB
TOK
NN
Timex-Date
TOK
JJ
TOK
NN
BOW
Event-Actual
TOK
scheduled
TOK
to
TOK
undergo
TOK
surgery
Timex-Date
TOK
next
TOK
week
Figure 1: Two trees indicating the flat tree kernel
representation. Above is the bag of POS tags ver-
sion; below is the bag of words version.
hopefully allow for the representation of longer
distance relations by taking advantage of syntactic
sub-structure with smaller productions. The rep-
resentations used here are known as Feature Trees
(FT), Path Trees (PT) and Path-Enclosed Trees
(PET).
The Feature Tree representation takes the en-
tire syntax tree for the sentence containing both
arguments and inserts semantic information about
those arguments. That information includes the ar-
gument type (EVENT or TIMEX) as an additional
tree node above the constituent enclosing the argu-
ment. We also append semantic class information
to the argument (contextual modality for events,
time class for times), as in the flat features.
The Feature Tree representation is not com-
monly used, as it includes an entire sentence
around the arguments of interest, and that may in-
clude a great deal of unrelated structure that adds
noise to the classifier. Here we include it in an at-
tempt to get to the root of an apparent discrepancy
in the tree kernel literature, as explained in Sec-
tion 2, in which Hovy et al (2012) report a nega-
tive result and Mirroshandel et al (2009) report a
positive result for using constituency structure in
tree kernels for temporal relation extraction.
The Path Tree representation uses a sub-tree of
the whole constituent tree, but removes all nodes
that are not along the path between the two argu-
ments. Path information has been used in standard
feature kernels (Pradhan et al, 2008), with each
individual path being a possible boolean feature.
VP
Arg1-Event-Actual
arg1
S
VP
VP
Arg2-Timex-Date
arg2
Figure 2: Path Tree (PT) representation
Another representation making use of the path tree
takes contiguous subsections of the path tree, or
?path n-grams,? in an attempt to combat the spar-
sity of using the whole path (Zheng et al, 2012).
By using the path representation with a tree ker-
nel, the model should get the benefit of all different
sizes of path n-grams, up to the size of the whole
path. This representation is augmented by adding
in argument nodes with event and time features, as
in the Feature Tree. Unlike the Feature Tree and
the PET below, the Path Tree representation does
not include word nodes, because the important as-
pect of this representation is the labels of the nodes
on the path between arguments. Figure 2 shows an
example of what this representation looks like.
The Path-Enclosed Tree representation is based
on the smallest sub-tree that encloses the two pro-
posed arguments. This is a representation that has
shown value in other work using tree kernels for
relation extraction (Zhang et al, 2006; Mirroshan-
del et al, 2009). The information contained in
the PET representation is a superset of that con-
tained in the Path Tree representation, since it in-
cludes the full path between arguments as well
as the structure between arguments and the ar-
gument text. This means that it can take into
account path information while also considering
constituent structure between arguments that may
play a role in determining whether the two ar-
guments are related. For example, temporal cue
words like after or during may occur between ar-
guments and will not be captured by Path Trees.
Like the PT representation, the PET representa-
tion is augmented with the semantic information
specified above in the Feature Tree representation.
Figure 3 shows an example of this representation.
22
VP
Arg1-Event-Actual
VBN
scheduled
VP
TO
to
VP
VB
undergo
NP
NN
surgery
Arg2-Timex-Date
NP
JJ
next
NN
week
Figure 3: Path-Enclosed Tree representation
4 Evaluation
The corpus we used for evaluations was described
in Section 2. There are 78 total notes in the corpus,
with three notes for each of 26 patients. The data
is split into training (50%), development (25%),
and test (25%) sections based on patient number,
so that each patient?s notes are all in the same
section. The combined training and development
set used for final training consists of 4378 sen-
tences with 49,050 tokens, and 7372 events, 849
time expressions, and 2287 CONTAINS relations.
There were 774 positive instances of CONTAINS
in the training data, with 1513 negative instances.
For constituent structure and features we use the
gold standard treebank and event and time features
from our corpus. Preliminary work suggests that
automatic parses from cTAKES do not harm per-
formance very much, but the focus of this work is
on the relation extraction so we use gold standard
parses. All preliminary experiments were done us-
ing the development set for testing.
We designed a set of experiments to exam-
ine several hypotheses regarding extraction of the
CONTAINS relation and the efficacy of different
tree kernel representations. The first two config-
urations test simple rule-based baseline systems,
CLOSEST-P and CLOSEST-R, for distance-related
decision rule systems meant to optimize precision
and recall, respectively. CLOSEST-P hypothesizes
a CONTAINS link between every TIMEX3 and the
closest annotated EVENT, which will make few
links overall. CLOSEST-R hypothesizes a CON-
TAINS link between every EVENT and the closest
TIMEX3, which will make many more links.
The next configuration, Flat Features, uses the
token and part of speech features along with ar-
gument semantics features, as described in Sec-
tion 3. While this feature set may not seem ex-
haustive, in preliminary work many traditional re-
lation extraction features were tried and found to
not have much effect. This particular configura-
tion was tested because it is most comparable to
the bag of word and bag of POS kernels from
Hovy et al (2012), and should help show whether
the tree kernel is providing anything over an equiv-
alent set of basic features.
We then examine several composite kernels, all
using the same feature kernel, but using different
tree kernel-based representations. First, we use a
composite kernel which uses the bag of word and
bag of POS tree views, as in Hovy et al (2012).
Next, we add in two additional tree views to the
tree kernel, Path-Enclosed Tree and Path Tree,
which are intended to examine the effect of using
traditional syntax, and the long distance features
that they enable. The final experimental config-
uration replaces the PET and PT representations
from the last configuration with the Feature Tree
representation. This tests the hypothesis that the
difference between positive results for tree kernels
in this task (as in, say, Mirroshandel et al (2009))
and negative results reported by Hovy et al (2012)
is the difference between using a full-parse tree
and using standard sub-tree representations.
For the rule-based systems, there are no param-
eters to tune. Our machine-learning systems are
based on support vector machines (SVM), which
require tuning of several parameters, including
kernel type (linear, polynomial, and radial basis
function), the parameters for each kernel, and c,
the cost of misclassification. Tree kernels intro-
duce an additional parameter ? for weighting large
structures, and the use of a composite kernel in-
troduces parameters for which kernel combination
operator to use, and how to weight the different
kernels for the sum operator.
For each machine learning configuration, we
performed a large grid search over the combined
parameter space, where we trained on the train-
ing set and tested on the development set. For
the final experiments, the parameters were chosen
that optimized the F1 score on the development
set. Qualitatively, the parameter tuning strongly
favored configurations which combined the ker-
nels using the sum operator, and recall and pre-
cision were strongly correlated with the SVM pa-
rameter c. Using these parameters, we then trained
23
on the combined training and development sets
and tested on the official test set.
4.1 Evaluation Metrics
The state of evaluating temporal relations has been
evolving over the past decade. This is partially
due to the inferential properties of temporal rela-
tions, because it is possible to define the same set
of relations using different set of axioms. To take
a very simple example, given a gold set of rela-
tions A<B and B<C, and given the system output
A<B, A<C and B<C, if one were to compute a
plain precision/recall metric, then the axiom A<C
would be counted against the system, when one
can easily infer from the gold set of relations that
it is indeed correct. With more relations the infer-
ence process becomes more complex.
Recently there has been some work trying
to address the shortcomings of the plain F1
score (Muller and Tannier, 2004; Setzer et al,
2006; UzZaman and Allen, 2011; Tannier and
Muller, 2008; Tannier and Muller, 2011). How-
ever, the community has not yet come to a consen-
sus on the best evaluation approach. Two recent
evaluations, TempEval-3 (UzZaman et al, 2013)
and the 2012 i2b2 Challenge (Sun et al, 2013),
used an implementation of the proposal by (Uz-
Zaman and Allen, 2011). However, as described
in Cherry et al (2013), this algorithm, which uses
a greedy graph minimization approach, is sensi-
tive to the order in which the temporal relations
are presented to the scorer. In addition, the scorer
is not able to give credit for non-redundant, non-
minimum links (Cherry et al, 2013) as with the
the case of the relation A<C mentioned earlier.
Considering that the measures for evaluating
temporal relations are still evolving, we decided to
use plain F-score, with recall and precision scores
also reported. This score is computed across all
intra-sentential EVENT-TIMEX3 pairs in the gold
standard, where precision = # correct predictions# predictions ,
recall = # correct predictions# gold standard relations , and F1 score =
2?precision?recall
precision+recall .
4.2 Experimental Results
Results are shown in Table 1. Rule-based base-
lines perform reasonably well, but are heavily bi-
ased in terms of precision or recall. The ma-
chine learning baseline cannot even obtain the
same performance as the CLOSEST-R rule-based
system, though it is more balanced in terms of pre-
System Precision Recall F1
CLOSEST-P 0.754 0.537 0.627
CLOSEST-R 0.502 0.947 0.656
Flat Features (FF) 0.705 0.593 0.645
FF+Bag Trees (BT) 0.649 0.728 0.686
FF+BT+PET+PT 0.770 0.707 0.737
FF+BT+FT 0.691 0.691 0.691
Table 1: Table of results of main experiments.
cision and recall. Using a composite kernel which
adds in the flat token-based tree kernels improves
performance over the standard feature kernel by
4.1 points. Adding in the Path Tree and Path-
Enclosed Tree constituency-based trees along with
the flat trees improves F1 score to our best result
of 73.7. Finally, replacing PT and PET representa-
tions with the Feature Tree representation does not
offer any performance improvement over the Flat
Features + Bag Trees configuration.
4.3 Error Analysis
We performed error analysis on the outputs of the
best-performing system (FF+BT+PET+PT in Ta-
ble 1). First, we note that the parameter search
was optimized for F1. This resulted in the highest-
scoring configuration using a composite kernel
with the sum operator, polynomial kernel for the
secondary kernel, ? = 0.5, tree kernel weight (T )
of 0.1, and c = 10.0. This high value of c and low
value of T results in higher precision and lower
recall, but there were configurations with lower c
and higher T which made the opposite tradeoff,
with only marginally worse F1-score. For the pur-
poses of error analysis, however, this configuration
leads to a focus on false negatives.
First, the false positives contained many rela-
tions that were legitimately ambiguous or possible
annotator errors. An example ambiguous case is
She is currently being treated on the Surgical Ser-
vice for..., in which the system generates the re-
lation CONTAINS(currently, treated), but the gold
standard labels as OVERLAP. This example is am-
biguous because it is not clear from just the lin-
guistic context whether the treatment is wholly
contained in the small time window denoted by
currently, or whether it started a while ago or will
continue into the future. There are many similar
cases where the event is a disease/disorder type,
and the specific nature of the disease is impor-
tant to understanding whether this is a CONTAINS
24
or OVERLAP relation, specifically understanding
whether the disease is chronic or more acute.
Another source of false positives were where
the event and time were clearly related, but not
with CONTAINS. In the example reports that she
has been having intermittent bleeding since May
of 1998, the term since clearly indicates that this
is a BEGINS-ON relation between bleeding and
May of 1998. This is a case where having other
temporal relation classifiers may be useful, as they
can compete and the relation can be assigned to
whichever classifier is more confident.
False negatives frequently occurred in contexts
where the event and time were far apart. Syn-
tactic tree kernels were introduced to help im-
prove recall on longer-distance relations, and were
successful up to a limit. However, certain ex-
amples are so far apart that the algorithm may
have had difficulty sorting noise from important
structure. For example, the system did not find
the CONTAINS(October 27, 2010, oophorectomy)
relation in the sentence:
October 27, 2010, Dr. XXX performed
exploratory laparotomy with an trans-
verse colectomy and Dr. YYY performed
a total abdominal hysterectomy with a
bilateral salpingo-oophorectomy.
Here, while the date may be part of the same sen-
tence as the event, the syntactic relation between
the pair is not what makes the relation; the date is
acting as a kind of discourse marker that indicates
that the following events are contained. This sug-
gests that discourse-level features may be useful
even for the intra-sentential classification task.
Other false negatives occurred where there was
syntactic complexity, even on shorter examples.
The subset tree kernel used here matches com-
plete rule productions, and across complex struc-
ture with large productions, the chances of finding
similarity decreases substantially. Thus, events
within coordination or separated from the time by
clause breaks are more difficult to relate to the
time due to the multiple different ways of relating
these different syntactic elements.
Finally, there are some examples where the an-
chor of a narrative container is an event with mul-
tiple sub-events. In these cases, the system per-
forms well at relating a time expression to the an-
chor event, but may miss the sub-events that are
farther away. This is a case where having an event-
event TLINK classifier, then applying determinis-
tic closure rules, would allow a combined system
to link the sub-events to the time expression.
5 Discussion and Conclusion
In this paper we have developed a system for auto-
matically identifying CONTAINS relations in clini-
cal text. The experiments show first that a machine
learning approach that intelligently integrates con-
stituency information can greatly improve perfor-
mance over rule-based baselines. We also show
that the tree kernel approach, which can model se-
quence better than a bag of tokens-style approach,
is beneficial even when it uses the same features.
Finally, the experiments show that choosing the
correct representation is important for tree kernel
approaches, and specifically that using a full parse
tree may give inferior performance compared to
sub-trees focused on the structure of interest.
In general, there is much work to be done in the
area of representing temporal information in clin-
ical records. Many of the inputs to the algorithm
described in this paper need to be extracted auto-
matically, including time expressions and events.
Work on relations will focus on adding features
to represent discourse information and richer rep-
resentation of event semantics. Discourse infor-
mation may help with the longer-distance errors,
where the time expression acts almost as a topic
for an extended description of events. Better un-
derstanding of event semantics, such as whether
a disease is chronic or acute, or typical duration
for a treatment, may help constrain relations. In
addition, we will explore the effectiveness of us-
ing dependency tree structure, which has been use-
ful in the domain of extracting relations from the
biomedical literature (Tikk et al, 2013).
Acknowledgements
The work described was supported by Tempo-
ral History of Your Medical Events (THYME)
NLM R01LM010090 and Integrating Informat-
ics and Biology to the Bedside (i2b2) NCBO
U54LM008748. Thanks to the anonymous re-
viewers for thorough and insightful comments.
References
Naushad UzZaman, Hector Llorens, et al 2013. Semeval-
2013 task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
25
Volume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Computa-
tional Linguistics.
Ashwin N Ananthakrishnan, Tianxi Cai, et al 2013. Improv-
ing case definition of Crohn?s disease and ulcerative col-
itis in electronic medical records using natural language
processing: a novel informatics approach. Inflammatory
bowel diseases.
Steven Bethard and James H. Martin. 2007. CU-TMP: Tem-
poral relation classification using syntactic and semantic
features. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
129?132.
Colin Cherry, Xiaodan Zhu, et al 2013. A la recherche du
temps perdu: extracting temporal relations from medical
text in the 2012 i2b2 NLP challenge. Journal of the Amer-
ican Medical Informatics Association, March.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Neural Information Processing
Systems.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Dirk Hovy, James Fan, et al 2012. When did that happen?:
linking events and relations to timestamps. In Proceedings
of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 185?193.
Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. Universita?t Dortmund.
Chen Lin, Helena Canhao, et al 2012. Feature engineering
and selection for rheumatoid arthritis disease activity clas-
sification using electronic medical records. In Proceed-
ings of ICML Workshop on Machine Learning for Clinical
Data.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (english and spanish): Evaluating crfs and seman-
tic roles in tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 284?291.
Association for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for clas-
sifying temporal relations between events. Proc. of the
PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Ma-
chine Learning: ECML 2006, pages 318?329. Springer.
Philippe Muller and Xavier Tannier. 2004. Annotating and
measuring temporal relations in texts. In Proceedings of
the 20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009. ClearTK: a framework for statistical natural lan-
guage processing. In Unstructured Information Manage-
ment Architecture Workshop at the Conference of the Ger-
man Society for Computational Linguistics and Language
Technology, 9.
Sameer S Pradhan, Wayne Ward, and James H Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
James Pustejovsky and Amber Stubbs. 2011. Increasing in-
formativeness in temporal annotation. In Proceedings of
the 5th Linguistic Annotation Workshop, pages 152?160.
James Pustejovsky, Patrick Hanks, et al 2003a. The time-
bank corpus. In Corpus linguistics, volume 2003, page 40.
James Pustejovsky, Jose? Casta no, et al 2003b. Timeml:
Robust specification of event and temporal expressions in
text. In Fifth International Workshop on Computational
Semantics (IWCS-5).
Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.
2012. Temporal classification of medical events. In Pro-
ceedings of the 2012 Workshop on Biomedical Natural
Language Processing, pages 29?37. Association for Com-
putational Linguistics.
Guergana K. Savova, James J. Masanz, et al 2010. Mayo
clinical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and appli-
cations. J Am Med Inform Assoc, 17(5):507?513.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple. 2006.
The role of inference in the temporal annotation and anal-
ysis of text. Language Resources and Evaluation, 39(2-
3):243?265, February.
Stephanie Strassel, Dan Adams, et al 2010. The DARPA
machine reading program - encouraging linguistic and rea-
soning research with a series of reading tasks. In Proceed-
ings of the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics
Association, April.
Xavier Tannier and Philippe Muller. 2008. Evaluation met-
rics for automatic temporal annotation of texts. Proceed-
ings of the Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08).
Xavier Tannier and Philippe Muller. 2011. Evaluating tem-
poral graphs built from texts via transitive reduction. J.
Artif. Int. Res., 40(1):375413, January.
Domonkos Tikk, Ille?s Solt, et al 2013. A detailed error anal-
ysis of 13 kernel methods for protein-protein interaction
extraction. BMC bioinformatics, 14(1):12.
Naushad UzZaman and James Allen. 2011. Temporal eval-
uation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Lan-
guage Technologies, page 351?356.
Marc Verhagen, Robert Gaizauskas, et al 2007. Semeval-
2007 task 15: Tempeval temporal relation identification.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, et al 2010. Semeval-2010 task
13: Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Uppsala,
Sweden, July. Association for Computational Linguistics.
RA Wilke, H Xu, et al 2011. The emerging role of electronic
medical records in pharmacogenomics. Clinical Pharma-
cology & Therapeutics, 89(3):379?386.
Yan Xu, Yining Wang, et al 2013. An end-to-end system to
identify temporal relation in discharge summaries: 2012
i2b2 challenge. Journal of the American Medical Infor-
matics Association : JAMIA.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syn-
tactic features for relation extraction using a convolution
tree kernel. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of the
ACL, pages 288?295.
Jiaping Zheng, Wendy W Chapman, et al 2012. A sys-
tem for coreference resolution for the clinical narrative.
Journal of the American Medical Informatics Association,
19:660?667.
26
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
