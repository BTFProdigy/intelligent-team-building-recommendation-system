Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1103?1111, Prague, June 2007. c?2007 Association for Computational Linguistics
Building Domain-Specific Taggers without Annotated (Domain) Data 
John E. Miller1 Manabu Torii2 K. Vijay-Shanker1 
1Computer & Information Sciences University of Delaware Newark, DE 19716 {jmiller,vijay}@cis.udel.edu 
2Biostatistics, Bioinformatics and Biomathematics  Georgetown University Medical Center  Washington, DC 20057 mt352@georgetown.edu  Abstract Part of speech tagging is a fundamental component in many NLP systems. When taggers developed in one domain are used in another domain, the performance can degrade considerably. We present a method for developing taggers for new domains without requiring POS annotated text in the new domain. Our method involves using raw domain text and identifying related words to form a domain specific lexicon. This lexicon provides the initial lexical probabilities for EM training of an HMM model. We evaluate the method by apply-ing it in the Biology domain and show that we achieve results that are comparable with some taggers developed for this domain.  1 Introduction As Natural Language Processing (NLP) technol-ogy advances and more text becomes available, it is being applied more and often in specialized do-mains. Part of Speech (POS) tagging is often a fundamental component to these NLP applications and hence its accuracy can have a significant im-pact on the application?s success. The success that the taggers have attained is often not replicated when the domain is changed. Degradation of accu-racy in a new domain can be overcome by devel-oping an annotated corpus for that specific domain, e.g., as in the Biology domain. However, this solu-tion is feasible only if there is sufficient interest in the use of NLP technology in that domain, and there are sufficient funding and resources. In con-trast, our approach is to use existing resources, and 
rapidly develop taggers for new domains without using the time and effort to develop annotated data. In this work, we use the Wall Street Journal (WSJ) corpus (Marcus et al 1993) and large amounts of domain-specific raw text to develop taggers. We evaluate our methodology in the Biol-ogy domain and show the resulting performance is competitive with some taggers built with super-vised learning for that domain. Also, we note that the accuracy of taggers trained on the WSJ corpus drops off considerably when applied to this domain. Smith et al (2005) report that the Brill tagger (1995) has an accuracy of 86.8% on 1000 sen-tences taken from Medline, and that the Xerox tag-ger (Cutting et al.1992) has an accuracy of 93.1% on the same sentences. They attribute this drop off to the fact that only 57.8% of the 10,000 most fre-quent words can be found in WSJ corpus. This ob-servation provides further impetus to developing lexicon for taggers in the new domains.  In the next section, we discuss our general ap-proach. The details of the EM training of the HMM tagger are given in Section 3. Section 4 provides details of how a domain specific lexicon is created. Next, we discuss the evaluation of our models and analysis based on the results. Section 6 discusses related work and those works from which we have taken some ideas. Section 7 has some concluding remarks.  2 Basic Methodology Inadequate treatment of domain-specific vocabu-lary is often the primary cause in the degradation of performance when a tagger trained in one genre of text is ported to a new domain. The significance of out-of-vocabulary words has been noted in re-duced accuracy of NLP components in the Biology 
1103
domain (e.g., Lease and Charniak, 2005; Smith et al 2004). The handling of domain-specific vocabu-lary is the focus of our approach.  It is quite common to use suffix information in the prediction of POS tags for occurrences of new words. However, its effectiveness may be limited in English, which is not a highly inflected language. However, even for English, we find that not only can suffix information be used online during tag-ging, but also the presence or absence of morpho-logically related words can provide considerable information to pre-build a lexicon that associates possible tags with words.  Consider the example of the word ?broaden?. While the suffix ?en? may be utilized to predict the likelihood of verbal tags (VB and VBP) for the word during tagging, if we were to build a lexicon offline, the existence of the words ?broadened?, ?broadening?, ?broadens? and ?broad? give further evidence to treat ?broaden? as a verb. This type of information has been used before in (Cucerzan and Yarowsky, 2000).  In the above example, the presence or absence of words with the suffix morphemes suggests POS tag information in two ways: 1) The presence of a suffix morpheme in a word suggests a POS tag or a small set of POS tags for the word. This is the type of information most taggers use to predict tags for unknown words during the tagging process; 2) The presence of the morpheme can also indicate possi-ble tags for the words it attaches to. For example, the derivational morpheme ?ment? indicates ?gov-ernment? is likely to be an NN and also that the word it attaches to, ?govern? is likely to be a verb. Inflectional and derivational morphemes don?t at-tach to words of just any POS category; they are particular. Thus, we can propose the possibility of JJ (adjective) to ?broad? and VB or VBP to ?gov-ern? (based on the fact the derivational morphemes ?en? and ?ment? attach to them) even though by themselves they don?t have any suffix information that might be indicative of JJ and VB or VBP.  Additional suffixes (that may or may not be taken from a standard list of English inflectional and derivational morphemes) can also be used. As an example, the suffix ?ate? can be associated with a small set of tags: VB or VBP (?educate?, ?cre-ate?), JJ (?adequate?, ?appropriate?), and NN (?candidate?, ?climate?). Note the possibility or impossibility of the addition of ?tion? and ?ly? can help distinguish between the verbal and adjectival 
situations. In contrast, most taggers that use just suffix information during the tagging process will need strong contextual information (i.e., tags of nearby words) in making their prediction for each occurrence, as such suffixes can be associated with multiple tags. To utilize such information, we need a diction-ary of words in the domain for which we are inter-ested in building a tagger. Such a dictionary will allow us to propose possible tags for a domain word such as ?phosphorylate?. If we can verify whether words like ?phosphorylation?, ?phos-phorylates?, and ?phosphorylately,? are available in the domain then we can obtain considerable in-formation regarding the possible tags that can be associated with ?phosphorylate?. But we cannot assume the availability of a dictionary of words in the domain. However, it would suffice to have a large text corpus, which we call Text-Lex. We use it as a proxy for a domain dictionary by obtaining a list of words and their relative frequency of ap-pearance in the domain.  Rather than using manually developed rules that assign possible tags for words based on the pres-ence or absence of related words, we wish to apply a more empirical methodology. Since this sort of information is specific to a language rather than a domain, we can use an annotated corpus in another domain to provide exemplars. We use the WSJ (Marcus et al 1993) corpus, a POS annotated cor-pus, for this purpose.  For example, we can see that ?phosphorylate? in the Biology domain and ?cre-ate? in the WSJ corpus are similar in the sense both take on ?tion?, ?ed?, and ?ing? suffixes but not ?ly? for instance. Since the WSJ corpus would provide POS tag information for ?create?, we can use it to inform us for ?phosphorylate?. The above method forms the basis for our de-termination of the set of tags that are to be associ-ated with the domain words. However, the actual tag to be assigned for an occurrence in text de-pends on the context of use. We capture this in-formation by using a first-order HMM tagger model. For the transitional probabilities, we begin by using WSJ-based probabilities as a starting point and then adjust to the new domain by using a domain specific text and using EM training. EM also allows for adjusting lexical probabilities de-rived using WSJ words as exemplars. We call the domain specific text used for training of our HMM tagger as Text-EM. While this could be the same 
1104
as Text-Lex, we distinguish the two since Text-EM could be smaller than Text-Lex. From Text-Lex, we only extract a list of words and their frequency of occurrences. In contrast, we use Text-EM as a text and hence as a sequence of words.  In this work, the set of suffixes that we use is adapted those found in a GRE preparation web-page (DeForest, 2000). A few additional suffixes were obtained from the online English Dictionary AllWords.com (2005). In the future we expect to consider automatic mining of useful suffixes from a domain. Furthermore, prefixes are also useful for our purposes. However apart from a few prefixes used in hyphenated words, we haven?t yet incorpo-rated prefix information in a systematic way into our framework.  In this paper, our evaluation domain is molecu-lar biology. Large amounts of text are easily avail-able in the form of Medline abstracts. We use only about 1% of the Medline text database for Text-Lex. Another reason for selecting this evaluation domain is that we have a considerable amount POS-annotated text in this domain, and the most recent techniques of supervised POS tag learning have been used in developing taggers for this do-main. This allows us to evaluate our tagger using the annotated text for evaluation as well as to com-pare our tagger with others developed for this do-main. The POS-annotated text we use is the well-known GENIA (Tateisi et al 2003) corpus that was developed at University of Tokyo.  3 Expectation Maximization Training Our tagger is a first-order Hidden Markov Model (HMM) tagger that is trained using Expectation Maximization (EM) since we do not assume exis-tence of annotated data in the new domain.1 Al-though we use the GENIA corpus, we take only the raw text and strip off the annotated information for obtaining the Text-EM. Our HMM is based on bi-gram modeling and hence our transitional prob-abilities correspond to P(t | t?) where t and t? are POS tags. The emissions that label the transition edges will be discussed in the next section and in-clude domain words as well as certain types of ?coded words?.                                                   1 We considered a 2nd order model as well, but early work showed negligible advantage predicting to the same training set. Following Wang and Schuurmans (2005) we chose to focus on quality of estimation over model complexity. 
The initial transitional probabilities are not ran-domly chosen but rather taken from the WSJ cor-pus. If we take the transitional probabilities as a representation of syntactic preferences, then EM learning using Text-EM may be taken as adjust-ment of the grammatical preferences in the WSJ corpus to those in the new domain. In order to ad-just the grammatical preference to the new domain, we start from smoothed WSJ bigram probabilities. If we started from unsmoothed WSJ bigram prob-abilities, then EM would not allow us to account for transitions that are not observed in the WSJ corpus. For example, in scientific text, transition from RRB (the right round bracket) to VBG may be possible, while it does not occur in the WSJ corpus. Hence, we smooth the WSJ bigram prob-abilities with WSJ unigram probabilities.  We compute smoothed initial bigram probabili-ties as P(t | t?) = ? PWSJ(t | t?) + (1-?) PWSJ(t), where ?=0.9. We felt employing techniques sug-gested in (Brants, 2000) gave too high a preference for unigram probabilities.  The initial emit probability is obtained from the domain text Text-Lex. The process is described in the next section. This information is derived purely from suffix and suffix distribution, or from ortho-graphic information and does not account for the actual context of occurrences in the domain text. We take this suffix-based (and orthographic-based) emit probabilities as reasonable initial lexical probabilities. EM training will adjust them as nec-essary.  We made one minor modification to the stan-dard forward-backward EM algorithm. We dampen the change in transitional and emit probabilities for each iteration. Significant differences in lexical probabilities between the new domain and WSJ can make undue changes in transitional probabili-ties and this in turn can further lead the lexical probabilities to head in the wrong direction. By adding a damping factor, we can prevent the unsu-pervised training to spiral out of control. Hence we let the new transitional probability be given by  P(t | t?) = ? PNEW(t | t?) + (1-?) POLD(t | t?)  where POLD represents the transitional probability in the previous iteration and PNEW represents the probability by standard use of forward-backward algorithm. We use a damping factor of 0.5 for both transitional and emit probabilities. For the emit probabilities, this has the effect of moderating POS 
1105
preferences derived from the training data and pre-serving words and POSes from the lexicon for use in the test set. Even with the damping factor, EM learning fol-lowed the pattern of ?Early Maximum? described by Elworthy (1994), where with good initial esti-mates EM learning only improves accuracy for a few iterations.  For our EM training, we fixed it-eration 2 as our ?best? EM trained model. 4 Development of the Lexicon and Initial Probabilities As noted earlier, we use a domain text, Text-Lex, to develop the initial lexical probabilities for the HMM. The essential process is as follows. Let a word w appear a sufficient number of times in Text-Lex (at least 5 times). We look in Text-Lex for related words in order to assign a feature vector with this word. Each feature is written as ?x+y, where x and y represent suffixes or the empty string (here represented as _).  Features: The feature ?x+y represents the word formed by replacing some suffix x in w by some suffix y. Consider the word ?creation?. ??ion+_? corresponds to the stem word ?create? and ??ion+ion? corresponds to the word ?creation? itself. The feature ??ion+ed? captures information about the word ?created? whereas the feature ?-_+s? cor-responds to word ?creations?. Now consider a word like ?history?. While this might have non-zero values for ?-y+ic? (historic) or ?-_+s? (histories), we are likely to set zero value for ??ory+_? (unless ?hist? or ?histe? is found in Text-Lex). This zero value represents the fact that although ?history? has ?ory? as a suffix, it has no stem. Such a distinction (whether or not there is a stem) bears much information for suffixes like ?ate? and ?ory?.  We use suffix classes rather than actual suffixes as we believe this provides a more appropriate level of abstraction. Given a word w with a suffix x (for a word with no suffix from our list of suf-fixes, x is taken to be _. i.e., empty string), we ex-amine whether removal of x from w leads to an-other word by using a few basic variations that can be found in any rudimentary exposition on English morphology. For example, for the suffix ?ed?, we attempt to replace ?ied? with ?y? which relates ?purified? with ?purify? and recognizes the spell-ing alternation of i/y. Thus for the word ?purify? 
the feature ?-+ed? represents the presence of ?pu-rified? since ?+ed? represents the suffix class rather than the actual suffix. Similarly, we also consider removal of a suffix and, if necessary, add-ing an ?e? to see if such a word exists. This allows us to relate ?creation? with ?create? or ?activate? with ?active?. Also doubling of a few consonants is attempted to relate ?occurrence? and ?occur?. Finally, when a word could have two suffixes, the word is considered to always have the longer func-tional suffix. Hence, we consider ?government? to have ?ment? suffix rather than ?ent? suffix.  Feature Vectors: There are two different types of vectors we use for any word, one called Bin (for binary count) and other called RFreq (for relative frequency). In the Bin vector associated with ?creation?, all these four features will get the value one, assuming that the four corresponding words are found in Text-Lex. On the other hand, assuming ?creatory? is not found in Text-Lex, the feature ?-ion+ory? would get a zero value.  For RFreq vector, instead of ones and zeros, we first start with the frequency of occurrences of each word and then normalize so that the sum of all fea-ture values is one. Thus, for example, a word with 4 features having non-zero frequencies of 10, 20, 30 and 40 will have the respective values set to 0.1, 0.2, 0.3 and 0.4. A word with four features having non-zero frequency, which are 1, 2, 3 and 4, will also have same 4 relative frequency values.  Our intuition is that the Bin vector is helpful in determining the set of tags that can be associated with a word and that the RFreq vector can aug-ment this information regarding the likelihood of these tags. For example, a one for the ?-ing+_? feature in a Bin vector (thus disqualifying a word like ?during?) may be sufficient to predict VBG, JJ and NN tags. However, this may not suffice to provide the ordering of likelihood among these tags for this word. On the other hand, it seems to be the case that when the ?ing? form appears far more often than the ?ed? form, then the NN tag is most likely. But if the ?ed? form is more frequent, then VBG is most likely. Examples in the WSJ corpus include ?smoking?, ?marketing?, ?index-ing?, and ?restructuring? for the first kind, and ?calling?, ?counting?, ?advising?, and ?noting? for the second kind.  Exemplars in WSJ: Given a word w from Text-Lex, we look for similar words from the WSJ cor-pus. Even though the set of words used in this cor-
1106
pus may differ substantially from the domain text, our hypothesis is that words with similar suffix distribution will have similar POS tag assignments regardless of the domain. We follow Cucerzan and Yarowsky (2000) in using the kNN method for finding similar words, but we differ in details of the construction of the feature vectors and distance computation. For the word w we create the Bin and RFreq vectors based on distribution of words in Text-Lex. Following the same method, we create the Bin and RFreq vectors for a word v in the WSJ corpus by using the distributions in the WSJ cor-pus. Then we compute BinDist(w,v) as the number of features in which the two Bin vectors  differ. A similar RFDist is defined as a weighted sum of two distances: the first distance is L1-norm dis-tance based on values of features for which both words have non-zero values for and the second distance is based on values of features for which one word has a zero value and other does not. Thus, if the two words? RFreq vectors are 
? 
< w
1
,...,w
n
>  and 
? 
< v
1
,...,v
n
>respectively then  
? 
RFsame(w,v) = w
i
? v
i
w
i
?0?v
i
?0
?
  
? 
RFdiff (w,v) =
w
i
? v
i
w
i
=0? v
i
?0
?
+ w
i
? v
i
w
i
?0? v
i
=0
?
 and,   
? 
RFDist ( w,v ) = RFsame( w,v ) +?RFdiff ( w,v ) For RFDist(.), we used ? =2. Given a word w, we find the 5 nearest neighbors from the WSJ cor-pus and use their average lexical probabilities to obtain the lexical probabilities for w. We investi-gate the use of Bin vector information and RFreq vector information for computing the distances (i.e., BinDist(.) and RFDist(.)) as well as a hybrid measure that combines these two distances.  We also considered smoothing the lexical prob-abilities obtained in the above fashion. Let w be a word for which the above method suggests tags t1,?,tn in order of likelihood (t1 is most probable).  Then we consider sqrt-score(ti)= 
? 
n +1? i . We then assign probabilities based on this score after normalizing them so that the probabilities for the n tags will sum to 1. Thus, for example, if a word w has three possible tags, no matter what the original lexical probabilities were determined to be, if t1 is 
determined to be most probable, then P(t1|w) will be 0.418 by this method. The second most prob-able tag will be assigned 0.341.   The intuition behind this square root smoothing method is that this smoothing may be appropriate for low frequency words, where empirical prob-abilities based purely on a kNN basis may not be entirely appropriate if the new domain is very dif-ferent. The drawback of course is that if there is sufficient information, we lose useful information by such flattening. And when a tag is significantly more probable for a word then we lose this vital information. For example, the word ?high? is mostly annotated as JJ in WSJ corpus but RB and NN are also possible. Square root smoothing will flatten this distribution considerably. Nevertheless, we wish to investigate whether this method of smoothing the distribution is enough in conjunc-tion with EM. EM adjusts the probability from observing the number and context of occurrences in the domain text.2  Coded Words: No matter how large Text-Lex is, there will be words that do not appear a suffi-cient number of times (we take this number to be 5). We aggregate such words according to their suffixes, if they correspond to one of the prede-fined suffixes. Then each word with suffix x is considered to be an instance of a ?coded? word SFX-x. If a word does not have any of these suf-fixes then they fall into the coded class unknown. For each such coded word, we assign the tags and probabilities based on similarly aggregated words in the WSJ corpus. We have two other broad classes of words that we treat differently. Coded words are formed based on orthographic characteristics, which include but are not limited to Greek letters, Roman numerals, digits, upper or lower case single letters, upper case letter sequences, cardinals, certain prefix words, and their combinations.  Since they are rela-tively easy to tag, we do not use the WSJ corpus for them but handle it programmatically. Finally, if a word occurs often in WSJ or is assigned tags such as CD, FW, MD, PRP, DT, WDT, etc. (tags which can?t be predicted by means of suffix or suf-fix-related words), we add this word together with the tags and probability into the domain lexicon that we are building.                                                   2 We also considered linear and square functions for smooth-ing while reporting only the sqrt results in section 5. 
1107
5 Evaluation and Analysis As noted earlier, our evaluation is on molecular biology text. For Text-Lex, we used 133,666 ti-tles/abstracts of research papers, a small fraction of the Medline database available from the National Library of Medicine. These abstracts were con-tained in just five of the 500 compressed data files in the 2006 version of the Medline database. These abstracts cover topics more broadly in Biomedicine and not just molecular biology.  On the other hand, we use for Text-EM, text which can be regarded to be in a subfield of molecular biology.  Text-EM is the text from the GENIA corpus (version 3.02) described in (Tateisi et al 2003). This corresponds to about 2000 abstracts, which are annotated with POS tag information (using the same tags used in the WSJ corpus). We use a 5-fold cross-validation, i.e., 5 partitions are formed and experiments conducted 5 times and results av-eraged. For each test partition, the remainder parti-tions are used for ?training?. In our case, this is unsupervised since we use EM and hence we to-tally disregard the POS tag information that is as-sociated with the words. We note that both the text for EM training as well as for testing come from the same domain.   We first evaluate the process of building the lexicon. This time we consider the entire GENIA corpus and not any partition. We first considered all words in the GENIA corpus for which we can expect our kNN method to assign a tag. Hence all words that would be treated as coded words are ignored. For each such word, we consider the tags assigned to them in the GENIA corpus and form pairs <w,t>.  We are interested in the word type and not token and hence we will not have any mul-tiple occurrences of a pair <w,t>. Our kNN method identifies 96.3% of these pairs; we can think of this as recall. This makes our approach effective, espe-cially given the fact that the kNN method only as-signs 1.92 tags on an average to these words in the GENIA corpus. Next considering all words appear-ing in the GENIA corpus, our lexicon includes a correct tag in 99.0% of the cases on a word-token basis. These results are summarized below.  Characteristic Statistic kNN Recall (word-type) 96.3% Average Number Tags/Word 1.92 tags Lexicon Recall (word-token) 99.0% 
We now turn to the evaluation of the accuracy of our HMM. As mentioned earlier, these results are based on 5-fold cross-validation experiments. The best results (95.77%) were obtained for the case where we took the lexical probabilities directly from kNN using only RFDist and by discarding all tags assigned with probability less than 0.02.3  These results compare favorably to other taggers developed for the Biology domain. The MedPost tagger (see Section 6) achieved an accuracy of 94.1% when we applied it to the GENIA abstracts. The PennBioIE tagger (see Section 6) achieved an accuracy of 95.1%. Note that output from the PennBioIE tagger is not fully compatible with GENIA annotation due to some differences in its tokenization.  Even if the differences in accuracies can be discounted due to tokenization or even sys-tematic differences in annotation between the train-ing and test corpora, our main point is that our re-sults compare favorably (our tagger competitive) with taggers that were developed for the Biomedi-cine domain using supervised training.   These results are summarized in the table below.  POS Tagger %Accuracy Our HMM (5-fold) 95.77% MedPost 94.1% PennBioIE 95.1% GENIA supervised 98.26%  MedPost seems intended to cover all of Bio-medicine, since its lexicon is based on the 10,000 most frequently occurring words from Medline and for which the set of possible tags were manually specified. The PennBioIE tagger was developed using 315 Medline abstracts using another subfield of molecular biology. None of these accuracies however are as high as those of the GENIA tagger (Tsuruoka et al 2005) which was trained (supervised) using GENIA cor-pus and uses a machine learning model more so-phisticated than the simple first-order HMM tagger we use. This model considers more features includ-ing words to the right. The best results (98.26%) were obtained when lexicon from three different sources were aggregated.                                                    3 Banko and Moore (2004) showed only slight improvement in tag accuracy between .01 and .1 cutoffs with a lexicon built from annotated data. We opted for the .02 cutoff because of our ?noisier? lexicon. 
1108
Returning to the results for our taggers, we also tried BinDist in the kNN method, with and without square root smoothing. These results were typi-cally less than the above-mentioned result. We also compared using a square root smooth on RFDist obtaining results approximately 1% lower than without the square root smooth. We next present some examples that illustrate strengths and weaknesses of the current model. An example that shows that EM training makes good adjustment to the domain is the improvement in tagging of verbal categories. We conducted a de-tailed error analysis on one of the cross-validation partitions and noted that the accuracy on all verbal POS tags improved after EM training. A notewor-thy case is the improvement in tagging of VBP originally misclassified as VB. Since most English words that are VB can also be VBP, and since they are annotated more frequently in WSJ as VB, the initial lexicon usually has a higher probability as-signed to VB for most words. As EM training pro-gresses, we noted that the frequency of VBP mistagged as VB decreases. Similarly, misclassifi-cations of VBG as NN also drops in the final model (by 40.3% on Text-EM) as compared to the initial model based on WSJ transitional probabili-ties and initial lexicon derived using WSJ words as exemplars. Previously, in the context of parsing Biomedical text, Lease and Charniak (2004) mention the oc-currences of sequences of multiple NN is more frequent in the GENIA corpus than in the WSJ corpus and that it could lead to parsing errors. We didn?t observe this problem here, but rather the contrary situation where many JJs were initially mistagged as NN.  About 22% of these misclassifi-cations are corrected after EM training.  While our model adjusts well in these cases to the new domain, sometimes the drift leads to worse performance. An example is in the misclassifica-tion of VBN as JJ. The most frequent word for which this misclassification occurs in the word ?activated?. These misclassifications occur in the context such as ?the activated cells?. The use of VBN rather than JJ is hard to determine on basis of just surface features and perhaps has to do more with the meaning of the word. In supervised set-ting, if sufficient such cases were annotated then this would be learned. But in an unsupervised set-ting this turns out to be a problem case. Despite the fact that RFDist predicted VBN as most probable 
tag for ?activated?, EM training makes this situa-tion worse.  Analysis of words with most frequent errors re-vealed many cases from orthographic coded words. Many occurrences of single lower case letters (which could have LS, SYM or NN tags) were la-beled as LS whereas the GENIA tagging used NN. Our model tagged ?+/-? always as SYM whereas because of the context of use, GENIA annotations were CC. (In fact, GENIA does not appear to use the SYM tag.) Similarly, ?<? and ?>? were often mistagged as SYM by our model whereas based on context they are annotated as JJR.  6 Related Work The impact of out-of-vocabulary words on NLP applications has been noted before. The degrada-tion in performance of components, which were trained on the WSJ corpus, but used on biomedical text has been noted (Lease and Charniak, 2004, Smith et al 2005). Smith et al (2005) use this ob-servation in the design of their POS tagger, Med-Post, by building a Markov model with a lexicon containing the 10,000 most frequent words from Medline, and using annotated text from the Bio-medical text for supervised training.  There are many unsupervised approaches to POS tagging. We focus now on those that are most closely related to our work and contain ideas that have influenced this work. There have been many uses of EM training to build HMM taggers (Kupiec, 1992; Elworthy, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Banko and Moore (2004) achieved better accuracy by restrict-ing the set of possible tags that are associated with words. By eliminating possibilities that may appear rarely with a word, they reduce the chances of un-supervised training spiraling along an unlikely path.  We believe by using our approach we con-siderably reduce the set of tags to what is appropri-ate for each word. Further, we too remove any tag associated with low probability by kNN method.  Usually these tags are noise introduced by some inappropriate exemplar.  Wang and Schuurman (2005) suggest that EM algorithm be modified such that at any iteration the unigram tag probability be held constant to the true probability for each tag. Again, this might serve to stop a drift in unsupervised methods towards mak-ing a tag?s probability become larger than it should 
1109
be. However, the true probability cannot be known ahead of time and certainly not in a new domain. While a WSJ bigram probability need not reflect the corresponding preferences in the new domain, our use of starting from WSJ probabilities and then damping changes to transition probabilities was motivated by a similar concern of not letting a drift towards making some (bigram) tags too frequent during EM iterations.  Using suffixation patterns for purposes of pre-dicting POS tags has been considered before. Al-though as far as we know, we are the first to apply it for domain adaptation purposes. Schone and Ju-rafsky (2001) consider clusters of words (obtained by some ?perfect? clustering algorithm) and then compute a measure of how ?affixy? a cluster is. For example, a cluster containing words ?climb? and ?jump? may be related by suffixing operation +s to a cluster that contains words ?climbs? and ?jumps?. The percentage of words in a cluster that are so related provides a measure of how ?affixy? a cluster. This together with five other attributes of clusters (such as whether words in a cluster pre-cede those of another cluster, optionality) and lan-guage universals induce POS tags for these clusters from corpora. This method does not use POS tagged corpora (although in the reported experi-ment the initial ?perfect? clusters were obtained from the Brown corpus using the POS tag informa-tion). In contrast, we use the POS tagged WSJ cor-pus to assist in the induction of tag information for our lexicon. In this respect, our method is closer to the approach of Cucerzan and Yarowsky (2000). Our use of the kNN method to identify tags and their probabilities for words was inspired by this work.  However, their use of kNN method was in the context of supervised learning. The method was applied for handling words unseen in the train-ing data. The estimated probabilities were used during the tagging process. Instead of just applying the method for unknown words, i.e., words not present in the training data, our approach is to cre-ate the entire lexicon in the new domain. As Lease and Charniak (2004), among others, have noted, the distribution of NN tag sequences as well as tag distributions in the Biomedical domain could differ from WSJ text. Since our aim is to adjust to the new domain, we employed unsupervised learning in the form of EM training, unlike the supervised tagging model development approach of Cucerzan and Yarowsky. Another significant difference is 
that their method determines nearest neighbors not only on the basis of suffix-related words but also on the basis of nearby words context. Since our motivation, on the other hand, is to move to a new domain, we didn?t consider detection of similarity on the basis of word contexts. In contrast, we have shown that the approach of identifying words on the basis of suffixation patterns and using them as exemplars can be applied effectively even when the domain of application is substantially different from the text (the WSJ corpus) providing the ex-emplars. 7 Conclusions As NLP technology continues to be applied in new domains, it becomes more important to consider the issue of portability to new domains. To cope with domain-specific vocabulary and also different use of vocabulary in a new domain, we exploited suffix information of words. While use of suffix information per se has been employed in many ex-isting POS taggers, its use is often limited to an online manner, where each word is examined inde-pendently from the existence of its morphologi-cally related words. As shown in (Cucerzan and Yarowsky, 2000), such information can provide considerable information to build a lexicon that associates possible tags with words. However, we use this information only to provide the initial val-ues. We apply EM algorithm to adjust these initial probabilities to the new domain.  The results in Section 5 show that we achieve good performance in the evaluation domain, which is comparable with two recently developed taggers for this domain. We also show in section 5 exam-ples of how EM unlearns some WSJ bias and ad-justs to the new domain. While we introduce a damping factor to slow down changes in iterations of EM training, we believe there is scope for fur-ther improvement to minimize drift. Furthermore, there is scope to improve our kNN method as dis-cussed at the end of Section 5. In the future, we also expect to consider methods that may auto-matically mine suffixes in a new domain and use these domain-specific suffixes. We used the kNN method to associate words in the new domain with possible POS tags.  Despite the often-stated notion that English is not morphologically rich, we find that suffix-based methods can still help make significant inroads. 
1110
Our method offers the chance to develop good tag-gers for specialized domains. For example, the GENIA corpus and PennBioIE corpus are speciali-zations within molecular biology, but taggers de-veloped on one corpus degrades in performance on the other. Using our method, we could use differ-ent Text-EM for these specializations even if we retain Medline as Text-Lex. In the same way, we could develop a tagger for the medical domain, which has a distinct vocabulary from biology.  References Banko, M. and Moore, R.C. 2004.  Part of Speech Tag-ging in Context. In Proceedings, 20th International Conference on Computational Linguistics (Coling 2004), Geneva, Switzerland, pp.556-561. Baum, L. 1972. An inequality and associated maximiza-tion technique in statistical estimation for probabilis-tic functions of a Markov process. Inequalities 3:1-8.  Brants, T. 2000. TNT - a statistical part-of-speech tag-ger.  In Proceedings of the Sixth Applied Natural Language Processing Conference ANLP-2000.   Brill, E. 1995. Transformation-based error-driven learn-ing and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.   Cucerzan, S. and Yarowsky, D. 2000. Language inde-pendent minimally supervised induction of lexical probabilities. Proceedings of ACL-2000, Hong Kong, pages 270-277. Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P. 1992. A practical part of sppech tagger. Proceedings of 3rd Conference on Applied Natural Language Process-ing, 53-58.  DeForest, J. 2000. Graduate Record Exam Suffixed web page. Michigan State University. http://www.msu.edu/~defores1/gre/sufx/gre_suffx.htm Elworthy, D. 1994. Does Baum-Welch re-estimation help taggers. In Proceedings of the Fourth Confer-ence on Applied Natural Language Processing, ACL. Kulick, S., Bies, A., Liberman, M., Mandel, M.,  McDonald, R., Palmer, M., Schein, A. and Ungar, L. Integrated Annotation for Biomedical Information Extraction. HLT/NAACL, 2004. Kupiec, J. 1992. Robust Part-of-speech Tagging Using a Hidden Markov Model. Computer Speech and Lan-guage, 6. 
Lease, M. and Charniak, E. 2005. Parsing Biomedical Literature.  IJCNLP-2005: 58-69.   Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993. Building a large annotated corpus of English: The Penn Treebank.  Computational Linguistics, 19:313-330.   PennBioIE. 2005. Mining The Bibliome Project. http://bioie.ldc.upenn.edu/. Schone, P. and Jurafsky, D. 2001, Language Independ-ent Induction of Part of Speech Class Labels Using Language Universals. IJCAI Workshop on Text Learning: Beyond Supervision.  Smith, L., Rindflesch, T., Wilbur, W.J. 2004. MedPost: a part-of-speech tagger for biological text.  Bioin-formatics 20 (14):2320-2321.   Smith, L., Rindflesch, T., Wilbur, W.J. 2005. The im-portance of the lexicon in tagging biomedical text. Natural Language Engineering 12(2) 1-17. Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J. 2003. The GENIA corpus: Medline ab-stracts annotated with linguistic information.  In: Third meeting of SIG on Text Mining, Intelligent Systems for Molecular Biology (ISMB).   Tsuruoka, Y., Tateishi, Y., Kim, J. D., Ohta, T., McNaught, J., Ananiadou, S., and Tsujii, J.. 2005. Developing a Robust Part-of-Speech Tagger for Biomedical Text, Advances in Informatics - 10th Panhellenic Conference on Informatics, LNCS 3746: 382-392. Wang, Q. and Schuurmans, D. 2005. Improved estima-tion for unsupervised part-of-speech tagging. In IEEE NLP-KE www.AllWords.com. 2005. English Dictionary and Language Guide. AllSites LLC. www.AllSitesllc.com  
1111
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 118?119,
New York City, June 2006. c?2006 Association for Computational Linguistics
Rapid Adaptation of POS Tagging for Domain Specific Uses John E. Miller1 Michael Bloodgood1 Manabu Torii2 K. Vijay-Shanker1 1Computer & Information Sciences 2Biostatistics, Bioinformatics and Biomathematics University of Delaware Georgetown University Medical Center Newark, DE 19716 Washington, DC 20057 {jmiller,bloodgoo,vijay}@cis.udel.edu mt352@georgetown.edu 
1 Introduction Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and ques-tion answering.  When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramati-cally.  We present a methodology for rapid adapta-tion of POS taggers to new domains.  Our technique is unsupervised in that a manually anno-tated corpus for the new domain is not necessary.  We use suffix information gathered from large amounts of raw text as well as orthographic infor-mation to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain.   Many machine-learning and statistical tech-niques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank (Marcus et al 1993). Most state-of-the-art POS taggers use two main sources of information: 1) Information about neighboring tags, and 2) Infor-mation about the word itself. Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and Transformation Based Learning (Brill, 1995).  In moving to a new domain, performance can degrade dramatically because of the increase in the unknown word rate as well as domain-specific word use. We improve tagging performance by attacking these problems. Since our goal is to em-ploy minimal manual effort or domain-specific knowledge, we consider only orthographic, inflec-tional and derivational information in deriving POS. We bypass the time, cost, resource, and con-tent expert intensive approach of annotating a cor-pus for a new domain. 
2 Methodology and Experiment The initial components in our POS tagging process are a lexicon and part of speech (POS) tagger trained on a generic domain corpus. The lexicon is updated to include domain specific information based on suffix rules applied to an un-annotated corpus. Documents in the new domain are POS tagged using the updated lexicon and orthographic information. So, the POS tagger uses the domain specific updated lexicon, along with what it knows from generic training, to process domain specific text and output POS tags. In demonstrating feasibility of the approach, we used the fnTBL-1.0 POS tagger (Ngai and Florian, 2001) based on Brill?s Transformation Based Learning (Brill, 1995) along with its lexicon and contextual rules trained on the Wall Street Journal corpus.  To update the lexicon, we processed 104,322 abstracts from five of the 500 compressed data files in the 2005 PubMed/Medline database (Smith et al 2004). As a result of this update, coverage of words with POS tags from the lexicon increased from 73.0% to 89.6% in our test corpus. Suffix rules were composed based on informa-tion from Michigan State University?s Suffixes and Parts of Speech web page for Graduate Record Exams (DeForest, 2000). The suffix endings indi-cate the POS used for new words. However, as seen in the table of suffix examples below, there can be significant lack of precision in assigning POS based just on suffixes.  Suffix POS #uses/ %acc ize; izes VB VBP; VBZ 23/100% ous JJ 195/100% er, or; ers, ors NN; NNS 1471/99.5% ate; ates VB VBP 576/55.7% 
118
Most suffixes did well in determining the actual POS assigned to the word. Some such as ?-er? and ?-or? had very broad use as well. ?-ate? typically forms a verb from a noun or adjective in a generic domain. However in scientific domains it often indicates a noun or adjective word form. (In work just begun, we add POS assignment confirmation tests to suffix rules so as to confirm POS tags while maintaining our domain independent and unsupervised analysis of un-annotated corpora.) Since the fnTBL POS tagger gives preliminary assignment of POS tags based on the first POS listed for that word in the lexicon, it is vital that the first POS tag for a common word be correct. Words ending in ?-ing? can be used in a verbal (VBG), adjectival (JJ) or noun (NN) sense. Our intuition is that the ?-ed? form should also appear often when the verbal sense dominates. In contrast, if the ratio heavily favors the ?-ing? form then we expect the noun sense to dominate.  We incorporated this reasoning into a computa-tionally defined process which assigned the NN tag first to the following words: binding, imaging, learning, nursing, processing, screening, signal-ing, smoking, training, and underlying. Only un-derlying seems out of place in this list.  In addition to inflectional and derivational suf-fixes, we used rules based on orthographic charac-teristics. These rules defined proper noun and number or code categories. 3 Results and Conclusion For testing purposes, we used approximately half the abstracts of the GENIA corpus (version 3.02) described in (Tateisi et al 2003). As the GENIA corpus does not distinguish between common and proper nouns we dropped that distinction in evalu-ating tagger performance.  POS tagging accuracy on our GENIA test set (second half of abstracts) consisting of 243,577 words is shown in the table below. Source Accuracy Original fnTBL lexicon 92.58% Adapted lexicon (Rapid) 94.13% MedPost 94.04% PennBioIE1 93.98%                                                            1 Note that output from the tagger is not fully compatible with GENIA annotation. 
The original fnTBL tagger has an accuracy of 92.58% on the GENIA test corpus showing that it deals well with unknown words from this domain. Our rapid adaptation tagger achieves a modest 1.55% absolute improvement in accuracy, which equates to a 21% error reduction.   There is little difference in performance be-tween our rapid adaptation tagger and the MedPost (Smith et al 2004) and PennBioIE (Kulick et al 2004) taggers. The PennBioIE tagger employs maximum entropy modeling and was developed using 315 manually annotated Medline abstracts. The MedPost tagger also used domain-specific annotated corpora and a 10,000 word lexicon, manually updated with POS tags. We have improved the accuracy of the fnTBL-1.0 tagger for a new domain by adding words and POS tags to its lexicon via unsupervised methods of processing raw text from the new domain. The accuracy of the resulting tagger compares well to those that have been trained to this domain using annotation effort and domain-specific knowledge.  References Brill, E. 1995. Transformation-based error-driven learn-ing and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.   DeForest, Jessica. 2000. Graduate Record Exam Suffix web page. Michigan State University. http:// www.msu.edu/~defores1/gre/sufx/gre_suffx.htm.  Kulick, S., Bies, A., Liberman, M., Mandel, M., McDonald, R., Palmer, M., Schein, A., Ungar, L. 2004. Integrated annotation for biomedical informa-tion extraction. HLT/NAACL-2004: 61-68. Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993. Building a large annotated corpus of English: The Penn Treebank.  Computational Linguistics, 19:313-330.   Ngai, G. and Florian, R. 2001.  Transformation-based learning in the fast lane.  In Proceedings of North America ACL 2001(June): 40-47.   Smith, L., Rindflesch, T., Wilbur, W.J. 2004. MedPost: a part-of-speech tagger for bioMedical text.  Bioin-formatics 20 (14):2320-2321.   Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J. 2003. The GENIA corpus: Medline ab-stracts annotated with linguistic information.  In: Third meeting of SIG on Text Mining, Intelligent Systems for Molecular Biology (ISMB).   
119
BioNLP 2007: Biological, translational, and clinical language processing, pages 179?180,
Prague, June 2007. c?2007 Association for Computational Linguistics
 Adaptation of POS Tagging for Multiple BioMedical Domains John E. Miller1 Manabu Torii2 K. Vijay-Shanker1 1Computer & Information Sciences University of Delaware Newark, DE 19716 {jmiller,vijay}@cis.udel.edu 2Biostatistics, Bioinformatics and Biomathematics Georgetown University Medical Center  Washington, DC 20057 mt352@georgetown.edu   1 Introduction Part of Speech (POS) tagging is often a prerequi-site for tasks such as partial parsing and informa-tion extraction. However, when a POS tagger is simply ported to another domain the tagger?s accu-racy drops. This problem can be addressed through hand annotation of a corpus in the new domain and supervised training of a new tagger. In our meth-odology, we use existing raw text and a generic POS annotated corpus to develop taggers for new domains without hand annotation or supervised training. We focus in particular on out-of-vocabulary words since they reduce accuracy (Lease and Charniak. 2005; Smith et al 2005).  There is substantial information in the deriva-tional suffixes and few inflectional suffixes of English.  We look at individual words and their suffixes along with the morphologically related words to build a domain specific lexicon contain-ing POS tags and probabilities for each word.  2 Adaptation Methodology Our methodology is described in detail in Miller et al(2007) and summarized here: 1) Process ge-neric POS annotated text to obtain state and lexical POS tag probabilities. 2) Obtain a frequency table of words from a large corpus of raw sub-domain text. 3) Construct a partial sub-domain lexicon matching relative frequencies of morphologically related words with words from the generic anno-tated text averaging POS probabilities of the k nearest neighbors. 4) Combine common generic words and orthographic word categories with the partial lexicon making the sub-domain lexicon. 5) Train a first order Hidden Markov Model (HMM) by Expectation Maximization (EM). 6) Apply the Viterbi algorithm with the HMM to tag sub-domain text. 
3 Adaptation to Multiple Domains Molecular Biology Domain: We used the Wall Street Journal corpus (WSJ) (Marcus et al 1993) as our generic POS annotated corpus. For our raw un-annotated text we used 133,666 abstracts from the MEDLINE distribution covering molecular biology and biomedicine sub-domains. We split the GENIA database  (Tateisi et al 2003) into training and test portions and ignored the POS tags for training. We ran a 5-fold cross validation study and obtained an average accuracy of 95.77%.  Medical Domain: Again we used the WSJ as our generic POS annotated corpus. For our raw un-annotated text we used 164,670 abstracts from the MEDLINE distribution with selection based on 83 journals from the medical domain. For our HMM EM training we selected 1966 abstracts (same journals). For evaluation purposes, we selected 1932 POS annotated sentences from the MedPost (Smith et al 2004) distribution (same journals). The MedPost tag set coding was converted to the Penn Treebank tag set using the utilities provided with the MedPost tagger distribution. We obtained an accuracy of 93.17% on the single medical test corpus, a substantial drop from the 95.77% average accuracy obtained in the GENIA corpus.  4 Coding Differences We looked at high frequency tagging errors in the medical test set and found that many errors resulted directly from the differences in the coding styles between GENIA and MedPost. Our model reflects the coding style of the WSJ, used for our generic POS annotated text. GENIA largely fol-lowed the WSJ coding conventions. Annotation in the 1932 sentences taken from MedPost had some systematic differences in coding style from this.  
179
Identified Differences: Lexical differences: 1) Words such as ?more? and ?less? are JJR or RBR in WSJ/GENIA but JJ or RB in MedPost. 2) Tokens such as %, =, /, <, > are typically NN or JJ in WSJ/GENIA but SYM in MedPost. 3)?be? is VB in WSJ/GENIA but VB or VBP in MedPost. 4) Some orthographic categories are JJ in WSJ/GENIA but NN in MedPost. Transition discrepancies: 1) Verbs are tagged VB following a TO or MD in WSJ/GENIA but only following a TO in MedPost. 2) MedPost prefers NN and NN-NN sequences. Ad Hoc Adjustments: We constructed a new lexicon accounting for some of the lexical differ-ences and attained an accuracy of 94.15% versus the previous 93.17%. Next we biased a few initial state transition probabilities, changing P(VB|MD) from very high to a very low and increasing P(NN|NN), and attained an accuracy of 94.63%.  As the coding differences had nothing to do with suffixes and suffix distributions, the central part of our methodology, we tried some ad hoc fixes to determine what our performance might have been. We suffered at least a 1.46% drop in accuracy due to differences in coding, not language use. 5 Evaluation The table shows the accuracy of our tagger and a few well-known taggers in our target biomedical sub-domains.   Molecular Biology %Accuracy - Our  tagger (5-fold) 95.8% - MedPost  94.1% - Penn BioIE1 95.1% - GENIA supervised 98.3% Medical Domain  - Our  tagger  93.17% - Our  tagger (+ lex bias) 94.15% - Our tagger (+ lex & trans bias) 94.63% - MedPost supervised2 96.9% The MedPost and Penn BioIE taggers used an-notated text and supervised training in other bio-medical domains, but they were not trained spe-cifically for the GENIA Molecular Biology sub-domain. Our tagger seems competitive with these                                                  1 PennBioIE. 2005. Mining The Bibliome Project. http://bioie.ldc.upenn.edu/. 2 Based on Medpost test set of 1000 sentences, not on our test set of 1932 sentences.  
taggers.  We cannot claim superior accuracy as these taggers may suffer the same coding bias ef-fects we have noted. The superior performance of the GENIA tagger (Tsuruoka et al 2005) in the Molecular Biology/GENIA domain and the Med-Post tagger (Smith et al 2004) in its biomedical domain owes to their use of supervised training on an annotated training set with evaluation on a test set from the same domain. The approximate 1.5% bias effect due to coding differences is attributable to organizational differences in POS.  6 Conclusions  To cope with domain specific vocabulary and uses of vocabulary, we exploited the suffix information of words and related words to build domain spe-cific lexicons. We trained our HMM using EM and un-annotated text from the specialized domains. We assessed accuracy versus annotated test sets in the specialized domains, noting discrepancies in our results across specialized domains, and con-cluding that our methodology performs competi-tively versus well-known taggers that used anno-tated text and supervised training in other biomedi-cal domains. References M. Lease and E. Charniak. 2005. Parsing Biomedical Literature.  IJCNLP-05: 58-69.   M. Marcus, B. Santorini, M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank.  Comp. Ling., 19:313-330.   J.E. Miller, M. Torii, K. Vijay-Shanker. 2007. Building Domain-Specific Taggers Without Annotated (Do-main) Data. EMNLP-07.   L. Smith, T. Rindflesch, W.J. Wilbur. 2004. MedPost: a part-of-speech tagger for bioMedical text.  Bioinfor-matics 20 (14):2320-2321.   L. Smith, T. Rindflesch, W.J. Wilbur. 2005. The impor-tance of the lexicon in tagging biomedical text. Natu-ral Language Engineering 12(2) 1-17. Y. Tateisi, T. Ohta, J. Dong Kim, H. Hong, S. Jian, J. Tsujii. 2003. The GENIA corpus: Medline abstracts annotated with linguistic information. Third meeting of SIG on Text Mining, ISMB.   Y. Tsuruoka, Y. Tateishi, J.D. Kim, T. Ohta, J. McNaught, S. Ananiadou, J. Tsujii. 2005. Developing a Robust Part-of-Speech Tagger for Biomedical Text, Advances in Informatics, LNCS 3746: 382-392. 
180
Proceedings of the 8th International Natural Language Generation Conference, pages 74?82,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Experimental Design to Improve Topic Analysis Based Summarization
John E. Miller
Computer & Information Sciences
University of Delaware
Newark, DE 19711
jmiller@udel.edu
Kathleen F. McCoy
Computer & Information Sciences
University of Delaware
Newark, DE 19711
mccoy@udel.edu
Abstract
We use efficient screening experiments
to investigate and improve topic analysis
based multi-document extractive summa-
rization. In our summarization process,
topic analysis determines the weighted
topic content vectors that characterize the
corpora, and then Jensen-Shannon diver-
gence extracts sentences that best match
the weighted content vectors to assemble
the summaries. We use screening experi-
ments to investigate several control param-
eters in this process, gaining better under-
standing of and improving the topic anal-
ysis based summarization process.
1 Introduction
We use efficient experimental design to investi-
gate and improve topic analysis based multiple
document extractive summarization. Our process
proceeds in two steps: Latent Dirichlet Anal-
ysis (LDA) topic analysis determines the top-
ics that characterize the multi-document corpus,
and Jensen-Shannon divergence selects sentences
from the corpus. This process offers many poten-
tial control settings for understanding and improv-
ing the summarization process.
Figure 1 shows topic analysis with corpus input,
control settings, and product outputs of topics and
probability estimates of topic compositions and
document mixtures. There are controls for doc-
ument preparation (headlines) and analysis (num-
ber of topics, initial ? and ?, number of iterations,
and whether to optimize ? and ? in process).
Figure 2 shows summarization with corpus and
topic inputs, control settings, and the text summa-
rization product. There are controls for extraction
of sentences (Extract ? and JSD Divisor) and for
composing the summary (Order policy).
Topic analysis has become a popular choice for
text summarization as seen in Text Analysis Con-
CorpusAnalyze 
Topics
# Iterations!
Optimize !, "
# Topics!
Initial !, "
Topics
#
$
Headlines
Figure 1: Topic Analysis
CorpusAnalyze 
Topics
Order policy
Extract !!
JSD divisor
Summary
Topics
Figure 2: Text Summarization
ferences (TAC, 2010; TAC, 2011) with individual
team reports (Delort and Alfonseca, 2011; Lui et
al., 2011; Mason and Charniak, 2011). Nenkova
and McKeown (2012; 2011) included topic anal-
ysis among standard methods in their surveys of
text summarization methodologies. Haghighi and
Vanderwende (2009) explored extensions of LDA
topic analysis for use in multiple document sum-
marization tasks. Yet there are many control set-
tings that can affect summarization that have not
been explicitly studied or documented, and that
are important for reproducing research results.
In this text summarization pilot study, we exper-
iment with several control settings. As in Mason
and Charniak (2011) we do a general rather than
guided summarization. Our primary contribution
is illustrating the use of efficient experimental de-
sign on control settings to help understand and im-
prove the text summarization process. We enjoy
some success in this endeavor even as we are sur-
prised by some of our results.
74
2 Technical Background
2.1 LDA Topic Analysis
LDA topic analysis uses a per document bag of
words approach to determine topic compositions
of words and document mixtures of topics. Anal-
ysis constructs topic compositions and document
mixtures by assigning words to topics within doc-
uments. Weighted topic compositions can then be
used as a basis for selecting the most informative
text to include in summarizations.
LDA topic analysis is based on a generative
probabilistic model. Document mixtures of top-
ics are generated by a multinomial distribution,
?, and topic compositions of words are gener-
ated by a multinomial distribution, ?. Both ? and
? in turn are generated by Dirichlet distributions
with parameters ? and ? respectively. Figure 3
(Steyvers and Griffiths, 2007) shows a corpus ex-
plained as the product of topic word compositions
(?) and document topic mixtures (?).
Corpus
w
o
r
d
s
documents
w
o
r
d
s
topics documents
t
o
p
i
c
s
=
x
!
"
Figure 3: Topic Model
The joint distribution of words and topics (Grif-
fiths and Steyvers (2004)) is given by P (w, z) =
P (w|z)P (z) where in generating a document the
topics are generated with probability P (z) and the
words given the topics are generated with proba-
bility P (w|z). Here
P (w|z) =
(
? (??)
? (?)V
)Z Z?
z=1
?
v ? (nzv + ?)
? (nz? + ??)
,
(1)
where nzv is the number of times word v occurs in
topic z, nz? is the number of times topic z occurs,
?? is the sum of the ? scalar over all word types,
and ? ( ) is the gamma function (Knuth, 2004),
and
P (z) =
(
? (??)
? (?)Z
)D D?
d=1
?
z ? (nzd + ?)
? (n?d + ??)
, (2)
where nzd is the number of times topic z occurs in
document d, n?d is the number of times document
d occurs, and ?? is the sum of ?s over topics.
Analysis reverses the generative model. Given
a corpus, topic analysis identifies weighted topic
word compositions and document topic mixtures
from the corpus. We assign topics to words in
the training corpus using Gibbs sampling (Gel-
man et al., 2004) where each word is considered in
turn in making the topic assignment. We monitor
training progress by logP (w, z) where a greater
logP (w, z) indicates better fit. After sufficient it-
erations through the corpus the logP (w, z) typi-
cally converges to steady state.
Analysis products are topic determinations for
the corpus as well as weighted estimates of topic
word compositions ? and document topic mix-
tures ?. The ? and ? priors are optimized (re-
estimated) during training and the asymmetric ?
which varies by topic can be used as a measure of
topic importance in our summarization step.
The topic analysis implementation used in this
pilot study borrows from the UMass Mallet topic
analysis (McCallum, 2002).
2.2 Jensen-Shannon Divergence
From the topic word compositions and optimized
?s, we form a weighted aggregate vector of the
prominent topics, and select sentences from the
corpus that have minimal divergence from the ag-
gregate topic. The operating assumption is that the
aggregate topic vector adequately represents the
content of an ideal summary. So the closer to zero
divergence from the aggregate topic, the closer we
are to the ideal summary.
We seek to minimize the Jensen-Shannon Di-
vergence, JSD(C||T ), a symmetric Kullback-
Liebler (KL) divergence, between the extractive
summary content, C, and the aggregate topic, T,
using a greedy search method of adding at each
pass through the corpus the sentence that most
reduces the divergence. Haghighi and Vander-
wende (2009) made similar use of KL divergence
in their Topic Sum method.
In preliminary studies, this minimize JSD cri-
terion seemed to give overly long sentences be-
cause the greedy method favored the greatest re-
duction in JSD regardless of the length of the sen-
tence. This affected readability and rapidly used
up all available target document size. Therefore
we modified the greedy search method to consider
sentence length as well.1
1Global optimization of JSD(C||T ) could address both
of these issues; we will investigate this option in a future ef-
fort.
75
In selecting each new sentence we seek to maxi-
mize the reduction in divergence corrected for sen-
tence length
(JSD(Ct?1||T )? JSD(St, Ct?1||T ))
function(length(St))
, (3)
where St is the sentence under consideration and
Ct?1 is the content from the previously completed
iterations, and the function of length of St, is ei-
ther the constant 1 (i.e. no correction for sentence
length) or?length(St).
3 Pilot Study Using TAC 2010 Samples
Our goal is to investigate and optimize factors that
impact multi-document extractive summarization.
We hope to subsequently extend our findings and
experience to abstractive summarization as well.
For our pilot, we?ve chosen summarization of
the 2010 Text Analysis Conference (2010) sam-
ple themes, which are conveniently available and
of a manageable size. The three sample themes
are from different summarization categories out of
a total of 46 news themes over five different cat-
egories, with 10 original and 10 follow-up news
reports each. In the original TAC 2010 task, par-
ticipants were asked to do focused queries varying
with the summarization category. In our pilot we
perform an undirected summarization of the orig-
inal news reports.
NIST provides 4 model summaries for each
news theme annotated for the focused summary,
and we use these model summaries in scoring our
extractive summarizations.2 We also include a
measure of fluency in our assessment.
Our document summarization task is then: mul-
tiple document extractive summarization using 10
documents of less than 250 words each to con-
struct summaries of 100 words.
3.1 Preliminary Results of Topic Analysis
Topic analysis is such a complex methodology that
it makes sense to fix some parameters before using
it in the summarization process.
We use the commonly accepted initial ? value
of 1 for each topic giving a sum of ? values equal
to the number of topics. Later, we experiment with
a single individual topic initial ? value, but we al-
ways maintain an initial ? sum equal to the num-
ber of topics. Likewise we use the scalar ? value
2Comparison of our summarization results versus the
TAC 2010 task will necessarily be imprecise given the dif-
ferences in focus of our pilot study from TAC 2010.
0.1 typical of a modest number of word types (less
than 1000 in this study).
In prior studies, we found that re-estimating ?
and ? frequently adds little cost to topic analysis
and drives better and more rapid convergence. We
optimize ? and ? every 5 iterations, starting at it-
eration 50.
How Many Topics to Use
The number of topics depends on the problem it-
self. The problem of size of ? 2000 words per
news theme would indicate a number of topics be-
tween 3 and 20 as adequate to explain document
word use where the log(|Corpus|) is the mini-
mum and?|Corpus| is the maximum number of
topics to use (Meila?, 2007).
A common way to select the correct number of
topics is to optimize logP (w) on held-out doc-
uments, where greater log likelihoods indicate a
better number of topics. While it would be im-
practical to do such a study for each news theme
or each document summary, it is reasonable to do
so on a few sample themes and then generalize to
similar corpora. We look at log likelihood for 3, 5,
and 10 topics using the TAC 2010 sample themes.
As there are only 10 documents for each theme,
we use the TAC 2010 update documents as held-
out documents for calculating the log likelihoods.
Topic word distributions, ?, from training are
used to infer document mixtures, ?, on the held-
out data, and the log P (w) is calculated (Teh et
al., 2007) as:
P (w) =
?
d,i
(?
z
nzwi + ?
nz? + ??
nzd + ?
n?d + ??
)
, (4)
where the sum is over all possible topics for a
given word and the product is over all documents
and words.
Table 1 shows mean log likelihoods for the news
themes at 3, 5 and 10 topics each. There is lit-
tle practical difference between the log likelihood
measures even though the 3 topic model has a sig-
nificantly lower log likelihood (p < 0.05) than
the 5 and 10 topic models. We assess topic quality
more directly to see which model is better.
3 Topics 5 Topics 10 Topics
-6.00 -5.97 -5.96
Table 1: Held-out Log Likelihood Number Topics.
76
Useful topic quality measures are:
Importance measured by number of documents
(or optimized ?s). Low importance topics,
with very few documents related to a topic,
indicate that we have more topics than neces-
sary. While not a fatal flaw, the topic model
may be over fit.
Coherence measured as a log sum of co-
occurrence proportions of each topic?s high
frequency words across multiple docu-
ments (Mimno et al., 2011). The more neg-
ative the coherence measure, the poorer the
coherence. A few poor coherence topics is
not fatal, but the topic model may be over fit.
Similarity to other topics measured by cosine
distance between topic vectors is undesirable.
The more similar the topics, the more diffi-
cult it is to distinguish between them. Many
similar topics makes it difficult to discrimi-
nate among topics over the corpus.
Reviewing the document quality for 3, 5 and 10
topics we find:
? More low importance topics in 10 versus 5
and 3 topic models,
? Somewhat better topic coherence in 3 and 5
topic models,
? Undesirable greater topic similarity for the 3
versus 5 versus 10 topic models.
We choose the 10 topic model giving higher pri-
ority to the problem of undesirable topic similar-
ity, recognizing that we may get some unimportant
or less coherent topics. As our summarization pro-
cess only uses the most important topics for the ag-
gregate topic, the occasional unimportant and less
coherent topic should not matter.
Document Preparation
Document cleaning removed all HTML, as well
as all header information not related to the articles
themselves; document dates, references, and head-
lines were saved for use in the document summa-
rization step. Document headlines were optionally
folded into the document text. Stop words were re-
moved and remaining words lemmatized for topic
analysis.
4 Design of Experiments
As our information about the various controls in
the process and the expected results is fairly rudi-
mentary, we use efficient screening experimental
designs to evaluate several factors at the same time
with a minimum number of trials. We define the
factors (control parameters) in our experiment, the
dependent variables we will measure, and finally
select the screening design itself.
Most of the process of topic analysis will re-
main fixed such as the use of 10 topics, initial ?
sum of 10, initial scalar ? of 0.1, optimization of
? and ? every 5 iterations and 500 total iterations
before saving the final topic vector weights and
corresponding topic alphas.
From our experimentation we hope to find:
? Factors impacting dependent variables,
? Gross magnitude of impact on dependent
variables,
? Factors to followup with in more detail.
4.1 Experimental Factors
In screening experiments, we chose factors about
which we have crude information, and which we
think could impact intermediate or final product
results. To learn as much as possible about factor
effects, we choose to vary them between default
and extreme settings or between two extremes
where we hope to see some positive impact.
Our experimental factors are:
Save headline text as part of document prepara-
tion (Yes, No). Headlines often contain im-
portant summary information. We test to see
if such information improves summaries.
Single fixed ? proportion of the ? sum (*, 0.5).
Topic analysis typically selects (weights) a
few important topic vectors with substantial
proportions of the ? sum. We want to see if
biasing selection of a single important vec-
tor at a 0.5 proportion of the ? sum improves
summaries versus unbiased ? weighting (*).
Aggregate topic policy as a proportion of the ?
sum for selecting the topic aggregate used in
summarization (0.5, 0.75). We order topics
based on the optimized (re-estimated) ?s and
aggregate topics summing and weighting by
the ?s until we reach the aggregate topic pol-
icy proportion. We want to see which policy
(0.5 or 0.75) proportion of the ? sum results
in better summaries.
JSD divisor to use with iterative greedy search
for sentences (ONE, SQRT). Prior work
shows the JSD Divisor impacts the length of
77
sentences selected. We test the impact on the
summaries themselves.
Order policy for constructing the summary
from selected sentences (DATE-DOC,
SALIENCE-DOC). Ordering sentences by
news report date or by salience as measured
by reduction in JSD should impact the
fluency of summaries.
4.2 Dependent Variables
We want readable and informative text that sum-
marizes content of the input documents in the al-
lowable space. We measure several intermedi-
ate process variables as well as evaluate the sum-
maries themselves.
Intermediate measures include:
? Initial selected sentence Jensen-Shannon di-
vergence from the aggregate topic. The first
sentence selected should substantially reduce
divergence.
? Final selected sentence Jensen-Shannon di-
vergence from the aggregate topic. Diver-
gence close to zero would indicate broad cov-
erage of the aggregate topic; it may be related
to summary content.
? Number of topics in the aggregate topic.
? Average sentence length. This should be im-
pacted by the JSD divisor; it may be related
to summary fluency.
ROUGE (Lin, 2011) is a package for auto-
matic evaluation of summaries that compares sys-
tem produced summaries to model (gold stan-
dard) summaries and reports statistics such as R-
2, bi-gram co-occurrence statistics between sys-
tem and model summaries, and SU4, skip bi-gram
co-occurrence statistics where word pairs no more
than 4 words apart may also be counted as bi-
grams. The R-2 and SU4 are automated content
measures reported for TAC 2010, and the gold
standard summaries are readily available for the
samples topics. We use ROUGE R-2 and SU4 as
reliable dependent measures and for comparison
to TAC 2010 results.
We add a simple measure of fluency focused on
across sentence issues. The fluency score starts
at a value of 5 and then subtracts: 1 for each
non sequitur or obvious out of order sentence, 1/2
for each missing co-reference, non-informative,
ungrammatical, or redundant sentence. For sen-
tences of less than 20 words, when more than one
penalty applies only the most severe penalty is ap-
plied, so as not to penalize the same short phrase
multiple times. Scoring is done by one of the au-
thors without knowing the combination of experi-
mental factors of the summary (blind scoring).
Summary measures thus include: ROUGE R-2,
ROUGE SU4, and Fluency.
4.3 Select Experimental Design
Screening designs focus on detecting and assess-
ing main effects and optionally low order inter-
action effects. When all experimental factors are
continuous, center points may also be included in
some designs. In subsequent stages of experimen-
tation, when factors have been reduced to a min-
imum, one can use more fine grained factor set-
tings to better map the response surface for those
factors. Two common families of screening de-
signs (Montgomery, 1997) are:
Two level fractional factorial Uses a power of
1/2 fraction of a full two level factorial design.
For example, instead of running all possible
combinations of 5 factors (i.e. 32 trials), you
could choose a 1/2 or even 1/4 fraction of the
design, based on how many experiments you
can run and how much confounding you are
willing to accept between main effects and
various interaction effects. The 1/2 fraction of
a 5 factor design would result in 16 trials be-
ing run with the main effects estimated clear
of any 2-way or 3-way interactions.
Plackett-Burman These screening designs are
available in multiples of 4 trials and can have
as many factors as the number of trials less
one. Main effects are confounded with all
other effects in the Plackett-Burman design
and so not estimable, but the confounding is
spread evenly among all main effects rather
than concentrated in specific interactions as
in the fractional factorial.
We?ve chosen the 12 run Plackett-Burman de-
sign with 5 factors and 6 degrees of freedom from
the unassigned (dummy) factors available to esti-
mate error. Assuming sparsity of effects (or equiv-
alently invoking the Pareto principal), there will
likely only be a few critical factors explaining
much of the variation in dependent variables.
Table 2 shows the resulting Plackett-Burman
design excluding dummy factors.
78
Run Fixed Aggr JSD Order Head
Alpha Topic Div Line
1 .5 .75 ONE SAL YES
2 * .75 SQRT DATE YES
3 .5 .5 SQRT SAL NO
4 * .75 ONE SAL YES
5 * .5 SQRT DATE YES
6 * .5 ONE SAL NO
7 .5 .5 ONE DATE YES
8 .5 .75 ONE DATE NO
9 .5 .75 SQRT DATE NO
10 * .75 SQRT SAL NO
11 .5 .5 SQRT SAL YES
12 * .5 ONE DATE NO
Table 2: Plackett-Burman 12 DOE.
5 Experimental Results
We analyze our experiment using conventional
analysis of variance (ANOVA) and show tables of
means for the various experimental conditions. As
this is a screening experiment, we treat a p-value<
0.20 as informative and consider the correspond-
ing factor worth further consideration. To save
space, only significant p-values are reported rather
than the full ANOVAs.
5.1 Intermediate Measures
Number of topics in the aggregate topic is directly
impacted by the AggrTopic setting; we simply re-
port the mean number of topics selected by Aggr-
Topic value (Table 3). The 1.0 average number of
topics for AggrTopic set to 0.5 indicates that only
one topic was ever selected for the aggregate topic
at this setting. This implies that the most impor-
tant topic always had an ? proportion > 0.50 of
the ? sum even when the FixedAlpha setting was
* (for unbiased ? weighting). This is unexpected
in that we thought the most important topic ? de-
termined by topic analysis would be more variable
and show some ? values with proportions less than
0.5 of the ? sum.
Aggr Number
Topic Topics
0.50 1.00
0.75 4.55
Table 3: Average Number Topics.
Average sentence length in the summary may be
affected by any of the independent variables ex-
cept sentence order policy. JSD Divisor has a dra-
matic impact (p < 0.0001) and AggrTopic a mod-
est impact (p < 0.01) on average sentence length.
Using a divisor of ONE in the JSD based sentence
selection results in much longer sentences while
using AggrTopic of 0.5 results in shorter sentences
(Table 4).
Aggr Sentence JSD Sentence
Topic Length Divisor Length
0.5 20.3 ONE 26.8
0.75 23.9 SQRT 17.4
Standard Error of the mean = 0.78
Table 4: Average Sentence Length.
Initial selected sentence Jensen-Shannon diver-
gence (JSD) should be affected directly by JSD
Divisor in iterative sentence selection, but may
also be affected by any of the other independent
variables except for sentence order policy. Aggr-
Topic and JSD Divisor strongly impact initial sen-
tence JSD (p < 0.00005).
The table of JSD initial sentence means by Ag-
grTopic and JSD Divisor is revealing (Table 5).
The JSD for the initial sentence selected is lower
for AggrTopic of 0.5. We observed above that only
one topic is selected for the aggregate topic when
AggrTopic is 0.5. Thus we achieve a lower di-
vergence of the initial sentence from the aggregate
topic when the aggregate is composed of only one
topic. For initial sentence JSD, aggregating topics
seems ineffective.
Similarly a JSD Divisor of ONE gives a lower
initial divergence than using the SQRT as the di-
visor. The interpretation is problematic here in
that a divisor of ONE seems to give lower ini-
tial divergence because it selects longer sentences,
which means that less space remains in the sum-
mary to select other sentences minimizing total di-
vergence.
Aggr JSD JSD JSD
Topic Initial Divisor Initial
0.5 0.665 ONE 0.658
0.75 0.735 SQRT 0.742
Standard Error of the mean = 0.0056
Table 5: Average Initial JSD.
Table 6 shows the impact of AggrTopic and JSD
Divisor together on the JSD for the initial sen-
tence. There is still the issue of whether using a
JSD Divisor of ONE is appropriate given the ef-
fect on the remaining summary size, but the effects
appear additive.
79
Aggr JSD JSD
Topic Divisor initial
0.50 ONE 0.627
0.50 SQRT 0.703
0.75 ONE 0.690
0.75 SQRT 0.780
Standard Error of the mean = 0.0080
Table 6: Average Initial JSD.
Final sentence Jensen-Shannon Divergence
(JSD) may be affected by any but the sentence or-
der policy variable. AggrTopic (p < 0.00001) and
JSD Divisor (p < 0.001) strongly impact the final
sentence JSD; there is also a possible effect from
including headlines in the summary (p < 0.1).
The effect of the JSD Divisor has reversed from
the initial JSD; using a divisor of ONE results here
in a less desirable higher divergence for the final
sentence. The AggrTopic effect is about the same
as for initial JSD divergence; a single dominant
topic seems more effective than using an aggre-
gate topic.
Aggr JSD JSD JSD
Topic Final Divisor Final
0.5 0.422 ONE 0.487
0.75 0.513 SQRT 0.448
Standard Error of the mean = 0.0047
Table 7: Average Initial JSD.
Impact of AggrTopic and JSD Divisor together
on the JSD for the initial sentence (Table 8) seems
additive.
Aggr JSD JSD
Topic Divisor final
0.50 ONE 0.437
0.50 SQRT 0.407
0.75 ONE 0.537
0.75 SQRT 0.490
Standard Error of the mean = 0.0066
Table 8: Average Final JSD.
5.2 Product Measures
Based on the analysis of intermediate measures, it
would seem that using a JSD Divisor of the SQRT
and selecting only the dominant topic gives less
divergence from the aggregate topic. However,
we have to be careful here in drawing conclusions
based on intermediate variables; selecting only the
dominant topic may result in reduced divergence,
but this does not necessarily mean that the domi-
nant topic is representative of good summaries.
We examine product variables to provide direct
support in our study, and so we ask how ROUGE
R-2 and SU4, and fluency evaluations vary with
the experimental factors. This pilot studies un-
guided summarization of initial stories from the 3
sample news themes from 3 separate categories.
While results are not directly comparable with
those of the full TAC 2010 test corpus, we will use
the TAC 2010 results as a reference point versus
our own results. The average of all experiments
are reported along with the TAC 2010 results (Ta-
ble 9). Our ROUGE R-2 and SU4 performance
seems reasonable showing results better than the
baseline but not as good as the best system.
Reference System R-2 SU4
Baseline - Lead sentences 5.4 8.6
Baseline - MEAD? 5.9 9.1
Best System 9.6 13.0
Pilot Average 6.7 10.1
Pilot Minimum 5.6 8.7
Pilot Maximum 8.1 11.9
?Text summarization system (Radevet al., 2004)
Table 9: TAC 2010 ROUGE Scores.
ROUGE R-2 results show no significant impact
from our experimental factors. This is disappoint-
ing as it gives us no handle on how to improve
performance.
ROUGE SU4 shows a modest impact for Aggr-
Topic (p < 0.025) and the possible impact of JSD
Divisor (p < 0.20). Note that we dropped Or-
der and FixedAlpha factors from the model; Or-
der because it can only effect sentence order and
FixedAlpha because the most important ? deter-
mined automatically by topic analysis did not vary
much from the 0.5 FixedAlpha. A benefit of drop-
ping terms from the model is that we have more
dummy factors to estimate error.
The ROUGE SU4 means (Table 10) show the
same pattern as for the JSD final sentence, but the
differences are not as clear cut. Box and whiskers
plots for AggrTopic and JSD Divisor (Figures 4
and 5) offer more insight into the AggrTopic and
JSD Divisor effects.
There is a clear distinction between AggrTopic
80
Aggr ROUGE JSD ROUGE
Topic SU4 Divisor SU4
0.5 10.75 ONE 9.70
0.75 9.48 SQRT 10.53
Standard Error of the mean = 0.32
Table 10: Average ROUGE SU4.
levels 0.5 and 0.75 with better results at the 0.5
level, except for an outlier value of 9.1. Investiga-
tion shows no data coding error and nothing spe-
cial about the experimental conditions other than
if uses a JSD Divisor of ONE which also gives
lower SU4 scores. The box and whiskers plots for
JSD Divisor effects also suggest a positive effect
for JSD Divisor of SQRT, but the whiskers over-
lap the boxes indicating no strong effect.
l
0.5 0.75
9.0
9.5
10.0
10.5
11.0
11.5
12.0
Alpha Extract Proportion
ROUGE 
SU4
Figure 4: ROUGE
SU4 by Aggr Topic
ONE SQRT
9.0
9.5
10.0
10.5
11.0
11.5
12.0
JSD Divisor
ROUGE 
SU4
Figure 5: ROUGE
SU4 by JSD Divisor
We had speculated that the final sentence diver-
gence might be related to some of the end prod-
uct measures. Indeed, we find that JSD final sen-
tence is strongly inversely related to ROUGE SU4
as shown by regression analysis (Table 11). While
the residual error of 0.73 indicates that we can
only reliably predict ROUGE SU4 within 1.5 units
(for averages of 3 trials), this is still important.
A 0.1 reduction in final sentence divergence cor-
responds on the average to a 1.4 unit increase in
ROUGE SU4.
Estimate StdErr t Pr(>|t|)
Intercept 16.865 1.934 8.721 ?0.0
JSDfinal -14.435 4.112 -3.510 0.006
Residual standard error: 0.73 on 10 degrees of freedom
F-statistic: 12.32 on 1 and 10 DF, p-value: 0.0056
Table 11: Regression - ROUGE SU4.
We thought Simple Fluency would show an ef-
fect for sentence order policy and maybe other fac-
tors. Analysis shows an effect for JSD Divisor
(p < 0.05) and possible effects of Order policy
and Head lines (p < 0.20).
Fluency means (Table 12) show that fluency is
better for JSD Divisor ONE. From our experience
of scoring Fluency, this would seem to be because
the fewer and longer sentences with JSD Divisor
of ONE offer fewer chances for disfluencies. The
better Fluency with DATE ordering likely comes
from fewer out of order or non sequitur sentences,
and the better Fluency with NO headlines likely
results from fewer short ungrammatical headlines
as part of the text.
JSD Flu- Order Flu- Head Flu-
Div ency ency Lines ency
ONE 3.95 DATE 3.80 NO 3.80
SQRT 3.33 SAL 3.47 YES 3.47
Standard Error of the mean = 0.16
Table 12: Average Fluency.
6 Summary and Discussion
Our pilot studied topic analysis based multi-
document extractive summarization using the
2010 TAC sample topics. Our experimental design
process identified control factors with their default
and extreme settings, defined intermediate and fi-
nal product dependent measures, designed the ex-
periment, ran, and analyzed the experiment.
We identified an intermediate variable, final se-
lected sentence divergence, that could be used as a
stand-in for the product content measure, ROUGE
SU4. We found that using a single dominant topic,
instead of an aggregate topic, and using a divisor
of the square root of sentence length in sentence
selection, improved final sentence divergence and
ROUGE SU4. However, using a divisor of one in
sentence selection improved fluency of summaries
which is at odds with the benefit of using square
root of sentence length to improve content.
Our planned experimentation has made obvious
and objective the process of describing and im-
proving our extractive summarization process. It
is an extremely useful process and furthermore a
process that when documented permits sharing of
results and even duplicating of results by others
working in this area.
81
References
Jean-Yves Delort and Enrique Alfonseca. 2011. De-
scription of the Google Update Summarizer. 2011
TAC Proceedings.
Andrew Gelman, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2004. Bayesian Data Analysis.
Chapman and Hall/CRC, New York, USA.
Tom L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. PNAS, 101(Suppl. 1):5228-5235.
Aria Haghighi and Lucy Vanderwalde. 2009.
Exploring Content Models for Multi-Document
Summarization. 2009 NACL Conference, HLT
Proceedings:362-370.
Donald E. Knuth. 1997. The Art of Computer Pro-
gramming, Volume 1 (Fundamental Algorithms).
Addison Wesley, New York, USA.
Chin-Yew Lin. 204. ROUGE: A Package for Auto-
matic Evaluation of Summaries. ACL 2004 Proceed-
ings of Workshop: Text Summarization Branches
Out.
Hongyan Liu, Pingan Liu, Wei Heng, and Lei Li.
2011. The CIST Summarization System at TAC
2011. 2011 TAC Proceedings.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi,
Stanko Dimitrov, Elliott Drabek, Ali Hakim,
Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi,
Horacio Saggion, Simone Teufel, Michael Topper,
Adam Winkel, and Zhu Zhang. 2004. MEAD
? A platform for multidocument multilingual
text summarization. Conference on Language
Resources and Evaluation LREC, Lisbon, Portugal,
(May 2004).
Rebecca Mason and Eugene Charniak. 2011. BLLIP
at TAC 2011: A General Summarization System for
a Guided Summarization Task. 2011 TAC Proceed-
ings.
Andres K. McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Marina Meila?. 2007. Comparing Clusterings ? an in-
formation based distance. J. Multivariate Analysis,
98(5):873-895.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
2011 EMNLP Conference, Proceedings:262-272.
Douglas C. Montgomery. 1997. Design and Analysis
of Experiments. John Wiley and Sons, New York,
USA.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic Summarization. Foundations and Trends in
Information Retrieval, 5(2-3):1003-233.
Ani Nenkova and Kathleen McKeown. 2012. A Sur-
vey of Text Summarization Techniques. Mining Text
Data. In Charu C. Aggarwal and ChengXiang Zhai
(eds.) Springer.
Mark Steyvers and Tom Griffiths. 2007. Probabilisitic
Topic Models. Latent Semantic Analysis: A road to
Meaning. In T. Landauer, S. D. McNamara & W.
Kintsch (eds.) Laurence Erlbaum.
Task Analysis Conference 2010 ?
Summarization Track. 2010.
http://www.nist.gov/tac/2010/Summarization/.
Task Analysis Conference 2011 ?
Summarization Track. 2011.
http://www.nist.gov/tac/2011/Summarization/.
Yee Whye Teh, Dave Newman, and Max Welling.
2007. Collapsed Variational Inference for HDP.
Advances in Neural Information Processing
Systems:1481-1488.
82
