Proceedings of ACL-08: HLT, pages 941?949,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Resolving Personal Names in Email Using Context Expansion
Tamer Elsayed,?Douglas W. Oard,? and Galileo Namata?
Human Language Technology Center of Excellence and
UMIACS Laboratory for Computational Linguistics and Information Processing (CLIP)
University of Maryland, College Park, MD 20742
{telsayed, oard, gnamata}@umd.edu
Abstract
This paper describes a computational ap-
proach to resolving the true referent of a
named mention of a person in the body of an
email. A generative model of mention gener-
ation is used to guide mention resolution. Re-
sults on three relatively small collections indi-
cate that the accuracy of this approach com-
pares favorably to the best known techniques,
and results on the full CMU Enron collection
indicate that it scales well to larger collections.
1 Introduction
The increasing prevalence of informal text from
which a dialog structure can be reconstructed (e.g.,
email or instant messaging), raises new challenges if
we are to help users make sense of this cacophony.
Large collections offer greater scope for assembling
evidence to help with that task, but they pose addi-
tional challenges as well. With well over 100,000
unique email addresses in the CMU version of the
Enron collection (Klimt and Yang, 2004), common
names (e.g., John) might easily refer to any one of
several hundred people. In this paper, we associate
named mentions in unstructured text (i.e., the body
of an email and/or the subject line) to modeled iden-
tities. We see at least two direct applications for this
work: (1) helping searchers who are unfamiliar with
the contents of an email collection (e.g., historians or
lawyers) better understand the context of emails that
they find, and (2) augmenting more typical social
networks (based on senders and recipients) with ad-
ditional links based on references found in unstruc-
tured text.
Most approaches to resolving identity can be de-
composed into four sub-problems: (1) finding a ref-
erence that requires resolution, (2) identifying can-
didates, (3) assembling evidence, and (4) choosing
?Department of Computer Science
?College of Information Studies
among the candidates based on the evidence. For
the work reported in this paper, we rely on the user
to designate references requiring resolution (which
we model as a predetermined set of mention-queries
for which the correct referent is known). Candidate
identification is a computational expedient that per-
mits the evidence assembly effort to be efficiently
focused; we use only simple techniques for that task.
Our principal contributions are the approaches we
take to evidence generation (leveraging three ways
of linking to other emails where evidence might be
found: reply chains, social interaction, and topical
similarity) and our approach to choosing among can-
didates (based on a generative model of reference
production). We evaluate the effectiveness of our
approach on four collections, three of which have
previously reported results for comparison, and one
that is considerably larger than the others.
The remainder of this paper is as follows. Sec-
tion 2 surveys prior work. Section 3 then describes
our approach to modeling identity and ranking can-
didates. Section 4 presents results, and Section 5
concludes.
2 Related Work
The problem of identity resolution in email is a spe-
cial case of the more general problem referred to as
?Entity Resolution.? Entity resolution is generically
defined as a process of determining the mapping
from references (e.g., names, phrases) observed in
data to real-world entities (e.g., persons, locations).
In our case, the problem is to map mentions in emails
to the identities of the individuals being referred to.
Various approaches have been proposed for en-
tity resolution. In structured data (e.g., databases),
approaches have included minimizing the number
of ?matching? and ?merging? operations (Benjel-
loun et al, 2006), using global relational informa-
tion(Malin, 2005; Bhattacharya and Getoor, 2007;
Reuther, 2006) and using a probabilistic generative
941
model (Bhattacharya and Getoor, 2006). None of
these approaches, however, both make use of con-
versational, topical, and time aspects, shown impor-
tant in resolving personal names (Reuther, 2006),
and take into account global relational informa-
tion. Similarly, approaches in unstructured data
(e.g., text) have involved using clustering techniques
over biographical facts (Mann and Yarowsky, 2003),
within-document resolution (Blume, 2005), and dis-
criminative unsupervised generative models (Li et
al., 2005). These too are insufficient for our prob-
lem since they suffer from inability scale or to han-
dle early negotiation.
Specific to the problem of resolving mentions in
email collections, Abadi (Abadi, 2003) used email
orders from an online retailer to resolve product
mentions in orders and Holzer et al (Holzer et al,
2005) used the Web to acquire information about
individuals mentioned in headers of an email col-
lection. Our work is focused on resolving personal
name references in the full email including the mes-
sage body; a problem first explored by Diehl et al
(Diehl et al, 2006) using header-based traffic anal-
ysis techniques. Minkov et al(Minkov et al, 2006)
studied the same problem using a lazy graph walk
based on both headers and content. Those two re-
cent studies reported results on different test collec-
tions, however, making direct comparisons difficult.
We have therefore adopted their test collections in
order to establish a common point of reference.
3 Mention Resolution Approach
The problem we are interested in is the resolution
of a personal-name mention (i.e., a named reference
to a person) m, in a specific email em in the given
collection of emails E, to its true referent. We as-
sume that the user will designate such mention. This
can be formulated as a known-item retrieval problem
(Allen, 1989) since there is always only one right an-
swer. Our goal is to develop a system that provides a
list of potential candidates, ranked according to how
strongly the system believes that a candidate is the
true referent meant by the email author. In this pa-
per, we propose a probabilistic approach that ranks
the candidates based on the estimated probability of
having been mentioned. Formally, we seek to esti-
mate the probability p(c|m) that a potential candi-
date c is the one referred to by the given mention m,
over all candidates C.
We define a mention m as a tuple < lm, em >,
where lm is the ?literal? string of characters that rep-
resentsm and em is the email wherem is observed.1
We assume that m can be resolved to a distinguish-
able participant for whom at least one email address
is present in the collection.2
The probabilistic approach we propose is moti-
vated by a generative scenario of mentioning people
in email. The scenario begins with the author of the
email em, intending to refer to a person in that email.
To do that s/he will:
1. Select a person c to whom s/he will refer
2. Select an appropriate context xk to mention c
3. Select a specific lexical reference lm to refer to
c given the context xk.
For example, suppose ?John? is sending an email
to ?Steve? and wants to mention a common friend
?Edward.? ?John? knows that he and Steve know
2 people named Edward, one is a friend of both
known by ?Ed? and the other is his soccer trainer.
If ?John? would like to talk about the former, he
would use ?Ed? but he would likely use ?Edward?
plus some terms (e.g., ?soccer?, ?team?, etc) for the
latter. ?John? relies on the social context, or the topi-
cal context, for ?Steve? to disambiguate the mention.
The steps of this scenario impose a certain struc-
ture to our solution. First, we need to have a
representational model for each candidate identity.
Second, we need to reconstruct the context of the
queried mention. Third, it requires a computational
model of identity that supports reasoning about iden-
tities. Finally, it requires a resolution technique that
leverages both the identity models and the context
to rank the potential candidates. In this section,
we will present our resolution approach within that
structure. We first discuss how to build both repre-
sentational and computational models of identity in
section 3.1. Next, we introduce a definition of the
contextual space and how we can reconstruct it in
1The exact position in em where lm is observed should also
be included in the definition, but we ignore it assuming that all
matched literal mentions in one email refer to the same identity.
2Resolving mentions that refer to non-participants is outside
the scope of this paper.
942
section 3.2. Finally, we link those pieces together
by the resolution algorithm in section 3.3.
3.1 Computational Model of Identity
Representation: In a collection of emails, indi-
viduals often use different email addresses, multi-
ple forms of their proper names, and different nick-
names. In order to track references to a person over
a large collection, we need to capture as many as
possible of these referential attributes in one rep-
resentation. We extend our simple representation
of identity proposed in (Elsayed and Oard, 2006)
where an identity is represented by a set of pair-
wise co-occurrence of referential attributes (i.e., co-
occurrence ?associations?), and each extracted as-
sociation has a frequency of occurrence. The at-
tributes are extracted from the headers and saluta-
tion and signature lines. For example, an ?address-
nickname? association < a, n > is inferred when-
ever a nickname n is usually observed in signature
lines of emails sent from email address a. Three
types of referential attributes were identified in the
original representation: email addresses, names, and
nicknames. We add usernames as well to account
for the absence of any other type of names. Names,
nicknames, and usernames are distinguishable based
on where each is extracted: email addresses and
names from headers, nicknames from salutation
and signature lines, and usernames from email ad-
dresses. Since (except in rare cases) an email ad-
dress is bound to one personal identity, the model
leverages email addresses as the basis by mandat-
ing that at least one email address must appear in
any observed association. As an off-line preprocess-
ing step, we extract the referential attributes from the
whole collection and build the identity models. The
first step in the resolution process is to determine the
list of identity models that are viable candidates as
the true referent. For the experiments reported in this
paper, any identity model with a first name or nick-
name that exactly matches the mention is considered
a candidate.
Labeling Observed Names: For the purpose of re-
solving name mentions, it is necessary to compute
the probability p(l|c) that a person c is referred to by
a given ?literal? mention l. Intuitively, that probabil-
ity can be estimated based on the observed ?name-
type? of l and how often that association occurs in
the represented model. We define T as the set of
3 different types of single-token name-types: first,
last, and nickname. We did not handle middle names
and initials, just for simplicity. Names that are ex-
tracted from salutation and signature lines are la-
beled as nicknames whereas full names extracted
from headers are first normalized to ?First Last?
form and then each single token is labeled based on
its relative position as being the first or last name.
Usernames are treated similarly to full names if they
have more than one token, otherwise they are ig-
nored. Note that the same single-token name may
appear as a first name and a nickname.
Figure 1: A computational model of identity.
Reasoning: Having tokenized and labeled all
names, we propose to model the association of a
single-token name l of type t to an identity c by a
simple 3-node Bayesian network illustrated in Fig-
ure 1. In the network, the observed mention l is
distributed conditionally on both the identity c and
the name-type t. p(c) is the prior probability of ob-
serving the identity c in the collection. p(t|c) is the
probability that a name-type t is used to refer to c.
p(l|t, c) is the probability of referring to c by l of
type t. These probabilities can be inferred from the
representational model as follows:
p(c) =
|assoc(c)|
?
c??C |assoc(c
?)|
p(t|c) =
freq(t, c)
?
t??T freq(t
? , c)
p(l|t, c) =
freq(l, t, c)
?
l??assoc(c) freq(l
? , t, c)
where assoc(c) is the set of observed associations of
referential attributes in the represented model c.
The probability of observing a mention l given
that it belongs to an identity c, without assuming a
specific token type, can then be inferred as follows:
p(l|c) =
?
t?T
p(t|c) p(l|t, c)
In the case of a multi-token names (e.g., John
Smith), we assume that the first is either a first name
943
or nickname and the last is a last name, and compute
it accordingly as follows:
p(l1l2|c) = {
?
t?{f,n}
p(t|c) p(l1|t, c)} ? p(l2|last, c)
where f and n above denotes first name and nick-
name respectively.
Email addresses are also handled, but in a differ-
ent way. Since we assume each of them uniquely
identifies the identity, all email addresses for one
identity are mapped to just one of them, which then
has half of the probability mass (because it appears
in every extracted co-occurrence association).
Our computational model of identity can be
thought of as a language model over a set of per-
sonal references and thus it is important to account
for unobserved references. If we know that a spe-
cific first name often has a common nickname (by a
dictionary of commonly used first to nickname map-
pings (e.g., Robert to Bob)), but this nickname was
not observed in the corpus, we will need to apply
smoothing. We achieve that by assuming the nick-
name would have been observed n times where n is
some fraction (0.75 in our experiments) of the fre-
quency of the observed name. We repeat that for
each unobserved nickname and then treat them as if
they were actually observed.
3.2 Contextual Space
Figure 2: Contextual Space
It is obvious that understanding the context of an
ambiguous mention will help with resolving it.
Fortunately, the nature of email as a conversa-
tional medium and the link-relationships between
emails and people over time can reveal clues that can
be exploited to partially reconstruct that context.
We define the contextual space X(m) of a men-
tion m as a mixture of 4 types of contexts with ?k as
the mixing coefficient of context xk. The four con-
texts (illustrated in Figure 2) are:
(1) Local Context: the email em where the named
person is mentioned.
(2) Conversational Context: emails in the broader
discussion that includes em, typically the thread that
contains it.
(3) Social Context: discussions that some or all of
the participants (sender and receivers) of em joined
or initiated at around the time of the mention-email.
These might bear some otherwise-undetected rela-
tionship to the mention-email.
(4) Topical Context: discussions that are topically
similar to the mention-discussion that took place at
around the time of em, regardless of whether the dis-
cussions share any common participants.
These generally represent a growing (although not
strictly nested) contextual space around the queried
mention. We assume that all mentions in an email
share the same contextual space. Therefore, we can
treat the context of a mention as the context of its
email. However, each email in the collection has
its own contextual space that could overlap with an-
other email?s space.
3.2.1 Formal Definition
We define K as the set of the 4 types of contexts.
A context xk is represented by a probability distri-
bution over all emails in the collection. An email ej
belongs to the kth context of another email ei with
probability p(ej |xk(ei)). How we actually represent
each context and estimate the distribution depends
upon the type of the context. We explain that in de-
tail in section 3.2.2.
3.2.2 Context Reconstruction
In this section, we describe how each context is
constructed.
Local Context: Since this is simply em, all of the
probability mass is assigned to it.
Conversational Context: Threads (i.e., reply
chains) are imperfect approximations of focused
discussions, since people sometimes switch topics
within a thread (and indeed sometimes within the
same email). We nonetheless expect threads to ex-
hibit a useful degree of focus and we have there-
fore adopted them as a computational representation
of a discussion in our experiments. To reconstruct
threads in the collection, we adopted the technique
introduced in (Lewis and Knowles, 1997). Thread
944
reconstruction results in a unique tree containing the
mention-email. Although we can distinguish be-
tween different paths or subtrees of that tree, we
elected to have a uniform distribution over all emails
in the same thread. This also applies to threads re-
trieved in the social and topical contexts as well.
Social Context: Discussions that share common
participants may also be useful, though we expect
their utility to decay somewhat with time. To recon-
struct that context, we temporally rank emails that
share at least one participant with em in a time pe-
riod around em and then expand each by its thread
(with duplicate removal). Emails in each thread are
then each assigned a weight that equals the recip-
rocal of its thread rank. We do that separately for
emails that temporally precede or follow em. Fi-
nally, weights are normalized to produce one distri-
bution for the whole social context.
Topical Context: Identifying topically-similar con-
tent is a traditional query-by-example problem that
has been well researched in, for example, the TREC
routing task (Lewis, 1996) and the Topic Detection
and Tracking evaluations (Allan, 2002). Individual
emails may be quite terse, but we can exploit the
conversational structure to obtain topically related
text. In our experiments, we tracked back to the
root of the thread in which em was found and used
the subject line and the body text of that root email
as a query to Lucene3 to identify topically-similar
emails. Terms found in the subject line are dou-
bled in the query to emphasize what is sometimes
a concise description of the original topic. Subse-
quent processing is then similar to that used for the
social context, except that the emails are first ranked
by their topical, rather than temporal, similarity.
The approaches we adopted to reconstruct the so-
cial and topical contexts were chosen for their rel-
ative simplicity, but there are clearly more sophis-
ticated alternatives. For example, topic modeling
techniques (McCallum et al, 2005) could be lever-
aged in the reconstruction of the topical context.
3.3 Mention Resolution
Given a specific mention m and the set of identity
models C, our goal now is to compute p(c|m) for
each candidate c and rank them accordingly.
3http://lucene.apache.org
3.3.1 Context-Free Mention Resolution
If we resolve m out of its context, then we can
compute p(c|m) by applying Bayes? rule as follows:
p(c|m) ? p(c|lm) =
p(lm|c) p(c)
?
c??C p(l
m|c?) p(c?)
All the terms above are estimated as discussed ear-
lier in section 3.1. We call this approach ?backoff?
since it can be used as a fall-back strategy. It is con-
sidered the baseline approach in our experiments.
3.3.2 Contextual Mention Resolution
We now discuss the more realistic situation in
which we use the context to resolve m. By expand-
ing the mention with its context, we get
p(c|m) = p(c|lm, X(em))
We then apply Bayes? rule to get
p(c|lm, X(em)) =
p(c, lm, X(em))
p(lm, X(em))
where p(lm, X(em)) is the probability of observ-
ing lm in the context. We can ignore this probabil-
ity since it is constant across all candidates in our
ranking. We now restrict our focus to the numera-
tor p(c, lm, X(em)), that is the probability that the
sender chose to refer to c by lm in the contextual
space. As we discussed in section 3.2, X is defined
as a mixture of contexts therefore we can further ex-
pand it as follows:
p(c, lm, X(em)) =
?
k
?k p(c, l
m, xk(e
m))
Following the intuitive generative scenario we intro-
duced earlier, the context-specific probability can be
decomposed as follows:
p(c, lm, xk(e
m)) = p(c)
? p(xk(e
m)|c)
? p(lm|xk(e
m), c)
where p(c) is the probability of selecting a can-
didate c, p(xk(em)|c) is the probability of select-
ing xk as an appropriate context to mention c, and
p(lm|xk(em), c) is the probability of choosing to
mention c by lm given that xk is the appropriate con-
text.
Choosing person to mention: p(c) can be estimated
as discussed in section 3.1.
Choosing appropriate context: By applying Bayes?
rule to compute p(xk(em)|c) we get
p(xk(e
m)|c) =
p(c|xk(em)) p(xk(em))
p(c)
945
p(xk(em)) is the probability of choosing xk to gen-
erally mention people. In our experiments, we
assumed a uniform distribution over all contexts.
p(c|xk(em)) is the probability of mentioning c in
xk(em). Given that the context is defined as a distri-
bution over emails, this can be expanded to
p(c|xk(e
m)) =
?
ei?E
p(ei|xk(e
m) p(c|ei))
where p(c|ei) is the probability that c is mentioned
in the email ei. This, in turn, can be estimated us-
ing the probability of referring to c by at least one
unique reference observed in that email. By assum-
ing that all lexical matches in the same email refer to
the same person, and that all lexically-unique refer-
ences are statistically independent, we can compute
that probability as follows:
p(c|ei) = 1? p(c is not mentioned in ei)
= 1?
?
m??M(ei)
(1? p(c|m?))
where p(c|m
?
) is the probability that c is the true
referent of m
?
. This is the same general problem
of resolving mentions, but now concerning a related
mention m
?
found in the context of m. To handle
this, there are two alternative solutions: (1) break the
cycle and compute context-free resolution probabil-
ities for those related mentions, or (2) jointly resolve
all mentions. In this paper, we will only consider the
first, leaving joint resolution for future work.
Choosing a name-mention: To estimate
p(lm|xk(em), c), we suggest that the email au-
thor would choose either to select a reference (or a
modified version of a reference) that was previously
mentioned in the context or just ignore the context.
Hence, we estimate that probability as follows:
p(lm|xk(e
m), c) = ? p(lm ? xk(e
m)|c)
+(1? ?) p(lm|c)
where ? ? [0, 1] is a mixing parameter (set at 0.9
in our experiments), and p(lm|c) is estimated as in
section 3.1. p(lm ? xk(em)|c) can be estimated as
follows:
p(lm ? xk(e
m)|c) =
?
m??xk
p(lm|lm
?
)p(lm
?
|xk) p(c|l
m
?
)
where p(lm|lm
?
) is the probability of modifying lm
?
into lm. We assume all possible mentions of c
are equally similar to m and estimate p(lm|lm
?
) by
1
|possible mentions of c| . p(l
m
?
|xk) is the probability of
observing lm
?
in xk, which we estimate by its rel-
ative frequency in that context. Finally, p(c|lm
?
) is
again a mention resolution problem concerning the
reference ri which can be resolved as shown earlier.
The Aho-Corasick linear-time algorithm (Aho
and Corasick, 1975) is used to find mentions of
names, using a corpus-based dictionary that includes
all names, nicknames, and email addresses extracted
in the preprocessing step.
4 Experimental Evaluation
We evaluate our mention resolution approach using
four test collections, all are based on the CMU ver-
sion of the Enron collection; each was created by se-
lecting a subset of that collection, selecting a set of
query-mentions within emails from that subset, and
creating an answer key in which each query-mention
is associated with a single email address.
The first two test collections were created by
Minkov et al(Minkov et al, 2006). These test col-
lections correspond to two email accounts, ?sager-
e? (the ?Sager? collection) and ?shapiro-r? (the
?Shapiro? collection). Their mention-queries and
answer keys were generated automatically by iden-
tifying name mentions that correspond uniquely to
individuals referenced in the cc header, and elimi-
nating that cc entry from the header.
The third test collection, which we call the
?Enron-subset? is an extended version of the test
collection created by Diehl at al (Diehl et al, 2006).
Emails from all top-level folders were included
in the collection, but only those that were both
sent by and received by at least one email address
of the form <name1>.<name2>@enron.com were
retained. A set of 78 mention-queries were manu-
ally selected and manually associated with the email
address of the true referent by the third author using
an interactive search system developed specifically
to support that task. The set of queries was lim-
ited to those that resolve to an address of the form
<name1>.<name2>@enron.com. Names found in
salutation or signature lines or that exactly match
<name1> or <name2> of any of the email partic-
ipants were not selected as query-mentions. Those
78 queries include the 54 used by Diehl et al
946
Table 1: Test collections used in the experiments.
Test Coll. Emails IDs Queries Candidates
Sager 1,628 627 51 4 (1-11)
Shapiro 974 855 49 8 (1-21)
Enron-sub 54,018 27,340 78 152 (1-489)
Enron-all 248,451 123,783 78 518 (3-1785)
For our fourth test collection (?Enron-all?), we
used the same 78 mention-queries and the answer
key from the Enron-subset collection, but we used
the full CMU version of the Enron collection (with
duplicates removed). We use this collection to as-
sess the scalability of our techniques.
Some descriptive statistics for each test collection
are shown in Table 1. The Sager and Shapiro col-
lections are typical of personal collections, while
the other two represent organizational collections.
These two types of collections differ markedly in
the number of known identities and the candidate
list sizes as shown in the table (the candidate list
size is presented as an average over that collection?s
mention-queries and as the full range of values).
4.1 Evaluation Measures
There are two commonly used single-valued eval-
uation measures for ?known item?-retrieval tasks.
The ?Success @ 1? measure characterizes the ac-
curacy of one-best selection, computed as the mean
across queries of the precision at the top rank for
each query. For a single-valued figure of merit that
considers every list position, we use ?Mean Recip-
rocal Rank? (MRR), computed as the mean across
queries of the inverse of the rank at which the cor-
rect referent is found.
4.2 Results
There are four basic questions which we address in
our experimental evaluation: (1) How does our ap-
proach perform compared to other approaches?, (2)
How is it affected by the size of the collection and
by increasing the time period?, (3) Which context
makes the most important contribution to the resolu-
tion task? and (4) Does the mixture help?
In our experiments, we set the mixing coefficients
?k and the context priors p(xk) to a uniform distri-
bution over all reconstructed contexts.
To compare our system performance with results
Table 2: Accuracy results with different time periods.
Period MRR Success @ 1
(days) Prob. Minkov Prob. Minkov
10 0.899 0.889 0.843 0.804
Sager 100 0.911 0.889 0.863 0.804
200 0.911 0.889 0.863 0.804
10 0.913 0.879 0.857 0.779
Shapiro 100 0.910 0.879 0.837 0.779
200 0.911 0.837 0.878 0.779
10 0.878 - 0.821 -
Enron-sub 100 0.911 - 0.846 -
200 0.911 - 0.846 -
10 0.890 - 0.821 -
Enron-all 100 0.888 - 0.821 -
200 0.888 - 0.821 -
previously reported, we experimented with differ-
ent (symmetric) time periods for selecting threads
in the social and topical contexts. Three represen-
tative time periods, in days, were arbitrarily chosen:
10 (i.e., +/- 5) days, 100 (i.e., +/- 50) days, and 200
(i.e., +/- 100) days. In each case, the mention-email
defines the center of this period.
A summary of the our results (denoted by ?Prob.?)
are shown in Table 2 with the best results for each
test collection highlighted in bold. The table also in-
cludes the results reported in Minkov et al(Minkov
et al, 2006) for the small collections for comparison
purposes.4 Each score for our system was the best
over all combinations of contexts for these collec-
tions and time periods. Given these scores, our re-
sults compare favorably with the previously reported
results for both Sager and Shapiro collections.
Another notable thing about our results is that
they seem to be good enough for practical appli-
cations. Specifically, our one-best selection (over
all tried conditions) is correct at least 82% of the
time over all collections, including the largest one.
Of course, the Enron-focused selection of mention-
queries in every case is an important caveat on these
results; we do not yet know how well our techniques
will hold up with less evidence, as might be the case
for mentions of people from outside Enron.
It is encouraging that testing on the largest col-
4For the ?Enron-subset? collection, we do not know which
54 mention-queries Diehl et alused in (Diehl et al, 2006)
947
lection (with all unrelated and thus noisy data) did
not hurt the effectiveness much. For the three differ-
ent time periods we tried, there was no systematic
effect.
Figure 3: Individual contexts, period set to 100 days.
Individual Contexts: Our choice of contexts was
motivated by intuition rather than experiments, so
we also took this opportunity to characterize the
contribution of each context to the results. We
did that by setting some of the context mixing-
coefficients to zero and leaving the others equally-
weighted. Figure 3 shows the MRR achieved with
each context. In that figure, the ?backoff? curve in-
dicates how well the simple context-free resolution
would do. The difference between the two small-
est and the two largest collections is immediately
apparent?this backoff is remarkably effective for the
smaller collections, and almost useless for the larger
ones, suggesting that the two smaller collections are
essentially much easier. The social context is clearly
quite useful, more so than any other single context,
for every collection. This tends to support our ex-
pectation that social networks can be as informative
as content networks in email collections. The topical
context also seems to be useful on its own. The con-
versational context is moderately useful on its own
in the larger collections. The local context alone is
not very informative for the larger collections.
Mixture of Contexts: The principal motivation for
combining different types of contexts is that differ-
ent sources may provide complementary evidence.
To characterize that effect, we look at combinations
of contexts. Figure 4 shows three such context com-
binations, anchored by the social context alone, with
a 100-day window (the results for 10 and 200 day
periods are similar). Reassuringly, adding more con-
texts (hence more evidence) turns out to be a rea-
Figure 4: Mixture of contexts, period set to 100 days.
sonable choice in most cases. For the full combi-
nation, we notice a drop in the effectiveness from
the addition of the topical context.5 This suggests
that the construction of the topical context may need
more careful design, and/or that learned ?k?s could
yield better evidence combination (since these re-
sults were obtained with equal ?k?s).
5 Conclusion
We have presented an approach to mention resolu-
tion in email that flexibly makes use of expanding
contexts to accurately resolve the identity of a given
mention. Our approach focuses on four naturally
occurring contexts in email, including a message,
a thread, other emails with senders and/or recipi-
ents in common, and other emails with significant
topical content in common. Our approach outper-
forms previously reported techniques and it scales
well to larger collections. Moreover, our results
serve to highlight the importance of social context
when resolving mentions in social media, which is
an idea that deserves more attention generally. In fu-
ture work, we plan to extend our test collection with
mention queries that must be resolved in the ?long
tail? of the identity distribution where less evidence
is available. We are also interested in exploring iter-
ative approaches to jointly resolving mentions.
Acknowledgments
The authors would like to thank Lise Getoor for her
helpful advice.
5This also occurs even when topical context is combined
with only social context.
948
References
Daniel J. Abadi. 2003. Comparing domain-specific and
non-domain-specific anaphora resolution techniques.
Cambridge University MPhil Dissertation.
Alfred V. Aho and Margaret J. Corasick. 1975. Effi-
cient string matching: an aid to bibliographic search.
In Communications of the ACM.
James Allan, editor. 2002. Topic detection and tracking:
event-based information organization. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
Bryce Allen. 1989. Recall cues in known-item retrieval.
JASIS, 40(4):246?252.
Omar Benjelloun, Hector Garcia-Molina, Hideki Kawai,
Tait Eliott Larson, David Menestrina, Qi Su, Sut-
thipong Thavisomboon, and Jennifer Widom. 2006.
Generic entity resolution in the serf project. IEEE
Data Engineering Bulletin, June.
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective
entity resolution in relational data. ACM Transactions
on Knowledge Discovery from Data, 1(1), March.
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis, May.
Chris Diehl, Lise Getoor, and Galileo Namata. 2006.
Name reference resolution in organizational email
archives. In Proceddings of SIAM International Con-
ference on Data Mining, Bethesda, MD , USA, April
20-22.
Tamer Elsayed and Douglas W. Oard. 2006. Modeling
identity in archival collections of email: A prelimi-
nary study. In Proceedings of the 2006 Conference
on Email and Anti-Spam (CEAS 06), pages 95?103,
Mountain View, California, July.
Ralf Holzer, Bradley Malin, and Latanya Sweeney. 2005.
Email alias detection using social network analysis. In
LinkKDD ?05: Proceedings of the 3rd international
workshop on Link discovery, pages 52?57, New York,
NY, USA. ACM Press.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Conference on Email and Anti-Spam,
Mountain view, CA, USA, July 30-31.
David D. Lewis and Kimberly A. Knowles. 1997.
Threading electronic mail: a preliminary study. Inf.
Process. Manage., 33(2):209?217.
David D. Lewis. 1996. The trec-4 filtering track. In The
Fourth Text REtrieval Conference (TREC-4), pages
165?180, Gaithersburg, Maryland.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic inte-
gration in text: from ambiguous names to identifiable
entities. AI Magazine. Special Issue on Semantic Inte-
gration, 26(1):45?58.
Bradley Malin. 2005. Unsupervised name disambigua-
tion via social network similarity. In Workshop on
Link Analysis, Counter-terrorism, and Security, in
conjunction with the SIAM International Conference
on Data Mining, Newport Beach, CA, USA, April 21-
23.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003, pages 33?40, Morristown,
NJ, USA. Association for Computational Linguistics.
Andrew McCallum, Andres Corrada-Emmanuel, and
XueruiWang Wang. 2005. Topic and role discovery
in social networks. In IJCAI.
Einat Minkov, William W. Cohen, and Andrew Y. Ng.
2006. Contextual search and name disambiguation in
email using graphs. In SIGIR ?06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 27?34, New York, NY, USA. ACM Press.
Patric Reuther. 2006. Personal name matching: New test
collections and a social network based approach.
949
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 265?268,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pairwise Document Similarity in Large Collections with MapReduce
Tamer Elsayed,?Jimmy Lin,? and Douglas W. Oard?
Human Language Technology Center of Excellence and
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
{telsayed,jimmylin,oard}@umd.edu
Abstract
This paper presents a MapReduce algorithm
for computing pairwise document similarity
in large document collections. MapReduce is
an attractive framework because it allows us
to decompose the inner products involved in
computing document similarity into separate
multiplication and summation stages in a way
that is well matched to efficient disk access
patterns across several machines. On a col-
lection consisting of approximately 900,000
newswire articles, our algorithm exhibits lin-
ear growth in running time and space in terms
of the number of documents.
1 Introduction
Computing pairwise similarity on large document
collections is a task common to a variety of prob-
lems such as clustering and cross-document coref-
erence resolution. For example, in the PubMed
search engine,1 which provides access to the life sci-
ences literature, a ?more like this? browsing feature
is implemented as a simple lookup of document-
document similarity scores, computed offline. This
paper considers a large class of similarity functions
that can be expressed as an inner product of term
weight vectors.
For document collections that fit into random-
access memory, the solution is straightforward. As
collection size grows, however, it ultimately be-
comes necessary to resort to disk storage, at which
point aligning computation order with disk access
patterns becomes a challenge. Further growth in the
?Department of Computer Science
?The iSchool, College of Information Studies
1http://www.ncbi.nlm.nih.gov/PubMed
document collection will ultimately make it desir-
able to spread the computation over several proces-
sors, at which point interprocess communication be-
comes a second potential bottleneck for which the
computation order must be optimized. Although
tailored implementations can be designed for spe-
cific parallel processing architectures, the MapRe-
duce framework (Dean and Ghemawat, 2004) offers
an attractive solution to these challenges. In this pa-
per, we describe how pairwise similarity computa-
tion for large collections can be efficiently imple-
mented with MapReduce. We empirically demon-
strate that removing high frequency (and therefore
low entropy) terms results in approximately linear
growth in required disk space and running time with
increasing collection size for collections containing
several hundred thousand documents.
2 MapReduce Framework
MapReduce builds on the observation that many
tasks have the same structure: a computation is ap-
plied over a large number of records (e.g., docu-
ments) to generate partial results, which are then ag-
gregated in some fashion. Naturally, the per-record
computation and aggregation vary by task, but the
basic structure remains fixed. Taking inspiration
from higher-order functions in functional program-
ming, MapReduce provides an abstraction that in-
volves the programmer defining a ?mapper? and a
?reducer?, with the following signatures:
map: (k1, v1)? [(k2, v2)]
reduce: (k2, [v2])? [(k3, v3)]
Key/value pairs form the basic data structure in
MapReduce. The ?mapper? is applied to every input
265
Shu
fflin
g: g
rou
p va
lues
 by 
key
s
ma
p
ma
p
ma
p
ma
p
red
uce
red
uce
red
uce
inp
ut
inp
ut
inp
ut
inp
ut
out
put
out
put
out
put
Figure 1: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The ?reducer? is applied
to all values associated with the same intermediate
key to generate output key/value pairs (see Figure 1).
On top of a distributed file system (Ghemawat
et al, 2003), the runtime transparently handles all
other aspects of execution (e.g., scheduling and fault
tolerance), on clusters ranging from a few to a few
thousand nodes. MapReduce is an attractive frame-
work because it shields the programmer from dis-
tributed processing issues such as synchronization,
data exchange, and load balancing.
3 Pairwise Document Similarity
Our work focuses on a large class of document simi-
larity metrics that can be expressed as an inner prod-
uct of term weights. A document d is represented as
a vector Wd of term weights wt,d, which indicate
the importance of each term t in the document, ig-
noring the relative ordering of terms (?bag of words?
model). We consider symmetric similarity measures
defined as follows:
sim(di, dj) =
?
t?V
wt,di ? wt,dj (1)
where sim(di, dj) is the similarity between docu-
ments di and dj and V is the vocabulary set. In this
type of similarity measure, a term will contribute to
the similarity between two documents only if it has
non-zero weights in both. Therefore, t ? V can be
replaced with t ? di ? dj in equation 1.
Generalizing this to the problem of computing
similarity between all pairs of documents, we note
Algorithm 1 Compute Pairwise Similarity Matrix
1: ?i, j : sim[i, j]? 0
2: for all t ? V do
3: pt ? postings(t)
4: for all di, dj ? pt do
5: sim[i, j]? sim[i, j] + wt,di ? wt,dj
that a term contributes to each pair that contains it.2
For example, if a term appears in documents x, y,
and z, it contributes only to the similarity scores be-
tween (x, y), (x, z), and (y, z). The list of docu-
ments that contain a particular term is exactly what
is contained in the postings of an inverted index.
Thus, by processing all postings, we can compute
the entire pairwise similarity matrix by summing
term contributions.
Algorithm 1 formalizes this idea: postings(t) de-
notes the list of documents that contain term t. For
simplicity, we assume that term weights are also
stored in the postings. For small collections, this al-
gorithm can be run efficiently to compute the entire
similarity matrix in memory. For larger collections,
disk access optimization is needed?which is pro-
vided by the MapReduce runtime, without requiring
explicit coordination.
We propose an efficient solution to the pairwise
document similarity problem, expressed as two sep-
arate MapReduce jobs (illustrated in Figure 2):
1) Indexing: We build a standard inverted in-
dex (Frakes and Baeza-Yates, 1992), where each
term is associated with a list of docid?s for docu-
ments that contain it and the associated term weight.
Mapping over all documents, the mapper, for each
term in the document, emits the term as the key, and
a tuple consisting of the docid and term weight as the
value. The MapReduce runtime automatically han-
dles the grouping of these tuples, which the reducer
then writes out to disk, thus generating the postings.
2) Pairwise Similarity: Mapping over each post-
ing, the mapper generates key tuples corresponding
to pairs of docids in the postings: in total, 12m(m?1)
pairs where m is the posting length. These key tu-
ples are associated with the product of the corre-
sponding term weights?they represent the individ-
2Actually, since we focus on symmetric similarity functions,
we only need to compute half the pairs.
266
d 1
(A,(
d 1,
2))
(B,(
d 1,
1))
(C,(
d 1,
1))
(B,(
d 2,
1))
(D,(
d 2,
2))
(A,(
d 3,
1))
(B,(
d 3,
2))
(E,(
d 3,
1))
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
d 2 d 3
((d 1
,d 3
),2)
((d 1
,d 2
),1)
((d 1
,d 3
),2)
((d 2
,d 3
),2)
((d 1
,d 2
),[1]
)
((d 1
,d 3
),[2, 2
])
((d 2
,d 3
),[2]
)
((d 1
,d 2
),1)
((d 1
,d 3
),4)
((d 2
,d 3
),2)
?A 
A B
 
C?
?B 
D D
?
?A 
B B
 
E?
ma
p
ma
p
ma
p
re
duc
e
re
duc
e
re
duc
e
ma
p
ma
p
ma
p
shu
ffle
ma
p
ma
p
shu
ffle
Ind
ex
ing
Pa
irw
ise
Sim
ilar
ity
re
duc
e
re
duc
e
re
duc
e
re
duc
e
re
duc
e
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
Figure 2: Computing pairwise similarity of a toy collection of 3 documents. A simple term weighting scheme (wt,d =
tft,d) is chosen for illustration.
ual term contributions to the final inner product. The
MapReduce runtime sorts the tuples and then the re-
ducer sums all the individual score contributions for
a pair to generate the final similarity score.
4 Experimental Evaluation
In our experiments, we used Hadoop ver-
sion 0.16.0,3 an open-source Java implementation
of MapReduce, running on a cluster with 20 ma-
chines (1 master, 19 slave). Each machine has two
single-core processors (running at either 2.4GHz or
2.8GHz), 4GB memory, and 100GB disk.
We implemented the symmetric variant of Okapi-
BM25 (Olsson and Oard, 2007) as the similarity
function. We used the AQUAINT-2 collection of
newswire text, containing 906k documents, totaling
approximately 2.5 gigabytes. Terms were stemmed.
To test the scalability of our technique, we sampled
the collection into subsets of 10, 20, 25, 50, 67, 75,
80, 90, and 100 percent of the documents.
After stopword removal (using Lucene?s stop-
word list), we implemented a df-cut, where a frac-
tion of the terms with the highest document frequen-
cies is eliminated.4 This has the effect of remov-
ing non-discriminative terms. In our experiments,
we adopt a 99% cut, which means that the most fre-
quent 1% of terms were discarded (9,093 terms out
of a total vocabulary size of 909,326). This tech-
nique greatly increases the efficiency of our algo-
rithm, since the number of tuples emitted by the
3http://hadoop.apache.org/
4In text classification, removal of rare terms is more com-
mon. Here we use df-cut to remove common terms.
R2  = 0.
997
020406080100120140 0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Computation Time (minutes)
Figure 3: Running time of pairwise similarity compar-
isons, for subsets of AQUAINT-2.
mappers in the pairwise similarity phase is domi-
nated by the length of the longest posting (in the
worst case, if a term appears in all documents, it
would generate approximately 1012 tuples).
Figure 3 shows the running time of the pairwise
similarity phase for different collection sizes.5 The
computation for the entire collection finishes in ap-
proximately two hours. Empirically, we find that
running time increases linearly with collection size,
which is an extremely desirable property. To get a
sense of the space complexity, we compute the num-
ber of intermediate document pairs that are emit-
ted by the mappers. The space savings are large
(3.7 billion rather than 8.1 trillion intermediate pairs
for the entire collection), and space requirements
grow linearly with collection size over this region
(R2 = 0.9975).
5The entire collection was indexed in about 3.5 minutes.
267
01,0002,0003,0004,0005,0006,0007,0008,0009,000
0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Intermediate Pairs (billions)
df-cut
 
at 99%
df-cut
 
at 99.9
%
df-cut
 
at 99.9
9%
df-cut
 
at 99.9
99%
no df-
cut
Figure 4: Effect of changing df -cut thresholds on the
number of intermediate document-pairs emitted, for sub-
sets of AQUAINT-2.
5 Discussion and Future Work
In addition to empirical results, it would be desir-
able to derive an analytical model of our algorithm?s
complexity. Here we present a preliminary sketch of
such an analysis and discuss its implications. The
complexity of our pairwise similarity algorithm is
tied to the number of document pairs that are emit-
ted by the mapper, which equals the total number of
products required in O(N2) inner products, where
N is the collection size. This is equal to:
1
2
?
t?V
dft(dft ? 1) (2)
where dft is the document frequency, or equivalently
the length of the postings for term t. Given that to-
kens in natural language generally obey Zipf?s Law,
and vocabulary size and collection size can be re-
lated via Heap?s Law, it may be possible to develop
a closed form approximation to the above series.
Given the necessity of computing O(N2) inner
products, it may come as a surprise that empirically
our algorithm scales linearly (at least for the collec-
tion sizes we explored). We believe that the key to
this behavior is our df-cut technique, which elimi-
nates the head of the df distribution. In our case,
eliminating the top 1% of terms reduces the number
of document pairs by several orders of magnitude.
However, the impact of this technique on effective-
ness (e.g., in a query-by-example experiment) has
not yet been characterized. Indeed, a df-cut thresh-
old of 99% might seem rather aggressive, removing
meaning-bearing terms such as ?arthritis? and ?Cor-
nell? in addition to perhaps less problematic terms
such as ?sleek? and ?frail.? But redundant use of
related terms is common in news stories, which we
would expect to reduce the adverse effect on many
applications of removing these low entropy terms.
Moreover, as Figure 4 illustrates, relaxing the df-
cut to a 99.9% threshold still results in approxi-
mately linear growth in the requirement for interme-
diate storage (at least over this region).6 In essence,
optimizing the df-cut is an efficiency vs. effective-
ness tradeoff that is best made in the context of a
specific application. Finally, we note that alternative
approaches to similar problems based on locality-
sensitive hashing (Andoni and Indyk, 2008) face
similar tradeoffs in tuning for a particular false pos-
itive rate; cf. (Bayardo et al, 2007).
6 Conclusion
We present a MapReduce algorithm for efficiently
computing pairwise document similarity in large
document collections. In addition to offering spe-
cific benefits for a number of real-world tasks, we
also believe that our work provides an example of
a programming paradigm that could be useful for a
broad range of text analysis problems.
Acknowledgments
This work was supported in part by the Intramural
Research Program of the NIH/NLM/NCBI.
References
A. Andoni and P. Indyk. 2008. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. CACM, 51(1):117?122.
R. Bayardo, Y. Ma, and R. Srikant. 2007. Scaling up all
pairs similarity search. In WWW ?07.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI ?04.
W. Frakes and R. Baeza-Yates. 1992. Information Re-
trieval: Data Structures and Algorithms.
S. Ghemawat, H. Gobioff, and S. Leung. 2003. The
Google File System. In SOSP ?03.
J. Olsson and D. Oard. 2007. Improving text classifi-
cation for oral history archives with temporal domain
knowledge. In SIGIR ?07.
6More recent experiments suggest that a df-cut of 99.9% re-
sults in almost no loss of effectiveness on a query-by-example
task, compared to no df-cut.
268
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
