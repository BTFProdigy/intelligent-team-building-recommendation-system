Base Noun Phrase Translation
Using Web Data and the EM Algorithm
Yunbo Cao
Microsoft Research Asia
i-yuncao@microsoft.com
Hang Li
Microsoft Research Asia
hangli@microsoft.com
Abstract
We consider here the problem of Base Noun
Phrase translation. We propose a new method
to perform the task. For a given Base NP, we
first search its translation candidates from the
web. We next determine the possible
translation(s) from among the candidates
using one of the two methods that we have
developed. In one method, we employ an
ensemble of Na?ve Bayesian Classifiers
constructed with the EM Algorithm. In the
other method, we use TF-IDF vectors also
constructed with the EM Algorithm.
Experimental results indicate that the
coverage and accuracy of our method are
significantly better than those of the baseline
methods relying on existing technologies.
1. Introduction
We address here the problem of Base NP
translation, in which for a given Base Noun
Phrase in a source language (e.g., ?information
age? in English), we are to find out its possible
translation(s) in a target language (e.g., ?
? in Chinese).
We define a Base NP as a simple and
non-recursive noun phrase. In many cases, Base
NPs represent holistic and non-divisible concepts,
and thus accurate translation of them from one
language to another is extremely important in
applications like machine translation, cross
language information retrieval, and foreign
language writing assistance.
In this paper, we propose a new method for
Base NP translation, which contains two steps: (1)
translation candidate collection, and (2)
translation selection. In translation candidate
collection, for a given Base NP in the source
language, we look for its translation candidates in
the target language. To do so, we use a
word-to-word translation dictionary and corpus
data in the target language on the web. In
translation selection, we determine the possible
translation(s) from among the candidates. We use
non-parallel corpus data in the two languages on
the web and employ one of the two methods
which we have developed. In the first method, we
view the problem as that of classification and
employ an ensemble of Na?ve Bayesian
Classifiers constructed with the EM Algorithm.
We will use ?EM-NBC-Ensemble? to denote this
method, hereafter. In the second method, we view
the problem as that of calculating similarities
between context vectors and use TF-IDF vectors
also constructed with the EM Algorithm. We will
use ?EM-TF-IDF? to denote this method.
Experimental results indicate that our method
is very effective, and the coverage and top 3
accuracy of translation at the final stage are
91.4% and 79.8%, respectively. The results are
significantly better than those of the baseline
methods relying on existing technologies. The
higher performance of our method can be
attributed to the enormity of the web data used
and the employment of the EM Algorithm.
2. Related Work
2.1 Translation with Non-parallel
Corpora
A straightforward approach to word or phrase
translation is to perform the task by using parallel
bilingual corpora (e.g., Brown et al 1993).
Parallel corpora are, however, difficult to obtain
in practice.
To deal with this difficulty, a number of
methods have been proposed, which make use of
relatively easily obtainable non-parallel corpora
(e.g., Fung and Yee, 1998; Rapp, 1999; Diab and
Finch, 2000). Within these methods, it is usually
assumed that a number of translation candidates
for a word or phrase are given (or can be easily
collected) and the problem is focused on
translation selection.
All of the proposed methods manage to find out
the translation(s) of a given word or phrase, on
the basis of the linguistic phenomenon that the
contexts of a translation tend to be similar to the
contexts of the given word or phrase. Fung and
Yee (1998), for example, proposed to represent
the contexts of a word or phrase with a
real-valued vector (e.g., a TF-IDF vector), in
which one element corresponds to one word in
the contexts. In translation selection, they select
the translation candidates whose context vectors
are the closest to that of the given word or phrase.
Since the context vector of the word or phrase
to be translated corresponds to words in the
source language, while the context vector of a
translation candidate corresponds to words in the
target language, and further the words in the
source language and those in the target language
have a many-to-many relationship (i.e.,
translation ambiguities), it is necessary to
accurately transform the context vector in the
source language to a context vector in the target
language before distance calculation.
The vector-transformation problem was not,
however, well-resolved previously. Fung and
Yee assumed that in a specific domain there is
only one-to-one mapping relationship between
words in the two languages. The assumption is
reasonable in a specific domain, but is too strict in
the general domain, in which we presume to
perform translation here. A straightforward
extension of Fung and Yee?s assumption to the
general domain is to restrict the many-to-many
relationship to that of many-to-one mapping (or
one-to-one mapping). This approach, however,
has a drawback of losing information in vector
transformation, as will be described.
For other methods using non-parallel corpora,
see also (Tanaka and Iwasaki, 1996; Kikui, 1999,
Koehn and Kevin 2000; Sumita 2000; Nakagawa
2001; Gao et al 2001).
2.2 Translation Using Web Data
Web is an extremely rich source of data for
natural language processing, not only in terms of
data size but also in terms of data type (e.g.,
multilingual data, link data). Recently, a new
trend arises in natural language processing, which
tries to bring some new breakthroughs to the field
by effectively using web data (e.g., Brill et al
2001).
Nagata et al(2001), for example, proposed to
collect partial parallel corpus data on the web to
create a translation dictionary. They observed
that there are many partial parallel corpora
between English and Japanese on the web, and
most typically English translations of Japanese
terms (words or phrases) are parenthesized and
inserted immediately after the Japanese terms in
documents written in Japanese.
3. Base Noun Phrase Translation
Our method for Base NP translation comprises of
two steps: translation candidate collection and
translation selection. In translation candidate
collection, we look for translation candidates of a
given Base NP. In translation selection, we find
out possible translation(s) from the translation
candidates.
In this paper, we confine ourselves to
translation of noun-noun pairs from English to
Chinese; our method, however, can be extended
to translations of other types of Base NPs
between other language pairs.
3.1 Translation Candidate Collection
We use heuristics for translation candidate
collection. Figure 1 illustrates the process of
collecting Chinese translation candidates for an
English Base NP ?information age? with the
heuristics.
1. Input ?information age?;
2. Consult English-Chinese word translation dictionary:
information ->
age -> (how old somebody is)
 (historical era)
 (legal adult hood)
3. Compositionally create translation candidates in
Chinese:
;;
4. Search the candidates on web sites in Chinese and
obtain the document frequencies of them (i.e., numbers
of documents containing them):
 10000
 10
 0
5. Output candidates having non-zero document
frequencies and the document frequencies:
 10000
 10
Figure 1. Translation candidate collection
3.2 Translation Selection --
EM-NBC-Ensemble
We view the translation selection problem as that
of classification and employ EM-NBC-Ensemble
to perform the task. For the ease of explanation,
we first describe the algorithm of using only
EM-NBC and next extend it to that of using
EM-NBC-Ensemble.
Basic Algorithm
Let e~ denote the Base NP to be translated and C~
the set of its translation candidates (phrases).
Suppose that kC =|~| . Let c~ represent a random
variable on C~ . Let E denote a set of words in
English, and C a set of words in Chinese.
Suppose that nCmE == ||and|| . Let e
represent a random variable on E and c a random
variable on C. Figure 2 describes the algorithm.
Input: e~ , C~ , contexts containing e~ , contexts containing all
Cc ~~ ? ;
1. create a frequency vector )),(,),(),(( 21 mefefef L
),,1(, miEei L=? using contexts containing e~ ;
transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L
),,1(, niCci L=? , using a translation dictionary
and the EM algorithm;
2. for each ( Cc ~~ ? ){
estimate with Maximum Likelihood Estimation the prior
probability )~(cP using contexts containing all Cc ~~ ? ;
create a frequency vector )),(,),(),(( 21 ncfcfcf L
),,1(, niCci L=? using contexts containing c~ ;
normalize the frequency vector , yielding
),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =? ;
calculate the posterior probability )|~( DcP with EM-NBC
(generally EM-NBC-Ensemble), where
),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D
3. Sort Cc ~~ ? in descending order of )|~( DcP ;
Output: the top sorted results
Figure 2. Algorithm of EM-NBC-Ensemble
Context Information
As input data, we use ?contexts? in English which
contain the phrase to be translated. We also use
contexts in Chinese which contain the translation
candidates.
Here, a context containing a phrase is defined
as the surrounding words within a window of a
predetermined size, which window covers the
phrase. We can easily obtain the data by
searching for them on the web. Actually, the
contexts containing the candidates are obtained at
the same time when we conduct translation
candidate collection (Step 4 in Figure 1).
EM Algorithm
We define a relation between E and C
as CER ?? , which represents the links in a
translation dictionary. We further define
}),(|{ Rceec ?=? .
At Step 1, we assume that all the instances in
))(),..,(),(( 21 mefefef are independently generated
according to the distribution defined as:
?
?
=
Cc
cePcPeP )|()()( (1)
We estimate the parameters of the distribution by
using the Expectation and Maximization (EM)
Algorithm (Dempster et al, 1977).
Initially, we set for all Cc ?
||
1)(
C
cP = ,
??
??
?
??
??
?=
c
c
c
e
e
ceP
if,0
if,||
1
)|(
Next, we estimate the parameters by iteratively
updating them, until they converge (cf., Figure 3).
Finally, we calculate )(cf E for all Cc ? as:?
?
=
Ee
E efcPcf )()()( (2)
In this way, we can transform the frequency
vector in English ))(),..,(),(( 21 mefefef into a vector
in Chinese ))(),..,(),(( 21 nEEE cfcfcf=D .
Prior Probability Estimation
At Step 2, we approximately estimate the prior
probability )~(cP by using the document
frequencies of the translation candidates. The
data are obtained when we conduct candidate
collection (Step 4 in Figure 1).
?
?
?
?
?
?
?
??
??
Ee
Ee
Cc
ecPef
ecPef
ceP
ecPefcP
cePcP
cePcP
ecP
)|()(
)|()()|(
)|()()(StepM
)|()(
)|()()|(StepE
Figure 3. EM Algorithm
EM-NBC
At Step 2, we use an EM-based Na?ve
Bayesian Classifier (EM-NBC) to select the
candidates c~ whose posterior probabilities are
the largest:
??
???
?
+= ?
??
?
)~|(log)()~(logmaxarg
)|~(maxarg
~
~
~
~
ccPcfcP
cP
Cc
E
Cc
Cc
D
(3)
Equation (3) is based on Bayes? rule and the
assumption that the data in D are independently
generated from CcccP ?),~|( .
In our implementation, we use an equivalent
??
???
?
?? ?
??
)~|(log)()~(logminarg
~
~
ccPcfcP
Cc
E
Cc
? (4)
where 1?? is an additional parameter used to
emphasize the prior information. If we ignore the
first term in Equation (4), then the use of one
EM-NBC turns out to select the candidate whose
frequency vector is the closest to the transformed
vector D in terms of KL divergence (cf., Cover
and Tomas 1991).
EM-NBC-Ensemble
To further improve performance, we use an
ensemble (i.e., a linear combination) of
EM-NBCs (EM-NBC-Ensemble), while the
classifiers are constructed on the basis of the data
in different contexts with different window sizes.
More specifically, we calculate
where s),1,(i, L=iD denotes the data in different
contexts.
3.3 Translation Selection -- EM-TF-IDF
We view the translation selection problem as that
of calculating similarities between context
vectors and use as context vectors TF-IDF
vectors constructed with the EM Algorithm.
Figure 4 describes the algorithm in which we use
the same notations as those in
EM-NBC-Ensemble.
The idf value of a Chinese word c is calculated
in advance and as
)/)(log()( Fcdfcidf ?= (6)
where )cdf( denotes the document frequency of
c and F the total document frequency.
Input: e~ , C~ , contexts containing e~ , contexts containing
all Cc ~~ ? , Cc),cidf( ? ;
1. create a frequency vector )),(,),(),(( 21 mefefef L
),,1(, miEei L=? using contexts containing e~ ;
transforming the vector into
21 )),c(f,),c(f),c(f( nEEE L
),,1(, niCci L=? , using a translation dictionary and
the EM algorithm;
create a TF-IDF vector
11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=?
2. for each ( Cc ~~ ? ){
create a frequency vector )),(,),(),(( 21 ncfcfcf L
),,1(, niCci L=? using contexts containing c~ ;
create a TF-IDF vector
11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=? ;
calculate ),cos()c~tfidf( BA= ; }
3. Sort Cc ~~ ? in descending order of )c~tfidf( ;
Output: the top sorted results
Figure 4. Algorithm of EM-TF-IDF
3.4 Advantage of Using EM Algorithm
The uses of EM-NBC-Ensemble and EM-TF-IDF
can be viewed as extensions of existing methods
for word or phrase translation using non-parallel
corpora. Particularly, the use of the EM
Algorithm can help to accurately transform a
frequency vector from one language to another.
Suppose that we are to determine if ?
? is a translation of ?information age? (actually
it is). The frequency vectors of context words for
?information age? and ?? are given in A
and D in Figure 5, respectively. If for each
English word we only retain the link connecting
to the Chinese translation with the largest
frequency (a link represented as a solid line) to
establish a many-to-one mapping and transform
vector A from English to Chinese, we obtain
vector B. It turns out, however, that vector B is
quite different from vector D, although they
should be similar to each other. We will refer to
this method as ?Major Translation? hereafter.
With EM, vector A in Figure 5 is transformed
into vector C, which is much closer to vector D,
as expected. Specifically, EM can split the
frequency of a word in English and distribute
them into its translations in Chinese in a
theoretically sound way (cf., the distributed
frequencies of ?internet?). Note that if we assume
a many-to-one (or one-to-one) mapping
?
=
=
s
i
icP
s
cP
1
)|~(1)|~( DD (5)
relationship, then the use of EM turns out to be
equivalent to that of Major Translation.
3.5 Combination
In order to further boost the performance of
translation, we propose to also use the translation
method proposed in Nagata et al Specifically, we
combine our method with that of Nagata et alby
using a back-off strategy.
Figure 6 illustrates the process of collecting
Chinese translation candidates for an English
Base NP ?information asymmetry? with Nagata et
al?s method.
In the combination of the two methods, we first
use Nagata et als method to perform translation;
if we cannot find translations, we next use our
method. We will denote this strategy ?Back-off?.
4. Experimental Results
We conducted experiments on translation of the
Base NPs from English to Chinese.
We extracted Base NPs (noun-noun pairs) from
the Encarta 1 English corpus using the tool
developed by Xun et al(2000). There were about
1 http://encarta.msn.com/Default.asp
3000 Base NPs extracted. In the experiments, we
used the HIT English-Chinese word translation
dictionary2 . The dictionary contains about 76000
Chinese words, 60000 English words, and
118000 translation links. As a web search engine,
we used Google (http://www.google.com).
Five translation experts evaluated the
translation results by judging whether or not they
were acceptable. The evaluations reported below
are all based on their judgements.
4.1 Basic Experiment
In the experiment, we randomly selected 1000
Base NPs from the 3000 Base NPs. We next used
our method to perform translation on the 1000
phrases. In translation selection, we employed
EM-NBC-Ensemble and EM-TF-IDF.
Table 1. Best translation result for each method
Accuracy (%)
Top 1 Top 3
Coverage
(%)
EM-NBC-Ensemble 61.7 80.3
Prior 57.6 77.6
MT-NBC-Ensemble 59.9 78.1
EM-KL-Ensemble 45.9 72.3
EM-NBC 60.8 78.9
EM-TF-IDF 61.9 80.8
MT-TF-IDF 58.2 77.6
EM-TF 55.8 77.8
89.9
Table 1 shows the results in terms of coverage
and top n accuracy. Here, coverage is defined as
the percentage of phrases which have translations
selected, while top n accuracy is defined as the
percentage of phrases whose selected top n
translations include correct translations.
For EM-NBC-Ensemble, we set the ? !in (4) to
be 5 on the basis of our preliminary experimental
results. For EM-TF-IDF, we used the non-web
data described in Section 4.4 to estimate idf
values of words. We used contexts with window
sizes of ?1, ?3, ?5, ?7, ?9, ?11.
2 The dictionary is created by the Harbin Institute of Technology.
A B C D


















Figure 5. Example of frequency vector transformation
1. Input ?information asymmetry?;
2. Search the English Base NP on web sites in Chinese
and obtain documents as follows (i.e., using partial parallel
corpora):
	


Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 334?342, Prague, June 2007. c?2007 Association for Computational Linguistics
Low-Quality Product Review Detection in Opinion Summarization 
Jingjing Liu 
Nankai University 
Tianjin, China 
v-jingil@microsoft.com 
Yunbo Cao 
Microsoft Research Asia 
Beijing, China 
yucao@microsoft.com 
 
Chin-Yew Lin 
Microsoft Research Asia 
Beijing, China 
cyl@microsoft.com 
Yalou Huang 
Nankai University 
Tianjin, China 
huangyl@nankai.edu.cn 
Ming Zhou 
Microsoft Research Asia 
Beijing, China 
mingzhou@microsoft.com 
Abstract 
Product reviews posted at online shopping 
sites vary greatly in quality. This paper ad-
dresses the problem of detecting low-
quality product reviews. Three types of bi-
ases in the existing evaluation standard of 
product reviews are discovered. To assess 
the quality of product reviews, a set of spe-
cifications for judging the quality of re-
views is first defined. A classification-
based approach is proposed to detect the 
low-quality reviews. We apply the pro-
posed approach to enhance opinion sum-
marization in a two-stage framework. Ex-
perimental results show that the proposed 
approach effectively (1) discriminates low-
quality reviews from high-quality ones and 
(2) enhances the task of opinion summari-
zation by detecting and filtering low-
quality reviews. 
1 Introduction 
In the past few years, there has been an increasing 
interest in mining opinions from product reviews 
(Pang, et al 2002; Liu, et al 2004; Popescu and 
Etzioni, 2005). However, due to the lack of 
editorial and quality control, reviews on products 
vary greatly in quality. Thus, it is crucial to have a 
mechanism capable of assessing the quality of 
reviews and detecting low-quality/noisy reviews.  
Some shopping sites already provide a function 
of assessing the quality of reviews. For example, 
Amazon1 allows users to vote for the helpfulness 
of each review and then ranks the reviews based on 
the accumulated votes. However, according to our 
survey in Section 3, users? votes at Amazon have 
three kinds of biases as follows: (1) imbalance vote 
bias, (2) winner circle bias, and (3) early bird bias. 
Existing studies (Kim et al 2006; Zhang and Va-
radarajan, 2006) used these users? votes for train-
ing ranking models to assess the quality of reviews, 
which therefore are subject to these biases.  
In this paper, we demonstrate the aforemen-
tioned biases and define a standard specification to 
measure the quality of product reviews. We then 
manually annotate a set of ground-truth with real 
world product review data conforming to the speci-
fication.  
To automatically detect low-quality product re-
views, we propose a classification-based approach 
learned from the annotated ground-truth. The pro-
posed approach explores three aspects of product 
reviews, namely informativeness, readability, and 
subjectiveness.  
We apply the proposed approach to opinion 
summarization, a typical opinion mining task. The 
proposed approach enhances the existing work in a 
two-stage framework, where the low-quality re-
view detection is applied right before the summari-
zation stage.  
Experimental results show that the proposed ap-
proach can discriminate low-quality reviews from 
high-quality ones effectively. In addition, the task 
of opinion summarization can be enhanced by de-
tecting and filtering low-quality reviews. 
                                                 
1 http://www.amazon.com 
334
The rest of the paper is organized as follows: 
Section 2 introduces the related work. In Section 3, 
we define the quality of product reviews. In Sec-
tion 4, we present our approach to detecting low-
quality reviews. In Section 5, we empirically verify 
the effectiveness of the proposed approach and its 
use for opinion summarization. Section 6 summa-
rizes our work in this paper and points out the fu-
ture work. 
2 Related Work 
2.1 Evaluating Helpfulness of Reviews 
The problem of evaluating helpfulness of reviews 
(Kim et al 2006), also known as learning utility of 
reviews (Zhang and Varadarajan, 2006), is quite 
similar to our problem of assessing the quality of 
reviews.  
In practice, researchers in this area considered 
the problem as a ranking problem and solved it 
with regression models. In the process of model 
training and testing, they used the ground-truth 
derived from users? votes of helpfulness provided 
by Amazon. As we will show later in Section 3, 
these models all suffered from three types of vot-
ing bias.  
In our work, we avoid using users? votes by de-
veloping a specification on the quality of reviews 
and building a ground-truth according to the speci-
fication.  
2.2 Mining Opinions from Reviews 
One area of research on opinion mining from 
product reviews is to judge whether a review 
expresses a positive or a negative opinion. For 
example, Turney (2006) presented a simple 
unsupervised learning algorithm in judging 
reviews as ?thumbs up? (recommended) or 
?thumbs down? (not recommended). Pang et al
(2002) considered the same problem and presented 
a set of supervised machine learning approaches to 
it. For other work see also Dave et al (2003), Pang 
and Lee (2004, 2005). 
Another area of research on opinion mining is to 
extract and summarize users? opinions from prod-
uct reviews (Hu and Liu, 2004; Liu et al, 2005; 
Popescu and Etzioni, 2005). Typically, a sentence 
or a text segment in the reviews is treated as the 
basic unit. The polarity of users? sentiments on a 
product feature in each unit is extracted. Then the 
aggregation of the polarities of individual senti-
ments is presented to users so that they can have an 
at-a-glance view on how other experienced users 
rated on a certain product. The major weakness in 
the existing studies is that all the reviews, includ-
ing low-quality ones, are taken into consideration 
and treated equally for generating the summary. In 
this paper, we enhance the application by detecting 
and filtering low-quality reviews. In order to 
achieve that, we first define what the quality of 
reviews is. 
3 Quality of Product Reviews 
In this section, we will first show three biases of 
users? votes observed on Amazon, and then present 
our specification on the quality of product reviews. 
3.1 Amazon Ground-truth 
In our study, we use the product reviews on digital 
cameras crawled from Amazon as our data set. The 
data set consists of 23,141 reviews on 946 digital 
cameras. At the Amazon site, users could vote for 
a review with a ?helpful? or ?unhelpful? label. 
Thus, for each review there are two numbers 
indicating the statistics of these two labels, namely 
the number of ?helpful? votes and that of 
?unhelpful? ones. Kim et al(2006) used the 
percentage of ?helpful? votes as the measure of 
evaluating the ?quality of reviews? in their 
experiments. We call the ground-truth based on 
this measure as ?Amazon ground-truth?. 
Certainly, the ground-truth has the advantage of 
convenience. However, we identify three types of 
biases that make the Amazon ground-truth not al-
ways suitable for determining the quality of re-
views. We describe these biases in details in the 
rest of this section. 
3.1.1 Imbalance Vote Bias 
 
Figure 1. Reviews? percentage scores 
At the Amazon site, users tend to value others? 
opinions positively rather than negatively. From 
Figure 1, we can see that a half of the 23,141 
0
2000
4000
6000
8000
10000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
# 
R
e
vi
e
w
s
Percentage of 'helpful' votes
335
reviews (corresponding to the two bars on the right 
of the figure) have more than 90% ?helpful? votes, 
including 9,100 reviews with 100% ?helpful? 
votes. From an in-depth investigation on these 
highly-voted reviews, we observed that some did 
not really have as good quality as the votes hint. 
For example, in Figure 2, the review about Canon 
PowerShot S500 receives 40 ?helpful? votes out of 
40 votes although it only gives very brief 
description on the product features in its second 
paragraph. We call this type of bias ?imbalance 
vote? bias. 
 
This is my second Canon digital elph camera. Both were great 
cameras. Recently upgraded to the S500. About 6 months later I get 
the dreaded E18 error. I searched the Internet and found numerous 
people having problems. When I determined the problem to be the 
lens not fully extending I decided to give it a tug. It clicked and the 
camera came on, ready to take pictures. Turning it off and on pro-
duced the E18 again. While turning it on I gave it a nice little bump 
on the side (where the USB connector is) and the lens popped out 
on its own. No problems since. 
 It?s a nice compact and light camera and takes great photos and 
videos. Only complaint (other than E18) is the limit of 30-second 
videos on 640x480 mode. I've got a 512MB compact flash card, I 
should be able to take as much footage as I have memory in one 
take. 
Figure 2. An example review 
3.1.2 Winner Circle Bias 
 
Figure 3. Votes of the top-50 ranked reviews 
There also exists a bootstrapping effect of ?hot? 
reviews at the Amazon site. Figure 3 shows the 
?helpful? votes for the top 50 ranked reviews. The 
numbers are averaged over 127 digital cameras 
which have no less than 50 reviews. As shown in 
this figure, the top two reviews hold more than 250 
and 140 votes respectively on average; while the 
numbers of votes held by lower-ranked reviews 
decrease exponentially. This is so-called the 
?winner circle? bias: the more votes a review 
gains, the more default authority it would appear to 
the readers, which in turn will influence the 
objectivity of the readers? votes. Also, the higher 
ranked reviews would attract more eyeballs and 
therefore gain more people?s votes. This mutual 
influence among labelers should be avoided when 
the votes are used as the evaluation standard. 
3.1.3 Early Bird Bias 
 
   
Figure 4. Dependency on publication date 
Publication date can influence the accumulation of 
users? votes. In Figure 4, the n?th publication date 
represents the n?th month after the product is 
released. The number in the figure is averaged over 
all the digital cameras in the data set. We can 
observe a clear trend that the earlier a review is 
posted, the more votes it will get. This is simply 
because reviews posted earlier are exposed to users 
for a longer time. Therefore, some high quality 
reviews may get fewer users? vote because of later 
publication. We call this ?early bird? bias. 
3.2 Specification of Quality 
Besides these aforementioned biases, using the raw 
rating from readers directly also fails to provide a 
clear guideline for what a good review consists of. 
In this section, we provide such a guideline, which 
we name as the specification (SPEC). 
In the SPEC, we define four categories of re-
view quality which represent different values of 
the reviews to users? purchase decision: ?best re-
view?, ?good review?, ?fair review?, and ?bad re-
view?. A generic description of the SPEC is as fol-
lows: 
A best review must be a rather complete and de-
tailed comment on a product. It presents several 
aspects of a product and provides convincing opi-
nions with enough evidence. Usually a best review 
could be taken as the main reference that users on-
ly need to read before making their purchase deci-
sion on a certain product. The first review in Fig-
ure 5 is a best review. It presents several product 
features and provides convincing opinions with 
sufficient evidence. It is also in a good format for 
readers to easily understand. Note that we omit 
some words in the example to save the space. 
0
50
100
150
200
250
300
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
#V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Ranking positions of reviews
0
10
20
30
40
50
60
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
# 
V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Publication Date
336
A good review is a relatively complete comment 
on a product, but not with as much supporting evi-
dence as necessary. It could be used as a strong 
and influential reference, but not as the only rec-
ommendation. The second review in Figure 5 is 
such an example. 
A fair review contains a very brief description 
on a product. It does not supply detailed evaluation 
on the product, but only comments on some as-
pects of the product. For example, the third review 
in Figure 5 mainly talks about ?the delay between 
pictures?, but less about other aspects of the cam-
era. 
A bad review is usually an incorrect description 
of a product with misleading information. It talks 
little about a specific product but much about some 
general topics (e.g. photography). For example, the 
last review in Figure 5 talks about the topic of ?ge-
neric battery?, but does not specify any digital 
camera. A bad review is an ?unhelpful? review that 
can be ignored.  
 
Best Review: 
I purchased this camera about six months ago after my Kodak 
Easyshare camera completely died on me. I did a little research 
and read only good things about this Canon camera so I decided to 
go with it because it was very reasonably priced (about $200). Not 
only did the camera live up to my expectations, it surpassed them 
by leaps and bounds! Here are the things I have loved about this 
camera: 
 
BATTERY - this camera has the best battery of any digital cam-
era I have ever owned or used. ? 
 
EASY TO USE - I was able to ? 
 
PICTURE QUALITY - all of the pictures I've taken and printed 
out have been great. ? 
 
FEATURES - I love the ability to quickly and easily ? 
 
LCD SCREEN - I was hoping ? 
 
SD MEMORY CARD - I was also looking for a camera that used 
SD memory cards. Mostly because? 
 
I cannot stress how highly I recommend this camera. I will never 
buy another digital camera besides Canon again. And the A610 (as 
well as the A620 - the 7.0MP version) is the best digital camera I've 
ever used. 
Good Review: 
The Sony DSC "P10" Digital Camera is the top pick for CSC. 
Running against cameras like Olympus stylus, Canon Powereshot, 
Sony V1, Nikon, Fuji, and More. The new release of 5.0 mega pix-
els has shot prices for digital cameras up to $1000+. This camera I 
purchased through a Private Dealer cost me $400.86. The Retail 
Price is Running $499.00 to $599.00. Purchase this camera from a 
wholesale dealer for the best price $377.00. Great Photo Even in 
dim light w/o a flash. The p10 is very compact. Can easily fit into 
any pocket. The camera can record 90 minutes of mpeg like a home 
movie. There are a lot of great digital cameras on the market that 
shoot good pictures and video. What makes the p10 the top pick is 
it comes with a rechargeable lithium battery. Many use AA batte-
ries, the digital camera consumes theses AA batteries in about two 
hours time while the unit is on. That can add continuous expense to 
the camera. It's also the best resolution on the market. 6.0 megapix 
is out, though only a few. And the smallest that we found. Also the 
best price for a major brand. 
Fair Review: 
There is nothing wrong with the 2100 except for the very notice-
able delay between pics. The camera's digital processor takes 
about 5 seconds after a photo is snapped to ready itself for the next 
one. Otherwise, the optics, the 3X optical zoom and the 2 megapixel 
resolution are fine for anything from Internet apps to 8" x 10" print 
enlarging. It is competent, not spectacular, but it gets the job done 
at an agreeable price point. 
Bad Review: 
I want to point out that you should never buy a generic battery, 
like the person from San Diego who reviewed the S410 on May 15, 
2004, was recommending. Yes you'd save money, but there have 
been many reports of generic batteries exploding when charged for 
too long. And don't think if your generic battery explodes you can 
sue somebody and win millions. These batteries are made in sweat-
shops in China, India and Korea, and I doubt you can find anybody 
to sue. So play it safe, both for your own sake and the camera's 
sake. If you want a spare, get a real Canon one. 
Figure 5. Example reviews 
3.3 Annotation of Quality 
According to the SPEC defined above, we built a 
ground-truth from the Amazon data set. We 
randomly selected 100 digital cameras and 50 
reviews for each camera. Totally we have 4,909 
reviews since some digital cameras have fewer 
than 50 unique reviews. Then we hired two 
annotators to label the reviews with the SPEC as 
their guideline. As the result, we have two 
independent copies of annotations on 4,909 
reviews, with the labels of ?best?, ?good?, ?fair?, 
and ?bad?. Table 1 shows the confusion matrix 
between the two copies of annotation. The value of 
the kappa statistic (Cohen, 1960) calculated from 
the matrix is 0.8142. This shows that the two 
annotators achieved highly consistent results by 
following the SPEC, although they worked 
independently.  
 Annota-
tion 1 
Annotation 2 
best good fair bad total 
best 294 44 2 0 340 
good 66 639 113 0 818 
fair 0 200 1,472 113 1,785 
bad 1 2 78 1,885 1,966 
total 361 885 1,665 1,998 4,909 
Table 1. Confusion matrix bet. the annotations 
 
In order to examine the difference between our 
annotations and Amazon ground-truth, we evaluate 
the Amazon ground-truth against the annotations, 
337
with the measure of ?error rate of preference pairs? 
(Herbrich et al 1999).  
????????? =
|????????? ?????????? ????? |
|??? ?????????? ?????|
 (1) 
where the ?preference pair? is defined as a pair of 
reviews with a order. For example, a best review 
and a good review correspond to a preference pair 
with the order of ?best review preferring to good 
review?. The ?all preference pairs? are collected 
from one of the annotations (the annotation 1 or 
the annotation 2) by ignoring the pairs from the 
same category. The ?incorrect preference pairs? 
are the preference pairs collected from the Ama-
zon ground-truth but not with the same order as 
that in the all preference pairs. The order of the 
preference pair collected from the Amazon 
ground-truth is evaluated on the basis of the per-
centage score as described in Section 3.1.  
The error rate of preference pairs based on the 
annotation 1 and that based on the annotation 2 are 
0.448 and 0.446, respectively, averaged over 100 
digital cameras. The high error rate of preference 
pairs demonstrates that the Amazon ground-truth 
diverges from the annotations (our ground-truth) 
significantly. 
To discover which kind of ground-truth is more 
reasonable, we ask an additional annotator (the 
third annotator) to compare these two kinds of 
ground-truth. More specifically, we randomly se-
lected 100 preference pairs whose orders the two 
kinds of ground-truth don?t agree on (called incor-
rect preference pairs in the evaluation above). As 
for our ground-truth, we choose the Annotation 1 
in the new test. Then, the third annotator is asked 
to assign a preference order for each selected pair. 
Note that the third annotator is blind to both our 
specification and the existing preference order.  
Last, we evaluate the two kinds of ground-truth 
with the new annotation. Among 100 pairs, our 
ground-truth agrees to the new annotation on 85 
pairs while the Amazon ground-truth agrees to the 
new annotation on 15 pairs. To confirm the result, 
yet another annotator (the fourth annotator) is 
called to repeat the same annotation independently 
as the third one. And we obtain the same statistical 
result (85 vs. 15) although the fourth annotator 
does not agree with the third annotator on some 
pairs. 
In practice, we treat the reviews in the first three 
categories (?best?, ?good? and ?fair?) as high-
quality reviews and those in the ?bad? category as 
low-quality reviews, since our goal is to identify 
low quality reviews that should not be considered 
when creating product review summaries. 
4 Classification of Product Reviews  
We employ a statistical machine learning approach 
to address the problem of detecting low-quality 
products reviews.  
Given a training data set? =  ?? ,?? 1
? , we 
construct a model that can minimize the error in 
prediction of y given x (generalization error). Here 
?? ? ?  and ?? = {???? ??????? , ??? ???????} 
represents a product review and a label, 
respectively. When applied to a new instance x, the 
model predicts the corresponding y and outputs the 
score of the prediction. 
4.1 The Learning Model 
In our study, we focus on differentiating low-
quality product reviews from high-quality ones. 
Thus, we treat the task as a binary classification 
problem.  
We employ SVM (Support Vector Machines) 
(Vapnik, 1995) as the model of classification. 
Given an instance x (product review), SVM assigns 
a score to it based on 
? ? = ??? + ? (2) 
where w denotes a vector of weights and b denotes 
an intercept. The higher the value of f(x) is, the 
higher the quality of the instance x is. In 
classification, the sign of f(x) is used. If it is 
positive, then x is classified into the positive 
category (high-quality reviews), otherwise into the 
negative category (low-quality reviews). 
The construction of SVM needs labeled training 
data (in our case, the categories are ?high-quality 
reviews? and ?low-quality reviews?). Briefly, the 
learning algorithm creates the ?hyper plane? in (2), 
such that the hyper plane separates the positive and 
negative instances in the training data with the 
largest ?margin?.  
4.2 Product Feature Resolution 
Product features (e.g., ?image quality? for digital 
camera) in a review are good indicators of review 
quality. However, different product features may 
refer to the same meaning (e.g., ?battery life? and 
?power?), which will bring redundancy in the 
study. In this paper, we formulize the problem as 
the ?resolution of product features?. Thus, the 
338
problem is reduced to how to determine the equi-
valence of a product feature in different forms.  
In (Hu and Liu, 2004), the matching of different 
product features is mentioned briefly and ad-
dressed by fuzzy matching. However, there exist 
many cases where the method fails to match the 
multiple mentions, e.g., ?battery life? and ?power?, 
because it only considers string similarity. In this 
paper we propose to resolve the problem by leve-
raging two kinds of evidence: one is ?surface string? 
evidence, the other is ?contextual evidence?.  
We use edit distance (Ukkonen, 1985) to com-
pare the similarity between the surface strings of 
two mentions, and use contextual similarity to re-
flect the semantic similarity between two mentions. 
When using contextual similarity, we split all 
the reviews into sentences. For each mention of a 
product feature, we take it as a query and search 
for all the relevant sentences. Then we construct a 
vector for the mention, by taking each unique term 
in the relevant sentences as a dimension of the vec-
tor. The cosine similarity between two vectors of 
mentions is then present to measure the contextual 
similarity between two mentions.  
4.3 Feature Development for Learning 
To detect low-quality reviews, our proposed 
approach explores three aspects of product reviews, 
namely informativeness, subjectiveness, and 
readability. We denote the features employed for 
learning as ?learning features?, discriminative from 
the ?product features? we discussed above. 
4.3.1 Features on Informativeness 
As for informativeness, the resolution of product 
features is employed when we generate the 
learning features as listed below. Pairs mapping to 
the same product feature will be treated as the 
same product feature, when we calculate the 
frequency and the number of product features. We 
apply the approach proposed in (Hu and Liu, 2004) 
to extract product features.  
We also use a list of product names and a list of 
brand names to generate the learning features. Both 
lists can be collected from the Amazon site be-
cause they are relatively stable within a time inter-
val. 
The learning features on the informativeness of 
a review are as follows. 
? Sentence level (SL) 
? The number of sentences in the review 
? The average length of sentences  
? The number of sentences with product features 
? Word level (WL) 
? The number of words in the review 
? The number of products (e.g., DMC-FZ50, 
EX-Z1000) in the review 
? The number of products in the title of a review  
? The number of brand names (e.g., Canon, Sony) 
in the review  
? The number of brand names in the title of a 
review 
? Product feature level (PFL) 
? The number of product features in the review 
? The total frequency of product features in the 
review 
? The average frequency of product features in 
the review 
? The number of product features in the title of a 
review 
?  The total frequency of product features in the 
title of a review 
4.3.2 Features on Readability 
We make use of several features at paragraph level 
which indicate the underlying structure of the 
reviews.  These features include, 
? The number of paragraphs in the review 
? The average length of paragraphs in the review 
? The number of paragraph separators in the re-
view 
Here, we refer to the keywords, such as ?Pros? 
vs. ?Cons? as ?paragraph separators?. The key-
words usually appear at the beginning of para-
graphs for categorizing two contrasting aspects of 
a product. We extract the nouns and noun phrases 
at the beginning of each paragraph from the 4,909 
reviews and use the most frequent 30 pairs of key-
words as paragraph separators. Table 2 provides 
some examples of the extracted separators. 
Separators Separators 
Positive Negative Positive Negative 
Pros Cons The Good The Bad 
Strength Weakness Thumb up Bummer 
PLUSES MINUSES Positive Negative 
Advantages Drawbacks Likes Dislikes 
The  upsides Downsides 
GOOD 
THINGS 
BAD 
THINGS 
Table 2. Examples of paragraph separators 
339
4.3.3 Features on Subjectiveness 
We also take the subjectiveness of reviews into 
consideration. Unlike previous work (Kim et al 
2006; Zhang and Varadarajan, 2006) using shallow 
syntactic information directly, we use a sentiment 
analysis tool (Hu and Liu, 2004) which aggregates 
a set of shallow syntactic information. The tool is a 
classifier capable of determining the sentiment 
polarity of each sentence. We create three learning 
features regarding the subjectiveness of reviews. 
? The percentage of positive sentences in the 
review 
? The percentage of negative sentences in the 
review 
? The percentage of subjective sentences (re-
gardless of positive or negative) in the review 
5 Experiments 
In this section, we describe our experiments with 
the proposed classification-based approach to low-
quality review detection, and its effectiveness on 
the task of opinion summarization. 
5.1 Detecting Low-quality Reviews 
In our proposed approach, the problem of assessing 
quality of reviews is formalized as a binary classi-
fication problem. We conduct experiments by tak-
ing reviews in the categories of ?best?, ?good?, and 
?fair? as high-quality reviews and those in the 
?bad? category as low-quality reviews.  
As for classification model, we utilize the 
SVMLight toolkit (Joachims, 2004). We randomly 
divide the 100 queries of digital cameras into two 
sets, namely a training set of 50 queries and a test 
set of 50 queries. For the two copies of annota-
tions, we use the same division. We use the train-
ing set from ?annotation 1? to train the model and 
apply the model to the test sets from both ?annota-
tion 1? and ?annotation 2?, respectively. Table 3 
reports the accuracies of our approach to review 
classification. The accuracy is defined as the per-
centage of correctly classified reviews. 
We take the approach that utilizes only the cate-
gory of features on sentence level (SL) as the base-
line, and incrementally add other categories of fea-
tures on informativeness, readability and subjec-
tiveness. We can see that both the features on word 
level (WL) and those on product feature level (PFL) 
can improve the performance of classification 
much. The features on readability can still increase 
the accuracy although the contribution is much 
less. The features on subjectiveness, however, 
make no contribution.   
 
Feature Category Annotation1 Annotation2 
Informative-
ness  
SL 73.59% 72.81% 
WL 80.41% 79.15% 
PFL 83.30% 82.37% 
Readability 83.93% 82.91% 
Subjectiveness 83.84% 82.96% 
Table 3. Low-quality reviews detection 
We also conduct a more detailed analysis on 
each individual feature. Two categories of features 
on ?title? and ?brand name? have poor perfor-
mance, which is due to the lack of information in 
the title and the low coverage of brand names in a 
review, respectively. 
5.2 Summarizing Sentiments of Reviews 
One potential application of low-quality review 
detection is the opinion summarization of reviews.  
The process of opinion summarization of re-
views with regards to a query of a product consists 
of the following steps (Liu et al 2005): 
1. From each of the reviews, identify every text 
segment with opinion in the review, and de-
termine the polarities of the opinion segments. 
2. For each product feature, generate a positive 
opinion set and a negative opinion set of opi-
nion segments, denoted as POS(?) 
and NOS(?). 
3. For each product feature, aggregate the num-
bers of segments in POS(?)  andNOS(?) , as 
opinion summarization on the product feature. 
In this process, all the reviews contribute the 
same. However, different reviews do hold different 
authorities. A positive/negative opinion from a 
high-quality review should not have the same 
weight as that from a low-quality review.  
We use a two-stage approach to enhance the re-
liability of summarization. That is, we add a 
process of low-quality review detection before the 
summarization process, so that the summarization 
result is obtained based on the high-quality reviews 
only. We are to demonstrate how much difference 
the proposed two-stage approach can bring into the 
opinion summarization. 
We use the best classification model trained as 
described in Section 5.1 to filter low-quality re-
views, and do summarization on the high-quality 
340
reviews associated to the 50 test queries. We de-
note the proposed approach and the old approach 
as ?two-stage? and ?one-stage?, respectively. Due 
to the limited space, we only give a visual compar-
ison of the two approaches on ?image quality? in 
Figure 6. The upper figure shows the summariza-
tion of positive opinions and the lower figure 
shows that of negative opinions. From the figures 
we can see that the two-stage approach preserves 
fewer text segments as the result of filtering out 
many low-quality product reviews. 
 
 
 
Figure 6. Summarization on ?image quality? 
To show the comparison on more features in a 
compressed space, we give the statistic ratio of 
change between two approaches instead. As for the 
evaluation measure, we define ?RatioOfChange? 
(ROC) on a feature f as, 
 
ROC ? =
Rateone?stage  ? ? Ratetwo?stage (?)
Rateone?stage (?)
 (3) 
 
where Rate *(f) is defined as, 
  
Rate?(?) =
|POS(?)|
|POS(?)| + |NOS(?)|
 (4) 
 
Table 4 shows some statistic results on ROC on 
five product features, namely ?image quality?(IQ), 
?battery?, ?LCD screen? (LCD), ?flash? and ?mov-
ie mode? (MM). The values in the cells are the 
percentage of queries whose ROC is larger/smaller 
than the respective thresholds. We can see that a 
large portion of queries have big changes on the 
values of ROC. This means that the result achieved 
by the two-stage approach is substantially different 
from that achieved by the one-stage approach. 
 
%Query 
RatioOfChange (+) 
>0.30 >0.25 >0.20 >0.15 >0.10 >0.05 
IQ 2% 4% 4% 10% 14% 22% 
Battery 10% 14% 18% 30% 38% 50% 
LCD  12% 18% 20% 22% 24% 28% 
Flash  6% 10% 16% 20% 26% 42% 
MM 6% 8% 8% 12% 18% 26% 
%Query 
RatioOfChange (-) 
<-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05 
IQ 4% 6% 10% 14% 18% 44% 
Battery 2% 4% 4% 10% 14% 22% 
LCD  4% 4% 8% 12% 22% 28% 
Flash  4% 6% 8% 16% 18% 28% 
MM 8% 10% 16% 18% 34% 42% 
Table 4. RatioOfChange on five features 
There is no standard way to evaluate the quality 
of opinion summarization as it is rather a subjec-
tive problem. In order to demonstrate the impact of 
the two-stage approach, we turn to external author-
itative sources other than Amazon.com as the ob-
jective evaluation reference. We observe that 
CNET2 provides a professional ?editor?s review? 
for many products, which gives a rating in the 
range of 1~10 on product features. 9 digital cam-
eras out of the 50 test queries are found to have the 
editor?s rating on ?image quality? at CNET. We 
use this rating to compare with the results of our 
opinion summarization. We rescale the Rate scores 
obtained by both the one-stage approach and the 
two-stage approach into the range of 1-10 in order 
to perform the comparison.  
Figure 7 provides the visual comparison. We 
can see that the result achieved by the two-stage 
approach has a much better (closer) resemblance to 
CNET rating than one-stage approach does. This 
indicates that our two-stage approach can achieve a 
more consistent summarization result to the profes-
sional evaluations by the editors. Although the 
CNET rating is not the absolute standard for prod-
uct evaluation, it provides a professional yet objec-
tive evaluation of the products. Therefore, the ex-
perimental results demonstrate that our proposed 
approach could achieve more reliable opinion 
summarization which is closer to the generic eval-
uation from authoritative sources. 
 
                                                 
2 http://www.cnet.com 
0
30
60
90
120
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(P
o
si
ti
ve
)
QueryID
One-stage Two-stage
0
20
40
60
80
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(N
e
ga
ti
ve
)
QueryID
One-stage Two-stage
341
 Figure 7. Comparison with CNET rating 
6 Conclusion 
In this paper, we studied the problem of detecting 
low-quality product reviews. Our contribution can 
be summarized in two-fold: (1) we discovered 
three types of biases in the ground-truth used ex-
tensively in the existing work, and proposed a spe-
cification on the quality of product reviews. The 
three biases that we discovered are imbalance vote 
bias, winner circle bias, and early bird bias. (2) 
Rooting on the new ground-truth (conforming to 
the proposed specification), we proposed a classi-
fication-based approach to low-quality product 
review detection, which yields better performance 
of opinion summarization. 
We hope to explore our future work in several 
areas, such as further consolidating the new 
ground-truth from different points of view and ve-
rifying the effectiveness of low-quality review de-
tection with other applications. 
References 
Jacob Cohen. 1960. A coefficient of agreement for no-
minal scales, Educational and Psychological Mea-
surement 20: 37?46.  
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. 
WWW?03. 
Harris Drucker, Chris J.C., Burges Linda Kaufman, 
Alex Smola and Vladimir Vapnik. 1997. Support 
vector regression machines. Advances in Neural In-
formation Processing Systems.  
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database, MIT Press. 
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 
1999. Support Vector Learning for Ordinal Regres-
sion. In Proc. of the 9th International Conference on 
Artificial Neural Networks. 
Minqing Hu and Bing Liu. 2004a. Mining and Summa-
rizing Customer Reviews. KDD?04.  
Minqing Hu and Bing Liu. 2004b. Mining Opinion Fea-
tures in Customer Reviews. AAAI?04. 
Kalervo Jarvelin & Jaana Kekalainen. 2000. IR: evalua-
tion methods for retrieving highly relevant docu-
ments. SIGIR?00.  
Nitin Jindal and Bing Liu. 2006. Identifying Compara-
tive Sentences in Text Documents. SIGIR?06. 
Nitin Jindal and Bing Liu. 2006. Mining comparative 
sentences and relations. AAAI?06. 
Thorsten Joachims. SVMlight -- Support Vector Ma-
chine. http://svmlight.joachims.org/, 2004. 
Soo-Min Kim, Patrick Pantel, Tim Chklovski, Marco 
Pennacchiotti. 2006. Automatically Assessing Re-
view Helpfulness. EMNLP?06. 
Dekang Lin. 1998, Automatic retrieval and clustering of 
similar words. COLING-ACL?98. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: analyzing and comparing opinions 
on the web. WWW ?05.  
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summari-
zation based on minimum cuts. ACL?04. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. ACL?05. 
Bo Pang and Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? sentiment classification using machine 
learning techniques. EMNLP?02.  
Ana-Maria Popescu and O Etzioni. 2005. Extracting 
product    features and opinions from reviews. HLT-
EMNLP?05.  
Peter D. Turney. 2001. Thumbs up or thumbs down?: 
semantic orientation applied to unsupervised classifi-
cation of reviews. ACL?02  
Esko Ukkonen. 1985. Algorithms for approximate string 
matching. Information and Control, pp. 100 ? 118. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Zhu Zhang and Balaji Varadarajan. 2006. Utility Scor-
ing of Product Reviews. CIKM?06 
 
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
R
at
in
g 
Sc
o
re
QueryID
One-stage
Two-stage
CNET Ground-truth
342
Uncertainty Reduction in Collaborative Bootstrapping:  
Measure and Algorithm 
Yunbo Cao 
Microsoft Research Asia  
5F Sigma Center,  
No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
 i-yucao@microsoft.com 
Hang Li 
Microsoft Research Asia 
5F Sigma Center,  
No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
hangli@microsoft.com 
Li Lian 
Computer Science Department 
Fudan University 
No. 220 Handan Road 
Shanghai, China, 200433 
leelix@yahoo.com 
 
Abstract 
This paper proposes the use of uncertainty 
reduction in machine learning methods 
such as co-training and bilingual boot-
strapping, which are referred to, in a gen-
eral term, as ?collaborative bootstrapping?. 
The paper indicates that uncertainty re-
duction is an important factor for enhanc-
ing the performance of collaborative 
bootstrapping. It proposes a new measure 
for representing the degree of uncertainty 
correlation of the two classifiers in col-
laborative bootstrapping and uses the 
measure in analysis of collaborative boot-
strapping. Furthermore, it proposes a new 
algorithm of collaborative bootstrapping 
on the basis of uncertainty reduction. Ex-
perimental results have verified the cor-
rectness of the analysis and have 
demonstrated the significance of the new 
algorithm. 
1 Introduction 
We consider here the problem of collaborative 
bootstrapping. It includes co-training (Blum and 
Mitchell, 1998; Collins and Singer, 1998; Nigam 
and Ghani, 2000) and bilingual bootstrapping (Li 
and Li, 2002).  
Collaborative bootstrapping begins with a small 
number of labelled data and a large number of 
unlabelled data. It trains two (types of) classifiers 
from the labelled data, uses the two classifiers to 
label some unlabelled data, trains again two new 
classifiers from all the labelled data, and repeats 
the above process. During the process, the two 
classifiers help each other by exchanging the la-
belled data. In co-training, the two classifiers have 
different feature structures, and in bilingual boot-
strapping, the two classifiers have different class 
structures. 
Dasgupta et al(2001) and Abney (2002) con-
ducted theoretical analyses on the performance 
(generalization error) of co-training. Their analyses, 
however, cannot be directly used in studies of co-
training in (Nigam & Ghani, 2000) and bilingual 
bootstrapping. 
In this paper, we propose the use of uncertainty 
reduction in the study of collaborative bootstrap-
ping (both co-training and bilingual bootstrapping). 
We point out that uncertainty reduction is an im-
portant factor for enhancing the performances of 
the classifiers in collaborative bootstrapping. Here, 
the uncertainty of a classifier is defined as the por-
tion of instances on which it cannot make classifi-
cation decisions. Exchanging labelled data in 
bootstrapping can help reduce the uncertainties of 
classifiers. 
Uncertainty reduction was previously used in 
active learning. We think that it is this paper which 
for the first time uses it for bootstrapping.  
We propose a new measure for representing the 
uncertainty correlation between the two classifiers 
in collaborative bootstrapping and refer to it as 
?uncertainty correlation coefficient? (UCC). We 
use UCC for analysis of collaborative bootstrap-
ping. We also propose a new algorithm to improve 
the performance of existing collaborative boot-
strapping algorithms. In the algorithm, one classi-
fier always asks the other classifier to label the 
most uncertain instances for it. 
Experimental results indicate that our theoreti-
cal analysis is correct. Experimental results also 
indicate that our new algorithm outperforms exist-
ing algorithms. 
2 Related Work 
2.1 Co-Training and Bilingual Bootstrapping 
Co-training, proposed by Blum and Mitchell 
(1998), conducts two bootstrapping processes in 
parallel, and makes them collaborate with each 
other. More specifically, it repeatedly trains two 
classifiers from the labelled data, labels some 
unlabelled data with the two classifiers, and ex-
changes the newly labelled data between the two 
classifiers. Blum and Mitchell assume that the two 
classifiers are based on two subsets of the entire 
feature set and the two subsets are conditionally 
independent with one another given a class. This 
assumption is called ?view independence?. In their 
algorithm of co-training, one classifier always asks 
the other classifier to label the most certain in-
stances for the collaborator. The word sense dis-
ambiguation method proposed in Yarowsky (1995) 
can also be viewed as a kind of co-training. 
Since the assumption of view independence 
cannot always be met in practice, Collins and 
Singer (1998) proposed a co-training algorithm 
based on ?agreement? between the classifiers. 
As for theoretical analysis, Dasgupta et al 
(2001) gave a bound on the generalization error of 
co-training within the framework of PAC learning. 
The generalization error is a function of ?dis-
agreement? between the two classifiers. Dasgupta 
et als result is based on the view independence 
assumption, which is strict in practice. 
Abney (2002) refined Dasgupta et als result by 
relaxing the view independence assumption with a 
new constraint. He also proposed a new co-training 
algorithm on the basis of the constraint. 
Nigam and Ghani (2000) empirically demon-
strated that bootstrapping with a random feature 
split (i.e. co-training), even violating the view in-
dependence assumption, can still work better than 
bootstrapping without a feature split (i.e., boot-
strapping with a single classifier). 
For other work on co-training, see (Muslea et al
200; Pierce and Cardie 2001). 
Li and Li (2002) proposed an algorithm for 
word sense disambiguation in translation between 
two languages, which they called ?bilingual boot-
strapping?. Instead of making an assumption on the 
features, bilingual bootstrapping makes an assump-
tion on the classes. Specifically, it assumes that the 
classes of the classifiers in bootstrapping do not 
overlap. Thus, bilingual bootstrapping is different 
from co-training. 
Because the notion of agreement is not involved 
in bootstrapping in (Nigam & Ghani 2000) and 
bilingual bootstrapping, Dasgupta et aland 
Abney?s analyses cannot be directly used on them. 
2.2 Active Learning 
Active leaning is a learning paradigm. Instead of 
passively using all the given labelled instances for 
training as in supervised learning, active learning 
repeatedly asks a supervisor to label what it con-
siders as the most critical instances and performs 
training with the labelled instances. Thus, active 
learning can eventually create a reliable classifier 
with fewer labelled instances than supervised 
learning. One of the strategies to select critical in-
stances is called ?uncertain reduction? (e.g., Lewis 
and Gale, 1994). Under the strategy, the most un-
certain instances to the current classifier are se-
lected and asked to be labelled by a supervisor. 
The notion of uncertainty reduction was not 
used for bootstrapping, to the best of our knowl-
edge. 
3 Collaborative Bootstrapping and Un-
certainty Reduction 
We consider the collaborative bootstrapping prob-
lem.  
Let  denote a set of instances (feature vectors) 
and let denote a set of labels (classes). Given a 
number of labelled instances, we are to construct a 
function  ?:h . We also refer to it as a classi-
fier.  
In collaborative bootstrapping, we consider the 
use of two partial functions 1h  and 2h , which either 
output a class label or a special symbol ? denoting 
?no decision?.  
Co-training and bilingual bootstrapping are two 
examples of collaborative bootstrapping.  
In co-training, the two collaborating classifiers 
are assumed to be based on two different views, 
namely two different subsets of the entire feature 
set. Formally, the two views are respectively inter-
preted as two functions )(1 xX and )x(X2 , ?x . 
Thus, the two collaborating classifiers 1h  and 2h  in 
co-training can be respectively represented as 
))(( 11 xXh  and ))(( 22 xXh . 
In bilingual bootstrapping, a number of classifi-
ers are created in the two languages. The classes of 
the classifiers correspond to word senses and do 
not overlap, as shown in Figure 1. For example, the 
classifier )E|x(h 11  in language 1 takes sense 2 
and sense 3 as classes. The classifier )C|x(h 12  in 
language 2 takes sense 1 and sense 2 as classes, 
and the classifier )C|x(h 22  takes sense 3 and 
sense 4 as classes. Here we use 211 ,, CCE to de-
note different words in the two languages. Collabo-
rative bootstrapping is performed between the 
classifiers )(h ?1  in language 1 and the classifiers 
)(h ?2  in language 2. (See Li and Li 2002 for de-
tails). 
For the classifier )E|x(h 11 in language 1, we 
assume that there is a pseudo classifier 
)C,C|x(h 212 in language 2, which functions as a 
collaborator of )E|x(h 11 . The pseudo classifier 
)C,C|x(h 212  is based on )C|x(h 12  and 
)C|x(h 22 , and takes sense 2 and sense 3 as classes. 
Formally, the two collaborating classifiers (one 
real classifier and one pseudo classifier) in bilin-
gual bootstrapping are respectively represented as 
)|(1 Exh  and )|(2 Cxh , ?x . 
Next, we introduce the notion of uncertainty re-
duction in collaborative bootstrapping. 
Definition 1 The uncertainty )(hU of a classi-
fier h is defined as: 
}),)(|({)( ?=?= xxhxPhU  
 (1) 
In practice, we define )(hU  as  
}),  ,))((|({)(  ???<== xyyxhCxPhU ?  (2) 
where ?  denotes a predetermined threshold and 
)(?C denotes the confidence score of the classifier 
h. 
Definition 2 The conditional uncer-
tainty )|( yhU of a classifier h given a class y is 
defined as: 
)|},)(|({)|( yYxxhxPyhU =?=?=   (3) 
We note that the uncertainty (or conditional un-
certainty) of a classifier (a partial function) is an 
indicator of the accuracy of the classifier. Let us 
consider an ideal case in which the classifier 
achieves 100% accuracy when it can make a classi-
fication decision and achieves 50% accuracy when 
it cannot (assume that there are only two classes). 
Thus, the total accuracy on the entire data space is 
)(5.01 hU?? . 
Definition 3 Given the two classifiers 1h and 2h  
in collaborative bootstrapping, the uncertainty re-
duction of 1h  with respect to 2h   (denoted as 
)\( 21 hhUR ), is defined as 
}),)(,)(|({)\( 2121 ???=?= xxhxhxPhhUR  (4) 
Similarly, we have 
}),)(,)(|({)\( 2112 ?=???= xxhxhxPhhUR   
Uncertainty reduction is an important factor for 
determining the performance of collaborative boot-
strapping. In collaborative bootstrapping, the more 
the uncertainty of one classifier can be reduced by 
the other classifier, the higher the performance can 
be achieved by the classifier (the more effective 
the collaboration is). 
4 Uncertainty Correlation Coefficient 
Measure 
4.1 Measure 
We introduce the measure of uncertainty correla-
tion coefficient (UCC) to collaborative bootstrap-
ping. 
Definition 4 Given the two classifiers 1h and 2h , 
the conditional uncertainty correlation coefficient 
(CUCC) between 1h and 2h given a class y (denoted 
as yhhr 21 ), is defined as 
)|)(()|)((
)|)(,)((
21 21
21
yYxhPyYxhP
yYxhxhP
yhhr ==?==?
==?=?
=
 
(5) 
Definition 5 The uncertainty correlation coeffi-
cient (UCC) between 1h and 2h  (denoted as 21hhR ), 
is defined as 
=
y
yhhhh r)y(PR 2121  (6) 
UCC represents the degree to which the uncer-
 
Figure 1:  Bilingual Bootstrapping  
tainties of the two classifiers are related. If UCC is 
high, then there are a large portion of instances 
which are uncertain for both of the classifiers. Note 
that UCC is a symmetric measure from both classi-
fiers? perspectives, while UR is an asymmetric 
measure from one classifier?s perspective (ei-
ther )\( 21 hhUR or )\( 12 hhUR ). 
4.2 Theoretical Analysis 
Theorem 1 reveals the relationship between the 
CUCC (UCC) measure and uncertainty reduction.  
Assume that the classifier 1h can collaborate 
with either of the two classifiers 2h and 2'h . The 
two classifiers 2h and 2h? have equal conditional 
uncertainties. The CUCC values between 1h and 
2h? are smaller than the CUCC values between 1h  
and 2h . Then, according to Theorem 1, 1h should 
collaborate with 2h? , because 2h ? can help reduce its 
uncertainty more, thus, improve its accuracy more. 
Theorem 1 Given the two classifier pairs 
),( 21 hh and ),( 21 hh ? , if ?? ? yrr yhhyhh ,2121 and 
),|()|( 22 yhUyhU ?=  ?y , then we have 
)\()\( 2121 hhURhhUR ??  
Proof: 
We can decompose the uncertainty )( 1hU of 1h  as 
follows: 
)())|},)(,)(|({               
)|},)(,)(|({(          
)()|},)(|({)(
21
21
11
yYPyYxxhxhxP
yYxxhxhxP
yYPyYxxhxPhU
y
y
==???=?+
=?=?=?=
==?=?=





 
 )())|},)(,)(|({          
)|},)(|({             
)|},)(|({(       
21
2
121
yYPyYxxhxhxP
yYxxhxP
yYxxhxPr
y
yhh
==???=?+
=?=??
=?=?=



 
)())|},)(,)(|({          
)|()|((       
21
2121
yYPyYxxhxhxP
yhUyhUr
y
yhh
==???=?+
=

 
 
})),)(,)(|({                
)()|()|((        
21
2121
???=?+
==
xxhxhxP
yYPyhUyhUr
y
yhh
 
Thus,  
 =?=
???=?=
y
yhh yYPyhUyhUrhU
xxhxhxPhhUR
)()|()|()(                  
}),)(,)(|({)\(
211
2121
21

Similarly we have 
 =??=? ?
y
yhh yYPyhUyhUrhUhhUR )()|()|()()\( 21121 21  
Under the conditions, yhhyhh rr 2121 ?? , ?y  and 
),|()|( 22 yhUyhU ?= ?y , we have 
)\()\( 2121 hhURhhUR ??   
Theorem 1 states that the lower the CUCC val-
ues are, the higher the performances can be 
achieved in collaborative bootstrapping. 
Definition 6 The two classifiers in co-training 
are said to satisfy the view independence assump-
tion (Blum and Mitchell, 1998), if the following 
equations hold for any class y. 
)|(),|(
)|(),|(
221122
112211
yYxXPxXyYxXP
yYxXPxXyYxXP
======
======
 
Theorem 2 If the view independence assump-
tion holds, then 0.1
21
=yhhr holds for any class y. 
Proof: 
According to (Abney, 2002), view independence 
implies classifier independence: 
)|(),|(
)|(),|(
212
121
yYvhPuhyYvhP
yYuhPvhyYuhP
======
======
 
We can rewrite them as  
)|()|()|,,( 2121 yYvhPyYuhPyYvhuhP ========
 
Thus, we have 
)|})(|({)|},)(|({
)|},)(,)(|({
21
21
yYxxhxPyYxxhxP
yYxxhxhxP
=?=?=?=?=
=?=?=?


 
It means  
??= yr yhh     ,0.121   
Theorem 2 indicates that in co-training with 
view independence, the CUCC values 
( ??yr yhh ,21 ) are small, since by defini-
tion ?<< yhhr 210 . According to Theorem 1, it is 
easy to reduce the uncertainties of the classifiers. 
That is to say, co-training with view independence 
can perform well. 
How to conduct theoretical evaluation on the 
CUCC measure in bilingual bootstrapping is still 
an open problem. 
4.3 Experimental Results 
We conducted experiments to empirically evaluate 
the UCC values of collaborative bootstrapping. We 
also investigated the relationship between UCC 
and accuracy. The results indicate that the theoreti-
cal analysis in Section 4.2 is correct. 
In the experiments, we define accuracy as the 
percentage of instances whose assigned labels 
agree with their ?true? labels. Moreover, when we 
refer to UCC, we mean that it is the UCC value on 
the test data. We set the value of ? in Equation (2) 
to 0.8. 
Co-Training for Artificial Data Classification 
We used the data in (Nigam and Ghani 2000) to 
conduct co-training. We utilized the articles from 
four newsgroups (see Table 1). Each group had 
1000 texts.  
By joining together randomly selected texts 
from each of the two newsgroups in the first row as 
positive instances and joining together randomly 
selected texts from each of the two newsgroups in 
the second row as negative instances, we created a 
two-class classification data with view independ-
ence. The joining was performed under the condi-
tion that the words in the two newsgroups in the 
first column came from one vocabulary, while the 
words in the newsgroups in the second column 
came from the other vocabulary. 
We also created a set of classification data 
without view independence. To do so, we ran-
domly split all the features of the pseudo texts into 
two subsets such that each of the subsets contained 
half of the features. 
We next applied the co-training algorithm to the 
two data sets.  
We conducted the same pre-processing in the 
two experiments. We discarded the header of each 
text, removed stop words from each text, and made 
each text have the same length, as did in (Nigam 
and Ghani, 2000). We discarded 18 texts from the 
entire 2000 texts, because their main contents were 
binary codes, encoding errors, etc. 
We randomly separated the data and performed 
co-training with random feature split and co-
training with natural feature split in five times. The 
results obtained (cf., Table 2), thus, were averaged 
over five trials. In each trial, we used 3 texts for 
each class as labelled training instances, 976 texts 
as testing instances, and the remaining 1000 texts 
as unlabelled training instances. 
From Table 2, we see that the UCC value of the 
natural split (in which view independence holds) is 
lower than that of the random split (in which view 
independence does not hold). That is to say, in 
natural split, there are fewer instances which are 
uncertain for both of the classifiers. The accuracy 
of the natural split is higher than that of the random 
split. Theorem 1 states that the lower the CUCC 
values are, the higher the performances can be 
achieved. The results in Table 2 agree with the 
claim of Theorem 1. (Note that it is easier to use 
CUCC for theoretical analysis, but it is easier to 
use UCC for empirical analysis). 
Table 2: Results with Artificial Data 
Feature Accuracy  UCC  
Natural Split  0.928 1.006 
Random Split 0.712 2.399 
We also see that the UCC value of the natural 
split (view independence) is about 1.0. The result 
agrees with Theorem 2. 
Co-Training for Web Page Classification  
We used the same data in (Blum and Mitchell, 
1998) to perform co-training for web page classifi-
cation. 
The web page data consisted of 1051 web pages 
collected from the computer science departments 
of four universities. The goal of classification was 
to determine whether a web page was concerned 
with an academic course. 22% of the pages were 
actually related to academic courses. The features 
for each page were possible to be separated into 
two independent parts. One part consisted of words 
occurring in the current page and the other part 
consisted of words occurring in the anchor texts 
pointed to the current page.  
We randomly split the data into three subsets: 
labelled training set, unlabeled training set, and test 
set. The labelled training set had 3 course pages 
and 9 non-course pages. The test set had 25% of 
the pages. The unlabelled training set had the re-
maining data.  
Table 3: Results with Web Page Data and Bilin-
gual Bootstrapping Data 
Data Accuracy UCC 
Web Page 0.943 1.147 
bass 0.925 2.648 
drug 0.868 0.986 
duty 0.751 0.840 
palm 0.924 1.174 
plant 0.959 1.226 
space 0.878 1.007 
Word Sense Dis-
ambiguation 
tank 0.844 1.177 
We used the data to perform co-training and 
web page classification. The setting for the  
Table 1: Artificial Data for Co-Training 
Class Feature Set A Feature Set B 
Pos comp.os.ms-windows.misc talk.politics.misc 
Neg comp.sys.ibm.pc.hardware talk.politics.guns 
experiment was almost the same as that of Nigam 
and Ghani?s. One exception was that we did not 
conduct feature selection, because we were not 
able to follow their method from their paper. 
We repeated the experiment five times and 
evaluated the results in terms of UCC and accuracy. 
Table 3 shows the average accuracy and UCC 
value over the five trials. 
Bilingual Bootstrapping 
We also used the same data in (Li and Li, 2002) to 
conduct bilingual bootstrapping and word sense 
disambiguation. 
The sense disambiguation data were related to 
seven ambiguous English words, each having two 
Chinese translations. The goal was to determine 
the correct Chinese translations of the ambiguous 
English words, given English sentences containing 
the ambiguous words.  
For each word, there were two seed words used 
as labelled instances for training, a large number of 
unlabeled instances (sentences) in both English and 
Chinese for training, and about 200 labelled in-
stances (sentences) for testing. Details on data are 
shown in Table 4. 
We used the data to perform bilingual boot-
strapping and word sense disambiguation. The set-
ting for the experiment was exactly the same as 
that of Li and Li?s. Table 3 shows the accuracy and 
UCC value for each word. 
From Table 3 we see that both co-training and 
bilingual bootstrapping have low UCC values 
(around 1.0). With lower UCC (CUCC) values, 
higher performances can be achieved, according to 
Theorem 1. The accuracies of them are indeed high. 
Note that since the features and classes for each 
word in bilingual bootstrapping and those for web 
page classification in co-training are different, it is 
not meaningful to directly compare the UCC val-
ues of them. 
5 Uncertainty Reduction Algorithm 
5.1 Algorithm 
We propose a new algorithm for collaborative 
bootstrapping (both co-training and bilingual boot-
strapping). 
In the algorithm, the collaboration between the 
classifiers is driven by uncertainty reduction. Spe-
cifically, one classifier always selects the most un-
certain unlabelled instances for it and asks the 
other classifier to label.  Thus, the two classifiers 
can help each other more effectively. 
There exists, therefore, a similarity between our 
algorithm and active learning. In active learning 
the learner always asks the supervisor to label the 
Table 4: Data for Bilingual Bootstrapping 
Unlabelled instances Word 
English Chinese 
Seed words Test instances 
bass 142 8811 fish / music 200 
drug 3053 5398 treatment / smuggler 197 
duty 1428 4338 discharge / export 197 
palm 366 465 tree / hand 197 
plant 7542 24977 industry / life 197 
Space 3897 14178 volume / outer 197 
tank 417 1400 combat / fuel 199 
Total 16845 59567 - 1384 
Input: A set of labeled instances and a set of unla-
belled instances. 
Loop while there exist unlabelled instances{ 
Create classifier 1h using the labeled instances; 
Create classifier 2h using the labeled instances; 
For each class ( yY = ){ 
Pick up yb  unlabelled instances whose labels 
( yY = ) are most certain for 1h and are most 
uncertain for 2h , label them with 1h and add 
them into the set of labeled instances; 
      
Pick up yb  unlabelled instances whose labels 
( yY = ) are most certain for 2h and are most 
uncertain for 1h , label them with 2h  and add 
them into the set of labeled instances; 
} 
} 
Output: Two classifiers 1h and 2h  
Figure 2: Uncertainty Reduction Algorithm 
most uncertain examples for it, while in our algo-
rithm one classifier always asks the other classifier 
to label the most uncertain examples for it. 
Figure 2 shows the algorithm. Actually, our 
new algorithm is different from the previous algo-
rithm only in one point. Figure 2 highlights the 
point in italic fonts. In the previous algorithm, 
when a classifier labels unlabeled instances, it la-
bels those instances whose labels are most certain 
for the classifier. In contrast, in our new algorithm, 
when a classifier labels unlabeled instances, it la-
bels those instances whose labels are most certain 
for the classifier, but at the same time most uncer-
tain for the other classifier. 
As one implementation, for each class y, 1h first 
selects its most certain ya instances, 2h  next se-
lects from them its most uncertain yb  instances 
( yy ba ? ), and finally 1h labels the yb instances 
with label y (Collaboration from the opposite di-
rection is performed similarly.). We use this im-
plementation in our experiments described below. 
5.2 Experimental Results 
We conducted experiments to test the effectiveness 
of our new algorithm. Experimental results indi-
cate that the new algorithm performs better than 
the previous algorithm. We refer to them as ?new? 
and ?old? respectively. 
Co-Training for Artificial Data Classification 
We used the artificial data in Section 4.3 and con-
ducted co-training with both the old and new algo-
rithms. Table 5 shows the results.  
We see that in co-training the new algorithm 
performs as well as the old algorithm when UCC is 
low (view independence holds), and the new algo-
rithm performs significantly better than the old al-
gorithm when UCC is high (view independence 
does not hold). 
Co-Training for Web Page Classification 
We used the web page classification data in Sec-
tion 4.3 and conducted co-training using both the 
old and new algorithms. Table 6 shows the results. 
We see that the new algorithm performs as well as 
the old algorithm for this data set. Note that here 
UCC is low. 
Table 6: Accuracies with Web Page Data 
Accuracy Data Old New UCC 
Web Page 0.943 0.943 1.147 
Bilingual Bootstrapping 
We used the word sense disambiguation data in 
Section 4.3 and conducted bilingual bootstrapping 
using both the old and new algorithms. Table 7 
shows the results. We see that the performance of 
the new algorithm is slightly better than that of the 
old algorithm. Note that here the UCC values are 
also low. 
We conclude that for both co-training and bi-
lingual bootstrapping, the new algorithm performs 
significantly better than the old algorithm when 
UCC is high, and performs as well as the old algo-
rithm when UCC is low. Recall that when UCC is 
high, there are more instances which are uncertain 
for both classifiers and when UCC is low, there are 
fewer instances which are uncertain for both classi-
fiers. 
Note that in practice it is difficult to find a 
situation in which UCC is completely low (e.g., the 
view independence assumption completely holds), 
and thus the new algorithm will be more useful 
than the old algorithm in practice. To verify this, 
we conducted an additional experiment. 
Again, since the features and classes for each 
word in bilingual bootstrapping and those for web 
page classification in co-training are different, it is 
not meaningful to directly compare the UCC val-
ues of them. 
Co-Training for News Article Classification 
In the additional experiment, we used the data 
Table 5: Accuracies with Artificial Data 
Accuracy Feature Old New UCC 
Natural Split 0.928 0.924 1.006 
Random Split 0.712 0.775 2.399 
Table 7: Accuracies with Bilingual Bootstrapping 
Data 
Accuracy Word Old New UCC 
bass 0.925 0.955 2.648 
drug 0.868 0.863 0.986 
duty 0.751 0. 797 0.840 
palm 0.924 0.914 1.174 
plant 0.959 0.944 1.226 
space 0.878 0.888 1.007 
tank 0.844 0.854 1.177 
Average 0.878 0.888 - 
from two newsgroups (comp.graphics and 
comp.os.ms-windows.misc) in the dataset of 
(Joachims, 1997) to construct co-training and text 
classification. 
There were 1000 texts for each group. We 
viewed the former group as positive class and the 
latter group as negative class. We applied the new 
and old algorithms. We conducted 20 trials in the 
experimentation. In each trial we randomly split 
the data into labelled training, unlabeled training 
and test data sets. We used 3 texts per class as la-
belled instances for training, 994 texts for testing, 
and the remaining 1000 texts as unlabelled in-
stances for training. We performed the same pre-
processing as that in (Nigam and Ghani 2000). 
Table 8 shows the results with the 20 trials. The 
accuracies are averaged over each 5 trials. From 
the table, we see that co-training with the new al-
gorithm significantly outperforms that using the 
old algorithm and also ?single bootstrapping?. Here, 
?single bootstrapping? refers to the conventional 
bootstrapping method in which a single classifier 
repeatedly boosts its performances with all the fea-
tures. 
The above experimental results indicate that our 
new algorithm for collaborative bootstrapping per-
forms significantly better than the old algorithm 
when the collaboration is difficult. It performs as 
well as the old algorithm when the collaboration is 
easy. Therefore, it is better to always employ the 
new algorithm. 
Another conclusion from the results is that we 
can apply our new algorithm into any single boot-
strapping problem. More specifically, we can ran-
domly split the feature set and use our algorithm to 
perform co-training with the split subsets. 
6 Conclusion 
This paper has theoretically and empirically dem-
onstrated that uncertainty reduction is the essence 
of collaborative bootstrapping, which includes 
both co-training and bilingual bootstrapping. 
The paper has conducted a new theoretical 
analysis of collaborative bootstrapping, and has 
proposed a new algorithm for collaborative boot-
strapping, both on the basis of uncertainty reduc-
tion. Experimental results have verified the 
correctness of the analysis and have indicated that 
the new algorithm performs better than the existing 
algorithms. 
References 
S. Abney, 2002. Bootstrapping. In Proceedings of the 
40th Annual Meeting of the Association for Compu-
tational Linguistics. 
A. Blum and T. Mitchell, 1998. Combining Labeled 
Data and Unlabelled Data with Co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational learning Theory. 
M. Collins and Y. Singer, 1999. Unsupervised Models 
for Named Entity Classification. In Proceedings of 
the 1999 Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. 
S. Dasgupta, M. Littman and D. McAllester, 2001. PAC 
Generalization Bounds for Co-Training. In Proceed-
ings of Neural Information Processing System, 2001. 
T. Joachims, 1997. A Probabilistic Analysis of the Roc-
chio Algorithm with TFIDF for Text Categorization. 
In Proceedings of the 14th International Conference 
on Machine Learning. 
D. Lewis and W. Gale, 1994. A Sequential Algorithm 
for Training Text Classifiers. In Proceedings of the 
17th International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval. 
C. Li and H. Li, 2002. Word Translation Disambigua-
tion Using Bilingual Bootstrapping. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics. 
I. Muslea, S.Minton, and C. A. Knoblock 2000. Selec-
tive Sampling With Redundant Views. In Proceed-
ings of the Seventeenth National Conference on 
Artificial Intelligence. 
K. Nigam and R. Ghani, 2000. Analyzing the Effective-
ness and Applicability of Co-Training. In Proceed-
ings of the 9th International Conference on 
Information and Knowledge Management.  
D. Pierce and C. Cardie 2001. Limitations of Co-
Training for Natural Language Learning from Large 
Datasets. In Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-2001). 
D. Yarowsky, 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association 
for Computational Linguistics. 
Table 8:  Accuracies with News Data 
Collaborative Boot-
strapping Average Accuracy 
Single Boot-
strapping Old  New  
Trial 1-5 0.725 0.737 0.768 
Trial 6-10 0.708 0.702 0.793 
Trial 11-15 0.679 0.647 0.769 
Trial 16-20 0.699 0.689 0.767 
All 0.703 0.694 0.774 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497?504
Manchester, August 2008
 
Understanding and Summarizing Answers in Community-Based 
Question Answering Services 
Yuanjie Liu1, Shasha Li2, Yunbo Cao1,3, Chin-Yew Lin3, Dingyi Han1, Yong Yu1 
1Shanghai Jiao Tong University, 
Shanghai, China, 200240 
{lyjgeorge,handy,yyu} 
@apex.sjtu.edu.cn 
2National University of 
Defense Technology, 
Changsha, China, 410074
Shashali 
@nudt.edu.cn 
3Microsoft Research Asia,
Beijing, China, 100080 
{yunbo.cao,cyl} 
@microsoft.com 
 
Abstract 
Community-based question answering 
(cQA) services have accumulated millions 
of questions and their answers over time. 
In the process of accumulation, cQA ser-
vices assume that questions always have 
unique best answers. However, with an in-
depth analysis of questions and answers 
on cQA services, we find that the assump-
tion cannot be true. According to the anal-
ysis, at least 78% of the cQA best answers 
are reusable when similar questions are 
asked again, but no more than 48% of 
them are indeed the unique best answers. 
We conduct the analysis by proposing 
taxonomies for cQA questions and an-
swers. To better reuse the cQA content, 
we also propose applying automatic sum-
marization techniques to summarize an-
swers. Our results show that question-type 
oriented summarization techniques can 
improve cQA answer quality significantly. 
1 Introduction 
Community-based question and answering (cQA) 
service is becoming a popular type of search re-
lated activity. Major search engines around the 
world have rolled out their own versions of cQA 
service. Yahoo! Answers, Baidu Zhidao, and 
Naver Ji-Sik-In1 are some examples.  
In general, a cQA service has the following 
workflow. First, a question is posted by the asker 
in a cQA service and then people in the commu-
nity can answer the question. After enough num-
ber of answers are collected, a best answer can 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution- Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
be chosen by the asker or voted by the communi-
ty. The resulting question and answer archives 
are large knowledge repositories and can be used 
to complement online search. For example, Nav-
er?s Ji-Sik-In (Knowledge iN) has accumulated 
about 70 million entries2.  
In an ideal scenario, a search engine can serve 
similar questions or use best answers as search 
result snippets when similar queries are submit-
ted. To support such applications, we have to 
assume the best answers from cQA services are 
good and relevant answers for their pairing ques-
tions. However, the assumption might not be true 
as exemplified by the following examples. 
Question Title 
Which actress has the most seductive 
voice?..could range from a giggly goldie 
hawn..to a sultry anne bancroft? 
Question  
Description 
or any other type of voice that you find allur-
ing. .. 
Best Answer 
(Polls & Surveys) Fenella Fielding, wow!!!! 
Best Answer 
(Movies) i think joanna lumlley has a really sexy voice 
Table 1. Same Question / Different Best Answers
 
Question Title Does anyone know of any birthdays coming up soon? 
Question  
Description 
Celerities, people you know, you? Anyway I 
need the name and the date. If you want to 
know it is for my 
site,  http://www.jessicaparke2.piczo.com... 
and that is not site advertising.  
Answer 
Novembers Are: 
Paul Dickov nov 1st 
Nelly (not furtado) nov 2nd ? 
Best Answer Check imdb.com, they have this celebrity birthdays listed. 
Table 2. Question with Alternative Answers 
Table 1 presents a question asking communi-
ty opinions about ?who is the actress has the 
most seductive voice?. The asker posted the same 
question twice at different Yahoo! Answers cate-
gories: one in Polls & Surveys and one in Movies. 
                                                 
1 Yahoo! Answers: answers.yahoo.com; Baidu Zhidao: zhi-
dao.baidu.com; Naver Ji-Sik-In: kin.naver.com 
2www.iht.com/articles/2007/07/04/technology/naver.php 
497
 Two different best answers were chosen by the 
same asker due to non-overlapping of answers. 
Table 2 shows another example, it asks about 
?the coming birthdays of stars?. The best answer 
chosen by the asker is very good because it pro-
vides useful URL information where the asker 
can find her answers. However, other answers 
listed a variety of birthdays of stars that also 
answered the question. These two examples indi-
cate that the conventional cQA policy of allow-
ing askers or voters to choose best answers might 
be working fine with the purpose of cQA but it 
might not be a good one if we want to reuse these 
best answers without any post-processing. 
To find out what might be the alternatives to 
the best answers, we first carried out an in-depth 
analysis of cQA data by developing taxonomies 
for questions and answers. Then we propose 
summarizing answers in a consideration of ques-
tion type, as the alternative to the best answers. 
For example, for the ?actress voice? question, a 
summary of different people?s opinions ranked 
by popularity might be a better way for express-
ing the question?s answers. Similar to the ?ac-
tress voice? question, the ?celebrity birthday? 
question does not have a fix set of answers but is 
different from the ?actress voice? question that its 
answers are facts not opinions. For fact-based 
open ended questions, combining different an-
swers will be useful for reuse of those answers.  
The rest of this paper is arranged as follows. 
We review related work in Section 2. We devel-
op a framework for answer type taxonomy in 
Section 3 and a cQA question taxonomy in Sec-
tion 4. Section 5 presents methods to summarize 
cQA answers. Finally, we conclude this paper 
and discuss future work in Section 6. 
2 Related Work 
Previous research on cQA (community-based 
Question and Answering) domain focused on 
three major areas: (1) how to find similar ques-
tions given a new question (Jeon et al 2005a; 
Jeon et al, 2005b), (2) how to find experts given 
a community network(Liu et al, 2005; Jurczyk & 
Agichtein, 2007), and (3) how to measure answer 
quality and its effect on question retrieval. The 
third area of focus is the most relevant to our re-
search. Jeon et al (2006)?s work on assessing 
cQA answer quality is one typical example. They 
found that about 1/3 of the answers among the 
1,700 Q&A pairs from Naver.com cQA data 
have quality problems and approximately 1/10 of 
them have bad answers 3 . They used 13 non-
textual features and trained a maximum entropy 
model to predict answer quality. They showed 
that retrieval relevance was significantly im-
proved when answer quality measure was inte-
grated in a log likelihood retrieval model.  
As mentioned in Section 1, cQA services 
provide an alternative way for users to find in-
formation online. Questions posted on cQA sites 
should reflect users? needs as queries submitted 
to search engines do. Broder (2002) proposed 
that search queries can be classified into three 
categories, i.e. navigational, informational, and 
transactional. Ross and Levinson (2004) sug-
gested a more elaborated taxonomy with five 
more subcategories for informational queries and 
four more subcategories for resource (transac-
tional) queries. In open-domain question answer-
ing research that automatic systems are required 
to extract exact answers from a text database 
given a set of factoid questions (Voorhees and M. 
Ellen, 2003), all top performing systems had in-
corporated question taxonomies (Hovy et al, 
2001; Moldovan et al, 2000; Lytinen et al, 2002; 
Jijkoun et al, 2005). Based on the past expe-
riences from the annual NIST TREC Question 
and Answering Track 4  (TREC QA Track), an 
international forum dedicating to evaluate and 
compare different open-domain question answer-
ing systems, we conjecture that a cQA question 
taxonomy would help us determine what type of 
best answer is expected given a question type.  
Automatic summarization of cQA answers is 
one of the main focuses of this paper. We pro-
pose that summarization techniques (Hovy and 
Lin, 1999; Lin and Hovy, 2002) can be used to 
create cQA answer summaries for different ques-
tion types. Creating an answer summary given a 
question and its answers can be seen as a multi-
document summarization task. We simply re-
place documents with answers and apply these 
techniques to generate the answer summary. The 
task has been one of the main tasks the Docu-
ment Understanding Conference5 since 2004. 
3 A Framework  for Answer Type 
To study how to exploit the best answers of cQA, 
we need to first analyze cQA answers. We would 
like to know whether the existing best answer of 
a specific question is good for reuse. If not, we 
                                                 
3 Answers in Jeon el al.?s work were rated in three levels: 
good, medium, and bad. 
4 http://trec.nist.gov/data/qamain.html 
5 http://duc.nist.gov 
498
want to 
are. We 
by cQA 
ferentiat
tomatica
We m
swers fo
for answ
al categ
examini
the 4 mo
ries (100
tainmen
(S&C), H
we dev
based on
termines
can be r
ilar to th
One o
ercise an
The tax
cussions
notators 
category
annotato
on a sin
made t
taxonom
discuss 
answer t
Figur
Figur
onomy. 
Reusabl
means th
similar 
while a 
reused. 
Factual 
that can 
jective B
as the be
F
Unique
understand w
will refer to
askers or vo
e it with be
lly generate
ade use of
r developin
er type. The
ories in Yah
ng 400 rando
st popular 
 questions f
t & Music 
ealth, and 
eloped a cQ
 the princip
 a BA?s ans
eused or not
e BA?s ques
f the author
d developed
onomy was 
 among the 
to do the a
 label that 
rs. If none o
gle categor
he final d
y is describ
the question
axonomy in 
e 1. cQA Ser
e 1 shows th
It first divid
e and Not 
at it can be 
question to 
Not Reusab
The Reusabl
and Subject
be used as t
A is one of 
st answer. 
Reusable
actual
Not?
Unique
Direct Indire
Su
hy and wha
 the ?best a
ters as BA h
st answers 
d in our expe
 questions fr
g and testing
re are over 
oo! Answe
mly selecte
top Yahoo! 
rom each ca
(E&M), So
Computers &
A answer 
le of BA reu
wer type bas
 when a que
tion is asked
s carried ou
 the initial a
then modif
authors. We
nnotation. W
was agreed 
f the three a
y label, one
ecision. Th
ed in this 
 type and t
next section
vices BA Ty
e resulting 
es BA into
Reusable. A
reused as the
its question
le BA mea
e BA is fur
ive. A Fact
he best answ
the opinions
Best?
Answer
ct
bjective Re
t the alterna
nswers? sele
enceforth to
annotated or
riments. 
om Yahoo!
 our framew
1,000 hierar
rs. By manu
d questions 
Answers cat
tegory) ? E
ciety & Cu
 Internet (C
type taxon
sability tha
ed on ?if th
stion that is 
 again?. 
t this manua
nswer taxon
ied through
 asked three
e assigned
by at least
nnotators ag
 of the aut
e answer 
section and
he relation 
. 
pe Taxonom
answer type
 two catego
 Reusable
 best answe
 is asked a
ns it canno
ther divided
ual BA is a
er; while a 
 that can be 
Not??
Reusable
levant Irre
 
tives 
cted 
 dif-
 au-
 An-
ork 
chic-
ally 
from 
ego-
nter-
lture 
&I), 
omy 
t de-
e BA 
sim-
l ex-
omy. 
 dis-
 an-
 the 
 two 
reed 
hors 
type 
 we 
with 
 
y. 
 tax-
ries: 
BA 
r if a 
gain; 
t be 
 into 
 fact 
Sub-
used 
T
Uniq
a un
answ
Uniq
The 
type
its q
swer
ple, 
Indi
whil
birth
A
for o
ques
Each
T
vant
used
leva
aske
Nick
?I'm
ly So
answ
er?s 
ques
best
its q
tion 
give
answ
ema
To b
Answ
Uniqu
Direc
Indire
Factu
Subje
Reusa
Relev
Irrelev
Not R
T
type
mor
ries 
two 
 
A
mos
(50%
the o
or v
answ
levant
he Factual
ue and Not
ique best an
er add m
ue BA has
Not Unique
s: Direct an
uestion dire
s its questio
the question
rect BA wh
e there is al
day lists. 
 Subjective
pinions or r
tion asked ?
 answerer w
he Not Reus
 and Irrelev
 as a best an
nt to its qu
d ?Why was
 Lachey so 
 not sure wh
uth Jersey, 
er is releva
location wh
tion; an Irre
answer to i
uestion. Th
period has 
n that meets
er?.? of th
il without sh
ox? is in thi
er Type 
e 
t 
ct 
al Total 
ctive 
ble Total 
ant
ant
eusable Total
Table 3. D
able 3 show
s on four ca
e than 48%. 
tend to hav
categories.
mong the fo
tly not uniq
) of subjec
ne BA per c
oters is not g
er. Howeve
BA type 
 Unique. A 
swer to its q
ore informa
 other alter
BA type is d
d Indirect. A
ctly; while 
n through i
 mentioned
ich gives a
so a Direct a
BA answers
ecommenda
Which is th
ould have h
able BA has
ant. A Relev
swer to its 
estion, for 
 "I Can't Ha
shortlived??
ere you live
that song wa
nt but witho
ich does n
levant BA c
ts question a
e BA ?It ap
expired. If 
 your needs
e question ?
owing the em
s case. 
C&I
47%
28%
9%
84%
4%
88%
3%
9%
12%
istribution o
s the dist
tegories. Un
The C&I an
e more fac
ur categorie
ue and hav
tive answers
QA questio
ood enough
r, we might
has two s
Unique BA 
uestion and 
tion; while
native best 
ivided into 
 Direct BA
an Indirect
nference. Fo
 in section 1
 website re
nswer just g
 questions t
tions. For ex
e best sci-fi 
is own idea.
 two subtyp
ant BA cou
question bu
example, a 
te You Anym
 A Relevant
, but in NJ, 
s played ou
ut knowing
ot really an
ould not be u
nd it is irre
pears that t
an answer h
, please pic
how to for
ail address
E&M Heal
28% 48
7% 30
3% 5
38% 83%
40% 7
78% 90%
1% 1
21% 9
22% 10%
f Answer Ty
ribution of 
ique answer
d the Health
tual BAs th
s, S&C ans
e a high pe
. This indic
n chosen by
 for reuse as
 be able to a
ubtypes: 
has only 
no other 
 a Not 
answers. 
two sub-
 answers 
 BA an-
r exam-
 has the 
ference, 
ives the 
hat look 
ample, a 
movie?? 
 
es: Rele-
ld not be 
t it is re-
question 
ore" by 
BA said 
especial-
t??, this 
 the ask-
swer the 
sed as a 
levant to 
he ques-
as been 
k a ?best 
ward an 
es in the 
th S&C
% 13%
% 18%
% 2%
 33%
% 50%
 83%
% 0%
% 17%
 17%
pe 
Answer 
s are no 
 catego-
an other 
wers are 
rcentage 
ates that 
 its asker 
 the best 
pply au-
499
tomatic 
summar
(but not 
ible solu
E
T
Table
over wh
on a sin
the ques
stable (o
4 A C
As we w
my, we 
themselv
well. As
question
best answ
Rose an
engine q
their tax
engine q
we follo
onomy a
modate t
Fi
Figur
my. We
and prop
Informa
ilar as in
ry consi
an answ
with peo
Navig
seeking 
would li
know the
Trans
tend to g
compute
Navigat
summariza
ized answers
unique) answ
tions in Sect
Category
Computer & In
ntertainment &
Health 
Society & Cul
able 4. Disag
 4 shows t
ich none of
gle category
tion taxonom
ver at least 7
QA Quest
ere develop
often could
es and had 
 we discuss
 would help
er types.  
d Levinson?
ueries has 
onomy was 
ueries. Inste
wed the ba
nd made so
he particula
gure 2.  Que
e 2 shows th
 retain Brod
ose a new S
tional and Tr
 Broder?s ta
sts of questi
er but just w
ple in cQA 
ational ca
URLs of sp
ke to visit, 
 fan sites of
actional ca
et resources
r program th
ional Inform
Constant
Opinion
tion techni
 for at least
ers. We pro
ion 5. 
 
ternet 
 Music 
ture 
reement on 
he percenta
 the three a
 label. The r
y develope
9% question
ion Taxon
ing our answ
 not solely 
to consider t
ed in Sectio
 us determ
s (2004) tax
similar goal
developed t
ad of starti
sic hierarchy
me modific
r of cQA ser
stion Type T
e resulting 
er?s taxono
ocial catego
ansactional
xonomy whi
ons that do 
ere used to 
services. 
tegory con
ecific websit
for example
 Hannah Mo
tegory con
. A typical o
at lets you c
cQA
Question
ational
Dynamic
Context?
Dependent
Trans
ques to c
 half of reus
vide some p
Percenta
18%
17%
21%
20%
Answer Typ
ge of ques
nnotators ag
esults show
d above is p
s). 
omy 
er type tax
rely on ans
heir question
n 2, the typ
ine the expe
onomy of se
 to ours th
o classify se
ng from scr
 of R&L?s 
ations to acc
vices. 
axonomy
question tax
my at top le
ry. Navigati
 are defined 
le Social cat
not intend to
elicit intera
tains ques
es that the a
, ?Does any
ntana?? 
tains ques
ne is ?Is the
reate a plan
Open
actional
 
reate 
able 
oss-
ge 
e 
tions 
reed 
 that 
retty 
ono-
wers 
s as 
e of 
cted 
arch 
ough 
arch 
atch, 
tax-
om-
 
ono-
vels 
onal, 
sim-
ego-
 get 
ction 
tions 
sker 
body 
tions 
re a 
et?? 
F
to t
Con
answ
dich
port
betw
taxo
R&L
ques
latio
wou
F
tego
Opin
Que
peop
think
jects
ple.
tion
diffe
?Wh
diffe
Ope
som
have
selv
tion 
com
follo
clud
cont
T
serv
to g
joke
tially
or o
lazy
toge
beco
goog
will 
they
a ne
who
T
ques
cate
only
ques
occu
sinc
sear
Social
or Informati
wo subcateg
stant questio
ers while d
otomy of in
 our intentio
een the que
nomy. Cons
?s closed q
tion is ?Whi
n?? but ?W
ld be a dyna
or Dynamic
ries: Opinio
ion questio
stions in th
le in cQA 
 of some p
. ?Is Micros
Context-dep
s having dif
rent contex
at is the pop
rent answer
n category
e facts or m
 a variety o
es may have
?Can you li
ing week??
ws R&L?s 
es what is n
ext-depende
he new Soc
ices. Questio
et an answer
s and expre
, askers trea
nline forums
people com
ther with th
me a hacker
le search?
continue to 
 can give up
gative sentim
 asked how t
able 5 show
tion types 
gories. We 
 occupy 1
tions are ev
r in the sam
e people ve
ch engines 
on category,
ories: Con
ns have a f
ynamic que
formational
n to establi
stion taxon
tant questio
uery type. A
ch country h
hat is the po
mic question
category, w
n, Context-D
ns are those 
is category 
communiti
eople, some
oft Vista wo
endent ques
ferent answ
t. For exa
ulation of C
s according 
contains q
ethods. Th
f answers o
 unconstrain
st some birt
is an exam
open query 
ot covered 
nt categories
ial category
ns in this ca
. These que
ssing askers
t cQA servi
. The questi
e on here si
e question 
? It really is
hopefully so
ask, will cli
 faster? ? a
ent toward
o become a 
s the distr
on 4 differe
observe tha
1% ~ 20%
en fewer su
ple question
ry likely w
to discover 
 we first div
stant and D
ixed or stab
stions do n
 category is
sh intuitive 
omy and the
n type is s
n example 
as the large
pulation of 
. 
e define thre
ependent an
asking for o
seek opinio
es about w
 events, or s
rth it?? is a
tions are tho
ers accordin
mple, the 
hina?? sho
to the differ
uestions ask
e questions
r their answ
ed depth. T
hdays of sta
ple. This es
category. It
by the opin
. 
 is specific
tegory do n
stions includ
? own ideas
ce as chattin
on ?Why do 
mply just to
description 
n't that har
me of the pe
ck the link b
ctually is ex
s a number o
hacker. 
ibution of 
nt Yahoo! 
t constant q
 while nav
ch that they
s. This is re
ould be abl
answers of
ide it in-
ynamic. 
le set of 
ot. This 
 to sup-
mapping 
 answer 
imilar to 
constant 
st popu-
China?? 
e subca-
d Open. 
pinions. 
ns from 
hat they 
ome ob-
n exam-
se ques-
g to the 
question 
uld have 
ent date. 
ing for 
 usually 
er them-
he ques-
rs in the 
sentially 
 also in-
ion and 
 to cQA 
ot intend 
e telling 
. Essen-
g rooms 
so many 
 ask...?? 
?how to 
d to do a 
ople that 
elow so 
pressing 
f people 
different 
Answers 
uestions 
igational 
 do not 
asonable 
e to use 
 naviga-
500
 tional and constant questions. They do not have 
to ask these types of question on community-
based question answering services. On the con-
trary, open and opinion questions are frequently 
asked, it ranges from 56%~83%.  
Question Type C&I E&M Health S&C
Navigational Total 0% 0% 0% 0%
Constant 15% 20% 15% 11%
Opinion 8% 37% 16% 60%
Context     Dependent 0% 1% 1% 0%
Open 59% 19% 67% 18%
Dynamic Total 67% 57% 84% 78%
Informational Total 82% 77% 99% 89%
Transactional Total 14% 8% 0% 1%
Social Total 4% 15% 1% 10%
Table 5 Distribution of Question Type 
Intersection Number UNI DIR IND SUB REL IRR
Navigational 0 0 0 0 0 0
Constant 48 9 3 0 1 0
Open 51 62 13 15 5 17
Context-dep 0 0 1 0 0 1
Opinion 15 13 1 84 0 8
Transactional 10 7 4 1 0 1
Social 0 0 0 1 0 29
Table 6. Question Answer Correlation 
Table 6 (UNI: unique, DIR: direct, IND: indi-
rect, SUB: subjective, REL: relevant, IRR: irre-
levant) gives the correlation statistics of question 
type vs. answer type. There exists a strong corre-
lation between question type and answer type. 
Every question type tends to be associated with 
only one or two answer types (bold numbers in 
Table 6).  
5 Question-Type Oriented Answer 
Summarization 
Since the BAs for at least half of questions do 
not cover all useful information of other answers, 
it is better to adopt post-processing techniques 
such as answer summarization for better reuse of 
the BAs. As observed in the previous sections, 
answer types can be basically predicted by ques-
tion type. Thus, in this section, we propose to use 
multi-document summarization (MDS) tech-
niques for summarizing answers according to 
question type. Here we assume that question type 
can be determined automatically. In the follow-
ing sub-sections, we will focus on the summari-
zation of answers to open or opinion questions as 
they occupy more than half of the cQA questions. 
5.1 Open Questions 
Algorithm: For open questions, we follow typi-
cal MDS procedure: topic identification, inter-
pretation & fusion, and then summary generation 
(Hovy and Lin, 1999; Lin and Hovy, 2002). Ta-
ble 7 describes the algorithm.  
1. Employ the clustering algorithm on answers 
2. Extract the noun phrases in each cluster, using a shallow parser.6 
3. For each cluster and each label (or noun phrase), calculate the 
score by using the Relevance Scoring Function:  
?p?w|??PMI?w, l|C? ?  D??|C?
?
 
Where ? is the cluster, w is the word, l is the label or noun phrase, C 
is the background context which is composed of 5,000 questions 
in the same category, p(?) is conditional probability, PMI(?) is 
pointwise mutual information, and D(?) is KL-divergence 
4. Extract the key answer which contains the noun phrase that has 
the highest score in each cluster 
5. Rank these key answers by cluster size and present the results. 
Table 7. Summarization Algorithm(Open-Type) 
In the first step, we use a bottom-up approach 
for clustering answers to do topic identification. 
Initially, each answer forms a cluster. Then we 
combine the most similar two clusters as a new 
cluster if their similarity is higher than a thre-
shold. This process is repeated until no new clus-
ters can be formed. For computing similarities, 
we regard the highest cosine similarity of two 
sentences from two different clusters as the simi-
larity of the two clusters. Then we extract salient 
noun phrases, i.e. cluster labels, from each clus-
ter using the first-order relevance scoring func-
tion proposed by Mei et al (2007), (step 2,3 in 
Table 7).  In the fusion phase (step 4), these 
phrases are then used to rank answers within 
their cluster. Finally in the generation phase (step 
5), we present the summarized answer by ex-
tracting the most important answer in every clus-
ter and sort them according to the cluster size 
where they come from.  
Case Example: Table 8 presents an example 
of summarization results of open-type questions. 
The question asks how to change Windows XP 
desktop to Mac style. There are many softwares 
providing such functionalities. The BA only lists 
one choice ? the StarDock products, while other 
answers suggest Flyakite and LiteStep. The au-
tomatic summarized answer (ASA) contains a 
variety of for turning Windows XP desktop into 
Mac style with their names highlighted as cluster 
labels. Compared with manually-summarized 
answer (MSA), ASA contains most information 
of MSA while retains similar length with BA and 
MSA. 
5.2 Opinion Questions 
Algorithm: For opinion questions, a comprehen-
sive investigation of this topic would be beyond 
the scope of this paper since this is still a field 
                                                 
6 http://opennlp.sourceforge.net 
501
 under active development (Wiebe et al, 2003; 
Kim and Hovy, 2004). We build a simple yet 
novel opinion-focused answer summarizer which 
provides a global view of all answers. We divide 
opinion questions into two subcategories. One is 
sentiment-oriented question that asks the senti-
ment about something, for example, ?what do 
you think of ??. The other is list-oriented ques-
tion that intends to get a list of answers and see 
what item is the most popular.  
For sentiment-oriented questions, askers care 
about how many people support or against some-
thing. We use an opinion word dictionary7, a cue 
phrase list, a simple voting strategy, and some 
heuristic rules to classify the sentences into Sup-
port, Neutral, or Against category and use the 
overall attitude with key sentences to build sum-
marization. For list-oriented questions, a simple 
counting algorithm that tallies different answers 
of questions together with their supporting votes 
would be good answer summaries. Details of the 
algorithm are shown in Table 9, 10. 
Case Example: Table 11 presents the summa-
rization result of an sentiment-oriented question, 
it asks ?whether it is strange for a 16-year child 
to talk to a teddy bear??, the BA is a negative 
response. However, if we consider all answers, 
                                                 
7 Inquirer dictionary  http://www.wjh.harvard.edu/~inquirer. 
we find that half of the answers agree but another 
half of them disagree. The distribution of differ-
ent sentiments is similar as MSA. Table 12 
shows the summarization result of a list-oriented 
question, the question asks ?what is the best sci-fi 
movie?? The BA just gives one choice ?Indepen-
dence day? while the summarized answer gives a 
list of best sci-fi movies with the number of sup-
porting vote. Though it is not complete compared 
with MSA, it contains most of the options which 
has highest votes among all answers. 
1. Employ the same cluster procedure of Open-Type question. 
2. If an answer begins with negative cue phrase (e.g. ?No, it isn?t? 
etc.), it is annotated as Against. If a response begins with positive 
cue phrase (e.g. ?Yes, it is? etc.), it is annotated as Support. 
3. For a clause, if number of positive sentiment word is larger than 
negative sentiment word, the sentiment of the clause is Positive. 
Otherwise, the sentiment of the clause is Negative. 
4. If there are negative indicators such as ?don?t/never/?? in front 
of the clause, the sentiment should be reversed. 
5. If number of negative clauses is larger than number of positive 
clauses, the sentiment of the answer is Negative. Otherwise, the 
sentiment of the answer is Positive. 
6. Denote the sentiment value of question as s(q), the sentiment 
value of an answer as s(a), and then the final sentiment of the an-
swer is logical AND of s(q) and s(a) 
7. Present key sentiments with attitude label 
Table 9. Summarization Algorithm (Senti-
ment-Opinion) 
1. Segment the answers into sentences 
2. Cluster sentences  by using similar process in open-type 
3. For each cluster, choose the key sentence based on mutual infor-
mation between itself and other sentences within the cluster 
4. Rank the key sentences by the cluster size and present them 
ogether with votes 
Table 10. Summarization Algorithm (List-
Opinion) 
Question (http://answers.yahoo.com/question/?qid=1006050125145) 
I am 16 and i stil talk to my erm..teddy bear..am i wierd??? 
Best Answer Chosen 
not at all i'm 14 and i too do that 
Auto-summarized Answer 
Support 
A: It's might be a little uncommon for a 16 year old to talk to a 
teddy bear but there would be a serious problem if you told me that 
your teddy bear answered back as you talked to him!!:)  
A: I slept with my teddy bear until I graduated.  Can't say that I 
ever had a conversation with him, but if I had I'm sure he would've 
been a very good listener. 
Against 
A: i talk to a  seed im growing .. its not weird .... :)  
A: No, you're not weird.....you're Pratheek! :D  
A: no, i like to hold on to my old memories too. i do it sometimes 
too.  
A: It will get weird when he starts to answer back!  
A: not really. it depends how you talk i mean not if you talk to it 
like its a little kid like my brother does.  
Overall Attitude: Support 5 / Neutral 1 / Against 5  
Manually-summarized Answer 
support (vote 4) 
neutral (vote 2) 
against (vote 5) reasons: i like to hold on to my old memories too. 
(vote 1) I slept with my teddy bear until I graduated. (vote 1) i'm 14 
and i too do that (vote 1) 
Table 11. Summary of Sentiment-Opinion 
Question 
 
 
Question 
(http://answers.yahoo.com/question/?qid=1005120801427) 
What is the best way to make XP look like Mac osX? 
Best Answer Chosen 
I found the best way to do this is to use WindowsBlinds. A pro-
gram that, if you use the total StarDock, package will allow you to 
add the ObjectBar in addition to changed the toolbars to be OS X 
stylized. If you want added functionality you can download pro-
grams off the internet that will mimic the Expose feature which will 
show you a tiled set of all open windows. Programs that will do this 
include: WinPlosion, Windows Exposer, and Top Desk 
Auto-summarized Answer 
LiteStep:An additional option is LiteStep - a "Shell Replacement" 
for Windows that has a variety of themes you can install. Undoub-
tedly there are various Mac OSX themes avaialable for LiteStep. I 
have included a source to a max osx theme for Litestep at custom-
ize.org.  
Flyakite:Flyakite is a transformation pack and the most compre-
hensive in terms of converting an XP system's look to that of an OS 
X system, google it up and you should find it, v3 seems to be in 
development and should be out soon. 
Window Blinds:http://www.stardock.com/products/windowb... 
Manually-summarized Answer 
One way is to use WindowsBlinds. The package will allow you to 
add the ObjectBar for changing to the OSX theme. You can also 
make added functionality of Expose feature by downloading the 
programs like WinPlosion, Windows Exposer and Top Desk. The  
URL of it is http://www.stardock.com/products/windowblinds/. 
Another option is to use Flyakite which is a transformation pack. 
The third Option is the LiteStep, it is a "Shell Replacement" for 
windows that has a variety of Mac OSX tehmes you can install. 
The url is http://litestep.net and I have included a source of Mac OS 
theme for Litestep at http://www.customize.org/details/33409. 
Table 8. Summary of Open-Question 
502
  
Question (http://answers.yahoo.com/question/?qid= 
20060718083151AACYQJn) 
What is the best sci-fi movie u ever saw? 
Best Answer Chosen 
Independance Day 
Auto-summarized Answer 
star wars (5)  
Blade Runner (3) 
fi movie has to be Night of the Lepus (2)  
But the best "B" sci (2)  
I liked Stargate it didn't scare me and I thought they did a great job 
recreating Egypt (3)  
Independance Day (3) 
Manually-summarized Answer 
Star Wars (vote 6); The Matrix (vote 3); Independence Day (vote 
2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote 
2); Alien v.s Predator (vote 1); MST3K (vote 1);  
Table 12. Summary of List-Opinion Question
5.3 Experiments 
Information Content: To evaluate the effec-
tiveness of automatic summarization, we use the 
information content criterion for comparing ASA 
with BA. It focuses on whether ASA or BA con-
tains more useful information to the question. 
Information point is used in the evaluation. 
Usually, one kind of solution for open questions 
or one kind of reason for opinion questions can 
contribute one information point. By summing 
all information points in both ASA and BA, we 
then can compare which one contains more in-
formation. Intuitively, longer texts would contain 
more information. Thus, when comparing the 
information content, we limit the length of ASA 
with several levels to do the evaluation. Take 
question in Table 8 as an example, the BA just 
gives one software, which contributes one infor-
mation point while the ASA lists three kinds of 
software which contributes three information 
points. Thus, ASA is considered better than BA.  
For each question, we generate 100%, 150%, 
and 200% BA word-length ASAs. Three annota-
tors are asked to determine whether an ASA is 
better than, equal to, or worse than its corres-
ponding BA in terms of information content. 
Voting strategy is used to determine the final 
label. If three labels are all different, it is labeled 
as Unknown. We extract 163 open questions and 
121 opinion questions from all four categories by 
using final question category labels mentioned in 
Section 4. To make meaningful comparison, 
questions having unique answers or having only 
one answer are excluded. After the removal, 
there are 104 open questions and 99 opinion 
questions left for comparison. The results are 
shown in Table 13.  
We are encouraged by the evaluation results 
that our automatic summarization methods gen-
erate better coverage of contents in most of the 
cases at every answer summary length. We ob-
serve a big difference between 100% and 150% 
answer summaries. It should not be a surprise 
since a 150% answer summary contains 50% 
more content than its corresponding BA. While 
at the 100% length, we still have about 30% 
ASAs better than BA. Questions which have bet-
ter ASA than BA usually have a long BA but 
with little information. Table 14 provides the 
example. By using summarization, answers that 
are compact and direct to the question can be 
included. The results indicate that summary 
compression technique might be helpful to pack 
more information in short answers. 
Open ASA Better BA Better Equal Unknown 
100% 30% 12% 45% 13% 
150% 55% 7% 28% 10% 
200% 63% 4% 24% 9% 
Opinion ASA Better BA Better Equal Unknown
100% 37% 20% 32% 11% 
150% 44% 16% 30% 10% 
200% 54% 16% 23% 7% 
Table 13. Evaluation by Information Content 
Q Why wont japanese characters burn onto the DVD? 
BA man, the answers here are too stupid for hteir own.You are 
creating a DVD on Western Platform. I take it, you are 
using an OS that is in English?In order to "view" japanese 
as part of your filenames, you need your operating system 
to accept Japanese coding (characters).If you are using 
Windows, then you will need ot isntall the Japanese cha-
racter Set for your operating system 
If you are using MacOS . i have no idea. 
100% 
ASA
 
The dvd writer 
Probably because your burner, the DVD writer, doesn't 
support double bytes code, such as Japanese, Korean, and 
Chinese. Check the supporting language of your software. 
Or change all the file name in single byte code, like alpha-
bets. man, the answers here are too stupid for hteir own. 
You are creating a DVD on Western Platform. I take it, 
you are using an OS that is in English? 
Table 14. Examples of 100% ASA 
Readability: Besides the information content, 
we would also like to study the readability of 
automatic summarized answers. 10 questions 
(each from open and opinion category) are ex-
tracted and we make both manual summarized 
answer (MSA) and automatic summarized an-
swer (ASA) for comparison with BA. We used 
the information content (INFO) and readability 
(READ) criteria for evaluation. The readability is 
judged basically by the time for understanding. 
We make two kinds of comparison: ASA vs. BA 
and MSA vs. BA. The first one is used to judge 
whether the current summarization method is 
better than current cQA scenario. The second one 
is used as an expectation for how much the 
summarization methods can be better than BA. 
503
 For ASA vs. BA, the results in Table 15 show 
that all the annotators agree ASAs providing 
more information content but not being with sa-
tisfying readability. For MSA vs. BA, better re-
sults in readability can be achieved as Table 16. 
This suggests that the proposed approach can 
succeed as more sophisticated summarization 
techniques are developed. 
Open Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 40% 10% 90% 10% 80% 0%
Equal 60% 60% 10% 80% 20% 60%
Worse 0% 30% 0% 10% 0% 40%
Opinion Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 90% 10% 90% 10% 70% 40%
Equal 10% 60% 10% 60% 10% 20%
Worse 0% 30% 0% 30% 20% 40%
Table 15. ASA vs. BA Evaluation 
Open Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 100% 30% 100% 90% 100% 90%
Equal 0% 50% 0% 0% 0% 0%
Worse 0% 20% 0% 10% 0% 10%
Opinion Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 90% 20% 60% 70% 100% 100%
Equal 10% 80% 40% 30% 0% 0%
Worse 0% 0% 0% 0% 0% 0%
Table 16. MSA vs. BA Evaluation 
6 Conclusion and Future Work 
In this paper, we have carried out a comprehen-
sive analysis of the question types in community-
based question answering (cQA) services and 
have developed taxonomies for questions and 
answers. We find that questions do not always 
have unique best answers. Open and opinion 
questions usually have multiple good answers. 
They occupied about 56%~83% and most of 
their best answers can be improved. By using 
question type as a guide, we propose applying 
automatic summarization techniques to summa-
rization answers or improving cQA best answers 
through answer editing. Our results show that 
customized question-type focused summarization 
techniques can improve cQA answer quality sig-
nificantly.  
Looking into the future, we are to develop au-
tomatic question type identification methods to 
fully automate answer summarization. Further-
more, we would also like to utilize more sophis-
ticated summarization techniques to improve 
content compaction and readability. 
Acknowledgements 
We thank the anonymous reviewers for their val-
uable suggestions and comments to this paper. 
References 
Broder  A. A taxonomy of web search. 2002. SIGIR 
Forum Vol.36, No. 2, 3-10. 
Hovy Edward, Laurie Gerber,  Ulf Hermjakob, Chin-
Yew Lin,   Deepak Ravichandran. 2001. Toward 
Semantics-Based Answer Pinpointing. In Proc. of 
HLT?01. 
Hovy E., C. Lin. 1999. Automated Text Summariza-
tion and the SUMMARIST System. In Advances 
in Automated Text Summarization 
Jeon J., W. B. Croft, and J. Lee. 2005a. Finding se-
mantically similar questions based on their an-
swers. In Proc. of SIGIR?05. 
Jeon J., W. B. Croft, and J. Lee. 2005b. Finding simi-
lar questions in large question and answer arc-
hives. In Proc. of CIKM?05. 
Jurczyk P., E. Agichtein. 2007. Hits on question an-
swer portals: exploration of link analysis for au-
thor ranking. In Proc. of SIGIR '07. 
Jeon J. , W.B. Croft, J. Lee, S. Park. 2006. A Frame-
work to predict the quality of answers with non-
textual features. In Proc. of SIGIR ?06. 
Jijkoun V., M. R. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Kleinberg J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, vol. 46,  
Kim S., E.  Hovy. 2004. Determining the Sentiment of 
Opinions. In Proc. of COLING?04. 
Liu X., W.B. Croft, M. Koll. 2005. Finding experts in 
community-based question-answering services. 
In Proc. of CIKM '05. 
Lin C.Y., E. Hovy. 2002.  From single to multi-
document summarization: a prototype system and 
its evaluation.  In Proc. of ACL'02. 
Lytinen S., N. Tomuro. 2002. The Use of Question 
Types to Match Questions in FAQFinder. In Proc. 
of AAAI?02. 
Moldovan D., S. Harabagiu, et al 2000. The Structure 
and an Open-Domain Question Answering Sys-
tem. In Proc. of ACL?00. 
Mei Q., X. Shen, C. Zhai. 2007. Automatic labeling of 
multinomial topic models. In Proc. of   KDD'07. 
Rose  D. E., D. Levinson. 2004. Understanding user 
goals in web search. In Proc. of WWW '04. 
Voorhees, M. Ellen. 2003. Overview of the TREC 
2003 Question Answering Track. In Proc. of 
TREC?03. 
Wiebe J., E. Breck, et al 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press 
504
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 514?523,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Structural Support Vector Method for Extracting Contexts and
Answers of Questions from Online Forums
Wen-Yun Yang
??
Yunbo Cao
??
Chin-Yew Lin
?
?
Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, China
?
Microsoft Research Asia, Beijing, China
wenyun.yang@gmail.com {yunbo.cao; cyl}@microsoft.com
Abstract
This paper addresses the issue of extract-
ing contexts and answers of questions
from post discussion of online forums.
We propose a novel and unified model by
customizing the structural Support Vector
Machine method. Our customization has
several attractive properties: (1) it gives a
comprehensive graphical representation of
thread discussion. (2) It designs special
inference algorithms instead of general-
purpose ones. (3) It can be readily ex-
tended to different task preferences by
varying loss functions. Experimental re-
sults on a real data set show that our meth-
ods are both promising and flexible.
1 Introduction
Recently, extracting questions, contexts and an-
swers from post discussions of online forums in-
curs increasing academic attention (Cong et al,
2008; Ding et al, 2008). The extracted knowl-
edge can be used either to enrich the knowledge
base of community question answering (QA) ser-
vices such as Yahoo! Answers or to augment the
knowledge base of chatbot (Huang et al, 2007).
Figure 1 gives an example of a forum thread
with questions, contexts and answers annotated.
This thread contains three posts and ten sentences,
among which three questions are discussed. The
three questions are proposed in three sentences,
S3, S5 and S6. The context sentences S1 and
S2 provide contextual information for question
sentence S3. Similarly, the context sentence S4
provides contextual information for question sen-
tence S5 and S6. There are three question-context-
answer triples in this example, (S3) ? (S1,S2) ?
(S8,S9), (S5)? (S4)? (S10) and (S6)? (S4)?
?
This work was done while the first author visited Mi-
crosoft Research Asia.
Post1: <context id=1> S1: Hi I am looking for
a pet friendly hotel in Hong Kong because all of
my family is going there for vacation. S2: my fam-
ily has 2 sons and a dog. </context> <question
id=1> S3: Is there any recommended hotel near
Sheung Wan or Tsing Sha Tsui? </question>
<context id=2, 3> S4: We also plan to go shopping
in Causeway Bay. </context> <question id=2>
S5: What?s the traffic situation around those com-
mercial areas? </question> <question id=3> S6:
Is it necessary to take a taxi? </question> S7: Any
information would be appreciated.
Post2: <answer id=1> S8: The Comfort Lodge
near Kowloon Park allows pet as I know, and usu-
ally fits well within normal budgets. S9: It is also
conveniently located, nearby the Kowloon railway
station and subway. </answer>
Post3: <answer id=2, 3> S10: It?s very crowd in
those areas, so I recommend MTR in Causeway Bay
because it is cheap to take you around. </answer>
Figure 1: An example thread with three posts and
ten sentences
(S10). As shown in the example, a forum question
usually requires contextual information to com-
plement its expression. For example, the ques-
tion sentence S3 would be of incomplete meaning
without the contexts S1 and S2, since the impor-
tant keyword pet friendly would be lost.
The problem of extracting questions, contexts,
and answers can be solved in two steps: (1) iden-
tify questions and then (2) extract contexts and an-
swers for them. Since identifying questions from
forum discussions is already well solved in (Cong
et al, 2008), in this paper, we are focused on step
(2) while assuming questions already identified.
Previously, Ding et al (2008) employ general-
purpose graphical models without any customiza-
tions to the specific extraction problem (step 2).
In this paper, we improve the existing models in
514
three aspects: graphical representation, inference
algorithm and loss function.
Graphical representation. We propose a more
comprehensive and unified graphical representa-
tion to model the thread for relational learning.
Our graphical representation has two advantages
over previous work (Ding et al, 2008): unifying
sentence relations and incorporating question in-
teractions.
Three types of relation should be considered for
context and answer extraction: (a) relations be-
tween successive sentences (e.g., context sentence
S2 occurs immediately before question sentence
S3); (b) relations between context sentences and
answer sentences (e.g., context S4 presents the
phrase Causeway Bay linking to answer which is
absent from question S6); and (c) relations be-
tween multiple labels for one sentence (e.g., one
question sentence is unlikely to be the answer to
another question although one sentence can serve
as contexts for more than one questions). Our pro-
posed graphical representation improves the mod-
eling of the three types of sentence relation (Sec-
tion 2.2).
Certain interactions exist among questions. For
example, question sentences S5 and S6 interact by
sharing context sentence S4. Our proposed graphi-
cal representation can naturally model the interac-
tions. Previous work (Ding et al, 2008) performs
the extraction of contexts and answers in multiple
passes of the thread (with each pass corresponding
to one question), which cannot address the interac-
tions well. In comparison, our model performs the
extraction in one pass of the thread.
Inference algorithm. Inference is usually a
time-consuming process for structured prediction.
We design special inference algorithms, instead of
general-purpose inference algorithms used in pre-
vious works (Cong et al, 2008; Ding et al, 2008),
by taking advantage of special properties of our
task. Specifically, we utilize two special properties
of thread structure to reduce the inference (time)
cost. First, context sentences and question sen-
tences usually occur in the same post while answer
sentences can only occur in the following posts.
With this properties, we can greatly reduce context
(or answer) candidate sets of a question, which
results in a significant decrease in inference cost
(Section 3). Second, context candidate set is usu-
ally much smaller than the number of sentences
in a thread. This property enables our proposal to
have an exact and efficient inference (Section 4.1).
Moreover, an approximate inference algorithm is
also given (Section 4.2).
Loss function. In practice, different applica-
tion settings usually imply different requirements
for system performance. For example, we expect
a higher recall for the purpose of archiving ques-
tions but a higher precision for the purpose of re-
trieving questions. A flexible framework should
be able to cope with various requirements. We
employ structural Support Vector Machine (SVM)
model that could naturally incorporate different
loss functions (Section 5).
We use a real data set to evaluate our approach
to extracting contexts and answers of questions.
The experimental results show both the effective-
ness and the flexibility of our approach.
In the next section, we formalize the problem
of context and answer extraction and introduce the
structural model. In Sections 3, 4 and 5 we give
the details of customizing structural model for our
task. In Section 6, we evaluate our methods. In
Section 7, we discuss the related work. Finally,
we conclude this paper in Section 8.
2 Problem Statement
We first introduce our notations in Section 2.1 and
then in Section 2.2 introduce how we model the
problem of extracting contexts and answers for
questions with a novel form of graphical represen-
tation. In Section 2.3 we introduce the structured
model based on the new representation.
2.1 Notations
Assuming that a given thread contains p posts
{p
1
, . . . , p
p
}, which are authored by a set of
users {u
1
, . . . , u
p
}. The p posts can be further
segmented into n sentences x = {x
1
, . . . , x
n
}.
Among the n sentences, m question sentences q =
{x
q
1
, . . . , x
q
m
} have been identified. Our task is
to identify the context sentences and the answer
sentences for those m question sentences. More
formally, we use four types of label {C,A,Q, P}
to stand for context, answer, question and plain la-
bels. Then, our task is to predict an m ? n label
matrix y = (y
ij
)
1?i?m,1?j?n
, except m elements
{y
1,q
1
, . . . , y
m,q
m
} which correspond to (known)
question labels. The element y
ij
in label matrix y
represents the role that the jth sentence plays for
the ith question. We denote the ith row and jth
column of the label matrix y by y
i.
and y
.j
.
515
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(a) Skip-chain model
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(b) Complete skip-chain model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(c) 2D model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(d) Label group model
Figure 2: Structured models
2.2 Graphical Representation
Recently, Ding et al (2008) use skip-chain and
2D Conditional Random Fields (CRFs) (Lafferty
et al, 2001) to perform the relational learning for
context and answer extraction. The skip-chain
CRFs (Sutton and McCallum, 2004; Galley, 2006)
model the long distance dependency between con-
text and answer sentences and the 2D CRFs (Zhu
et al, 2005) model the dependency between con-
tiguous questions. The graphical representation
of those two models are shown in Figures 2(a)
and 2(c), respectively. Those two CRFs are both
extensions of the linear chain CRFs for the sake
of powerful relational learning. However, di-
rectly using the skip-chain and 2D CRFs with-
out any customization has obvious disadvantages:
(a) the skip-chain model does not model the de-
pendency between answer sentence and multiple
context sentences; and (b) the 2D model does not
model the dependency between non-contiguous
questions.
To better model the problem of extracting con-
texts and answers of questions, we propose two
more comprehensive models, complete skip-chain
model and label group model to improve the ca-
pability of the two previous models. These two
models are shown in Figures 2(b) and 2(d).
In Figures 2(a) and 2(b), each label node is an-
notated with its allowed labels and the labels C, A,
Q and P stand for context, answer, question and
plain sentence labels, respectively. Note that the
complete skip-chain model completely links each
two context and answer candidates and the label
group model combines the labels of one sentence
into one label group.
2.3 Structured Model
Following the standard machine learning setup,
we denote the input and output spaces by X and
Y , then formulate our task as learning a hypoth-
esis function h : X ? Y to predict a y when
given x. In this setup, x represents a thread of n
sentences and m identified questions. y represents
the m? n label matrix to be predicted.
Given a set of training examples, S =
{(x
(i)
,y
(i)
) ? X ? Y : i = 1, . . . , N}, we
restrict ourselves to the supervised learning sce-
nario. We focus on hypothesis functions that
take the form h(x;w) = argmax
y?Y
F(x,y;w)
with discriminant function F : X ? Y ? R
where F(x,y;w) = w
T
?(x,y). As will be
introduced in Section 4, we employ structural
SVMs (Joachims et al, 2009) to find the optimal
parameters w. The structural SVMs have sev-
eral competitive properties as CRFs. First, it fol-
lows from the maximum margin strategy, which
has been shown with competitive or even better
516
performance (Tsochantaridis et al, 2005; Nguyen
and Guo, 2007). Second, it allows flexible choices
of loss functions to users. Moreover, in general,
it has theoretically proved convergence in polyno-
mial time (Joachims et al, 2009).
To use structural SVMs in relational learning,
one needs to customize three steps according to
specific tasks. The three steps are (a) definition of
joint feature mapping for encoding relations, (b)
algorithm of finding the most violated constraint
(inference) for efficient trainings and (c) definition
of loss function for flexible uses.
In the following Sections 3, 4 and 5, we describe
the customizations of the three steps for our con-
text and answer extraction task, respectively.
3 Encoding Relations
We use a joint feature mapping to model the rela-
tions between sentences in a thread. For context
and answer extraction, the joint feature mapping
can be defined as follows,
?(x,y) =
?
?
?
n
(x,y)
?
h
(x,y)
?
v
(x,y)
?
?
,
where the sub-mappings ?
n
(x,y), ?
h
(x,y), and
?
v
(x,y) encode three types of feature mappings,
node features, edge features and label group fea-
tures. The node features provide the basic infor-
mation for the output labels. The edge features
consist of the sequential edge features and skip-
chain edge features for successive label dependen-
cies. The label group features encode the relations
within each label group.
Before giving the detail definitions of the sub-
mappings, we first introduce the context and an-
swer candidate sets, which will be used for the
definitions and inferences. Each row of the label
matrix y corresponds to one question. Assuming
that the ith row y
i.
corresponds to the question
with sentence index q
i
, we thus have two candi-
date sets of contexts and answers for this question
denoted by C and A, respectively. We denote the
post indices and the author indices for the n sen-
tences as p = (p
1
, . . . , p
n
) and u = (u
1
, . . . , u
n
).
Then, we can formally define the two candidate
sets for the y
i.
as
C =
{
c
j
?
?
?
?
?
p
c
j
= p
q
i
? ?? ?
In Question Post
, c
j
6= q
i
? ?? ?
Not Question Sentence
}
,
A =
{
a
j
?
?
?
?
?
p
a
j
> p
q
i
? ?? ?
After Question Post
, u
a
j
6= u
q
i
? ?? ?
Not by the Same User
}
.
In the following, we describe formally about the
definitions of the three feature sub-mappings.
The node feature mapping ?
n
(x,y) encodes
the relations between sentence and label pairs, we
define it as follows,
?
n
(x,y) =
m
?
i=1
n
?
j=1
?
n
(x
j
, y
ij
),
where ?
n
(x
j
, y
ij
) is a feature mapping for a given
sentence and a label. It can be formally defined as
follows,
?
n
(x
j
, y
ij
) = ?(y
ij
)? ?
q
i
(x
j
), (1)
where ? denotes a tensor product, ?
q
i
(x
j
) and
?(y
ij
) denote two vectors. ?
q
i
(x
j
) contains ba-
sic information for output label. ?(y
ij
) is a 0/1
vector defined as
?(y
ij
) = [?
C
(y
ij
), ?
A
(y
ij
), ?
P
(y
ij
)]
T
,
where ?
C
(y
ij
) equal to one if y
ij
= C, otherwise
zero. The ?
A
(y
ij
) and ?
P
(y
ij
) are similarly de-
fined. Thus, for example, writing out ?
n
(x
j
, y
ij
)
for y
ij
= C one gets,
?
n
(x
j
, y
ij
) =
?
?
?
q
i
(x
j
)
0
0
?
?
? context
? answer
? plain
.
Note that the node feature mapping does not in-
corporate the relations between sentences.
The edge feature mapping ?
h
(x,y) is used
to incorporate two types of relation, the relation
between successive sentences and the relation be-
tween context and answer sentences. It can be de-
fined as follows,
?
h
(x,y) =
[
?
hn
(x,y)
?
hc
(x,y)
]
,
where ?
hn
(x,y) and ?
hc
(x,y) denote the two
types of feature mappings corresponding to se-
quential edges and skip-chain edges, respectively.
Their formal definitions are given as follows,
?
hn
(x,y) =
m
?
i=1
n?1
?
j=1
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
517
Descriptions Dimensions
?
q
i
(x
j
) (32 dimensions) in ?
n
(x,y)
The cosine, WordNet and KL-divergence similarities with the question x
q
i
3
The cosine, WordNet and KL-divergence similarities with the questions other than x
q
i
3
The cosine, WordNet and KL-divergence similarities with previous and next sentences 6
Is this sentence x
j
exactly x
q
i
or one of the questions in {x
q
1
, . . . , x
q
m
}? 2
Is this sentence x
j
in the three beginning sentences? 3
The relative position of this sentence x
j
to questions 4
Is this sentence x
j
share the same author with the question sentence x
q
i
? 1
Is this sentence x
j
in the same post with question sentences? 2
Is this sentence x
j
in the same paragraph with question sentences? 2
The presence of greeting (e.g., ?hi?) and acknowledgement words in this sentence x
j
2
The length of this sentence x
j
1
The number of nouns, verbs and pronouns in this sentence x
j
, respectively 3
?
h
(x,y) (704 dimensions)
For ?
hn
(x,y), the above 32 dimension features w.r.t. 4? 4 = 16 transition patterns 512
For ?
hc
(x,y), 12 types of pairwise or merged similarities w.r.t. 16 transition patterns 192
?
v
(x,y) (32 dimensions)
The transition patterns for any two non-contiguous labels in a label group 16
The transition patterns for any two contiguous labels in a label group 16
Table 1: Feature descriptions and demisions
?
hc
(x,y) =
m
?
i=1
?
j?C
?
k?A
? ?? ?
Complete Edges
?
hc
(x
j
, x
k
, y
ij
, y
ik
),
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
)
= ?(y
ij
, y
i,j+1
)? ?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
?
hc
(x
j
, x
k
, y
ij
, y
ik
)
= ?(y
ij
, y
ik
)? ?
hc
(x
j
, x
k
, y
ij
, y
ik
)
where ?(y
ij
, y
ik
) is a 16-dimensional vector. It in-
dicates all 4?4 pairwise transition patterns of four
types of labels, the context, answer, question and
plain. Note that apart from previous work (Ding
et al, 2008) we use complete skip-chain (context-
answer) edges in ?
hc
(x,y).
The label group feature mapping ?
v
(x,y) is
defined as follows,
?
v
(x,y) =
n
?
j=1
?
v
(x
j
,y
.j
),
where ?
v
(x
j
,y
.j
) encodes each label group pat-
tern into a vector.
The detail descriptions and vector dimensions
of the used features are listed in Table 1.
4 Structural SVMs and Inference
Given a training set S = {(x
(i)
,y
(i)
) ? X ?
Y : i = 1, . . . , N}, we use the structural
SVMs (Taskar et al, 2003; Tsochantaridis et
al., 2005; Joachims et al, 2009) formulation, as
shown in Optimization Problem 1 (OP1), to learn
a weight vector w.
OP 1 (1-Slack Structural SVM)
min
w,??0
1
2
||w||
2
+
C
N
?
s.t. ?(y?
(1)
, . . . , y?
(N)
) ? Y
n
,
1
N
w
T
N
?
i=1
[?(x
(i)
,y
(i)
)??(x
(i)
, y?
(i)
)]
?
1
N
N
?
i=1
?(y
(i)
, y?
(i)
)? ?,
where ? is a slack variable, ?(x,y) is the joint
feature mapping and ?(y, y?) is the loss func-
tion that measures the loss caused by the dif-
ference between y and y?. Though OP1 is al-
ready a quadratic optimization problem, directly
using off-the-shelf quadratic optimization solver
will fail, due to the large number of constraints.
Instead, a cutting plane algorithm is used to ef-
ficiently solve this problem. For the details of the
518
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
(a) Original graph
{P P P , P P C , P C P , P C C , C P P , C P C , C C P , C C C }
{Q } {P } {A, P } {A, P }
(b) Transformed graph
{P P P }
{Q } {P } {A, P } {A, P }
{C C C }
{Q } {P } {A, P } {A, P }
....
(c) Decomposed graph
Figure 3: The equivalent transform of graphs
Algorithm 1 Exact Inference Algorithm
1: Input: (C
i
,A
i
) for each q
i
, w, x, y
2: for i ? {1, . . . ,m} do
3: for C
s
? C
i
do
4: [R(C
s
), y?
i.
(C
s
)] ? Viterbi(w,x; C
s
)
5: end for
6: C
?
s
= argmax
C
s
?C
i
R(C
s
)
7: y?
?
i.
= y?
i.
(C
?
s
)
8: end for
9: return y?
?
structural SVMs, please refer to (Tsochantaridis et
al., 2005; Joachims et al, 2009).
The most essential and time-consuming step in
structural SVMs is finding the most violated con-
straint, which is equivalent to solve
argmax
y?Y
w
T
?(x
(i)
,y) + ?(y
(i)
,y). (2)
Without the ability to efficiently find the most vio-
lated constraint, the cutting plane algorithm is not
tractable.
In the next sub-sections, we introduce the al-
gorithms for finding the most violated constraint,
also called loss-augmented inference. The algo-
rithms are essential for the success of customizing
structural SVMs to our problem.
4.1 Exact Inference
The exact inference algorithm is designed for a
simplified model with two sub-mappings ?
n
and
?
h
, except ?
v
.
One naive approach to finding the most violated
constraint for the simplified model is to enumer-
ate all the 2
|C|+|A|
cases for each row of the label
matrix. However, it would be intractable for large
candidate sets.
An important property is that the context can-
didate set is usually much smaller than the whole
number of sentences in a thread. This property en-
ables us to design efficient and exact inference al-
gorithm by transforming from the original graph
representation in Figure 2 to the graphs in Fig-
ure 3. This graph transform merges all the nodes
in the context candidate set C to one node with 2
|C|
possible labels.
We design an exact inference algorithm in Algo-
rithm 1 based on the graph in Figure 3(c). The al-
gorithm can be summarized in three steps: (1) enu-
merate all the 2
|C|
possible labels
1
for the merged
node (line 3). (2) For each given label of the
merged node, perform the Viterbi algorithm (Ra-
biner, 1989) on the decomposed graph (line 4) and
store the Viterbi algorithm outputs in R and y?
i.
.
(3) From the 2
|C|
Viterbi algorithm outputs, select
the one with highest score as the output (lines 6
and 7).
The use of the Viterbi algorithm is assured by
the fact that there exists certain equivalence be-
tween the decomposed graph (Figure 3(c)) and a
linear chain. By fixing the the label of the merged
node, we could remove the dashed edges in the
decomposed graph and regard the rest graph as a
linear chain, which results in the Viterbi decoding.
4.2 Approximate Inference
The exact inference cannot handle the complete
model with three sub-mappings, ?
n
, ?
h
, and
?
v
, since the label group defeats the graph trans-
form in Figure 3. Thus, we design two ap-
proximate algorithms by employing undergener-
ating and overgenerating approaches (Finley and
Joachims, 2008).
First, we develop an undergenerating local
greedy search algorithm shown in Algorithm 2. In
the algorithm, there are two loops, inner and outer
loops. The outer loop terminates when no labels
change (steps 3-11). The inner loop enumerates
the whole label matrix and greedily determines
each label (step 7) by maximizing the Equation
(2). Since the whole algorithm terminates only if
1
Since the merged node is from context candidate set C,
enumerating its label is equivalent to enumerating subsets C
s
of the candidate set C
519
Algorithm 2 Greedy Inference Algorithm
1: Input: w, x, y
2: initialize solution: y? ? y
0
3: repeat
4: y
?
? y?
5: for i ? {1, . . . ,m} do
6: for j ? {1, . . . , n} do
7:
y?
?
ij
? argmax
y?
ij
w
T
?(x, y?)
+4(y, y?)
8: y?
ij
? y?
?
ij
9: end for
10: end for
11: until y? = y
?
12: y?
?
? y?
13: return y?
?
the label matrix does not change during the last
outer loop. This indicates that at least a local opti-
mal solution is obtained.
Second, an overgenerating method can be
designed by using linear programming relax-
ation (Finley and Joachims, 2008). To save the
space, we skip the details of this algorithm here.
5 Loss Functions
Structural SVMs allow users to customize the loss
function 4 : Y ? Y ? R according to different
system requirements. In this section, we introduce
the loss functions used in our work.
Basic loss function. The simplest way to quan-
tify the prediction quality is counting the number
of wrongly predicted labels. Formally,
4
b
(y, y?) =
m
?
i=1
n
?
j=1
I[y
ij
6= y?
ij
], (3)
where I[.] is an indicative function that equals to
one if the condition holds and zero otherwise.
Recall-vs-precision loss function. In practice,
we may place different emphasis on recall and pre-
cision according to application settings. We could
include this preference into the model by defining
the following loss function,
4
p
(y,
?
y) =
m
?
i=1
n
?
j=1
I[y
ij
6= P, y?
ij
= P ] ? c
r
+I[y
ij
= P, y?
ij
6= P ] ? c
p
. (4)
This function penalizes the wrong prediction de-
creasing recall and that decreasing precision with
Items in the data set #items
Thread 515
Post 2, 035
Sentence 8, 500
question annotation 1, 407
context annotation 1, 962
answer annotation 4, 652
plain annotation 18, 198
Table 2: The data statistics
two weights c
r
and c
p
respectively. Specifically,
we denote the loss function with c
p
/c
r
= 2 and
that with c
r
/c
p
= 2 by 4
p
p
and 4
r
p
, respectively.
Various types of loss function can be defined in
a similar fashion. To save the space, we skip the
definitions of other loss functions and only use the
above two types of loss functions to show the flex-
ibility of our approach.
6 Experiments
6.1 Experimental Setup
Corpus. We made use of the same data set as
introduced in (Cong et al, 2008; Ding et al,
2008). Specifically, the data set includes about
591 threads from the forum TripAdvisor
2
. Each
sentence in the threads is tagged with the labels
?question?, ?context?, ?answer?, or ?plain? by two
annotators. We removed 76 threads that have no
question sentences or more than 40 sentences and
6 questions. The remaining 515 forum threads
form our data set.
Table 2 gives the statistics on the data set. On
average, each thread contains 3.95 posts and 2.73
questions, and each question has 1.39 context sen-
tences and 3.31 answer sentences. Note that the
number of annotations is much larger than the
number of sentences because one sentence can be
annotated with multiple labels.
Experimental Details. In all the experiments,
we made use of linear models for the sake of com-
putational efficiency. As a preprocessing step, we
normalized the value of each feature value into
the interval [0, 1] and then followed the heuristic
used in SVM-light (Joachims, 1998) to set C to
1/||x||
2
, where ||x|| is the average length of input
samples (in our case, sentences). The tolerance pa-
rameter ? was set to 0.1 (the value also used in (Cai
2
TripAdvisor (http://www.tripadvisor.com/
ForumHome) is one of the most popular travel forums
520
and Hofmann, 2004)) in all the runs of the experi-
ments.
Evaluation. We calculated the standard preci-
sion (P), recall (R) and F
1
-score (F
1
) for both tasks
(context extraction and answer extraction). All the
experimental results were obtained through 5-fold
cross validation.
6.2 Baseline Methods
We employed binary SVMs (B-SVM), multiclass
SVMs (M-SVM), and C4.5 (Quinlan, 1993) as our
baseline methods:
B-SVM. We trained two binary SVMs for con-
text extraction (context vs. non-context) and an-
swer extraction (answer vs. non-answer), respec-
tively. We used the feature mapping ?
q
i
(x
j
) de-
fined in Equation (1) while training the binary
SVM models.
M-SVM. We extended the binary SVMs by
training multiclass SVMs for three category labels
(context, answer, plain).
C4.5. This decision tree algorithm solved the
same classification problem as binary SVMs and
made use of the same set of features.
6.3 Modeling Sentence Relations and
Question Interactions
We demonstrate in Table 3 that our approach can
make use of the three types of relation among sen-
tences well to boost the performance.
In Table 3, S-SVM represents the structural
SVMs only using the node features ?
n
(x,y). The
suffixes H, C, and V denote the models using
horizontal sequential edges, complete skip-chain
edges and vertical label groups, respectively. The
suffixes C* and V* denote the models using in-
complete skip-chain edges and vertical sequential
edges proposed in (Ding et al, 2008), as shown
in Figures 2(a) and 2(c). All the structural SVMs
were trained using basic loss function ?
b
in Equa-
tion (3). From Table 3, we can observe the follow-
ing advantages of our approaches.
Overall improvement. Our structural approach
steadily improves the extraction as more types of
relation (corresponding to more types of edge) are
included. The best results obtained by using the
three types of relation together improve the base-
line methods binary SVMs by about 6% and 20%
in terms of F
1
values for context extraction and
answer extraction, respectively.
The usefulness of relations. The relations
encoded by horizontal sequential edges and la-
Method 4
b
P (%) R (%) F
1
(%)
Context Extraction
C4.5 ? 74.2 68.7 71.2
B-SVM ? 78.3 72.2 74.9
M-SVM ? 68.0 77.6 72.1
S-SVM 8.86 75.6 71.7 73.4
S-SVM-H 8.60 77.5 75.5 76.3
S-SVM-HC* 8.65 77.9 74.1 75.8
S-SVM-HC 8.62 77.5 75.2 76.2
S-SVM-HCV* 8.08 79.5 79.6 79.5
S-SVM-HCV 7.98 79.7 80.2 79.9
Answer Extraction
C4.5 ? 61.3 45.2 51.8
B-SVM ? 69.7 42.0 51.8
M-SVM ? 63.2 51.5 55.8
S-SVM 8.86 67.0 48.0 55.6
S-SVM-H 8.60 66.9 49.7 56.7
S-SVM-HC* 8.65 66.5 49.4 56.4
S-SVM-HC 8.62 65.7 51.5 57.4
S-SVM-HCV* 8.08 65.5 58.7 61.7
S-SVM-HCV 7.98 65.1 61.2 63.0
Table 3: The effectiveness of our approach
bel groups are useful for both context extraction
and answer extraction. The relation encoded by
complete skip-chain edges is useful for answer
extraction. The complete skip-chain edges not
only avoid preprocessing but also boost the per-
formance when compared with the preprocessed
skip-chain edges. The label groups improve the
vertical sequential edges.
Interactions among questions. The interac-
tions encoded by label groups are especially use-
ful. We conducted significance tests (sign test) on
the experimental results. The test result shows that
S-SVM-HCV outperforms all the other methods
without vertical edges statistically significantly (p-
value < 0.01). Our proposed graphical represen-
tation in Figure 2(d) eases us to model the complex
interactions. In comparison, the 2D model in Fig-
ure 2(c) used in previous work (Ding et al, 2008)
can only model the interaction between adjacent
questions.
6.4 Loss Function Results
We report in Table 4 the comparison between
structural SVMs using different loss functions.
Note that ?
p
p
prefers precision and ?
r
p
prefers re-
call. From Table 4, we can observe that the ex-
perimental results also exhibit this kind of system
521
Method P (%) R (%) F
1
(%)
Context Extraction
S-SVM-HCV-4
b
79.7 80.2 79.9
S-SVM-HCV-4
p
p
82.0 70.3 75.6
S-SVM-HCV-4
r
p
75.7 84.2 79.7
Answer Extraction
S-SVM-HCV-4
b
65.1 61.2 63.0
S-SVM-HCV-4
p
p
71.8 52.2 60.2
S-SVM-HCV-4
r
p
61.8 66.1 63.7
Table 4: The use of different loss functions
preference. Moreover, we further demonstrate the
capability of the loss function ?
p
in Figure 4. The
curves are achieved by varying the ratio between
two parameters c
p
/c
r
in Equation (4). The curves
confirm our intuition: when log(c
p
/c
r
) becomes
larger, the precisions increase but the recalls de-
crease and vice versa.
7 Related work
Previous work on extracting questions, answers
and contexts is most related with our work. Cong
et al (2008) proposed a supervised approach for
question detection and an unsupervised approach
for answer detection without considering contexts.
Ding et al (2008) used CRFs to detect contexts
and answers of questions from forum threads.
Some researches on summarizing discussion
threads and emails are related to our work, too.
Zhou and Hovy (2005) segmented internet re-
lay chat, clustered segments into sub-topics, and
identified responding segments of the first seg-
ment in each sub-topic by assuming the first seg-
ment to be focus. In (Nenkova and Bagga, 2003;
Wan and McKeown, 2004; Rambow et al, 2004),
email summaries were organized by extracting
overview sentences as discussion issues. The
work (Shrestha and McKeown, 2004) used RIP-
PER as a classifier to detect interrogative questions
and their answers then used the resulting question
and answer pairs as summaries. We also note the
existing work on extracting knowledge from dis-
cussion threads. Huang et al (2007) used SVMs
to extract input-reply pairs from forums for chat-
bot knowledge. Feng et al (2006) implemented
a discussion-bot which used cosine similarity to
match students? query with reply posts from an an-
notated corpus of archived threaded discussions.
Moreover, extensive researches have been done
within the area of question answering (Burger et
?1.5 ?1 ?0.5 0 0.5 1 1.50.6
0.7
0.8
0.9
1
Log loss ratio
Pre
cisi
on
 
 ContextAnswer
?1.5 ?1 ?0.5 0 0.5 1 1.50.4
0.6
0.8
1
Log loss ratio
Rec
all
 
 ContextAnswer
Figure 4: Balancing between precision and recall
al., 2006; Jeon et al, 2005; Harabagiu and Hickl,
2006; Cui et al, 2005; Dang et al, 2006). They
mainly focused on using sophisticated linguistic
analysis to construct answer from a large docu-
ment collection.
8 Conclusion and Future Work
We have proposed a new form of graphical rep-
resentation for modeling the problem of extract-
ing contexts and answers of questions from online
forums and then customized structural SVM ap-
proach to solve it.
The proposed graphical representation is able
to naturally express three types of relation among
sentences: relation between successive sentences,
relation between context sentences and answer
sentences, and relation between multiple labels for
one sentence. The representation also enables us
to address interactions among questions. We also
developed the inference algorithms for the struc-
tural SVM model by exploiting the special struc-
ture of thread discussions.
Experimental results on a real data set show that
our approach significantly improves the baseline
methods by effectively utilizing various types of
relation among sentences.
Our future work includes: (a) to summa-
rize threads and represent the forum threads in
question-context-answer triple, which will change
the organization of online forums; and (b) to en-
hance QA services (e.g., Yahoo! Answers) by the
contents extracted from online forums.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for their comments to improve this paper.
522
References
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2006. Issues, tasks and program struc-
tures to roadmap research in question and answering
(qna). ARAD: Advanced Research and Development
Activity (US).
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal document categorization with support vector ma-
chines. In Proceedings of CIKM, pages 78?87.
Gao Cong, Long Wang, Chin-Yew Lin, and Young-In
Song. 2008. Finding question-answer pairs from
online forums. In Proceedings of SIGIR, pages 467?
474.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of SIGIR, pages 400?407.
Hoa Dang, Jimmy Lin, and Diane Kelly. 2006.
Overview of the trec 2006 question answering track.
In Proceedings of TREC, pages 99?116.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random field to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL, pages 710?718.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Proceedings of IUI, pages 171?177.
Thomas Finley and Thorsten Joachims. 2008. Training
structural SVMs when exact inference is intractable.
In Proceedings of ICML, pages 304?311.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
364?372.
Sanda M. Harabagiu and Andrew Hickl. 2006. Meth-
ods for using textual entailment in open-domain
question answering. In Proceedings of ACL, pages
905?912.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In Proceedings of IJCAI, pages 423?428.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of CIKM, pages 84?
90.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs.
Machine Learning.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of ECML, pages 137?
142.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ani Nenkova and Amit Bagga. 2003. Facilitating
email thread access by extractive summary genera-
tion. In Proceedings of RANLP, pages 287?296.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of sequence labeling algorithms and extensions. In
Proceedings of ICML, pages 681?688.
John Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publisher Incorpora-
tion.
Lawrence Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of IEEE, pages 257?286.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email
threads. In Proceedings of HLT-NAACL, pages 105?
108.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of COLING, pages 889?895.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. Technical Report 04-49.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Advances
in Neural Information Processing Systems 16. MIT
Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484.
Stephen Wan and Kathy McKeown. 2004. Generating
overview summaries of ongoing email thread discus-
sions. In Proceedings of COLING, pages 549?555.
Liang Zhou and Eduard Hovy. 2005. Digesting vir-
tual ?geek? culture: The summarization of technical
internet relay chats. In Proceedings of ACL, pages
298?305.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and
Wei-Ying Ma. 2005. 2d conditional random fields
for web information extraction. In Proceedings of
ICML, pages 1044?1051.
523
Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Searching Questions by Identifying Question Topic and Question Focus 
 
Huizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu1 
1Shanghai Jiao Tong University,  
Shanghai, China, 200240 
{summer, yyu}@apex.sjtu.edu.cn 
2Microsoft Research Asia,  
Beijing, China, 100080 
{yunbo.cao, cyl}@microsoft.com 
 
 
Abstract 
This paper is concerned with the problem of 
question search. In question search, given a 
question as query, we are to return questions 
semantically equivalent or close to the queried 
question. In this paper, we propose to conduct 
question search by identifying question topic 
and question focus. More specifically, we first 
summarize questions in a data structure con-
sisting of question topic and question focus. 
Then we model question topic and question 
focus in a language modeling framework for 
search. We also propose to use the MDL-
based tree cut model for identifying question 
topic and question focus automatically. Expe-
rimental results indicate that our approach of 
identifying question topic and question focus 
for search significantly outperforms the base-
line methods such as Vector Space Model 
(VSM) and Language Model for Information 
Retrieval (LMIR).  
1 Introduction 
Over the past few years, online services have been 
building up very large archives of questions and 
their answers, for example, traditional FAQ servic-
es and emerging community-based Q&A services 
(e.g., Yahoo! Answers1 , Live QnA2, and Baidu 
Zhidao3).   
To make use of the large archives of questions 
and their answers, it is critical to have functionality 
facilitating users to search previous answers. Typi-
cally, such functionality is achieved by first re-
trieving questions expected to have the same 
answers as a queried question and then returning 
the related answers to users. For example, given 
question Q1 in Table 1, question Q2 can be re-
                                                          
1 http://answers.yahoo.com 
2 http://qna.live.com 
3 http://zhidao.baidu.com 
turned and its answer will then be used to answer 
Q1 because the answer of Q2 is expected to par-
tially satisfy the queried question Q1. This is what 
we called question search. In question search, re-
turned questions are semantically equivalent or 
close to the queried question.  
 
Query: 
Q1: Any cool clubs in Berlin or Hamburg? 
Expected: 
Q2: What are the best/most fun clubs in Berlin? 
Not Expected: 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
Table 1. An Example on Question Search 
Many methods have been investigated for tack-
ling the problem of question search. For example, 
Jeon et al have compared the uses of four different 
retrieval methods, i.e. vector space model, Okapi, 
language model, and translation-based model, 
within the setting of question search (Jeon et al, 
2005b).  However, all the existing methods treat 
questions just as plain texts (without considering 
question structure). For example, obviously, Q2 
can be considered semantically closer to Q1 than 
Q3-Q5 although all questions (Q2-Q5) are related 
to Q1. The existing methods are not able to tell the 
difference between question Q2 and questions Q3, 
Q4, and Q5 in terms of their relevance to question 
Q1. We will clarify this in the following. 
In this paper, we propose to conduct question 
search by identifying question topic and question 
focus.  
The question topic usually represents the major 
context/constraint of a question (e.g., Berlin, Ham-
burg) which characterizes users? interests. In con-
trast, question focus (e.g., cool club, cheap hotel) 
presents certain aspect (or descriptive features) of 
the question topic. For the aim of retrieving seman-
tically equivalent (or close) questions, we need to 
156
assure that returned questions are related to the 
queried question with respect to both question top-
ic and question focus. For example, in Table 1, Q2 
preserves certain useful information of Q1 in the 
aspects of both question topic (Berlin) and ques-
tion focus (fun club) although it loses some useful 
information in question topic (Hamburg). In con-
trast, questions Q3-Q5 are not related to Q1 in 
question focus (although being related in question 
topic, e.g. Hamburg, Berlin), which makes them 
unsuitable as the results of question search.  
We also propose to use the MDL-based (Mini-
mum Description Length) tree cut model for auto-
matically identifying question topic and question 
focus. Given a question as query, a structure called 
question tree is constructed over the question col-
lection including the queried question and all the 
related questions, and then the MDL principle is 
applied to find a cut of the question tree specifying 
the question topic and the question focus of each 
question. 
In a summary, we summarize questions in a data 
structure consisting of question topic and question 
focus. On the basis of this, we then propose to 
model question topic and question focus in a lan-
guage modeling framework for search. To the best 
of our knowledge, none of the existing studies ad-
dressed question search by modeling both question 
topic and question focus. 
We empirically conduct the question search with 
questions about ?travel? and ?computers & internet?. 
Both kinds of questions are from Yahoo! Answers. 
Experimental results show that our approach can 
significantly improve traditional methods (e.g. 
VSM, LMIR) in retrieving relevant questions.  
The rest of the paper is organized as follow. In 
Section 2, we present our approach to question 
search which is based on identifying question topic 
and question focus. In Section 3, we empirically 
verify the effectiveness of our approach to question 
search. In Section 4, we employ a translation-based 
retrieval framework for extending our approach to 
fix the issue called ?lexical chasm?. Section 5 sur-
veys the related work. Section 6 concludes the pa-
per by summarizing our work and discussing the 
future directions.  
2 Our Approach to Question Search 
Our approach to question search consists of two 
steps: (a) summarize questions in a data structure 
consisting of question topic and question focus; (b) 
model question topic and question focus in a lan-
guage modeling framework for search. 
In the step (a), we employ the MDL-based (Min-
imum Description Length) tree cut model for au-
tomatically identifying question topic and question 
focus. Thus, this section will begin with a brief 
review of the MDL-based tree cut model and then 
follow that by an explanation of steps (a) and (b). 
2.1 The MDL-based tree cut model 
Formally, a tree cut model ? (Li and Abe, 1998) 
can be represented by a pair consisting of a tree cut 
?, and a probability parameter vector ? of the same 
length, that is, 
? ? ??, ??  (1) 
where ? and ? are 
? ? ???, ??, . . ???,  
? ? ??????, ?????, ? , ??????  
(2) 
where ??, ??, ????are classes determined by a cut 
in the tree and ? ????? ? 1
?
??? . A ?cut? in a tree is 
any set of nodes in the tree that defines a partition 
of all the nodes, viewing each node as representing 
the set of child nodes as well as itself. For example, 
the cut indicated by the dash line in Figure 1 cor-
responds to three classes:???, ????,????, ????, and 
????, ???, ???, ????. 
Figure 1. An Example on the Tree Cut Model 
A straightforward way for determining a cut of a 
tree is to collapse the nodes of less frequency into 
their parent nodes. However, the method is too 
heuristic for it relies much on manually tuned fre-
quency threshold. In our practice, we turn to use a 
theoretically well-motivated method based on the 
MDL principle. MDL is a principle of data com-
pression and statistical estimation from informa-
tion theory (Rissanen, 1978). 
Given a sample ? and a tree cut ?, we employ 
MLE to estimate the parameters of the correspond-
ing tree cut model ?? ? ??, ??? , where ??  denotes 
the estimated parameters.  
According to the MDL principle, the description 
length (Li and Abe, 1998)  ????, ?? of the tree cut 
model ??  and the sample ?? is the sum of the model 
?? 
??? ??? ??? 
??? ??? ??? ??? 
157
description length ????, the parameter description 
length ????|?? , and the data description length 
???|?, ???, i.e. 
????, ?? ? ???? ? ??????? ? ???|?, ???  (3) 
The model description length ???? is a subjec-
tive quantity which depends on the coding scheme 
employed. Here, we simply assume that each tree 
cut model is equally likely a priori. 
The parameter description length ????|?? is cal-
culated as  
??????? ? ?
?
? log |?|  (4) 
where |?|  denotes the sample size and ?  denotes 
the number of free parameters in the tree cut model, 
i.e. ? equals the number of nodes in ? minus one. 
The data description length ???|?, ???  is calcu-
lated as 
?????, ??? ? ?? ???????????   (5) 
where 
 ????? ? ?
|?|
? ????
|?|
 (6) 
where ?  is the class that ?  belongs to and ???? 
denotes the total frequency of instances in class ? 
in the sample ?. 
With the description length defined as (3), we 
wish to select a tree cut model with the minimum 
description length and output it as the result. Note 
that the model description length ???? can be ig-
nored because it is the same for all tree cut models. 
The MDL-based tree cut model was originally 
introduced for handling the problem of generaliz-
ing case frames using a thesaurus (Li and Abe, 
1998). To the best of our knowledge, no existing 
work utilizes it for question search. This may be 
partially because of the unavailability of the re-
sources (e.g., thesaurus) which can be used for 
embodying the questions in a tree structure. In Sec-
tion 2.2, we will introduce a tree structure called 
question tree for representing questions. 
2.2 Identifying question topic and question 
focus 
In principle, it is possible to identify question topic 
and question focus of a question by only parsing 
the question itself (for example, utilizing a syntac-
tic parser). However, such a method requires accu-
rate parsing results which cannot be obtained from 
the noisy data from online services. 
Instead, we propose using the MDL-based tree 
cut model which identifies question topics and 
question foci for a set of questions together. More 
specifically, the method consists of two phases: 
1) Constructing a question tree: represent the 
queried question and all the related questions 
in a tree structure called question tree; 
2) Determining a tree cut: apply the MDL prin-
ciple to the question tree, which yields the cut 
specifying question topic and question focus.  
2.2.1 Constructing a question tree 
In the following, with a series of definitions, we 
will describe how a question tree is constructed 
from a collection of questions. 
Let?s begin with explaining the representation of 
a question. A straightforward method is to 
represent a question as a bag-of-words (possibly 
ignoring stop words). However, this method cannot 
discern ?the hotels in Paris? from ?the Paris hotel?. 
Thus, we turn to use the linguistic units carrying on 
more semantic information. Specifically, we make 
use of two kinds of units: BaseNP (Base Noun 
Phrase) and WH-ngram. A BaseNP is defined as a 
simple and non-recursive noun phrase (Cao and Li, 
2002). A WH-ngram is an ngram beginning with 
WH-words. The WH-words that we consider in-
clude ?when?, ?what?, ?where?, ?which?, and ?how?.  
We refer to these two kinds of units as ?topic 
terms?. With ?topic terms?, we represent a question 
as a topic chain and a set of questions as a question 
tree.  
Definition 1 (Topic Profile) The topic profile 
?? of a topic term ? in a categorized question col-
lection is a probability distribution of categories 
????|??????  where ? is a set of categories.  
???|?? ? ???????,??
? ???????,?????
  (7) 
where ???????, ??  is the frequency of the topic 
term ?  within category ? . Clearly, we 
have?? ???|????? ? 1.  
By ?categorized questions?, we refer to the ques-
tions that are organized in a tree of taxonomy. For 
example, at Yahoo! Answers, the question ?How 
do I install my wireless router? is categorized as 
?Computers & Internet ? Computer Networking?. 
Actually, we can find categorized questions at oth-
er online services such as FAQ sites, too. 
Definition 2 (Specificity) The specificity ?????of 
a topic term ?? is the inverse of the entropy of the 
topic profile???. More specifically, 
???? ? 1 ??? ???|?? log ???|????? ? ???
  (8) 
158
where ?  is a smoothing parameter used to cope 
with the topic terms whose entropy is 0. In our ex-
periments, the value of ? was set 0.001. 
We use the term specificity to denote how spe-
cific a topic term is in characterizing information 
needs of users who post questions. A topic term of 
high specificity (e.g., Hamburg, Berlin) usually 
specifies the question topic corresponding to the 
main context of a question because it tends to oc-
cur only in a few categories. A topic term of low 
specificity is usually used to represent the question 
focus (e.g., cool club, where to see) which is rela-
tively volatile and might occur in many categories. 
Definition 3 (Topic Chain) A topic chain ?? of 
a question ? is a sequence of ordered topic terms 
?? ? ?? ? ? ? ?? such that  
1) ?? is included in 1  ,? ? ? ? ?;  
2) ????? ? ?????,  1 ? ? ? ? ? ?.  
For example, the topic chain of ?any cool clubs 
in Berlin or Hamburg?? is ?Hamburg ? Berlin ?
cool?club? because the specificities for ?Hamburg?, 
?Berlin?, and ?cool club? are 0.99, 0.62, and 0.36. 
Definition 4 (Question Tree) A question tree of 
a question set ? ? ???????
?  is a prefix tree built 
over the topic chains ?? ? ???
?????
?  of the question 
set ?. Clearly, if a question set contains only one 
question, its question tree will be exactly same as 
the topic chain of the question. 
Note that the root node of a question tree is as-
sociated with empty string as the definition of pre-
fix tree requires (Fredkin, 1960). 
 
Figure 2. An Example of a Question Tree 
 
Given the topic chains with respect to the ques-
tions in Table 1 as follow, 
? Q1: Hamburg ? Berlin ? cool?club?
? Q2: Berlin ? fun?club?
? Q3: Hamburg ? Berlin ? nice?hotel?
? Q4: Hamburg ? Berlin ? how?long?does?it?take?
? Q5: Berlin ? cheap?hotel?
we can have the question tree presented in Figure 2.  
2.2.2 Determining the tree cut 
According to the definition of a topic chain, the 
topic terms in a topic chain of a question are or-
dered by their specificity values. Thus, a cut of a 
topic chain naturally separates the topic terms of 
low specificity (representing question focus) from 
the topic terms of high specificity (representing 
question topic). Given a topic chain of a question 
consisting of ?  topic terms, there exist (? ? 1? 
possible cuts. The question is: which cut is the best?  
We propose using the MDL-based tree cut mod-
el for the search of the best cut in a topic chain. 
Instead of dealing with each topic chain individual-
ly, the proposed method handles a set of questions 
together. Specifically, given a queried question, we 
construct a question tree consisting of both the 
queried question and the related questions, and 
then apply the MDL principle to select the best cut 
of the question tree. For example, in Figure 2, we 
hope to get the cut indicated by the dashed line. 
The topic terms on the left of the dashed line 
represent the question topic and those on the right 
of the dashed line represent the question focus. 
Note that the tree cut yields a cut for each individ-
ual topic chain (each path) within the question tree 
accordingly.  
A cut of a topic chain ??? of a question q sepa-
rates the topic chain in two parts: HEAD and TAIL. 
HEAD (denoted as ?????) is the subsequence of 
the original topic chain ???  before the cut. TAIL 
(denoted as ?????) is the subsequence of ??? after 
the cut. Thus,??? ? ????? ? ?????. For instance, 
given the tree cut specified in Figure 2, for the top-
ic chain of Q1 ?Hamburg ? Berlin ? cool?club?, 
the HEAD and TAIL are ?Hamburg ? Berlin? 
and ?cool?club? respectively. 
2.3 Modeling question topic and question fo-
cus for search 
We employ the framework of language modeling 
(for information retrieval) to develop our approach 
to question search. 
In the language modeling approach to informa-
tion retrieval, the relevance of a targeted question 
?? to a queried question ? is given by the probabili-
ty ???|???  of generating the queried question ? 
Q1: Any cool clubs in Berlin or Hamburg? 
Q2: What are the most/best fun clubs in Berlin? 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
ROOT 
Hamburg 
Berlin 
Berlin 
cheap hotel 
fun club 
cool club
nice hotel
how long does it take
159
from the language model formed by the targeted 
question ??.  The targeted question ?? is from a col-
lection ? of questions. 
Following the framework, we propose a mixture 
model for modeling question structure (namely, 
question topic and question focus) within the 
process of searching questions: 
???|??? ? ? ? ??????|??????
????????1 ? ??? ? ??????|??????
 (9) 
In the mixture model, it is assumed that the 
process of generating question topics and the 
process of generating question foci are independent 
from each other.  
In traditional language modeling, a single multi-
nomial model ???|??? over terms is estimated for 
each targeted question ?? . In our case, two multi-
nomial models ??????????  and ??????????  need to 
be estimated for each targeted question ??. 
If unigram document language models are used, 
the equation (9) can then be re-written as, 
???|??? ? ? ? ? ??????????
???????,??
?????? ?
?1 ? ??? ? ? ??????????
???????,??
??????   
(10)
where ???????, ?? is the frequency of ? within ?. 
To avoid zero probabilities and estimate more 
accurate language models, the HEAD and TAIL of 
questions are smoothed using background collec-
tion, 
?????????? ? ? ? ???????????  
?????????????????????????1 ? ?? ? ????|??  
 
(11)
?????????? ? ? ? ???????????  
??????????????????????????1 ? ?? ? ????|??  
 
(12)
where ????|?????? , ????|?????? , and ????|??  are the 
MLE  estimators with respect to the HEAD of ??, 
the TAIL of ??, and the collection ?.  
3 Experimental Results  
We have conducted experiments to verify the ef-
fectiveness of our approach to question search. 
Particularly, we have investigated the use of identi-
fying question topic and question focus for search. 
3.1 Dataset and evaluation measures 
We made use of the questions obtained from Ya-
hoo! Answers for the evaluation. More specifically, 
we utilized the resolved questions under two of the 
top-level categories at Yahoo! Answers, namely 
?travel? and ?computers & internet?. The questions 
include 314,616 items from the ?travel? category 
and 210,785 items from the ?computers & internet? 
category. Each resolved question consists of three 
fields: ?title?, ?description?, and ?answers?. For 
search we use only the ?title? field. It is assumed 
that the titles of the questions already provide 
enough semantic information for understanding 
users? information needs. 
We developed two test sets, one for the category 
?travel? denoted as ?TRL-TST?, and the other for 
?computers & internet? denoted as ?CI-TST?. In 
order to create the test sets, we randomly selected 
200 questions for each category.  
To obtain the ground-truth of question search, 
we employed the Vector Space Model (VSM) (Sal-
ton et al, 1975) to retrieve the top 20 results and 
obtained manual judgments. The top 20 results 
don?t include the queried question itself. Given a 
returned result by VSM, an assessor is asked to 
label it with ?relevant? or ?irrelevant?. If a returned 
result is considered semantically equivalent (or 
close) to the queried question, the assessor will 
label it as ?relevant?; otherwise, the assessor will 
label it as ?irrelevant?. Two assessors were in-
volved in the manual judgments. Each of them was 
asked to label 100 questions from ?TRL-TST? and 
100 from ?CI-TST?. In the process of manually 
judging questions, the assessors were presented 
only the titles of the questions (for both the queried 
questions and the returned questions). Table 2 pro-
vides the statistics on the final test set. 
 
 # Queries # Returned # Relevant
TRL-TST 200 4,000 256 
CI-TST 200 4,000 510 
Table 2. Statistics on the Test Data 
 
We utilized two baseline methods for demon-
strating the effectiveness of our approach, the 
VSM and the LMIR (language modeling method 
for information retrieval) (Ponte and Croft, 1998).  
We made use of three measures for evaluating 
the results of question search methods. They are 
MAP, R-precision, and MRR.  
3.2 Searching questions about ?travel? 
In the experiments, we made use of the questions 
about ?travel? to test the performance of our ap-
proach to question search. More specifically, we 
used the 200 queries in the test set ?TRL-TST? to 
search for ?relevant? questions from the 314,616 
160
questions categorized as ?travel?. Note that only the 
questions occurring in the test set can be evaluated. 
We made use of the taxonomy of questions pro-
vided at Yahoo! Answers for the calculation of 
specificity of topic terms. The taxonomy is orga-
nized in a tree structure. In the following experi-
ments, we only utilized as the categories of 
questions the leaf nodes of the taxonomy tree (re-
garding ?travel?), which includes 355 categories. 
We randomly divided the test queries into five 
even subsets and conducted 5-fold cross-validation 
experiments. In each trial, we tuned the parameters 
?, ?, and ? in the equation (10)-(12) with four of 
the five subsets and then applied it to one remain-
ing subset. The experimental results reported be-
low are those averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.198 0.138 0.228 
LMIR 0.203 0.154 0.248 
LMIR-CUT 0.236 0.192 0.279 
Table 3. Searching Questions about ?Travel? 
 
In Table 3, our approach denoted by LMIR-
CUT is implemented exactly as equation (10).  
Neither VSM nor LMIR uses the data structure 
composed of question topic and question focus.  
From Table 3, we see that our approach outper-
forms the baseline approaches VSM and LMIR in 
terms of all the measures. We conducted a signi-
ficance test (t-test) on the improvements of our 
approach over VSM and LMIR. The result indi-
cates that the improvements are statistically signif-
icant (p-value < 0.05) in terms of all the evaluation 
measures.  
 
 
Figure 3. Balancing between Question Topic and Ques-
tion Focus 
 
In equation (9), we use the parameter ? to bal-
ance the contribution of question topic and the con-
tribution of question focus. Figure 3 illustrates how 
influential the value of ? is on the performance of 
question search in terms of MRR. The result was 
obtained with the 200 queries directly, instead of 
5-fold cross-validation. From Figure 3, we see that 
our approach performs best when ? is around 0.7. 
That is, our approach tends to emphasize question 
topic more than question focus.  
We also examined the correctness of question 
topics and question foci of the 200 queried ques-
tions. The question topics and question foci were 
obtained with the MDL-based tree cut model au-
tomatically. In the result, 69 questions have incor-
rect question topics or question foci. Further 
analysis shows that the errors came from two cate-
gories: (a) 59 questions have only the HEAD parts 
(that is, none of the topic terms fall within the 
TAIL part), and (b) 10 have incorrect orders of 
topic terms because the specificities of topic terms 
were estimated inaccurately. For questions only 
having the HEAD parts, our approach (equation (9)) 
reduces to traditional language modeling approach.  
Thus, even when the errors of category (a) occur, 
our approach can still work not worse than the tra-
ditional language modeling approach. This also 
explains why our approach performs best when ? is 
around 0.7. The error category (a) pushes our mod-
el to emphasize more in question topic. 
 
Methods Results 
VSM 
1. How cold does it usually get in Charlotte, 
NC during winters? 
2. How long and cold are the winters in 
Rochester, NY? 
3. How cold is it in Alaska? 
LMIR 
1. How cold is it in Alaska? 
2. How cold does it get really in Toronto in 
the winter? 
3. How cold does the Mojave Desert get in 
the winter? 
LMIR-
CUT 
1. How cold is it in Alaska? 
2. How cold is Alaska in March and out-
door activities? 
3. How cold does it get in Nova Scotia in the 
winter? 
Table 4. Search Results for 
?How cold does it get in winters in Alaska?? 
 
Table 4 provides the TOP-3 search results which 
are given by VSM, LMIR, and LMIR-CUT (our 
approach) respectively. The questions in bold are 
labeled as ?relevant? in the evaluation set. The que-
ried question seeks for the ?weather? information 
about ?Alaska?. Both VSM and LMIR rank certain 
0.05
0.1
0.15
0.2
0.25
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
161
?irrelevant? questions higher than ?relevant? ques-
tions. The ?irrelevant? questions are not about 
?Alaska? although they are about ?weather?. The 
reason is that neither VSM nor PVSM is aware that 
the query consists of the two aspects ?weather? 
(how cold, winter) and ?Alaska?.  In contrast, our 
approach assures that both aspects are matched. 
Note that the HEAD part of the topic chain of the 
queried question given by our approach is ?Alaska? 
and the TAIL part is ?winter ? how?cold?. 
3.3 Searching questions about ?computers & 
internet? 
In the experiments, we made use of the questions 
about ?computers & internet? to test the perfor-
mance of our proposed approach to question search. 
More specifically, we used the 200 queries in the 
test set ?CI-TST?? to search for ?relevant? questions 
from the 210,785 questions categorized as ?com-
puters & internet?. For the calculation of specificity 
of topic terms, we utilized as the categories of 
questions the leaf nodes of the taxonomy tree re-
garding ?computers & Internet?, which include 23 
categories.  
We conducted 5-fold cross-validation for the pa-
rameter tuning. The experimental results reported 
in Table 5 are averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.236 0.175 0.289 
LMIR 0.248 0.191 0.304 
LMIR-CUT 0.279 0.230 0.341 
Table 5. Searching Questions about ?Computers & In-
ternet? 
 
Again, we see that our approach outperforms the 
baseline approaches VSM and LMIR in terms of 
all the measures. We conducted a significance test 
(t-test) on the improvements of our approach over 
VSM and LMIR. The result indicates that the im-
provements are statistically significant (p-value < 
0.05) in terms of all the evaluation measures.  
We also conducted the experiment similar to 
that in Figure 3. Figure 4 provides the result. The 
trend is consistent with that in Figure 3.  
We examined the correctness of (automatically 
identified) question topics and question foci of the 
200 queried questions, too. In the result, 65 ques-
tions have incorrect question topics or question 
foci. Among them, 47 fall in the error category (a) 
and 18 in the error category (b). The distribution of 
errors is also similar to that in Section 3.2, which 
also justifies the trend presented in Figure 4. 
 
 
Figure 4. Balancing between Question Topic and Ques-
tion Focus 
4 Using Translation Probability 
In the setting of question search, besides the topic 
what we address in the previous sections, another 
research topic is to fix lexical chasm between ques-
tions.  
Sometimes, two questions that have the same 
meaning use very different wording. For example, 
the questions ?where to stay in Hamburg?? and 
?the best hotel in Hamburg?? have almost the same 
meaning but are lexically different in question fo-
cus (where to stay vs. best hotel). This is the so-
called ?lexical chasm?. 
Jeon and Bruce (2007) proposed a mixture mod-
el for fixing the lexical chasm between questions. 
The model is a combination of the language mod-
eling approach (for information retrieval) and 
translation-based approach (for information re-
trieval). Our idea of modeling question structure 
for search can naturally extend to Jeon et al?s 
model. More specifically, by using translation 
probabilities, we can rewrite equation (11) and (12) 
as follow: 
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????  
??1 ? ?? ? ??? ? ????|??  
(13)
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????   
??1 ? ?? ? ??? ? ????|??  
 
(14)
where ????|???  denotes the probability that topic 
term ? is the translation of ??. In our experiments, 
to estimate the probability ????|???, we used the 
collections of question titles and question descrip-
tions as the parallel corpus and the IBM model 1 
(Brown et al, 1993) as the alignment model. 
0.15
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
162
Usually, users reiterate or paraphrase their ques-
tions (already described in question titles) in ques-
tion descriptions. 
We utilized the new model elaborated by equa-
tion (13) and (14) for searching questions about 
?travel? and ?computers & internet?. The new mod-
el is denoted as ?SMT-CUT?. Table 6 provides the 
evaluation results. The evaluation was conducted 
with exactly the same setting as in Section 3. From 
Table 6, we see that the performance of our ap-
proach can be further boosted by using translation 
probability.  
 
Data Methods MAP R-Precision MRR
TRL-
TST 
LMIR-CUT 0.236 0.192 0.279
SMT-CUT 0.266 0.225 0.308
CI-
TST 
LMIR-CUT 0.279 0.230 0.341
SMT-CUT 0.282 0.236 0.337
Table 6. Using Translation Probability 
5 Related Work 
The major focus of previous research efforts on 
question search is to tackle the lexical chasm prob-
lem between questions.  
The research of question search is first con-
ducted using FAQ data. FAQ Finder (Burke et al, 
1997) heuristically combines statistical similarities 
and semantic similarities between questions to rank 
FAQs. Conventional vector space models are used 
to calculate the statistical similarity and WordNet 
(Fellbaum, 1998) is used to estimate the semantic 
similarity. Sneiders (2002) proposed template 
based FAQ retrieval systems. Lai et al (2002) pro-
posed an approach to automatically mine FAQs 
from the Web. Jijkoun and Rijke (2005) used su-
pervised learning methods to extend heuristic ex-
traction of Q/A pairs from FAQ pages, and treated 
Q/A pair retrieval as a fielded search task.  
Harabagiu et al (2005) used a Question Answer 
Database (known as QUAB) to support interactive 
question answering. They compared seven differ-
ent similarity metrics for selecting related ques-
tions from QUAB and found that the concept-
based metric performed best. 
Recently, the research of question search has 
been further extended to the community-based 
Q&A data. For example, Jeon et al (Jeon et al, 
2005a; Jeon et al, 2005b) compared four different 
retrieval methods, i.e. vector space model, Okapi, 
language model (LM), and translation-based model, 
for automatically fixing the lexical chasm between 
questions of question search. They found that the 
translation-based model performed best. 
However, all the existing methods treat ques-
tions just as plain texts (without considering ques-
tion structure). In this paper, we proposed to 
conduct question search by identifying question 
topic and question focus. To the best of our know-
ledge, none of the existing studies addressed ques-
tion search by modeling both question topic and 
question focus. 
Question answering (e.g., Pasca and Harabagiu, 
2001; Echihabi and Marcu, 2003; Voorhees, 2004; 
Metzler and Croft, 2005) relates to question search. 
Question answering automatically extracts short 
answers for a relatively limited class of question 
types from document collections. In contrast to that, 
question search retrieves answers for an unlimited 
range of questions by focusing on finding semanti-
cally similar questions in an archive. 
6 Conclusions and Future Work 
In this paper, we have proposed an approach to 
question search which models question topic and 
question focus in a language modeling framework. 
The contribution of this paper can be summa-
rized in 4-fold: (1) A data structure consisting of 
question topic and question focus was proposed for 
summarizing questions; (2) The MDL-based tree 
cut model was employed to identify question topic 
and question focus automatically; (3) A new form 
of language modeling using question topic and 
question focus was developed for question search; 
(4) Extensive experiments have been conducted to 
evaluate the proposed approach using a large col-
lection of real questions obtained from Yahoo! An-
swers.  
Though we only utilize data from community-
based question answering service in our experi-
ments, we could also use categorized questions 
from forum sites and FAQ sites. Thus, as future 
work, we will try to investigate the use of the pro-
posed approach for other kinds of web services.  
Acknowledgement 
We would like to thank Xinying Song, Shasha Li, 
and Shilin Ding for their efforts on developing the 
evaluation data. We would also like to thank Ste-
phan H. Stiller for his proof-reading of the paper. 
 
163
References  
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proc. of ACL?03. 
C. Fellbaum. 1998. WordNet: An electronic lexical da-
tabase. MIT Press. 
D. Metzler and W. B. Croft. 2005. Analysis of statistical 
question classification for fact-based questions. In-
formation Retrieval, 8(3), pages 481-504. 
E. Fredkin. 1960. Trie memory. Communications of the 
ACM, D. 3(9):490-499. 
E. M. Voorhees. 2004. Overview of the TREC 2004 
question answering track. In Proc. of TREC?04. 
E. Sneiders. 2002. Automated question answering using 
question templates that cover the conceptual model 
of the database. In Proc. of the 6th International 
Conference on Applications of Natural Language to 
Information Systems, pages 235-239. 
G. Salton, A. Wong, and C. S. Yang 1975. A vector 
space model for automatic indexing. Communica-
tions of the ACM, vol. 18, nr. 11, pages 613-620.  
H.  Li and N. Abe. 1998. Generalizing case frames us-
ing a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2), pages 217-244. 
J. Jeon and W.B. Croft. 2007. Learning translation-
based language models using Q&A archives. Tech-
nical report, University of Massachusetts. 
J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding seman-
tically similar questions based on their answers. In 
Proc. of SIGIR?05. 
J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar 
questions in large question and answer archives. In 
Proc. of CIKM ?05, pages 84-90. 
J. Rissanen. 1978. Modeling by shortest data description. 
Automatica, vol. 14,  pages. 465-471 
J.M. Ponte, W.B. Croft. 1998. A language modeling 
approach to information retrieval. In Proc. of 
SIGIR?98. 
M. A. Pasca and S. M. Harabagiu. 2001. High perfor-
mance question/answering. In Proc. of SIGIR?01, 
pages 366-374. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2):263-311. 
R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. 
Lytinen, N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question files: 
Experiences with the FAQ finder system. Technical 
report, University of Chicago. 
S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan. 
2005. Experiments with Interactive Question-
Answering. In Proc. of ACL?05. 
V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Y. Cao and H. Li. 2002. Base noun phrase translation 
using web data and the EM algorithm. In Proc. of 
COLING?02. 
Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining 
via list detection. In Proc. of the Workshop on Multi-
lingual Summarization and Question Answering, 
2002. 
 
164
Proceedings of ACL-08: HLT, pages 914?922,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Probabilistic Model for Fine-Grained Expert Search   Shenghua Bao1, Huizhong Duan1, Qi Zhou1, Miao Xiong1, Yunbo Cao1,2, Yong Yu1 1Shanghai Jiao Tong University,  2Microsoft Research Asia Shanghai, China, 200240 Beijing, China, 100080 {shhbao,summer,jackson,xiongmiao,yyu} @apex.sjtu.edu.cn yunbo.cao@microsoft.com    Abstract 
Expert search, in which given a query a ranked list of experts instead of documents is returned, has been intensively studied recently due to its importance in facilitating the needs of both information access and knowledge discovery. Many approaches have been pro-posed, including metadata extraction, expert profile building, and formal model generation. However, all of them conduct expert search with a coarse-grained approach. With these, further improvements on expert search are hard to achieve. In this paper, we propose conducting expert search with a fine-grained approach. Specifically, we utilize more spe-cific evidences existing in the documents. An evidence-oriented probabilistic model for ex-pert search and a method for the implementa-tion are proposed. Experimental results show that the proposed model and the implementa-tion are highly effective. 
1 Introduction Nowadays, team work plays a more important role than ever in problem solving. For instance, within an enterprise, people handle new problems usually by leveraging the knowledge of experienced col-leagues. Similarly, within research communities, novices step into a new research area often by learning from well-established researchers in the research area. All these scenarios involve asking the questions like ?who is an expert on X?? or ?who knows about X?? Such questions, which cannot be answered easily through traditional document search, raise a new requirement of searching people with certain expertise.  To meet that requirement, a new task, called ex-pert search, has been proposed and studied inten-sively. For example, TREC 2005, 2006, and 2007 
provide the task of expert search within the enter-prise track. In the TREC setting, expert search is defined as: given a query, a ranked list of experts is returned. In this paper, we engage our study in the same setting.  Many approaches to expert search have been proposed by the participants of TREC and other researchers. These approaches include metadata extraction (Cao et al, 2005), expert profile build-ing (Craswell, 2001, Fu et al, 2007), data fusion (Maconald and Ounis, 2006), query expansion (Macdonald and Ounis, 2007), hierarchical lan-guage model (Petkova and Croft, 2006), and for-mal model generation (Balog et al, 2006; Fang et al, 2006). However, all of them conduct expert search with what we call a coarse-grained ap-proach. The discovering and use of evidence for expert locating is carried out under a grain of document. With it, further improvements on expert search are hard to achieve. This is because differ-ent blocks (or segments) of electronic documents usually present different functions and qualities and thus different impacts for expert locating.  In contrast, this paper is concerned with propos-ing a probabilistic model for fine-grained expert search. In fine-grained expert search, we are to extract and use evidence of expert search (usually blocks of documents) directly. Thus, the proposed probabilistic model incorporates evidence of expert search explicitly as a part of it. A piece of fine-grained evidence is formally defined as a quadru-ple, <topic, person, relation, document>, which denotes the fact that a topic and a person, with a certain relation between them, are found in a spe-cific document. The intuition behind the quadruple is that a query may be matched with phrases in various forms (denoted as topic here) and an expert candidate may appear with various name masks (denoted as person here), e.g., full name, email, or abbreviated names. Given a topic and person, rela-tion type is used to measure their closeness and 
914
document serves as a context indicating whether it is good evidence. Our proposed model for fine-grained expert search results in an implementation of two stages.  1) Evidence Extraction: document segments in various granularities are identified and evidences are extracted from them. For example, we can have segments in which an expert candidate and a que-ried topic co-occur within a same section of docu-ment-001:  ??later, Berners-Lee describes a semantic web search engine experience?? As the result, we can extract an evidence by using same-section relation, i.e., <semantic web search engine, Berners-Lee, same-section, document-001>.   2) Evidence Quality Evaluation: the quality (or reliability) of evidence is evaluated. The quality of a quadruple of evidence consists of four aspects, namely topic-matching quality, person-name-matching quality, relation quality, and document quality. If we regard evidence as link of expert candidate and queried topic, the four aspects will correspond to the strength of the link to query, the strength of the link to expert candidate, the type of the link, and the document context of the link re-spectively. All the evidences with their scores of quality are merged together to generate a single score for each expert candidate with regard to a given query. We empirically evaluate our proposed model and im-plementation on the W3C corpus which is used in the expert search task at TREC 2005 and 2006. Experimental results show that both explored evi-dences and evaluation of evidence quality can im-prove the expert search significantly. Compared with existing state-of-the-art expert search methods, the probabilistic model for fine-grained expert search shows promising improvement.  The rest of the paper is organized as follows. Section 2 surveys existing studies on expert search. Section 3 and Section 4 present the proposed prob-abilistic model and its implementation, respec-tively. Section 5 gives the empirical evaluation. Finally, Section 6 concludes the work. 2 Related Work  
2.1 Expert Search Systems One setting for automatic expert search is to as-sume that data from specific resources are avail-able. For example, Expertise Recommender (Kautz 
et al, 1996), Expertise Browser (Mockus and Herbsleb, 2002) and the system in (McDonald and Ackerman, 1998) make use of log data in software development systems to find experts. Yet another approach is to mine expert and expertise from email communications (Campbell et al, 2003; Dom et al 2003; Sihn and Heeren, 2001). Searching expert from general documents has also been studied (Davenport and Prusak, 1998; Mattox et al, 1999; Hertzum and Pejtersen, 2000). P@NOPTIC employs what is referred to as the ?profile-based? approach in searching for experts (Craswell et al, 2001). Expert/Expert-Locating (EEL) system (Steer and Lochbaum, 1988) uses the same approach in searching for expert groups. DEMOIR (Yimam, 1996) enhances the profile-based approach by separating co-occurrences into different types. In essence, the profile-based ap-proach utilizes the co-occurrences between query words and people within documents. 2.2 Expert Search at TREC A task on expert search was organized within the enterprise track at TREC 2005, 2006 and 2007 (Craswell et al, 2005; Soboroff  et al, 2006; Bai-ley et al, 2007).  Many approaches have been proposed for tack-ling the expert search task within the TREC track. Cao et al (2005) propose a two-stage model with a set of extracted metadata. Balog et al (2006) com-pare two generative models for expert search. Fang et al (2006) further extend their generative model by introducing the prior of expert distribution and relevance feedback. Petkova and Croft (2006) fur-ther extend the profile based method by using a hierarchical language model. Macdonald and Ounis (2006) investigate the effectiveness of the voting approach and the associated data fusion techniques. However, such models are conducted in a coarse-grain scope of document as discussed before. In contrast, our study focuses on proposing a model for conducting expert search in a fine-grain scope of evidence (local context). 3 Fine-grained Expert Search Our research is to investigate a direct use of the local contexts for expert search. We call each local context of such kind as fine-grained evidence.  In this work, a fine-grained evidence is formally defined as a quadruple, <topic, person, relation, 
915
document>. Such a quadruple denotes that a topic and a person occurrence, with a certain relation between them, are found in a specific document.  Recall that topic is different from query. For ex-ample, given a query ?semantic web coordination?, the corresponding topic may be either ?semantic web? or ?web coordination?. Similarly, person here is different from expert candidate. E.g, given an expert candidate ?Ritu Raj Tiwari?, the matched person may be ?Ritu Raj Tiwari?, ?Tiwari?, or ?RRT? etc. Although both the topics and persons may not match the query and expert candidate ex-actly, they do have certain indication on the con-nection of query ?semantic web coordination? and expert ?Ritu Raj Tiwari?. 3.1 Evidence-Oriented Expert Search Model We conduct fine-grained expert search by incorpo-rating evidence of local context explicitly in a probabilistic model which we call an evidence-oriented expert search model. Given a query q, the probability of a candidate c being an expert (or knowing something about q) is estimated as 
( | ) ( , | )
( | , ) ( | )
e
e
P c q P c e q
P c e q P e q
=
=
?
?
, (1) 
where e denotes a quadruple of evidence.  Using the relaxation that the probability of c is independent of a query q given an evidence e, we can reduce Equation (1) as, 
( | ) ( | ) ( | )
e
P c q P c e P e q=
?
. (2) Compared to previous work, our model conducts expert search with a new way in which local con-texts of evidence are used to bridge a query q and an expert candidate c. The new way enables the expert search system to explore various local con-texts in a precise manner.  In the following sub-sections, we will detail two sub-models: the expert matching model P(c|e) and the evidence matching model P(e|q).  3.2 Expert Matching Model We expand the evidence e as quadruple <topic, people, relation, document> (<t, p, r, d> for short) for expert matching. Given a set of related evi-dences, we assume that the generation of an expert candidate c is independent with topic t and omit it 
in expert matching. Therefore, we simplify the ex-pert matching formula as below:  
),|()|(),,|()|( drpPpcPdrpcPecP ==
, (3) where P(c|p) depends on how an expert candidate c matches to a person occurrence p (e.g. full name or email of a person). The different ways of matching an expert candidate c with a person occurrence p results in varied qualities. P(c|p) represents the quality. P(p|r,d) expresses the probability of an occurrence p given a relation r and a document d. P(p|r,d) is estimated in MLE as, 
),(
),,(
),|(
drL
drpfreq
drpP = , (4) 
where freq(p,r,d) is the frequency of person p matched by relation r in document d, and L(r, d) is the frequency of all the persons matched by rela-tion r in d. This estimation can further be smoothed by using the evidence collection as follows:  
?
?
?+=
Dd
S
D
drpP
drpPdrpP
'
||
)',|(
)1(),|(),|( ?? , (5) 
where D denotes the whole document collection. |D| is the total number of documents.  We use Dirichlet prior in smoothing of parame-ter ?:  
KdrL
drL
+
=
),(
),(? , (6) 
where K is the average frequency of all the experts in the collection.  
3.3 Evidence Matching Model By expanding the evidence e and employing inde-pendence assumption, we have the following for-mula for evidence matching: 
)|()|()|()|(
)|,,,()|(
qdPqrPqpPqtP
qdrptPqeP
=
= . (7) 
In the following, we are to explain what these four terms represent and how they can be estimated. The first term P(t|q) represents the probability that a query q matches to a topic t in evidence. Re-call that a query q may match a topic t in various ways, not necessarily being identical to t. For ex-ample, both topic ?semantic web? and ?semantic web search engine? can match the query ?semantic web search engine?. The probability is defined as 
916
( )),()|( qttypePqtP ? , (8) where type(t, q) represents the way that q matches to t, e.g., phrase matching. Different matching methods are associated with different probabilities. The second term P(p|q) represents the probabil-ity that a person p is generated from a query q. The probability is further approximated by the prior probability of p, 
)()|( pPqpP ? . (9) The prior probability can be estimated by MLE, i.e., the ratio of total occurrences of person p in the collection. The third term represents the probability that a relation r is generated from a query q. Here, we approximate the probability as  
))(()|( rtypePqrP ?
, (10) where type(r) represents the way r connecting query and expert. P(type(r)) represents the reliabil-ity of relation type of r. Following the Bayes rule, the last term can be transformed as 
)()|(
)(
)()|(
)|( dPdqP
qP
dPdqP
qdP ?= , (11) 
where priority distribution P(d) can be estimated based on static rank, e.g., PageRank (Brin and Page, 1998). P(q|d) can be estimated by using a standard language model for IR (Ponte and Croft, 1998). In summary, Equation (7) is converted to 
( ) )()|())(()(),()|( dPdqPrtypePpPqttypePqeP ?
. (12) 
3.4 Evidence Merging  We assume that the ranking score of an expert can be acquired by summing up together all scores of the supporting evidences. Thus we calculate ex-perts? scores by aggregating the scores from all evidences as in Equation (1). 4 Implementation  The implementation of the proposed model con-sists of two stages, namely evidence extraction and evidence quality evaluation.  
4.1 Evidence Extraction Recall that we define an evidence for expert search as a quadruple <topic, person, relation, document>. The evidence extraction covers the extraction of the first three elements, namely person identifica-tion, topic discovering and relation extraction. 4.1.1 Person Identification  The occurrences of an expert can be in various forms, such as name and email address. We call each type of form an expert mask. Table 1 provides a statistic on various masks on the basis of W3C corpus. In Table 1, rate is the proportion of the person occurrences with relevant masks to the per-son occurrences with any of the masks, and ambi-guity is defined as the probability that a mask is shared by more than one expert.   Mask Rate/Ambiguity Sample Full Name(NF) 48.2% / 0.0000 Ritu Raj Tiwari  Email Name(NE) 20.1% / 0.0000 rtiwari@nuance.com Combined Name (NC) 4.2% /0.3992 Tiwari, Ritu R;            R R Tiwari Abbr. Name(NA) 21.2% / 0.4890 Ritu Raj ; Ritu Short Name(NS) 0.7% / 0.6396 RRT Alias, new email (NAE)  7% / 0.4600 Ritiwari rti-wari@hotmail.com Table 1. Various masks and their ambiguity 
1) Every occurrence of a candidate?s email address is normalized to the appropriate candidate_id. 2) Every occurrence of a candidate?s full_name is normalized to the appropriate candidate_id if there is no ambiguity; otherwise, the occurrence is normalized to the candidate_id of the most frequent candidate with that full_name. 3) Every occurrence of combined name, abbrevi-ated name, and email alias is normalized to the appropriate candidate_id if there is no ambigu-ity; otherwise, the occurrence may be normal-ized to the candidate_id of a candidate whose full name also appears in the document. 4) All the personal occurrences other than those covered by Heuristic 1) ~ 3) are ignored. Table 2. Heuristic rules for expert extraction As Table 1 demonstrates, it is not an easy task to identify all the masks with regards to an expert. On one hand, the extraction of full name and email address is straightforward but suffers from low coverage. On the other hand, the extraction of 
917
combined name and abbreviated name can com-plement the coverage, while needs handling of am-biguity.  Table 2 provides the heuristic rules that we use for expert identification. In the step 2) and 3), the rules use frequency and context discourse for re-solving ambiguities respectively. With frequency, each expert candidate actually is assigned a prior probability. With context discourse, we utilize the intuition that person names appearing similar in a document usually refers to the same person. 4.1.2 Topic Discovering A queried topic can occur within documents in various forms, too. We use a set of query process-ing techniques to handle the issue. After the proc-essing, a set of topics transformed from an original query will be obtained and then be used in the search for experts. Table 3 shows five forms of topic discovering from a given query.   Forms Description Sample Phrase Match(QP) The exact match with origi-nal query given by users ?semantic web search engine? Bi-gram Match(QB) A set of matches formed by extracting bi-gram of words in the original query ?semantic web? ?search en-gine? Proximity Match(QPR) Each query term appears as a neighborhood within a window of specified size ?semantic web enhanced search engine? Fuzzy Match(QF) A set of matches, each of which resembles the origi-nal query in appearance. ?sementic web seerch engine? Stemmed Match(QS) A match formed by stem-ming the original query. ?sementic web seerch engin? Table 3. Discovered topics from query ?semantic web search engine? 
4.1.3 Relation Extraction We focus on extracting relations between topics and expert candidates within a span of a document. To make the extraction easier, we partition a document into a pre-defined layout. Figure 1 pro-vides a template in Backus?Naur form. Figure 2 provides a practical use of the template. Note that we are not restricting the use of the template only for certain corpus. Actually the tem-plate can be applied to many kinds of documents. For example, for web pages, we can construct the <Title> from either the ?title? metadata or the con-
tent of web pages (Hu et al, 2006). As for e-mail, we can use the ?subject? field as the <Title>. 
 Figure. 1. A template of document layout 
...
...
...
RDF Primer
Editors: Frank Manola, fmanola@acm.org
   Eric Miller, W3C, em@w3.org
2. Making Statements About Resources
RDF is intended to provide a simple way to make state
These capabilities (the normative specification describe)
2.1 Basic Concepts
Imagine trying to state that someone named John Smith
The form of a simple statement such as:
<Title>
<Author>
<Body>
<Section Title>
<Section>
<Section Body>
 Figure 2. An example use of the layout template  With the layout of partitioned documents, we can then explore many types of relations among different blocks. In this paper, we demonstrate the use of five types of relations by extending the study in (Cao et al, 2005).  Section Relation (RS): The queried topic and the expert candidate occur in the same <Section>. Windowed Section Relation (RWS): The que-ried topic and the expert candidate occur within a fixed window of a <Section>. In our experiment, we used a window of 200 words. Reference Section Relation (RRS): Some <Sec-tion>s should be treated specially. For example, the <Section> consisting of reference information like a list of <book, author> can serve as a reliable source connecting a topic and an expert candidate. We call the relation appearing in a special type of <Section> a special reference section relation. It might be argued whether the use of special sections can be generalized. According to our survey, the special <Section>s can be found in various sites such as Wikipedia as well as W3C. Title-Author Relation (RTA): The queried topic appears in the <Title> and the expert candidate appears in the <Author>.  
918
Section Title-Body Relation (RSTB): The que-ried topic and the expert candidate appear in the <Section Title> and <Section Body> of the same <Section>, respectively. Reversely, the queried topic and the expert candidate can appear in the <Section Body> and <Section Title> of a <Section>. The latter case is used to characterize the docu-ments introducing certain expert or the expert in-troducing certain document. Note that our model is not restricted to use these five relations. We use them only for the aim of demonstrating the flexibility and effectiveness of fine-grained expert search. 4.2 Evidence Quality Evaluation In this section, we elaborate the mechanism used for evaluating the quality of evidence.  4.2.1 Topic-Matching Quality In Section 4.1.2, we use five techniques in process-ing query matches, which yield five sets of match types for a given query. Obviously, the different query matches should be associated with different weights because they represent different qualities.  We further note that different bi-grams gener-ated from the same query with the bi-gram match-ing method might also present different qualities. For example, both topic ?css test? and ?test suite? are the bi-gram matching for query ?css test suite?; however, the former might be more informative. To model that, we use the number of returned documents to refine the query weight. The intuition behind that is similar to the thought of IDF popu-larly used in IR as we prefer to the distinctive bi-grams. Taking into consideration the above two factors, we calculate the topic-matching quality Qt (corre-sponding to P(type(t,q)) in Equation (12) ) for the given query q as 
t
tt
t
df
dfMIN
qttypeWQ
)(
)),((
''
= , (13) 
where t means the discovered topic from a docu-ment and type(t,q) is the matching type between topic t and query q. W(type(t,q)) is the weight for a certain query type, dft is the number of returned documents matched by topic t. In our experiment, we use the 10 training topics of TREC2005 as our training data, and the best quality scores for phrase match, bi-gram match, proximity match, fuzzy 
match, and stemmed match are 1, 0.01, 0.05, 10-8, and 10-4, respectively.  4.2.2 Person-Matching Quality An expert candidate can occur in the documents in various ways. The most confident occurrence should be the ones in full name or email address. Others can include last name only, last name plus initial of first name, etc. Thus, the action of reject-ing or accepting a person from his/her mask (the surface expression of a person in the text) is not simply a Boolean decision, but a probabilistic one with a reliability weight Qp (corresponding to P(c|p) in Equation (3) ). Similarly, the best trained weights for full name, email name, combined name, abbreviated name, short name, and alias email are set to 1, 1, 0.8, 0.2, 0.2, and 0.1, respectively. 4.2.3 Relation Type Quality The relation quality consists of two factors. One factor is about the type of the relation. Different types of relations indicate different strength of the connection between expert candidates and queried topics. In our system, the section title-body rela-tion is given the highest confidence. The other fac-tor is about the degree of proximity between a query and an expert candidate. The intuition is that, the more distant are a query and an expert candi-date within a relation, the looser the connection between them is. To include these two factors, the quality score Qr (corresponding to P(type(r)) in Equation (12) )of a relation r is defined as:  
1),( +
=
tpdis
C
WQ
r
rr
, (14) 
where Wr is the weight of relation type r, dis(p, t) is the distance from the person occurrence p to the queried topic t and Cr is a constant for normaliza-tion. Again, we optimize the Wr based on the train-ing topics, the best weights for section relation, windowed section relation, reference section rela-tion, title-author relation, and section title-body relation  are 1, 4, 10, 45, and 1000 respectively.  4.2.4 Document Quality The quality of evidence also depends on the quality of the document, the context in which it is found. The document context can affect the credibility of the evidence in two ways:  
919
Static quality: indicating the authority of a document. In our experiment, the static quality Qd (corresponding to P(d) in Equation (12) ) is esti-mated by the PageRank, which is calculated using a standard iterative algorithm with a damping fac-tor of 0.85 (Brin and Page, 1998).  Dynamic quality: by ?dynamic?, we mean the quality score varies for different queries q. We de-note the dynamic quality as QDY(d,q) (correspond-ing to P(q|d) in Equation (12) ), which is actually the document relevance score returned by a stan-dard language model for IR(Ponte and Croft, 1998). 5 Experimental Results 
5.1  The Evaluation Data In our experiment, we used the data set in the ex-pert search task of enterprise search track at TREC 2005 and 2006. The document collection is a crawl of the public W3C sites in June 2004. The crawl comprises in total 331,307 web pages. In the fol-lowing experiments, we used the training set of 10 topics of TREC 2005 for tuning the parameters aforementioned in Section 4.2, and used the test set of 50 topics of TREC 2005 and 49 topics of TREC 2006 as the evaluation data sets.  5.2 Evaluation Metrics We used three measures in evaluation: Mean aver-age precision (MAP), R-precision (R-P), and Top N precision (P@N). They are also the standard measures used in the expert search task of TREC. 5.3 Evidence Extraction In the following experiments, we constructed the baseline by using the query matching methods of phrase matching, the expert matching methods of full name matching and email matching, and the relation of section relation. To show the contribu-tion of each individual method for evidence extrac-tion, we incrementally add the methods to the baseline method. In the following description, we will use ?+? to denote applying new method on the previous setting. 5.3.1 Query Matching Table 4 shows the results of expert search achieved by applying different methods of query matching. 
QB, QPR, QF, and QS denote bi-gram match, prox-imity match, fuzzy match, and stemmed match, respectively. The performance of the proposed model increases stably on MAP when new query matches are added incrementally. We also find that the introduction of QF and QS bring some drop on R-Precision and P@10. It is reasonable because both QF and QS bring high recall while affect the precision a bit. The overall relative improvement of using query matching compared to the baseline is presented in the row ?Improv.?. We performed t-tests on MAP. The p-values (< 0.05) are presented in the ?T-test? row, which shows that the im-provement is statistically significant.   TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.1840 0.2136 0.3060 0.3752 0.4585 0.5604 +QB 0.1957 0.2438 0.3320 0.4140 0.4910 0.5799 +QPR  0.2024 0.2501 0.3360 0.4530 0.5137 0.5922 +QF ,QS 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 Improv. 10.33% 17.09% 9.80% 22.07% 11.49% 5.30% T-test 0.0084   0.0000   Table 4. The effects of query matching 
5.3.2 Person Matching For person matching, we considered four types of masks, namely combined name (NC), abbreviated name (NA), short name (NS) and alias and new email (NAE). Table 5 provides the results on person matching at TREC 2005 and 2006. The baseline is the best model achieved in previous section. It seems that there is little improvement on P@10 while an improvement of 6.21% and 14.00% is observed on MAP. This might be due to the fact that the matching method such as NC has a higher recall but lower precision.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 +NC 0.2056 0.2539 0.3463 0.4709 0.5152 0.5931 +NA  0.2106 0.2545 0.3400 0.5010 0.5181 0.6000 +NS  0.2111 0.2578 0.3400 0.5121 0.5192 0.6000 +NAE  0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 Improv. 6.21% 3.60% 1.19% 14.00% 1.96% 1.68% T-test 0.0064   0.0057   Table 5. The effects of person matching 
920
5.3.3 Multiple Relations For relation extraction, we experimentally demon-strated the use of each of the five relations pro-posed in Section 4.1.3, i.e., section relation (RS), windowed section relation (RWS), reference section relation (RRS), title-author relation (RTA), and sec-tion title-body relation (RSTB). We used the best model achieved in previous section as the baseline. From Table 6, we can see that the section title-body relation contributes the most to the improve-ment of the performance. By using all the discov-ered relations, a significant improvement of 19.94% and 8.35% is achieved.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 +RWS 0.2158 0.2633 0.3380 0.5255 0.5311 0.6082 +RRS 0.2160 0.2630 0.3380 0.5272 0.5314 0.6061 +RTA 0.2234 0.2634 0.3580 0.5354 0.5355 0.6245 +RSTB 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 Improv. 19.94% 19.91% 10.00% 8.35% 8.77% 8.50% T-test 0.0013   0.0043   Table 6. The effects of relation extraction 
5.4 Evidence Quality  The performance of expert search can be further improved by considering the evidence quality. Ta-ble 7 shows the results by considering the differ-ences in quality.  We evaluated two kinds of evidence quality: context static quality (Qd) and context dynamic quality (QDY). Each of the evidence quality con-tributes about 1%-2% improvement for MAP. The improvement from the PageRank that we calcu-lated from the corpus implies that the web scaled rank technique is also effective in the corpus of documents. Finally, we find a significant relative improvement of 6.13% and 2.86% on MAP by us-ing evidence qualities.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 +Qd 0.2711 0.3188 0.3720 0.5900 0.5813 0.6796 +QDY 0.2755 0.3252 0.3880 0.5943 0.5877 0.7061 Improv. 6.13% 4.67% 3.74% 2.86% 3.67% 8.61% T-test 0.0360   0.0252   Table 7. The effects of using evidence quality  
5.5 Comparison with Other Systems In Table 8, we juxtapose the results of our prob-abilistic model for fine-grained expert search with automatic expert search systems from the TREC evaluation. The performance of our proposed model is rather encouraging, which achieved com-parable results to the best automatic systems on the TREC 2005 and 2006.    MAP R-prec Prec@10 TREC2005 0.2749 0.3330 0.4520 Rank-1 System TREC20061 0.5947 0.5783 0.7041 TREC2005 0.2755 0.3252 0.3880 Our System TREC2006 0.5943 0.5877 0.7061 Table 8. Comparison with other systems 
6 Conclusions This paper proposed to conduct expert search using a fine-grained level of evidence. Specifically, quadruple evidence was formally defined and served as the basis of the proposed model. Differ-ent implementations of evidence extraction and evidence quality evaluation were also comprehen-sively studied. The main contributions are:  1. The proposal of fine-grained expert search, which we believe to be a promising direc-tion for exploring subtle aspects of evidence.  2. The proposal of probabilistic model for fine-grained expert search. The model facilitates investigating the subtle aspects of evidence.  3. The extensive evaluation of the proposed probabilistic model and its implementation on the TREC data set. The evaluation shows promising expert search results.   In future, we are to explore more domain inde-pendent evidences and evaluate the proposed model on the basis of the data from other domains. Acknowledgments The authors would like to thank the three anony-mous reviewers for their elaborate and helpful comments. The authors also appreciate the valu-able suggestions of Hang Li, Nick Craswell, Yangbo Zhu and Linyun Fu.                                                            1 This system, where cluster-based re-ranking is used, is a variation of the fine-grained model proposed in this paper. 
921
References  Bailey, P.,  Soboroff , I., Craswell, N., and Vries A.P., Overview of the TREC 2007 Enterprise Track. In: Proc. of TREC 2007.  Balog, K., Azzopardi, L., and Rijke, M. D., 2006. Formal models for expert finding in enterprise cor-pora. In: Proc. of SIGIR?06,pp.43-50. Brin, S. and Page, L., 1998. The anatomy of a rlarge-scale hypertextual Web search engine, Computer Networks and ISDN Systems (30), pp.107-117. Campbell, C.S., Maglio, P., Cozzi, A. and Dom, B., 2003. Expertise identification using email communi-cations. In: Proc. of CIKM ?03 pp.528?531.  Cao, Y., Liu, J., and Bao, S., and Li, H., 2005. Research on expert search at enterprise track of TREC 2005. In: Proc. of TREC 2005. Craswell, N., Hawking, D., Vercoustre, A. M. and Wil-kins, P., 2001. P@NOPTIC Expert: searching for ex-perts not just for documents. In: Proc. of Ausweb?01. Craswell, N., Vries, A.P., and Soboroff, I., 2005. Over-view of the TREC 2005 Enterprise Track. In: Proc. of TREC 2005. Davenport, T. H. and Prusak, L., 1998. Working Knowledge: how organizations manage what they know. Howard Business, School Press, Boston, MA. Dom, B., Eiron, I., Cozzi A. and Yi, Z., 2003. Graph-based ranking algorithms for e-mail expertise analy-sis, In: Proc. of SIGMOD?03 workshop on Research issues in data mining and knowledge discovery. Fang, H., Zhou, L., Zhai, C., 2006. Language models for expert finding-UIUC TREC 2006 Enterprise Track Experiments, In: Proc. of TREC2006. Fu, Y., Xiang, R., Liu, Y., Zhang, M., Ma, S., 2007. A CDD-based Formal Model for Expert Finding. In Proc. of CIKM 2007. Hertzum, M. and Pejtersen, A. M., 2000. The informa-tion-seeking practices of engineers: searching for documents as well as for people. Information Proc-essing and Management, 36(5), pp.761?778. Hu, Y., Li, H., Cao, Y., Meyerzon, D. Teng, L., and Zheng, Q., 2006. Automatic extraction of titles from general documents using machine learning, IPM. Kautz, H., Selman, B. and Milewski, A., 1996. Agent amplified communication. In: Proc. of AAAI?96, pp. 3?9. Mattox, D., Maybury, M. and Morey, D., 1999. Enter-prise expert and knowledge discovery. Technical Re-port.  McDonald, D. W. and Ackerman, M. S., 1998. Just Talk to Me: a field study of expertise location. In: Proc. of CSCW?98, pp.315-324. Mockus, A. and Herbsleb, J.D., 2002. Expertise Browser: a quantitative approach to identifying ex-pertise, In: Proc. of ICSE?02. 
Maconald, C. and Ounis, I., 2006. Voting for candi-dates: adapting data fusion techniques for an expert search task. In: Proc. of CIKM'06, pp.387-396. Macdonald, C. and Ounis, I., 2007. Expertise Drift and Query Expansion in Expert Search. In Proc. of CIKM 2007.  Petkova, D., and Croft, W. B., 2006. Hierarchical lan-guage models for expert finding in enterprise cor-pora, In: Proc. of ICTAI?06, pp.599-608. Ponte, J. and Croft, W., 1998. A language modeling approach to information retrieval, In: Proc. of SIGIR?98, pp.275-281. Sihn, W. and Heeren F., 2001. Xpertfinder-expert find-ing within specified subject areas through analysis of e-mail communication. In: Proc. of the 6th Annual Scientific conference on Web Technology.  Soboroff, I., Vries, A.P., and Craswell, N., 2006. Over-view of the TREC 2006 Enterprise Track. In: Proc. of TREC 2006. Steer, L.A. and Lochbaum, K.E., 1988. An ex-pert/expert locating system based on automatic repre-sentation of semantic structure, In: Proc. of the 4th IEEE Conference on Artificial Intelligence Applica-tions. Yimam, D., 1996. Expert finding systems for organiza-tions: domain analysis and the DEMOIR approach. In: ECSCW?99 workshop of beyond knowledge man-agement: managing expertise, pp. 276?283. 
922
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 380?390,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Collective Tweet Wikification based on Semi-supervised Graph
Regularization
Hongzhao Huang
1
, Yunbo Cao
2
, Xiaojiang Huang
2
, Heng Ji
1
, Chin-Yew Lin
2
1
Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
2
Microsoft Research Asia, Beijing 100080, P.R.China
{huangh9,jih}@rpi.edu
1
,
{yunbo.cao,xiaojih,cyl}@microsoft.com
2
Abstract
Wikification for tweets aims to automat-
ically identify each concept mention in a
tweet and link it to a concept referent in
a knowledge base (e.g., Wikipedia). Due
to the shortness of a tweet, a collective
inference model incorporating global ev-
idence from multiple mentions and con-
cepts is more appropriate than a non-
collecitve approach which links each men-
tion at a time. In addition, it is chal-
lenging to generate sufficient high quality
labeled data for supervised models with
low cost. To tackle these challenges, we
propose a novel semi-supervised graph
regularization model to incorporate both
local and global evidence from multi-
ple tweets through three fine-grained re-
lations. In order to identify semantically-
related mentions for collective inference,
we detect meta path-based semantic rela-
tions through social networks. Compared
to the state-of-the-art supervised model
trained from 100% labeled data, our pro-
posed approach achieves comparable per-
formance with 31% labeled data and ob-
tains 5% absolute F1 gain with 50% la-
beled data.
1 Introduction
With millions of tweets posted daily, Twitter en-
ables both individuals and organizations to dis-
seminate information, from current affairs to
breaking news in a timely fashion. In this
work, we study the wikification (Disambiguation
to Wikipedia) task (Mihalcea and Csomai, 2007)
for tweets, which aims to automatically identify
each concept mention in a tweet, and link it to a
concept referent in a knowledge base (KB) (e.g.,
Wikipedia). For example, as shown in Figure 1,
Hawks is an identified mention, and its correct ref-
erent concept in Wikipedia is Atlanta Hawks. An
end-to-end wikification system needs to solve two
sub-problems: (i) concept mention detection, (ii)
concept mention disambiguation.
Wikification is a particularly useful task for
short messages such as tweets because it allows
a reader to easily grasp the related topics and en-
riched information from the KB. From a system-
to-system perspective, wikification has demon-
strated its usefulness in a variety of applica-
tions, including coreference resolution (Ratinov
and Roth, 2012) and classification (Vitale et al,
2012).
Sufficient labeled data is crucial for supervised
models. However, manual wikification annota-
tion for short documents is challenging and time-
consuming (Cassidy et al, 2012). The challenges
are: (i) unlinkability, a valid concept may not ex-
ist in the KB. (ii) ambiguity, it is impossible to
determine the correct concept due to the dearth
of information within a single tweet or multiple
correct answer. For instance, it would be diffi-
cult to determine the correct referent concept for
?Gators? in t
1
in Figure 1. Linking ?UCONN?
in t
3
to University of Connecticut may also be ac-
ceptable since Connecticut Huskies is the athletic
team of the university. (iii) prominence, it is chal-
lenging to select a set of linkable mentions that
are important and relevant. It is not tricky to select
?Fans?, ?slump?, and ?Hawks? as linkable men-
tions, but other mentions such as ?stay up? and
?stay positive? are not prominent. Therefore, it
is challenging to create sufficient high quality la-
beled tweets for supervised models and worth con-
sidering semi-supervised learning with the explo-
ration of unlabeled data.
380
    
Stay up Hawk Fans. We are going 
through a slump now, but we have to 
stay positive. Go Hawks!
Congrats to UCONN and Kemba Walker. 
5 wins in 5 days, very impressive...
Just getting to the Arena, we play the 
Bucks tonight. Let's get it!
Fan (person); Mechanical fan
Slump (geology);  Slump (sports)
Atlanta Hawks;  Hawks (film)
University of Connecticut; Connecticut Huskies
Kemba Walker
Arena; Arena (magazine); Arena (TV series)
Bucks County, Pennsylvania; Milwaukee Bucks
Tweets Concept Candidates
Go Gators!!! Florida Gators football; Florida Gators men's basketballt1
t2
t3
t4
Figure 1: An illustration of Wikification Task for Tweets. Concept mentions detected in tweets are
marked as bold, and correctly linked concepts are underlined. The concept candidates are ranked by
their prior popularity which will be explained in section 4.1, and only top 2 ranked concepts are listed.
However, when selecting semi-supervised
learning frameworks, we noticed another unique
challenge that tweets pose to wikification due
to their informal writing style, shortness and
noisiness. The context of a single tweet usually
cannot provide enough information for prominent
mention detection and similarity computing for
disambiguation. Therefore, a collective inference
model over multiple tweets in the semi-supervised
setting is desirable. For instance, the four tweets
in Figure 1 are posted by the same author within
a short time period. If we perform collective
inference over them we can reliably link am-
biguous mentions such as ?Gators?, ?Hawks?,
and ?Bucks? to basketball teams instead of other
concepts such as the county Bucks County.
In order to address these unique challenges
for wikification for the short tweets, we employ
graph-based semi-supervised learning algorithms
(Zhu et al, 2003; Smola and Kondor, 2003; Blum
et al, 2004; Zhou et al, 2004; Talukdar and
Crammer, 2009) for collective inference by ex-
ploiting the manifold (cluster) structure in both
unlabeled and labeled data. These approaches
normally assume label smoothness over a defined
graph, where the nodes represent a set of labeled
and unlabeled instances, and the weighted edges
reflect the closeness of each pair of instances. In
order to construct a semantic-rich graph capturing
the similarity between mentions and concepts for
the model, we introduce three novel fine-grained
relations based on a set of local features, social
networks and meta paths.
The main contributions of this paper are sum-
marized as follows:
? To the best of our knowledge, this is the first
effort to explore graph-based semi-supervised
learning algorithms for the wikification task.
? We propose a novel semi-supervised graph reg-
ularization model performing collective infer-
ence for joint mention detection and disam-
biguation. Our approach takes advantage of
three proposed principles to incorporate both lo-
cal and global evidence from multiple tweets.
? We propose a meta path-based unified frame-
work to detect both explicitly and implicitly rel-
evant mentions.
2 Preliminaries
Concept and Concept Mention We define a con-
cept c as a Wikipedia article (e.g., Atlanta Hawks),
and a concept mentionm as an n-gram from a spe-
cific tweet. Each concept has a set of textual repre-
sentation fields (Meij et al, 2012), including title
(the title of the article), sentence (the first sentence
of the article), paragraph (the first paragraph of
the article), content (the entire content of the arti-
cle), and anchor (the set of all anchor texts with
incoming links to the article).
Wikipedia Lexicon Construction We first
construct an offline lexicon with each entry as
?m, {c
1
, ..., c
k
}?, where {c
1
, ..., c
k
} is the set of
possible referent concepts for the mention m.
Following the previous work (Bunescu, 2006;
Cucerzan, 2007; Hachey et al, 2013), we extract
the possible mentions for a given concept c using
the following resources: the title of c; the aliases
appearing in the introduction and infoboxes of c
(e.g., The Evergreen State is an alias of Wash-
ington state); the titles of pages redirecting to c
(e.g., State of Washington is a redirecting page of
Washington (state)); the titles of the disambigua-
381
tion pages containing c; and all the anchor texts
appearing in at least 5 pages with hyperlinks to c
(e.g., WA is a mention for the concept Washing-
ton (state) in the text ?401 5th Ave N [[Seattle]],
[[Washington (state)?WA]] 98109 USA?. We also
propose three heuristic rules to extract mentions
(i.e., different combinations of the family name
and given name for a person, the headquarters of
an organization, and the city name for a sports
team).
Concept Mention Extraction Based on the
constructed lexicon, we then consider all n-grams
of size? n (n=7 in this paper) as concept mention
candidates if their entries in the lexicon are not
empty. We first segment @usernames and #hash-
tags into regular tokens (e.g., @amandapalmer is
segmented as amanda palmer and #WorldWater-
Day is split as World Water Day) using the ap-
proach proposed by (Wang et al, 2011). Segmen-
tation assists finding concept candidates for these
non-regular mentions.
3 Principles and Approach Overview
    
R elational Graph Construction
Knowledge Base 
(Wikipedia)
Labeled and 
Unlabeled Tweets
Wikipedia Lex icon Construction
Concept Mention and 
Concept Candidate E x traction
Local Compatibility
(local features, 
cosine similarity)
Coreference
(meta path,
mention 
similarity)
Semantic R elatedness
(meta path, concept 
semantic relatedness)
Semi- Supervised Graph R egularization
<Mention, Concept>
Pairs
Figure 2: Approach Overview.
3.1 Principles
A single tweet may not provide enough evidence
to identify prominent mentions and infer their cor-
rect referent concepts due to the lack of contextual
information. To tackle this problem, we propose to
incorporate global evidence from multiple tweets
and performing collective inference for both men-
tion identification and disambiguation. We first in-
troduce the following three principles that our ap-
proach relies on.
Principle 1 (Local compatibility): Two pairs
of ?m, c? with strong local compatibility tend to
have similar labels. Mentions and their correct
referent concepts usually tend to share a set of
characteristics such as string similarity betweenm
and c (e.g., ?Chicago, Chicago? and ?Facebook,
Facebook?). We define the local compatibility to
model such set of characteristics.
Principle 2 (Coreference): Two coreferential
mentions should be linked to the same concept.
For example, if we know ?nc? and ?North Car-
olina? are coreferential, then they should both be
linked to North Carolina.
Principle 3 (Semantic Relatedness): Two
highly semantically-related mentions are more
likely to be linked to two highly semantically-
related concepts. For instance, when ?Sweet 16?
and ?Hawks? often appear together within rel-
evant contexts, they can be reliably linked to
two baseketball-related concepts NCAA Men?s Di-
vision I Basketball Championship and Atlanta
Hawks, respectively.
3.2 Approach Overview
Given a set of tweets ?t
1
, ..., t
|T |
?, our system first
generates a set of candidate concept mentions, and
then extracts a set of candidate concept referents
for each mention based on the Wikipedia lexicon.
Given a pair of mention and its candidate referent
concept ?m, c?, the remaining task of wikification
is to assign either a positive label if m should be
selected as a prominently linkable mention and c
is its correct referent concept, or otherwise a neg-
ative label. The label assignment is obtained by
our semi-supervised graph regularization frame-
work based on a relational graph, which is con-
structed from local compatibility, coreference, and
semantic relatedness relations. The overview of
our approach is as illustrated in Figure 2.
4 Relational Graph Construction
We first construct the relational graphG = ?V,E?,
where V = {v
1
, ..., v
n
} is a set of nodes and E =
{e
1
, ..., e
m
} is a set of edges. Each v
i
= ?m
i
, c
i
?
represents a tuple of mention m
i
and its referent
concept candidate c
i
. An edge is added between
two nodes v
i
and v
j
if there is a proposed rela-
tion based on the three principles described in sec-
tion 3.1.
4.1 Local Compatibility
We first compute local compatibility (Principle 1)
by considering a set of novel local features to cap-
382
ture the importance and relevance of a mention m
to a tweet t, as well as the correctness of its link-
age to a concept c. We have designed a number
of features which are similar to those commonly
used in wikification and entity linking work (Meij
et al, 2012; Guo et al, 2013; Mihalcea and Cso-
mai, 2007).
Mention Features We define the following fea-
tures based on information from mentions.
? IDF
f
(m) = log(
|C|
df(m)
), where |C| is the total
number of concepts in Wikipedia and df(m) is
the total number of concepts in whichm occurs,
and f indicates the field property, including ti-
tle, content, and anchor.
? Keyphraseness(m) =
|C
a
(m)|
df(m)
to measure
how likely m is used as an anchor in Wikipedia,
where C
a
(m) is the set of concepts where m
appears as an anchor.
? LinkProb(m) =
?
c?C
a
(m)
count(m,c)
?
c?C
count(m,c)
, where
count(m, c) indicates the number of occurrence
of m in c.
? SNIL(m) and SNCL(m) to count the number
of concepts that are equal to or contain a sub-n-
gram of m, respectively (Meij et al, 2012).
Concept Features The concept features are
solely based on Wikipedia, including the number
of incoming and outgoing links for c, and the num-
ber of words and characters in c.
Mention + Concept Features This set of fea-
tures considers information from both mentions
and concepts:
? prior popularity prior(m, c) =
count(m,c)?
c
?
count(m,c
?
)
, where count(m, c) mea-
sures the frequency of the anchor links from m
to c in Wikipedia.
? TF
f
(m, c) =
count
f
(m,c)
|f |
to measure the rela-
tive frequency of m in each field representation
f of c, normalized by the length of f . The fields
include title, sentence, paragraph, content and
anchor.
? NCT (m, c), TCN(m, c), and TEN(m, c) to
measure whether m contains the title of c,
whether the title of c contains m, and whether
m equals to the title of c, respectively.
Context Features This set of features include
(i) Context Capitalization features, which indicate
whether the current mention, the token before, and
the token after are capitalized. (ii) tf-idf based fea-
tures, which include the dot product of two word
vectors v
c
and v
t
, and the average tf-idf value of
common items in v
c
and v
t
, where v
c
and v
t
are
the top 100 tf-idf word vectors in c and t.
Local Compatibility Computation For each
node v
i
= ?m
i
, c
i
?, we collect its local features
as a feature vector F
i
= ?f
1
, f
2
, ..., f
d
?. To avoid
features with large numerical values that domi-
nate other features, the value of each feature is
re-scaled using feature standardization approach.
The cosine similarity is then adopted to compute
the local compatibility of two nodes and construct
a k nearest neighbor (kNN) graph, where each
node is connected to its k nearest neighboring
nodes. We compute the weight matrix that rep-
resents the local compatibility relation as:
W
loc
ij
=
{
cosine(F
i
, F
j
) j ? kNN(i)
0 Otherwise
4.2 Meta Path
    
Mention
Hashtag
Tweet User
post - 1
post
contain - 1
contain
contain - 1 contain
Figure 3: Schema of the Twitter network.
In this subsection, we introduce the concept
meta path which will be used to detect corefer-
ence (section 4.3) and semantic relatedness rela-
tions (section 4.4).
A meta-path is a path defined over a network
and composed of a sequence of relations between
different object types (Sun et al, 2011b). In our
experimental setting, we can construct a natu-
ral Twitter network summarized by the network
schema in Figure 3. The network contains four
types of objects: Mention (M), Tweet (T), User
(U), and Hashtag (H). Tweets and mentions are
connected by links ?contain? and ?contained by?
(denoted as ?contain
?1
?); and other linked rela-
tionships can be described similarly.
We then define the following five types of meta
paths to connect two mentions as:
? ?M - T - M?,
? ?M - T - U - T - M?,
? ?M - T - H - T - M?,
? ?M - T - U - T - M - T - H - T - M?,
? ?M - T - H - T - M - T - U - T - M?.
383
Each meta path represents one particular seman-
tic relation. For instance, the first three paths are
basic ones expressing the explicit relations that
two mentions are from the same tweet, posted by
the same user, and share the same #hashtag, re-
spectively. The last two paths are concatenated
ones which are constructed by concatenating the
first three simple paths to express the implicit rela-
tions that two mentions co-occur with a third men-
tion sharing either the same authorship or #hash-
tag. Such complicated paths can be exploited to
detect more semantically-related mentions from
wider contexts. For example, the relational link
between ?narita airport? and ?Japan? would be
missed without using the path ?narita airport - t
1
- u
1
- t
2
- american - t
3
- h
1
- t
4
- Japan? since they
don?t directly share any authorships or #hashtags.
4.3 Coreference
A coreference relation (Principle 2) usually occurs
across multiple tweets due to the highly redundant
information in Twitter. To ensure high precision,
we propose a simple yet effective approach utiliz-
ing the rich social network relations in Twitter.
We consider two mentions m
i
and m
j
corefer-
ential if m
i
and m
j
share the same surface form
or one is an abbreviation of the other, and at least
one meta path exists betweenm
i
andm
j
. Then we
define the weight matrix representing the corefer-
ential relation as:
W
coref
ij
=
?
?
?
1.0 if m
i
and m
j
are coreferential,
and c
i
= c
j
0 Otherwise
4.4 Semantic Relatedness
Ensuring topical coherence (Principle 3) has been
beneficial for wikification on formal texts (e.g.,
News) by linking a set of semantically-related
mentions to a set of semantically-related concepts
simultaneously (Han et al, 2011; Ratinov et al,
2011; Cheng and Roth, 2013). However, the short-
ness of a single tweet means that it may not pro-
vide enough topical clues. Therefore, it is impor-
tant to extend this evidence to capture semantic re-
latedness information from multiple tweets.
We define the semantic relatedness score be-
tween two mentions as SR(m
i
,m
j
) = 1.0 if at
least one meta path exists between m
i
and m
j
,
otherwise SR(m
i
,m
j
) = 0. In order to compute
the semantic relatedness of two concepts c
i
and
c
j
, we adopt the approach proposed by (Milne and
Witten, 2008a):
SR(c
i
, c
j
) = 1?
logmax(|C
i
|, |C
j
|)? log |C
i
? C
j
|
log(|C|)? logmin(|C
i
|, |C
j
|)
,
where |C| is the total number of concepts in
Wikipedia, and C
i
and C
j
are the set of concepts
that have links to c
i
and c
j
, respectively.
Then we compute a weight matrix representing
the semantic relatedness relation as:
W
rel
ij
=
{
SR(N
i
, N
j
) if SR(N
i
, N
j
) ? ?
0 Otherwise
where SR(N
i
, N
j
) = SR(m
i
,m
j
) ? SR(c
i
, c
j
)
and ? = 0.3, which is optimized from a develop-
ment set.
4.5 The Combined Relational Graph
    
hawks, 
 Atlanta Hawks
uconn, 
Connecticut 
Huskies
bucks, 
Milwaukee 
Bucks
kemba walker, 
Kemba Walker
0 .40 4
gators, 
Florida Gators 
men's basketball
now, 
N ow
days, 
D ay
tonight, 
Tonight
0 .9 32
0 .7 6 4
0 .6 6 5
0 .46 7
0 .56 3 0 .538
0 .447
Figure 4: A example of the relational graph con-
structed for the example tweets in Figure 1. Each
node represents a pair of ?m, c?, separated by a
comma. The edge weight is obtained from the lin-
ear combination of the weights of the three pro-
posed relations. Not all mentions are included due
to the space limitations.
Based on the above three weight matricesW
loc
,
W
coref
, and W
rel
, we first obtain their corre-
sponding transition matrices P
loc
, P
coref
, and
P
rel
, respectively. The entry P
ij
of the transition
matrix P for a weight matrix W is computed as
P
ij
=
W
ij?
k
W
ik
such that
?
k
P
ik
= 1. Then we
obtain the combined graph G with weight matrix
W , where W
ij
= ?P
loc
ij
+ ?P
coref
ij
+ ?P
rel
ij
. ?,
?, and ? are three coefficients between 0 and 1
with the constraint that ?+ ? + ? = 1. They con-
trol the contributions of these three relations in our
semi-supervised graph regularization model. We
choose transition matrix to avoid the domination
of one relation over others. An example graph of
G is shown in Figure 4. Compared to the referent
graph which considers each mention or concept
as a node in previous graph-based re-ranking ap-
proaches (Han et al, 2011; Shen et al, 2013), our
384
novel graph representation has two advantages: (i)
It can easily incorporate more features related to
both mentions and concepts. (ii) It is more appro-
priate for our graph-based semi-supervised model
since it is difficult to assign labels to a pair of men-
tion and concept in the referent graph.
5 Semi-supervised Graph Regularization
Given the constructed relational graph with the
weighted matrix W and the label vector Y of all
nodes, we assume the first l nodes are labeled as
Y
l
and the remaining u nodes (u = n? l) are ini-
tialized with labels Y
0
u
. Then our goal is to refine
Y
0
u
and obtain the final label vector Y
u
.
Intuitively, if two nodes are strongly connected,
they tend to hold the same label. We propose a
novel semi-supervised graph regularization frame-
work based on the graph-based semi-supervised
learning algorithm (Zhu et al, 2003):
Q(Y) = ?
n
?
i=l+1
(y
i
?y
0
i
)
2
+
1
2
?
i,j
W
ij
(y
i
?y
j
)
2
.
The first term is a loss function that incorporates
the initial labels of unlabeled examples into the
model. In our method, we adopt prior popular-
ity (section 4.1) to initialize the labels of the un-
labeled examples. The second term is a regular-
izer that smoothes the refined labels over the con-
structed graph. ? is a regularization parameter that
controls the trade-off between initial labels and the
consistency of labels on the graph. The goal of the
proposed framework is to ensure that the refined
labels of unlabeled nodes are consistent with their
strongly connected nodes, as well as not too far
away from their initial labels.
The above optimization problem can be solved
directly since Q(Y) is convex (Zhu et al, 2003;
Zhou et al, 2004). Let I be an identity matrix
and D
W
be a diagonal matrix with entries D
ii
=
?
j
W
ij
. We can split the weighted matrix W into
four blocks as W =
[
W
ll
W
lu
W
ul
W
uu
]
, where W
mn
is
anm?nmatrix. D
w
is split similarly. We assume
that the vector of the labeled examples Y
l
is fixed,
so we only need to infer the refined label vector of
the unlabeled examples Y
u
. In order to minimize
Q(Y), we need to find Y
?
u
such that
?Q
?Y
u
?
?
?
?
Y
u
=Y
?
u
= (D
uu
+ ?I
uu
)Y
u
?W
uu
Y
u
?
W
ul
Y
l
? ?Y
0
u
= 0.
Therefore, a closed form solution can be derived
as Y
?
u
= (D
uu
+ ?I
uu
?W
uu
)
?1
(W
ul
Y
l
+ ?Y
0
u
).
However, for practical application to a large-
scale data set, an iterative solution would be more
efficient to solve the optimization problem. Let
Y
t
u
be the refined labels after the t
th
iteration, the
iterative solution can be derived as:
Y
t+1
u
= (D
uu
+?I
uu
)
?1
(W
uu
Y
t
u
+W
ul
Y
l
+?Y
0
u
).
The iterative solution is more efficient since
(D
uu
+ ?I
uu
) is a diagonal matrix and its inverse
is very easy to compute.
6 Experiments
In this section we compare our approach with
state-of-the-art methods as shown in Table 1.
6.1 Data and Scoring Metric
For our experiments we use a public data set (Meij
et al, 2012) including 502 tweets posted by 28
verified users. The data set was annotated by two
annotators. We randomly sample 102 tweets for
development and the remaining for evaluation. We
use a Wikipedia dump on May 3, 2013 as our
knowledge base, which includes 30 million pages.
For computational efficiency, we also filter some
mention candidates by applying the preprocess-
ing approach proposed in (Ferragina and Scaiella,
2010), and remove all the concepts with prior pop-
ularity less than 2% from an mention?s concept set
for each mention, similar to (Guo et al, 2013).
A mention and concept pair ?m, c? is judged as
correct if and only if m is linkable and c is the
correct referent concept for m. To evaluate the
performance of a wikification system, we use the
standard precision, recall and F1 measures.
6.2 Experimental Results
The overall performance of various approaches
is shown in Table 2. The results of the super-
vised method proposed by (Meij et al, 2012) are
obtained from 5-fold cross validation. For our
semi-supervised setting, we experimentally sam-
ple 200 tweets for training and use the remain-
ing set as unlabeled and testing sets. In our semi-
supervised regularization model, the matrix W
loc
is constructed by a kNN graph (k = 20). The reg-
ularization parameter ? is empirically set to 0.1,
and the coefficients ?, ?, and ? are learnt from the
development set by considering all the combina-
385
Methods Descriptions
TagMe The same approach that is described in (Ferragina and Scaiella, 2010), which aims to annotate short
texts based on prior popularity and semantic relatedness of concepts. It is basically an unsupervised
approach, except that it needs a development set to tune the probability threshold for linkable mentions.
Meij A state-of-the-art system described in (Meij et al, 2012), which is a supervised approach based on the
random forest model. It performs mention detection and disambiguation jointly, and it is trained from
400 labeled tweets.
SSRegu
1
Our proposed model based on Principle 1, using 200 labeled tweets.
SSRegu
12
Our proposed model based on Principle 1 and 2, using 200 labeled tweets.
SSRegu
13
Our proposed model based on Principle 1 and 3, using 200 labeled tweets.
SSRegu
123
Our proposed full model based on Principle 1, 2 and 3, using 200 labeled tweets.
Table 1: Description of Methods.
Methods Precision Recall F1
TagMe 0.329 0.423 0.370
Meij 0.393 0.598 0.475
SSRegu
1
0.538 0.435 0.481
SSRegu
12
0.638 0.438 0.520
SSRegu
13
0.541 0.457 0.495
SSRegu
123
0.650 0.441 0.525
Table 2: Overall Performance.
tions of values from 0 to 1 at 0.1 intervals
1
. In
order to randomize the experiments and make the
comparison fair, we conduct 20 test runs for each
method and report the average scores across the 20
trials.
The relatively low performance of the baseline
system TagMe demonstrates that only relying on
prior popularity and topical information within a
single tweet is not enough for an end-to-end wik-
ification system for the short tweets. As an exam-
ple, it is difficult to obtain topical clues in order
to link the mention ?Clinton? to Hillary Rodham
Clinton by relying on the single tweet ?wolfblitzer-
cnn: Behind the scenes on Clinton?s Mideast trip
#cnn?. Therefore, the system mistakenly links it
to the most popular concept Bill Clinton.
In comparision with the supervised baseline
proposed by (Meij et al, 2012), our model
SSRegu
1
relying on local compatibility already
achieves comparable performance with 50% of
labeled data. This is because that our model
performs collective inference by making use of
the manifold (cluster) structure of both labeled
and unlabeled data, and that the local compat-
ibility relation is detected with high precision
2
(89.4%). For example, the following three pairs
of mentions and concepts ?pelosi, Nancy Pelosi?,
?obama, Barack Obama?, and ?gaddafi, Muam-
1
These three coefficients are slightly different with differ-
ent training data, a sample of them is: ? = 0.4, ? = 0.5, and
? = 0.1
2
Here we define precision as the percentage of links that
holds the same label.
mar Gaddafi? have strong local compatibility with
each other since they share many similar char-
acteristics captured by the local features such as
string similarity between the mention and the con-
cept. Suppose the first pair is labeled, then its pos-
itive label will be propagated to other unlabeled
nodes through the local compatibility relation, and
correctly predict the labels of other nodes.
Incorporating coreferential or semantic related-
ness relation into SSRegu
1
provides further gains,
demonstrating the effectiveness of these two re-
lations. For instance, ?wh? is correctly linked to
White House by incorporating evidence from its
coreferential mention ?white house?. The corefer-
ential relation (Principle 2) is demonstrated to be
more beneficial than the semantic relatedness re-
lation (Principle 3) because the former is detected
with much higher precision (99.7%) than the latter
(65.4%).
Our full model SSRegu
123
achieves significant
improvement over the supervised baseline (5% ab-
solute F1 gain with 95.0% confidence level by
the Wilcoxon Matched-Pairs Signed-Ranks Test),
showing that incorporating global evidence from
multiple tweets with fine-grained relations is ben-
eficial. For instance, the supervised baseline fails
to link ?UCONN? and ?Bucks? in our examples
to Connecticut Huskies and Milwaukee Bucks, re-
spectively. Our full model corrects these two
wrong links by propagating evidence through the
semantic links as shown in Figure 4 to obtain mu-
tual ranking improvement. The best performance
of our full model also illustrates that the three re-
lations complement each other.
We also study the disambiguation performance
for the annotated mentions, as shown in Table 3.
We can easily see that our proposed approach
using 50% labeled data achieves similar perfor-
mance with the state-of-the-art supervised model
with 100% labeled data. When the mentions are
given, the unpervised approach TagMe has already
386
Methods TagMe Meij SSRegu
123
Accuracy 0.710 0.779 0.772
Table 3: Disambiguation Performance.
Methods Precision Recall F1
SSRegu
12
0.644 0.423 0.510
SSRegu
13
0.543 0.441 0.486
SSRegu
123
0.657 0.419 0.512
Table 4: The Performance of Systems Without Us-
ing Concatenated Meta Paths.
achieved reasonable performance. In fact, mention
detection actually is the performance bottleneck of
a tweet wikification system (Guo et al, 2013). Our
system performs better in identifying the promi-
nent mention.
6.3 Effect of Concatenated Meta Paths
In this work, we propose a unified framework uti-
lizing meta path-based semantic relations to ex-
plore richer relevant context. Beyond the basic
meta paths, we introduce concatenated ones by
concatenating the basic ones. The performance of
the system without using the concatenated meta
paths is shown in Table 4. In comparison with
the system based on all defined meta paths, we
can clearly see that the systems using concate-
nated ones outperform those relying on the sim-
ple ones. This is because the concatenated meta
paths can incorporate more relevant information
with implicit relations into the models by increas-
ing 1.6% coreference links and 9.3% semantic re-
latedness links. For example, the mention ?narita
airport? is correctly disambiguated to the concept
?Narita International Airport? with higher confi-
dence since its semantic relatedness relation with
?Japan? is detected with the concatenated meta
path as described in section 4.2.
6.4 Effect of Labeled Data Size
5 0 1 0 0 1 5 0 2 0 0 2 5 0 3 0 0 3 5 0 4 0 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 L a b e l e d  T w e e t  S i z e S S R e g u 1 2 3 M e i j
Figure 5: The effect of Labeled Tweet Size.
In previous experiments, we experimentally set
the number of labeled tweets to be 200 for over-
all performance comparision with the baselines.
In this subsection, we study the effect of labeled
data size on our full model. We randomly sam-
ple 100 tweets as testing data, and randomly se-
lect 50, 100, 150, 200, 250, and 300 tweets as
labeled data. 20 test runs are conducted and the
average results are reported across the 20 trials,
as shown in Figure 5. We find that as the size
of the labeled data increases, our proposed model
achieves better performance. It is encouraging to
see that our approach, with only 31.3% labeled
tweets (125 out of 400), already achieves a perfor-
mance that is comparable to the state-of-the-art su-
pervised model trained from 100% labeled tweets.
6.5 Parameter Analysis
0 . 1 0 . 5 1 2 5 1 0 2 0 3 0 4 0 5 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 R e g u l a r i z a t i o n  P a r a m e t e r  ? S S R e g u 1 2 3
Figure 6: The effect of parameter ?.
In previous experiments, we empirically set the
parameter ? = 0.1. ? is the regularization pa-
rameter that controls the trade-off between initial
labels and the consistency of labels on the graph.
When ? increases, the model tends to trust more in
the initial labels. Figure 6 shows the performance
of our models by varying ? from 0.02 to 50. We
can easily see that the system performce is stable
when ? < 0.4. However, when ? ? 0.4, the sys-
tem performance dramatically decreases, showing
that prior popularity is not enough for an end-to-
end wikification system.
7 Related Work
The task of linking concept mentions to a knowl-
edge base has received increased attentions over
the past several years, from the linking of concept
mentions in a single text (Mihalcea and Csomai,
2007; Milne and Witten, 2008b; Milne and Witten,
2008a; Kulkarni et al, 2009; He et al, 2011; Rati-
nov et al, 2011; Cassidy et al, 2012; Cheng and
Roth, 2013), to the linking of a cluster of corefer-
387
ent named entity mentions spread throughout dif-
ferent documents (Entity Linking) (McNamee and
Dang, 2009; Ji et al, 2010; Zhang et al, 2010; Ji et
al., 2011; Zhang et al, 2011; Han and Sun, 2011;
Han et al, 2011; Gottipati and Jiang, 2011; He et
al., 2013; Li et al, 2013; Guo et al, 2013; Shen et
al., 2013; Liu et al, 2013).
A significant portion of recent work considers
the two sub-problems mention detection and men-
tion disambiguation separately and focus on the
latter by first defining candidate concepts for a
deemed mention based on anchor links. Men-
tion disambiguation is then formulated as a rank-
ing problem, either by resolving one mention at
each time (non-collective approaches), or by dis-
ambiguating a set of relevant mentions simulta-
neously (collective approaches). Non-collective
methods usually rely on prior popularity and con-
text similarity with supervised models (Mihalcea
and Csomai, 2007; Milne and Witten, 2008b; Han
and Sun, 2011), while collective approaches fur-
ther leverage the global coherence between con-
cepts normally through supervised or graph-based
re-ranking models (Cucerzan, 2007; Milne and
Witten, 2008b; Han and Zhao, 2009; Kulkarni et
al., 2009; Pennacchiotti and Pantel, 2009; Ferrag-
ina and Scaiella, 2010; Fernandez et al, 2010;
Radford et al, 2010; Cucerzan, 2011; Guo et al,
2011; Han and Sun, 2011; Han et al, 2011; Rati-
nov et al, 2011; Chen and Ji, 2011; Kozareva et
al., 2011; Cassidy et al, 2012; Shen et al, 2013;
Liu et al, 2013). Especially note that when apply-
ing the collective methods to short messages from
social media, evidence from other messages usu-
ally needs to be considered (Cassidy et al, 2012;
Shen et al, 2013; Liu et al, 2013). Our method
is a collective approach with the following novel
advancements: (i) A novel graph representation
with fine-grained relations, (ii) A unified frame-
work based on meta paths to explore richer rele-
vant context, (iii) Joint identification and linking
of mentions under semi-supervised setting.
Two most similar methods to ours were pro-
posed by (Meij et al, 2012; Guo et al, 2013)
by performing joint detection and disambiguation
of mentions. (Meij et al, 2012) studied several
supervised machine learning models, but without
considering any global evidence either from a sin-
gle tweet or other relevant tweets. (Guo et al,
2013) explored second order entity-to-entity rela-
tions but did not incorporate evidence from multi-
ple tweets.
This work is also related to graph-based semi-
supervised learning (Zhu et al, 2003; Smola
and Kondor, 2003; Zhou et al, 2004; Talukdar
and Crammer, 2009), which has been success-
fully applied in many Natural Language Process-
ing tasks (Niu et al, 2005; Chen et al, 2006).
We introduce a novel graph that incorporates three
fine-grained relations. Our work is further re-
lated to meta path-based heterogeneous informa-
tion network analysis (Sun et al, 2011b; Sun et
al., 2011a; Kong et al, 2012; Huang et al, 2013),
which has demonstrated advantages over homoge-
neous information network analysis without dif-
ferentiating object types and relational links.
8 Conclusions
We have introduced a novel semi-supervised graph
regularization framework for wikification to si-
multaneously tackle the unique challenges of an-
notation and information shortage in short tweets.
To the best of our knowledge, this is the first work
to explore the semi-supervised collective inference
model to jointly perform mention detection and
disambiguation. By studying three novel fine-
grained relations, detecting semantically-related
information with semantic meta paths, and ex-
ploiting the data manifolds in both unlabeled and
labeled data for collective inference, our work can
dramatically save annotation cost and achieve bet-
ter performance, thus shed light on the challenging
wikification task for tweets.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149, U.S.
DARPA Award No. FA8750-13-2-0041 in the
Deep Exploration and Filtering of Text (DEFT)
Program, IBM Faculty Award, Google Research
Award and RPI faculty start-up grant. The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
388
References
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2004. Semi-supervised learning using randomized
mincuts. In Proceedings of the Twenty-first Interna-
tional Conference on Machine Learning, ICML ?04.
Razvan Bunescu. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In EACL,
pages 9?16.
T. Cassidy, H. Ji, L. Ratinov, A. Zubiaga, and
H. Huang. 2012. Analysis and enhancement of wik-
ification for microblogs with context expansion. In
Proceedings of COLING 2012.
Z. Chen and H. Ji. 2011. Collaborative ranking: A
case study on entity linking. In Proc. EMNLP2011.
J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Rela-
tion extraction using label propagation based semi-
supervised learning. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
X. Cheng and D. Roth. 2013. Relational inference
for wikification. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007.
S. Cucerzan. 2011. Tac entity linking by performing
full-document entity extraction and disambiguation.
In Proc. TAC 2011 Workshop.
N. Fernandez, J. A. Fisteus, L. Sanchez, and E. Mar-
tin. 2010. Webtlab: A cooccurence-based approach
to kbp 2010 entity-linking task. In Proc. TAC 2010
Workshop.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10.
S. Gottipati and J. Jiang. 2011. Linking entities to a
knowledge base with query expansion. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing.
Y. Guo, W. Che, T. Liu, and S. Li. 2011. A graph-
based method for entity linking. In Proc. IJC-
NLP2011.
S. Guo, M. Chang, and E. Kiciman. 2013. To link
or not to link? a study on end-to-end tweet entity
linking. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
B. Hachey, W. Radford, J. Nothman, M. Honnibal, and
J. Curran. 2013. Evaluating entity linking with
wikipedia. Artif. Intell.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proc. ACL2011.
X. Han and J. Zhao. 2009. Named entity disam-
biguation by leveraging wikipedia semantic knowl-
edge. In Proceedings of the 18th ACM conference
on Information and knowledge management, CIKM
2009.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: A graph-based method. In Proc.
SIGIR2011.
J. He, M. de Rijke, M. Sevenster, R. van Ommering,
and Y. Qian. 2011. Generating links to background
knowledge: A case study using narrative radiology
reports. In Proceedings of the 20th ACM inter-
national conference on Information and knowledge
management. ACM.
Z. He, S. Liu, Y. Song, M. Li, M. Zhou, and H. Wang.
2013. Efficient collective entity linking with stack-
ing. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing.
H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and
H. Li. 2013. Resolving entity morphs in censored
data. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
X. Kong, P. Yu, Y. Ding, and J. Wild. 2012. Meta
path-based collective classification in heterogeneous
information networks. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ?12.
Z. Kozareva, K. Voevodski, and S. Teng. 2011. Class
label enhancement via related instances. In Proc.
EMNLP2011.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Y. Li, C. Wang, F. Han, J. Han, D. Roth, and X. Yan.
2013. Mining evidences for named entity dis-
ambiguation. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?13.
389
X. Liu, Y. Li, H. Wu, M. Zhou, F. Wei, and Y. Lu.
2013. Entity linking for tweets. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC) 2009.
E. Meij, W. Weerkamp, and M. de Rijke. 2012.
Adding semantics to microblog posts. In Proceed-
ings of the fifth ACM international conference on
Web search and data mining, WSDM ?12.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking
documents to encyclopedic knowledge. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07.
D. Milne and I.H. Witten. 2008a. Learning to link
with wikipedia. In An effective, low-cost measure of
semantic relatedness obtained from wikipedia links.
the Wikipedia and AI Workshop of AAAI.
D. Milne and I.H. Witten. 2008b. Learning to link
with wikipedia. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
Z. Niu, D. Ji, and C. Tan. 2005. Word sense dis-
ambiguation using label propagation based semi-
supervised learning. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
M. Pennacchiotti and P. Pantel. 2009. Entity extraction
via ensemble semantics. In Proc. EMNLP2009.
W. Radford, B. Hachey, J. Nothman, M. Honnibal, and
J. R. Curran. 2010. Cmcrc at tac10: Document-
level entity linking with graph-based re-ranking. In
Proc. TAC 2010 Workshop.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proc. of the Annual Meeting of
the Association of Computational Linguistics (ACL).
W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base
via user interest modeling. In Proceedings of the
19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?13.
A. Smola and R. Kondor. 2003. Kernels and regular-
ization on graphs. COLT.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han.
2011a. Co-author relationship prediction in hetero-
geneous bibliographic networks. In Proceedings of
the 2011 International Conference on Advances in
Social Networks Analysis and Mining, ASONAM
?11.
Y. Sun, J. Han, X. Yan, P. Yu, and T. Wu. 2011b. Path-
sim: Meta path-based top-k similarity search in het-
erogeneous information networks. PVLDB, 4(11).
P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proceed-
ings of the European Conference on Machine Learn-
ing and Knowledge Discovery in Databases: Part II,
ECML PKDD ?09.
D. Vitale, P. Ferragina, and U. Scaiella. 2012. Clas-
sification of short texts by deploying topical annota-
tions. In ECIR, pages 376?387.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale
nlp: A case study on url word breaking. In Proceed-
ings of the 20th International Conference on World
Wide Web, WWW ?11.
W. Zhang, J. Su, C. Tan, and W. Wang. 2010. En-
tity linking leveraging automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010).
W. Zhang, J. Su, and C. L. Tan. 2011. A wikipedia-lda
model for entity linking with batch size changing. In
Proc. IJCNLP2011.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and
B. Sch?olkopf. 2004. Learning with local and global
consistency. In Advances in Neural Information
Processing Systems 16.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In ICML.
390
