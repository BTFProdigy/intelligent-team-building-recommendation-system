Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 47?53,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Reversing Morphological Tokenization in English-to-Arabic SMT
Mohammad Salameh? Colin Cherry? Grzegorz Kondrak?
?Department of Computing Science ?National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Morphological tokenization has been used
in machine translation for morphologically
complex languages to reduce lexical sparsity.
Unfortunately, when translating into a mor-
phologically complex language, recombining
segmented tokens to generate original word
forms is not a trivial task, due to morpho-
logical, phonological and orthographic adjust-
ments that occur during tokenization. We re-
view a number of detokenization schemes for
Arabic, such as rule-based and table-based ap-
proaches and show their limitations. We then
propose a novel detokenization scheme that
uses a character-level discriminative string
transducer to predict the original form of a
segmented word. In a comparison to a state-
of-the-art approach, we demonstrate slightly
better detokenization error rates, without the
need for any hand-crafted rules. We also
demonstrate the effectiveness of our approach
in an English-to-Arabic translation task.
1 Introduction
Statistical machine translation (SMT) relies on to-
kenization to split sentences into meaningful units
for easy processing. For morphologically complex
languages, such as Arabic or Turkish, this may in-
volve splitting words into morphemes. Through-
out this paper, we adopt the definition of tokeniza-
tion proposed by Habash (2010), which incorpo-
rates both morphological segmentation as well as
orthographic character transformations. To use an
English example, the word tries would be morpho-
logically tokenized as ?try + s?, which involves
orthographic changes at morpheme boundaries to
match the lexical form of each token. When trans-
lating into a tokenized language, the tokenization
must be reversed to make the generated text read-
able and evaluable. Detokenization is the process
of converting tokenized words into their original or-
thographically and morphologically correct surface
form. This includes concatenating tokens into com-
plete words and reversing any character transforma-
tions that may have taken place.
For languages like Arabic, tokenization can facil-
itate SMT by reducing lexical sparsity. Figure 1
shows how the morphological tokenization of the
Arabic word ???	J?J
?? ?and he will prevent them?
simplifies the correspondence between Arabic and
English tokens, which in turn can improve the qual-
ity of word alignment, rule extraction and decoding.
When translating from Arabic into English, the to-
kenization is a form of preprocessing, and the out-
put translation is readable, space-separated English.
However, when translating from English to Arabic,
the output will be in a tokenized form, which cannot
be compared to the original reference without detok-
enization. Simply concatenating the tokenized mor-
phemes cannot fully reverse this process, because of
character transformations that occurred during tok-
enization.
The techniques that have been proposed for the
detokenization task fall into three categories (Badr
et al, 2008). The simplest detokenization approach
concatenates morphemes based on token markers
without any adjustment. Table-based detokenization
maps tokenized words into their surface form with a
look-up table built by observing the tokenizer?s in-
47
Figure 1: Alignment between tokenized form of
?wsymn?hm? ???	J?J
?? and its English translation.
put and output on large amounts of text. Rule-based
detokenization relies on hand-built rules or regular
expressions to convert the segmented form into the
original surface form. Other techniques use combi-
nations of these approaches. Each approach has its
limitations: rule-based approaches are language spe-
cific and brittle, while table-based approaches fail to
deal with sequences outside of their tables.
We present a new detokenization approach that
applies a discriminative sequence model to predict
the original form of the tokenized word. Like
table-based approaches, our sequence model re-
quires large amounts of tokenizer input-output pairs;
but instead of building a table, we use these pairs
as training data. By using features that consider
large windows of within-word input context, we are
able to intelligently transition between rule-like and
table-like behavior.
Our experimental results on Arabic text demon-
strate an improvement in terms of sentence error
rate1 of 11.9 points over a rule-based approach, and
1.1 points over a table-based approach that backs
off to rules. More importantly, we achieve a slight
improvement over the state-of-the-art approach of
El Kholy and Habash (2012), which combines rules
and tables, using a 5-gram language model to dis-
ambiguate conflicting table entries. In addition, our
detokenization method results in a small BLEU im-
provement over a rule-based approach when applied
to English-to-Arabic SMT.
1Sentence error rate is the percentage of sentences contain-
ing at least one error after detokenization.
2 Arabic Morphology
Compared to English, Arabic has rich and complex
morphology. Arabic base words inflect to eight fea-
tures. Verbs inflect for aspect, mood, person and
voice. Nouns and adjectives inflect for case and
state. Verbs, nouns and adjectives inflect for both
gender and number. Furthermore, inflected base
words can attract various optional clitics. Clitical
prefixes include determiners, particle proclitics, con-
junctions and question particles in strict order. Clit-
ical suffixes include pronominal modifiers. As a re-
sult of clitic attachment, morpho-syntactic interac-
tions sometimes cause changes in spelling or pro-
nunciations.
Several tokenization schemes can be defined for
Arabic, depending on the clitical level that the to-
kenization is applied to. In this paper, we use
Penn Arabic Treebank (PATB) tokenization scheme,
which El Kholy and Habash (2012) report as pro-
ducing the best results for Arabic SMT. The PATB
scheme detaches all clitics except for the definite ar-
ticle Al ?@. Multiple prefix clitics are treated as one
token.
Some Arabic letters present further ambiguity in
text.2 For example, the different forms of Hamzated
Alif ?@ @? are usually written without the Hamza ?Z?.
Likewise, when the letter Ya ?Y? ?
 is present at theend of the word, it is sometimes written in the form
of ?Alif Maqsura? letter ??? ?. Also, short vow-
els in Arabic are represented using diacritics, which
are usually absent in written text. In order to deal
with these ambiguities in SMT, normalization is of-
ten performed as a preprocessing step, which usu-
ally involves converting different forms of Alif and
Ya to a single form. This decreases Arabic?s lexical
sparsity and improves SMT performance.
3 Related Work
Sadat and Habash (2006) address the issue of lex-
ical sparsity by presenting different preprocessing
schemes for Arabic-to-English SMT. The schemes
include simple tokenization, orthographic normal-
ization, and decliticization. The combination of
these schemes results in improved translation out-
2We use Habash-Soudi-Buckwalter transliteration scheme
(Habash, 2007) for all Arabic examples.
48
put. This is one of many studies on normalization
and tokenization for translation from Arabic, which
we will not attempt to review completely here.
Badr et al (2008) show that tokenizing Arabic
also has a positive influence on English-to-Arabic
SMT. They apply two tokenization schemes on
Arabic text, and introduce detokenization schemes
through a rule-based approach, a table-based ap-
proach, and a combination of both. The combina-
tion approach detokenizes words first using the ta-
ble, falling back on rules for sequences not found in
the table.
El Kholy and Habash (2012) extend Badr?s work
by presenting a larger number of tokenization and
detokenization schemes, and comparing their effects
on SMT. They introduce an additional detokeniza-
tion schemes based on the SRILM disambig util-
ity (Stolcke, 2002), which utilizes a 5-gram untok-
enized language model to decide among different al-
ternatives found in the table. They test their schemes
on naturally occurring Arabic text and SMT output.
Their newly introduced detokenization scheme out-
performs the rule-based and table-based approaches
introduced by Badr et al (2008), establishing the
current state-of-the-art.
3.1 Detokenization Schemes in Detail
Rule-based detokenization involves manually defin-
ing a set of transformation rules to convert a se-
quence of segmented tokens into their surface form.
For example, the noun ?llry?ys? ?
KQ?? ?to the pres-
ident? is tokenized as ?l+ Alry?ys? ( l+ ?to? Alry?ys
?the president?) in the PATB tokenization scheme.
Note that the definite article ?Al? ?@ is kept attached
to the noun. In this case, detokenization requires
a character-level transformation after concatenation,
which we can generalize using the rule:
l+Al ? ll.
Table 1 shows the rules provided by El Kholy and
Habash (2012), which we employ throughout this
paper.
There are two principal problems with the rule-
based approach. First, rules fail to account for un-
usual cases. For example, the above rule mishandles
cases where ?Al? ?@ is a basic part of the stem and
not the definite article ?the?. Thus, ?l+ Al?Ab? (l+
?to? Al?Ab ?games?) is erroneously detokenized to
Rule Input Output
l+Al+l? ? ll l+ Alry?ys llry?ys
?+(pron) ? t(pron) Abn?+hA AbnthA
y+(pron) ? A(pron) Alqy+h AlqAh
?+(pron) ? y? AntmA?+hm AntmAy?hm
y+y ? y ?yny+y ?yny
n+n ? n mn+nA mnA
mn+m ? mm mn+mA mmA
?n+m ? ?m ?n+mA ?mA
An+lA ? AlA An+lA AlA
Table 1: Detokenization rules of El Kholy and Habash
(2012), with examples. pron stands for pronominal clitic.
llEAb H. A??? instead of the correct form is ?lAl?Ab?
H. A??B. Second, rules may fail to handle sequences
produced by tokenization errors. For example, the
word ?bslT?? ????. ?with power? can be erro-
neously tokenized as ?b+slT+h?, while the correct
tokenizations is ?b+slT??. The erroneous tokeniza-
tion will be incorrectly detokenized as ?bslTh?.
The table-based approach memorizes mappings
between words and their tokenized form. Such a
table is easily constructed by running the tokenizer
on a large amount of Arabic text, and observing the
input and output. The detokenization process con-
sults this table to retrieve surface forms of tokenized
words. In the case where a tokenized word has sev-
eral observed surface forms, the most frequent form
is selected. This approach fails when the sequence
of tokenized words is not in the table. In morpholog-
ically complex languages like Arabic, an inflected
base word can attrract many optional clitics, and ta-
bles may not include all different forms and inflec-
tions of a word.
The SRILM-disambig scheme introduced by
El Kholy and Habash (2012) extends the table-based
approach to use an untokenized Arabic language
model to disambiguate among the different alter-
natives. Hence, this scheme can make context-
dependent detokenization decisions, rather than al-
ways producing the most frequent surface form.
Both the SRILM-disambig scheme and the table-
based scheme have the option to fall back on either
rules or simple concatenation for sequences missing
from the table.
49
4 Detokenization as String Transduction
We propose to approach detokenization as a string
transduction task. We train a discriminative trans-
ducer on a set of tokenized-detokenized word pairs.
The set of pairs is initially aligned on the charac-
ter level, and the alignment pairs become the opera-
tions that are applied during transduction. For deto-
kenization, most operations simply copy over char-
acters, but more complex rules such as l+ Al ? ll
are learned from the training data as well.
The tool that we use to perform the transduction is
DIRECTL+, a discriminative, character-level string
transducer, which was originally designed for letter-
to-phoneme conversion (Jiampojamarn et al, 2008).
To align the characters in each training example.
DIRECTL+ uses an EM-based M2M-ALIGNER (Ji-
ampojamarn et al, 2007). After alignment is com-
plete, MIRA training repeatedly decodes the train-
ing set to tune the features that determine when each
operation should be applied. The features include
both n-gram source context and HMM-style target
transitions. DIRECTL+ employs a fully discrimina-
tive decoder to learn character transformations and
when they should be applied. The decoder resem-
bles a monotone phrase-based SMT decoder, but is
built to allow for hundreds of thousands of features.
The following example illustrates how string
transduction applies to detokenization. The seg-
mented and surface forms of bbrA?thm ?? D?@Q. K.
?with their skill? constitute a training instance:
b+_brA??_+hm ? bbrA?thm
The instance is aligned during the training phase as:
b+ _b r A ? ?_ + h m
| | | | | | | | |
b b r A ? t  h m
The underscore ?_? indicates a space, while ?? de-
notes an empty string. The following operations are
extracted from the alignment:
b+ ? b, _b ? b, r ? r, A ? A, E ? E, p_ ? t,
+ ? , h ? h, m ? m
During training, weights are assigned to features that
associate operations with context. In our running ex-
ample, the weight assigned to the b+ ? b operation
accounts for the operation itself, for the fact that the
operation appears at the beginning of a word, and for
the fact that it is followed by an underscore; in fact,
we employ a context window of 5 characters to the
left or right of the source substring ?b+?, creating a
feature for each n-gram within that window.
Modeling the tokenization problem as string
transduction has several advantages. The approach
is completely language-independent. The context-
sensitive rules are learned automatically from ex-
amples, without human intervention. The rules
and features can be represented in a more com-
pact way than the full mapping table required by
table-based approaches, while still elegantly han-
dling words that were not seen during training.
Also, since the training data is generalized more
efficiently than in simple memorization of com-
plete tokenized-detokenized pairs, less training data
should be needed to achieve good accuracy.
5 Experiments
This section presents two experiments that evaluate
the effect of the detokenization schemes on both nat-
urally occurring Arabic and SMT output.
5.1 Data
To build our data-driven detokenizers, we use the
Arabic part of 4 Arabic-English parallel datasets
from the Linguistic Data Consortium as train-
ing data. The data sets are: Arabic News
(LDC2004T17), eTIRR (LDC2004E72), English
translation of Arabic Treebank (LDC2005E46), and
Ummah (LDC2004T18). The training data has
107K sentences. The Arabic part of the training data
constitutes around 2.8 million words, 3.3 million to-
kens after tokenization, and 122K word types after
filtering punctuation marks, Latin words and num-
bers (refer to Table 2 for detailed counts).
For training the SMT system?s translation and re-
ordering models, we use the same 4 datasets from
LDC. We also use 200 Million words from LDC
Arabic Gigaword corpus (LDC2011T11) to gener-
ate a 5-gram language model using SRILM toolkit
(Stolcke, 2002).
We use NIST MT 2004 evaluation set for tun-
ing (1075 sentences), and NIST MT 2005 evalua-
tions set for testing (1056 sentences). Both MT04
and MT05 have multiple English references in or-
der to evaluate Arabic-to-English translation. As we
are translating into Arabic, we take the first English
50
Data set Before After
training set 122,720 61,943
MT04 8,201 2,542
MT05 7,719 2,429
Table 2: Type counts before and after tokenization.
translation to be our source in each case. We also
use the Arabic halves of MT04 and MT05 as devel-
opment and test sets for our experiments on natu-
rally occurring Arabic. The tokenized Arabic is our
input, with the original Arabic as our gold-standard
detokenization.
The Arabic text of the training, development, test-
ing set and language model are all tokenized using
MADA 3.2 (Habash et al, 2009) with the Penn Ara-
bic Treebank tokenization scheme. The English text
in the parallel corpus is lower-cased and tokenized
in the traditional sense to strip punctuation marks.
5.2 Experimental Setup
To train the detokenization systems, we generate a
table of mappings from tokenized forms to surface
forms based on the Arabic part of our 4 parallel
datasets, giving us complete coverage of the out-
put vocabulary of our SMT system. In the table-
based approaches, if a tokenized form is mapped to
more than one surface form, we use the most fre-
quent surface form. For out-of-table words, we fall
back on concatenation (in T) or rules (in T+R). For
SRILM-Disambig detokenization, we maintain am-
biguous table entries along with their frequencies,
and we introduce a 5-gram language model to dis-
ambiguate detokenization choices in context. Like
the table-based approaches, the Disambig approach
can back off to either simple concatenation (T+LM)
or rules (T+R+LM) for missing entries. The latter
is a re-implementation of the state-of-the-art system
presented by El Kholy and Habash (2012).
We train our discriminative string transducer us-
ing word types from the 4 LDC catalogs. We
use M2M-ALIGNER to generate a 2-to-1 charac-
ter alignments between tokenized forms and surface
forms. For the decoder, we set Markov order to one,
joint n-gram features to 5, n-gram size to 11, and
context size to 5. This means the decoder can uti-
lize contexts up to 11 characters long, allowing it to
Detokenization WER SER BLEU
Baseline 1.710 34.3 26.30
Rules (R) 0.590 14.0 28.32
Table (T) 0.192 4.9 28.54
Table + Rules (T+R) 0.122 3.2 28.55
Disambig (T+LM) 0.164 4.1 28.53
Disambig (T+R+LM) 0.094 2.4 28.54
DIRECTL+ 0.087 2.1 28.55
Table 3: Word and sentence error rate of detokenization
schemes on the Arabic reference text of NIST MT05.
BLEU score refers to English-Arabic SMT output.
effectively memorize many words. We found these
settings using grid search on the development set,
NIST MT04.
For the SMT experiment, we use GIZA++ for
the alignment between English and tokenized Ara-
bic, and perform the translation using Moses phrase-
based SMT system (Hoang et al, 2007), with a max-
imum phrase length of 5. We apply each detokeniza-
tion scheme on the SMT tokenized Arabic output
test set, and evaluate using the BLEU score (Pap-
ineni et al, 2002).
5.3 Results
Table 3 shows the performance of several detok-
enization schemes. For evaluation, we use the sen-
tence and word error rates on naturally occurring
Arabic text, and BLEU score on tokenized Arabic
output of the SMT system. The baseline scheme,
which is a simple concatenation of morphemes, in-
troduces errors in over a third of all sentences. The
table-based approach outperforms the rule-based ap-
proach, indicating that there are frequent excep-
tions to the rules in Table 1 that require memoriza-
tion. Their combination (T+R) fares better, lever-
aging the strengths of both approaches. The addi-
tion of SRILM-Disambig produces further improve-
ments as it uses a language model context to disam-
biguate the correct detokenized word form. Our sys-
tem outperforms SRILM-Disambig by a very slight
margin, indicating that the two systems are roughly
equal. This is interesting, as it is able to do so by
using only features derived from the tokenized word
itself; unlike SRILM-Disambig, it has no access to
the surrounding words to inform its decisions. In ad-
51
dition, it is able to achieve this level of performance
without any manually constructed rules.
Improvements in detokenization do contribute to
the BLEU score of our SMT system, but only to
a point. Table 3 shows three tiers of performance,
with no detokenization being the worst, the rules be-
ing better, and the various data-driven approaches
performing best. After WER dips below 0.2, further
improvements seem to no longer affect SMT quality.
Note that BLEU scores are much lower overall than
one would expect for the translation in the reverse
direction, because of the morphological complexity
of Arabic, and the use of one (as opposed to four)
references for evaluation.
5.4 Analysis
The sentence error rate of 2.1 represents only 21
errors that our approach makes. Among those 21,
11 errors are caused by changing p to h and vice
versa. This is due to writing p and h interchange-
ably. For example, ?AjmAly+h? was detokenized
as ?AjmAly?? ?J
?A?g. @ instead of ?AjmAlyh? ?J
?A?g. @.Another 4 errors are caused by the lack of dia-
critization, which affects the choice of the Hamza
form. For example,?bnAw?h? ? ?A 	JK. , ?bnAy?h? ?KA 	JK.
and ?bnA?h? ?ZA 	JK. (?its building?) are 3 different
forms of the same word where the choice of Hamza
Z is dependent on its diacritical mark or the mark
of the character that precedes it. Another 3 errors
are attributed to the case of the nominal which it in-
flects for. The case is affected by the context of the
noun which DIRECTL+ has no access to. For ex-
ample, ?mfkry+hm? (?thinkers/Dual-Accusative?)
was detokenized as ?mfkrAhm? ??@Q? 	?? (Dual-
Nominative) instead of ?mfkryhm? ??E
Q? 	??. The
last 3 errors are special cases of ?An +y? which
can be detokenized correctly as either ?Any? ?

	G @ or
?Anny? ?

	? 	K @.
The table-based detokenization scheme fails in
54 cases. Among these instances, 44 cases are not
in the mapping table, hence resolving back to sim-
ple concatenation ended with an error. Our trans-
duction approach succeeds in detokenizing 42 cases
out of the 54. The majority of these cases involves
changing p to h and vice versa, and changing l+Al
to ll. The only 2 instances where the tokenized
word is in the mapping table but DIRECTL+ incor-
rectly detokenizes it are due to hamza case and p
to h case described above. There are 4 instances
of the same word/case where both the table scheme
and DIRECTL+ fails due to error of tokenization
by MADA, where the proper name qwh ??? is er-
roneously tokenized as qw+p. This shows that DI-
RECTL+ handles the OOV words correctly.
The Disambig(T+R+LM) erroneously detok-
enizes 27 instances, where 21 out of them are cor-
rectly tokenized by DIRECTL+. Most of the er-
rors are due to the Hamza and p to h reasons. It
seems that even with a large size language model,
the SRILM utility needs a large mapping table to
perform well. Only 4 instances were erroneously
detokenized by both Disambig and DIRECTL+ due
to Hamza and the case of the nominal.
The analysis shows that using small size training
data, DIRECTL+ can achieve slightly better accu-
racy than SRILM scheme. The limitations of using
table and rules are handled with DIRECTL+ as it is
able to memorize more rules.
6 Conclusion and Future Work
In this paper, we addressed the detokenization prob-
lem for Arabic using DIRECTL+, a discriminative
training model for string transduction. Our system
performs the best among the available systems. It
manages to solve problems caused by limitations of
table-based and rule-based systems. This allows us
to match the performance of the SRILM-disambig
approach without using a language model or hand-
crafted rules. In the future, we plan to test our ap-
proach on other languages that have morphological
characteristics similar to Arabic.
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL, pages 153?156.
Ahmed El Kholy and Nizar Habash. 2012. Orthographic
and morphological processing for English-Arabic sta-
tistical machine translation. Machine Translation,
26(1-2):25?45, March.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, dia-
critization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
52
the Second International Conference on Arabic Lan-
guage Resources and Tools.
Nizar Habash. 2007. Arabic morphological represen-
tations for machine translation. In Arabic Computa-
tional Morphology: Knowledge-based and Empirical
Methods.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Synthesis Lectures on Human
Language Technologies. Morgan & Claypool Publish-
ers.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
drej Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, pages 177?180.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings of ACL-08: HLT, pages 905?913.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Fatiha Sadat and Nizar Habash. 2006. Combination of
Arabic preprocessing schemes for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 1?8.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, pages 901?904.
53
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100?110,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Lattice Desegmentation for Statistical Machine Translation
Mohammad Salameh
?
Colin Cherry
?
Grzegorz Kondrak
?
?
Department of Computing Science
?
National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Morphological segmentation is an effec-
tive sparsity reduction strategy for statis-
tical machine translation (SMT) involv-
ing morphologically complex languages.
When translating into a segmented lan-
guage, an extra step is required to deseg-
ment the output; previous studies have de-
segmented the 1-best output from the de-
coder. In this paper, we expand our trans-
lation options by desegmenting n-best lists
or lattices. Our novel lattice desegmenta-
tion algorithm effectively combines both
segmented and desegmented views of the
target language for a large subspace of
possible translation outputs, which allows
for inclusion of features related to the de-
segmentation process, as well as an un-
segmented language model (LM). We in-
vestigate this technique in the context of
English-to-Arabic and English-to-Finnish
translation, showing significant improve-
ments in translation quality over deseg-
mentation of 1-best decoder outputs.
1 Introduction
Morphological segmentation is considered to be
indispensable when translating between English
and morphologically complex languages such as
Arabic. Morphological complexity leads to much
higher type to token ratios than English, which
can create sparsity problems during translation
model estimation. Morphological segmentation
addresses this issue by splitting surface forms into
meaningful morphemes, while also performing or-
thographic transformations to further reduce spar-
sity. For example, the Arabic noun ??Y?? lldwl
?to the countries? is segmented as l+ ?to? Aldwl
?the countries?. When translating from Arabic,
this segmentation process is performed as input
preprocessing and is otherwise transparent to the
translation system. However, when translating
into Arabic, the decoder produces segmented out-
put, which must be desegmented to produce read-
able text. For example, l+ Aldwl must be con-
verted to lldwl.
Desegmentation is typically performed as a
post-processing step that is independent from the
decoding process. While this division of labor is
useful, the pipeline approach may prevent the de-
segmenter from recovering from errors made by
the decoder. Despite the efforts of the decoder?s
various component models, the system may pro-
duce mismatching segments, such as s+ hzymp,
which pairs the future particle s+ ?will? with a
noun hzymp ?defeat?, instead of a verb. In this sce-
nario, there is no right desegmentation; the post-
processor has been dealt a losing hand.
In this work, we show that it is possible to
maintain the sparsity-reducing benefit of segmen-
tation while translating directly into unsegmented
text. We desegment a large set of possible de-
coder outputs by processing n-best lists or lat-
tices, which allows us to consider both the seg-
mented and desegmented output before locking in
the decoder?s decision. We demonstrate that sig-
nificant improvements in translation quality can be
achieved by training a linear model to re-rank this
transformed translation space.
2 Related Work
Translating into morphologically complex lan-
guages is a challenging and interesting task that
has received much recent attention. Most tech-
niques approach the problem by transforming the
target language in some manner before training the
translation model. They differ in what transforma-
tions are performed and at what stage they are re-
versed. The transformation might take the form of
a morphological analysis or a morphological seg-
mentation.
100
2.1 Morphological Analysis
Many languages have access to morphological an-
alyzers, which annotate surface forms with their
lemmas and morphological features. Bojar (2007)
incorporates such analyses into a factored model,
to either include a language model over target mor-
phological tags, or model the generation of mor-
phological features. Other approaches train an
SMT system to predict lemmas instead of surface
forms, and then inflect the SMT output as a post-
processing step (Minkov et al, 2007; Clifton and
Sarkar, 2011; Fraser et al, 2012; El Kholy and
Habash, 2012b). Alternatively, one can reparame-
terize existing phrase tables as exponential mod-
els, so that translation probabilities account for
source context and morphological features (Jeong
et al, 2010; Subotin, 2011). Of these approaches,
ours is most similar to the translate-then-inflect ap-
proach, except we translate and then desegment.
In particular, Toutanova et al (2008) inflect and
re-rank n-best lists in a similar manner to how we
desegment and re-rank n-best lists or lattices.
2.2 Morphological Segmentation
Instead of producing an abstract feature layer,
morphological segmentation transforms the tar-
get sentence by segmenting relevant morphemes,
which are then handled as regular tokens during
alignment and translation. This is done to reduce
sparsity and to improve correspondence with the
source language (usually English). Such a seg-
mentation can be produced as a byproduct of anal-
ysis (Oflazer and Durgar El-Kahlout, 2007; Badr
et al, 2008; El Kholy and Habash, 2012a), or may
be produced using an unsupervised morphological
segmenter such as Morfessor (Luong et al, 2010;
Clifton and Sarkar, 2011). Work on target lan-
guage morphological segmentation for SMT can
be divided into three subproblems: segmentation,
desegmentation and integration. Our work is con-
cerned primarily with the integration problem, but
we will discuss each subproblem in turn.
The usefulness of a target segmentation de-
pends on its correspondence to the source lan-
guage. If a morphological feature does not man-
ifest itself as a separate token in the source, then
it may be best to leave its corresponding segment
attached to the stem. A number of studies have
looked into what granularity of segmentation is
best suited for a particular language pair (Oflazer
and Durgar El-Kahlout, 2007; Badr et al, 2008;
Clifton and Sarkar, 2011; El Kholy and Habash,
2012a). Since our focus here is on integrating seg-
mentation into the decoding process, we simply
adopt the segmentation strategies recommended
by previous work: the Penn Arabic Treebank
scheme for English-Arabic (El Kholy and Habash,
2012a), and an unsupervised scheme for English-
Finnish (Clifton and Sarkar, 2011).
Desegmentation is the process of converting
segmented words into their original surface form.
For many segmentations, especially unsupervised
ones, this amounts to simple concatenation. How-
ever, more complex segmentations, such as the
Arabic tokenization provided by MADA (Habash
et al, 2009), require further orthographic adjust-
ments to reverse normalizations performed dur-
ing segmentation. Badr et al (2008) present
two Arabic desegmentation schemes: table-based
and rule-based. El Kholy and Habash (2012a)
provide an extensive study on the influence of
segmentation and desegmentation on English-to-
Arabic SMT. They introduce an additional deseg-
mentation technique that augments the table-based
approach with an unsegmented language model.
Salameh et al (2013) replace rule-based deseg-
mentation with a discriminatively-trained char-
acter transducer. In this work, we adopt the
Table+Rules approach of El Kholy and Habash
(2012a) for English-Arabic, while concatenation
is sufficient for English-Finnish.
Work on integration attempts to improve SMT
performance for morphologically complex target
languages by going beyond simple pre- and post-
processing. Oflazer and Durgar El-Kahlout (2007)
desegment 1000-best lists for English-to-Turkish
translation to enable scoring with an unsegmented
language model. Unlike our work, they replace
the segmented language model with the unseg-
mented one, allowing them to tune the linear
model parameters by hand. We use both seg-
mented and unsegmented language models, and
tune automatically to optimize BLEU.
Like us, Luong et al (2010) tune on un-
segmented references,
1
and translate with both
segmented and unsegmented language models
for English-to-Finnish translation. However,
they adopt a scheme of word-boundary-aware
1
Tuning on unsegmented references does not require sub-
stantial modifications to the standard SMT pipeline. For ex-
ample, Badr et al (2008) also tune on unsegmented refer-
ences by simply desegmenting SMT output before MERT
collects sufficient statistics for BLEU.
101
morpheme-level phrase extraction, meaning that
target phrases include only complete words,
though those words are segmented into mor-
phemes. This enables full decoder integration,
where we do n-best and lattice re-ranking. But
it also comes at a substantial cost: when target
phrases include only complete words, the system
can only generate word forms that were seen dur-
ing training. In this setting, the sparsity reduc-
tion from segmentation helps word alignment and
target language modeling, but it does not result
in a more expressive translation model. Further-
more, it becomes substantially more difficult to
have non-adjacent source tokens contribute mor-
phemes to a single target word. For example,
when translating ?with his blue car? into the Ara-
bic ZA

P?
	
Q? @ ?

KPAJ


?
.
bsyArth AlzrqA?, the target word
bsyArth is composed of three tokens: b+ ?with?,
syArp ?car? and +h ?his?. With word-boundary-
aware phrase extraction, a phrase pair containing
all of ?with his blue car? must have been seen in
the parallel data to translate the phrase correctly at
test time. With lattice desegmentation, we need
only to have seen AlzrqA? ?blue? and the three
morphological pieces of bsyArth for the decoder
and desegmenter to assemble the phrase.
3 Methods
Our goal in this work is to benefit from
the sparsity-reducing properties of morphological
segmentation while simultaneously allowing the
system to reason about the final surface forms of
the target language. We approach this problem by
augmenting an SMT system built over target seg-
ments with features that reflect the desegmented
target words. In this section, we describe our vari-
ous strategies for desegmenting the SMT system?s
output space, along with the features that we add
to take advantage of this desegmented view.
3.1 Baselines
The two obvious baseline approaches each decode
using one view of the target language. The un-
segmented approach translates without segment-
ing the target. This trivially allows for an unseg-
mented language model and never makes deseg-
mentation errors. However, it suffers from data
sparsity and poor token-to-token correspondence
with the source language.
The one-best desegmentation approach seg-
ments the target language at training time and
then desegments the one-best output in post-
processing. This resolves the sparsity issue, but
does not allow the decoder to take into account
features of the desegmented target. To the best of
our knowledge, we are the first group to go beyond
one-best desegmentation for English-to-Arabic
translation. In English-to-Finnish, although alter-
native integration strategies have seen some suc-
cess (Luong et al, 2010), the current state-of-
the-art performs one-best-desegmentation (Clifton
and Sarkar, 2011).
3.2 n-best Desegmentation
The one-best approach can be extended easily by
desegmenting n-best lists of segmented decoder
output. Doing so enables the inclusion of an
unsegmented target language model, and with a
small amount of bookkeeping, it also allows the
inclusion of features related to the operations per-
formed during desegmentation (see Section 3.4).
With new features reflecting the desegmented out-
put, we can re-tune our enhanced linear model on
a development set. Following previous work, we
will desegment 1000-best lists (Oflazer and Dur-
gar El-Kahlout, 2007).
Once n-best lists have been desegmented, we
can tune on unsegmented references as a side-
benefit. This could improve translation quality,
as it brings our training scenario closer to our test
scenario (test BLEU is always measured on unseg-
mented references). In particular, it could address
issues with translation length mismatch. Previous
work that has tuned on unsegmented references
has reported mixed results (Badr et al, 2008; Lu-
ong et al, 2010).
3.3 Lattice Desegmentation
An n-best list reflects a tiny portion of a decoder?s
search space, typically fixed at 1000 hypotheses.
Lattices
2
can represent an exponential number of
hypotheses in a compact structure. In this section,
we discuss how a lattice from a multi-stack phrase-
based decoder such as Moses (Koehn et al, 2007)
can be desegmented to enable word-level features.
Finite State Analogy
A phrase-based decoder produces its output from
left to right, with each operation appending
the translation of a source phrase to a grow-
ing target hypothesis. Translation continues un-
2
Or forests for hierarchical and syntactic decoders.
102
0 1b+ 2lEbp
5
+hm
4+hA
3
AlTfl
(a)	 ?
(b)	 ?
(c)	 ?
1
AlTfl:AlTfl
0b+:<epsilon>
2
lEbp:<epsilon>
<epsilon>:blEbp
+hA:blEbthA
+hm:blEbthm
0
5blEbthm
4blEbthA
2
blEbp
3AlTfl
Transduces	 ?
into	 ?
Figure 1: The finite state pipeline for a lattice translating the English fragment ?with the child?s game?.
The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to
produce the word lattice (c). The tokens in (a) are: b+ ?with?, lEbp ?game?, +hm ?their?, +hA ?her?,
and AlTfl ?the child?.
til each source word has been covered exactly
once (Koehn et al, 2003).
The search graph of a phrase-based decoder can
be interpreted as a lattice, which can be interpreted
as a finite state acceptor over target strings. In its
most natural form, such an acceptor emits target
phrases on each edge, but it can easily be trans-
formed into a form with one edge per token, as
shown in Figure 1a. This is sometimes referred to
as a word graph (Ueffing et al, 2002), although in
our case the segmented phrase table also produces
tokens that correspond to morphemes.
Our goal is to desegment the decoder?s output
lattice, and in doing so, gain access to a compact,
desegmented view of a large portion of the trans-
lation search space. This can be accomplished by
composing the lattice with a desegmenting trans-
ducer that consumes morphemes and outputs de-
segmented words. This transducer must be able
to consume every word in our lattice?s output vo-
cabulary. We define a word using the following
regular expression:
[prefix]* [stem] [suffix]* | [prefix]+ [suffix]+
(1)
where [prefix], [stem] and [suffix] are non-
overlapping sets of morphemes, whose members
are easily determined using the segmenter?s seg-
ment boundary markers.
3
The second disjunct of
Equation 1 covers words that have no clear stem,
such as the Arabic ?? lh ?for him?, segmented as l+
?for? +h ?him?. Equation 1 may need to be modi-
fied for other languages or segmentation schemes,
but our techniques generalize to any definition that
can be written as a regular expression.
A desegmenting transducer can be constructed
by first encoding our desegmenter as a table that
maps morpheme sequences to words. Regardless
of whether the original desegmenter was based
on concatenation, rules or table-lookup, it can be
encoded as a lattice-specific table by applying it
to an enumeration of all words found in the lat-
tice. We can then transform that table into a fi-
nite state transducer with one path per table en-
try. Finally, we take the closure of this trans-
ducer, so that the resulting machine can transduce
any sequence of words. The desegmenting trans-
3
Throughout this paper, we use ?+? to mark morphemes
as prefixes or suffixes, as in w+ or +h. In Equation 1 only,
we overload ?+? as the Kleene cross: X+ == XX?.
103
ducer for our running example is shown in Fig-
ure 1b. Note that tokens requiring no desegmen-
tation simply emit themselves. The lattice (Fig-
ure 1a) can then be desegmented by composing it
with the transducer (1b), producing a desegmented
lattice (1c). This is a natural place to introduce
features that describe the desegmentation process,
such as scores provided by a desegmentation table,
which can be incorporated into the desegmenting
transducer?s edge weights.
We now have a desegmented lattice, but it has
not been annotated with an unsegmented (word-
level) language model. In order to annotate lattice
edges with an n-gram LM, every path coming into
a node must end with the same sequence of (n?1)
tokens. If this property does not hold, then nodes
must be split until it does.
4
This property is main-
tained by the decoder?s recombination rules for the
segmented LM, but it is not guaranteed for the de-
segmented LM. Indeed, the expanded word-level
context is one of the main benefits of incorporating
a word-level LM. Fortunately, LM annotation as
well as any necessary lattice modifications can be
performed simultaneously by composing the de-
segmented lattice with a finite state acceptor en-
coding the LM (Roark et al, 2011).
In summary, we are given a segmented lattice,
which encodes the decoder?s translation space as
an acceptor over morphemes. We compose this
acceptor with a desegmenting transducer, and then
with an unsegmented LM acceptor, producing a
fully annotated, desegmented lattice. Instead of
using a tool kit such as OpenFst (Allauzen et
al., 2007), we implement both the desegmenting
transducer and the LM acceptor programmatically.
This eliminates the need to construct intermediate
machines, such as the lattice-specific desegmenter
in Figure 1b, and facilitates working with edges
annotated with feature vectors as opposed to sin-
gle weights.
Programmatic Desegmentation
Lattice desegmentation is a non-local lattice trans-
formation. That is, the morphemes forming a word
might span several edges, making desegmentation
non-trivial. Luong et al (2010) address this prob-
lem by forcing the decoder?s phrase table to re-
spect word boundaries, guaranteeing that each de-
segmentable token sequence is local to an edge.
4
Or the LM composition can be done dynamically, ef-
fectively decoding the lattice with a beam or cube-pruned
search (Huang and Chiang, 2007).
Inspired by the use of non-local features in forest
decoding (Huang, 2008), we present an algorithm
to find chains of edges that correspond to deseg-
mentable token sequences, allowing lattice deseg-
mentation with no phrase-table restrictions. This
algorithm can be seen as implicitly constructing a
customized desegmenting transducer and compos-
ing it with the input lattice on the fly.
Before describing the algorithm, we define
some notation. An input morpheme lattice is a
triple ?n
s
,N , E?, where N is a set of nodes, E is
a set of edges, and n
s
? N is the start node that
begins each path through the lattice. Each edge
e ? E is a 4-tuple ?from, to, lex , w?, where from ,
to ? N are head and tail nodes, lex is a single
token accepted by this edge, and w is the (po-
tentially vector-valued) edge weight. Tokens are
drawn from one of three non-overlapping morpho-
syntactic sets: lex ? Prefix ? Stem ? Suffix ,
where tokens that do not require desegmentation,
such as complete words, punctuation and num-
bers, are considered to be in Stem . It is also useful
to consider the set of all outgoing edges for a node
n.out = {e ? E|e.from = n}.
With this notation in place, we can define a
chain c to be a sequence of edges [e
1
. . . e
l
] such
that for 1 ? i < l : e
i
.to = e
i+1
.from . We
denote singleton chains with [e], and when unam-
biguous, we abbreviate longer chains with their
start and end node [e
1
.from ? e
l
.to]. A chain
is valid if it emits the beginning of a word as de-
fined by the regular expression in Equation 1. A
valid chain is complete if its edges form an entire
word, and if it is part of a path through the lat-
tice that consists only of words. In Figure 1a, the
complete chains are [0 ? 2], [0 ? 4], [0 ? 5],
and [2 ? 3]. The path restriction on complete
chains forces words to be bounded by other words
in order to be complete.
5
For example, if we re-
moved the edge 2 ? 3 (AlTfl) from Figure 1a,
then [0? 2] ([b+ lEbp]) would cease to be a com-
plete chain, but it would still be a valid chain. Note
that in the finite-state analogy, the path restriction
is implicit in the composition operation.
Algorithm 1 desegments a lattice by finding all
complete chains and replacing each one with a sin-
gle edge. It maintains a work list of nodes that
lie on the boundary between words, and for each
node on this list, it launches a depth first search
5
Sentence-initial suffix morphemes and sentence-final
prefix morphemes represent a special case that we omit for
the sake of brevity. Lacking stems, they are left segmented.
104
Algorithm 1 Desegment a lattice ?n
s
,N , E?
{Initialize output lattice and work list WL}
n
?
s
= n
s
, N
?
= ?, E
?
= ?, WL = [n
s
]
while n = WL.pop() do
{Work on each node only once}
if n ? N
?
then continue
N
?
= N
?
? {n}
{Initialize the chain stack C}
C = ?
for e ? n.out do
if [e] is valid then C.push([e])
{Depth-first search for complete chains}
while [e
1
, . . . , e
l
] = C.pop() do
{Attempt to extend chain}
for e ? e
l
.to.out do
if [e
1
. . . e
l
, e] is valid then
C.push([e
1
, . . . , e
l
, e])
else
Mark [e
1
, . . . , e
l
] as complete
{Desegment complete chains}
if [e
1
, . . . , e
l
] is complete then
WL.push(e
l
.to)
E
?
= E
?
? {deseg([e
1
, . . . , e
l
])}
return ?n
?
s
,N
?
, E
?
?
to find all complete chains extending from it. The
search recognizes the valid chain c to be complete
by finding an edge e such that c+ e forms a chain,
but not a valid one. By inspection of Equation 1,
this can only happen when a prefix or stem fol-
lows a stem or suffix, which always marks a word
boundary. The chains found by this search are de-
segmented and then added to the output lattice as
edges. The nodes at end points of these chains are
added to the work list, as they lie at word bound-
aries by definition. Note that although this algo-
rithm creates completely new edges, the resulting
node set N
?
will be a subset of the input node set
N . The complementN ?N
?
will consist of nodes
that are word-internal in all paths through the input
lattice, such as node 1 in Figure 1a.
Programmatic LM Integration
Programmatic composition of a lattice with an
n-gram LM acceptor is a well understood prob-
lem. We use a dynamic program to enumerate all
(n ? 1)-word contexts leading into a node, and
then split the node into multiple copies, one for
each context. With each node corresponding to a
single LM context, annotation of outgoing edges
with n-gram LM scores is straightforward.
3.4 Desegmentation Features
Our re-ranker has access to all of the features used
by the decoder, in addition to a number of features
enabled by desegmentation.
Desegmentation Score We use a table-based
desegmentation method for Arabic, which is based
on segmenting an Arabic training corpus and
memorizing the observed transformations to re-
verse them later. Finnish does not require a ta-
ble, as all words can be desegmented with sim-
ple concatenation. The Arabic table consists of
X ? Y entries, where X is a target morpheme
sequence and Y is a desegmented surface form.
Several entries may share the same X , resulting
in multiple desegmentation options. For the sake
of symmetry with the unambiguous Finnish case,
we augment Arabic n-best lists or lattices with
only the most frequent desegmentation Y .
6
We
provide the desegmentation score log p(Y |X)=
log
(
count of X ? Y
count of X
)
as a feature, to indicate the en-
try?s ambiguity in the training data.
7
When an X is
missing from the table, we fall back on a set of de-
segmentation rules (El Kholy and Habash, 2012a)
and this feature is set to 0. This feature is always
0 for English-Finnish.
Contiguity One advantage of our approach is
that it allows discontiguous source words to trans-
late into a single target word. In order to maintain
some control over this powerful capability, we cre-
ate three binary features that indicate the contigu-
ity of a desegmentation. The first feature indicates
that the desegmented morphemes were translated
from contiguous source words. The second indi-
cates that the source words contained a single dis-
contiguity, as in a word-by-word translation of the
?with his blue car? example from Section 2.2. The
third indicates two or more discontiguities.
Unsegmented LM A 5-gram LM trained on un-
segmented target text is used to assess the fluency
of the desegmented word sequence.
4 Experimental Setup
We train our English-to-Arabic system using 1.49
million sentence pairs drawn from the NIST 2012
training set, excluding the UN data. This training
set contains about 40 million Arabic tokens before
6
Allowing the re-ranker to choose between multiple Y s is
a natural avenue for future work.
7
We also experimented on log p(X|Y ) as an additional
feature, but observed no improvement in translation quality.
105
segmentation, and 47 million after segmentation.
We tune on the NIST 2004 evaluation set (1353
sentences) and evaluate on NIST 2005 (1056 sen-
tences). As these evaluation sets are intended for
Arabic-to-English translation, we select the first
English reference to use as our source text.
Our English-to-Finnish system is trained on the
same Europarl corpus as Luong et al (2010) and
Clifton and Sarkar (2011), which has roughly one
million sentence pairs. We also use their develop-
ment and test sets (2000 sentences each).
4.1 Segmentation
For Arabic, morphological segmentation is per-
formed by MADA 3.2 (Habash et al, 2009), using
the Penn Arabic Treebank (PATB) segmentation
scheme as recommended by El Kholy and Habash
(2012a). For both segmented and unsegmented
Arabic, we further normalize the script by convert-
ing different forms of Alif @


@

@ @ and Ya ? ?


to
bare Alif @ and dotless Ya ?. To generate the de-
segmentation table, we analyze the segmentations
from the Arabic side of the parallel training data
to collect mappings from morpheme sequences to
surface forms.
For Finnish, we adopt the Unsup L-match seg-
mentation technique of Clifton and Sarkar (2011),
which uses Morfessor (Creutz and Lagus, 2005)
to analyze the 5,000 most frequent Finnish words.
The analysis is then applied to the Finnish side of
the parallel text, and a list of segmented suffixes
is collected. To improve coverage, words are fur-
ther segmented according to their longest match-
ing suffix from the list. As Morfessor does not
perform any orthographic normalizations, it can be
desegmented with simple concatenation.
4.2 Systems
We align the parallel data with GIZA++ (Och et
al., 2003) and decode using Moses (Koehn et al,
2007). The decoder?s log-linear model includes a
standard feature set. Four translation model fea-
tures encode phrase translation probabilities and
lexical scores in both directions. Seven distor-
tion features encode a standard distortion penalty
as well as a bidirectional lexicalized reordering
model. A KN-smoothed 5-gram language model
is trained on the target side of the parallel data with
SRILM (Stolcke, 2002). Finally, we include word
and phrase penalties. The decoder uses the default
parameters for English-to-Arabic, except that the
maximum phrase length is set to 8. For English-
to-Finnish, we follow Clifton and Sarkar (2011) in
setting the hypothesis stack size to 100, distortion
limit to 6, and maximum phrase length to 20.
The decoder?s log-linear model is tuned with
MERT (Och, 2003). Re-ranking models are tuned
using a batch variant of hope-fear MIRA (Chi-
ang et al, 2008; Cherry and Foster, 2012), us-
ing the n-best variant for n-best desegmentation,
and the lattice variant for lattice desegmentation.
MIRA was selected over MERT because we have
an in-house implementation that can tune on lat-
tices very quickly. During development, we con-
firmed that MERT and MIRA perform similarly,
as is expected with fewer than 20 features. Both
the decoder?s log-linear model and the re-ranking
models are trained on the same development set.
Historically, we have not seen improvements from
using different tuning sets for decoding and re-
ranking. Lattices are pruned to a density of 50
edges per word before re-ranking.
We test four different systems. Our first base-
line is Unsegmented, where we train on unseg-
mented target text, requiring no desegmentation
step. Our second baseline is 1-best Deseg, where
we train on segmented target text and desegment
the decoder?s 1-best output. Starting from the sys-
tem that produced 1-best Deseg, we then output ei-
ther 1000-best lists or lattices to create our two ex-
perimental systems. The 1000-best Deseg system
desegments, augments and re-ranks the decoder?s
1000-best list, while Lattice Deseg does the same
in the lattice. We augment n-best lists and lattices
using the features described in Section 3.4.
8
We evaluate our system using BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006). Fol-
lowing Clark et al (2011), we report average
scores over five random tuning replications to ac-
count for optimizer instability. For the baselines,
this means 5 runs of decoder tuning. For the de-
segmenting re-rankers, this means 5 runs of re-
ranker tuning, each working on n-best lists or lat-
tices produced by the same (representative) de-
coder weights. We measure statistical significance
using MultEval (Clark et al, 2011), which imple-
ments a stratified approximate randomization test
to account for multiple tuning replications.
8
Development experiments on a small-data English-to-
Arabic scenario indicated that the Desegmentation Score was
not particularly useful, so we exclude it from the main com-
parison, but include it in the ablation experiments.
106
5 Results
Tables 1 and 2 report results averaged over 5 tun-
ing replications on English-to-Arabic and English-
to-Finnish, respectively. In all scenarios, both
1000-best Deseg and Lattice Deseg significantly
outperform the 1-best Deseg baseline (p < 0.01).
For English-to-Arabic, 1-best desegmentation
results in a 0.7 BLEU point improvement over
training on unsegmented Arabic. Moving to lat-
tice desegmentation more than doubles that im-
provement, resulting in a BLEU score of 34.4 and
an improvement of 1.0 BLEU point over 1-best
desegmentation. 1000-best desegmentation also
works well, resulting in a 0.6 BLEU point im-
provement over 1-best. Lattice desegmentation is
significantly better (p < 0.01) than 1000-best de-
segmentation.
For English-to-Finnish, the Unsup L-match seg-
mentation with 1-best desegmentation does not
improve over the unsegmented baseline. The seg-
mentation may be addressing issues with model
sparsity, but it is also introducing errors that would
have been impossible had words been left un-
segmented. In fact, even with our lattice deseg-
menter providing a boost, we are unable to see
a significant improvement over the unsegmented
model. As we attempted to replicate the approach
of Clifton and Sarkar (2011) exactly by working
with their segmented data, this difference is likely
due to changes in Moses since the publication of
their result. Nonetheless, the 1000-best and lattice
desegmenters both produce significant improve-
ments over the 1-best desegmentation baseline,
with Lattice Deseg achieving a 1-point improve-
ment in TER. These results match the established
state-of-the-art on this data set, but also indicate
that there is still room for improvement in identi-
fying the best segmentation strategy for English-
to-Finnish translation.
We also tried a similar Morfessor-based seg-
mentation for Arabic, which has an unsegmented
test set BLEU of 32.7. As in Finnish, the 1-best
desegmentation using Morfessor did not surpass
the unsegmented baseline, producing a test BLEU
of only 31.4 (not shown in Table 1). Lattice deseg-
mentation was able to boost this to 32.9, slightly
above 1-best desegmentation, but well below our
best MADA desegmentation result of 34.4. There
appears to be a large advantage to using MADA?s
supervised segmentation in this scenario.
Model Dev Test
BLEU BLEU TER
Unsegmented 24.4 32.7 49.4
1-best Deseg 24.4 33.4 48.6
1000-best Deseg 25.0 34.0 48.0
Lattice Deseg 25.2 34.4 47.7
Table 1: Results for English-to-Arabic translation
using MADA?s PATB segmentation.
Model Dev Test
BLEU BLEU TER
Unsegmented 15.4 15.1 70.8
1-best Deseg 15.3 14.8 71.9
1000-best Deseg 15.4 15.1 71.5
Lattice Deseg 15.5 15.1 70.9
Table 2: Results for English-to-Finnish translation
using unsupervised segmentation.
5.1 Ablation
We conducted an ablation experiment on English-
to-Arabic to measure the impact of the various fea-
tures described in Section 3.4. Table 3 compares
different combinations of features using lattice de-
segmentation. The unsegmented LM alone yields
a 0.4 point improvement over the 1-best deseg-
mentation score. Adding contiguity indicators on
top of the unsegmented LM results in another 0.6
point improvement. As anticipated, the tuner as-
signs negative weights to discontiguous cases, en-
couraging the re-ranker to select a safer transla-
tion path when possible. Judging from the out-
put on the NIST 2005 test set, the system uses
these discontiguous desegmentations very rarely:
only 5% of desegmented tokens align to discon-
tiguous source phrases. Adding the desegmenta-
tion score to these two feature groups does not im-
prove performance, confirming the results we ob-
served during development. The desegmentation
score would likely be useful in a scenario where
we provide multiple desegmentation options to the
re-ranker; for now, it indicates only the ambiguity
of a fixed choice, and is likely redundant with in-
formation provided by the language model.
5.2 Error Analysis
In order to better understand the source of our
improvements in the English-to-Arabic scenario,
we conducted an extensive manual analysis of
the differences between 1-best and lattice deseg-
107
Features dev test
1-best Deseg 24.5 33.4
+ Unsegmented LM 24.9 33.8
+ Contiguity 25.2 34.4
+ Desegmentation Score 25.2 34.3
Table 3: The effect of feature ablation on BLEU
score for English-to-Arabic translation with lattice
desegmentation.
mentation on our test set. We compared the
output of the two systems using the Unix tool
wdiff , which transforms a solution to the longest-
common-subsequence problem into a sequence
of multi-word insertions and deletions (Hunt and
McIlroy, 1976). We considered adjacent insertion-
deletion pairs to be (potentially phrasal) substitu-
tions, and collected them into a file, omitting any
unpaired insertions or deletions. We then sampled
650 cases where the two sides of the substitution
were deemed to be related, and divided these cases
into categories based on how the lattice desegmen-
tation differs from the one-best desegmentation.
We consider a phrase to be correct only if it can
be found in the reference.
Table 4 breaks down per-phrase accuracy ac-
cording to four manually-assigned categories: (1)
clitical ? the two systems agree on a stem, but at
least one clitic, often a prefix denoting a prepo-
sition or determiner, was dropped, added or re-
placed; (2) lexical ? a word was changed to a mor-
phologically unrelated word with a similar mean-
ing; (3) inflectional ? the words have the same
stem, but different inflection due to a change in
gender, number or verb tense; (4) part-of-speech
? the two systems agree on the lemma, but have
selected different parts of speech.
For each case covering a single phrasal differ-
ence, we compare the phrases from each system
to the reference. We report the number of in-
stances where each system matched the reference,
as well as cases where they were both incorrect.
The majority of differences correspond to clitics,
whose correction appears to be a major source of
the improvements obtained by lattice desegmen-
tation. This category is challenging for the de-
coder because English prepositions tend to corre-
spond to multiple possible forms when translated
into Arabic. It also includes the frequent cases
involving the nominal determiner prefix Al ?the?
(left unsegmented by the PATB scheme), and the
Lattice
Correct
1-best
Correct
Both
Incorrect
Clitical 157 71 79
Lexical 61 39 80
Inflectional 37 32 47
Part-of-speech 19 17 11
Table 4: Error analysis for English-to-Arabic
translation based on 650 sampled instances.
sentence-initial conjunction w+ ?and?. The sec-
ond most common category is lexical, where the
unsegmented LM has drastically altered the choice
of translation. The remaining categories show no
major advantage for either system.
6 Conclusion
We have explored deeper integration of morpho-
logical desegmentation into the statistical machine
translation pipeline. We have presented a novel,
finite-state-inspired approach to lattice desegmen-
tation, which allows the system to account for a
desegmented view of many possible translations,
without any modification to the decoder or any
restrictions on phrase extraction. When applied
to English-to-Arabic translation, lattice desegmen-
tation results in a 1.0 BLEU point improvement
over one-best desegmentation, and a 1.7 BLEU
point improvement over unsegmented translation.
We have also applied our approach to English-to-
Finnish translation, and although segmentation in
general does not currently help, we are able to
show significant improvements over a 1-best de-
segmentation baseline.
In the future, we plan to explore introducing
multiple segmentation options into the lattice, and
the application of our method to a full morpho-
logical analysis (as opposed to segmentation) of
the target language. Eventually, we would like
to replace the functionality of factored transla-
tion models (Koehn and Hoang, 2007) with lattice
transformation and augmentation.
Acknowledgments
Thanks to Ann Clifton for generously provid-
ing the data and segmentation for our English-to-
Finnish experiments, and to Marine Carpuat and
Roland Kuhn for their helpful comments on an
earlier draft. This research was supported by the
Natural Sciences and Engineering Research Coun-
cil of Canada.
108
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of
Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL, pages
153?156.
Ond?rej Bojar. 2007. English-to-Czech factored ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
232?239, Prague, Czech Republic, June.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of HLT-NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, pages 224?233.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling for
optimizer instability. In Proceedings of ACL, pages
176?181.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 32?42, Portland, Oregon, USA, June.
Mathias Creutz and Krista Lagus. 2005. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
Adaptive Knowledge Representation and Reasoning
(AKRR05, pages 106?113.
Ahmed El Kholy and Nizar Habash. 2012a. Ortho-
graphic and morphological processing for English?
Arabic statistical machine translation. Machine
Translation, 26(1-2):25?45, March.
Ahmed El Kholy and Nizar Habash. 2012b. Trans-
late, predict or generate: Modeling rich morphology
in statistical machine translation. Proceeding of the
Meeting of the European Association for Machine
Translation.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664?674, Avi-
gnon, France, April. Association for Computational
Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools, Cairo, Egypt, April.
The MEDAR Consortium.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June.
James W. Hunt and M. Douglas McIlroy. 1976. An
algorithm for differential file comparison. Technical
report, Bell Laboratories, June.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joesef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation for
machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 148?157, Cambridge, MA, October.
109
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128?135, Prague, Czech Republic, June.
Franz Josef Och, Hermann Ney, Franz Josef, and
Och Hermann Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 25?32, Prague,
Czech Republic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Brian Roark, Richard Sproat, and Izhak Shafran. 2011.
Lexicographic semirings for exact automata encod-
ing of sequence models. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1?5, Portland, Oregon, USA, June.
Mohammad Salameh, Colin Cherry, and Grzegorz
Kondrak. 2013. Reversing morphological tokeniza-
tion in English-to-Arabic SMT. In Proceedings of
the 2013 NAACL HLT Student Research Workshop,
pages 47?53, Atlanta, Georgia, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language
Processing, pages 901?904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 230?238, Portland, Ore-
gon, USA, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514?522, Columbus, Ohio, June.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine
translation. In Proceedings of EMNLP, pages 156?
163, Philadelphia, PA, July.
110
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 71?75,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transliteration Experiments on Chinese and Arabic
Grzegorz Kondrak, Xingkai Li and Mohammad Salameh
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
{gkondrak,xingkai,msalameh}@ualberta.ca
Abstract
We report the results of our transliteration ex-
periments with language-specific adaptations
in the context of two language pairs: English
to Chinese, and Arabic to English. In particu-
lar, we investigate a syllable-based Pinyin in-
termediate representation for Chinese, and a
letter mapping for Arabic.
1 Introduction
Transliteration transforms an orthographic form of
a word in one writing script into an orthographic
form of the same word in another writing script. The
problem is challenging because the relationship be-
tween the source and target representations is often
ambiguous. The process is further complicated by
restrictions in the target phonological system.
DIRECTL+ (Jiampojamarn et al, 2010a) is an
online discriminative training system that incorpo-
rates joint n-gram features and many-to-many align-
ments, which are generated by M2M-ALIGNER (Ji-
ampojamarn et al, 2007). Our team employed vari-
ants of DIRECTL+ in the previous editions of the
Shared Task on Transliteration (Jiampojamarn et al,
2009; Jiampojamarn et al, 2010b; Bhargava et al,
2011). Recently, Bhargava and Kondrak (2012)
show significant improvement in accuracy for the
English-to-Japanese task by leveraging supplemen-
tal transliterations from other scripts.
In this edition of the Shared Task on Translitera-
tion, we experiment with language-specific adapta-
tions for the EnCh and ArEn data sets. The struc-
ture of the paper is as follows. In Section 2, we
provide details about the system parameters used in
M2M-ALIGNER and DIRECTL+. Section 3 pro-
vides details of our strategies adopted in the EnCh
task, which incorporate Chinese-specific knowledge
and system combination algorithm. In Section 4 we
elaborate on the difficulty of Arabic name transliter-
ation and propose a letter mapping scheme. In Sec-
tion 5 we present the official test results.
2 Base System
We run DIRECTL+ with all of the features described
in (Jiampojamarn et al, 2010a). System parameters
were determined during development. For the EnCh
experiments, we set the context feature size to 5, the
transition feature size to 2, and the joint n-gram fea-
ture size to 6. For the ArEn experiments, we used
the same settings, except that we set the joint n-gram
feature size to 5.
The M2M-ALIGNER parameters were set as fol-
lows. For the English-Pinyin alignment, the maxi-
mum substring length was 1 on the English side, and
2 on the Pinyin side, with empty substrings (nulls)
allowed only on the Pinyin side. For ArEn, the max-
imum substring length was 2 for both sides.
3 English to Chinese
In this section, we introduce the strategies for im-
proving DIRECTL+ performance on the EnCh task,
including the use of Chinese Pinyin for preprocess-
ing, and the combination of different models.
3.1 Data preprocessing and cleaning
In general, the preprocessing is limited to remov-
ing letter case distinctions in English names, and re-
71
placing every non-initial letter x with ks. However,
we observed that the provided development set con-
tains a number of entries (about 3%) that contain
multiple English words on the source side, but no
corresponding separators on the target side, whereas
no such entries occur in the training or testing set.
Since this discrepancy between sets may cause prob-
lems for alignment and generation, we separated
the multi-word entries into individual words (using
whitespace and apostrophes as delimiters) and man-
ually selected proper transliteration targets for them.
We also removed individual words that have no cor-
responding transliterations on the target side. The
cleaned development set contains 2483 entries.
3.2 Alignment via Pinyin
Following Jiampojamarn et al (2009; 2010b), we
utilize Pinyin as an intermediate representation of
Chinese characters during M2M alignment with the
objective of improving its quality. Pinyin is the
formally-adopted Romanization system for Stan-
dard Mandarin for the mapping of Chinese charac-
ters to Roman alphabet. It uses the 26 letters of the
English alphabet except for the letter v, with the ad-
dition of the letter u?. Every Chinese character can be
represented by a sequence of Pinyin letters accord-
ing to the way it is pronounced. Numerous freely
available online tools exist for facilitating Chinese-
Pinyin conversion1 .
In our experiments, the original Chinese charac-
ters from the target side of the training set are con-
verted to Pinyin before M2M alignment. A small
part of them (about 50 out of approximately 500
distinct Chinese characters in the Shared Task data)
have multiple pronunciations, and can thus be rep-
resented by different Pinyin sequences. For those
characters we manually select the pronunciations
that are normally used for names.
After the alignment between English and Pinyin
representation has been generated by M2M-
ALIGNER, we use it to derive the alignment between
English and Chinese characters, which is then used
for training DIRECTL+. This preprocessing step re-
sults in a more accurate alignment as it substantially
reduces the number of target symbols from around
500 distinct Chinese characters to 26 Pinyin letters.
1For instance, http://www.chinesetopinyin.com
Our approach is to utilize Pinyin only in the align-
ment phase, and converts it back to Chinese charac-
ters before the training phase. We do not incorporate
Pinyin into the generation phase in order to avoid
problems involved in converting the transliteration
results from Pinyin back to Chinese characters. For
example, a Pinyin subsequence may have multiple
Chinese character mappings because of the fact that
many Chinese characters have the same Pinyin rep-
resentation. In addition, it is not always clear how to
partition the Pinyin sequence into substrings corre-
sponding to individual Chinese characters.
The choice of the appropriate Chinese character
sequence is the problem further complicating the
conversion from Pinyin. We experimented with a tri-
gram language model trained on the target Chinese
side of the training set for the purpose of identify-
ing the correct transliteration result. However, this
approach yielded low accuracy on the development
set. In contrast, the strategy of using Pinyin only for
the alignment introduces no ambiguity because we
know the mapping between Pinyin sequences and
the target Chinese side of the training set.
3.3 Syllabic Pinyin
The Pinyin sequences representing the pronuncia-
tions of Chinese characters should not be interpreted
as combinations of individual letters. Rather, a Man-
darin phonetic syllable (the pronunciation of one
Chinese character) is composed of an optional on-
set (?initial?) followed by an obligatory rhyme (?fi-
nal?). The rhyme itself is composed of an obligatory
nucleus followed by an optional coda. Phonetically,
the onset contains a single consonant, the nucleus
contains a vowel or a diphthong, and the coda con-
tains a single consonant ([r], [n] or [N]). Both the on-
set and the rhyme can be represented by either a sin-
gle letter or sequence of two or three letters. It is the
initials and finals listed in Table 1 rather than Pinyin
letters that are the phonemic units of Pinyin for Stan-
dard Mandarin. The pronunciation of a multi-letter
initial/final is often different from the pronunciation
of the sequence of its individual letters. Treating
converted Pinyin as a sequence of separate letters
may result in an incorrect phonetic transcription.
In this paper, we further experiment with encod-
ing the converted sequences of Pinyin letters as the
sequences of initials and finals for M2M alignment.
72
Initials
b p m f d t n l
g k h j q x zh ch
sh r z c s y w
Finals
a o e i u u? ai ei
ui ao ou iu ie u?e er an
en in un u?n ang eng ing ong
Table 1: The initials and finals in Chinese Pinyin.
Although the size of the alphabet increases from 26
letters to 47 initials and finals, the original Chinese
pronunciation is represented more precisely. We re-
fer to the new model which is trained on Pinyin
initials and finals as PINYIN-SYL, and to the pre-
viously proposed model which is trained on Pinyin
letters as PINYIN-LET.
3.4 System combination
The combination of models based on different
principles may lead to improved prediction accu-
racy. We adopt the simple voting algorithm for
system combination proposed by Jiampojamarn et
al. (2009), with minor modifications. Since here
we combine only two systems (PINYIN-LET and
PINYIN-SYL), the algorithm becomes even simpler.
We first rank the participating models according to
their overall top-1 accuracy2 on the development set.
Note that the n-best list produced by DIRECTL+
may contain multiple copies of the same output
which differ only in the implied input-output align-
ment. We allow such duplicates to contribute to the
voting tally. The top-1 prediction is selected from
the set of top-1 predictions produced by the partic-
ipating models, with ties broken by voting and the
preference for the highest-ranking system. For con-
structing n-best candidate lists, we order the candi-
date transliterations according to the highest rank
assigned by either of the systems, with ties again
broken by voting and the preference for the highest-
ranking system. We refer to this combined model as
COMBINED.
Table 2 shows the results of the three discussed
approaches trained on the original training set, and
2Word accuracy in top-1 evaluates only the top translitera-
tion candidate produced by a transliteration system.
System top-1 F-score
PINYIN-LET 0.296 0.679
PINYIN-SYL 0.302 0.681
COMBINED 0.304 0.682
Table 2: Development results on EnCh.
tested on the cleaned development set. PINYIN-SYL
performs slightly better than PINYIN-LET, which
hints at the advantage of using Pinyin initials and fi-
nals over Pinyin letters as the intermediate represen-
tation during the alignment. The combination of the
two models produces a marginally higher F-score3.
The likely reason for the limited gain is the strong
similarity of the two combined models. We exper-
imented with adding a third model that is trained
directly on the original Chinese characters without
using Pinyin as the intermediate representation, but
its accuracy was lower, and the accuracy of the re-
sulting combined model was below PINYIN-SYL.
4 Arabic to English
Arabic script has 36 letters and 9 diacritics. Among
these letters, the letters Alif and Yaa can be repre-
sented in different forms (

@

@ @ @ and ?
 ? ,
respectively). The ArEn data set contains Arabic
names without diacritics, which adds ambiguity to
the transliteration task. When transliterated, such
diacritics would appear as an English vowel. For
example, it is difficult to tell whether the correct
transliteration of the two-letter name l .Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 140?145,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Cognate and Misspelling Features for Natural Language Identification
Garrett Nicolai, Bradley Hauer, Mohammad Salameh, Lei Yao, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, Canada
{nicolai,bmhauer,msalameh,lyao1,gkondrak}@ualberta.ca
Abstract
We apply Support Vector Machines to differ-
entiate between 11 native languages in the
2013 Native Language Identification Shared
Task. We expand a set of common language
identification features to include cognate inter-
ference and spelling mistakes. Our best results
are obtained with a classifier which includes
both the cognate and the misspelling features,
as well as word unigrams, word bigrams, char-
acter bigrams, and syntax production rules.
1 Introduction
As the world becomes more inter-connected, an in-
creasing number of people devote effort to learn-
ing one of the languages that are dominant in the
global community. English, in particular, is stud-
ied in many countries across the globe. The goal is
often related to increasing one?s chances to obtain
employment and succeed professionally. The lan-
guage of work-place communication is often not a
speaker?s native language (L1) but their second lan-
guage (L2). Speakers and writers of the same L1
can sometimes be identified by similar L2 errors.
The weak Contrastive Analysis Hypothesis (Jarvis
and Crossley, 2012) suggests that these errors may
be a result of L1 causing linguistic interference; that
is, common tendencies of a speaker?s L1 are super-
imposed onto their L2. Native Language Identifi-
cation, or NLI, is an attempt to exploit these errors
in order to identify the L1 of the speaker from texts
written in L2.
Our group at the University of Alberta was unfa-
miliar with the NLI research prior to the announce-
ment of a shared task (Tetreault et al, 2013). How-
ever, we saw it as an opportunity to apply our exper-
tise in character-level NLP to a new task. Our goal
was to propose novel features, and to combine them
with other features that have been previously shown
to work well for language identification.
In the end, we managed to define two feature sets
that are based on spelling errors made by L2 writers.
Cognate features relate a spelling mistake to cognate
interference with the writer?s L1. Misspelling fea-
tures identify common mistakes that may be indica-
tive of the writer?s L1. Both feature sets are meant
to exploit the Contrastive Analysis Hypothesis, and
benefit from the writer?s L1 influence on their L2
writing.
2 Related Work
Koppel et al (2005b) approach the NLI task using
Support Vector Machines (SVMs). They experi-
ment with features such as function-word unigrams,
rare part-of-speech bigrams, character bigrams, and
spelling and syntax errors. They report 80% accu-
racy across 5 languages. We further investigate the
role of word unigrams and spelling errors in native
language identification. We consider not only func-
tion words, but also content words, as well as word
bigrams. We also process spell-checking errors with
a text aligner to find common spelling errors among
writers with the same L1.
Tsur and Rappoport (2007) also use SVMs on the
NLI task, but limit their feature set to character bi-
grams. They report 65% accuracy on 5 languages,
and hypothesize that the choice of words when writ-
ing in L2 is strongly affected by the phonology of
140
their L1. We also consider character bigrams in our
feature set, but combine them with a number of other
features.
Wong and Dras (2011) opt for a maximum en-
tropy classifier, and focus more on syntax errors than
lexical errors. They find that syntax tree production
rules help their classifier in a seven language clas-
sification task. They only consider non-lexicalized
rules, and rules with function words. In contrast, we
consider both lexicalized and non-lexicalized pro-
duction rules, and we include content words.
Bergsma et al (2012) consider the NLI task as a
sub-task of the authorship attribution task. They fo-
cus on the following three questions: (1) whether the
native language of the writer of a paper is English,
(2) what is the gender of the writer, and (3) whether
a paper is a conference or workshop paper. The au-
thors conclude that syntax aids the native language
classification task, further motivating our decision to
use part-of-speech n-grams and production rules as
features for our classifier. Furthermore, the authors
suggest normalizing text to reduce sparsity, and im-
plement several meta-features that they claim aid the
classification.
3 Classifier
Following Koppel et al (2005b) and others, we
perform classification with SVMs. We chose the
SVM-Multiclass package, a version of the SVM-
light package(Joachims, 1999) specifically modified
for multi-class classification problems. We use a lin-
ear kernel, and two hyperparameters that were tuned
on the development set: the c soft-margin regular-
ization parameter, which measures the tradeoff be-
tween training error and the size of the margin, and
, which is used as a stopping criterion for the SVM.
C was tuned to a value of 5000, and epsilon to a
value of 0.1.
4 Features
As features for our SVM, we used a combination of
features common in the literature and new features
developed specifically for this task. The features are
listed in the following section.
4.1 Word n-grams
Following previous work, we use word n-grams as
the primary feature set. We normalize the text before
selecting n-grams using the method of Bergsma et
al. (2012). In particular, all digits are replaced with
a representative ?0? character; for example, ?22? and
?97? are both represented as ?00?. However, unlike
Koppel et al (2005b), we incorporate word bigrams
in addition to word unigrams, and utilize both func-
tion words and content words.
4.1.1 Function Words
Using a list of 295 common function words, we
reduce each document to a vector of values repre-
senting their presence or absence in a document. All
other tokens in the document are ignored. When
constructing vectors of bigrams, any word that is not
on the list of function words is converted to a place-
holder token. Thus, most of our function-word bi-
grams consist of a single function word preceded or
followed by a placeholder token.
4.1.2 Content Words
Other than the normalization mentioned in Sec-
tion 4.1, all tokens in the documents are allowed as
possible word unigrams. No spelling correction is
used for reducing the number of word n-grams. Fur-
thermore, we consider all token unigrams that occur
in the training data, regardless of their frequency.
An early concern with token bigrams was that
they were both large in number, and sparse. In an
attempt to reduce the number of bigrams, we con-
ducted experiments on the development set with dif-
ferent numbers of bigrams that exhibited the highest
information gain. It was found that using all combi-
nations of word bigrams improved predictive accu-
racy the most, and did not lead to a significant cost
to the SVM. Thus, for experiments on the test set, all
token bigrams that were encountered in the training
set were used as features.
4.2 Character n-grams
Following Tetreault et al (2012), we utilize all char-
acter bigrams that occur in the training data, rather
than only the most frequent ones. However, where
the literature uses either binary indicators or relative
frequency of bigrams as features, we use a modi-
fied form of the relative frequency in our classifier.
141
In a pre-processing step, we calculate the average
frequency of each character bigram across all train-
ing documents. Then, during feature extraction, we
again determine the relative frequency of each char-
acter bigram across documents. We then use bi-
nary features to indicate if the frequency of a bigram
is higher than the average frequency. Experiments
conducted on the development set showed that al-
though this modified frequency was out-performed
by the original relative frequency on its own, our
method performed better when further features were
incorporated into the classifier.
4.3 Part-of-speech n-grams
All documents are tagged with POS tags using the
Stanford parser (Klein and Manning, 2003), From
the documents in the training data, a list of all POS
bigrams was generated, and documents were repre-
sented by binary indicators of the presence or ab-
sence of a bigram in the document. As with char-
acter bigrams, we did not simply use the most com-
mon bigrams, but rather considered all bigrams that
appeared in the training data.
4.4 Syntax Production Rules
After generating syntactic parse trees with the Stan-
ford Parser. we extract all possible production rules
from each document, including lexicalized rules.
The features are binary; if a production rule occurs
in an essay, its value is set to 1, and 0 otherwise. For
each language, we use information gain for feature
selection to select the most informative production
rules as suggested by Wong and Dras (2011). Ex-
periments on the development set indicated that the
information gain is superior to raw frequency for the
purpose of syntax feature selection. Since the accu-
racy increased as we added more production rules,
the feature set for final testing includes all produc-
tion rules encountered in the training set. The ma-
jority of the rules are of the form POS? terminal.
We hypothesized that most of the information con-
tained in these rules may be already captured by the
word unigram features. However, experiments on
the development set suggested that the lexicalized
rules contain information that is not captured by the
unigrams, as they led to an increase in predictive ac-
curacy.
4.5 Spelling Errors
Koppel et al (2005a) suggested spelling errors
could be helpful as writers might be affected by
the spelling convention in their native languages.
Moreover, spelling errors also reflect the pronun-
ciation characteristics of the writers? native lan-
guages. They identified 8 types of spelling errors
and collected the statistics of each error type as
their features. Unlike their approach, we focus on
the specific spelling errors made by the writers be-
cause 8 types may be insufficient to distinguish the
spelling characteristics of writers from 11 differ-
ent languages. We extract the spelling error fea-
tures from character-level alignments between the
misspelled word and the intended word. For ex-
ample, if the word abstract is identified as the in-
tended spelling of a misspelling abustruct, the char-
acter alignments are as follows:
a bu s t ru ct
| | | | | |
a b s t ra ct
Only the alignments of the misspelled parts, i.e.
(bu,b) and (ru,ra) in this case, are used as fea-
tures. The spell-checker we use is aspell1, and the
character-level alignments are generated by m2m-
aligner (Jiampojamarn et al, 2007).
4.6 Cognate Interference
Cognates are words that share their linguistic origin.
For example, English become and German bekom-
men have evolved from the same word in a com-
mon ancestor language. Other cognates are words
that have been transfered between languages; for ex-
ample, English system comes from the Greek word
??????? via Latin and French. On average, pairs
of cognates exhibit higher orthographic similarity
than unrelated translation pairs (Kondrak, 2013).
Cognate interference may cause an L1-speaker
to use a cognate word instead of a correct English
translation (for example, become instead of get).
Another instance of cognate interference is mis-
spelling of an English word under the influence of
the L1 spelling (Table 1).
We aim to detect cognate interference by identi-
fying the cases where the cognate word is closer to
1http://aspell.net
142
Misspelling Intended Cognate
developped developed developpe? (Fre)
exemple example exemple (Fre)
organisation organization organisation (Ger)
conzentrated concentrated konzentrierte (Ger)
comercial commercial comercial (Spa)
sistem system sistema (Spa)
Table 1: Examples of cognate interference in the data.
the misspelling than to the intended word (Figure 1).
We define one feature to represent each language L,
for which we could find a downloadable bilingual
English-L dictionary. We use the following algo-
rithm:
1. For each misspelled English word m found in
a document, identify the most likely intended
word e using a spell-checking program.
2. For each language L:
(a) Look up the translation f of the intended
word e in language L.
(b) Compute the orthographic edit distance D
between the words.
(c) If D(e, f) < t then f is assumed to be a
cognate of e.
(d) If f is a cognate and D(m, f) < D(e, f)
then we consider it as a clue that L = L1.
We use a simple method of computing ortho-
graphic distance with threshold t = 0.58 defined
as the baseline method by Bergsma and Kondrak
(2007). However, more accurate methods of cog-
nate identification discussed in that paper could also
be used.
Misspellings can betray cognate interference even
if the misspelled word has no direct cognate in
language L1. For example, a Spanish speaker
might spell the word quick as cuick because of
the existence of numerous cognates such as ques-
tion/cuestio?n. Our misspelling features can detect
such phenomena at the character level; in this case,
qu:cu corresponds to an individual misspelling fea-
ture.
4.7 Meta-features
We included a number of document-specific meta-
features as suggested by Bergsma et al (2012): the
conzentratedconcentrated konzentrierte0.30.4
Figure 1: A cognate word influencing the spelling.
average number of words per sentence, the average
word length, as well as the total number of char-
acters, words, and sentences in a document. We
reasoned that writers from certain linguistic back-
grounds may prefer many short sentences, while
other writers may prefer fewer but longer sentences.
Similarly, a particular linguistic background may in-
fluence the preference for shorter or longer words.
5 Results
The dataset used for experiments was the TOEFL11
Non-Native English Corpus (Blanchard et al, 2013).
The dataset was split into three smaller datasets: the
Training set, consisting of 9900 essays evenly dis-
tributed across 9 languages, the Development set,
which contained a further 1100 essays, and the Test
set, which also contained 1100 essays. As the data
had a staggered release, we used the data differently.
We further split the Training set, with a split of 80%
for training, and 10% for development and testing.
We then used the Development set as a held-out test
set. For held-out testing, the classifier was trained on
all data in the Training set, and for final testing, the
classifier was trained on all data in both the Training
and Development sets.
We used four different combinations of features
for our task submissions. The results are shown in
Table 2. We include the following accuracy values:
(1) the results that we obtained on the Development
set before the Test data release, (2) the official Test
set results provided by the organizers (Tetreault et
al., 2013), (3) the actual Test set results, and (4) the
mean cross-validation results (for submissions 1 and
3). The difference between the official and the ac-
tual Test set results is attributed to two mistakes in
our submissions. In submission 1, the feature lists
used for training and testing did not match. In sub-
missions 3 and 4, only non-lexicalized syntax pro-
duction rules were used, whereas our intention was
to use all of them.
143
No. Features Dev Org Test CV
1 Base 82.0 61.2 80.4 58.2
2 ? cont. words 67.4 68.7 68.7 ?
3 + char 81.4 80.3 81.7 58.5
4 + char + meta 81.2 80.0 80.8 ?
Table 2: Accuracy of our submissions.
All four submissions used the following base
combination of features:
? word unigrams
? word bigrams
? error alignments
? syntax production rules
? word-level cognate interference features
In addition, submission 3 includes character bi-
grams, while submission 4 includes both character
bigrams and meta-features. In submission 2, only
function words are used, with the exclusion of con-
tent words.
Our best submission, which achieves 81.73% ac-
curacy on the Test set, includes all features discussed
in Section 4 except POS bigrams. Early tests in-
dicated that any gains obtained with POS bigrams
were absorbed by the production rules, so they were
excluded form the final experiments. Character bi-
grams help on the Test set but not on the Devel-
opment set. The meta-features decrease accuracy
on both sets. Finally, the content words dramati-
cally improve accuracy. The reason we included a
submission which did not use content words is that
it is a common practice in previous work. In our
analysis of the data, we found content words that
were highly indicative of the language of the writer.
Particularly, words and phrases which contained the
speaker?s home country were useful in predicting the
language. It should be noted that this correspon-
dence may be dependent upon the prompt given to
the writer. Furthermore, it may lead to false posi-
tives for L1 speakers who live in multi-lingual coun-
tries.
5.1 Confusion Matrix
We present the confusion matrix for our best submis-
sion in Table 5.1. The highest number of incorrect
A C F G H I J K S T Tu
ARA 83 0 0 0 2 2 2 1 4 5 1
CHI 1 81 2 0 1 0 8 6 1 0 0
FRE 6 0 82 2 1 3 0 0 1 0 5
GER 1 0 0 90 1 1 1 0 2 0 4
HIN 1 2 2 0 76 1 0 0 0 16 2
ITA 1 1 0 1 0 89 1 0 5 1 1
JPN 2 1 1 1 0 1 86 6 0 0 2
KOR 1 8 0 0 0 0 11 78 0 1 1
SPA 2 2 7 0 3 5 0 2 75 0 4
TEL 2 0 0 2 15 0 0 0 1 80 0
TUR 4 3 2 1 0 1 1 5 2 2 79
Table 3: Confusion Matrix for our best classifier.
Features Test
Full system 81.7
w/o error alignments 81.3
w/o word unigrams 81.1
w/o cognate features 81.0
w/o production rules 80.6
w/o character bigrams 80.4
w/o word bigrams 76.7
Table 4: Accuracy of various feature combinations.
classifications are between languages that are either
linguistically or culturally related (Jarvis and Cross-
ley, 2012). For example, Korean is often misclassi-
fied as Japanese or Chinese. The two languages are
not linguistically related to Korean, but both have
historically had cultural ties with Korean. Likewise,
while Hindi and Telugu are not related linguistically,
they are both spoken in the same geographic area,
and speakers are likely to have contact with each
other.
5.2 Ablation Study
Table 4 shows the results of an ablation experiment
on our best-performing submission. The word bi-
grams contribute the most to the classification; their
removal increases the relative error rate by 27%. The
word unigrams contribute much less., This is un-
surprising, as much of the information contained in
the word unigrams is also contained in the bigrams.
The remaining features are also useful. In particu-
lar, our cognate interference features, despite apply-
ing to only 4 of 11 languages, reduce errors by about
4%.
144
6 Conclusions and Future Work
We have described the system that we have devel-
oped for the NLI 2013 Shared Task. The system
combines features that are prevalent in the litera-
ture with our own novel character-level spelling fea-
tures and word cognate interference features. Most
of the features that we experimented with appear
to increase the overall accuracy, which contradicts
the view that simple bag-of-words usually perform
better than more complex feature sets (Sebastiani,
2002).
Our cognate features can be expanded by includ-
ing languages that do not use the Latin script, such
as Russian and Greek, as demonstrated by Bergsma
and Kondrak (2007). We utilized bilingual dictio-
naries representing only four of the eleven languages
in this task2; yet our cognate interference features
still improved classifier accuracy. With more re-
sources and with better methods of cognate identi-
fication, the cognate features have the potential to
further contribute to native language identification.
Our error-alignment features can likewise be fur-
ther investigated in the future. Currently, after ana-
lyzing texts with a spell-checker, we automatically
accept the first suggestion as the correct one. In
many cases, this leads to faulty corrections, and mis-
leading alignments. By using context sensitive spell-
checking, we can choose better corrections, and ob-
tain information which improves classification.
This shared task was a wonderful introduction
to Native Language Identification, and an excellent
learning experience for members of our group,
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 656?663.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 327?337,
Montre?al, Canada.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
2French, Spanish, German, and Italian.
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372?379.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in ker-
nel methods, pages 169?184. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430.
Grzegorz Kondrak. 2013. Word similarity, cognation,
and translational equivalence. To appear.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, pages 9?16, Prague,
Czech Republic.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK.
145
