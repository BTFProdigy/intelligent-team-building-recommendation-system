Coling 2010: Poster Volume, pages 605?613,
Beijing, August 2010
Best Topic Word Selection for Topic Labelling
Jey Han Lau,?? David Newman,?? Sarvnaz Karimi? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California
jhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.net
Abstract
This paper presents the novel task of best
topic word selection, that is the selection
of the topic word that is the best label for
a given topic, as a means of enhancing the
interpretation and visualisation of topic
models. We propose a number of features
intended to capture the best topic word,
and show that, in combination as inputs to
a reranking model, we are able to consis-
tently achieve results above the baseline of
simply selecting the highest-ranked topic
word. This is the case both when training
in-domain over other labelled topics for
that topic model, and cross-domain, us-
ing only labellings from independent topic
models learned over document collections
from different domains and genres.
1 Introduction
In the short time since its inception, topic mod-
elling (Blei et al, 2003) has become a main-
stream technique for tasks as diverse as multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008) and information retrieval (Wei
and Croft, 2006). For many of these tasks, the
multinomial topics learned by the topic model can
be interpreted natively as probabilities, or mapped
onto a pre-defined discrete class set. However,
for tasks where the learned topics are provided
to humans as a first-order output, e.g. for use in
document collection analysis/navigation, it can be
difficult for the end-user to interpret the rich sta-
tistical information encoded in the topics. This
research is concerned with making topics more
readily human interpretable, by selecting a single
term with which to label the topic.
Although topics are formally a multinomial dis-
tribution over terms, with every term having finite
probability in every topic, topics are usually dis-
played by printing the top-10 terms (i.e. the 10
most probable terms) in the topic. These top-10
terms typically account for about 30% of the topic
mass for reasonable setting of number of topics,
and usually provide sufficient information to de-
termine the subject area and interpretation of a
topic, and distinguish one topic from another.
Our research task can be illustrated via the top-
10 terms in the following topic, learned from a
book collection. Terms wi are presented in de-
scending order of P (wi|tj) for the topic tj :
trout fish fly fishing water angler stream rod
flies salmon
Clearly the topic relates to fishing, and indeed,
the fourth term fishing is an excellent label for the
topic. The task is thus termed best word or most
representative word selection, as we are selecting
the label from the closed set of the top-N topic
words in that topic.
Naturally, not all topics are equally coherent,
however, and the lower the topic coherence, the
more difficult the label selection task becomes.
For example:
oct sept nov aug dec july sun lite adv globe
appears to conflate months with newspaper
names, and no one of these topic words is able to
capture the topic accurately. As such, our method-
ology presupposes an automatic means of rating
topics for coherence. Fortunately, recent research
by Newman et al (2010) has shown that this is
achievable at levels approaching human perfor-
mance, meaning that this is not an unreasonable
assumption.
Labelling topics has applications across a di-
verse range of tasks. Our original interest in the
605
problem stems from work in document collection
visualisation/navigation, and the realisation that
presenting users with topics natively (e.g. as rep-
resented by the top-N terms) is ineffective, and
would be significantly enhanced if we could au-
tomatically predict succinct labels for each topic.
Another application area where labelling has been
shown to enhance the utility of topic models is se-
lectional preference learning via topic modelling
(Ritter et al, to appear). Here, topic labelling via
taxonomic classes (e.g. WordNet synsets) can lead
to better topic generalisation, in addition to better
human readability.
This paper is based around the assumption that
an appropriate label for a topic can be found
among the high-ranking (high probability) terms
in that topic. We assess the suitability of each term
by way of comparison with other high-ranking
terms in that same topic, using simple pointwise
mutual information and conditional probabilities.
We first experiment with a simple ranking method
based on the component scores, and then move
on to using those scores, along with features from
WordNet and from the original topic model, in a
ranking support vector regression (SVR) frame-
work. Our experiments demonstrate that we are
able to perform the task significantly better than
the baseline of selecting the topic word of high-
est marginal probability, including when training
the ranking model on labelled topics from other
document collections.
2 Related Work
Predictably, there has been significant work on in-
terpreting topics in the context of topic modelling.
Topic are conventionally interpreted via the top-
N words in each topic (Blei et al, 2003; Grif-
fiths and Steyvers, 2004), or alternatively by post-
hoc manual labelling of each topic based on do-
main knowledge and subjective interpretation of
each topic (Wang and McCallum, 2006; Mei et
al., 2006).
Mei et al (2007) proposed various approaches
for automatically suggesting phrasal labels for
topics, based on first extracting phrases from the
document collection, and subsequently ranking
the phrases based on KL divergence with a given
topic.
Magatti et al (2009) proposed a method for la-
belling topics induced by hierarchical topic mod-
elling, based on ontological alignment with the
Google Directory (gDir) hierarchy, and optionally
expanding topics based on a thesaurus or Word-
Net. Preliminary experiments suggest the method
has promise, but the method crucially relies on
both a hierarchical topic model and a pre-existing
ontology, so has limited applicability.
Over the general task of labelling a learned se-
mantic class, Pantel and Ravichandran (2004) pro-
posed the use of lexico-semantic patterns involv-
ing each member of that class to learn a (usu-
ally hypernym) label. The proposed method was
shown to perform well over the semantically ho-
mogeneous, fine-grained clusters learned by CBC
(Pantel and Lin, 2002), but for the coarse-grained,
heterogeneous topics learned by topic modelling,
it is questionable whether it would work as well.
The first works to report on human scoring of
topics were Chang et al (2009) and Newman et
al. (2010). The first study used a novel but syn-
thetic intruder detection task where humans eval-
uate both topics (that had an intruder word), and
assignment of topics to documents (that had an in-
truder topic). The second study had humans di-
rectly score topics learned by a topic model. This
latter work introduced the pointwise mutual infor-
mation (PMI) score to model human scoring. Fol-
lowing this work, we use PMI as features in the
ranking SVR model.
3 Methodology
Our task is to predict which words annotators
tend to select as most representative or best words
when presented with a list of ten words. Since
annotators are not generally unanimous in their
choice of best word, we formulate this as a rank-
ing task, and treat the top-1, 2 and 3 system-
ranked items as the best words, and compare that
to the top-1, 2 and 3 words chosen most frequently
by annotators. In this section, we describe the fea-
tures that may be useful for this ranking task. We
start with features motivated by word association.
An obvious idea is that the most representative
word should be readily evoked by other words
in the topic. For example, given a list of words
?space, earth, moon, nasa, mission?, which is a
606
Space Exploration topic, space could arguably be
the most representative word. This is because
it is natural to think about the word space after
seeing the words earth, moon and nasa individ-
ually. A good candidate for best word could be
the word that has high average conditional proba-
bility given each of the other words. To calculate
conditional probability, we use word counts from
the entire collection of English Wikipedia articles.
Conditional probability is defined as:
P (wi|wj) =
P (wi, wj)
P (wj)
,
where i 6= j and P (wi, wj) is the probability of
observing both wi and wj in the same sliding win-
dow, and P (wi) is the overall probability of word
wi in the corpus. In the above example, evoked by
means that space would fill the slot of wi. The av-
erage conditional probability for word wi is given
by:
avg-CP1(wi) = 19
?
j
P (wi|wj),
for j = 1 . . . 10, j 6= i (this range of indices ap-
plies to all following average quantities).
In other cases, we have the flip situation, where
the most representative word may evoke (rather
than be evoked by) other words in the list of ten
words. Imagine a NASCAR Racing topic, which
has a list of words ?race, car, nascar, driver, rac-
ing?. Given the word nascar, words from the list
such as race, car, racing and driver might come
to mind because nascar is heavily associated with
these words. Therefore, a good candidate, wi,
might also correlate with high P (wj |wi). As be-
fore, the average conditional probability (here de-
noted with CP2) for word wi is given by:
avg-CP2(wi) = 19
?
j
P (wj |wi).
Another approach to measuring word associa-
tion is by calculating pointwise mutual informa-
tion (PMI) between word pairs. Unlike condi-
tional probability, PMI is symmetric and thus the
order of words in a pair does not matter. We
calculate PMI using word counts from English
Wikipedia as follows:
PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .
The average PMI for word wi is given by:
avg-PMI(wi) = 19
?
j
PMI(wi, wj).
The topic model produces an ordered list of
words for each topic, and the ordering is given by
the marginal probability of each word given that
topic, P (wi|tj). The ranking of words based on
these probabilities indicates the importance of a
word in a topic, and it is also a feature that we use
for predicting the most representative word.
We also observe that sometimes the most repre-
sentative words are generalized concepts of other
words. As such, hypernym relations could be an-
other feature that may be relevant to predicting the
best word. To this end, we use WordNet to find
hypernym relations between pairs of words in a
topic and obtain a set of boolean-valued relation-
ships for each topic word.
Our last feature is the distributional similar-
ity scores of Pantel et al (2009), as trained over
Wikipedia.1 This takes the form of representing
the distributional similarity between each pairing
of terms sim(wi|wj); if wi is not in the top-200
most similar terms for a given wj , we assume it to
have a similarity of 0.
While the above features can be used alone
to get a ranking on the ten topic words, we can
also use various combinations of features in a
reranking model such as support vector regres-
sion (SVMrank: Joachims (2006)). Applying the
features described above ? conditional probabil-
ities, PMI, WordNet hypernym relations, the topic
model word rank, and Pantel?s distributional simi-
larity score ? as features for SVMrank, a ranking
of words is produced and candidates for the most
representative word are selected by choosing the
top-ranked words.
607
NEWS stock market investor fund trading investment firm exchange ...
police gun officer crime shooting death killed street victim ...
food restaurant chef recipe cooking meat meal kitchen eat...
patient doctor medical cancer hospital heart blood surgery ...
BOOKS loom cloth thread warp weaving machine wool cotton yarn ...
god worship religion sacred ancient image temple sun earth ...
crop land wheat corn cattle acre grain farmer manure plough ...
sentence verb noun adjective grammar speech pronoun ...
Figure 1: Selected topics from the two collections
(each line is one topic, with fewer than ten topic
words displayed because of limited space)
4 Datasets
We used two collections of text documents from
different genres for our experiments. The first col-
lection (NEWS) was created by selecting 55,000
news articles from the LDC Gigaword corpus.
The second collection (BOOKS) was 12,000 En-
glish language books selected from the Inter-
net Archive American Libraries collection. The
NEWS and BOOKS collections provide a diverse
range of content for topic modeling. In the first
case ? news articles from the past decade written
by journalists ? each article usually attempts to
clearly and concisely convey information to the
reader, and hence the learned topics tend to be
fairly interpretable. For BOOKS (with publication
dates spanning more than a century), the writing
style often uses lengthy and descriptive prose, so
one sees a different style to the learned topics.
The input to the topic model is a bag-of-words
representation of the collection of text documents,
where word counts are preserved, but word order
is lost. After performing fairly standard tokeniza-
tion and limited lemmatisation, and creating a vo-
cabulary of terms that occurred at least ten times,
each corpus was converted into its bag-of-words
representation. We learned topic models for the
two collections, choosing a setting of T = 200
topics for NEWS and T = 400 topics for BOOKS.
After computing the PMI-score for each topic (ac-
cording to Newman et al (2010)), we selected 60
topics with high PMI-score, and 60 topics with
low PMI-score, from both corpora, resulting in a
total of 240 topics for human evaluation.
The 240 topics selected for human scoring were
1Accessed from http://demo.patrickpantel.
com/Content/LexSem/thesaurus.htm.
Features Description
PMI Pointwise mutual information
CP1 Conditional probability P (wi|?)
CP2 Conditional probability P (?|wi)
TM Rank Original topic model word rank
Hypernym WordNet hypernym relationships
PDS Pantel distributional similarity score
Table 1: Description of feature sets
each evaluated by between 10 and 20 users. For
the two topic models, we used the conventional
approach of displaying each topic with its top-10
terms. In a typical survey, a user was asked to
evaluate anywhere from 60 to 120 topics. The in-
structions asked the user to perform the following
tasks, for each topic in the survey: (a) score the
topic for ?usefulness? or ?coherence? on a scale
of 1 to 3; and (b) select the single best word that
exemplifies the topic (when score=3).
From both NEWS and BOOKS, the 40 topics
with the highest average human scores had rela-
tively complete data for the ?best word? selection
task (i.e. every time a user gave a topics score=3,
they also selected a ?best word?). The remain-
der of this paper is concerned with the 40 NEWS
topics and 40 BOOKS topics where we had ?best
word? data from the annotators. Sample topics
from these two sets are given in Figure 1.
To measure presentational bias (i.e. the extent
to which annotators tend to choose a word seen
earlier rather than later, particularly when armed
with the knowledge that words are presented in or-
der of probability), we reissued a survey using the
40 NEWS topics to ten additional annotators, but
this time the top-10 topic words were presented
in random order. Again, these ten new annotators
were asked to select the best word.
5 Experiments
We used average PMI and conditional probabili-
ties, CP1 and CP2, to rank the ten words in each
topic. Candidates for the best words were selected
by choosing the top-1, 2 and 3 ranked words.
We used the following weighted scoring func-
tion for evaluation:
Best-N score =
?N
i=1 n(wrevi)?N
i=1 n(wi)
608
Features Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
PMI 0.25 0.38 0.49
CP1 0.30 0.42 0.51
CP2 0.15 0.27 0.45
Upper bound 0.48 ? ?
Table 2: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for NEWS
Features Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
PMI 0.25 0.38 0.49
CP1 0.30 0.38 0.47
CP2 0.15 0.30 0.49
Upper bound 0.64 ? ?
Table 3: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for BOOKS
where wrevi is the ith term ranked by the system
and wi is the ith most popular term selected by
annotators; revi gives the index of the word wi
in the annotator?s list; and n(w) is the number of
votes given by annotators for word w.
The baseline is obtained using the original word
rank produced by the topic model based on topic
word probabilities P (wi|tj). An upperbound is
calculated by evaluating the decision of an annota-
tor against others for each topic. This upperbound
signifies the maximum accuracy for human anno-
tators on average; since the annotators were asked
to pick a single best word in the survey, only the
Best-1 upperbound can be obtained.
The Best-1/2/3 results are summarized in Ta-
ble 2 for NEWS and Table 3 for BOOKS. These
Best-N scores are computed just using the single
feature of PMI, CP1 and CP2 (each in turn) to rank
the words in each topic. None of these features
alone produces a result that exceeds baseline per-
formance.
To make better use of all the features described
in Section 3, namely the PMI score, conditional
probabilities (both directions), topic model word
rank, WordNet Hypernym relationships and Pan-
tel?s distributional similarity score, we build a
ranking classifier using SVMrank and evaluating
Feature Set Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
All Features 0.43 0.56 0.62
?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)
?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)
?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)
?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)
?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)
?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)
Upper bound 0.48 ? ?
Table 4: SVR-based best topic word results for
NEWS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
Feature Set Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
All Features 0.40 0.51 0.62
?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)
?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)
?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)
?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)
?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)
?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)
Upper bound 0.64 ? ?
Table 5: SVR-based best topic word results for
BOOKS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
using 10-fold cross validation. Our first approach
is to use the entire set of features to train the clas-
sifier. Following this, we also measure the effect
of each feature by ablating (removing) one fea-
ture at a time. The drop in Best-N score indicates
which features are the strongest predictors of the
best words (a larger drop in score indicates that
feature is more important). The results for Best-1,
Best-2 and Best-3 scores are summarized in Ta-
ble 4 for NEWS, and Table 5 for BOOKS (averaged
across the 10 iterations of cross validation).
We then produced a condensed set of features,
consisting of the conditional probabilities, the
original topic model word rank and the WordNet
hypernym relationships. This ?best? set of fea-
tures is used to make predictions of best words.
Results are improved in most cases, and are sum-
marized in Table 6 for both NEWS and BOOKS.
609
Dataset Best-1 Best-2 Best-3
NEWS
Baseline 0.35 0.50 0.59
Best Feat. Set 0.45 0.50 0.65
Upper bound 0.48 ? ?
BOOKS
Baseline 0.38 0.48 0.60
Best Feat. Set 0.48 0.56 0.66
Upper bound 0.64 ? ?
Table 6: Results with the best feature set com-
pared to the baseline
Dataset Best-1 Best-2 Best-3
NEWS baseline 0.35 0.50 0.59
BOOKS ? NEWS 0.38 0.56 0.62
NEWS upper bound 0.48 ? ?
BOOKS baseline 0.38 0.48 0.60
NEWS ? BOOKS 0.48 0.56 0.65
BOOKS upper bound 0.64 ? ?
Table 7: Results for cross-domain learning
We also tested whether the SVM classifier
could be trained using data from one domain, and
run on data from another domain. Using our two
datasets as these different domains, we trained a
model using BOOKS data and made predictions
for NEWS, and then we trained a model using
NEWS data and made predictions for BOOKS.
The results, shown in Table 7, indicate that
we are still able to outperform the baseline, even
when the ranking classifier is trained on a differ-
ent domain. In fact, when we trained a model
using NEWS, we saw almost no drop in perfor-
mance for predicting best words for BOOKS, and
improvement is seen for Best-2 score from NEWS.
This implies that the SVM classifier generalizes
well across domains and suggests the possibility
of having a fixed training model to predict best
words for any data.
In these experiments, topic words are presented
in the original order that the topic model produces,
i.e. in descending order of probability of a word
under a topic P (wi|tj). We noticed that the first
words of the topics are frequently selected as the
best words by annotators, and suspected that this
was introducing a bias towards the first word. As
our baseline scores are derived from this topic
word ordering, such a bias could give rise to an
artificially high baseline.
To investigate this effect, we ran a second anno-
Word Order Best-1 Best-2 Best-3
Original 0.35 0.50 0.59
Randomized 0.23 0.33 0.46
Table 8: Reduction of baseline scores for NEWS
when words are presented in random order to an-
notators.
2 4 6 8 10
0.
0
0.
1
0.
2
0.
3
0.
4
Rank
Fr
ac
tio
n 
of
 h
um
an
 s
el
ec
te
d 
be
st
 w
or
d
ordered
random
Figure 2: Bias for humans selecting the best word,
when the topic words are presented in their origi-
nal ordering (ordered) or randomised (random)
tation exercise over the same set of topics (but dif-
ferent annotators), to obtain a new set of best word
annotations for NEWS, with the topic words pre-
sented in random order. In Figure 2, we plot the
cumulative proportion of words selected as best
word by the annotators across the topics, in the
case of the random topic word order, mapping the
topic words back onto their original ranks in the
topic model. A slight drop can be observed in the
proportion of first- and second-ranked topic words
being selected when we randomise the topic word
order. When we recalculate the baseline accuracy
for NEWS on the basis of the new set of annota-
tions, we observe an appreciable drop in the scores
(see Table 8).
6 Discussion
From the experiments in Section 5, perhaps the
first thing to observe is: (a) the high performance
of the baseline, and (b) the relatively low (Best-
1) upper bound accuracy for the task. The first is
perhaps unsurprising, given that it represents the
610
topic model?s own interpretation of the word(s)
which are most representative of that topic. In this
sense, we have set our sights high in attempting to
better the baseline. The upper bound accuracy is
a reflection of both the inter-annotator agreement,
and the best that we can meaningfully expect to
do for the task. That is, any result higher than this
would paradoxically suggest that we are able to do
better at a task than humans, where we are evalu-
ating ourselves relative to the labellings of those
humans. The upper bound for NEWS was slightly
less than 0.5, indicating that humans agree on the
best topic word only 50% of the time. To better
understand what is happening here, consider the
following topic from Figure 1:
health drug patient medical doctor hospital
care cancer treatment disease
This is clearly a coherent topic, but at least two
topic words suggest themselves as labels: health
and medical. By way of having between 10 and 20
annotators (uniquely) label a given topic, and in-
terpreting the multiple labellings probabilistically,
we are side-stepping the inter-annotator agree-
ment issue, but ultimately, for the Best-1 evalu-
ation, we are forced to select one term only, and
consider any alternative to be wrong. Because an-
notators selected only one best topic word, we un-
fortunately have no way of performing Best-2 or
Best-3 upper bound evaluation and deal with top-
ics such as this, but would expect the numbers to
rise appreciably.
Looking at the original feature rankings in Ta-
bles 2 and 3, no clear picture emerges as to which
of the three methods (PMI, CP1 and CP2) was
most successful, but there were certainly clear dif-
ferences in the relative numbers for each, point-
ing to possible complementarity in the scoring.
This expectation was born out in the results for
the reranking model in Tables 4 and 5, where the
combined feature set surpassed the baseline in all
cases, and feature ablation tended to lead to a drop
in results, with the single most effective feature set
being CP1 (P (wi|?)), followed by CP2 (P (?|wi))
and topic model rank. The lexical semantic fea-
tures of WordNet hypernymy and PDS (Pantel?s
distributional similarity) were the worst perform-
ers, often having no or negative impact on the re-
sults.
Comparing the best results for the SVR-based
reranking model and the upper bound Best-1
score, we approach the upper bound performance
for NEWS, but are still quite a way off with
BOOKS when training in-domain. This is encour-
aging, but a slightly artificial result in terms of the
broader applicability of this research, as what it
means in practical terms is that if we can access
multi-annotator best word labelling for the ma-
jority of topics in a given topic model, we can
use those annotations to predict the best word for
the remainder of the topics with reasonably suc-
cess. When we look to the cross-domain results,
however, we see that we almost perfectly replicate
the best-achieved Best-1, Best-2 and Best-3 in-
domain results for BOOKS by training on NEWS
(making no use of the annotations for BOOKS).
Applying the annotations for BOOKS to NEWS is
less successful in terms of Best-1 accuracy, but we
actually achieve higher Best-2, and largely mir-
ror the Best-3 results as compared to the best of
the in-domain results in Table 6. This leads to
the much more compelling conclusion that we can
take annotations from an independent topic model
(based on a completely unrelated document col-
lection), and apply them to successfully model the
best topic word for a new topic model, without
requiring any additional annotation. As we now
have two sets of topics multiply-annotated for best
words, this result suggests that we can perform the
best topic word selection task with high success
over novel topic models.
We carried out manual analysis of topics where
the model did particularly poorly, to get a sense
for how and where our model is being led astray.
One such example is the topic:
race car nascar driver racing cup winston team
gordon season
where the following topic words were selected by
our annotators: nascar (8 people), race (2 peo-
ple), and racing (2 people). First, we observe the
split between race and racing, where more judi-
cious lemmatisation/stemming would make both
the annotation easier and the evaluation cleaner.
The SVR model tends to select more common,
general terms, so in this case chose race as the
best word, and ranked nascar third. This is one
611
instance were nascar evokes all of the other words
effectively, but not conversely (racing is asso-
ciated with many events/sports beyond nascar,
e.g.).
Another topic where our model had difficulty
was:
window nave aisle transept chapel tower arch
pointed arches roof
where our best model selected nave, while the hu-
man annotators selected chapel (6 people), arch
(2 people), nave, roof , tower and transept (1 per-
son each). Clearly, our annotators struggled to
come up with a best word here, despite the topic
again being coherent. This is an obvious candi-
date for labelling with a hypernym/holonym of
the topic words (e.g. church or church architec-
ture), and points to the limitations of best word la-
belling ? there are certainly many topics where
best word labelling works, as our upper bound
analysis demonstrated, but there are equally many
topics where the most natural label is not found
in the top-ranked topic words. While this points
to slight naivety in the current task set up ? we
are forcing annotators to label words with topic
words, where we know that this is sub-optimal
for a significant number of topics ? we contend
that our numbers suggest that: (a) consistent best
topic word labelling is possible at least 50% of
the time; and (b) we have developed a method
which is highly adept at labelling these topics. As
a way forward, we intend to relax the constraint
on the topic label needing to be based on a topic
word, and explore the possibility of predicting
which topics are best labelled with topic words,
and which require independent labels. For topics
which can be labelled with topic words, we can
use the methodology developed here, and for top-
ics where this is predicted to be sub-optimal, we
intend to build on the work of Mei et al (2007),
Pantel and Ravichandran (2004) and others in se-
lecting phrasal/hypernym labels for topics. We are
also interested in applying the methodology pro-
posed herein to the closely-related task of intruder
word, or worst topic word, detection, as proposed
by Chang et al (2009).
Finally, looking to the question of the impact of
the presentation order of the topic words on best
word selection, it would appear that our baseline
is possibly an over-estimate (based on Table 8).
Having said that, the flipside of the bias is that it
leads to more consistency in the annotations, and
tends to help in tie-breaking of examples such as
race and racing from above, for example. In sup-
port of this claim, the upper bound Best-1 accu-
racy of the randomised annotations, relative to the
original gold-standard is 0.44, slightly below the
original upper bound for NEWS. More work is
needed to determine the real impact of this bias
on the overall task setup and evaluation.
7 Conclusion
This paper has presented the novel task of best
topic word selection, that is the selection of the
topic word that is the best label for a given topic.
We proposed a number of features intended to
capture the best topic word, and demonstrated
that, while they were relatively unsuccessful in
isolation, in combination as inputs to a rerank-
ing model, we were able to consistently achieve
results above the baseline of simply selecting the
highest-ranked topic word, both when training in-
domain over other labelled topics for that topic
model, and cross-domain, using only labellings
from independent topic models learned over docu-
ment collections from different domains and gen-
res.
Acknowledgements
NICTA is funded by the Australian government as repre-
sented by Department of Broadband, Communication and
Digital Economy, and the Australian Research Council
through the ICT centre of Excellence programme. DN has
also been supported by a grant from the Institute of Museum
and Library Services, and a Google Research Award.
References
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Brody, S. and M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the EACL (EACL 2009), pages 103?111, Athens,
Greece.
Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the 23rd
612
Annual Conference on Neural Information Process-
ing Systems (NIPS 2009), pages 288?296, Vancou-
ver, Canada.
Griffiths, T. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101:5228?5235.
Haghighi, A. and L. Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics ?
Human Language Technologies 2009 (NAACL HLT
2009), pages 362?370, Boulder, USA.
Joachims, T. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226, Philadelphia, USA.
Magatti, D., S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In Proceedings of the
International Conference on Intelligent Systems De-
sign and Applications, pages 1227?1232, Pisa, Italy.
Mei, Q., C. Liu, H. Su, and C. Zhai. 2006. A prob-
abilistic approach to spatiotemporal theme pattern
mining on weblogs. In Proceedings of the 15th
International World Wide Web Conference (WWW
2006), pages 533?542.
Mei, Q., X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2007), pages 490?499, San Jose, USA.
Newman, D., J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
Pantel, P. and D. Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 613?619, Ed-
monton, Canada.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 321?
328, Boston, USA.
Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,
and V. Vyas. 2009. Web-scale distributional sim-
ilarity and entity set expansion. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
938?947, Singapore.
Ritter, A, Mausam, and O Etzioni. to appear. A la-
tent Dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the ACL (ACL 2010), Uppsala, Sweden.
Titov, I. and R. McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International World Wide Web
Conference (WWW 2008), pages 111?120, Beijing,
China.
Wang, X. and A. McCallum. 2006. Topics over time:
A non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), pages 424?433,
Philadelphia, USA.
Wei, S. and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of 29th
International ACM-SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2006), pages 178?185, Seattle, USA.
613
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Word Sense Induction for Novel Sense Detection
Jey Han Lau,?? Paul Cook,? Diana McCarthy, ?
David Newman,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
? Lexical Computing
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
Abstract
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
1 Introduction
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling ? using La-
tent Dirichlet Allocation (LDA: Blei et al(2003))
and derivative approaches ? and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics ? in the form of
multinomial probability distributions over words
? and per-document topic assignments ? in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
2 Methodology
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
591
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document?s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models?s probabilistic assignment of
topics to words for the WSI task.
2.1 Data Representation and Pre-processing
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The ?document? in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pado? and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
2.2 Topic Modelling
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (?instance?
in WSI parlance) as our distribution over senses
for that word.
1Notwithstanding the one sense per discourse heuristic
(Gale et al 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al 2006).
In our initial experiments, we use LDA topic
modelling, which requires us to set T , the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P (t = z|d) then; (2) draw a word w from
the chosen topic P (w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
P (w|d) =
T
?
z=1
P (w|t = z)P (t = z|d).
In standard LDA, the user needs to specify the
number of topics T . In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al(2006)),3 where, for each
document, a distribution of mixture components
P (t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ?
DP (?,H); (2) for each document d, generate dis-
tribution P (t|d) ? DP (?0, G0); (3) draw a la-
tent topic z from the document?s mixture compo-
nent distribution P (t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P (w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, argmaxz P (t = z|d).
3 SemEval Experiments
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/?blei/
topicmodeling.html) in our experiments.
4The two HDP parameters ? and ?0 control the variabil-
ity of senses in the documents. In particular, ? controls the
degree of sharing of topics across documents ? a high ?
value leads to more topics, as topics for different documents
are more dissimilar. ?0, on the other hand, controls the de-
gree of mixing of topics within a document? a high ?0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
592
Soroa, 2007; Manandhar et al 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
3.1 SemEval-2010
3.1.1 Dataset and Methodology
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al 2010), and
are hence omitted from this paper.
In our original experiments with LDA, we set
the number of topics (T ) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
1. LDA+Variable T : LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T : LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T ), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom ? randomly assign each test instance to one
of four senses; (2) Baseline MFS ? most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
3.2 SemEval-2010 Results
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
593
System WSD (80%/20%)All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark
UoY 0.62 0.67 0.59
Table 1: WSD F-score over the SemEval-2010 dataset
Looking first at the results for LDA, we see
that the first LDA approach (variable T ) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn?t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn?t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are ?junk? senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the ?Vari-
able T ? version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the ?genuine? topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly ?split? a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
594
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
3.3 Comparison with other Topic Modelling
Approaches to WSI
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a ?lay-
ered? feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn?t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al(2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
595
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
3.4 Discussion
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD?s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD?s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
4 Identifying Novel Senses
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
1. We all know Frankie is adorable, but does he
have swag? [swag = ?style?]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ?send a mes-
sage on Twitter?]
These senses of swag and tweet are not included
in many dictionaries or computational lexicons ?
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) ? yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin?
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
596
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
4.1 Method
Given two corpora ? a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts ? we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (?Nov?) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
Nov(w) = max
({
ps(ti)? pr(ti)
pr(ti)
: ti ? T
})
(1)
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
4.2 Data
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC ? consisting primar-
ily of British English text from the late 20th cen-
tury ? as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al 2008) ? a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types ? as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
597
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation ? only for selecting items
for our dataset ? we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) ?In-
ternet domain?; export (v) ?export data?; mirror
(n) ?mirror website?; poster (n) ?one who posts
online?; and worm (n) ?malicious program?. For
each of the 5 lemmas with novel senses, a sec-
ond annotator ? also a native English-speaking
author of this paper ? annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
4.3 Results
We compute novelty (?Nov?, Equation 1) for all
10 items in our dataset, based on the output of the
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 ?
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 ?
cinema (n) 9.7 1.30 ?
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 ?
symptom (n) 2.1 1.16 ?
Table 4: Novelty score (?Nov?), ratio of frequency in
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
?Novelty? in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p < .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn ?Freq. ratio? in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p > .05).
Examining the frequency of the novel senses?
shown in column ?Novel sense freq.? in Table 4
? we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n?t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
598
Lemma
Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (?Internet domain?) being split across two
induced senses ? the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma?s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (?Nov?), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense ? in terms of F-score ?
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses ? in
terms of F-score ? as the novel sense could of-
fer further improvements. In this case ? results
shown in the final three columns of Table 5 ?
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
5 Conclusion
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
599
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1?10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103?111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233?237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67?71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-He?le`ne Corre?ard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125?
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355?358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116?126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177?182,
Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161?199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
600
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, Oregon.
601
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530?539,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence
and Topic Model Quality
Jey Han Lau
Dept of Philosophy
King?s College London
jeyhan.lau@gmail.com
David Newman
Google
dnewman@google.com
Timothy Baldwin
Dept of Computing and
Information Systems
The University of Melbourne
tb@ldwin.net
Abstract
Topic models based on latent Dirichlet al-
location and related methods are used in a
range of user-focused tasks including doc-
ument navigation and trend analysis, but
evaluation of the intrinsic quality of the
topic model and topics remains an open
research area. In this work, we explore
the two tasks of automatic evaluation of
single topics and automatic evaluation of
whole topic models, and provide recom-
mendations on the best strategy for per-
forming the two tasks, in addition to pro-
viding an open-source toolkit for topic and
topic model evaluation.
1 Introduction
Topic modelling based on Latent Dirichlet Alloca-
tion (LDA: Blei et al. (2003)) and related methods
is increasingly being used in user-focused tasks, in
contexts such as the evaluation of scientific impact
(McCallum et al., 2006; Hall et al., 2008), trend
analysis (Bolelli et al., 2009; Lau et al., 2012a)
and document search (Wang et al., 2007). The
LDA model is based on the assumption that doc-
ument collections have latent topics, in the form
of a multinomial distribution of words, which is
typically presented to users via its top-N highest-
probability words. In NLP, topic models are gener-
ally used as a means of preprocessing a document
collection, and the topics and per-document topic
allocations are fed into downstream applications
such as document summarisation (Haghighi and
Vanderwende, 2009), novel word sense detection
methods (Lau et al., 2012b) and machine transla-
tion (Zhao and Xing, 2007). In fields such as the
digital humanities, on the other hand, human users
interact directly with the output of topic models. It
is this context of topic modelling for direct human
consumption that we target in this paper.
The topics produced by topic models have a
varying degree of human-interpretability. To il-
lustrate this, we present two topics automatically
learnt from a collection of news articles:
1. ?farmers, farm, food, rice, agriculture?
2. ?stories, undated, receive, scheduled, clients?
The first topic is clearly related to agriculture.
The subject of the second topic, however, is less
clear, and may confuse users if presented to them
as part of a larger topic model. Measuring the
human-interpretability of topics and the overall
topic model is the core topic of this paper.
Various methodologies have been proposed for
measuring the semantic interpretability of topics.
In Chang et al. (2009), the authors proposed an
indirect approach based on word intrusion, where
?intruder words? are randomly injected into topics
and human users are asked to identify the intruder
words. The word intrusion task builds on the as-
sumption that the intruder words are more iden-
tifiable in coherent topics than in incoherent top-
ics, and thus the interpretability of a topic can be
estimated by measuring how readily the intruder
words can be manually identified by annotators.
Since its inception, the method of Chang et
al. (2009) has been used variously as a means
of assessing topic models (Paul and Girju, 2010;
Reisinger et al., 2010; Hall et al., 2012). Despite
its wide acceptance, the method relies on manual
annotation and has never been automated. This is
one of the primary contributions of this work: the
demonstration that we can automate the method of
Chang et al. (2009) at near-human levels of accu-
racy, as a result of which we can perform auto-
matic evaluation of the human-interpretability of
topics, as well as topic models.
There has been prior work to directly estimate
the human-interpretability of topics through au-
tomatic means. For example, Newman et al.
530
(2010) introduced the notion of topic ?coher-
ence?, and proposed an automatic method for es-
timating topic coherence based on pairwise point-
wise mutual information (PMI) between the topic
words. Mimno et al. (2011) similarly introduced
a methodology for computing coherence, replac-
ing PMI with log conditional probability. Musat
et al. (2011) incorporated the WordNet hierarchy
to capture the relevance of topics, and in Aletras
and Stevenson (2013a), the authors proposed the
use of distributional similarity for computing the
pairwise association of the topic words. One ap-
plication of these methods has been to remove in-
coherent topics before generating labels for topics
(Lau et al., 2011; Aletras and Stevenson, 2013b).
Ultimately, all these methodologies, and also
the word intrusion approach, attempt to assess the
same quality: the human-interpretability of top-
ics. The relationship between these methodolo-
gies, however, is poorly understood, and there is
no consensus on what is the best approach for
computing the semantic interpretability of topic
models. This is a second contribution of this pa-
per: we perform a systematic empirical compar-
ison of the different methods and find apprecia-
ble differences between them. We further go on to
propose an improved formulation of Newman et
al. (2010) based on normalised PMI. Finally, we
release a toolkit which implements the topic inter-
pretability measures described in this paper.
2 Related Work
Chang et al. (2009) challenged the conventional
wisdom that held-out likelihood ? often com-
puted as the perplexity of test data or unseen doc-
uments ? is the only way to evaluate topic mod-
els. To measure the human-interpretability of top-
ics, the authors proposed a word intrusion task
and conducted experiments using three topic mod-
els: Latent Dirichlet Allocation (LDA: Blei et al.
(2003)), Probabilistic Latent Semantic Indexing
(PLSI: Hofmann (1999)) and the Correlated Topic
Model (CTM: Blei and Lafferty (2005)). Contrary
to expectation, they found that perplexity corre-
lates negatively with topic interpretability.
In the word intrusion task, each topic is pre-
sented as a list of six words ? the five most proba-
ble topic words and a randomly-selected ?intruder
word?, which has low probability in the topic of
interest, but high probability in other topics ?
and human users are asked to identify the intruder
word that does not belong to the topic in question.
Newman et al. (2010) capture topic inter-
pretability using a more direct approach, by asking
human users to rate topics (represented by their
top-10 topic words) on a 3-point scale based on
how coherent the topic words are (i.e. their ob-
served coherence). They proposed several ways of
automating the estimation of the observed coher-
ence, and ultimately found that a simple method
based on PMI term co-occurrence within a sliding
context window over English Wikipedia produces
the consistently best result, nearing levels of inter-
annotator agreement over topics learnt from two
distinct document collections.
Mimno et al. (2011) proposed a closely-related
method for evaluating semantic coherence, replac-
ing PMI with log conditional probability. Rather
than using Wikipedia for sampling the word co-
occurrence counts, Mimno et al. (2011) used the
topic-modelled documents, and found that their
measure correlates well with human judgements
of observed coherence (where topics were rated
in the same manner as Newman et al. (2010),
based on a 3-point ordinal scale). To incorpo-
rate the evaluation of semantic coherence into the
topic model, the authors proposed to record words
that co-occur together frequently, and update the
counts of all associated words before and after the
sampling of a new topic assignment in the Gibbs
sampler. This variant of topic model was shown to
produce more coherent topics than LDA based on
the log conditional probability coherence measure.
Aletras and Stevenson (2013a) introduced dis-
tributional semantic similarity methods for com-
puting coherence, calculating the distributional
similarity between semantic vectors for the top-N
topic words using a range of distributional similar-
ity measures such as cosine similarity and the Dice
coefficient. To construct the semantic vector space
for the topic words, they used English Wikipedia
as the reference corpus, and collected words that
co-occur in a window of ?5 words. They showed
that their method correlates well with the observed
coherence rated by human judges.
3 Dataset
As one of the primary foci of this paper is the au-
tomation of the intruder word task of Chang et
al. (2009), our primary dataset is that used in the
original paper by Chang et al. (2009), which pro-
vides topics and human annotations for a range of
531
domains and topic model types. In the dataset,
two text collections were used: (1) 10,000 articles
from English Wikipedia (WIKI); and (2) 8,447 arti-
cles from the New York Times dating from 1987 to
2007 (NEWS). For each document collection, top-
ics were generated by three topic modelling meth-
ods: LDA, PLSI and CTM (see Section 2). For
each topic model, three settings of T (the num-
ber of topics) were used: T = 50, T = 100
and T = 150. In total, there were 9 topic mod-
els (3 models ? 3 T ) and 900 topics (3 models ?
(50 + 100 + 150)) for each dataset.
1
For some of topic interpretability estimation
methods, we require a reference corpus to sam-
ple lexical probabilities. We use two reference
corpora: (1) NEWS-FULL, which contains 1.2 mil-
lion New York Times articles from 1994 to 2004
(from the English Gigaword); and (2) WIKI-FULL,
which contains 3.3 million English Wikipedia ar-
ticles (retrieved November 28th 2009).
2
The ratio-
nale for choosing the New York Times and English
Wikipedia as the reference corpora is to ensure do-
main consistency with the word intrusion dataset;
the full collections are used to more robustly esti-
mate lexical probabilities.
4 Human-Interpretability at the Model
Level
In this section, we evaluate measures for estimat-
ing human-interpretability at the model level. That
is, for a measure ? human-judged or automated
? we first aggregate its coherence/interpretability
scores for all topics from a given topic model to
obtain the topic model?s average coherence score.
We then calculate the Pearson correlation coeffi-
cients between the two measures using the topic
models? average coherence scores. In summary,
the correlation is computed over nine sets of top-
ics (3 topic modellers ? 3 settings of T ) for each
of WIKI and NEWS.
4.1 Indirect Approach: Word Intrusion
The word intrusion task measures topic inter-
pretability indirectly, by computing the fraction
of annotators who successfully identify the in-
truder word. A limitation of the word intrusion
1
In the WIKI topics there were corrupted symbols in the
topic words for 24 topics. We removed these topics, reducing
the total number of topics to 876.
2
For both corpora we perform tokenisation and POS tag-
ging using OpenNLP and lemmatisation using Morpha (Min-
nen et al., 2001).
task is that it requires human annotations, there-
fore preventing large-scale evaluation. We begin
by proposing a methodology to fully automate the
word intrusion task.
Lau et al. (2010) proposed a methodology that
learns the most representative or best topic word
that summarises the semantics of the topic. Ob-
serving that the word intrusion task ? the task
of detecting the least representative word ? is
the converse of the best topic word selection task,
we adapt their methodology to automatically iden-
tify the intruder word for the word intrusion task,
based on the knowledge that there is a unique in-
truder word per topic.
The methodology works as follows: given a set
of topics (including intruder words), we compute
the word association features for each of the top-
N topic words of a topic,
3
and combine the fea-
tures in a ranking support vector regression model
(SVM
rank
: Joachims (2006)) to learn the intruder
words. Following Lau et al. (2010), we use three
word association measures:
PMI(w
i
) =
N?1
?
j
log
P (w
i
, w
j
)
P (w
i
)P (w
j
)
CP1(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
j
)
CP2(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
i
)
We additionally experiment with normalised
pointwise mutual information (NPMI: Bouma
(2009)):
NPMI(w
i
) =
N?1
?
j
log
P (w
i
,w
j
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
In the dataset of Chang et al. (2009) (see Sec-
tion 3), each topic was presented to 8 annota-
tors, with small variations in the displayed topic
words (including the intruder word) for each an-
notator. That is, each topic has essentially 8 subtly
different representations. To measure topic inter-
pretability, the authors defined ?model precision?:
the relative success of human annotators at identi-
fying the intruder word, across all representations
of the different topics. The model precision scores
produced by human judges are henceforth referred
to as WI-Human, and the scores produced by our
3
N is the number of topic words displayed to the human
users in the word intrusion task, including the intruder word.
532
Topic Ref. Pearson?s r with WI-Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI
WIKI
WIKI-FULL 0.947 0.936
NEWS-FULL 0.801 0.835
NEWS
NEWS-FULL 0.913 0.831
WIKI-FULL 0.811 0.750
Table 1: Pearson correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the model level.
automated method for the PMI and NPMI vari-
ants as WI-Auto-PMI and WI-Auto-NPMI respec-
tively.
4
The Pearson correlation coefficients between
WI-Human and WI-Auto-PMI/WI-Auto-NPMI at
the model level are presented in Table 1. Note
that our two reference corpora are used to inde-
pendently sample the lexical probabilities for the
word association features.
We see very strong correlation for in-domain
pairings (i.e. WIKI+WIKI-FULL and NEWS+NEWS-
FULL), achieving r > 0.9 in most cases for both
WI-Auto-PMI or WI-Auto-NPMI, demonstrating
the effectiveness of our methodology at automat-
ing the word intrusion task for estimating human-
interpretability at the model level. Overall, WI-
Auto-PMI outperforms WI-Auto-NPMI.
Note that although our proposed methodology
is supervised, as intruder words are synthetically
generated and no annotation is needed for the su-
pervised learning, the whole process of computing
topic coherence via word intrusion is fully auto-
matic, without the need for hand-labelled training
data.
4.2 Direct Approach: Observed Coherence
Newman et al. (2010) defined topic interpretabil-
ity based on a more direct approach, by asking hu-
man judges to rate topics based on the observed
coherence of the top-N topic words, and various
methodologies have since been proposed to auto-
mate the computation of the observed coherence.
In this section, we present all these methods and
compare them.
The word intrusion dataset is not annotated with
human ratings of observed coherence. To cre-
ate gold-standard coherence judgements, we used
Amazon Mechanical Turk:
5
we presented the top-
ics (with intruder words removed) to the Turkers
and asked them to rate the topics using on a 3-point
4
Note that both variants use CP1 and CP2 features, i.e.
WI-Auto-PMI uses PMI+CP1+C2 while WI-Auto-NPMI
uses NPMI+CP1+C2 features.
5
https://www.mturk.com/mturk/
ordinal scale, following Newman et al. (2010). In
total, we collected six to fourteen annotations per
topic (an average of 8.4 annotations per topic).
The observed coherence of a topic is computed
as the arithmetic mean of the annotators? ratings,
once again following Newman et al. (2010). The
human-judged observed topic coherence is hence-
forth referred to as OC-Human.
For the automated methods, we experimented
with the following methods for estimating the
human-interpretability of a topic t:
1. OC-Auto-PMI: Pairwise PMI of top-N
topic words (Newman et al., 2010):
OC-Auto-PMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)P (w
j
)
2. OC-Auto-NPMI: NPMI variant of OC-
Auto-PMI:
OC-Auto-NPMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
,w
i
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
3. OC-Auto-LCP: Pairwise log conditional
probability of top-N topic words (Mimno et
al., 2011):
6
OC-Auto-LCP(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)
4. OC-Auto-DS: Pairwise distributional simi-
larity of the top-N topic words, as described
in Aletras and Stevenson (2013a).
For OC-Auto-PMI, OC-Auto-NPMI and OC-
Auto-LCP, all topics are lemmatised and intruder
words are removed before coherence is com-
puted.
7
In-domain and cross-domain pairings of
6
Although the original method uses the topic-modelled
document collection and document co-occurrence for sam-
pling word counts, for a fairer comparison we use log condi-
tional probability only as a replacement to the PMI compo-
nent of the coherence computation (i.e. words are still sam-
pled using a reference corpus and a sliding window). For ad-
ditional evidence that the original method performs at a sub-
par level, see Lau et al. (2013) and Aletras and Stevenson
(2013a).
7
We once again use Morpha to do the lemmatisation, and
determine POS via the majority POS for a given word, aggre-
gated over all its occurrences in English Wikipedia.
533
Topic Ref. Pearson?s r with OC-Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL 0.490 0.903 0.959
0.859
NEWS-FULL 0.696 0.844 0.913
NEWS
NEWS-FULL 0.965 0.979 0.887
0.941
WIKI-FULL 0.931 0.964 0.872
Table 2: Pearson correlation of OC-Human and the automated methods ? OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS ? at the model level.
the topic domain and reference corpus are experi-
mented with for these measures.
For OC-Auto-DS, all topics are lemmatised, in-
truder words are removed and English Wikipedia
is used to generate the vector space for the topic
words. The size of the context window is set to
?5 word (i.e. 5 words to either side of the tar-
get word). We use PMI to weight the vectors,
cosine similarity for measuring the distributional
similarity between the top-N topic words, and the
?Topic Word Space? approach to reduce the di-
mensionality of the vector space. A complete de-
scription of the parameters can be found in Aletras
and Stevenson (2013a). Note that cross-domain
pairings of the topic domain and reference corpus
are not tested: in line with the original paper, we
use only English Wikipedia to generate the vector
space before distributional similarity.
We present the Pearson correlation coefficient
of OC-Human and the four automated methods at
the model level in Table 2. For OC-Auto-NPMI,
OC-Auto-LCP and OC-Auto-DS, we see that they
correlate strongly with the human-judged coher-
ence. Overall, OC-Auto-NPMI has the best per-
formance among the methods, and in-domain pair-
ings generally produce the best results for OC-
Auto-NPMI and OC-Auto-LCP. The results are
comparable to those for the automated intruder
word detection method in Section 4.1.
The non-normalised variant OC-Auto-PMI cor-
relates well for NEWS but performs poorly for WIKI,
producing a correlation of only 0.490 for the in-
domain pairing. We revisit this in Section 6, and
provide a qualitative analysis to explain the dis-
crepancy in results between OC-Auto-PMI and
OC-Auto-NPMI.
4.3 Word Intrusion vs. Observed Coherence
In the previous sections, we showed for both the
direct and indirect approaches that the automated
methods correlate strongly with the manually-
annotated human-interpretability of topics at the
model level (with the exception of OC-Auto-PMI).
One question that remains unanswered, however,
is whether word intrusion measures topic inter-
pretability differently to observed coherence. This
is the focus of this section.
From the results in Table 3 for the intruder
word model vs. observed coherence, we see a
strong correlation between WI-Human and OC-
Human. This observation is insightful: it shows
that the topic interpretability estimated by the two
approaches is almost identical at the model level.
Between WI-Human and the observed coher-
ence methods automated methods, overall we see
a strong correlation for the OC-Auto-NPMI, OC-
Auto-LCP and OC-Auto-DS methods. OC-Auto-
PMI once again performs poorly over WIKI, but
this is unsurprising given its previous results (i.e.
its poor correlation with OC-Human). In-domain
pairings tend to perform better, and the per-
formance of OC-Auto-NPMI, OC-Auto-LCP and
OC-Auto-DS is comparable, with no one clearly
best method.
5 Human-Interpretability at the Topic
Level
In this section, we evaluate the various methods
at the topic level. We group together all topics
for each dataset (without distinguishing the topic
models that produce them) and calculate the cor-
relation of one measure against another. That is,
the correlation coefficient is computed for 900 top-
ics/data points in the case of each of WIKI and
NEWS.
5.1 Indirect Approach: Word Intrusion
In Section 4.1, we proposed a novel methodol-
ogy to automate the word intrusion task (WI-Auto-
PMI and WI-Auto-NPMI). We now evaluate its
performance at the topic level, and present its
correlation with the human gold standard (WI-
Human) in Table 4.
The correlation of WI-Human and WI-Auto-
PMI/WI-Auto-NPMI at the topic level is consid-
erably worse, compared to its results at the model
534
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.900
0.638 0.927 0.911
0.907
NEWS-FULL 0.614 0.757 0.821
NEWS
NEWS-FULL
0.915
0.865 0.866 0.867
0.925
WIKI-FULL 0.838 0.874 0.893
Table 3: Word intrusion vs. observed coherence: Pearson correlation coefficient at the model level.
Topic Ref. Pearson?s r with WI-Human Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI Agreement
WIKI
WIKI-FULL 0.554 0.573
0.735
NEWS-FULL 0.622 0.592
NEWS
NEWS-FULL 0.602 0.612
0.770
WIKI-FULL 0.638 0.648
Table 4: Pearson correlation coefficient of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topic
level.
level (Table 1). The performance between WI-
Auto-PMI and WI-Auto-NPMI is not very differ-
ent, and the cross-domain pairing slightly outper-
forms the in-domain pairing.
To better understand the difficulty of the task,
we compute the agreement between human anno-
tators by calculating the Pearson correlation co-
efficient of model precisions produced by ran-
domised sub-group pairs in the topics.
8
That is, for
each topic, we randomly split the annotations into
two sub-groups, and compute the Pearson correla-
tion coefficient of the model precisions produced
by the first sub-group and that of the second sub-
group.
The original dataset has 8 annotations per topic.
Splitting the annotations into two sub-groups re-
duces the number of annotations to 4 per group,
which is not ideal for computing model precision.
We thus chose to expand the number of annota-
tions by sampling 300 random topics from each
domain (for a total of 600 topics) and following
the same process as Chang et al. (2009) to get in-
truder word annotations using Amazon Mechani-
cal Turk. On average, we obtained 11.7 additional
annotations per topic for these 600 topics. The hu-
man agreement scores (i.e. the Pearson correlation
coefficient of randomised sub-group pairs) for the
sampled 600 topics are presented in the last col-
umn of Table 4.
The sub-group correlation is around r = 0.75
for the topics from both datasets. As such, esti-
mating topic interpretability at the topic level is a
much harder task than model-level evaluation. Our
automated methods perform at a highly credible
8
To counter for the fact that annotators labelled varying
numbers of topics.
r = 0.6, but there is certainly room for improve-
ment. Note that the correlation values reported in
Newman et al. (2010) are markedly higher than
ours, as they evaluated based on Spearman rank
correlation, which isn?t attuned to the relative dif-
ferences in coherence values and returns higher
values for the task.
5.2 Direct Approach: Observed Coherence
We repeat the experiments of observed coherence
in Section 4.2, and evaluate the correlation of
the automated methods (OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS) on the
human gold standard (OC-Human) at the topic
level. Results are summarised in Table 5.
OC-Auto-PMI performs poorly at the topic
level in the WIKI domain, similar to what was
seen at the model level in Section 4.2. Over-
all, both OC-Auto-NPMI and OC-Auto-DS are the
most consistent methods. OC-Auto-LCP performs
markedly worse than these two methods.
To get a better understanding of how well hu-
man annotators perform at the task, we compute
the one-vs-rest Pearson correlation coefficient us-
ing the gold standard annotations. That is, for
each topic, we single out each rating/annotation
and compare it to the average of all other rat-
ings/annotations. The one-vs-rest correlation re-
sult is displayed in the last column (titled ?Hu-
man Agreement?) in Table 5. The best auto-
mated methods surpass the single-annotator per-
formance, indicating that they are able to per-
form the task as well as human annotators (unlike
the topic-level results for the word intrusion task
where humans were markedly better at the task
than the automated methods).
535
Topic Ref. Pearson?s r with OC-Human Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS Agreement
WIKI
WIKI-FULL 0.533 0.638 0.579
0.682 0.624
NEWS-FULL 0.582 0.667 0.496
NEWS
NEWS-FULL 0.719 0.741 0.471
0.682 0.634
WIKI-FULL 0.671 0.722 0.452
Table 5: Pearson correlation of OC-Human and the automated methods at the topic level.
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.665
0.472 0.557 0.547
0.639
NEWS-FULL 0.504 0.571 0.455
NEWS
NEWS-FULL
0.641
0.629 0.634 0.407
0.649
WIKI-FULL 0.604 0.633 0.390
Table 6: Word intrusion vs. observed coherence: pearson correlation results at the topic level.
5.3 Word Intrusion vs. Observed Coherence
In this section, we bring together the indirect ap-
proach of word intrusion and the direct approach
of observed coherence, and evaluate them against
each other at the topic level. Results are sum-
marised in Table 6.
We see that the correlation between the human
ratings of intruder words and observed coherence
is only modest, implying that there are topic-level
differences in the output of the two approaches. In
Section 6, we provide a qualitative analysis and
explanation as to what constitutes the differences
between the approaches.
For the automated methods, OC-Auto-DS has
the best performance, with OC-Auto-NPMI per-
forming relatively well (in particularly in the NEWS
domain).
6 Discussion
Normalised PMI (NPMI) was first introduced by
Bouma (2009) as a means of reducing the bias for
PMI towards words of lower frequency, in addition
to providing a standardised range of [?1, 1] for the
calculated values.
We introduced NPMI to the automated meth-
ods of word intrusion (WI-Auto-NPMI) and ob-
served coherence (OC-Auto-NPMI) to explore its
suitability for the task. For the latter, we saw
that NPMI achieves markedly higher correlation
than OC-Human (in particular, at the model level).
To better understand the impact of normalisation,
we inspected a list of WIKI topics that have simi-
lar scores for OC-Human and OC-Auto-NPMI but
very different OC-Auto-PMI scores. A sample of
these topics is presented in Table 7. WIKI-FULL
is used as the reference corpus for computing the
scores. Note that the presented OC-Auto-NPMI*
and OC-Auto-PMI* scores are post-normalised to
the range [0, 1] for ease of interpretation. To give
a sense of how readily these topic words occur in
the reference corpus, we additionally display the
frequency of the first topic word in the reference
corpus (last column).
All topics presented have an OC-Human score
of 3.0 (i.e. these topics are rated as being very co-
herent by human judges) and similar OC-Auto-
NPMI values. Their OC-Auto-PMI scores, how-
ever, are very different between the top-3 and
bottom-3 topics. The bias of PMI towards lower
frequency words is clear: topic words that occur
frequently in the corpus receive a lower OC-Auto-
PMI score compared to those that occur less fre-
quently, even though the human-judged observed
coherence is the same. OC-Auto-NPMI on the
other hand, correctly estimates the coherence.
We observed, however, that the impact of nor-
malising PMI is less in the word intrusion task.
One possible explanation is that for the automated
methods WI-Auto-PMI and WI-Auto-NPMI, the
PMI/NPMI scores are used indirectly as a feature
to a machine learning framework, and the bias
could be reduced/compensated by other features.
On the subject of the difference between ob-
served coherence and word intrusion in estimat-
ing topic interpretability, we observed that WI-
Human and OC-Human correlate only moderately
(r ? 0.6) at the topic level (Table 6). To better
understand this effect, we manually analysed top-
ics that have differing WI-Human and OC-Human
scores. A sample of topics with high divergence
in estimated coherence score is given in Table 8.
As before, the presented the OC-Human* and WI-
536
Topic
OC- OC- OC- Word
Human Auto-NPMI* Auto-PMI* Count
cell hormone insulin muscle receptor 3.0 0.59 0.61 #(cell) = 1.1M
electron laser magnetic voltage wavelength 3.0 0.52 0.54 #(electron) = 0.3M
magnetic neutrino particle quantum universe 3.0 0.55 0.55 #(magnetic) = 0.4M
album band music release song 3.0 0.56 0.37 #(album) = 12.5M
college education school student university 3.0 0.57 0.38 #(college) = 9.8M
city county district population town 3.0 0.52 0.34 #(city) = 22.0M
Table 7: A list of WIKI topics to illustrate the impact of NPMI.
Topic # Topic OC-Human* WI-Human*
1 business company corporation cluster loch shareholder 0.94 0.25
2 song actor clown play role theatre 1.00 0.50
3 census ethnic female male population village 0.92 0.25
4 composer singer jazz music opera piano 1.00 0.63
5 choice count give i.e. simply unionist 0.14 1.00
6 digital clown friend love mother wife 0.17 1.00
Table 8: A list of WIKI topics to illustrate the difference between observed coherence and word intrusion.
Boxes denote human chosen intruder words, and boldface denotes true intruder words.
Human* scores in the table are post-normalised to
the range [0, 1] for ease of comparison.
In general, there are two reasons for topics to
have high OC-Human and low WI-Human scores.
First, if a topic has an outlier word that is mildly
related to the topic, users tend to choose this word
as the intruder word in the word intrusion task,
yielding a low WI-Human score. If they are asked
to rate the observed coherence, however, the single
outlier word often does not affect its overall coher-
ence, resulting in a high OC-Human score. This is
observed in topics 1 and 2 in Table 8, where loch
and clown are chosen by annotators in the word in-
trusion task, as they detract from the semantics of
the topic. This results in low WI-Human scores,
but high observed coherence scores (OC-Human).
The second reason is the random selection of
intruder words related to the original topic. We
see this in topics 3 and 4, where related intruder
words (village and singer) were selected.
For topics with low OC-Human and high WI-
Human scores, the true intruder words are often
very different to the domain/focus of other topic
words. As such, annotators are consistently able
to single them out to yield high WI-Human scores,
even though the topic as a whole is not coherent.
Topics 5 and 6 in Table 8 exhibit this.
All topic evaluation measures described in this
paper are implemented in an open-source toolkit.
9
9
https://github.com/jhlau/topic_
interpretability
7 Conclusion
In this paper, we examined various methodologies
that estimate the semantic interpretability of top-
ics, at two levels: the model level and the topic
level. We looked first at the word intrusion task
proposed by Chang et al. (2009), and proposed
a method that fully automates the task. Next we
turned to observed coherence, a more direct ap-
proach to estimate topic interpretability. At the
model level, results were very positive for both the
word intrusion and observed coherence methods.
At the topic level, however, the results were more
mixed. For observed coherence, our best methods
(OC-Auto-NPMI and OC-Auto-DS) were able to
emulate human performance. For word intrusion,
the automated methods were slightly below human
performance, with some room for improvement.
We finally observed that there are systematic dif-
ferences in the topic-level scores derived from the
two task formulations.
Acknowledgements
This work was supported in part by the Australian
Research Council, and for author JHL, also partly
funded by grant ES/J022969/1 from the Economic
and Social Research Council of the UK. The au-
thors acknowledge the generosity of Nikos Ale-
tras and Mark Stevenson in providing their code
for OC-Auto-DS, and Jordan Boyd-Graber in pro-
viding the data used in Chang et al. (2009).
537
References
N. Aletras and M. Stevenson. 2013a. Evaluating
topic coherence using distributional semantics. In
Proceedings of the Tenth International Workshop on
Computational Semantics (IWCS-10), pages 13?22,
Potsdam, Germany.
N. Aletras and M. Stevenson. 2013b. Representing
topics using images. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
158?167, Atlanta, USA.
D. Blei and J. Lafferty. 2005. Correlated topic mod-
els. In Advances in Neural Information Processing
Systems 17 (NIPS-05), pages 147?154, Vancouver,
Canada.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning
Research, 3:993?1022.
L. Bolelli, S?. Ertekin, and C.L. Giles. 2009. Topic
and trend detection in text collections using Latent
Dirichlet Allocation. In Proceedings of ECIR 2009,
pages 776?780, Toulouse, France.
G. Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, pages 31?40,
Potsdam, Germany.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Advances in Neural In-
formation Processing Systems 21 (NIPS-09), pages
288?296, Vancouver, Canada.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics ? Human
Language Technologies 2009 (NAACL HLT 2009),
pages 362?370.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Study-
ing the history of ideas using topic models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 363?371, Honolulu, USA.
M. Hall, P. Clough, and M. Stevenson. 2012. Evalu-
ating the use of clustering for automatically organ-
ising digital library collections. In Proceedings of
the Second International Conference on Theory and
Practice of Digital Libraries, pages 323?334, Pa-
phos, Cyprus.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of 22nd International ACM-
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR?99), pages 50?57,
Berkeley, USA.
T. Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), Philadelphia, USA.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin.
2010. Best topic word selection for topic labelling.
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
Posters Volume, pages 605?613, Beijing, China.
J.H. Lau, K. Grieser, D. Newman, and T. Baldwin.
2011. Automatic labelling of topic models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT 2011), pages 1536?
1545, Portland, USA.
J.H. Lau, N. Collier, and T. Baldwin. 2012a. On-
line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1519?
1534, Mumbai, India.
J.H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012b. Word sense induction for novel
sense detection. In Proceedings of the 13th Con-
ference of the EACL (EACL 2012), pages 591?601,
Avignon, France.
J.H. Lau, T. Baldwin, and D. Newman. 2013. On
collocations and topic models. ACM Transactions
on Speech and Language Processing, 10(3):10:1?
10:14.
A McCallum, G.S. Mann, and D Mimno. 2006. Bib-
liometric impact measures leveraging topic analysis.
In Proceedings of the 6th ACM/IEEE-CS Joint Con-
ference on Digital Libraries 2006 (JCDL?06), pages
65?74, Chapel Hill, USA.
D. Mimno, H. Wallach, E. Talley, M. Leenders, and
A. McCallum. 2011. Optimizing semantic coher-
ence in topic models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2011), pages 262?272,
Edinburgh, UK.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
C. Musat, J. Velcin, S. Trausan-Matu, and M.A. Rizoiu.
2011. Improving topic evaluation using concep-
tual knowledge. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-2011), pages 1866?1871, Barcelona, Spain.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
538
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics.
In Proceedings of the 24th Annual Conference on
Artificial Intelligence (AAAI-10), Atlanta, USA.
J. Reisinger, A. Waters, B. Silverthorn, and R.J.
Mooney. 2010. Spherical topic models. In Proceed-
ings of the 27th International Conference on Ma-
chine Learning (ICML 2010), pages 903?910, Haifa,
Israel.
X. Wang, A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discovery, with an ap-
plication to information retrieval. In Proceedings
of the Seventh IEEE International Conference on
Data Mining (ICDM 2007), pages 697?702, Omaha,
USA.
B. Zhao and E.P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In Advances in Neural Information Processing
Systems (NIPS 2007), pages 1689?1696, Vancouver,
Canada.
539
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100?108,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Topic Coherence
David Newman,?? Jey Han Lau,? Karl Grieser?, and Timothy Baldwin,??
? NICTA Victoria Research Laboratory, Australia
? Dept of Computer Science, University of California, Irvine
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Dept of Information Systems, University of Melbourne, Australia
newman@uci.edu, depthchargex@gmail.com,
kgrieser@csse.unimelb.edu.au, tb@ldwin.net
Abstract
This paper introduces the novel task of topic
coherence evaluation, whereby a set of words,
as generated by a topic model, is rated for
coherence or interpretability. We apply a
range of topic scoring models to the evaluation
task, drawing on WordNet, Wikipedia and the
Google search engine, and existing research
on lexical similarity/relatedness. In compar-
ison with human scores for a set of learned
topics over two distinct datasets, we show a
simple co-occurrence measure based on point-
wise mutual information over Wikipedia data
is able to achieve results for the task at or
nearing the level of inter-annotator correla-
tion, and that other Wikipedia-based lexical
relatedness methods also achieve strong re-
sults. Google produces strong, if less consis-
tent, results, while our results over WordNet
are patchy at best.
1 Introduction
There has traditionally been strong interest within
computational linguistics in techniques for learning
sets of words (aka topics) which capture the latent
semantics of a document or document collection, in
the form of methods such as latent semantic analysis
(Deerwester et al, 1990), probabilistic latent seman-
tic analysis (Hofmann, 2001), random projection
(Widdows and Ferraro, 2008), and more recently, la-
tent Dirichlet alocation (Blei et al, 2003; Griffiths
and Steyvers, 2004). Such methods have been suc-
cessfully applied to a myriad of tasks including word
sense discrimination (Brody and Lapata, 2009), doc-
ument summarisation (Haghighi and Vanderwende,
2009), areal linguistic analysis (Daume III, 2009)
and text segmentation (Sun et al, 2008). In each
case, extrinsic evaluation has been used to demon-
strate the effectiveness of the learned topics in the
application domain, but standardly, no attempt has
been made to perform intrinsic evaluation of the top-
ics themselves, either qualitatively or quantitatively.
In machine learning, on the other hand, researchers
have modified and extended topic models in a vari-
ety of ways, and evaluated intrinsically in terms of
model perplexity (Wallach et al, 2009), but there has
been less effort on qualitative understanding of the
semantic nature of the learned topics.
This research seeks to fill the gap between topic
evaluation in computational linguistics and machine
learning, in developing techniques to perform intrin-
sic qualitative evaluation of learned topics. That
is, we develop methods for evaluating the qual-
ity of a given topic, in terms of its coherence to
a human. After learning topics from a collection
of news articles and a collection of books, we ask
humans to decide whether individual learned top-
ics are coherent, in terms of their interpretability
and association with a single over-arching seman-
tic concept. We then propose models to predict
topic coherence, based on resources such as Word-
Net, Wikipedia and the Google search engine, and
methods ranging from ontological similarity to link
overlap and term co-occurrence. Over topics learned
from two distinct datasets, we demonstrate that there
is remarkable inter-annotator agreement on what is
a coherent topic, and additionally that our methods
based on Wikipedia are able to achieve nearly perfect
agreement with humans over the evaluation of topic
coherence.
This research forms part of a larger research
agenda on the utility of topic modelling in gist-
ing and visualising document collections, and ulti-
mately enhancing search/discovery interfaces over
100
document collections (Newman et al, to appeara).
Evaluating topic coherence is a component of the
larger question of what are good topics, what char-
acteristics of a document collection make it more
amenable to topic modelling, and how can the po-
tential of topic modelling be harnessed for human
consumption (Newman et al, to appearb).
2 Related Work
Most earlier work on intrinsically evaluating learned
topics has been on the basis of perplexity results,
where a model is learned on a collection of train-
ing documents, then the log probability of the un-
seen test documents is computed using that learned
model. Usually perplexity is reported, which is the
inverse of the geometric mean per-word likelihood.
Perplexity is useful for model selection and adjust-
ing parameters (e.g. number of topics T ), and is
the standard way of demonstrating the advantage of
one model over another. Wallach et al (2009) pre-
sented efficient and unbiased methods for computing
perplexity and evaluating almost any type of topic
model.
While statistical evaluation of topic models is
reasonably well understood, there has been much
less work on evaluating the intrinsic semantic qual-
ity of topics learned by topic models, which could
have a far greater impact on the overall value of
topic modeling for end-user applications. Some re-
searchers have started to address this problem, in-
cluding Mei et al (2007) who presented approaches
for automatic labeling of topics (which is core to the
question of coherence and semantic interpretabil-
ity), and Griffiths and Steyvers (2006) who applied
topic models to word sense discrimination tasks.
Misra et al (2008) used topic modelling to identify
semantically incoherent documents within a docu-
ment collection (vs. coherent topics, as targeted in
this research). Chang et al (2009) presented the
first human-evaluation of topic models by creating
a task where humans were asked to identify which
word in a list of five topic words had been ran-
domly switched with a word from another topic.
This work showed some possibly counter-intuitive
results, where in some cases humans preferred mod-
els with higher perplexity. This type of result shows
the need for further exploring measures other than
perplexity for evaluating topic models. In earlier
work, we carried out preliminary experimentation
using pointwise mutual information and Google re-
sults to evaluate topic coherence over the same set
of topics as used in this research (Newman et al,
2009).
Part of this research takes inspiration from the
work on automatic evaluation in machine translation
(Papineni et al, 2002) and automatic summarisation
(Lin, 2004). Here, the development of automated
methods with high correlation with human subjects
has opened the door to large-scale automated evalua-
tion of system outputs, revolutionising the respective
fields. While our aspirations are more modest, the
basic aim is the same: to develop a fully-automated
method for evaluating a well-grounded task, which
achieves near-human correlation.
3 Topic Modelling
In order to evaluate topic modelling, we require a
topic model and set of topics for a given document
collection. While the evaluation methodology we
describe generalises to any method which gener-
ates sets of words, all of our experiments are based
on Latent Dirichlet Allocation (LDA, aka Discrete
Principal Component Analysis), on the grounds that
it is a state-of-the-art method for generating topics.
LDA is a Bayesian graphical model for text docu-
ment collections represented by bags-of-words (see
Blei et al (2003), Griffiths and Steyvers (2004),
Buntine and Jakulin (2004)). In a topic model, each
document in the collection of D documents is mod-
elled as a multinomial distribution over T topics,
where each topic is a multinomial distribution over
W words. Typically, only a small number of words
are important (have high likelihood) in each topic,
and only a small number of topics are present in each
document.
The collapsed Gibbs sampled topic model simul-
taneously learns the topics and the mixture of topics
in documents by iteratively sampling the topic as-
signment z to every word in every document, using
the Gibbs sampling update:
p(zid = t|xid = w, z?id) ?
N?idwt + ?
?
w N?idwt + W?
N?idtd + ?
?
t N?idtd + T?
101
where zid = t is the assignment of the ith word in
document d to topic t, xid = w indicates that the
current observed word is w, and z?id is the vector of
all topic assignments not including the current word.
Nwt represents integer count arrays (with the sub-
scripts denoting what is counted), and ? and ? are
Dirichlet priors.
The maximum a posterior (MAP) estimates of the
topics p(w|t), t = 1 . . . T are given by:
p(w|t) = Nwt + ??
w Nwt + W?
We will follow the convention of representing a
topic via its top-n words, ordered by p(w|t). Here,
we use the top-ten words, as they usually provide
sufficient detail to convey the subject of a topic,
and distinguish one topic from another. For the
remainder of this paper, we will refer to individ-
ual topics by its list of top-ten words, denoted by
w = (w1, . . . , w10).
4 Topic Evaluation Methods
We experiment with scoring methods based on
WordNet (Section 4.1), Wikipedia (Section 4.2) and
the Google search engine (Section 4.3). In the case
of Google, we query for the entire topic, but with
WordNet and Wikipedia, this takes the form of scor-
ing each word-pair in a given topic w based on the
component words (w1, . . . , w10). Given some (sym-
metric) word-similarity measure D(wi, wj), two
straightforward ways of producing a combined score
from the 45 (i.e.
(10
2
)
) word-pair scores are: (1) the
arithmetic mean, and (2) the median, as follows:
Mean-D-Score(w) =
mean{D(wi, wj), ij ? 1 . . . 10, i < j}
Median-D-Score(w) =
median{D(wi, wj), ij ? 1 . . . 10, i < j}
Intuitively, the median seems the more natural rep-
resentation, as it is less affected by outlier scores,
but we experiment with both, and fall back to empir-
ical verification of which is the better combination
method.
4.1 WordNet similarity
WordNet (Fellbaum, 1998) is a lexical ontology
that represents word sense via ?synsets?, which
are structured in a hypernym/hyponym hierarchy
(nouns) or hypernym/troponym hierarchy (verbs).
WordNet additionally links both synsets and words
via lexical relations including antonymy, morpho-
logical derivation and holonymy/meronym.
In parallel with the development of WordNet, a
number of computational methods for calculating
the semantic relatedness/similarity between synset
pairs (i.e. sense-specified word pairs) have been de-
veloped, as we outline below. These methods ap-
ply to synset rather than word pairs, so to generate a
single score for a given word pair, we look up each
word in WordNet and exhaustively generate scores
for each sense pairing defined by them, and calcu-
late their arithmetic mean.1
The majority of the methods (all methods other
than HSO, VECTOR and LESK) are restricted to op-
erating strictly over hierarchical links within a sin-
gle hierarchy. As the verb and noun hierarchies are
not connected (other than via derivational links), this
means that it is generally not possible to calculate
the similarity between noun and verb senses, for ex-
ample. In such cases, we simply drop the synset
pairing in question from our calculation of the mean.
The least common subsumer (LCS) is a common
feature to a number of the measures, and is defined
as the deepest node in the hierarchy that subsumes
both of the synsets under question.
For all our experiments over WordNet, we use the
WordNet::Similarity package.
Path distance (PATH)
The simplest of the WordNet-based measures is
to count the number of nodes visited while going
from one word to another via the hypernym hierar-
chy. The path distance between two nodes is de-
fined as the number of nodes that lie on the short-
est path between two words in the hierarchy. This
1We also experimented with the median, and trialled filter-
ing the set of senses in a variety of ways, e.g. using only the
first sense (the sense with the highest prior) for a given word,
or using only the word senses associated with the POS with the
highest prior. In all cases, the overall trend was for the correla-
tion with the human scores to drop relative to the mean, so we
only present the numbers for the mean in this paper.
102
count of nodes includes the beginning and ending
word nodes.
Leacock-Chodorow (LCH)
The measure of semantic similarity devised by
Leacock et al (1998) finds the shortest path between
two WordNet synsets (sp(c1, c2)) using hypernym
and synonym relationships. This path length is then
scaled by the maximum depth of WordNet (D), and
the log likelihood taken:
simlch(c1, c2) = ? log
sp(c1, c2)
2 ?D
Wu-Palmer (WUP)
Wu and Palmer (1994) proposed to scale the depth
of the two synset nodes (depthc1 and depthc2) by
the depth of their LCS (depth(lcsc1,c2)):
simwup(c1, c2) =
2 ? depth(lcsc1,c2)
depthc1 + depthc2 + 2 ? depth(lcsc1,c2)
The scaling means that specific terms (deeper in the
hierarchy) that are close together are more semanti-
cally similar than more general terms, which have a
short path distance between them. Only hypernym
relationships are used in this measure, as the LCS
is defined by the common member in the concepts?
hypernym path.
Hirst-St Onge (HSO)
Hirst and St-Onge (1998) define a measure of se-
mantic similarity based on length and tortuosity of
the path between nodes. Hirst and St-Onge attribute
directions (up, down and horizontal) to the larger set
of WordNet relationships, and identify the path from
one word to another utilising all of these relation-
ships. The relatedness score is then computed by
the weighted sum of the path length between the two
words (len(c1, c2)) and the number of turns the path
makes (turns(c1, c2)) to take this route:
relhso(c1, c2) =
C ? len(c1, c2)? k ? turns(c1, c2)
where C and k are constants. Additionally, a set of
restrictions is placed on the path so that it may not
be more than a certain length, may not contain more
than a set number of turns, and may only take turns
in certain directions.
Resnik Information Content (RES)
Resnik (1995) presents a method for weighting
edges in WordNet (avoiding the assumption that all
edges between nodes have equal importance), by
weighting edges between nodes by their frequency
of use in textual corpora.
Resnik found that the most effective measure of
comparison using this methodology was to measure
the Information Content (IC(c) = ? log p(c)) of
the subsumer with the greatest Information Content
from the set of all concepts that subsumed the two
initial concepts (S(c1, c2)) being compared:
simres(c1, c2) = max
c?S(c1,c2)
[? log p(c)]
Lin (LIN)
Lin (1998) expanded on the Information Theo-
retic approach presented by Resnik by scaling the
Information Content of each node by the informa-
tion content of their LCS:
simlin(c1, c2) =
2? log p(lcsc1,c2)
log p(c1) + log p(c2)
This measure contrasts the joint content of the two
concepts with the difference between them.
Jiang-Conrath (JCN)
Jiang and Conrath (1997) define a measure that
utilises the components of the information content
of the LCS in a different manner:
simjcn(c1, c2) =
1
IC(a) + IC(b)? 2? IC(lcsa,b)
Instead of defining commonality and difference as
with Lin?s measure, the key determinant is the speci-
ficity of the two nodes compared with their LCS.
Lesk (LESK)
Lesk (1986) proposed a significantly different ap-
proach to lexical similarity to that proposed in the
methods presented above, using the lexical over-
lap in dictionary definitions (or glosses) to disam-
biguate word sense. The sense definitions that con-
tain the most words in common indicate the most
likely sense of the word given its co-occurrence with
similar word senses. Banerjee and Pedersen (2002)
103
adapted this method to utilise WordNet sense glosses
rather than dictionary definitions, and expand the
dictionary definitions via ontological links, and it is
this method we experiment with in this paper.
Vector (VECTOR)
Schu?tze (1998) uses the words surrounding a term
in a piece of text to form a context vector that de-
scribes the context in which the word sense appears.
For a set of words associated with a target sense, a
context vector is computed as the centroid vector of
these words. The centroid context vectors each rep-
resent a word sense. To compare word senses, the
cosine similarity of the context vectors is used.
4.2 Wikipedia
In the last few years, there has been a surge of in-
terest in using Wikipedia to calculate semantic sim-
ilarity, using the Wikipedia article content, in-article
links and document categories (Stru?be and Ponzetto,
2006; Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008). We present a selection of such meth-
ods below. There are a number of Wikipedia-based
scoring methods which we do not present results
for here (notably Stru?be and Ponzetto (2006) and
Gabrilovich and Markovitch (2007)), due to their
computational complexity and uncertainty about the
full implementation details of the methods.
As with WordNet, a given word will often have
multiple entries in Wikipedia, grouped in a disam-
biguation page. For MIW, RACO and DOCSIM,
we apply the same strategy as we did with Word-
Net, in exhaustively calculating the pairwise scores
between the sets of documents associated with each
term, and averaging across them.
Milne-Witten (MIW)
Milne and Witten (2008) adapted the Resnik
(1995) methodology to utilise the count of links
pointing to an article. As Wikipedia is self-
referential (articles link to related articles), no ex-
ternal data is needed to find the ?referred-to-edness?
of a concept. Milne and Witten use an adapted In-
formation Content measure that weights the number
of links from one article to another (c1 ? c2) by the
total number of links to the second article:
w(c1 ? c2) = |c1 ? c2| ? log
?
x?W
|W |
|c1, x)|
where x is an article in W , Wikipedia. This mea-
sure provides the similarity of one article to another,
however this is asymmetrical. The above metric is
used to find the weights of all outlinks from the two
articles being compared:
~c1 = (w(c1 ? l1), w(c1 ? l2), ? ? ? , w(c1 ? ln))
~c2 = (w(c2 ? l1), w(c2 ? l2), ? ? ? , w(c2 ? ln))
for the set of links l that is the union of the sets of
outlinks from both articles. The overall similarity
of the two articles is then calculated by taking the
cosine similarity of the two vectors.
Related Article Concept Overlap (RACO)
We also determine the category overlap of two
articles by examining the outlinks of both articles,
in the form of the Related Article Concept Overlap
(RACO) measure. The concept overlap of the sets
of respective outlinks is given by the union of the
two sets of categories from the outlinks from each
article:
overlap(c1, c1) =
?
?
(
?
l?ol(c1)
cat(l)
)
?
(
?
l?ol(c2)
cat(l)
)
?
?
where ol(c1) is the set of outlinks from article c1,
and cat(l) is the set of categories of which the arti-
cle at outlink l is a member. To account for article
size (and differing number of outlinks), the Jaccard
coefficient is used:
relraco(c1, c2) =
?
?
(
?
l?ol(c1) cat(l)
)
?
(
?
l?ol(c2) cat(l)
)
?
?
?
?
?
l?ol(c1) cat(l)
?
?+
?
?
?
l?ol(c2) cat(l)
?
?
Document Similarity (DOCSIM)
In addition to these two measures of semantic re-
latedness, we experiment with simple cosine simi-
larity of the text of Wikipedia articles as a measure
of semantic relatedness.
Term Co-occurrence (PMI)
Another variant is to treat Wikipedia as a single
meta-document and score word pairs using term co-
occurrence. Here, we calculate the pointwise mu-
tual information (PMI) of each word pair, estimated
104
Selected high-scoring topics (unanimous score=3):
[NEWS] space earth moon science scientist light nasa mission planet mars ...
[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...
[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...
[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...
Selected low-scoring topics (unanimous score=1):
[NEWS] king bond berry bill ray rate james treas byrd key ...
[NEWS] dog moment hand face love self eye turn young character ...
[BOOKS] soon short longer carried rest turned raised filled turn allowed ...
[BOOKS] act sense adv person ppr plant sax genus applied dis ...
Table 1: A selection of high-scoring and low-scoring topics
from the entire corpus of over two million English
Wikipedia articles (?1 billion words). PMI has been
studied variously in the context of collocation ex-
traction (Pecina, 2008), and is one measure of the
statistical independence of observing two words in
close proximity. Using a sliding window of 10-
words to identify co-occurrence, we computed the
PMI of all a given word pair (wi, wj) as, following
Newman et al (2009):
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
4.3 Search engine-based similarity
Finally, we present two search engine-based scor-
ing methods, based on Newman et al (2009). In
this case the external data source is the entire World
Wide Web, via the Google search engine. Unlike
the methods presented above, here we query for the
topic in its entirety,2 meaning that we return a topic-
level score rather than scores for individual word or
word sense pairs. In each case, we mark each search
term with the advanced search option + to search
for the terms exactly as is and prevent Google from
using synonyms or lexical variants of the term. An
example query is: +space +earth +moon +science
+scientist +light +nasa +mission +planet +mars.
Google title matches (TITLES)
Firstly, we score topics by the relative occurrence
of their component words in the titles of documents
returned by Google:
Google-titles-match(w) = 1 [wi = vj ]
2All queries were run on 15/09/2009.
where i = 1, . . . , 10 and j = 1, . . . , |V |, vj are
all the unique terms mentioned in the titles from the
top-100 search results, and 1 is the indicator function
to count matches. For example, in the top-100 re-
sults for our query above, there are 194 matches with
the ten topic words, so Google-titles-match(w) =
194.
Google log hits matches (LOGHITS)
Second, we issue queries as above, but return the
log number of hits for our query:
Google-log-hits(w) =
log10(# results from search for w)
where w is the search string +w1 +w2 +w3 . . .
+w10. For example, our query above returns
171,000 results, so Google-log-hits(w) = 5.2. and
the URL titles from the top-100 results include a to-
tal of 194 matches with the ten topic words, so for
this topic Google-titles-match(w)=194.
5 Experimental Setup
We learned topics for two document collections: a
collection of news articles, and a collection of books.
These collections were chosen to produce sets of
topics that have more variable quality than one typi-
cally observes when topic modeling highly uniform
content. The collection of D = 55, 000 news arti-
cles was selected from English Gigaword, and the
collection of D = 12, 000 books was downloaded
from the Internet Archive. We refer to these collec-
tions as NEWS and BOOKS, respectively.
Standard procedures were used to tokenize each
collection and create the bags-of-words. We learned
105
Resource Method Median Mean
WordNet
HSO ?0.29 0.34
JCN 0.08 0.22
LCH ?0.18 ?0.07
LESK 0.38 0.37
LIN 0.18 0.25
PATH 0.19 0.11
RES ?0.10 0.13
VECTOR 0.07 0.20
WUP 0.03 0.10
Wikipedia
RACO 0.61 0.63
MIW 0.69 0.60
DOCSIM 0.45 0.50
PMI 0.78 0.77
Google TITLES 0.80LOGHITS 0.46
Gold-standard IAA 0.79 0.73
Table 2: Spearman rank correlation ? values for the
different scoring methods over the NEWS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
topic models of NEWS and BOOKS using T = 200
and T = 400 topics respectively. We randomly
selected a total of 237 topics from the two collec-
tions for user scoring. We asked N = 9 users to
score each of the 237 topics on a 3-point scale where
3=?useful? (coherent) and 1=?useless? (less coher-
ent).
We provided annotators with a rubric and guide-
lines on how to judge whether a topic was useful
or useless. In addition to showing several examples
of useful and useless topics, we instructed users to
decide whether the topic was to some extent coher-
ent, meaningful, interpretable, subject-heading-like,
and something-you-could-easily-label. For our pur-
poses, the usefulness of a topic can be thought of
as whether one could imagine using the topic in a
search interface to retrieve documents about a par-
ticular subject. One indicator of usefulness is the
ease by which one could think of a short label to de-
scribe a topic.
Table 1 shows a selection of high- and low-
scoring topics, as scored by the N = 9 users. The
first topic illustrates the notion of labelling coher-
ence, as space exploration, e.g., would be an obvi-
ous label for the topic. The low-scoring topics dis-
play little coherence, and one would not expect them
Resource Method Median Mean
WordNet
HSO 0.15 0.59
JCN ?0.20 0.19
LCH ?0.31 ?0.15
LESK 0.53 0.53
LIN 0.09 0.28
PATH 0.29 0.12
RES 0.57 0.66
VECTOR ?0.08 0.27
WUP 0.41 0.26
Wikipedia
RACO 0.62 0.69
MIW 0.68 0.70
DOCSIM 0.59 0.60
PMI 0.74 0.77
Google TITLES 0.51LOGHITS ?0.19
Gold-standard IAA 0.82 0.78
Table 3: Spearman rank correlation ? values for the dif-
ferent scoring methods over the BOOKS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
to be useful as categories or facets in a search inter-
face. Note that the useless topics from both collec-
tions are not chance artifacts produced by the mod-
els, but are in fact stable and robust statistical fea-
tures in the data sets.
6 Results
The results for the different topic scoring methods
over the NEWS and BOOKS collections are pre-
sented in Tables 2 and 3, respectively. In each ta-
ble, we separate out the scoring methods into those
based on WordNet (from Section 4.1), those based
on Wikipedia (from Section 4.2), and those based on
Google (from Section 4.3).
As stated in Section 4, we experiment with two
methods for combining the word-pair scores (for all
methods other than the two Google methods, which
operate natively over a word set), namely the arith-
metic mean and median. We present the numbers
for these two methods in each table. In each case,
we evaluate via Spearman rank correlation, revers-
ing the sign of the calculated ? value for PATH (as it
is the only instance of a distance metric, where the
gold-standard is made up of similarity values).
We include the inter-annotator agreement (IAA)
in the final row of each table, which we consider
106
to be the upper bound for the task. This is calcu-
lated as the average Spearman rank correlation be-
tween each annotator and the mean/median of the
remaining annotators for that topic. Encouragingly,
there is relatively little difference in the IAA be-
tween the two datasets; the median-based calcula-
tion produces slightly higher ? values and is empiri-
cally the method of choice.3
Of all the topic scoring methods tested, PMI
(term co-occurrence via simple pointwise mutual in-
formation) is the most consistent performer, achiev-
ing the best or near-best results over both datasets,
and approaching or surpassing the inter-annotator
agreement. This indicates both that the task of
topic evaluation as defined in this paper is com-
putationally tractable, and that word-pair based co-
occurrence is highly successful at modelling topic
coherence.
Comparing the different resources, Wikipedia is
far and away the most consistent performing, with
PMI producing the best results, followed by MIW
and RACO, and finally DOCSIM. There is rela-
tively little difference in results between NEWS and
BOOKS for the Wikipedia methods. Google achieves
the best results over NEWS, for TITLES (actually
slightly above the IAA), but the results fall away
sharply over BOOKS. The reason for this can be
seen in the sample topics in Table 1: the topics for
BOOKS tend to be more varied in word class than
for NEWS, and contain less proper names; also, the
genre of BOOKS is less well represented on the web.
We hypothesise that Wikipedia?s encyclopedic na-
ture means that it has good coverage over both do-
mains, and thus more robust.
Turning to WordNet, the overall results are
markedly better over BOOKS, again largely because
of the relative sparsity of proper names in the re-
source. The results for individual methods are some-
what surprising. Whereas JCN and LCH have been
shown to be two of the best-performing methods
over lexical similarity tasks (Budanitsky and Hirst,
2005; Agirre et al, 2009), they perform abysmally
at the topic scoring task. Indeed, the spread of re-
sults across the WordNet similarity methods (no-
3Note that the choice of mean or median for IAA is in-
dependent of that for the scoring methods, as they are com-
bining different things: annotator scores in the one hand, and
word/concept pair scores on the other.
tably HSO, JCN, LCH, LIN, RES and WUP) is
much greater than we had expected. The single most
consistent method is LESK, which is based on lexi-
cal overlap in definition sentences and makes rela-
tively modest use of the WordNet hierarchy. Supple-
mentary evaluation where we filtered out all proper
nouns from the topics (based on simple POS priors
for each word learned from an automatically-tagged
version of the British National Corpus) led to a slight
increase in results for the WordNet methods; the full
results are omitted for reasons of space. In future
work, we intend to carry out error analysis to deter-
mine why some of the methods performed so badly,
or inconsistently across the two datasets.
There is no clear answer to the question of
whether the mean or median is the best method for
combining the pair-wise scores.
7 Conclusions
We have proposed the novel task of topic coher-
ence evaluation as a form of intrinsic topic evalu-
ation with relevance in document search/discovery
and visualisation applications. We constructed
a gold-standard dataset of topic coherence scores
over the output of a topic model for two distinct
datasets, and evaluated a wide range of topic scor-
ing methods over this dataset, drawing on WordNet,
Wikipedia and the Google search engine. The sin-
gle best-performing method was term co-occurrence
within Wikipedia based on pointwise mutual infor-
mation, which achieve results very close to the inter-
annotator agreement for the task. Google was also
found to perform well over one of the two datasets,
while the results for the WordNet-based methods
were overall surprisingly low.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
References
E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas?ca,
and A Soroa. 2009. A study on similarity and re-
107
latedness using distributional and WordNet-based ap-
proaches. In Proc. of HLT: NAACL 2009, pages 19?
27, Boulder, Colorado.
S Banerjee and T Pedersen. 2002. An adapted Lesk algo-
rithm for word sense disambiguation using WordNet.
Proc. of CICLing?02, pages 136?145.
DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S Brody and M Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL 2009, pages 103?111,
Athens, Greece.
A Budanitsky and G Hirst. 2005. Evaluating WordNet-
based Measures of Lexical Sematic Relatedness.
Computational Linguistics, 32(1):13?47.
WL Buntine and A Jakulin. 2004. Applying discrete
PCA in data analysis. In Proc. of UAI 2004, pages
59?66.
J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.
2009. Reading tea leaves: How humans interpret topic
models. In Proc. of NIPS 2009.
H Daume III. 2009. Non-parametric bayesian areal lin-
guistics. In Proc. of HLT: NAACL 2009, pages 593?
601, Boulder, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science, 41(6).
C Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, USA.
E Gabrilovich and S Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proc. of IJCAI?07, pages 1606?
1611, Hyderabad, India.
T Griffiths and M Steyvers. 2004. Finding scientific top-
ics. In Proc. of the National Academy of Sciences, vol-
ume 101, pages 5228?5235.
T Griffiths and M Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to
Meaning.
A Haghighi and L Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proc. of HLT: NAACL 2009, pages 362?370, Boulder,
USA.
G Hirst and D St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropism. In Fellbaum (Fellbaum, 1998), pages
305?332.
T Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of COLING?97, pages 19?33, Taipei, Taiwan.
C Leacock, G A Miller, and M Chodorow. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147?65.
M Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC?86,
pages 24?26, Toronto, Canada.
D Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING/ACL?98, pages 768?
774, Montreal, Canada.
C-Y Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proc. of the ACL 2004
Workshop on Text Summarization Branches Out (WAS
2004), pages 74?81, Barcelona, Spain.
Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling
of multinomial topic models. In Proc. of KDD 2007,
pages 490?499.
D Milne and IH Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence, pages 25?30,
Chicago, USA.
H Misra, O Cappe, and F Yvon. 2008. Using LDA to
detect semantically incoherent documents. In Proc. of
CoNLL 2008, pages 41?48, Manchester, England.
D Newman, S Karimi, and L Cavedon. 2009. External
evaluation of topic models. In Proc. of ADCS 2009,
pages 11?18, Sydney, Australia.
D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-
tinez, and J Zobel. to appeara. Visualizing docu-
ment collections and search results using topic map-
ping. Journal of Web Semantics.
D Newman, Y Noh, E Talley, S Karimi, and T Bald-
win. to appearb. Evaluating topic models for digital
libraries. In Proc. of JCDL/ICADL 2010, Gold Coast,
Australia.
K Papineni, S Roukos, T Ward, and W-J Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL 2002, pages 311?318,
Philadelphia, USA.
P Pecina. 2008. Lexical Association Measures: Colloca-
tion Extraction. Ph.D. thesis, Charles University.
P Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of IJ-
CAI?95, pages 448?453, Montreal, Canada.
H Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
M Stru?be and SP Ponzetto. 2006. WikiRelate! comput-
ing semantic relateness using Wikipedia. In Proc. of
AAAI?06, pages 1419?1424, Boston, USA.
Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation
with LDA-based Fisher kernel. In Proc. of ACL-08:
HLT, pages 269?272.
HM Wallach, I Murray, R Salakhutdinov, and
DM Mimno. 2009. Evaluation methods for
topic models. In Proc. of ICML 2009, page 139.
D Widdows and K Ferraro. 2008. Semantic Vectors:
A scalable open source package and online technol-
ogy management application. In Proc. of LREC 2008,
Marrakech, Morocco.
Z Wu and M Palmer. 1994. Verb selection and lexical
selection. In Proc. of ACL?94, pages 133?138, Las
Cruces, USA.
108
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1536?1545,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Labelling of Topic Models
Jey Han Lau,?? Karl Grieser,? David Newman,?? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
jhlau@csse.unimelb.edu.au, kgrieser@csse.unimelb.edu.au, newman@uci.edu, tb@ldwin.net
Abstract
We propose a method for automatically la-
belling topics learned via LDA topic models.
We generate our label candidate set from the
top-ranking topic terms, titles of Wikipedia ar-
ticles containing the top-ranking topic terms,
and sub-phrases extracted from the Wikipedia
article titles. We rank the label candidates us-
ing a combination of association measures and
lexical features, optionally fed into a super-
vised ranking model. Our method is shown to
perform strongly over four independent sets of
topics, significantly better than a benchmark
method.
1 Introduction
Topic modelling is an increasingly popular frame-
work for simultaneously soft-clustering terms and
documents into a fixed number of ?topics?, which
take the form of a multinomial distribution over
terms in the document collection (Blei et al,
2003). It has been demonstrated to be highly ef-
fective in a wide range of tasks, including multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008), information retrieval (Wei and
Croft, 2006) and image labelling (Feng and Lapata,
2010).
One standard way of interpreting a topic is to use
the marginal probabilities p(wi|tj) associated with
each term wi in a given topic tj to extract out the 10
terms with highest marginal probability. This results
in term lists such as:1
stock market investor fund trading invest-
ment firm exchange companies share
1Here and throughout the paper, we will represent a topic tj
via its ranking of top-10 topic terms, based on p(wi|tj).
which are clearly associated with the domain of
stock market trading. The aim of this research is to
automatically generate topic labels which explicitly
identify the semantics of the topic, i.e. which take us
from a list of terms requiring interpretation to a sin-
gle label, such as STOCK MARKET TRADING in the
above case.
The approach proposed in this paper is to first
generate a topic label candidate set by: (1) sourc-
ing topic label candidates from Wikipedia by query-
ing with the top-N topic terms; (2) identifying the
top-ranked document titles; and (3) further post-
processing the document titles to extract sub-strings.
We translate each topic label into features extracted
from Wikipedia, lexical association with the topic
terms in Wikipedia documents, and also lexical fea-
tures for the component terms. This is used as the
basis of a support vector regression model, which
ranks each topic label candidate.
Our contributions in this work are: (1) the genera-
tion of a novel evaluation framework and dataset for
topic label evaluation; (2) the proposal of a method
for both generating and scoring topic label candi-
dates; and (3) strong in- and cross-domain results
across four independent document collections and
associated topic models, demonstrating the ability
of our method to automatically label topics with re-
markable success.
2 Related Work
Topics are conventionally interpreted via their top-
N terms, ranked based on the marginal probability
p(wi|tj) in that topic (Blei et al, 2003; Griffiths and
Steyvers, 2004). This entails a significant cognitive
load in interpretation, prone to subjectivity. Topics
are also sometimes presented with manual post-hoc
labelling for ease of interpretation in research pub-
lications (Wang and McCallum, 2006; Mei et al,
1536
2006). This has obvious disadvantages in terms of
subjectivity, and lack of reproducibility/automation.
The closest work to our method is that of Mei et
al. (2007), who proposed various unsupervised ap-
proaches for automatically labelling topics, based
on: (1) generating label candidates by extracting ei-
ther bigrams or noun chunks from the document col-
lection; and (2) ranking the label candidates based
on KL divergence with a given topic. Their proposed
methodology generates a generic list of label can-
didates for all topics using only the document col-
lection. The best method uses bigrams exclusively,
in the form of the top-1000 bigrams based on the
Student?s t-test. We reimplement their method and
present an empirical comparison in Section 5.3.
In other work, Magatti et al (2009) proposed a
method for labelling topics induced by a hierarchi-
cal topic model. Their label candidate set is the
Google Directory (gDir) hierarchy, and label selec-
tion takes the form of ontological alignment with
gDir. The experiments presented in the paper are
highly preliminary, although the results certainly
show promise. However, the method is only applica-
ble to a hierarchical topic model and crucially relies
on a pre-existing ontology and the class labels con-
tained therein.
Pantel and Ravichandran (2004) addressed the
more specific task of labelling a semantic class
by applying Hearst-style lexico-semantic patterns
to each member of that class. When presented
with semantically homogeneous, fine-grained near-
synonym clusters, the method appears to work well.
With topic modelling, however, the top-ranking
topic terms tended to be associated and not lexically
similar to one another. It is thus highly questionable
whether their method could be applied to topic mod-
els, but it would certainly be interesting to investi-
gate whether our model could conversely be applied
to the labelling of sets of near-synonyms.
In recent work, Lau et al (2010) proposed to ap-
proach topic labelling via best term selection, i.e.
selecting one of the top-10 topic terms to label the
overall topic. While it is often possible to label top-
ics with topic terms (as is the case with the stock
market topic above), there are also often cases where
topic terms are not appropriate as labels. We reuse
a selection of the features proposed by Lau et al
(2010), and return to discuss it in detail in Section 3.
While not directly related to topic labelling,
Chang et al (2009) were one of the first to propose
human labelling of topic models, in the form of syn-
thetic intruder word and topic detection tasks. In the
intruder word task, they include a term w with low
marginal probability p(w|t) for topic t into the top-
N topic terms, and evaluate how well both humans
and their model are able to detect the intruder.
The potential applications for automatic labelling
of topics are many and varied. In document col-
lection visualisation, e.g., the topic model can be
used as the basis for generating a two-dimensional
representation of the document collection (Newman
et al, 2010a). Regions where documents have a
high marginal probability p(di|tj) of being associ-
ated with a given topic can be explicitly labelled
with the learned label, rather than just presented
as an unlabelled region, or presented with a dense
?term cloud? from the original topic. In topic model-
based selectional preference learning (Ritter et al,
2010; O` Se?aghdha, 2010), the learned topics can
be translated into semantic class labels (e.g. DAYS
OF THE WEEK), and argument positions for individ-
ual predicates can be annotated with those labels for
greater interpretability/portability. In dynamic topic
models tracking the diachronic evolution of topics
in time-sequenced document collections (Blei and
Lafferty, 2006), labels can greatly enhance the inter-
pretation of what topics are ?trending? at any given
point in time.
3 Methodology
The task of automatic labelling of topics is a natural
progression from the best topic term selection task
of Lau et al (2010). In that work, the authors use
a reranking framework to produce a ranking of the
top-10 topic terms based on how well each term ? in
isolation ? represents a topic. For example, in our
stock market investor fund trading ... topic example,
the term trading could be considered as a more rep-
resentative term of the overall semantics of the topic
than the top-ranked topic term stock.
While the best term could be used as a topic la-
bel, topics are commonly ideas or concepts that are
better expressed with multiword terms (for example
STOCK MARKET TRADING), or terms that might not
be in the top-10 topic terms (for example, COLOURS
1537
would be a good label for a topic of the form red
green blue cyan ...).
In this paper, we propose a novel method for au-
tomatic topic labelling that first generates topic label
candidates using English Wikipedia, and then ranks
the candidates to select the best topic labels.
3.1 Candidate Generation
Given the size and diversity of English Wikipedia,
we posit that the vast majority of (coherent) topics
or concepts are encapsulated in a Wikipedia article.
By making this assumption, the difficult task of gen-
erating potential topic labels is transposed to find-
ing relevant Wikipedia articles, and using the title of
each article as a topic label candidate.
We first use the top-10 topic terms (based on the
marginal probabilities from the original topic model)
to query Wikipedia, using: (a) Wikipedia?s native
search API; and (b) a site-restricted Google search.
The combined set of top-8 article titles returned
from the two search engines for each topic consti-
tutes the initial set of primary candidates.
Next we chunk parse the primary candidates us-
ing the OpenNLP chunker,2 and extract out all noun
chunks. For each noun chunk, we generate all com-
ponent n-grams (including the full chunk), out of
which we remove all n-grams which are not in them-
selves article titles in English Wikipedia. For exam-
ple, if the Wikipedia document title were the single
noun chunk United States Constitution, we would
generate the bigrams United States and States Con-
stitution, and prune the latter; we would also gen-
erate the unigrams United, States and Constitution,
all of which exist as Wikipedia articles and are pre-
served.
In this way, an average of 30?40 secondary labels
are produced for each topic based on noun chunk n-
grams. A good portion of these labels are commonly
stopwords or unigrams that are only marginally re-
lated to the topic (an artifact of the n-gram gener-
ation process). To remove these outlier labels, we
use the RACO lexical association method of Grieser
et al (2011).
RACO (Related Article Conceptual Overlap) uses
Wikipedia?s link structure and category membership
to identify the strength of relationship between arti-
2http://opennlp.sourceforge.net/
cles via their category overlap. The set of categories
related to an article is defined as the union of the cat-
egory membership of all outlinks in that article. The
category overlap of two articles (a and b) is the in-
tersection of the related category sets of each article.
The formal definition of this measure is as follows:
|(?p?O(a)C(p)) ? (?p?O(b)C(p))|
where O(a) is the set of outlinks from article a, and
C(p) is the set of categories of which article p is a
member. This is then normalised using Dice?s co-
efficient to generate a similarity measure. In the in-
stance that a term maps onto multiple Wikipedia ar-
ticles via a disambiguation page, we return the best
RACO score across article pairings for a given term
pair. The final score for each secondary label can-
didate is calculated as the average RACO score with
each of the primary label candidates. All secondary
labels with an average RACO score of 0.1 and above
are added to the label candidate set.
Finally, we add the top-5 topic terms to the set of
candidates, based on the marginals from the origi-
nal topic model. Doing this ensures that there are
always label candidates for all topics (even if the
Wikipedia searches fail), and also allows the pos-
sibility of labeling a topic using its own topic terms,
which was demonstrated by Lau et al (2010) to be a
baseline source of topic label candidates.
3.2 Candidate Ranking
After obtaining the set of topic label candidates, the
next step is to rank the candidates to find the best la-
bel for each topic. We will first describe the features
that we use to represent label candidates.
3.2.1 Features
A good label should be strongly associated with
the topic terms. To learn the association of a label
candidate with the topic terms, we use several lexical
association measures: pointwise mutual information
(PMI), Student?s t-test, Dice?s coefficient, Pearson?s
?2 test, and the log likelihood ratio (Pecina, 2009).
We also include conditional probability and reverse
conditional probability measures, based on the work
of Lau et al (2010). To calculate the association
measures, we parse the full collection of English
Wikipedia articles using a sliding window of width
1538
20, and obtain term frequencies for the label candi-
dates and topic terms. To measure the association
between a label candidate and a list of topic terms,
we average the scores of the top-10 topic terms.
In addition to the association measures, we in-
clude two lexical properties of the candidate: the raw
number of terms, and the relative number of terms in
the label candidate that are top-10 topic terms.
We also include a search engine score for each
label candidate, which we generate by querying a
local copy of English Wikipedia with the top-10
topic terms, using the Zettair search engine (based
on BM25 term similarity).3 For a given label candi-
date, we return the average score for the Wikipedia
article(s) associated with it.
3.2.2 Unsupervised and Supervised Ranking
Each of the proposed features can be used as the
basis for an unsupervised model for label candidate
selection, by ranking the label candidates for a given
topic and selecting the top-N . Alternatively, they
can be combined in a supervised model, by training
over topics where we have gold-standard labelling
of the label candidates. For the supervised method,
we use a support vector regression (SVR) model
(Joachims, 2006) over all of the features.
4 Datasets
We conducted topic labelling experiments using
document collections constructed from four distinct
domains/genres, to test the domain/genre indepen-
dence of our method:
BLOGS : 120,000 blog articles dated from August
to October 2008 from the Spinn3r blog dataset4
BOOKS : 1,000 English language books from the
Internet Archive American Libraries collection
NEWS : 29,000 New York Times news articles
dated from July to September 1999, from the
English Gigaword corpus
PUBMED : 77,000 PubMed biomedical abstracts
published in June 2010
3http://www.seg.rmit.edu.au/zettair/
4http://www.icwsm.org/data/
The BLOGS dataset contains blog posts that cover
a diverse range of subjects, from product reviews
to casual, conversational messages. The BOOKS
topics, coming from public-domain out-of-copyright
books (with publication dates spanning more than
a century), relate to a wide range of topics includ-
ing furniture, home decoration, religion and art,
and have a more historic feel to them. The NEWS
topics reflect the types and range of subjects one
might expect in news articles such as health, finance,
entertainment, and politics. The PUBMED topics
frequently contain domain-specific terms and are
sharply differentiated from the topics for the other
corpora. We are particularly interested in the perfor-
mance of the method over PUBMED, as it is a highly
specialised domain where we may expect lower cov-
erage of appropriate topic labels within Wikipedia.
We took a standard approach to topic modelling
each of the four document collections: we tokenised,
lemmatised and stopped each document,5 and cre-
ated a vocabulary of terms that occurred at least
ten times. From this processed data, we created a
bag-of-words representation of each document, and
learned topic models with T = 100 topics in each
case.
To focus our experiments on topics that were rela-
tively more coherent and interpretable, we first used
the method of Newman et al (2010b) to calculate
the average PMI-score for each topic, and filtered
all topics that had an average PMI-score lower than
0.4. We additionally filtered any topics where less
than 5 of the top-10 topic terms are default nomi-
nal in Wikipedia.6 The filtering criteria resulted in
45 topics for BLOGS, 38 topics for BOOKS, 60 top-
ics for NEWS, and 85 topics for PUBMED. Man-
ual inspection of the discarded topics indicated that
they were predominantly hard-to-label junk topics or
mixed topics, with limited utility for document/term
clustering.
Applying our label candidate generation method-
ology to these 228 topics produced approximately
6000 labels ? an average of 27 labels per topic.
5OpenNLP is used for tokenization, Morpha for lemmatiza-
tion (Minnen et al, 2001).
6As determined by POS tagging English Wikipedia with
OpenNLP, and calculating the coarse-grained POS priors (noun,
verb, etc.) for each term.
1539
Figure 1: A screenshot of the topic label evaluation task on Amazon Mechanical Turk. This screen constitutes a
Human Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated. Note
that been would be the stopword label in this example.
4.1 Topic Candidate Labelling
To evaluate our methods and train the supervised
method, we require gold-standard ratings for the la-
bel candidates. To this end, we used Amazon Me-
chanical Turk to collect annotations for our labels.
In our annotation task, each topic was presented
in the form of its top-10 terms, followed by 10 sug-
gested labels for the topic. This constitutes a Human
Intelligence Task (HIT); annotators are paid based
on the number of HITs they have completed. A
screenshot of a HIT seen by annotator is presented
in Figure 1.
In each HIT, annotators were asked to rate the la-
bels based on the following ordinal scale:
3: Very good label; a perfect description of the
topic.
2: Reasonable label, but does not completely cap-
ture the topic.
1: Label is semantically related to the topic, but
would not make a good topic label.
0: Label is completely inappropriate, and unrelated
to the topic.
To filter annotations from workers who did not
perform the task properly or from spammers, we ap-
1540
Domain Topic Terms Label Candidate AverageRating
BLOGS china chinese olympics gold olympic team win beijing medal sport 2008 summer olympics 2.60
BOOKS church arch wall building window gothic nave side vault tower gothic architecture 2.40
NEWS israel peace barak israeli minister palestinian agreement prime leader palestinians israeli-palestinian conflict 2.63
PUBMED cell response immune lymphocyte antigen cytokine t-cell induce receptor immunity immune system 2.36
Table 1: A sample of topics and topic labels, along with the average rating for each label candidate
plied a few heuristics to automatically detect these
workers. Additionally, we inserted a small num-
ber of stopwords as label candidates in each HIT
and recorded workers who gave high ratings to these
stopwords. Annotations from workers who failed to
passed these tests are removed from the final set of
gold ratings.
Each label candidate was rated in this way by at
least 10 annotators, and ratings from annotators who
passed the filter were combined by averaging them.
A sample of topics, label candidates, and the average
rating is presented in Table 1.7
Finally, we train the regression model over all
the described features, using the human rating-based
ranking.
5 Experiments
In this section we present our experimental results
for the topic labelling task, based on both the unsu-
pervised and supervised methods, and the methodol-
ogy of Mei et al (2007), which we denote MSZ for
the remainder of the paper.
5.1 Evaluation
We use two basic measures to evaluate the perfor-
mance of our predictions. Top-1 average rating is
the average annotator rating given to the top-ranked
system label, and has a maximum value of 3 (where
annotators unanimously rated all top-ranked system
labels with a 3). This is intended to give a sense of
the absolute utility of the top-ranked candidates.
The second measure is normalized discounted
cumulative gain (nDCG: Jarvelin and Kekalainen
(2002), Croft et al (2009)), computed for the top-1
(nDCG-1), top-3 (nDCG-3) and top-5 ranked sys-
tem labels (nDCG-5). For a given ordered list of
7The dataset is available for download from
http://www.csse.unimelb.edu.au/research/
lt/resources/acl2011-topic/.
scores, this measure is based on the difference be-
tween the original order, and the order when the list
is sorted by score. That is, if items are ranked op-
timally in descending order of score at position N ,
nDCG-N is equal to 1. nDCG is a normalised score,
and indicates how close the candidate label ranking
is to the optimal ranking within the set of annotated
candidates, noting that an nDCG-N score of 1 tells
us nothing about absolute values of the candidates.
This second evaluation measure is thus intended to
reflect the relative quality of the ranking, and com-
plements the top-1 average rating.
Note that conventional precision- and recall-based
evaluation is not appropriate for our task, as each
label candidate has a real-valued rating.
As a baseline for the task, we use the unsuper-
vised label candidate ranking method based on Pear-
son?s ?2 test, as it was overwhelmingly found to be
the pick of the features for candidate ranking.
5.2 Results for the Supervised Method
For the supervised model, we present both in-
domain results based on 10-fold cross-validation,
and cross-domain results where we learn a model
from the ratings for the topic model from a given
domain, and apply it to a second domain. In each
case, we learn an SVR model over the full set of fea-
tures described in Section 3.2.1. In practical terms,
in-domain results make the unreasonable assump-
tion that we have labelled 90% of labels in order
to be able to label the remaining 10%, and cross-
domain results are thus the more interesting data
point in terms of the expected results when apply-
ing our method to a novel topic model. It is valuable
to compare the two, however, to gauge the relative
impact of domain on the results.
We present the results for the supervised method
in Table 2, including the unsupervised baseline and
an upper bound estimate for comparison purposes.
The upper bound is calculated by ranking the candi-
1541
Test Domain Training Top-1 Average Rating nDCG-1 nDCG-3 nDCG-5All 1? 2? Top5
BLOGS
Baseline (unsupervised) 1.84 1.87 1.75 1.74 0.75 0.77 0.79
In-domain 1.98 1.94 1.95 1.77 0.81 0.82 0.83
Cross-domain: BOOKS 1.88 1.92 1.90 1.77 0.77 0.81 0.83
Cross-domain: NEWS 1.97 1.94 1.92 1.77 0.80 0.83 0.83
Cross-domain: PUBMED 1.95 1.95 1.93 1.82 0.80 0.82 0.83
Upper bound 2.45 2.26 2.29 2.18 1.00 1.00 1.00
BOOKS
Baseline (unsupervised) 1.75 1.76 1.70 1.72 0.77 0.77 0.79
In-domain 1.91 1.90 1.83 1.74 0.84 0.81 0.83
Cross-domain: BLOGS 1.82 1.88 1.79 1.71 0.79 0.81 0.82
Cross-domain: NEWS 1.82 1.87 1.80 1.75 0.79 0.81 0.83
Cross-domain: PUBMED 1.87 1.87 1.80 1.73 0.81 0.82 0.83
Upper bound 2.29 2.17 2.15 2.04 1.00 1.00 1.00
NEWS
Baseline (unsupervised) 1.96 1.76 1.87 1.70 0.80 0.79 0.78
In-domain 2.02 1.92 1.90 1.82 0.82 0.82 0.84
Cross-domain: BLOGS 2.03 1.92 1.89 1.85 0.83 0.82 0.84
Cross-domain: BOOKS 2.01 1.80 1.93 1.73 0.82 0.82 0.83
Cross-domain: PUBMED 2.01 1.93 1.94 1.80 0.82 0.82 0.83
Upper bound 2.45 2.31 2.33 2.12 1.00 1.00 1.00
PUBMED
Baseline (unsupervised) 1.73 1.74 1.68 1.63 0.75 0.77 0.79
In-domain 1.79 1.76 1.74 1.67 0.77 0.82 0.84
Cross-domain: BLOGS 1.80 1.77 1.73 1.69 0.78 0.82 0.84
Cross-domain: BOOKS 1.77 1.70 1.74 1.64 0.77 0.82 0.83
Cross-domain: NEWS 1.79 1.76 1.73 1.65 0.77 0.82 0.84
Upper bound 2.31 2.17 2.22 2.01 1.00 1.00 1.00
Table 2: Supervised results for all domains
dates based on the annotated human ratings. The up-
per bound for top-1 average rating is thus the high-
est average human rating of all label candidates for
a given topic, while the upper bound for the nDCG
measures will always be 1.
In addition to results for the combined candidate
set, we include results for each of the three candi-
date subsets, namely the primary Wikipedia labels
(?1??), the secondary Wikipedia labels (?2??) and
the top-5 topic terms (?Top5?); the nDCG results
are over the full candidate set only, as the numbers
aren?t directly comparable over the different subsets
(due to differences in the number of candidates and
the distribution of ratings).
Comparing the in-domain and cross-domain re-
sults, we observe that they are largely compara-
ble, with the exception of BOOKS, where there is
a noticeable drop in both top-1 average rating and
nDGC-1 when we use cross-domain training. We
see an appreciable drop in scores when we train
BOOKS against BLOGS (or vice versa), which we
analyse as being due to incompatibility in document
content and structure between these two domains.
Overall though, the results are very encouraging,
and point to the plausibility of using labelled topic
models from independent domains to learn the best
topic labels for a new domain.
Returning to the question of the suitability of la-
bel candidates for the highly specialised PUBMED
document collection, we first notice that the up-
per bound top-1 average rating is comparable to
the other domains, indicating that our method has
been able to extract equivalent-quality label can-
didates from Wikipedia. The top-1 average rat-
ings of the supervised method are lower than the
other domains. We hypothesise that the cause of
the drop is that the lexical association measures are
trained over highly diverse Wikipedia data rather
than biomedical-specific data, and predict that the
results would improve if we trained our features over
PubMed.
The results are uniformly better than the unsuper-
vised baselines for all four corpora, although there
is quite a bit of room for improvement relative to the
upper bound. To better gauge the quality of these
results, we carry out a direct comparison of our pro-
posed method with the best-performing method of
MSZ in Section 5.3.
1542
Looking to the top-1 average score results over the
different candidate sets, we observe first that the up-
per bound for the combined candidate set (?All?) is
higher than the scores for the candidate subsets in all
cases, underlining the complementarity of the differ-
ent candidate sets. We also observe that the top-5
topic term candidate set is the lowest performer out
of the three subsets across all four corpora, in terms
of both upper bound and the results for the super-
vised method. This reinforces our comments about
the inferiority of the topic word selection method of
Lau et al (2010) for topic labelling purposes. For
NEWS and PUBMED, there is a noticeable differ-
ence between the results of the supervised method
over the full candidate set and each of the candidate
subsets. In contrast, for BOOKS and BLOGS, the re-
sults for the primary candidate subset are at times
actually higher than those over the full candidate set
in most cases (but not for the upper bound). This is
due to the larger search space in the full candidate
set, and the higher median quality of candidates in
the primary candidate set.
5.3 Comparison with MSZ
The best performing method out of the suite of
approaches proposed by MSZ method exclusively
uses bigrams extracted from the document collec-
tion, ranked based on Student?s t-test. The potential
drawbacks to this approach are: all labels must be
bigrams, there must be explicit token instances of
a given bigram in the document collection for it to
be considered as a label candidate, and furthermore,
there must be enough token instances in the docu-
ment collection for it to have a high t score.
To better understand the performance difference
of our approach to that of MSZ, we perform direct
comparison of our proposed method with the bench-
mark method of MSZ.
5.3.1 Candidate Ranking
First, we compare the candidate ranking method-
ology of our method with that of MSZ, using the
label candidates extracted by the MSZ method.
We first extracted the top-2000 bigrams using the
N -gram Statistics Package (Banerjee and Pedersen,
2003). We then ranked the bigrams for each topic
using the Student?s t-test. We included the top-5 la-
bels generated for each topic by the MSZ method
in our Mechanical Turk annotation task, and use the
annotations to directly compare the two methods.
To measure the performance of candidate rank-
ing between our supervised method and MSZ?s, we
re-rank the top-5 labels extracted by MSZ using
our SVR methodology (in-domain) and compare the
top-1 average rating and nDCG scores. Results are
shown in Table 3. We do not include results for the
BOOKS domain because the text collection is much
larger than the other domains, and the computation
for the MSZ relevance score ranking is intractable
due to the number of n-grams (a significant short-
coming of the method).
Looking at the results for the other domains, it is
clear that our ranking system has the upper hand:
it consistently outperforms MSZ over every evalu-
ation metric.8 Comparing the top-1 average rating
results back to those in Table 2, we observe that
for all three domains, the results for MSZ are be-
low those of the unsupervised baseline, and well be-
low those of our supervised method. The nDCG re-
sults are more competitive, and the nDCG-3 results
are actually higher than our original results in Ta-
ble 2. It is important to bear in mind, however, that
the numbers are in each case relative to a different la-
bel candidate set. Additionally, the results in Table 3
are based on only 5 candidates, with a relatively flat
gold-standard rating distribution, making it easier to
achieve higher nDCG-5 scores.
5.3.2 Candidate Generation
The method of MSZ makes the implicit assump-
tion that good bigram labels are discoverable within
the document collection. In our method, on the other
hand, we (efficiently) access the much larger and
variable n-gram length set of English Wikipedia ar-
ticle titles, in addition to the top-5 topic terms. To
better understand the differences in label candidate
sets, and the relative coverage of the full label can-
didate set in each case, we conducted another survey
where human users were asked to suggest one topic
label for each topic presented.
The survey consisted, once again, of presenting
annotators with a topic, but in this case, we gave
them the open task of proposing the ideal label for
8Based on a single ANOVA, the difference in results is sta-
tistically significant at the 5% level for BLOGS, and 1% for
NEWS and PUBMED.
1543
Test Domain Candidate Ranking Top-1 nDCG-1 nDCG-3 nDCG-5System Avg. Rating
BLOGS
MSZ 1.26 0.65 0.76 0.87
SVR 1.41 0.75 0.85 0.92
Upper bound 1.87 1.00 1.00 1.00
NEWS
MSZ 1.37 0.73 0.81 0.90
SVR 1.66 0.88 0.90 0.95
Upper bound 1.86 1.00 1.00 1.00
PUBMED
MSZ 1.53 0.77 0.85 0.93
SVR 1.73 0.87 0.91 0.96
Upper bound 1.98 1.00 1.00 1.00
Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZ
the topic. In this, we did not enforce any restrictions
on the type or size of label (e.g. the number of terms
in the label).
Of the manually-generated gold-standard labels,
approximately 36% were contained in the original
document collection, but 60% were Wikipedia arti-
cle titles. This indicates that our method has greater
potential to generate a label of the quality of the ideal
proposed by a human in a completely open-ended
task.
6 Discussion
On the subject of suitability of using Amazon Me-
chanical Turk for natural language tasks, Snow et al
(2008) demonstrated that the quality of annotation
is comparable to that of expert annotators. With that
said, the PUBMED topics are still a subject of inter-
est, as these topics often contain biomedical terms
which could be difficult for the general populace to
annotate.
As the number of annotators per topic and the
number of annotations per annotator vary, there is
no immediate way to calculate the inter-annotator
agreement. Instead, we calculated the MAE score
for each candidate, which is an average of the ab-
solute difference between an annotator?s rating and
the average rating of a candidate, summed across all
candidates to get the MAE score for a given corpus.
The MAE scores for each corpus are shown in Ta-
ble 4, noting that a smaller value indicates higher
agreement.
As the table shows, the agreement for the
PUBMED domain is comparable with the other
datasets. BLOGS and NEWS have marginally better
Corpus MAE
BLOGS 0.50
BOOKS 0.56
NEWS 0.52
PUBMED 0.56
Table 4: Average MAE score for label candidate rating
over each corpus
agreement, almost certainly because of the greater
immediacy of the topics, covering everyday areas
such as lifestyle and politics. BOOKS topics are oc-
casionally difficult to label due to the breadth of the
domain; e.g. consider a topic containing terms ex-
tracted from Shakespeare sonnets.
7 Conclusion
This paper has presented the task of topic labelling,
that is the generation and scoring of labels for a
given topic. We generate a set of label candidates
from the top-ranking topic terms, titles of Wikipedia
articles containing the top-ranking topic terms, and
also a filtered set of sub-phrases extracted from the
Wikipedia article titles. We rank the label candidates
using a combination of association measures, lexical
features and an Information Retrieval feature. Our
method is shown to perform strongly over four inde-
pendent sets of topics, and also significantly better
than a competitor system.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
1544
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML 2006.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. In EACL 2009, pages 103?111.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans in-
terpret topic models. In NIPS, pages 288?296.
B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison
Wesley.
Y. Feng and M. Lapata. 2010. Topic models for im-
age annotation and text illustration. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 831?839, Los Angeles, USA, June.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using ontological and document similarity to
estimate museum exhibit relatedness. ACM Journal
on Computing and Cultural Heritage, 3(3):1?20.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In PNAS, volume 101, pages 5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
HLT: NAACL 2009, pages 362?370.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), pages 217?226,
New York, NY, USA. ACM.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin. 2010.
Best topic word selection for topic labelling. In Coling
2010: Posters, pages 605?613, Beijing, China.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA 2009, pages
1227?1232, Pisa, Italy.
Q. Mei, C. Liu, H. Su, and C. Zhai. 2006. A probabilistic
approach to spatiotemporal theme pattern mining on
weblogs. In WWW 2006, pages 533?542.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic labeling
of multinomial topic models. In SIGKDD, pages 490?
499.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Journal of Nat-
ural Language Processing, 7(3):207?223.
D. Newman, T. Baldwin, L. Cavedon, S. Karimi, D. Mar-
tinez, and J. Zobel. 2010a. Visualizing document col-
lections and search results using topic mapping. Jour-
nal of Web Semantics, 8(2-3):169?175.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010b. Automatic evaluation of topic coherence. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 100?108, Los Angeles,
USA, June. Association for Computational Linguis-
tics.
D. O` Se?aghdha. 2010. Latent variable models of selec-
tional preference. In ACL 2010.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In HLT/NAACL-04, pages
321?328.
P. Pecina. 2009. Lexical Association Measures: Collo-
cation Extraction. Ph.D. thesis, Charles University.
A. Ritter, Mausam, and O. Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL 2010.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?
263, Morristown, NJ, USA.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In WWW ?08,
pages 111?120.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In KDD, pages 424?433.
S. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR ?06, pages 178?
185.
1545
