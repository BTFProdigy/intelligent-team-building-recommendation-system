Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 69?72,
New York, June 2006. c?2006 Association for Computational Linguistics
MMR-based Active Machine Learning  
for Bio Named Entity Recognition 
Seokhwan Kim1 Yu Song2 Kyungduk Kim1 Jeong-Won Cha3 Gary Geunbae Lee1
1 Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea 
2 AIA Information Technology Co., Ltd. Beijing, China 
3 Dept. of Computer Science, Changwon National University, Changwon, Korea 
megaup@postech.ac.kr, Song-Y.Song@AIG.com, getta@postech.ac.kr
jcha@changwon.ac.kr, gblee@postech.ac.kr
Abstract 
This paper presents a new active learning 
paradigm which considers not only the 
uncertainty of the classifier but also the 
diversity of the corpus. The two measures 
for uncertainty and diversity were com-
bined using the MMR (Maximal Marginal 
Relevance) method to give the sampling 
scores in our active learning strategy. We 
incorporated MMR-based active machine-
learning idea into the biomedical named-
entity recognition system. Our experimen-
tal results indicated that our strategies for 
active-learning based sample selection 
could significantly reduce the human ef-
fort. 
1 Introduction 
Named-entity recognition is one of the most ele-
mentary and core problems in biomedical text min-
ing. To achieve good recognition performance, we 
use a supervised machine-learning based approach 
which is a standard in the named-entity recognition 
task. The obstacle of supervised machine-learning 
methods is the lack of the annotated training data 
which is essential for achieving good performance. 
Building a training corpus manually is time con-
suming, labor intensive, and expensive. Creating 
training corpora for the biomedical domain is par-
ticularly expensive as it requires domain specific 
expert knowledge. 
One way to solve this problem is through active 
learning method to select the most informative 
samples for training. Active selection of the train-
ing examples can significantly reduce the neces-
sary number of labeled training examples without 
degrading the performance. 
Existing work for active learning explores two 
approaches: certainty or uncertainty-based methods 
(Lewis and Gale 1994; Scheffer and Wrobel 2001; 
Thompson et al 1999) and committee-based 
methods (Cohn et al 1994; Dagan and Engelson 
1995; Freund et al 1997; Liere and Tadepalli 
1997). Uncertainty-based systems begin with an 
initial classifier and the systems assign some un-
certainty scores to the un-annotated examples. The 
k examples with the highest scores will be anno-
tated by human experts and the classifier will be 
retrained. In the committee-based systems, diverse 
committees of classifiers were generated. Each 
committee member will examine the un-annotated 
examples. The degree of disagreement among the 
committee members will be evaluated and the ex-
amples with the highest disagreement will be se-
lected for manual annotation. 
Our efforts are different from the previous ac-
tive learning approaches and are devoted to two 
aspects: we propose an entropy-based measure to 
quantify the uncertainty that the current classifier 
holds. The most uncertain samples are selected for 
human annotation. However, we also assume that 
the selected training samples should give the dif-
ferent aspects of learning features to the classifica-
tion system. So, we try to catch the most 
representative sentences in each sampling. The 
divergence measures of the two sentences are for 
the novelty of the features and their representative 
levels, and are described by the minimum similar-
ity among the examples. The two measures for un-
certainty and diversity will be combined using the 
MMR (Maximal Marginal Relevance) method 
(Carbonell and Goldstein 1998) to give the sam-
pling scores in our active learning strategy. 
69
We incorporate MMR-based active machine-
learning idea into the POSBIOTM/NER (Song et 
al. 2005) system which is a trainable biomedical 
named-entity recognition system using the Condi-
tional Random Fields (Lafferty et al 2001) ma-
chine learning technique to automatically identify 
different sets of biological entities in the text. 
2 MMR-based Active Learning for Bio-
medical Named-entity Recognition 
2.1 Active Learning 
We integrate active learning methods into the 
POSBIOTM/NER (Song et al 2005) system by the 
following procedure: Given an active learning 
scoring strategy S and a threshold value th, at each 
iteration t, the learner uses training corpus TMt   to 
train the NER module Mt. Each time a user wants 
to annotate a set of un-labeled sentences U, the 
system first tags the sentences using the current 
NER module Mt. At the same time, each tagged 
sentence is assigned with a score according to our 
scoring strategy S. Sentences will be marked if its 
score is larger than the threshold value th. The tag 
result is presented to the user, and those marked 
ones are rectified by the user and added to the 
training corpus. Once the training data accumulates 
to a certain amount, the NER module Mt will be 
retrained. 
2.2 Uncertainty-based Sample Selection 
We evaluate the uncertainty degree that the current 
NER module holds for a given sentence in terms of 
the entropy of the sentence. Given an input se-
quence o, the state sequence set S is a finite set. 
And  is the probability distribu-
tion over S. By using the equation for CRF 
(Lafferty et al 2001) module, we can calculate the 
probability of any possible state sequence s given 
an input sequence o. Then the entropy of  
is defined to be: 
Sso|s ??   ),(p
)( o|s?p
? ???=
s
o|so|s )]([log)( 2 PPH  
The number of possible state sequences grows 
exponentially as the sentence length increases. In 
order to measure the uncertainty by entropy, it is 
inconvenient and unnecessary to compute the 
probability of all the possible state sequences. In-
stead we implement N-best Viterbi search to find 
the N state sequences with the highest probabilities. 
The entropy H(N) is defined as the entropy of the 
distribution of the N-best state sequences: 
? ??= = ?
?
= ?
?
???
?
???
?
?=
N
i
N
i i
i
N
i i
i
P
P
P
P
NH
1
1
2
1
)(
)(
log
)(
)(
)(
o|s
o|s
o|s
o|s .  (1) 
The range of the entropy H(N) is [0, 
N
1
log 2? ] which varies according to different N. 
We could use the equation (2) to normalize the 
H(N) to [0, 1]. 
N
NH
NH
1
log
)(
)(
2?
=? .  (2) 
2.3 Diversity-based Sample Selection 
We measure the sentence structure similarity to 
represent the diversity and catch the most represen-
tative ones in order to give more diverse features to 
the machine learning-based classification systems. 
We propose a three-level hierarchy to represent 
the structure of a sentence. The first level is NP 
chunk, the second level is Part-Of-Speech tag, and 
the third level is the word itself. Each word is rep-
resented using this hierarchy structure. For exam-
ple in the sentence "I am a boy", the word "boy" is 
represented as w
r
=[NP, NN, boy]. The similarity 
score of two words is defined as: 
)()(
),(2
)(
21
21
21 wDepthwDepth
wwDepth
wwsim rr
rrrr
+
?=?  
Where ),( 21 wwDepth
rr
 is defined from the top 
level as the number of levels that the two words are 
in common. Under our three-level hierarchy 
scheme above, each word representation has depth 
of 3. 
The structure of a sentence S is represented as 
the word representation vectors ],  ,,[ 21 Nwww
rKrr . 
We measure the similarity of two sentences by the 
standard cosine-similarity measure. The similarity 
score of two sentences is defined as: 
2211
21
21 ),(
SSSS
SS
SSsimilarity rrrr
rrrr
??
?= , 
?? ?=?
i j
ji wwsimSS )( 2121
rrrr
. 
70
2.4 MMR Combination for Sample Selection 
We would like to score the sample sentences with 
respect to both the uncertainty and the diversity. 
The following MMR (Maximal Marginal Rele-
vance) (Carbonell and Goldstein 1998) formula is 
used to calculate the active learning score: 
),(Similaritymax                   
)1(),(yUncertaint)(
jiTs
i
def
i
ss
Mssscore
Mj??
???= ??   (3) 
where si is the sentence to be selected, Uncertainty 
is the entropy of si given current NER module M, 
and Similarity indicates the divergence degree be-
tween the si and the sentence sj in the training cor-
pus TM of M. The combination rule could be 
interpreted as assigning a higher score to a sen-
tence of which the NER module is uncertain and 
whose configuration differs from the sentences in 
the existing training corpus. The value of parame-
ter ?  coordinates those two different aspects of 
the desirable sample sentences. 
After initializing a NER module M and an ap-
propriate value of the parameter? , we can assign 
each candidate sentence a score under the control 
of the uncertainty and the diversity. 
3 Experiment and Discussion 
3.1 Experiment Setup 
We conducted our active learning experiments us-
ing pool-based sample selection (Lewis and Gale 
1994). The pool-based sample selection, in which 
the learner chooses the best instances for labeling 
from a given pool of unlabelled examples, is the 
most practical approach for problems in which 
unlabelled data is relatively easily available. 
For our empirical evaluation of the active learn-
ing methods, we used the training and test data 
released by JNLPBA (Kim et al 2004). The train-
ing corpus contains 2000 MEDLINE abstracts, and 
the test data contains 404 abstracts from the 
GENIA corpus. 100 abstracts were used to train 
our initial NER module. The remaining training 
data were taken as the pool. Each time, we chose k 
examples from the given pool to train the new 
NER module and the number k varied from 1000 
to 17000 with a step size 1000. 
We test 4 different active learning methods: Ran-
dom selection, Entropy-based uncertainty selection, 
Entropy combined with Diversity, and Normalized 
Entropy (equation (2)) combined with Diversity. 
When we compute the active learning score using 
the entropy based method and the combining 
methods we set the values of parameter N (from 
equation (1)) to 3 and ?  (from equation (3)) to 0.8 
empirically. 
 
Fig1. Comparison of active learning strategies with the ran-
l in the y-axis shows the 
per
bin  
ies consistently outperform 
the
dom selection 
3.2 Results and Analyses 
The initial NER module gets an F-score of 52.54, 
while the F-score performance of the NER module 
using the whole training data set is 67.19. We plot-
ted the learning curves for the different sample 
selection strategies. The interval in the x-axis be-
tween the curves shows the number of examples 
selected and the interva
formance improved. 
We compared the entropy, entropy combined 
with sentence diversity, normalized entropy com-
ed with sentence diversity and random selection.
The curves in Figure 1 show the relative per-
formance. The F-score increases along with the 
number of selected examples and receives the best 
performance when all the examples in the pool are 
selected. The results suggest that all three kinds of 
active learning strateg
 random selection.  
The entropy-based example selection has im-
proved performance compared with the random 
selection. The entropy (N=3) curve approaches to 
the random selection around 13000 sentences se-
lected, which is reasonable since all the methods 
choose the examples from the same given pool. As 
71
the number of selected sentences approaches the 
pool size, the performance difference among the 
different methods gets small. The best performance 
of the entropy strategy is 67.31 when 17000 exam-
ple
the
 normalized combined strategy 
behaves the worst. 
4 Conclusion 
ction could significantly reduce 
the human effort. 
by Minis-
try of Commerce, Industry and Energy. 
s are selected. 
Comparing with the entropy curve, the com-
bined strategy curve shows an interesting charac-
teristic. Up to 4000 sentences, the entropy strategy 
and the combined strategy perform similarly. After 
the 11000 sentence point, the combined strategy 
surpasses the entropy strategy. It accords with our 
belief that the diversity increases the classifier's 
performance when the large amount of samples is 
selected.  The normalized combined strategy dif-
fers from the combined strategy. It exceeds the 
other strategies from the beginning and maintains 
 best performance up until 12000 sentence point. 
   The entropy strategy reaches 67.00 in F-score 
when 11000 sentences are selected. The combined 
strategy receives 67.17 in F-score while 13000 sen-
tences are selected, while the end performance is 
67.19 using the whole training data. The combined 
strategy reduces 24.64 % of training examples 
compared with the random selection. The normal-
ized combined strategy achieves 67.17 in F-score 
when 11000 sentences are selected, so 35.43% of 
the training examples do not need to be labeled to 
achieve almost the same performance as the end 
performance. The normalized combined strategy's 
performance becomes similar to the random selec-
tion strategy at around 13000 sentences, and after 
14000 sentences the
 
We incorporate active learning into the biomedical 
named-entity recognition system to enhance the 
system's performance with only small amount of 
training data. We presented the entropy-based un-
certainty sample selection and combined selection 
strategies using the corpus diversity. Experiments 
indicate that our strategies for active-learning 
based sample sele
Acknowledgement  
This research was supported as a Brain Neuroin-
formatics Research Program sponsored 
References 
Carbonell J., & Goldstein J. (1998). The Use of MMR, 
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In Proceedings of 
the 21st Annual International ACM-SIGIR Confer-
ence on Research and Development in Information 
Retrieval, pages 335-336. 
Cohn, D. A., Atlas, L., & Ladner, R. E. (1994). Improv-
ing generalization with active learning, Machine 
Learning, 15(2), 201-221. 
Dagan, I., & Engelson S. (1995). Committee-based 
sampling for training probabilistic classifiers. In Pro-
ceedings of the Twelfth International Conference on 
Machine Learning, pages 150-157, San Francisco, 
CA, Morgan Kaufman. 
Freund Y., Seung H.S., Shamir E., & Tishby N. (1997). 
Selective sampling using the query by committee al-
gorithm, Machine Learning, 28, 133-168. 
Kim JD., Ohta T., Tsuruoka Y., & Tateisi Y. (2004). 
Introduction to the Bio-Entity Recognition Task at 
JNLPBA, Proceedings of the International Workshop 
on Natural Language Processing in Biomedicine and 
its Application (JNLPBA). 
Lafferty, J., McCallum, A., & Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the 
18th International Conf. on Machine Learning, pages 
282-289, Williamstown, MA, Morgan Kaufmann. 
Lewis D., & Gale W. (1994). A Sequential Algorithm 
for Training Text Classifiers, In: Proceedings of the 
Seventeenth Annual International ACM-SIGIR Con-
ference on Research and Development in Information 
Retrieval. pp. 3-12, Springer-Verlag. 
Liere, R., & Tadepalli, P. (1997). Active learning with 
committees for text categorization, In proceedings of 
the Fourteenth National Conference on Artificial In-
telligence, pp. 591-596 Providence, RI. 
Scheffer T., & Wrobel S. (2001). Active learning of 
partially hidden markov models. In Proceedings of 
the ECML/PKDD Workshop on Instance Selection. 
Song Y., Kim E., Lee G.G., & Yi B-k. (2005). 
POSBIOTM-NER: a trainable biomedical named-
entity recognition system. Bioinformatics, 21 (11): 
2794-2796. 
Thompson C.A., Califf M.E., & Mooney R.J. (1999). 
Active Learning for Natural Language Parsing and 
Information Extraction, In Proceedings of the Six-
teenth International Machine Learning Conference, 
pp.406-414, Bled, Slovenia. 
72
Proceedings of NAACL HLT 2009: Short Papers, pages 169?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Local Tree Alignment-based Soft Pattern Matching Approach for
Information Extraction
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Science and Technology
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Korea
{megaup, stardust, gblee}@postech.ac.kr
Abstract
This paper presents a new soft pattern match-
ing method which aims to improve the recall
with minimized precision loss in information
extraction tasks. Our approach is based on a
local tree alignment algorithm, and an effec-
tive strategy for controlling flexibility of the
pattern matching will be presented. The ex-
perimental results show that the method can
significantly improve the information extrac-
tion performance.
1 Introduction
The goal of information extraction (IE) is to ex-
tract structured information from unstructured natu-
ral language documents. Pattern induction to gener-
ate extraction patterns from a number of training in-
stances is one of the most widely applied approaches
for IE.
A number of pattern induction approaches have
recently been researched based on the dependency
analysis (Yangarber, 2003) (Sudo et al, 2001)
(Greenwood and Stevenson, 2006) (Sudo et al,
2003). The natural language texts in training in-
stances are parsed by dependency analyzer and con-
verted into dependency trees. Each subtree of a de-
pendency tree is considered as a candidate of ex-
traction patterns. An extraction pattern is gener-
ated by selecting the subtree which indicates the de-
pendency relationships of each labeled slot value
in the training instance and agrees on the selec-
tion criteria defined by each pattern representation
model. A number of dependency tree-based pat-
tern representation models have been proposed. The
predicate-argument (SVO) model allows subtrees
containing only a verb and its direct subject and
object as extraction pattern candidates (Yangarber,
2003). The chain model represents extraction pat-
terns as a chain-shaped path from each target slot
value to the root node of the dependency tree (Sudo
et al, 2001). A couple of chain model patterns shar-
ing the same verb are linked to each other and con-
struct a linked-chain model pattern (Greenwood and
Stevenson, 2006). The subtree model considers all
subtrees as pattern candidates (Sudo et al, 2003).
Regardless of the applied pattern representation
model, the methods have concentrated on extracting
only exactly equivalent subtrees of test instances to
the extraction patterns, which we call hard pattern
matching. While the hard pattern matching policy
is helpful to improve the precision of the extracted
results, it can cause the low recall problem. In or-
der to tackle this problem, a number of soft pattern
matching approaches which aim to improve recall
with minimized precision loss have been applied to
the linear vector pattern models by introducing a
probabilistic model (Xiao et al, 2004) or a sequence
alignment algorithm (Kim et al, 2008).
In this paper, we propose an alternative soft
pattern matching method for IE based on a local
tree alignment algorithm. While other soft pattern
matching approaches have been able to handle the
matching among linear vector instances with fea-
tures from tree structures only, our method aims to
directly solve the low recall problem of tree-to-tree
pattern matching by introducing the local tree align-
ment algorithm which is widely used in bioinformat-
ics to analyze RNA secondary structures. Moreover,
169
(a) Example pattern
(b) Dependency Tree of the example sentence
(c) Local alignment-based tree pattern matching
Figure 1: An example of local alignment-based tree pat-
tern matching
we present an effective policy for controlling degree
of flexibility in the pattern matching by setting the
optimal threshold values for each extracted pattern.
2 Methods
The low recall problem of information extraction
based on hard pattern matching is caused by lack
of flexibility in pattern matching. For example, the
tree pattern in Figure 1(a) cannot be matched with
the tree in Figure 1(b) by considering only exactly
equivalent subtrees, because the first tree has an ad-
ditional root node ?said? which is not in the second
one. However, the matching between two trees can
be performed by omitting just a node as shown in
Figure 1(c).
In order to improve and control the degree of flex-
ibility in tree pattern matching, we have adopted a
local tree alignment approach as the pattern match-
ing method instead of hard pattern matching strat-
egy. The local tree alignment problem is to find the
most similar subtree between two trees.
We have adopted the Hochsmann algorithm
(Hochsmann et al, 2003) which is a local tree align-
ment algorithm used in bioinformatics to analyze
RNA secondary structures. The goal of the Hochs-
mann algorithm is to find the local closed forest
alignment which maximizes the similarity score for
ordered trees. The algorithm can be implemented
by a dynamic programming approach which solves a
problem based on the previous results of its subprob-
lems. The main problem of Hochsmann algorithm
is to compute the similarity score between two sub-
forests according to the defined order from the sin-
gle node level to the entire tree level. The similarity
score is defined based on three tree edit operations
which are insertion, deletion, and replacement (Tai,
1979). For each pair of subforests, the maximum
similarity score among three edit operations is com-
puted, and the kind and the position of performed
edit operations are recorded.
The adaptation of Hochsmann algorithm to the IE
problem is performed by redefining the ?-function,
the similarity score function between two nodes, as
follows:
?(v,w) =
?
?????
?????
1 if lnk(v)=lnk(w),
and lbl(v)=lbl(w),
?(p(w), p(v)) if lbl(v)=<SLOT>,
0 otherwise.
where v and w are nodes to be compared, lnk(v) is
the link label of v, lbl(v) is the node label of v, and
p(v) denotes a parent node of v. While general local
tree alignment problems consider only node labels
to compute the node-level similarities, our method
considers not only node labels, but also link labels to
the head node, because the class of link to the head
node is important as the node label itself for depen-
dency trees. Moreover, the method should consider
the alignment of slot value nodes in the tree patterns
for adopting information extraction tasks. If the pat-
tern node v is a kind of slot value nodes, the similar-
ity score between v and w is inherited from parents
of both nodes.
After computing for all pairs of subforests, the
optimal alignment is obtained by trace-back based
on the recorded information of edit operation which
maximizes the similarity score for each subforest
pair. On the optimal alignment, the target node
aligned to a slot value node on the pattern is regarded
as an argument candidate of the extraction. Each ex-
170
traction candidate has its confidence score which is
computed from the alignment score, defined as:
score(TPTN, TTGT) = S(TPTN, TTGT)|TPTN|
where |T | denotes the total number of nodes in tree
T and S(T1, T2) is the similarity score of both trees
computed by Hochsmann algorithm.
Only the extraction candidates with alignment
score larger than the given threshold value, ?, are
accepted and regarded as extraction results. For the
simplest approach, the same threshold value, ?, can
be applied to all the patterns. However, we assumed
that each pattern has its own optimal threshold value
as its own confidence score, which is different from
other patterns? threshold values. The optimal thresh-
old value ?i and the confidence score confi for the
pattern Pi are defined as:
?i = argmax
0.5<??1.0
{evalfscore (Dtrain, Pi, ?)}
confi = max0.5<??1.0 {evalfscore (Dtrain, Pi, ?)}
where evalfscore(D,P, ?) is the evaluation result in
F-score of the extraction for the data set D using the
pattern P with the threshold value ?. For each pat-
tern, the threshold value which maximizes the eval-
uation result in F-score for the training data set and
the maximum evaluation result in F-score are as-
signed as the optimal threshold value and the con-
fidence score for the pattern respectively.
3 Experiment
In order to evaluate the effectiveness of our method,
we performed an experiment for the scenario tem-
plate extraction task on the management succession
domain in MUC-6. The task aims to extract sce-
nario template instances which consist of person-in,
person-out, position, organization slot values from
news articles about management succession events.
We used a modified version of the MUC-6 corpus
including 599 training documents and 100 test doc-
uments described by Soderland (1999). While the
scenario templates on the original MUC-6 corpus
are labeled on each document, this version has sce-
nario templates for each sentence.
All the sentences in both training and test
documents were converted into dependency trees
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
F-
sc
or
e
Proportion of Patterns Used
SOFT(SUBTREE)
SOFT(LINKED)
SOFT(CHAIN)
HARD(LINKED)
HARD(CHAIN)
HARD(SUBTREE)
SOFT/HARD(SVO)
Figure 2: Comparison of soft pattern matching strategy
with the hard pattern matching
by Berkeley Parser1 and LTH Constituent-to-
Dependency Conversion Tool2. From the depen-
dency trees and scenario templates on the training
data, we constructed pattern candidate sets for four
types of pattern representation models which are
SVO, chain, linked-chain, and subtree models. For
each pattern candidate, corresponding confidence
score and optimal threshold value were computed.
The pattern candidates for each pattern represen-
tation model were arranged in descending order of
confidence score. According to the arranged order,
each pattern was matched with test documents and
the extracted results were accumulated. Extracted
templates for test documents are evaluated by com-
paring with the answer templates on the test corpus.
The curves in Figure 2 show the relative perfor-
mance of the pattern matching strategies for each
pattern representation model. The results suggest
that soft pattern matching strategy with optimal
threshold values requires less number of patterns
for the performance saturation than the hard pat-
tern matching strategy for all pattern models except
the SVO model. For the SVO model, the result of
soft pattern matching strategy is equivalent to that
of hard pattern matching strategy. It is because most
of patterns represented in SVO model are relatively
shorter than those represented in other models.
In order to evaluate the flexibility controlling
strategy, we compared the result of optimally de-
termined threshold values with the cases of using
1http://nlp.cs.berkeley.edu/pages/Parsing.html
2http://nlp.cs.lth.se/pennconverter/
171
? SVO Chain Linked-Chain SubtreeP R F P R F P R F P R F
0.7 32.1 18.0 23.1 27.6 55.0 36.8 26.8 57.0 36.4 26.6 58.0 36.5
0.8 32.1 18.0 23.1 43.8 35.0 38.8 43.4 36.0 39.3 44.7 34.0 38.6
0.9 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
1.0 (hard) 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
optimal 32.1 18.0 23.1 36.0 49.0 41.5 40.7 48.0 44.0 43.0 46.0 44.4
Table 1: Experimental Results
various fixed threshold values. Table 1 represents
the final results for all pattern representation mod-
els and threshold values. For the SVO model, all
the results are equivalent regardless of the thresh-
old strategy because of extremely short length of the
patterns. For the other pattern models, precisions are
increased and recalls are decreased by increasing the
threshold. The maximum performances in F-score
are achieved by our optimal threshold determining
strategy for all pattern representation models. The
experimental results of our method show the better
recall than the cases of hard pattern matching and
controlled precision than the cases of extremely soft
pattern matching.
4 Conclusion
We presented a local tree alignment based soft pat-
tern matching approach for information extraction.
The softness of the pattern matching method is con-
trolled by the threshold value of the alignment score.
The optimal threshold values are determined by self-
evaluation on the training data. Experimental results
indicate that our soft pattern matching approach is
helpful to improve the pattern coverage and our
threshold learning strategy is effective to reduce the
precision loss followed by the soft pattern matching
method.
The goal of local tree alignment algorithm is to
measure the structural similarity between two trees.
It is similar to the kernel functions in the tree kernel
method which is another widely applied approach to
solve the IE problems. In the future, we plan to in-
corporate our alignment-based soft pattern matching
method into the tree kernel method for IE.
Acknowledgments
This work was supported by the Korea Science and
Engineering Foundation(KOSEF) grant funded by
the Korea government(MEST) (No. R01-2008-000-
20651-0)
References
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of Workshop on Informa-
tion Extraction Beyond The Document, pp. 29?35.
Matthias Hochsmann, Thomas Toller, Robert Giegerich,
and Stefan Kurtz. 2003. Local similarity in rna sec-
ondary structures. In Proceedings of the IEEE Com-
puter Society Bioinformatics Conference , pp. 159?68.
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae
Lee. 2008. An alignment-based pattern representa-
tion model for information extraction. In Proceedings
of the ACM SIGIR ?08, pp. 875?876.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1):233?272.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic pattern acquisition for japanese in-
formation extraction. In Proceedings of the first inter-
national conference on Human language technology
research, pp. 1?7.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the ACL ?03, pp. 224?231.
Kuo-Chung Tai. 1979. The tree-to-tree correction prob-
lem. Journal of the ACM (JACM), 26(3):422?433.
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In Pro-
ceedings of COLING ?04, pp. 542?548.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings of the ACL ?03,
pp. 343?350.
172
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 564?571,
Beijing, August 2010
A Cross-lingual Annotation Projection Approach
for Relation Detection
Seokhwan Kim?, Minwoo Jeong?, Jonghoon Lee?, Gary Geunbae Lee?
?Department of Computer Science and Engineering,
Pohang University of Science and Technology
{megaup|jh21983|gblee}@postech.ac.kr
?Saarland University
m.jeong@mmci.uni-saarland.de
Abstract
While extensive studies on relation ex-
traction have been conducted in the last
decade, statistical systems based on su-
pervised learning are still limited because
they require large amounts of training data
to achieve high performance. In this pa-
per, we develop a cross-lingual annota-
tion projection method that leverages par-
allel corpora to bootstrap a relation detec-
tor without significant annotation efforts
for a resource-poor language. In order to
make our method more reliable, we intro-
duce three simple projection noise reduc-
tion methods. The merit of our method is
demonstrated through a novel Korean re-
lation detection task.
1 Introduction
Relation extraction aims to identify semantic re-
lations of entities in a document. Many rela-
tion extraction studies have followed the Rela-
tion Detection and Characterization (RDC) task
organized by the Automatic Content Extraction
project (Doddington et al, 2004) to make multi-
lingual corpora of English, Chinese and Ara-
bic. Although these datasets encourage the de-
velopment and evaluation of statistical relation
extractors for such languages, there would be a
scarcity of labeled training samples when learn-
ing a new system for another language such as
Korean. Since manual annotation of entities and
their relations for such resource-poor languages
is very expensive, we would like to consider in-
stead a weakly-supervised learning technique in
order to learn the relation extractor without sig-
nificant annotation efforts. To do this, we propose
to leverage parallel corpora to project the relation
annotation on the source language (e.g. English)
to the target (e.g. Korean).
While many supervised machine learning ap-
proaches have been successfully applied to the
RDC task (Kambhatla, 2004; Zhou et al, 2005;
Zelenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005; Zhang et al, 2006),
few have focused on weakly-supervised relation
extraction. For example, (Zhang, 2004) and (Chen
et al, 2006) utilized weakly-supervised learning
techniques for relation extraction, but they did
not consider weak supervision in the context of
cross-lingual relation extraction. Our key hypoth-
esis on the use of parallel corpora for learning
the relation extraction system is referred to as
cross-lingual annotation projection. Early stud-
ies of cross-lingual annotation projection were ac-
complished for lexically-based tasks; for exam-
ple part-of-speech tagging (Yarowsky and Ngai,
2001), named-entity tagging (Yarowsky et al,
2001), and verb classification (Merlo et al, 2002).
Recently, there has been increasing interest in ap-
plications of annotation projection such as depen-
dency parsing (Hwa et al, 2005), mention de-
tection (Zitouni and Florian, 2008), and semantic
role labeling (Pado and Lapata, 2009). However,
to the best of our knowledge, no work has reported
on the RDC task.
In this paper, we apply a cross-lingual anno-
tation projection approach to binary relation de-
tection, a task of identifying the relation between
two entities. A simple projection method propa-
gates the relations in source language sentences to
564
word-aligned target sentences, and a target rela-
tion detector can bootstrap from projected annota-
tion. However, this automatic annotation is unre-
liable because of mis-classification of source text
and word alignment errors, so it causes a critical
falling-off in annotation projection quality. To al-
leviate this problem, we present three noise reduc-
tion strategies: a heuristic filtering; an alignment
correction with dictionary; and an instance selec-
tion based on assessment, and combine these to
yield a better result.
We provide a quantitive evaluation of our
method on a new Korean RDC dataset. In our
experiment, we leverage an English-Korean par-
allel corpus collected from the Web, and demon-
strate that the annotation projection approach and
noise reduction method are beneficial to build an
initial Korean relation detection system. For ex-
ample, the combined model of three noise reduc-
tion methods achieves F1-scores of 36.9% (59.8%
precision and 26.7% recall), favorably comparing
with the 30.5% shown by the supervised base-
line.1
The remainder of this paper is structured as fol-
lows. In Section 2, we describe our cross-lingual
annotation projection approach to relation detec-
tion task. Then, we present the noise reduction
methods in Section 3. Our experiment on the pro-
posed Korean RDC evaluation set is shown in Sec-
tion 4 and Section 5, and we conclude this paper
in Section 6.
2 Cross-lingual Annotation Projection
for Relation Detection
The annotation projection from a resource-rich
language L1 to a resource-poor language L2 is
performed by a series of three subtasks: annota-
tion, projection and assessment.
The annotation projection for relation detection
can be performed as follows:
1) For a given pair of bi-sentences in parallel cor-
pora between a resource-rich language L1 and
a target language L2, the relation detection task
is carried out for the sentence in L1.
1The dataset and the parallel corpus are available on the
author?s website,
http://isoft.postech.ac.kr/?megaup/research/resources/.
2) The annotations obtained by analyzing the sen-
tence in L1 are projected onto the sentence in
L2 based on the word alignment information.
3) The projected annotations on the sentence in
L2 are utilized as resources to perform the re-
lation detection task for the language L2.
2.1 Annotation
The first step to projecting annotations from L1
onto L2 is obtaining annotations for the sentences
in L1. Since each instance for relation detection
is composed of a pair of entity mentions, the in-
formation about entity mentions on the given sen-
tences should be identified first. We detect the
entities in the L1 sentences of the parallel cor-
pora. Entity identification generates a number of
instances for relation detection by coupling two
entities within each sentence. For each instance,
the existence of semantic relation between entity
mentions is explored, which is called relation de-
tection. We assume that there exist available mod-
els or systems for all annotation processes, includ-
ing not only an entity tagger and a relation de-
tector themselves, but also required preprocessors
such as a part-of-speech tagger, base-phrase chun-
ker, and syntax parser for analyzing text in L1.
Figure 1 shows an example of annotation pro-
jection for relation detection of a bitext in En-
glish and Korean. The annotation of the sentence
in English shows that ?Jan Mullins? and ?Com-
puter Recycler Incorporated? are entity mentions
of a person and an organization, respectively. Fur-
thermore, the result indicates that the pair of en-
tities has a semantic relationship categorized as
?ROLE.Owner? type.
2.2 Projection
In order to project the annotations from the sen-
tences in L1 onto the sentences in L2, we utilize
the information of word alignment which plays
an important role in statistical machine transla-
tion techniques. The word alignment task aims
to identify translational relationships among the
words in a bitext and produces a bipartite graph
with a set of edges between words with transla-
tional relationships as shown in Figure 1. In the
same manner as the annotation in L1, entities are
565
????????
(keom-pyu-teo-ri-sa-i-keul-reo)
?
(ui)
??
(sa-jang)
?
(eun)
? ??
(ra-go)
???
(mal-haet-da)
Mullins, owner of Incorporated said that ...
?
(jan)
???
(meol-rin-seu)
Jan Computer Recycler
ROLE.Owner
PER ORG
ORG PER
ROLE.Owner
Figure 1: An example of annotation projection for relation detection of a bitext in English and Korean
considered as the first units to be projected. We as-
sume that the words of the sentences in L2 aligned
with a given entity mention in L1 inherit the infor-
mation about the original entity in L1.
After projecting the annotations of entity men-
tions, the projections for relational instances fol-
low. A projection is performed on a projected in-
stance in L2 which is a pair of projected entities
by duplicating annotations of the original instance
in L1.
Figure 1 presents an example of projection of a
positive relational instance between ?Jan Mullins?
and ?Computer Recycler Incorporated? in the
English sentence onto its translational counter-
part sentence in Korean. ?Jan meol-rin-seu? and
?keom-pyu-teo-ri-sa-i-keul-reo? are labeled as en-
tity mentions with types of a person?s name and an
organization?s name respectively. In addition, the
instance composed of the two projected entities is
annotated as a positive instance, because its orig-
inal instance on the English sentence also has a
semantic relationship.
As the description suggests, the annotation pro-
jection approach is highly dependant on the qual-
ity of word alignment. However, the results of au-
tomatic word alignment may include several noisy
or incomplete alignments because of technical dif-
ficulties. We present details to tackle the problem
by relieving the influence of alignment errors in
Section 3.
2.3 Assessment
The most important challenge for annotation pro-
jection approaches is how to improve the robust-
ness against the erroneous projections. The noise
produced by not only word alignment but also
mono-lingual annotations in L1 accumulates and
brings about a drastic decline in the quality of pro-
jected annotations.
The simplest policy of utilizing the projected
annotations for relation detection in L2 is to con-
sider that all projected instances are equivalently
reliable and to employ entire projections as train-
ing instances for the task without any filtering. In
contrast with this policy, which is likely to be sub-
standard, we propose an alternative policy where
the projected instances are assessed and only the
instances judged as reliable by the assessment are
utilized for the task. Details about the assessment
are provided in Section 3.
3 Noise Reduction Strategies
The efforts to reduce noisy projections are consid-
ered indispensable parts of the projection-based
relation detection method in a resource-poor lan-
guage. Our noise reduction approach includes the
following three strategies: heuristic-based align-
ment filtering, dictionary-based alignment correc-
tion, and assessment-based instance selection.
3.1 Heuristic-based Alignment Filtering
In order to improve the performance of annotation
projection approaches, we should break the bottle-
neck caused by the low quality of automatic word
alignment results. As relation detection is carried
out for each instance consisting of two entity men-
tions, the annotation projection for relation detec-
tion concerns projecting only entity mentions and
566
their relational instances. Since this is different
from other shallower tasks such as part-of-speech
tagging, base phrase chunking, and dependency
parsing which should consider projections for all
word units, we define and apply some heuristics
specialized to projections of entity mentions and
relation instances to improve robustness of the
method against erroneous alignments, as follows:
? A projection for an entity mention should
be based on alignments between contiguous
word sequences. If there are one or more
gaps in the word sequence in L2 aligned
with an entity mention in the sentence in
L1, we assume that the corresponding align-
ments are likely to be erroneous. Thus, the
alignments of non-contiguous words are ex-
cluded in projection.
? Both an entity mention in L1 and its projec-
tion in L2 should include at least one base
noun phrase. If no base noun phrase oc-
curs in the original entity mention in L1, it
may suggest some errors in annotation for
the sentence in L1. The same case for the
projected instance raises doubts about align-
ment errors. The alignments between word
sequences without any base noun phrase are
filtered out.
? The projected instance in L2 should sat-
isfy the clausal agreement with the original
instance in L1. If entities of an instance
are located in the same clause (or differ-
ent clauses), its projected instance should be
in the same manner. The instances without
clausal agreement are ruled out.
3.2 Dictionary-based Alignment Correction
The errors in word alignment are composed of
not only imprecise alignments but also incomplete
alignments. If an alignment of an entity among
two entities of a relation instance is not provided
in the result of the word alignment task, the pro-
jection for the corresponding instance is unavail-
able. Unfortunately, the above-stated alignment
filtering heuristics for improving the quality of
projections make the annotation loss problems
worse by filtering out several alignments likely to
be noisy.
In order to solve this problem, a dictionary-
based alignment correction strategy is incorpo-
rated in our method. The strategy requires a bilin-
gual dictionary for entity mentions. Each entry of
the dictionary is a pair of entity mention in L1 and
its translation or transliteration in L2. For each
entity to be projected from the sentence in L1,
its counterpart in L2 is retrieved from the bilin-
gual dictionary. Then, we seek the retrieved entity
mention from the sentence in L2 by finding the
longest common subsequence. If a subsequence
matched to the retrieved mention is found in the
sentence in L2, we make a new alignment between
it and its original entity on the L1 sentence.
3.3 Assessment-based Instance Selection
The reliabilities of instances projected via a series
of independent modules are different from each
other. Thus, we propose an assessment strategy
for each projected instance. To evaluate the reli-
ability of a projected instance in L2, we use the
confidence score of monolingual relation detec-
tion for the original counterpart instance in L1.
The acceptance of a projected instance is deter-
mined by whether the score of the instance is
larger than a given threshold value ?. Only ac-
cepted instances are considered as the results of
annotation projection and applied to solve the re-
lation detection task in target language L2.
4 Experimental Setup
To demonstrate the effectiveness of our cross-
lingual annotation projection approach for rela-
tion detection, we performed an experiment on
relation detection in Korean text with propagated
annotations from English resources.
4.1 Annotation
The first step to evaluate our method was annotat-
ing the English sentences in a given parallel cor-
pus. We use an English-Korean parallel corpus
crawled from an English-Korean dictionary on the
web. The parallel corpus consists of 454,315 bi-
sentence pairs in English and Korean 2. The En-
glish sentences in the parallel corpus were prepro-
2The parallel corpus collected and other resources are all
available in our website
http://isoft.postech.ac.kr/?megaup/research/resources/
567
cessed by the Stanford Parser 3 (Klein and Man-
ning, 2003) which provides a set of analyzed re-
sults including part-of-speech tag sequences, a de-
pendency tree, and a constituent parse tree for a
sentence.
The annotation for English sentences is di-
vided into two subtasks: entity mention recogni-
tion and relation detection. We utilized an off-
the-shelf system, Stanford Named Entity Recog-
nizer 4 (Finkel et al, 2005) for detecting entity
mentions on the English sentences. The total
number of English entities detected was 285,566.
Each pair of recognized entities within a sentence
was considered as an instance for relation detec-
tion.
A classification model learned with the train-
ing set of the ACE 2003 corpus which con-
sists of 674 documents and 9,683 relation in-
stances was built for relation detection in English.
In our implementation, we built a tree kernel-
based SVM model using SVM-Light 5 (Joachims,
1998) and Tree Kernel Tools 6 (Moschitti, 2006).
The subtree kernel method (Moschitti, 2006) for
shortest path enclosed subtrees (Zhang et al,
2006) was adopted in our model. Our rela-
tion detection model achieved 81.2/69.8/75.1 in
Precision/Recall/F-measure on the test set of the
ACE 2003 corpus, which consists of 97 docu-
ments and 1,386 relation instances.
The annotation of relations was performed by
determining the existence of semantic relations
for all 115,452 instances with the trained model
for relation detection. The annotation detected
22,162 instances as positive which have semantic
relations.
4.2 Projection
The labels about entities and relations in the En-
glish sentences of the parallel corpora were propa-
gated into the corresponding sentences in Korean.
The Korean sentences were preprocessed by our
part-of-speech tagger 7 (Lee et al, 2002) and a de-
pendency parser implemented by MSTParser with
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/CRF-NER.shtml
5http://svmlight.joachims.org/
6http://disi.unitn.it/?moschitt/Tree-Kernel.htm
7http://isoft.postech.ac.kr/?megaup/research/postag/
Filter Without assessing With assessing
none 97,239 39,203
+ heuristics 31,652 12,775
+ dictionary 39,891 17,381
Table 1: Numbers of projected instances
a model trained on the Sejong corpus (Kim, 2006).
The annotation projections were performed on
the bi-sentences of the parallel corpus followed
by descriptions mentioned in Section 2.2. The
bi-sentences were processed by the GIZA++ soft-
ware (Och and Ney, 2003) in the standard con-
figuration in both English-Korean and Korean-
English directions. The bi-direcional alignments
were joined by the grow-diag-final algorithm,
which is widely used in bilingual phrase extrac-
tion (Koehn et al, 2003) for statistical machine
translation. This system achieved 65.1/41.6/50.8
in Precision/Recall/F-measure in our evaluation
of 201 randomly sampled English-Korean bi-
sentences with manually annotated alignments.
The number of projected instances varied with
the applied strategies for reducing noise as shown
in Table 1. Many projected instances were fil-
tered out by heuristics, and only 32.6% of the in-
stances were left. However, several instances were
rescued by dictionary-based alignment correction
and the number of projected instances increased
from 31,652 to 39,891. For all cases of noise re-
duction strategies, we performed the assessment-
based instance selection with a threshold value ?
of 0.7, which was determined empirically through
the grid search method. About 40% of the pro-
jected instances were accepted by instance selec-
tion.
4.3 Evaluation
In order to evaluate our proposed method, we pre-
pared a dataset for the Korean RDC task. The
dataset was built by annotating the information
about entities and relations in 100 news docu-
ments in Korean. The annotations were performed
by two annotators following the guidelines for the
ACE corpus processed by LDC. Our Korean RDC
corpus consists of 835 sentences, 3,331 entity
mentions, and 8,354 relation instances. The sen-
568
Model w/o assessing with assessingP R F P R F
Baseline 60.5 20.4 30.5 - - -
Non-filtered 22.5 6.5 10.0 29.1 13.2 18.2
Heuristic 51.4 15.5 23.8 56.1 22.9 32.5
Heuristic + Dictionary 55.3 19.4 28.7 59.8 26.7 36.9
Table 2: Experimental Results
tences of the corpus were preprocessed by equiva-
lent systems used for analyzing Korean sentences
for projection. We randomly divided the dataset
into two subsets with the same number of in-
stances for use as a training set to build the base-
line system and for evaluation.
For evaluating our approach, training instance
sets to learn models were prepared for relation
detection in Korean. The instances of the train-
ing set (half of the manually built Korean RDC
corpufs) were used to train the baseline model.
All other sets of instances include these baseline
instances and additional instances propagated by
the annotation projection approach. The train-
ing sets with projected instances are categorized
into three groups by the level of applied strategies
for noise reduction. While the first set included
all projections without any noise reduction strate-
gies, the second included only the instances ac-
cepted by the heuristics. The last set consisted of
the results of a series of heuristic-based filtering
and dictionary-based correction. For each training
set with projected instances, an additional set was
derived by performing assessment-based instance
selection.
We built the relation detection models for all
seven training sets (a baseline set, three pro-
jected sets without assessing, and three pro-
jected sets with assessing). Our implementations
are based on the SVM-Light and Tree Kernel
Tools described in the former subsection. The
shortest path dependency kernel (Bunescu and
Mooney, 2005) implemented by the subtree kernel
method (Moschitti, 2006) was adopted to learn all
models.
The performance for each model was evaluated
with the predictions of the model on the test set,
which was the other half of Korean RDC corpus.
We measured the performances of the models on
true entity mentions with true chaining of coref-
erence. Precision, Recall and F-measure were
adopted for our evaluation.
5 Experimental Results
Table 2 compares the performances of the differ-
ent models which are distinguished by the applied
strategies for noise reduction. It shows that:
? The model with non-filtered projections
achieves extremely poor performance due to
a large number of erroneous instances. This
indicates that the efforts for reducing noise
are urgently needed.
? The heuristic-based alignment filtering helps
to improve the performance. However, it is
much worse than the baseline performance
because of a falling-off in recall.
? The dictionary-based correction to our pro-
jections increased both precision and recall
compared with the former models with pro-
jected instances. Nevertheless, it still fails to
achieve performance improvement over the
baseline model.
? For all models with projection, the
assessment-based instance selection boosts
the performances significantly. This means
that this selection strategy is crucial in
improving the performance of the models
by excluding unreliable instances with low
confidence.
? The model with heuristics and assessments
finally achieves better performance than the
baseline model. This suggests that the pro-
jected instances have a beneficial influence
569
on the relation detection task when at least
these two strategies are adopted for reducing
noises.
? The final model incorporating all proposed
noise reduction strategies outperforms the
baseline model by 6 in F-measure. This is
due to largely increased recall by absorbing
more useful features from the well-refined
set of projected instances.
The experimental results show that our pro-
posed techniques effectively improve the perfor-
mance of relation detection in the resource-poor
Korean language with a set of annotations pro-
jected from the resource-rich English language.
6 Conclusion
This paper presented a novel cross-lingual annota-
tion projection method for relation extraction in a
resource-poor language. We proposed methods of
propagating annotations from a resource-rich lan-
guage to a target language via parallel corpora. In
order to relieve the bad influence of noisy projec-
tions, we focused on the strategies for reducing the
noise generated during the projection. We applied
our methods to the relation detection task in Ko-
rean. Experimental results show that the projected
instances from an English-Korean parallel corpus
help to improve the performance of the task when
our noise reduction strategies are adopted.
We would like to introduce our method to the
other subtask of relation extraction, which is re-
lation categorization. While relation detection is
a binary classification problem, relation catego-
rization can be solved by a classifier for multi-
ple classes. Since the fundamental approaches
of the two tasks are similar, we expect that our
projection-based relation detection methods can
be easily adapted to the relation categorization
task.
For this further work, we are concerned about
the problem of low performance for Korean,
which was below 40 for relation detection. The re-
lation categorization performance is mostly lower
than detection because of the larger number of
classes to be classified, so the performance of
projection-based approaches has to be improved
in order to apply them. An experimental result
of this work shows that the most important factor
in improving the performance is how to select the
reliable instances from a large number of projec-
tions. We plan to develop more elaborate strate-
gies for instance selection to improve the projec-
tion performance for relation extraction.
Acknowledgement
This research was supported by the MKE (The
Ministry of Knowledge Economy), Korea, un-
der the ITRC (Information Technology Research
Center) support program supervised by the NIPA
(National IT Industry Promotion Agency) (NIPA-
2010-C1090-1031-0009).
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, page 724731.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2006. Relation extraction using la-
bel propagation based semi-supervised learning. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 129?136, Sydney, Australia. Associ-
ation for Computational Linguistics.
Culotta, Aron and Jaffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL, volume 4.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) programtasks, data, and evalua-
tion. In Proceedings of LREC, volume 4, page
837840.
Finkel, Jenny R., Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
volume 43, page 363.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
570
Joachims, Thorsten. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of the European Con-
ference on Machine Learning, pages 137?142.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22, Barcelona, Spain. Associa-
tion for Computational Linguistics.
Kim, Hansaem. 2006. Korean national corpus in the
21st century sejong project. In Proceedings of the
13th NIJL International Symposium, page 4954.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan. Association for Computational Lin-
guistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1, pages 48?54.
Lee, Gary Geunbae, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-
of-speech tagging of korean. Computational Lin-
guistics, 28(1):53?70.
Merlo, Paola, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 207?214, Philadelphia,
Pennsylvania. Association for Computational Lin-
guistics.
Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL06.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Pado, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of semantic
roles. Journal of Artificial Intelligence Research,
36(1):307340.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual POS taggers and NP bracketers via ro-
bust projection across aligned corpora. In Second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies 2001, pages 1?8, Pittsburgh, Pennsyl-
vania. Association for Computational Linguistics.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8, San Diego. Association for Computational Lin-
guistics.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832, Sydney, Australia. Associ-
ation for Computational Linguistics.
Zhang, Zhu. 2004. Weakly-supervised relation clas-
sification for information extraction. In Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
581?588, Washington, D.C., USA. ACM.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, page
434.
Zitouni, Imed and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 600?609, Honolulu,
Hawaii. Association for Computational Linguistics.
571
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?53,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Graph-based Cross-lingual Projection Approach for
Weakly Supervised Relation Extraction
Seokhwan Kim
Human Language Technology Dept.
Institute for Infocomm Research
Singapore 138632
kims@i2r.a-star.edu.sg
Gary Geunbae Lee
Dept. of Computer Science and Engineering
Pohang University of Science and Technology
Pohang, 790-784, Korea
gblee@postech.ac.kr
Abstract
Although researchers have conducted exten-
sive studies on relation extraction in the last
decade, supervised approaches are still limited
because they require large amounts of training
data to achieve high performances. To build
a relation extractor without significant anno-
tation effort, we can exploit cross-lingual an-
notation projection, which leverages parallel
corpora as external resources for supervision.
This paper proposes a novel graph-based pro-
jection approach and demonstrates the mer-
its of it by using a Korean relation extrac-
tion system based on projected dataset from
an English-Korean parallel corpus.
1 Introduction
Relation extraction aims to identify semantic rela-
tions of entities in a document. Although many
supervised machine learning approaches have been
successfully applied to relation extraction tasks (Ze-
lenko et al, 2003; Kambhatla, 2004; Bunescu and
Mooney, 2005; Zhang et al, 2006), applications of
these approaches are still limited because they re-
quire a sufficient number of training examples to ob-
tain good extraction results. Several datasets that
provide manual annotations of semantic relation-
ships are available from MUC (Grishman and Sund-
heim, 1996) and ACE (Doddington et al, 2004)
projects, but these datasets contain labeled training
examples in only a few major languages, includ-
ing English, Chinese, and Arabic. Although these
datasets encourage the development of relation ex-
tractors for these major languages, there are few la-
beled training samples for learning new systems in
other languages, such as Korean. Because manual
annotation of semantic relations for such resource-
poor languages is very expensive, we instead con-
sider weakly supervised learning techniques (Riloff
and Jones, 1999; Agichtein and Gravano, 2000;
Zhang, 2004; Chen et al, 2006) to learn the rela-
tion extractors without significant annotation efforts.
But these techniques still face cost problems when
preparing quality seed examples, which plays a cru-
cial role in obtaining good extractions.
Recently, some researchers attempted to use ex-
ternal resources, such as treebank (Banko et al,
2007) and Wikipedia (Wu and Weld, 2010), that
were not specially constructed for relation extraction
instead of using task-specific training or seed exam-
ples. We previously proposed to leverage parallel
corpora as a new kind of external resource for rela-
tion extraction (Kim et al, 2010). To obtain training
examples in the resource-poor target language, this
approach exploited a cross-lingual annotation pro-
jection by propagating annotations that were gener-
ated by a relation extraction system in a resource-
rich source language. In this approach, projected
annotations were determined in a single pass pro-
cess by considering only alignments between entity
candidates; we call this action direct projection.
In this paper, we propose a graph-based projec-
tion approach for weakly supervised relation extrac-
tion. This approach utilizes a graph that is con-
stucted with both instance and context information
and that is operated in an iterative manner. The goal
of our graph-based approach is to improve the ro-
bustness of the extractor with respect to errors that
are generated and accumulated by preprocessors.
48
fE (<Barack Obama, Honolulu>) = 1
fK  ( <  ?? zj  ,   ??F>  > ) = 1
?? zj
(beo-rak-o-ba-ma)
&r
(e-seo)
?
(neun)
??F>
(ho-nol-rul-ru)
???
(ha-wa-i)
2
:.
(tae-eo-nat-da)
?
(ui)
Barack Obama was born in Honolulu Hawaii, .
(beo-rak-o-ba-ma) (ho-nol-rul-ru)
Figure 1: An example of annotation projection for rela-
tion extraction of a bitext in English and Korean
2 Cross-lingual Annotation Projection for
Relation Extraction
Relation extraction can be considered to be a classi-
fication problem by the following classifier:
f
(
ei, ej
)
=
{
1 if ei and ej have a relation,
?1 otherwise. ,
where ei and ej are entities in a sentence.
Cross-lingual annotation projection intends to
learn an extractor ft for good performance with-
out significant effort toward building resources for
a resource-poor target language Lt. To accomplish
that goal, the method automatically creates a set of
annotated text for ft, utilizing a well-made extractor
fs for a resource-rich source language Ls and a par-
allel corpus of Ls and Lt. Figure 1 shows an exam-
ple of annotation projection for relation extraction
with a bi-text in Lt Korean and Ls English. Given an
English sentence, an instance ?Barack Obama, Hon-
olulu? is extracted as positive. Then, its translational
counterpart ?beo-rak-o-ba-ma, ho-nol-rul-ru? in the
Korean sentence also has a positive annotation by
projection.
Early studies in cross-lingual annotation projec-
tion were accomplished for various natural lan-
guage processing tasks (Yarowsky and Ngai, 2001;
Yarowsky et al, 2001; Hwa et al, 2005; Zitouni and
Florian, 2008; Pado and Lapata, 2009). These stud-
ies adopted a simple direct projection strategy that
propagates the annotations in the source language
sentences to word-aligned target sentences, and a
target system can bootstrap from these projected an-
notations.
For relation extraction, the direct projection strat-
egy can be formularized as follows: ft
(
eit, e
j
t
)
=
fs
(
A(eit), A(e
j
t )
)
, where A(et) is the aligned entity
of et. However, these automatic annotations can be
unreliable because of source text mis-classification
and word alignment errors; thus, it can cause a criti-
cal falling-off in the annotation projection quality.
Although some noise reduction strategies for pro-
jecting semantic relations were proposed (Kim et al,
2010), the direct projection approach is still vulner-
able to erroneous inputs generated by submodules.
We note two main causes for this limitation: (1)
the direct projection approach considers only align-
ments between entity candidates, and it does not
consider any contextual information; and, (2) it is
performed by a single pass process. To solve both of
these problems at once, we propose a graph-based
projection approach for relation extraction.
3 Graph Construction
The most crucial factor in the success of graph-
based learning approaches is how to construct a
graph that is appropriate for the target task. Das
and Petrov (Das and Petrov, 2011) proposed a graph-
based bilingual projection of part-of-speech tagging
by considering the tagged words in the source lan-
guage as labeled examples and connecting them to
the unlabeled words in the target language, while re-
ferring to the word alignments. Graph construction
for projecting semantic relationships is more com-
plicated than part-of-speech tagging because the unit
instance of projection is a pair of entities and not a
word or morpheme that is equivalent to the align-
ment unit.
3.1 Graph Vertices
To construct a graph for a relation projection, we
define two types of vertices: instance vertices V and
context vertices U .
Instance vertices are defined for all pairs of en-
tity candidates in the source and target languages.
Each instance vertex has a soft label vector Y =
[ y+ y? ], which contains the probabilities that
the instance is positive or negative, respectively. The
larger the y+ value, the more likely the instance has
a semantic relationship. The initial label values of an
instance vertex vijs ? Vs for the instance
?
eis, e
j
s
?
in
the source language are assigned based on the con-
fidence score of the extractor fs. With respect to the
target language, every instance vertex vijt ? Vt has
49
the same initial values of 0.5 in both y+ and y?.
The other type of vertices, context vertices, are
used for identifying relation descriptors that are con-
textual subtexts that represent semantic relationships
of the positive instances. Because the characteristics
of these descriptive contexts vary depending on the
language, context vertices should be defined to be
language-specific. In the case of English, we define
the context vertex for each trigram that is located be-
tween a given entity pair that is semantically related.
If the context vertices Us for the source language
sentences are defined, then the units of context in
the target language can also be created based on the
word alignments. The aligned counterpart of each
source language context vertex is used for generat-
ing a context vertex uit ? Ut in the target language.
Each context vertex us ? Us and ut ? Ut also has
y+ and y?, which represent how likely the context
is to denote semantic relationships. The probability
values for all of the context vertices in both of the
languages are initially assigned to y+ = y? = 0.5.
3.2 Edge Weights
The graph for our graph-based projection is con-
structed by connecting related vertex pairs by
weighted edges. If a given pair of vertices is likely to
have the same label, then the edge connecting these
vertices should have a large weight value.
We define three types of edges according to com-
binations of connected vertices. The first type of
edges consists of connections between an instance
vertex and a context vertex in the same language.
For a pair of an instance vertex vi,j and a context
vertex uk, these vertices are connected if the context
sequence of vi,j contains uk as a subsequence. If
vij is matched to uk, the edge weight w
(
vi,j , uk)
)
is assigned to 1. Otherwise, it should be 0.
Another edge category is for the pairs of context
vertices in a language. Because each context vertex
is considered to be an n-gram pattern in our work,
the weight value for each edge of this type represents
the pattern similarity between two context vertices.
The edge weight w(uk, ul) is computed by Jaccard?s
coefficient between uk and ul.
While the previous two categories of edges are
concerned with monolingual connections, the other
type addresses bilingual alignments of context ver-
tices between the source language and the target lan-
guage. We define the weight for a bilingual edge
connecting uks and ult as the relative frequency of
alignments, as follows:
w(uks , u
l
t) = count
(
uks , u
l
t
)
/
?
umt
count
(
uks , u
m
t
)
,
where count (us, ut) is the number of alignments
between us and ut across the whole parallel corpus.
4 Label Propagation
To induce labels for all of the unlabeled vertices on
the graph constructed in Section 3, we utilize the
label propagation algorithm (Zhu and Ghahramani,
2002), which is a graph-based semi-supervised
learning algorithm.
First, we construct an n ? n matrix T that rep-
resents transition probabilities for all of the vertex
pairs. After assigning all of the values on the ma-
trix, we normalize the matrix for each row, to make
the element values be probabilities. The other input
to the algorithm is an n ? 2 matrix Y , which indi-
cates the probabilities of whether a given vertex vi is
positive or not. The matrix T and Y are initialized
by the values described in Section 3.
For the input matrices T and Y , label propagation
is performed by multiplying the two matrices, to up-
date the Y matrix. This multiplication is repeated
until Y converges or until the number of iterations
exceeds a specific number. The Y matrix, after fin-
ishing its iterations, is considered to be the result of
the algorithm.
5 Implementation
To demonstrate the effectiveness of the graph-based
projection approach for relation extraction, we de-
veloped a Korean relation extraction system that was
trained with projected annotations from English re-
sources. We used an English-Korean parallel cor-
pus 1 that contains 266,892 bi-sentence pairs in En-
glish and Korean. We obtained 155,409 positive in-
stances from the English sentences using an off-the-
shelf relation extraction system, ReVerb 2 (Fader et
al., 2011).
1The parallel corpus collected is available in our website:
http://isoft.postech.ac.kr/?megaup/acl/datasets
2http://reverb.cs.washington.edu/
50
Table 1: Comparison between direct and graph-based
projection approaches to extract semantic relationships
for four relation types
Type Direct Graph-basedP R F P R F
Acquisition 51.6 87.7 64.9 55.3 91.2 68.9
Birthplace 69.8 84.5 76.4 73.8 87.3 80.0
Inventor Of 62.4 85.3 72.1 66.3 89.7 76.3
Won Prize 73.3 80.5 76.7 76.4 82.9 79.5
Total 63.9 84.2 72.7 67.7 87.4 76.3
The English sentence annotations in the parallel
corpus were then propagated into the correspond-
ing Korean sentences. We used the GIZA++ soft-
ware 3 (Och and Ney, 2003) to obtain the word align-
ments for each bi-sentence in the parallel corpus.
The graph-based projection was performed by the
Junto toolkit 4 with the maximum number of itera-
tions of 10 for each execution.
Projected instances were utilized as training ex-
amples to learn the Korean relation extractor. We
built a tree kernel-based support vector machine
model using SVM-Light 5 (Joachims, 1998) and
Tree Kernel tools 6 (Moschitti, 2006). In our model,
we adopted the subtree kernel method for the short-
est path dependency kernel (Bunescu and Mooney,
2005).
6 Evaluation
The experiments were performed on the manu-
ally annotated Korean test dataset. The dataset
was built following the approach of Bunescu and
Mooney (Bunescu and Mooney, 2007). The dataset
consists of 500 sentences for four relation types: Ac-
quisition, Birthplace, Inventor of, and Won Prize. Of
these, 278 sentences were annotated as positive in-
stances.
The first experiment aimed to compare two sys-
tems constructed by the direct projection (Kim et al,
2010) and graph-based projection approach. Table 1
shows the performances of the relation extraction of
the two systems. The graph-based system achieved
better performances in precision and recall than the
3http://code.google.com/p/giza-pp/
4http://code.google.com/p/junto/
5http://svmlight.joachims.org/
6http://disi.unitn.it/ moschitt/Tree-Kernel.htm
Table 2: Comparisons of our projection approach to
heuristic and Wikipedia-based approaches
Approach P R F
Heuristic-based 92.31 17.27 29.09
Wikipedia-based 66.67 66.91 66.79
Projection-based 67.69 87.41 76.30
system with direct projection for all of the four re-
lation types. It outperformed the baseline system by
an F-measure of 3.63.
To demonstrate the merits of our work against
other approaches based on monolingual external re-
sources, we performed comparisons with the fol-
lowing two baselines: heuristic-based (Banko et
al., 2007) and Wikipedia-based approaches (Wu and
Weld, 2010). The heuristic-based baseline was built
on the Sejong treebank corpus (Kim, 2006) and the
Wikipedia-based baseline used Korean Wikipedia
articles 7. Table 2 compares the performances of the
two baseline systems and our method. Our proposed
projection-based approach obtained better perfor-
mance than the other systems. It outperformed the
heuristic-based system by 47.21 and the Wikipedia-
based system by 9.51 in the F-measure.
7 Conclusions
This paper presented a novel graph-based projection
approach for relation extraction. Our approach per-
formed a label propagation algorithm on a proposed
graph that represented the instance and context fea-
tures of both the source and target languages. The
feasibility of our approach was demonstrated by our
Korean relation extraction system. Experimental re-
sults show that our graph-based projection helped to
improve the performance of the cross-lingual anno-
tation projection of the semantic relations, and our
system outperforms the other systems, which incor-
porate monolingual external resources.
In this work, we operated the graph-based pro-
jection under very restricted conditions, because of
high complexity of the algorithm. For future work,
we plan to relieve the complexity problem for deal-
ing with more expanded graph structure to improve
the performance of our proposed approach.
7We used the Korean Wikipedia database dump as of June
2011.
51
Acknowledgments
This research was supported by the MKE(The
Ministry of Knowledge Economy), Korea, un-
der the ITRC(Information Technology Research
Center) support program (NIPA-2012-(H0301-12-
3001)) supervised by the NIPA(National IT Industry
Promotion Agency) and Industrial Strategic technol-
ogy development program, 10035252, development
of dialog-based spontaneous speech interface tech-
nology on mobile platform, funded by the Ministry
of Knowledge Economy(MKE, Korea).
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Proceedings of the fifth ACM conference on Digital li-
braries, pages 85?94.
M. Banko, M. J Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extrac-
tion from the web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
pages 2670?2676.
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 724?731.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Proceedings of the 45th annual meeting of the Associ-
ation for Computational Linguistics, volume 45, pages
576?583.
J. Chen, D. Ji, C. L Tan, and Z. Niu. 2006. Relation ex-
traction using label propagation based semi-supervised
learning. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 129?136.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 600?609.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proceedings of LREC, volume 4,
pages 837?840.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1535?1545.
R. Grishman and B. Sundheim. 1996. Message under-
standing conference-6: A brief history. In Proceedings
of the 16th conference on Computational linguistics,
volume 1, pages 466?471.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Natural language engineer-
ing, 11(3):311?325.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
N. Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, pages 22?25.
S. Kim, M. Jeong, J. Lee, and G. G Lee. 2010. A cross-
lingual annotation projection approach for relation de-
tection. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 564?571.
H. Kim. 2006. Korean national corpus in the 21st cen-
tury sejong project. In Proceedings of the 13th NIJL
International Symposium, pages 49?54.
A. Moschitti. 2006. Making tree kernels practical for
natural language learning. In Proceedings of the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, volume 6, pages
113?120.
F. J Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
S. Pado and M. Lapata. 2009. Cross-lingual annotation
projection of semantic roles. Journal of Artificial In-
telligence Research, 36(1):307?340.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the National Conference on Artifi-
cial Intelligence, pages 474?479.
F. Wu and D. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 1?8.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of the
52
First International Conference on Human Language
Technology Research, pages 1?8.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. The Journal of Ma-
chine Learning Research, 3:1083?1106.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities with
both flat and structured features. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 825?832.
Z. Zhang. 2004. Weakly-supervised relation classifica-
tion for information extraction. In Proceedings of the
thirteenth ACM international conference on Informa-
tion and knowledge management, pages 581?588.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. School
Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA,
Tech. Rep. CMU-CALD-02-107.
I. Zitouni and R. Florian. 2008. Mention detection cross-
ing the language barrier. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 600?609.
53
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328?332,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Meta Learning Approach to Grammatical Error Correction 
 Hongsuck Seo1, Jonghoon Lee1, Seokhwan Kim2, Kyusong Lee1 Sechun Kang1, Gary Geunbae Lee1 1Pohang University of Science and Technology 2Institute for Infocomm Research {hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg {kyusonglee, freshboy, gblee}@postech.ac.kr     Abstract We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1. Introduction As language learning has drawn significant attention in the community, grammatical error correction (GEC), consequently, has attracted a fair amount of attention. Several organizations have built diverse resources including grammatical error (GE) tagged corpora. Although there are some publicly released GE tagged corpora, it is still challenging to train a good GEC model due to the lack of large GE tagged learner corpus. The available GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers 
focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al, 2000; Lee, 2004; Nagata et al, 2006; Han et al, 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al, 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allowing for quick analysis of the method. Our approach is distinguished from others by integrating the predictive models trained on several GE tagged learner corpora, rather than just one GE tagged corpus. Moreover, the framework is compatible to any classification technique. In this study, we also use a native corpus employing Dahlmeier and Ng?s approach. We demonstrate the effectiveness of the proposed method against baseline models in article error correction tasks. 
328
The remainder of this paper is organized as follows: Section 2 explains our proposed method. The experiments are presented in Section 3. Finally, Section 4 concludes the paper. 2. Method Our method predicts the type of article for a noun phrase within three classes: null, definite, and indefinite. A correction arises when the prediction disagrees with the observed article. The meta-learning technique is applied to this task to deal with multiple corpora obtained from different sources. A meta-classifier decides the final output based on the intermediate results obtained from several base classifiers. Each base classifier is trained on a different corpus than are the other classifiers. In this work, the feature extraction processes used for the base classifiers are identical to each other for simplicity, although they need not necessarily be identical. The meta-classifier takes the output scores of the base classifiers as its input and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the 
input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al, 2007; Zhang, 2007; Ayd?n, 2009; Menahem et al, 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the maximum score. A common design of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enha-
Figure 1: Overview of the proposed method 
329
nces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning.  We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a native corpus and two GE tagged corpora. For the native corpus, we used                                                             1 http://nlp.stanford.edu/software/corenlp.shtml 
news data2 which is a large English text extracted from news articles. The First Certificate in English exams in the Cambridge Learner Corpus 3 (hereafter, CLC-FCE; Yannakoudakis et al, 2011) and the Japanese Learner English corpus (Izumi et. al., 2005) were used for the GE tagged corpora. We extracted noun phrases from each corpus by parsing the text of the respective corpora. (1) We parsed the native corpus from the beginning until approximately a million noun phrases are extracted. (2) About 90k noun phrases containing ~3,300 mistakes in article usage were extracted from the entire CLC-FCE corpus, and (3) about 30k noun phrases containing ~2,500 mistakes were extracted from the JLE corpus.  The extracted noun phrases were used for our training and test data. We hold out 10% of the data for the test. We applied 20% under-sampling to the training instances that do not have any errors to alleviate data imbalance in the training set. We emphasize the fact that the two learner corpora differ from each other in three aspects. The first aspect is the styles of the texts: the CLC is literary whereas the JLE is colloquial. The second is the error rate: about 3.5% for CLC-FCE and   8.5% for JLE. Finally, the third is the distribution of L1 languages of the learners: the learners of the CLC corpus have various L1 backgrounds whereas the learners of the JLE consist of only Japanese. These experiments demonstrate the effectiveness of the proposed method relying on the diversity of the corpora. The native corpus was used to find the common structure using structural learning and two GE tagged learner corpora are used to train the base classifiers by structural learning with the common structure obtained from the news corpus. We trained three classifiers for comparison; (1) the classifier (INTEG) trained with the integrated training set of the two GE tagged corpora, and two base classifiers used for the ensemble: (2) the base classifier (CB) trained only with the CLC-FCE and (3) the other base classifier (JB) trained with the JLE. 3.2. Results The accuracy obtained from the word selection task with the news corpus was 76.10%. Upon                                                             2 http://www.statmt.org/wmt09/translation-task.html 3 http://www.ilexir.com/ 
330
obtaining the parameters of the word selection task, the structural parameter ?  was calculated by singular value decomposition and was used for the structural learning of the main GEC task. We used three different test data sets: the CLC-FCE, the JLE and an integrated test set of the two. The accuracy (Acc.) and the precision (Prec.) of the INTEG was poorer than CB on the CLC-FCE test set (Table 1), whereas INTEG outperformed JB on the JLE test (Table 2).  Some instances extracted from the CLC-FCE corpus have similar characteristics to the instances from the JLE corpus. This overlap of instances affected the performance in both positive and negative ways. Prediction of instances similar to those in the JLE was enhanced. Consequently, INTEG model demonstrated better accuracy and precision for the JLE test set. Unfortunately, for the CLC test set, the instances resulted in lower accuracy and precision. The proposed model is able to alleviate this model bias due to similar instances observed in the INTEG model. The accuracy of the proposed model consistently increased by over 10% for all three data sets. The relative performance gain in terms of F1-score (F1) was 15% on the integrated set. This performance gain stems from the over   25% relative improvement of the precision (Table 1, 2 and 3). We believe the improvement comes from the contribution of reconfirming procedures performed 
by the meta-classifier. When the prediction of the two base classifiers conflicts with each other, the meta-classifier tends to choose the one with a higher confidence score; this choice improves the accuracy and precision because known features generate a higher confidence whereas unseen or less-weighted features generate a lower score. Although the proposed model introduced a tradeoff between precision and recall (Rec.), this tradeoff was tolerable in order to improve the overall F1-score. Since GEC is a task where false alarm is critical, obtaining high precision is very important. The low precision on the whole experiments is due to the data imbalance. Instances in the dataset are mostly not erroneous, e.g., only 3.5% of erroneous instances for the CLC corpus. The standard for correct prediction is also very strict and does not allow multiple answers. Performance can be evaluated in a more realistic way by applying a softer standard, e.g., by evaluating manually. 4. Conclusion We have presented a novel approach to grammatical error correction by building a meta-classifier using multiple GE tagged corpora with different characteristics in various aspects. The experiments showed that building a meta-classifier overcomes the interference that occurs when training with a set of heterogeneous corpora. The proposed method also outperforms the base classifier themselves tested on the same class of test set as the training set with which the base classifiers are trained. A better automatic evaluation metric would be needed as further research. Acknowledgments Industrial Strategic technology development program, 10035252, development of dialog-based spontaneous speech interface technology on mobile platform, funded by the Ministry of Knowledge Economy (MKE, Korea).   
Model Acc. Prec. Rec. F1 INTEG 73.37 4.69 72.39 8.82 CB 77.20 5.39 71.17 10.03 Proposed 86.99 6.17 45.77 10.88 Table 1: Best results for GEC task on CLC-FCE test set.  Model Acc. Prec. Rec. F1 INTEG 78.87 14.88 85.47 25.35 JB 78.02 14.49 86.32 24.82 Proposed 89.61 19.28 46.60 27.27 Table 2: Best results for GEC task on JLE test set. Model Acc. Prec. Rec. F1 INTEG 74.64 6.84 77.86 12.58 Proposed 87.50 8.61 46.12 14.52 Table 3: Best results for GEC task on the integrated set of CLC-FCE and JLE test sets.  
331
References  R.K. Ando and T. Zhang. 2005. A framework for learn- ing predictive structures from multiple tasks and un- labeled data. Journal of Machine Learning Research, 6, pp. 1817-1853. U. Ayd?n, S. Murat, Olcay T Y?ld?z, A. Ethem, 2009, Incremental construction of classifier and discriminant ensembles, Information Science, 179 (9), pp. 144-152. L. Breiman, 1996, Bagging predictors, Machine Learning, pp. 123?140. S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree instance space decomposition with grouped gain-ratio, Information Science, 177 (17), pp. 3592?3612. D. Dahlmeier, H. T. Ng, 2011, Grammatical error correction with alternating structure optimization, In Proceedings of the 49th Annual Meeting of the ACL-HLT 2011, pp. 915-923. R. De Felice. 2008. Automatic Error Detection in Non- native English. Ph.D. thesis, University of Oxford. S. Dzeroski, B. Zenko, 2004, Is combining classifiers with stacking better than selecting the best one?, Machine Learning, 54 (3), pp. 255?273. J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the ACL, pp. 363-370. N.R. Han, M. Chodorow, and C. Leacock. 2006. De- tecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02), pp. 115-129. N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC. D. Klein and C.D. Manning. 2003a. Accurate unlexical- ized parsing. In Proceedings of ACL, pp. 423-430. D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15, pp. 3-10. K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI, pp. 779-784. J. Lee. 2004. Automatic article restoration. In Proceed- ings of HLT-NAACL, pp. 31-36. R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in 
the writing of learners of English. In Proceedings of COLING-ACL, pp. 241--248. A. Mariko, 2007, Grammatical errors across proficiency levels in L2 spoken and written English, The Economic Journal of Takasaki City University of Economics, 49 (3, 4), pp. 117-129. E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An imporoved stacking schema for classification tasks, Information Science, 179 (24), pp. 4097-4122. G. Minnen, F. Bond, and A. Copestake. 2000. Memory- based learning for article generation. In Proceedings of CoNLL, pp. 43-48. E. Izumi, K. Uchimoto, H. Isahara, 2005, Error annotation for corpus of Japanese learner English, In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora, pp. 71-80. A. Rozovskaya and D. Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Pro- ceedings of HLT-NAACL, pp. 154-162. K. Toutanova and C. D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000, pp. 63-70. H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A new dataset and method for automatically grading ESOL texts, In Proceedings of ACL, pp. 180-189. G. P. Zhang, 2007, A neural network ensemble method with jittered training data for time series forecasting, Information Sciences: An International Journal, 177 (23), pp. 5329?5346. 
332
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 19?23,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Composite Kernel Approach for Dialog Topic Tracking with
Structured Domain Knowledge from Wikipedia
Seokhwan Kim, Rafael E. Banchs, Haizhou Li
Human Language Technology Department
Institute for Infocomm Research
Singapore 138632
{kims,rembanchs,hli}@i2r.a-star.edu.sg
Abstract
Dialog topic tracking aims at analyzing
and maintaining topic transitions in on-
going dialogs. This paper proposes a com-
posite kernel approach for dialog topic
tracking to utilize various types of do-
main knowledge obtained fromWikipedia.
Two kernels are defined based on history
sequences and context trees constructed
based on the extracted features. The ex-
perimental results show that our compos-
ite kernel approach can significantly im-
prove the performances of topic tracking
in mixed-initiative human-human dialogs.
1 Introduction
Human communications in real world situations
interlace multiple topics which are related to each
other in conversational contexts. This fact sug-
gests that a dialog system should be also capable
of conducting multi-topic conversations with users
to provide them a more natural interaction with the
system. However, the majority of previous work
on dialog interfaces has focused on dealing with
only a single target task. Although some multi-
task dialog systems have been proposed (Lin et al,
1999; Ikeda et al, 2008; Celikyilmaz et al, 2011),
they have aimed at just choosing the most proba-
ble one for each input from the sub-systems, each
of which is independently operated from others.
To analyze and maintain dialog topics from a
more systematic perspective in a given dialog flow,
some researchers (Nakata et al, 2002; Lagus and
Kuusisto, 2002; Adams and Martell, 2008) have
considered this dialog topic identification as a sep-
arate sub-problem of dialog management and at-
tempted to solve it with text categorization ap-
proaches for the recognized utterances in a given
turn. The major obstacle to the success of these
approaches results from the differences between
written texts and spoken utterances. In most text
categorization tasks, the proper category for each
textual unit can be assigned based only on its own
content. However, the dialog topic at each turn
can be determined not only by the user?s inten-
tions captured from the given utterances, but also
by the system?s decisions for dialog management
purposes. Thus, the text categorization approaches
can only be effective for the user-initiative cases
when users tend to mention the topic-related ex-
pressions explicitly in their utterances.
The other direction of dialog topic tracking ap-
proaches made use of external knowledge sources
including domain models (Roy and Subramaniam,
2006), heuristics (Young et al, 2007), and agen-
das (Bohus and Rudnicky, 2003; Lee et al, 2008).
These knowledge-based methods have an advan-
tage of dealing with system-initiative dialogs, be-
cause dialog flows can be controlled by the sys-
tem based on given resources. However, this as-
pect can limit the flexibility to handle the user?s
responses which are contradictory to the system?s
suggestions. Moreover, these approaches face cost
problems for building a sufficient amount of re-
sources to cover broad states of complex dialogs,
because these resources should be manually pre-
pared by human experts for each specific domain.
In this paper, we propose a composite kernel
to explore various types of information obtained
from Wikipedia for mixed-initiative dialog topic
tracking without significant costs for building re-
sources. Composite kernels have been success-
fully applied to improve the performances in other
NLP problems (Zhao and Grishman, 2005; Zhang
et al, 2006) by integrating multiple individual ker-
nels, which aim to overcome the errors occurring
at one level by information from other levels. Our
composite kernel consists of a history sequence
and a domain context tree kernels, both of which
are composed based on similar textual units in
Wikipedia articles to a given dialog context.
19
t Speaker Utterance Topic Transition
0 Guide How can I help you? NONE?NONE
1
Tourist Can you recommend some good places to visit
in Singapore?
NONE?ATTR
Guide Well if you like to visit an icon of Singapore,
Merlion park will be a nice place to visit.
2
Tourist Merlion is a symbol for Singapore, right?
ATTR?ATTR
Guide Yes, we use that to symbolise Singapore.
3
Tourist Okay.
ATTR?ATTR
Guide The lion head symbolised the founding of the is-
land and the fish body just symbolised the hum-
ble fishing village.
4
Tourist How can I get there from Orchard Road?
ATTR?TRSP
Guide You can take the north-south line train from Or-
chard Road and stop at Raffles Place station.
5
Tourist Is this walking distance from the station to the
destination?
TRSP?TRSP
Guide Yes, it?ll take only ten minutes on foot.
6
Tourist Alright.
TRSP?FOOD
Guide Well, you can also enjoy some seafoods at the
riverside near the place.
7
Tourist What food do you have any recommendations
to try there?
FOOD?FOOD
Guide If you like spicy foods, you must try chilli crab
which is one of our favourite dishes here in Sin-
gapore.
8 Tourist Great! I?ll try that. FOOD?FOOD
Figure 1: Examples of dialog topic tracking on
Singapore tour guide dialogs
2 Dialog Topic Tracking
Dialog topic tracking can be considered as a clas-
sification problem to detect topic transitions. The
most probable pair of topics at just before and after
each turn is predicted by the following classifier:
f(x
t
) = (y
t?1
, y
t
), where x
t
contains the input
features obtained at a turn t, y
t
? C , and C is a
closed set of topic categories. If a topic transition
occurs at t, y
t
should be different from y
t?1
. Oth-
erwise, both y
t
and y
t?1
have the same value.
Figure 1 shows an example of dialog topic
tracking in a given dialog fragment on Singapore
tour guide domain between a tourist and a guide.
This conversation is divided into three segments,
since f detects three topic transitions at t
1
, t
4
and
t
6
. Then, a topic sequence of ?Attraction?, ?Trans-
portation?, and ?Food? is obtained from the results.
3 Wikipedia-based Composite Kernel for
Dialog Topic Tracking
The classifier f can be built on the training exam-
ples annotated with topic labels using supervised
machine learning techniques. Although some fun-
damental features extracted from the utterances
mentioned at a given turn or in a certain number of
previous turns can be used for training the model,
this information obtained solely from an ongoing
dialog is not sufficient to identify not only user-
initiative, but also system-initiative topic transi-
tions.
To overcome this limitation, we propose to
leverage on Wikipedia as an external knowledge
source that can be obtained without significant
effort toward building resources for topic track-
ing. Recently, some researchers (Wilcock, 2012;
Breuing et al, 2011) have shown the feasibility
of using Wikipedia knowledge to build dialog sys-
tems. While each of these studies mainly focuses
only on a single type of information including cat-
egory relatedness or hyperlink connectedness, this
work aims at incorporating various knowledge ob-
tained from Wikipedia into the model using a com-
posite kernel method.
Our composite kernel consists of two different
kernels: a history sequence kernel and a domain
context tree kernel. Both represent the current di-
alog context at a given turn with a set of relevant
Wikipedia paragraphs which are selected based on
the cosine similarity between the term vectors of
the recently mentioned utterances and each para-
graph in the Wikipedia collection as follows:
sim (x, p
i
) =
?(x) ? ?(p
i
)
|?(x)||?(p
i
)|
,
where x is the input, p
i
is the i-th paragraph in
the Wikipedia collection, ?(p
i
) is the term vector
extracted from p
i
. The term vector for the input x,
?(x), is computed by accumulating the weights in
the previous turns as follows:
?(x) =
(
?
1
, ?
2
, ? ? ? , ?
|W |
)
? R
|W |
,
where ?
i
=
?
h
j=0
(
?
j
? tf idf(w
i
, u
(t?j)
)
)
, u
t
is
the utterance mentioned in a turn t, tf idf(w
i
, u
t
)
is the product of term frequency of a word w
i
in
u
t
and inverse document frequency of w
i
, ? is a
decay factor for giving more importance to more
recent turns, |W | is the size of word dictionary,
and h is the number of previous turns considered
as dialog history features.
After computing this relatedness between the
current dialog context and every paragraph in the
Wikipedia collection, two kernel structures are
constructed using the information obtained from
the highly-ranked paragraphs in the Wikipedia.
3.1 History Sequence Kernel
The first structure to be constructed for our com-
posite kernel is a sequence of the most similar
paragraph IDs of each turn from the beginning of
the session to the current turn. Formally, the se-
quence S at a given turn t is defined as:
S = (s
0
, ? ? ? , s
t
),
where s
j
= argmax
i
(sim (x
j
, p
i
)).
20
Since our hypothesis is that the more similar the
dialog histories of the two inputs are, the more
similar aspects of topic transtions occur for them,
we propose a sub-sequence kernel (Lodhi et al,
2002) to map the data into a new feature space de-
fined based on the similarity of each pair of history
sequences as follows:
K
s
(S
1
, S
2
) =
?
u?A
n
?
i:u=S
1
[i]
?
j:u=S
2
[j]
?
l(i)+l(j)
,
where A is a finite set of paragraph IDs, S is a fi-
nite sequence of paragraph IDs, u is a subsequence
of S, S[j] is the subsequence with the i-th charac-
ters ?i ? j, l(i) is the length of the subsequence,
and ? ? (0, 1) is a decay factor.
3.2 Domain Context Tree Kernel
The other kernel incorporates more various types
of domain knowledge obtained from Wikipedia
into the feature space. In this method, each in-
stance is encoded in a tree structure constructed
following the rules in Figure 2. The root node of
a tree has few children, each of which is a subtree
rooted at each paragraph node in:
P
t
= {p
i
|sim (x
t
, p
i
) > ?},
where ? is a threshold value to select the relevant
paragraphs. Each subtree consists of a set of fea-
tures from a given paragraph in the Wikipedia col-
lection in a hierarchical structure. Figure 3 shows
an example of a constructed tree.
Since this constructed tree structure represents
semantic, discourse, and structural information
extracted from the similar Wikipedia paragraphs
to each given instance, we can explore these more
enriched features to build the topic tracking model
using a subset tree kernel (Collins and Duffy,
2002) which computes the similarity between each
pair of trees in the feature space as follows:
K
t
(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
? (n
1
, n
2
) ,
where N
T
is the set of T ?s nodes, ? (n
1
, n
2
) =
?
i
I
i
(n
i
) ? I
i
(n
2
), and I
i
(n) is a function that is
1 iff the i-th tree fragment occurs with root at node
n and 0 otherwise.
3.3 Kernel Composition
In this work, a composite kernel is defined by com-
bining the individual kernels including history se-
quence and domain context tree kernels, as well as
<TREE>:=(ROOT <PAR>...<PAR>)
<PAR>:=(PAR_ID <PARENTS>
<PREV_PAR><NEXT_PAR><LINKS>)
<PARENTS>:=(?PARENTS? <ART><SEC>)
<ART>:=(ART_ID <ART_NAME><CAT_LIST>)
<ART_NAME>:=(?ART_NAME? ART_NAME)
<CAT_LIST>:=(?CAT? <CAT>...<CAT>)
<CAT>:=(CAT_ID
*
)
<SEC>:=(SEC_ID <SEC_NAME><PARENT_SEC>
<PREV_SEC><NEXT_SEC>)
<SEC_NAME>:=(?SEC_NAME? SEC_NAME)
<PARENT_SEC>:=(?PRN_SEC?, PRN_SEC_ID)
<PREV_SEC>:=(?PREV_SEC?, PREV_SEC_NAME)
<NEXT_SEC>:=(?NEXT_SEC?, NEXT_SEC_NAME)
<PREV_PAR>:=(?PREV_PAR?, PREV_PAR_ID)
<NEXT_PAR>:=(?NEXT_PAR?, NEXT_PAR_ID)
<LINKS>:=(?LINKS? <LINK>...<LINK>)
<LINK>:=(LINK_NAME
*
)
Figure 2: Rules for constructing a domain context
tree from Wikipedia: PAR, ART, SEC, and CAT
are acronyms for paragraph, article, section, and
category, respectively
Figure 3: An example of domain context tree
the linear kernel between the vectors representing
fundamental features extracted from the utterances
themselves and the results of linguistic preproces-
sors. The composition is performed by linear com-
bination as follows:
K(x
1
, x
2
) =? ?K
l
(V
1
, V
2
) + ? ?K
s
(S
1
, S
2
)
+ ? ?K
t
(T
1
, T
2
),
where V
i
, S
i
, and T
i
are the feature vector, his-
tory sequence, and domain context tree of x
i
, re-
spectively, K
l
is the linear kernel computed by in-
ner product of the vectors, ?, ?, and ? are coeffi-
cients for linear combination of three kernels, and
? + ? + ? = 1.
4 Evaluation
To demonstrate the effectiveness of our proposed
kernel method for dialog topic tracking, we per-
formed experiments on the Singapore tour guide
dialogs which consists of 35 dialog sessions col-
lected from real human-human mixed initiative
conversations related to Singapore between guides
21
and tourists. All the recorded dialogs with the total
length of 21 hours were manually transcribed, then
these transcribed dialogs with 19,651 utterances
were manually annotated with the following nine
topic categories: Opening, Closing, Itinerary, Ac-
commodation, Attraction, Food, Transportation,
Shopping, and Other.
Since we aim at developing the system which
acts as a guide communicating with tourist users,
an instance for both training and prediction of
topic transition was created for each turn of
tourists. The annotation of an instance is a pair of
previous and current topics, and the actual number
of labels occurred in the dataset is 65.
For each instance, the term vector was gener-
ated from the utterances in current user turn, previ-
ous system turn, and history turns within the win-
dow sizes h = 10. Then, the history sequence and
tree context structures for our composite kernel
were constructed based on 3,155 articles related
to Singapore collected from Wikipedia database
dump as of February 2013. For the linear ker-
nel baseline, we used the following features: n-
gram words, previous system actions, and current
user acts which were manually annotated. Finally,
8,318 instances were used for training the model.
We trained the SVM models using
SVM
light 1
(Joachims, 1999) with the follow-
ing five different combinations of kernels: K
l
only, K
l
withP as features, K
l
+K
s
,K
l
+K
t
, and
K
l
+K
s
+K
t
. The threshold value ? for selecting
P was 0.5, and the combinations of kernels were
performed with the same ?, ?, or ? coefficient
values for all sub-kernels. All the evaluations
were done in five-fold cross validation to the man-
ual annotations with two different metrics: one
is accuracy of the predicted topic label for every
turn, and the other is precision/recall/F-measure
for each event of topic transition occurred either
in the answer or the predicted result.
Table 1 compares the performances of the five
combinations of kernels. When just the para-
graph IDs were included as additional features,
it failed to improve the performances from the
baseline without any external features. However,
our proposed kernels using history sequences and
domain context trees achieved significant perfor-
mances improvements for both evaluation metrics.
While the history sequence kernel enhanced the
coverage of the model to detect topic transitions,
1
http://svmlight.joachims.org/
Turn-level Transition-level
Accuracy P R F
K
l
62.45 42.77 24.77 31.37
K
l
+ P 62.44 42.76 24.77 31.37
K
l
+ K
s
67.19 39.94 40.59 40.26
K
l
+ K
t
68.54 45.55 35.69 40.02
All 69.98 44.82 39.83 42.18
Table 1: Experimental Results
0
500
1000
1500
2000
2500
3000
K
l
K
l
+ P K
l
+K
s
K
l
+K
t
ALL
N
um
be
r
of
T
ra
ns
it
io
n
E
rr
or
s FP(SYS)
FN(SYS)
FP(USR)
FN(USR)
Figure 4: Error distibutions of topic transitions:
FN and FP denotes false negative and false posi-
tive respectively. USR and SYS in the parentheses
indicate the initiativity of the transitions.
the domain context tree kernel contributed to pro-
duce more precise outputs. Finally, the model
combining all the kernels outperformed the base-
line by 7.53% in turn-level accuracy and 10.81%
in transition-level F-measure.
The error distributions in Figure 4 indicate that
these performance improvements were achieved
by resolving the errors not only on user-initiative
topic transitions, but also on system-initiative
cases, which implies the effectiveness of the struc-
tured knowledge from Wikipedia to track the top-
ics in mixed-initiative dialogs.
5 Conclusions
This paper presented a composite kernel approach
for dialog topic tracking. This approach aimed to
represent various types of domain knowledge ob-
tained from Wikipedia as two structures: history
sequences and domain context trees; then incor-
porate them into the model with kernel methods.
Experimental results show that the proposed ap-
proaches helped to improve the topic tracking per-
formances in mixed-initiative human-human di-
alogs with respect to the baseline model.
22
References
P. H. Adams and C. H. Martell. 2008. Topic detection
and extraction in chat. In Proceedings of the 2008
IEEE International Conference on Semantic Com-
puting, pages 581?588.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Proceedings of
the European Conference on Speech, Communica-
tion and Technology, pages 597?600.
A. Breuing, U. Waltinger, and I. Wachsmuth. 2011.
Harvesting wikipedia knowledge to identify topics
in ongoing natural language dialogs. In Proceedings
of the IEEE/WIC/ACM International Conference on
Web Intelligence and Intelligent Agent Technology
(WI-IAT), pages 445?450.
A. Celikyilmaz, D. Hakkani-Tu?r, and G. Tu?r. 2011.
Approximate inference for domain detection in
spoken language understanding. In Proceedings
of the 12th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 713?716.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270.
S. Ikeda, K. Komatani, T. Ogata, H. G. Okuno, and
H. G. Okuno. 2008. Extensibility verification of ro-
bust domain selection against out-of-grammar utter-
ances in multi-domain spoken dialogue system. In
Proceedings of the 9th INTERSPEECH, pages 487?
490.
T. Joachims. 1999. Making large-scale SVM learn-
ing practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
K. Lagus and J. Kuusisto. 2002. Topic identification
in natural language dialogues using neural networks.
In Proceedings of the 3rd SIGdial workshop on Dis-
course and dialogue, pages 95?102.
C. Lee, S. Jung, and G. G. Lee. 2008. Robust dia-
log management with n-best hypotheses using di-
alog examples and agenda. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 630?637.
B. Lin, H. Wang, and L. Lee. 1999. A distributed
architecture for cooperative spoken dialogue agents
with coherent dialogue state and history. In Pro-
ceedings of the IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU).
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419?444.
T. Nakata, S. Ando, and A. Okumura. 2002. Topic de-
tection based on dialogue history. In Proceedings of
the 19th international conference on Computational
linguistics (COLING), pages 1?7.
S. Roy and L. V. Subramaniam. 2006. Automatic gen-
eration of domain models for call centers from noisy
transcriptions. In Proceedings of COLING/ACL,
pages 737?744.
G. Wilcock. 2012. Wikitalk: a spoken wikipedia-
based open-domain knowledge access system. In
Proceedings of the Workshop on Question Answer-
ing for Complex Domains, page 5770.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye.
2007. The hidden information state approach to di-
alog management. In Proceedings of the Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 149?152.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 419?426.
23
Proceedings of the SIGDIAL 2013 Conference, pages 145?147,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
AIDA: Artificial Intelligent Dialogue Agent 
 
Rafael E. Banchs, Ridong Jiang, Seokhwan Kim, Arthur Niswar, Kheng Hui Yeo 
Natural Language Understanding Lab, Human Language Technology Department  
Institute for Infocomm Research, Singapore 138632 
{rembanchs,rjiang,kims,aniswar,yeokh}@i2r.a-star.edu.sg 
 
 
Abstract 
This demo paper describes our Artificial Intel-
ligent Dialogue Agent (AIDA), a dialogue 
management and orchestration platform under 
development at the Institute for Infocomm Re-
search. Among other features, it integrates dif-
ferent human-computer interaction engines 
across multiple domains and communication 
styles such as command, question answering, 
task-oriented dialogue and chat-oriented dia-
logue. The platform accepts both speech and 
text as input modalities by either direct micro-
phone/keyboard connections or by means of 
mobile device wireless connection. The output 
interface, which is supported by a talking ava-
tar, integrates speech and text along with other 
visual aids. 
1 Introduction 
Some recent efforts towards the development of 
a more comprehensive framework for dialogue 
supported applications include research on multi-
domain or multi-task dialogue agents (Komatani 
et. al 2006, Lee et. al 2009, Nakano et. al 2011, 
Lee et. al 2012). With this direction in mind, our 
Artificial Intelligent Dialogue Agent (AIDA) has 
been created aiming the following two objec-
tives: (1) serving as a demonstrator platform for 
showcasing different dialogue systems and relat-
ed technologies, and (2) providing an experi-
mental framework for conducting research in the 
area of dialogue management and orchestration. 
The main objective of this paper is to present 
and describe the main characteristics of AIDA. 
The rest of the paper is structured as follows. 
First, in section 2, a description of APOLLO, the 
software integration platform supporting AIDA 
is presented. Then, in section 3, the main features 
of AIDA as a dialogue management and orches-
tration platform are described, and a real exam-
ple of human interaction with AIDA is reported. 
Finally, in section 4, our conclusions and future 
work plans are presented.  
2 The APOLLO Integration Platform 
APOLLO (Jiang et al 2012) is a component 
pluggable dialogue framework, which allows for 
the interconnection and control of the different 
components required for the implementation of 
dialogue systems. This framework allows for the 
interoperability of four different classes of com-
ponents: dialogue (ASR, NLU, NLG, TTS, etc.), 
managers (vertical domain-dependent task man-
agers), input/output (speech, text, image and vid-
eo devices), and backend (databases, web crawl-
ers and indexes, rules and inference engines). 
 The different components can be connected to 
APOLLO either by means of specifically created 
plug-ins or by using TCP-IP based socket com-
munications. All component interactions are con-
trolled by using XML scripts. Figure 1 presents a 
general overview of the APOLLO framework. 
  
 
Figure 1: The APOLLO framework 
145
3 Main Features of AIDA  
AIDA (Artificial Intelligent Dialogue Agent) is a 
dialogue management and orchestration plat-
form, which is implemented over the APOLLO 
framework. In AIDA, different communication 
task styles (command, question answering, task-
oriented dialogue and chatting) are hierarchically 
organized according to their atomicity; i.e. more 
atomic (less interruptible) tasks are given prefer-
ence over less atomic (more interruptible) tasks.  
In the case of the chatting engine, as it is the 
least atomic task of all, it is located in the bottom 
of the hierarchy. This engine also behaves as a 
back-off system, which is responsible for taking 
care of all the user interactions that other engines 
fail to resolve properly.    
In AIDA, a dialogue orchestration mechanism 
is used to simultaneously address the problems 
of domain switching and task selection. One of 
the main components of this mechanism is the 
user intention inference module, which makes 
informed decisions for selecting and assigning 
turns across the different individual engines in 
the platform.  
Domain and task selection decisions are made 
based on three different sources of information: 
the current user utterance, which includes stand-
ard semantic and pragmatic features extracted 
from the user utterance; engine information 
states, which takes into account individual in-
formation states from all active engines in the 
platform; and system expectations, which is con-
structed based on the most recent history of user-
system interactions, the task hierarchy previously 
described and the archived profile of the current 
user interacting with the system. 
Our current implementation of AIDA inte-
grates six different dialogue engines: (BC) a basic 
command application, which is responsible for 
serving basic requests such as accessing calendar 
and clock applications, interfacing with search 
engines, displaying maps, etc.; (RA) a reception-
ist application, which consists of a question an-
swering system for providing information about 
the Fusionopolis Complex; (IR) I2R information 
system, which implements as question answering 
system about our institute; (FR) a flight reserva-
tion system, which consists of a frame-based dia-
logue engine that uses statistical natural language 
understanding; (RR) a restaurant recommenda-
tion system, which implements a three-stage 
frame-based dialogue system that uses rule-base 
natural language understanding, and (CH) our 
IRIS chatting agent (Banchs and Li, 2012). 
Regarding input/output modalities, speech and 
text can be used as input channels for user utter-
ances. Direct connections via microphone and 
keyboard are supported, as well as remote con-
nections via mobile devices.  
Additionally, audio and video inputs are used 
to provide AIDA with user identification and 
tracking capabilities. In the first case, speaker 
identification techniques are used to compare the 
voice profile of the current speaker with a set of 
users already known by the system. In the second 
case, face detection and tracking are used in 
combination with sound localization to deter-
mine what the current speaker?s location is when 
dealing with multi-party dialogue scenarios. 
The main output of AIDA is composed of a 
browser interface in which several frames, in-
cluding different visual elements, are presented 
along with a talking avatar. The different visual 
elements include a banner, a text display and a 
general purpose frame for displaying different 
html files, images or URL?s as required.  
For avatar and text-to-speech, AIDA relies on 
the SitePal API (SitePal 2013). This API allows 
for controlling both the avatar speech and gaze. 
Speech functionalities are used for generating a 
spoken form of the system?s output (which is 
also displayed as text in one of the frames of the 
interface) and gaze functionalities are used in 
combination with face tracking to direct the ava-
tar attention to the user who is speaking.    
Figure 2 illustrates the main output interface 
of AIDA. 
 
 
Figure 2: AIDA?s output interface. 
146
Finally, an actual example of a user interaction 
with the AIDA platform is presented in Table 1. 
In the table, the following information is provid-
ed for each of the turns composing the dialogue: 
the user utterance, the corresponding system re-
sponse, the engine that was responsible for gen-
erating the response, and the action taken by the 
system (if any action was taken).  
 
Turn Speaker Utterance/Action 
1 
USER Good morning, how are you? 
AIDA-CH How can I help you? 
2 
USER Can you help me to book a flight? 
AIDA-FR Flight reservation, how can I help you? 
(action) Displays flight reservation main screen 
3 
USER I want to fly from Singapore to Tokyo 
AIDA-FR What day do you want to leave? 
4 
USER Next Monday in the evening 
AIDA-FR What day do you want to return? 
5 
USER The second of April in the morning 
AIDA-FR Here is your itinerary 
(action) Displays flight schedule on the screen 
6 
USER Is there any coffee shop nearby? 
AIDA-RA 
There are three coffee shops in this buil-
ding: Jass Caf?, Starbucks and Ya Kun 
(action) Shows logos of retrieved coffee shops  
7 
USER Do you drink coffee? 
AIDA-CH Coffee? 
8 
USER Yes, coffee 
AIDA-CH No 
9 
USER Why not? 
AIDA-CH Well, for one thing you can?t spell.  
10 
USER Please suggest a good Italian restaurant 
AIDA-RR 
What area of the city do you want me to 
search for? 
(action) Displays restaurant system main screen 
11 
USER Near Sentosa 
AIDA-RR 
Ok, I have found one restaurant for you: 
Alkaff Mansion Ristorante. Do you 
want more information or do you want 
to make a reservation? 
(action) Displays http://www.alkaff.com.sg/  
12 
USER Yes please 
AIDA-RR 
Ok, I will do a reservation for you! And, 
how many guests is this reservation for? 
13 
USER Only two persons 
AIDA-RR 
And, will you be interested in dinner or 
lunch?  
14 
USER Please make it for tonight 
AIDA-RR Your reservation has been made. 
(action) Displays reservation details 
15 
USER Great, thank you 
AIDA-CH You are welcome 
Table 2: Example of a user interaction with AIDA 
4 Conclusions and Future Work 
In this demo paper we have presented AIDA (Ar-
tificial Intelligent Dialogue Agent), the dialogue 
management and orchestration platform currently 
under development at the Institute for Infocomm 
Research in Singapore.  
AIDA integrates different communication task 
styles (command, question answering, dialogue 
and chat) into a single conversational agent. It 
also combines speech recognition and synthesis, 
image processing and avatar technologies to im-
prove the user-system interaction experience. 
Our future plans include the incorporation of 
more vertical applications into the platform, as 
well as the integration of machine translation 
engines to develop multilingual capabilities. 
Demonstration Plan 
During the SIGDIAL demo presentation, the fol-
lowing functionalities will be demonstrated: text 
and speech input; dialogue orchestration among 
receptionist, flight reservation, I2R information 
system, restaurant booking and chatting agent; 
and avatar-supported speech and visual output 
interface. For the case of speech input and ava-
tar-supported output, the use of these technolo-
gies is subject to the availability of internet con-
nection at the location of the demo.  
References  
R. E. Banchs and H. Li. 2012. IRIS: a chat-oriented 
dialogue system based on the vector space model, 
in Demo Session of Association of Computational 
Linguistics, pp. 37?42. 
R. Jiang, Y. K. Tan, D. K. Limbu and H. Li. 2012. 
Component pluggable dialogue framework and its 
application to social robots. In Proc. Int?l Work-
shop on Spoken Language Dialog Systems. 
K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. 
Tsujino, T. Ogata and H. G. Okuno. 2006. Multi-
domain spoken dialogue system with extensibility 
and robustness against speech recognition errors. In 
Proc. SIGdial Workshop on Discourse and Dia-
logue, pp. 9?17.  
C. Lee, S. Jung, S. Kim and G. G. Lee. 2009. Exam-
ple-based dialog modeling for practical multi-
domain dialog system. Speech Communication, 51, 
pp. 466?484. 
I. Lee, S. Kim, K. Kim, D. Lee, J. Choi, S. Ryu and 
G. G. Lee. 2012. A two step approach for efficient 
domain selection in multi-domain dialog systems. 
In Proc. Int?l Workshop on Spoken Dialogue Sys-
tems.  
M. Nakano, S. Sato, K. Komatani, K. Matsutama, K. 
Funakoshi and H. G. Okuno. 2011. A two stage 
domain selection framework for extensible multi-
domain spoken dialogue systems. In Proc. SIGdial 
Workshop on Discourse and Dialogue. 
SitePal API & Programmer Information, accessed on 
June 27th, 2013 http://www.sitepal.com/support/  
147
Proceedings of the SIGDIAL 2014 Conference, pages 332?336,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Sequential Labeling for Tracking Dynamic Dialog States
Seokhwan Kim, Rafael E. Banchs
Human Language Technology Department
Institute for Infocomm Research
Singapore 138632
{kims,rembanchs}@i2r.a-star.edu.sg
Abstract
This paper presents a sequential labeling
approach for tracking the dialog states for
the cases of goal changes in a dialog ses-
sion. The tracking models are trained us-
ing linear-chain conditional random fields
with the features obtained from the results
of SLU. The experimental results show
that our proposed approach can improve
the performances of the sub-tasks of the
second dialog state tracking challenge.
1 Introduction
A dialog manager is one of the key components
of a dialog system, which aims at determining the
system actions to generate appropriate responses
to users. To make the system capable of conduct-
ing a dialog in a more natural and effective man-
ner, the dialog manager should take into account
not only a given user utterance itself, but also
the dialog state which represents various conver-
sational situations obtained from the dialog ses-
sion progress. Dialog state tracking is a sub-task
of dialog management that analyzes and maintains
this dialog state at each moment. The major ob-
stacle to dialog state tracking is that the inputs to
the tracker are likely to be noisy because of the
errors produced by automatic speech recognition
(ASR) and spoken language understanding (SLU)
processes which are required to be performed prior
to the tracking.
Thus, many researchers have focused on im-
proving the robustness of dialog state trackers
against ASR and SLU errors. The simplest ways
to tackle this problem have been based on hand-
crafted rules mainly on the confidence scores ob-
tained from ASR and SLUmodules (Nakano et al.,
1999; Wang and Lemon, 2013). However, these
approaches have the limitation that building the
quality rules manually is expensive and, what is
worse, the confidence scores could be unreliable
and inconsistent in some cases.
The other direction of dialog state tracking ap-
proaches have utilized statistical machine learn-
ing techniques to obtain the distribution over a set
of hypotheses. Although the most widely studied
approaches have been based on generative mod-
els (Williams and Young, 2007; Williams, 2010;
Young et al., 2010; Thomson and Young, 2010;
Gas?ic? and Young, 2011; Raux and Ma, 2011), re-
cently, some researchers have reported that dis-
criminative models (Bohus and Rudnicky, 2006;
Lee, 2013; Zilka et al., 2013) achieved compara-
ble, or even better, performances than generative
models, especially in the tasks of the first dialog
state tracking challenge (DSTC) (Williams et al.,
2013).
This work focuses on the second phase of
DSTC (Henderson et al., 2014). The major dif-
ference of DSTC 2 from the previous challenge is
that user goals can be changed even in a single di-
alog session. This aspect can cause the limitations
of the previous approaches assuming the fixed user
goal for each session. To solve this dynamic state
tracking problem, we propose a sequential label-
ing approach using linear-chain conditional ran-
dom fields (CRFs) (Lafferty et al., 2001). This
approach aims to improve the performances of
the tracker in the case of goal changes by jointly
performing prediction and segmentation of dialog
states.
2 Problem Definition
A dialog state defined in DSTC 2 consists of the
following three components: goals, method, and
requested slots.
2.1 Goals Tracking
Goals represent the constraint values which are
truly intended by a user at each moment. These
values can be represented by using a slot filling
332
Utterance
Goals
Food Area
S
1
Hello, How may I help you?
U
1
I need a Persian restaurant in the south part of
town.
Persian South
S
2
What kind of food would you like?
U
2
Persian. Persian South
S
3
I?m sorry but there is no restaurant serving persian
food
U
3
How about Portuguese food? Portuguese South
S
4
Peking restaurant is a nice place in the south of
town.
U
4
Is that Portuguese? Portuguese South
S
5
Nandos is a nice place in the south of town serving
tasty Portuguese food.
U
5
Alright. Whats the phone number? Portuguese South
S
6
The phone number of nandos is 01223 327908 .
U
6
And the address? Portuguese South
S
7
Sure, nandos is on Cambridge Leisure Park Clifton
Way.
U
7
Thank you good bye.
Figure 1: Examples of user goals tracking on a
dialog in the restaurant information domain
over the following four categories: area, food,
name, and price range. Assuming the possible
value set for each slot is fixed, this task can be
considered to be a problem of finding the distri-
butions over these hypotheses. While the previous
challenge aims at identifying a single fixed goal
for each session, the models for DSTC 2 should
be able to handle goal changes during a session,
as shown in Figure 1.
2.2 Method Tracking
Method tracking is performed by classifying the
way of requesting information by a user into the
following four categories: ?by constraints?, ?by al-
ternatives?, ?by name?, and ?finished?. The prob-
ability distribution over these four hypotheses is
computed for each turn. For example, a meth-
ods sequence {byconstraints, byconstraints, byal-
ternatives, byalternatives, byalternatives, byalter-
natives, finished} can be obtained for the dialog
session in Figure 1.
2.3 Requested Slots Tracking
The other component for dialog state tracking is to
specify the slots requested by a user. The tracker
should output the binary distributions with the
probabilities whether each slot is requested or not.
Since the requestable slots are area, food, name,
pricerange, addr, phone, postcode, and signature,
eight different distributions are obtained at each
turn. In the previous example dialog, ?phone? and
?addr? are requested in the 5th and 6th turns re-
spectively.
(a) Goal chain on the food slot
(b) Method chain
(c) Requested chain on the phone slot
Figure 2: Examples of dialog state tracking as se-
quential labeling with liner-chain CRFs
3 Method
Although some discriminative approaches (Lee,
2013; Zilka et al., 2013; Lee and Eskenazi, 2013;
Ren et al., 2013) have successfully applied to the
dialog state tracking tasks of DSTC 1 by explor-
ing various features, they have limited ability to
perform the DSTC 2 tasks, because the previous
models trained based on the features mostly ex-
tracted under the assumption that the user goal in
a session is unchangeable. To overcome this limi-
tation, we propose a sequential labeling approach
using linear-chain CRFs for dynamic dialog state
tracking.
3.1 Sequential Labeling of Dialog States
The goal of sequential labeling is to produce the
most probable label sequence y = {y
1
, ? ? ? , y
n
}
of a given input sequence x = {x
1
, ? ? ? , x
n
},
where n is the length of the input sequence, x
i
?
X , X is the finite set of the input observation,
y
i
? Y , and Y is the set of output labels. The
input sequence for dialog state tracking at a given
turn t is defined as x
t
= {x
1
, ? ? ? , x
t
}, where x
i
denotes the i-th turn in a given dialog session, then
a tracker should be able to output a set of label se-
quences for every sub-task.
333
For the goals and requested slots tasks, a la-
bel sequence is assigned to each target slot, which
means the number of output sequences for these
sub-tasks are four and eight in total, respectively.
On the other hand, only a single label sequence is
defined for the method tracking task.
Due to discourse coherences in conversation,
the same labels are likely to be located contigu-
ously in a label sequence. To detect the bound-
aries of these label chunks, the BIO tagging
scheme (Ramshaw and Marcus, 1999) is adopted
for all the label sequences, which marks beginning
of a chunk as ?B?, continuing of a chunk as ?I?, and
outside a chunk as ?O?. Figure 2 shows the exam-
ples of label sequences according to this scheme
for the input dialog session in Figure 1.
3.2 Linear Chain CRFs
In this work, all the sequential labeling tasks were
performed by the tracking models trained using
first-order linear-chain CRFs. Linear-chain CRFs
are conditional probability distributions over the
label sequences y conditioned on the input se-
quence x, which are defined as follows:
p (y|x) =
1
Z (x)
n
?
t=1
?(y
t
, y
t?1
,x),
?(y
t
, y
t?1
,x) = ?
1
(y
t
,x) ? ?
2
(y
t
, y
t?1
),
?
1
(y
t
,x) = exp
(
?
k
?
k
f
k
(y
t
,x)
)
,
?
2
(y
t
, y
t?1
) = exp
(
?
k
?
k
f
k
(y
t
, y
t?1
)
)
,
where Z(x) is the normalization function which
makes that the distribution sums to 1, {f
k
} is a set
of feature functions for observation and transition,
and {?
k
} is a set of weight parameters which are
learnt from data.
3.3 Features
To train the tracking models, a set of feature func-
tions were defined based on the n-best list of user
actions obtained from the live SLU results at a
given turn and the system actions corresponding
to the previous system output.
The most fundamental information to capture a
user?s intentions can be obtained from the SLU hy-
potheses with ?inform? action type. For each ?in-
form? action in the n-best SLU results, a feature
function is defined as follows:
f
i
(inf, s, v) =
{
S
i
(inf, s, v), if inf(s, v) ? UA
i
,
0, otherwise,
where S
i
(a, s, v) is the confidence score of the
hypothesis (a, s, v) assigned by SLU for the i-th
turn, a is the action type, s is the target slot, v is
its value, and UA
i
is the n-best list of SLU results.
Similarly, the actions with ?confirm? and ?deny?
types derive the corresponding feature functions
defined as:
f
i
(con, s, v) =
{
S
i
(con, s, v), if con(s, v) ? UA
i
,
0, otherwise,
f
i
(den, s, v) =
{
S
i
(den, s, v), if den(s, v) ? UA
i
,
0, otherwise.
In contrast with the above action types, both ?af-
firm? and ?negate? don?t specify any target slot and
value information on the SLU results. The feature
functions for these types are defined with (s, v)
derived from the previous ?expl-conf? and ?impl-
conf? system actions as follows:
f
i
(aff, s, v) =
?
?
?
?
?
max
j
(S
ij
(aff)) , if expl-conf(s, v) ? SA
i
,
or impl-conf(s, v) ? SA
i
0, otherwise,
f
i
(neg, s, v) =
?
?
?
?
?
max
j
(S
ij
(neg)) , if expl-conf(s, v) ? SA
i
,
or impl-conf(s, v) ? SA
i
0, otherwise,
where SA
i
is the system actions at the i-th turn.
The user actions with ?request? and ?reqalts?
could be able to play a crucial role to track the
requested slots with the following functions:
f
i
(req, s) =
{
S
i
(req, s), if req(s) ? UA
i
,
0, otherwise,
f
i
(reqalts, s) =
{
S
i
(reqalts, s), if reqalts ? UA
i
,
0, otherwise.
The other function is to indicate whether the
system is able to provide the information on (s, v)
using the ?canthelp? actions as follows:
f
i
(canthelp, s, v) =
{
1, if canthelp(s, v) ? SA
i
,
0, otherwise.
334
Dev set Test set
Acc L2 ROC Acc L2 ROC
Joint Goals
ME 0.638 0.551 0.144 0.596 0.671 0.036
CRF 0.644 0.545 0.103 0.601 0.649 0.064
Method
ME 0.839 0.260 0.398 0.877 0.204 0.397
CRF 0.875 0.202 0.181 0.904 0.155 0.187
Requested Slots
ME 0.946 0.099 0.000 0.957 0.081 0.000
CRF 0.942 0.107 0.000 0.960 0.073 0.000
Table 1: Comparisons of dialog state tracking performances
4 Experiment
To demonstrate the effectiveness of our proposed
sequential labeling approach for dialog state track-
ing, we performed experiments on the DSTC 2
dataset which consists of 3,235 dialog sessions
on restaurant information domain which were col-
lected using Amazon Mechanical Turk. The re-
sults of ASR and SLU are annotated for every
turn in the dataset, as well as the gold standard
annotations are also provided for evaluation. We
used this dataset following the original division
into training/development/test sets, which have
1,612/506/1,117 sessions, respectively.
Using this dataset, we trained two different
types of models: one is based on CRFs for our pro-
posed sequential labeling approach; and the other
is a baseline using maximum entropy (ME) that
performs the prediction for each individual turn
separately from others in a given session. All the
models for both approaches were trained on the
training set with the same feature functions de-
fined in Section 3.3 using MALLET
1
toolkit.
The trained models were used for predicting
goals, method, and requested slots of each turn in
the development and test sets, the results of which
were then organized into a tracker output object
defined as the input format to the evaluation script
of DSTC 2. Since we omitted the joint goals dis-
tributions in the output, the evaluations on the joint
goals were performed on the independent combi-
nations of the slot distributions.
Among the various combinations of evaluation
variables listed in the results of the evaluation
script, the following three featured metrics were
selected to report the performances of the tracker
in this paper: Accuracy, L2 norm, and ROC CA 5.
All these metrics were computed for the predicted
joint goals, method and requested slots.
1
http://mallet.cs.umass.edu/
Table 1 compares the performances of our pro-
posed approach (CRF) and the baseline method
(ME) for three sub-tasks on the development and
test sets. The results indicate that our proposed
sequential labeling approach achieved better per-
formances than the baseline for most cases. Es-
pecially, CRF models produced better joint goals
and method predictions in terms of accuracy and
L2 norm on both development and test sets. For
the requested slots task, our proposed approach
failed to generate better results than the baseline
on the development set. However, this situation
was reversed on the test set, which means our pro-
posed approach achieved better performances on
all three sub-tasks on the test set in two of the three
evaluation metrics.
5 Conclusions
This paper presented a sequential labeling ap-
proach for dialog state tracking. This approach
aimed to solve the cases of goal changes using
linear-chain CRFs. Experimental results show
the merits of our proposed approach with the im-
proved performances on all the sub-tasks of DSTC
2 compared to the baseline which doesn?t consider
sequential aspects.
However, these results are still not enough to
be competitive with the other participants in the
challenge. One possible reason is that our trackers
were trained only on the very basic features in this
work. If we discover more advanced features that
help to track the proper dialog states, they can raise
the overall performances further.
The other direction of our future work is to inte-
grate these dialog state trackers with our existing
dialog systems which accept the 1-best results of
ASR and SLU as they are, then to see their impacts
on the whole system level.
335
References
Dan Bohus and Alex Rudnicky. 2006. A k-
hypotheses+ other belief updating model. In Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Milica Gas?ic? and Steve Young. 2011. Effective
handling of dialogue state in the hidden informa-
tion state pomdp-based dialogue manager. ACM
Transactions on Speech and Language Processing
(TSLP), 7(3):4.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414?422.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the SIG-
DIAL 2013 Conference, pages 442?451.
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-
time spoken dialogue systems. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 200?207.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
Antoine Raux and Yi Ma. 2011. Efficient probabilistic
tracking of user goal and dialog history for spoken
dialog systems. In Proceedings of INTERSPEECH,
pages 801?804.
Hang Ren,Weiqun Xu, Yan Zhang, and YonghongYan.
2013. Dialog state tracking using conditional ran-
dom fields. In Proceedings of the SIGDIAL 2013
Conference, pages 457?461.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A pomdp framework for
spoken dialogue systems. Computer Speech & Lan-
guage, 24(4):562?588.
Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
observed information. In Proceedings of the SIG-
DIAL 2013 Conference, pages 423?432.
Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393?422.
Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference, pages 404?413.
Jason D Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dialog
states. In Acoustics Speech and Signal Processing
(ICASSP), 2010 IEEE International Conference on,
pages 5382?5385. IEEE.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language,
24(2):150?174.
Lukas Zilka, David Marek, Matej Korvas, and Filip Ju-
rcicek. 2013. Comparison of bayesian discrimina-
tive and generative models for dialogue state track-
ing. In Proceedings of the SIGDIAL 2013 Confer-
ence, pages 452?456.
336
