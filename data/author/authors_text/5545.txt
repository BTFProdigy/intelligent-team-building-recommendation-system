BioNLP 2007: Biological, translational, and clinical language processing, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
Syntactic complexity measures for detecting Mild Cognitive Impairment
Brian Roark, Margaret Mitchell and Kristy Hollingshead
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006 USA
{roark,meg.mitchell,hollingk}@cslu.ogi.edu
Abstract
We consider the diagnostic utility of vari-
ous syntactic complexity measures when ex-
tracted from spoken language samples of
healthy and cognitively impaired subjects.
We examine measures calculated from man-
ually built parse trees, as well as the same
measures calculated from automatic parses.
We show statistically significant differences
between clinical subject groups for a num-
ber of syntactic complexity measures, and
these differences are preserved with auto-
matic parsing. Different measures show dif-
ferent patterns for our data set, indicating
that using multiple, complementary mea-
sures is important for such an application.
1 Introduction
Natural language processing (NLP) techniques are
often applied to electronic health records and other
clinical datasets. Another potential clinical use of
NLP is for processing patient language samples,
which can be used to assess language development
(Sagae et al, 2005) or the impact of neurodegenera-
tive impairments on speech and language (Roark et
al., 2007). In this paper, we present methods for au-
tomatically measuring syntactic complexity of spo-
ken language samples elicited during neuropsycho-
logical exams of elderly subjects, and examine the
utility of these measures for discriminating between
clinically defined groups.
Mild Cognitive Impairment (MCI), and in par-
ticular amnestic MCI, the earliest clinically de-
fined stage of Alzheimer?s-related dementia, often
goes undiagnosed due to the inadequacy of com-
mon screening tests such as the Mini-Mental State
Examination (MMSE) for reliably detecting rela-
tively subtle impairments. Linguistic memory tests,
such as word list and narrative recall, are more ef-
fective than the MMSE in detecting MCI, yet are
still individually insufficient for adequate discrimi-
nation between healthy and impaired subjects. Be-
cause of this, a battery of examinations is typically
used to improve psychometric classification. Yet the
summary recall scores derived from these linguistic
memory tests (total correctly recalled) ignore poten-
tially useful information in the characteristics of the
spoken language itself.
Narrative retellings provide a natural, conversa-
tional speech sample that can be analyzed for many
of the characteristics of speech and language that
have been shown to discriminate between healthy
and impaired subjects, including syntactic complex-
ity (Kemper et al, 1993; Lyons et al, 1994) and
mean pause duration (Singh et al, 2001). These
measures go beyond simply measuring fidelity to
the narrative, thus providing key additional dimen-
sions for improved diagnosis of impairment. Recent
work (Roark et al, 2007) has shown significant dif-
ferences between healthy and MCI groups for both
pause related and syntactic complexity measures de-
rived from transcripts and audio of narrative recall
tests. In this paper, we look more closely at syntac-
tic complexity measures.
There are two key considerations when choos-
ing how to measure syntactic complexity of spoken
language samples for the purpose of psychometric
evaluation. First and most importantly, the syntactic
complexity measures will be used for discrimination
between groups, hence high discriminative utility is
desired. It has been demonstrated in past studies
(Cheung and Kemper, 1992) that many competing
measures are in fact very highly correlated, so it may
be the case that many measures are equally discrimi-
native. For this reason, previous results (Roark et al,
2007) have focused on a single syntactic complexity
metric, that of Yngve (1960).
A second key consideration, however, is the fi-
delity of the measure when derived from transcripts
via automatic parsing. Different syntactic complex-
ity measures rely on varying levels of detail from
1
the parse tree. Some syntactic complexity measures,
such as that of Yngve (1960), make use of unla-
beled tree structures to derive their scores; others,
such as that of Frazier (1985), rely on labels within
the tree, in addition to the tree structure, to pro-
vide the scores. Given these different uses of detail,
some measures may be less reliable with automa-
tion, hence dis-preferred in the context of automated
evaluation. Ideally, simple, easy-to-automate mea-
sures with high discriminative utility are preferred.
In the current paper, we demonstrate that various
syntactic complexity measures capture complemen-
tary systematic differences between subject groups,
suggesting that the best approach to discriminating
between healthy and impaired subjects is to collect
various measures, as a way of capturing language
?signatures? of the impairment.
For many measures of syntactic complexity, the
nature of the syntactic annotation is critical ? differ-
ent conventions of structural annotation will yield
different scores. We will thus spend the next sec-
tion briefly detailing the syntactic annotation con-
ventions that were followed for this work. This is
followed by a section describing a range of complex-
ity measures to be derived from these annotations.
Finally, we present empirical results on the samples
of spoken narrative retellings.
2 Syntactic annotation
For manual syntactic annotation of collected data
(see Section 4), we followed the syntactic annota-
tion conventions of the well-known Penn Treebank
(Marcus et al, 1993). This provides several key ben-
efits. First, there is an extensive annotation guide
that has been developed, not just for written but also
for spoken language, so that consistent annotation
was facilitated. Second, the large out-of-domain
corpora, in particular the 1 million words of syn-
tactically annotated Switchboard telephone conver-
sations, provide a good starting point for training
domain adapted parsing models. Finally, we can use
multiple domains for evaluating the correlations be-
tween various syntactic complexity measures.
There are characteristics of Penn Treebank anno-
tation that can impact syntactic complexity scoring.
First, prenominal modifiers are typically grouped in
a flat constituent with no internal structure. This an-
notation choice can result in very long noun phrases
(NPs) which pose very little difficulty in terms of
human processing performance, but can inflate com-
plexity measures that measure deviation from right-
branching structures, such as that of Yngve (1960).
Second, in spoken language annotations, a reparan-
dum1 is denoted with a special non-terminal cate-
gory EDITED. For this paper, we remove from the
tree these non-terminals, and the structures under-
neath them, prior to evaluating syntactic complexity.
3 Syntactic complexity
There is no single agreed-upon measurement of
syntactic complexity. A range of measures have
been proposed, with different primary considera-
tions driving the notion of complexity for each.
Many measures focus on the order in which vari-
ous constructions are acquired by children learning
the syntax of their native language ? later acquisi-
tions being taken as higher complexity. Examples
of this sort of complexity measure are: mean length
of utterance (MLU), which is typically measured
in morphemes (Miller and Chapman, 1981); the
Index of Productive Syntax (Scarborough, 1990),
a multi-point scale which has recently been auto-
mated for child-language transcript analysis (Sagae
et al, 2005); and Developmental Level (Rosenberg
and Abbeduto, 1987), a 7-point scale of complex-
ity based on the presence of specific grammatical
constructions. Other approaches have relied upon
the right-branching nature of English syntactic trees
(Yngve, 1960; Frazier, 1985), under the assump-
tion that deviations from that correspond to more
complexity in the language. Finally, there are ap-
proaches focused on the memory demands imposed
by ?distance? between dependent words (Lin, 1996;
Gibson, 1998).
3.1 Yngve scoring
The scoring approach taken in Yngve (1960) is re-
lated to the size of a ?first in/last out? stack at each
word in a top-down, left-to-right parse derivation.
Consider the tree in Figure 1. If we knew exactly
which productions to use, the parse would begin
with an S category on the stack and advance as
follows: pop the S and push VP and NP onto the
stack; pop NP and push PRP onto the stack; pop
PRP from the stack; pop VP from the stack and
push NP and VBD onto the stack; and so on. At
the point when the word ?she? is encountered, only
VP remains on the stack of the parser. When ?was?
1A reparandum is a sequence of words that are aborted by
the speaker, then repaired within the same utterance.
2
S
1 0
NP
0
VP
1 0
PRP VBD NP
1 0
NP
1 0
PP
1 0
DT NN IN NP
2 1 0
DT NN NN
Yngve score:she1 was1 a2 cook1 in1 a2 school1 cafeteria0
1
Figure 1: Parse tree with branch scores for Yngve scoring.
is reached, just NP is on the stack. Thus, the Yn-
gve score for these two words is 1. When the next
word ?a? is reached, however, there are two cate-
gories on the stack: PP and NN, so this word re-
ceives an Yngve score of 2. Stack size has been re-
lated by some (Resnik, 1992) to working memory
demands, although it most directly measures devia-
tion from right-branching trees.
To calculate the size of the stack at each word,
we can use the following simple algorithm. At each
node in the tree, label the branches from that node
to each of its children, beginning with zero at the
rightmost child and continuing to the leftmost child,
incrementing the score by one for each child. Hence,
each rightmost branch in the tree of Figure 1 is la-
beled with 0, the leftmost branch in all binary nodes
is labeled with 1, and the leftmost branch in the
ternary node is labeled with 2. Then the score for
each word is the sum of the branch scores from the
root of the tree to the word.
Given the score for each word, we can then de-
rive an overall complexity score by summing them
or taking the maximum or mean. For this paper,
we report mean scores for this and other word-based
measures, since we have found these means to pro-
vide better performing scores than either total sum
or maximum. For the tree in Figure 1, the maximum
is 2, the total is 9 and the mean over 8 words is 118 .
3.2 Frazier scoring
Frazier (1985) proposed an approach to scoring syn-
tactic complexity that traces a path from a word up
the tree until reaching either the root of the tree or
the lowest node which is not the leftmost child of its
parent.2 For example, Figure 2 shows the tree from
2An exception is made for empty subject NPs, in which case
S
1.5
NP
1
VP
1
PRP VBD NP
1
NP
1
PP
1
DT NN IN NP
1
DT NN NN
Frazier score:
she
2.5
was
1
a
2
cook
0
in
1
a
1
school
0
cafeteria
0
1
Figure 2: Parse tree fragments with scores for Frazier scoring.
Figure 1 broken into distinct paths for each word
in the string. The first word has a path up to the
root, while the second word just up to the VP, since
the VP has an NP sibling to its left. The word is
then scored, as in the Yngve measure, by summing
the scores on the links along the path. Each non-
terminal node in the path contributes a score of 1,
except for sentence nodes and sentence-complement
nodes,3 which score 1.5 rather than 1. Thus em-
bedded clauses contribute more to the complexity
measure than other embedded categories, as an ex-
plicit acknowledgment of sentence embeddings as a
source of syntactic complexity.
As with the Yngve score, we can calculate the
total and the mean of these word scores. In con-
trast to the maximum score calculated for the Yngve
measure, Frazier proposed summing the word scores
for each 3-word sequence in the sentence, then tak-
ing the maximum of these sums as a measure of
highly-localized concentrations of grammatical con-
stituents. For the example in Figure 2, the maximum
is 2.5, the maximum 3-word sum is 5.5, and the total
is 7.5, yielding a mean of 1516 .
3.3 Dependency distance
Rather than examining the tree structure itself, one
might also extract measures from lexical depen-
dency structures. These dependencies can be de-
rived from the tree using standard rules for estab-
lishing head children for constituents, originally at-
the succeeding verb receives an additional score of 1 (for the
deleted NP), and its path continues up the tree. Empty NPs are
annotated in our manual parse trees but not in the automatic
parses, which may result in a small disagreement in the Frazier
scores for manual and automatic trees.
3Every non-terminal node beginning with an S, including
SQ and SINV, were counted as sentence nodes. Sequences of
sentence nodes, i.e. an SBAR appearing directly under an S
node, were only counted as a single sentence node and thus only
contributed to the score once.
3
she was a cook in a school cafeteria
1
Figure 3: Dependency graph for the example string.
tributed to Magerman (1995), to percolate lexical
heads up the tree. Figure 3 shows the dependency
graph that results from this head percolation ap-
proach, where each link in the graph represents a de-
pendency relation from the modifier to the head. For
example, conventional head percolation rules spec-
ify the VP as the head of the S, so ?was?, as the head
of the VP, is thus the lexical head of the entire sen-
tence. The lexical heads of the other children of the
S node are called modifiers of the head of the S node;
thus, since ?she? is the head of the subject NP, there
is a dependency relation between ?she? and ?was?.
Lin (1996) argued for the use of this sort of depen-
dency structure to measure the difficulty in process-
ing, given the memory overhead of very long dis-
tance dependencies. Both Lin (1996) and Gibson
(1998) showed that human performance on sentence
processing tasks could be predicted with measures
of this sort. While details may differ ? e.g., how
to measure distance, what counts as a dependency ?
we can make use of the general approach given Tree-
bank style parses and head percolation, resulting in
graphs of the sort in Figure 3. For the current paper,
we count the distance between words for each de-
pendency link. For Figure 3, there are 7 dependency
links, a distance total of 11, and a mean of 147 .
3.4 Developmental level (D-Level)
D-Level defines eight levels of sentence complex-
ity, from 0-7, based on the development of complex
sentences in normal-development children. Each
level is defined by the presence of specific grammat-
ical constructions (Rosenberg and Abbeduto, 1987);
we follow Cheung and Kemper (1992) in assigning
scores equivalent to the defined level of complex-
ity. A score of zero corresponds to simple, single-
clause sentences; embedded infinitival clauses get
a score of 1 (She needs to pay the rent); conjoined
clauses (She worked all day and worried all night),
compound subjects (The woman and her four chil-
dren had not eaten for two days), and wh-predicate
complements score 2. Object noun phrase rela-
tive clauses or complements score 3 (The police
caught the man who robbed the woman), whereas
the same constructs in subject noun phrases score
5 (The woman who worked in the cafeteria was
robbed). Gerundive complements and comparatives
(They were hungrier than her) receive a score of 4;
subordinating conjunctions (if, before, as soon as)
score 6. Finally, a score of 7 is used as a catch-all
category for sentences containing more than one of
any of these grammatical constructions.
3.5 POS-tag sequence cross entropy
One possible approach for detecting rich syntactic
structure is to look for infrequent or surprising com-
binations of parts-of-speech (POS). We can measure
this over an utterance by building a simple bi-gram
model over POS tags, then measuring the cross en-
tropy of each utterance.4
Given a bi-gram model over POS-tags, we can
calculate the probability of the sequence as a whole.
Let ?i be the POS-tag of word wi in a sequence of
wordsw1 . . . wn, and assume that ?0 is a special start
symbol, and that ?n+1 is a special stop symbol. Then
the probability of the POS-tag sequence is
P(?1 . . . ?n) =
n+1?
i=1
P(?i | ?i?1) (1)
The cross entropy is then calculated as
H(?1 . . . ?n) = ?
1
n
log P(?1 . . . ?n) (2)
With this formulation, this basically boils down to
the mean negative log probability of each tag given
the previous tag.
4 Data
4.1 Subjects
We collected audio recordings of 55 neuropsycho-
logical examinations administered at the Layton Ag-
ing & Alzheimer?s Disease Center, an NIA-funded
Alzheimer?s center for research at OHSU. For this
study, we partitioned subjects into two groups: those
who were assigned a Clinical Dementia Rating
(CDR) of 0 (healthy) and those who were assigned
a CDR of 0.5 (Mild Cognitive Impairment; MCI).
The CDR (Morris, 1993) is assigned with access to
clinical and cognitive test information, independent
of performance on the battery of neuropsychologi-
cal tests used for this research study, and has been
shown to have high expert inter-annotator reliability
(Morris et al, 1997).
4For each test domain, we used cross-validation techniques
to build POS-tag bi-gram models and evaluate with them in that
domain.
4
CDR = 0 CDR = 0.5
(n=29) (n=18)
Measure M SD M SD t(45)
Age 88.1 9.0 91.9 4.4 ?1.65
Education (Y) 15.0 2.2 14.3 2.8 1.04
MMSE 28.4 1.4 25.9 2.6 4.29***
Word List (A) 20.0 4.0 15.4 3.3 4.06***
Word List (R) 6.8 2.0 3.9 1.7 5.12***
Wechsler LM I 17.2 4.0 10.9 4.2 5.20***
Wechsler LM II 15.8 4.3 9.5 5.4 4.45***
Cat.Fluency (A) 17.2 4.1 13.9 4.2 2.59*
Cat.Fluency (V) 12.8 4.5 9.6 3.6 2.57*
Digits (F) 6.6 1.4 6.1 1.2 1.11
Digits (B) 4.7 1.0 4.7 1.1 ?0.04
Table 1: Neuropsychological test results for subjects.
***p < 0.001; **p < 0.01 ; *p < 0.05
Of the collected recordings, three subjects were
recorded twice; for the current study only one
recording was used for each subject. Three subjects
were assigned a CDR of 1.0 and were excluded from
the study; two further subjects were excluded for er-
rors in the recording that resulted in missing audio.
Of the remaining 47 subjects, 29 had CDR = 0, and
18 had CDR = 0.5.
4.2 Neuropsychological tests
Table 1 presents means and standard deviations for
age, years of education and the manually-calculated
scores of a number of standard neuropsychological
tests that were administered during the recorded ses-
sion. These tests include: the Mini Mental State Ex-
amination (MMSE); the CERAD Word List Acqui-
sition (A) and Recall (R) tests; the Wechsler Logical
Memory (LM) I (immediate) and II (delayed) narra-
tive recall tests; Category Fluency, Animals (A) and
Vegetables (V); and Digit Span (WAIS-R) forward
(F) and backward (B).
The Wechsler Logical Memory I/II tests are the
basis of our study on syntactic complexity measures.
The original narrative is a short, 3 sentence story:
Anna Thompson of South Boston, employed as a cook in a
school cafeteria, reported at the police station that she had
been held up on State Street the night before and robbed of
fifty-six dollars. She had four small children, the rent was
due, and they had not eaten for two days. The police, touched
by the woman?s story, took up a collection for her.
Subjects are asked to re-tell this story immediately
after it is told to them (LM I), as well as after ap-
proximately 30 minutes of unrelated activities (LM
II). We transcribed each retelling, and manually an-
notated syntactic parse trees according to the Penn
Treebank annotation guidelines. Algorithms for au-
tomatically extracting syntactic complexity markers
from parse trees were written to accept either man-
System LR LP F-measure
Out-of-domain (WSJ) 77.7 80.1 78.9
Out-of-domain (SWBD) 84.0 86.2 85.1
Domain adapted from SWBD 87.9 88.3 88.1
Table 2: Parser accuracy on Wechsler Logical Memory re-
sponses using just out-of-domain data (either from the Wall St.
Journal (WSJ) or Switchboard (SWBD) treebanks) versus using
a domain adapted system.
ually annotated trees or trees output from an auto-
matic parser, to demonstrate the plausibility of using
automatically generated parse trees.
4.3 Parsing
For automatic parsing, we made use of the well-
known Charniak parser (Charniak, 2000). Following
best practices (Charniak and Johnson, 2001), we re-
moved sequences covered by EDITED nodes in the
tree from the strings prior to parsing. For this pa-
per, EDITED nodes were identified from the manual
parse, not automatically. Table 2 shows parsing ac-
curacy of our annotated retellings under three pars-
ing model training conditions: 1) trained on approx-
imately 1 million words of Wall St. Journal (WSJ)
text; 2) trained on approximately 1 million words
of Switchboard (SWBD) corpus telephone conver-
sations; and 3) using domain adaptation techniques
starting from the SWBD Treebank. The SWBD out-
of-domain system reaches quite respectable accura-
cies, and domain adaptation achieves 3 percent ab-
solute improvement over that.
For domain adaptation, we used MAP adapta-
tion techniques (Bacchiani et al, 2006) via cross-
validation over the entire set of retellings. For
each subject, we trained a model using the SWBD
treebank as the out-of-domain treebank, and the
retellings of the other 46 subjects as in-domain train-
ing. We used a count merging approach, with the
in-domain counts scaled by 1000 relative to the out-
of-domain counts. See Bacchiani et al (2006) for
more information on stochastic grammar adaptation
using these techniques.
5 Experimental results
5.1 Correlations
Our first set of experimental results regard correla-
tions between measures. Table 3 shows results for
five of our measures over all three treebanks that we
have been considering: Penn WSJ Treebank, Penn
SWBD Treebank, and the Wechsler LM retellings.
The correlations along the diagonal are between the
same measure when extracted from manually an-
notated trees and when extracted from automatic
5
Penn WSJ Treebank Penn SWBD Treebank Wechsler LM Retellings
(a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)
(a) Frazier 0.89 0.96 0.94
(b) Yngve -0.31 0.96 -0.72 0.96 -0.69 0.95
(c) Tree nodes 0.91 -0.16 0.92 0.58 -0.06 0.93 0.93 -0.48 0.85
(d) Dep len -0.29 0.75 -0.13 0.93 -0.74 0.97 -0.08 0.96 -0.72 0.96 -0.51 0.96
(e) Cross Ent 0.17 0.18 0.15 0.19 0.93 -0.55 0.76 0.09 0.76 0.98 -0.13 0.45 0.05 0.41 0.97
Table 3: Correlation matrices for several measures on an utterance-by-utterance basis. Correlations along the diagonal are between
the manual measures and the measures when automatically parsed. All other correlations are between measures when derived from
manual parse trees.
parses. All other correlations are between mea-
sures derived from manual trees. All correlations
are taken per utterance.
From this table, we can see that all of the mea-
sures derived from automatic parses have a high
correlation with the manually derived measures, in-
dicating that they may preserve any discriminative
utility of these markers. Interestingly, the num-
ber of nodes in the tree per word tends to corre-
late well with the Frazier score, while the depen-
dency length tends to correlate well with the Yngve
score. Cross entropy correlates with Yngve and de-
pendency length for the SWBD and Wechsler tree-
banks, but not for the WSJ treebank.
5.2 Manually derived measures
Table 4 presents means and standard deviations
for measures derived from the LM I and LM II
retellings, along with the t-value and level of sig-
nificance. The first three measures presented in the
table are available without syntactic annotation: to-
tal number of words, total number of utterances, and
words per utterance in the retelling. None of these
three measures on either retelling show statistically
significant differences between the groups.
The first measure to rely upon syntactic annota-
tions is words per clause. The number of clauses are
automatically extracted from the parses by counting
the number of S nodes in the tree.5 Normalizing the
number of words by the number of clauses rather
than the number of utterances (as in words per ut-
terance) results in statistically significant differences
between the groups for LM I though not for LM II.
The other measures are as described in Section
3. Interestingly, Frazier score per word, the number
of tree nodes per word, and POS-tag cross entropy
all show a significant negative t-value on the LM I
retellings, meaning the CDR 0.5 subjects had sig-
nificantly higher scores than the CDR 0 subjects for
5For coordinated S nodes, the root of the coordination,
which in Penn Treebank style annotation also has an S label,
does not count as an additional clause.
these measures on this task. These measures showed
no significant difference on the LM II retellings.
The Yngve score per word and the dependency
length per word showed no significant difference on
LM I retellings but a statistically significant differ-
ence on LM II, with the expected outcome of higher
scores for the CDR 0 subjects. The D-Level measure
showed no significant differences.
5.3 Automatically derived measures
In addition to manual-parse derived measures, Table
4 also presents the same measures when automatic,
rather than manual, parses are used. Given the rela-
tively high quality of the automatic parses, most of
the means and standard deviations are quite close,
and all of the patterns observed in the upper half of
Table 4 are preserved, except that the Yngve score
per word no longer shows a statistically significant
difference for the LM II retelling.
5.4 Left-corner trees
For the tree-based complexity metrics (Frazier and
Yngve), we also investigated alternative imple-
mentations that make use of the left-corner trans-
formation (Rosenkrantz and Lewis II, 1970) of
the tree from which the measures were extracted.
This transformation is widely known for remov-
ing left-recursion from a context-free grammar, and
it changes the tree shape by transforming left-
branching structures into right-branching structures,
while leaving center-embedded structures center-
embedded. This property led Resnik (1992) to pro-
pose left-corner processing as a plausible mecha-
nism for human sentence processing, since it is pre-
cisely these center-embedded structures, and not the
left- or right-branching structures, that are problem-
atic for humans to process.
Table 5 presents results using either manually an-
notated trees or automatic parses to extract the Yn-
gve and Frazier measures after a left-corner trans-
form has been applied to the tree. The Frazier
scores are very similar to those without the left-
6
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Total words in retelling 71.0 26.0 58.1 31.9 1.49 70.6 21.5 58.5 36.7 1.43
Total utterances in retelling 8.86 4.16 7.72 3.28 0.99 8.17 2.77 7.06 4.86 1.01
Words per utterance in retelling 8.57 2.44 7.78 3.67 0.89 9.16 3.06 7.82 4.76 1.18
Manually extracted: Words per clause 6.33 1.39 5.25 1.25 2.68* 6.12 1.20 5.48 3.37 0.93
Frazier score per word 1.19 0.09 1.26 0.11 ?2.68* 1.19 0.09 1.13 0.43 0.67
Tree nodes per word 1.96 0.07 2.01 0.10 ?2.08* 1.96 0.07 1.80 0.66 1.36
Yngve score per word 1.44 0.23 1.39 0.30 0.61 1.53 0.27 1.26 0.62 2.01*
Dependency length per word 1.54 0.25 1.47 0.27 0.90 1.63 0.30 1.34 0.60 2.19*
POS-tag Cross Entropy 1.83 0.16 1.96 0.26 ?2.18* 1.93 0.14 1.86 0.59 0.54
D-Level 1.07 0.75 1.03 1.23 0.14 1.23 0.81 1.68 1.41 ?1.42
Auto extracted: Words per clause 6.42 1.53 5.10 1.16 3.13** 6.04 1.25 5.61 3.67 0.59
Frazier score per word 1.16 0.10 1.24 0.10 ?2.92** 1.15 0.10 1.09 0.41 0.69
Tree nodes per word 1.96 0.07 2.03 0.10 ?2.55* 1.96 0.08 1.79 0.66 1.38
Yngve score per word 1.41 0.23 1.37 0.29 0.54 1.50 0.27 1.28 0.64 1.70
Dependency length per word 1.51 0.25 1.47 0.28 0.54 1.61 0.28 1.35 0.61 2.04*
POS-tag Cross Entropy 1.83 0.17 1.96 0.26 ?2.12* 1.92 0.14 1.86 0.58 0.53
D-Level 1.09 0.73 1.11 1.20 ?0.08 1.28 0.77 1.61 1.22 ?1.15
Table 4: Syntactic complexity measure group differences when measures are derived from either manual or automatic parse trees.
**p < 0.01 ; *p < 0.05
corner transform, while the Yngve scores are re-
duced across the board. With the left-corner trans-
formed tree, the automatically derived Yngve mea-
sure retains the statistically significant difference
shown by the manually derived measure.
6 Discussion and future directions
The results presented in the last section demonstrate
that NLP techniques applied to clinically elicited
spoken language samples can be used to automat-
ically derive measures that may be useful for dis-
criminating between healthy and MCI subjects. In
addition, we see that different measures show differ-
ent patterns when applied to these language samples,
with Frazier scores and tree nodes per word giving
quite different results than Yngve scores and depen-
dency length. It would thus appear that, for Penn
Treebank style annotations at least, these measures
are quite complementary.
There are two surprising aspects of these results:
the significantly higher means of three measures on
LM I samples for MCI subjects, and the fact that one
set of measures show significant differences on LM
I while another shows differences on LM II. We do
not have definitive explanations for these phenom-
ena, but we can speculate about why such results
were obtained.
First, there is an important difference between the
manner of elicitation for LM I versus LM II. LM I
is an immediate recall, so there will likely be, for
unimpaired subjects, much higher verbatim recall of
the story than in the delayed recall of LM II. For
the MCI group, which exhibits memory impairment,
there will be little in the way of verbatim recall, and
potentially much more in the way of spoken lan-
guage phenomena such as filled pauses, parenthet-
icals and off-topic utterances. This may account for
the higher Frazier score per word for the MCI group
on LM I. Such differences will likely be lessened in
the delayed recall.
Second, the Frazier and Yngve metrics differ in
how they score long, flat phrases, such as typical
base NPs. Consider the ternary NP in Figure 1. The
first word in that NP (?a?) receives an Yngve score
of 2, but a Frazier score of only 1 (Figure 2), while
the second word in the NP receives an Yngve score
of 1 and a Frazier score of 0. For a flat NP with
5 children, that difference would be 4 to 1 for the
first child, 3 to 0 for the second child, and so forth.
This difference in scoring relatively common syn-
tactic constructions, even those which may not affect
human memory load, may account for such different
scores achieved with these different measures.
In summary, we have demonstrated an important
clinical use for NLP techniques, where automatic
syntactic annotation provides sufficiently accurate
parse trees for use in automatic extraction of syntac-
tic complexity measures. Different syntactic com-
plexity measures appear to be measuring quite com-
plementary characteristics of the retellings, yielding
statistically significant differences from both imme-
diate and delayed retellings.
There are quite a number of questions that we will
7
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Manually extracted: Left-corner Frazier 1.20 0.10 1.28 0.12 ?2.60* 1.20 0.11 1.18 0.45 0.29
Left-corner Yngve 1.33 0.20 1.25 0.23 1.20 1.37 0.21 1.14 0.52 2.14*
Auto extracted: Left-corner Frazier 1.16 0.10 1.27 0.13 ?3.02** 1.15 0.11 1.10 0.42 0.64
Left-corner Yngve 1.31 0.19 1.23 0.21 1.33 1.36 0.21 1.13 0.51 2.11*
Table 5: Syntactic complexity measure group differences when measures are derived from left-corner parse trees.
**p < 0.01 ; *p < 0.05
continue to pursue. Most importantly, we will con-
tinue to examine this data, to try to determine what
characteristics of the spoken language are leading to
the unexpected patterns in the results. In addition,
we will begin to explore composite measures, such
as differences in measures between LM I and LM II,
which promise to better capture some of the patterns
we have observed. Ultimately, we would like to
build classifiers making use of a range of measures
as features, although in order to demonstrate statisti-
cally significant differences between classifiers, we
will need much more data than we currently have.
Eventually, longitudinal tracking of subjects may be
the best application of such measures on clinically
elicited spoken language samples.
Acknowledgments
This research was supported in part by NSF Grant #IIS-
0447214 and pilot grants from the Oregon Center for Aging
& Technology (ORCATECH, NIH #1P30AG024978-01) and
the Oregon Partnership for Alzheimer?s Research. Also, the
third author of this paper was supported under an NSF Grad-
uate Research Fellowship. Any opinions, findings, conclusions
or recommendations expressed in this publication are those of
the authors and do not necessarily reflect the views of the NSF.
Thanks to Jeff Kaye, John-Paul Hosom, Jan van Santen, Tracy
Zitzelberger, Jessica Payne-Murphy and Robin Guariglia for
help with the project.
References
M. Bacchiani, M. Riley, B. Roark, and R. Sproat. 2006. MAP
adaptation of stochastic grammars. Computer Speech and
Language, 20(1):41?68.
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proceedings of the 2nd Confer-
ence of the North American Chapter of the ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the ACL, pages 132?139.
H. Cheung and S. Kemper. 1992. Competing complexity met-
rics and adults? production of complex sentences. Applied
Psycholinguistics, 13:53?76.
L. Frazier. 1985. Syntactic complexity. In D.R. Dowty,
L. Karttunen, and A.M. Zwicky, editors, Natural Language
Parsing. Cambridge University Press, Cambridge, UK.
E. Gibson. 1998. Linguistic complexity: locality of syntactic
dependencies. Cognition, 68(1):1?76.
S. Kemper, E. LaBarge, F.R. Ferraro, H. Cheung, H. Cheung,
and M. Storandt. 1993. On the preservation of syntax in
Alzheimer?s disease. Archives of Neurology, 50:81?86.
D. Lin. 1996. On structural complexity. In Proceedings of
COLING-96.
K. Lyons, S. Kemper, E. LaBarge, F.R. Ferraro, D. Balota, and
M. Storandt. 1994. Oral language and Alzheimer?s disease:
A reduction in syntactic complexity. Aging and Cognition,
1(4):271?281.
D.M. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
ACL, pages 276?283.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
J.F. Miller and R.S. Chapman. 1981. The relation between
age and mean length of utterance in morphemes. Journal of
Speech and Hearing Research, 24:154?161.
J. Morris, C. Ernesto, K. Schafer, M. Coats, S. Leon, M. Sano,
L. Thal, and P. Woodbury. 1997. Clinical dementia
rating training and reliability in multicenter studies: The
Alzheimer?s disease cooperative study experience. Neurol-
ogy, 48(6):1508?1510.
J. Morris. 1993. The clinical dementia rating (CDR): Current
version and scoring rules. Neurology, 43:2412?2414.
P. Resnik. 1992. Left-corner parsing and psychological plausi-
bility. In Proceedings of COLING-92, pages 191?197.
B. Roark, J.P. Hosom, M. Mitchell, and J.A. Kaye. 2007. Au-
tomatically derived spoken language markers for detecting
mild cognitive impairment. In Proceedings of the 2nd Inter-
national Conference on Technology and Aging (ICTA).
S. Rosenberg and L. Abbeduto. 1987. Indicators of linguis-
tic competence in the peer group conversational behavior of
mildly retarded adults. Applied Psycholinguistics, 8:19?32.
S.J. Rosenkrantz and P.M. Lewis II. 1970. Deterministic left
corner parsing. In IEEE Conference Record of the 11th An-
nual Symposium on Switching and Automata, pages 139?
152.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic
measurement of syntactic development in child langugage.
In Proceedings of the 43rd Annual Meeting of the ACL.
H.S. Scarborough. 1990. Index of productive syntax. Applied
Psycholinguistics, 11:1?22.
S. Singh, R.S. Bucks, and J.M. Cuerden. 2001. Evaluation of
an objective technique for analysing temporal variables in
DAT spontaneous speech. Aphasiology, 15(6):571?584.
V.H. Yngve. 1960. A model and an hypothesis for language
structure. Proceedings of the American Philosophical Soci-
ety, 104:444?466.
8
Proceedings of the 12th European Workshop on Natural Language Generation, pages 50?57,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Class-Based Ordering of Prenominal Modifiers
Margaret Mitchell
Center for Spoken Language Understanding
Portland, Oregon, U.S.A
itallow@cslu.ogi.edu
Abstract
This paper introduces a class-based ap-
proach to ordering prenominal modifiers.
Modifiers are grouped into broad classes
based on where they tend to occur prenom-
inally, and a framework is developed to or-
der sets of modifiers based on their classes.
This system is developed to generate sev-
eral orderings for modifiers with more
flexible positional constraints, and lends
itself to bootstrapping for the classification
of previously unseen modifiers.
1 Introduction
Ordering prenominal modifiers is a necessary task
in the generation of natural language. For a system
to effectively generate fluent utterances, the sys-
tem must determine the proper order for any given
set of modifiers. The order of modifiers before a
noun affects the meaning and fluency of generated
utterances. Determining ways to order modifiers
prenominally has been an area of considerable re-
search (cf. Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000).
In this paper, we establish and evaluate a classi-
fication system that can be used to order prenom-
inal modifiers automatically. This may be im-
plemented in a surface realization component of
a natural language generation system, or may be
used to help specify the ordering of properties that
feed into a referring expression generation algo-
rithm. Predictions of prenominal modifier order-
ing based on these classes are shown to be robust
and accurate.
The work here diverges from the approaches
commonly employed in modifier classification by
assuming no underlying relationship between se-
mantics and prenominal order or morphology and
prenominal order. The approach instead relies
on generalizing empirical evidence from a corpus.
This allows the system to be robust and portable to
a variety of applications, without precluding any
underlying linguistic constraints.
In the next section, we discuss prior work on
this topic, and address the differences in our ap-
proach. Section 3 discusses the relationship be-
tween modifier ordering and referring expression
generation, a principal component of natural lan-
guage generation. Section 4 describes the ideas
behind the modifier classification system. Sec-
tions 5 and 6 present the materials and method-
ology of the current study, with a discussion of the
corpus involved and the basic modules used in the
process. In Section 7 we discuss the results of our
study. Finally, in Section 8, we outline areas for
improvement and possible future work.
2 Related Work
Discerning the rules governing the ordering of ad-
jectives has been an area of research for quite some
time (see, for example, Panini?s work on San-
skrit grammar ca. 350 BCE). Most approaches as-
sume an underlying relationship between seman-
tics and prenominal position (cf. Whorf, 1945;
Ziff, 1960; Bever, 1970; Danks and Glucksberg,
1971). These approaches can be characterized as
predicting modifier order based on degrees of se-
mantic closeness to the noun. This follows what
is known as Behaghel?s First Law (Behaghel,
1930):
Word groups: What belongs together
mentally is placed close together syntac-
tically.
(Clark and Clark, 1977: 545)
However, there is disagreement on the exact
qualities that affect position. These theories are
also difficult to implement in a generation system,
as they require determining the semantic proper-
ties of each modifier used, relative to the context
in which it occurs. If a modifier classification
50
scheme is to be implemented, it should be able to
create a variety of natural, unmarked orders; be ro-
bust enough to handle a wide variety of modifiers;
and be flexible enough to allow different natural
orderings.
Shaw and Hatzivassiloglou (1999) examine this
problem, and develop ways to order all prenominal
modifier types. This includes adjectives as well
as nouns, such as ?baseball? in ?baseball field?;
gerunds, such as ?running? in ?running man?; and
past participles, such as ?heated? in ?heated de-
bate?. The authors devise three different meth-
ods that may be implemented in a generation sys-
tem to order these kinds of prenominal modifiers.
These are the direct evidence method, the transi-
tivity method, and the clustering method.
Briefly, given prenominal modifiers a and b in
a training corpus, the direct evidence method uti-
lizes probabilistic reasoning to determine whether
the frequency count of the ordered sequence
<a,b> or <b,a> is stronger. The transitiv-
ity method makes inferences about unseen order-
ings among prenominal modifiers; given a third
prenominal modifier c, where a precedes b and b
precedes c, the authors can conclude that a pre-
cedes c. In the clustering method, an order sim-
ilarity metric is used to group modifiers together
that share a similar relative order to other modi-
fiers.
Shaw and Hatzivassiloglou achieve their high-
est prediction accuracy of 90.67% using their tran-
sitivity technique on prenominal modifiers from
a medical corpus. However, with their system
trained on the medical corpus and then tested
on the Wall Street Journal corpus (Marcus et al,
1993), they achieve an overall prediction accuracy
of only 54%. The authors conclude that prenomi-
nal modifier ordering is domain-specific.
Malouf (2000) continues this work, determin-
ing the order for sequences of prenominal adjec-
tives by examining several different statistical and
machine learning techniques. These achieve good
results, ranging from 78.28% to 89.73% accuracy.
Malouf achieves the best results by combining
memory-based learning and positional probabil-
ity, which reaches 91.85% accuracy at predicting
the prenominal adjective orderings in the first 100
million tokens of the BNC. However, this analysis
does not extend to other kinds of prenominal mod-
ifiers. The model is also not tested on a different
domain.
The approach to modifier classification taken
here is similar to the clustering method discussed
by Shaw and Hatzivassiloglou. Modifiers are
grouped into classes based on where they occur
prenominally. This approach differs, however, in
how classes are assigned. In our approach, modi-
fiers are grouped into classes based on the frequen-
cies with which they occur in different prenominal
positions. Classes are built based not on where
modifiers are positioned in respect to other mod-
ifiers, but on where modifiers are positioned in
general. Grouping modifiers into classes based on
prenominal positions mitigates the problems noted
by Shaw and Hatzivassiloglou that ordering pre-
dictions cannot be made (1) when both a and b be-
long to the same class, (2) when either a or b are
not associated to a class that can be ordered with
respect to the other, and (3) when the evidence for
one class preceding the other is equally strong for
both classes.
This approach allows modifiers with strong
positional preferences to be in a class separate
from modifiers with weaker positional prefer-
ences. This also ensures that any prenominal mod-
ifiers a and b seen in the training corpus can be
ordered, regardless of which particular modifiers
they appear with and whether they occur together
in the training data at all. This approach also has
the added benefit of developing modifier classes
that are usable across many different domains.
Further, this method is conceptually simple and
easy to implement. Although this approach is less
context-sensitive than earlier work, we find that it
is highly accurate, with comparable token preci-
sion. We discuss this in greater detail in Sections
6 and 7.
3 The Problem of Ordering Prenominal
Modifiers
Generating referring expressions in part requires
generating the adjectives, verbs, and nouns that
modify head nouns. In order for these expressions
to clearly convey the intended referent, the mod-
ifiers must appear in an order that sounds natural
and mimics human language use.
For example, consider the alternation given in
Figure 1. Some combinations of modifiers be-
fore a noun are more marked than others, although
all are strictly speaking grammatical. This speaks
to the need for a broad modifier classes to order
prenominal modifiers, where individual modifiers
51
(1) big beautiful white wooden house
(2) ?white wooden beautiful big house
(3) comfortable red chair
(4) ?red comfortable chair
(5) big rectangular green Chinese silk carpet
(6) ?Chinese big silk green rectangular carpet
Figure 1: Grammatical Modifier Alternations
(Vendler, 1968: 122)
may be ordered separately as required by particu-
lar contexts.
Along these lines, almost all referring expres-
sion generation algorithms rely on the availability
of a predefined ordering or weighting of properties
(Dale and Reiter, 1995; van Deemter, 2002; Krah-
mer et al, 2003). This requires that for every refer-
ent, an ordered or weighted listing of all the prop-
erties that can apply to it must be created before
referring expression generation begins. In these
models, the order or weights of the input proper-
ties map to the order of the output modifiers.
However, the method used to determine the or-
dering or weighting of properties is an open is-
sue. The difficulty with capturing the ordering of
properties and their corresponding modifiers stems
from the problem of data sparsity. In the example
in Figure 1, the modifier silkmay be rare enough in
any corpus that finding it in combination with an-
other modifier, in order to create a generalization
about its ordering constraints, is nearly impossi-
ble. Malouf (2000) examined the first million sen-
tences of the British National Corpus and found
only one sequence of adjectives for every twenty
sentences. With sequences of adjectives occurring
so rarely, the chances of finding information on
any particular sequence is small. The data is just
too sparse.
4 Towards a Solution
To create a flexible system capable of predicting a
wide variety of orderings, we used several corpora
to build broad modifier classes. Modifiers are clas-
sified by where they tend to appear prenominally,
and ordering constraints between the classes de-
termine the order for any set of modifiers. This
system incorporates three main ideas:
1. Not all modifiers have equally stringent or-
dering preferences.
2. Modifier ordering preferences can be learned
empirically.
3. Modifiers can be grouped into classes indica-
tive of their ordering preferences.
The classification scheme therefore allows rigid
as well as more loose orders (compare big red
ball and ?red big ball with white floppy hat and
floppy white hat). It is not based on any mapping
between position and semantics, morphology, or
phonology, but does not exclude any such rela-
tionship in the classification: This classification
scheme builds on what there is direct evidence for,
independent of why each modifier appears where
it does.
To create our model, all simplex noun phrases
(NPs) are extracted from parsed corpora. A sim-
plex NP is defined as a maximal noun phrase
that includes premodifiers such as determiners and
possessives but no post-nominal constituents such
as prepositional phrases or relative clauses (Shaw
and Hatzivassiloglou, 1999: 137). From these
simplex NPs, we extract all those headed by a
noun and preceded by only prenominal modifiers.
This includes modifiers tagged as adjectives (JJ),
nouns (NN), gerunds (VBG), and past participles
(VBN). The counts and relative positions of each
modifier are stored, and these are converted into
position probabilities in vector file format. Modi-
fiers are classified based on the positions in which
they have the highest probabilities of occurring.
Examples of the intermediary files in this pro-
cess are given in Tables 1 and 2. Table 1 lists
modifiers followed by their frequency counts in
each prenominal position. Table 2 lists these mod-
ifiers associated to their classes, with the propor-
tions that determine the class.
wealthy four 2 three 2 two 3 one 1
red four 13 three 35 two 35 one 21
golden four 1 three 5 two 5 one 3
strongest four 5 three 5 two 5 one 5
Table 1: Example Modifier Classification Interme-
diate File: Step 3
5 Materials
To create the training and test data, we utilize the
Penn Treebank-3 (Marcus et al, 1999) releases of
52
wealthy two two 0.38
red two three three 0.34 two 0.34
golden one two three three 0.33 two 0.33 one 0.29
strongest two three four four 0.33 three 0.33 two 0.33
Table 2: Example Modifier Classification Interme-
diate File: Step 4
the parsed Wall Street Journal corpus, the parsed
Brown corpus, and the parsed Switchboard cor-
pus. The Wall Street Journal corpus is a selec-
tion of over one million words collected from the
Wall Street Journal over a three-year period. The
Brown corpus is over one million words of prose
written in various genres, including mystery, hu-
mor, and popular lore, collected from newspapers
and periodicals in 1961. The Switchboard corpus
is over one million words of spontaneous speech
collected from thousands of five-minute telephone
conversations. Several programs were constructed
to analyze the information provided by these data.
The details of each module are outlined below.
5.1 Code Modules
The following five components were developed (in
Python) for this project.
Modifier Extractor ? This program takes as in-
put a parsed corpus, and outputs a list of all
occurrences of all noun phrases in that cor-
pus.
input: Parsed Corpus
output: List of simplex NPs
Modifier Organizer ? This program takes as in-
put a list of simplex NPs and filters out words
that appear prenominally and are occasion-
ally mistagged as modifiers. A list of these
filtered words is available in Table 3. This
returns a vector with frequency counts for
all positions in which each observed modifier
occurs.
input: Modifier-rich noun phrases and their
frequencies
output: Vector file with distributional infor-
mation for each modifier position
Modifier Classifier ? This program takes as in-
put a vector file with distributional informa-
tion for each modifier?s position, and from
this builds our model by determining the clas-
sification for each modifier.
about behind on
above in under
after inside out
outside up over
down like past
near through off
the a
Table 3: Filtered Mistagged Words
input: Vector file with distributional infor-
mation for each modifier position
output: Ordering model: File with each
modifier associated to a class
Prenominal Modifier Ordering Predictor ?
This program takes as input two files: an or-
dering model and a list of simplex NPs (for
testing). The program then uses the model
to assign a class to each modifier seen in the
testing data, and predicts the ordering for all
the modifiers that appear prenominally. A
discussion of the ordering decisions is given
below. This program then compares its pre-
dicted ordering of modifiers prenominally to
the observed ordering of modifiers prenom-
inally. It returns precision and recall values
for its predictions.
input: Vector file with each modifier associ-
ated to a class, list of simplex NPs
output: Precision and recall for modifier or-
dering predictions
6 Method
6.1 Classification Scheme
To develop modifier classes and create our model,
we assume four primary modifier positions. This
assumption is based on the idea that people rarely
produce more than four modifiers before a noun.
This assumption covers 99.70% of our data (see
Table 5). The longest noun phrases for this ex-
periment are therefore those with five words: Four
modifiers followed by a noun.
small smiling white fuzzy bunny
four three two one
Figure 2: Example Simplex NP with Prenominal
Positions
Each modifier?s class is determined by counting
the frequency of each modifier in each position.
53
Class 1: one Class 6: two-three
Class 2: two Class 7: three-four
Class 3: three Class 8: one-two-three
Class 4: four Class 9: two-three-four
Class 5: one-two
Table 4: Modifier Classes
This is turned into a probability over all four posi-
tions. All position probabilities ? 0.25 (baseline)
are discarded. Those positions that remain deter-
mine the modifier class.
To calculate modifier position for each phrase,
counts were incremented for all feasible positions.
This is a way of sharing evidence among sev-
eral positions. For example, in the phrase clean
wooden spoon, the adjective clean was counted as
occurring in positions two, three, and four, while
the adjective wooden was counted as occurring in
positions one, two, and three.
The classification that emerges after applying
this technique to a large body of data gives rise
to the broad positional preferences of each modi-
fier. In this way, a modifier with a strict positional
preference can emerge as occurring in just one po-
sition; a modifier with a less strict preference can
emerge as occurring in three.
The final class for each modifier is dependent
on the positions the modifier appears in more than
25% of the time. Since there are four possible
positions, 25% is the baseline: A single modifier
preceding a noun has equal probability of being in
each of the four positions. There are nine derivable
modifier classes in this approach, listed in Table 4.
A diagram of how a modifier is associated to a
class is shown in Figure 3. In this example, red
appears in several simplex NPs. In each sequence,
we associate red to its possible positions within
the four prenominal slots. We see that red occurs
in positions one, two and three; two, three, and
four; and three and four. With only this data, red
has a 12.5% probability of being in position one; a
25% probability of being in position two; a 37.5%
probability of being in position three; and a 25%
probability of being in position four. It can there-
fore be classified as belonging to Class 3, the class
for modifiers that tend to occur in position three.
This kind of classification allows the system to
be flexible to the idea that some modifiers exhibit
stringent ordering constraints, while others have
more loose constraints. Some modifiers may al-
ways appear immediately before the noun, while
Figure 3: Constructing the Class of the Modifier
red
others may generally appear close to or far from
the noun. By counting the occurrences of each
modifier in each position, classes for all observed
modifiers may be derived.
The frequencies of all extracted groupings of
prenominal modifiers used to build our model are
shown in Table 5. The frequencies of the extracted
classes are shown in Table 6.
Mods Count Percentage
2 15856 88.90%
3 1770 9.92%
4 155 0.87%
5 21 0.12%
6 1 .03%
Table 5: Frequency of Prenominal Modifier
Amounts
Class Position Count Percentage
1 one 18 0.23%
2 two 46 0.68%
3 three 62 0.92%
4 four 21 0.31%
5 one-two 329 4.88%
6 two-three 1136 16.86%
7 three-four 261 3.87%
8 one-two-three 2671 39.65%
9 two-three-four 2193 32.55%
Table 6: Modifier Class Distribution
Modifiers of Class 8, the class for modifiers that
show a general preference to be closer to the head
noun but do not have a strict positional preference,
make up the largest portion of the data. An exam-
ple of a modifier from Class 8 is golden. The next
54
Class Position Generated Before Class
1 one 2 3 4 5 6 7 8 9
2 two 3 4 6 7 9
3 three 4 7
4 four
5 one-two 2 3 4 6 7 8 9
6 two-three 3 4 7 9
7 three-four 4
8 one-two-three 4 6 7 9
9 two-three-four 4 7
Table 7: Proposed Modifier Ordering
largest portion of the data are modifiers of Class 9,
the class for modifiers that show a general prefer-
ence to be farther from the head noun. An exam-
ple of a modifier from Class 9 is strongest. With
these defined, strongest golden arch is predicted
to sound grammatical and unmarked, but ?golden
strongest arch is not.
Some expected patterns also emerge in these
groupings. For example, green, yellow, red and
other colors are determined to be Class 6. Ex-
plained and unexplained are both determined to be
Class 5, and big and small are both determined to
be Class 9.
Once classified, modifiers may be ordered ac-
cording to their classes. The proposed ordering
constraints for these classes are listed in Table 7.
Note that using this classification scheme, the or-
dering of modifiers that belong to the same class
is not predicted. This seems to be reflective of nat-
ural language use. For example, both wealthy and
performing are predicted to be Class 2. This seems
reasonable; whether wealthy performing man or
performing wealthy man is a more natural order-
ing of prenominal modifiers is at least debatable.
The freedom of intra-class positioning allows for
some randomization in the generation of prenom-
inal modifiers, where other factors may be used to
determine the final ordering.
6.2 Evaluation
In order to test how well the proposed system
works, 10-fold cross-validation was used on the
extracted corpora. The held-out data was selected
as random lines from the corpus, with a list stor-
ing the index of each selected line to ensure no
line was selected more than once. In each trial,
modifier classification was learned from 90% of
the data and the resulting model was used to pre-
dict the prenominal ordering of modifiers in the
remaining 10%.
The modifiers preceding each noun were stored
in unordered groups, and the ordering for each un-
ordered prenominal modifier pair {a,b} was pre-
dicted based on the classes of the modifiers in
our model. The ordering predictions followed the
constraints listed in Table 7. When the class was
known for one modifier but not for the other, the
two modifiers were ordered based on the class of
the known modifier: Modifiers in Classes 1, 2, 5,
and 8 were placed closer to the head noun than the
unknown modifier, while modifiers in Classes 3,
4, 7, and 9 were placed farther from the head noun
than the unknown modifier. If the known modifier
was of Class 6 (occurring in position two-three), a
random guess decided the ordering. This reflects
the idea that Classes 1, 2, 5, and 8 are all classes
for modifiers that broadly prefer to be closer to
the head noun, while Classes 3, 4, 7, and 9 are
all classes for modifiers that broadly prefer to be
farther from the head noun.
In the context of classification tasks, precision
and recall measurements provide useful informa-
tion of system accuracy. Precision, as defined in
(7), is the number of true positives divided by the
number of true positives plus false positives. This
is calculated here as tp/(tp + fp), where tp is the
number of orderings that were correctly predicted,
and fp is the number of orderings not correctly pre-
dicted. This measure provides information about
how accurate the modifier classification is. Recall,
as defined in (8), is the number of true positives
divided by the number of true positives plus false
negatives. This is calculated here as tp/(tp + fn),
where tp is the number of orderings that were cor-
rectly predicted, and fn is the total number of or-
derings that could not be predicted by our system.
This measure provides information about the pro-
portion of modifiers in the training data that can be
correctly ordered.
(7) Precision = tp/(tp + fp)
tp = number of orderings correctly predicted
fp = number of orderings not correctly
predicted
(8) Recall = tp/(tp + fn)
tp = number of orderings correctly predicted
fn = number of orderings that could not be
predicted
55
Precision Recall
Token 89.63% (0.02) 74.14% (0.03)
Type 90.26% (0.02) 67.17% (0.03)
Table 8: Precision and Recall for Prenominal
Modifier Ordering
7 Results
Results are shown in Table 8. Our model pre-
dicts the correct order for 89.63% of unordered
modifiers {a,b} for which an ordering decision
can be made, making correct predictions for
74.14% of all unordered modifiers in the test data.
The system also correctly predicts 90.26% of the
unordered modifier {a,b} types in the test data for
which an ordering decision can be made. This
covers 67.17% of the modifier pair types in the
test data. This lower value appears to be due to
the large amount of modifier pairs that are in the
data only once.
The values given are averages over each trial.
The standard deviation for each average is given
in parentheses. On average, 191 modifier pairs
were ordered in each trial, based on the assigned
orders of 273 individual modifiers, with an aver-
age of 23.01% of the modifiers outside of the vo-
cabulary in each trial.
The system precision and recall here are compa-
rable to previously reported results (see Section 2).
Extending our analysis over entire simplex NPs,
where we generate all possible orderings based on
our system constraints, we are able to predict an
average of 94.44% of the sequences for which a
determination can be made. This is a correct pre-
diction for 78.59% of all the simplex NPs in the
data.
Previous attempts have achieved very poor re-
sults when testing their models on a new domain.
We conclude our analysis by testing the accuracy
of our models on different domains. To do this, we
combine two corpora to build our model and then
test this model on the third.
Combining the WSJ corpus and the Brown cor-
pus to build our modifier classes and then testing
on the Switchboard (Swbd) corpus, we achieve
quite promising results. Our token precision is
89.57% and our type precision is 94.17%. How-
ever, our recall values are much lower than those
reported above (63.47% and 58.18%). Other train-
ing and testing combinations follow this pattern:
A model built from the Switchboard corpus and
Training Testing Token Token
Corpus Corpus Precision Recall
Brown+WSJ Swbd 89.57% 63.47%
Swbd+WSJ Brown 82.75% 57.14%
Swbd+Brown WSJ 79.82% 39.55%
Training Testing Type Type
Corpus Corpus Precision Recall
Brown+WSJ Swbd 94.17% 58.18%
Swbd+WSJ Brown 87.00% 51.18%
Swbd+Brown WSJ 82.43% 27.16%
Table 9: Precision and Recall for Prenominal
Modifier Ordering of a New Domain
the WSJ corpus achieves 82.75% token precision
and 87% type precision when tested on the Brown
corpus (57.14% token recall, 51.18% type recall),
while a model built from the Switchboard corpus
and the Brown corpus achieves 79.82% token pre-
cision and 82.43% type precision when tested on
the WSJ corpus (39.55% token recall and 27.16%
type recall).
8 Discussion
The system precision is comparable to previously
reported results. The results show that order-
ing modifiers based on this classification system
can aid in generating simplex noun phrases with
prenominal modifiers ordered in a way that sounds
natural. We now turn to a discussion of areas for
future work.
It seems reasonable that the classes for previ-
ously unseen modifiers could be developed based
on the known classes of surrounding modifiers.
This system lends itself to bootstrapping, where
a lexical acquisition task that constructed class
probabilities based on the surrounding context
could classify previously unseen modifiers:
grey shining metallic chain
three-four unknown one-two head-noun
Given its position and the classes of the surround-
ing modifiers, unknown could be two-three.
Grouping modifiers into classes that determine
their order also lends itself to incorporation into
generative grammars. For example, Head-driven
Phrase Structure Grammar (Sag et al, 2003),
a constraint-based grammatical framework that
groups lexical items into broader classes, could
utilize the classes proposed here to determine
modifier positions prenominally. Advancing re-
56
search in this area could help grow the generative
capabilities of class-based grammars.
It bears mentioning that this same system was
attempted on the Google Web 1T 5-Gram corpus
(Brants and Franz, 2006), where we used WordNet
(Miller et al, 2006) to extract sequences of nouns
preceded by modifiers. The precision and recall
were similar to the values reported here, however,
the proportions of prenominal modifiers belied a
problem in using such a corpus for this approach:
82.56% of our data had two prenominal modifiers,
16.79% had four, but only 0.65% had three. This
pattern was due to the many extracted sequences
of modifiers preceding a noun that were not actu-
ally simplex NPs. That is, the 5-Grams include
many sequences of words in which the final one
has a use as a noun and the earlier ones have uses
as adjectives, but the 5-Gram itself may not be a
noun phrase. We found that many of our extracted
5-Grams were actually lists of words (for example,
Chinese Polish Portuguese Romanian Russian was
observed 115 times). In the future, we would like
to examine ways to use the 5-Gram corpus to sup-
plement our system.
The results reported here are encouraging, and
we hope to continue this work on a parsed version
of the Gutenberg corpus (Hart, 2009). This cor-
pus is a collection of text versions of novels and
other written works, and is available online. Using
a corpus of modifier-rich text such as this would
aid the system in classifying a greater number of
modifiers. Further work should also test how ro-
bust the acquisition of unseen modifiers is using
these classes, and examine implementing this or-
dering system into a language generation system.
References
Otto Behaghel. 1930. Von Deutscher Wortstellung,
volume 44. Zeitschrift Fu?r Deutschen, Unterricht.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structures. In J. R. Hayes, editor, Cogni-
tion and the Development of Language. Wiley, New
York.
Gemma Boleda and Laura Alonso. 2003. Cluster-
ing adjectives for class acquisition. In Proceedings
of the EACL?03 Student Session, pages 9?16, Bu-
dapest.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1. http://www.ldc.upenn.edu. Lin-
guistic Data Consortium.
H. H. Clark and E. V. Clark. 1976. Psychology
and language: An introduction to psycholinguistics.
Harcourt Brace Jovanovich, New York.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
M.A.K. Halliday and Christian Matthiessen. 1999.
Construing experience as meaning: a language-
based approach to cognition. Cassell, London.
Michael Hart. 2009. Project Gutenberg collection.
http://www.gutenberg.org. Project Gutenberg.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Robert Malouf. 2000. The order of prenominal ad-
jectives in natural language generation. In Proceed-
ings of the 38th Annual Meeting of the Association
for Computational Linguistics, pages 85?92, Hong
Kong.
Christopher D. Manning. 1993. Automatic acquisi-
tion of a large subcategorization dictionary from cor-
pora. In Meeting of the Association for Computa-
tional Linguistics, pages 235?242.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium.
George A. Miller, Christiane Fellbaum, Randee Tengi,
PamelaWakefield, Helen Langone, and Benjamin R.
Haskell. 2006. WordNet: A lexical database for the
english language.
Ivan Sag, Tom Wasow, and Emily Bender. 2003. Syn-
tactic Theory: A Formal Introduction. CSLI Publi-
cations, Stanford University.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 135?143, Morristown, NJ, USA. Association
for Computational Linguistics.
Kees van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1?11.
Paul Ziff. 1960. Semantic Analysis. Cornell Univer-
sity Press, Ithaca, New York.
57
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643?1654,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Open Domain Targeted Sentiment
Margaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
{m.mitchell,jacqui.aguilar}@jhu.edu, Theresa.Wilson@oberlin.edu, vandurme@cs.jhu.edu
Abstract
We propose a novel approach to sentiment
analysis for a low resource setting. The in-
tuition behind this work is that sentiment
expressed towards an entity, targeted senti-
ment, may be viewed as a span of sentiment
expressed across the entity. This represen-
tation allows us to model sentiment detec-
tion as a sequence tagging problem, jointly
discovering people and organizations along
with whether there is sentiment directed to-
wards them. We compare performance in
both Spanish and English on microblog data,
using only a sentiment lexicon as an exter-
nal resource. By leveraging linguistically-
informed features within conditional random
fields (CRFs) trained to minimize empiri-
cal risk, our best models in Spanish signifi-
cantly outperform a strong baseline, and reach
around 90% accuracy on the combined task of
named entity recognition and sentiment pre-
diction. Our models in English, trained on a
much smaller dataset, are not yet statistically
significant against their baselines.
1 Introduction
Sentiment analysis is a multi-faceted problem. De-
termining when a positive or negative sentiment is
being expressed is a large part of the challenge, but
identifying other attributes, such as the target of the
sentiment, is also crucial if the ultimate goal is to
pinpoint and extract opinions. Consider the exam-
ples below, all of which contain a positive sentiment:
(1) So happy that Kentucky lost to Tennessee!
(2) Kentucky versus Kansas I can hardly wait...
(3) Kentucky is the best alley-oop throwing team
since Sherman Douglas? Syracuse squads!!
The entities in these examples are college basket-
ball teams, and the events referred to are games. In
(1), although there is a positive sentiment, the tar-
get of the sentiment is an event (Kentucky losing to
Tennessee). However, from the positive sentiment
toward this event, we can infer that the speaker has
a negative sentiment toward Kentucky and a positive
sentiment toward Tennessee. In (2), the positive sen-
timent is toward a future event, but we are not given
enough information to infer a sentiment toward the
mentioned entities. In (3), Kentucky is the direct
target of the positive sentiment. We can also in-
fer a positive sentiment toward Douglas?s Syracuse
teams, and even toward Douglas himself.
These examples illustrate the importance of the
target when interpreting sentiment in context. If we
are looking for sentiments toward Kentucky, for ex-
ample, we would want to identify (1) as negative, (2)
as neutral (no sentiment) and (3) as positive. How-
ever, if we are looking for sentiment toward Ten-
nessee, we would want to identify (1) as positive,
and (2) and (3) as neutral.
The expression of these and other kinds of sen-
timent can be understood as involving three items:
(1) An experiencer
(2) An attitude
(3) A target (optionally)
Research in sentiment analysis often focuses on (2),
predicting overall sentiment polarity (Agarwal et al,
2011; Bora, 2012). Recent work has begun to com-
bine (2) with (3), examining how to automatically
predict the sentiment polarity expressed towards a
target entity (Jiang et al, 2011; Chen et al, 2012)
for a fixed set of targets. This topic-dependent sen-
timent classification requires that the target entity be
1643
Figure 1: Sentiment expressed across an entity.
given, and returns statements expressing sentiment
towards the given entity.
In this paper, we take a step towards open-domain,
targeted sentiment analysis by investigating how to
detect both the named entity and the sentiment ex-
pressed toward it. We observe that sentiment ex-
pressed towards a target entity may be possible to
learn in a graphical model along the span of the en-
tity itself: Similar to how named entity recognition
(NER) learns labels along the span of each word in
an entity name, sentiment may be expressed along
the entity as well. A small example is shown in Fig-
ure 1. We focus on people and organizations (voli-
tional named entities), which are the primary targets
of sentiment in our microblog data (see Table 1).
Both NER and opinion expression extraction have
achieved impressive results using conditional ran-
dom fields (CRFs) (Lafferty et al, 2001) to define
the conditional probability of entity categories (Mc-
Callum and Li, 2003; Choi et al, 2006; Yang and
Cardie, 2013). We develop such models to jointly
predict the NE and the sentiment expressed towards
it using minimum risk training (Stoyanov and Eis-
ner, 2012). We learn our models on informal Span-
ish and English language taken from the social net-
work Twitter,1 where the language variety makes
NLP particularly challenging (see Figure 2).
Our ultimate goal is to develop models that will
be useful for low resource languages, where a sen-
timent lexicon may be known or bootstrapped, but
more sophisticated linguistic tools may not be read-
ily available. We therefore do not rely on an external
part-of-speech tagger or parser, which are often used
for features in fine-grained sentiment analysis; such
tools are not available in many languages, and if they
are, are not usually adapted for noisy social media.
Instead, we use information from sentiment lex-
icons and some simple hand-written features, and
otherwise use only features of the word that can be
1www.twitter.com
@[user] le dijo erralo muy por lo bajo jaja un grande
juancito grandes amigos mios
@[user] he told him it was very on the dl haha a great
juancito great friends of mine
@[user] buenos d??as Profe!! Nos quedamos acciden-
tados otra vez en la carretera vieja guarenas echando
gasoil, estamos a la interperie
@[user] good morning, Prof!! We were wrecked again
on the old guarenas highway while getting diesel, we?re
out in the open
Sin a?nimo de ofender a los Militares, que realmente
se merecen ese aumento y ma?s. Pero, do?nde queda la
misma recompensa para Me?dicos.
I do not intend to offend the military in the slightest,
they truly deserve the raise and more. However, I?m
wondering whether doctors will ever receive a similar
compensation.
Figure 2: Messages on Twitter use a wide range of
formality, style, and errors, which makes extracting in-
formation particularly difficult. Examples from Spanish
(screen names anonymized), with approximate transla-
tions in English.
extracted without supervision. These include fea-
tures based on unsupervised word tags (Brown clus-
ters) and a method that automatically syllabifies a
word based on the orthography of the language. All
tools and code used for this research are released
with this paper.2
2 Related Work
As the scale of social media has grown, using
sources such as Twitter to mine public sentiment
has become increasingly promising. Commer-
cial systems include Sentiment1403 (products and
brands) and tweetfeel4 (suggests searching for pop-
ular movies, celebrities and companies).
The majority of academic research has focused on
supervised classification of message sentiment irre-
spective of target (Barbosa and Feng, 2010; Pak and
Paroubek, 2010; Bifet and Frank, 2010; Davidov et
al., 2010; Kouloumpis et al, 2011; Agarwal et al,
2011). Large datasets are collected for this work by
leveraging the sentiment inherent in emoticons (e.g.,
smilies and frownies) and/or select Twitter hashtags
(e.g., #bestdayever, #fail), resulting in noisy collec-
2www.m-mitchell.com/code
3www.sentiment140.com
4www.tweetfeel.com
1644
tions appropriate for initial exploration. Prior work
includes: the use of a social network (Speriosu et
al., 2011; Tan et al, 2011; Calais Guerra et al,
2011; Jiang et al, 2011; Li et al, 2012; Hu et
al., 2013); user-adapted models based on collabo-
rative online-learning (Li et al, 2010b); unsuper-
vised, joint sentiment-topic modeling (Saif et al,
2012); tracking changing sentiment during debates
(Diakopoulos and Shamma, 2010); and how ortho-
graphic conventions such as word-lengthening can
be used to adapt a Twitter-specific sentiment lexicon
(Brody and Diakopoulos, 2011).
Efforts in targeted sentiment (Bermingham and
Smeaton, 2010; Jin and Ho, 2009; Li et al, 2010a;
Jiang et al, 2011; Tan et al, 2011; Wang et al,
2011; Li et al, 2012; Chen et al, 2012), have mostly
focused on topic-dependent analysis. In these ap-
proaches, messages are collected on a fixed set of
topics/targets, such as products or sports teams, and
sentiment is learned for the given set. In contrast,
we aim to predict sentiment in tweets for any named
person or organization. We refer to this task as open
domain targeted sentiment analysis.
Within topic-dependent sentiment analysis, sev-
eral approaches have explored applying CRFs or
HMMs to extract sentiment and target words from
text (Jin and Ho, 2009; Li et al, 2010a). In these
approaches, opinion expressions are extracted, and
polarity is annotated across the opinion expression.
However, as noted by many researchers in senti-
ment, opinion orientation towards a specific target
is often not equal to the orientation of a neighbor-
ing opinion expression; and opinion expressions in
one context may not be opinion expressions in an-
other (Kim and Hovy, 2006), making open domain
approaches particularly challenging.
The above work by Jiang et al (2011) is most
similar to our own. They do not use joint learning,
but they do incorporate a number of parse-based fea-
tures designed to capture relationships between sen-
timent terms and topic references. In our work these
relationships are captured by the CRF model, and
we compare against their approach in Section 6.
Recent work by Yang and Cardie (2013) is sim-
ilar in spirit to our own, where the identification
of opinion holders, opinion targets, and opinion ex-
pressions is modeled as a sequence tagging problem
using a CRF. However, similar to previous work ap-
plying CRFs to extract sentiment, Yang and Cardie
use syntactic relations to connect an opinion target
to an opinion expression. In contrast, we model
the expression of sentiment polarity across the senti-
ment target itself, extracting both the sentiment tar-
get and the sentiment expressed towards it within the
same span of words. This allows us to use surround-
ing context to determine sentiment polarity without
identifying explicit opinion expressions or relying
on a parser to help link expression to target.
Most work in targeted sentiment outside the mi-
croblogging domain has been in relation to prod-
uct review mining (e.g., Yi et al (2003), Hu and
Liu (2004), Popescu and Etzioni (2005), Qiu et al
(2011)). Rather than identify named entities (NEs),
this work seeks to identify products and their fea-
tures mentioned in reviews, and classify these for
sentiment. Recent work by Qui et al jointly learns
targets and opinion words, and Jakob and Gurevych
(2010) use CRFs to extract the targets of opinions,
but do not attempt to classify the sentiment toward
these targets. To the best of our knowledge, this is
the first work to approach targeted sentiment in a low
resource setting and to jointly predict NEs and tar-
geted sentiment.
3 Data
Twitter Collection We use the Spanish/English
Twitter dataset of Etter et al (2013) to train and test
our models. Approximately 30,000 Spanish tweets
and 10,000 English were labeled for named entities
in BIO encoding: The start of an NE is labeled B-
{NE} and the rest of the NE is labeled I-{NE}. The
NE COUNT NEUTRAL POS NEG
PERSON 5462 80% 20% 0%
ORGANIZATION 4408 80% 20% 0%
LOCATION 1405 100% 0% 0%
URL 1030 100% 0% 0%
TIME 535 70% 10% 20%
DATE 222 100% 0% 0%
MONEY 95 90% 0% 10%
PERCENT 81 80% 20% 0%
TELEPHONE 23 100% 0% 0%
EMAIL 8 100% 0% 0%
Table 1: Distribution of named entities in our Spanish
Twitter corpus. Targeted sentiment percentages are based
on expert annotations from a random sample of 10 (or
all) of of each entity. Most entities are not sentiment tar-
gets (NEUTRAL). PERSON and ORGANIZATION are most
frequent, and among the top recipients of sentiment.
1645
full set of NE categories are shown in Table 1. For
example, the sequence ?Mark Twain? would be la-
beled B-PERSON, I-PERSON. We are interested in both
PERSON and ORGANIZATION entities, which make
up the majority of named entities in this data, and we
evaluate these using the more general entity category
VOLITIONAL. Removing retweets, 7,105 Spanish
tweets contained a total of 9,870 volitional entities
and 2,350 English tweets contained a total of 3,577
volitional entities.
Sentiment Lexicons We use two sentiment lex-
icon sources in each language. For English, we
use the MPQA lexicon (Wilson et al, 2005), which
identifies 12,296 manually and semi-automatically
produced subjective terms along with their polarity.
For the second lexicon, we use SentiWordNet 3.0
(Baccianella et al, 2010), which assigns positive and
negative polarity scores to WordNet synsets. We use
the majority polarity of all words with a subjectivity
score above 0.5.
For Spanish, the first lexicon is obtained from
Volkova et al (2013), who automatically trans-
lated strongly subjective terms from the MPQA lex-
icon (Wilson et al, 2005) into Spanish. The re-
sulting Spanish lexicon contains about 65K words.
The second lexicon is available from Perez-Rosas
et al (2012). This contains approximately 1000
sentiment-bearing words collected leveraging man-
ual resources and 2000 collected leveraging auto-
matic resources.
Annotation To collect sentiment labels, we
use crowdsourcing through Amazon?s Mechanical
Turk.5 Annotators (?Turkers?) were shown six
tweets at a time, each with a single highlighted
named entity. Turkers were instructed to (1) se-
lect the sentiment being expressed towards the en-
tity (positive, negative, or no sentiment); and (2)
rate their level of confidence in their selection. Fol-
lowing best practices on collecting language data
with Mechanical Turk (Callison-Burch and Dredze,
2010), two controls were placed among each set of
six tweets to screen out unreliable judgments. An
example prompt is shown in Figure 3.
Each ?tweet, NE? pair was shown to three Turk-
ers, and those with majority consensus on sentiment
polarity were extracted. Tweets without sentiment
5www.mturk.com/mturk
ORGANIZATION PERSON
Named Entity
Freq
uenc
y in T
weet
s
0
500
1000
1500
2000
2500
Positive
Negative
Neutral
Figure 4: Targeted sentiment annotated for Spanish.
Majority
POS NEUTRAL NEG
M
in
or
it
y POS 757 1249 130
NEUTRAL 707 2151 473
NEG 129 726 452
Table 2: Number of targeted sentiment instances where
at least two of the three annotators (Majority) agreed.
Common disagreements with a third annotator (Minority)
were over whether no sentiment or positive sentiment was
expressed, and whether no sentiment or negative sent-
ment was expressed.
consensus on all NEs were removed. In Spanish, this
yielded 6,658 unique ?tweet, NE? pairs. In English,
which is a smaller data set, this yielded 3,288 unique
pairs. We split the data into folds for 10-fold cross-
validation, developing on the data from one fold and
reporting results for the remaining nine.
The distribution of sentiment for the named en-
tities annotated by Turkers is shown in Figure 4.
Neutral (no targeted sentiment) dominates, followed
by positive sentiment for both organizations and
people. As shown in Table 2, common disagree-
ments were over whether or not there was targeted
positive sentiment, and whether or not there was
targeted negative sentiment. This is in line with
previous research showing that distinguishing pos-
itive sentiment from no sentiment (and distinguish-
ing negative sentiment from no sentiment) is often
more challenging than distinguishing between pos-
itive and negative sentiment (Wilson et al, 2009).
Indeed, we see that it was more common for annota-
tors to disagree than to agree on targeted sentiment,
particularly for negative targeted sentiment, where
more instances had NEUTRAL/NEGATIVE disagree-
ment than NEGATIVE three-way agreement.
1646
Figure 3: Example Tweet shown to Turkers.
Variable Possible values
Sentiment (s) NOT-TARG, SENT-TARG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+SENT-TARG, I+SENT-TARG
Table 3: Possible values for random variables, targeted
subjectivity (is/is not sentiment target). COLL models
collapse targeted subjectivity and NE label into one node.
Variable Possible values
Sentiment (s) NOT-TARG, POS, NEG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+POS, I+POS
B+NEG, I+NEG
Table 4: Possible values for random variables, targeted
sentiment. The COLL models collapse both targeted sen-
timent and NE label into one node.
4 Targeted Subjectivity and Sentiment
Formally, we define the problem as follows: Given
an observed message w = (w1 . . . wn), where n is
the number of words in the message and wj(1 ?
j ? n) is a word, we learn the probability of a
label sequence l = (l1 . . . ln), where li ? the set
of named entity values; and a sentiment sequence
s = (s1 . . . sn), where si ? the set of sentiment val-
ues. We additionally explore simpler linear-chain
models that learn the probability of a single label
sequence y = (y1 . . . yn), where yi ? the set of con-
joined entity+sentiment values (Tables 3 and 4).
Our basic model is a linear conditional random
field, an undirected graph that represents the con-
ditional distribution p(l, s|w).6 Sentiment towards
a named entity may be modeled in a CRF as a se-
6For the COLL models, this is instead the conditional distri-
bution p(y|w), where entity and sentiment labels are conjoined
in one sequence assignment y.
quence of random variables for sentiment s con-
nected to named entities l. In all models, entity vari-
ables are connected by a factor to their neighbors
in sequence, and we include skip-chains (Finkel and
Manning, 2010) connecting identical words where
at least one is capitalized. Our model strategies in-
clude: a pipeline that first learns volitional entities
then sentiment directed towards them (PIPE); one
that jointly learns volitional entities along with sen-
timent directed towards them (JOINT); and one that
learns volitional entities and targeted sentiment with
combined labels (COLL) (Figure 5).
Using these models, we explore two primary
tasks: (1) the task of detecting whether sentiment
is targeted at an entity, which we refer to as targeted
subjectivity; and (2) the task of detecting whether
positive, negative, or neutral sentiment (no senti-
ment) is targeted at an entity, which we refer to as
targeted sentiment. Moving from targeted subjectiv-
ity prediction to targeted sentiment prediction is pos-
sible by changing the sentiment target (SENT-TARG)
variable into two variables, one for positive targeted
sentiment (POS) and one for negative (NEG). Possi-
ble values for targeted subjectivity are shown in Ta-
ble 3, and possible values for targeted sentiment are
shown in Table 4.
In the pipeline models (PIPE), we first build a
CRF where each word is connected by a factor to
an entity label li ? l. In a second model, every ob-
served volitional entity node is connected by a factor
to a sentiment label si ? s. An example is shown in
Figure 5 (1).
In the joint models (JOINT), each si ? s is con-
nected by a factor to the corresponding entity label
in the sequence, li ? l. Sentiment in this model
is partially observed: All sentiment variables are
treated as latent except for the sentiment connected
to the volitional entity. An example is shown in Fig-
ure 5 (2).
1647
In the collapsed models (COLL), we combine sen-
timent and named entity into one label sequence
(e.g., O, B+SENT-TARG, I+SENT-TARG). An example
is shown in Figure 5 (3). The JOINT and PIPE mod-
els therefore predict named entity sequences, their
category labels, and the sentiment expressed towards
volitional named entities.7 The collapsed models
predict volitional labels and targeted sentiment as
combined categories. The COLL and PIPE models
are considerably faster than JOINT models, where
exact inference is intractable.
1. PIPELINE MODEL (PIPE)
Step 1: Volitional Named Step 2: Sentiment
Entity Recognition
2. JOINT MODEL 3. COLLAPSED MODEL
(JOINT) (COLL)
Figure 5: Example CRFs for targeted subjectivity with
observed variables (dark nodes), predicted variables
(white nodes) and hidden variables (light grey nodes).
5 Training
Minimum-Risk CRF Training We use the
ERMA system (Stoyanov et al, 2011) to learn our
models.8 ERMA (Empirical Risk Minimization un-
der Approximations) learns parameters to minimize
loss on the training data. Predicting NE labels using
a linear-chain CRF trained with empirical risk mini-
mization has been shown to result in a statistically
significant improvement over the common approach
of maximum likelihood estimation (Stoyanov and
Eisner, 2012). All models are trained to optimize
7We found that learning the VOLITIONAL categories dur-
ing training rather than maintaining beliefs about separate
named entities during inference (ORGANIZATION, PERSON)
and then post-processing to VOLITIONAL leads to slightly bet-
ter accuracy.
8sites.google.com/site/ermasoftware
log likelihood using 20 iterations of stochastic
gradient descent, and a maximum of 100 iterations
of belief propagation to compute the marginals for
each example.
Features Features of the models are shown in Ta-
ble 5. For an observed word, features are extracted
for the word itself as well as within a context win-
dow of three words in either direction. Words seen
only once are treated as out-of-vocabulary. Surface
features and linguistic features are concatenated in
groups of two and three to create further features.
All algorithms and code that we have developed for
feature extraction are available online.9
Because we aim to develop models that do not
heavily rely on language-specific resources, we are
interested in exploring unsupervised and lightly
supervised methods for learning relevant features.
Rather than use part-of-speech tags, we therefore
use Brown cluster labels as unsupervised word tags
(Brown et al, 1992; Koo et al, 2008). Brown
clustering is a distributional similarity method that
merges pairs of word clusters in the training data10
to create the smallest decrease in corpus likelihood,
using a bigram language model on the clusters. For
our task, we cut clusters at length 3 and length 5,
and these serve as rough part-of-speech tags without
the need to train additional models. For example,
the word hello is tagged as belonging to cluster 011
(length 3) and 01111 (length 5).
During development, we found that being able
to syllabify the word (break the word into sylla-
bles) was a positive indicator of people names, but
a negative indicator of organization names. This
observation can be approximated automatically us-
ing constraints from the sonority sequencing princi-
ple (Hooper, 1976; Clements, 1990; Blevins, 1996;
Morelli, 2003) on a language?s orthography. This
is a phonotactic principle that states that syllables
will tend to have a sonority peak, usually a vowel,
in the center of the syllable, followed on either side
by consonants with decreasing sonority. Although
languages may violate this principle, the core idea
that a vowel forms the nucleus of a syllable with op-
9www.m-mitchell.com/code
10For Spanish, we train on a sample of ?7 million Spanish
tweets. For English, we train on the essays (Pennebaker et al,
2007) and Facebook data (Kosinskia et al, 2013) available from
ICWSM 2013.
1648
tional consonants before (the onset) and after (the
coda) can be used to begin to automatically learn
syllable structure.11 We learn this in an unsuper-
vised way, using the most frequent (seen more than
1,000 times) word-initial non-vowel sequences from
the Brown cluster data as allowable syllable onset
consonants. Similarly, the most frequent word-final
non-vowel sequences are learned as possible sylla-
ble codas. For each word, we then attempt to seg-
ment syllables using the learned onsets and codas
around each vowel. If a word cannot be syllabified,
it is often an initialism (e.g., CND, lsat).
We follow the approach from the out-of-
vocabulary assignment in the Berkeley parser
(Petrov et al, 2006) to encode common surface
patterns such as capitalization and lexical patterns
such as verb endings as a single feature for words
we have seen once or less. We also use the Jer-
boa toolkit (Van Durme, 2012) to extract further
language-independent features from the data, such
as features for emoticons and binning for repeated
characters (like !!!). In addition, we include features
for whether the word is three or four letters, which
is often used for acronyms and initialisms in several
languages (including Spanish and English); whether
the word is neighbored by a punctuation mark; word
identity; word length; message length; and position
in the sentence.
We utilize a speaker of each language to simply
list word forms for sentiment features that may be
indicative of sentiment, totaling less than two hours
of annotation time. This set includes intensifiers
(e.g., hella, freakin? in English; e.g., muy, suma-
mente in Spanish), positive/negative abbreviations
(WTF, pso), positive/negative slang words, and pos-
itive/negative prefix and suffixes (e.g., anti- in En-
glish and Spanish, -ito in Spanish).
6 Experiments
We are interested in both PERSON and ORGANIZA-
TION entities, and evaluate these in the collapsed
category VOLITIONAL. This suggests that the data
may be pre-processed to label all volitional entities
as VOLITIONAL NEs, or the models may be learned
with the traditional named entities in place, and post-
11Further development is necessary to extend a similar idea
to languages that do not ordinarily mark all vowels in their or-
thography, such as Hebrew and Arabic.
SURFACE FEATURES
binned word length, message length, and sen-
tence position; Jerboa features; word identity; word
lengthening; punctuation characters, has digit; has
dash; is lower case; is 3 or 4 letters; first letter capi-
talized; more than one letter capitalized, etc.
LINGUISTIC FEATURES
function words; can syllabify; curse words; laugh
words; words for good, bad, no, my; slang words; ab-
breviations; intensifiers; subjective suffixes and pre-
fixes (such as diminutive forms); common verb end-
ings; common noun endings
BROWN CLUSTERING FEATURES
cluster at length 3; cluster at length 5
SENTIMENT FEATURES
is sentiment-bearing word; prior sentiment polarity
Table 5: Features used in model.
processed to identify those that are VOLITIONAL.
We explored results using both methods, and found
that training models on VOLITIONAL tags yielded
the best performance overall; we report numbers for
this approach below.
We compare against a baseline (BASE-NS) where
we use our volitional entity labels and assign no
sentiment directed towards the entity (the majority
case). This is a strong baseline to isolate how our
methods perform specifically for the task of identi-
fying sentiment targeted at an entity.
We report on precision, recall, and sensitivity for
the tasks of NER and targeted subjectivity/sentiment
prediction in isolation; and we report on accuracy
for the targeted subjectivity and targeted sentiment
models. For sentiment, a true positive is an instance
where the label has sentiment, and a true negative is
an instance where the label has no sentiment (neu-
tral). For NER, a true positive is an instance where
the label is a B- or I- label; a true negative is an
instance where the label is O. The three systems
are evaluated against one another for NER, subjec-
tivity (entity has/does not have sentiment expressed
towards it), and sentiment (positive/negative/no sen-
timent) using paired t-tests across folds, with a Bon-
ferroni correction to set ? to 0.02.
NER We include results for the isolated task of vo-
litional named entity recognition in Table 6. In both
Spanish and English, all three models are roughly
comparable for precision, recall, and specificity. The
task of finding O tags ? spans that are not named en-
tities ? works especially well (NE spec). Common
1649
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
NE prec 65.2 64.3 65.1 59.8 62.3 60.5
NE rec 65.8 64.7 61.2 60.2 57.2 56.5
NE spec 95.4 95.2 95.6 94.3 95.1 94.7
Table 6: Average precision, recall, and specificity for vo-
litional entity NER (in %).
mistakes include confusing B- labels with I- labels.
Subjectivity and Sentiment Table 7 shows results
for the isolated task of predicting the presence of
sentiment about a volitional entity. In Spanish, the
pipeline models (PIPE) perform optimally for sub-
jectivity recall (Subj rec), and significantly above
the COLL models (p<.001). Precision and speci-
ficity are comparable across models. In English as
in Spanish, the collapsed model is particularly poor
at subjectivity recall.
As discussed in Section 2, the subtask of predict-
ing whether subjectivity is expressed towards an en-
tity is comparable to the main task of Jiang et al
(2011), and so we compare our approach here. The
Jiang et al study is similar to the current study in that
they aim to detect targeted sentiment, but it differs
from the current study in that they focus exclusively
on subjectivity towards five manually selected enti-
ties: {Obama, Google, iPad, Lakers, Lady Gaga}.
They also evaluate on artificially balanced evalu-
ation data, and evaluate sentiment polarity (posi-
tive/negative) separately from subjectivity (has/does
not have sentiment).
Our dataset includes any entity labeled as PERSON
or ORGANIZATION, and is not balanced (most tar-
gets have no sentiment expressed towards them; see
Table 1), thus we can only roughly compare against
their approach. Lakers and Lady Gaga are rare in
our collection (appearing less than 3 times), and so
we updated the comparison set prior to evaluation to:
{Obama, Google, iPad, BBC, Tebow}. On this set, a
baseline that always guesses no sentiment reaches an
accuracy of 66.9%, compared to Jiang et al?s 65.5%
accuracy on a balanced set (not strictly compara-
ble, but provided for reference). The JOINT mod-
els reach an accuracy of 71.04% on this set, demon-
strating this approach as potentially useful for topic-
dependent targeted sentiment.
Table 8 shows results for the task of predicting
the polarity of the sentiment expressed about an en-
tity. In Spanish, the PIPE models significantly out-
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Subj prec 58.3 58.8 58.9 46.6 52.2 45.9
Subj rec 40.1 50.9 19.1 44.5 48.5 16.4
Subj spec 79.6 77.5 77.8 77.6 80.8 74.0
Table 7: Average precision, recall, and specificity (in %)
for subjectivity prediction (has/does not have sentiment)
along the target entity.
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Sent prec 36.6 45.8 42.5 31.6 42.9 38.5
Sent rec 38.0 40.6 15.5 36.6 34.8 9.7
Sent spec 67.1 75.2 73.3 72.3 82.0 78.1
Table 8: Average precision, recall, and specificity (in %)
for sentiment prediction (positive/negative/no sentiment)
along the target entity.
perform the COLL models on sentiment recall, and
the JOINT models on sentiment precision (p<.01).
In English, PIPE significantly outperforms JOINT on
precision (p<.001).
Targeted Subjectivity and Targeted Sentiment
The JOINT and PIPE models work reasonably
well for the isolated tasks of NER and subjectiv-
ity/sentiment prediction. We now examine results
for targeted subjectivity ? labeling an entity and pre-
dicting whether there is sentiment directed towards
it ? in Table 9; and targeted sentiment ? labeling an
entity and predicting what the sentiment directed to-
wards it is ? in Table 10.
We evaluate using two accuracy metrics: Acc-all,
which measures the accuracy of the entire named en-
tity span along with the sentiment span; and Acc-
Bsent, which measures the accuracy of identifying
the start of a named entity (B- labels) along with
the sentiment expressed towards it. Acc-all primar-
ily measures the correctness of O labels, while Acc-
Bsent focuses on the beginning of named entities.
For the targeted subjectivity task, our JOINT mod-
els perform optimally in Spanish, and significantly
above their baselines. For the Acc-Bsent task, JOINT
models perform best, significantly outperforming
their baseline for subjectivity prediction. In English,
where our data is half the size, we do not see a statis-
tically significant difference between the predictive
models and the no sentiment baselines.
For the targeted sentiment task, the JOINT mod-
els again perform relatively well in Spanish (Table
10), labeling volitional entities, predicting whether
or not there is sentiment targeted towards them, and
1650
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.5* 89.3 89.3** 89.1 89.5* 89.3
Acc-Bsent 32.1*** 29.5 30.9*** 28.3 30.1** 28.1
E
ng Acc-all 88.0 88.1 88.6 88.6 87.9 88.1
Acc-Bsent 30.4 30.8 30.7 30.3 28.1 29.2
***p<.001 **p<.01 *p<.05
Table 9: Average accuracy on Targeted Subjectivity Pre-
diction: Identifying volitional entities and whether they
are a sentiment target. In the core task, Acc-Bsent, the
best model in Spanish is JOINT, significantly outperform-
ing the baseline. In English, the best model (PIPE) does
not significantly improve over its baseline.
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.4 89.4 89.0 89.0 89.2 89.3
Acc-Bsent 29.7* 29.0 30.0 29.2 28.9 29.0
E
ng Acc-all 88.0 88.1 88.2 88.4 87.7 88.1
Acc-Bsent 30.4 30.6 30.5 30.8 27.9 29.8
*p<.05
Table 10: Average accuracy on Targeted Sentiment Pre-
diction: Identifying volitional entities and the polarity
of the sentiment expressed towards them. The Spanish
JOINT models significantly improve over their baseline
for the core task. In English, no models outperform their
baseline.
the sentiment polarity above their no sentiment base-
lines. We find this to be the most difficult task: It
may be clear that sentiment is being expressed to-
wards an entity, but it is not always clear what the
polarity of that sentiment is. Error analysis is given
below in this section. In the smaller English set, the
models do not outperform the no sentiment baseline.
7 Discussion
Feature Analysis Examples of some of the top-
weighted features in the Spanish models are shown
in Table 11. In addition to lexical identity and Brown
cluster, we find that positive indicators include pos-
itive suffixes such as diminutive forms, whether the
word can be syllabized (Section 5), and whether it is
three or four letters.
Error Analysis Because it is relatively common
for there not to be sentiment targeted at a named en-
tity, it is difficult to tease out the polarity in instances
where there is targeted sentiment. Similarly, our pre-
dictions are most reliable for detecting the absence
of a named entity (O labels).
Label confusions are shown in Table 12. Mistakes
are often made by confusing B- labels (the start of
B-VOLITIONAL FEATURES
Negative is a function word; jerboa tags; followed by a word
with 3 or 4 letters that cannot be syllabified
Positive ends in -a, -o, or -s; is capitalized; has one non-
initial capital letter; is 3 or 4 letters
B-VOLITIONAL, POS FEATURES
Negative preceded by a curse word; followed by a word
with a positive suffix; immediately preceded by a
word with a negative prefix
Positive not in a sentiment lexicon; preceded by a happy
emoticon; followed by an exclamation or a ?my?
word; immediately preceded by a laugh; has two
or more sentiment-bearing words in the sentence
B-VOLITIONAL, NEG FEATURES
Negative is immediately followed by a question mark or
positive abbreviation word
Positive preceded by a ?bad? word or curse word; has four
or more sentiment lexicon items
B-VOLITIONAL, NOT-TARG FEATURES
Negative immediately followed by a ?no? word or word with
a negative prefix; is preceded by a question mark;
is immediately preceded by a curse word or laugh;
is followed by an exclamation mark
Positive not followed by sentiment lexicon word
Table 11: Example strongly weighted features for a
Spanish joint sentiment model. In addition to lexical
identity, we find that curse words and positive and neg-
ative prefixes are used to detect volitional entities and the
sentiment directed towards them.
an entity) with I- labels (inside an entity); and by
predicting sentiment polarity when the gold annota-
tions say there is not sentiment targeted at the entity.
Some example errors are shown in Figure 13. In
(1), ?CANSADO? (?TIRED?) was predicted to be
volitional, while ?Matthew? was not. In (2), ?Ma-
tias del r??o? was not predicted to be an entity, likely
due to the fact that the capitalization patterns we see
in this sentence are indicative of the start of a sen-
tence rather than a proper name (similar to 1). In (3),
a.
Observed
B I O
P
re
di
ct
ed B 423 21 186
I 36 236 135
O 197 90 7168
b.
Observed
POS NEG NEUT
POS 68 24 42
NEG 58 65 102
NEUT 115 61 468
Table 12: Predicted vs. observed values for a joint model.
(a) For named entities, most common confusions were
between B-VOLITIONAL and O labels. (b) For sentiment,
most common mistakes were to predict that a positive
sentiment was neutral (no sentiment), and that a neutral
sentiment was negative.
1651
NE prediction errors
1.
Spanish: Cuando estoy CANSADO , e?l es mi DESCANSO . Mateo . 11 : 29 .
Predicted: O O B-VOLITIONAL O O O O O O O O O O O O
Gold: O O O O O O O O O B-VOLITIONAL O O O O O
English: When I?m TIRED , he is my REST . Matthew . 11 : 29 .
2.
Spanish: Matias del r??o fue una lata . . .
Predicted: O O O O O O . . .
Gold: B-VOLITIONAL I-VOLITIONAL I-VOLITIONAL O O O . . .
English: Matias del r??o was a drag . . .
Sentiment prediction errors
3.
Spanish: Mario que dio este contigo
Predicted: NOT-TARG - - - -
Gold: POSITIVE - - - -
English: Mario may God be with you
4.
Spanish: . . . si de verdad estas en cielo , ayudame Superman !!!
Predicted: - - - - - - - - POSITIVE -
Gold: - - - - - - - - NOT-TARG -
English: . . . if you really are in the skies , help me Superman !!!
Sentiment and NE prediction errors
5.
Spanish: Salen del gobierno de Humala dos connotados izquierdistas, Giesecke y Eiguiguren
Predicted:
O O O O B-VOLITIONAL I-VOLITIONAL O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG NOT-TARG - - - NOT-TARG - NOT-TARG
Gold:
O O O O B-VOLITIONAL O O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG - - - - NEGATIVE - NOT-TARG
English: Leaving the Humala government are two notorious leftists , Giesecke and Eiguiguren
Table 13: Example errors made by joint models.
sentiment may not be clear without spelling correc-
tion: ?dio? should be ?dios?, meaning ?God?; other-
wise, ?dio? is the word for ?gave?. Humans can eas-
ily fix the spelling error, which changes the overall
reading of the expression. In (4), the positive polar-
ity item ?verdad? (?believe?) and the exclamation
marks (!!!) were likely used as indicators of posi-
tive sentiment; however, in this case the annotators
marked the targeted sentiment as neutral. In (5), the
?Humala? entity was predicted to be longer than it is
(?Hamala dos? or ?Hamala two?). It was also pre-
dicted that both ?Giesecke? and ?Eiguiguren? had
no sentiment expressed towards them; annotators
disagreed, with the majority of those who annotated
?Giesecke? marking negative sentiment, and the ma-
jority of those who annotated ?Eiguiguren? mark-
ing no sentiment. This highlights some of the diffi-
culty in predicting sentiment discussed in Section 3,
where annotators will often disagree as to whether
there is no sentiment or positive/negative sentiment.
During development, we found that the collapsed
model (COLL) performed best on small amounts of
data. However, as we scaled up the amount of data
we trained on, the PIPE and JOINT models signif-
icantly improved, while the COLL models did not
have significant performance gains.
8 Conclusion
We have introduced the task of open domain targeted
sentiment: predicting sentiment directed towards an
entity along with discovering the entity itself. Our
approach is developed to find targeted sentiment to-
wards both person and organization named entities
by modeling sentiment as a span along the entity.
We find that by modeling targeted sentiment in
this way, we can reliably detect entities and whether
or not they are sentiment targets above a no senti-
ment baseline. How best to determine the polarity
of the sentiment expressed towards the entity, how-
ever, is still an open issue. Our data suggests that
it is usually not clear-cut whether sentiment is being
expressed or not; the strong disagreement between
annotators suggests that detecting sentiment polar-
ity in microblogs is difficult even for humans.
In future work, we hope to explore further meth-
ods for teasing apart sentiment polarity expressed to-
wards a target. This research has achieved promis-
ing results for detecting sentiment targets without re-
lying on external supervised models, and we hope
that the features and approaches developed here can
aid in sentiment analysis in noisy text and languages
without rich linguistic resources.
1652
References
A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-
sonneau. 2011. Sentiment analysis of twitter data. In
Proceedings of the Workshop on Language in Social
Media.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of Coling: Posters.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of CIKM-2010.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the International Conference on Discovery Sci-
ence (DS-2010).
Juliette Blevins. 1996. The syllable in phonological the-
ory. In John A. Goldswmith, editor, The Handbook
of Phonological Theory. Blackwell Publishing, Black-
well Reference Online.
N. N. Bora. 2012. Summarizing public opinions in
tweets. In Proceedings of CICLing-2012.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP-2011.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg??lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the KDD-2011.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL:HLT Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from twitter. In Proceedings of ICWSM-2012.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. Proceedings of EMNLP 2006.
G. N. Clements. 1990. The role of the sonority cycle in
core syllabification. In J. Kingston and M. Beckman,
editors, Papers in Laboratory Phonology, pages 283?
333. CUP, Cambridge.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of Coling: Posters.
Nicholas A Diakopoulos and David A Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of CHI-2010.
David Etter, Francis Ferraro, Ryan Cotterell, Olivia
Buzek, and Benjamin Van Durme. 2013. Nerit:
Named entity recognition for informal text. Techni-
cal Report 11, Human Language Technology Center
of Excellence, Johns Hopkins University, July.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL-2010.
Joan B. Hooper. 1976. The syllable in phonological the-
ory. Language, 48(3):525?540.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD.
Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Ex-
ploiting social relations for sentiment analysis in mi-
croblogging. In Proceedings of the 6th ACM Inter-
national Conference on Web Search and Data Mining
(WSDM-2013).
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
EMNLP.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011.
Target-dependent twitter sentiment classification. In
Proceedings of ACL-2011.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
hmm-based learning framework for web opinion min-
ing. Proceedings of ICML 2009.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and
analyzing judgment opinions. Proceedings of NAACL
2006.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT.
Michal Kosinskia, David Stillwell, and Thore Graepel.
2013. Private trains and attributes are predictable from
digital records of human behavior. Proc. of the Na-
tional Academy of Sciences of the USA, 110(5).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of ICWSM-
2011.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-2001.
1653
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization.
Proceedings of Coling 2010.
Guangxia Li, Steven CH Hoi, Kuiyu Chang, and Ramesh
Jain. 2010b. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
ICDM-2010.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and De-
quan Zheng. 2012. Combining social cognitive theo-
ries with linguistic features for multi-genre sentiment
analysis. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC-2012).
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction, and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Frida Morelli. 2003. The relative harmony of /s+stop/
onsets: Obstruent clusters and the sonority sequenc-
ing principle. In C. Fery and R. van de Vijver, edi-
tors, The syllable in optimality theory, pages 356?371.
CUP, New York.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC-2010.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word count:
Liwc2007, operator?s manual.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in span-
ish. Proceedings of the Conference on Language Re-
sources and Evaluations (LREC 2012).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
Coling:ACL-2006.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT:EMNLP-2005.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1).
Hassan Saif, Yulan He, and Harith Alani. 2012. Allevi-
ating data sparsity for twitter sentiment analysis. Pro-
ceedings of the WWW Workshop on Making Sense of
Microposts (# MSM2012).
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the EMNLP-2011
Workshop on Unsupervised Learning in NLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate crf-based nlp systems. In
Proceedings of NAACL:HLT-2012.
Veselin Stoyanov, Alexander Ropson, and Jason Eis-
ner. 2011. Empirical risk minimization of graphi-
cal model parameters given approximate inference, de-
coding, and model structure. In AIStats.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the KDD-2011.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical report,
Human Language Technology Center of Excellence,
Johns Hopkins University.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual twitter
streams. In Association for Computational Linguistics
(ACL).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of CIKM-2011.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009.
Recognizing contextual polarity: An exploration of
features for phrase-level sentiment analysis. Compu-
tational Linguistics, 35(3).
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. Proceedings of ACL
2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
ICDM-2003.
1654
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747?756,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Midge: Generating Image Descriptions From Computer Vision
Detections
Margaret Mitchell?
Xufeng Han?
Jesse Dodge??
Alyssa Mensch??
Amit Goyal??
Alex Berg?
Kota Yamaguchi?
Tamara Berg?
Karl Stratos?
Hal Daume? III??
?U. of Aberdeen and Oregon Health and Science University, m.mitchell@abdn.ac.uk
? Stony Brook University, {aberg,tlberg,xufhan,kyamagu}@cs.stonybrook.edu
??U. of Maryland, {hal,amit}@umiacs.umd.edu
?Columbia University, stratos@cs.columbia.edu
??U. of Washington, dodgejesse@gmail.com, ??MIT, acmensch@mit.edu
Abstract
This paper introduces a novel generation
system that composes humanlike descrip-
tions of images from computer vision de-
tections. By leveraging syntactically in-
formed word co-occurrence statistics, the
generator filters and constrains the noisy
detections output from a vision system to
generate syntactic trees that detail what
the computer vision system sees. Results
show that the generation system outper-
forms state-of-the-art systems, automati-
cally generating some of the most natural
image descriptions to date.
1 Introduction
It is becoming a real possibility for intelligent sys-
tems to talk about the visual world. New ways of
mapping computer vision to generated language
have emerged in the past few years, with a fo-
cus on pairing detections in an image to words
(Farhadi et al 2010; Li et al 2011; Kulkarni et
al., 2011; Yang et al 2011). The goal in connect-
ing vision to language has varied: systems have
started producing language that is descriptive and
poetic (Li et al 2011), summaries that add con-
tent where the computer vision system does not
(Yang et al 2011), and captions copied directly
from other images that are globally (Farhadi et al
2010) and locally similar (Ordonez et al 2011).
A commonality between all of these ap-
proaches is that they aim to produce natural-
sounding descriptions from computer vision de-
tections. This commonality is our starting point:
We aim to design a system capable of producing
natural-sounding descriptions from computer vi-
sion detections that are flexible enough to become
more descriptive and poetic, or include likely in-
The bus by the road with a clear blue sky
Figure 1: Example image with generated description.
formation from a language model, or to be short
and simple, but as true to the image as possible.
Rather than using a fixed template capable of
generating one kind of utterance, our approach
therefore lies in generating syntactic trees. We
use a tree-generating process (Section 4.3) simi-
lar to a Tree Substitution Grammar, but preserv-
ing some of the idiosyncrasies of the Penn Tree-
bank syntax (Marcus et al 1995) on which most
statistical parsers are developed. This allows us
to automatically parse and train on an unlimited
amount of text, creating data-driven models that
flesh out descriptions around detected objects in a
principled way, based on what is both likely and
syntactically well-formed.
An example generated description is given in
Figure 1, and example vision output/natural lan-
guage generation (NLG) input is given in Fig-
ure 2. The system (?Midge?) generates descrip-
tions in present-tense, declarative phrases, as a
na??ve viewer without prior knowledge of the pho-
tograph?s content.1
Midge is built using the following approach:
An image processed by computer vision algo-
rithms can be characterized as a triple <Ai, Bi,
Ci>, where:
1Midge is available to try online at:
http://recognition.cs.stonybrook.edu:8080/?mitchema/midge/.
747
stuff: sky .999
id: 1
atts: clear:0.432, blue:0.945
grey:0.853, white:0.501 ...
b. box: (1,1 440,141)
stuff: road .908
id: 2
atts: wooden:0.722 clear:0.020 ...
b. box: (1,236 188,94)
object: bus .307
id: 3
atts: black:0.872, red:0.244 ...
b. box: (38,38 366,293)
preps: id 1, id 2: by id 1, id 3: by id 2, id 3: below
Figure 2: Example computer vision output and natu-
ral language generation input. Values correspond to
scores from the vision detections.
? Ai is the set of object/stuff detections with
bounding boxes and associated ?attribute?
detections within those bounding boxes.
? Bi is the set of action or pose detections as-
sociated to each ai ? Ai.
? Ci is the set of spatial relationships that hold
between the bounding boxes of each pair
ai, aj ? Ai.
Similarly, a description of an image can be char-
acterized as a triple <Ad, Bd, Cd> where:
? Ad is the set of nouns in the description with
associated modifiers.
? Bd is the set of verbs associated to each ad ?
Ad.
? Cd is the set of prepositions that hold be-
tween each pair of ad, ae ? Ad.
With this representation, mapping <Ai, Bi, Ci>
to <Ad, Bd, Cd> is trivial. The problem then
becomes: (1) How to filter out detections that
are wrong; (2) how to order the objects so that
they are mentioned in a natural way; (3) how to
connect these ordered objects within a syntacti-
cally/semantically well-formed tree; and (4) how
to add further descriptive information from lan-
guage modeling alone, if required.
Our solution lies in usingAi andAd as descrip-
tion anchors. In computer vision, object detec-
tions form the basis of action/pose, attribute, and
spatial relationship detections; therefore, in our
approach to language generation, nouns for the
object detections are used as the basis for the de-
scription. Likelihood estimates of syntactic struc-
ture and word co-occurrence are conditioned on
object nouns, and this enables each noun head in
a description to select for the kinds of structures it
tends to appear in (syntactic constraints) and the
other words it tends to occur with (semantic con-
straints). This is a data-driven way to generate
likely adjectives, prepositions, determiners, etc.,
taking the intersection of what the vision system
predicts and how the object noun tends to be de-
scribed.
2 Background
Our approach to describing images starts with
a system from Kulkarni et al(2011) that com-
poses novel captions for images in the PASCAL
sentence data set,2 introduced in Rashtchian et
al. (2010). This provides multiple object detec-
tions based on Felzenszwalb?s mixtures of multi-
scale deformable parts models (Felzenszwalb et
al., 2008), and stuff detections (roughly, mass
nouns, things like sky and grass) based on linear
SVMs for low level region features.
Appearance characteristics are predicted using
trained detectors for colors, shapes, textures, and
materials, an idea originally introduced in Farhadi
et al(2009). Local texture, Histograms of Ori-
ented Gradients (HOG) (Dalal and Triggs, 2005),
edge, and color descriptors inside the bounding
box of a recognized object are binned into his-
tograms for a vision system to learn to recognize
when an object is rectangular, wooden, metal,
etc. Finally, simple preposition functions are used
to compute the spatial relations between objects
based on their bounding boxes.
The original Kulkarni et al(2011) system gen-
erates descriptions with a template, filling in slots
by combining computer vision outputs with text
based statistics in a conditional random field to
predict the most likely image labeling. Template-
based generation is also used in the recent Yang et
al. (2011) system, which fills in likely verbs and
prepositions by dependency parsing the human-
written UIUC Pascal-VOC dataset (Farhadi et al
2010) and selecting the dependent/head relation
with the highest log likelihood ratio.
Template-based generation is useful for auto-
matically generating consistent sentences, how-
ever, if the goal is to vary or add to the text pro-
duced, it may be suboptimal (cf. Reiter and Dale
(1997)). Work that does not use template-based
generation includes Yao et al(2010), who gener-
ate syntactic trees, similar to the approach in this
2http://vision.cs.uiuc.edu/pascal-sentences/
748
Kulkarni et al This is a pic-
ture of three persons, one bot-
tle and one diningtable. The
first rusty person is beside the
second person. The rusty bot-
tle is near the first rusty per-
son, and within the colorful
diningtable. The second per-
son is by the third rusty per-
son. The colorful diningtable
is near the first rusty person,
and near the second person,
and near the third rusty person.
Kulkarni et al This is
a picture of two potted-
plants, one dog and one
person. The black dog is
by the black person, and
near the second feathered
pottedplant.
Yang et al Three people
are showing the bottle on the
street
Yang et al The person is
sitting in the chair in the
room
Midge: people with a bottle at
the table
Midge: a person in black
with a black dog by potted
plants
Figure 3: Descriptions generated by Midge, Kulkarni
et al(2011) and Yang et al(2011) on the same images.
Midge uses the Kulkarni et al(2011) front-end, and so
outputs are directly comparable.
paper. However, their system is not automatic, re-
quiring extensive hand-coded semantic and syn-
tactic details. Another approach is provided in
Li et al(2011), who use image detections to se-
lect and combine web-scale n-grams (Brants and
Franz, 2006). This automatically generates de-
scriptions that are either poetic or strange (e.g.,
?tree snowing black train?).
A different line of work transfers captions of
similar images directly to a query image. Farhadi
et al(2010) use <object,action,scene> triples
predicted from the visual characteristics of the
image to find potential captions. Ordonez et al
(2011) use global image matching with local re-
ordering from a much larger set of captioned pho-
tographs. These transfer-based approaches result
in natural captions (they are written by humans)
that may not actually be true of the image.
This work learns and builds from these ap-
proaches. Following Kulkarni et aland Li et al
the system uses large-scale text corpora to esti-
mate likely words around object detections. Fol-
lowing Yang et al the system can hallucinate
likely words using word co-occurrence statistics
alone. And following Yao et al the system aims
black, blue, brown, colorful, golden, gray,
green, orange, pink, red, silver, white, yel-
low, bare, clear, cute, dirty, feathered, flying,
furry, pine, plastic, rectangular, rusty, shiny,
spotted, striped, wooden
Table 1: Modifiers used to extract training corpus.
for naturally varied but well-formed text, generat-
ing syntactic trees rather than filling in a template.
In addition to these tasks, Midge automatically
decides what the subject and objects of the de-
scription will be, leverages the collected word co-
occurrence statistics to filter possible incorrect de-
tections, and offers the flexibility to be as de-
scriptive or as terse as possible, specified by the
user at run-time. The end result is a fully au-
tomatic vision-to-language system that is begin-
ning to generate syntactically and semantically
well-formed descriptions with naturalistic varia-
tion. Example descriptions are given in Figures 4
and 5, and descriptions from other recent systems
are given in Figure 3.
The results are promising, but it is important to
note that Midge is a first-pass system through the
steps necessary to connect vision to language at
a deep syntactic/semantic level. As such, it uses
basic solutions at each stage of the process, which
may be improved: Midge serves as an illustration
of the types of issues that should be handled to
automatically generate syntactic trees from vision
detections, and offers some possible solutions. It
is evaluated against the Kulkarni et alsystem, the
Yang et alsystem, and human-written descrip-
tions on the same set of images in Section 5, and
is found to significantly outperform the automatic
systems.
3 Learning from Descriptive Text
To train our system on how people describe im-
ages, we use 700,000 (Flickr, 2011) images with
associated descriptions from the dataset in Or-
donez et al(2011). This is separate from our
evaluation image set, consisting of 840 PASCAL
images. The Flickr data is messier than datasets
created specifically for vision training, but pro-
vides the largest corpus of natural descriptions of
images to date.
We normalize the text by removing emoticons
and mark-up language, and parse each caption
using the Berkeley parser (Petrov, 2010). Once
parsed, we can extract syntactic information for
individual (word, tag) pairs.
749
a cow with sheep with a gray sky people with boats a brown cow people at
green grass by the road a wooden table
Figure 4: Example generated outputs.
Awkward Prepositions Incorrect Detections
a person boats under a black bicycle at the sky a yellow bus cows by black sheep
on the dog the sky a green potted plant with people by the road
Figure 5: Example generated outputs: Not quite right
We compute the probabilities for different
prenominal modifiers (shiny, clear, glowing, ...)
and determiners (a/an, the, None, ...) given a
head noun in a noun phrase (NP), as well as the
probabilities for each head noun in larger con-
structions, listed in Section 4.3. Probabilities are
conditioned only on open-class words, specifi-
cally, nouns and verbs. This means that a closed-
class word (such as a preposition) is never used to
generate an open-class word.
In addition to co-occurrence statistics, the
parsed Flickr data adds to our understanding of
the basic characteristics of visually descriptive
text. Using WordNet (Miller, 1995) to automati-
cally determine whether a head noun is a physical
object or not, we find that 92% of the sentences
have no more than 3 physical objects. This in-
forms generation by placing a cap on how many
objects are mentioned in each descriptive sen-
tence: When more than 3 objects are detected,
the system splits the description over several sen-
tences. We also find that many of the descriptions
are not sentences as well (tagged as S, 58% of the
data), but quite commonly noun phrases (tagged
as NP, 28% of the data), and expect that the num-
ber of noun phrases that form descriptions will be
much higher with domain adaptation. This also
informs generation, and the system is capable of
generating both sentences (contains a main verb)
and noun phrases (no main verb) in the final im-
age description. We use the term ?sentence? in the
rest of this paper to refer to both kinds of complex
phrases.
4 Generation
Following Penn Treebank parsing guidelines
(Marcus et al 1995), the relationship between
two head nouns in a sentence can usually be char-
acterized among the following:
1. prepositional (a boy on the table)
2. verbal (a boy cleans the table)
3. verb with preposition (a boy sits on the table)
4. verb with particle (a boy cleans up the table)
5. verb with S or SBAR complement (a boy
sees that the table is clean)
The generation system focuses on the first three
kinds of relationships, which capture a wide range
of utterances. The process of generation is ap-
proached as a problem of generating a semanti-
cally and syntactically well-formed tree based on
object nouns. These serve as head noun anchors
in a lexicalized syntactic derivation process that
we call tree growth.
Vision detections are associated to a {tag
word} pair, and the model fleshes out the tree de-
tails around head noun anchors by utilizing syn-
tactic dependencies between words learned from
the Flickr data discussed in Section 3. The anal-
ogy of growing a tree is quite appropriate here,
where nouns are bundles of constraints akin to
seeds, giving rise to the rest of the tree based on
the lexicalized subtrees in which the nouns are
likely to occur. An example generated tree struc-
ture is shown in Figure 6, with noun anchors in
bold.
750
NP
PP
NP
NN
table
DT
the
IN
at
NP
PP
NP
NN
bottle
DT
a
IN
with
NP
NN
people
DT
-
Figure 6: Tree generated from tree growth process.
Midge was developed using detections run on
Flickr images, incorporating action/pose detec-
tions for verbs as well as object detections for
nouns. In testing, we generate descriptions for
the PASCAL images, which have been used in
earlier work on the vision-to-language connection
(Kulkarni et al 2011; Yang et al 2011), and al-
lows us to compare systems directly. Action and
pose detection for this data set still does not work
well, and so the system does not receive these de-
tections from the vision front-end. However, the
system can still generate verbs when action and
pose detectors have been run, and this framework
allows the system to ?hallucinate? likely verbal
constructions between objects if specified at run-
time. A similar approach was taken in Yang et al
(2011). Some examples are given in Figure 7.
We follow a three-tiered generation process
(Reiter and Dale, 2000), utilizing content determi-
nation to first cluster and order the object nouns,
create their local subtrees, and filter incorrect de-
tections; microplanning to construct full syntactic
trees around the noun clusters, and surface real-
ization to order selected modifiers, realize them as
postnominal or prenominal, and select final out-
puts. The system follows an overgenerate-and-
select approach (Langkilde and Knight, 1998),
which allows different final trees to be selected
with different settings.
4.1 Knowledge Base
Midge uses a knowledge base that stores models
for different tasks during generation. These mod-
els are primarily data-driven, but we also include
a hand-built component to handle a small set of
rules. The data-driven component provides the
syntactically informed word co-occurrence statis-
tics learned from the Flickr data, a model for or-
dering the selected nouns in a sentence, and a
model to change computer vision attributes to at-
tribute:value pairs. Below, we discuss the three
main data-driven models within the generation
Unordered Ordered
bottle, table, person ? person, bottle, table
road, sky, cow ? cow, road, sky
Figure 8: Example nominal orderings.
pipeline. The hand-built component contains plu-
ral forms of singular nouns, the list of possible
spatial relations shown in Table 3, and a map-
ping between attribute values and modifier sur-
face forms (e.g., a green detection for person is to
be realized as the postnominal modifier in green).
4.2 Content Determination
4.2.1 Step 1: Group the Nouns
An initial set of object detections must first be
split into clusters that give rise to different sen-
tences. If more than 3 objects are detected in the
image, the system begins splitting these into dif-
ferent noun groups. In future work, we aim to
compare principled approaches to this task, e.g.,
using mutual information to cluster similar nouns
together. The current system randomizes which
nouns appear in the same group.
4.2.2 Step 2: Order the Nouns
Each group of nouns are then ordered to deter-
mine when they are mentioned in a sentence. Be-
cause the system generates declarative sentences,
this automatically determines the subject and ob-
jects. This is a novel contribution for a general
problem in NLG, and initial evaluation (Section
5) suggests it works reasonably well.
To build the nominal ordering model, we use
WordNet to associate all head nouns in the Flickr
data to all of their hypernyms. A description is
represented as an ordered set [a1...an] where each
ap is a noun with position p in the set of head
nouns in the sentence. For the position pi of each
hypernym ha in each sentence with n head nouns,
we estimate p(pi|n, ha).
During generation, the system greedily maxi-
mizes p(pi|n, ha) until all nouns have been or-
dered. Example orderings are shown in Figure 8.
This model automatically places animate objects
near the beginning of a sentence, which follows
psycholinguistic work in object naming (Branigan
et al 2007).
4.2.3 Step 3: Filter Incorrect Attributes
For the system to be able to extend coverage as
new computer vision attribute detections become
available, we develop a method to automatically
751
A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog
Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong.
COLOR purple blue green red white ...
MATERIAL plastic wooden silver ...
SURFACE furry fluffy hard soft ...
QUALITY shiny rust dirty broken ...
Table 2: Example attribute classes and values.
group adjectives into broader attribute classes,3
and the generation system uses these classes when
deciding how to describe objects. To group adjec-
tives, we use a bootstrapping technique (Kozareva
et al 2008) that learns which adjectives tend to
co-occur, and groups these together to form an at-
tribute class. Co-occurrence is computed using
cosine (distributional) similarity between adjec-
tives, considering adjacent nouns as context (i.e.,
JJ NN constructions). Contexts (nouns) for adjec-
tives are weighted using Pointwise Mutual Infor-
mation and only the top 1000 nouns are selected
for every adjective. Some of the learned attribute
classes are given in Table 2.
In the Flickr corpus, we find that each attribute
(COLOR, SIZE, etc.), rarely has more than a single
value in the final description, with the most com-
mon (COLOR) co-occurring less than 2% of the
time. Midge enforces this idea to select the most
likely word v for each attribute from the detec-
tions. In a noun phrase headed by an object noun,
NP{NN noun}, the prenominal adjective (JJ v) for
each attribute is selected using maximum likeli-
hood.
4.2.4 Step 4: Group Plurals
How to generate natural-sounding spatial rela-
tions and modifiers for a set of objects, as opposed
to a single object, is still an open problem (Fu-
nakoshi et al 2004; Gatt, 2006). In this work, we
use a simple method to group all same-type ob-
jects together, associate them to the plural form
listed in the KB, discard the modifiers, and re-
turn spatial relations based on the first recognized
3What in computer vision are called attributes are called
values in NLG. A value like red belongs to a COLOR at-
tribute, and we use this distinction in the system.
member of the group.
4.2.5 Step 5: Gather Local Subtrees Around
Object Nouns
1 2
NP
NN
n
JJ* ?DT{0,1} ? S
VP{VBZ} ?NP{NN n}
3 4
NP
VP{VB(G|N)} ?NP{NN n}
NP
PP{IN} ?NP{NN n}
5 6
PP
NP{NN n}IN ?
VP
PP{IN} ?VB(G|N|Z) ?
7
VP
NP{NN n}VB(G|N|Z) ?
Figure 9: Initial subtree frames for generation, present-
tense declarative phrases. ? marks a substitution site,
* marks ? 0 sister nodes of this type permitted, {0,1}
marks that this node can be included of excluded.
Input: set of ordered nouns, Output: trees preserving
nominal ordering.
Possible actions/poses and spatial relationships
between objects nouns, represented by verbs and
prepositions, are selected using the subtree frames
listed in Figure 9. Each head noun selects for its
likely local subtrees, some of which are not fully
formed until the Microplanning stage. As an ex-
ample of how this process works, see Figure 10,
which illustrates the combination of Trees 4 and
5. For simplicity, we do not include the selection
of further subtrees. The subject noun duck se-
lects for prepositional phrases headed by different
prepositions, and the object noun grass selects
for prepositions that head the prepositional phrase
in which it is embedded. Full PP subtrees are cre-
ated during Microplanning by taking the intersec-
tion of both.
The leftmost noun in the sequence is given a
rightward directionality constraint, placing it as
the subject of the sentence, and so it will only se-
752
a over b a above b b below a b beneath a a by b b by a a on b b under a
b underneath a a upon b a over b
a by b a against b b against a b around a a around b a at b b at a a beside b
b beside a a by b b by a a near b b near a b with a a with b
a in b a in b b outside a a within b a by b b by a
Table 3: Possible prepositions from bounding boxes.
Subtree frames:
NP
PP{IN} ?NP{NN n1}
PP
NP{NN n2}IN ?
Generated subtrees:
NP
PP
IN
above, on, by
NP
NN
duck
PP
NP
NN
grass
IN
on, by, over
Combined trees:
NP
PP
NP
NN
grass
IN
on
NP
NN
duck
NP
PP
NP
NN
grass
IN
by
NP
NN
duck
Figure 10: Example derivation.
lect for trees that expand to the right. The right-
most noun is given a leftward directionality con-
straint, placing it as an object, and so it will only
select for trees that expand to its left. The noun in
the middle, if there is one, selects for all its local
subtrees, combining first with a noun to its right
or to its left. We now walk through the deriva-
tion process for each of the listed subtree frames.
Because we are following an overgenerate-and-
select approach, all combinations above a proba-
bility threshold ? and an observation cutoff ? are
created.
Tree 1:
Collect all NP? (DT det) (JJ adj)* (NN noun)
and NP? (JJ adj)* (NN noun) subtrees, where:
? p((JJ adj)|(NN noun)) > ? for each adj
? p((DT det)|JJ, (NN noun)) > ?, and the proba-
bility of a determiner for the head noun is higher
than the probability of no determiner.
Any number of adjectives (including none) may
be generated, and we include the presence or ab-
sence of an adjective when calculating which de-
terminer to include.
The reasoning behind the generation of these
subtrees is to automatically learn whether to treat
a given noun as a mass or count noun (not taking a
determiner or taking a determiner, respectively) or
as a given or new noun (phrases like a sky sound
unnatural because sky is given knowledge, requir-
ing the definite article the). The selection of de-
terminer is not independent of the selection of ad-
jective; a sky may sound unnatural, but a blue sky
is fine. These trees take the dependency between
determiner and adjective into account.
Trees 2 and 3:
Collect beginnings of VP subtrees headed by
(VBZ verb), (VBG verb), and (VBN verb), no-
tated here as VP{VBX verb}, where:
? p(VP{VBX verb}|NP{NN noun}=SUBJ) > ?
Tree 4:
Collect beginnings of PP subtrees headed by (IN
prep), where:
? p(PP{IN prep}|NP{NN noun}=SUBJ) > ?
Tree 5:
Collect PP subtrees headed by (IN prep) with
NP complements (OBJ) headed by (NN noun),
where:
? p(PP{IN prep}|NP{NN noun}=OBJ) > ?
Tree 6:
Collect VP subtrees headed by (VBX verb) with
embedded PP complements, where:
? p(PP{IN prep}|VP{VBX verb}=SUBJ) > ?
Tree 7:
Collect VP subtrees headed by (VBX verb) with
embedded NP objects, where:
? p(VP{VBX verb}|NP{NN noun}=OBJ) > ?
4.3 Microplanning
4.3.1 Step 6: Create Full Trees
In Microplanning, full trees are created by tak-
ing the intersection of the subtrees created in Con-
tent Determination. Because the nouns are or-
dered, it is straightforward to combine the sub-
trees surrounding a noun in position 1 with sub-
trees surrounding a noun in position 2. Two
753
VP
VP* ?
NP
NP ?CC
and
NP ?
Figure 11: Auxiliary trees for generation.
further trees are necessary to allow the subtrees
gathered to combine within the Penn Treebank
syntax. These are given in Figure 11. If two
nouns in a proposed sentence cannot be combined
with prepositions or verbs, we backoff to combine
them using (CC and).
Stepping through this process, all nouns will
have a set of subtrees selected by Tree 1. Prepo-
sitional relationships between nouns are created
by substituting Tree 1 subtrees into the NP nodes
of Trees 4 and 5, as shown in Figure 10. Verbal
relationships between nouns are created by substi-
tuting Tree 1 subtrees into Trees 2, 3, and 7. Verb
with preposition relationships are created between
nouns by substituting the VBX node in Tree 6
with the corresponding node in Trees 2 and 3 to
grow the tree to the right, and the PP node in Tree
6 with the corresponding node in Tree 5 to grow
the tree to the left. Generation of a full tree stops
when all nouns in a group are dominated by the
same node, either an S or NP.
4.4 Surface Realization
In the surface realization stage, the system se-
lects a single tree from the generated set of pos-
sible trees and removes mark-up to produce a fi-
nal string. This is also the stage where punctua-
tion may be added. Different strings may be gen-
erated depending on different specifications from
the user, as discussed at the beginning of Section
4 and shown in the online demo. To evaluate the
system against other systems, we specify that the
system should (1) not hallucinate likely verbs; and
(2) return the longest string possible.
4.4.1 Step 7: Get Final Tree, Clear Mark-Up
We explored two methods for selecting a final
string. In one method, a trigram language model
built using the Europarl (Koehn, 2005) data with
start/end symbols returns the highest-scoring de-
scription (normalizing for length). In the second
method, we limit the generation system to select
the most likely closed-class words (determiners,
prepositions) while building the subtrees, over-
generating all possible adjective combinations.
The final string is then the one with the most
words. We find that the second method produces
descriptions that seem more natural and varied
than the n-gram ranking method for our develop-
ment set, and so use the longest string method in
evaluation.
4.4.2 Step 8: Prenominal Modifier Ordering
To order sets of selected adjectives, we use the
top-scoring prenominal modifier ordering model
discussed in Mitchell et al(2011). This is an n-
gram model constructed over noun phrases that
were extracted from an automatically parsed ver-
sion of the New York Times portion of the Giga-
word corpus (Graff and Cieri, 2003). With this
in place, blue clear sky becomes clear blue sky,
wooden brown table becomes brown wooden ta-
ble, etc.
5 Evaluation
Each set of sentences is generated with ? (likeli-
hood cutoff) set to .01 and ? (observation count
cutoff) set to 3. We compare the system against
human-written descriptions and two state-of-the-
art vision-to-language systems, the Kulkarni et al
(2011) and Yang et al(2011) systems.
Human judgments were collected using Ama-
zon?s Mechanical Turk (Amazon, 2011). We
follow recommended practices for evaluating an
NLG system (Reiter and Belz, 2009) and for run-
ning a study on Mechanical Turk (Callison-Burch
and Dredze, 2010), using a balanced design with
each subject rating 3 descriptions from each sys-
tem. Subjects rated their level of agreement on
a 5-point Likert scale including a neutral mid-
dle position, and since quality ratings are ordinal
(points are not necessarily equidistant), we evalu-
ate responses using a non-parametric test. Partici-
pants that took less than 3 minutes to answer all 60
questions and did not include a humanlike rating
for at least 1 of the 3 human-written descriptions
were removed and replaced. It is important to note
that this evaluation compares full generation sys-
tems; many factors are at play in each system that
may also influence participants? perception, e.g.,
sentence length (Napoles et al 2011) and punc-
tuation decisions.
The systems are evaluated on a set of 840
images evaluated in the original Kulkarni et al
(2011) system. Participants were asked to judge
the statements given in Figure 12, from Strongly
Disagree to Strongly Agree.
754
Grammaticality Main Aspects Correctness Order Humanlikeness
Human 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96)
Midge 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17)
Kulkarni et al2011 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23)
Yang et al2011 3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23)
Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the
rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.
GRAMMATICALITY:
This description is grammatically correct.
MAIN ASPECTS:
This description describes the main aspects of this
image.
CORRECTNESS:
This description does not include extraneous or in-
correct information.
ORDER:
The objects described are mentioned in a reasonable
order.
HUMANLIKENESS:
It sounds like a person wrote this description.
Figure 12: Mechanical Turk prompts.
We report the scores for the systems in Table
4. Results are analyzed using the non-parametric
Wilcoxon Signed-Rank test, which uses median
values to compare the different systems. Midge
outperforms all recent automatic approaches on
CORRECTNESS and ORDER, and Yang et alad-
ditionally on HUMANLIKENESS and MAIN AS-
PECTS. Differences between Midge and Kulkarni
et alare significant at p< .01; Midge and Yang et
al. at p< .001. For all metrics, human-written de-
scriptions still outperform automatic approaches
(p < .001).
These findings are striking, particularly be-
cause Midge uses the same input as the Kulka-
rni et alsystem. Using syntactically informed
word co-occurrence statistics from a large corpus
of descriptive text improves over state-of-the-art,
allowing syntactic trees to be generated that cap-
ture the variation of natural language.
6 Discussion
Midge automatically generates language that is as
good as or better than template-based systems,
tying vision to language at a syntactic/semantic
level to produce natural language descriptions.
Results are promising, but, there is more work to
be done: Evaluators can still tell a difference be-
tween human-written descriptions and automati-
cally generated descriptions.
Improvements to the generated language are
possible at both the vision side and the language
side. On the computer vision side, incorrect ob-
jects are often detected and salient objects are of-
ten missed. Midge does not yet screen out un-
likely objects or add likely objects, and so pro-
vides no filter for this. On the language side, like-
lihood is estimated directly, and the system pri-
marily uses simple maximum likelihood estima-
tions to combine subtrees. The descriptive cor-
pus that informs the system is not parsed with
a domain-adapted parser; with this in place, the
syntactic constructions that Midge learns will bet-
ter reflect the constructions that people use.
In future work, we hope to address these issues
as well as advance the syntactic derivation pro-
cess, providing an adjunction operation (for ex-
ample, to add likely adjectives or adverbs based
on language alone). We would also like to incor-
porate meta-data ? even when no vision detection
fires for an image, the system may be able to gen-
erate descriptions of the time and place where an
image was taken based on the image file alone.
7 Conclusion
We have introduced a generation system that uses
a new approach to generating language, tying a
syntactic model to computer vision detections.
Midge generates a well-formed description of an
image by filtering attribute detections that are un-
likely and placing objects into an ordered syntac-
tic structure. Humans judge Midge?s output to be
the most natural descriptions of images generated
thus far. The methods described here are promis-
ing for generating natural language descriptions
of the visual world, and we hope to expand and
refine the system to capture further linguistic phe-
nomena.
8 Acknowledgements
Thanks to the Johns Hopkins CLSP summer
workshop 2011 for making this system possible,
and to reviewers for helpful comments. This
work is supported in part by Michael Collins and
by NSF Faculty Early Career Development (CA-
REER) Award #1054133.
755
References
Amazon. 2011. Amazon mechanical turk: Artificial
artificial intelligence.
Holly P. Branigan, Martin J. Pickering, and Mikihiro
Tanaka. 2007. Contributions of animacy to gram-
matical function assignment and word order during
production. Lingua, 118(2):172?189.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon?s Me-
chanical Turk. NAACL 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detections. Proceed-
ings of CVPR 2005.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. Proceedings of CVPR 2009.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
Proceedings of ECCV 2010.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
maman. 2008. A discriminatively trained, mul-
tiscale, deformable part model. Proceedings of
CVPR 2008.
Flickr. 2011. http://www.flickr.com. Accessed
1.Sep.11.
Kotaro Funakoshi, Satoru Watanabe, Naoko
Kuriyama, and Takenobu Tokunaga. 2004.
Generating referring expressions using perceptual
groups. Proceedings of the 3rd INLG.
Albert Gatt. 2006. Generating collective spatial refer-
ences. Proceedings of the 28th CogSci.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
PA. LDC Catalog No. LDC2003T05.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. MT Summit.
http://www.statmt.org/europarl/.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C. Berg, and Tamara
Berg. 2011. Baby talk: Understanding and gener-
ating image descriptions. Proceedings of the 24th
CVPR.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. Proceedings of the 36th ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
Proceedings of CoNLL 2011.
Mitchell Marcus, Ann Bies, Constance Cooper, Mark
Ferguson, and Alyson Littman. 1995. Treebank II
bracketing guide.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011. Semi-supervised modeling for prenomi-
nal modifier ordering. Proceedings of the 49th
ACL:HLT.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. ACL-
HLT Workshop on Monolingual Text-To-Text Gen-
eration.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. Proceedings of NIPS 2011.
Slav Petrov. 2010. Berkeley parser. GNU General
Public License v.2.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazon?s mechanical turk. Proceed-
ings of the NAACL HLT 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. Journal
of Natural Language Engineering, pages 57?87.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and
Yiannis Aloimonos. 2011. Corpus-guided sen-
tence generation of natural images. Proceedings of
EMNLP 2011.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2T: Image pars-
ing to text description. Proceedings of IEEE 2010,
98(8):1485?1508.
756
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600?608,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Prenominal Modifier Ordering via Multiple Sequence Alignment
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
Producing a fluent ordering for a set of
prenominal modifiers in a noun phrase
(NP) is a problematic task for natural lan-
guage generation and machine translation
systems. We present a novel approach
to this issue, adapting multiple sequence
alignment techniques used in computa-
tional biology to the alignment of modi-
fiers. We describe two training techniques
to create such alignments based on raw
text, and demonstrate ordering accuracies
superior to earlier reported approaches.
1 Introduction
Natural language generation and machine trans-
lation systems must produce text which not only
conforms to a reasonable grammatical model,
but which also sounds smooth and natural to
a human consumer. Ordering prenominal mod-
ifiers in noun phrases is particularly difficult
in these applications, as the rules underlying
these orderings are subtle and not well under-
stood. For example, the phrase ?big red ball?
seems natural, while ?red big ball? seems more
marked, suitable only in specific contexts. There
is some consensus that the order of prenom-
inal modifiers in noun phrases is governed in
part by semantic constraints, but there is no
agreement on the exact constraints necessary to
specify consistent orderings for any given set of
modifiers. General principles of modifier order-
ing based on semantic constraints also fall short
on larger domains, where it is not always clear
how to map prenominal modifiers to proposed
semantic groups.
With the recent advantages of large corpora
and powerful computational resources, work
on automatically ordering prenominal modifiers
has moved away from approaches based on gen-
eral principles, and towards learning ordering
preferences empirically from existing corpora.
Such approaches have several advantages: (1)
The predicted orderings are based on prior evi-
dence from ?real-world? texts, ensuring that they
are therefore reasonably natural. (2) Many (if
not all) prenominal modifiers can be ordered.
(3) Expanding the training data with more and
larger corpora often improves the system with-
out requiring significant manual labor.
In this paper, we introduce a novel approach
to prenominal modifier ordering adapted from
multiple sequence alignment (MSA) techniques
used in computational biology. MSA is generally
applied to DNA, RNA, and protein sequences,
aligning three or more biological sequences in or-
der to determine, for example, common ancestry
(Durbin et al, 1999; Gusfield, 1997; Carrillo and
Lipman, 1988). MSA techniques have not been
widely applied in NLP, but have produced some
promising results for building a generation map-
ping dictionary (Barzilay and Lee, 2002), para-
phrasing (Barzilay and Lee, 2003), and phone
recognition (White et al, 2006).
We believe that multiple sequence alignment
is well-suited for aligning linguistic sequences,
and that these alignments can be used to predict
prenominal modifier ordering for any given set
of modifiers. Our technique utilizes simple fea-
tures within the raw text, and does not require
any semantic information. We achieve good per-
formance using this approach, with results com-
petitive with earlier work (Shaw and Hatzivas-
siloglou, 1999; Malouf, 2000; Mitchell, 2009) and
higher recall and F-measure than that reported
in Mitchell (2009) when tested on the same cor-
pus.
600
2 Related work
In one of the first attempts at automatically or-
dering prenominal modifiers, Shaw and Hatzi-
vassiloglou (1999) present three empirical meth-
ods to order a variety of prenominal modifier
types. Their approach provides ordering deci-
sions for adjectives, gerunds (such as ?running?
in ?running man?), and past participles (such
as ?heated? in ?heated debate?), as well as for
modifying nouns (such as ?baseball? in ?base-
ball field?). A morphology module transforms
plural nouns and comparative/superlative forms
into their base forms, increasing the frequency
counts for each modifier. We will briefly re-
cap their three methods, which are categorized
as the direct evidence method, the transitivity
method, and the clustering method.
Given prenominal modifiers a and b in a train-
ing corpus, the direct evidence method com-
pares frequency counts of the ordered sequences
<a,b> and <b,a>. This approach works well,
but is limited by data sparsity; groups of two or
more modifiers before a noun are relatively in-
frequent in traditional corpora, and finding the
same pair of modifiers together more than once
is particularly rare.
To overcome this issue, Shaw and Hatzi-
vassiloglou?s transitivity and clustering meth-
ods make inferences about unseen orderings
among prenominal modifiers. In the transitiv-
ity method, given three modifiers a,b,c, where a
precedes b and b precedes c, the model concludes
that a precedes c. The clustering method calcu-
lates a similarity score between modifiers based
on where the modifiers occur in relation to the
other modifiers in the corpus. Those modifiers
that are most similar are clustered together, and
ordering decisions can be made between modi-
fiers in separate clusters. All three approaches
are designed to order pairs of modifiers; it is un-
clear how to extend these approaches to order
groups larger than a pair.
Shaw and Hatzivassiloglou find that NPs with
only adjectives as modifiers (including gerunds
and past participles) are considerably easier to
order than those which contain both adjectives
and nouns. They also find large differences in
accuracy across domains; their systems achieve
much lower overall accuracy on financial text
(the Wall Street Journal (WSJ) corpus (Marcus
et al, 1999)) than on medical discharge sum-
maries.
Looking at all modifier pairs, the authors
achieve their highest prediction accuracy of
90.7% using the transitivity technique on a med-
ical corpus. We do not have access to this cor-
pus, but we do have access to the WSJ corpus,
which provides a way to compare our methods.
On this corpus, their model produces predic-
tions for 62.5% of all modifier pairs and achieves
83.6% accuracy when it is able to make a predic-
tion. Random guessing on the remainder yields
an overall accuracy of 71.0%.
Malouf (2000) also examines the problem of
prenominal modifier ordering. He too proposes
several statistical techniques, achieving results
ranging from 78.3% to 91.9% accuracy. He
achieves his best results by combining memory-
based learning and positional probability to
modifiers from the first 100 million tokens of
the BNC. However, this evaluation is limited to
the ordering of prenominal adjectives, which is a
considerably simpler task than ordering all types
of prenominal modifiers. Malouf?s approaches
are also limited to ordering pairs of modifiers.
Mitchell (2009) proposes another approach,
grouping modifiers into classes and ordering
based on those classes. A modifier?s class is as-
signed based on its placement before a noun,
relative to the other modifiers it appears with.
Classes are composed of those modifiers that
tend to be placed closer to the head noun, those
modifiers that tend to be placed farther from the
head noun, etc., with each class corresponding
to a general positional preference. Unlike earlier
work, these classes allow more than one ordering
to be proposed for some pairs of modifiers.
Combining corpora of various genres,
Mitchell?s system achieves a token precision
of 89.6% (see Section 4 for discussion and
comparison of various evaluation metrics).
However, the model only makes predictions for
74.1% of all modifier pairs in the test data, so
recall is quite low (see Tables 4 and 6).
Overall, previous work in noun-phrase order-
601
ing has produced impressive accuracies in some
domains, but currently available systems tend
to adapt poorly to unseen modifiers and do not
generalize well to unseen domains.
3 Methods
3.1 Multiple Sequence Alignment
Multiple sequence alignment algorithms align
sequences of discrete tokens into a series of
columns. They attempt to align identical or
easily-substitutable tokens within a column, in-
serting gaps when such gaps will result in a bet-
ter alignment (more homogeneous token assign-
ments within each column). For example, con-
sider the simple alignment shown in Table 1.
The two sequences ?GAACTGAT? and ?AAGT-
GTAT? are aligned to maximize the number of
identical items that appear in the same column,
substituting tokens (column 3), and inserting
gaps (columns 1 and 6)1.
A full MSA is generally constructed by itera-
tively aligning each new sequence with an identi-
cal or similar sequence already in the MSA (so-
called ?progressive alignment?). The costs of
token substitution are often taken from a hand-
tuned substitution matrix. A cost may also be
associated with inserting a gap into the exist-
ing MSA (a ?gap penalty?). Once the full MSA
has been constructed, a Position Specific Score
Matrix (PSSM) can be induced, in which each
token (including a special gap token) is assigned
a separate alignment cost for each column. An
unseen sequence can then be aligned with the
full MSA by Viterbi search.
Predicting sequence ordering within a noun
phrase is a natural application for MSA tech-
niques, and it seems reasonable to propose that
aligning an unseen set of modifiers with such an
MSA model will yield acceptable orderings. Ta-
ble 2 illustrates how MSA may be applied to
modifiers before a noun. Given an NP preceded
by modifiers hungry, big, and Grizzly, alignment
of the modifiers with NPs seen in the training
corpus determines the prenominal ordering big
hungry Grizzly. We then align every permuta-
1See Durbin et al (1999) for details on standard align-
ment techniques.
G A C T G - A T
- A G T G T A T
1 2 3 4 5 6 7 8
Table 1: Alignment of the two DNA sequences
?GAACTGAT? and ?AAGTGTAT?.
small clumsy black bear
big - black cow
two-story - brown house
big clumsy - bull
small fuzzy brown duck
large - green house
big hungry Grizzly bear
Table 2: Example noun-phrase alignment.
tion of the NP and choose the best-scoring align-
ment.
The vocabulary for a linguistic alignment is
large enough to render a hand-tuned substitu-
tion matrix impractical, so we instead construct
a cost function based on features of the token
under consideration and those of the other to-
kens already aligned in a column.
We know of no prior work on methods for
training such an alignment. We present and
compare two training methods, each of which
produces competitive ordering accuracies. Both
training methods share the feature-set described
in Table 3. In each case, we train an MSA by
aligning each instance in the training data.
3.2 Maximum Likelihood Training
In our alignment approach, the features listed in
Table 3 are grouped into several classes. All ob-
served words are a class, all observed stems are
a class (Porter, 1980), and so on. We treat each
indicator feature as a separate class, and make
the assumption that classes are independent of
one another. This assumption is clearly false,
but serves as a reasonable first approximation,
similar to the independence assumption in Na??ve
Bayesian analysis. After aligning each instance,
we estimate the probability of a feature appear-
ing in a column as the simple maximum like-
lihood estimate given the observed occurrences
602
Identity Features
Word Token
Stem Word stem, derived by the Porter Stemmer
Length ?Binned? length indicators: 1, 2, 3, 4, 5-6, 7-8, 9-12, 13-18, >18 characters
Indicator Features
Capitalized Token begins with a capital
All-caps Entire token is capitalized
Hyphenated Token contains a hyphen
Numeric Entire token is numeric (e.g. 234)
Initial Numeric Token begins with a numeral (e.g. 123, 2-sided)
Endings Token ends with -al, -ble, -ed, -er, -est, -ic, -ing, -ive, -ly
Table 3: Description of the feature-set.
within its class.2 This produces a new PSSM
with which to align the next instance.
Our problem differs from alignment of biolog-
ical sequences in that we have little prior knowl-
edge of the similarity between sequences. ?Sim-
ilarity? can be defined in many ways; for bio-
logical sequences, a simple Levenshtein distance
is effective, using a matrix of substitution costs
or simple token identity (equivalent to a ma-
trix with cost 0 on the diagonal and 1 every-
where else). These matrices are constructed and
tuned by domain experts, and are used both in
choosing alignment order (i.e., which sequence
to align next) and during the actual alignment.
When aligning biological sequences, it is cus-
tomary to first calculate the pairwise distance
between each two sequences and then introduce
new sequences into the MSA in order of simi-
larity. In this way, identical sequences may be
aligned first, followed by less similar sequences
(Durbin et al, 1999).
However, we have no principled method of de-
termining the ?similarity? of two words in an NP.
We have no a priori notion of what the cost
of substituting ?two-story? for ?red? should be.
Lacking this prior knowledge, we have no opti-
mal alignment order and we must in effect learn
the substitution costs as we construct the MSA.
Therefore, we choose to add instances in the or-
der they occur in the corpus, and to iterate over
the entire MSA, re-introducing each sequence.
2We treat two special symbols for gaps and unknown
words as members of the word class.
This allows a word to ?move? from its original
column to a column which became more likely
as more sequences were aligned. Each iteration
is similar to a step in the EM algorithm: create a
model (build up an MSA and PSSM), apply the
model to the data (re-align all sequences), and
repeat. Randomly permuting the training cor-
pus did not change our results significantly, so
we believe our results are not greatly dependent
on the initial sequence order.
Instead of assigning substitution costs, we
compute the cost of aligning a word into a par-
ticular column, as follows:
C = The set of i feature classes, Ci ? C
j = Features 1 . . . |Ci| from class Ci
cnt(i, j, k) = The count of instances of
feature j from class
i in column k
?i = Laplace smoothing count
for feature class Ci
A = The number of aligned instances
f(w, i, j) =
?
??
??
1 if word w has feature j from
Ci,
0 otherwise
These help define feature positional probabilities
for column k:
p(i, j, k) =
cnt(i, j, k) + ?i
A+ ?i ? |Ci|
(1)
603
That is, the probability of feature j from class
i occurring in column k is a simple maximum-
likelihood estimate ? count the number of times
we have already aligned that feature in the col-
umn and divide by the number of sequences
aligned. We smooth that probability with sim-
ple Laplace smoothing.
We can now calculate the probability of align-
ing a word w into column k by multiplying the
product of the probabilities of aligning each of
the word?s features. Taking the negative log to
convert that probability into a cost function:
c(w, k) = ?
|C|?
i=1
|Ci|?
j=1
log (p(i, j, k) ? f(w, i, j)) (2)
Finally, we define the cost of inserting a new
column into the alignment to be equal to the
number of columns in the existing alignment,
thereby increasingly penalizing each inserted
column until additional columns become pro-
hibitively expensive.
i(j) = I ? Length of existing alignment (3)
The longest NPs aligned were 7 words, and
most ML MSAs ended with 12-14 columns.
We experimented with various column insertion
costs and values for the smoothing ? and found
no significant differences in overall performance.
3.3 Discriminative Training
We also trained a discriminative model, us-
ing the same feature-set. Discriminative train-
ing does not require division of the features
into classes or the independence assumption dis-
cussed in Section 3.2. We again produced a cost
vector for each column. We fixed the alignment
length at 8 columns, allowing alignment of the
longest instances in our test corpus.
Our training data consists of ordered se-
quences, but the model we are attempting to
learn is a set of column probabilities. Since we
have no gold-standard MSAs, we instead align
the ordered NPs with the current model and
treat the least cost alignment of the correct or-
dering as the reference for training.
We trained this model using the averaged per-
ceptron algorithm (Collins, 2002). A percep-
tron learns from classifier errors, i.e., when it
misorders an NP. At each training instance, we
align all possible permutations of the modifiers
with the MSA. If the least cost alignment does
not correspond to the correct ordering of the
modifiers, we update the perceptron to penal-
ize features occurring in that alignment and to
reward features occurring in the least cost align-
ment corresponding to the correct ordering, us-
ing standard perceptron updates.
Examining every permutation of the NP in-
volves a non-polynomial cost, but the sequences
under consideration are quite short (less than
1% of the NPs in our corpus have more than 3
modifiers, and the longest has 6; see Table 7). So
exhaustive search is practical for our problem; if
we were to apply MSA to longer sequences, we
would need to prune heavily.3
4 Evaluation
We trained and tested on the same corpus used
by Mitchell (2009), including identical 10-fold
cross-validation splits. The corpus consists of
all NPs extracted from the Penn Treebank,
the Brown corpus, and the Switchboard corpus
(Marcus et al, 1999; Kucera and Francis, 1967;
Godfrey et al, 1992). The corpus is heavily
biased toward WSJ text (74%), with approxi-
mately 13% of the NPs from each of the other
corpora.
We evaluated our system using several related
but distinct metrics, and on both modifier pairs
and full NPs.
We define:
T = The set of unique orderings found in the
test corpus
P = The set of unique orderings predicted by
the system
Type Precision (|P ? T|/|P|) measures the
probability that a predicted ordering is ?reason-
able? (where ?reasonable? is defined as orderings
which are found in the test corpus).
3The same issue arises when evaluating candidate or-
derings; see Section 4.
604
Token Accuracy Type Precision Type Recall Type F-measure
Mitchell N/A 90.3% (2.2) 67.2% (3.4) 77.1%
ML MSA 85.5% (1.0) 84.6% (1.1) 84.7% (1.1) 84.7%
Perceptron MSA 88.9% (0.7) 88.2% (0.8) 88.1% (0.8) 88.2%
Table 4: Results on the combined WSJ, Switchboard, and Brown corpus; averages and standard deviations
over a 10-fold cross validation. Winning scores are in bold.
Type Recall (|P?T|/|T|) measures the per-
centage of ?reasonable? orderings which the sys-
tem recreates.
Note that these two metrics differ only in no-
tation from those used by Mitchell (2009).
We also define a third metric, Token Accu-
racy, which measures accuracy on each individ-
ual ordering in the test corpus, rather than on
unique orderings. This penalizes producing or-
derings which are legal, but uncommon. For ex-
ample, if {a,b} occurs eight times in the test cor-
pus as <a,b> and two times as <b,a>, we will
be limited to a maximum accuracy of 80% (pre-
suming our system correctly predicts the more
common ordering). However, even though sug-
gesting <b,a> is not strictly incorrect, we gen-
erally prefer to reward a system that produces
more common orderings, an attribute not em-
phasized by type-based metrics. Our test cor-
pus does not contain many ambiguous pairings,
so our theoretical maximum token accuracy is
99.8%.
We define:
o1..N = All modifier orderings in the
test data
pred(oi) = The predicted ordering for
modifiers in oi
ai =
{
1 if pred(oi) = oi,
0 otherwise
Token Accuracy =
N?
i=0
ai
N
4.1 Pairwise Ordering
Most earlier work has focused on ordering pairs
of modifiers. The results in Table 4 are di-
rectly comparable to those found in Mitchell
(2009). Mitchell?s earlier approach does not gen-
erate a prediction when the system has insuffi-
cient evidence, and allows generation of multiple
predictions given conflicting evidence. In the-
ory, generating multiple predictions could im-
prove recall, but in practice her system appears
biased toward under-predicting, favoring preci-
sion. Our approach, in contrast, forces predic-
tion of a single ordering for each test instance,
occasionally costing some precision (in particu-
lar in cross-domain trials; see Table 5), but con-
sistently balancing recall and precision.
Our measurement of Token Accuracy is com-
parable to the accuracy measure reported in
Shaw and Hatzivassiloglou (1999) and Malouf
(2000) (although we evaluate on a different cor-
pus). Their approaches produce a single order-
ing for each test instance evaluated, so for each
incorrectly ordered modifier pair, there is a cor-
responding modifier pair in the test data that
was not predicted.
Shaw and Hatzivassiloglou found financial
text particularly difficult to order, and reported
that their performance dropped by 19% when
they included nouns as well as adjectives. Mal-
ouf?s system surpasses theirs, achieving an accu-
racy of 91.9%. However, his corpus was derived
from the BNC ? he did not attempt to order fi-
nancial text ? and he ordered only adjectives as
modifiers. In contrast, our test corpus consists
mainly of WSJ text, and we test on all forms
of prenominal modifiers. We believe this to be
a considerably more difficult task, so our peak
performance of 88.9% would appear to be ? at
worst ? quite competitive.
Table 5 presents an evaluation of cross-
domain generalization, splitting the same cor-
pus by genre ? Brown, Switchboard, and WSJ.
In each trial, we train on two genres and test on
605
Training Testing Token Type Type Type
Corpora Corpus Accuracy Precision Recall F-measure
Mitchell
Brown+WSJ Swbd N/A 94.2% 58.2% 72.0%
Swbd+WSJ Brown N/A 87.0% 51.2% 64.5%
Swbd+Brown WSJ N/A 82.4% 27.2% 40.9%
ML MSA
Brown+WSJ Swbd 74.6% 74.7% 75.3% 75.0%
Swbd+WSJ Brown 75.3% 74.7% 74.9% 74.8%
Swbd+Brown WSJ 70.2% 71.6% 71.8% 71.7%
Perceptron MSA
Brown+WSJ Swbd 77.2% 78.2% 77.6% 77.9%
Swbd+WSJ Brown 76.4% 76.7% 76.4% 76.5%
Swbd+Brown WSJ 77.9% 77.5% 77.3% 77.4%
Table 5: Cross-domain generalization.
Token Accuracy Token Precision Token Recall Token F-measure
Mitchell N/A 94.4% 78.6% (1.2) 85.7%
ML MSA 76.9% (1.6) 76.5% (1.4) 76.5% (1.4) 76.50%
Perceptron MSA 86.7% (0.9) 86.7% (0.9) 86.7% (0.9) 86.7%
Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To
compare directly with Mitchell (2009), we report token precision and recall instead of type. Our system
always proposes one and only one ordering, so token accuracy, precision, and recall are identical.
the third.4 Our results mirror those in the previ-
ous trials ? forcing a prediction costs some pre-
cision (vis-a-vis Mitchell?s 2009 system), but our
recall is dramatically higher, resulting in more
balanced performance overall.
4.2 Full NP Ordering
We now extend our analysis to ordering en-
tire NPs, a task we feel the MSA approach
should be particularly suited to, since (unlike
pairwise models) it can model positional prob-
abilities over an entire NP. To our knowledge,
the only previously reported work on this task
is Mitchell?s (2009). We train this model on
the full NP instead of on modifier pairs; this
makes little difference in pairwise accuracy, but
improves full-NP ordering considerably.
As seen in Table 6, both MSA models perform
quite well, the perceptron-trained MSA again
outperforming the maximum likelihood model.
However, we were somewhat disappointed in the
performance on longer sequences. We expected
the MSA to encode enough global information
4Note that the WSJ corpus is much larger than the
other two, comprising approximately 84% of the total.
Modifiers Frequency Token Pairwise
Accuracy Accuracy
2 89.1% 89.7% 89.7%
3 10.0% 64.5% 84.4%
4 0.9% 37.2% 80.7%
Table 7: Descriminative model performance on NPs
of various lengths, including pairwise measures.
to perform accurate full sequence ordering, but
found the accuracy drops off dramatically on
NPs with more modifiers. In fact, the accu-
racy on longer sequences is worse than we would
expect by simply extending a pairwise model.
For instance, ordering three modifiers requires
three pairwise decisions. We predict pairwise
orderings with 88% accuracy, so we would ex-
pect no worse than (.88)3, or 68% accuracy on
such sequences. However, the pairwise accu-
racy declines on longer NPs, so it underperforms
even that theoretical minimum. Sparse training
data for longer NPs biases the model strongly
toward short sequences and transitivity (which
our model does not encode) may become impor-
tant when ordering several modifiers.
606
5 Ablation Tests
We performed limited ablation testing on the
discriminative model, removing features individ-
ually and comparing token accuracy (see Table
8). We found that few of the features provided
great benefit individually; the overall system
performance remains dominated by the word.
The word and stem features appear to cap-
ture essentially the same information; note that
performance does not decline when the word
or stem features are ablated, but drops dras-
tically when both are omitted. Performance de-
clines slightly more when ending features are ab-
lated as well as words and stems, so it appears
that ? as expected ? the information captured
by ending features overlaps somewhat with lex-
ical identity. The effects of individual features
are all small and none are statistically signifi-
cant.
Feature(s) Gain/Loss
Word 0.0
Stem 0.0
Capitalization -0.1
All-Caps 0.0
Numeric -0.2
Initial-numeral 0.0
Length -0.1
Hyphen 0.0
-al 0.0
-ble -0.4
-ed -0.4
-er 0.0
-est -0.1
-ic +0.1
-ing 0.0
-ive -0.1
-ly 0.0
Word and stem -22.9
Word, stem, and endings -24.2
Table 8: Ablation test results on the discriminative
model.
6 Summary and Future Directions
We adapted MSA approaches commonly used
in computational biology to linguistic problems
and presented two novel methods for training
such alignments. We applied these techniques
to the problem of ordering prenominal modi-
fiers in noun phrases, and achieved performance
competitive with ? and in many cases, superior
to ? the best results previously reported.
In our current work, we have focused on rel-
atively simple features, which should be adapt-
able to other languages without expensive re-
sources or much linguistic insight. We are inter-
ested in exploring richer sources of features for
ordering information. We found simple morpho-
logical features provided discriminative clues for
otherwise ambiguous instances, and believe that
richer morphological features might be helpful
even in a language as morphologically impover-
ished as English. Boleda et al (2005) achieved
promising preliminary results using morphology
for classifying adjectives in Catalan.
Further, we might be able to capture some
of the semantic relationships noted by psycho-
logical analyses (Ziff, 1960; Martin, 1969) by
labeling words which belong to known seman-
tic classes (e.g., colors, size denominators, etc.).
We intend to explore deriving such labels from
resources such as WordNet or OntoNotes.
We also plan to continue exploration of MSA
training methods. We see considerable room
for refinement in generative MSA models; our
maximum likelihood training provides a strong
starting point for EM optimization, conditional
likelihood, or gradient descent methods. We are
also considering applying maximum entropy ap-
proaches to improving the discriminative model.
Finally (and perhaps most importantly), we
expect that our model would benefit from ad-
ditional training data, and plan to train on a
larger, automatically-parsed corpus.
Even in its current form, our approach im-
proves the state-of-the-art, and we believe MSA
techniques can be a useful tool for ordering
prenominal modifiers in NLP tasks.
7 Acknowledgements
This research was supported in part by NSF
Grant #IIS-0811745. Any opinions, findings,
conclusions or recommendations expressed in
this publication are those of the authors and do
not necessarily reflect the views of the NSF.
607
References
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence align-
ment. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing
- Volume 10, pages 164?171, Philadelphia. Asso-
ciation for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL), vol-
ume 15, pages 201?31, Edmonton, Canada. As-
sociation for Computational Linguistics.
Gemma Boleda, Toni Badia, and Sabine Schulte
im Walde. 2005. Morphology vs. syntax in adjec-
tive class acquisition. In Proceedings of the ACL-
SIGLEX Workshop on Deep Lexical Acquisition,
pages 77?86, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Humberto Carrillo and David Lipman. 1988. The
multiple sequence alignment problem in biol-
ogy. SIAM Journal on Applied Mathematics,
48(5):1073?1082, October.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8,
Philadelphia, July. Association for Computational
Linguistics.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1999. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, West
Nyack, NY, July.
John J. Godfrey, Edward C. Holliman, and Jane
McDaniel. 1992. SWITCHBOARD: telephone
speech corpus for research and development. In
Acoustics, Speech, and Signal Processing, IEEE
International Conference on, volume 1, pages 517?
520, Los Alamitos, CA, USA. IEEE Computer So-
ciety.
Dan Gusfield. 1997. Algorithms on Strings, Trees
and Sequences: Computer Science and Computa-
tional Biology. Cambridge University Press, West
Nyack, NY, May.
H. Kucera and W. N Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
Robert Malouf. 2000. The order of prenominal ad-
jectives in natural language generation. In Pro-
ceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 85?92,
Hong Kong, October. Association for Computa-
tional Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.
J. E. Martin. 1969. Semantic determinants of pre-
ferred adjective order. Journal of Verbal Learning
& Verbal Behavior. Vol, 8(6):697?704.
Margaret Mitchell. 2009. Class-Based ordering of
prenominal modifiers. In Proceedings of the 12th
European Workshop on Natural Language Gener-
ation (ENLG 2009), pages 50?57, Athens, Greece,
March. Association for Computational Linguis-
tics.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
James Shaw and Vasileios Hatzivassiloglou. 1999.
Ordering among premodifiers. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 135?143, Col-
lege Park, Maryland, USA, June. Association for
Computational Linguistics.
Christopher White, Izhak Shafran, and Jean luc
Gauvain. 2006. Discriminative classifiers for
language recognition. In Proceedings of the
2006 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP),
pages 213?216, Toulouse, France. IEEE.
Paul Ziff. 1960. Semantic Analysis. Cornell Univer-
sity Press, Ithaca, New York.
608
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detecting Visual Text
Jesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6
Kota Yamaguchi3, Yejin Choi3, Hal Daume? III2, Alexander C. Berg3 and Tamara L. Berg3
1University of Washington, 2University of Maryland, 3Stony Brook University
4MIT, 5Oregon Health & Science University, 6Columbia University
dodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.edu
acmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edu
kyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edu
me@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.edu
Abstract
When people describe a scene, they often in-
clude information that is not visually apparent;
sometimes based on background knowledge,
sometimes to tell a story. We aim to sepa-
rate visual text?descriptions of what is being
seen?from non-visual text in natural images
and their descriptions. To do so, we first con-
cretely define what it means to be visual, an-
notate visual text and then develop algorithms
to automatically classify noun phrases as vi-
sual or non-visual. We find that using text
alone, we are able to achieve high accuracies
at this task, and that incorporating features
derived from computer vision algorithms im-
proves performance. Finally, we show that we
can reliably mine visual nouns and adjectives
from large corpora and that we can use these
effectively in the classification task.
1 Introduction
People use language to describe the visual world.
Our goal is to: formalize what ?visual text? is (Sec-
tion 2.2); analyze naturally occurring written lan-
guage for occurrences of visual text (Section 2); and
build models that can detect visual descriptions from
raw text or from image/text pairs (Section 3). This
is a challenging problem. One challenge is demon-
strated in Figure 1, which contains two images that
contain the noun ?car? in their human-written cap-
tions. In one case (the top image), there actually is a
car in the image; in the other case, there is not: the
car refers to the state of the speaker.
The ability to automatically identify visual text is
practically useful in a number of scenarios. One can
Another dream car to
add to the list, this one
spotted in Hanbury St.
Shot out my car win-
dow while stuck in traf-
fic because people in
Cincinnati can?t drive in
the rain.
Figure 1: Two image/caption pairs, both containing the
noun ?car? but only the top one in a visual context.
imagine automatically mining image/caption data
(like that in Figure 1) to train object recognition sys-
tems. However, in order to do so reliably, one must
know whether the ?car? actually appears or not.
When building image search engines, it is common
to use text near an image as features; this is more
useful when this text is actually visual. Or when
training systems to automatically generate captions
of images (e.g., for visually impaired users), we
need good language models for visual text.
One of our goals is to define what it means for a
bit of text to be visual. As inspiration, we consider
image/description pairs automatically crawled from
Flickr (Ordonez et al, 2011). A first pass attempt
might be to say ?a phrase in the description of an
image is visual if you can see it in the corresponding
image.? Unfortunately, this is too vague to be useful;
the biggest issues are discussed in Section 2.2.
762
Based on our analysis, we settled on the follow-
ing definition: A piece of text is visual (with re-
spect to a corresponding image) if you can cut out
a part of that image, paste it into any other image,
and a third party could describe that cut-out part in
the same way. In the car example, the claim is that I
could cut out the car, put it in the middle of any other
image, and someone else might still refer to that car
as ?dream car.? The car in the bottom image in Fig-
ure 1 is not visual because there?s nothing you could
cut out that would retain car-ness.
2 Data Analysis
Before embarking on the road to building models of
visual text, it is useful to obtain a better understand-
ing of what visual text is like, and how it compares to
the more standard corpora that we are used to work-
ing with. We describe the two large data sets that we
use (one visual, one non-visual), then describe the
quantitative differences between them, and finally
discuss our annotation effort for labeling visual text.
2.1 Data sets
We use the SBU Captioned Photo Dataset (Ordonez
et al, 2011) as our primary source of image/caption
data. This dataset contains 1 million images with
user associated captions, collected in the wild by in-
telligent filtering of a huge number of Flickr pho-
tos. Past work has made use of this dataset to re-
trieve whole captions for association with a query
image (Ordonez et al, 2011). Their method first
used global image descriptors to retrieve an initial
matched set, and then applied more local estimates
of content to re-rank this (relatively small) set (Or-
donez et al, 2011). This means that content based
matching was relatively constrained by the bottle-
neck of global descriptors, and local content (e.g.,
objects) had relatively small effect on accuracy.
As an auxiliary source of information for (largely)
non-visual text, we consider a large corpus of text
obtained by concatenating ukWaC1 and the New
York Times Newswire Service (NYT) section of the
Gigaword (Graff, 2003) Corpus. The Web-derived
ukWaC is already tokenized and POS-tagged with
the TreeTagger (Schmid, 1995). NYT is tokenized,
1ukWaC is a freely available Wikipedia-derived corpus from
2009; see http://wacky.sslmit.unibo.it/doku.php.
and POS-tagged using TagChunk (Daume? III and
Marcu, 2005). This consists of 171 million sen-
tences (4 billion words). We refer to this generic
text corpus as Large-Data.
2.2 Formalizing visual text
We begin our analysis by revisiting the definition
of visual text from the introduction, and justifying
this particular definition. In order to arrive at a suf-
ficiently specific definition of ?visual text,? we fo-
cused on the applications of visual text that we care
about. As discussed in the introduction, these are:
training object detectors, building image search en-
gines and automatically generating captions for im-
ages. Our definition is based on access to image/text
pairs, but later we discuss how to talk about it purely
based on text. To make things concrete, consider an
image/text pair like that in the top of Figure 1. And
then consider a phrase in the text, like ?dream car.?
The question is: is ?dream car? visual or not?
One of the challenges in arriving at such a defi-
nition is that the description of an image in Flickr
is almost always written by the photographer of that
image. This means the descriptions often contain in-
formation that is not actually pictured in the image,
or contain references that are only relevant to the
photographer (referring to a person/pet by name).
One might think that this is an artifact of this par-
ticular dataset, but it appears to be generic to all cap-
tions, even those written by a viewer (rather than the
photographer). Figure 2 shows an image from the
Pascal dataset (Everingham et al, 2010), together
with captions written by random people collected
via crowd-sourcing (Rashtchian et al, 2010). There
is much in this caption that is clearly made-up by the
author, presumably to make the caption more inter-
esting (e.g., meta-references like ?the camera? or ?A
photo? as well as ?guesses? about the image, such as
?garage? and ?venison?).
Second, there is a question of how much inference
you are allowed to do when you say that you ?see?
something. For example, in the top image in Fig-
ure 1, the street is pictured, but does that mean that
?Hanbury St.? is visual? What if there were a street
sign that clearly read ?Hanbury St.? in the image?
This problem comes up all the time, when people
say things like ?in London? or ?in France? in their
captions. If it?s just a portrait of people ?in France,?
763
1. A distorted photo of a man cutting up a large cut of meat in a garage.
2. A man smiling at the camera while carving up meat.
3. A man smiling while he cuts up a piece of meat.
4. A smiling man is standing next to a table dressing a piece of venison.
5. The man is smiling into the camera as he cuts meat.
Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing. Measurements on the
SMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest are
non-visual (underlined). See Section 2.4 for details.
it?s hard to say that this is visual. If you see the Eif-
fel tower in the background, this is perhaps better
(though it could be Las Vegas!), but how does this
compare to a photo taken out of an airplane window
in which you actually do see France-the-country?
This problem becomes even more challenging
when you consider things other than nouns. For in-
stance, when is a verb visual? For instance, the most
common non-copula verb in our data is ?sitting,?
which appears in roughly two usages: (1) ?Took this
shot, sitting in a bar and enjoying a Portugese beer.?
and (2) ?Lexy sitting in a basket on top of her cat
tree.? The first one is clearly not visual; the second
probably is. A more nuanced case is for ?playing,?
as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-
ing in Nannie?s yard.? The corresponding image for
the latter description shows a sleeping cat.
Our final definition, based on cutting out the po-
tentially visual part of the image, allows us to say
that: (1) ?venison? is not visual (because you cannot
actually tell); (2) ?Hanbury St.? and ?Lexy? are not
visual (you can infer them, in the first case because
there is only one street and in the second case be-
cause there is only one cat); (3) that seeing the real
Eiffel tower in the background does not mean that
?France? is visual (but again, may be inferred); etc.
2.3 Most Pronounced Differences
To get an intuitive sense of how Flickr captions (ex-
pected to be predominantly visual) and generic text
(expected not to be so) differ, we computed some
simple statistics on sentences from these. In gen-
eral, the generic text had twice as many main verbs
as the Flickr data, four times as many auxiliaries or
light verbs, and about 50% more prepositions.
Flickr captions tended to have far more references
to physical objects (versus abstract objects) than the
generic text, according to the WordNet hierarchy.
Approximately 64% of the objects in Flickr were
physical (about 22% abstract and 14% unknown).
Whereas in the generic text, only 30% of the objects
were physical, 53% were abstract (17% unknown).
A third major difference between the corpora is
in terms of noun modifiers. In both corpora, nouns
tend not to have any modifiers, but modifiers are still
more prevalent in Flickr than in generic text. In par-
ticular, 60% of nouns in Flickr have zero modifiers,
but 70% of nouns in generic text have zero modi-
fiers. In Flickr, 30% of nouns have exactly one mod-
ifier, as compared to only 22% for generic text.
The breakdown of what those modifiers look like
is even more pronounced, even when restricted just
to physical objects (modifier types are obtained
through the bootstrapping process discussed in Sec-
tion 3.1). Almost 50% of nominal modifiers in the
Flickr data are color modifiers, whereas color ac-
counts for less than 5% of nominal modifiers in
generic text. In Flickr, 10% of modifiers talk about
beauty, in comparison to less than 5% in generic
text. On the other hand, less than 3% of modifiers
in Flickr reference ethnicity, as compared to almost
20% in generic text; and 20% of Flickr modifiers
reference size, versus 50% in generic text.
2.4 Annotating Visual Text
In order to obtain ground truth data, we rely on
crowdsourcing (via Amazon?s Mechanical Turk).
Each instance is an image, a paired caption, and a
highlighted noun phrase in that caption. The anno-
tation for this instance is a label of ?visual,? ?non-
visual? or ?error,? where the error category is re-
764
served for cases where the noun phrase segmenta-
tion was erroneous. Each worker is given five in-
stances to label and paid one cent per annotation.2
For a small amount of data (803 images contain-
ing 2339 instances), we obtained annotations from
three separate workers per instance to obtain higher
quality data. For a large amount of data (48k im-
ages), we obtained annotations from only a sin-
gle worker. Subsequently, we will refer to these
two data sets as the SMALL and LARGE data sets.
In both data sets, approximately 70% of the noun
phrases were visual, 28% were non-visual and 2%
were erroneous. For simplicity, we group erroneous
and non-visual for all learning and evaluation.
In the SMALL data set, the rate of disagreement
between annotators was relatively low. In 74% of the
annotations, there was no disagreement at all. We
reconciled the annotations using the quality manage-
ment technique of Ipeirotis et al (2010); only 14%
of the annotations need to be changed in order to ob-
tain a gold standard.
One immediate question raised in this process is
whether one needs to actually see the image to per-
form the annotation. In particular, if we expect an
NLP system to be able to classify noun phrases as
visual or non-visual, we need to know whether peo-
ple can do this task sans image. We therefore per-
formed the same annotation on the SMALL data set,
but where the workers were not shown the image.
Their task was to imagine an image for this caption
and then annotate the noun phrase based on whether
they thought it would be pictured or not. We ob-
tained three annotations as before and reconciled
them (Ipeirotis et al, 2010). The accuracy of this
reconciled version against the gold standard (pro-
duced by people who did see the image) was 91%.
This suggests that while people are able to do this
task with some reliability, seeing the image is very
important (recall that always guessing ?visual? leads
to an accuracy of 70%).
3 Visual Features from Raw Text
Our first goal is to attempt to obtain relatively large
knowledge bases of terms that are (predominantly)
visual. This is potentially useful in its own right
2Data available at http://hal3.name/dvt/, with direct links
back to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determine
which query terms are likely to be pictured). We
have explored two techniques for performing this
task, the first based on bootstrapping (Section 3.1)
and the second based on label propagation (Sec-
tion 3.2). We then use these lists to generate features
for a classifier that predicts whether a noun phrase?
in context?is visual or not (Section 4).
In addition, we consider the task of separating ad-
jectives into different visual categories (Section 3.3).
We have already used the results of this in Sec-
tion 2.3 to understand the differences between our
two corpora. It is also potentially useful for the
purpose of building new object detection systems or
even attribute detection systems, to get a vocabulary
of target detections.
3.1 Bootstrapping for Visual Text
In this section, we learn visual and non-visual nouns
and adjectives automatically based on bootstrapping
techniques. First, we construct a graph between ad-
jectives by computing distributional similarity (Tur-
ney and Pantel, 2010) between them. For comput-
ing distributional similarity between adjectives, each
target adjective is defined as a vector of nouns which
are modified by the target adjective. To be exact, we
use only those adjectives as modifiers which appear
adjacent to a noun (that is, in a JJ NN construction).
For example, in ?small red apple,? we consider only
red as a modifier for noun. We use Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1989)
to weight the contexts, and select the top 1000 PMI
contexts for each adjective.3
Next, we apply cosine similarity to find the top
10 distributionally similar adjectives with respect to
each target adjective based on our large generic cor-
pus (Large-Data from Section 2.1). This creates a
graph with adjectives as nodes and cosine similarity
as weight on the edges. Analogously, we construct a
graph with nouns as nodes (here, adjectives are used
as contexts for nouns).
We then apply bootstrapping (Kozareva et al,
2008) on the noun and adjective graphs by select-
ing 10 seeds for visual and non-visual nouns and
adjectives (see Table 1). We use in-degree (sum of
weights of incoming edges) to compute the score for
3We are interested in descriptive adjectives, which ?typi-
cally ascribe to a noun a value of an attribute? (Miller, 1998).
765
Visual car house tree horse animal
nouns man table bottle
seeds woman computer
Non-visual idea bravery deceit trust
nouns dedication anger humour luck
seeds inflation honesty
Visual brown green wooden striped
adjectives orange rectangular furry
seeds shiny rusty feathered
Non-visual public original whole righteous
adjectives political personal intrinsic
seeds individual initial total
Table 1: Example seeds for bootstrapping.
each node that has connections with known (seeds)
or automatically labeled nodes, previously exploited
to learn hyponymy relations from the web (Kozareva
et al, 2008). Intuitively, in-degree captures the pop-
ularity of new instances among instances that have
already been identified as good instances. We learn
visual and non-visual words together (known as the
mutual exclusion principle in bootstrapping (The-
len and Riloff, 2002; McIntosh and Curran, 2008)):
each word (node) is assigned to only one class.
Moreover, after each iteration, we harmonically de-
crease the weight of the in-degree associated with
instances learned in later iterations. We added 25
new instances at each iteration and ran 500 iterations
of bootstrapping, yielding 11955 visual and 11978
non-visual nouns, and 7746 visual and 7464 non-
visual adjectives.
Based on manual inspection, the learned visual
and non-visual lists look great. In the future, we
would like to do a Mechanical Turk evaluation to
directly evaluate the visual and non-visual nouns
and adjectives. For now, we show the coverage of
these classes in the Flickr data-set: Visual nouns:
53.71%; Non-visual nouns: 14.25%; Visual ad-
jectives: 51.79%; Non-visual adjectives: 14.40%.
Overall, we find more visual nouns and adjectives
are covered in the Flickr data-set, which makes
sense, since the Flickr data-set is largely visual.
Second, we show the coverage of these classes
on the large text corpora (Large-Data from Sec-
tion 2.1): Visual nouns: 26.05%; Non-visual nouns:
41.16%; Visual adjectives: 20.02%; Non-visual ad-
Visual: attend, buy, clean, comb, cook, drink, eat,
fry, pack, paint, photograph, smash, spill, steal,
taste, tie, touch, watch, wear, wipe
Non-visual: achieve, admire, admit, advocate, al-
leviate, appreciate, arrange, criticize, eradicate,
induce, investigate, minimize, overcome, pro-
mote, protest, relieve, resolve, review, support,
tolerate
Table 2: Predicates that are visual and non-visual.
Visual: water, cotton, food, pumpkin, chicken,
ring, hair, mouth, meeting, kind, filter, game, oil,
show, tear, online, face, class, car
Non-visual: problem, poverty, pain, issue, use,
symptom, goal, effect, thought, government,
share, stress, work, risk, impact, concern, obsta-
cle, change, disease, dispute
Table 3: Learned visual/non-visual nouns.
jectives: 40.00%. Overall, more non-visual nouns
and adjectives cover text data, since Large-Data is
a non-visual data-set.
3.2 Label Propagation for Visual Text
To propagate visual labels, we construct a bipartite
graph between visually descriptive predicates and
their arguments. Let VP be the set of nodes that cor-
responds to predicates, and let VA be the set of nodes
that corresponds to arguments. To learn the visually
descriptive words, we set VP to 20 visually descrip-
tive predicates shown in the top of Table 2, and VA
to all nouns that appear in the object argument posi-
tion with respect to the seed predicates. We approx-
imate this by taking nouns on the right hand side
of the predicates within a window of 4 words using
the Web 1T Google N-gram data (Brants and Franz.,
2006). For edge weights, we use conditional prob-
abilities between predicates and arguments so that
w(p? a) := pr(a|p) and w(a? p) := pr(p|a).
In order to collectively induce the visually de-
scriptive words from this graph, we apply the graph
propagation algorithm of Velikovich et al (2010),
a variant of label propagation algorithms (Zhu and
Ghahramani, 2002) that has been shown to be ef-
fective for inducing a web-scale polarity lexicon
based on word co-occurrence statistics. This algo-
766
Color purple blue maroon beige green
Material plastic cotton wooden metallic silver
Shape circular square round rectangular triangular
Size small big tiny tall huge
Surface coarse smooth furry fluffy rough
Direction sideways north upward left down
Pattern striped dotted checked plaid quilted
Quality shiny rusty dirty burned glittery
Beauty beautiful cute pretty gorgeous lovely
Age young mature immature older senior
Ethnicity french asian american greek hispanic
Table 4: Attribute Classes with their seed values
rithm iteratively updates the semantic distance be-
tween each pair of nodes in the graph, then produces
a score for each node that represents how visually
descriptive each word is. To learn the words that
are not visually descriptive, we use the predicates
shown in the bottom of Table 2 as VP instead. Ta-
ble 3 shows the top ranked nouns that are visually
descriptive and not visually descriptive.
3.3 Bootstrapping Visual Adjectives
Our goal in this section is to automatically gener-
ate comprehensive lists of adjectives for different at-
tributes, such as color, material, shape, etc. To our
knowledge, this is the first significant effort of this
type for adjectives: most bootstrapping techniques
focus exclusively on nouns, although Almuhareb
and Poesio (2005) populated lists of attributes us-
ing web-based similarity measures. We found that
in some ways adjectives are easier than nouns, but
require slightly different representations.
One might conjecture that listing attributes by
hand is difficult. Colors names are well known to
be quite varied. For instance, our bootstrapping
approach is able to discover colors like ?grayish,?
?chestnut,? ?emerald,? and ?rufous? that would be
hard to list manually (the last is a reddish-brown
color, somewhat like rust). Although perhaps not
easy to create, the Wikipedia list of colors (http:
//en.wikipedia.org/wiki/List of colors) includes all of these
except ?grayish?. On the other hand, it includes
color terms that might be difficult to make use of as
colors, such as ?bisque,? ?bone? and ?bubbles? (the
last is a very light cyan), which might over-generate
hits. For shape, we find ?oblong,? ?hemispherical,?
?quadrangular? and, our favorite, ?convex?.
We use essentially the same bootstrapping process
as described earlier in Section 3.1, but on a slightly
different data representation. The only difference is
that instead of linking adjectives to their 10 most
similar neighbors, we link them only to 25 neigh-
bors to attempt to improve recall.
We begin with seeds for each attribute class from
Table 4. We conduct a manual evaluation to di-
rectly measure the quality of attribute classes. We
recruited 3 annotators and developed annotation
guidelines that instructed each recruiter to judge
whether a learned value belongs to an attribute class
or not. The annotators assigned ?1? if a learned
value belongs to a class, otherwise ?0?.
We conduct an Information Retrieval (IR) Style
human evaluation. Analogous to an IR evaluation,
here the total number of relevant values for attribute
classes can not be computed. Therefore, we assume
the correct output of several systems as the total re-
call which can be produced by any system. Now,
with the help of our 3 manual annotators, we obtain
the correct output of several systems from the total
output produced by these systems.
First, we measured the agreement on whether
each learned value belongs to a semantic class or
not. We computed ? to measure inter-annotator
agreement for each pair of annotators. We focus
our evaluation on 4 classes: age, beauty, color, and
direction; between Human 2 and Human 3 and be-
tween Human 1 and Human 3, the ? value was 0.48;
between Human 1 and Human 2 it was 0.45. These
numbers are somewhat lower than we would like,
but not terrible. If we evaluate the classes individu-
ally, we find that age has the lowest ?. If we remove
?age,? the pairwise ?s rise to 0.59, 0.57 and 0.55.
Second, we compute Precision (Pr), Recall (Rec)
and F-measure (F1) for different bootstrapping sys-
tems (based on the number of iterations and the
number of new words added in each iteration).
Two parameter settings performed consistently bet-
ter than others (10 iterations with 25 items, and 5 it-
erations with 50 items). The former system achieves
a precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-
man 2; the latter achieves scores of 0.54, 0.72, 0.62.
4 Recognizing Visual Text
We train a logistic regression (aka maximum en-
tropy) model (Daume? III, 2004) to classify text as
visual or non-visual. The features we use fall into
767
the following categories: WORDS (the actual lexi-
cal items and stems); BIGRAMS (lexical bigrams);
SPELL (lexical features such as capitalization pat-
tern, and word prefixes and suffixes); WORDNET
(set of hypernyms according to WordNet); and
BOOTSTRAP (features derived from bootstrapping
or label propagation).
For each of these feature categories, we compute
features inside the phrase being categorized (e.g.,
?the car?), before the phrase (two words to the left)
and after the phrase (two words to the right). We
additionally add a feature that computes the num-
ber of words in a phrase, and a feature that com-
putes the position of the phrase in the caption (first
fifth through last fifth of the description). This leads
to seventeen feature templates that are computed for
each example. In the SMALL data set, there are 25k
features (10k non-singletons); in the LARGE data
set, there are 191k features (79k non-singletons).
To train models on the SMALL data set, we use
1500 instances as training, 200 as development and
the remaining 639 as test data. To train models on
the LARGE data set, we use 45000 instances as train-
ing and the remaining 4401 as development. We
always test on the 639 instances from the SMALL
data, since it has been redundantly annotated. The
development data is used only to choose the regular-
ization parameter for a Gaussian prior on the logis-
tic regression model; this parameter is chosen in the
range {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.
Because of the imbalanced data problem, evalu-
ating according to accuracy is not appropriate for
this task. Even evaluating by precision/recall is not
appropriate, because a baseline system that guesses
that everything is visual obtains 100% recall and
70% precision. Due to these issues, we instead
evaluate according to the area under the ROC curve
(AUC). To check statistical significance, we com-
pute standard deviations using bootstrap resampling,
and consider there to be a significant difference if a
result falls outside of two standard deviations of the
baseline (95% confidence).
Figure 3 shows learning curves for the two data
sets. The SMALL data achieves an AUC score of
71.3 in the full data setting (1700 examples); the
LARGE data needs 12k examples to achieve similar
accuracy due to noise. However, with 49k examples,
we are able to achieve a AUC score of 75.3 using the
101 102 103 104 105
0.55
0.6
0.65
0.7
0.75
0.8
Figure 3: Learning curves for training on SMALL data
(blue solid) and LARGE data (black dashed). X-axis (in
log-scale) is number of training examples; Y-axis is AUC.
large data set. By pooling the data (and weighting
the small data), this boosts results to 76.1. The con-
fidence range on these data is approximately ?1.9,
meaning that this boost is likely not significant.
4.1 Using Image Features
As discussed previously, humans are only able to
achieve 90% accuracy on the visual/non-visual task
when they are not allowed to view the image.
This potentially upper-bounds the performance of a
learned system that can only look at text. In order to
attempt to overcome this, we augment our basic sys-
tem with a number of features computed from the
corresponding images. These features are derived
from the output of state of the art vision algorithms
to detect 121 different objects, stuff and scenes.
As our object detectors, we use standard state
of the art deformable part-based models (Felzen-
szwalb et al, 2010) for 89 common object cate-
gories, including: the original 20 objects from Pas-
cal, 49 objects from Object Bank (Li-Jia Li and Fei-
Fei, 2010), and 20 from Im2Text (Ordonez et al,
2011). We additionally use coarse image parsing
to estimate background elements in each database
image. Six possible background (stuff) categories
are considered: sky, water, grass, road, tree, and
building. For this we use detectors (Ordonez et
al., 2011) which compute color, texton, HoG (Dalal
and Triggs, 2005) and Geometric Context (Hoiem
et al, 2005) as input features to a sliding win-
dow based SVM classifier. These detectors are run
on all database images, creating a large pool of
background elements for retrieval. Finally, we ob-
768
Figure 4: (Left) Highest confidence flower detected in an
image; (Right) All detections in the same image.
tain scene descriptors for each image by comput-
ing scene classification scores for 26 common scene
categories, using the features, methods and training
data from the SUN dataset (Xiao et al, 2010).
Figure 4 shows an example image on which sev-
eral detectors have been run. From each image, we
extract the following features: which object detec-
tors fired; how many times they fired; the confidence
of the most-likely firing; the percentage of the image
(in pixels) that the bounding box corresponding to
this object occupies; and the percentage of the width
(and height) of the image that it occupies.
Unfortunately, object detection is a highly noisy
process. The right image in Figure 4 shows all de-
tections for that image, which includes, for instance,
a chair detection that spans nearly the entire image,
and a person detection in the bottom-right corner.
For an average image, if a single detector (e.g., the
flower detector) fires once, it actually fires 40 times
(?? = 1.8). Moreover, of the 120 detectors, on
an average image over 22 (?? = 5.6) of them fire
at least once (though certainly in an average image
only a few objects are actually present). Exacerbat-
ing this problem, although the confidence scores for
a single detector can be compared, the scores be-
tween different detectors are not at all comparable.
In order to attenuate this problem, we include dupli-
cate copies of all the above features restricted to the
most confident object for each object type.
On the SMALL data set, this adds 400 new fea-
CATEGORY POSITION AUC
Bootstrap Phrase 65.2
+ Spell Phrase 68.6
+ Image - 69.2
+ Words Phrase 70.0
+ Length - 69.8
+ Wordnet Phrase 70.4
+ Wordnet Before 70.6
+ Spell Before 71.8
+ Words Before 72.2
+ Bootstrap Before 72.4
+ Spell After 71.5
Table 5: Results of feature ablation on SMALL data set.
Best result is in bold; results that are not statistically sig-
nificantly worse are italicized.
tures (300 of which are non-singletons4); on the
LARGE data set, this adds 500 new features (480
non-singletons). Overall, the AUC scores trained on
the small data set increase from 71.3 to 73.9 (a sig-
nificant improvement). On the large data set, the in-
crease is only from 76.1 to 76.8, which is not likely
to be significant. In general, the improvement ob-
tained by adding image features is most pronounced
in the setting of small training data, perhaps because
these features are more generic than the highly lexi-
calized features used in the textual model. But once
there is a substantial amount of text data, the noisy
image features become less useful.
4.2 Feature Ablations
In order to ascertain the degree to which each feature
template is useful, we perform an ablation study. We
first perform feature selection at the template level
using the information gain criteria, and then train
models using the corresponding subset of features.
The results on the SMALL data set are shown in
Table 5. Here, the bootstrapping features computed
on words within the phrase to be classified were
judged as the most useful, followed by spelling fea-
tures. Image features were judged third most use-
ful. In general, features in the phrase were most use-
ful (not surprisingly), and then features before the
phrase (presumably to give context, for instance as
in ?out of the window?). Features from after the
phrase were not useful.
4Non-singleton features appear more than once in the data.
769
CATEGORY POSITION AUC
Words Phrase 74.7
+ Image - 74.4
+ Bootstrap Phrase 74.3
+ Spell Phrase 75.3
+ Length - 74.7
+ Words Before 76.2
+ Wordnet Phrase 76.1
+ Spell After 76.0
+ Spell Before 76.8
+ Wordnet Before 77.0
+ Wordnet After 75.6
Table 6: Results of feature ablation on LARGE data set.
Corresponding results on the LARGE data set are
shown in Table 6. Note that the order of features
selected is different because the training data is dif-
ferent. Here, the most useful features are simply the
words in the phrase to be classified, which alone al-
ready gives an AUC score of 74.7, only a few points
off from the best performance of 77.0 once image
features, bootstrap features and spelling features are
added. As before, these features are rated as very
useful for classification performance.
Finally, we consider the effect of using Bootstrap-
based features or label-propagation-based features.
In all the above experiments, the features used
are based on the union of word lists created by
these two techniques. We perform three experi-
ments. Beginning with the system that contains all
features (SMALL=73.9, LARGE=76.8), we first re-
move the bootstrap-based features (SMALL?71.8,
LARGE?75.5) or remove the label-propagation-
based features (SMALL?71.2, LARGE?74.9) or
remove both (SMALL?70.7, LARGE?74.2). From
these results, we can see that these techniques are
useful, but somewhat redundant: if you had to
choose one, you should choose label-propagation.
5 Discussion
As connections between language and vision be-
come stronger, for instance in the contexts of ob-
ject detection (Hou and Zhang, 2007; Kim and Tor-
ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;
Gu et al, 2009), attribute detection (Ferrari and Zis-
serman, 2007; Farhadi et al, 2009; Kumar et al,
2009; Berg et al, 2010), visual phrases (Farhadi and
Sadeghi, 2011), and automatic caption generation
(Farhadi et al, 2010; Feng and Lapata, 2010; Or-
donez et al, 2011; Kulkarni et al, 2011; Yang et
al., 2011; Li et al, 2011; Mitchell et al, 2012), it
becomes increasingly important to understand, and
to be able to detect, text that actually refers to ob-
served phenomena. Our results suggest that while
this is a hard problem, it is possible to leverage large
text resources and state-of-the-art computer vision
algorithms to address it with high accuracy.
Acknowledgments
T.L. Berg and K. Yamaguchi were supported in part
by NSF Faculty Early Career Development (CA-
REER) Award #1054133; A.C. Berg and Y. Choi
were partially supported by the Stony Brook Uni-
versity Office of the Vice President for Research; H.
Daume? III and A. Goyal were partially supported by
NSF Award IIS-1139909; all authors were partially
supported by a 2011 JHU Summer Workshop.
References
B. Alexe, T. Deselaers, and V. Ferrari. 2010. What is an
object? In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pages 73 ?80.
A. Almuhareb and M. Poesio. 2005. Finding concept at-
tributes in the web. In Corpus Linguistics Conference.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteriza-
tion from noisy web data. In European Conference on
Computer Vision (ECCV).
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In CVPR.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/#daume04cg-bfgs, implementation
available at http://hal3.name/megam/, August.
770
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2010. The PASCAL
Visual Object Classes Challenge 2010 (VOC2010)
Results. http://www.pascal-network.org/challenges/VOC/
voc2010/workshop/index.html.
Ali Farhadi and Amin Sadeghi. 2011. Recognition us-
ing visual phrases. In Computer Vision and Pattern
Recognition (CVPR).
A. Farhadi, I. Endres, D. Hoiem, and D.A. Forsyth. 2009.
Describing objects by their attributes. In Computer
Vision and Pattern Recognition (CVPR).
A. Farhadi, M. Hejrati, M.A. Sadeghi, P. Young,
C. Rashtchian1, J. Hockenmaier, and D.A. Forsyth.
2010. Every picture tells a story: Generating sentences
from images. In ECCV.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
2010. Discriminatively trained deformable part
models, release 4. http://people.cs.uchicago.edu/?pff/
latent-release4/.
Y. Feng and M. Lapata. 2010. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. In Advances in Neural Information Process-
ing Systems (NIPS).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik. 2009.
Recognition using regions. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Confer-
ence on, pages 1030 ?1037.
Derek Hoiem, Alexei A. Efros, and Martial Hebert.
2005. Geometric context from a single image. In
ICCV.
Xiaodi Hou and Liqing Zhang. 2007. Saliency detection:
A spectral residual approach. In Computer Vision and
Pattern Recognition, 2007. CVPR ?07. IEEE Confer-
ence on, pages 1 ?8.
P. Ipeirotis, F. Provost, and J. Wang. 2010. Quality man-
agement on amazon mechanical turk. In Proceedings
of the Second Human Computation Workshop (KDD-
HCOMP).
Gunhee Kim and Antonio Torralba. 2009. Unsupervised
Detection of Regions of Interest using Iterative Link
Analysis. In Annual Conference on Neural Informa-
tion Processing Systems (NIPS 2009).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C
Berg, and T. L Berg. 2011. Babytalk: Understanding
and generating simple image descriptions. In CVPR.
N. Kumar, A.C. Berg, P. Belhumeur, and S.K. Nayar.
2009. Attribute and simile classifiers for face verifi-
cation. In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CONLL.
Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei. 2010. Ob-
ject bank: A high-level image representation for scene
classification and semantic feature sparsification. In
NIPS.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
K.J. Miller. 1998. Modifiers in WordNet. In C. Fell-
baum, editor, WordNet, chapter 2. MIT Press.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon?s mechanical turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements in part?of?speech tag-
ging with an application to german. In Proceedings of
the EACL SIGDAT Workshop.
J. Sivic, B.C. Russell, A. Zisserman, W.T. Freeman, and
A.A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research (JAIR),
37:141.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
771
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.
2010. Sun database: Large-scale scene recognition
from abbey to zoo. In CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In EMNLP.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
772
Proceedings of NAACL-HLT 2013, pages 1174?1184,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Generating Expressions that Refer to Visible Objects
Margaret Mitchell
Johns Hopkins HLTCOE
m.mitchell@jhu.edu
Kees van Deemter
University of Aberdeen
k.vdeemter@abdn.ac.uk
Ehud Reiter
University of Aberdeen
e.reiter@abdn.ac.uk
Abstract
We introduce a novel algorithm for generat-
ing referring expressions, informed by human
and computer vision and designed to refer to
visible objects. Our method separates abso-
lute properties like color from relative proper-
ties like size to stochastically generate a di-
verse set of outputs. Expressions generated
using this method are often overspecified and
may be underspecified, akin to expressions
produced by people. We call such expressions
identifying descriptions. The algorithm out-
performs the well-known Incremental Algo-
rithm (Dale and Reiter, 1995) and the Graph-
Based Algorithm (Krahmer et al, 2003; Vi-
ethen et al, 2008) across a variety of images
in two domains. We additionally motivate
an evaluation method for referring expression
generation that takes the proposed algorithm?s
non-determinism into account.
1 Introduction
Referring expression generation (REG) is the task
of generating an expression that can identify a ref-
erent to a listener. These expressions generally take
the form of a definite noun phrase such as ?the large
orange plate? or ?the furry running dog?. Research
in REG primarily focuses on the subtask of select-
ing a set of properties that may be used to construct
the final surface expression, e.g., ?color:orange,
size:large, type:plate?. This property selection task
is optimized to meet different goals: for example,
to be identical to those a person would generate in
the same situation, or to be unique to the intended
referent and no other item in the discourse.
We focus on the task of generating referring ex-
pressions for visible objects, specifically with the
goal of generating descriptive, human-like referring
expressions. We are motivated by the desire to con-
nect this algorithm to input from a computer vision
system, and discuss how this may work through-
out the paper. Computer vision (CV) does not yet
reliably provide features for some of the most fre-
quent properties that people use in visual descrip-
tion (in particular, size-based features), and so we
use a gold-standard visual input, evaluating purely
on REG. The proposed algorithm, which we call
the Visible Objects Algorithm, is designed to ap-
proximate human variation identifying an object in
a group of visible, real world objects.
Our primary contributions are the following.
Background for each issue is provided in Section 2:
1. An approach accounting for overspecification,
underspecification, and some of the known ef-
fects of vision on reference.
2. A function to approximate the stochastic nature
of reference. This reflects that people will pro-
duce different references to the same object.
3. A separation between absolute properties like
color, which may be detected directly by CV,
from relative properties like size and loca-
tion, which require reasoning over visual fea-
tures to determine an appropriate form (e.g.,
height/width and distance features between
pixels are available from a visual input; saying
an object is ?tall? requires further reasoning).
4. An evaluation method for non-deterministic
REG that aligns generated and observed data
and calculates accuracy over alignments.
1174
2 Motivation & Overview
Most implemented algorithms for referring expres-
sion generation focus on unique identification of a
referent, determining the set of properties that dis-
tinguish a particular target object from the other ob-
jects in the scene (the contrast set) (Dale, 1989; Re-
iter and Dale, 1992; Dale and Reiter, 1995; Krahmer
et al, 2003; Areces et al, 2008). This view of refer-
ence was first outlined by Olson (1970), ?the spec-
ification of an intended referent relative to a set of
alternatives?. A substantial body of evidence now
shows that contrastive value relative to alternatives
is not the only factor motivating speakers? property
choices, specifically in visual domains. The phe-
nomena of overspecification and redundancy, where
speakers select properties that have little or no con-
trastive value, was observed in early developmen-
tal studies in visual domains (Ford and Olson, 1975;
Whitehurst, 1976; Sonnenschein, 1985) as well as
later studies on adult speakers in visual domains
(Pechmann, 1989; Engelhardt et al, 2006; Koolen et
al., 2011). The related phenomenon of underspecifi-
cation, where speakers select a set of properties that
do not linguistically specify the referent, has also re-
ceived some attention, particularly in visual domains
(Clark et al, 1983; Kelleher et al, 2005).
These findings make sense in light of visual ev-
idence that some properties ?pop out? in the scene
(Treisman and Gelade, 1980), and speakers may be-
gin referring before scanning the full set of scene ob-
jects (Pechmann, 1989), selecting those properties
that are salient for them (Horton and Keysar, 1996;
Bard et al, 2009) without spending a great amount
of cognitive effort considering the perception of a
hearer (Keysar and Henly, 2002).
We take this evidence to suggest an approach for
a visual reference algorithm that generates natural,
human-like reference by generating visual proper-
ties that are salient for a speaker.1 We can under-
stand what is salient visually (what does the visual
system first respond to, what guides attention?), lin-
guistically (what do people tend to mention in visual
scenes?), and cognitively, which we will not have
room to discuss in this paper (what is atypical for
1We can also add functionality to ensure that a referent is
uniquely identified against the contrast set (whether or not that
reflects what a person would do), which we will describe.
Figure 1: Relative properties, like size and location, are
difficult to obtain from a two-dimensional image. We find
it easy to perceive the background object as larger than
the one in the front; but they are technically the same size
in the image (from Murray et al (2006)).
this object?); as well as in terms of broader notions
of salience, e.g., discourse salience (Krahmer and
Theune, 2002).
This suggests a paradigm shift in the generation
task when referring to visible objects, if the goal is
to produce human-like reference. In particular, this
suggests moving from selecting properties that rule
out other scene objects to selecting properties that
are salient for the speaker (visually, conversation-
ally, based on previous experiences, etc.). This mir-
rors related research on the tradeoff between audi-
ence design and egocentrism in language production
(Clark and Murphy, 1982; Horton and Keysar, 1996;
Bard et al, 2009; Gann and Barr, 2013). Under-
and overspecification naturally fall out from such an
approach, with no need to specifically model either
phenomenon.
Perhaps unsurprisingly, the set of properties that
are visually salient and the set of properties that are
linguistically salient largely overlap. Color is the
first property our visual system processes, followed
soon after by size (Murray et al, 2006; Fang et al,
2008; Schwarzkopf et al, 2010); and people tend
to use color (Pechmann, 1989; Viethen et al, 2012)
and size when identifying objects, with size com-
mon when there is another object of the same type
in the scene (Brown-Schmidt and Tanenhaus, 2006).
Following this, our algorithm gives a privileged
position to these properties, processing them first.
Using computer vision techniques to determine an
object?s color works reasonably well (Berg et al,
2011), and the relevant visual features for this task
may be useful in future work to return several pos-
sible color labels that capture differences in lexical
choice (cf. Reiter and Sripada (2002)).
Detecting size does not work well (Forsyth,
1175
2011); and when it does, it will likely not take the
form supposed in recent generation work. Most
REG algorithms use a predefined single-featured
value, such as ?big?; however, given an image-based
input, obtaining such a value requires (1) determin-
ing how the object is situated in a three-dimensional
space, difficult to obtain from a two-dimensional im-
age (see Figure 1); and (2) determining what the
value should be: object detectors currently can pro-
vide the height and width of the location where an
object is likely to exist (its bounding box), as well as
the x- and y-axis locations of the pixels within the
object detection; but a value from these features like
?big?, ?tall?, or ?long? requires further reasoning.
As such, we incorporate the top-performing size al-
gorithm introduced in Mitchell et al (2011), which
takes as input the height and widths of objects in the
image and outputs a size value or NONE, indicating
that size should not be used to describe the object.
In addition to color and size, location and orien-
tation begin to be processed early on in the visual
system (Treisman, 1985; Itti and Koch, 2001), with
our first perception of location corresponding to ba-
sic cues of where an object is relative to our focus
of attention. For an input image, this simple type of
location corresponds to surface forms such as, e.g.,
?on the right of the image? or ?at the top of the im-
age?. Along with size, location and orientation make
up the three primary relative properties that we aim
to generate language for.
After the simple forms for color, size, location,
and orientation properties are processed, our visual
system feeds forward to two parallel pathways, the
so-called ?what? and ?where? pathways (Ungerlei-
der and Mishkin, 1982), which process properties
with growing complexity. The ?what? pathway in-
cludes absolute properties like shape and material,
which computer vision has had some success de-
tecting (Ferrari and Zisserman, 2007; Farhadi et al,
2009) while the ?where? pathway corresponds to
more complex spatial orientation and location infor-
mation, such as where objects are relative to one an-
other and which way they are facing.
To begin connecting this process to the genera-
tion of human-like descriptions of visible objects,
we start with the following simplification: Color and
size have a privileged status, the first properties pro-
cessed. These are followed by the relative properties
Figure 2: Initial model for generating visual reference.
of location and orientation, which may feed forward
to more complex location and orientation properties
in one pathway; and absolute properties following
color, like material and shape, which may be pro-
cessed in another pathway.
This gives us the basic model for generating ref-
erence to visible objects shown in Figure 2. To gen-
erate reference in this model, nodes correspond to
general visual attributes and may generate forms for
visual properties (attribute:value pairs). That is, a
property such as color:red is generated from the at-
tribute node color and a property such as size:tall is
generated from the attribute node size. We are lim-
ited by existing REG corpora in which properties we
can evaluate; in this paper, we examine the effect of
the independent selection of color and size, followed
by location and orientation.2
Generating human-like expressions in this setting
begins to be possible by adopting recent propos-
als that REG handle speaker variation (Viethen and
Dale, 2010) and the non-deterministic nature of ref-
erence (van Gompel et al, 2012; van Deemter et
al., 2012b). We can capture such variation simply
by estimating ?att, the likelihood that an attribute
att generates a corresponding visual property. Dur-
ing generation, the algorithm passes through each at-
tribute node, and uses this estimate to stochastically
add each property to the output property set.
Such a non-deterministic process means that the
algorithm will not return the same output every time,
which offers new challenges for evaluation. If we
run the algorithm 1,000 times, we have a distribu-
tion over several possible output property sets. From
this we can obtain the majority set and check if it
matches the majority observed set. Similarly, we can
2We have also built an algorithm and corpus with more com-
plex properties in order to tease out further details of visual ref-
erence, but must leave these details for follow up work; for now,
we focus on the properties common to REG corpora.
1176
run the algorithm for as many instances as we have
in our test data, and see how well the property sets
it produces aligns to the observed property sets. We
discuss evaluation using both methods in Section 6.
3 The State of the Art in REG
3.1 Algorithms
In order to understand how this approach compares
to the state of the art in REG, we evaluate against
two of the most well-known algorithms, the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Graph-Based Algorithm (Krahmer et. al, 2003, as
implemented in Viethen et al, 2008). Details on
these algorithms are available in their corresponding
papers. As a brief summary, both algorithms formal-
ize the objects in the discourse as a set of properties
(attribute:value pairs). For example, one object may
be represented as ?type:box, color:red, size:large?.
The task is to find the set of properties that uniquely
specify the referent. This is known as a content se-
lection problem, and the set of properties chosen by
the algorithm is called a distinguishing description.
The Incremental Algorithm (IA) proceeds by it-
erating through attributes in a predefined order (a
preference order), and for each attribute, it checks
whether specifying a value would rule out at least
one item in the contrast set that has not already been
ruled out. If it will, the attribute:value is added to
the distinguishing description. This process contin-
ues until all contrast items (distractors) are ruled out
or all available properties have been checked. We
use the implementation of the IA available from the
NLTK (Bird et al, 2009).3
In the Graph-Based Algorithm (GB), the objects
in the discourse are represented within a labeled di-
rected graph, and content selection is a subgraph
construction problem. Each object is represented as
a vertex, with properties for an object represented as
self-edges on the object vertex, and spatial relations
between objects represented as edges between ver-
tices. The algorithm seeks to find the cheapest sub-
graph, calculated from the edge costs. We use the
implementation available from Viethen et al (2008),
which adds a preference order to decide between
edges with the same cost during search. This has
3https://github.com/nltk/nltk contrib/blob/master/
nltk contrib/referring.py retrieved 1.Aug.2012.
been one of the best-performing systems in recent
generation challenges (Gatt and Belz, 2008; Gatt et
al., 2009).
An important commonality between these algo-
rithms, and much of the work on REG that they
have influenced, is the focus on unique identifica-
tion and operating deterministically. Both produce
one property set (and only one), and stop once a tar-
get item has been uniquely identified (or else fail).
Their driving goal is to rule out distractor objects.
In the approach introduced here, the algorithm
produces a distribution over several possible out-
puts, and the initial driving mechanism is based on
likelihood estimates for each attribute independent
of the other objects in the scene, rather than ruling
out all distractors. This offers a way to capture some
aspects of human-like reference, including under-
and overspecification and speaker variation. Due to
the fundamentally different objective of this algo-
rithm, we will call the kind of expression it generates
an identifying description, following Searle (1969).
This is a description that the system finds (1) useful
to describe the referent and (2) true of the referent.
4 The Algorithm
The Visible Objects Algorithm iterates through lists
of visible attributes, stochastically adding properties
to the property set it will generate. After this initial
search, the algorithm then scans through the objects
in the scene, roughly corresponding to how people
scan a scene when referring (Pechmann, 1989). The
target referent type, corresponding to the head noun
in the final generated description, is added to the
property set at the end of the algorithm.
We represent the basic components of the algo-
rithm graphically in Figure 3. Full code is available
online.4 After START, the algorithm proceeds in par-
allel through a list of absolute attributes and a list
of relative attributes. The likelihood of generating a
property is a function of the prior likelihood ?att and
?, a penalty on the length of the constructed prop-
erty set up to that point. This ensures that only a few
properties are generated for a referent, and the ex-
pression will not be too complex. This is also in line
with recent research suggesting that there are rarely
more than three adjectives in a visual noun phrase
4https://github.com/itallow/VisibleObjectsAlgorithm.
1177
(Berg et al, 2011). Once the algorithm hits END,
it scans through the objects in the scene. If it finds
an object that is the same type as the referent object,
the algorithm checks through the attributes again in
a preference order akin to the IA, comparing the ob-
ject?s properties against the referent?s and generating
properties as a function of the length penalty alone.
If the algorithm does not find an object that it is the
same type, no further properties are added.
4.1 Requirements
The algorithm requires the following:
1. Prior likelihood estimates on the inclusion of
different attributes. Represented as ?att.
2. Ordered list of absolute attributes beyond color.
Represented as AP.
3. Ordered list of relative attributes beyond size.
Represented as RP.
4. Ordered list of all attributes. Represented as P.
5. Ordered list specifying the order in which to
scan through other scene objects. The current
implementation uses the order in which the ob-
jects are listed in the corpora it is run on.
(1) is similar to the cost functions for GB, but
attributes are selected non-deterministically using
prior likelihoods. (2), (3), and (4) are similar to
the IA?s and GB?s preference order. For our eval-
uation corpora, AP is empty and RP contains loca-
tion and orientation. (5) is novel to this algorithm,
defining an order in which to compare the target ob-
ject against other objects in the scene. This is in-
spired by the process of incremental speech produc-
tion (Pechmann, 1989), where speakers scan objects
during naming, incrementally producing properties.
4.2 The Stochastic Process
Generally speaking, we want to penalize longer de-
scriptions and encourage the attributes that we know
people are likely to use. We can encourage a likely
attribute by using its prior likelihood as an estimate
of whether to include it. We can penalize longer de-
scriptions with a penalty proportional to the length
of the property set under construction. In other
words, given a prior likelihood estimate for includ-
ing an attribute att, ?att, and the property set con-
structed so farA, we compute whether to add a prop-
a. b.
TUNA corpus GRE3D3 corpus
Figure 4: Example scenes from corpora.
erty for att toA as a function of ?att and the length-
based penalty ?:
f(A ? {x}) = ??att
where
? =
{
1
?|A| if |A| > 0
1 otherwise
and ? is an empirically determined weight. The
algorithm then chooses a random number n, 0 ?
n ? 1. If n < f(A ? {x}), it adds the property.
4.3 Scanning Through Objects
After the initial pass through the properties, the al-
gorithm compares each object in the scene that is
the same type as the target. If the values for an
attribute are different, then the corresponding prop-
erty is added to the property set based on the length
penalty alone; when the goal is unique identification,
the algorithm can use no penalty. In development,
we found that incrementally scanning through ob-
jects after initially adding properties resulted in bet-
ter performance than an algorithm that did not con-
tain this step.
4.4 Worked Example
Suppose the input in Figure 6 (visualized in Figure
4a), with the goal of referring to obj1 by producing
a property set A. First, the algorithm scans through
color and size in parallel. For color, it finds the cor-
responding value grey; with a computer vision in-
put, this would be possible using the object pixels
as features. There is no length penalty at this point
(|A|=0), so it adds the property color:grey to A with
likelihood ?color. For our evaluation domains, ?color
is around .90 across folds, and so a color property is
usually added.
For size, the algorithm finds an appropriate value
using the Size Algorithm from Mitchell et al (2011).
The Size Algorithm uses the average height and
1178
Figure 3: Basic model for generating visual reference.
width of all objects that are the same type as the ref-
erent object; in this case, obj2, obj3, obj4. This re-
turns a size value large, and so the property size:large
is added toAwith likelihood ?size (around .40 to .70
across folds, depending on the domain).
The most likely property set at this point is sim-
ply ?color:grey?. The next most likely is ?color:grey,
size:large?, then ?size:large?. There are no fur-
ther absolute properties in this example, but there
are values for the relative attributes loc (location)
and ori (orientation). Assuming RP=?location,
orientation?, the algorithm first analyzes location,
then orientation. A location property is added to A
with likelihood ?loc multiplied by the length penalty
?= 1(??1) if A=?color:grey?; ?=
1
(??2) if A=?color:grey,
size:large?, etc.; and an orientation property is added
to A with likelihood ?ori multiplied by the length
penalty ?= 1(??1) if the property set is ?color:grey?,
etc. At this point, the likelihood of adding further
properties quickly diminishes.
Once all properties have been analyzed, the algo-
rithm scans through the objects in the scene. For
each object obj2. . . objn, if the object is the same
type as the target object obj1, then any different
property of the target referent is added to A with
a likelihood based on the length penalty alone ?.
?type:desk? is added at the end.
For this example scene, the algorithm will gen-
erate the property sets ?color:grey, type:desk?,
?color:grey, size:large, type:desk?, ?size:large,
type:desk?, ?color:grey, ori:front, type:desk?,
?color:grey, loc:(3, 1), type:desk?, etc., with dif-
ferent frequencies. Due to the length penalty,
generated property sets will almost never have more
than 3 properties.
tg color:yellow size:(63,63) type:ball loc:right-hand
lm color:red size:(345,345) type:cube loc:right-hand
obj3 color:yellow size:(70,70) type:cube loc:left-hand
Figure 5: Example input scene: GRE3D3 corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
obj1 colour:grey size:(454,454) type:desk loc:(3,1) ori:front
obj2 colour:blue size:(454,454) type:desk loc:(2,1) ori:front
obj3 colour:red size:(454,454) type:desk loc:(3,2) ori:back
obj4 colour:green size:(254,254) type:desk loc:(4,1) ori:left
obj5 colour:blue size:(454,454) type:fan loc:(1,1) ori:front
obj6 colour:red size:(454,454) type:fan loc:(5,1) ori:back
obj7 colour:green size:(254,254) type:fan loc:(2,2) ori:left
Figure 6: Example input scene: TUNA corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
As such, although ?color:grey, type:desk? would
sufficiently distinguish the intended referent, we
instead produce a variety of sets, overspecify-
ing in some instances (e.g., ?color:grey, ori:front,
type:desk?), and with a small chance of underspec-
ifying in others (e.g., ?size:large, type:desk?).
5 Evaluation Algorithms & Corpora
5.1 Corpora
We evaluate on two well-known REG corpora, the
GRE3D3 corpus (Viethen and Dale, 2008) and the
singular furniture section of the TUNA corpus (van
Deemter et al, 2006). Both corpora contain expres-
sions elicited to computer-generated objects, and so
provide a reasonable starting point for evaluating
reference to visible objects. For all algorithms, we
evaluate on the selection of referent attributes. Lex-
ical choice and word order are not taken into ac-
count. Example images from GRE3D3 and TUNA
are shown in Figure 4, and example algorithm input
1179
from these corpora are shown in Figures 5 and 6.
In GRE3D3, we evaluate on the selection of type,
color, size, and location, but leave aside proper-
ties of relatum objects, which are not currently ad-
dressed by this algorithm or the IA. In TUNA, we
evaluate on the selection of type, color, size and
orientation.5
5.2 Algorithms
5.2.1 The Incremental Algorithm
The Incremental Algorithm requires a preference
order list (PO) specifying the order to iterate through
scene attributes. We determine the preference or-
der from corpus frequencies using cross-validation
to hold out a test scene and list attributes from the
training scenes in descending order. We find that
color precedes size in the preference orders, in line
with recent research showing that this allows the al-
gorithm to perform optimally on the TUNA corpus
(van Deemter et al, 2012a). In development, we find
that IA performs best with type as the last attribute
in the PO, and report on numbers with this approach.
5.2.2 The Graph-Based Algorithm
The version of the Graph-Based Algorithm that
we use is available from Viethen et al (2008). This
algorithm requires (1) a set of cost functions for each
edge, and (2) a PO for deciding between properties
in the case of a tie. For (1), we use the method from
Theune et al (2011) to assign two costs (0, 1) to
the edges. We first determine the relative frequency
with which each property is mentioned for a target
object, and then create costs for each property using
k-means clustering (k=2) in the Weka toolkit (Hall
et al, 2009). We refer interested readers to the The-
une et al paper for further details. For (2), we follow
the same method as for the Incremental Algorithm.
5.2.3 The Visual Objects Algorithm
The proposed algorithm requires ?att, which we
estimate as the relative frequency of each attribute
att in the training data. The ordered attribute lists for
the algorithm (AP, RP and P) are built in the same
way as the preference order list for the IA and GB,
listing attributes from the training data in order of
5We remove location from evaluation in this corpus. Lo-
cation is not annotated directly, but split such that only x-
dimension or y-dimension may be marked for a reference.
descending frequency. For these corpora, there are
not absolute properties beyond color, so AP is empty.
6 Evaluation
Previous evaluation of REG algorithms have used
measurements such as Uniqueness, Minimality,
Dice (Belz and Gatt, 2008), and Accuracy (Gatt et
al., 2009; Reiter and Belz, 2009). Uniqueness is
the proportion of outputs that identify the referent
uniquely, and Minimality is the proportion of out-
puts that are both minimal and unique. As our goal
is to mimic human reference, these metrics are not
as useful for the evaluations as the others.
The Dice metric provides a value for the similar-
ity between a generated description and a human-
produced description, and therefore serves as a rea-
sonable objective measure for how human-like the
produced sets are. Given the generated property set
(DS) and the human-produced property set (DH ),
Dice is calculated as:
2? |DS ?DH |
|DS |+ |DH |
For each input domain, we evaluate over boolean
values (included or excluded) for the attributes D
(see Table 1). Note that this means the specific val-
ues for the attributes are not compared. In this for-
mulation based on boolean values, |DS |=|DH |=|D|
and Dice reduces to:
|DS ?DH |
|D|
Calculating Dice over the same number of at-
tributes for both the observed and generated data
has the nice mathematical property of making Dice
equal to other common metrics for evaluating a
model, including Accuracy, Precision, and Recall.6
Since the proposed algorithm is stochastic, this in-
troduces a problem in using a metric that compares
single expressions. We therefore seek to find the
best alignment between the set of expressions pro-
duced by the algorithm and the set of expressions
produced by people. We formulate this alignment as
an assignment problem weighted by Dice. For the
corpus of observed property sets H and the corpus
of generated property sets S, we find the best align-
6A false positive is a false negative, and there are no true
negatives, so all four metrics are equivalent.
1180
Example Corresponding Evaluated
Expression Property Set Property Set
the red ball ?color:red, type:ball? type:1 color:1
size:0 loc:0
Table 1: Example human expression and corresponding
boolean-valued property set for evaluation in GRE3D3,
with D={type, color, size, and location}.
ment x out of all possible alignmentsX between the
corpora:
argmaxx?X
?
(S,H)?x
Dice(DS , DH )
This may be solved in polynomial time using the
Hungarian method (Kuhn, 1955; Munkres, 1957).
Note that because IA and GB are deterministic, find-
ing an optimal alignment is trivial. We call this
method ALIGNED DICE.
It is an open question whether an alignment-based
evaluation is fair: the proposed algorithm has more
than one chance to match the human descriptions.
In the second evaluation method (MAJORITY) we
address this issue, comparing how often the most
frequent generated set compares with the most fre-
quent observed set. We run the proposed algorithm
1,000 times, and the generated property sets are or-
dered by frequency. The most frequent generated
set is compared against the most frequent human-
produced set. The majority score is the percentage
of folds where these two sets match. For IA and FB,
the most frequent generated set is the only gener-
ated set. This is a simple way to fairly compare the
output of deterministic and non-deterministic algo-
rithms. There are no ties in the generated sets, but
in the case of a tie in the observed data, we count a
match if any match the most frequent generated set.
6.1 GRE3D3
We randomly select two scenes (7, 9) from Set 1
and their mirrored counterparts in Set 2 (17, 19) for
development. We empirically determine ?=5 for the
length-based penalty ? in the proposed algorithm.
We use the eight remaining scenes in each Set
for eight-fold cross-validation, estimating parame-
ters for the algorithms on the seven training scenes
in each fold, as discussed in Section 5.2.
For ALIGNED DICE, we run the proposed algo-
rithm five times in each fold and report the average
Algorithm
ALIGNED DICE MAJORITY
Set 1 Set 2 Set 1 Set 2
Proposed Alg. 88.23 90.06 62.50 50.00
IA 87.71 85.13 62.50 25.00
GB 87.71 88.73 62.50 50.00
Table 2: GRE3D3: Results (in %).
Algorithm
ALIGNED DICE MAJORITY
+LOC -LOC +LOC -LOC
Proposed Alg. 88.75 86.07 40.00 40.00
IA 81.79 81.55 0.00 100.00
GB 75.36 66.04 20.00 20.00
Table 3: TUNA: Results (in %).
score. Results are shown in Table 2.7
The proposed Visible Objects Algorithm achieves
higher accuracy than either version of the Incremen-
tal Algorithm or the Graph-Based Algorithm using
ALIGNED DICE. In MAJORITY, the Graph-Based
and the Visible Objects Algorithm both predict the
majority property set in this evaluation at least 50%
of the time. The algorithm is competitive with the
state of the art on this corpus.
6.2 TUNA
TUNA is split into two conditions: subjects discour-
aged to use location (-LOC) or not (+LOC). We ran-
domly hold out two scenes from both conditions (1
and 2), and find a value of ?=5 again works well on
the development data.
As in the GRE3D3 corpus, we use the TUNA
scenes in five-fold cross-validation, estimating pa-
rameters on the four training scenes in each fold. For
ALIGNED DICE, we average over five runs of the al-
gorithm, and for MAJORITY, we run the proposed
algorithm 1,000 times for each test scene.
Results are shown in Table 3. Again we see that
the proposed Visible Objects Algorithm is compet-
itive with the IA and GB for both ALIGNED DICE
and MAJORITY. GB performs poorly here, and this
may be due to the data sparsity issue that arises when
requiring the algorithm to train on properties.8 In
7We do not report statistical significance; the proposed algo-
rithm produces several possible outputs for one input, while the
IA and GB produce only one.
8The original property-based weighting approach (Theune
et al, 2011; Koolen et al, 2012, see Section 5.2) trained on ob-
ject collections that were identical to their test data in all proper-
ties except x- and y-dimension, and so this was less of an issue.
We hope to explore whether basing weights on attributes alone
1181
MAJORITY, the Visible Objects Algorithm is rela-
tively stable across conditions, generating the ma-
jority property set in 40% of the test scenes. It does
not outperform the IA in the -LOC condition, but the
IA has a large range across the two conditions (0%
and 100%).
7 Conclusions and Future Work
We have introduced a new algorithm for generating
referring expressions, inspired by human and com-
puter vision and aiming to refer in a human-like way
to visible objects. The algorithm successfully gener-
ates the most common attributes that people choose
for different objects, and offers a varied output to
capture speaker variation. In contrast to most algo-
rithms for the generation of referring expressions,
which have aimed to produce distinguishing descrip-
tions when these exist (Krahmer and van Deemter,
2012), the core idea behind this algorithm is to gen-
erate what is likely for a speaker in a visual domain.
Since the driving mechanism behind the algorithm
is not to uniquely identify the object, but rather to
pipeline the analysis of properties in a way similar
to human visual processing, the generated expres-
sion may be overspecified or underspecified.
We are limited by available REG corpora to re-
liably assess methods for generating more com-
plex absolute properties like shape and material, but
adding such properties would help advance the gen-
eration of human-like reference in visual scenes and
offers further points of connection between the gen-
eration process and computer vision property detec-
tion. Models for generating more complex spatial
relations are currently available, and are a natural
extension to this framework (e.g., those of Kelleher
and Costello (2009)) as object detection becomes
more robust.
We may also be able to build more sophisticated
graphical models as larger corpora become avail-
able. For example, modeling the conditional proba-
bility of generating reference for a property vn given
the previously generated context p(vn|v1 . . . vn?1)
may bring us closer to human-like output.
There are several additional issues that do not
arise in this evaluation, but we expect must be ac-
counted for when referring to naturalistic objects in
improves performance.
visual domains. These include:
? The interconnected nature of properties, where
some properties entail others; for example, a
wooden object is likely to be called wooden, re-
ferring to its material, rather than tan or brown.
? The role of typicality, where properties are se-
lected because they are atypical for the object.
? Referring to more complex properties, e.g., ma-
terial, texture, etc., and object parts.
? Better methods for determining the length
penalty and attribute likelihoods.
We hope to discuss extensions to this algorithm
covering these aspects of reference in future work.
Acknowledgments
Funding for this research has been provided by
SICSA and ORSAS. We thank the anonymous re-
viewers for useful comments on this paper.
References
Carlos Areces, Alexander Koller, and Kristina Striegnitz.
2008. Referring expressions as formulas of descrip-
tion logic. Proceedings of the 5th International Nat-
ural Language Generation Conference (INLG 2008),
pages 42?29.
Ellen Gurman Bard, Robin Hill, Manabu Arai, and
Mary Ellen Foster. 2009. Accessibility and attention
in situated dialogue: Roles and regulations. Proceed-
ings of the Workshop on the Production of Referring
Expressions (PRE-CogSci 2009).
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrin-
sic evaluation measures for referring expression gen-
eration. Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL
2008), pages 197?200.
Alexander C. Berg, Tamara L. Berg, Hal Daume? III,
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, and Kota Yam-
aguchi. 2011. An exploration of how to learn from
visually descriptive text. JHU-CLSP Summer Work-
shop Whitepaper.
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc., Sebastopol, CA.
Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006.
Watching the eyes when talking about size: An investi-
gation of message formulation and utterance planning.
Journal of Memory and Language, 54:592?609.
1182
Herbert H. Clark and Gregory L. Murphy. 1982. Audi-
ence Design in Meaning and Reference. In J. F. LeNy
and W. Kintsch, editors, Language and Comprehen-
sion, volume 9 of Advances in Psychology, pages 287?
299. North-Holland, Amsterdam.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understanding
of demonstrative reference. Journal of Verbal Learn-
ing and Verbal Behavior, 22:245?258.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the gricean maxims in the generation
of referring expressions. Cognitive Science, 19:233?
263.
Robert Dale. 1989. Cooking up referring expressions.
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 1989).
P. E. Engelhardt, K. Bailey, and F. Ferreira. 2006. Do
speakers and listeners observe the gricean maxim of
quantity? Journal of Memory and Language, 54:554?
573.
Fang Fang, Huseyin Boyaci, Daniel Kersten, and Scott O.
Murray. 2008. Attention-dependent representation
of a size illusion in human V1. Current biology,
18(21):1707?1712.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR 2009).
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. Advances in Neural Information Processing
Systems (NIPS 2007).
William Ford and David Olson. 1975. The elabora-
tion of the noun phrase in children?s description of ob-
jects. The Journal of Experimental Child Psychology,
19:371?382.
David A. Forsyth. 2011. Personal communica-
tion. Video clip of communication available from:
http://vimeo.com/40553150. At 1:06:46.
T. M. Gann and D. J. Barr. 2013. Speaking from expe-
rience: Audience design as expert performance. Lan-
guage and Cognitive Processes. In press.
Albert Gatt and Anja Belz. 2008. Attribute selec-
tion for referring expression generation: New algo-
rithms and evaluation methods. Proceedings of 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 50?58.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA
REG challenge 2009: Overview and evaluation results.
Proceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 174?
182.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
William S. Horton and Boaz Keysar. 1996. When do
speakers take into account common ground? Cogni-
tion, 59(1):91?117.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neuro-
science, 2:194?203.
John Kelleher and Fintan Costello. 2009. Applying
computational models of spatial prepositions to vi-
sually situated dialog. Computational Linguistics,
35(2):271?306.
John Kelleher, Fintan Costello, and Josef van Genabith.
2005. Dynamically structuring, updating and interre-
lating representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62?102.
Boaz Keysar and Anne S. Henly. 2002. Speakers? over-
estimation of their effectiveness. Psychological Sci-
ence, 13(3):207?212.
Ruud Koolen, Martijn Goudbeek, and Emiel Krahmer.
2011. Effects of scene variation on referential over-
specification. Proceedings of the 33rd Annual Meeting
of the Cognitive Science Society (CogSci 2011).
Ruud Koolen, Emiel Krahmer, and Marie?t Theune. 2012.
Learning preferences for referring expression genera-
tion: Effects of domain, language and algorithm. Pro-
ceedings of the 7th International Workshop on Natural
Language Generation (INLG 2012).
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
Information Sharing: Reference and Presupposition
in Language Generation and Interpretation, 143:223?
263.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173?218.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly,
2:83?97.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
Proceedings of the 13th European Workshop on Natu-
ral Language Generation (ENLG 2011).
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of Industrial and
Applied Mathematics, 5(1):32?38.
Scott O. Murray, Huseyin Boyaci, and Daniel Kersten.
2006. The representation of perceived angular size in
human primary visual cortex. Nature Neuroscience,
9(3):429?434.
1183
David R. Olson. 1970. Language and thought: Aspects
of a cognitive theory of semantics. Psychological Re-
view, 77:257?273.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:89?110.
Ehud Reiter and Anja Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Computa-
tional Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1992. A fast algorithm
for the generation of referring expressions. Proceed-
ings of the 14th International Conference on Compu-
tational Linguistics (COLING 1992), 1:232?238.
Ehud Reiter and Somayajulu Sripada. 2002. Human
variation and lexical choice. Computational Linguis-
tics, 28:545?553.
D. Samuel Schwarzkopf, Chen Song, and Geraint Rees.
2010. The surface area of human V1 predicts the
subjective experience of object size. Nature Neuro-
science, 14(1):28?30.
J. R. Searle. 1969. Speech Acts: An Essay in the Philos-
ophy of Language. Cambridge University Press, Cam-
bridge.
Susan Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489?508.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? how much
data is required to train a REG algorithm? Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL 2011).
Anne M. Treisman and Garry Gelade. 1980. A feature
integration theory of attention. Cognitive Psychology,
12:97?13.
Anne Treisman. 1985. Preattentive processing in vi-
sion. Computer Vision, Graphics, and Image Process-
ing, 31:156177.
L. G. Ungerleider and M. Mishkin. 1982. Two Corti-
cal Visual Systems. In D. J. Ingle, M. Goodale, and
R. J. W. Mansfield, editors, Analysis of Visual Be-
haviour, chapter 18, pages 549?586. The MIT Press.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. Proceedings
of the 4th International Conference on Natural Lan-
guage Generation (INLG 2006).
Kees van Deemter, Albert Gatt, Ielka van der Sluis, and
Richard Power. 2012a. Generation of referring ex-
pressions: Assessing the incremental algorithm. Cog-
nitive Science, 36(5):799?836.
Kees van Deemter, Emiel Krahmer, Roger van Gompel,
and Albert Gatt. 2012b. Towards a computational psy-
cholinguistics of reference production. TopiCS: Pro-
duction of Referring Expressions - Bridging the Gap
between Computational and Empirical Approaches to
Reference.
Roger P. G. van Gompel, Albert Gatt, Emiel Krahmer,
and Kees van Deemter. 2012. PRO: A computational
model of referential overspecification. Architectures
and Mechanisms for Language Processing (AMLaP
2012).
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 59?67.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. Proceedings of the 8th Australasian Lan-
guage Technology Workshop (ALTW 2010), pages 81?
89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t The-
une, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008).
Jette Viethen, Martijn Goudbeek, and Emiel Krahmer.
2012. The impact of colour difference and coloure
codability on reference production. Proceedings of the
34th Annual Meeting of the Cognitive Science Society
(CogSci 2012).
G. J. Whitehurst. 1976. The development of communi-
cation: Changes with age and modeling. Child Devel-
opment, 47:473?482.
1184
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 236?241,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Modeling for Prenominal Modifier Ordering
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
In this paper, we argue that ordering prenom-
inal modifiers ? typically pursued as a su-
pervised modeling task ? is particularly well-
suited to semi-supervised approaches. By
relying on automatic parses to extract noun
phrases, we can scale up the training data
by orders of magnitude. This minimizes
the predominant issue of data sparsity that
has informed most previous approaches. We
compare several recent approaches, and find
improvements from additional training data
across the board; however, none outperform
a simple n-gram model.
1 Introduction
In any given noun phrase (NP), an arbitrary num-
ber of nominal modifiers may be used. The order of
these modifiers affects how natural or fluent a phrase
sounds. Determining a natural ordering is a key task
in the surface realization stage of a natural language
generation (NLG) system, where the adjectives and
other modifiers chosen to identify a referent must be
ordered before a final string is produced. For ex-
ample, consider the alternation between the phrases
?big red ball? and ?red big ball?. The phrase ?big
red ball? provides a basic ordering of the words big
and red. The reverse ordering, in ?red big ball?,
sounds strange, a phrase that would only occur in
marked situations. There is no consensus on the ex-
act qualities that affect a modifier?s position, but it is
clear that some modifier orderings sound more natu-
ral than others, even if all are strictly speaking gram-
matical.
Determining methods for ordering modifiers
prenominally and investigating the factors underly-
ing modifier ordering have been areas of consider-
able research, including work in natural language
processing (Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000; Mitchell, 2009; Dunlop et al, 2010), lin-
guistics (Whorf, 1945; Vendler, 1968), and psychol-
ogy (Martin, 1969; Danks and Glucksberg, 1971).
A central issue in work on modifier ordering is how
to order modifiers that are unobserved during sys-
tem development. English has upwards of 200,000
words, with over 50,000 words in the vocabulary of
an educated adult (Aitchison, 2003). Up to a quar-
ter of these words may be adjectives, which poses a
significant problem for any system that attempts to
categorize English adjectives in ways that are useful
for an ordering task. Extensive in-context observa-
tion of adjectives and other modifiers is required to
adequately characterize their behavior.
Developers of automatic modifier ordering sys-
tems have thus spent considerable effort attempting
to make reliable predictions despite sparse data, and
have largely limited their systems to order modifier
pairs instead of full modifier strings. Conventional
wisdom has been that direct evidence methods such
as simple n-gram modeling are insufficient for cap-
turing such a complex and productive process.
Recent approaches have therefore utilized in-
creasingly sophisticated data-driven approaches.
Most recently, Dunlop et al (2010) used both dis-
criminative and generative methods for estimat-
ing class-based language models with multiple-
sequence alignments (MSA). Training on manually
curated syntactic corpora, they showed excellent in-
domain performance relative to prior systems, and
decent cross-domain generalization.
However, following a purely supervised training
approach for this task is unduly limiting and leads
to conventional assumptions that are not borne out
in practice, such as the inapplicability of simple n-236
gram models. NP segmentation is one of the most
reliable annotations that automatic parsers can now
produce, and may be applied to essentially arbitrary
amounts of unlabeled data. This yields orders-of-
magnitude larger training sets, so that methods that
are sensitive to sparse data and/or are domain spe-
cific can be trained on sufficient data.
In this paper, we compare an n-gram language
model and a hidden Markov model (HMM) con-
structed using expectation maximization (EM) with
several recent ordering approaches, and demonstrate
superior performance of the n-gram model across
different domains, particularly as the training data
size is scaled up. This paper presents two important
results: 1) N-gram modeling performs better than
previously believed for this task, and in fact sur-
passes current class-based systems.1 2) Automatic
parsers can effectively provide essentially unlimited
training data for learning modifier ordering prefer-
ences. Our results point the way to larger scale data-
driven approaches to this and related tasks.
2 Related Work
In one of the earliest automatic prenominal mod-
ifier ordering systems, Shaw and Hatzivassiloglou
(1999) ordered pairs of modifiers, including adjec-
tives, nouns (?baseball field?); gerunds, (?running
man?); and participles (?heated debate?). They
described a direct evidence method, a transitivity
method, and a clustering method for ordering these
different kinds of modifiers, with the transitivity
technique returning the highest accuracy of 90.67%
on a medical text. However, when testing across
domains, their accuracy dropped to 56%, not much
higher than random guessing.
Malouf (2000) continued this work, ordering
prenominal adjective pairs in the BNC. He aban-
doned a bigram model, finding it achieved only
75.57% prediction accuracy, and instead pursued
statistical and machine learning techniques that are
more robust to data sparsity. Malouf achieved an
accuracy of 91.85% by combining three systems.
However, it is not clear whether the proposed or-
dering approaches extend to other kinds of modi-
fiers, such as gerund verbs and nouns, and he did
not present analysis of cross-domain generalization.
1But note that these approaches may still be useful, e.g.,
when the goal is to construct general modifier classes.
Dataset 2 mods 3 mods 4 mods
WSJ 02-21 auto 10,070 1,333 129
WSJ 02-21 manu 9,976 1,311 129
NYT 1,616,497 191,787 18,183
Table 1: Multi-modifier noun phrases in training data
Dataset 2 mods 3 mods 4 mods
WSJ 22-24 1,366 152 20
SWBD 1,376 143 19
Brown 1,428 101 9
Table 2: Multi-modifier noun phrases in testing data
Later, Mitchell (2009) focused on creating a class-
based model for modifier ordering. Her system
mapped each modifier to a class based on the fre-
quency with which it occurs in different prenominal
positions, and ordered unseen sequences based on
these classes. Dunlop et al (2010) used a Multiple
Sequence Alignment (MSA) approach to order mod-
ifiers, achieving the highest accuracy to date across
different domains. In contrast to earlier work, both
systems order full modifier strings.
Below, we evaluate these most recent systems,
scaling up the training data by several orders of mag-
nitude. Our results indicate that an n-gram model
outperforms previous systems, and generalizes quite
well across different domains.
3 Corpora
Following Dunlop et al (2010), we use the Wall St.
Journal (WSJ), Switchboard (SWBD) and Brown
corpus sections of the Penn Treebank (Marcus et al,
1993) as our supervised training and testing base-
lines. For semi-supervised training, we automati-
cally parse sections 02-21 of the WSJ treebank using
cross-validation methods, and scale up the amount
of data used by parsing the New York Times (NYT)
section of the Gigaword (Graff and Cieri, 2003) cor-
pus using the Berkeley Parser (Petrov and Klein,
2007; Petrov, 2010).
Table 1 lists the NP length distributions for each
training corpus. The WSJ training corpus yields just
under 5,100 distinct modifier types (without normal-
izing for capitalization), while the NYT data yields
105,364. Note that the number of NPs extracted
from the manual and automatic parses of the WSJ
are quite close. We find that the overlap between the
two groups is well over 90%, suggesting that extract-237
ing NPs from a large, automatically parsed corpus
will provide phrases comparable to manually anno-
tated NPs.
We evaluate across a variety of domains, includ-
ing (1) the WSJ sections 22-24, and sections com-
mensurate in size of (2) the SWBD corpus and (3)
the Brown corpus. Table 2 lists the NP length distri-
butions for each test corpus.
4 Methods
In this section, we present two novel prenominal
modifier ordering approaches: a 5-gram model and
an EM-trained HMM. In both systems, modifiers
that occur only once in the training data are given the
Berkeley parser OOV class labels (Petrov, 2010).
In Section 5, we compare these approaches to the
one-class system described in Mitchell (2010) and
the discriminative MSA described in Dunlop et al
(2010). We refer the interested reader to those pa-
pers for the details of their learning algorithms.
4.1 N-Gram Modeling
We used the SRILM toolkit (Stolcke, 2002) to build
unpruned 5-gram models using interpolated mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998). In the testing phase, each possible permuta-
tion is assigned a probability by the model, and the
highest probability sequence is chosen.
We explored building n-gram models based on
entire observed sequences (sentences) and on ex-
tracted multiple modifier NPs. As shown in Table
3, we found a very large (12% absolute) accuracy
improvement in a model trained with just NP se-
quences. This is likely due to several factors, in-
cluding the role of the begin string symbol <s>,
which helps to capture word preferences for occur-
ring first in a modifier sequence; also the behav-
ior of modifiers when they occur in NPs may dif-
fer from how they behave in other contexts. Note
that the full-sentence n-gram model performs sim-
ilarly to Malouf?s bigram model; although the re-
sults are not directly comparable, this may explain
the common impression that n-gram modeling is not
effective for modifier ordering. We find that syntac-
tic annotations are critical for this task; all n-gram
results presented in the rest of the paper are trained
on extracted NPs.
Training data for n-gram model Accuracy
Full sentences 75.9
Extracted multi-modifier NPs 88.1
Table 3: Modifier ordering accuracy on WSJ sections 22-
24, trained on sections 2-21
4.2 Hidden Markov Model
Mitchell?s single-class system and Dunlop et. al?s
MSA approach both group tokens into position clus-
ters. The success of these systems suggests that a
position-specific class-based HMM might perform
well on this task. We use EM (Dempster et al, 1977)
to learn the parameterizations of such an HMM.
The model is defined in terms of state transition
probabilities P(c? | c), i.e., the probability of transi-
tioning from a state labeled c to a state labeled c?;
and state observation probabilities P(w | c), i.e.,
the probability of emitting word w from a particu-
lar class c. Since the classes are predicting an or-
dering, we include hard constraints on class tran-
sitions. Specifically, we forbid a transition from a
class closer to the head noun to one farther away.
More formally, if the subscript of a class indicates
its distance from the head, then for any i, j, P(ci |
cj) = 0 if i ? j; i.e., ci is stipulated to never occur
closer to the head than cj .
We established 8 classes and an HMM Markov
order of 1 (along with start and end states) based
on performance on a held-out set (section 00 of the
WSJ treebank). We initialize the model with a uni-
form distribution over allowed transition and emis-
sion probabilities, and use add-? regularization in
the M-step of EM at each iteration. We empirically
determined ? smoothing values of 0.1 for emissions
and 500 for transitions. Rather than training to full
convergence of the corpus likelihood, we stop train-
ing when there is no improvement in ordering accu-
racy on the held-out dataset for five iterations, and
output the best scoring model.
Because of the constraints on transition probabil-
ities, straightforward application of EM leads to the
transition probabilities strongly skewing the learn-
ing of emission probabilities. We thus followed a
generalized EM procedure (Neal and Hinton, 1998),
updating only emission probabilities until no more
improvement is achieved, and then training both
emission and transition probabilities. Often, we238
WSJ Accuracy SWBD Accuracy Brown Accuracy
Training data Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA
WSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5
auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6
NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.4
20% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.1
50% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6
all 92.4 80.2 89.3 92.1 86.4 74.5 81.4 83.4 82.3 69.3 79.3 82.0
NYT+WSJ auto 93.7 81.1 89.7 92.2 86.3 74.5 81.3 83.4 82.3 69.3 79.3 81.8
Table 4: Results on WSJ sections 22-24, Switchboard test set, and Brown test set for n-gram model (Ngr), Mitchell?s
single-class system (1-cl), HMM and MSA systems, under various training conditions.
find no improvement with the inclusion of transition
probabilities, and they are left uniform. In this case,
test ordering is determined by the class label alone.
5 Empirical results
Several measures have been used to evaluate the
accuracy of a system?s modifier ordering, includ-
ing both type/token accuracy, pairwise accuracy, and
full string accuracy. We evaluate full string ordering
accuracy over all tokens in the evaluation set. For
every NP, if the model?s highest-scoring ordering is
identical to the actual observed order, it is correct;
otherwise, it is incorrect. We report the percentage
of orders correctly predicted.
We evaluate under a variety of training conditions,
on WSJ sections 22-24, as well as the testing sec-
tions from the Switchboard and Brown corpus por-
tions of the Penn Treebank. We perform no domain-
specific tuning, so the results on the Switchboard
and Brown corpora demonstrate cross-domain appli-
cability of the approaches.
5.1 Manual parses versus automatic parses
We begin by comparing the NPs extracted from
manual parses to those extracted from automatic
parses. We parsed Wall Street Journal sections 02
through 21 using cross-validation to ensure that the
parses are as errorful as when sentences have never
been observed by training.
Table 4 compares models trained on these two
training corpora, as evaluated on the manually-
annotated test set. No system?s accuracy degrades
greatly when using automatic parses, indicating that
we can likely derive useful training data by automat-
ically parsing a large, unlabeled training corpus.
5.2 Semi-supervised models
We now evaluate performance of the models on the
scaled up training data. Using the Berkeley parser,
we parsed 169 million words of NYT text from the
English Gigaword corpus (Graff and Cieri, 2003),
extracted the multiple modifier NPs, and trained our
various models on this data. Rows 3-6 of Table
4 show the accuracy on WSJ sections 22-24 after
training on 10%, 20%, 50% and 100% of this data.
Note that this represents approximately 150 times
the amount of training data as the original treebank
training data. Even with just 10% of this data (a
15-fold increase in the training data), we see across
the board improvements. Using all of the NYT data
results in approximately 5% absolute performance
increase for the n-gram and MSA models, yielding
roughly commensurate performance, over 92% ac-
curacy. Although we do not have space to present
the results in this paper, we found further improve-
ments (over 1% absolute, statistically significant) by
combining the four models, indicating a continued
benefit of the other models, even if none of them
best the n-gram individually.
Based on these results, this task is clearly
amenable to semi-supervised learning approaches.
All systems show large accuracy improvements.
Further, contrary to conventional wisdom, n-gram
models are very competitive with recent high-
accuracy frameworks. Additionally, n-gram models
appear to be domain sensitive, as evidenced by the
last row of Table 4, which presents results when the
1.8 million NPs in the NYT corpus are augmented
with just 11 thousand NPs from the WSJ (auto) col-
lection. The n-gram model still outperforms the
other systems, but improves by well over a percent,
while the class-based HMM and MSA approaches239
are relatively static. (The single-class system shows
some domain sensitivity, improving nearly a point.)
5.3 Cross-domain evaluation
With respect to cross-domain applicability, we see
that, as with the WSJ evaluation, the MSA and n-
gram approaches are roughly commensurate on the
Brown corpus; but the n-gram model shows a greater
advantage on the Switchboard test set when trained
on the NYT data. Perhaps this is due to higher re-
liance on conventionalized collocations in the spo-
ken language of Switchboard. Finally, it is clear
that the addition of the WSJ data to the NYT data
yields improvements only for the specific newswire
domain ? none of the results change much for these
two new domains when the WSJ data is included
(last row of the table).
We note that the improvements observed when
scaling the training corpus with in-domain data per-
sist when applied to very diverse domains. Interest-
ingly, n-gram models, which may have been consid-
ered unlikely to generalize well to other domains,
maintain their superior performance in each trial.
6 Discussion
In this paper, we demonstrated the efficacy of scal-
ing up training data for prenominal modifier or-
dering using automatic parses. We presented two
novel systems for ordering prenominal modifiers,
and demonstrated that with sufficient data, a simple
n-gram model outperforms position-specific models,
such as an EM-trained HMM and the MSA approach
of Dunlop et al (2010). The accuracy achieved by
the n-gram model is particularly interesting, since
such models have previously been considered inef-
fective for this task. This does not obviate the need
for a class based model ? modifier classes may in-
form linguistic research, and system combination
still yields large improvements ? but points to new
data-rich methods for learning such models.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
References
Jean Aitchison. 2003. Words in the mind: an intro-
duction to the mental lexicon. Blackwell Publishing,
Cornwall, United Kindgom, third edition. p. 7.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
Joseph H. Danks and Sam Glucksberg. 1971. Psycho-
logical scaling of adjective order. Journal of Verbal
Learning and Verbal Behavior, 10(1):63?67.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Se-
ries B, 39(1):1?38.
Aaron Dunlop, Margaret Mitchell, and Brian Roark.
2010. Prenominal modier ordering via multiple se-
quence alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL (HLT-NAACL 2010), pages 600?
608, Los Angeles, CA, USA. Association for Compu-
tational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. Linguistic Data Consortium, Philadelphia, PA,
USA.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings of
the 38th ACL (ACL 2000), pages 85?92, Hong Kong.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
J. E. Martin. 1969. Semantic determinants of preferred
adjective order. Journal of Verbal Learning and Verbal
Behavior, 8(6):697?704.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 50?57, Athens, Greece. Associa-
tion for Computational Linguistics.
Margaret Mitchell. 2010. A flexible approach to class-
based ordering of prenominal modifiers. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, volume 5980 of Lec-
ture Notes in Computer Science. Springer, Berlin /
Heidelberg.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. Kluwer Academic Publish-
ers, Dordrecht.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American240
Chapter of the ACL (HLT-NAACL 2007), pages 404?
411, Rochester, NY, USA. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Berkeley parser. GNU General Pub-
lic License v.2.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
ACL (ACL 1999), pages 135?143, College Park, Mary-
land. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP
2002), volume 2, pages 901?904.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton, The Netherlands.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1?11.
241
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177?1187,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Resource Semantic Role Labeling
Matthew R. Gormley
1
Margaret Mitchell
2
Benjamin Van Durme
1
Mark Dredze
1
1
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21211
2
Microsoft Research
Redmond, WA 98052
mrg@cs.jhu.edu | memitc@microsoft.com | vandurme@cs.jhu.edu | mdredze@cs.jhu.edu
Abstract
We explore the extent to which high-
resource manual annotations such as tree-
banks are necessary for the task of se-
mantic role labeling (SRL). We examine
how performance changes without syntac-
tic supervision, comparing both joint and
pipelined methods to induce latent syn-
tax. This work highlights a new applica-
tion of unsupervised grammar induction
and demonstrates several approaches to
SRL in the absence of supervised syntax.
Our best models obtain competitive results
in the high-resource setting and state-of-
the-art results in the low resource setting,
reaching 72.48% F1 averaged across lan-
guages. We release our code for this work
along with a larger toolkit for specifying
arbitrary graphical structure.
1
1 Introduction
The goal of semantic role labeling (SRL) is to
identify predicates and arguments and label their
semantic contribution in a sentence. Such labeling
defines who did what to whom, when, where and
how. For example, in the sentence ?The kids ran
the marathon?, ran assigns a role to kids to denote
that they are the runners; and a role to marathon to
denote that it is the race course.
Models for SRL have increasingly come to rely
on an array of NLP tools (e.g., parsers, lem-
matizers) in order to obtain state-of-the-art re-
sults (Bj?orkelund et al, 2009; Zhao et al, 2009).
Each tool is typically trained on hand-annotated
data, thus placing SRL at the end of a very high-
resource NLP pipeline. However, richly annotated
data such as that provided in parsing treebanks is
expensive to produce, and may be tied to specific
domains (e.g., newswire). Many languages do
1
http://www.cs.jhu.edu/
?
mrg/software/
not have such supervised resources (low-resource
languages), which makes exploring SRL cross-
linguistically difficult.
The problem of SRL for low-resource lan-
guages is an important one to solve, as solutions
pave the way for a wide range of applications: Ac-
curate identification of the semantic roles of enti-
ties is a critical step for any application sensitive to
semantics, from information retrieval to machine
translation to question answering.
In this work, we explore models that minimize
the need for high-resource supervision. We ex-
amine approaches in a joint setting where we
marginalize over latent syntax to find the optimal
semantic role assignment; and a pipeline setting
where we first induce an unsupervised grammar.
We find that the joint approach is a viable alterna-
tive for making reasonable semantic role predic-
tions, outperforming the pipeline models. These
models can be effectively trained with access to
only SRL annotations, and mark a state-of-the-art
contribution for low-resource SRL.
To better understand the effect of the low-
resource grammars and features used in these
models, we further include comparisons with (1)
models that use higher-resource versions of the
same features; (2) state-of-the-art high resource
models; and (3) previous work on low-resource
grammar induction. In sum, this paper makes
several experimental and modeling contributions,
summarized below.
Experimental contributions:
? Comparison of pipeline and joint models for
SRL.
? Subtractive experiments that consider the re-
moval of supervised data.
? Analysis of the induced grammars in un-
supervised, distantly-supervised, and joint
training settings.
1177
Modeling contributions:
? Simpler joint CRF for syntactic and semantic
dependency parsing than previously reported.
? New application of unsupervised grammar
induction: low-resource SRL.
? Constrained grammar induction using SRL
for distant-supervision.
? Use of Brown clusters in place of POS tags
for low-resource SRL.
The pipeline models are introduced in ? 3.1 and
jointly-trained models for syntactic and semantic
dependencies (similar in form to Naradowsky et
al. (2012)) are introduced in ? 3.2. In the pipeline
models, we develop a novel approach to unsu-
pervised grammar induction and explore perfor-
mance using SRL as distant supervision. The joint
models use a non-loopy conditional random field
(CRF) with a global factor constraining latent syn-
tactic edge variables to form a tree. Efficient exact
marginal inference is possible by embedding a dy-
namic programming algorithm within belief prop-
agation as in Smith and Eisner (2008).
Even at the expense of no dependency path fea-
tures, the joint models best pipeline-trained mod-
els for state-of-the-art performance in the low-
resource setting (? 4.4). When the models have ac-
cess to observed syntactic trees, they achieve near
state-of-the-art accuracy in the high-resource set-
ting on some languages (? 4.3).
Examining the learning curve of the joint and
pipeline models in two languages demonstrates
that a small number of labeled SRL examples may
be essential for good end-task performance, but
that the choice of a good model for grammar in-
duction has an even greater impact.
2 Related Work
Our work builds upon research in both seman-
tic role labeling and unsupervised grammar in-
duction (Klein and Manning, 2004; Spitkovsky
et al, 2010a). Previous related approaches to se-
mantic role labeling include joint classification of
semantic arguments (Toutanova et al, 2005; Jo-
hansson and Nugues, 2008), latent syntax induc-
tion (Boxwell et al, 2011; Naradowsky et al,
2012), and feature engineering for SRL (Zhao et
al., 2009; Bj?orkelund et al, 2009).
Toutanova et al (2005) introduced one of
the first joint approaches for SRL and demon-
strated that a model that scores the full predicate-
argument structure of a parse tree could lead to
significant error reduction over independent clas-
sifiers for each predicate-argument relation.
Johansson and Nugues (2008) and Llu??s et al
(2013) extend this idea by coupling predictions of
a dependency parser with predictions from a se-
mantic role labeler. In the model from Johans-
son and Nugues (2008), the outputs from an SRL
pipeline are reranked based on the full predicate-
argument structure that they form. The candidate
set of syntactic-semantic structures is reranked us-
ing the probability of the syntactic tree and seman-
tic structure. Llu??s et al (2013) use a joint arc-
factored model that predicts full syntactic paths
along with predicate-argument structures via dual
decomposition.
Boxwell et al (2011) and Naradowsky et al
(2012) observe that syntax may be treated as la-
tent when a treebank is not available. Boxwell
et al (2011) describe a method for training a se-
mantic role labeler by extracting features from a
packed CCG parse chart, where the parse weights
are given by a simple ruleset. Naradowsky et
al. (2012) marginalize over latent syntactic depen-
dency parses.
Both Boxwell et al (2011) and Naradowsky
et al (2012) suggest methods for SRL without
supervised syntax, however, their features come
largely from supervised resources. Even in their
lowest resource setting, Boxwell et al (2011) re-
quire an oracle CCG tag dictionary extracted from
a treebank. Naradowsky et al (2012) limit their
exploration to a small set of basic features, and
included high-resource supervision in the form
of lemmas, POS tags, and morphology available
from the CoNLL 2009 data.
There has not yet been a comparison of tech-
niques for SRL that do not rely on a syntactic
treebank, and no exploration of probabilistic mod-
els for unsupervised grammar induction within an
SRL pipeline that we have been able to find.
Related work for the unsupervised learning of
dependency structures separately from semantic
roles primarily comes from Klein and Manning
(2004), who introduced the Dependency Model
with Valence (DMV). This is a robust generative
model that uses a head-outward process over word
classes, where heads generate arguments.
Spitkovsky et al (2010a) show that Viterbi
(hard) EM training of the DMV with simple uni-
form initialization of the model parameters yields
higher accuracy models than standard soft-EM
1178
  
ParsingModel SemanticDependencyModelCorpusText
Text LabeledWith SemanticRoles
Train Time, Constrained Grammar Induction:Observed Constraints
Figure 1: Pipeline approach to SRL. In this sim-
ple pipeline, the first stage syntactically parses the
corpus, and the second stage predicts semantic
predicate-argument structure for each sentence us-
ing the labels of the first stage as features. In our
low-resource pipelines, we assume that the syntac-
tic parser is given no labeled parses?however, it
may optionally utilize the semantic parses as dis-
tant supervision. Our experiments also consider
?longer? pipelines that include earlier stages: a
morphological analyzer, POS tagger, lemmatizer.
training. In Viterbi EM, the E-step finds the max-
imum likelihood corpus parse given the current
model parameters. The M-step then finds the
maximum likelihood parameters given the corpus
parse. We utilize this approach to produce unsu-
pervised syntactic features for the SRL task.
Grammar induction work has further demon-
strated that distant supervision in the form of
ACE-style relations (Naseem and Barzilay, 2011)
or HTML markup (Spitkovsky et al, 2010b)
can lead to considerable gains. Recent work in
fully unsupervised dependency parsing has sup-
planted these methods with even higher accuracies
(Spitkovsky et al, 2013) by arranging optimiz-
ers into networks that suggest informed restarts
based on previously identified local optima. We do
not reimplement these approaches within the SRL
pipeline here, but provide comparison of these
methods against our grammar induction approach
in isolation in ? 4.5.
In both pipeline and joint models, we use fea-
tures adapted from state-of-the-art approaches to
SRL. This includes Zhao et al (2009) features,
who use feature templates from combinations
of word properties, syntactic positions including
head and children, and semantic properties; and
features from Bj?orkelund et al (2009), who utilize
features on syntactic siblings and the dependency
path concatenated with the direction of each edge.
Features are described further in ? 3.3.
3 Approaches
We consider an array of models, varying:
1. Pipeline vs. joint training (Figures 1 and 2)
2. Types of supervision
3. The objective function at the level of syntax
3.1 Unsupervised Syntax in the Pipeline
Typical SRL systems are trained following a
pipeline where the first component is trained on
supervised data, and each subsequent component
is trained using the 1-best output of the previous
components. A typical pipeline consists of a POS
tagger, dependency parser, and semantic role la-
beler. In this section, we introduce pipelines that
remove the need for a supervised tagger and parser
by training in an unsupervised and distantly super-
vised fashion.
Brown Clusters We use fully unsupervised
Brown clusters (Brown et al, 1992) in place of
POS tags. Brown clusters have been used to good
effect for various NLP tasks such as named entity
recognition (Miller et al, 2004) and dependency
parsing (Koo et al, 2008; Spitkovsky et al, 2011).
The clusters are formed by a greedy hierachi-
cal clustering algorithm that finds an assignment
of words to classes by maximizing the likelihood
of the training data under a latent-class bigram
model. Each word type is assigned to a fine-
grained cluster at a leaf of the hierarchy of clusters.
Each cluster can be uniquely identified by the path
from the root cluster to that leaf. Representing this
path as a bit-string (with 1 indicating a left and 0
indicating a right child) allows a simple coarsen-
ing of the clusters by truncating the bit-strings. We
train 1000 Brown clusters for each of the CoNLL-
2009 languages on Wikipedia text.
2
Unsupervised Grammar Induction Our first
method for grammar induction is fully unsuper-
vised Viterbi EM training of the Dependency
Model with Valence (DMV) (Klein and Manning,
2004), with uniform initialization of the model pa-
rameters. We define the DMV such that it gener-
ates sequences of word classes: either POS tags
or Brown clusters as in Spitkovsky et al (2011).
The DMV is a simple generative model for pro-
jective dependency trees. Children are generated
recursively for each node. Conditioned on the par-
ent class, the direction (right or left), and the cur-
rent valence (first child or not), a coin is flipped to
decide whether to generate another child; the dis-
tribution over child classes is conditioned on only
the parent class and direction.
2
The Wikipedia text was tokenized for Polyglot (Al-Rfou?
et al, 2013): http://bit.ly/embeddings
1179
Constrained Grammar Induction Our second
method, which we will refer to as DMV+C, in-
duces grammar in a distantly supervised fashion
by using a constrained parser in the E-step of
Viterbi EM. Since the parser is part of a pipeline,
we constrain it to respect the downstream SRL an-
notations during training. At test time, the parser
is unconstrained.
Dependency-based semantic role labeling can
be described as a simple structured prediction
problem: the predicted structure is a labeled di-
rected graph, where nodes correspond to words
in the sentence. Each directed edge indicates that
there is a predicate-argument relationship between
the two words; the parent is the predicate and the
child the argument. The label on the edge indi-
cates the type of semantic relationship. Unlike
syntactic dependency parsing, the graph is not re-
quired to be a tree, nor even a connected graph.
Self-loops and crossing arcs are permitted.
The constrained syntactic DMV parser treats
the semantic graph as observed, and constrains the
syntactic parent to be chosen from one of the se-
mantic parents, if there are any. In some cases,
imposing this constraint would not permit any pro-
jective dependency parses?in this case, we ignore
the semantic constraint for that sentence. We parse
with the CKY algorithm (Younger, 1967; Aho and
Ullman, 1972) by utilizing a PCFG corresponding
to the DMV (Cohn et al, 2010). Each chart cell al-
lows only non-terminals compatible with the con-
strained sets. This can be viewed as a variation of
Pereira and Schabes (1992).
Semantic Dependency Model As described
above, semantic role labeling can be cast as a
structured prediction problem where the structure
is a labeled semantic dependency graph. We de-
fine a conditional random field (CRF) (Lafferty et
al., 2001) for this task. Because each word in a
sentence may be in a semantic relationship with
any other word (including itself), a sentence of
length n has n
2
possible edges. We define a single
L+1-ary variable for each edge, whose value can
be any of L semantic labels or a special label indi-
cating there is no predicate-argument relationship
between the two words. In this way, we jointly
perform identification (determining whether a se-
mantic relationship exists) and classification (de-
termining the semantic label). This use of an L+1-
ary variable is in contrast to the model of Narad-
owsky et al (2012), which used a more complex
  
DEPTREE
Dep
1,1
Role
1,1
Role
1,2
Role
1,3
Role
n,n
Dep
1,2
Dep
1,3
Dep
n,n ...
 ...
Figure 2: Factor graph for the joint syntac-
tic/semantic dependency parsing model.
set of binary variables and required a constraint
factor permitting AT-MOST-ONE. We include one
unary factor for each variable.
We optionally include additional variables that
perform word sense disambiguation for each pred-
icate. Each has a unary factor and is completely
disconnected from the semantic edge (similar to
Naradowsky et al (2012)). These variables range
over all the predicate senses observed in the train-
ing data for the lemma of that predicate.
3.2 Joint Syntactic and Semantic Parsing
Model
In Section 3.1, we introduced pipeline-trained
models for SRL, which used grammar induction
to predict unlabeled syntactic parses. In this sec-
tion, we define a simple model for joint syntactic
and semantic dependency parsing.
This model extends the CRF model in Section
3.1 to include the projective syntactic dependency
parse for a sentence. This is done by includ-
ing an additional n
2
binary variables that indicate
whether or not a directed syntactic dependency
edge exists between a pair of words in the sen-
tence. Unlike the semantic dependencies, these
syntactic variables must be coupled so that they
produce a projective dependency parse; this re-
quires an additional global constraint factor to en-
sure that this is the case (Smith and Eisner, 2008).
The constraint factor touches all n
2
syntactic-edge
variables, and multiplies in 1.0 if they form a pro-
jective dependency parse, and 0.0 otherwise. We
couple each syntactic edge variable to its semantic
edge variable with a binary factor. Figure 2 shows
the factor graph for this joint model.
Note that our factor graph does not contain any
loops, thereby permitting efficient exact marginal
inference just as in Naradowsky et al (2012). We
1180
Property Possible values
1 word form all word forms
2 lower case word form all lower-case forms
3 5-char word form prefixes all 5-char form prefixes
4 capitalization True, False
5 top-800 word form top-800 word forms
6 brown cluster 000, 1100, 010110001, ...
7 brown cluster, length 5 length 5 prefixes of brown clusters
8 lemma all word lemmas
9 POS tag NNP, CD, JJ, DT, ...
10 morphological features Gender, Case, Number, ...
(different across languages)
11 dependency label SBJ, NMOD, LOC, ...
12 edge direction Up, Down
Table 1: Word and edge properties in templates.
i, i-1, i+1 noFarChildren(w
i
) linePath(w
p
, w
c
)
parent(w
i
) rightNearSib(w
i
) depPath(w
p
, w
c
)
allChildren(w
i
) leftNearSib(w
i
) depPath(w
p
, w
lca
)
rightNearChild(w
i
) firstVSupp(w
i
) depPath(w
c
, w
lca
)
rightFarChild(w
i
) lastVSupp(w
i
) depPath(w
lca
, w
root
)
leftNearChild(w
i
) firstNSupp(w
i
)
leftFarChild(w
i
) lastNSupp(w
i
)
Table 2: Word positions used in templates. Based
on current word position (i), positions related to
current word w
i
, possible parent, child (w
p
, w
c
),
lowest common ancestor between parent/child
(w
lca
), and syntactic root (w
root
).
train our CRF models by maximizing conditional
log-likelihood using stochastic gradient descent
with an adaptive learning rate (AdaGrad) (Duchi
et al, 2011) over mini-batches.
The unary and binary factors are defined with
exponential family potentials. In the next section,
we consider binary features of the observations
(the sentence and labels from previous pipeline
stages) which are conjoined with the state of the
variables in the factor.
3.3 Features for CRF Models
Our feature design stems from two key ideas.
First, for SRL, it has been observed that fea-
ture bigrams (the concatenation of simple fea-
tures such as a predicate?s POS tag and an ar-
gument?s word) are important for state-of-the-art
(Zhao et al, 2009; Bj?orkelund et al, 2009). Sec-
ond, for syntactic dependency parsing, combining
Brown cluster features with word forms or POS
tags yields high accuracy even with little training
data (Koo et al, 2008).
We create binary indicator features for each
model using feature templates. Our feature tem-
plate definitions build from those used by the top
performing systems in the CoNLL-2009 Shared
Task, Zhao et al (2009) and Bj?orkelund et al
(2009) and from features in syntactic dependency
parsing (McDonald et al, 2005; Koo et al, 2008).
Template Possible values
relative position before, after, on
distance, continuity Z
+
binned distance > 2, 5, 10, 20, 30, or 40
geneological relationship parent, child, ancestor, descendant
path-grams the NN went
Table 3: Additional standalone templates.
Template Creation Feature templates are de-
fined over triples of ?property, positions, order?.
Properties, listed in Table 1, are extracted from
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word w
i
include
its syntactic parent, its leftmost farthest child
(leftFarChild), its rightmost nearest sibling (rightNearSib),
etc. Following Zhao et al (2009), we include the
notion of verb and noun supports and sections of
the dependency path. Also following Zhao et al
(2009), properties from a set of positions can be
put together in three possible orders: as the given
sequence, as a sorted list of unique strings, and re-
moving all duplicated neighbored strings. We con-
sider both template unigrams and bigrams, com-
bining two templates in sequence.
Additional templates we include are the relative
position (Bj?orkelund et al, 2009), geneological re-
lationship, distance (Zhao et al, 2009), and binned
distance (Koo et al, 2008) between two words in
the path. From Llu??s et al (2013), we use 1, 2, 3-
gram path features of words/POS tags (path-grams),
and the number of non-consecutive token pairs in
a predicate-argument path (continuity).
3.4 Feature Selection
Constructing all feature template unigrams and bi-
grams would yield an unwieldy number of fea-
tures. We therefore determine the top N template
bigrams for a dataset and factor a according to an
information gain measure (Martins et al, 2011):
IG
a,m
=
?
f?T
m
?
x
a
p(f, x
a
) log
2
p(f, x
a
)
p(f)p(x
a
)
where T
m
is the mth feature template, f is a par-
ticular instantiation of that template, and x
a
is an
assignment to the variables in factor a. The proba-
bilities are empirical estimates computed from the
training data. This is simply the mutual informa-
tion of the feature template instantiation with the
variable assignment.
This filtering approach was treated as a sim-
ple baseline in Martins et al (2011) to contrast
with increasingly popular gradient based regular-
ization approaches. Unlike the gradient based ap-
1181
proaches, this filtering approach easily scales to
many features since we can decompose the mem-
ory usage over feature templates.
As an additional speedup, we reduce the dimen-
sionality of our feature space to 1 million for each
clique using a common trick referred to as fea-
ture hashing (Weinberger et al, 2009): we map
each feature instantiation to an integer using a hash
function
3
modulo the desired dimentionality.
4 Experiments
We are interested in the effects of varied super-
vision using pipeline and joint training for SRL.
To compare to prior work (i.e., submissions to the
CoNLL-2009 Shared Task), we also consider the
joint task of semantic role labeling and predicate
sense disambiguation. Our experiments are sub-
tractive, beginning with all supervision available
and then successively removing (a) dependency
syntax, (b) morphological features, (c) POS tags,
and (d) lemmas. Dependency syntax is the most
expensive and difficult to obtain of these various
forms of supervision. We explore the importance
of both the labels and structure, and what quantity
of supervision is useful.
4.1 Data
The CoNLL-2009 Shared Task (Haji?c et al, 2009)
dataset contains POS tags, lemmas, morpholog-
ical features, syntactic dependencies, predicate
senses, and semantic roles annotations for 7 lan-
guages: Catalan, Chinese, Czech, English, Ger-
man, Japanese,
4
Spanish. The CoNLL-2005 and
-2008 Shared Task datasets provide English SRL
annotation, and for cross dataset comparability we
consider only verbal predicates (more details in
? 4.4). To compare with prior approaches that use
semantic supervision for grammar induction, we
utilize Section 23 of the WSJ portion of the Penn
Treebank (Marcus et al, 1993).
4.2 Feature Template Sets
Our primary feature set IG
C
consists of 127 tem-
plate unigrams that emphasize coarse properties
(i.e., properties 7, 9, and 11 in Table 1). We also
explore the 31 template unigrams
5
IG
B
described
3
To reduce hash collisions, We use MurmurHash v3
https://code.google.com/p/smhasher.
4
We do not report results on Japanese as that data was
only made freely available to researchers that competed in
CoNLL 2009.
5
Because we do not include a binary factor between pred-
icate sense and semantic role, we do not include sense as a
by Bj?orkelund et al (2009). Each of IG
C
and IG
B
also include 32 template bigrams selected by in-
formation gain on 1000 sentences?we select a
different set of template bigrams for each dataset.
We compare against the language-specific fea-
ture sets detailed in the literature on high-resource
top-performing SRL systems: From Bj?orkelund et
al. (2009), these are feature sets for German, En-
glish, Spanish and Chinese, obtained by weeks of
forward selection (B
de,en,es,zh
); and from Zhao et
al. (2009), these are features for Catalan Z
ca
.
6
4.3 High-resource SRL
We first compare our models trained as a pipeline,
using all available supervision (syntax, morphol-
ogy, POS tags, lemmas) from the CoNLL-2009
data. Table 4(a) shows the results of our model
with gold syntax and a richer feature set than
that of Naradowsky et al (2012), which only
looked at whether a syntactic dependency edge
was present. This highlights an important advan-
tage of the pipeline trained model: the features can
consider any part of the syntax (e.g., arbitrary sub-
trees), whereas the joint model is limited to those
features over which it can efficiently marginalize
(e.g., short dependency paths). This holds true
even in the pipeline setting where no syntactic su-
pervision is available.
Table 4(b) contrasts our high-resource results
for the task of SRL and sense disambiguation
with the top systems in the CoNLL-2009 Shared
Task, giving further insight into the performance
of the simple information gain feature selection
technique. With supervised syntax, our sim-
ple information gain feature selection technique
(? 3.4) performs admirably. However, the orig-
inal unigram Bj?orkelund features (B
de,en,es,zh
),
which were tuned for a high-resource model, ob-
tain higher F1 than our information gain set us-
ing the same features in unigram and bigram tem-
plates (IG
B
). This suggests that further work on
feature selection may improve the results. We
find that IG
B
obtain higher F1 than the original
Bj?orkelund feature sets (B
de,en,es,zh
) in the low-
resource pipeline setting with constrained gram-
mar induction (DMV+C).
feature for argument prediction.
6
This covers all CoNLL languages but Czech, where fea-
ture sets were not made publicly available in either work. In
Czech, we disallowed template bigrams involving path-grams.
1182
(a)
(b)
(c)
SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh
Pipeline IG
C
Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35
Pipeline IG
B
Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95
Naradowsky et al (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97
Bj?orkelund et al (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60
Zhao et al (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72
Pipeline IG
C
Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35
Pipeline Z
ca
Supervised *77.62 77.62 ? ? ? ? ?
Pipeline B
de,en,es,zh
Supervised *76.49 ? ? 72.17 81.15 76.65 75.99
Pipeline IG
B
Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44
Joint IG
C
Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14
Joint IG
B
Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21
Naradowsky et al (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32
Pipeline IG
C
DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86
Pipeline Z
ca
DMV+C (bc) *69.67 69.67 ? ? ? ? ?
Pipeline IG
C
DMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35
Pipeline IG
B
DMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28
Pipeline IG
B
DMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92
Pipeline B
de,en,es,zh
DMV+C (bc) *63.06 ? ? 57.75 68.32 63.70 62.45
Table 4: Test F1 for SRL and sense disambiguation on CoNLL?09 in high-resource and low-resource
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
*Indicates partial averages for the language-specific feature sets (Z
ca
and B
de,en,es,zh
), for which we show results only on the
languages for which the sets were publicly available.
train
test
2008
heads
2005
spans
2005
spans
(oracle
tree)
X PRY?08
2
0
0
5
s
p
a
n
s
84.32 79.44
 B?11 (tdc) ? 71.5
 B?11 (td) ? 65.0
X JN?08
2
0
0
8
h
e
a
d
s
85.93 79.90
 Joint, IG
C
72.9 35.0 72.0
 Joint, IG
B
67.3 37.8 67.1
Table 5: F1 for SRL approaches (without sense
disambiguation) in matched and mismatched
train/test settings for CoNLL 2005 span and 2008
head supervision. We contrast low-resource ()
and high-resource settings (X), where latter uses a
treebank. See ? 4.4 for caveats to this comparison.
4.4 Low-Resource SRL
CoNLL-2009 Table 4(c) includes results for our
low-resource approaches and Naradowsky et al
(2012) on predicting semantic roles as well as
sense. In the low-resource setting of the CoNLL-
2009 Shared task without syntactic supervision,
our joint model (Joint) with marginalized syntax
obtains state-of-the-art results with features IG
C
described in ? 4.2. This model outperforms prior
work (Naradowsky et al, 2012) and our pipeline
model (Pipeline) with contrained (DMV+C) and
unconstrained grammar induction (DMV) trained
on brown clusters (bc).
In the low-resource setting, training and decod-
ing times for the pipeline and joint methods are
similar as computation time tends to be dominated
by feature extraction.
These results begin to answer a key research
question in this work: The joint models outper-
form the pipeline models in the low-resource set-
ting. This holds even when using the same feature
selection process. Further, the best-performing
low-resource features found in this work are those
based on coarse feature templates and selected
by information gain. Templates for these fea-
tures generalize well to the high-resource setting.
However, analysis of the induced grammars in
the pipeline setting suggests that the book is not
closed on the issue. We return to this in ? 4.5.
CoNLL-2008, -2005 To finish out comparisons
with state-of-the-art SRL, we contrast our ap-
proach with that of Boxwell et al (2011), who
evaluate on SRL in isolation (without sense disam-
biguation, as in CoNLL-2009). They report results
on Prop-CCGbank (Boxwell and White, 2008),
which uses the same training/testing splits as the
CoNLL-2005 Shared Task. Their results are there-
fore loosely
7
comparable to results on the CoNLL-
2005 dataset, which we can compare here.
There is an additional complication in com-
paring SRL approaches directly: The CoNLL-
2005 dataset defines arguments as spans instead of
7
The comparison is imperfect for two reasons: first, the
CCGBank contains only 99.44% of the original PTB sen-
tences (Hockenmaier and Steedman, 2007); second, because
PropBank was annotated over CFGs, after converting to CCG
only 99.977% of the argument spans were exact matches
(Boxwell and White, 2008). However, this comparison was
adopted by Boxwell et al (2011), so we use it here.
1183
heads, which runs counter to our head-based syn-
tactic representation. This creates a mismatched
train/test scenario: we must train our model to pre-
dict argument heads, but then test on our models
ability to predict argument spans.
8
We therefore
train our models on the CoNLL-2008 argument
heads,
9
and post-process and convert from heads
to spans using the conversion algorithm available
from Johansson and Nugues (2008).
10
The heads
are either from an MBR tree or an oracle tree. This
gives Boxwell et al (2011) the advantage, since
our syntactic dependency parses are optimized to
pick out semantic argument heads, not spans.
Table 5 presents our results. Boxwell et al
(2011) (B?11) uses additional supervision in the
form of a CCG tag dictionary derived from su-
pervised data with (tdc) and without (tc) a cut-
off. Our model does very poorly on the ?05 span-
based evaluation because the constituent bracket-
ing of the marginalized trees are inaccurate. This
is elucidated by instead evaluating on the ora-
cle spans, where our F1 scores are higher than
Boxwell et al (2011). We also contrast with rela-
vant high-resource methods with span/head con-
versions from Johansson and Nugues (2008): Pun-
yakanok et al (2008) (PRY?08) and Johansson and
Nugues (2008) (JN?08).
Subtractive Study In our subsequent experi-
ments, we study the effectiveness of our models
as the available supervision is decreased. We in-
crementally remove dependency syntax, morpho-
logical features, POS tags, then lemmas. For these
experiments, we utilize the coarse-grained feature
set (IG
C
), which includes Brown clusters.
Across languages, we find the largest drop in
F1 when we remove POS tags; and we find a
gain in F1 when we remove lemmas. This indi-
cates that lemmas, which are a high-resource an-
notation, may not provide a significant benefit for
this task. The effect of removing morphological
features is different across languages, with little
change in performance for Catalan and Spanish,
8
We were unable to obtain the system output of Boxwell
et al (2011) in order to convert their spans to dependencies
and evaluate the other mismatched train/test setting.
9
CoNLL-2005, -2008, and -2009 were derived from Prop-
Bank and share the same source text; -2008 and -2009 use
argument heads.
10
Specifically, we use their Algorithm 2, which produces
the span dominated by each argument, with special handling
of the case when the argument head dominates that of the
predicate. Also following Johansson and Nugues (2008), we
recover the ?05 sentences missing from the ?08 evaluation set.
Rem #FT ca de es
? 127+32 74.46 72.62 74.23
Dep 40+32 67.43 64.24 67.18
Mor 30+32 67.84 59.78 66.94
POS 23+32 64.40 54.68 62.71
Lem 21+32 64.85 54.89 63.80
Table 6: Subtractive experiments. Each row con-
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies
(Dep), morphology (Mor), POS tags (POS), and
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams).
20
30
40
50
60
70
0 20000 40000 60000
Number of Training Sentences
Lab
eled
 F1
Language / Dependency Parser
Catalan / Marginalized
Catalan / DMV+C
German / Marginalized
German / DMV+C
Figure 3: Learning curve for semantic dependency
supervision in Catalan and German. F1 of SRL
only (without sense disambiguation) shown as the
number of training sentences is increased.
but a drop in performance for German. This may
reflect a difference between the languages, or may
reflect the difference between the annotation of the
languages: both the Catalan and Spanish data orig-
inated from the Ancora project,
11
while the Ger-
man data came from another source.
Figure 3 contains the learning curve for SRL su-
pervision in our lowest resource setting for two
example languages, Catalan and German. This
shows how F1 of SRL changes as we adjust
the number of training examples. We find that
the joint training approach to grammar induction
yields consistently higher SRL performance than
its distantly supervised counterpart.
4.5 Analysis of Grammar Induction
Table 7 shows grammar induction accuracy in
low-resource settings. We find that the gap be-
tween the supervised parser and the unsupervised
methods is quite large, despite the reasonable ac-
curacy both methods achieve for the SRL end task.
11
http://clic.ub.edu/corpus/ancora
1184
Dependency
Parser
Avg. ca cs de en es zh
Supervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7
DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2
DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6
DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0
DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5
Marginal, IG
C
43.8 50.3 45.8 27.2 44.2 46.3 48.5
Marginal, IG
B
50.2 52.4 43.4 41.3 52.6 55.2 56.2
Table 7: Unlabeled directed dependency accuracy
on CoNLL?09 test set in low-resource settings.
DMV models are trained on either POS tags (pos)
or Brown clusters (bc). *Indicates the supervised parser
outputs provided by the CoNLL?09 Shared Task.
WSJ
?
Distant
Supervision
SAJM?10 44.8 none
SAJ?13 64.4 none
SJA?10 50.4 HTML
NB?11 59.4 ACE05
DMV (bc) 24.8 none
DMV+C (bc) 44.8 SRL
Marginalized, IG
C
48.8 SRL
Marginalized, IG
B
58.9 SRL
Table 8: Comparison of grammar induction ap-
proaches. We contrast the DMV trained with
Viterbi EM+uniform initialization (DMV), our
constrained DMV (DMV+C), and our model?s
MBR decoding of latent syntax (Marginalized)
with other recent work: Spitkovsky et al (2010a)
(SAJM?10), Spitkovsky et al (2010b) (SJA?10),
Naseem and Barzilay (2011) (NB?11), and the CS
model of Spitkovsky et al (2013) (SAJ?13).
This suggests that refining the low-resource gram-
mar induction methods may lead to gains in SRL.
Interestingly, the marginalized grammars best
the DMV grammar induction method; however,
this difference is less pronounced when the DMV
is constrained using SRL labels as distant super-
vision. This could indicate that a better model for
grammar induction would result in better perfor-
mance for SRL. We therefore turn to an analysis of
other approaches to grammar induction in Table 8,
evaluated on the Penn Treebank. We contrast with
methods using distant supervision (Naseem and
Barzilay, 2011; Spitkovsky et al, 2010b) and fully
unsupervised dependency parsing (Spitkovsky et
al., 2013). Following prior work, we exclude
punctuation from evaluation and convert the con-
stituency trees to dependencies.
12
The approach from Spitkovsky et al (2013)
12
Naseem and Barzilay (2011) and our results use the
Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et
al. (2010b; 2013) use Collins (1999) head percolation rules.
(SAJ?13) outperforms all other approaches, in-
cluding our marginalized settings. We therefore
may be able to achieve further gains in the pipeline
model by considering better models of latent syn-
tax, or better search techniques that break out
of local optima. Similarly, improving the non-
convex optimization of our latent-variable CRF
(Marginalized) may offer further gains.
5 Discussion and Future Work
We have compared various approaches for low-
resource semantic role labeling at the state-of-the-
art level. We find that we can outperform prior
work in the low-resource setting by coupling the
selection of feature templates based on informa-
tion gain with a joint model that marginalizes over
latent syntax.
We utilize unlabeled data in both generative and
discriminative models for dependency syntax and
in generative word clustering. Our discriminative
joint models treat latent syntax as a structured-
feature to be optimized for the end-task of SRL,
while our other grammar induction techniques op-
timize for unlabeled data likelihood?optionally
with distant supervision. We observe that careful
use of these unlabeled data resources can improve
performance on the end task.
Our subtractive experiments suggest that lemma
annotations, a high-resource annotation, may not
provide a large benefit for SRL. Our grammar in-
duction analysis indicates that relatively low accu-
racy can still result in reasonable SRL predictions;
still, the models do not outperform those that use
supervised syntax, and we aim to explore how well
the pipeline models in particular improve when we
apply higher accuracy unsupervised grammar in-
duction techniques.
We have utilized well studied datasets in order
to best understand the quality of our models rela-
tive to prior work. In future work, we hope to ex-
plore the effectiveness of our approaches on truly
low resource settings by using crowdsourcing to
develop semantic role datasets in other languages
and domains.
Acknowledgments We thank Richard Johans-
son, Dennis Mehay, and Stephen Boxwell for help
with data. We also thank Jason Naradowsky, Jason
Eisner, and anonymous reviewers for comments
on the paper.
1185
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the 17th
Conference on Computational Natural Language
Learning (CoNLL 2013). Association for Computa-
tional Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task. Association for Computational Linguistics.
Stephen Boxwell and Michael White. 2008. Project-
ing propbank roles onto the CCGbank. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2008). European
Language Resources Association.
Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis
Mehay, and Sujith Ravi. 2011. Semantic role label-
ing without treebanks? In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Asian Federation of Natural
Language Processing.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4).
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning Research, 11.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL 2009): Shared Task. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008). Association for Computational Lin-
guistics.
Dan Klein and Christopher Manning. 2004. Corpus-
Based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL 2004). Association for Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT. Association for
Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning (ICML 2001). Morgan Kaufmann.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics (TACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The Penn Treebank. Com-
putational linguistics, 19(2).
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Structured sparsity in struc-
tured prediction. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011). Association for Compu-
tational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005).
Association for Computational Linguistics.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In Proceedings of the
2012 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2012). Association
for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using
semantic cues to learn syntax. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI 2011). AAAI Press.
1186
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting of
the Association for Computational Linguistics (ACL
1992).
Nugues Pierre and Kalep Heiki-Jaan. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. NODALIDA 2007 Proceedings.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D Manning. 2010a. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of the 14th Conference on Computa-
tional Natural Language Learning (CoNLL 2010).
Association for Computational Linguistics.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011). Association for Computational Lin-
guistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2013). Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005). Association for Computational Lin-
guistics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In L?eon
Bottou and Michael Littman, editors, Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009). Omnipress.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n
3
. Information and
Control, 10(2).
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task. Association for Com-
putational Linguistics.
1187
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
Natural Reference to Objects in a Visual Domain
Margaret Mitchell
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Kees van Deemter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
{m.mitchell, k.vdeemter, e.reiter}@abdn.ac.uk
Ehud Reiter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Abstract
This paper discusses the basic structures
necessary for the generation of reference
to objects in a visual scene. We construct
a study designed to elicit naturalistic re-
ferring expressions to relatively complex
objects, and find aspects of reference that
have not been accounted for in work on
Referring Expression Generation (REG).
This includes reference to object parts,
size comparisons without crisp measure-
ments, and the use of analogies. By draw-
ing on research in cognitive science, neu-
rophysiology, and psycholinguistics, we
begin developing the input structure and
background knowledge necessary for an
algorithm capable of generating the kinds
of reference we observe.
1 Introduction
One of the dominating tasks in Natural Language
Generation (NLG) is the generation of expressions
to pick out a referent. In recent years there has
been increased interest in generating referential
expressions that are natural, e.g., like those pro-
duced by people. Although research on the gener-
ation of referring expressions has examined differ-
ent aspects of how people generate reference, there
has been surprisingly little research on how people
refer to objects in a real-world setting. This paper
addresses this issue, and we begin formulating the
requirements for an REG algorithm that refers to
visible three-dimensional objects in the real world.
Reference to objects in a visual domain pro-
vides a straightforward extension of the sorts of
reference REG research already tends to consider.
Toy examples outline reference to objects, peo-
ple, and animals that are perceptually available
before the speaker begins generating an utterance
(Dale and Reiter, 1995; Krahmer et al, 2003; van
Deemter et al, 2006; Areces et al, 2008). Exam-
ple referents may be referred to by their color, size,
type (?dog? or ?cup?), whether or not they have a
beard, etc.
Typically, the reference process proceeds by
comparing the properties of the referent with the
properties of all the other items in the set. The
final expression roughly conforms to the Gricean
maxims (Grice, 1975).
However, when the goal is to generate natural
reference, this framework is too simple. The form
reference takes is profoundly affected by modality,
task, and audience (Chapanis et al, 1977; Cohen,
1984; Clark and Wilkes-Gibbs, 1986), and even
when these aspects are controlled, different people
will refer differently to the same object (Mitchell,
2008). In light of this, we isolate one kind of nat-
ural reference and begin building the algorithmic
framework necessary to generate the observed lan-
guage.
Psycholinguistic research has examined refer-
ence in a variety of settings, which may inform
research on natural REG, but it is not always clear
how to extend this work to a computational model.
This is true in part because these studies favor an
analysis of reference in the context of collabora-
tion; reference is embedded within language, and
language is often a joint activity. However, most
research on referring expression generation sup-
poses a solitary generating agent.1 This tacitly
assumes that reference will be taking place in a
monologue setting, rather than a dialogue or group
setting. Indeed, the goal of most REG algorithms
is to produce uniquely distinguishing, one-shot re-
ferring expressions.
Studies on natural reference usually use a
two person (speaker-listener) communication task
(e.g., Flavell et al, 1968; Krauss and Glucksberg,
1969; Ford and Olson, 1975). This research has
1A notable exception is Heeman and Hirst (1995).
shown that reference is more accurate and efficient
when it incorporates things like gesture and gaze
(Clark and Krych, 2004). There is a trade-off in
effort between initiating a noun phrase and refash-
ioning it so that both speakers understand the ref-
erent (Clark and Wilkes-Gibbs, 1986), and speak-
ers communicate to form lexical pacts on how
to refer to an object (Sacks and Schegloff, 1979;
Brennan and Clark, 1996). Mutual understanding
of referents is achieved in part by referring within
a subset of potential referents (Clark et al, 1983;
Beun and Cremers, 1998). A few studies have
compared monologue to dialogue reference, and
have shown that monologue references tend to be
harder for a later listener to disambiguate (Clark
and Krych, 2004) and that subsequent references
tend to be longer than those in dialogues (Krauss
and Weinheimer, 1967).
Aiming to generate natural reference in a mono-
logue setting raises questions about what an algo-
rithm should use to produce utterances like those
produced by people. In a monologue setting, the
speaker (or algorithm) gets no feedback from the
listener; the speaker?s reference is not tied to in-
teractions with other participants. The speaker
is therefore in a difficult position, attempting to
clearly convey a referent without being able to
check if the reference is understood along the way.
Recent studies that have focused on monologue
reference do so rather explicitly, which may af-
fect participant responses. These studies utilize
2D graphical depictions of simple 3D objects (van
Deemter et al, 2006; Viethen and Dale, 2008),
where a small set of properties can be used to dis-
tinguish one item from another. The expressions
are elicited in isolation, typed and then submitted,
which may hide some of the underlying referen-
tial processes. None of these studies utilize actual
objects. It is therefore difficult to use these data
to draw conclusions about how reference works in
naturalistic settings. It is unclear if these experi-
mental settings are natural enough, i.e., if they get
at reference as it may occur every day.
The study in this paper attempts to bring out in-
formation about reference in a number of ways.
First, we conduct the study in-person, using real-
world objects. This design invites referential phe-
nomena that may not have been previously ob-
served in simpler domains. Second, the refer-
ring expressions are produced orally. This allows
us access to reference as it is generated, without
the participants revising and so potentially obscur-
ing information about their reference. Third, we
use a relatively complicated task, where partici-
pants must explain how to use pieces to put to-
gether a picture of a face. The fact that we are
looking at reference is not made explicit, which
lessens any experimental effects caused by sub-
jects guessing the purpose of the study. This ap-
proach also situates reference within a larger task,
which may draw out aspects of reference not usu-
ally seen in experiments that elicit reference in iso-
lation. Fourth, the objects used display a variety
of different features: texture, material, color, size
along several dimensions, etc. This brings the data
set closer to objects that people interact with every
day. A monologue setting offers a picture of the
phenomena at play during a single individual?s re-
ferring expression generation.
The referring expressions gathered in this study
exhibit several aspects of reference that have not
yet been addressed in REG. This includes (1) part-
whole modularity; (2) size comparisons across
three dimensions; and (3) analogies. Work in cog-
nitive sciences suggests that these phenomena are
interrelated, and may be possible to represent in a
computational framework. This research also of-
fers connections to further aspects of natural refer-
ence that were not directly observed in the study,
but will need to be accounted for in future work on
naturalistic referring expression generation. Us-
ing these ideas, we begin formulating the struc-
tures that an REG algorithm would need in order
to produce reference to real-world objects in a vi-
sual setting.
Approaching REG in this way allows us to tie
research in the generation of referring expressions
to computational models of visual perception and
cognitively-motivated computer vision. Moving in
this direction offers the prospect of eventually de-
veloping an application for the generation of nat-
ural reference to objects automatically recognized
by a computer vision system.
In the next section, we describe our study. In
Section 3, we analyze the results and discuss what
they tell us about natural reference. In Section 4,
we draw on our results and cognitive models of ob-
ject recognition to begin building the framework
for a referring expression algorithm that generates
naturalistic reference to objects in a visual scene.
In Section 5, we offer concluding remarks and out-
line areas for further study.
Figure 1: Object Board.
2 Method
2.1 Subjects
The subjects were 20 residents of Aberdeen, Scot-
land, and included undergraduates, graduates, and
professionals. All were native speakers of English,
had normal or corrected vision, and had no other
known visual issues (such as color-blindness).
Subjects were paid for their participation. Two
recordings were left out of the analysis: one par-
ticipant?s session was not fully recorded due to a
software error, and one participant did not pick out
many objects in each face and so was not included.
The final set of participants included 18 people, 10
female and 8 male.
2.2 Materials
A board was prepared with 51 craft objects. The
objects were chosen from various craft sets, and
included pom-poms, pipe-cleaners, beads, and
feathers (see Table 1). The motley group of objects
had different colors, textures, shapes, patterns, and
were made of different materials. Similar objects
were grouped together on the board, with a label
placed underneath. This was done to control the
head noun used in each reference. The objects
were used to make up 5 different craft ?face? pic-
tures. Subjects sat at a desk facing the board and
the stack of pictures. A picture of the board is
shown in Figure 1.
Subjects were recorded on a head-mounted mi-
crophone, which fed directly into a laptop placed
on the left of the desk. The open-source audio-
recording program Audacity (Mazzoni, 2010) was
used to record the audio signal and export it to
wave format.
2.3 Procedure
Subjects were told to give instructions on how to
construct each face using the craft supplies on the
board. They were instructed to be clear enough for
a listener to be able to reconstruct each face with-
out the pictures, with only the board items in front
of them. A pilot study revealed that such open-
ended instructions left some subjects spending an
inordinate amount of time on the exact placement
of each piece, and so in the current study sub-
jects were told that each face should take ?a cou-
ple? minutes, and that the instructions should be
as clear as possible for a listener to use the same
objects in reconstructing the pictures without be-
ing ?overly concerned? with the details of exactly
how each piece is angled in relation to the other.
Subjects were first given a practice face to de-
scribe. This face was the same face for all subjects.
They were then allowed to voice any concerns or
ask questions, but the experimenter only repeated
portions of the original instructions; no new infor-
mation was given. The subject could then proceed
to the next four faces, which were in a random or-
der for each subject. A transcript of a single face
from a session is provided in Figure 2.
2.4 Analysis
The recordings of each monologue were tran-
scribed, including disfluencies, and each face sec-
tion (?eyes?, ?chin?, etc.) was marked. First refer-
ence to items on the board were annotated with
their corresponding item numbers, yielding 722
references.2 Initial references to single objects
were extracted, creating a final data set with 505
references to single objects.
3 Results
Each reference was annotated in terms of the prop-
erties used to pick out the referent. For exam-
ple, ?the red feather? was annotated as contain-
ing the <ATTRIBUTE:value> pairs <COLOR:red,
TYPE:feather>. Discerning properties from the
modifiers used in reference is generally straight-
forward, and all of the references produced may
be partially deconstructed using such properties.
2This corpus is available at
http://www.csd.abdn.ac.uk/?mitchema/craft corpus.
14 foam shapes 2 large red hearts 2 small red hearts 2 small neon green hearts
2 small blue hearts 1 small green heart 1 green triangle 1 red circle
1 red square 1 red rectangle 1 white rectangle
11 beads 4 large round wooden beads 2 small white plastic beads 2 brown patterned beads
1 gold patterned bead 1 shiny gold patterned heart 1 red patterned heart
9 pom poms 2 big green pom-poms 2 small neon green pom-poms 2 small silver pom-poms
1 small metallic green pom-pom 1 large white pom-pom 1 medium white pom-pom
8 pipe cleaners 1 gold pipe-cleaner 1 gold pipe-cleaner in half 1 silver pipe-cleaner
1 circular neon yellow soft pipe-cleaner 1 neon orange puffy pipe-cleaner 1 grey puffy pipe-cleaner
1 purple/yellow striped pipe-cleaner 1 brown/grey striped pipe-cleaner
5 feathers 2 purple feathers 2 red feathers 1 yellow feather
3 ribbons 1 gold sequined wavy ribbon 1 silver wavy ribbon 1 small silver wavy ribbon
1 star 1 gold star
Table 1: Board items.
<CHIN> Okay so this face again um this face has um uh
for the chin, it uses (10 a gold pipe-cleaner in a V shape)
where the bottom of the V is the chin. </CHIN>
<MOUTH> The mouth is made up of (9 a purple feather).
And the mouth is slightly squint, um as if the the person
is smiling or even smirking. So this this smile is almost
off to one side. </MOUTH>
<NOSE> The nose is uh (5 a wooden bead, a medium-
sized wooden bead with a hole in the center). </NOSE>
<EYES> And the eyes are made of (2,3 white pom-poms),
em just uh em evenly spaced in the center of the face.
</EYES>
<FOREHEAD> Em it?s see the person?s em top of the per-
son?s head is made out of (1 another, thicker pipe-cleaner
that?s uh a grey color, it?s kind of uh a knotted blue-type
pipe-cleaner). So that that acts as the top of the person?s
head. </FOREHEAD>
<HAIR> And down the side of the person?s face, there are
(7,8 two ribbons) on each side. (7,8 And those are silver
ribbons). Um and they just hang down the side of the face
and they join up the the grey pipe-cleaner and the top um
of the person?s head to the to the chin and then hang down
either side of the chin. </HAIR>
<EARS> And the person?s ears are made up of (4,6 two
beads, which are um love-heart-shaped beads), where the
points of the love-hearts are facing outwards. And those
are just placed um around same em same em horizontal
line as the nose of the person?s face is. </EARS>
Figure 2: Excerpt Transcript.
Using sets of properties to distinguish referents
is nothing new in REG. Algorithms for the genera-
tion of referring expressions commonly use this as
a starting point, proposing that properties are orga-
nized in some linear order (Dale and Reiter, 1995)
or weighted order (Krahmer et al, 2003) as input.
However, we find evidence that more is at play. A
breakdown of our findings is listed in Table 2.
3.1 Spatial Reference
In addition to properties that pick out referents,
throughout the data we see reference to objects
as they exist in space. Size is compared across
different dimensions of different objects, and ref-
erence is made to different parts of the objects,
picking out pieces within the whole. These two
phenomena ? relative size comparisons and part-
whole modularity ? point to an underlying spatial
object representation that may be utilized during
reference.
3.1.1 Relative Size Comparisons
A total of 122 (24.2%) references mention size
with a vague modifier (e.g., ?big?, ?wide?). This
includes comparative (e.g, ?larger?) and superla-
tive (e.g., ?largest?) size modifiers, which occur 40
(7.9%) times in the data set. Examples are given
below.
(1) ?the bigger pom-pom?
(2) ?the green largest pom-pom?
(3) ?the smallest long ribbon?
(4) ?the large orange pipe-cleaner?
Of the references that mention size, 35 (6.9%)
use a vague modifier that applies to one or two di-
mensions. This includes modifiers for height (?the
short silver ribbon?), width (?quite a fat rectan-
gle?), and depth (?the thick grey pipe-cleaner?).
87 (17.2%) use a modifier that applies to the over-
all size of the object (e.g., ?big? or ?small?). Table
3 lists these values. Crisp measurements (such as
?1 centimeter?) occur only twice (0.4%), with both
produced by the same participant.
Comparative/Superlative: 40 (7.9%)
Base: 82 (16.2%)
Height/Width/Depth: 35 (6.9%)
Overall size: 87 (17.2%)
Table 3: Size Modifier Breakdown.
Part-whole modularity Relative size Analogies
?a green pom-pom. . . ?a red foam-piece. . . ?a natural-looking piece
with the tinsel on the outside? which is more square of pipe-cleaner, it looks
?your gold twisty ribbon. . . in shape rather than a bit like a rope?
with sequins on it? the longer rectangle? ?a pipe-cleaner that
?a wooden bead. . . ?the grey pipe-cleaner. . . looks a bit like. . .
with a hole in the center? which is the thicker one. . . a fluffy caterpillar?
?one of the green pom-poms. . . ?the slightly larger one? ?the silver ribbon
with the sort of strands ?the smaller silver ribbon? that?s almost like
coming out from it.? ?the short silver ribbon? a big S shape.?
?the silver ribbon. . . with the chainmail ?quite a fat rectangle? ?a. . . pipe-cleaner
detail down through the middle of it.? ?thick grey pipe-cleaner? that looks like tinsel.?
11 References 122 References 16 References
Table 2: Examples of Observed Reference.
Participants produce such modifiers without
sizes or measurements explicitly given; with an
input of a visual object presentation, the output
includes size modifiers. Such data suggests that
natural reference in a visual domain utilizes pro-
cesses comparing the length, width, and height of
a target object with other objects in the set. Indeed,
5 references (1.0%) in our data set include explicit
comparison with the size of other objects.
(5) ?a red foam-piece. . . which is more square in
shape rather than the longer rectangle?
(6) ?the grey pipe-cleaner. . . which is the thicker
one. . . of the selection?
(7) ?the shorter of the two silver ribbons?
(8) ?the longer one of the ribbons?
(9) ?the longer of the two silver ribbons?
In Example (5), height and width across two
different objects are compared, distinguishing a
square from a rectangle. In (6) ?thicker? marks
the referent as having a larger circumference than
other items of the same type. (7) (8) and (9) com-
pare the height of the target referent to the height
of similar items.
The use of size modifiers in a domain without
specified measurements suggests that when peo-
ple refer to an object in a visual domain, they
are sensitive to its size and structure within a di-
mensional, real-world space. Without access to
crisp measurements, people compare relative size
across different objects, and this is reflected in the
expressions they generate. These comparisons are
not only limited to overall size, but include size
in each dimension. This suggests that objects?
structures within a real-world space are relevant
to REG in a visual domain.
3.1.2 Part-Whole Modularity
The role that a spatial object understanding has
within reference is further detailed by utterances
that pick out the target object by mentioning an ob-
ject part. 11 utterances (2.2%) in our data include
mention of an object part within reference to the
whole object. This is spread across participants,
such that half of the participants make reference
to an object part at least once.
(10) ?a green pom-pom, which is with the tinsel
on the outside?
(11) ?your gold twisty ribbon...with sequins on
it?
(12) ?a wooden bead...with a hole in the center?
In (10), pieces of tinsel are isolated from the
whole object and specified as being on the outside.
In (11), smaller pieces that lay on top of the ribbon
are picked out. And in (12), a hole within the bead
is isolated.
The use of part-whole modularity suggests an
understanding that parts of the object take up their
own space within the object. An object is not only
viewed as a whole during reference, but parts in,
on, and around it may be considered as well. For
an REG algorithm to generate these kinds of ref-
erences, it must be provided with a representation
that details the structure of each object.
3.2 ANALOGIES
The data from this study also provide information
on what can be expected from a knowledge base
in an algorithm that aims to generate naturalistic
reference. Reference is made 16 times (3.2%) to
objects not on the board, where the intended refer-
ent is compared against something it is like. Some
examples are given below.
(13) ?a gold. . . pipe-cleaner. . . completely
straight, like a ruler?
(14) ?a natural-looking piece of pipe-cleaner, it
looks a bit like a rope?
(15) ?a pipe-cleaner that looks a bit like. . . a
fluffy caterpillar. . . ?
In (13), a participant makes reference to a
SHAPE property of an object not on the board. In
(14) and (15), participants refer to objects that may
share a variety of properties with the referent, but
are also not on the board.
Reference to these other items do not pick out
single objects, but types of objects (e.g., an object
type, not token). They correspond to some pro-
totypical idea of an object with properties similar
to those of the referent. Work by Rosch (1975)
has examined this tendency, introducing the idea
of prototype theory, which proposes that there may
be some central, ?prototypical? notions of items. A
knowledge base with stored prototypes could be
utilized by an REG algorithm to compare the tar-
get referent to item prototypes. Such representa-
tions would help guide the generation of reference
to items not in the scene, but similar to the target
referent.
4 Discussion
We have discussed several different aspects of ref-
erence in a study where referring expressions are
elicited for objects in a spatial, visual scene. Ref-
erence in this domain draws on object forms as
they exist in a three-dimensional space and uti-
lizes background knowledge to describe referents
by analogy to items outside of the scene. This
is undoubtedly not an exhaustive account of the
phenomena at play in such a domain, but offers
some initial conclusions that may be drawn from
exploratory work of this kind.
Before continuing with the discussion, it is
worthwhile to consider whether some of our data
might be seen as going beyond reference. Perhaps
the participants are doing something else, which
could be called describing. How to draw the line
between a distinguishing reference and a descrip-
tion, and whether such a line can be drawn at all, is
an interesting question. If the two are clearly dis-
tinct, then both are interesting to NLG research.
If the two are one in the same, then this sheds
some light on how REG algorithms should treat
reference. We leave a more detailed discussion of
this for future work, but note recent psycholinguis-
tic work suggesting that referring establishes (1)
an individual as the referent; (2) a conceptualiza-
tion or perspective on that individual (Clark and
Bangerter, 2004). Schematically, referring = indi-
cating + describing.
We now turn to a discussion of how the ob-
served phenomena may be best represented in an
REG algorithm. We propose that an algorithm ca-
pable of generating natural reference to objects in
a visual scene should utilize (1) a spatial object
representation; (2) a non-spatial feature-based rep-
resentation; and (3) a knowledge base of object
prototypes.
4.1 Spatial and Visual Properties
It is perhaps unsurprising to find reference that ex-
hibits spatial knowledge in a study where objects
are presented in three-dimensional space. Hu-
man behavior is anchored in space, and spatial in-
formation is essential for our ability to navigate
the world we live in. However, referring expres-
sion generation algorithms geared towards spa-
tial representations have oversimplified this ten-
dency, keeping objects within the realm of two-
dimensions and only looking at the spatial rela-
tions between objects.
For example, Funakoshi et al (2004) and Gatt
(2006) focus on how objects should be clustered
together to form groups. This utilizes some of
the spatial information between objects, but does
not address the spatial, three-dimensional nature
of objects themselves. Rather, objects exist as en-
tities that may be grouped with other entities in a
set or singled out as individual objects; they do not
have their own spatial characteristics. Similarly,
one of the strengths of the Graph-Based Algorithm
(Krahmer et al, 2003) is its ability to generate ex-
pressions that involve relations between objects,
and these include spatial ones (?next to?, ?on top
of?, etc.). In all these approaches, however, ob-
jects are essentially one-dimensional, represented
as individual nodes.
Work that does look at the spatial information
of different objects is provided by Kelleher et al
(2005). In this approach, the overall volume of
each object is calculated to assign salience rank-
ings, which then allow the Incremental Algorithm
(Dale and Reiter, 1995) to produce otherwise ?un-
derspecified? reference. The spatial properties of
the objects are kept relatively simple. They are
not used in constructing the referring expression,
but one aspect of the object?s three-dimensional
shape (volume) affects the referring expression?s
final form. To the authors? knowledge, the cur-
rent work is the first to suggest that objects them-
selves should have their spatial properties repre-
sented during reference.
Research in cognitive modelling supports the
idea that we attend to the spatial properties of ob-
jects when we view them (Blaser et al, 2000), and
that we have purely spatial attentional mechanisms
operating alongside non-spatial, feature-based at-
tentional mechanisms (Treue and Trujillo, 1999).
These feature-based attentional mechanisms pick
out properties commonly utilized in REG, such as
texture, orientation, and color. They also pick out
edges and corners, contrast, and brightness. Spa-
tial attentional mechanisms provide information
about where the non-spatial features are located in
relation to one another, size, and the spatial inter-
relations between component parts.
Applying these findings to our study, an REG
algorithm that generates natural reference should
utilize a visual, feature-based representation of ob-
jects alongside a structural, spatial representation
of objects. A feature-based representation is al-
ready common to REG, and could be represented
as a series of <ATTRIBUTE:value> pairs. A spa-
tial representation is necessary to define how the
object is situated within a dimensional space, pro-
viding information about the relative distances be-
tween object components, edges, and corners.
With such information provided by a spatial
representation, the generation of part-whole ex-
pressions, such as ?the pom-pom with the tinsel on
the outside?, is possible. This also allows for the
generation of size modifiers (?big?, ?small?) with-
out the need for crisp measurements, for example,
by comparing the difference in overall height of
the target object with other objects in the scene, or
against a stored prototype (discussed below). Rel-
ative size comparisons across different dimensions
would also be possible, used to generate size mod-
ifiers such as ?wide? and ?thick? that refer to one
dimensional axis.
4.2 Analogies
A feature-based and a spatial representation may
also play a role in analogies. When we use analo-
gies, as in ?the pipe-cleaner that looks like a cater-
pillar?, we use world knowledge about items that
are not themselves visible. Such an expression
draws on similarity that does not link the referent
with a particular object, but with a general type of
object: the pipe-cleaner is caterpillar-like.
To generate these kinds of expressions, an REG
algorithm would first need a knowledge base with
prototypes listing prototypical values of attributes.
For example, a banana prototype might have a pro-
totypical COLOR of yellow. With prototypes in the
knowledge base, the REG algorithm would need
to calculate similarity of a target referent to other
known items. This would allow a piece of yellow
cloth, for example, to be described as being the
color of a banana.
Implementing such similarity measures in an
REG algorithm will be challenging. One difficulty
is that prototype values may be different depend-
ing on what is known about an item; a prototypical
unripe banana may be green, or a prototypical rot-
ten banana brown. Another difficulty will be in
determining when a referent is similar enough to
a prototype to warrant an analogy. Additional re-
search is needed to explore how these properties
can be reasoned about.
4.3 Further Implications
A knowledge base containing prototypes opens up
the possibility of generating many other kinds of
natural references. In particular, such knowledge
would allow the algorithm to compute which prop-
erties a given kind of referent may be expected
to have, and which properties may be unexpected.
Unexpected properties may therefore stand out as
particularly salient.
For example, a dog missing a leg may be de-
scribed as a ?three-legged dog? because the pro-
totypical dog has four legs. We believe that this
perspective, which hinges on the unexpectedness
of a property, suggests a new approach to at-
tribute selection. Unlike the Incremental Algo-
rithm, the Preference Order that determines the or-
der in which attributes are examined would not be
fixed, but would depend on the nature of the refer-
ent and what is known about it.
Approaching REG in this way follows work in
cognitive science and neurophysiology that sug-
gests that expectations about objects? visual and
spatial characteristics are derived from stored rep-
resentations of object ?prototypes? in the infe-
rior temporal lobe of the brain (Logothetis and
- A spatial representation (depicting size, inter-
relations between component parts)
- A non-spatial, propositional representation
(describing color, texture, orientation, etc.)
- A knowledge base with stored prototypical ob-
ject propositional and spatial representations
Table 4: Requirements for an REG algorithm that
generates natural reference to visual objects.
Sheinberg, 1996; Riesenhuber and Poggio, 2000;
Palmeri and Gauthier, 2004). Most formal theo-
ries of object perception posit some sort of cate-
gory activation system (Kosslyn, 1994), a system
that matches input properties of objects to those
of stored prototypes, which then helps guide ex-
pectations about objects in a top-down fashion.3
This appears to be a neurological correlate of the
knowledge base we propose to underlie analogies.
Such a system contains information about pro-
totypical objects? component parts and where they
are placed relative to one another, as well as rele-
vant values for material, color, etc. This suggests
that the spatial and non-spatial feature-based rep-
resentations proposed for visible objects could be
used to represent prototype objects as well. In-
deed, how we view and refer to objects appears to
be influenced by the interaction of these structures:
Expectations about an object?s spatial properties
guide our attention towards expected object parts
and non-spatial, feature-based properties through-
out the scene (Kosslyn, 1994; Itti and Koch, 2001).
This affects the kinds of things we are most likely
to generate language about (Itti and Arbib, 2005).
We can now outline some general requirements
for an algorithm capable of generating naturalis-
tic reference to objects in a visual scene: Input to
such an algorithm should include a feature-based
representation, which we will call a propositional
representation, with values for color, texture, etc.,
and a spatial representation, with symbolic infor-
mation about objects? size and the spatial relation-
ships between components. A system that gener-
ates naturalistic reference must also use a knowl-
edge base storing information about object proto-
types, which may be represented in terms of their
own propositional/spatial representations.
3Note that this is not the only proposed matching structure
in the brain ? an exemplar activation system matches input to
stored exemplars.
5 Conclusions and Future Work
We have explored the interaction between view-
ing objects in a three-dimensional, spatial domain
and referring expression generation. This has led
us to propose structures that may be used to con-
nect vision in a spatial modality to naturalistic ref-
erence. The proposed structures include a spatial
representation, a propositional representation, and
a knowledge base with representations for object
prototypes. Using structures that define the propo-
sitional and spatial content of objects fits well with
work in psycholinguistics, cognitive science and
neurophysiology, and may provide the basis to
generate a variety of natural-sounding references
from a system that recognizes objects.
It is important to note that any naturalistic ex-
perimental design limits the kinds of conclusions
that can be drawn about reference. A study that
elicits reference to objects in a visual scene pro-
vides insight into reference to objects in a visual
scene; these conclusions cannot easily be extended
to reference to other kinds of phenomena, such as
reference to people in a novel. We therefore make
no claims about reference as a whole in this paper;
generalizations from this research can provide hy-
potheses for further testing in different modalities
and with different sorts of referents.
Our data leave open many areas for further
study, and we hope to address these in future work.
Experiments designed specifically to elicit relative
size modifiers, reference to object components,
and reference to objects that are like other things
would help further detail the form our proposed
structures take.
What is clear from our data is that both a spa-
tial understanding and a non-spatial feature-based
understanding appear to play a role in reference
to objects in a visual scene, and further, refer-
ence in such a setting is bolstered by a knowl-
edge base with stored prototypical object repre-
sentations. Utilizing structures representative of
these phenomena, we may be able to extend ob-
ject recognition research into object reference re-
search, generating natural-sounding reference in
everyday settings.
Acknowledgements
Thanks to Advaith Siddarthan for thought-
provoking discussions and to the anonymous re-
viewers for useful suggestions.
References
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 42?29.
Robbert-Jan Beun and Anita H. M. Cremers. 1998.
Object reference in a shared domain of conversation.
Pragmatics and Cognition, 6:121?52.
Erik Blaser, Zenon W. Pylyshyn, and Alex O. Hol-
combe. 2000. Tracking an object through feature
space. Nature, 408:196?199.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22:1482?93.
Alphonse Chapanis, Robert N. Parrish, Robert B.
Ochsman, and Gerald D. Weeks. 1977. Studies
in interactive communication: II. the effects of four
communication modes on the linguistic performance
of teams during cooperative problem solving. Hu-
man Factors, 19:101?125.
Herbert H. Clark and Adrian Bangerter. 2004. Chang-
ing ideas about reference. In Ira A. Noveck and Dan
Sperber, editors, Experimental pragmatics, pages
25?49. Palgrave Macmillan, Basingstoke, England.
Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking while monitoring addressees for under-
standing. Journal of Memory and Language, 50:62?
81.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understand-
ing of demonstrative reference. Journal of Verbal
Learning and Verbal Behavior, 22:1?39.
Philip R. Cohen. 1984. The pragmatics of referring
and the modality of communication. Computational
Linguistics, 10(2):97?146.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
J. H. Flavell, P. T. Botkin, D. L. Fry Jr., J. W. Wright,
and P. E. Jarvice. 1968. The Development of Role-
Taking and Communication Skills in Children. John
Wiley, New York.
William Ford and David Olson. 1975. The elaboration
of the noun phrase in children?s description of ob-
jects. The Journal of Experimental Child Psychol-
ogy, 19:371?382.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generating refer-
ring expressions using perceptual groups. In Pro-
ceedings of the 3rd International Conference on Nat-
ural Language Generation, pages 51?60.
Albert Gatt. 2006. Structuring knowledge for refer-
ence generation: A clustering algorithm. Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 321?328.
Paul H. Grice. 1975. Logic and conversation. Syntax
and Semantics, 3:41?58.
Peter A. Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21.
Laurent Itti and Michael A. Arbib. 2005. Attention and
the minimal subscene. In Michael A. Arbib, editor,
Action to Language via the Mirror Neuron System.
Cambridge University Press.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neu-
roscience.
J. Kelleher, F. Costello, and J. van Genabith. 2005.
Dynamically structuring, updating and interrelating
representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62?102.
Stephen M. Kosslyn. 1994. Image and Brain: The
Resolution of the Imagery Debate. MIT Press, Cam-
bridge, MA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Robert M. Krauss and Sam Glucksberg. 1969. The
development of communication: Competence as a
function of age. Child Development, 40:255?266.
Robert M. Krauss and Sidney Weinheimer. 1967. Ef-
fect of referent similarity and communication mode
on verbal encoding. Journal of Verbal Learning and
Verbal Behavior, 6:359?363.
Nikos K. Logothetis and David L. Sheinberg. 1996.
Visual object recognition. Annual Review Neuro-
science, 19:577?621.
Dominic Mazzoni. 2010. Audacity.
Margaret Mitchell. 2008. Towards the generation
of natural reference. Master?s thesis, University of
Washington.
Thomas J. Palmeri and Isabel Gauthier. 2004. Vi-
sual object understanding. Nature Reviews Neuro-
science, 5:291?303.
Maximilian Riesenhuber and Tomaso Poggio. 2000.
Models of object recognition. Nature Neuroscience
Supplement, 3:1199?1204.
Eleanor Rosch. 1975. Cognitive representation of
semantic categories. Journal of Experimental Psy-
chology, 104:192?233.
Harvey Sacks and Emanuel A. Schegloff. 1979. Two
preferences in the organization of reference to per-
sons in conversation and their interaction. In George
Psathas, editor, Everyday Language: Studies in Eth-
nomethodology, pages 15?21. Irvington Publishers,
New York.
Stegan Treue and Julio C. Martinez Trujillo. 1999.
Feature-based attention influences motion process-
ing gain in macaque visual cortex. Nature, 399:575?
579.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Pro-
ceedings of the 4th International Conference on Nat-
ural Language Generation, Sydney, Australia. ACL.
Jette Viethen and Robert Dale. 2008. The use of spatial
descriptions in referring expressions. In Proceed-
ings of the 5th International Conference on Natural
Language Generation, INLG-08, Salt Fork, Ohio.
ACL.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 131?133,
Utica, May 2012. c?2012 Association for Computational Linguistics
Midge: Generating Descriptions of Images?
Margaret Mitchell
University of Aberdeen
m.mitchell@abdn.ac.uk
Xufeng Han
Stony Brook University
xufhan@cs.stonybrook.edu
Jeff Hayes
SignWorks of Oregon
jeff@signworksoforegon.com
Abstract
We demonstrate a novel, robust vision-to-
language generation system called Midge.
Midge is a prototype system that connects
computer vision to syntactic structures with
semantic constraints, allowing for the auto-
matic generation of detailed image descrip-
tions. We explain how to connect vision de-
tections to trees in Penn Treebank syntax,
which provides the scaffolding necessary to
further refine data-driven statistical generation
approaches for a variety of end goals.
1 Introduction
There has been a growing interest in tackling the
problem of how to describe an image using com-
puter vision detections. This problem is difficult in
part because computer vision detections are often
wrong: State-of-the-art vision technology predicts
things that are not there, and misses things that are
obvious to a human observer. This problem is also
difficult because it is not clear what kind of language
should be generated ? the language that makes up a
?description? can take many forms.
At the bare minimum, an automatic vision-to-
language system, given an image with a single de-
tection of, for example, a dog, should be able to gen-
erate a dog, and a longer phrase if requested. To be
useful in real-world applications, it should be able to
create basic descriptions that are as true as possible
to the image, as well as descriptions that guess prob-
able information based on language analysis alone.
To our knowledge, no current system provides this
functionality. Midge is built based on these goals.
Our approach converts object detections to de-
scriptive sentences using a tree-generating deriva-
tion process that fleshes out lexicalized syntactic
? Thanks to the CLSP 2011 summer workshop
at Johns Hopkins for making this system possible.
Midge is available to try online at
http://recognition.cs.stonybrook.edu:8080/?mitchema/midge/
and http://mcvl.cewit.stonybrook.edu//?mitchema/midge/ and
screenshots at http://www.abdn.ac.uk/?r07mm9/midge/
structure around object nouns. Likely subtrees are
learned from a cleaned version of the Flickr dataset
(Ordonez et al, 2011) parsed using the Berkeley
parser. The final structures generated by the system
are present-tense declarative sentences in Penn Tree-
bank syntax.
With this in place, the system can generate a dog,
a black dog sleeping, a furry black dog sleeping by a
cat, etc., while also suggesting further detectors for
the vision system to run. Approaching the problem
in this way, Midge provides a starting point for gen-
eration to meet different goals: from automatically
creating stories or summaries based on visual data,
to suggesting phrases that a speech-impaired AAC
user can select to assist in conversation. There is
still much work to be done, but we believe that the
basic architecture used by this system is a solid start-
ing point for generating a wide variety of descriptive
content, and makes clear some of the issues a vision-
to-language system must handle in order to generate
natural-sounding descriptions.
2 Background
Previous work on generating image descriptions can
be characterized as prioritizing among several goals:
? Creating language that is poetic or metaphori-
cal (Li et al, 2011)
? Creating automatic captions with syntactic
variation based on semantic visual information
(Farhadi et al, 2010)
? Creating language describing the scene in a ba-
sic template-driven way, utilizing attribute de-
tections (Kulkarni et al, 2011) or likely verbs
from a language model (Yang et al, 2011)
To meet one goal, other goals are often compro-
mised. Yang et al (2011) fill in likely verbs to form
complete sentences, but limit the generated struc-
tures to a simple template, without capturing natu-
ral variation in sentence length or surface structure.
131
Li et al (2011) aim at more metaphorical and var-
ied language, but the generated structures are often
syntactically and semantically ill-formed. Farhadi et
al. (2010) generate natural, varied, descriptive lan-
guage, but this is created by copying captions di-
rectly from similar images, resulting in captions that
are often not true to the actual query image content.
Midge builds on ideas from these systems, ad-
ditionally mapping the structures underlying vision
detections to syntactic structures and data-driven
distributional information underlying natural lan-
guage descriptions. With this in place, the door is
opened for language and vision to communicate at
a deep syntactic-semantic level. The language com-
ponents of the system can filter and expand on given
visual information, and can also call back to the vi-
sual system itself, specifying further detectors to run
(or train) based on semantically related or expected
information. We hope that this system not only ad-
vances work in generating visual descriptions, but
work in training visual detectors as well.
3 Vision to Language Issues
The process of developing Midge brought to light
several key issues that any vision-to-language sys-
tem aiming to generate descriptive, varied, human-
like language must handle:
Descriptiveness: Should the system include infor-
mation about everything there is evidence for, limit
that information, or add to it?
World knowledge: What sorts of things in an image
are remarkable, and should be mentioned, and which
may go without saying?
Object grouping: Which objects should be men-
tioned together? How do people divide objects
among sentences when they describe an image?
Which detections should not be mentioned?
Noun ordering: In what order should the objects be
named?
Reference plurals and sets: How should sets of
objects be described as a whole? Should the exact
number be included (four chairs), a vague term (a
few chairs) or a general plural form (chairs)?
Modifier ordering: How should the different modi-
fiers common to descriptions be ordered to make the
utterances sound fluent?
Determiner selection: When should objects be
treated as given (the sky), new (a boy), mass (grass),
or count (a blade)?
Verb selection: Given that action/pose detection in
computer vision does not function reliably, should
verbs be hallucinated from a language model alone?
Should they be left out?
Preposition selection: How should spatial relations
between objects be analyzed, and how does this
translate to language describing the scene layout?
Surface realization: What final lexicalization deci-
sions need to be made to realize the generated strings
within the output language?
Final string selection: Given a set of possible out-
puts, how is the final output string decided?
Nonsense detections: How should the system han-
dle computer vision detections that are often wrong?
Many of these issues are well-suited to statisti-
cal NLP techniques, and some (modifier ordering,
final string selection) have already been addressed in
the NLP community. Where appropriate, Midge in-
corporates this technology alongside novel solutions
to issues that have not yet been heavily researched
(determiner selection, nominal ordering). We hope
to further refine Midge?s solutions as technology in
these areas advances.
Separating Midge?s architecture into components
that handle each of these issues separately means
that the system is flexible to change the kind of lan-
guage it generates depending on the goals of the
end user. The system offers general solutions to
the issues listed above, and can have many of its
goals changed if specified at run-time, resulting in
different kinds of generated utterances. Midge can
successfully create natural, varied descriptions that
add descriptive content based on language modeling
alone; it can also generate descriptions that are more
limited, but as true as possible to the image.
4 Natural Language Generation in Midge
- id: 1, type: 1, label: bus, score: 0.73, bbox: [65.0, 65.0, 415.0,
191.0], attrs: {?blue?: 0.01, ?furry?:.02, . . . , ?shiny?: 0.69}
- id: 2, type: 1, label: road, score: 0.95, bbox: [1.0, 95.0, 440.0,
235.0], attrs: {?blue?: 0.01, . . .}
- preps {1,2}: ?by?
Figure 1: Computer Vision Out / Midge In (Excerpt)
The input to Midge is the output of vision detec-
tions, with detectors run for objects and attributes
within each object?s bounding box. In this demon-
stration, we incorporate the Kulkarni et al (2011)
vision detections. This provides objects/stuff and as-
sociated attributes, bounding boxes, and spatial rela-
tions between object pairs derived from the bound-
132
ing boxes. Object detections are based on Felzen-
szwalb?s multi-scale deformable parts models, and
stuff detections are based on linear SVMs for low
level region features.
Language generation in Midge is driven by a lex-
icalized derivation process that uses likely syntac-
tic and distributional information for object nouns
to create present-tense declarative sentences. Object
detections form the basis of the computer vision de-
tections, and these in turn are linked to nouns that
form the basis of the generated output string.
The syntactic trees used to collect and generate
likely subtrees for object nouns is outlined in Figure
2. Each anchor noun selects for a set of likely ad-
jectives a, determiners d, prepositions p and present
tense verbs v.
1 2
NP
NN
bottle
JJ* ? (a)DT ? (d)
S
VP
VBZ ? (v)
NP{NN, bottle}
3 4
NP
VP
VB{G|N} ? (v)
NP{NN, bottle}
NP
PP
IN ? (p)
NP {NN, bottle}
5 6
VP
NP{NN, bottle}VB{G|N|Z} ? (v)
PP
NP{NN, bottle}IN ? (p)
7 8
VP
PP
NP{NN, bottle}IN ? (p)
VB{G|N|Z} ? (v) VP
VP* ?
9 10
S
VP
PP
IN ? (p)
VBZ ? (v)
NP{NN, bottle} NP
NP ?CC
and
NP ?
11
NP
VP
PP
IN ? (p)
VB{G|N} ? (v)
NP{NN, bottle}
Figure 2: Trees for generation. Each {NN, noun} selects
for its local subtrees. ? marks a substitution site, * marks
? 0 sister nodes of this type permitted. Input: set of or-
dered nouns, Output: trees preserving nominal ordering.
5 Architecture
Midge can be explained at a high level as a pipelined
system incorporating the following steps:
Step 1: Run detectors for objects, stuff, action/pose
and attributes; pass as <detection, score> pairs to
Midge. Vision output/NLG input is displayed in Fig-
ure 1 and in the system demo.
Step 2: Group objects together that will be men-
tioned together.
Step 3: Order objects within each group ? this au-
tomatically sets the subject and objects of the sen-
tence. Midge currently order nouns based on Word-
Net hypernyms.
Step 4: Create all tree structures that can be gen-
erated from the object noun node. (See Figure 2).
Noun anchors select for adjectives (JJ), determin-
ers (DT), prepositions (IN) and if specified, verbs
(VBG, VBN, or VBZ).
Step 5: Limit adjectives (JJ) to the set that are not
mutually exclusive ? different values for the same at-
tribute class. REG comes into play at this step.
Step 6: Create all trees that combine following the
given trees until all object nouns in a group are un-
der one node (either NP or S).
Step 7: Order selected adjectives. We use the top-
scoring ngram model from (Mitchell et al, 2011).
Step 8: Choose final tree from set of generated trees.
Users can select a longest-string or cross entropy
calculation.
References
A. Farhadi, M. Hejrati, P. Young Sadeghi, C. Rashtchian,
J. Hockenmaier, and D. A. Forsyth. 2010. Every
picture tells a story: generating sentences for images.
Proc. ECCV 2010.
G. Kulkarni, V. Premraj, and S. Dhar, et al 2011. Baby
talk: Understanding and generating image descrip-
tions. Proc. CVPR 2011.
S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi.
2011. Composing simple image descriptions using
web-scale n-grams. Proc. CoNLL 2011.
M. Mitchell, A. Dunlop, and B. Roark. 2011. Semi-
supervised modeling for prenominal modifier order-
ing. Proc. ACL 2011.
V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. Proc. NIPS 2011.
Y. Yang, C. L. Teo, H. Daume? III, and Y. Aloimonos.
2011. Corpus-guided sentence generation of natural
images. Proc. EMNLP 2011.
133
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 9?18,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Discourse-Based Modeling for AAC
Margaret Mitchell Richard Sproat
Center for Spoken Language Understanding
Oregon Health & Science University
m.mitchell@abdn.ac.uk, rws@xoba.com
Abstract
This paper presents a method for an AAC sys-
tem to predict a whole response given features
of the previous utterance from the interlocu-
tor. It uses a large corpus of scripted dialogs,
computes a variety of lexical, syntactic and
whole phrase features for the previous utter-
ance, and predicts features that the response
should have, using an entropy-based measure.
We evaluate the system on a held-out portion
of the corpus. We find that for about 3.5% of
cases in the held-out corpus, we are able to
predict a response, and among those, over half
are either exact or at least reasonable substi-
tutes for the actual response. We also present
some results on keystroke savings. Finally
we compare our approach to a state-of-the-art
chatbot, and show (not surprisingly) that a sys-
tem like ours, tuned for a particular style of
conversation, outperforms one that is not.
Predicting possible responses automatically
by mining a corpus of dialogues is a
novel contribution to the literature on whole
utterance-based methods in AAC. Also useful,
we believe, is our estimate that about 3.5-4.0%
of utterances in dialogs are in principle pre-
dictable given previous context.
1 Introduction
One of the overarching goals of Augmentative and
Alternative Communication technology is to help
impaired users communicate more quickly and more
naturally. Over the past thirty years, solutions
that attempt to reduce the amount of effort needed
to input a sentence have include semantic com-
paction (Baker, 1990), and lexicon- or language-
model-based word prediction (Darragh et al, 1990;
Higginbotham, 1992; Li and Hirst, 2005; Trost et
al., 2005; Trnka et al, 2006; Trnka et al, 2007;
Wandmacher and Antoine, 2007), among others. In
recent years, there has been an increased interest
in whole utterance-based and discourse-based ap-
proaches (see Section 2). Such approaches have
been argued to be beneficial in that they can speed up
the conversation, thus making it appear more felici-
tous (McCoy et al, 2007). Most commercial tablets
sold as AAC devices contain an inventory of canned
phrases, comprising such items as common greet-
ings, polite phrases, salutations and so forth. Users
can also enter their own phrases, or indeed entire se-
quences of phrases (e.g., for a prepared talk).
The work presented here attempts to take whole
phrase prediction one step further by automatically
predicting appropriate responses to utterances by
mining conversational text. In an actual deploy-
ment, one would present a limited number of pre-
dicted phrases in a prominent location on the user?s
device, along with additional input options. The user
could then select from these phrases, or revert to
other input methods. In actual use, one would also
want such a system to incorporate speech recogni-
tion (ASR), but for the present we restrict ourselves
to typed text ? which is perfectly appropriate for
some modes of interaction such as on-line social me-
dia domains. Using a corpus of 72 million words
from American soap operas, we isolate features use-
ful in predicting an appropriate set of responses for
the previous utterance of an interlocutor. The main
results of this work are a method that can automati-
9
cally produce appropriate responses to utterances in
some cases, and an estimate of what percentage of
dialog may be amenable to such techniques.
2 Previous Work
Alm et al (1992) discuss how AAC technology can
increase social interaction by having the utterance,
rather than the letter or word, be the basic unit
of communication. Findings from conversational
analysis suggest a number of utterances common to
conversation, including short conversational openers
and closers (hello, goodbye), backchannel responses
(yeah?), and quickfire phrases (That?s too bad.). In-
deed ?small talk? is central to smooth-flowing con-
versation (King et al, 1995). Many modern AAC
systems therefore provide canned small-talk phrases
(Alm et al, 1993; Todman et al, 2008).
More complex conversational utterances are chal-
lenging to predict, and recent systems have used
a variety of approaches to generate longer phrases
from minimal user input. One approach relies on
telegraphic input, where full sentences are con-
structed from a set of uninflected words, as in the
Compansion system (McCoy et al, 1998). This
system employs a semantic parser to capture the
meaning of the input words and generates using
the Functional Unification Formalism (FUF) system
(Elhadad, 1991). One of the limitations of this ap-
proach is that information associated with each word
is primarily hand-coded on the basis of intuition; as
a result, the system cannot handle the problem of un-
restricted vocabulary. Similar issues arise in seman-
tic authoring systems (Netzer and Elhadad, 2006),
where at each step of the sentence creation process,
the system offers possible symbols for a small set of
concepts, and the user can select which is intended.
Recent work has also tried to handle the complex-
ity of conversation by providing full sentences with
slots that can be filled in by the user. Dempster et
al. (2010) define an ontology where pieces of hand-
coded knowledge are stored and realized within sev-
eral syntactic templates. Users can generate utter-
ances by entering utterance types and topics, and
these are filled into the templates. The Frametalker
system (Higginbotham et al, 1999) uses contextual
frames ? basic sentences for different contexts ?
with a set vocabulary for each. The intuition be-
hind this system is that there are typical linguistic
structures for different situations and the kinds of
words that the user will need to fill in will be se-
mantically related to the context. Wisenburn and
Higginbotham (2008) extend this technology using
ASR on the speech of the interlocutor. The system
extracts noun phrases from the speech and presents
those noun phrases on the AAC device, with frame
sentences that the user can then select. Thus, if the
interlocutor says Paris, the AAC user will be able to
select from phrases like Tell me more about Paris or
I want to talk about Paris.
Other approaches provide a way for users to
quickly find canned utterances. WordKeys (Langer
and Hickey, 1998) allows users to access stored
phrases by entering key words. This system ap-
proaches generation as a text retrieval task, using a
lexicon derived from WordNet to expand user input
to find possible utterances. Dye et al (1998) intro-
duce a system that utilizes scripts for specific situa-
tions. Although pre-stored scripts work reasonably
well for specific contexts, the authors find (not unex-
pectedly) that a larger number of scripts are needed
for the system to be generally effective.
3 The Soap Opera Corpus
In this work we attempt a different approach, devel-
oping a system that can learn appropriate responses
to utterances given a corpus of conversations.
Part of the difficulty in automatically generating
conversational utterances is that very large corpora
of naturally occurring dialogs are non-existent. The
closest such corpus is Switchboard (Godfrey and
Holliman, 1997), which contains 2,400 two-sided
conversations with about 1.4 million words. The in-
terlocutors in Switchboard are not acquainted with
each other and they are instructed to discuss a par-
ticular topic. While the dialogs are ?natural? to a
point, because they involve people who have never
previously met, they are not particularly reflective of
the kinds of conversations between intimates that we
are interested in helping impaired users with.
We thus look instead to a corpus of scripted di-
alogs taken from American soap operas. The web-
site tvmegasite.net contains soap opera scripts
that have been transcribed by aficionados of the var-
ious series. The scripts include utterances marked
10
with information on which character is speaking,
and a few dramatic cues. We downloaded 72 mil-
lion words of text, with 5.5 million utterances. Soap
opera series downloaded were: All my Children, As
the World Turns, The Bold and the Beautiful, Days
of our Lives, General Hospital, Guiding Light, One
Life to Live and The Young and the Restless. The text
was cleaned to remove HTML markup and other ex-
traneous material, and the result was a set of 550,000
dialogs, with alternating utterances by (usually) two
speakers. These dialogs were split 0.8/0.1/0.1 into
training, development testing and testing portions,
respectively. All results reported in this paper are on
the development test set.
While soap operas may not be very representative
of most people?s lives, the corpus nonetheless has
three advantages. First of all, the corpus is large.
Second, the language tends to be fairly colloquial.
Third, many of the dialogs take place between char-
acters who are supposed to know each other well,
often intimately; thus the topics might be more re-
flective of casual conversation between friends and
intimates than the dialogs one finds in Switchboard.
4 Data Analysis, Feature Extraction and
Utterance Prediction
Each dialog was processed using the Stanford Core
NLP tools. The Stanford tools perform part of
speech tagging (Toutanova et al, 2003), constituent
and dependency parsing (Klein and Manning, 2003),
named entity recognition (Finkel et al, 2005), and
coreference resolution (Lee et al, 2011). From
the output of the Stanford tools, the following fea-
tures were extracted for each utterance: word bi-
grams (pairs of adjacent words); dependency-head
relations, along with the type of dependency rela-
tion (basically, governors ? e.g., verbs ? and their
dependents ? e.g., nouns); named entities (per-
sons, organizations, etc.); and the whole utterance.
Extracted named entities include noun phrases that
were explicitly tagged as named entities, as well as
any phrases that were marked as coreferential with
named entities. Thus if the pronoun she occurred in
an utterance, and was marked as coreferential with a
previous or following named entity Amelia, then the
feature Amelia as a named entity was added for this
utterance. We also include the whole utterance as a
feature, which turns out to be the most useful predic-
tor for an appropriate response to an input utterance.
The dialogs were divided into turns, with each
turn consisting of one or more utterances. For our
experiments, we are interested in predicting the first
utterance of a turn (which in many cases may be the
whole turn) given features of all the utterances of
the previous turns ? the exception being that for
the whole sentence feature, only the last sentence of
the previous turn is used. The method of using fea-
tures of a turn to predict features of the next turn is
related to the work reported in Purandare and Lit-
man (2008), though their goal was to analyze dialog
coherence rather than to predict the next utterance.
We are particularly interested in feature values
that are highly skewed in their predictions, mean-
ing that if the turn has a given value, then the first
sentence of the next utterance is much more likely
to have some values than others. A useful measure
of this is the difference between the entropy of the
predicted feature values fi of a feature g:
H(g) = ?
n?
i=0
log(p(fi)) ? p(fi) (1)
and the maximum possible entropy of g given n pre-
dicted features, namely:
Hmax(g) = ?log(
1
n
) (2)
The larger the difference Hmax(g)?H(g), the more
skewed the distribution.
For the purposes of this experiment and to keep
the computation reasonably tractable, we computed
the entropic values described above for like features:
thus we used bigram features to predict bigram fea-
tures, dependency features to predict dependency
features, and so forth. We also filtered the output of
the process so that each feature of the prior context
had a minimum of 10 occurrences, and the entropy
of the feature was no greater than 0.9 of the max-
imum entropy as defined above. For each feature
value, the 2 most strongly associated values for the
predicted utterance were stored.
To take a simple example (Figure 1) the bigram ?m
fine has a strong association with the bigrams you ?re
and , I, these co-occurring 486 and 464 times in the
training corpus, respectively. For this feature, the
11
?m fine 8.196261 9.406976 you ?re 486
?m fine 8.196261 9.406976 , i 464
you?re kidding . __SENT 4.348040 4.852030
no. . __SENT 32
you?re kidding . __SENT 4.348040 4.852030
i wish . __SENT 7
Figure 1: Examples of bigram and full-sentence features.
entropy is 8.20 and the maximum entropy is 9.41.
Or consider a full-sentence feature You?re kidding.
This is strongly associated with the predicted sen-
tence features no.. and I wish..
Utterances in the training data were stored and as-
sociated with predicted features. In order to pro-
duce a rank-ordered list of possible responses to a
test utterance, the features of the test utterance are
extracted. For each of these features, the predicted
features and their entropies are retrieved. Those
training data utterances that match on one or more
of these predicted features are retrieved in this step,
and a score is assigned which is simply the sum of
the predicted feature entropies. However, since we
want to favor full-sentence matches, entropies for
full-sentence matches are multiplied by a positive
number (currently set to 100).
5 Experimental Results
5.1 Whole sentence prediction
The first question we were interested in is how of-
ten, based on the approach described here, one could
predict a sentence that is close to what the speaker
actually intended to say. For this purpose, we sim-
ply took as the gold standard the utterance that was
written in the script for the speaker, and considered
the prediction of the system described above, when
it was able to make one. The prediction could be
an exact match to what was actually said, something
close enough to be a reasonable substitute, some-
thing appropriate given the context but not the one
intended, or something that is wholly inappropriate.
In the ensuing discussion we will focus on whole
sentence features, since these were the most useful
for predicting reasonable whole sentences. We re-
turn to the use of other features in Section 5.2.
Some examples can be found in Figure 2. In
each case, we give the final sentence of the previous
turn, the actual utterance, and the two predicted ut-
PREV really ?
ACTUAL yeah .
PRED 232.3099 yeah . __SENT 4
PRED 230.9528 mm-hmm . __SENT 3
PREV love you .
ACTUAL i love you , too , baby doll .
PRED 83.4519 i love you , too . __SENT 3
PRED 74.1185 love you . __SENT 3
PREV ok ?
ACTUAL i?m sorry , laurie , about j.r. ,
about everything .
PRED 86.2623 yeah . __SENT 2
PRED 86.2623 ok . __SENT 2
Figure 2: Whole sentence prediction examples.
terances, along with the predicted utterances? scores
and the counts with which they co-occurred in the
training data with the previous utterance in question.
For the first example Really?, the actual response
was Yeah, and this was also the highest ranked re-
sponse of the system. In the second example, the ac-
tual response was I love you, too, baby doll, whereas
a response of the system was I love you too. While
not exact, this is arguably close enough, and could
be selected by an impaired user who did not wish to
type the whole message. In the third example, the
predictions Yeah. and Ok. do not substitute at all for
the actual response.
Of the 276,802 utterance-response pairs in the de-
velopment test data, the system was able to make
predictions for 9,794 cases, or 3.5%. Evaluating
9,794 responses is labor intensive, so two evalua-
tions based on random samples were performed.
In the first, the authors evaluated a random sam-
ple of 455 utterance pairs, assigning the following
scores to each response: 4 exact match; 3 equiva-
lent meaning; 2 good answer but not the right one;
1 inappropriate. The results are given in Table 1, for
the best score of the pair of responses generated. In
other words, if the first response has a score of 2 and
the second a score of 3, then the pair of responses
will receive a score of 3: in that pair, there was one
generated response that was close enough to use.
From Table 1, we see that between 38% to 40.7%
of the response pairs contained a response that was
exact, or close enough to have the same meaning.
59.3% to 62% had at best a reasonable answer, but
not the one intended. Finally, none contained only
12
Score Judge 1 Judge 2
Exact match 110 24.2% 109 24.0%
Equivalent meaning 63 13.8% 76 16.7%
Good answer (but wrong) 282 62.0% 270 59.3%
Inappropriate 0 0.0% 0 0.0%
Table 1: Judgments of a sample of 455 utterance pairs by
the authors.
inappropriate answers: this is not surprising, given
that all of the predicted responses were based on
what was found in the training data, which one may
assume involved largely felicitous interactions.
We also used Amazon?s Mechanical Turk (AMT)
to collect judgements from unbiased judges. Based
on our previous evaluation, we expanded the equiv-
alent meaning category into two more fine-grained
categories, essentially the same and similar mean-
ing, in order to capture phrases with slightly differ-
ent connotations. This results in the 4-point scale
in Table 2. Exact matches were found automatically
before giving response pairs to Turkers, and account
for a large portion of the data ? 2,330 of the 9,794
response pairs, or 23.8%. For the remaining 76.2%,
138 participants were asked to judge how close the
predicted response was to the actual response.
Each AMT participant was presented with six
prompts (three entropy-based conversational turns
and three chatbot-based conversational turns, dis-
cussed below). Each prompt listed the utterance,
actual response, and predicted response. Two ad-
ditional prompts with known answers were included
to automatically flag participants who were not fo-
cusing on the task. Evaluation results are given in
4 Essentially
the same:
They?re pretty close, and mean
basically the same thing.
3 Similar
meaning:
They?re similar, but the pre-
dicted response has a slightly
different connotation from the
actual response.
2 Good answer,
but not the
right one:
They?re different, but the pre-
dicted response is still a reason-
able response to the comment.
1 Inappropriate: Different, and the predicted re-
sponse is a totally unreasonable
response to the comment.
Table 2: Four-point scale for AMT evaluation. Exact
matches were found automatically.
Essentially the same 89 16.4%
Similar meaning 81 14.9%
Good answer (but wrong) 165 30.4%
Inappropriate 79 14.5%
Table 3: Evaluation results from AMT on a random
sample of 414 predicted utterances (excluding exact
matches).
Table 3. Percentages are multiplied by the propor-
tion of results they represent (.762). Of the evalu-
ated cases, we find that 31.3% of the predicted re-
sponses were judged to be essentially the same or
similar to the actual response. 30.4% were judged
to be a reasonable answer, and the remaining 14.5%
were judged to be inappropriate.
Evaluation by AMT judges was thus much more
favorable towards the prediction-based system than
the authors? evaluation. Where the authors found
13.8%-16.7% to be essentially the same or similar,
unbiased judges found just under a third of the data
to meet these criteria. Coupled with the automati-
cally detected exact matches, 55.1% of the predicted
responses were found to be a reasonable approxima-
tion of (or exactly) the intended response. A smaller
portion of the data was thought to be a good answer
(but wrong), or wholly inappropriate.
5.2 Prediction with features plus a prefix of the
intended utterance
It is of course not necessary for the system to predict
the whole response without any input from the user.
As with word prediction, the user might type a pre-
fix of the intended utterance, and the system could
then produce a small set of corresponding responses,
among which would often be the one desired.
In order to evaluate such a scenario, we consid-
ered the shortest prefix of the actual intended re-
sponse that would be consistent with a maximum
of five sentences predicted from the features of the
previous turn. Thus, we gathered the entire set of
sentences from the training data that matched one or
more of the predicted features, then began (virtually)
typing the actual response. There are two possible
outcomes. If the actual response is not in the set,
then at some point the typed prefix will be consistent
with none of the sentences in the set. In this worst
case, the user would simply have to type the whole
sentence (possibly using whatever word-completion
13
technology is already available on the device). But
if the intended response is in the set, then at some
point the set consistent with the prefix will be win-
nowed down to at most five members. The length of
the prefix at that point, subtracted from the length of
the intended sentence, is the keystroke savings.
Of the 276,802 utterances in the development test
responses, 11,665 (4.2%) had a keystroke savings
of greater than zero: thus, in 4.2% of cases, the in-
tended utterance was to be found among the set of
sentences consistent with the predicted features. The
total keystroke savings was 102,323 characters out
of a total of 8,725,508, or about 1%. While this is
clearly small, note that it is over and above whatever
keystroke savings one would gain by other methods,
such as language modeling.
5.3 ALICE
A final experiment involved using a chatbot to gen-
erate responses. Previous approaches have used
stored sentence templates that are generated based
on keyword input from the user; a similar approach
is used in a chatbot, where the input utterances are
themselves triggers for the generated content. For
this experiment, we used the publicly available AL-
ICE (Wallace, 2012), which won the Loebner Prize
(a Turing test) in 2000, 2001, and 2004. ALICE
makes use of a large library of pattern-action pairs
written in AIML (Artificial Intelligence Markup
Language): if an input sentence matches a partic-
ular pattern, a response is generated by a rule that is
associated with that pattern. ALICE follows conver-
sational context by using a notion of TOPIC (what
the conversation is currently about, based on key-
words) and of THAT (the bot?s previous utterance).
Both are used along with the input utterance when
selecting what next to say. In essence, ALICE is a
much more sophisticated version of the 1960s Eliza
program (Weizenbaum, 1966).
In order to use the chatbot for this task, we use an
AIML interpreter (Stratton, 2010) on the most recent
set of ALICE knowledge.1 ALICE was given the
utterances for each conversation in our development
testing set, which allows the system to store some
of the dialogue context under its THAT and TOPIC
1http://code.google.com/p/aiml-en-us-foundation-alice/, re-
trieved February 2012.
Essentially the same 45 10.7%
Similar meaning 96 22.9%
Good answer (but wrong) 135 32.1%
Inappropriate 138 32.9%
Table 4: Evaluation results from AMT on a random sam-
ple of 414 chatbot utterances (excluding exact matches).
variables.
Example responses are given in Figure 3. As with
the previous experiments, some responses are close
to the actual intended message (first example in Fig-
ure 3). In some other cases (second example), the
response is reasonable, though not the one intended.
But in many cases, the response is too ?cute?, as in
the examples on the righthand side.
Evaluation with AMT is given in Table 4, us-
ing the same scoring criteria as in Table 3. Exact
matches are again automatically removed, and ac-
count for 142 responses (1.5%). For the remaining
data, participants were asked to judge how close the
chatbot response was to the actual response. Per-
centages are multiplied by the proportion of results
they represent (.9855).
The chatbot is judged to produce less acceptable
utterances than the entropy-based approach, with
10.7% essentially the same as the actual response,
and 22.9% with similar meaning. Coupled with the
automatically detected exact matches, 35% of the
chatbot responses were found to be a reasonable ap-
proximation of (or exactly) the intended response.
Over half of the data (65%) was thought to be a good
answer (but wrong), or wholly inappropriate.
6 Discussion and Future Work
Of the two different methods for automatically gen-
erating responses, the entropy-based approach in
particular produces responses ranging from similar
to exact in a majority of the cases for which it can
make a prediction. These are promising findings
for expanding a system that populates possible re-
sponses from an input utterance.
An obvious limitation of the work presented here
is that it is based on scripted data. More to the point,
to build an effective dialog-based system for an im-
paired user, the best possible data to use would be
data involving the user him/herself along with his
or her interlocutors. Indeed, we are in the process
14
PREV okay .
ACTUAL okay .
ALICE Great.
PREV please don?t leave .
ACTUAL i?m right here .
ALICE I?m not going anywhere.
PREV you need to drink water .
ACTUAL no , stay .
ALICE I need to wait and see if the
opportunity presents itself.
PREV okay ?
ACTUAL you were so mad at me
lately , dixie .
ALICE Dude!
Figure 3: Some responses generated by ALICE.
of collecting a corpus of such interactions from a
small number of AAC users in the Portland, Ore-
gon area. But the resulting corpora will obviously
be tiny in comparison with the data used in the ex-
periments here, in no small measure because of the
extreme slowness with which most AAC users are
able to communicate. What can be done about this?
One thing would be to use the results of this work
directly even if it does not model the particular user:
even if it comes from soap opera dialogs, Are you
mad at me? No, I?m not mad at you, still makes
for a perfectly reasonable utterance/response pair.
This, to some extent, counters potential objections
that soap opera dialogs are not reflective of natural
interactions. These kinds of pairs could be supple-
mented by whatever data we are able to learn from a
particular user.
Even better, though, would be to collect large
amounts of data from users before they become im-
paired. Many disorders, such as ALS, are often de-
tected early, before they start to impair communi-
cation. In such cases, one could consider language-
banking the user?s interactions, and building a model
of the ways in which the user interacts with other
speakers, in order to get a good model of that par-
ticular user. While there are obviously privacy con-
cerns, a person who knows that they will lose the
ability to speak over time will likely be very moti-
vated to try to preserve samples of their speech and
language, assuming there exists technology that can
use those samples to provide more sophisticated as-
sistance when it becomes needed.
It may also be possible to use features from the
text to generate utterances, similar to the telegraphic
approaches to generation discussed in Section 2, but
automatically learning words that can be used to
generate appropriate responses to an utterance. As
a first look at the feasibility of this approach, we use
the Midge generator (Mitchell et al, 2012), rebuild-
ing its models from the soap dialogues. Midge re-
quires as input a set of nouns and then builds likely
syntactic structure around them, and so we use the
dialogues to predict possible nouns in response to
an input utterance. For each <utterance, response>
pair in the dialogues, we gather all utterance nouns
nu and all response nouns nr. We then compute nor-
malized pointwise mutual information (nPMI) for
each nu, nr pair type in the corpus. Given a novel in-
put utterance, we tag it to extract the nouns and cre-
ate the set of highest nPMI nouns from the model.
This is then input to Midge, which uses the set to
generate present-tense declarative sentences. Some
examples are given in Figure 4. We hope to expand
on this approach in future work.
A further improvement is to take advantage of
synonymy. There are many ways to convey the same
basic message: i am sick, i am not feeling well, i?m
under the weather, are all ways for a speaker to con-
vey that he or she is not in the best of health. In
the current system, these are all treated separately.
Clearly what is needed is a way of recognizing that
these are all paraphrases of each other. Fortunately,
there has been a lot of progress in recent years on
paraphrasing ? see Ganitkevitch et al (2011) for a
recent example ? and such work could in princi-
ple be adapted to the problem here. Indeed it seems
likely that incorporating paraphrasing into the sys-
tem will be a major source of improved coverage.
A limitation of the work described here is that
it only models turn-to-turn interactions. Clearly
discourse models need to have more memory than
this, so features that relate to earlier turns would be
needed. The downside is that this would quickly
lead to data sparsity.
There are a variety of machine learning tech-
niques that could also be tried, beyond the rather
15
Input: this is n?t the same . this is not like anything i have been
through before . i mean , how am i supposed to make it work with
somebody who ...
Pred. nouns: strength, somebody
Output: strength comes with somebody
Input: i ?ve been a little bit too busy to socialize . i did have an
interesting conversation with your sister , however .
Pred. nouns: bit, conversation, sister
Output: a bit about this conversation with sister
Figure 4: Generating with nPMI: Creating syntactic structure around likely nouns.
simple methods employed in this work. For exam-
ple, particular classes of response types, comprising
a variety of related utterances, may be predictable
using the extracted features.
Finally, we have assumed for this discussion that
the AAC system is only within the control of the im-
paired user. There is no reason to make that assump-
tion in general: many AAC situations in real life in-
volve a helper who will often co-construct with the
impaired user. Such helpers usually know the im-
paired user very well and can often make reasonable
guesses as to the whole utterance intended by the
impaired user. Recent work reported in Roark et al
(2011) suggests one way in which the results of a
language modeling system and those of a human co-
constructor may be integrated into a single system,
and such an approach could easily be applied here.
7 Conclusions
We have proposed and evaluated an approach to
whole utterance prediction for AAC. While the ap-
proach is fairly simple, it is able to generate correct
or at least reasonable responses in some cases. Such
a system could be used in conjunction with other
techniques, such as language-model-based predic-
tion, or co-construction. One of the potentially use-
ful side-effects of this work is an estimate of what
percentage of interactions in a dialog are likely to be
easily handled by such techniques. In other words,
how many interactions in dialog are sufficiently pre-
dictable that a system could have a reasonable guess
as to what a speaker is going to say given the pre-
vious context? A rough estimate based on what we
have found here is something on the order of 3.5%-
4.0%. Obviously this does not mean that the sys-
tem will always make the right prediction: a reason-
able response to congratulations on your promotion
would often be thank you, but a speaker may wish
to say something else. But what it does mean is that
in about 3.5%-4.0% of cases, one has a reasonable
chance of being able to guess. This percentage is
certainly small, and one might be inclined to con-
clude that the approach does not work. On the other
hand, it is important to bear in mind that not all per-
centages are created equal. Rapid responses to ba-
sic phrases (e.g. Are you mad at me? ? No, I?m
not mad at you), could help with the perceived flow
of conversation, even if they do not occur that fre-
quently.
As we noted at the outset, whole utterance pre-
diction is an area that has received increased inter-
est in recent years, because of its potential to speed
communication, and its contribution to increasing
the naturalness of conversational interactions. When
coupled with gains in utterance generation achieved
by other methods, automatically generating utter-
ances can further the range of comments and re-
sponses available to AAC users. The work reported
here is a small contribution towards this goal.
Acknowledgments
This work was supported under grant NIH-
K25DC011308. Sproat thanks his K25 mentor,
Melanie Fried-Oken, for discussion and support. We
also thank four anonymous reviewers, as well as the
audience at a Center for Spoken Language Under-
standing seminar, for their comments.
16
References
N. Alm, J. L. Arnott, and A. F. Newell. 1992. Predic-
tion and conversational momentum in an augmentative
communication system. Communications of the ACM,
35(5):46?57.
N. Alm, J. Todman, Leona Elder, and A. F. Newell. 1993.
Computer aided conversation for severely physically
impaired non-speaking people. Proceedings of IN-
TERCHI ?93, pages 236?241.
Bruce Baker. 1990. Semantic compaction: a basic tech-
nology for artificial intelligence in AAC. In 5th An-
nual Minspeak Conference.
J. J. Darragh, I. H. Witten, and M. L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
Martin Dempster, Norman Alm, and Ehud Reiter. 2010.
Automatic generation of conversational utterances and
narrative for augmentative and alternative communi-
cation: A prototype system. Proceedings of the Work-
shop on Speech and Language Processing for Assistive
Technologies (SLPAT), pages 10?18.
R. Dye, N. Alm, J. L. Arnott, G. Harper, and A Morrison.
1998. A script-based AAC system for transactional
interaction. Natural Language Engineering, 4(1):57?
71.
Michael Elhadad. 1991. FUF: The universal unifer-user
manual version 5.0. Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
363?370.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. Proceedings of Empirical
Methods in Natural Language Processing (EMNLP).
John Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Con-
sortium, Philadelphia.
D. J. Higginbotham, D. P. Wilkins, G. W. Lesher, and
B. J. Moulton. 1999. Frametalker: A communication
frame and utterance-based augmentative communica-
tion device. Technical Report.
D. Jeffery Higginbotham. 1992. Evaluation of keystroke
savings across five assistive communication technolo-
gies. Augmentative and Alternative Communication,
8:258?272.
Julia King, Tracie Spoeneman, Sheela Stuart, and David
Beukelman. 1995. Small talk in adult conversations:
Implications for AAC vocabulary selection. Augmen-
tative and Alternative Communication, 11:260?264.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics
(ACL), pages 423?430.
S. Langer and M. Hickey. 1998. Using semantic lexicons
for full text message retrieval in a communication aid.
Natural Language Engineering, 4(1):41?55.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. Proceedings of the
CoNLL-2011 Shared Task.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
K. McCoy, C. A. Pennington, and A. L. Badman. 1998.
Compansion: From research prototype to practical in-
tegration. Natural Language Engineering, 4(1):73?
95.
Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,
and Dallas E. Johnson. 2007. Brevity and speed of
message delivery trade-offs in augmentative and alter-
native communication. Augmentative and Alternative
Communication, 23(1):76?88.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Sratos, Xufeng Han, Alysssa Mensch,
Alex Berg, Tamara L. Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Y. Netzer and M. Elhadad. 2006. Using semantic author-
ing for Blissymbols communication boards. Proceed-
ings of HLT 2006, pages 105?108.
Amruta Purandare and Diane Litman. 2008. Analyzing
dialog coherence using transition patterns in lexical
and semantic features. In FLAIRS Conference, pages
195?200.
Brian Roark, Andrew Fowler, Richard Sproat, Christo-
pher Gibbons, and Melanie Fried-Oken. 2011. To-
wards technology-assisted co-construction with com-
munication partners. Proceedings of the Workshop on
Speech and Language Processing for Assistive Tech-
nologies (SLPAT).
Cort Stratton. 2010. PyAIML, a Python AIML inter-
preter. http://pyaiml.sourceforge.net/.
J. Todman, A. Norman, J. Higginbotham, and P. File.
2008. Whole utterance approaches in AAC. Augmen-
tative and Alternative Communication, 24(3):235?
254.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. Proceed-
ings of HLT-NAACL, pages 252?259.
17
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
Richard Wallace. 2012. A.L.I.C.E. (Artificial Linguistic
Internet Computer Entity). http://www.alicebot.org/.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP), pages 506?513.
Joseph Weizenbaum. 1966. Eliza ? a computer program
for the study of natural language communication be-
tween man and machine. Proceedings of the ACM,
9(1).
Bruce Wisenburn and D. Jeffery Higginbotham. 2008.
An AAC application using speaking partner speech
recognition to automatically produce contextually rel-
evant utterances: Objective results. Augmentative and
Alternative Communication, 24(2):100?109.
18
Proceedings of the 14th European Workshop on Natural Language Generation, pages 72?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Graphs and Spatial Relationsin the Generation of Referring ExpressionsJette Viethen
h.a.e.viethen@uvt.nl
TiCC
University of Tilburg
Tilburg, The Netherlands
Margaret Mitchell
m.mitchell@jhu.edu
HLT Centre of Excellence
Johns Hopkins University
Baltimore, USA
Emiel Krahmer
e.j.krahmer@uvt.nl
TiCC
University of Tilburg
Tilburg, The NetherlandsAbstract
When they introduced the Graph-Based
Algorithm (GBA) for referring expression
generation, Krahmer et al (2003) flaunted
the natural way in which it deals with re-
lations between objects; but this feature
has never been tested empirically. We fill
this gap in this paper, exploring referring
expression generation from the perspec-
tive of the GBA and focusing in particu-
lar on generating human-like expressions
in visual scenes with spatial relations. We
compare the original GBA against a variant
that we introduce to better reflect human
reference, and find that although the orig-
inal GBA performs reasonably well, our
new algorithm offers an even better match
to human data (77.91% Dice). Further, it
can be extended to capture speaker vari-
ation, reaching an 82.83% Dice overlap
with human-produced expressions.1 Introduction
Ten years ago, Krahmer et al (2003) published the
Graph-Based Algorithm (GBA) for referring ex-
pression generation (REG). REG has since become
one of the most researched areas within Natural
Language Generation, due in a large part to the
central role it plays in communication: referring
allows humans and language generation systems
alike to invoke the entities that the discourse is
about in the mind of a listener or reader.
Like most REG algorithms, the GBA is focussed
on the task of selecting the semantic content for a
referring expression, uniquely identifying a target
referent among all objects in its visual or linguistic
context. The framework used by the GBA is par-
ticularly attractive because it provides fine-grained
control for finding the ?best? referring expression,
encompassing several previous approaches. This
control is made possible by defining a desired
cost function over object properties to guide the
construction of the output expression and using a
search mechanism that does not stop at the first
solution found.
One characteristic of the GBA particularly em-
phasized by Krahmer et al (2003), advancing
from research on algorithms such as the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Greedy Algorithm (Dale, 1989), was the treatment
of relations between entities. Relations such as ontop of or to the left of fall out naturally from the
graph-based representation of the domain, a facet
missing in earlier algorithms. We believe that this
makes the GBA particularly well-suited for gener-
ating language in spatial visual domains.
In the years since the inception of the GBA,
the REG community has become increasingly in-
terested in evaluating algorithms against human-
produced data in visual domains, aiming to mimic
human references to objects. This interest has
manifested most prominently in the 2007-2009
REG Challenges (Belz and Gatt, 2007; Gatt et al,
2008; Gatt et al, 2009) based on the TUNA Cor-
pus (van Deemter et al, 2012). The GBA per-
formed among the best algorithms in all three of
these challenges. However, in particular its abil-
ity to analyze relational information could not be
assessed, because the TUNA Corpus does not con-
tain annotated relational descriptions.
We rectify this omission in the current work by
testing the GBA on the GRE3D3 Corpus, which
was designed to study the use of spatial rela-
tions in referring expressions (Viethen and Dale,
2008). We compare against a variant of the GBA
that we introduce to build longer referring expres-
72
sions, following the observation that humans tend
to overspecify (i.e., not be maximally brief) in
their referring expressions (Sonnenschein, 1985;
Pechmann, 1989; Engelhardt et al, 2006; Arts et
al., 2011). For both algorithms, we experiment
with cost functions defined at different granular-
ities to produce the best match to human data. We
find that we can match human data better than
the original GBA with the variant that encourages
overspecification.
With this model, we aim to further ad-
vance towards human-like reference by develop-
ing a method to capture speaker-specific varia-
tion. Speaker variation cannot easily be modeled
by the classic input variables of REG algorithms,
but a number of authors have shown that system
output can be improved by using speaker identity
as an additional feature; this has often been ac-
companied by the observation that commonalities
can be found in the reference behaviour of differ-
ent speakers (Bohnet, 2008; Di Fabbrizio et al,
2008a; Mitchell et al, 2011b), particularly for spa-
tial relations (Viethen and Dale, 2009). In the sec-
ond experiment reported in this paper, we combine
these insights by automatically clustering groups
of speakers with similar behaviour and then defin-
ing separate cost functions for each group to better
guide the algorithms.
Before we assess the ability of the GBA and our
variant to produce human-like referring expres-
sions containing relations (Sections 5 and 6), we
will give an overview of the relevant background
to the treatment of relations in REG, a short history
of the GBA, and the relevance of individual vari-
ation (Section 2). We introduce our new variant
graph-based algorithm, LongestFirst, in Section 3.2 Relations, Graphs and IndividualVariation2.1 Relations in REG
In the knowledge representation underlying most
work in REG, each object in a scene is modeled as
a set of attribute-value pairs describing the object?s
properties, such as hsize, largei. Such a represen-
tation is used in the two of the classic algorithms,
the Greedy Algorithm (Dale, 1989) and the Incre-
mental Algorithm (IA) (Dale and Reiter, 1995).
Neither of these was originally intended to process
relations between objects.
Several attempts have been made to adapt the
traditional REG algorithms to include relations be-
tween objects in their output, but all of them suf-
fer from problems with the knowledge representa-
tion not being suited to relations. Dale and Had-
dock (1991) use a constraint network and a recur-
sive loop to extend the Greedy Algorithm, which
uses the discriminatory power of an attribute as
the main selection criterion. They treat relations
the same as other attributes; but in most cases a
certain spatial relation to a particular other ob-
ject is fully distinguishing, which easily leads to
strange chains of relations in the output omitting
most other attributes (Viethen and Dale, 2006).
Krahmer and Theune (2002) suggest a simi-
lar adjustment for the IA by introducing a re-
cursive loop if a relation to another object is in-
troduced to the referring expression under con-
struction. They treat relations as fundamentally
different from other attributes in order to recog-
nize when to enter the recursive loop, however,
they fail to address the problem of infinite regress,
whereby the objects in a domain might be de-
scribed in a circular manner by the relations hold-
ing between them. Another relational extension to
the IA has been proposed by Kelleher and Kruijff
(2006), treating relations as a completely different
class from other attributes. Both extensions of the
IA make the simplifying assumption that relations
should only be considered if it is not possible to
fully distinguish the target referent from the sur-
rounding objects in any other way, with the idea
that it takes less effort to consider and describe
only one object (Krahmer and Theune, 2002; Vie-
then and Dale, 2008).2.2 A Short History of the GBA
A new approach to REG was proposed by Krah-
mer et al (2003). In this approach, a scene is
represented as a labeled directed graph (see Fig-
ure 1(b)), and content selection is a subgraph con-
struction problem. Assuming a scene graph G =
hVG, EGi, where vertices VG represent objects and
edges EG represent the properties and relations of
these objects with associated costs, their algorithm
returns the cheapest distinguishing subgraph that
uniquely refers to the target object v 2 VG. Re-
lations between objects (i.e., edges between dif-
ferent vertices) are a natural part of this repre-
sentation, without requiring special computational
mechanisms. In addition to cost functions, the
GBA requires a preference ordering (PO) over the
edges to arbitrate between equally cheap descrip-
tions (Viethen et al, 2008).
73
(a) Scene 7 from the GRE3D3 Corpus.
b
e
l
o
w
a
b
o
v
e
left-of
right-of
yellow
small
ball
large
small
ball
red
cube
yellow
right-
hand
l
e
f
t
-
o
f
r
i
g
h
t
-
o
f
right-
hand
right-
hand
(b) A graph representing the scene to the left.
Figure 1: An example scene from the GRE3D3 Corpus and the corresponding domain graph.
As the cost functions and preference orders are
specified over edges (i.e., properties), they allow
much more fine-grained control over which prop-
erties to generate for a target referent than the
attribute-based preference orders employed by the
IA and its descendants. The cost functions can be
used to give preference to a commonly used size
value, such as large, over a rarely used color value,
such as mauve, although in general color is de-
scribed more often than size. This process is aided
by a branch-and-bound search that guarantees to
find the cheapest (i.e., ?best?) referring expression.
Since its inception, the GBA has been shown to
be useful for several referential phenomena. Krah-
mer and van der Sluis (2003) combined verbal
descriptions with pointing gestures by modelling
each such gesture as additional looping edges on
all objects that it might be aimed at. While the au-
thors confirmed the ideas implemented in the al-
gorithm in psycholinguistic studies (van der Sluis,
2005), they never assessed its output in an actual
domain.
van Deemter and Krahmer (2007) demonstrated
how the GBA could be used to generate reference
to sets as well as to negated and gradable prop-
erties by representing implicit information as ex-
plicit edges in domain graphs. They also presented
a simple way to account for discourse salience
based on restricting the distractor set. Its ability
to cover such a breadth of referential phenomena
makes the GBA a reasonably robust algorithm for
further exploring the generation of human-like ref-
erence.
The GBA was systematically tested against
human-produced referring expressions for the first
time in the ASGRE Challenge 2007 (Belz and
Gatt, 2007). This entry is described in detail in
(Viethen et al, 2008) and was very successful as
well in the following 2008 and 2009 REG Chal-
lenges (Gatt et al, 2008; Gatt et al, 2009) with
a free-na??ve cost function. This cost function as-
signs 0 cost to the most common attributes, 2 to
the rarest, and 1 to all others. By making the most
common attributes free, it became possible to in-
clude these attributes redundantly in a referring
expression, even if they were not strictly neces-
sary for identifying the target. The cost functions
used in the challenges were attribute-based, and
did therefore not make use of the refined control
capabilities of the GBA.
Theune et al (2011) used k-means clustering
on the property frequencies in order to provide
a more systematic method to transfer the FREE-
NAI?VE cost function to new domains. They found
that using only two clusters (a high frequency and
a low frequency group with associated costs of 0
and 1) achieves the best results, with no significant
differences to the FREE-NAI?VE cost function on
the TUNACorpus. Subsequently they showed that
on this corpus, a training set of only 20 descrip-
tions suffices to determine a 2-means cost function
that performs as well as one based on 165 descrip-
tions. In (Koolen et al, 2012), the same authors
extended these experiments to a Dutch version of
the TUNA Corpus (Koolen and Krahmer, 2010)
and came to a similar conclusion. Neither of the
corpora used in these experiments included rela-
tions between objects.2.3 Individual Variation in REG
A number of authors have argued that to be able to
produce human-like referring expressions, an al-
gorithm must account for speaker variation: Dif-
ferent speakers will refer to the same object in
different ways, and modeling this variation can
bring us closer to generating the rich variety of ex-
74
pressions that people produce. Several approaches
have been made in this direction.
Although this was not explicitly discussed in
(Jordan and Walker, 2005), the machine-learned
models presented there performed significantly
better at replicating human-produced referring
expressions when a feature set was used that
included information about the identity of the
speaker. In (Viethen and Dale, 2010), the impact
of speaker identity as a machine-learning feature
is more systematically tested. They show that ex-
act knowledge about which speaker produced a
referring expression boosts performance, but also
find many commonalities between different speak-
ers? strategies for content selection. Mitchell et
al. (2011b) used participant identity in a machine
learner to successfully predict the kind of size
modifier to be used in a referring expression. Ad-
ditionally, various submissions to the REG chal-
lenges, particularly by Bohnet and Fabbrizio et al
(Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al,
2008a; Di Fabbrizio et al, 2008b) used speaker-
specific POs to increase performance in their adap-
tations of the IA.
All of these systems used the exact speaker
identity as input, although many of the authors
noted that groups of speakers behave similarly
(Viethen and Dale, 2010; Mitchell et al, 2011b).
We build off of this idea by clustering similar
speakers together before learning parameters, and
then generate for speaker-specific clusters. This
method results in a significant improvement in per-
formance.3 LongestFirst: a New Search Strategy
The GBA guarantees to return the cheapest pos-
sible subgraph that fully distinguishes the target.
However, many distinguishing subgraphs can have
the same cost, for example, if a target can be iden-
tified either by its color or by its size, and color
and size have the same cost. Viethen et al (2008)
discuss some examples in more detail.
In the case that more than one cheapest sub-
graph exists, the original GBA will generate the
first it encountered. Due to its branch-and-bound
search strategy, this is also the smallest subgraph,
corresponding to the shortest possible description
that can be found at the cheapest cost. Because
its pruning mechanism does not allow further ex-
pansion of a graph once it is distinguishing, the
number of attributes that the algorithm can include
redundantly is limited, in particular if relations
are involved. Attributes of visually salient nearby
landmark objects that are introduced to the refer-
ring subgraph by a relation are only considered af-
ter all other attributes of the target object. This is
the case even if these attributes are free and feature
early in the preference order.
The GBA is therefore not able to replicate many
overspecified descriptions that human speakers
may use: if a subgraph containing a relation is
already distinguishing before the attributes of a
landmark object are considered, the algorithm will
not include any information about the landmark.
Not only is it unlikely that a landmark object
should be included in a description without any
further information about it, it also seems intu-
itive that speakers with a preference for certain
attributes (such as color) would include these at-
tributes not only for the target referent, but for a
landmark object as well.
We solve this problem by amending the search
algorithm in a way that finds the longest of all
the cheapest subgraphs, and call the resulting al-
gorithm LongestFirst. This search strategy results
in a much larger number of subgraphs to check, in
particular, when used with cost functions that in-
volve a lot of free edges. In order to keep our sys-
tems tractable, we therefore limit the number of
attributes the LongestFirst algorithm can include
to four, based on the finding from (Mitchell et al,
2011a) that people rarely include more than four
modifiers in a noun phrase. In Experiment 2 we
additionally test a setting in which the maximum
number of attributes is determined on the basis of
the average description length in the training data.4 Implementation Note
The original implementation of the GBA did not
provide a method to specify the order in which
edges were tried, although the edge order deter-
mines the order in which distinguishing subgraphs
are found by the algorithm (Krahmer et al, 2003).
This was fixed in (Viethen et al, 2008) by adding
a PO as parameter to the GBA to arbitrate between
equally cheap solutions.
A further issue arose in this implementation
when tested on the GRE3D3 domain, because
there was no simple way to specify which object
each property belonged to; for the TUNA domain
where the GBA has traditionally been evaluated, it
is safe to always assume a property belongs to the
75
target referent. We have therefore provided addi-
tional functionality to the GBA that requires that
not only hattribute, valuei pairs are specified, but
hentity1, attribute, value, entity2i tuples, which
can be translated directly into graph edges. For ex-
ample the tuple htg:relation:above:lmi represents
the edge labelled above between the yellow ball
and the red cube in Figure 1. For direct attributes,
such as size or color, entity1 and entity2 in these
tuples are identical, resulting in loop edges. This
Java implementation of the GBA and the Python
implementation of the LongestFirst algorithm are
available at www.m-mitchell.com/code.5 Experiment 1: Relational Descriptions
In our first experiment, we evaluate how well the
GBA produces human-like reference in a corpus
that uses spatial relations. We compare against the
LongestFirst variant that encourages overspecifi-
cation.5.1 Material
To evaluate the different systems, we use the
GRE3D3 Corpus. It consists of 630 distinguish-
ing descriptions for objects in simple 3D scenes.
Each of the 20 scenes contains three objects in
different spatial relations relative to one another
(see Figure 1). The target referent, marked by an
arrow, was always in a direct adjacency relation
(on   top   of or in   front   of) to one of the
other two objects, while the third object was al-
ways placed at a small distance to the left or right.
The objects are either spheres or cubes and differ
in size and color. In addition to these attributes, the
63 human participants who contributed to the cor-
pus used the objects? location as well as the spatial
relation between the target referent and the closest
landmark object. Each participant described one
of two sets of 10 scenes. The scenes in the two sets
are not identical, but equivalent, so the sets can be
conflated for most analyses. Spatial relations were
used in 36.6% (232) of the descriptions, although
they were never necessary to distinguish the target
object. Further details about the corpus may be
found in (Viethen and Dale, 2008).5.2 Approaches to Parameter Settings
As discussed above, the GBA behaves differently
depending on the PO and the cost functions over
its edges. To find the best match with human
data, we explore several different approaches to
setting these two parameters. An important dis-
tinction between the approaches we try hinges
on the difference between attributes and proper-ties. Attributes correspond to, e.g., color, size, orlocation, while properties are attribute-value pairs,
e.g., hcolor, redi, hsize, largei, hlocation,middlei.
Previous evaluations of the GBA typically used
parameter settings based on either attribute fre-
quency (Viethen et al, 2008) or property fre-
quency (Koolen et al, 2012). We compare both
methods for setting the parameters. Because the
scenes on which the corpus is based were not bal-
anced for the different attribute-values, the fre-
quency of a property is calculated as the pro-
portion of descriptions in which it was used for
those scenes where the target actually possessed
this property. For our evaluation, the trainable
costs and the POs are determined using cross-
validation (see Section 5.3). We use the following
approaches:
0-COST-PROP: All edges have 0 cost, and the
PO is based on property frequency. Each property
is included (regardless of how distinguishing it is)
until a distinguishing subgraph is found.
0-COST-ATT: As 0-COST-PROP, but the PO is
based on attribute frequency.
FREE-NAI?VE-PROP: Properties that occur in
more than 75% of descriptions where they could
be used cost 0, properties with a frequency below
20% cost 2, and all others cost 1 (Viethen et al,
2008). The PO is based on property frequency.
FREE-NAI?VE-ATT: As FREE-NAI?VE-PROP:, but
costs and PO are based on attribute frequency.
K-PROP: Costs are assigned using k-means clus-
tering over property frequencies with k=2 (Theune
et al, 2011). The PO is based on property fre-
quency.
K-ATT: As K-PROP, but the k-means clustering
and the PO are based on attribute frequency.5.3 Evaluation Setup
We evaluate the version of the GBA used by Vie-
then et al (2008), with additional handling for
relations between entities (see Section 4). We
compare against our LongestFirst algorithm from
Section 3 on all approaches described in Sec-
tion 5.2. As baselines, we compare against the
Incremental Algorithm (Dale and Reiter, 1995)
and a simple informed approach that includes at-
tributes/properties seen in more than 50% of the
76
training descriptions. We do not use the IA?s re-
lational extensions (Krahmer and Theune, 2002;
Kelleher and Kruijff, 2006), because these would
deliver the same relation-free output as the basic
IA (relations are never necessary for identifying
the target in GRE3D3). These two baselines are
tried with an attribute-based PO and a property-
based one. We do not expect a difference between
the attribute- and the property-based PO on the IA,
as this difference would only come to the fore in a
situation where a choice has to be made between
two values of the same attribute. In the IA?s anal-
ysis of the GRE3D3 domain, this can only happen
with relations, which it will not use in this domain.
We use Accuracy and Dice, the two most com-
mon metrics for human-likeness in REG (Gatt and
Belz, 2008; Gatt et al, 2009), to assess our sys-
tems. Accuracy reports the relative frequency with
which the generated attribute set and the human-
produced attribute set match exactly. Dice mea-
sures the overlap between the two attribute sets.
For details, see, for example, Krahmer and van
Deemter?s (2012) survey paper. We train and test
our systems using 10-fold cross-validation.5.4 Results
The original version of the Graph-Based Algo-
rithm shows identical performance for all ap-
proaches (See Table 1). All use a preference order
starting with type, followed by color and size, and
a cost function that favors the same attributes. As
these attributes always suffice to distinguish the in-
tended referent, the algorithm stops before spatial
relations are considered. For the scene in Figure 1
it includes the minimal content htg:type:balli, but
for a number of scenes it overspecifies the descrip-
tion.
The LongestFirst/0-COST systems and the
LongestFirst/K-PROP system are the only sys-
tems that include relations in their output.
The LongestFirst/0-COST systems both in-
clude a relation in every description; however,
not always the one that was included in the
human-produced reference, resulting in 521
false-positives for the attribute-based version
and 398 for the property-based one. For the
scene in Figure 1 they include htg:color:yellow,tg:size:small, tg:type:ball, tg:right of:obj3i and
htg:color:yellow, tg:size:small, tg:type:ball,tg:on top of:lmi, respectively. The first
one of these two attribute sets (produced by
Original Longest
GBA First
0-COST- Acc 39.21 0.16
PROP Dice 73.40 68.75
0-COST- Acc 39.21 0.00
ATT Dice 73.40 64.34
FREE-NAI?VE Acc 39.21 46.51
-ATT Dice 73.40 77.91
FREE-NAI?VE Acc 39.21 38.10
-PROP Dice 73.40 74.99
K-PROP Acc 39.21 35.08Dice 73.40 74.66
K-ATT Acc 39.21 35.08Dice 73.40 74.56
50%-Base IA
prop- Acc 27.30 37.14
based PO Dice 72.17 72.21
att- Acc 24.92 37.14
based PO Dice 71.16 72.21
Table 1: Experiment 1: System performance in %.
We used  2 on Accuracy and paired t-tests on Dice
to check for statistical significance. The best per-
formance is highlighted in boldface. It is statisti-
cally significantly different from all other systems
(Acc: p < 0.02, Dice: p < 0.0001).
LongestFirst/0-COST-ATT) includes the rela-
tion between the target and the third object
to the right, which was almost never included
in the human-produced references, leading to
many false-positives. The LongestFirst/K-PROP
system results in only 45 true-positives and
81 false-positives. It includes the attribute set
htg:color:yellow, tg:type:balli for Figure 1.
One of its relational descriptions (for Scene 5)
contains the set htg:size:small, tg:color:blue,tg:on top of:lmi.
The 50%-baseline system outperforms the
LongestFirst/0-COST systems, which illustrates
the utility of cost functions in combination with
a PO. It includes the attribute set htg:color:yellow,tg:type:balli for the scene in Figure 1. The best
performing system is the LongestFirst algorithm
with the attribute-based FREE-NAI?VE approach,
although this system produces no spatial relations.6 Experiment 2: Individual Variation
We now extend our methods to take into account
individual variation in the content selection for
referring expressions, and evaluate whether we
have better success at reproducing participants? re-
lational descriptions. Rather than using speaker
identity as an input parameter to the system (Sec-
tion 2.3), we automatically find groups of people
77
who behave similarly to each other, but signifi-
cantly different to speakers in the other groups.6.1 Evaluation Setup
We use k-means clustering to group the speak-
ers in the GRE3D3 Corpus based on the number
of times they used each attribute and the average
length of their descriptions. We tried values be-
tween 2 and 5 for k, but found that any value
above 2 resulted in two very large clusters accom-
panied by a number of extremely small clusters.
As these small clusters would not be suitable for
x-fold cross-validation, we proceed with two clus-
ters, one consisting of speakers preferring rela-
tively long descriptions that often contain spatial
relations (Cluster CL0, 16 speakers, 160 descrip-
tions), and one consisting of speakers preferring
short, non-relational descriptions (Cluster CL1, 47
speakers, 470 descriptions).
We train cost functions and POs separately for
the two clusters in order to capture the different
behaviour patterns they are based on. We use the
FREE-NAI?VE cost functions for this experiment,
which outperformed all others in Experiment 1.
We again use 10-fold cross-validation for the eval-
uation. In this experiment, we vary the maximum
length setting for the LongestFirst algorithm. In
Experiment 1, the maximum length for a referring
expression was set to 4 based on previous empiri-
cal findings. Here we additionally test setting it to
the rounded average length for each training fold.
On Cluster CL0 this average length is 6 in all folds,
on Cluster CL1 it is 3.6.2 Results
As shown in Table 2, the LongestFirst algorithm
performs best at generating human-like spatial re-
lations (Cluster CL0), with property-based param-
eters and a maximum description length deter-
mined by the training set. It produces the attribute
set hlm:type:cube, tg:on top of:lm, tg:type:ball,tgcolouryellow, lm:colour:redi for Figure 1. The
difference to the other systems is statistically sig-
nificant for both Accuracy ( 2>15, p<0.0001)
and Dice (t>13, p<0.0001). The attribute-based
parameters and the original GBA perform very
badly on this cluster. For participants who do
not tend to use spatial relations (Cluster CL1),
the maximum length setting has no influence,
but attribute-based parameters perform better than
property-based ones. The attribute-based Longest-
First systems also outperform the original GBA
CL0 CL1 avg
FN Acc 19.38 48.94 41.43
LongestFirst -PROP Dice 75.61 80.27 79.08
-max-av FN Acc 0.00 60.00 44.76
-ATT Dice 55.74 85.28 77.78
FN Acc 0.63 48.94 36.67
LongestFirst -PROP Dice 72.15 80.21 78.17
-max4 FN Acc 0.00 60.00 44.76
-ATT Dice 59.01 85.28 78.61
FN Acc 5.00 48.30 37.30
Original -PROP Dice 49.36 80.77 72.79
GBA FN Acc 5.00 50.85 39.21
-ATT Dice 49.36 81.58 73.40
Table 2: Experiment 2: Performance in % of the
LongestFirst and OriginalGraph algorithms on the
two speaker clusters and overall using the FREE-
NAI?VE (FN) approaches. We used  2 on Accu-
racy and paired t-tests on Dice to check for statis-
tical significance. The best performance in each
column and those that are statistically not signifi-
cantly different are highlighted in boldface.
on CL1, but interestingly none of the differences
are as large as on CL0. For the scene in Fig-
ure 1 they produce the attribute set htg:type:ball,tg:colour:yellowi.
The average results over both clusters (shown
in the last column Table 2) are not conclusive
as to which setting should be used overall, al-
though it is clear that the LongestFirst version is
preferable when evaluated by Dice. The differ-
ent result patterns on the two clusters suggest that
the different referential behaviour of the partici-
pants in the two clusters are ideally modeled us-
ing different parameters. In particular, it appears
that property-based costs are useful for replicat-
ing descriptions containing relations to other ob-
jects, while attribute-based costs are useful for
replicating shorter descriptions. The best over-
all performance, achieved by combining the best
performing systems on each cluster (LongestFirst-
max-av/FN-PROP on CL0 and LongestFirst/FN-
ATT with either maximum length setting on CL1),
lies at 49.68% Accuracy and 82.83% Dice. The
Dice score in this combined model is significantly
higher than the best achieved by LongestFirst-
max-av/FN-PROP and from the best Dice score
achieved on the unclustered data in Experiment 1
(t=8.2, p<0.0001). The difference in Accuracy is
not significant ( 2=1.2, p> 0.2).
To get an idea of how successful the new
LongestFirst approach is at replicating the use of
relations on the clustered data, we take a closer
look at the output of the best-performing systems
78
on the two clusters. On CL0, the cluster of partic-
ipants who produce longer descriptions contain-
ing more spatial relations, the best match to the
human data comes from LongestFirst-max-av/FN-
PROP. 147 of the 160 descriptions in this cluster
contain a relation, and the system includes the cor-
rect relation for all 147. It falsely also includes a
relation for the remaining 13 descriptions. This
shows that with the appropriate parameter settings
the LongestFirst algorithm is able to replicate hu-
man relational reference behaviour, but personal
speaker preferences are the main driving factor for
the human use of relations.
CL1, the cluster with shorter descriptions,
contains only 85 (18%) relational descriptions.
The best performing system on this cluster
(LongestFirst/FN-ATT) does not produce any rela-
tions. This is not surprising as the cost functions
and POs for this cluster are necessarily dominated
by the non-relational attributes used more regu-
larly. The cases in which relations are used stem
from participants who do not show a clear prefer-
ence for or against relations and would therefore
be hard to model in any system. With more data it
might be possible to group these participants into
a third cluster and find suitable parameter settings
for them. This would only be possible if their use
of relations is influenced by other factors available
to the algorithm, such as the spatial configuration
of the scene. Viethen and Dale?s (2008) analysis of
the GRE3D3 Corpus suggests that this is the case
at least to some extent.7 Conclusions and Future Work
We have evaluated the Graph-Based Algorithm for
REG (Krahmer et al, 2003) as well as a novel
search algorithm, LongestFirst, that functions on
the same graph-based representation, to assess
their ability to generate referring expressions that
contain spatial relations. We coupled the search
algorithms with a number of different approaches
to setting the cost functions and preference orders
that guide the search.
In Experiment 1, we found that ignoring the cost
function (our 0-cost approaches) is not helpful; but
the LongestFirst algorithm, which produces longer
descriptions, leads to more human-like output for
the visuospatial domain we evaluate on than the
original Graph-Based Algorithm or the Incremen-
tal Algorithm (Dale and Reiter, 1995). However,
in order for spatial relations to be included in a
human-like way, it was necessary to take into ac-
count speaker preferences. We modeled these in
Experiment 2 by clustering the participants who
had contributed to the evaluation corpus based on
their referential behaviour. By training separate
cost functions and preference orders for the dif-
ferent clusters, we enabled the LongestFirst al-
gorithm to correctly reproduce 100% of relations
used by people who regularly mentioned relations.
Our findings suggest that the graph-based rep-
resentation proposed by Krahmer et al (2003)
can be used to successfully generate relational de-
scriptions, however their original search algorithm
needs to be amended to allow more overspecifica-
tion. Furthermore, we have shown that variation
in the referential behaviour of individual speak-
ers has to be taken into account in order to suc-
cessfully model the use of relations in referring
expressions. We have proposed a clustering ap-
proach to advance this goal based directly on the
referring behaviour of speakers rather than speaker
identity. We have found that the best models use
fine-grained property-based parameters for speak-
ers who tend to use spatial relations, and coarser
attribute-based parameters for speakers who tend
to use shorter descriptions.
In future work, we hope to expand to more
complex domains, beyond the simple properties
available in the GRE3D3 Corpus. We also aim
to explore further graph-based representations and
search strategies, modeling non-spatial properties
as separate vertices, similar to the approach by
Croitoru and van Deemter (2007).8 Acknowledgements
Viethen and Krahmer received financial support
from The Netherlands Organization for Scientific
Research, (NWO, Vici grant 277-70-007), and
Mitchell received financial support from the Scot-
tish Informatics and Computer Science Alliance
(SICSA), which is gratefully acknowledged.References
Anja Arts, AlfonsMaes, Leonard Noordman, and Carel
Jansen. 2011. Overspecification in written instruc-
tion. Linguistics, 49(3):555?574.
Anja Belz and Albert Gatt. 2007. The Attribute Se-
lection for GRE Challenge: Overview and evalu-
ation results. In Proceedings of the Workshop onUsing Corpora for NLG: Language Generation and
79
Machine Translation (UCNLG+MT), pages 75?83,
Copenhagen, Denmark.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 207?210, Salt Fork OH, USA.
Bernd Bohnet. 2009. Generation of referring expres-
sion with an individual imprint. In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, pages 185?186, Athens, Greece.
Madalina Croitoru and Kees van Deemter. 2007. A
conceptual graph approach to the generation of re-
ferring expressions. In Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence, pages 2456?2461, Hyderabad, India.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. InProceedings of the 5th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 161?166, Berlin, Germany.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-sociation for Computational Linguistics, pages 68?
75, Vancouver BC, Canada.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008a. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 211?214, Salt Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008b. Referring expression generation
using speaker-based attribute selection and train-
able realization. In Twelfth Conference on Com-putational Natural Language Learning, Manchester,
UK.
Paul E. Engelhardt, Karl D. Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
gricean maxim of quantity? Journal of Memory andLanguage, 54:554?573.
Albert Gatt and Anja Belz. 2008. Attribute selection
for referring expression generation: New algorithms
and evaluation methods. In Proceedings of the5th International Conference on Natural LanguageGeneration, pages 50?58, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation
results. In Proceedings of the 5th InternationalConference on Natural Language Generation, pages
198?206, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation, pages
174?182, Athens, Greece.
Pamela W. Jordan and Marilyn Walker. 2005. Learn-
ing content selection rules for generating object de-
scriptions in dialogue. Journal of Artificial Intelli-gence Research, 24:157?194.
John Kelleher and Geert-Jan Kruijff. 2006. Incre-
mental generation of spatial referring expressions in
situated dialog. In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 1041?1048, Syd-
ney, Australia.
Ruud Koolen and Emiel Krahmer. 2010. The D-
TUNA Corpus: A dutch dataset for the evaluation
of referring expression generation algorithms. InProceedings of the 7th International Conference onLanguage Resources and Evaluation, Valetta, Malta.
Ruud Koolen, Emiel Krahmer, and Marie?t Theune.
2012. Learning preferences for referring expression
generation: Effects of domain, language and algo-
rithm. In Proceedings of the 7th International Nat-ural Language Generation Conference, pages 3?11,
Starved Rock, IL, USA.
Emiel Krahmer and Marie?t Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing: Reference and Pre-supposition in Language Generation and Interpre-tation, pages 223?264. CSLI Publications, Stanford
CA, USA.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Emiel Krahmer and Ielka van der Sluis. 2003. A new
model for generating multimodal referring expres-
sions. In Proceedings of the 9th European Workshopon Natural Language Generation, pages 47?57, Bu-
dapest, Hungary.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011a. Semi-supervised modeling for prenominal
modifier ordering. In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages
236?241, Portland OR, USA.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2011b. Applying machine learning to the
choice of size modifiers. In Proceedings of the 2ndWorkshop on the Production of Referring Expres-sions, Boston MA, USA.
80
Thomas Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
Susan Sonnenschein. 1985. The development of ref-
erential communication skills: Some situations in
which speakers give redundant messages. Journalof Psycholinguistic Research, 14(5):489?508.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter - how
much data is required to train a REG algorithm? InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 660?664, Portland OR,
USA.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring ex-
pressions. In Harry C. Bunt and Reinhard Muskens,
editors, Computing Meaning, volume 3, pages 397?
422. Kluwer, Dordrecht, The Netherlands.
Kees van Deemter, Albert Gatt, Ielka van der Sluis,
and Richard Power. 2012. Generation of referring
expressions: Assessing the incremental algorithm.Cognitive Science, 36(5):799?836.
Ielka van der Sluis. 2005. Multimodal Reference, Stud-ies in Automatic Generation of Multimodal Refer-ring Expressions. Ph.D. thesis, Tilburg University,
The Netherlands.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the 4th InternationalConference on Natural Language Generation, pages
63?70, Sydney, Australia.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-ceedings of the 5th International Conference on Nat-ural Language Generation, pages 59?67, Salt Fork
OH, USA.
Jette Viethen and Robert Dale. 2009. Referring ex-
pression generation: What can we learn from human
data? In Proceedings of the 2009 Workshop on Pro-duction of Referring Expressions: Bridging the GapBetween Computational and Empirical Approachesto Reference, Amsterdam, The Netherlands.
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for refer-
ring expression generation. In Proceedings of the8th Australasian Language Technology Workshop,
pages 81?89, Melbourne, Australia.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedingsof the 6th International Conference on LanguageResources and Evaluation, Marrakech, Morocco.
81
Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 16?24,
Philadelphia, Pennsylvania, 19 June 2014. c 2014 Association for Computational Linguistics
Crowdsourcing Language Generation Templates for Dialogue Systems
Margaret Mitchell
Microsoft Research
Redmond, WA USAmemitc@microsoft.com Dan BohusMicrosoft ResearchRedmond, WA USAdbohus@microsoft.com Ece KamarMicrosoft ResearchRedmond, WA USAeckamar@microsoft.com
Abstract
We explore the use of crowdsourcing to
generate natural language in spoken dia-
logue systems. We introduce a method-
ology to elicit novel templates from the
crowd based on a dialogue seed corpus,
and investigate the effect that the amount
of surrounding dialogue context has on the
generation task. Evaluation is performed
both with a crowd and with a system de-
veloper to assess the naturalness and suit-
ability of the elicited phrases. Results indi-
cate that the crowd is able to provide rea-
sonable and diverse templates within this
methodology. More work is necessary be-
fore elicited templates can be automati-
cally plugged into the system.
1 Introduction
A common approach for natural language gener-
ation in task-oriented spoken dialogue systems is
template-based generation: a set of templates is
manually constructed by system developers, and
instantiated with slot values at runtime. When
the set of templates is limited, frequent interac-
tions with the system can quickly become repet-
itive, and the naturalness of the interaction is lost.
In this work, we propose and investigate a
methodology for developing a corpus of natural
language generation templates for a spoken dia-
logue system via crowdsourcing. We use an ex-
isting dialogue system that generates utterances
from templates, and explore how well a crowd
can generate reliable paraphrases given snippets
from the system?s original dialogues. By utiliz-
ing dialogue data collected from interactions with
an existing system, we can begin to learn differ-
ent ways to converse while controlling the crowd
to stay within the scope of the original system.
The proposed approach aims to leverage the sys-
tem?s existing capabilities together with the power
of the crowd to expand the system?s natural lan-
guage repertoire and create richer interactions.
Our methodology begins with an existing cor-
pus of dialogues, extracted from a spoken dia-
logue system that gives directions in a building.
Further details on this system are given in ?4.1.
The extracted dialogue corpus contains phrases
the system has generated, and crowd-workers con-
struct alternates for these phrases, which can be
plugged back into the system as crowd templates.
We investigate via crowdsourcing the effect of the
amount of surrounding context provided to work-
ers on the perceived meaning, naturalness, and di-
versity of the alternates they produce, and study
the acceptability of these alternates from a sys-
tem developer viewpoint. Our results indicate that
the crowd provides reasonable and diverse tem-
plates with this methodology. The developer eval-
uation suggests that additional work is necessary
before we can automatically plug crowdsourced
templates directly into the system.
We begin by discussing related work in ?2. In?3, we detail the proposed methodology. In ?4, we
describe the experimental setup and results. Di-
rections for future work are discussed in ?5.
2 Related Work
Online crowdsourcing has gained popularity in
recent years because it provides easy and cheap
programmatic access to human intelligence. Re-
searchers have proposed using crowdsourcing
for a diverse set of natural language process-
ing tasks, including paired data collection for
training machine translation systems (Zaidan and
Callison-Burch, 2011), evaluation of NLP systems
(Callison-Burch and Dredze, 2010) and speech
transcriptions (Parent and Eskenazi, 2010). A
popular task targeting language diversity is para-
phrase generation, which aims at collecting di-
verse phrases while preserving the original mean-
ing. Crowdsourcing paraphrase generation has
16
been studied for the purposes of plagiarism detec-
tion (Burrows and Stein, 2013), machine transla-
tion (Buzek et al., 2010), and expanding language
models used in mobile applications (Han and Ju,
2013). Automated and crowd-based methods have
been proposed for evaluating paraphrases gener-
ated by the crowd (Denkowski and Lavie, 2010;
Tschirsich and Hintz, 2013). Researchers have
proposed workflows to increase the diversity of
language collected with crowd-based paraphrase
generation (Negri et al., 2012) and for reducing
the language bias in generation by initiating gen-
eration with visual input (Chen and Dolan, 2011).
While paraphrase generation typically aims to pre-
serve the meaning of a phrase without considering
its use beyond the sentence level, we focus on col-
lecting diverse language to be used directly in a
dialogue system in a way that agrees with the full
dialogue context.
Manually authoring dialogue systems has been
identified as a challenging and time-consuming
task (Ward and Pellom, 1999), motivating re-
searchers to explore opportunities to use the crowd
to improve and evaluate dialogue systems. Wang
et al. (2012) proposed methods to acquire corpora
for NLP systems using semantic forms as seeds,
and for analyzing the quality of the collected cor-
pora. Liu et al. (2010) used crowdsourcing for
free-form language generation and for semantic
labeling, with the goal of generating language cor-
pora for new domains. Crowd-workers contribute
to dialogue generation in real-time in the Chorus
system by providing input about what the system
should say next (Lasecki et al., 2013). Crowd-
sourcing has also been used with some success for
dialogue system evaluation (Jurc???c?ek et al., 2011).
Previous work on increasing language diversity
in dialogue systems with crowdsourcing has fo-
cused on learning about diversity in user input
to improve components such as speech recogni-
tion and language understanding (e.g., Wang et al.
(2012)). Instead, our work focuses on adding di-
versity to system outputs. Mairesse et al. (2010)
followed a similar approach to the work reported
here, using crowdsourcing to collect paraphrases
for a dialogue system in the restaurant domain.
However, the focus of the Mairesse et al. work was
on training an NLG module using this data. Our
work focuses on crowdsourcing techniques to ex-
tract relevant paraphrases, examining the effect of
context on their suitability and generalizability.
3 Methodology
Our methodology for developing natural language
generation templates is illustrated by the pipeline
in Figure 1. This pipeline is designed for di-
alogue systems that use a template-based natu-
ral language generation component. It assumes
that the given system has an initial set of lan-
guage generation templates that have been man-
ually authored, and expands from there. The ini-
tial system is used to collect a corpus of dialogues,
which we will refer to as the dialogue seed cor-
pus, through interactions with users. Based on the
dialogue seed corpus, we automatically construct
a set of generation HITs, web-based crowdsourc-
ing tasks that are used to elicit paraphrases from
crowd-workers for instantiated system templates.
A generation HIT displays one of the system turns
extracted from a system dialogue, with a phrase
highlighted, and different amounts of surround-
ing context in different conditions. The worker is
asked to replace the phrase with another one that
keeps the same meaning and the coherence of the
interaction. If slots are marked in the original, they
must be preserved by the worker, which allows us
to easily convert the elicited paraphrases to crowd
templates. Once a corpus of crowd templates are
collected in this fashion, a system developer may
filter and decide which to add as viable alternatives
to the system?s existing list of language generation
templates (top path in the pipeline from Figure 1).
We also construct a set of evaluation HITs and
post them to the crowd to assess the suitability and
relative naturalness of the crowd templates (bot-
tom path in the pipeline from Figure 1.) We study
how the scores obtained in this crowd-evaluation
may be used to help filter the set of new templates
that are presented as candidates to the system de-
veloper. In the following subsections, we describe
each of the pipeline components in detail.
3.1 Dialogue Seed Corpus
We assume as a starting point an existing dialogue
system that uses a template-based language gener-
ation component. The system uses a set of tem-
plates T , which are instantiated with slots filled to
generate system phrases. A system turn may con-
tain one or more such phrases connected together.
For instance, in the dialogue fragments shown in
Figure 2, the template ?Sorry, that was [Place]
you wanted, right?? generates at runtime ?Sorry,
that was Ernestine Patrick?s office you wanted,
17
Figure 1: Pipeline for crowd-based development of natural language generation templates.
right??. Statistics on the dialogue seed corpus
used in this study are provided in ?4.2.
The proposed methodology does not require
transcriptions of user utterances in the dialogue
seed corpus; instead, it utilizes the recognition re-
sults for each user turn. The primary reason be-
hind this choice is that a dialogue that contains
recognized user turns may be more coherent than
one that contains transcripts and can be generated
automatically, as the dialogue manager generates
system responses based on the recognition results.
However, turn-overtaking issues and recognition
problems sometimes resulted in incoherent dia-
logue interactions. Improving speech recognition
remains an area for future work.
3.2 Generation HITs
We use the dialogue seed corpus to produce gener-
ation HITs to elicit paraphrases for system phrases
from crowd-workers. In the simplest form, a gen-
eration HIT might present a single system phrase
to the worker. We hypothesize that the surround-
ing context may be an important factor in facili-
tating the construction of appropriate paraphrases,
affecting their diversity, naturalness, generaliz-
ability, etc.; we therefore investigate the effect of
presenting varying amounts of dialogue context to
the worker.
Specifically, given a system phrase correspond-
ing to a template t instantiated in a dialogue, we
investigate six different dialogue context condi-
tions. A phrase in a condition presented to a
crowd-worker will be referred to as a seed, p. Ex-
amples of seeds in each condition are illustrated in
Figure 2. In the first condition, denoted Phrase,
a seed is presented to the worker in isolation. In
the second condition, denoted S, the entire sys-
tem turn containing p is presented to the worker,
with p highlighted. In the next 4 conditions, de-
noted suS, suSu, susuS, susuSu, seeds are pre-
sented in increasingly larger contexts including
one or two previous system and user turns (de-
noted with lowercase ?s? and ?u? in the encoding
Figure 2: Generation HIT excerpts in six different
context conditions (w/o instructions, examples).
above), followed by the system turn S that con-
tains the highlighted seed p, followed in two con-
ditions (susuSu and suSu) by another user turn.
Not all context conditions are applicable for each
instantiated template, e.g., conditions that require
previous context, such as suS, cannot be con-
structed for phrases appearing in the first system
turn. We follow a between-subjects design, such
18
that each worker works on only a single condition.
Each generation HIT elicits a paraphrase for a
seed. The HIT additionally contains instructions
and examples of what workers are expected to do
and not to do.1 We instruct workers to read the
dialogue presented and rephrase the highlighted
phrase (seed) so as to preserve the meaning and
the cohesion of the interaction. To identify slots
accurately in the crowd-generated paraphrases, we
mark slot values in the given seed with bold italics
and instruct workers to keep this portion exactly
the same in their paraphrases (see Figure 2). These
paraphrases are then turned into crowd templates
following 3 basic steps: (1) Spelling error cor-
rection; (2) Normalization;2 and (3) Replacing
filled slots in the worker?s paraphrase with the slot
name. We ask workers to provide paraphrases (in
English) that differ from the original phrase more
substantially than by punctuation changes, and im-
plement controls to ensure that workers enter slot
values.
In completing the generation tasks, the crowd
produces a corpus of paraphrases, one paraphrase
for each seed. For example, ?I apologize, are you
looking for Ernestine Patrick?s office??, is a para-
phrase for the highlighted seed shown in Figure 2.
As we have asked the workers not to alter slot val-
ues, crowd templates can easily be recovered, e.g.,
?I apologize, are you looking for [Place]??
3.3 Evaluation HITs
A good crowd template must minimally satisfy
two criteria: (1) It should maintain the meaning
of the original template; and (2) It should sound
natural in any dialogue context where the original
template was used by the dialogue manager, i.e., it
should generalize well, beyond the specifics of the
dialogue from which it was elicited.
To assess crowd template quality, we construct
evaluation HITs for each crowd template. Instan-
tiated versions of the original template and the
crowd template are displayed as options A and
B (with randomized assignment) and highlighted
as part of the entire dialogue in which the origi-
nal template was used (see Figure 3). In this in-
context (IC) evaluation HIT, the worker is asked
whether the instantiated crowd template has the
same meaning as the original, and which is more
natural. In addition, because the original dialogues
1Instructions available at m-mitchell.com/corpora.html.
2We normalize capitalization, and add punctuation identi-
cal to the seed when no punctuation was provided.
Figure 3: Example evaluation HIT excerpt.
were sometimes incoherent (see ?3.1), we also
asked the evaluation workers to judge whether the
given phrases made sense in the given context.
Finally, in order to assess how well the crowd
template generalizes across different dialogues,
we use a second, out-of-context (OOC) eval-
uation HIT. For each crowd template, we ran-
domly selected a new dialogue where the tem-
plate t appeared. The out-of-context evaluation
HIT presents the instantiated original template and
crowd template in this new dialogue. The crowd-
workers thus assess the crowd template in a dia-
logue context different from the one in which it
was collected. We describe the evaluation HITs in
further detail in ?4.
3.4 Developer Filtering
While a crowd-based evaluation can provide in-
sights into the quality of the crowd templates, ul-
timately, whether or not a template is appropriate
for use in the dialogue system depends on many
other factors (e.g., register, style, expectations,
system goals, etc.). The last step in the proposed
methodology is therefore a manual inspection of
the crowd templates by a system developer, who
assesses which are acceptable for use in the sys-
tem without changes.
19
Figure 4: Directions Robot system.
4 Experiments and Results
We now describe our experiments and results. We
aim to discover whether there is an effect of the
amount of surrounding context on perceived crowd
template naturalness. We additionally explore
whether the crowd template retains the meaning
of the original template, whether they both make
sense in the given context, and the diversity of
the templates that the crowd produced for each
template type. We report results when the tem-
plates are instantiated in-context, in the original
dialogue; and out-of-context, in a new dialogue.
We first describe the experimental test-bed and the
corpora used and collected below.
4.1 Experimental Platform
The test-bed for our experiments is Directions
Robot, a situated dialogue system that provides
directions to peoples? offices, conference rooms,
and other locations in our building (Bohus et al.,
2014). The system couples a Nao humanoid
robot with a software infrastructure for multi-
modal, physically situated dialogue (Bohus and
Horvitz, 2009) and has been deployed for several
months in an open space, in front of the elevator
bank on the 3rd floor of our building (see Figure
4). While some of the interactions are need-based,
e.g., visitors coming to the building for meetings,
many are also driven by curiosity about the robot.
The Directions Robot utilizes rule-based natu-
ral language generation, with one component for
giving directions based on computed paths, and
another component with 38 templates for the rest
of the dialogue. Our experimentation focuses on
these 38 templates. As the example shown in Fig-
ure 2 illustrates, slots are dynamically filled in at
run-time, based on the dialogue history.
We conducted our experiments on a general-
Cond.
Crowd Generation Crowd Eval.
# Gen # w Time/ # Uniq. # Eval Time/
HITs HIT Para. HITs HIT
(? 3) (sec) (? 5) (sec)
Phrase 767 26 34.7 1181 1126 29.4
S 860 28 30.8 1330 1260 39.2
suS 541 26 33.3 1019 772 30.5
suSu 265 24 38.8 531 392 32.6
susuS 360 24 41.0 745 572 32.3
susuSu 296 28 42.9 602 440 34.4
Total 3089 - - 5408 4562 -
Average - 26 36.9 - - 33.1
Table 1: Statistics for the crowd-based generation
and evaluation processes. Each generation HIT
was seen by 3 unique workers and each evaluation
HIT was seen by 5 unique workers. #w represents
number of workers. For evaluation, #w = 231.
purpose crowdsourcing marketplace, the Univer-
sal Human Relevance System (UHRS).3 The mar-
ketplace connects human intelligence tasks with a
large population of workers across the globe. It
provides controls for selecting the country of res-
idence and native languages for workers, and for
limiting the maximum number of tasks that can be
done by a single worker.
4.2 Crowd-based Generation
Dialogue seed corpus We used 167 dialogues
collected with the robot over a period of one week
(5 business days) as the dialogue seed corpus. The
number of turns in these dialogues (including sys-
tem and user) ranges from 1 to 41, with a mean of
10 turns. 30 of the 38 templates (79%) appeared
in this corpus.
Generation HITs We used the dialogue seed
corpus to construct generation HITs, as described
in ?3.2. In a pilot study, we found that for every
10 instances of a template submitted to the crowd,
we received approximately 6 unique paraphrases
in return, with slightly different ratios for each of
the six conditions. We used the ratios observed for
each condition in the pilot study to down-sample
the number of instances we created for each tem-
plate seen more than 10 times in the corpus. The
total number of generation HITs resulting for each
condition is shown in Table 1.
Crowd generation process Statistics on crowd
generation are shown in Table 1. Each worker
could complete at most 1/6 of the total HITs for
that condition. We paid 3 cents for each genera-
3This is a Microsoft-internal crowdsourcing platform.
20
tion HIT, and each HIT was completed by 3 unique
workers. From this set, we removed corrupt re-
sponses, and all paraphrases for a generation HIT
where at least one of the 3 workers did not cor-
rectly write the slot values. This yielded a total of
9123 paraphrases, with 5408 unique paraphrases.
4.3 Crowd-based Evaluation
Evaluation HITs To keep the crowd evaluation
tractable, we randomly sampled 25% of the para-
phrases generated for all conditions to produce
evaluation HITs. We excluded paraphrases from
seeds that did not receive paraphrases from all 3
workers or were missing required slots. As dis-
cussed in ?3, paraphrases were converted to crowd
templates, and each crowd template was instanti-
ated in the original dialogue, in-context (IC) and
in a randomly selected out-of-context (OOC) dia-
logue. The OOC templates were instantiated with
slots relevant to the chosen dialogue. This process
yielded 2281 paraphrases, placed into each of the
two contexts.
Crowd evaluation process As discussed in?3.3, instantiated templates (crowd and original)
were displayed as options A and B, with random-
ized assignment (see Figure 3). Workers were
asked to judge whether the original and the crowd
template had the same meaning, and whether they
made sense in the dialogue context. Workers then
rated which was more natural on a 5-point ordi-
nal scale ranging from -2 to 2, where a -2 rating
marked that the original was much more natural
than the crowd template. Statistics on the judg-
ments collected in the evaluation HITs are shown
in Table 1. Workers were paid 7 cents for each
HIT. Each worker could complete at most 5% of
all HITs, and each HIT was completed by 5 unique
workers.
Outlier elimination One challenge with crowd-
sourced evaluations is noise introduced by spam-
mers. While questions with known answers may
be used to detect spammers in objective tasks, the
subjective nature of our evaluation tasks makes
this difficult: a worker who does not agree with the
majority may simply have different opinions about
the paraphrase meaning or naturalness. Instead of
spam detection, we therefore seek to identify and
eliminate outliers; in addition, as previously dis-
cussed, each HIT was performed by 5 workers, in
an effort to increase robustness.
We focused attention on workers who per-
formed at least 20 HITs (151 of 230 workers, cov-
ering 98% of the total number of HITs). Since
we randomized the A/B assignment of instantiated
original templates and crowd templates, we expect
to see a symmetric distribution over the relative
naturalness scores of all judgments produced by a
worker. To identify workers violating this expec-
tation, we computed a score that reflected the sym-
metry of the histogram of the naturalness votes for
each worker. We considered as outliers 6 work-
ers that were more than z=1.96 standard deviations
away from the mean on this metric (corresponding
to a 95% confidence interval). Secondly, we com-
puted a score that reflected the percentage of tasks
where a worker was in a minority, i.e., had the
single opposing vote to the other workers on the
same meaning question. We eliminated 4 work-
ers, who fell in the top 97.5 percentile of this dis-
tribution. We corroborated these analyses with a
visual inspection of scatterplots showing these two
metrics against the number of tasks performed by
each judge.4 As one worker failed on both criteria,
overall, 9 workers (covering 9% of all judgements)
were considered outliers and their responses were
excluded.
4.4 Crowd Evaluation Results
Meaning and Sense Across conditions, we find
that most crowd templates are evaluated as hav-
ing the same meaning as the original and mak-
ing sense by the majority of workers. Evaluation
percentages are shown in Table 2, and are around
90% across the board. This suggests that in most
cases, the generation task yields crowd templates
that meet the goal of preserving the meaning of the
original template.
Naturalness To evaluate whether the amount of
surrounding context has an effect on the perceived
naturalness of a paraphrase relative to the original
phrase, we use a Kruskal-Wallis (KW) test on the
mean scores for each of the paraphrases, setting
our significance level to .05. A Kruskal-Wallis
test is a non-parametric test useful for significance
testing when the independent variable is categor-
ical and the data is not assumed to be normally
distributed. We find that there is an effect of con-
dition on the relative naturalness score (KW chi-
squared = 15.9156, df = 5, p = 0.007) when crowd
4Scatterplots available at m-mitchell.com/corpora.html.
21
Crowd Evaluation Developer Evaluation
Cond. % Same % Makes Avg. Relative Avg. % Dev. Avg.
Meaning Sense Naturalness D-score Accepted D-score
IC OOC IC OOC IC OOC IC OOC All Seen>1
Phrase 92 91 90 90 -.54 (.66) -.50 (.61) .67 .67 37 67 .30
S 91 89 88 88 -.50 (.65) -.47 (.66) .68 .64 35 53 .29
suS 84 87 85 87 -.37 (.65) -.37 (.61) .70 .70 40 63 .41
suSu 88 85 95 88 -.48 (.62) -.43 (.61) .76 .71 38 50 .39
susuS 94 94 91 94 -.43 (.70) -.39 (.67) .81 .80 38 78 .34
susuSu 91 89 92 86 -.40 (.61) -.38 (.66) .73 .74 45 67 .42
Table 2: % same meaning, % makes sense, and average relative naturalness (standard deviation in paren-
theses), measured in-context (IC) and out-of-context (OOC); crowd-based and developer-based diversity
score (D-score); developer acceptance rate computed over all templates, and those seen more than once.
The susuS condition yields the most diverse templates using crowd-based metrics; removing templates
seen once in the evaluation corpus, this condition has the highest acceptance in the developer evaluation.
templates are evaluated in-context, but not out-of-
context (KW chi-squared = 9.4102, df = 5, p-value
= 0.09378). Average relative naturalness scores in
each condition are shown in Table 2.
Diversity We also assess the diversity of the
templates elicited from the crowd, based on the
evaluation set. Specifically, we calculate a diver-
sity score (D-score) for each template type t. We
calculate this score as the number of unique crowd
template types for t voted to make sense and have
the same meaning as the original by the majority,
divided by the total number of seeds for t with
evaluated crowd templates. More formally, let P
be the original template instantiations that have
evaluated crowd templates, M the set of unique
crowd template types voted as having the same
meaning as the original template by the majority
of workers, and S the set of unique crowd tem-
plate types voted as making sense in the dialogue
by the majority of workers. Then:
D-score(t) =
|M \ S||P |
The average diversity scores across all tem-
plates for each condition are shown in Table 2.
We find the templates that yield the most di-
verse crowd templates include WL Retry ?Where
are you trying to get to in this building?? andOK Help, ?Okay, I think I can help you with
that?, which have a diversity rating of 1.0 in sev-
eral conditions: for each template instance we in-
stantiate (i.e., each generation HIT), we get a new,
unique crowd template back. Example crowd tem-
plates for the OK Help category include ?I be-
lieve I can help you find that? and ?I can help
you ok?. The templates with the least diversity are
those for Hi, which has a D-score around 0.2 in
the S and Phrase conditions.
4.5 Developer Acceptability Results
For the set of crowd templates used in the crowd-
based evaluation process, one of the system de-
velopers5 provided binary judgments on whether
each template could be added (without making any
changes) to the system or not. The developer had
access to the original template, extensive knowl-
edge about the system and domain, and the way in
which each of these templates are used.
Results indicate that the developer retained 487
of the 1493 unique crowd templates that were used
in crowd-evaluation (33%). A breakdown of this
acceptance rate by condition is shown in Table 2.
When we eliminate templates seen only once in
the evaluation corpus, acceptability increases, at
the expense of recall. We additionally calculate
a diversity score from those templates accepted
by the developer, which is simply the number of
crowd template types accepted by the developer,
divided by the total number of seeds used to elicit
the crowd templates in the developer?s evaluation,
for each template type t.
The developer evaluation revealed a wide range
of reasons for excluding crowd templates. Some
of the most common were lack of grammatical-
ity, length (some paraphrases were too long/short),
stylistic mismatch with the system, and incorrect
punctuation. Other reasons included register is-
sues, e.g., too casual/presumptive/impolite, issues
of specificity, e.g., template was too general, and
issues of incompatibility with the dialogue state
and turn construction process. Overall, the de-
veloper interview highlighted very specific system
5The developer was not an author of this paper.
22
Figure 5: Precision and recall for heuristics.
and domain knowledge in the selection process.
4.6 Crowd-based Evaluation and Developer
Acceptability
We now turn to an investigation of whether statis-
tics from the crowd-based generation and evalu-
ation processes can be used to automatically fil-
ter crowd templates. Specifically, we look at two
heuristics, with results plotted in Figure 5. These
heuristics are applied across the evaluation cor-
pus, collating data from all conditions. The first
heuristic, Heur1, uses a simple threshold on the
number of times a crowd template occurred in the
evaluation corpus.6 We hypothesize that more fre-
quent paraphrases are more likely to be acceptable
to the developer, and in fact, as we increase the
frequency threshold, precision increases and recall
decreases.
The second heuristic, Heur2, combines the
threshold on counts with additional scores col-
lected in the out-of-context crowd-evaluation: It
only considers templates with an aggregated judg-
ment on the same meaning question greater than
50% (i.e., the majority of the crowd thought the
paraphrase had the same meaning as the origi-
nal), and with an aggregated relative naturalness
score above the overall mean. As Figure 5 illus-
trates, different tradeoffs between precision and
recall can be achieved via these heuristics, and by
varying the count threshold.
These results indicate that developer filtering re-
mains a necessary step for adding new dialogue
system templates, as the filtering process cannot
yet be replaced by the crowd-evaluation. This is
not surprising since the evaluation HITs did not
6Since the evaluation corpus randomly sampled 25% of
the generation HITs output, this is a proxy for the frequency
with which that template was generated by the crowd.
express all the different factors that we found the
developer took into account when selecting tem-
plates, such as style decisions and how phrases are
combined in the system to form a dialogue. Future
work may consider expanding evaluation HITs to
reflect some of these aspects. By using signals ac-
quired through crowd generation and evaluation,
we should be able to reduce the load for the de-
veloper by presenting a smaller and more precise
candidate list at the expense of reductions in recall.
5 Discussion
We proposed and investigated a methodology for
developing a corpus of natural language genera-
tion templates for a spoken dialogue system via
crowdsourcing. We investigated the effect of the
context we provided to the workers on the per-
ceived meaning, naturalness, and diversity of the
alternates obtained, and evaluated the acceptabil-
ity of these alternates from a system developer
viewpoint.
Our results show that the crowd is able to pro-
vide suitable and diverse paraphrases within this
methodology, which can then be converted into
crowd templates. However, more work is nec-
essary before elicited crowd templates can be
plugged directly into a system.
In future work, we hope to continue this pro-
cess and investigate using features from the crowd
and judgments from system developers in a ma-
chine learning paradigm to automatically identify
crowd templates that can be directly added to the
dialogue system. We would also like to extend be-
yond paraphrasing single templates to entire sys-
tem turns. With appropriate controls and feature
weighting, we may be able to further expand dia-
logue capabilities using the combined knowledge
of the crowd. We expect that by eliciting lan-
guage templates from multiple people, as opposed
to a few developers, the approach may help con-
verge towards a more natural distribution of al-
ternative phrasings in a dialogue. Finally, future
work should also investigate the end-to-end effects
of introducing crowd elicited templates on the in-
teractions with the user.
Acknowledgments
Thanks to members of the ASI group, Chit W.
Saw, Jason Williams, and anonymous reviewers
for help and feedback with this research.
23
References
D. Bohus and E. Horvitz. 2009. Dialog in the open
world: Platform and applications. Proceedings of
ICMI?2009.
Dan Bohus, C. W. Saw, and Eric Horvitz. 2014. Di-
rections robot: In-the-wild experiences and lessons
learned. Proceedings of AAMAS?2014.
Martin Potthast Burrows, Steven and Benno Stein.
2013. Paraphrase acquisition via crowdsourcing and
machine learning. ACM Transactions on Intelligent
Systems and Technology (TIST), 43.
Olivia Buzek, Philip Resnik, and Benjamin B. Beder-
son. 2010. Error driven paraphrase annotation using
mechanical turk. Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1.
Michael Denkowski and Alon Lavie. 2010. Exploring
normalization techniques for human judgments of
machine translation adequacy collected using ama-
zon mechanical turk. Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk.
Matthai Philipose Han, Seungyeop and Yun-Cheng Ju.
2013. Nlify: lightweight spoken natural language
interfaces via exhaustive paraphrasing. Proceedings
of the 2013 ACM international joint conference on
Pervasive and ubiquitous computing.
Filip Jurc???c?ek, Simon Keizer, Milica Gas?ic?, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2011. Real user evaluation of spoken dia-
logue systems using amazon mechanical turk. Pro-
ceedings of INTERSPEECH, 11.
Walter S. Lasecki, Rachel Wesley, Jeffrey Nichols,
Anand Kulkarni, James F. Allen, and Jeffrey P.
Bigham. 2013. Chorus: a crowd-powered con-
versational assistant. Proceedings of the 26th an-
nual ACM symposium on User interface software
and technology.
Sean Liu, Stephanie Seneff, and James Glass. 2010.
A collective data generation method for speech lan-
guage models. Spoken Language Technology Work-
shop (SLT), IEEE.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation
using graphical models and active learning. Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012.
Chinese whispers: Cooperative paraphrase acquisi-
tion. Proceedings of LREC.
Gabriel Parent and Maxine Eskenazi. 2010. To-
ward better crowdsourced transcription: Transcrip-
tion of a year of the let?s go bus information sys-
tem data. Spoken Language Technology Workshop
(SLT), IEEE.
Martin Tschirsich and Gerold Hintz. 2013. Leveraging
crowdsourcing for paraphrase recognition. LAW VII
& ID, 205.
William Yang Wang, Dan Bohus, Ece Kamar, and
Eric Horvitz. 2012. Crowdsourcing the acquisi-
tion of natural language corpora: Methods and ob-
servations. Spoken Language Technology Workshop
(SLT), IEEE.
W. Ward and B. Pellom. 1999. The cu communicator
system. Proceedings of IEEE ASRU.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1.
24
