Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430?433,
Prague, June 2007. c?2007 Association for Computational Linguistics
UPV-SI: Word Sense Induction using Self Term Expansion?
David Pinto(1,2) and Paolo Rosso1
1Polytechnic University of Valencia
DSIC, Valencia, Spain, 46022
2B. Autonomous University of Puebla
FCC, Puebla, Mexico, 72570
{dpinto, prosso}@dsic.upv.es
He?ctor Jime?nez-Salazar
Autonomous Metropolitan University
Department of Information Technologies
Cuajimalpa, DF, Mexico, 11850
hgimenezs@gmail.com
Abstract
In this paper we are reporting the re-
sults obtained participating in the ?Eval-
uating Word Sense Induction and Dis-
crimination Systems? task of Semeval
2007. Our totally unsupervised system
performed an automatic self-term expan-
sion process by mean of co-ocurrence
terms and, thereafter, it executed the
unsupervised KStar clustering method.
Two ranking tables with different eval-
uation measures were calculated by the
task organizers, every table with two
baselines and six runs submitted by dif-
ferent teams. We were ranked third
place in both ranking tables obtaining a
better performance than three different
baselines, and outperforming the average
score.
1 Introduction
Word Sense Disambiguation (WSD) is a partic-
ular problem of computational linguistics which
consists in determining the correct sense for a
given ambiguous word. It is well-known that su-
pervised algorithms have obtained the best re-
sults in public evaluations, but their accuracy
is close related with the amount of hand-tagged
data available. The construction of that kind
of training data is difficult for real applications.
The unsupervised WSD overcomes this draw-
back by using clustering algorithms which do
?This work has been partially supported by the MCyT
TIN2006-15265-C06-04 project, as well as by the BUAP-
701 PROMEP/103.5/05/1536 grant
not need training data in order to determine the
possible sense for a given ambiguous word.
This paper describes a simple technique for
unsupervised sense induction for ambiguous
words. The approach is based on a self term ex-
pansion technique which constructs a set of co-
ocurrence terms and, thereafter, it uses this set
to expand the target dataset. The implemented
system was performed in the task ?SemEval-
2007 Task 2: Evaluating Word Sense Induc-
tion and Discrimination Systems?(Agirre and
A., 2007). The aim of the task was to per-
mit a comparison across sense-induction and dis-
crimination systems. Moreover, the comparison
with other supervised and knowledge-based sys-
tems may be also done, since the test corpus was
borrowed from the well known ?English lexical-
sample? task in SemEval-2007, with the usual
training + test split.
The self term expansion method consists in
replacing terms of a document by a set of co-
related terms. The goal is to improve natu-
ral language processing tasks such as cluster-
ing narrow-domain short texts. This process
may be done by mean of different ways, of-
ten just by using a knowledge database. In
information retrieval, for instance, the expan-
sion of query terms is a very investigated topic
which has shown to improve results with respect
to when query expansion is not employed (Qiu
and Frei, 1993; Ruge, 1992; R.Baeza-Yates and
Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsber-
gen, 1979).
The availability of Machine Readable Re-
sources (MRR) like ?Dictionaries?, ?Thesauri?
and ?Lexicons? has allowed to apply term ex-
430
pansion to other fields of natural language pro-
cessing like WSD. In (Banerjee and Pedersen,
2002) we may see the typical example of using
a external knowledge database for determining
the correct sense of a word given in some con-
text. In this approach, every word close to the
one we would like to determine its correct sense
is expanded with its different senses by using
the WordNet lexicon (Fellbaum, 1998). Then,
an overlapping factor is calculated in order to
determine the correct sense of the ambiguous
word. Different other approaches have made use
of a similar procedure. By using dictionaries,
the proposals presented in (Lesk, 1986; Wilks et
al., 1990; Nancy and Ve?ronis, 1990) are the most
sucessful in WSD. Yarowsky (Yarowsky, 1992)
used instead thesauri for their experiments. Fi-
nally, in (Sussna, 1993; Resnik, 1995; Baner-
jee and Pedersen, 2002) the use of lexicons in
WSD has been investigated. Although in some
cases the knowledge resource seems not to be
used strictly for term expansion, the aplication
of co-occurrence terms is included in their algo-
rithms. Like in information retrieval, the appli-
cation of term expansion in WSD by using co-
related terms has shown to improve the baseline
results if we carefully select the external resource
to use, with a priori knowledge of the domain
and the broadness of the corpus (wide or nar-
row domain). Evenmore, we have to be sure that
the Lexical Data Base (LDB) has been suitable
constructed. Due to the last facts, we consider
that the use of a self automatically constructed
LDB (using the same test corpora), may be of
high benefit. This assumption is based on the
intrinsic properties extracted from the corpus it-
self. Our proposal is related somehow with the
investigations presented in (Schu?tze, 1998) and
(Purandare and Pedersen, 2004), where words
are also expanded with co-ocurrence terms for
word sense discrimination. The main difference
consists in the use of the same corpora for con-
structing the co-ocurrence list.
Following we describe the self term expan-
sion method used and, thereafter, the results
obtained in the task #2 of Semeval 2007 com-
petition.
2 The Self Term Expansion Method
In literature, co-ocurrence terms is the most
common technique used for automatic construc-
tion of LDBs (Grefenstette, 1994; Frakes and
Baeza-Yates, 1992). A simple approach may use
n-grams, which allows to predict a word from
previous words in a sample of text. The fre-
quency of each n-gram is calculated and then
filtered according to some threshold. The re-
sulting n-grams constitutes a LDB which may
be used as an ?expansion dictionary? for each
term.
On the other hand, an information theory-
based co-ocurrence measure is discussed in
(Manning and Schu?tze, 2003). This measure
is named pointwise Mutual Information (MI),
and its applications for finding collocations are
analysed by determining the co-ocurrence de-
gree among two terms. This may be done by cal-
culating the ratio between the number of times
that both terms appear together (in the same
context and not necessarily in the same order)
and the product of the number of times that
each term ocurrs alone. Given two terms X1
and X2, the pointwise mutual information be-
tween X1 and X2 can be calculated as follows:
MI(X1,X2) = log2
P (X1X2)
P (X1)? P (X2)
The numerator could be modified in order to
take into account only bigrams, as presented
in (Pinto et al, 2006), where an improvement
of clustering short texts in narrow domains has
been obtained.
We have used the pointwise MI for obtaining
a co-ocurrence list from the same target dataset.
This list is then used to expand every term of the
original data. Since the co-ocurrence formula
captures relations between related terms, it is
possible to see that the self term expansion mag-
nifies less the noisy than the meaninful informa-
tion. Therefore, the execution of the clustering
algorithm in the expanded corpus should out-
perform the one executed over the non-expanded
data.
In order to fully appreciate the self term ex-
pansion method, in Table 1 we show the co-
431
ocurrence list for some words related with the
verb ?kill? of the test corpus. Since the MI
is calculated after preprocessing the corpus, we
present the stemmed version of the terms.
Word Co-ocurrence terms
soldier kill
rape women think shoot peopl old man
kill death beat
grenad todai live guerrilla fight explod
death shoot run rape person peopl outsid
murder life lebanon kill convict...
temblor tuesdai peopl least kill earthquak
Table 1: An example of co-ocurrence terms
For the task #2 of Semeval 2007, a set of 100
ambiguous words (35 nouns and 65 verbs) were
provided. We preprocessed this original dataset
by eliminating stopwords and then applying the
Porter stemmer (Porter, 1980). Thereafter,
when we used the pointwise MI, we determined
that the single ocurrence of each term should
be at least three (see (Manning and Schu?tze,
2003)), whereas the maximum separation among
the two terms was five. Finally, we selected
the unsupervised KStar clustering method (Shin
and Han, 2003) for our experiments, defining the
average of similarities among all the sentences
for a given ambiguous word as the stop criterion
for this clustering method. The input similarity
matrix for the clustering method was calculated
by using the Jaccard coefficient.
3 Evaluation
The task organizers decided to use two differ-
ent measures for evaluating the runs submitted
to the task. The first measure is called unsuper-
vised one, and it is based on the Fscore measure.
Whereas the second measure is called supervised
recall. For further information on how these
measures are calculated refer to (Agirre et al,
2006a; Agirre et al, 2006b). Since these mea-
sures give conflicting information, two different
evaluation results are reported in this paper.
In Table 2 we may see our ranking and the Fs-
core measure obtained (UPV-SI). We also show
the best and worst team Fscores; as well as the
total average and two baselines proposed by the
task organizers. The first baseline (Baseline1)
assumes that each ambiguous word has only one
sense, whereas the second baseline (Baseline2) is
a random assignation of senses. We are ranked
as third place and our results are better scored
than the other teams except for the best team
score. However, given the similar values with
the ?Baseline1?, we may assume that that team
presented one cluster per ambiguous word as its
result as the Baseline1 did; whereas we obtained
9.03 senses per ambiguous word in average.
Name Rank All Nouns Verbs
Baseline1 1 78.9 80.7 76.8
Best Team 2 78.7 80.8 76.3
UPV-SI 3 66.3 69.9 62.2
Average - 63.6 66.5 60.3
Worst Team 7 56.1 65.8 45.1
Baseline2 8 37.8 38.0 37.6
Table 2: Unsupervised evaluation (Fscore per-
formance).
In Table 3 we show our ranking and the super-
vised recall obtained (UPV-SI). We again show
the best and worst team recalls. The total av-
erage and one baseline is also presented (the
other baseline obtained the same Fscore). In
this case, the baseline tags each test instance
with the most frequent sense obtained in a train
split. We are ranked again in third place and
our score is slightly above the baseline.
Name Rank All Nouns Verbs
Best Team 1 81.6 86.8 76.2
UPV-SI 3 79.1 82.5 75.3
Average - 79.1 82.8 75.0
Baseline 4 78.7 80.9 76.2
Worst Team 6a 78.5 81.8 74.9
Worst Team 6b 78.5 81.4 75.2
Table 3: Supervised evaluation (Recall).
The results show that the technique employed
have learned, since our simple approach ob-
tained a better performance than the baselines,
especially the one that have chosen the most fre-
quent sense as baseline.
432
4 Conclusions
We have reported the performance of a single
approach based on self term expansion. The
technique uses the pointwise mutual information
for calculating a set of co-ocurrence terms which
then are used to expand the original dataset.
Once the expansion has been done, the unsu-
pervised KStar clustering method was used to
induce the sense for the different ocurrences of
each ambiguous word. We obtained the third
place in the two measures proposed in the task.
We will further investigate whether an improve-
ment may be obtained by applying term selec-
tion methods to the expanded corpus.
References
E. Agirre and Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimina-
tion Systems. In SemEval-2007. Association for
Computational Linguistics.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006a. Evaluating and opti-
mizing the parameters of an unsupervised graph-
based WSD algorithm. In Textgraphs 2006 work-
shop, NAACL06, pages 89?96.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006b. Two graph-based
algorithms for state-of-the-art WSD. In EMNLP,
pages 585?593. ACL.
S. Banerjee and T. Pedersen. 2002. An Adapted
Lesk Algorithm for Word Sense Disambiguation
Using WordNet. In CICLing 2002 Conference,
volume 3878 of LNCS, pages 136?145. Springer-
Verlang.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W. B. Frakes and R. A. Baeza-Yates. 1992. Infor-
mation Retrieval: Data Structures & Algorithms.
Prentice-Hall.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic.
M. Lesk. 1986. Automatic sense disambiguation:
How to tell a pine cone from an ice cream cone.
In ACM SIGDOC Conference, pages 24?26. ACM
Press.
D. C. Manning and H. Schu?tze. 2003. Foundations
of Statistical Natural Language Processing. MIT
Press. Revised version May 1999.
I. Nancy and J. Ve?ronis. 1990. Mapping dictionar-
ies: A spreading activation approach. In 6th An-
nual Conference of the Centre for the New Oxford
English Dictionary, pages 52?64.
D. Pinto, H. Jime?nez-Salazar, and P. Rosso. 2006.
Clustering abstracts of scientific texts using the
transition point technique. In CICLing, volume
3878 of LNCS, pages 536?546. Springer-Verlang.
M. F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3).
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing, pages 41?48, Boston, MA.
Y. Qiu and H. P. Frei. 1993. Concept based Query
Expansion. In ACM SIGIR on R&D in informa-
tion retrieval, pages 160?169. ACM Press.
R.Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern information retrieval. New York: ACM Press;
Addison-Wesley.
P. Resnik. 1995. Disambiguating Noun Groupings
with Respect to WordNet Senses. In 3rd Work-
shop on Very Large Corpora, pages 54?68. ACL.
C. J. Van Rijsbergen. 1979. Information Retrieval,
2nd edition. Dept. of Computer Science, Univer-
sity of Glasgow.
G. Ruge. 1992. Experiments on linguistically-
based term associations. Information Processing
& Management, 28(3):317?332.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Shin and S. Y. Han. 2003. Fast clustering algo-
rithm for information organization. In CICLing,
volume 2588 of LNCS, pages 619?622. Springer-
Verlang.
M. Sussna. 1993. Word sense disambiguation for
free-test indexing using a massive semantic net-
work. In 2nd International Conference on Infor-
mation and Knowledge Management, pages 67?74.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate,
and B. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99?
154.
D. Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Rogets categories trained
on large corpora. In 14th Conference on Compu-
tational Linguistics, pages 454?460. ACL.
433
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 174?177,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BUAP: An Unsupervised Approach to Automatic Keyphrase Extraction
from Scientific Articles
Roberto Ortiz, David Pinto, Mireya Tovar
Faculty of Computer Science, BUAP
Puebla, Mexico
korn resorte2003@hotmail.com,
{dpinto, mtovar}@cs.buap.mx
He?ctor Jime?nez-Salazar
Information Technologies Dept., UAM
DF, Mexico
hgimenezs@gmail.com
Abstract
In this paper, it is presented an unsuper-
vised approach to automatically discover
the latent keyphrases contained in scien-
tific articles. The proposed technique is
constructed on the basis of the combi-
nation of two techniques: maximal fre-
quent sequences and pageranking. We
evaluated the obtained results by using
micro-averaged precision, recall and F-
scores with respect to two different gold
standards: 1) reader?s keyphrases, and 2)
a combined set of author?s and reader?s
keyphrases. The obtained results were
also compared against three different base-
lines: one unsupervised (TF-IDF based)
and two supervised (Na??ve Bayes and
Maximum Entropy).
1 Introduction
The task of automatic keyphrase extraction has
been studied for several years. Firstly, as semantic
metadata useful for tasks such as summarization
(Barzilay and Elhadad, 1997; Lawrie et al, 2001;
DAvanzo and Magnini, 2005), but later rec-
ognizing the impact that good keyphrases
would have on the quality of various Nat-
ural Language Processing (NLP) applica-
tions (Frank et al, 1999; Witten et al, 1999;
Turney, 1999; Barker and Corrnacchia, 2000;
Medelyan and Witten, 2008). Thus, the selection
of important, topical phrases from within the
body of a document may be used in order to
improve the performance of systems dealing
with different NLP problems such as, clustering,
question-answering, named entity recognition,
information retrieval, etc.
In general, a keyphrase may be considered as
a sequence of one or more words that capture the
main topic of the document, as that keyphrase is
expected to represent one of the key ideas ex-
pressed by the document author. Following the
previously mentioned hypothesis, we may take ad-
vantage of two different techniques of text analy-
sis: maximal frequent sequences to extract a se-
quence of one or more words from a given text,
and pageranking, expecting to extract those word
sequences that represent the key ideas of the au-
thor.
The interest on extracting high quality
keyphrases from raw text has motivated forums,
such as SemEval, where different systems may
evaluate their performances. The purpose of
SemEval is to evaluate semantic analysis systems.
In particular, in this paper we are reporting the
results obtained in Task #5 of SemEval-2 2010,
which has been named: ?Automatic Keyphrase
Extraction from Scientific Articles?. We focused
this paper on the description of our approach and,
therefore, we do not describe into detail the task
nor the dataset used. For more information about
this information read the ?Task #5 Description
paper?, also published in this proceedings volume
(Nam Kim et al, 2010).
The rest of this paper is structured as follows.
Section 2 describes into detail the components of
the proposed approach. In Section 3 it is shown
the performance of the presented system. Finally,
in Section 4 a discussion of findings and further
work is given.
2 Description of the approach
The approach presented in this paper relies on the
combination of two different techniques for select-
ing the most prominent terms of a given text: max-
imal frequent sequences and pageranking. In Fig-
ure 1 we may see this two step approach, where
we are considering a sequence to be equivalent to
an n-gram. The complete description of the pro-
cedure is given as follows.
We select maximal frequent sequences which
174
we consider to be candidate keyphrases and, there-
after, we ranking them in order to determine which
ones are the most importants (according to the
pageranking algorithm). In the following subsec-
tions we give a brief description of these two tech-
niques. Afterwards, we provide an algorithm of
the presented approach.
Figure 1: Two step approach of BUAP Team at the
Task #5 of SemEval-2
2.1 Maximal Frequent Sequences
Definition: If a sequence p is a subsequence of q
and the number of elements in p is equal to n, then
the p is called an n-gram in q.
Definition: A sequence p = a
1
? ? ? a
k
is a sub-
sequence of a sequence q if all the items a
i
occur
in q and they occur in the same order as in p. If
a sequence p is a subsequence of a sequence q we
say that p occurs in q.
Definition: A sequence p is frequent in S if p is
a subsequence of at least ? documents in S where
? is a given frequency threshold. Only one oc-
currence of sequence in the document is counted.
Several occurrences within one document do not
make the sequence more frequent.
Definition: A sequence p is a maximal frequent
sequence in S if there does not exists any sequence
q in S such that p is a subsequence of q and p is
frequent in S.
2.2 PageRanking
The algorithm of PageRanking was defined by
Brin and Page in (Brin and Page, 1998). It is a
graph-based algorithm used for ranking webpages.
The algorithm considers input and output links of
each page in order to construct a graph, where
each vertex is a webpage and each edge may be
the input or output links for this webpage. They
denote as In(V
i
) the set of input links of webpage
V
i
, and Out(V
i
) their output links. The algorithm
proposed to rank each webpage based on the vot-
ing or recommendation of other webpages. The
higher the number of votes that are cast for a ver-
tex, the higher the importance of the vertex. More-
over, the importance of the vertex casting the vote
determines how important the vote itself is, and
this information is also taken into account by the
ranking model.
Although this algoritm has been initially pro-
posed for webpages ranking, it has been also used
for other NLP applications which may model their
corresponding problem in a graph structure. Eq.
(1) is the formula proposed by Brin and Page.
S(V
i
) = (1 ? d) + d ?
?
j?In(V
i
)
1
|Out(V
j
)|
S(V
j
)
(1)
where d is a damping factor that can be set be-
tween 0 and 1, which has the role of integrat-
ing into the model the probability of jumping
from a given vertex to another random vertex
in the graph. This factor is usually set to 0.85
(Brin and Page, 1998).
There are some other propossals, like the one
presented in (Mihalcea and Tarau, 2004), where a
textranking algorithm is presented. The authors
consider a weighted version of PageRank and
present some applications to NLP using unigrams.
They also construct multi-word terms by exploring
the conections among ranked words in the graph.
Our algorithm differs from textranking in that we
use MFS for feeding the PageRanking algorithm.
2.3 Algorithm
The complete algoritmic description of the pre-
sented approach is given in Algorithm 1. Read-
ers and writers keyphrases may be quite dif-
ferent. In particular, writers usually introduce
acronyms in their text, but they use the complete
or expanded representation of these acronyms
for their keyphrases. Therefore, we have in-
cluded a module (Extract Acronyms) for ex-
tracting both, acronyms with their corresponding
expanded version, which are used afterwards as
output of our system. We have preprocessed the
dataset removing stopwords and punctuation sym-
bols. Lemmatization (TreeTagger1) and stemming
(Porter Stemmer (Porter, 1980)) were also applied
in some stages of preprocessing.
The Maximal Freq Sequences module ex-
tracts maximal frequent sequences of words and
we feed the PageRaking module (PageRanking)
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
175
with all these sequences for determining the most
important ones. We use the structure of the sci-
entific articles in order to determine in and out
links of the sequences found. In fact, we use a
neighborhood criterion (a pair of MFS in the same
sentence) for determining the links between those
MFS?s. Once the ranking is calculated, we may se-
lect those sequences of a given length (unigrams,
bigrams and trigrams) as output of our system. We
also return a maximum of three acronyms, and
their associated multiterm phrases (MultiTerm),
as candidate keyphrases. Determining the length
and quantity of the sequences (n-grams) was ex-
perimentally deduced from the training corpus.
Algorithm 1: Algorithm of the Two Step ap-
proach for the Task #5 at SemEval-2
Input: A document set: D = {d
1
, d
2
, ? ? ? }
Output: A set K = {K
1
,K
2
, ? ? ? } of
keyphrases for each document d
i
:
K
i
= {k
i,1
, k
i,2
, ? ? ? }
foreach d
i
? D do1
AcronymSet = Extract Acronyms(d
i
);2
d
1
i
= Pre Processing(d
i
);3
MFS = Maximal Freq Sequences(d1
i
);4
CK = PageRanking(d1
i
, MFS);5
CU = Top Nine Unigrams(CK);6
CT = Top Three Trigrams(CK);7
K
i
= CT ;8
NU = 0;9
Acronyms = 0;10
foreach unigram ? CU do11
if unigram ? AcronymSet then12
if Acronyms < 3 then13
K
i
= K
i
?
{unigram};14
EA = MultiTerm(unigram);15
K
i
= K
i
?
{EA};16
Acronyms++;17
end18
else19
K
i
= K
i
?
{unigram};20
NU++;21
end22
end23
N = (15?(2?Acronyms+|CT |+NU));24
CB = Top N Bigrams(CK, N );25
K
i
= K
i
?
CB;26
end27
return K = {K
1
,K
2
, ? ? ? }28
In this edition of the Task #5 of SemEval-2
2010, we tested three different runs, which were
named: BUAP ? 1, BUAP ? 2 and BUAP ? 3.
Definition and differences among the three runs
are given in Table 3.
The results obtained with each run, together
with three different baselines are given in the fol-
lowing section.
3 Experimental results
In all tables, P , R, F mean micro-averaged pre-
cision, recall and F -scores. For baselines, there
were provided 1,2,-3 grams as candidates and
TFIDF as features. In Table 2, TFIDF is an
unsupervised method to rank the candidates based
on TFIDF scores. NB and ME are super-
vised methods using Na??ve Bayes and maximum
entropy in WEKA. In second column, R means
to use the reader-assigned keyword set as gold-
standard data and C means to use both author-
assigned and reader-assigned keyword sets as an-
swers.
Notice from Tables 2 and 3 that we outper-
formed all the baselines for the Top 15 candidates.
However, the Top 10 candidates were only outper-
formed by the Reader-Assigned keyphrases found.
This implies that the Writer keyphrases we ob-
tained were not of as good as the Reader ones. As
we mentioned, readers and writers assign different
keywords. The former write keyphrases based on
the lecture done, by the latter has a wider context
and their keyphrases used to be more complex. We
plan to investigate this issue in the future.
4 Conclusions
We have presented an approach based on the ex-
traction of maximal frequent sequences which are
then ranked by using the pageranking algorithm.
Three different runs were tested, modifying the
preprocessing stage and the number of bigrams
given as output. We did not see an improve-
ment when we used lemmatization of the docu-
ments. The run which obtained the best results
was ranking by the organizer according to the top
15 best keyphrases, however, we may see that our
runs need to be analysed more into detail in order
to provide a re-ranking procedure for the best 15
keyphrases found. This procedure may improve
the top 5 candidates precision.
176
Run name Description
BUAP ? 1 : This run is exactly the one described in Algorithm 1.
BUAP ? 2 : Same as BUAP ? 1 but lemmatization was applied a priori and stemming at the end.
BUAP ? 3 : Same as BUAP ? 2 but output twice the number of bigrams.
Table 1: Description of the three runs submitted to the Task #5 of SemEval-2 2010
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF ? IDF R 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%
C 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%
NB R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
ME R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
Table 2: Baselines
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
BUAP ? 1 R 10.40% 4.32% 6.10% 13.90% 11.54% 12.61% 14.93% 18.60% 16.56%
C 13.60% 4.64% 6.92% 17.60% 12.01% 14.28% 19.00% 19.44% 19.22%
BUAP ? 2 R 10.40% 4.32% 6.10% 13.80% 11.46% 12.52% 14.67% 18.27% 16.27%
C 14.40% 4.91% 7.32% 17.80% 12.14% 14.44% 18.73% 19.17% 18.95%
BUAP ? 3 R 10.40% 4.32% 6.10% 12.10% 10.05% 10.98% 12.33% 15.37% 13.68%
C 14.40% 4.91% 7.32% 15.60% 10.64% 12.65% 15.67% 16.03% 15.85%
Table 3: The three different runs submitted to the competition
Acknowledgments
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
References
[Barker and Corrnacchia2000] K. Barker and N. Cor-
rnacchia. 2000. Using noun phrase heads to extract
document keyphrases. In 13th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence: Advances in Artificial Intelligence.
[Barzilay and Elhadad1997] R. Barzilay and M. El-
hadad. 1997. Using lexical chains for text sum-
marization. In ACL/EACL 1997 Workshop on Intel-
ligent Scalable Text Summarization, pages 10?17.
[Brin and Page1998] S. Brin and L. Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. In COMPUTER NETWORKS AND ISDN
SYSTEMS, pages 107?117. Elsevier Science Pub-
lishers B. V.
[DAvanzo and Magnini2005] E. DAvanzo and
B. Magnini. 2005. A keyphrase-based approach
to summarization:the lake system. In Document
Understanding Conferences (DUC-2005).
[Frank et al1999] E. Frank, G.W. Paynter, I. Witten,
C. Gutwin, and C.G. Nevill-Manning. 1999. Do-
main specific keyphrase extraction. In 16th Interna-
tional Joint Conference on AI, pages 668?673.
[Lawrie et al2001] D. Lawrie, W. B. Croft, and
A. Rosenberg. 2001. Finding topic words for hi-
erarchical summarization. In SIGIR 2001.
[Medelyan and Witten2008] O. Medelyan and I. H.
Witten. 2008. Domain independent automatic
keyphrase indexing with small training sets. Jour-
nal of American Society for Information Science and
Technology, 59(7):1026?1040.
[Mihalcea and Tarau2004] R. Mihalcea and P. Tarau.
2004. Textrank: Bringing order into texts. In
EMNLP 2004, ACL, pages 404?411.
[Nam Kim et al2010] S. Nam Kim, O. Medelyan, and
M.Y. Kan. 2010. Semeval-2010 task5: Auto-
matic keyphrase extraction from scientific articles.
In Proceedings of the Fifth International Workshop
on Semantic Evaluations (SemEval-2010). Associa-
tion for Computational Linguistics.
[Porter1980] M. F. Porter. 1980. An algorithm for suf-
fix stripping. Program, 14(3).
[Turney1999] P. Turney. 1999. Learning to extract
keyphrases from text. Technical Report ERB-1057.
(NRC #41622), National Research Council, Institute
for Information Technology.
[Witten et al1999] I. Witten, G. Paynter, E. Frank,
C. Gutwin, and G. Nevill-Manning. 1999.
Kea:practical automatic key phrase extraction. In
fourth ACM conference on Digital libraries, pages
254?256.
177
