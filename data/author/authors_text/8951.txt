Chinese Generation in a Spoken Dialogue Translation System 
Hua Wu, Taiyi Huang, Chengqing Zong and Bo Xu 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of Sciences, Beijing 100080, China 
E-mail: { wh, huang, cqzong, xubo } @nlpr.ia.ac.cn 
Abstract:  
A Chinese generation module in a speech to 
speech dialogue translation system is presented 
he:re. The input of the generation module is the 
underspecified semantic representation. Its design 
is strongly influenced by the underspecification f 
the inlmtS and the necessity of real-time and 
robust processing. We design an efficient 
generation system comprising a task-oriented 
microplanner and a general surface realization 
module for Chinese. The microplanner performs 
the lexical and syntactic choice and makes 
inferences fiOln the input and domain knowledge. 
The output of the microplanner is fully 
instantiated. This enables the surface realizer to 
traverse ltle input in a top-down, depth-first 
fashion, which in turn speeds the whole 
generation procedure. The surface realizer also 
combines the template method and deep 
generation technology in the same formalism. 
Preliminary results are also presented in this 
paper. 
1,, In t roduct ion  
In this paper, we will present the core aspects 
of the generation component of our speech to 
speech dialogue translation system, the domain of 
which is hotel reservation. The whole system 
consists of five modules: speech recognizen 
translator, dialogue manageh generator and speech 
synthesizer. And the system takes the interlingua 
method in order to achieve multilinguality. Here 
the interlingua is an underspecified selnantic 
representation (USR). And the target language is 
Chinese in this paper. 
Reiter (Reiter 1995) made a clear distinction 
between templates and deep generation. The 
template method is rated as efficient but inflexible, 
while deep generation method is considered as 
flexible but inefficient. So the hybrid method to 
combine both the methods has been adopted in the 
last few years. Busemann (Busemann 1996) used 
hybrid method to allow template, canned texts and 
general rules appearing in one formalism and to 
tackle the problem of the inefficiency of the 
grammar-based surface generation system. Pianta 
(Pianta 1999) used the mixed representation 
approach to allow the system to choose between 
deep generation technology and template method. 
Our system keeps the surface generation 
module general for Chinese. At the same time, we 
can also deal with templates in tile input without 
changing tile whole generation process. If tile 
attribute in the feature structure is "template", 
then the value must be taken as a word string, 
which will appear in the output without 
modification. The surface generation module 
assumes  the input as a predicate-argument 
structure, which is called intermediate 
representation here. And any input of it must be 
first converted into an intermediate r presentation. 
The whole generation process can be 
modularized fimher into two separate components: 
microplanner and syntactic realizer. The 
microplanner is task-oriented. The input is an 
USR and the function of it is to plan an utterance 
on a phrase- or sentence-level. It maps concepts 
defined in the domain to a functional 
representation which is used by the syntactic 
generation components to realize an appropriate 
surface string for it. The functional description is 
made of feature structures, the attribute-value 
1141 
pairs. And the functional representation serves as 
the intermediate representation between the 
microplanner and the syntactic generator. The 
intermediate representation is fully instantiated. 
This enables the surface realizer to traverse the 
input in a top-down, depth-first fashion to work 
out a grammatically correct word string for the 
input, which in turn speeds the whole generatiou 
procedure. So our system use a task-oriented 
microplanner and a general surface realizer. The 
main advantage is that it is easy to adapt the 
system to other domains and maintain the 
flexibility of the system. 
In this paper, section 2 gives a brief 
description of our semantic representation. 
Section 3 presents our method on the 
microplanning procedure. Section 4 describes the 
syntactic generation module. Section 5 presents 
the preliminary results of our generation system. 
Section 6 presents discussions and future work. 
2. Semantic Representation 
The most obvious characteristics of the 
selnantic representation are its independence of 
peculiarities of any language and its 
underspecification. But it lnUSt capture the 
speaker's intent. The whole semantic 
representation has up to four components as 
shown iu figure l: speaker tag, speech act, topic 
and arguments. 
The speaker tag is either "a" lbr agent or "c" 
for customer to indicate who is speaking. The 
speech act indicates the speaker's intent. The topic 
expresses the current focus. The arguments 
indicate other inforlnatiou which is necessary to 
express the entire meaning of the source sentence. 
USR::= speaker: speech act: topic: m'gument 
Speaker::= alc 
Speech_act ::= give-information I request- 
information\[... 
Topic ::= (concept = attribute) ^
Argument ::= (concept=attribute)l* 
Figure 1 Underspecified Semantic Representation 
Both the topic and arguments are made up of 
attribute-value pairs in functional formalisms. The 
attribute can be any concept defined in the dolnain 
of hotel reservation. The value can be an atomic 
symbol or recursively an attribute-value pair. The 
symbol "^" in the topic expression indicate that 
the expression can appears zero to one time, while 
The symbol "*" iu the argument expression shows 
that the expression can appears zero to any times. 
And the attribute-value pairs are order free. Both 
topic and arguments are optional parts in the USR. 
Let us consider a complex semantic 
expression extracted from our corpus. It is shown 
in Example I: 
a: give-information: (available -- (room = 
(room- type = double ))) : (price = (quantity 
=200&240,currency=dollor)) I (1) 
In Example 1, the speech act is give- 
information, which means that the agent is 
offering information to the customer. The topic 
indicates there are double rooms. The arguments 
list the prices of double rooms, which shows that 
there are two kinds of double rooms available. So 
the meaning of this representation is " We have 
two kinds of double rooms which cost 200 mad 
240 dollars respectively". From the USR, the 
kinds of rooms are not expressed explicitly in the 
format. Only from the composite value of the 
concept "price " can we judge there are two kinds 
of rooms because the price is different. This is 
only one example of underspecification, which 
needs inferences from the input and the domain 
knowledge. 
3. The Microplanner 
The input to our microplanner is the 
underspecified semantic representation. From the 
above semantic representation, we can see that it 
is underspecified because it lacks infornlation 
such as predicate-argument structure, cognitive 
status of referents, or restrictive/attribute fimction 
of semantic properties. Some of the non-specified 
pieces of ilfformation such as predicate/argument 
structure are essential to generate a correct 
translation of the source sentence. Fortmmtely, 
much of the information which is not explicitly 
represented can be inferred fiom default 
knowledge about the specific domain and the 
general world knowledge. 
The lnicroplanner includes two parts: 
sentence-level planning and phrase-level planning. 
1142 
The sentence planner maps the semantic 
representation into predicate argument structure. 
And the phrase planner maps the concepts defined 
in the domain into Chinese phrases. 
In order to express rules, we design a format 
t'or them. The rules are represented as pattern- 
constraints-action triples. A pattern is to be 
matched with part of the input on the sentence 
level and with the concepts on the phrase level. 
The constraints describe additional context- 
dependent requirements to be fulfilled by the 
input. And the action part describes the predicate 
argument structure and other information such as 
mood and sentence type. An example describiug a 
sentence-level rule is shown in Figure 2. 
((speaker= a ) ( speech_act =give-information )( topic 
= available ) ( topic_value = room )); 
//pattern 
(exist(concept, 'price' )); //constraint 
( (cat = clause) ( mood = declarative) 
( tense = present) (voice = active) 
(sentence type = possessive) 
(predicate ='4f') 
(args = (((case - pos) 
(lex :- #get(attribute, 'room' ))) 
((case = bel) 
(cat = de) 
(modifier-(#gct(altributc, 'price' ))) 
(auxiliary = 'l'l{j')))) 
(!optiolml: pre_mod = ( time = #get ( attribute, 
'lime')))); //action 
Figure 2 Example Microplanning l>,ule 
First, we match the pattern part with the input 
USI>,. If matched, the constraint is tested. In the 
example, the concept price lnust exist in the input. 
The action part describes the whole sentence 
structure such as predicate argument structure, 
sentence type, voice, mood. The symbol "#get" in 
the action part indicates thai the value can be 
obtained by accessing the phrase rules or the 
dictionary to colnplete the structure recursively. 
The "#get" expression has two parameters. The 
first parameter can be "concept" or "attribute" to 
indicate to access the dictionary and phrase rues 
respectively. The second parameter is a concept 
defined in the domain. In the example, the "#get" 
expression is used to get the value of the domain 
concepts room and price respectively. The symbol 
"optionah" indicates that the attribute-value pair 
behind it is optional. If the input has the concept, 
we fill it. 
After the sentence- and phrase-level phmning, 
we must access the Chinese dictionary to get the 
part-of-speech of the lexicon and other syntactic 
information. If the input is the representation i  
Example I, the result of the microplanning is 
shown in Figure 3. 
(cat  = clat, se) 
( sentence_type =possessive) 
(mood = declarative) 
( tense = present) (voice = active) 
(predicate =((cat=vcm) (lex ='4f'))) 
(args=(((case = pos)(cat = nct)(lex ='~J, Vl'iq')) 
((case = bel) 
(cat =de) 
(modi fier=((cat=mp) 
(cardinal =((cat=nc) 
(n l=((cat=num) (lex='200')) 
(n2=((cat=num)(lex="240')) 
(qtf= ((cat=ncl) ( lex ='0~)t\]')))) 
(at, x il iary =(lex =' I'1I'.1')))) 
lVigure 3 Microplanning Result for Example 1 
In the above example, "cat" indicates the 
category of the sentence, plnases or words. "h'x" 
denotes the Chinese words. "case" describes the 
semantic roles of the arguments. 
Target language generation in dialogue 
translation systems imposes strong constraints on 
the whole generation. A prominent pmblena is the 
non-welformedness of the input. It forces the 
generation module to be robust to cope with the 
erroneous and incomplete input data. In this level, 
we design some general rules. The input is first to 
be matched with the specific rules. If there is no 
rules matched, we access the general rules to 
match with the input. In this way, although the 
input is somehow ill-formed, the output still 
includes the main information of the input. An 
example is shown in (2). The utterance is 
supposed for the custom to accept he single room 
offered by the agent. But the speech act is wrong 
because the speech act "ok" is only used to 
1143 
indicate' that the custol-u and tile agenl has agreed 
on one Iopic. 
c: ok: ( room = ( room-type := single, 
quantity= 1 )): (2) 
Although example (2) is ill formed, it 
includes most information of the source sentence. 
Our robust generator can produce the sentence 
shown in (3). 
Cl'i-)kf/iJ~J: ( yes, a single roont ) (3) 
4. Syntactic realizatim  
The syntactic realizer proceeds from the 
microplannir~g result as shown in t"igure 3. The 
realizer is based on a flmctional uuificati,,m 
fornmlism. 
lit tMs module, we also introduce the 
template nlethod. If lhe input includes an 
attribute~wflue pair which uses "template" as file 
attribute, then rite wflue is taken as canned lexts or 
word strhws wilh slots. It will appear in the output 
without any modificati(m. So we can embed tile 
template into the surface realization without 
modifying tlw whoh: generation l)rocedure. When 
the hybrid method is used, the input is first 
matched with the templates defined. If matched, 
the inputs will go lo llle surface realizer directly, 
skiplfing tl,c microplanning process. 
The task of the Chinese realizer i:; as tollows: 
, Define the sentence struclure 
? Provide ordering constraints among the 
syntactic onstituents of the sentence 
? Select the functional words 
4.1 \]Intermediate Representation 
The intermediate representation(IR) is made 
up of feature structures. It corresponds to the 
predicate argument structure. The aim is to 
normalize the input of tile surface realizer. It is of 
considerable practical benefit to keep the rule 
basis as independent as possible front external 
conditions (such as the domain and output of tile 
preceding system). 
The intermediate representation includes 
three parts: predicate int"ormation, obligatory 
arguments and optional arguments. The predicate 
inR)rmation describes the top-level information in 
a clause includiug the main verb, lhe mood, the 
voice, and so on. The obligatory arguments are 
slots of roles that must be filled in a clause for it 
to be contplete. And the optional arguments 
specify the location, the time, the purpose of the 
event etc. They arc optional because they do not 
affect rite contpleteness of a clause. An example is 
shown in Figure 4. The input is for the sentence 
"{~J~ l'f\] ~\]l~ 1{ 1'1 @) \  \[)iJ li!.~ ?" (Do you have single 
rooms now?). "agrs" antt '?opt" in Figure 4 
represent obligatory arguntents and optional 
arguments respectively. 
((cat = clause) 
( sentence )ype =possessive) 
(mood: yes-no) 
( lense = present) (wfice -: active) 
(predicate =((cat=veto) (lex ="(J"))) 
(args=(((case :-: pos)(ca! -pron)(lex ='{?j<{f\]')) 
((case "- bel) (cai ~:nct)(lex=' "l%)v. \['(iJ ')))) 
(opt=(d me=((cat=:adv) tie x=' J:l)lu (I i'))))) 
Fip, urc 4 F, xample Intermediate l),el)rescnlalion 
4.2 Chinese Reallizalion 
In tile synlaclic generation module, we use 
ihe \[unclional unification fommlism. At tile same 
lime, we make use of dlc systclnic viewpoirl/ of 
lhe systcrnic function grammar. The rule system is 
made up of many sub-.sysienls such as transitivily 
system, mood system, tense system and voice 
systcllt. The input 111ust depend on all of these 
systems to make difR:rent level decisions. 
In a spoken dialogue Iranslalion system, real= 
lime generation is tile basic requiremenl. As we 
see froln the input as shown in Figure 3, the inlmt 
to the syntaclic generation provides enough 
iuformation about sentence and phrase structme. 
Most of the informatiou in tile input ix instautiatcd, 
such as the verb, the subcategorization frame and 
the phrase members. So the generation engine can 
traverse the input in a top-down, depth-first 
fashion using tmification algorithm (Elhadad 
1992). The whole syntactic generation process is 
described in Figure 5. 
The input is an intermediate representation 
and the output is Chinese texts. The sentence 
unification phase defines the sentence structure 
and orders the components anloDg, tile sentence. 
1144 
The phrase unification phase dcl'ines the phrase 
structure, orders the co~nponenls inside the 
phrases and adds the function words. Unlike 
English, Chinese has no morphological markers 
for tenses and moods. They arc expressed with 
fmlclional words. Selecting functiolml words 
correctly is crilical for Chillesc generation. 
,qelltellCC\[ ~'t "~''~'-t ~unifica|ion ~lst? -- -- !'-;~7~II1 I - tlni fcatiOll \]~CXt 
Figure 5 Sleps of the Synlacfic generator 
The whole unification procedure is: 
,, Unify the input with the grammar at the 
sentence l vel. 
? identify the conslitules inside the inptll 
? Unify the constituents with tile grammar a! the 
phrase level recursively in a top-down, depth- 
first fashion. 
5. Results 
The current version of the system has been 
tested on our hotel reservation corpus (Chengqing 
Zong, 1999). The whole corpus includes about 90 
dialogues, annotated by hand with underspecificd 
semantic representation. I1 contains about 3000 
USRs. Now we have 23 speech acls and about 60 
concepts in lhe corpus. 
The generation lnodulc is tested on all 
sentences in the corpus. And 90% of the generated 
sentences arc rated as grammatically and 
semantically correct. The other 10% are rated as 
wrong because the mood of the sentences i not 
conect. This is mainly caused by the lack of the 
dialogue context. 
6. Discussion and Future Work 
In spoken language translation systems, one 
problem is the ill-formed input. How to tackle this 
problem robustly is very important. At the 
microplanning level, we design some general 
rules. The input is first to be matched with the 
sl~e<:ific roles. If there is no rules matched, we 
access the gene.ral roles to Inalch with the input. In 
this way, although the inl)U! is somehow ill- 
formed, the output includes the main information 
of the input. And at the surface realization level, 
we make some relaxation on tests to improve the 
robuslness, l;,.g, oMigatory arguments may be 
missing in the utterance. This can be caused by 
ellipsis in sentences such as the utterances "{:\]{: ~ 
J~." (stay for three days). We have to accept it as a 
sentence without the subject because they are 
acceptable in spoken Chinese and often appear in 
daily dialogues. 
We arc planning to l:tuther increase the 
robustness of the system. And if possible, we also 
hope to adapt our generation system to other 
(lolnaills. 
Acknowledgements 
The research work described in lhis paper is 
Sulsportcd by the National Natural Science 
t;oundation of China under grant number 
69835030 and by the National '863' Hi-Tcch 
Program under grant nunlber 863-306-ZT03-02-2. 
Thanks also go to several allonyl/lOtlS l'eVieWel'S 
for their valuable comments. 
Reference ;  
Stephan I} uscmann. (1996) Best- first surl'ace 
realization. In t i le Eighth lntcrnatiolml Natural 
l.anguagc Generation Workshop, Sussex, pages 101- 
I10 
Michael Elhadad and Jacques Robin. (1992) 
Controlling Content P.calization with Functional 
Unification Grammars. Aspects of Automated Natural 
Language Generation. t51)89 - 104 
E.Pianta, M.Tovcna. (1999) XIG: Generating from 
Interchange Format Using Mixed Representation. 
AAAI'99 
Ehud Reiter. (1995) NLG vs. Templates. In lhe Fiflh 
Et, ropcan Workshop on Natural Language Generation, 
Leiden, 
Chengqing Zong, Hua Wu, Taiyi Huang, Be Xu. (1999) 
Analysis on Characteristics of Chinese Spoken 
Language. In the Fiflh Natural Language Processing 
Pacific Rim Symposium, 151)358-362 
1145 
Synonymous Collocation Extraction Using Translation Information  
Hua WU, Ming ZHOU 
Microsoft Research Asia 
5F Sigma Center, No.49 Zhichun Road, Haidian District 
Beijing, 100080, China 
wu_hua_@msn.com, mingzhou@microsoft.com 
 
Abstract 
Automatically acquiring synonymous col-
location pairs such as <turn on, OBJ, light> 
and <switch on, OBJ, light> from corpora 
is a challenging task. For this task, we can, 
in general, have a large monolingual corpus 
and/or a very limited bilingual corpus. 
Methods that use monolingual corpora 
alone or use bilingual corpora alone are 
apparently inadequate because of low pre-
cision or low coverage. In this paper, we 
propose a method that uses both these re-
sources to get an optimal compromise of 
precision and coverage. This method first 
gets candidates of synonymous collocation 
pairs based on a monolingual corpus and a 
word thesaurus, and then selects the ap-
propriate pairs from the candidates using 
their translations in a second language. The 
translations of the candidates are obtained 
with a statistical translation model which is 
trained with a small bilingual corpus and a 
large monolingual corpus. The translation 
information is proved as effective to select 
synonymous collocation pairs. Experi-
mental results indicate that the average 
precision and recall of our approach are 
74% and 64% respectively, which outper-
form those methods that only use mono-
lingual corpora and those that only use bi-
lingual corpora. 
1 Introduction 
This paper addresses the problem of automatically 
extracting English synonymous collocation pairs 
using translation information. A synonymous col-
location pair includes two collocations which are 
similar in meaning, but not identical in wording. 
Throughout this paper, the term collocation refers 
to a lexically restricted word pair with a certain 
syntactic relation. For instance, <turn on, OBJ, 
light> is a collocation with a syntactic relation 
verb-object, and <turn on, OBJ, light> and <switch 
on, OBJ, light> are a synonymous collocation pair. 
In this paper, translation information means trans-
lations of collocations and their translation prob-
abilities. 
Synonymous collocations can be considered as 
an extension of the concept of synonymous ex-
pressions which conventionally include synony-
mous words, phrases and sentence patterns. Syn-
onymous expressions are very useful in a number of 
NLP applications. They are used in information 
retrieval and question answering (Kiyota et al, 
2002; Dragomia et al, 2001) to bridge the expres-
sion gap between the query space and the document 
space. For instance, ?buy book? extracted from the 
users? query should also in some way match ?order 
book? indexed in the documents. Besides, the 
synonymous expressions are also important in 
language generation (Langkilde and Knight, 1998) 
and computer assisted authoring to produce vivid 
texts.  
Up to now, there have been few researches 
which directly address the problem of extracting 
synonymous collocations. However, a number of 
studies investigate the extraction of synonymous 
words from monolingual corpora (Carolyn et al, 
1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al, 
2001). The methods used the contexts around the 
investigated words to discover synonyms. The 
problem of the methods is that the precision of the 
extracted synonymous words is low because it 
extracts many word pairs such as ?cat? and ?dog?, 
which are similar but not synonymous. In addition, 
some studies investigate the extraction of synony-
mous words and/or patterns from bilingual corpora 
(Barzilay and Mckeown, 2001; Shimohata and 
Sumita, 2002). However, these methods can only 
extract synonymous expressions which occur in the 
bilingual corpus. Due to the limited size of the 
bilingual corpus, the coverage of the extracted 
expressions is very low. 
Given the fact that we usually have large mono-
lingual corpora (unlimited in some sense) and very 
limited bilingual corpora, this paper proposes a 
method that tries to make full use of these different 
resources to get an optimal compromise of preci-
sion and coverage for synonymous collocation 
extraction. We first obtain candidates of synony-
mous collocation pairs based on a monolingual 
corpus and a word thesaurus. We then select those 
appropriate candidates using their translations in a 
second language. Each translation of the candidates 
is assigned a probability with a statistical translation 
model that is trained with a small bilingual corpus 
and a large monolingual corpus. The similarity of 
two collocations is estimated by computing the 
similarity of their vectors constructed with their 
corresponding translations. Those candidates with 
larger similarity scores are extracted as synony-
mous collocations. The basic assumption behind 
this method is that two collocations are synony-
mous if their translations are similar. For example, 
<turn on, OBJ, light> and <switch on, OBJ, light> 
are synonymous because both of them are translated 
into < , OBJ, > (<kai1, OBJ, deng1>) and < , 
OBJ, > (<da3 kai1, OBJ, deng1>)  in Chinese.  
In order to evaluate the performance of our 
method, we conducted experiments on extracting 
three typical types of synonymous collocations. 
Experimental results indicate that our approach 
achieves 74% average precision and 64% recall 
respectively, which considerably outperform those 
methods that only use monolingual corpora or only 
use bilingual corpora. 
The remainder of this paper is organized as fol-
lows. Section 2 describes our synonymous colloca-
tion extraction method. Section 3 evaluates the 
proposed method, and the last section draws our 
conclusion and presents the future work. 
2 Our Approach 
Our method for synonymous collocation extraction 
comprises of three steps: (1) extract collocations 
from large monolingual corpora; (2) generate can-
didates of synonymous collocation pairs with a 
word thesaurus WordNet; (3) select synonymous 
collocation candidates using their translations.  
2.1 Collocation Extraction 
This section describes how to extract English col-
locations. Since Chinese collocations will be used 
to train the language model in Section 2.3, they are 
also extracted in the same way.  
Collocations in this paper take some syntactical 
relations (dependency relations), such as <verb, 
OBJ, noun>, <noun, ATTR, adj>, and <verb, MOD, 
adv>. These dependency triples, which embody the 
syntactic relationship between words in a sentence, 
are generated with a parser?we use NLPWIN in 
this paper1. For example, the sentence ?She owned 
this red coat? is transformed to the following four 
triples after parsing: <own, SUBJ, she>, <own, OBJ, 
coat>, <coat, DET, this>, and <coat, ATTR, red>. 
These triples are generally represented in the form 
of <Head, Relation Type, Modifier>. 
 The measure we use to extract collocations 
from the parsed triples is weighted mutual infor-
mation (WMI) (Fung and Mckeown, 1997), as 
described as  
)()|()|(
),,(log),,(),,(
21
21
2121
rprwprwp
wrwp
wrwpwrwWMI =      
Those triples whose WMI values are larger than a 
given threshold are taken as collocations. We do not 
use the point-wise mutual information because it 
tends to overestimate the association between two 
words with low frequencies. Weighted mutual 
information meliorates this effect by add-
ing ),,( 21 wrwp . 
For expository purposes, we will only look into 
three kinds of collocations for synonymous collo-
cation extraction: <verb, OBJ, noun>, <noun, 
ATTR, adj> and <verb, MOD, adv>.  
Table 1. English Collocations 
Class #Type  #Token 
verb, OBJ, noun 506,628 7,005,455 
noun, ATTR, adj 333,234 4,747,970 
verb, Mod, adv 40,748 483,911 
Table 2. Chinese Collocations 
Class #Type #Token 
verb, OBJ, noun 1,579,783 19,168,229 
noun, ATTR, adj 311,560 5,383,200 
verb, Mod, adv 546,054 9,467,103 
The English collocations are extracted from 
Wall Street Journal (1987-1992) and Association 
Press (1988-1990), and the Chinese collocations are 
                                                     
1
 The NLPWIN parser is developed at Microsoft Re-
search, which parses several languages including Chi-
nese and English. Its output can be a phrase structure 
parse tree or a logical form which is represented with 
dependency triples. 
extracted from People?s Daily (1980-1998). The 
statistics of the extracted collocations are shown in 
Table 1 and 2. The thresholds are set as 5 for both 
English and Chinese. Token refers to the total 
number of collocation occurrences and Type refers 
to the number of unique collocations in the corpus. 
2.2 Candidate Generation 
Candidate generation is based on the following 
assumption: For a collocation <Head, Relation 
Type, Modifier>, its synonymous expressions also 
take the form of <Head, Relation Type, Modifier> 
although sometimes they may also be a single word 
or a sentence pattern.  
The synonymous candidates of a collocation are 
obtained by expanding a collocation <Head, Rela-
tion Type, Modifier> using the synonyms of Head 
and Modifier. The synonyms of a word are obtained 
from WordNet 1.6. In WordNet, one synset consists 
of several synonyms which represent a single sense. 
Therefore, polysemous words occur in more than 
one synsets. The synonyms of a given word are 
obtained from all the synsets including it. For ex-
ample, the word ?turn on? is a polysemous word 
and is included in several synsets. For the sense 
?cause to operate by flipping a switch?, ?switch on? 
is one of its synonyms. For the sense ?be contingent 
on?, ?depend on? is one of its synonyms. We take 
both of them as the synonyms of ?turn on? regard-
less of its meanings since we do not have sense tags 
for words in collocations. 
If we use Cw to indicate the synonym set of a 
word w and U to denote the English collocation set 
generated in Section 2.1. The detail algorithm on 
generating candidates of synonymous collocation 
pairs is described in Figure 1. For example, given a 
collocation <turn on, OBJ, light>, we expand ?turn 
on? to ?switch on?, ?depend on?, and then expand 
?light? to ?lump?, ?illumination?. With these 
synonyms and the relation type OBJ, we generate 
synonymous collocation candidates of <turn on, 
OBJ, light>. The candidates are <switch on, OBJ, 
light>, <turn on, OBJ, lump>, <depend on, OBJ, 
illumination>, <depend on, OBJ, light> etc. Both 
these candidates and the original collocation <turn 
on, OBJ, light> are used to generate the synony-
mous collocation pairs.  
With the above method, we obtained candidates 
of synonymous collocation pairs. For example, 
<switch on, OBJ, light> and <turn on, OBJ, light> 
are a synonymous collocation pair. However, this 
method also produces wrong synonymous colloca-
tion candidates. For example, <depend on, OBJ, 
illumination> and <turn on, OBJ, light> is not a 
synonymous pair. Thus, it is important to filter out 
these inappropriate candidates. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Candidate Set Generation Algorithm 
2.3 Candidate Selection 
In synonymous word extraction, the similarity of 
two words can be estimated based on the similarity 
of their contexts. However, this method cannot be 
effectively extended to collocation similarity esti-
mation. For example, in sentences ?They turned on 
the lights? and ?They depend on the illumination?, 
the meaning of two collocations <turn on, OBJ, 
light> and <depend on, OBJ, illumination> are 
different although their contexts are the same.  
Therefore, monolingual information is not enough 
to estimate the similarity of two collocations. 
However, the meanings of the above two colloca-
tions can be distinguished if they are translated into 
a second language (e.g., Chinese).  For example, 
<turn on, OBJ, light> is translated into < , OBJ, 
> (<kai1, OBJ, deng1) and < , OBJ, > (<da3 
kai1, OBJ, deng1>) in Chinese while <depend on, 
OBJ, illumination> is translated into < , OBJ, 
> (qu3 jue2 yu2, OBJ, guang1 zhao4 du4>). 
Thus, they are not synonymous pairs because their 
translations are completely different. 
In this paper, we select the synonymous collo-
cation pairs from the candidates in the following 
way. First, given a candidate of synonymous col-
location pair generated in section 2.2, we translate 
the two collocations into Chinese with a simple 
statistical translation model. Second, we calculate 
the similarity of two collocations with the feature 
vectors constructed with their translations. A can-
didate is selected as a synonymous collocation pair 
(1) For each collocation (Co1i=<Head, R, Modi-
fier>) U, do the following: 
a. Use the synonyms in WordNet 1.6 to expand 
Head and Modifier and get their synonym 
sets CHead and CModifier 
b. Generate the candidate set of its synonymous 
collocations Si={<w1, R, w2> | w1 {Head}
 CHead  & w2 {Modifier}  CModifier  & 
<w1, R, w2> U & <w1, R, w2> ?  Co1i } 
(2) Generate the candidate set of synonymous 
collocation  pairs  SC= {(Co1i, Co1j)| Co1i
Co1j Si  
 
if its similarity exceeds a certain threshold. 
2.3.1 Collocation Translation 
For an English collocation ecol=<e1, re, e2>, we 
translate it into Chinese collocations 2  using an 
English-Chinese dictionary. If the translation sets of 
e1 and e2 are represented as CS1 and CS2 respec-
tively, the Chinese translations can be represented 
as S={<c1, rc, c2>| c1 CS1 , c2 CS2 , rc  },  with R 
denoting the relation set. 
 Given an English collocation ecol=<e1, re, e2> 
and one of its Chinese collocation ccol=<c1, rc, 
c2> S, the probability that ecol is translated into ccol 
is calculated as in Equation (1). 
)(
),,(),,|,,()|( 212121
col
cce
colcol
ep
crcpcrcerep
ecp =  (1) 
According to Equation (1), we need to calculate the 
translation probability p(ecol|ccol) and the target 
language probability p(ccol). Calculating the trans-
lation probability needs a bilingual corpus. If the 
above equation is used directly, we will run into the 
data sparseness problem. Thus, model simplifica-
tion is necessary. 
2.3.2 Translation Model 
Our simplification is made according to the fol-
lowing three assumptions. 
Assumption 1: For a Chinese collocation ccol and re, 
we assume that e1 and e2 are conditionally inde-
pendent. The translation model is rewritten as:  
)|(),|(),|(
)|,,()|(
21
21
colecolecole
colecolcol
crpcrepcrep
cerepcep
=
=
           (2) 
Assumption 2: Given a Chinese collocation <c1, rc, 
c2>, we assume that the translation probability 
p(ei|ccol) only depends on ei and ci (i=1,2), and 
p(re|ccol) only depends on re and rc. Equation (2) is 
rewritten as:  
)|()|()|(
)|()|()|()|(
2211
21
ce
colecolcolcolcol
rrpcepcep
crpcepcepcep
=
=
  (3) 
It is equal to a word translation model if we take 
the relation type in the collocations as an element 
like a word, which is similar to Model 1 in (Brown 
et al, 1993). 
Assumption 3: We assume that one type of English 
                                                     
2
 Some English collocations can be translated into Chi-
nese words, phrases or patterns. Here we only consider 
the case of being translated into collocations. 
collocation can only be translated to the same type 
of Chinese collocations3. Thus, p(re| rc) =1 in our 
case. Equation (3) is rewritten as: 
)|()|(
)|()|()|()|(
2211
2211
cepcep
rrpcepcepcep
cecolcol
=
=
           (4) 
2.3.3 Language Model 
The language model p(ccol) is calculated with the 
Chinese collocation database extracted in section 
2.1. In order to tackle with the data sparseness 
problem, we smooth the language model with an 
interpolation method. 
When the given Chinese collocation occurs in 
the corpus, we calculate it as in (5). 
N
ccount
cp col
col
)()( =                      (5) 
where )(
colccount represents the count of the Chi-
nese collocation 
colc . N represents the total counts 
of all the Chinese collocations in the training cor-
pus.  
For a collocation <c1, rc, c2>, if we assume that 
two words c1 and c2 are conditionally independent 
given the relation rc, Equation (5) can be rewritten 
as in (6). 
)()|()|()( 21 ccccol rprcprcpcp =                          (6) 
where 
,*)(*,
,*),()|( 11
c
c
c
rcount
rccount
rcp =  
,*)(*,
),(*,)|( 22
c
c
c
rcount
crcount
rcp = , 
N
rcount
rp c
c
,*)(*,)( =  
,*),( 1 crccount : frequency of the collocations with c1 
as the head and rc as the relation type.  
),(*, 2crcount c : frequency of the collocations with 
c2 as the modifier and rc as the relation type 
,*)(*,
c
rcount : frequency of the collocations with rc 
as the relation type. 
With Equation (5) and (6), we get the interpolated 
language model as shown in (7). 
)()|()|()-(1 )()( 21 ccccolcol rprcprcpN
ccount
cp ?? +=                     
                                                                           (7) 
where 10 << ? . ? is a constant so that the prob-
abilities sum to 1. 
 
                                                     
3
 Zhou et al (2001) found that about 70% of the Chinese 
translations have the same relation type as the source 
English collocations. 
2.3.4 Word Translation Probability Estimation   
Many methods are used to estimate word translation 
probabilities from unparallel or parallel bilingual 
corpora (Koehn and Knight, 2000; Brown et al, 
1993). In this paper, we use a parallel bilingual 
corpus to train the word translation probabilities 
based on the result of word alignment with a bi-
lingual Chinese-English dictionary. The alignment 
method is described in (Wang et al, 2001). In order 
to deal with the problem of data sparseness, we 
conduct a simple smoothing by adding 0.5 to the 
counts of each translation pair as in (8).  
|_|*5.0)(
5.0),()|(
etransccount
cecount
cep
+
+
=       (8) 
where |_| etrans  represents the number of Eng-
lish translations for a given Chinese word c.  
2.3.5 Collocation Similarity Calculation 
For each synonymous collocation pair, we get its 
corresponding Chinese translations and calculate 
the translation probabilities as in section 2.3.1. 
These Chinese collocations with their correspond-
ing translation probabilities are taken as feature 
vectors of the English collocations, which can be 
represented as: 
>=< ),(, ... ),,(),,( 2211 im
col
im
col
i
col
i
col
i
col
i
col
i
col pcpcpcFe  
The similarity of two collocations is defined as in 
(9). The candidate pairs whose similarity scores 
exceed a given threshold are selected. 
( ) ( )

=
=
=
j
j
col
i
i
col
j
colc
i
colc
j
col
i
col
colcolcolcol
pp
pp
FeFeeesim
2221
21
21
2121
*
*
),cos(),(
                 (9) 
For example, given a synonymous collocation 
pair <turn on, OBJ, light> and <switch on, OBJ, 
light>, we first get their corresponding feature 
vectors.  
The feature vector of <turn on, OBJ, light>:  
< (< , OBJ, >, 0.04692), (< , OBJ, >,  
0.01602), ? , (< , OBJ, >, 0.0002710), (< , 
OBJ, >, 0.0000305) > 
The feature vector of <switch on, OBJ, light>: 
< (< , OBJ, >, 0.04238), (< , OBJ, >, 
0.01257), (< , OBJ, >, 0.002531), ? , (< , 
OBJ, >, 0.00003542) > 
The values in the feature vector are translation 
probabilities. With these two vectors, we get the 
similarity of <turn on, OBJ, light> and <switch on, 
OBJ, light>, which is 0.2348. 
2.4 Implementation of our Approach 
We use an English-Chinese dictionary to get the 
Chinese translations of collocations, which includes 
219,404 English words. Each source word has 3 
translation words on average. The word translation 
probabilities are estimated from a bilingual corpus 
that obtains 170,025 pairs of Chinese-English sen-
tences, including about 2.1 million English words 
and about 2.5 million Chinese words.  
 With these data and the collocations in section 
2.1, we produced 93,523 synonymous collocation 
pairs and filtered out 1,060,788 candidate pairs with 
our translation method if we set the similarity 
threshold to 0.01. 
3 Evaluation 
To evaluate the effectiveness of our methods, two 
experiments have been conducted. The first one is 
designed to compare our method with two methods 
that use monolingual corpora. The second one is 
designed to compare our method with a method that 
uses a bilingual corpus.  
3.1 Comparison with Methods using 
Monolingual Corpora 
We compared our approach with two methods that 
use monolingual corpora. These two methods also 
employed the candidate generation described in 
section 2.2. The difference is that the two methods 
use different strategies to select appropriate candi-
dates. The training corpus for these two methods is 
the same English one as in Section 2.1. 
3.1.1 Method Description 
Method 1: This method uses monolingual contexts 
to select synonymous candidates. The purpose of 
this experiment is to see whether the context 
method for synonymous word extraction can be 
effectively extended to synonymous collocation 
extraction.  
The similarity of two collocations is calculated 
with their feature vectors. The feature vector of a 
collocation is constructed by all words in sentences 
which surround the given collocation. The context 
vector for collocation i is represented as in (10).  
 >=< ),(),...,,(),,( 2211 imimiiiiicol pwpwpwFe   (10) 
where 
N
ewcount
p
i
colij
ij
),(
=  
ijw : context word j of collocation i. 
ijp : probability of ijw  co-occurring with icole .   
),( i
colij ewcount : frequency of the context word ijw  
co-occurring with the collocation i
cole  
N: all counts of the words in the training corpus. 
With the feature vectors, the similarity of two col-
locations is calculated as in (11). Those candidates 
whose similarities exceed a given threshold are 
selected as synonymous collocations. 
( ) ( )

=
=
=
j
j
i
i
jwiw
ji
colcolcolcol
pp
pp
FeFeeesim
2
2
2
1
21
21
2121
*
*
),cos(),(
          (11) 
Method 2: Instead of using contexts to calculate the 
similarity of two words, this method calculates the 
similarity of collocations with the similarity of their 
components. The formula is described in Equation 
(12). 
),(*),(*),(
),(
212
2
1
2
2
1
1
1
21
relrelsimeesimeesim
eesim
colcol
=
   (12) 
where ),,( 21 iiiicol erelee = . We assume that the rela-
tion type keeps the same, so 1),( 21 =relrelsim .  
The similarity of the words is calculated with the 
same method as described in (Lin, 1998), which is 
rewritten in Equation (13). The similarity of the 
words is calculated through the surrounding context 
words which have dependency relationships with 
the investigated words.   
),,(),,(
)),,(),,((
),(
2)2(),(
1)1(),(
21)2()1(),(
21
erelewerelew
erelewerelew
eeSim
eTereleTerel
eTeTerel
??
?
+
+
=

            (13) 
where T(ei) denotes the set of words which have the 
dependency relation rel with ei. 
)()|()|(
),,(
log),,(
),,(
relpreleprelep
erelep
erelep
erelew
ji
ji
ji
ji
=
 
3.1.2 Test Set 
With the candidate generation method as depicted 
in section 2.2, we generated 1,154,311 candidates 
of synonymous collocations pairs for 880,600 
collocations, from which we randomly selected 
1,300 pairs to construct a test set. Each pair was 
evaluated independently by two judges to see if it is 
synonymous. Only those agreed upon by two judges 
are considered as synonymous pairs. The statistics 
of the test set is shown in Table 3. We evaluated 
three types of synonymous collocations: <verb, 
OBJ, noun>, <noun, ATTR, adj>, <verb, MOD, 
adv>. For the type <verb, OBJ, noun>, among the 
630 synonymous collocation candidate pairs, 197 
pairs are correct. For <noun, ATTR, adj>, 163 pairs 
(among 324 pairs) are correct, and for <verb, MOD, 
adv>, 124 pairs (among 346 pairs) are correct. 
Table 3. The Test Set 
 Type #total #correct 
verb, OBJ, noun 630 197 
noun, ATTR, adj 324 163 
verb, MOD, adv 346 124 
3.1.3 Evaluation Results  
With the test set, we evaluate the performance of 
each method. The evaluation metrics are precision, 
recall, and f-measure. 
A development set including 500 synonymous 
pairs is used to determine the thresholds of each 
method. For each method, the thresholds for getting 
highest f-measure scores on the development set are 
selected. As the result, the thresholds for Method 1, 
Method 2 and our approach are 0.02, 0.02, and 0.01 
respectively. With these thresholds, the experi-
mental results on the test set in Table 3 are shown in 
Table 4, Table 5 and Table 6. 
Table 4. Results for <verb, OBJ, noun> 
Method Precision Recall F-measure 
Method 1 0.3148 0.8934 0.4656 
Method 2 0.3886 0.7614 0.5146 
Ours 0.6811 0.6396 0.6597 
Table 5. Results for <noun, ATTR, adj> 
Method Precision Recall F-measure 
Method 1 0.5161 0.9816 0.6765 
Method 2 0.5673 0.8282 0.6733 
Ours 0.8739 0.6380 0.7376 
Table 6. Results for <verb, MOD, adv> 
Method Precision Recall F-measure 
Method 1 0.3662 0.9597 0.5301 
Method 2 0.4163 0.7339 0.5291 
Ours 0.6641 0.7016 0.6824 
It can be seen that our approach gets the highest 
precision (74% on average) for all the three types of 
synonymous collocations. Although the recall (64% 
on average) of our approach is below other methods, 
the f-measure scores, which combine both precision 
and recall, are the highest.  In order to compare our 
methods with other methods under the same recall 
value, we conduct another experiment on the type 
<verb, OBJ, noun>4. We set the recalls of the two 
methods to the same value of our method, which is 
0.6396 in Table 4. The precisions are 0.3190, 
0.4922, and 0.6811 for Method 1, Method 2, and 
our method, respectively. Thus, the precisions of 
our approach are higher than the other two methods 
even when their recalls are the same. It proves that 
our method of using translation information to 
select the candidates is effective for synonymous 
collocation extraction. 
The results of Method 1 show that it is difficult 
to extract synonymous collocations with monolin-
gual contexts. Although Method 1 gets higher re-
calls than the other methods, it brings a large 
number of wrong candidates, which results in lower 
precision. If we set higher thresholds to get com-
parable precision, the recall is much lower than that 
of our approach. This indicates that the contexts of 
collocations are not discriminative to extract syn-
onymous collocations.   
The results also show that Model 2 is not suit-
able for the task. The main reason is that both high 
scores of ),( 2111 eesim and ),( 2212 eesim  does not mean 
the high similarity of the two collocations.  
The reason that our method outperforms the 
other two methods is that when one collocation is 
translated into another language, its translations 
indirectly disambiguate the words? senses in the 
collocation. For example, the probability of <turn 
on, OBJ, light> being translated into < , OBJ, 
> (<da3 kai1, OBJ, deng1>) is much higher than 
that of it being translated into < , OBJ, 
> (<qu3 jue2 yu2, OBJ, guang1 zhao4 du4>) while 
the situation is reversed for <depend on, OBJ, il-
lumination>. Thus, the similarity between <turn on, 
OBJ, light> and <depend on, OBJ, illumination> is 
low and, therefore, this candidate is filtered out. 
 
                                                     
4
 The results of the other two types of collocations are the 
same as <verb, OBJ, noun>. We omit them because of 
the space limit. 
3.2 Comparison with Methods using 
Bilingual Corpora 
Barzilay and Mckeown (2001), and Shimohata and 
Sumita (2002) used a bilingual corpus to extract 
synonymous expressions. If the same source ex-
pression has more than one different translation in 
the second language, these different translations are 
extracted as synonymous expressions. In order to 
compare our method with these methods that only 
use a bilingual corpus, we implement a method that 
is similar to the above two studies. The detail proc-
ess is described in Method 3. 
Method 3: The method is described as follows: 
(1) All the source and target sentences (here Chi-
nese and English, respectively) are parsed; (2) 
extract the Chinese and English collocations in the 
bilingual corpus; (3) align Chinese collocations 
ccol=<c1, rc, c2> and English collocations ecol=<e1, re, 
e2> if c1 is aligned with e1 and c2  is aligned with e2; 
(4) obtain two English synonymous collocations if 
two different English collocations are aligned with 
the same Chinese collocation and if they occur more 
than once in the corpus. 
The training bilingual corpus is the same one 
described in Section 2. With Method 3, we get 
9,368 synonymous collocation pairs in total. The 
number is only 10% of that extracted by our ap-
proach, which extracts 93,523 pairs with the same 
bilingual corpus. In order to evaluate Method 3 and 
our approach on the same test set. We randomly 
select 100 collocations which have synonymous 
collocations in the bilingual corpus. For these 100 
collocations, Method 3 extracts 121 synonymous 
collocation pairs, where 83% (100 among 121) are 
correct 5.  Our method described in Section 2 gen-
erates 556 synonymous collocation pairs with a 
threshold set in the above section, where 75% (417 
among 556) are correct. 
 If we set a higher threshold (0.08) for our 
method, we get 360 pairs where 295 are correct 
(82%). If we use |A|, |B|, |C| to denote correct pairs 
extracted by Method 3, our method, both Method 3 
and our method respectively, we get |A|=100, 
|B|=295, and 78|||| =?= BAC . Thus, the syn-
onymous collocation pairs extracted by our method 
cover 78% ( |||| AC ) of those extracted by Method 
                                                     
5
 These synonymous collocation pairs are evaluated by 
two judges and only those agreed on by both are selected 
as correct pairs. 
3 while those extracted by Method 3 only cover 
26% ( |||| BC ) of those extracted by our method. 
It can be seen that the coverage of Method 3 is 
much lower than that of our method even when their 
precisions are set to the same value. This is mainly 
because Method 3 can only extract synonymous 
collocations which occur in the bilingual corpus. In 
contrast, our method uses the bilingual corpus to 
train the translation probabilities, where the trans-
lations are not necessary to occur in the bilingual 
corpus. The advantage of our method is that it can 
extract synonymous collocations not occurring in 
the bilingual corpus. 
4 Conclusions and Future Work 
This paper proposes a novel method to automati-
cally extract synonymous collocations by using 
translation information. Our contribution is that, 
given a large monolingual corpus and a very limited 
bilingual corpus, we can make full use of these 
resources to get an optimal compromise of preci-
sion and recall. Especially, with a small bilingual 
corpus, a statistical translation model is trained for 
the translations of synonymous collocation candi-
dates. The translation information is used to select 
synonymous collocation pairs from the candidates 
obtained with a monolingual corpus. Experimental 
results indicate that our approach extracts syn-
onymous collocations with an average precision of 
74% and recall of 64%. This result significantly 
outperforms those of the methods that only use 
monolingual corpora, and that only use a bilingual 
corpus.  
Our future work will extend synonymous ex-
pressions of the collocations to words and patterns 
besides collocations. In addition, we are also inter-
ested in extending this method to the extraction of 
synonymous words so that ?black? and ?white?, 
?dog? and ?cat? can be classified into different 
synsets.  
Acknowledgements 
We thank Jianyun Nie, Dekang Lin, Jianfeng Gao, 
Changning Huang, and Ashley Chang for their 
valuable comments on an early draft of this paper.  
References 
Barzilay R. and McKeown K. (2001). Extracting Para-
phrases from a Parallel Corpus. In Proc. of 
ACL/EACL. 
Brown P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. 
Mercer (1993). The mathematics of statistical machine 
translation: Parameter estimation. Computational 
Linguistics, 19(2), pp263- 311. 
Carolyn J. Crouch and Bokyung Yang (1992). Experi-
ments in automatic statistical thesaurus construction. 
In Proc. of the Fifteenth Annual International ACM 
SIGIR conference on Research and Development in 
Information Retrieval, pp77-88. 
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha 
Blair-Goldensohn, Zhu Zhang, Waiguo Fan, and John 
Prager (2001). Mining the web for answers to natural 
language questions. In ACM CIKM 2001: Tenth In-
ternational Conference on Information and Knowledge 
Management, Atlanta, GA. 
Fung P. and Mckeown K. (1997). A Technical Word- and 
Term- Translation Aid Using Noisy Parallel Corpora 
across Language Groups. In: Machine Translation, 
Vol.1-2 (special issue), pp53-87.  
Gasperin C., Gamallo P, Agustini A., Lopes G., and Vera 
de Lima  (2001) Using Syntactic Contexts for Meas-
uring Word Similarity. Workshop on Knowledge 
Acquisition & Categorization, ESSLLI.  
Grefenstette G. (1994) Explorations in Automatic The-
saurus Discovery. Kluwer Academic Press, Boston. 
Kiyota Y., Kurohashi S., and Kido F. (2002) "Dialog 
Navigator":  A Question Answering System based on 
Large Text Knowledge Base.  In Proc. of the 19th In-
ternational Conference on Computational Linguistics, 
Taiwan. 
Koehn. P and Knight K. (2000). Estimating Word 
Translation Probabilities from Unrelated Monolin-
gual Corpora using the EM Algorithm. National 
Conference on Artificial Intelligence (AAAI 2000) 
Langkilde I. and Knight K. (1998). Generation that 
Exploits Corpus-based Statistical Knowledge. In Proc. 
of the COLING-ACL 1998. 
Lin D. (1998) Automatic Retrieval and Clustering of 
Similar Words. In Proc. of the 36th Annual Meeting of 
the Association for Computational Linguistics. 
Shimohata M. and Sumita E.(2002). Automatic Para-
phrasing Based on Parallel Corpus for Normalization. 
In Proc. of the Third International Conference on 
Language Resources and Evaluation. 
Wang W., Huang J., Zhou M., and Huang C.N. (2001). 
Finding Target Language Correspondence for Lexi-
calized EBMT System. In Proc. of the Sixth Natural 
Language Processing Pacific Rim Symposium. 
Zhou M., Ding Y., and Huang C.N. (2001). Improving 
Translation Selection with a New Translation Model 
Trained by Independent Monolingual Corpora. 
Computational Linguistics & Chinese Language 
Processing. Vol. 6 No, 1, pp1-26. 
Optimizing Synonym Extraction Using Monolingual and Bilingual 
Resources 
Hua WU, Ming ZHOU 
Microsoft Research Asia 
5F Sigma Center, No.49 Zhichun Road, Haidian District 
Beijing, 100080, China 
wu_hua_@msn.com, mingzhou@microsoft.com 
Abstract 
Automatically acquiring synonymous words 
(synonyms) from corpora is a challenging task. 
For this task, methods that use only one kind 
of resources are inadequate because of low 
precision or low recall. To improve the per-
formance of synonym extraction, we propose 
a method to extract synonyms with multiple 
resources including a monolingual dictionary, 
a bilingual corpus, and a large monolingual 
corpus. This approach uses an ensemble to 
combine the synonyms extracted by individ-
ual extractors which use the three resources. 
Experimental results prove that the three re-
sources are complementary to each other on 
synonym extraction, and that the ensemble 
method we used is very effective to improve 
both precisions and recalls of extracted 
synonyms. 
1 Introduction 
This paper addresses the problem of extracting 
synonymous English words (synonyms) from 
multiple resources: a monolingual dictionary, a 
parallel bilingual corpus, and a monolingual cor-
pus. The extracted synonyms can be used in a 
number of NLP applications. In information re-
trieval and question answering, the synonymous 
words are employed to bridge the expressions 
gaps between the query space and the document 
space (Mandala et al, 1999; Radev et al, 2001; 
Kiyota et al, 2002). In automatic text summari-
zation, synonymous words are employed to iden-
tify repetitive information in order to avoid re-
dundant contents in a summary (Barzilay and 
Elhadad, 1997). In language generation, syno-
nyms are employed to create more varied texts 
(Langkilde and Knight, 1998).  
Up to our knowledge, there are few studies in-
vestigating the combination of different resources 
for synonym extraction. However, many studies 
investigate synonym extraction from only one 
resource. The most frequently used resource for 
synonym extraction is large monolingual corpora 
(Hindle, 1990; Crouch and Yang, 1992; Grefen-
statte, 1994; Park and Choi, 1997; Gasperin et al, 
2001 and Lin, 1998). The methods used the con-
texts around the investigated words to discover 
synonyms. The problem of the methods is that the 
precision of the extracted synonymous words is 
low because it extracts many word pairs such as 
?cat? and ?dog?, which are similar but not syn-
onymous.  
Other resources are also used for synonym ex-
traction. Barzilay and Mckeown (2001), and Shi-
mohata and Sumita (2002) used bilingual corpora 
to extract synonyms. However, these methods can 
only extract synonyms which occur in the bilingual 
corpus. Thus, the extracted synonyms are limited. 
Besides, Blondel and Sennelart (2002) used mono-
lingual dictionaries to extract synonyms. Although 
the precision of this method is high, the coverage is 
low because the result of this method heavily de-
pends on the definitions of words. 
In order to improve the performance of syno-
nym extraction, Curran (2002) used an ensemble 
method to combine the results of different methods 
using a monolingual corpus. Although Curran 
(2002) showed that the ensemble extractors out-
performed the individual extractors, it still cannot 
overcome the deficiency of the methods using the 
monolingual corpus. 
To overcome the deficiencies of the methods 
using only one resource, our approach combines 
both monolingual and bilingual resources to auto-
matically extract synonymous words. By combin-
ing the synonyms extracted by the individual ex-
tractors using the three resources, our approach can 
combine the merits of the individual extractors to 
improve the performance of synonym extraction. 
In fact, our approach can be considered as an 
ensemble of different resources for synonym 
extraction. Experimental results prove that the 
three resources are complementary to each other 
on synonym extraction, and that the ensemble 
method we used is very effective to improve both 
precisions and recalls of extracted synonyms. 
The remainder of this paper is organized as 
follows. The next section presents our approach 
for synonym extraction. Section 3 describes an 
implementation of the three individual extractors. 
Section 4 presents the evaluation results. Section 5 
discusses our method. In the last section, we draw 
the conclusions of this work. 
2 Our Approach 
Instead of using only one kind of resource to 
extract synonyms, we combine both monolingual 
and bilingual resources for synonym extraction. 
The resources include a monolingual dictionary, 
an English-Chinese bilingual corpus, and a large 
corpus of monolingual documents. Before com-
bining them, we first propose three methods to 
extract synonyms from the three resources. Espe-
cially, a novel method is proposed to increase the 
coverage of the extracted synonyms using the 
bilingual corpus. Next, we develop an ensemble 
method to combine the individual extractors. The 
advantage of our approach is that it can combine 
the merits of the individual extractors to improve 
the precision and recalls of the extracted syno-
nyms. 
2.1 Synonym Extraction with a Monolin-
gual Dictionary 
This section proposes a method to extract syno-
nyms from a monolingual dictionary. In a mono-
lingual dictionary, each entry is defined by other 
words and may also be used in the definitions for 
other words. For a word in the dictionary, the 
words used to define it are called hubs and the 
words whose definitions include this word are 
called authorities as in (Blondel and Sennelart, 
2002). We use the hubs and authorities of a word 
to represent its meaning. The assumption behind 
this method is that two words are similar if they 
have common hubs and authorities. In this paper, 
we only use content words as members of hubs and 
authorities. 
We take these hubs and authorities as features of 
a word. The vector constructed with them is re-
ferred to as the feature vector of a word. The simi-
larity between two words is calculated through 
their feature vectors with the cosine measure as 
shown in Equation (1).  


=
=
=
j
j
i
i
ww
ji
vv
vv
FFwwsim
ji
2
2
2
1
21
21211
*
)*(
),cos(),(
21      
(1) 
where 
 ),( ... ),,( ),,( 2211 >=< imimiiiii vwvwvwF  
Fi is the feature vector of wi; 
1=ijv if word ijw is a hub or an authority of the 
word wi; else, 0=ijv ; 
2.2 Synonym Extraction with a Bilingual 
Corpus 
This section proposes a novel method to extract 
synonyms from a bilingual corpus. It uses the 
translations of a word to express its meaning. The 
assumption of this method is that two words are 
synonymous if their translations are similar.  
Given an English word, we get their translations 
with an English-Chinese bilingual dictionary. Each 
translation is assigned a translation probability, 
which is trained with a bilingual English-Chinese 
corpus based on the result of word alignment. The 
aligner use the model described in (Wang et al, 
2001). In order to deal with the problem of data 
sparseness, we conduct a simple smoothing by 
adding 0.5 to the counts of each translation pair as 
in (2).  
|_|*5.0)(
5.0),()|(
ctransecount
eccount
ecp
+
+
=       (2) 
where 
),( eccount  represents the co-occurring fre-
quency of the Chinese word c and the English 
word e in the sentence pairs. 
)(ecount  represents the frequency of the English 
word e occurring in the bilingual corpus. 
|_| ctrans  represents the number of Chinese 
translations for a given English word e.  
The translations and the translation probabili-
ties of a word are used to construct its feature 
vector. The similarity of two words is estimated 
through their feature vectors with the cosine 
measure as shown in (3).    


=
=
=
j
j
i
i
cc
ji
pp
pp
FFwwsim
ji
2
2
2
1
21
21212
*
)*(
),cos(),(
21              (3) 
where  
 ),( ... ),,( ),,( 2211 >=< imimiiiii pcpcpcF  
Fi is the feature vector of wi; 
ijc is the jth Chinese translation of the word wi; 
ijp is the translation probability of the word wi 
is translated into ijc  
For example, the feature vectors of two words 
? abandon?  and ? forsake?  are: 
forsake: < ( , 0.1333),  ( , 0.1333),  ( , 
0.0667) ( , 0.0667), ( , 0.0667), ?> 
abandon:  <( , 0.3018), ( , 0.1126), ( , 
0.0405), ( , 0.0225), ( , 0.0135),?>  
2.3 Synonym Extraction with a Monolin-
gual Corpus 
The context method described in Section 1 is also 
used for synonym extraction from large mono-
lingual corpora of documents. This method relies 
on the assumption that synonymous words tend to 
have similar contexts. In this paper, we use the 
words which have dependency relationships with 
the investigated word as contexts. The contexts are 
obtained by parsing the monolingual documents. 
The parsing results are represented by dependency 
triples which are denoted as <w1, Relation Type, 
w2>. For example, the sentence ? I declined the 
invitation?  is transformed into three triples after 
parsing: <decline, SUBJ, I>, <decline, OBJ, invi-
tation> and <invitation, DET, the>.  If we name 
<Relation Type, w2> as an attribute of the word w1,  
the verb ? decline?  in the above sentence has two 
attributes <OBJ, invitation> and <SUBJ, I> . Thus, 
the contexts of a word can be expressed using its 
attributes. In this case, two words are synonymous 
if they have similar attributes. 
We use a weighted version of the Dice measure 
to calculate the similarity of two words.  
),(),(
)),(),((
),(
2)2(
1)1(
21)2()1(
213
j
wAjatt
i
wAiatt
kk
wAwAkatt
attwWattwW
attwWattwW
wwsim
??
?
+
+
=

   (4) 
where  
kji attattatt  , ,  stands for  attributes of  words. 
),( ji attwW indicates the association strength 
between the attribute attj with the word iw . 
)( iwA denotes the attribute set of the word iw . 
The measure used to measure the association 
strength between a word and its attributes is 
weighted mutual information (WMI) (Fung and 
Mckeown, 1997) as described in (5). 
)()(
),(
log*),(
),(),(
ji
ji
ji
jiji
attpwp
attwp
attwp
attwWMIattwW
?
=
=
                 (5) 
where 
N
wcount
wp ii
,*,*)()( =
 
N
wrcount
attp j
),(*,)( =
, 
),( wratt j =
 
),(*, wrcount : frequency of the triples having 
dependency relation r with the word w. 
,*,*)( iwcount : frequency of the triples including 
word iw . 
N: number of triples in the corpus. 
We use it instead of point-wise mutual information 
in Lin (1998) because the latter tends to overesti-
mate the association between two parts with low 
frequencies. Weighted mutual information melio-
rates this effect by adding ),( ji attwp .  
2.4 Combining the Three Extractors 
In terms of combining the outputs of the different 
methods, the ensemble method is a good candidate. 
Originally, the ensemble method is a machine 
learning technique of combining the outputs of 
several classifiers to improve the classification 
performance (Dietterich, 2000). It has been suc-
cessfully used in many NLP tasks. For example, 
(Curran, 2002) proved that the ensembles of indi-
vidual extractors using different contexts in the 
monolingual corpus improve the performance of 
synonym extraction.  
 In fact, we can consider the extractors in the 
previous sections as binary classifiers. Thus, we 
use the ensemble method to combine the output of 
the individual extractors described in the previous 
sections for synonym extraction. The method is 
described in Equation (6). 
)),((),(
3
1
2121 
=
?=
i
ii wwsimawwsim             (6) 
where 
 3) 2, 1,(i ),( 21 =wwsim i stands for the different 
similarity measure using different resources 
described in the previous sections. 
)1 and ,3 ,2 ,1(
i
 == ii aia is the weight for the 
individual extractors. 
The reasons that we use the weighted ensemble 
method are as follows: (1) If the majority of three 
extractors select the same word as a synonym of a 
investigated word, it tend to be a real synonym. 
This method can ensure it has a high similarity 
score. Thus, it will improve the precision of the 
extracted synonyms. (2) With this method, it can 
improve the coverage of the extracted synonyms. 
This is because if the similarity score of a candi-
date with the investigated word is higher than a 
threshold, our method can select the candidate as a 
synonym even though it is only suggested by one 
extractor.  
3 Implementation of Individual 
Extractors 
For the extractor employing a monolingual dic-
tionary, we use the same online dictionary as in 
(Blondel and Sennelart, 2002), which is named the 
Online Plain Text Dictionary. The dictionary 
consists of 27 HTML files, which is available 
from the web site http://www.gutenberg.net/. With 
the method described in Section 2.1, the result for 
the extracted synonyms is shown in Table 1 when 
the similarity threshold is set to 0.04. An example 
is shown as follows: 
acclimatize: 
 (acclimate, 0.1481;  habituate, 0.0976) 
The numbers in the example are the similarity 
scores of two words.  
Table 1. Synonyms Extracted from the Monolingual 
Dictionary 
Category # Entries # Average 
Synonyms 
Noun 16963 4.7 
Verb 5084 7.1 
For synonym extraction from the bilingual 
corpus, we use an English-Chinese lexicon, which 
includes 219,404 English words with each source 
word having 3 translations on average. The word 
translation probabilities are estimated from a bi-
lingual corpus that obtains 170,025 pairs of Chi-
nese-English sentences, including about 2.1 million 
English words and about 2.5 million Chinese words. 
With the method described in Section 2.2, we 
extracted synonyms as shown in Table 2 when the 
similarity threshold is set to 0.04. 
Table 2. Synonyms Extracted from the Bilingual 
corpus 
Category #Entries #Average 
Synonyms 
Noun 26253 10.2 
Verb 7364 14.8 
For synonym extraction from a monolingual 
corpus, we use the Wall Street Journal from 1987 to 
1992, the size of which is about 500M bytes. In 
order to get contexts of words, we parse the corpus 
with an English parser ?NLPWIN 1 . From the 
parsing results, we extracted the following four 
types of dependency triples. 
(a) <verb, OBJ, noun> 
(b) <verb, SUBJ, noun> 
(c) <noun, ATTRIB, adjective> 
(d)  <verb, MODS, adjunct> 
The statistics are shown in Table 3. Token 
means the total number of triples in the triple set 
and type means a unique instance of triple in the 
corpus. These triples are used as contexts of words 
to calculate the similarity between words as de-
scribed in Section 2.3. The result is shown in Table 
4 when the similarity threshold is set to 0.1. 
                                                     
1
 The NLPWIN parser is developed at Microsoft Re-
search. Its output can be a phrase structure parse tree or a 
logical form which is represented with dependency 
triples. 
Table 3. Statistics for Triples 
 # Token # Type 
OBJ 7,041,382 1,487,543 
SUBJ 7,180,572 2,116,761 
ATTRIB 4,976,822 1,393,188 
MODS 3,557,737 94,004 
Total 22,756,512 5,937,496 
Table 4. Synonyms Extracted from the Monolingual 
Corpus 
Category Entries Average 
Synonyms 
Noun 16963 4.6 
Verb 5084 7.1 
4 Evaluation  
4.1 The Gold Standard 
The simplest evaluation measure is direct com-
parison of the extracted synonyms with the manu-
ally created thesaurus. However, the thesaurus 
coverage is a problem. In this paper, we combined 
two thesauri as a gold stardard: WordNet 1.6 
http://www.cogsci.princeton.edu/~wn/) and Roget 
(Roget?s II: The New Thesaurus, 1995. 
http://www.bartleby.com/thesauri/).  
In WordNet, one synset consists of several 
synonyms which represent a single sense. There-
fore, a polysemous word occurs in more than one 
synsets. For example, the polysemous word 
? abandon?  occur in five different synsets: 
(abandon,  forsake,  desolate,  desert,  lurch)  
(vacate,  empty,  abandon)  
(abandon,  give up, give) 
(abandon,  give up) 
(abandon) 
For a given word, we combine its synonyms from 
all synsets including the word. Thus, we get the 
synonyms of the word ? abandon?  as follows: 
abandon forsake, desolate, desert, lurch, vacate, 
empty, give up, give 
For synonyms in Roget, we also combine the 
synonyms in different synsets into one set as we 
do for WordNet. Thus, we get the synonyms of the 
word ? abandon? as follows: 
abandonbreak off, desist, discontinue, give up, leave 
off, quit, relinquish, remit, stop, desert, forsake, leave, 
throw over, abdicate, cede, demit, forswear, hand over, 
quitclaim, render, renounce, resign, surrender, waive, 
yield, give over, forgo, lay down  
Combining the results of WordNet and Roget, 
we can get the synonyms of the word ? abandon?  as 
follows. 
abandon desolate, lurch, vacate, empty, give, abdicate, 
break off, cede, demit, desert, desist, discontinue, forgo, 
forsake, forswear, give up, give over, hand over, lay 
down, lay off, leave off, leave, quit, quitclaim, relinquish, 
remit, stop, swear off, throw over, render, renounce, 
resign, surrender, waive, yield 
4.2 Evaluation Measures 
The evaluation metrics are precision, recall, and 
f-measure. If we use S to indicate the synonyms 
that our method extracts for a word and GS  to 
denote the synonyms of the word in WordNet and 
Roget, the methods to calculate the precision, recall, 
and f-measure of our methods are shown in Equa-
tion (7), (8), and (9). To investigate the results of 
more than one word, we calculate the average 
precision, recall and f-measure, which sum the 
individual values divided by the number of the 
investigated words. 
|S|
|SS| G?
=precision          (7) 
|S|
 |SS|
G
G?
=recall        (8) 
recallprecision
recallprecision2
measure-f
+
??
=
 
(9) 
4.3 Test Set 
In order to evaluate our methods, we build up a test 
set which includes three parts: 
(a) high-frequency words: occurring more than 
100 times;  
(b) middle-frequency words: occurring more than 
10 times and not greater than 100 times; 
(c) low-frequency words: occurring no greater 
than 10 times. 
Table 5. Statistics for Nouns and Verbs 
 High Fre-
quency 
Middle 
Frequency 
Low 
Frequency 
Noun 600 2000 1000 
Verb 340 1300 800 
The frequency counts are estimated from Wall 
Street Journal (1987-1992), from which we ran-
domly extracted 3600 nouns and 2440 verbs. These  
Table 6. Evaluation Results for Nouns 
 High-Frequency Nouns Middle-Frequency Nouns Low-Frequency Nouns 
 Pre Rec F Pre Rec F Pre Rec F 
1 0.174 0.140 0.155 0.212 0.137 0.167 0.198 0.119 0.149 
2 0.225 0.209 0.217 0.242 0.212 0.226 0.207 0.212 0.209 
3 0.118 0.109 0.114 0.117 0.104 0.109 0.099 0.096 0.098 
1+2+3 0.240 0.201 0.219 0.271 0.220 0.243 0.222 0.232 0.227 
Table 7. Evaluation Results for Verbs 
 High-Frequency Verbs Middle-Frequency Verbs Low-Frequency Verbs 
 Pre Rec F Pre Rec F Pre Rec F 
1 0.228 0.243 0.235 0.272 0.233 0.251 0.209 0.216 0.212 
2 0.226 0.312 0.262 0.224 0.292 0.253 0.184 0.275 0.220 
3 0.143 0.166 0.154 0.162 0.127 0.142 0.128 0.135 0.132 
1+2+3 0.295 0.323 0.308 0.311 0.304 0.307 0.238 0.302 0.266 
Note: 1, 2, and 3 represent the extractor using the monolingual dictionary, the bilingual corpus, and the monolingual 
corpus respectively. The symbols ? Pre? , ? Rec? , and ? F?  represent precision, recall, and f-measure scores. 
 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5
Recall
Pr
ec
isi
o
n
2 1 3 1+2+3
 
Figure 1. Recall-Precision curves for nouns 
 
Figure 2. Recall-Precision curves for verbs 
 
words have synonyms both in our results extracted 
from the three resources and in the thesauri 
WordNet and Roget. The statistics of the test set 
are shown in Table 5. 
4.4 Experimental  Results 
In this section, we compare the extracted syno-
nyms of the nouns and verbs in the test set with 
those in WordNet and Roget. For each method, we 
select those as synonyms whose similarity scores 
with the investigated word are larger than a given 
threshold. A development set is used to determine 
the thresholds of each method. The thresholds for 
getting highest f-measure scores on the develop-
ment set are selected. In our experiments, we get 
0.04, 0.04, 0.1 and 0.04 for Method 1, Method 2, 
Method 3 and the combined approach, respectively. 
The evaluation results for the individual extractors 
and the ensemble extractor are shown in Table 6 
and Table 7.  We set a1=0.4, a2=0.4 and a3=0.2 in 
Equation (6) for the ensemble to combine the re-
sults from the three resources. The weights are also 
obtained with the development set. 
In order to examine the performance of each 
method in more details, we also get the precisions 
and recalls under different thresholds. Figure 1 and 
Figure 2 shows the precision values under different 
recall values (different thresholds) for all nouns and 
verbs, respectively. 
Among all of the methods, the method com-
bining all of the three resources gets the best results 
in terms of both precision and recall. The effect is 
similar to the ensemble methods for synonym 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall 
Pr
ec
isi
o
n
 
2 1 3 1+2+3 
extraction in (Curran 2002). However, our method 
uses an ensemble of different resources instead of 
one single resource. During the experiments, we 
also find the ensemble combining all of the three 
extractors outperforms the ensembles only com-
bining any two of the three extractors. This indi-
cates that the extractors using the three different 
resources are complementary to each other. For 
example, the extractor using the monolingual 
dictionary gets a high precision and the extractor 
using the bilingual corpus gets a high recall. Al-
though the extractor using the monolingual corpus 
achieved much lower precision and recall on 
synonym extraction, it is still useful to be included 
in the ensemble.  This shows that the monolingual 
corpus is complementary to the other two re-
sources on synonym extraction.  The success of 
our method also indicates that our ensemble 
method by weighting all extractors is effective for 
synonym extraction. 
Among the methods only using one kind of 
resource, Method 2, which uses the bilingual 
corpus, has the highest f-measure scores on both 
nouns and verbs. From the results in Figure 1 and 
Figure 2, we can see that the coverage of syno-
nyms extracted by Method 2 is the highest. Al-
though it has lower precisions than Method 1 
under low recalls, its precisions are higher than 
those of Method 1 under higher recalls. This 
shows that Method 2 can get a good compromise 
between precision and recall. We also note that the 
maximum recall of Method 2 is much larger than 
that of Method 1. This is because (1) in Method 1, 
the words used in the definitions are highly limited. 
Thus, the coverage of the synonyms is limited; (2) 
the advantage of Method 2 is that the coverage of 
extracted synonyms is high because it can extract 
the synonyms not occurring in the corpus. It is 
different from the method in (Barzilay and 
Mckeown, 2001; Shimohata and Sumita, 2002), 
which can only extract the synonyms in the bi-
lingual corpus. 
The performance of Method 3 is the worst. It is 
caused by two factors: (1) the context model of 
Method 3 introduces much noise because of the 
errors of the parser; (2) this method is unable to 
distinguish synonyms, antonyms, and similar 
words because they tend to have similar contexts. 
From the contexts it uses, method 3 is suitable to 
extract related words which have the similar us-
ages from the view of syntax. 
5 Discussions 
This paper uses three different methods and re-
sources for synonym extraction. By using the cor-
pus-based method, we can get some synonyms or 
near synonyms which can not be found in the 
hand-built thesauri. For Example: ? handspring  
handstand? , ? audiology   otology? , ? roisterer 
 carouser?  and ? parmesan  gouda? . These 
kinds of synonyms are difficult for hand-built 
thesauri to cover because they occur too infrequent 
to be caught by humans. In addition, this cor-
pus-based method can get synonyms in specific 
domains while the general thesauri don?t provide 
such fine-grained knowledge. 
Comparing the results with the human-built 
thesauri is not the best way to evaluate synonym 
extraction because the coverage of the human-built 
thesaurus is also limited. However, manually 
evaluating the results is time consuming. And it 
also cannot get the precise evaluation of the ex-
tracted synonyms. Although the human-built 
thesauri cannot help to precisely evaluate the re-
sults, they can still be used to detect the effective-
ness of extraction methods. 
Conclusion 
This paper proposes a new method to extract 
synonyms from three resources: a monolingual 
dictionary, a bilingual corpus, and a large mono-
lingual corpus. This method uses a weighted en-
semble to combine all of the results of the indi-
vidual extractors using one of the three resources 
respectively. Experimental results prove that the 
three resources are complementary to each other on 
synonym extraction, and that the ensemble method 
we used is very effective to improve both preci-
sions and recalls when the results are compared 
with the manually-built thesauri WordNet and 
Roget.  
Further, we also propose a new method to ex-
tract synonyms from a bilingual corpus. This 
method uses the translations of a word to represent 
its meaning. The translation probabilities are 
trained with the bilingual corpus. The advantage of 
this method is that it can improve the coverage of 
the extracted synonyms. Experiments indicate that 
this method outperforms the other methods using a 
monolingual corpus or a monolingual dictionary.  
The contribution of this work lies in three as-
pects: (1) develop a method to combine the results 
of individual extractors using the three resources 
on synonym extraction; (2) investigate the per-
formance of the three extraction methods using 
different resources, exposing the merits and de-
merits of each method; (3) propose a new method 
to extract synonyms from a bilingual corpus, 
which greatly improves the coverage of the ex-
tracted synonyms.  
References  
Barzilay R. and Elhadad M. 1997. Using lexical chains 
for text summarization. In proceedings of the ACL 
Workshop on Intelligent Scalable Text Summariza-
tion, pp10-17. 
Barzilay R. and McKeown K. 2001. Extracting Para-
phrases from a Parallel Corpus. In Proc. of 
ACL/EACL. 
Blondel V. D. and Sennelart P. 2002. Automatic ex-
traction of synonyms in a dictionary. In Proc. of the 
SIAM Workshop on Text Mining. 
Crouch C. J. and Yang B. 1992. Experiments in auto-
matic statistical thesaurus construction. In Proc. of 
the 15th Annual International ACM SIGIR confer-
ence on Research and Development in Information 
Retrieval, pp77-88. 
Curran J. 2002 Ensemble Methods for Automatic The-
saurus Extraction. In Proc. of the Conference on 
Empirical Methods in Natural Language Processing. 
pp. 222-229. 
Dietterich T. 2000. Ensemble Methods in Machine 
Learning. In Proc. of the First International Work-
shop on Multiple Classier Systems. pp 1-15. 
Fung P., Mckeown K. 1997. A Technical Word- and 
Term- Translation Aid Using Noisy Parallel Corpora 
across Language Groups. In: Machine Translation, 
Vol.1-2 (special issue), pp53-87. 
Gasperin C., Gamallo P., Agustini A., Lopes G., Lima 
V. 2001 Using Syntactic Contexts for Measuring 
Word Similarity. Workshop on Knowledge Acquisi-
tion & Categorization, ESSLLI.  
Grefenstette G. 1994 Explorations in Automatic The-
saurus Discovery. Kluwer Academic Press. 
Hindle D. 1990. Noun Classification from Predi-
cate-Argument Structure. In Proc. of the 28th Annual 
Meeting of the Association for Computational Lin-
guistics. 
Kiyota Y., Kurohashi S., Kido F. 2002. "Dialog Navi-
gator":  A Question Answering System Based on 
Large Text Knowledge Base.  In Proc. of the 19th 
International Conference on Computational Linguis-
tics. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Proc. of 
the COLING-ACL. 
Lin D. 1998 Automatic Retrieval and Clustering of 
Similar Words. In Proc. of the 36th Annual Meeting of 
the Association for Computational Linguistics. 
Mandala R., Tokunaga T. Tanaka H. 1999. Combining 
Multiple Evidence from Different Type of Thesaurus 
for Query Expansion. In Proc. of the 22nd annual in-
ternational ACM SIGIR conference on Research and 
development in information retrieval. 
Park Y.C. and Choi K. S. 1997. Automatic Thesaurus 
Construction Using Baysian Networks.  Information 
Processing & Management. Vol. 32. 
Radev D., Qi H., Zheng Z., Goldensohn S., Zhang Z., 
Fan W., Prager J. 2001. Mining the Web for Answers 
to Natural Language Questions. In the Tenth Interna-
tional ACM Conference on Information and Knowl-
edge Management. 
Shimohata M. and Sumita E. 2002. Automatic Para-
phrasing Based on Parallel Corpus for Normalization. 
In Proc. of the Third International Conference on 
Language Resources and Evaluation. 
Wang W., Huang J., Zhou M., Huang C.N. 2001. Find-
ing Target Language Correspondence for Lexicalized 
EBMT System. In Proc. of the 6th Natural Language 
Processing Pacific Rim Symposium. 
Improving Statistical Word Alignment with a Rule-Based Machine 
Translation System 
WU Hua, WANG Haifeng  
Toshiba (China) Research & Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, China, 100738 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
 
Abstract 
The main problems of statistical word alignment 
lie in the facts that source words can only be 
aligned to one target word, and that the inappro-
priate target word is selected because of data 
sparseness problem. This paper proposes an ap-
proach to improve statistical word alignment 
with a rule-based translation system. This ap-
proach first uses IBM statistical translation 
model to perform alignment in both directions 
(source to target and target to source), and then 
uses the translation information in the rule-based 
machine translation system to improve the statis-
tical word alignment. The improved alignments 
allow the word(s) in the source language to be 
aligned to one or more words in the target lan-
guage. Experimental results show a significant 
improvement in precision and recall of word 
alignment. 
1 Introduction 
                                                          
Bilingual word alignment is first introduced as an 
intermediate result in statistical machine transla-
tion (SMT) (Brown et al 1993). Besides being 
used in SMT, it is also used in translation lexicon 
building (Melamed 1996), transfer rule learning 
(Menezes and Richardson 2001), example-based 
machine translation (Somers 1999), etc. In previ-
ous alignment methods, some researches mod-
eled the alignments as hidden parameters in a 
statistical translation model (Brown et al 1993; 
Och and Ney 2000) or directly modeled them 
given the sentence pairs (Cherry and Lin 2003). 
Some researchers used similarity and association 
measures to build alignment links (Ahrenberg et 
al. 1998; Tufis and Barbu 2002). In addition, Wu 
(1997) used a stochastic inversion transduction 
grammar to simultaneously parse the sentence 
pairs to get the word or phrase alignments. 
Generally speaking, there are four cases in 
word alignment: word to word alignment, word 
to multi-word alignment, multi-word to word 
alignment, and multi-word to multi-word align-
ment. One of the most difficult tasks in word 
alignment is to find out the alignments that in-
clude multi-word units. For example, the statisti-
cal word alignment in IBM translation models 
(Brown et al 1993) can only handle word to 
word and multi-word to word alignments.  
Some studies have been made to tackle this 
problem. Och and Ney (2000) performed transla-
tion in both directions (source to target and target 
to source) to extend word alignments. Their re-
sults showed that this method improved precision 
without loss of recall in English to German align-
ments. However, if the same unit is aligned to 
two different target units, this method is unlikely 
to make a selection. Some researchers used 
preprocessing steps to identity multi-word units 
for word alignment (Ahrenberg et al 1998; 
Tiedemann 1999; Melamed 2000). The methods 
obtained multi-word candidates based on con-
tinuous N-gram statistics. The main limitation of 
these methods is that they cannot handle sepa-
rated phrases and multi-word units in low fre-
quencies. 
In order to handle all of the four cases in word 
alignment, our approach uses both the alignment 
information in statistical translation models and 
translation information in a rule-based machine 
translation system. It includes three steps. (1) A 
statistical translation model is employed to per-
form word alignment in two directions1 (English 
to Chinese, Chinese to English). (2) A rule-based 
English to Chinese translation system is em-
ployed to obtain Chinese translations for each 
English word or phrase in the source language. (3) 
The translation information in step (2) is used to 
improve the word alignment results in step (1).  
A critical reader may pose the question ?why 
1 We use English-Chinese word alignment as a case study.  
not use a translation dictionary to improve statis-
tical word alignment?? Compared with a transla-
tion dictionary, the advantages of a rule-based 
machine translation system lie in two aspects: (1) 
It can recognize the multi-word units, particularly 
separated phrases, in the source language. Thus, 
our method is able to handle the multi-word 
alignments with higher accuracy, which will be 
described in our experiments. (2) It can perform 
word sense disambiguation and select appropriate 
translations while a translation dictionary can 
only list all translations for each word or phrase. 
Experimental results show that our approach im-
proves word alignments in both precision and 
recall as compared with the state-of-the-art tech-
nologies. 
2 
                                                          
Statistical Word Alignment 
Statistical translation models (Brown, et al 1993) 
only allow word to word and multi-word to word 
alignments. Thus, some multi-word units cannot 
be correctly aligned. In order to tackle this prob-
lem, we perform translation in two directions 
(English to Chinese and Chinese to English) as 
described in Och and Ney (2000). The GIZA++ 
toolkit is used to perform statistical alignment. 
Thus, for each sentence pair, we can get two 
alignment results. We use  and  to represent 
the alignment sets with English as the source lan-
guage and Chinese as the target language or vice 
versa. For alignment links in both sets, we use i 
for English words and j for Chinese words. 
1S 2S
}0  },{|),{(1 ?== jjjj aaAjAS  
}0  },{|),{(2 ?== iiii aaAAiS  
 Where, represents the index posi-
tion of the source word aligned to the target word 
in position x. For example, if a Chinese word in 
position j is connected to an English word in po-
sition i, then . If a Chinese word in position 
j is connected to English words in positions i  
and , then .
),( jixax =
ia j =
,{ 21 iiA j =
)1( >k
1
2i }
2  We call an element in 
the alignment set an alignment link. If the link 
includes a word that has no translation, we call it 
a null link. If k words have null links, we 
treat them as k different null links, not just one 
link. 
2 In the following of this paper, we will use the position 
number of a word to refer to the word.   
Based on  and , we obtain their intersec-
tion set, union set and subtraction set.  
1S 2S
Intersection: 21 SSS ?=  
Union: 21 SSP ?=  
Subtraction: S?= PF  
Thus, the subtraction set contains two differ-
ent alignment links for each English word.  
3 Rule-Based Translation System 
We use the translation information in a rule-
based English-Chinese translation system3 to im-
prove the statistical word alignment result. This 
translation system includes three modules: source 
language parser, source to target language trans-
fer module, and target language generator.  
From the transfer phase, we get Chinese trans-
lation candidates for each English word. This 
information can be considered as another word 
alignment result, which is denoted as 
)},{(3 kCkS = . C  the set including the trans-
lation candidates for the k-th  English word or 
phrase. The difference between S  and the 
common alignment set is that each English word 
or phrase in S  has one or more translation can-
didates. A translation example for the English 
sentence ?He is used to pipe smoking.? is shown 
in Table 1.  
k  is
3
3
English Words Chinese Translations 
He ? 
is used to ?? 
pipe ????? 
smoking ???? 
Table 1. Translation Example 
From Table 1, it can be seen that (1) the trans-
lation system can recognize English phrases (e.g. 
is used to); (2) the system can provide one or 
more translations for each source word or phrase; 
(3) the translation system can perform word se-
lection or word sense disambiguation. For exam-
ple, the word ?pipe? has several meanings such 
as ?tube?, ?tube used for smoking? and ?wind 
instrument?. The system selects ?tube used for 
smoking? and translates it into Chinese words 
???? and ????. The recognized translation 
                                                          
3 This system is developed based on the Toshiba English-
Japanese translation system (Amano et al 1989). It achieves 
above-average performance as compared with the English-
Chinese translation systems available in the market. 
candidates will be used to improve statistical 
word alignment in the next section.  
4 
4.1 
Word Alignment Improvement 
As described in Section 2, we have two align-
ment sets for each sentence pair, from which we 
obtain the intersection set S  and the subtraction 
set . We will improve the word alignments in S 
and  with the translation candidates produced 
by the rule-based machine translation system. In 
the following sections, we will first describe how 
to calculate monolingual word similarity used in 
our algorithm. Then we will describe the algo-
rithm used to improve word alignment results.  
F
F
Word Similarity Calculation 
This section describes the method for monolin-
gual word similarity calculation. This method 
calculates word similarity by using a bilingual 
dictionary, which is first introduced by Wu and 
Zhou (2003). The basic assumptions of this 
method are that the translations of a word can 
express its meanings and that two words are simi-
lar in meanings if they have mutual translations. 
Given a Chinese word, we get its translations 
with a Chinese-English bilingual dictionary. The 
translations of a word are used to construct its 
feature vector. The similarity of two words is 
estimated through their feature vectors with the 
cosine measure as shown in (Wu and Zhou 2003). 
If there are a Chinese word or phrase w  and a 
Chinese word set Z , the word similarity between 
them is calculated as shown in Equation (1). 
))',((),(
'
wwsimMaxZwsim
Zw?
=  (1)
4.2 Alignment Improvement Algorithm 
As the word alignment links in the intersection 
set are more reliable than those in the subtraction 
set, we adopt two different strategies for the 
alignments in the intersection set S  and the sub-
traction set . For alignments in S, we will mod-
ify them when they are inconsistent with the 
translation information in S . For alignments in 
, we classify them into two cases and make se-
lection between two different alignment links or 
modify them into a new link. 
F
3
F
In the intersection set S , there are only word 
to word alignment links, which include no multi-
word units. The main alignment error type in this 
set is that some words should be combined into 
one phrase and aligned to the same word(s) in the 
target sentence. For example, for the sentence 
pair in Figure 1, ?used? is aligned to the Chinese 
word ????, and ?is? and ?to? have null links in 
. But in the translation set , ?is used to" is a 
phrase. Thus, we combine the three alignment 
links into a new link. The words ?is?, ?used? and 
? to? are all aligned to the Chinese word ????, 
denoted as (is used to, ??). Figure 2 describes 
the algorithm employed to improve the word 
alignment in the intersection set S . 
S 3S
)j
 phk ,
3S
 
 
Figure 1. Multi-Word Alignment Example 
Input: Intersection set S , Translation set , 3S
            Final word alignment set WA  
For each alignment link?  in , do:  ,i S
(1) If all of the following three conditions are 
satisfied, add the new alignment link 
WA phk ??? w,  to WA . 
a) There is an element? , and 
the English word i is a constituent of the 
phrase  . 
3) SCk ?
kph
b) The other words in the phrase ph  also 
have alignment links in S .  
k
c) For each word s in ph , we get k
}),|{ St(stT ?= and combine 4  all words 
in T into a phrase w , and the similar-
ity 1),( ?>kCwsim .  
(2) Otherwise, add?  to WA .  ), ji
Output: Word alignment set WA  
Figure 2. Algorithm for the Intersection Set 
In the subtraction set, there are two different 
links for each English word. Thus, we need to 
select one link or to modify the links according to 
the translation information in .  
For each English word i in the subtraction set, 
there are two cases:  
                                                         
4 We define an operation ?combine? on a set consisting of 
position numbers of words. We first sort the position num-
bers in the set ascendly and then regard them as a phrase. 
For example, there is a set {{2,3}, 1, 4}, the result after 
applying the combine operation is (1, 2, 3, 4). 
Case 1: In , there is a word to word alignment 
link? . In , there is a word to word or 
word to multi-word alignment link
1S
1S ), ji ? 2S
 2), SAi i ?? 5.  
Case 2: In , there is a multi-word to word 
alignment link ( . In S , there 
is a word to word or word to multi-word align-
ment link? . 
1S
, Ai
jj AiSjA ?? &), 1
2) S?
2
 i
For Case 1, we first examine the translation 
set . If there is an element? , we cal-
culate the Chinese word similarity between j in 
 and  with Equation (1) shown in 
Section 4.1. We also combine the words in A  
) into a phrase and get the word simi-
larity between this new phrase and C . The align-
ment link with a higher similarity score is 
selected and added to WA .  
3S
)j ?
), Ai
 3), SCi i ?
i
1, Si?
( Si ??
iC
i
2
Input: Alignment sets S  and  1 2S
Translation unit?   3), SCph kk ?
(1) For each sub-sequence6 s of , get the 
sets and 
 
kph
}1)?,(|{ 111 StstT =
}) 22 St ?,(|{ 22 stT =
(2) Combine words in T  and T  into phrases 
 and  respectively. 
1 2
1w 2w
(3) Obtain the word similarities 
 and .  ),Csim(wws k11 = ),Csim(wws k22 =
(4) Add a new alignment link to WA  
according to the following steps. 
a) If ws and 21 ws> 11 ?>ws , add ?  
to WA ;  
), 1wphk
b) If ws  and 12 ws> 12 ?>ws , add?  
to WA ;  
 ), 2wphk
c) If 121 ?>= wsws
), 2wk
, add ?  or 
 to WA  randomly. 
), 1wphk
ph?
Output: Updated alignment set WA  
Figure 3. Multi-Word to Multi-Word Align-
ment Algorithm 
If, in S , there is an element?  and i 
is a constituent of , the English word i of the 
alignment links in both S  and  should be 
3  ), kk Cph
2S
kph
1
combined with other words to form phrases. In 
this case, we modify the alignment links into a 
multi-word to multi-word alignment link. The 
algorithm is described in Figure 3. 
                                                          
5  ? ), iAi represents both the word to word and word to 
multi-word alignment links. 
6  If a phrase consists of three words w , the sub-
sequences of this phrase are w . 
321 ww
221 ,, www 3321 ,, www
For example, given a sentence pair in Figure 4,  
in S , the word ?whipped? is aligned to ???? 
and ?out? is aligned to ????. In S , the word 
?whipped? is aligned to both ???? and ???? 
and ?out? has a null link. In , ?whipped out? is 
a phrase and translated into ?????". And the 
word similarity between ?????? and ????
?? is larger than the threshold 
1
2
1
3S
? . Thus, we 
combine the aligned target words in the Chinese 
sentence into ??????. The final alignment 
link should be (whipped out, ?? ??). 
 
Figure 4. Multi-Word to Multi-Word Alignment 
Example 
For Case 2, we first examine S  to see 
whether there is an element? . If true, 
we combine the words in  (? ) into a 
word or phrase and calculate the similarity be-
tween this new word or phrase and C  in the 
same way as in Case 1. If the similarity is higher 
than a threshold 
3
3S
2S?
i
 ), Ci i ?
), Ai iiA
1? , we add the alignment link 
 into WA .  ), iAi?
If there is an element?  and i is a 
constituent of ph , we combine the English 
words in A  ( ) into a phrase. If it is 
the same as the phrase  and 
 3), SCph kk ?
1
kph (
k
, jA jj )( S?
1), ?>kCjsim , 
we add (  into WA . Otherwise, we use the 
multi-word to multi-word alignment algorithm in 
Figure 3 to modify the links.  
),A j j
After applying the above two strategies, there 
are still some words not aligned. For each sen-
tence pair, we use E and C to denote the sets of 
the source words and the target words that are not 
aligned, respectively. For each source word in E, 
we construct a link with each target word in C. 
We use L },|),{( CjEiji ??=  to denote the 
alignment candidates. For each candidate in L, 
we look it up in the translation set S . If there is 
an element 
3
3), SCi i ??  and 2)( , ?>j iCsim , we 
add the link into the set WA . 
|C
|| C
C
S
S?
S
C
C =
|
|
5 Experiments 
5.1 
5.2 
Training and Testing Set 
We did experiments on a sentence aligned Eng-
lish-Chinese bilingual corpus in general domains. 
There are about 320,000 bilingual sentence pairs 
in the corpus, from which, we randomly select 
1,000 sentence pairs as testing data. The remain-
der is used as training data.  
The Chinese sentences in both the training set 
and the testing set are automatically segmented 
into words. The segmentation errors in the testing 
set are post-corrected. The testing set is manually 
annotated. It has totally 8,651 alignment links 
including 2,149 null links. Among them, 866 
alignment links include multi-word units, which 
accounts for about 10% of the total links. 
Experimental Results 
There are several different evaluation methods 
for word alignment (Ahrenberg et al 2000). In 
our evaluation, we use evaluation metrics similar 
to those in Och and Ney (2000). However, we do 
not classify alignment links into sure links and 
possible links. We consider each alignment as a 
sure link.  
If we use S  to indicate the alignments iden-
tified by the proposed methods and S  to denote 
the reference alignments, the precision, recall and 
f-measure are calculated as described in Equation 
(2), (3) and (4). According to the definition of the 
alignment error rate (AER) in Och and Ney 
(2000), AER can be calculated with Equation (5).  
G
C
|S|
SS|
G
G ?=precision       (2)
|S|
 |SS|
C
CG ?=recall    (3)
||
||*2
G
G
S
S
fmeasure +=  (4)
fmeasure
SS
S
AER
G
G ?+
??= 1
|||
|*2
1 (5)
In this paper, we give two different alignment 
results in Table 2 and Table 3. Table 2 presents 
alignment results that include null links. Table 3 
presents alignment results that exclude null links. 
The precision and recall in the tables are obtained 
to ensure the smallest AER for each method. 
 Precision Recall AER 
Ours 0.8531 0.7057 0.2276 
Dic 0.8265 0.6873 0.2495 
IBM E-C 0.7121 0.6812 0.3064 
IBM C-E 0.6759 0.7209 0.3023 
IBM Inter 0.8756 0.5516 0.3233 
IBM Refined 0.7046 0.6532 0.3235 
Table 2. Alignment Results Including Null Links 
 Precision Recall AER 
Ours 0.8827 0.7583 0.1842 
Dic 0.8558 0.7317 0.2111 
IBM E-C 0.7304 0.7136 0.2781 
IBM C-E 0.6998 0.6725 0.3141 
IBM Inter 0.9392 0.5513 0.3052 
IBM refined 0.8152 0.6926 0.2505 
Table 3. Alignment Results Excluding Null Links 
In the above tables, the row ?Ours? presents 
the result of our approach. The results are ob-
tained by setting the word similarity thresholds to 
1.01??  and 5.02?? . The Chinese-English dic-
tionary used to calculate the word similarity has 
66,696 entries. Each entry has two English trans-
lations on average. The row ?Dic? shows the re-
sult of the approach that uses a bilingual 
dictionary instead of the rule-based machine 
translation system to improve statistical word 
alignment. The dictionary used in this method is 
the same translation dictionary used in the rule-
based machine translation system. It includes 
57,684 English words and each English word has 
about two Chinese translations on average. The 
rows ?IBM E-C? and ?IBM C-E? show the re-
sults obtained by IBM Model-4 when treating 
English as the source and Chinese as the target or 
vice versa. The row ?IBM Inter? shows results 
obtained by taking the intersection of the align-
ments produced by ?IBM E-C? and ?IBM C-E?. 
The row ?IBM Refined? shows the results by 
refining the results of ?IBM Inter? as described in 
Och and Ney (2000). 
Generally, the results excluding null links are 
better than those including null links. This indi-
cates that it is difficult to judge whether a word 
has counterparts in another language. It is be-
cause the translations of some source words can 
be omitted. Both the rule-based translation sys-
tem and the bilingual dictionary provide no such 
information.  
It can be also seen that our approach performs 
the best among others in both cases. Our ap-
proach achieves a relative error rate reduction of 
26% and 25% when compared with ?IBM E-C? 
and ?IBM C-E? respectively7. Although the pre-
cision of our method is lower than that of the 
?IBM Inter? method, it achieves much higher 
recall, resulting in a 30% relative error rate re-
duction. Compared with the ?IBM refined? 
method, our method also achieves a relative error 
rate reduction of 30%. In addition, our method is 
better than the ?Dic? method, achieving a relative 
error rate reduction of 8.8%.  
In order to provide the detailed word align-
ment information, we classify word alignment 
results in Table 3 into two classes. The first class 
includes the alignment links that have no multi-
word units. The second class includes at least one 
multi-word unit in each alignment link. The de-
tailed information is shown in Table 4 and Table 
5. In Table 5, we do not include the method ?In-
ter? because it has no multi-word alignment links.  
 Precision Recall AER 
Ours 0.9213 0.8269 0.1284 
Dic 0.8898 0.8215 0.1457 
IBM E-C 0.8202 0.7972 0.1916 
IBM C-E 0.8200 0.7406 0.2217 
IBM Inter 0.9392 0.6360 0.2416 
IBM Refined 0.8920 0.7196 0.2034 
Table 4. Single Word Alignment Results 
 Precision Recall AER 
Ours 0.5123 0.3118 0.6124 
Dic 0.3585 0.1478 0.7907 
IBM E-C 0.1682 0.1697 0.8311 
IBM C-E 0.1718 0.2298 0.8034 
IBM Refined 0.2105 0.2910 0.7557 
Table 5. Multi-Word Alignment Results  
All of the methods perform better on single 
word alignment than on multi-word alignment. In 
Table 4, the precision of our method is close to 
the ?IBM Inter? approach, and the recall of our 
method is much higher, achieving a 47% relative 
error rate reduction. Our method also achieves a 
37% relative error rate reduction over the ?IBM 
Refined? method.  Compared with the ?Dic? 
method, our approach achieves much higher pre-
cision without loss of recall, resulting in a 12% 
relative error rate reduction. 
                                                          
6 Discussion 
7 The error rate reductions in this paragraph are obtained 
from Table 2. The error rate reductions in Table 3 are 
omitted.  
Our method also achieves much better results 
on multi-word alignment than other methods. 
However, our method only obtains one third of 
the correct alignment links. It indicates that it is 
the hardest to align the multi-word units. 
Readers may pose the question ?why the rule-
based translation system performs better on word 
alignment than the translation dictionary?? For 
single word alignment, the rule-based translation 
system can perform word sense disambiguation, 
and select the appropriate Chinese words as 
translation. On the contrary, the dictionary can 
only list all translations. Thus, the alignment pre-
cision of our method is higher than that of the 
dictionary method. Figure 5 shows alignment 
precision and recall values under different simi-
larity values for single word alignment including 
null links. From the figure, it can be seen that our 
method consistently achieves higher precisions as 
compared with the dictionary method. The t-
score value (t=10.37, p=0.05) shows the im-
provement is statistically significant.   
Figure 5. Recall-Precision Curves  
For multi-word alignment links, the translation 
system also outperforms the translation diction-
ary. The result is shown in Table 5 in Section 5.2. 
This is because (1) the translation system can 
automatically recognize English phrases with 
higher accuracy than the translation dictionary; (2) 
The translation system can detect separated 
phrases while the dictionary cannot. For example, 
for the sentence pairs in Figure 6, the solid link 
lines describe the alignment result of the rule-
base translation system while dashed lines indi-
cate the alignment result of the translation dic-
tionary. In example (1), the phrase ?be going to? 
indicates the tense not the phrase ?go to? as the 
dictionary shows. In example (2), our method 
detects the separated phrase ?turn ? on? while 
the dictionary does not. Thus, the dictionary 
method produces the wrong alignment link. 
 
Figure 6. Alignment Comparison Examples 
7 Conclusion and Future Work 
This paper proposes an approach to improve sta-
tistical word alignment results by using a rule-
based translation system. Our contribution is that, 
given a rule-based translation system that pro-
vides appropriate translation candidates for each 
source word or phrase, we select appropriate 
alignment links among statistical word alignment 
results or modify them into new links. Especially, 
with such a translation system, we can identify 
both the continuous and separated phrases in the 
source language and improve the multi-word 
alignment results. Experimental results indicate 
that our approach can achieve a precision of 85% 
and a recall of 71% for word alignment including 
null links in general domains. This result signifi-
cantly outperforms those of the methods that use 
a bilingual dictionary to improve word alignment, 
and that only use statistical translation models. 
Our future work mainly includes three tasks. 
First, we will further improve multi-word align-
ment results by using other technologies in natu-
ral language processing. For example, we can use 
named entity recognition and transliteration tech-
nologies to improve person name alignment. Sec-
ond, we will extract translation rules from the 
improved word alignment results and apply them 
back to our rule-based machine translation sys-
tem. Third, we will further analyze the effect of 
the translation system on the alignment results. 
References 
Lars Ahrenberg, Magnus Merkel, and Mikael Anders-
son 1998. A Simple Hybrid Aligner for Generating 
Lexical Correspondences in Parallel Texts. In Proc. 
of the 36th Annual Meeting of the Association for 
Computational Linguistics and the 17th Int. Conf. 
on Computational Linguistics, pp. 29-35. 
 Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein 
and Jorg Tiedemann 2000. Evaluation of word 
alignment systems. In Proc. of the Second Int. Conf. 
on Linguistic Resources and Evaluation, pp. 1255-
1261. 
ShinYa Amano, Hideki Hirakawa, Hiroyasu Nogami, 
and Akira Kumano 1989. Toshiba Machine Trans-
lation System. Future Computing Systems, 
2(3):227-246. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311. 
Colin Cherry and Dekang Lin 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 88-95. 
I. Dan Melamed 1996. Automatic Construction of 
Clean Broad-Coverage Translation Lexicons. In 
Proc. of the 2nd Conf. of the Association for Ma-
chine Translation in the Americas, pp. 125-134. 
I. Dan Melamed 2000. Word-to-Word Models of 
Translational Equivalence among Words. Compu-
tational Linguistics, 26(2): 221-249. 
Arul Menezes and Stephan D. Richardson 2001. A 
Best-first Alignment Algorithm for Automatic Ex-
traction of Transfer Mappings from Bilingual Cor-
pora. In Proc. of the ACL 2001 Workshop on Data-
Driven Methods in Machine Translation, pp. 39-46. 
Franz Josef Och and Hermann Ney 2000. Improved 
Statistical Alignment Models. In Proc.of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 440-447. 
Harold Somers 1999. Review Article: Example-Based 
Machine Translation. Machine Translation 14:113-
157. 
Jorg Tiedemann 1999. Word Alignment ? Step by Step. 
In Proc. of the 12th Nordic Conf. on Computational 
Linguistics, pp. 216-227. 
Dan Tufis and Ana Maria Barbu. 2002. Lexical Token 
Alignment: Experiments, Results and Application. 
In Proc. of the Third Int. Conf. on Language Re-
sources and Evaluation, pp. 458-465. 
Dekai Wu 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403. 
Hua Wu and Ming Zhou 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proc. of the 2nd Int. Workshop on Para-
phrasing, pp. 72-79. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993?1000
Manchester, August 2008
Domain Adaptation for Statistical Machine Translation with Domain 
Dictionary and Monolingual Corpora 
Hua Wu,  Haifeng Wang 
Toshiba (China) R&D Center 
Beijing, 100738, China 
 wuhua@rdc.toshiba.com.cn 
wanghaifeng@rdc.toshiba.com.cn 
Chengqing Zong 
NLPR, Institute of Automation 
Chinese Academy of Sciences 
 Beijing 100080, China 
cqzong@nlpr.ia.ac.cn 
 
Abstract tra
Statistical machine translation systems 
are usually trained on large amounts of 
bilingual text and monolingual text. In 
this paper, we propose a method to per-
form domain adaptation for statistical 
machine translation, where in-domain bi-
lingual corpora do not exist. This method 
first uses out-of-domain corpora to train a 
baseline system and then uses in-domain 
translation dictionaries and in-domain 
monolingual corpora to improve the in-
domain performance. We propose an al-
gorithm to combine these different re-
sources in a unified framework. Experi-
mental results indicate that our method 
achieves absolute improvements of 8.16 
and 3.36 BLEU scores on Chinese to 
English translation and English to French 
translation respectively, as compared 
with the baselines using only out-of-
domain corpora. 
1 Introduction 
In statistical machine translation (SMT), the 
translation process is modeled to obtain the 
translation  of the source sentence f  by 
maximizing the following posterior probability 
(Brown et al, 1993). 
beste
)()(maxarg
)(maxarg
|
|
ee
fee
fe
e
LM
best
pp
p
=
=
 (1)
State-of-the-art SMT systems are trained on 
large collections of bilingual corpora for the 
                                                 
?C 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
nslation model )( | efp  and monolingual tar-
get language corpora for the language model 
(LM) )(eLMp . The trained SMT systems are 
suitable for translating texts in the same domain 
as the training corpus. However, for some spe-
cific domains, it is difficult to obtain a bilingual 
corpus. In this case, the performance of SMT 
systems will be degraded. 
Generally, it is easier to obtain in-domain 
monolingual corpora in either source or target 
language. Moreover, in some specific domains, 
although in-domain bilingual corpora do not ex-
ist, in-domain translation dictionaries, which 
usually contain domain-specific terms and their 
translations, are available. And even if such dic-
tionaries are not available, it is easier to compile 
one than to build a bilingual corpus. Thus, in this 
paper, we address the problem of domain-
specific SMT, where only domain-specific dic-
tionaries and/or monolingual corpora exist. In a 
specific domain, there are two kinds of words: 
common words, which also frequently occur in 
out-of-domain corpora, and domain-specific 
words, which only occur in the specific domain. 
Thus, we can combine the out-of-domain bilin-
gual corpus, the in-domain translation dictionary, 
and monolingual corpora for in-domain transla-
tion. 
If an in-domain translation dictionary is avail-
able, we combine it with the out-of-domain 
translation model to improve translation quality. 
If an in-domain target language corpus (TLC) is 
available, we use it to build an in-domain lan-
guage model, which can be combined with the 
out-of-domain language model to further im-
prove translation quality. Moreover, if an in-
domain source language corpus (SLC) is avail-
able, we automatically translate it and obtain a 
synthetic in-domain bilingual corpus. By adding 
this synthetic bilingual corpus to the training data, 
we rebuild the translation model to improve 
993
translation quality. We can repeatedly translate 
the in-domain source language corpus with the 
improved model until no more improvement can 
be made. This is similar to transductive learning 
described in (Ueffing et al, 2007). 
We perform domain adaptation experiments 
on two tasks: one is the Chinese to English trans-
lation, using the test set released by the Interna-
tio
inese to English translation 
an
MT sys-
tem
lation model and language model adapta-
in domain adaptation for 
del adaptation has been 
 
shared task focused on do-
main adaptation for machine translation among 
Eu
rpora. Adding the extracted bilin-
gu
the performance of a SMT system 
tra
Moses (Koehn et al, 2007). In 
babilities, reorder-
e model probabili-
nal Workshop on Spoken Language Transla-
tion 2006 (IWSLT 2006), and the other is the 
English to French translation, using the data re-
leased by the Second Workshop on Statistical 
Machine Translation (WMT 2007) (Callison-
Burch et al, 2007). 
Experimental results indicate that our method 
achieves absolute improvements of 8.16 and 3.36 
BLEU scores on Ch
d English to French translation respectively, as 
compared with the baselines only using the out-
of-domain corpora. The results on both transla-
tion tasks also show that the translation quality 
achieved by our methods is comparable to that of 
the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
The remainder of the paper is organized as fol-
lows. In section 2, we describe the related work. 
Section 3 briefly introduces the baseline 
 used in our experiments. Section 4 describes 
our domain adaptation method of using in-
domain dictionary and monolingual corpora. And 
then we present the experimental results in sec-
tions 5. In the last section, we conclude this pa-
per. 
2 Related Work 
Trans
tion are usually used 
SMT. Language mo
widely used in speech recognition (Bacchiani 
and Roark, 2003). In recent years, language 
model adaptation has also been studied for SMT 
(Bulyko et al, 2007). They explored discrimina-
tive estimation of language model weights by 
directly optimizing machine translation perform-
ances such as BLEU score (Papineni et al, 2002).
Their experiments indicated about 0.4 BLEU 
score improvement. 
A shared task is organized as part of the Sec-
ond Workshop on Statistical Machine Transla-
tion. A part of this 
ropean languages. Several studies investigated 
mixture model adaptation for both translation 
model and language model in SMT (Civera and 
Juan, 2007; Foster and Kuhn, 2007). Koehn and 
Schroeder (2007) investigated different adapta-
tion methods for SMT. Their experiments indi-
cate an absolute improvement of more than 1 
BLEU score. 
To enlarge the in-domain bilingual corpus, 
Munteanu and Marcu (2005) automatically ex-
tracted in-domain bilingual sentence pairs from 
comparable co
al corpus to the training data improved the per-
formance of the MT system. In addition, Ueffing 
et al (2007) explored transductive learning for 
SMT, where source language corpora are used to 
train the models. They repeatedly translated 
source sentences from the development set and 
test set. Then the generated translations are used 
to improve the performance of the SMT system. 
This kind of transductive learning can be seen as 
a means to adapt the SMT system to a new type 
of texts. 
In this paper, we use an in-domain translation 
dictionary and/or in-domain monolingual corpora 
(in both source language and target language) to 
improve 
ined on the out-of-domain corpora. Thus, our 
method uses these resources, instead of an in-
domain bilingual corpus, to adapt a baseline sys-
tem trained on the out-of-domain corpora to in-
domain texts. 
3 Baseline MT System 
The phrase-based SMT system used in our ex-
periments is 
Moses, phrase translation pro
ing probabilities, and languag
ties are combined in the log-linear model to ob-
tain the best translation beste  of the source sen-
tence f : 
?
=
?
=
M
p | )(maxarg fee ebest
 (2)
m
mmh
1
,(maxarg f)ee ?
The weights are set by a discriminative train-
ing method using a held-out data set as describ
in (Och, 2003). The models or features which are 
employed by the decoder are (a) one or several 
ph
ed 
rases tables, (b) one or more language models 
trained with SRILM toolkit (Stolcke, 2002), (c) 
distance-based and lexicalized distortion models, 
(d) word penalty, (e) phrase penalty. 
994
Input  Out-of-domain training data OL  
ary D  In-domain translation diction  I
In-domain target language corpus IT  (optional) 
           In-d
, where  represents the general model. 
e 
f 
If  
         
e
ing step: Translate  with  to get a synthetic bilingual corpus 
Un
E
End 
Outpu l  for in-domain translation 
omain source language corpus I  (optional) S
Begin Assign translation probabilities to ID  
If IT  is available 
Training step: (L Estimate= ),, IIO TD?
Els
?
Training step: ),( IO DL Estimate=?  
End i
 S  is availableI
    ? =)0( ?,  0=i
R peat 
 
1+= ii  
Label IS
)1( ?i? IL  
Training step: ),, IIO
til no more improvement can be achieved 
()(i LDL estimate-Re=?  
)(i?  ?=
nd if 
t  Mode  ?
Figure 1.The domain adaptation algorithm 
4 The Framework 
n about the algorithm is 
 our algorithm, a phrase 
available, we train an in-domain LM, which is 
co
ilable, we use the built -linear 
m
). With the 
 order to 
 to assign prob-
nary.  
m
4.1 The Algorithm 
The detailed informatio
shown in Figure 1. In
table and a language model are first constructed 
based on the out-of-domain corpus OL . Then 
probabilities are automatically assigned to the 
entries in the in-domain translation dictionary 
ID , from which another phrase table is con-
structed. At last, the two phrase tables are com-
d. This is the procedure of the training step 
),( IO DL Estimate=? . 
If an in-domain target language corpus is 
bine
mbined with the out-of-domain LM. The built 
phrase tables and LMs are integrated in the log-
linear model as described in section 3. This is the 
procedure of ),,( IIO TDL Estimate=? . 
Moreover, if an in-domain source language 
corpus is ava log
odel to translate the in-domain source texts and 
obtain a synthetic bilingual corpus. And then we 
add the synthetic bilingual corpus into the train-
ing data to improve the current log-linear model 
improved model, we repeatedly translate the in-
domain source texts until no more improvement 
on a development set can be achieved.  
4.2 Dictionary Probabilities 
In general, there is no translation probability in a 
manually-made translation dictionary. In
( estima-Re=? ),,()( IIOi LDL te
construct a phrase table, we have
abilities to the entries in the dictio
Uniform Translation Probability: Since we 
have no parallel corpus to estimate the translation 
probabilities, we simply assign uniform prob-
abilities to the entries in the dictionary. With this 
ethod, if a source word has n  translations, then 
we assign n1  to each translation of this phrase 
for all the four scores of the phrase pair. 
Constant Translation Probability: For each 
entry in the dictionary, we as ign a fixed score. 
In this case e sum of the translation probability 
is not necessarily equal to 1. 
s
, th
anslation probabili-
ties for the entries in the dictionary. And for the 
Corpus Translation Probability: If an in-
domain monolingual source corpus exists, we 
translate it with the method as described in Fig-
ure 1, and then estimate the tr
995
en
s.  
mmonly-used 
tries whose translation probabilities are not 
estimated, we assign average probabilities that 
are calculated from the entries that have obtained 
probabilities. 
4.3 Combining Phrase Tables  
In the algorithm, there are two kinds of phrase 
tables. We need to combine them to translate the 
in-domain text
Mixture Model: The most co
method is linear interpolation.  
)()1()()( ||| fefefe oI ppp ?? ?+=  (3)
W
ion probabili-
ties. 
here )( | feIp  and )( | feop  are the in-
domain and out-of-domain translat
?  is the interpolation weight. 
Discriminative Model: An alternative is to 
two tables in the log-linea  
ranslation oses, for 
en uses them 
fo
he out-of-domain lan-
tigate two 
ation and 
two different tasks: one 
is the Chinese to English translation in IWSLT 
 the other is the English to 
n adaptation translation in the 
red task. 
airs, with about 3 mil-
lio
                                                
combine the r model.
During t with M each phrase in 
the sentence, the decoder obtains all of its trans-
lations in both phrase tables, and th
r translation expansion. 
4.4 Combining Language Models 
If an in-domain target language corpus exists, we 
use it to construct an in-domain language model, 
which is combined with t
guage model. In this paper, we inves
combination methods: linear interpol
log-linear interpolation. 
5 Experiments  
5.1 Setting 
We ran experiments on 
2006 evaluation, and
French domai
WMT 2007 sha
For the Chinese to English translation task, we 
use the Chinese-English bilingual corpus pro-
vided by the Chinese Linguistic Data Consortium 
(CLDC)2 as the out-of-domain corpus. It con-
tains 156,840 sentence p
n English words and about 5 million Chinese 
characters.  In addition, we use the Basic Travel-
ing Expression Corpus (BTEC) released by 
IWSLT 2006 (Paul, 2006) to construct an in-
domain phrase table, as a comparison with that 
one constructed with the in-domain dictionary. 
 
2  http://www.chineseldc.org/EN/index.htm. The catalog 
number is CLDC-LAC-2003-004. It is a balanced corpus 
containing sentence pairs in multiple domains. 
Corpora Sentences OOV 
CLDC 156,840 89 (6.31%) 
BTEC 39,953 179 (12.69%) 
IWSLT06-dev4 489 NA 
IWSLT06-test 500 NA 
Table 1. Ch sh cor
aries Entries 
inese-Engli pora 
Diction OOV 
LDC 82,090 228 (16.16%) 
in-domain 32  .57%) ,821 121 (8
T s 
ces 
able 2. Chinese-English dictionarie
Corpora Senten OOV 
Europarl 949,410 412 (5.90%) 
NC 43,060 599 (8.58%) 
WMT07 dev 1,057 NA 
WMT07 test 2,007 NA 
Table 3. E ch cor
I art and rt a sed 
a in m ual  ex-
p  ta rts of  CLDC and 
BTEC are used for language m  construction 
(see Tab uation, 
nglish-Fren pora 
ts source p  target p
g
a re separately u
ra in ours the in-doma
e
onolin corpo
eriments. Th rget pa  both
odel
le 1). From the IWSLT 2006 eval
we choose the devset4 as our development data. 
Evaluation was performed on IWSLT 2006 test 
set. The references for the test set contain lower-
case words and punctuations. The detailed in-
formation is shown in Table 1. 
We use two kinds of manually-made diction-
aries for comparison: one is the LDC Chinese-
English Translation Lexicon Version 3.0 
(LDC2002L27), and the other is the in-domain 
spoken language dictionary made by ourselves, 
which contains in-domain Chinese words and 
their English translations. The dictionary is ma-
nually constructed. Some entries of the diction-
ary are collected from phrase books. Some of 
them are collected from the general-domain dic-
tionaries. And then, the entries are filtered and 
modified by a Chinese native speaker specialized 
in English. The detailed information is shown in 
Table 2. If a source word has two translations, it 
is counted as two entries. The OOV rates of the 
test set uncovered by the LDC dictionary and the 
in-domain dictionary are 16.16% and 8.57%, 
respectively. 
For the English to French translation task, the 
out-of-domain corpus is the Europarl corpus dis-
tributed for the shared task of WMT 2007 (Calli-
son-Burch et al, 2007)3. We filter the sentence 
pairs whose lengths are above 40 words. For the 
                                                 
3 http://www.statmt.org/wmt07/shared-task.html 
996
in-domain corpus, we use the News Commentary 
(NC) corpus distributed in WMT 2007. We also 
use the same development set and test set in the 
domain adaptation shared task (see Table 3). We 
manually built an in-domain English-French dic-
tionary according to the in-domain bilingual cor-
pus, which includes 26,821 entries. It contains 
in-domain English words and their French trans-
lations. The OOV rate of the test set uncovered 
by this dictionary is 22.34%. 
5.2 Evaluation Measures 
To perform phrase-based SMT, we use the 
rt training scripts. 
efault settings and 
ary 
rase table. With the in-
domain translation dictionary, we construct in-
do
lts, log-linear 
tra
tionary into the training corpus. In 
th
omain 
corpus to train a phrase table. Then we use both  
Moses decoder and its suppo
We run the decoder with its d
then use Moses' implementation of minimum 
error rate training (Och, 2003) to tune the feature 
weights on the development set. Translation 
quality was evaluated using BLEU score (Pap-
ineni et al, 2002).  
5.3 Results on Chinese-English Translation 
Translation Diction
With the out-of-domain bilingual corpus, we 
train an out-of-domain ph
main phrase tables by assigning different 
translation probabilities with two different meth-
ods: uniform and constant. For the constant 
translation probability, we set the score using the 
development set. In our experiments, we set it to 
1. We use the target part of the out-of-domain 
corpus to train a language model4. 
With two phrase tables, we combine them in a 
linear or log-linear method as described in sec-
tion 4.3. In our experimental resu
nslation models outperform the linear models 
(16.38 vs. 15.12), where the entries of the dic-
tionary are assigned with the constant translation 
probabilities. Thus, we will use log-linear models 
for phrase table combination in the following 
experiments. 
Another method to combine the out-of-domain 
corpus and the translation dictionary is to add the 
in-domain dic
is case, only one phrase table is trained. 
Table 4 describes the results using the out-of-
domain corpus and the in-domain dictionary. The 
baseline method only uses the out-of-d
                                                 
4 We also used LDC English Gigaword to train a large lan-
guage model. However, this language model did not im-
prove the translation quality. 
Methods Resources Used BLEU(%)
baseline out-of-domain corpus 13.59 
+dictionary as corpus 15.52 
+uniform prob. 16.00 
+constant prob. 16.38 
baseline + 
dictionary
+corpus prob. 16.72 
Table 4. Translation results of using out-of-
d ictionary
the out-of-dom  the in-dom c-
5 . The 
 also 
im
bine it with the 
 linear inter-
polation and log-linear interpolation. The ex-
pe
oves 
th
ifference be-
tw
 
                                                
omain corpus and in-domain d  
ain corpus and ain di
tionary. The results indicate that adding an in- 
domain dictionary significantly improves the 
translation quality by 2.79 BLEU score
methods using the dictionary as a phrase table 
outperform the method adding it to the training 
corpus. And the method using constant transla-
tion probabilities significantly outperforms that 
using the uniform translation probabilities. 
For comparison, we also assign corpus prob-
abilities to the entries in the dictionary by trans-
lating the source part of the BTEC corpus with 
the method described in Section 4.2. This
proves the translation quality. 
In-Domain Monolingual Corpora 
We use the target part of the BTEC corpus to 
train an in-domain LM. We com
out-of-domain LM in two methods:
rimental results indicate that linear interpola-
tion outperforms log-linear interpolation (17.16 
vs. 16.20). Thus, we will use linear interpolation 
for LMs in all of the following experiments. 
Table 5 describes the results of using the in-
terpolated language model. As compared with 
the results in Table 4, it can be seen that adding 
the in-domain language model greatly impr
e translation quality. It achieves an absolute 
improvement of 3.57 BLEU score as compared 
with the baseline model. If the in-domain transla-
tion dictionary is used, the translation quality is 
further improved by 4 BLEU score. 
If the in-domain source language data is avail-
able, we translate it and obtain a synthetic bilin-
gual corpus. Then we perform transductive learn-
ing as described in Figure 1. The d
een our method and that in (Ueffing et al, 
2007) is that we translate a larger in-domain 
source corpus, and we use 1-best translation 
 
5 We use the method described in (Koehn and Monz, 2006) 
for significance test. In this paper, significant improvement 
means method A outperforms method B on a significance 
level of no less than 95%. 
997
Methods Models  Resources used BLEU(%)
baseline Model 1 out-of-domain corpus 13.59 
baseline + TLC  Model 2 + in-domain TLC 17.16 
Model 3 + in-domain TLC + dictionary (uniform prob.) 20.83 baseline + TLC 
 dictionary (constant prob.) + dictionary Model 4 + in-domain TLC + 21.16 
Model 5 + in-domain SLC 15.98 
Model 6 + in-domain SLC and TLC 18.19 
transductive 
learning 
nd TLC + dictionary (corpus prob.) Model 7 + in-domain SLC a 21.75 
Table 5. 
Diction  BLEU(%)
Translation results of using in-domain resources 
ary types Entries OOV
general domain LDC 228 (1 %) 82,090 6.16 19.11 
manual 121 ) 32,821 (8.57% 21.16 
in-domain 
extracted 11,765 330 (23.39%) 19.88 
LD al C + manu 106,572 45 (3.19%) 21.34 combined 
LDC + extracted 95,660 202 (14.31%) 20.49 
Table 6. Comparison of ictionar
result with full re-training.
that transductive learning 
ed, the transla-
tio
tionaries 
with concern to the translation quality. Besides 
ld as described in (Wu and 
Wang, 2007). 
 phrase table, extract the 
 their translations. 
 
us
 when an in-domain bi-
m
d  different ies 
 The results indicate 
improves translation 
? From the filtered
Chinese words and
quality in all cases. For example, Model 5 
achieves an absolute improvement of 2.39 BLEU 
score over Model 1, and Model 6 achieves 1.03 
BLEU score improvement over Model 2. Model 
7 uses the in-domain dictionary with corpus 
translation probabilities, which are obtained from 
the phrase table trained with the synthetic bilin-
gual corpus. The results indicate that Model 7 
outperforms Model 4, with a significant im-
provement of 0.59 BLEU score.  
The results also indicate that when only the in-
domain monolingual corpus is us
n quality is improved by 4.6 BLEU score 
(Model 6 vs. Model 1). By adding the in-domain 
dictionary, the translation quality is further im-
proved, achieving an absolute improvement of 
8.16 BLEU score (Model 7 vs. Model 1). 
Comparison of Different Dictionaries 
We compare the effects of different dic
the manually-made in-domain dictionary, we use 
other two dictionaries: the LDC dictionary and 
an automatically built dictionary, which is ex-
tracted from the BTEC corpus. This extracted 
dictionary only contains Chinese words and their 
translations. The extraction method is as follows:  
? Build a phrase table with the in-domain bi-
lingual corpus. 
? Filter those phrase pairs whose values are 
below a thresho
? Assign constant translation probabilities to 
the entries of the extracted dictionary. 
Table 6 shows the translation results. All of 
the methods use the out-of-domain corpus, the 
in-domain target language corpus, and the corre-
sponding translation dictionaries with constant 
translation probabilities. The results indicate that
ing the general-domain dictionary also im-
proves translation quality, achieving an im-
provement of about 2 BLEU score as compared 
with Model 2 in Table 5. It can also be seen that 
the in-domain dictionaries significantly outper-
form the LDC dictionary although the extracted 
dictionary has a higher OOV rate than the LDC 
dictionary. Further analysis shows that the LDC 
dictionary does not contain the in-domain trans-
lations of some words. Results also indicate that 
combining the two kinds of dictionaries helps to 
slightly improve translation quality since the 
OOV rates are reduced. 
Comparison with In-domain Bilingual Corpus 
The aim of this section is to investigate 
whether the in-domain dictionary helps to im-
prove translation quality
lingual corpus is available. And we will also 
compare the translation results with those of the 
ethods only using in-domain dictionaries and 
monolingual corpora. 
To train the in-domain translation model, we 
use the BTEC corpus. The translation results are 
998
13
14
15
16
17
18
19
20
21
22
23
100k 200k 300k all
In-domain sentence pairs
BL
EU
 (
%)
CLDC
BTEC
CLDC+BTEC
CLDC+BTEC+Dic
CLDC+Mono+Dic
Figure 2. Comparison of different methods using 
different resources. 
shown in Figure 2. CLDC and BTEC represent 
s linear interpolation of the 
Interpolated LM" means that 
the methods that only use the out-of-domain and 
the in-domain corpus, respectively. The method 
"CLDC+BTEC" use
phrase tables and LMs trained with CLDC and 
BTEC.   "Dic" means using the in-domain dic-
tionary, and "Mono" means using in-domain 
source and target language corpora. 
From the results, it can be seen that (a) even if 
an in-domain bilingual corpus exists, the in-
domain dictionary also helps to improve the 
translation quality, as "CLDC+BTEC+Dic" 
achieves an improvement of about 1 BLEU score 
in comparison with "CLDC+BTEC"; (b) the 
method "CLDC+Mono+Dic", which uses both 
the in-domain monolingual corpora and the in-
domain dictionary, achieves high translation 
quality. It achieves slightly higher translation 
quality than "CLDC+BTEC" that uses the in-
domain bilingual corpus (21.75 vs.  21.62)  and 
achieves slightly lower translation quality than 
"CLDC+BTEC+Dic" (21.75 vs. 22.05). But the 
differences are not significant. This indicates that 
our method using an in-domain dictionary and 
in-domain monolingual corpora is effective for 
domain adaptation. 
5.4 Results on English-French Translation 
We perform the same experiments for English to 
French translation. Table 7 describes the domain 
adaptation results. "
we use the target part of the NC corpus to train 
an in-domain LM, and then linearly interpolate it 
with the out-of-domain LM trained with the Eu-
roparl corpus. The results indicate that using an 
in-domain target corpus significantly improves 
the translation quality, achieving an improve-
ment of 2.19 BLEU score (from 25.44 to 27.63).  
Methods Out-of-domain LM 
Interpolated 
LM 
Europarl 25.44 27.63 
Europarl+Dic 26.24 28.22 
transductive 
learning - 2  8.80
Europarl+NC - 29.19 
Europarl+NC+Dic - 29.41 
T ation result f using i in 
d onolingual corpora 
Using the in-domain translation dictionary im-
used (from 
29.19 to 29.41). 
n Table 7. The results indicate 
th
28.80). Although the translation quality 
is 
This 
 an out-of-domain corpus to 
ystem, and then used an in-
ion dictionary, to improve the transla-
able 7. Transl
ictionary and m
s o n-doma
proves translation quality in all cases, even when 
the in-domain bilingual corpus is 
We also perform transductive learning with 
the source part of the NC corpus. The model 
used to translate the corpus is that one created by 
"Europarl+Dic" i
at transductive learning significantly improves 
translation quality, achieving an absolute im-
provement of 0.58 BLEU score (from 28.22 to 
28.80).  
In summary, using an in-domain dictionary 
and in-domain monolingual corpora improves the 
translation quality by 3.36 BLEU score (from 
25.44 to 
slightly lower than that method of using both 
in-domain and out-of-domain bilingual corpora, 
the difference is not statistically significant. 
6 Conclusion 
This paper proposed a domain adaptation ap-
proach for statistical machine translation. 
approach first used
build a baseline s
domain translation dictionary and in-domain 
monolingual corpora to adapt it to the in-domain 
texts. The contribution of this paper lies in the 
following points: 
? We proposed a method to integrate a do-
main-specific translation dictionary into a 
phrase-based SMT system for domain adap-
tation. 
? We investigated the way of using in-domain 
monolingual corpora in either source or tar-
get language, together with the in-domain 
translat
tion quality of a baseline system. 
999
We performed experiments on both Chinese to 
English and English to French translation. Ex-
perimental results on Chinese to English transla-
tio
-
vised Language Model Adaptation. In Proc. of the 
ational Conference on Acoustics, 
 Signal Processing (ICASSP-2003), 
Br
n. Computational Linguistics, 
Bu
f the 32nd International Confer-
Ca
 Statistical Ma-
Ci
 
, pages 177-180. 
Fo
Ko
ation of Machine Translation be-
Ko
erico, Nicola Bertoldi, 
Ko
istical Ma-
M
Oc
ranslation. In Proc. of 
Pa
2. BLEU: a Method for Auto-
Pa
ation Campaign. In Proc. of the International 
St
 Proc. of International 
Ue
rning for Statistical 
W
ics and Phrase-
n indicate that all of the in-domain resources 
are useful to improve in-domain translation qual-
ity, with an overall improvement of 8.16 BLEU 
score as compared with the baseline trained with 
out-of-domain corpora. Results on English to 
French translation also show that using in-
domain translation dictionaries and in-domain 
monolingual corpora is effective for domain ad-
aptation, achieving an absolute improvement of 
3.36 BLEU score. And the results on both trans-
lation tasks indicate that the translation quality 
achieved by our methods is comparable with that 
of the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
In the future work, we will investigate to as-
sign translation probabilities to the dictionaries 
using comparable in-domain corpora and exam-
ine its effect on the MT performance. And we 
will also examine the effect of an in-domain dic-
tionary on transductive learning in more details. 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper
28th Intern
Speech, and
pages 224-227. 
own, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimatio
19(2): 263-311. 
lyko, Ivan, Spyros Matsoukas, Richard Schwartz, 
Long Nguyen, and John Makhoul. 2007. Language 
Model Adaptation in Machine Translation from 
Speech. In Proc. o
ence on Acoustics, Speech, and Signal Processing 
(ICASSP-2007), pages 117-120. 
llison-Burch, Chris, Cameron Fordyce, Philipp 
Koehn, Christof Monz, and Josh Schroeder. 2007. 
(Meta-) Evaluation of Machine Translation. In 
Proc. of the Second Workshop on
chine Translation, pages 136-158. 
vera, Jorge and Alfons Juan. 2007. Domain Adapta-
tion in Statistical Machine Translation with Mix-
ture Modelling. In Proc. of the Second Workshop
on Statistical Machine Translation
ster, George and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second 
Workshop on Statistical Machine Translation, 
pages 128-135. 
ehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evalu
tween European Languages. In Proc. of the HLT-
NAACL 2006 Workshop on Statistical Machine 
Translation, pages 102-121. 
ehn, Philipp, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Fed
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proc. of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2007), 
demonstration session, pages 177-180. 
ehn, Philipp and Josh Schroeder. 2007. Experi-
ments in Domain Adaptation for Stat
chine Translation. In Proc. of the Second Workshop 
on Statistical Machine Translation, pages 224-227. 
unteanu, Dragos Stefan, and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
h, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine T
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), pages 160-
167. 
pineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 200
matic Evaluation of Machine Translation. In Proc. 
of the 40th Annual Meeting of the Association of 
Computational Linguistics (ACL-2002), pages 311-
318. 
ul, Michael. 2006. Overview of the IWSLT 2006 
Evalu
Workshop on Spoken Language Translation 
(IWSLT-2006), pages 1-15. 
olcke, Andrea. 2002. SRILM -- an Extensible Lan-
guage Modeling Toolkit. In
Conference on Spoken Language Processing 
(ICSLP-2002), pages 901-904. 
ffing, Nicola, Gholamreza Haffari, and Anoop 
Sarkar. 2007. Transductive Lea
Machine Translation. In Proc. of 45th Annual 
Meeting of the Association of Computational Lin-
guistics (ACL-2007), pages 25-32. 
u, Hua and Haifeng Wang. 2007. Comparative 
Study of Word Alignment Heurist
Based SMT. In Proc. of Machine Translation 
Summit XI, pages 507-514. 
1000
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 287?295, Prague, June 2007. c?2007 Association for Computational Linguistics
Using RBMT Systems to Produce Bilingual Corpus for SMT 
Xiaoguang Hu, Haifeng Wang, Hua Wu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cn 
 
 
Abstract 
This paper proposes a method using the ex-
isting Rule-based Machine Translation 
(RBMT) system as a black box to produce 
synthetic bilingual corpus, which will be 
used as training data for the Statistical Ma-
chine Translation (SMT) system. We use 
the existing RBMT system to translate the 
monolingual corpus into synthetic bilingual 
corpus. With the synthetic bilingual corpus, 
we can build an SMT system even if there 
is no real bilingual corpus. In our experi-
ments using BLEU as a metric, the system 
achieves a relative improvement of 11.7% 
over the best RBMT system that is used to 
produce the synthetic bilingual corpora. 
We also interpolate the model trained on a 
real bilingual corpus and the models 
trained on the synthetic bilingual corpora. 
The interpolated model achieves an abso-
lute improvement of 0.0245 BLEU score 
(13.1% relative) as compared with the in-
dividual model trained on the real bilingual 
corpus. 
1 Introduction 
Within the Machine Translation (MT) field, by far 
the most dominant paradigm is SMT, but many 
existing commercial systems are rule-based. In this 
research, we are interested in answering the ques-
tion of whether the existing RBMT systems could 
be helpful to the development of an SMT system. 
To find the answer, let us first consider the follow-
ing facts: 
? Existing RBMT systems are usually pro-
vided as a black box. To make use of such 
systems, the most convenient way might 
be working on the translation results di-
rectly. 
? SMT methods rely on bilingual corpus. As 
a data driven method, SMT usually needs 
large bilingual corpus as the training data. 
Based on the above facts, in this paper we pro-
pose a method using the existing RBMT system as 
a black box to produce a synthetic bilingual cor-
pus1, which will be used as the training data for the 
SMT system. 
For a given language pair, the monolingual cor-
pus is usually much larger than the real bilingual 
corpus. We use the existing RBMT system to 
translate the monolingual corpus into synthetic 
bilingual corpus. Then, even if there is no real bi-
lingual corpus, we can train an SMT system with 
the monolingual corpus and the synthetic bilingual 
corpus. If there exist n available RBMT systems 
for the desired language pair, we use the n systems 
to produce n synthetic bilingual corpora, and n 
translation models are trained with the n corpora 
respectively. We name such a model the synthetic 
model. An interpolated translation model is built 
by linear interpolating the n synthetic models. In 
our experiments using BLEU (Papineni et al, 2002) 
as the metric, the interpolated synthetic model 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora.  
                                                 
1 In this paper, to be distinguished from the real bilingual cor-
pus, the bilingual corpus generated by the RBMT system is 
called a synthetic bilingual corpus.  
287
Moreover, if a real bilingual corpus is available 
for the desired language pair, we build another 
translation model, which is named the standard 
model. Then we can build an interpolated model 
by interpolating the standard model and the syn-
thetic models. Experimental results show that the 
interpolated model achieves an absolute improve-
ment of 0.0245 BLEU score (13.1% relative) as 
compared with the standard model. 
The remainder of this paper is organized as fol-
lows. In section 2 we summarize the related work. 
We then describe our method Using RBMT sys-
tems to produce bilingual corpus for SMT in sec-
tion 3. Section 4 describes the resources used in the 
experiments. Section 5 presents the experiment 
result, followed by the discussion in section 6. Fi-
nally, we conclude and present the future work in 
section 7. 
2 Related Work 
In the MT field, by far the most dominant 
paradigm is SMT. SMT has evolved from the 
original word-based approach (Brown et al, 1993) 
into phrase-based approaches (Koehn et al, 2003; 
Och and Ney, 2004) and syntax-based approaches 
(Wu, 1997; Alshawi et al, 2000; Yamada and 
Knignt, 2001; Chiang, 2005). On the other hand, 
much important work continues to be carried out in 
Example-Based Machine Translation (EBMT) 
(Carl et al, 2005; Way and Gough, 2005), and 
many existing commercial systems are rule-based. 
Although we are not aware of any previous at-
tempt to use an existing RBMT system as a black 
box to produce synthetic bilingual training corpus 
for general purpose SMT systems, there exists a 
great deal of work on MT hybrids and Multi-
Engine Machine Translation (MEMT). 
Research into MT hybrids has increased over the 
last few years. Some research focused on the hy-
brid of various corpus-based MT methods, such as 
SMT and EBMT (Vogel and Ney, 2000; Marcu, 
2001; Groves and Way, 2006; Menezes and Quirk, 
2005). Others tried to exploit the advantages of 
both rule-based and corpus-based methods. Habash 
et al (2006) built an Arabic-English generation-
heavy MT system and boosted it with SMT com-
ponents. METIS-II is a hybrid machine translation 
system, in which insights from SMT, EBMT, and 
RBMT are used (Vandeghinste et al, 2006). Seneff 
et al (2006) combined an interlingual translation 
framework with phrase-based SMT for spoken 
language translation in a limited domain. They 
automatically generated a corpus of English-
Chinese pairs from the same interlingual represen-
tation by parsing the English corpus and then para-
phrasing each utterance into both English and Chi-
nese. 
Frederking and Nirenburg (1994) produced the 
first MEMT system by combining outputs from 
three different MT engines based on their knowl-
edge of the inner workings of the engines. Nomoto 
(2004) used voted language models to select the 
best output string at sentence level. Some recent 
approaches to MEMT used word alignment tech-
niques for comparison between the MT systems 
(Jayaraman and Lavie, 2005; Zaanen and Somers, 
2005; Matusov et al 2006). All the above MEMT 
systems operate on MT outputs for complete input 
sentences. Mellebeek et al (2006) presented a dif-
ferent approach, using a recursive decomposition 
algorithm that produces simple chunks as input to 
the MT engines. A consensus translation is pro-
duced by combining the best chunk translation. 
This paper uses RBMT outputs to improve the 
performance of SMT systems. Instead of RBMT 
outputs, other researchers have used SMT outputs 
to boost translation quality. Callision-Burch and 
Osborne (2003) used co-training to extend existing 
parallel corpora, wherein machine translations are 
selectively added to training corpora with multiple 
source texts. They also created training data for a 
language pair without a parallel corpus by using 
multiple source texts. Ueffing (2006) explored 
monolingual source-language data to improve an 
existing machine translation system via self-
training. The source data is translated by a SMT 
system, and the reliable translations are automati-
cally identified. Both of the methods improved 
translation quality. 
3 Method 
In this paper, we use the synthetic and real bilin-
gual corpus to train the phrase-based translation 
models. 
3.1  Phrase-Based Models 
According to the translation model presented in 
(Koehn et al, 2003), given a source sentence f , 
the best target translation  can be obtained 
using the following model 
beste
288
)()()(maxarg
)(maxarg
|
|
e
e
e
eef
fee
length
LM
best
?pp
p
=
=
 (1)
Where the translation model can be 
decomposed into  
)( | efp
?
=
??=
I
i
iiiiii
II
aefpbadef
efp
1
1
11
),|()()|(
)|(
?? w
(2)
Where )|( ii ef?  is the phrase translation prob-
ability.  denotes the start position of the source 
phrase that was translated into the ith target phrase, 
and  denotes the end position of the source 
phrase translated into the (i-1)th target phrase. 
 is the distortion probability. 
ia
1?ib
)( 1?? ii bad
),|( aefp iiw  is the lexical weight, and ?  is the 
strength of the lexical weight. 
3.2 Interpolated Models 
We train synthetic models with the synthetic bilin-
gual corpus produced by the RBMT systems. We 
can also train a translation model, namely standard 
model, if a real bilingual corpus is available. In 
order to make full use of these two kinds of cor-
pora, we conduct linear interpolation between them. 
In this paper, the distortion probability in equa-
tion (2) is estimated during decoding, using the 
same method as described in Pharaoh (Koehn, 
2004). For the phrase translation probability and 
lexical weight, we interpolate them as shown in (3) 
and (4). 
?
=
=
n
i
ii efef
0
)|()|( ???  (3)
?
=
=
n
i
ii aefpaefp
0
),|(),|( w,w ?  (4)
Where )|(0 ef?  and ),|( aefpw,0  denote the 
phrase translation probability and lexical weight 
trained with the real bilingual corpus, respectively. 
)|( efi?  and ),|( aefp iw,  ( ) are the 
phrase translation probability and lexical weight 
estimated by n  synthetic corpora produced by the 
RBMT systems. 
ni ,...,1=
i?  and i?  are interpolation coef-
ficients, ensuring  and . 1
0
=?
=
n
i
i? 1
0
=?
=
n
i
i?
4 Resources Used in Experiments 
4.1 Data 
In the experiments, we take English-Chinese trans-
lation as a case study. The real bilingual corpus 
includes 494,149 English-Chinese bilingual sen-
tence pairs. The monolingual English corpus is 
selected from the English Gigaword Second Edi-
tion, which is provided by Linguistic Data Consor-
tium (LDC) (catalog number LDC2005T12). The 
selected monolingual corpus includes 1,087,651 
sentences. 
For language model training, we use part of the 
Chinese Gigaword Second Edition provided by 
LDC (catalog number LDC2005T14). We use 
41,418 documents selected from the ZaoBao 
Newspaper and 992,261 documents from the Xin-
Hua News Agency to train the Chinese language 
model, amounting to 5,398,616 sentences. 
The test set and the development set are from 
the corpora distributed for the 2005 HTRDP 2  
evaluation of machine translation.  It can be ob-
tained from Chinese Linguistic Data Consortium 
(catalog number 2005-863-001). We use the same 
494 sentences in the test set and 278 sentences in 
the development set. Each source sentence in the 
test set and the development set has 4 different ref-
erences. 
4.2 Tools 
In this paper, we use two off-the-shelf commercial 
English to Chinese RBMT systems to produce the 
synthetic bilingual corpus. 
We also need a trainer and a decoder to perform 
phrase-based SMT. We use Koehn's training 
scripts 3  to train the translation model, and the 
SRILM toolkit (Stolcke, 2002) to train language 
model. For the decoder, we use Pharaoh (Koehn, 
2004). We run the decoder with its default settings 
(maximum phrase length 7) and then use Koehn's 
implementation of minimum error rate training 
(Och, 2003) to tune the feature weights on the de-
                                                 
2 The full name of HTRDP is National High Technology Re-
search and Development Program of China, also named as 863 
Program. 
3  It is located at http://www.statmt.org/wmt06/shared-
task/baseline.html. 
289
velopment set. The translation quality is evaluated 
using a well-established automatic measure: BLEU 
score (Papineni et al, 2002). We use the same 
method described in (Koehn and Monz, 2006) to 
perform the significance test. 
5 Experimental Results 
5.1 Results on Synthetic Corpus Only 
With the monolingual English corpus and the Eng-
lish side of the real bilingual corpus, we translate 
them into Chinese using the two commercial 
RBMT systems and produce two synthetic bilin-
gual corpora. With the corpora, we train two syn-
thetic models as described in section 3.1. Based on 
the synthetic models, we also perform linear inter-
polation as shown in section 3.2, without the stan-
dard models. We tune the interpolation weights 
using the development set, and achieve the best 
performance when 58.01 =? , 42.02 =? , 
58.01 =? , and 42.02 =? . The translation results 
on the test set are shown in Table 1. Synthetic 
model 1 and 2 are trained using the synthetic bilin-
gual corpora produced by RBMT system 1 and 
RBMT system 2, respectively. 
Method BLEU 
RBMT system 1 0.1681 
RBMT system 2 0.1453 
Synthetic Model 1 0.1644 
Synthetic Model 2 0.1668 
Interpolated Synthetic Model 0.1878 
Table 1. Translation Results Using Synthetic Bi-
lingual Corpus 
From the results, it can be seen that the interpo-
lated synthetic model obtains the best result, with 
an absolute improvement of the 0.0197 BLEU 
(11.7% relative) as compared with RBMT system 
1, and 0.0425 BLEU (29.2% relative) as compared 
with RBMT system 2. It is very promising that our 
method can build an SMT system that significantly 
outperforms both of the two RBMT systems, using 
the synthetic bilingual corpus produced by two 
RBMT systems. 
5.2 Results on Real and Synthetic Corpus 
With the real bilingual corpus, we build a standard 
model. We interpolate the standard model with the 
two synthetic models built in section 5.1 to obtain 
interpolated models. The translation results are 
shown in Table 2. The interpolation coefficients 
are both for phrase table probabilities and lexical 
weights. They are also tuned using the develop-
ment set.  
From the results, it can be seen that all the three 
interpolated models perform not only better than 
the RBMT systems but also better than the SMT 
system trained on the real bilingual corpus. The 
interpolated model combining the standard model 
and the two synthetic models performs the best, 
achieving a statistically significant improvement of 
about 0.0245 BLEU (13.1% relative) as compared 
with the standard model with no synthetic corpus. 
It also achieves 26.1% and 45.8% relative im-
provement as compared with the two RBMT sys-
tems respectively. The results indicate that using 
the corpus produced by RBMT systems, the per-
formance of the SMT system can be greatly im-
proved. The results also indicate that the more the 
RBMT systems are used, the better the translation 
quality is. 
Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
0.90 0.10 ? 0.2056 
0.86 ? 0.14 0.2040 
0.70 0.12 0.18 0.2119 
Table 2. Translation Results Using Standard and 
Synthetic Bilingual Corpus 
5.3 Effect of Synthetic Corpus Size 
To explore the relationship between the translation 
quality and the scale of the synthetic bilingual cor-
pus, we interpolate the standard model with the 
synthetic models trained with synthetic bilingual 
corpus of different sizes. In order to simplify the 
procedure, we only use RBMT system 1 to trans-
late the 1,087,651 monolingual English sentences 
to produce the synthetic bilingual corpus.  
We randomly select 20%, 40%, 60%, 80%, and 
100% of the synthetic bilingual corpus to train dif-
ferent synthetic models. The translation results of 
the interpolated models are shown in Figure 1. The 
results indicate that the larger the synthetic bilin-
gual corpus is, the better translation performance 
would be. 
290
0.13
0.15
0.17
0.19
0.21
20 40 60 80 100
Synthetic Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
0.12
0.14
0.16
0.18
0.2
0.22
20 40 60 80 100
Real Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
Figure 1. Comparison of Translation Results Using 
Synthetic Bilingual Corpus of Different Sizes 
Figure 2. Comparison of Translation Results Using 
Real Bilingual Corpus of Different Sizes 
5.4 Effect of Real Corpus Size Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
? 1 ? 0.1560 
? ? 1 0.1522 
0.80 0.10 0.10 0.1972 
Another issue is the relationship between the SMT 
performance and the size of the real bilingual cor-
pus. To train different standard models, we ran-
domly build five corpora of different sizes, which 
contain 20%, 40%, 60%, 80%, and 100% sentence 
pairs of the real bilingual corpus, respectively. As 
to the synthetic model, we use the same synthetic 
model 1 that is described in section 5.1. Then we 
build five interpolated models by performing linear 
interpolation between the synthetic model and the 
five standard models respectively.  The translation 
results are shown in Figure 2.  
Table 3. Translation Results without Additional 
Monolingual Corpus 
 Standard Model 
Synthetic 
Model 1 
Synthetic 
Model 2 
Standard 
Model 6,105,260 ? ? 
Synthetic 
Model 1 356,795 12,062,068 ? 
Synthetic 
Model 2 357,489 881,921 9,216,760
From the results, we can see that the larger the 
real bilingual corpus is, the better the performance 
of both standard models and interpolated models 
would be. The relative improvement of BLEU 
scores is up to 27.5% as compared with the corre-
sponding standard models. 
Table 4. Numbers of Phrase Pairs  5.5 Results without Additional Monolingual 
Corpus cant improvement of about 0.01 BLEU (5.2% rela-
tive) as compared with the standard model without 
using the synthetic corpus. In all the above experiments, we use an additional English monolingual corpus to get more synthetic 
bilingual corpus. We are also interested in the re-
sults without the additional monolingual corpus. In 
such case, the only English monolingual corpus is 
the English side of the real bilingual corpus. We 
use this smaller size of monolingual corpus and the 
real bilingual corpus to conduct similar experi-
ments as in section 5.2. The translation results are 
shown in Table 3. 
In order to further analyze the translation results, 
we examine the overlap and the difference among 
the phrase tables. The analytic results are shown in 
Table 4. More phrase pairs are extracted by the 
synthetic models, about twice by the synthetic 
model 1 in particular, than those extracted by the 
standard model. The overlap between each model 
is very low. For example, about 6% phrase pairs 
extracted by the standard model make appearance 
in both the standard model and the synthetic model 
1. This also explains why the interpolated model 
outperforms that of the standard model in Table 3.  
From the results, it can be seen that our method 
works well even if no additional monolingual cor-
pus is available. We achieve a statistically signifi- 
291
Methods English Sentence / Chinese Translations BLEU
 
This move helps spur the enterprise to strengthen technical innovation, man-
agement innovation and the creation of a brand name and to strengthen mar-
keting, after-sale service, thereby fundamentally enhance the enterprise's 
competitiveness; 
 
Standard 
model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
?? ? ?? ? ?? ? ?? ? ?? ?? ? ??? ? ? ?? ?? 0.5022
RBMT Sys-
tem 1 
?? ?? ?? ?? ?? ?? ?? ? ?? ? ?? ?? ? ?? ?? 
?? ?? ?? ?? ? ? ?? ?? ? ??? ?? ?? ?? ? ?
? ? 
0.1535
RBMT Sys-
tem 2 
?? ?? ?? ?? ?? ?? ?? ?? ?? ? ?? ?? ? ? ? 
?? ? ?? ? ?? ?? ?? ? ?? ???? ?? ?? ?? ?? 
? ?? ? 
0.1485
Interpolated 
Model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
? ?? ?? ?? ? ???? ? ?? ? ?? ? ?? ?? ? ??
? ? 
0.7198
Table 5. Translation Example 
This move  ? ? ?? This move  ? ? ?? 
helps  ??? helps  ??? 
spur  ?? spur  ?? 
the enterprise  ?? the enterprise  ?? 
to strengthen  ?? to strengthen  ?? 
technical  ?? technical  ?? 
innovation  ?? innovation  ?? 
, management  ? ?? , management  ? ?? 
innovation  ?? innovation  ?? 
and the creation of a  ? ?? and the creation of  ? ?? 
  (he jianli)  (he chuangzao) 
brand name  ?? a brand name  ?? 
  (pinpai)  (pinpai) 
and to strengthen  ?? ? and to strengthen  ? ?? 
marketing ,  ?? marketing ,  ?? ?? ? 
  (fuwu) after-sale service  ???? 
after-sale  ? ??  (shouhoufuwu) 
service  ? ?? ? , thereby  ? ?? 
, thereby  ?? fundamentally  ? ?? ? 
fundamentally  ?? enhance the  ?? 
enhance  ? ??? enterprise 's  ?? ? 
the enterprise  ? competitiveness  ??? 
's competitiveness  ? ?? ;  ? 
;  ??   
  (shouhou)   
(a) Results Produced by the Standard Model (b) Results Produced by the Interpolated Model 
Figure 3. Phrase Pairs Used for Translation 
 
292
6 Discussion 
6.1 Model Interpolation vs. Corpus Merge 
In section 5, we make use of the real bilingual cor-
pus and the synthetic bilingual corpora by perform-
ing model interpolation. Another available way is 
directly combining these two kinds of corpora to 
train a translation model, namely corpus merge. In 
order to compare these two methods, we use 
RBMT system 1 to translate the 1,087,651 mono-
lingual English sentences to produce synthetic bi-
lingual corpus. Then we train an SMT system with 
the combination of this synthetic bilingual corpus 
and the real bilingual corpus. The BLEU score of 
such system is 0.1887, while that of the model in-
terpolation system is 0.2020. It indicates that the 
model interpolation method is significantly better 
than the corpus merge method. 
6.2 Result Analysis 
As discussed in Section 5.5, the number of the 
overlapped phrase pairs among the standard model 
and the synthetic models is very small. The newly 
added phrase pairs from the synthetic models can 
assist to improve the translation results of the in-
terpolated model. In this section, we will use an 
example to further discuss the reason behind the 
improvement of the SMT system by using syn-
thetic bilingual corpus. Table 5 shows an English 
sentence and its Chinese translations produced by 
different methods. And Figure 3 shows the phrase 
pairs used for translation. The results show that 
imperfect translations of RBMT systems can be 
also used to boost the performance of an SMT sys-
tem. 
 Phrase Pairs 
Phrase 
Pairs 
Used 
New 
Pairs 
Used 
Standard 
Model 6,105,260 5,509 ? 
Interpolated 
Model 73,221,525 5,306 1993 
Table 6. Statistics of Phrase Pairs 
Further analysis is shown in Table 6. After add-
ing the synthetic corpus produced by the RBMT 
systems, the interpolated model outperforms the 
standard models mainly for the following two rea-
sons: (1) some new phrase pairs are added into the 
interpolated model. 37.6% phrase pairs (1993 out 
of 5306) are newly learned and used for translation. 
For example, the phrase pair "after-sale service <-> 
???? (shouhoufuwu)" is added; (2) The prob-
ability distribution of the phrase pairs is changed. 
For example, the probabilities of the two pairs "a 
brand name <-> ?? (pinpai)" and "and the crea-
tion of <-> ? ?? (he chuangzao)" increase. The 
probabilities of the other two pairs "brand name <-
> ?? (pinpai)" and "and the creation of a <-> ? 
??  (he jianli)" decrease. We found that 930 
phrase pairs, which are also in the phrase table of 
the standard model, are used by the interpolated 
model for translation but not used by the standard 
model. 
6.3 Human Evaluation 
According to (Koehn and Monz, 2006; Callison-
Burch et al, 2006), the RBMT systems are usually 
not adequately appreciated by BLEU. We also 
manually evaluated the RBMT systems and SMT 
systems in terms of both adequacy and fluency as 
defined in (Koehn and Monz, 2006). The evalua-
tion results show that the SMT system with the 
interpolated model, which achieves the highest 
BLEU scores in Table 2, achieves slightly better 
adequacy and fluency scores than the two RBMT 
systems. 
7 Conclusion and Future Work 
We presented a method using the existing RBMT 
system as a black box to produce synthetic bilin-
gual corpus, which was used as training data for 
the SMT system. We used the existing RBMT sys-
tem to translate the monolingual corpus into a syn-
thetic bilingual corpus. With the synthetic bilingual 
corpus, we could build an SMT system even if 
there is no real bilingual corpus. In our experi-
ments using BLEU as the metric, such a system 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora. It indicates that using the 
existing RBMT systems to produce a synthetic bi-
lingual corpus, we can build an SMT system that 
outperforms the existing RBMT systems. 
We also interpolated the model trained on a real 
bilingual corpus and the models trained on the syn-
thetic bilingual corpora, the interpolated model 
achieves an absolute improvement of 0.0245 
BLEU score (13.1% relative) as compared with the 
individual model trained on the real bilingual cor-
293
pus. It indicates that we can build a better SMT 
system by leveraging the real and the synthetic bi-
lingual corpus. 
Further result analysis shows that after adding 
the synthetic corpus produced by the RBMT sys-
tems, the interpolated model outperforms the stan-
dard models mainly because of two reasons: (1) 
some new phrase pairs are added to the interpo-
lated model; (2) the probability distribution of the 
phrase pairs is changed. 
In the future work, we will investigate the possi-
bility of training a reverse SMT system with the 
RBMT systems. For example, we will investigate 
to train Chinese-to-English SMT system based on 
natural English and RBMT-generated synthetic 
Chinese. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning Dependency Translation Models as 
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1): 45-60. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2): 
263-311. 
Chris Callison-Burch and Miles Osborne. 2003. Boot-
strapping Parallel Corpora. In Proceedings of the 
Human Language Technology conference / North 
American chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2003) Workshop on 
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 44-49. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn, 2006. Re-evaluating the Role of Bleu in Ma-
chine Translation Research. In Proceedings of the 
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
2006), pages 249-256. 
Michel Carl, Paul Schmidt, and Jorg Schutz. 2005. Re-
versible Template-based Shake & Bake Generation. 
In Proceedings of the 10th Machine Translation 
Summit Workshop on Example-Based Machine 
Translation, pages 17-25. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2005), 
pages 263-270. 
Robert Frederking and Sergei Nirenburg. 1994. Three 
Heads Are Better Than One. In Proceedings of the 
4th Applied Natural Language Processing Confer-
ence (ANLP-1994), pages 95-100. 
Declan Groves and Andy Way. 2006. Hybridity in MT: 
Experiments on the Europarl Corpus. In Proceedings 
of the 11th Annual Conference of the European As-
sociation for Machine Translation (EAMT-2006), 
pages 115-124. 
Nizar Habash, Bonnie Dorr, and Christof Monz. 2006 
Challenges in Building an Arabic-English GHMT 
System with SMT Components. In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation (EAMT-2006), pages 
56-65. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine Machine Translation Guided by Explicit 
Word Matching. In Proceedings of the 10th Annual 
Conference of the European Association for Machine 
Translation (EAMT-2005), pages 143-152. 
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder 
For Phrase-Based Statistical Machine Translation 
Models. In Proceedings of the 6th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2004), pages 115-124. 
Philipp Koehn and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proceedings of the 
Human Language Technology conference / North 
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2006) Workshop on 
Statistical Machine Translation, pages 102-121. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology con-
ference / North American Chapter of the Association 
for Computational Linguistics (HLT/NAACL-2003), 
pages 127-133. 
Daniel Marcu. 2001. Towards a Unified Approach to 
Memory- and Statistical-based Machine Translation. 
In Proceedings of the Association for Computational 
Linguistics / European Chapter of the Association for 
Computational Linguistics (ACL/EACL-2001), pages 
378-385. 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced 
Hypotheses Alignment. In Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), 
pages 33-40. 
294
Bart Mellebeek, Karolina Owczarzak, Josef Van 
Genabith, and Andy Way. 2006. Multi-engine Ma-
chine Translation by Recursive Sentence Decomposi-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas 
(AMTA-2006), pages 110-118. 
Arul Menezes and Chris Quirk. 2005. Dependency 
treelet translation: the convergence of statistical and 
example-based machine-translation? In Proceedings 
of the 10th Machine Translation Summit Workshop 
on Example-Based Machine Translation, pages 99-
108. 
Tadashi Nomoto. 2004. Multi-Engine machine transla-
tion with voted language model. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 494-
501. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-2003), pages 160-167. 
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach To Statistical Machine 
Translation. Computational Linguistics, 30(4):417-
449. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL-2002), pages 311-
318. 
Stephanie Seneff, Chao Wang, and John Lee. 2006. 
Combining Linguistic and Statistical Methods for Bi-
Directional English Chinese Translation in the Flight 
Domain. In Proceedings of the 7th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2006), pages 213-222. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the 5th 
International Conference on Spoken Language Proc-
essing (ICSLP-2002), pages 901-904. 
Nicola Ueffing. 2006. Using Monolingual Source-
Language Data to Improve MT Performance.  In 
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT-2006), pages 174-
181. 
Vincent Vandeghinste, Ineka Schuurman, Michael Carl, 
Stella Markantonatou, and Toni Badia. 2006. Metis-
II: Machine Translation for Low-Resource Lan-
guages. In Proceedings of the 5th International Con-
ference on Language Resources and Evaluation (L-
REC-2006), pages 1284-1289. 
Stephan Vogel and Hermann Ney. 2000. Construction 
of a Hierarchical Translation Memory. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING-2000), pages 1131-
1135. 
Andy Way and Nano Gough. 2005. Comparing Exam-
ple-Based and Statistical Machine Translation. Natu-
ral Language Engineering, 11(3): 295-309. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3): 377-403. 
Kenji Yamada and Kevin Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proceedings of the 
Association for Computational Linguistics / Euro-
pean Chapter of the Association for Computational 
Linguistics (ACL/EACL-2001), pages 523-530. 
Menno van Zaanen and Harold Somers. 2005. DE-
MOCRAT: Deciding between Multiple Outputs Cre-
ated by Automatic Translation. In Proceedings of the 
10th Machine Translation Summit, pages 173-180. 
 
 
295
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487?495,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Collocation Extraction Using Monolingual Word Alignment Method 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Toshiba (China) Research and Development Center, Beijing, China 
{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn 
lisheng@hit.edu.cn 
 
  
 
Abstract 
Statistical bilingual word alignment has been 
well studied in the context of machine trans-
lation. This paper adapts the bilingual word 
alignment algorithm to monolingual scenario 
to extract collocations from monolingual cor-
pus. The monolingual corpus is first repli-
cated to generate a parallel corpus, where 
each sentence pair consists of two identical 
sentences in the same language. Then the 
monolingual word alignment algorithm is 
employed to align the potentially collocated 
words in the monolingual sentences. Finally 
the aligned word pairs are ranked according 
to refined alignment probabilities and those 
with higher scores are extracted as colloca-
tions. We conducted experiments using Chi-
nese and English corpora individually. Com-
pared with previous approaches, which use 
association measures to extract collocations 
from the co-occurring word pairs within a 
given window, our method achieves higher 
precision and recall. According to human 
evaluation in terms of precision, our method 
achieves absolute improvements of 27.9% on 
the Chinese corpus and 23.6% on the English 
corpus, respectively. Especially, we can ex-
tract collocations with longer spans, achiev-
ing a high precision of 69% on the long-span 
(>6) Chinese collocations. 
1 Introduction 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). In this pa-
per, a collocation is composed of two words oc-
curring as either a consecutive word sequence or 
an interrupted word sequence in sentences, such 
as "by accident" or "take ? advice". The collo-
cations in this paper include phrasal verbs (e.g. 
"put on"), proper nouns (e.g. "New York"), idi-
oms (e.g. "dry run"), compound nouns (e.g. "ice 
cream"), correlative conjunctions (e.g. "either ? 
or"), and the other commonly used combinations 
in following types: verb+noun, adjective+noun, 
adverb+verb, adverb+adjective and adjec-
tive+preposition (e.g. "break rules", "strong tea", 
"softly whisper", "fully aware", and "fond of"). 
Many studies on collocation extraction are 
carried out based on co-occurring frequencies of 
the word pairs in texts (Choueka et al, 1983; 
Church and Hanks, 1990; Smadja, 1993; Dun-
ning, 1993; Pearce, 2002; Evert, 2004). These 
approaches use association measures to discover 
collocations from the word pairs in a given win-
dow. To avoid explosion, these approaches gen-
erally limit the window size to a small number. 
As a result, long-span collocations can not be 
extracted1. In addition, since the word pairs in 
the given window are regarded as potential col-
locations, lots of false collocations exist. Al-
though these approaches used different associa-
tion measures to filter those false collocations, 
the precision of the extracted collocations is not 
high. The above problems could be partially 
solved by introducing more resources into collo-
cation extraction, such as chunker (Wermter and 
Hahn, 2004), parser (Lin, 1998; Seretan and We-
hrli, 2006) and WordNet (Pearce, 2001). 
This paper proposes a novel monolingual 
word alignment (MWA) method to extract collo-
cation of higher quality and with longer spans 
only from monolingual corpus, without using 
any additional resources. The difference between 
MWA and bilingual word alignment (Brown et 
al., 1993) is that the MWA method works on 
monolingual parallel corpus instead of bilingual 
corpus used by bilingual word alignment. The 
                                                 
1  Here, "span of collocation" means the distance of two 
words in a collocation. For example, if the span of the col-
location (w1, w2) is 6, it means there are 5 words interrupt-
ing between w1 and w2 in a sentence. 
487
monolingual corpus is replicated to generate a 
parallel corpus, where each sentence pair con-
sists of two identical sentences in the same lan-
guage, instead of a sentence in one language and 
its translation in another language. We adapt the 
bilingual word alignment algorithm to the mono-
lingual scenario to align the potentially collo-
cated word pairs in the monolingual sentences, 
with the constraint that a word is not allowed to 
be aligned with itself in a sentence. In addition, 
we propose a ranking method to finally extract 
the collocations from the aligned word pairs. 
This method assigns scores to the aligned word 
pairs by using alignment probabilities multiplied 
by a factor derived from the exponential function 
on the frequencies of the aligned word pairs. The 
pairs with higher scores are selected as colloca-
tions. 
The main contribution of this paper is that the 
well studied bilingual statistical word alignment 
method is successfully adapted to monolingual 
scenario for collocation extraction. Compared 
with the previous approaches, which use associa-
tion measures to extract collocations, our method 
achieves much higher precision and slightly 
higher recall. The MWA method has the follow-
ing three advantages. First, it explicitly models 
the co-occurring frequencies and position infor-
mation of word pairs, which are integrated into a 
model to search for the potentially collocated 
word pairs in a sentence. Second, a new feature, 
fertility, is employed to model the number of 
words that a word can collocate with in a sen-
tence. Finally, our method can obtain the long-
span collocations. Human evaluations on the ex-
tracted Chinese collocations show that 69% of 
the long-span (>6) collocations are correct. Al-
though the previous methods could also extract 
long-span collocations by setting the larger win-
dow size, the precision is very low. 
In the remainder of this paper, Section 2 de-
scribes the MWA model for collocation extrac-
tion. Section 3 describes the initial experimental 
results. In Section 4, we propose a method to 
improve the MWA models. Further experiments 
are shown in Sections 5 and 6, followed by a dis-
cussion in Section 7. Finally, the conclusions are 
presented in Section 8. 
2 Collocation Extraction With Mono-
lingual Word Alignment Method 
2.1 Monolingual Word Alignment 
Given a bilingual sentence pair, a source lan-
guage word can be aligned with its correspond- 
 
Figure 1. Bilingual word alignment 
ing target language word. Figure 1 shows an ex-
ample of Chinese-to-English word alignment. 
In Figure 1, a word in one language is aligned 
with its counterpart in the other language. For 
examples, the Chinese word "??/tuan-dui" is 
aligned with its English translation "team", while 
the Chinese word "???/fu-ze-ren" is aligned 
with its English translation "leader". 
In the Chinese sentence in Figure 1, there are 
some Chinese collocations, such as (??/tuan-
dui, ???/fu-ze-ren). There are also some Eng-
lish collocations in the English sentence, such as 
(team, leader). We separately illustrate the collo-
cations in the Chinese sentence and the English 
sentence in Figure 2, where the collocated words 
are aligned with each other. 
 
(a) Collocations in the Chinese sentence 
 
(b) Collocations in the English sentence 
Figure 2. Word alignments of collocations in 
sentence 
Comparing the alignments in Figures 1 and 2, 
we can see that the task of monolingual colloca-
tions construction is similar to that of bilingual 
word alignment. In a bilingual sentence pair, a 
source word is aligned with its corresponding 
target word, while in a monolingual sentence, a 
word is aligned with its collocates. Therefore, it 
is reasonable to regard collocation construction 
as a task of aligning the collocated words in 
monolingual sentences. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
The team leader plays a key role in the project undertaking . 
The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
??  ??? ? ??  ??  ?  ? ??  ??   ?
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
488
Statistical bilingual word alignment method, 
which has been well studied in the context of 
machine translation, can extract the aligned bi-
lingual word pairs from a bilingual corpus. This 
paper adapts the bilingual word alignment algo-
rithm to monolingual scenario to align the collo-
cated words in a monolingual corpus. 
Given a sentence with l words },...,{ 1 lwwS = , 
the word alignments ]},1[|),{( liaiA i ?=  can be 
obtained by maximizing the word alignment 
probability of the sentence, according to Eq. (1). 
)|(maxarg SApA
A
?=
??
                    (1) 
Where Aai i ?),(  means that the word iw  is 
aligned with the word 
ia
w . 
In a monolingual sentence, a word never col-
locates with itself. Thus the alignment set is de-
noted as }&],1[|),{( ialiaiA ii ??= . 
We adapt the bilingual word alignment model, 
IBM Model 3 (Brown et al, 1993), to monolin-
gual word alignment. The probability of the 
alignment sequence is calculated using Eq. (2). 
???
==
l
j
jaj
l
i
ii lajdwwtwnSAp j
11
),|()|()|()|( ?   (2) 
Where i?  denotes the number of words that are 
aligned with iw . Three kinds of probabilities are 
involved: 
- Word collocation probability )|(
jaj
wwt , 
which describes the possibility of wj collo-
cating with 
ja
w ;  
- Position collocation probability d(j, aj, l), 
which describes the probability of a word 
in position aj collocating with another 
word in position j; 
- Fertility probability )|( ii wn ? , which de-
scribes the probability of the number of 
words that a word wi can collocate with 
(refer to subsection 7.1 for further discus-
sion). 
Figure 3 shows an example of word alignment 
on the English sentence in Figure 2 (b) with the 
MWA method. In the sentence, the 7th word 
"role" collocates with both the 4th word "play" 
and the 6th word "key". Thus, )|( 74 wwt  and 
)|( 76 wwt  describe the probabilities that the 
word "role" collocates with "play" and "key",  
 
Figure 3. Results of MWA method 
respectively. )12,7|4(d  and )12,7|6(d  describe 
the probabilities that the word in position 7 col-
locates with the words in position 4 and 6 in a 
sentence with 12 words. For the word "role", 7?  
is 2, which indicates that the word "role" collo-
cates with two words in the sentence. 
To train the MWA model, we implement a 
MWA tool for collocation extraction, which uses 
similar training methods for bilingual word 
alignment, except that a word can not be aligned 
to itself. 
2.2 Collocation Extraction 
Given a monolingual corpus, we use the trained 
MWA model to align the collocated words in 
each sentence. As a result, we can generate a set 
of aligned word pairs on the corpus. According 
to the alignment results, we calculate the fre-
quency for two words aligned in the corpus, de-
noted as ),( ji wwfreq . In our method, we filtered 
those aligned word pairs whose frequencies are 
lower than 5. Based on the alignment frequency, 
we estimate the alignment probabilities for each 
aligned word pair as shown in Eq. (3) and (4). 
? ?=
?w j
ji
ji wwfreq
wwfreq
wwp
),(
),(
)|(  (3) 
? ?=
?w i
ji
ij wwfreq
wwfreq
wwp
),(
),(
)|(  (4) 
With alignment probabilities, we assign scores 
to the aligned word pairs and those with higher 
scores are selected as collocations, which are 
estimated as shown in Eq. (5). 
2
)|()|(
),( ijjiji
wwpwwp
wwp
+=      (5) 
3 Initial Experiments 
In this experiment, we used the method as de-
scribed in Section 2 for collocation extraction. 
Since our method does not use any linguistic in-
formation, we compared our method with the  
The team leader plays a key role in the project undertaking . 
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12) 
The team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12) 
489
02
4
6
8
10
12
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Our method (Probability)
Log-likelihood ratio
 
Figure 4. Precision of collocations 
baseline methods without using linguistic knowl-
edge. These baseline methods take all co-
occurring word pairs within a given window as 
collocation candidates, and then use association 
measures to rank the candidates. Those candi-
dates with higher association scores are extracted 
as collocations. In this paper, the window size is 
set to [-6, +6]. 
3.1 Data 
The experiments were carried out on a Chinese 
corpus, which consists of one year (2004) of the 
Xinhua news corpus from LDC 2 , containing 
about 28 millions of Chinese words. Since punc-
tuations are rarely used to construct collocations, 
they were removed from the corpora. To auto-
matically estimate the precision of extracted col-
locations on the Chinese corpus, we built a gold 
set by collecting Chinese collocations from 
handcrafted collocation dictionaries, containing 
56,888 collocations. 
3.2 Results 
The precision is automatically calculated against 
the gold set according to Eq. (6). 
)(#
)(#
Top
goldTop
N
N
C
CC
precision
?
?= I            (6) 
Where CTop-N and Cgold denote the top colloca-
tions in the N-best list and the collocations in the 
gold set, respectively. 
We compared our method with several base-
line methods using different association meas-
ures3: co-occurring frequency, log-likelihood 
                                                 
2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog 
Entry.jsp?catalogId=LDC2007T03 
3 The definitions of these measures can be found in Man-
ning and Sch?tze (1999). 
0
20
40
60
80
100
0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9
log(frequency)
(%
)
Precision
Alignment Probability
 
Figure 5. Frequency vs. precision/alignment 
probability 
ratio, chi-square test, mutual information, and t-
test. Among them, the log-likelihood ratio meas-
ure achieves the best performance. Thus, in this 
paper, we only show the performance of the log-
likelihood ratio measure. 
Figure 4 shows the precisions of the top N col-
locations as N steadily increases with an incre-
ment of 1K, which are extracted by our method 
and the baseline method using log-likelihood 
ratio as the association measure. 
The absolute precision of collocations is not 
high in the figure. For example, among the top 
200K collocations, about 4% of the collocations 
are correct. This is because our gold set contains 
only about 57K collocations. Even if all colloca-
tions in the gold set are included in the 200K-
best list, the precision is only 28%. Thus, it is 
more useful to compare precision curves for col-
locations in the N-best lists extracted by different 
methods. In addition, since this gold set only in-
cludes a small number of collocations, the preci-
sion curves of our method and the baseline 
method are getting closer, as N increases. For 
example, when N is set to 200K, our method and 
the baseline method achieved precisions of 
4.09% and 3.12%, respectively. And when N is 
set to 400K, they achieved 2.78% and 2.26%, 
respectively. For convenience of comparison, we 
set N up to 200K in the experiments. 
From the results, it can also be seen that, 
among the N-best lists with N less than 20K, the 
precision of the collocations extracted by our 
method is lower than that of the collocations ex-
tracted by the baseline, and became higher when 
N is larger than 20K. 
In order to analyze the possible reasons, we 
investigated the relationships among the fre-
quencies of the aligned word pairs, the alignment 
490
xy
b =4
b =2
 
Figure 6.  xbey /?=  
probabilities, and precisions of collocations, 
which are shown in Figure 5. From the figure, 
we can see (1) that the lower the frequencies of 
the aligned word pairs are, the higher the align-
ment probabilities are; and (2) that the precisions 
of the aligned word pairs with lower frequencies 
is lower. According to the above observations, 
we conclude that it is the word pairs with lower 
frequencies but higher probabilities that caused 
the lower precision of the top 20K collocations 
extracted by our method. 
4 Improved MWA Method 
According to the analysis in subsection 3.2, we 
need to penalize the aligned word pairs with 
lower frequencies. In order to achieve the above 
goal, we need to refine the alignment probabili-
ties by using a penalization factor derived from a 
function on the frequencies of the aligned word 
pairs. This function )(xfy =  should satisfy the 
following two conditions, where x  represents 
the log function of frequencies. 
(1) The function is monotonic. When x  is set to 
a smaller number, y  is also small. This re-
sults in the penalization on the aligned word 
pairs with lower frequencies. 
(2) When ??x , y  is set to 1. This means that 
we don?t penalize the aligned word pairs 
with higher frequencies. 
According to the above descriptions, we pro-
pose to use the exponential function in Eq. (7).  
    xbey /?=  (7)
Figure 6 describes this function. The constant 
b in the function is used to adjust the shape of the 
line. The line is sharp with b set to a small num-
ber, while the line is flat with b set to a larger 
number. In our case, if b is set to a larger number,  
0
5
10
15
20
25
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Refined probability
Probability
Baseline (Log-likelihood ratio)
 
Figure 7. Precision of collocations extracted by 
the improved method 
we assign a larger penalization weight to those 
aligned word pairs with lower frequencies. 
According to the above discussion, we can use 
the following measure to assign scores to the 
aligned words pairs generated by the MWA 
method. 
)),(log(
 
2
)|()|(
),(
ji wwfreq
b
ijji
jir
e
wwpwwp
wwp
?
?+=
  (8) 
Where wi and wj are two aligned words. p(wi|wj) 
and p(wj|wi) are alignment probabilities as shown 
in Eq. (3) and (4). )),(log( ji wwfreq  is the log 
function of the frequencies of the aligned word 
pairs (wi, wj). 
5 Evaluation on Chinese corpus 
We used the same Chinese corpus described in 
Section 3 to evaluate the improved method as 
shown in Section 4. In the experiments, b  was 
tuned by using a development set and set to 25. 
5.1 Precision 
In this section, we evaluated the extracted collo-
cations in terms of precision using both auto-
matic evaluation and human evaluation. 
Automatic Evaluation 
Figure 7 shows the precisions of the colloca-
tions in the N-best lists extracted by our method 
and the baseline method against the gold set in 
Section 3. For our methods, we used two differ-
ent measures to rank the aligned word pairs: 
alignment probabilities in Eq. (5) and refined 
491
 Our method Baseline 
True 569 290 
A 25 16 
B 5 4 
C 240 251 
False 
D 161 439 
Table 1. Manual evaluation of the top 1K Chi-
nese collocations. The precisions of our method 
and the baseline method are 56.9% and 29.0%, 
respectively. 
alignment probabilities in Eq. (8). From the re-
sults, it can be seen that with the refined align-
ment probabilities, our method achieved the 
highest precision on the N-best lists, which 
greatly outperforms the best baseline method. 
For example, in the top 1K list, our method 
achieves a precision of 20.6%, which is much 
higher than the precision of the baseline method 
(11.7%). This indicates that the exponential func-
tion used to penalize the alignment probabilities 
plays a key role in demoting most of the aligned 
word pairs with low frequencies. 
Human Evaluation 
In automatic evaluation, the gold set only con-
tains collocations in the existing dictionaries. 
Some collocations related to specific corpora are 
not included in the set. Therefore, we selected 
the top 1K collocations extracted by our im-
proved method to manually estimate the preci-
sion. During human evaluation, the true colloca-
tions are denoted as "True" in our experiments. 
The false collocations were further classified into 
the following classes. 
A: The candidate consists of two words that 
are semantically related, such as (?? doctor,  
?? nurse). 
B: The candidate is a part of the multi-word 
(? 3) collocation. For example, (?? self, ??  
mechanism) is a part of the three-word colloca-
tion (?? self, ?? regulating, ?? mecha-
nism). 
C: The candidates consist of the adjacent 
words that frequently occur together, such as (? 
he, ? say) and (? very, ? good). 
D: Two words in the candidates have no rela-
tionship with each other, but occur together fre-
quently, such as (?? Beijing, ? month) and 
(? and, ? for). 
Table 1 shows the evaluation results. Our 
method extracted 569 true collocations, which  
0
2
4
6
8
10
12
0 1 2 3 4 5 6 7 8 9 10 11 12
Training corpus (Months)
Pr
ec
is
io
n 
(%
)
Our method
Baseline
 
Figure 8. Corpus size vs. precision 
are much more than those extracted by the base-
line method. Further analysis shows that, in addi-
tion to extracting short-span collocations, our 
method extracted collocations with longer spans 
as compared with the baseline method. For ex-
ample, (?? in, ?? state) and (?? because, 
?? so) are two long-span collocations. Among 
the 1K collocations, there are 48 collocation can-
didates whose spans are larger than 6, which are 
not covered by the baseline method since the 
window size is set to 6.  And 33 of them are true 
collocations, with a higher precision of 69%. 
Classes C and D account for the most part of 
the false collocations. Although the words in 
these two classes co-occur frequently, they can 
not be regarded as collocations. And we also 
found out that the errors in class D produced by 
the baseline method are much more than that of 
those produced by our method. This indicates 
that our MWA method can remove much more 
noise from the frequently occurring word pairs. 
In Class A, the two words are semantically re-
lated and occur together in the corpus. These 
kinds of collocations can not be distinguished 
from the true collocations by our method without 
additional resources. 
Since only bigram collocations were extracted 
by our method, the multi-word (? 3) collocations 
were split into bigram collocations, which caused 
the error collocations in Class B4. 
Corpus size vs. precision 
Here, we investigated the effect of the corpus 
size on the precision of the extracted collocations. 
We evaluated the precision against the gold set 
as shown in the automatic evaluation. First, the 
whole corpus (one year of newspaper) was split 
into 12 parts according to the published months. 
Then we calculated the precisions as the training 
                                                 
4 Since only a very small faction of collocations contain 
more than two words, a few error collocations belong to 
Class B. 
492
020
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 9. Recall on the Chinese corpus 
corpus increases part by part. The top 20K collo-
cations were selected for evaluation. 
Figure 8 shows the experimental results. The 
precision of collocations extracted by our method 
is obviously higher than that of collocations ex-
tracted by the baseline method. When the size of 
the training corpus became larger, the difference 
between our method and the baseline method 
also became bigger. When the training corpus 
contains more than 9 months of corpora, the pre-
cision of collocations extracted by the baseline 
method did not increase anymore. However, the 
precision of collocations extracted by our method 
kept on increasing. This indicates the MWA 
method can extract more true collocations of 
higher quality when it is trained with larger size 
of training data. 
5.2 Recall 
Recall was evaluated on a manually labeled sub-
set of the training corpus. The subset contains 
100 sentences that were randomly selected from 
the whole corpus. The sentence average length is 
24. All true collocations (660) were labeled 
manually. The recall was calculated according to 
Eq. (9). 
)(#
)(#
subset
subsetTop
C
CC
recall N
I?=               (9) 
Here, CTop-N denotes the top collocations in the 
N-best list and Csubset denotes the true colloca-
tions in the subset. 
Figure 9 shows the recalls of collocations ex-
tracted by our method and the baseline method 
on the labeled subset. The results show that our 
method can extract more true collocations than 
the baseline method. 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 10. Recall on the English corpus 
 Our method Baseline 
True 591 355 
A 11 4 
B 19 20 
C 200 136 
False
D 179 485 
Table 2. Manual evaluation of the top 1K Eng-
lish collocations. The precisions of our method 
and the baseline method are 59.1% and 35.5%, 
respectively. 
In our experiments, the baseline method ex-
tracts about 20 millions of collocation candidates, 
while our method only extracts about 3 millions 
of collocation candidates5. Although the colloca-
tions of our method are much less than that of the 
baseline, the experiments show that the recall of 
our method is higher. This again proved that our 
method has the stronger ability to distinguish 
true collocations from false collocations. 
6 Evaluation on English corpus 
We also manually evaluated the proposed 
method on an English corpus, which is a subset 
randomly extracted from the British National 
Corpus6. The English corpus contains about 20 
millions of words. 
6.1 Precision 
We estimated the precision of the top 1K collo-
cations. Table 2 shows the results. The classifica-
tion of the false collocations is the same as that 
in Table 1. The results show that our methods 
outperformed the baseline method using log- 
                                                 
5 We set the threshold to 7.88 with a confidence level  of 
005.0=?  (cf. page 174 of Chapter 5 in (McKeown and 
Radev, 2000) for more details). 
6 Available at: http://www.hcu.ox.ac.uk/BNC/ 
493
05
10
15
20
0 20 40 60 80 100 120 140 160 180
Top-N collocation (K)
Pr
ec
is
io
n 
(%
)
 
Figure 11. Fertility vs. precision 
likelihood ratio. And the distribution of the false 
collocations is similar to that on the Chinese cor-
pus. 
6.2 Recall 
We used the method described in subsection 5.2 
to calculate the recall. 100 English sentences 
were labeled manually, obtaining 205 true collo-
cations. Figure 10 shows the recall of the collo-
cations in the N-best lists. From the figure, it can 
be seen that the trend on the English corpus is 
similar to that on the Chinese corpus, which in-
dicates that our method is language-independent. 
7 Discussion 
7.1 The Effect of Fertility 
In the MWA model as described in subsection 
2.1, i?  denotes the number of words that can 
align with iw . Since a word only collocates with 
a few other words in a sentence, we should set a 
maximum number for ? , denote as max? . 
In order to set max? , we examined the true col-
locations in the manually labeled set described in 
subsection 5.2. We found that 78% of words col-
locate with only one word, and 17% of words 
collocate with two words. In sum, 95% of words 
in the corpus can only collocate with at most two 
words. According to the above observation, we 
set max?  to 2. 
In order to further examine the effect of max?  
on collocation extraction, we used several differ-
ent max?  in our experiments. The comparison 
0
1
2
3
4
5
6
7
8
0 20 40 60 80 100
Span of collocation
lo
g(
#(
al
ig
ne
d 
w
or
d 
pa
irs
))
 
Figure 12. Distribution of spans 
results are shown in Figure 11. The highest pre-
cision is achieved when max?  is set to 2. This 
result verifies our observation on the corpus. 
7.2 Span of Collocation 
One of the advantages of our method is that 
long-span collocations can be reliably extracted. 
In this subsection, we investigate the distribution 
of the span of the aligned word pairs. For the 
aligned word pairs occurring more than once, we 
calculated the average span as shown in Eq. (10). 
),(
);,(
),(
ji
corpuss
ji
ji wwfreq
swwSpan
wwAveSpan
?
= ?  (10) 
Where, );,( swwSpan ji  is the span of the words 
wi and wj in the sentence s; ),( ji wwAveSpan  is 
the average span. 
The distribution is shown in Figure 12. It can 
be seen that the number of the aligned word pairs 
decreased exponentially as the average span in-
creased. About 17% of the aligned word pairs 
have spans longer than 6. According to the hu-
man evaluation result for precision in subsection 
5.1, the precision of the long-span collocations is 
even higher than that of the short-span colloca-
tions. This indicates that our method can extract 
reliable collocations with long spans. 
8 Conclusion 
We have presented a monolingual word align-
ment method to extract collocations from mono-
lingual corpus. We first replicated the monolin-
gual corpus to generate a parallel corpus, in 
which each sentence pair consists of the two 
identical sentences in the same language. Then 
we adapted the bilingual word alignment algo-
rithm to the monolingual scenario to align the 
10
3
2
1
max
max
max
max
=
=
=
=
?
?
?
?
494
potentially collocated word pairs in the monolin-
gual sentences. In addition, a ranking method 
was proposed to finally extract the collocations 
from the aligned word pairs. It scores collocation 
candidates by using alignment probabilities mul-
tiplied by a factor derived from the exponential 
function on the frequencies. Those with higher 
scores are selected as collocations. Both Chinese 
and English collocation extraction experiments 
indicate that our method outperforms previous 
approaches in terms of both precision and recall. 
For example, according to the human evaluations 
on the Chinese corpus, our method achieved a 
precision of 56.9%, which is much higher than 
that of the baseline method (29.0%). Moreover, 
we can extract collocations with longer span. 
Human evaluation on the extracted Chinese col-
locations shows that 69% of the long-span (>6) 
collocations are correct. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983. 
Automatic Retrieval of Frequent Idiomatic and 
Collocational Expressions in a Large Corpus. 
Journal for Literary and Linguistic computing, 
4(1):34-38. 
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, 16(1):22-29. 
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational 
Linguistics, 19(1): 61-74. 
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, 
University of Stuttgart. 
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing, Cambridge, MA; London, U.K.: Bradford 
Book & MIT Press. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Darren Pearce. 2001. Synonymy in Collocation Ex-
traction. In Proceedings of NAACL-2001 Workshop 
on Wordnet and Other Lexical Resources: Applica-
tions, Extensions and Customizations, pp. 41-46. 
Darren Pearce. 2002. A Comparative Evaluation of 
Collocation Extraction Techniques. In Proceedings 
of the 3rd International Conference on Language 
Resources and Evaluation, pp. 651-658. 
Violeta Seretan and Eric Wehrli. 2006. Accurate Col-
location Extraction Using a Multilingual Parser. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual 
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pp. 953-960 
Frank Smadja. 1993. Retrieving Collocations from 
Text: Xtract. Computational Linguistics, 19(1): 
143-177. 
Joachim Wermter and Udo Hahn. 2004. Collocation 
Extraction Based on Modifiability Statistics. In 
Proceedings of the 20th International Conference 
on Computational Linguistics (COLING-2004), pp. 
980-986. 
495
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 462 ? 473, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Improving Statistical Word Alignment with Ensemble 
Methods 
Hua Wu and Haifeng Wang 
Toshiba (China) Research and Development Center, 5/F., Tower W2, Oriental Plaza, 
No.1, East Chang An Ave., Dong Cheng District, Beijing, 100738, China 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
Abstract. This paper proposes an approach to improve statistical word align-
ment with ensemble methods. Two ensemble methods are investigated: bagging 
and cross-validation committees. On these two methods, both weighted voting 
and unweighted voting are compared under the word alignment task. In addi-
tion, we analyze the effect of different sizes of training sets on the bagging 
method. Experimental results indicate that both bagging and cross-validation 
committees improve the word alignment results regardless of weighted voting 
or unweighted voting. Weighted voting performs consistently better than un-
weighted voting on different sizes of training sets. 
1   Introduction 
Bilingual word alignment is first introduced as an intermediate result in statistical 
machine translation (SMT) [3]. Besides being used in SMT, it is also used in transla-
tion lexicon building [9], transfer rule learning [10], example-based machine transla-
tion [14], etc. In previous alignment methods, some researchers employed statistical 
word alignment models to build alignment links [3], [4], [8], [11], [16]. Some re-
searchers used similarity and association measures to build alignment links [1], [15].  
One issue about word alignment is how to improve the performance of a word 
aligner when the training data are fixed. One possible solution is to use ensemble 
methods [5], [6]. The ensemble methods were proposed to improve the performance 
of classifiers. An ensemble of classifiers is a set of classifiers whose individual deci-
sions are combined in some way (weighted or unweighted voting) to classify new 
examples. Many methods for constructing ensembles have been developed [5]. One 
kind of methods is to resample the training examples. These methods include bagging 
[2], cross-validation committees [12] and boosting [7]. The two former methods gen-
erate the classifiers in parallel while boosting generates the classifiers sequentially. In 
addition, boosting changes the weights of the training instance that is provided as 
input to each inducer based on the previously built classifiers. 
In this paper, we propose an approach to improve word alignment with ensemble 
methods. Although word alignment is not a classification problem, we can still build 
different word aligners by resampling the training data. If these aligners perform  
accurately and diversely on the corpus [6], they can be employed to improve the word 
alignment results. Here, we investigate two ensemble methods: bagging and  
 Improving Statistical Word Alignment with Ensemble Methods 463 
cross-validation committees. For both of the ensemble methods, we employ weighted 
and unweighted voting to build different ensembles. Experimental results indicate that 
both bagging and cross-validation committees improve the word alignment results. 
The weighted ensembles perform much better than the unweighted ensembles accord-
ing to our word alignment results. In addition, we analyze the effect of different sizes 
of training data on the bagging algorithm. Experimental results also show that the 
weighted bagging ensembles perform consistently better than the unweighted bagging 
ensembles on different sizes of training sets. 
The remainder of the paper is organized as follows. Section 2 describes statistical 
word alignment. Section 3 describes the bagging algorithm. Section 4 describes the 
cross-validation committees. Section 5 describes how to calculate the weights used 
for voting. Section 6 presents the evaluation results. Section 7 discusses why the en-
semble methods used in this paper are effective for the word alignment task. The last 
section concludes this paper and presents the future work. 
2   Statistical Word Alignment 
In this paper, we use the IBM model 4 as our statistical word alignment model [3]. 
This model only allows word to word and multi-word to word alignments. Thus, some 
multi-word units cannot be correctly aligned. In order to tackle this problem, we per-
form word alignment in two directions (source to target and target to source) as de-
scribed in [11]. In this paper, we call these two aligners bi-directional aligners.1 Thus, 
for each sentence pair, we can get two alignment results. We use 1S  and 2S  to repre-
sent the bi-directional alignment sets. For alignment links in both sets, we use i for 
source words and j for target words. 
}}0  ,|{|),{(1 ?=== jjjj aaiiAjAS  (1) 
}}0  ,|{|),{(2 ?=== jjii aiajAAiS  (2) 
Where, aj represents the index position of the source word aligned to the target word 
in position j. For example, if a target word in position j is connected to a source word 
in position i, then aj=i. If a target word in position j is connected to source words in 
positions i1 and i2, then Aj={i1,i2}. We name an element in the alignment set an 
alignment link.2 
3   Bagging 
The bagging algorithm (derived from bootstrap aggregating) votes classifiers gener-
ated by different bootstrap replicates [2]. A bootstrap replicate is generated by uni-
formly sampling m instances from the training set with replacement. In general, T  
                                                          
1
  The GIZA++ toolkit is used to perform statistical alignment. It is located at 
http://www.fjoch.com/GIZA++.html. 
2
  Our definition of alignment link is different from that in [11]. In [11], alignment links are 
classified into possible links and sure links. In our paper, both one-to-one and non one-to-one 
links are taken as sure links.   
464 H. Wu and H. Wang 
bootstrap replicates are built in the sampling process. And T  different classifiers are 
built based on the bootstrap replicates. A final classifier is built from these T  sub-
classifiers using weighted voting or unweighted voting. The original unweighted 
bagging algorithm is shown in Figure 1. 
Input:  a training set }} ..., ,1{),,{( mixyS ii ?=  
an induction algorithm ?  
(1) For Tj    to1=  { 
(2) jS = bootstrap replicate of S  by sampling m  items from S  with replace-
ment 
(3) )( jj SC ?=  
(4) } 
(5) Create a final classifier with majority voting: 
?
?
=
j
j
Yy
yxCxC )),((maxarg)(* ?  
Where, 1),( =yx? if yx = ; else 0),( =yx? . 
Output: Classifier *C  
Fig. 1. The Unweighted Bagging Algorithm 
3.1   Bagging the Statistical Word Aligner 
In this section, we apply the technique of bagging to word alignment, the detailed 
algorithm is shown in Figure 2. In the algorithm, we first resample the training data 
to train the word aligners. We choose to resample the training set in the same 
way as the original bagging algorithm. With these different bootstrap repli-
cates, we build the different word aligners. As described in Section 2, we per-
form word alignment in two directions to improve multiword alignment. Thus, on 
each bootstrap replicate, we train a word aligner in the source to target direction and 
another word aligner in the target to source direction, which is described in b) of 
step (1). 
After building the different word aligners, we combine or aggregate the align-
ments generated by the individual alignment models to create the final alignments for 
each sentence pair. In this paper, the final alignment link for each word is chosen by 
performing a majority voting on the alignments provided by each instance of the 
model. The majority voting can be weighted or unweighted. For weighted voting, the 
weights of word alignment links produced by the bi-directional word aligners are 
trained from the training data, which will be further described in section 5. For un-
weighted voting, the best alignment link for a specific word or unit is voted by more 
than half of the word aligners in the ensemble. For those words that have no majority 
choice, the system simply does not align them. 
 Improving Statistical Word Alignment with Ensemble Methods 465 
Input: a training set }}...1{),,{( mixyS ii ?=   
a word alignment model M  
(1) For Tj    to1=   
a) jS = bootstrap replicate of S  by sampling m  items from S  with re-
placement 
b) Train the bi-directional alignment models stjM  and tsjM  with the 
bootstrap replicate jS  
(2) For Nk    to1=  (N is the number of sentence pairs) 
For each word s : 
a) For weighted voting 
    ? +=
j
ts
j
st
jj
t
tksMtksMtsWksM ))),,(()),,(((*),(maxarg),(* ??  
 t is the word or phrase in the target sentence; 
    ),( tsW j is the weight for the alignment link )  ,( ts  produced by the 
aligner stjM  or tsjM ; 
1),( =yx? if yx = ; else 0),( =yx? . 
b) For unweighted voting 
?
=>
+=
T
j
ts
j
st
j
T
tnt
tksMtksMksM
1
2
)(:
* ))),,(()),,(((maxarg),( ??  
where, n(t)= ?
=
+
T
j
ts
j
st
j tksMtksM
1
))),,(()),,((( ??  
Output: The final word alignment results 
Fig. 2. The Bagging Algorithm for Word Alignment 
4   Cross-Validation Committee 
The difference between bagging and cross-validation committees lies in the way to 
resample the training set. The cross-validation committees construct the training sets 
by leaving out disjoint subsets of the training data. For example, the training set can 
be randomly and evenly divided into N disjoint subsets. Then N overlapping training 
sets can be constructed by dropping out a different one of these N subsets. This pro-
cedure is the same as the one to construct training sets for N-fold cross-validation. 
Thus, ensembles constructed in this way are called cross-validation committees. 
For word alignment, we also divide the training set into N even parts and build N 
overlapping training sets. With the N sets, we build N alignment models as described 
466 H. Wu and H. Wang 
above. Since the training sets are different, the word alignment results may be differ-
ent for individual words. Using the same majority voting as described in Figure 2, we 
get the final word alignment results. 
5   Weight Calculation 
In this paper, we compare both weighted voting and unweighted voting under our 
word alignment task. The algorithm in Figure 2 shows that the weights are related 
with the specific word alignment links and the specific word aligner. We calculate the 
weights based on the word alignment results on the training data. 
As described in Section 3.1, on each bootstrap replicate j, we train a word aligner 
st
jM  in the source to target direction and a word aligner tsjM  in the target to source 
direction. That is to say, we obtain two different word alignment sets  stjS  and tsjS  for 
each of the bootstrap replicate. For each word alignment link )  ,( ts  produced by stjM  
or tsjM ,  we calculate its weight as shown in (3). This weight measures the association 
of the source part and the target part in an alignment link.  This measure is like the 
Dice Coefficient. Smadja et al [13] showed that the Dice Coefficient is a good indica-
tor of translation association. 
?? +=
''
),'()',(
),(*2),(
st
i
tscounttscount
tscount
tsW  (3) 
Where, ),( tscount  is the occurring frequency of the alignment link tsjstj SSts ??)  ,( .  
6   Experiments 
6.1   Training and Testing Set 
We perform experiments on a sentence aligned English-Chinese bilingual corpus in 
general domain. There are about 320,000 bilingual sentence pairs in the corpus, from 
which, we randomly select 1,000 sentence pairs as testing data. The remainder is used 
as training data. In the sentence pairs, the average length of the English sentences is 
13.6 words while the average length of the Chinese sentences is 14.2 words.  
The Chinese sentences in both the training set and the testing set are automatically 
segmented into words. The segmentation errors in the testing set are post-corrected. 
The testing set is manually annotated. It has totally 8,651 alignment links. Among 
them, 866 alignment links include multiword units, which accounts for about 10% of 
the total links.  
6.2   Evaluation Metrics 
We use the same evaluation metrics as in [17]. If we use GS  to represent the set of 
alignment links identified by the proposed methods and RS  to denote the reference 
 Improving Statistical Word Alignment with Ensemble Methods 467 
alignment set, the methods to calculate the precision, recall, f-measure, and alignment 
error rate (AER) are shown in Equation (4), (5), (6), and (7). In addition, t-test is used 
for testing statistical significance. From the evaluation metrics, it can be seen that the 
higher the f-measure is, the lower the alignment error rate is. Thus, we will only show 
precision, recall and AER scores in the experimental results. 
|S|
|SS|
G
RG ?
=precision      
  (4) 
|S|
 |SS|
R
RG ?
=recall   
       (5) 
||||
||*2
RG
RG
SS
SSfmeasure
+
?
=  (6) 
fmeasure
SS
SS
AER
RG
RG
?=
+
?
?= 1||||
||*21  (7) 
6.3   Evaluation Results for Bagging 
For the bagging method, we use ten word aligners trained on five different bootstrap 
replicates. Among them, five aligners are trained in the source to target direction. The 
other five aligners are trained in the target to source direction. The bagging method 
will be compared with a baseline method using the entire training data. For this base-
line method, we also train bi-directional models. Based on the alignment results on 
the entire training data, we calculate the alignment weights for the two word aligners 
as described in Section 5.  
The results using weighted voting are shown in Table 1. The number in brackets of 
the first column describes the number of word aligners used in the ensembles. For 
example, in the ensemble ?bagging (4)?, two word aligners are trained in the source to 
target direction and the other two are trained in the target to source direction. 
From the results, it can be seen that the bagging methods obtain significantly better 
results than the baseline. The best ensemble achieves an error rate reduction of 7.34% 
as compared with the baseline. The results show that increasing the number of word 
aligner does not greatly reduce the word alignment error rate. The reduction is even 
smaller when the number increases from 8 to 10.  
Table 1. Weighted Bagging Results 
Method Precision Recall AER 
Bagging (4) 0.8035 0.7898 0.2034 
Bagging (6) 0.8048 0.7922 0.2015 
Bagging (8) 0.8061 0.7948 0.1996 
Bagging (10) 0.8064 0.7948 0.1994 
Baseline  0.7870 0.7826 0.2152 
468 H. Wu and H. Wang 
In order to further analyze the effect of the weights on the word alignment results, 
we also use unweighted voting in the ensembles. The results are shown in Table 2. 
The baseline method also trains bi-directional aligners with the entire training data. 
The final word alignment results are obtained by taking an unweighted voting on the 
two alignment results produced by the bi-directional aligners. That is the same as that 
by taking the intersection of the two word alignment results. 
Table 2. Unweighted Bagging Results 
Method Precision Recall AER 
Bagging (4) 0.9230 0.6073 0.2674 
Bagging (6) 0.9181 0.6200 0.2598 
Bagging (8) 0.9167 0.6307 0.2527 
Bagging (10) 0.9132 0.6347 0.2511 
Baseline  0.9294 0.5756 0.2810 
Increasing the number of word aligners in the ensembles, the unweighted bagging 
method does not greatly reduce AER. However, the ensembles obtain much lower 
error rate as compared with the baseline. The best ensemble achieves a relative error 
rate reduction of 10.64%, indicating a significant improvement. From the experimen-
tal results, we find that there are no multiword alignment links selected in the ensem-
bles. This is because unweighted voting in this paper requires more than half of the 
word aligners in the ensembles to vote for the same link. Thus, there should be bi-
directional word aligners voting for the target algnment link. The intersection of bi-
directional word alignment results produced by the IBM models only creates single 
word alignments. It can also be seen from the Equations (1) and (2) in Section 2.  
Comparing the results obtained using weighted voting in Table 1 and those ob-
tained using unweighted voting in Table 2, we find that (1) the weighted bagging 
methods are much better than the unweighted bagging methods; (2) the ensembles 
using unweighted voting obtain higher precision but lower recall than those using 
weighted voting. For example, the weighted voting ?bagging (10)? achieves a relative 
error rate reduction of 20.59% as compared with the corresponding unweighted vot-
ing. This indicates that the method used to calculate voting weights described in sec-
tion 5 is very effective.  
6.4   Evaluation Results for Cross-Validation Committees 
For the cross-validation committees, we divide the entire training data into five dis-
joint subsets. For each bootstrap replicate, we leave one out. Thus, each replicate 
includes 80% sentence pairs of the full training data. For each replicate, we train bi-
directional word alignment models. Thus, we totally obtain ten individual word align-
ers. The baseline is the same as shown in Table 1. The results obtained using 
weighted voting are shown in Table 3.  The number in the brackets of the first column 
describes the number of word aligners used in the ensembles. 
 Improving Statistical Word Alignment with Ensemble Methods 469 
Table 3. Evaluation Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.8059 0.7913 0.2015 
Validation (6) 0.8070 0.7928 0.2002 
Validation (8) 0.8063 0.7933 0.2002 
Validation (10) 0.8068 0.7947 0.1993 
Baseline  0.7870 0.7826 0.2152 
From the results, it can be seen that the cross-validation committees perform better 
than the baseline. The best ensemble ?validation (10)? achieves an error rate reduction 
of 7.39% as compared with the baseline, indicating a significant improvement. The 
results also show that increasing the number of word aligner does not greatly reduce 
the word alignment error rate.  
As described in section 6.3, we also use unweighted voting for the cross-validation 
committees. The results are shown in Table 4. The baseline is the same as described 
in Table 2.  
Table 4. Evaluation Results for Unweighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.9199 0.5943 0.2779 
Validation (6) 0.9174 0.6124 0.2655 
Validation (8) 0.9154 0.6196 0.2610 
Validation (10) 0.9127 0.6245 0.2584 
Baseline  0.9294 0.5756 0.2810 
From the results, it can be seen that increasing the number of word aligners in the 
ensembles, the alignment error rate is reduced. The best ensemble achieves a relative 
error rate reduction of 8.04% as compared with the baseline, indicating a significant 
improvement. Comparing the results in Table 3 and Table 4, we find that the 
weighted methods are also much better than the unweighted ones. For example, the 
weighted method ?Validation (10)? achieves an error rate reduction of 22.87% as 
compared with the corresponding unweighted method. 
6.5   Bagging vs. Cross-Validation Committees 
According to the evaluation results, bagging and cross-validation committees achieve 
comparable results. In order to further compare bagging and cross-validation commit-
tees, we classify the alignment links in the weighted ensembles into two classes: sin-
gle word alignment links (SWA) and multiword alignment links (MWA). SWA links 
only include one-to-one alignments. MWA links refer to those including multiword 
units in the source language or/and in the target language. The SWA and MWA for 
the bagging ensembles are shown in Table 5 and Table 6. The SWA and MWA for 
the cross-validation committees are shown in Table 7 and Table 8. The AERs of the 
baselines for SWA and MWA are 0.1531 and 0.8469, respectively. 
470 H. Wu and H. Wang 
Table 5. Single Word Alignment Results for the Weighted Bagging Methods 
Method Precision Recall AER 
Bagging (4) 0.8263 0.8829 0.1463 
Bagging (6) 0.8270 0.8845 0.1452 
Bagging (8) 0.8270 0.8877 0.1437 
Bagging (10) 0.8265 0.8876 0.1440 
Table 6. Multiword Alignment Results for the Weighted Bagging Methods 
Method Precision Recall AER 
Bagging (4) 0.4278 0.1815 0.7451 
Bagging (6) 0.4432 0.1896 0.7344 
Bagging (8) 0.4540 0.1884 0.7336 
Bagging (10) 0.4620 0.1896 0.7311 
Table 7. Single Word Alignment Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.8282 0.8833 0.1452 
Validation (6) 0.8285 0.8847 0.1443 
Validation (8) 0.8275 0.8851 0.1447 
Validation (10) 0.8277 0.8867 0.1438 
Table 8. Multiword Alignment Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.4447 0.1908 0.7330 
Validation (6) 0.4538 0.1931 0.7291 
Validation (8) 0.4578 0.1942 0.7273 
Validation (10) 0.4603 0.1942 0.7268 
From the results, it can be seen that the single word alignment results are much bet-
ter than the multiword alignment results for both of the two methods. This indicates 
that it is more difficult to align the multiword units than to align single words. 
Comparing the bagging methods and validation committees, we find that these two 
methods obtain comparable results on both the single word alignment links and mul-
tiword alignment links. This indicates that the different resampling methods in these 
two ensemble methods do not much affect the results on our word alignment task. 
6.6   Different Sizes of Training Data 
In this section, we investigate the effect of the size of training data on the ensemble 
methods. Since the difference between bagging and cross-validation committees is 
very small, we only investigate the effect on the bagging ensembles.  
 Improving Statistical Word Alignment with Ensemble Methods 471 
We randomly select training data from the original training set described in Section 
6.1 to construct different training sets. We construct three training sets, which include 
1/4, 1/2 and 3/4 of sentence pairs of the original training set, respectively.    
For each of the training set, we obtain five bootstrap replicates and train ten word 
aligners. The results of ensembles consisting of ten word aligners are shown in Table 
9 and Table 10. Table 9 and Table 10 show the weighted and unweighted bagging 
results, respectively. The methods to construct the baselines for different training sets 
in Table 9 and Table 10 are the same as those in Table 1 and Table 2, respectively. 
For convenience, we also list the results using the original training set in the tables. 
The first column describes the size of the training sets used for the ensembles. The 
last column presents the relative error rate reduction (RERR) of the ensembles as 
compared with the corresponding baselines. From the results, it can be seen that both 
weighted and unweighted bagging ensembles are effective to improve word alignment 
results. The weighted ensembles perform consistently better than the unweigted en-
sembles on different sizes of training sets. 
Table 9. Weighted Bagging Results on Different Sizes of Training Sets 
Data Precision Recall AER Baseline (AER) RERR 
1/4 0.7684 0.7517 0.2316 0.2464 6.00% 
1/2 0.7977 0.7775 0.2125 0.2293 7.33% 
3/4 0.8023 0.7869 0.2055 0.2184 5.89% 
All 0.8064 0.7948 0.1994 0.2152 7.34% 
Table 10. Unweighted Bagging Results on Different Sizes of Training Sets 
Data Precision Recall AER Baseline (AER) RERR 
1/4 0.8960 0.6033 0.2789 0.3310 15.72% 
1/2 0.9077 0.6158 0.2662 0.3050 12.72% 
3/4 0.9140 0.6270 0.2562 0.2943 12.95% 
All 0.9132 0.6347 0.2511 0.2810 10.64% 
7   Discussion 
Both bagging and cross-validation committees utilize multiple classifiers to make 
different assumptions about the learning system. Bagging requires that the learning 
system should not be stable, so that small changes to the training set would lead to 
different classifiers. Breiman [2] also noted that poor predicators could be trans-
formed into worse ones by bagging.  
In this paper, the learning system is the word alignment model described in Section 
2. The classifiers refer to the different word aligners trained on different bootstrap 
replicates. In our experiments, although word alignment models do not belong to 
unstable learning systems, bagging obtains better results on all of the datasets. This is 
472 H. Wu and H. Wang 
because the training data is insufficient or subject to data sparseness problem. Thus, 
changing the training data or resampling the training data causes the alternation of the 
trained parameters of the alignment model. The word aligners trained on a different 
bootstrap replicate produce different word alignment links for individual words. Us-
ing majority voting, the ensembles can improve the alignment precision and recall, 
resulting in lower alignment error rates. 
The experiments also show that weighted voting is better than unweighted voting. 
The advantage of weighted voting is that it can select the good word alignment link 
even if only one aligner votes for it in the ensembles. This is because the selected 
alignment link gets much higher weight than the other links. 
8   Conclusion and Future Work 
Two ensemble methods are employed in this paper to improve word alignment re-
sults: bagging and cross-validation committees. Both of these two methods obtain 
better results than the original word aligner without increasing any training data. In 
this paper, we use two different voting methods: weighted voting and unweighted 
voting. Experimental results show that the weighted bagging method and weighted 
cross-validation committees achieve an error rate reduction of 7.34% and 7.39% re-
spectively, as compared with the original word aligner. Results also show that 
weighted voting is much better than unweighted voting on the word alignment task. 
Unweighted voting obtains higher precision but lower recall than weighted voting. In 
addition, the weighted voting used in this paper obtains multiword alignment links 
while the unweighted voting cannot. 
We also compare the two ensemble methods on the same training data and testing 
data. Bagging and cross-validation committees obtain comparable results on both 
single word alignment links and multiword alignment links. This indicates that the 
different resampling methods in these two ensemble methods do not much affect the 
results under our word alignment task. 
We also investigate the bagging method on different sizes of training sets. The re-
sults show that both weighted voting and unweighted voting are effective to improve 
word alignment results. Weighted voting performs consistently better than unweigted 
voting on different sizes of training sets.  
In future work, we will investigate more ensemble methods on the word alignment 
task such as the boosting algorithm. In addition, we will do more research on the 
weighting schemes in voting. 
References 
1. Ahrenberg, L., Merkel, M., Andersson, M.: A Simple Hybrid Aligner for Generating Lexi-
cal Correspondences in Parallel Texts. In Proc. of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and the 17th Int. Conf. on Computational Linguistics 
(ACL/COLING-1998), 29-35 
2. Breiman, L.: Bagging Predicators. Machine Learning (1996), 24(1): 123-140 
3. Brown, P. F., Pietra, S. D., Pietra, V. D., Mercer, R.: The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computational Linguistics (1993), 19(2): 263-311 
 Improving Statistical Word Alignment with Ensemble Methods 473 
4. Cherry, C., Lin, D.: A Probability Model to Improve Word Alignment. In Proc. of the 41st 
Annual Meeting of the Association for Computational Linguistics (ACL-2003), pp. 88-95 
5. Dietterich, T.: Machine Learning Research: Four Current Directions. AI Magazine (1997), 
18 (4): 97-136 
6. Dietterich, T.: Ensemble Methods in Machine Learning. In Proc. of the First Int. Work-
shop on Multiple Classifier Systems (2000), 1-15 
7. Freund, Y., Schapire, R.: Experiments with a new boosting algorithm. In Machine Learn-
ing: Proc. of the Thirteenth International Conference (1996), 148-156 
8. Matusov, E., Zens, R., Ney H.: Symmetric Word Alignments for Statistical Machine 
Translation. In Proc. of the 20th Int. Conf. on Computational Linguistics (COLING-2004), 
219-225  
9. Melamed, I. D.: Automatic Construction of Clean Broad-Coverage Translation Lexicons. 
In Proc. of the 2nd Conf. of the Association for Machine Translation in the Americas 
(AMTA-1996), 125-134 
10. Menezes, A., Richardson, S.D.: A Best-first Alignment Algorithm for Automatic Extrac-
tion of Transfer Mappings from Bilingual Corpora. In Proc. of the ACL 2001 Workshop 
on Data-Driven Methods in Machine Translation (2001), 39-46 
11. Och, F. J., Ney, H.: Improved Statistical Alignment Models. In Proc. of the 38th Annual 
Meeting of the Association for Computational Linguistics (ACL-2000), 440-447 
12. Parmanto, B., Munro, P., Doyle, H.: Improving Committee Diagnosis with Resampling 
Techniques. In Touretzky, D., Mozer, M., Hasselmo, M. (Ed..): Advances in Neural In-
formation Processing Systems (1996), Vol. 8,  882-888 
13. Smadja, F. A., McKeown, K. R., Hatzivassiloglou, V.: Translating Collocations 
for Bilingual Lexicons: a Statistical Approach. Computational Linguistics (1996), 
22 (1):1-38 
14. Somers, H.: Review Article: Example-Based Machine Translation. Machine Translation 
(1999), 14: 113-157 
15. Tufis, D., Barbu, M.: Lexical Token Alignment: Experiments, Results and Application. In 
Proc. of the 3rd Int. Conf. on Language Resources and Evaluation (LREC-2002), 458-465 
16. Wu, D.: Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics (1997), 23(3): 377-403 
17. Wu, H., Wang, H.: Improving Domain-Specific Word Alignment with a General Bilingual 
Corpus. In Frederking R., Taylor, K. (Eds.): Machine Translation: From Real Users to Re-
search: 6th Conf. of the Association for Machine Translation in the Americas (AMTA-
2004), 262-271 
Improving Domain-Specific Word Alignment for Computer Assisted 
Translation 
WU Hua, WANG Haifeng 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, China, 100738 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
 
Abstract 
This paper proposes an approach to improve 
word alignment in a specific domain, in which 
only a small-scale domain-specific corpus is 
available, by adapting the word alignment 
information in the general domain to the 
specific domain. This approach first trains two 
statistical word alignment models with the 
large-scale corpus in the general domain and the 
small-scale corpus in the specific domain 
respectively, and then improves the 
domain-specific word alignment with these two 
models. Experimental results show a significant 
improvement in terms of both alignment 
precision and recall. And the alignment results 
are applied in a computer assisted translation 
system to improve human translation efficiency. 
1 Introduction 
Bilingual word alignment is first introduced as an 
intermediate result in statistical machine translation 
(SMT) (Brown et al, 1993). In previous alignment 
methods, some researchers modeled the alignments 
with different statistical models (Wu, 1997; Och and 
Ney, 2000; Cherry and Lin, 2003). Some researchers 
use similarity and association measures to build 
alignment links (Ahrenberg et al, 1998; Tufis and 
Barbu, 2002). However, All of these methods 
require a large-scale bilingual corpus for training. 
When the large-scale bilingual corpus is not 
available, some researchers use existing dictionaries 
to improve word alignment (Ker and Chang, 1997).  
However, few works address the problem of 
domain-specific word alignment when neither the 
large-scale domain-specific bilingual corpus nor the 
domain-specific translation dictionary is available. 
This paper addresses the problem of word 
alignment in a specific domain, where only a small 
domain-specific corpus is available. In the 
domain-specific corpus, there are two kinds of 
words. Some are general words, which are also 
frequently used in the general domain. Others are 
domain-specific words, which only occur in the 
specific domain. In general, it is not quite hard to 
obtain a large-scale general bilingual corpus while 
the available domain-specific bilingual corpus is 
usually quite small. Thus, we use the bilingual 
corpus in the general domain to improve word 
alignments for general words and the corpus in the 
specific domain for domain-specific words. In other 
words, we will adapt the word alignment 
information in the general domain to the specific 
domain. 
In this paper, we perform word alignment 
adaptation from the general domain to a specific 
domain (in this study, a user manual for a medical 
system) with four steps. (1) We train a word 
alignment model using the large-scale bilingual 
corpus in the general domain; (2) We train another 
word alignment model using the small-scale 
bilingual corpus in the specific domain; (3) We build 
two translation dictionaries according to the 
alignment results in (1) and (2) respectively; (4) For 
each sentence pair in the specific domain, we use the 
two models to get different word alignment results 
and improve the results according to the translation 
dictionaries. Experimental results show that our 
method improves domain-specific word alignment in 
terms of both precision and recall, achieving a 
21.96% relative error rate reduction. 
The acquired alignment results are used in a 
generalized translation memory system (GTMS, a 
kind of computer assisted translation systems) 
(Simard and Langlais, 2001). This kind of system 
facilitates the re-use of existing translation pairs to 
translate documents. When translating a new 
sentence, the system tries to provide the 
pre-translated examples matched with the input and 
recommends a translation to the human translator, 
and then the translator edits the suggestion to get a 
final translation. The conventional TMS can only 
recommend translation examples on the sentential 
level while GTMS can work on both sentential and 
sub-sentential levels by using word alignment results. 
These GTMS are usually employed to translate 
various documents such as user manuals, computer 
operation guides, and mechanical operation manuals. 
2 
2.1 
Word Alignment Adaptation 
Bi-directional Word Alignment 
In statistical translation models (Brown et al, 1993), 
only one-to-one and more-to-one word alignment 
links can be found. Thus, some multi-word units 
cannot be correctly aligned. In order to deal with this 
problem, we perform translation in two directions 
(English to Chinese, and Chinese to English) as 
described in (Och and Ney, 2000). The GIZA++ 
toolkit 1  is used to perform statistical word 
alignment.  
For the general domain, we use  and  
to represent the alignment sets obtained with English 
as the source language and Chinese as the target 
language or vice versa. For alignment links in both 
sets, we use i for English words and j for Chinese 
words. 
1SG 2SG
}0 },{|),{(1 ?== jjjj aaAjASG  
}0  },{|),{(2 ?== iiii aaAAiSG  
Where, is the position of the source 
word aligned to the target word in position k. The set 
 indicates the words aligned to the same 
source word k. For example, if a Chinese word in 
position j is connect to an English word in position i, 
then . And if a Chinese word in position j is 
connect to English words in position i and k, then 
. 
),( jikak =
),( jikAk =
ia j =
},{ kiA j =
Based on the above two alignment sets, we 
obtain their intersection set, union set 2  and 
subtraction set.  
Intersection:  21 SGSGSG ?=
Union:  21 SGSGPG ?=
Subtraction:  SGMG ?= PG
For the specific domain, we use  and  
to represent the word alignment sets in the two 
directions. The symbols , 
1SF 2SF
SF PF  and MF  
represents the intersection set, union set and the 
subtraction set, respectively. 
2.2 
                                                       
 Translation Dictionary Acquisition 
When we train the statistical word alignment model 
with a large-scale bilingual corpus in the general 
domain, we can get two word alignment results for 
the training data. By taking the intersection of the 
two word alignment results, we build a new 
alignment set. The alignment links in this 
intersection set are extended by iteratively adding 
word alignment links into it as described in (Och and 
Ney, 2000).  
 
1 It is located at http://www.isi.edu/~och/GIZA++.html  
2  In this paper, the union operation does not remove the 
replicated elements. For example, if set one includes two 
elements {1, 2} and set two includes two elements {1, 3}, then 
the union of these two sets becomes {1, 1, 2, 3}. 
Based on the extended alignment links, we build 
an English to Chinese translation dictionary  
with translation probabilities. In order to filter some 
noise caused by the error alignment links, we only 
retain those translation pairs whose translation 
probabilities are above a threshold 
1D
1?  or 
co-occurring frequencies are above a threshold 2? . 
When we train the IBM statistical word 
alignment model with a limited bilingual corpus in 
the specific domain, we build another translation 
dictionary  with the same method as for the 
dictionary . But we adopt a different filtering 
strategy for the translation dictionary . We use 
log-likelihood ratio to estimate the association 
strength of each translation pair because Dunning 
(1993) proved that log-likelihood ratio performed 
very well on small-scale data. Thus, we get the 
translation dictionary  by keeping those entries 
whose log-likelihood ratio scores are greater than a 
threshold 
2D
1D
3
2D
2D
? .  
2.3 Word Alignment Adaptation Algorithm 
Based on the bi-directional word alignment, we 
define  as SI SFSGSI ?= and as UG
SIPFPGUG ??= . The word alignment links in 
the set SI  are very reliable. Thus, we directly 
accept them as correct links and add them into the 
final alignment set . WA
Input: Alignment set and  SI UG
(1) For alignment links in , we directly add 
them into the final alignment set . 
SI
WA
(2) For each English word i in the , we first 
find its different alignment links, and then do 
the following: 
UG
a) If there are alignment links found in 
dictionary , add the link with the largest 
probability to . 
1D
WA
b) Otherwise, if there are alignment links found 
in dictionary , add the link with the 
largest log-likelihood ratio score to . 
2D
WA
c) If both a) and b) fail, but three links select the 
same target words for the English word i, we 
add this link into . WA
d) Otherwise, if there are two different links for 
this word: one target is a single word, and 
the other target is a multi-word unit and the 
words in the multi-word unit have no link in 
, add this multi-word alignment link to 
. 
WA
WA
Output: Updated alignment set  WA
Figure 1. Word Alignment Adaptation Algorithm
For each source word in the set , there are 
two to four different alignment links. We first use 
translation dictionaries to select one link among 
them. We first examine the dictionary  and then 
 to see whether there is at least an alignment link 
of this word included in these two dictionaries.  If it 
is successful, we add the link with the largest 
probability or the largest log-likelihood ratio score to 
the final set . Otherwise, we use two heuristic 
rules to select word alignment links. The detailed 
algorithm is described in Figure 1. 
UG
1D
2D
WA
 
Figure 2. Alignment Example 
Figure 2 shows an alignment result obtained with 
the word alignment adaptation algorithm. For 
example, for the English word ?x-ray?, we have two 
different links in UG . One is (x-ray, X) and the 
other is (x-ray, X ??). And the single Chinese 
words ??? and ??? have no alignment links in the 
set . According to the rule d), we select the link 
(x-ray, X??). 
WA
3 Evaluation 
3.1 
3.2 
We compare our method with three other methods. 
The first method ?Gen+Spec? directly combines the 
corpus in the general domain and in the specific 
domain as training data. The second method ?Gen? 
only uses the corpus in the general domain as 
training data. The third method ?Spec? only uses the 
domain-specific corpus as training data. With these 
training data, the three methods can get their own 
translation dictionaries. However, each of them can 
only get one translation dictionary. Thus, only one 
of the two steps a) and b) in Figure 1 can be applied 
to these methods. The difference between these three 
methods and our method is that, for each word, our 
method has four candidate alignment links while the 
other three methods only has two candidate 
alignment links. Thus, the steps c) and d) in Figure 1 
should not be applied to these three methods. 
Training and Testing Data 
We have a sentence aligned English-Chinese 
bilingual corpus in the general domain, which 
includes 320,000 bilingual sentence pairs, and a 
sentence aligned English-Chinese bilingual corpus in 
the specific domain (a medical system manual), 
which includes 546 bilingual sentence pairs. From 
this domain-specific corpus, we randomly select 180 
pairs as testing data. The remained 366 pairs are 
used as domain-specific training data. 
The Chinese sentences in both the training set 
and the testing set are automatically segmented into 
words. In order to exclude the effect of the 
segmentation errors on our alignment results, we 
correct the segmentation errors in our testing set. 
The alignments in the testing set are manually 
annotated, which includes 1,478 alignment links. 
Overall Performance 
We use evaluation metrics similar to those in (Och 
and Ney, 2000). However, we do not classify 
alignment links into sure links and possible links. 
We consider each alignment as a sure link. If we use 
 to represent the alignments identified by the 
proposed methods and  to denote the reference 
alignments, the methods to calculate the precision, 
recall, and f-measure are shown in Equation (1), (2) 
and (3). According to the definition of the alignment 
error rate (AER) in (Och and Ney, 2000), AER can 
be calculated with Equation (4). Thus, the higher the 
f-measure is, the lower the alignment error rate is. 
Thus, we will only give precision, recall and AER 
values in the experimental results. 
GS
CS
|S|
|SS|
G
CG ?=precision       (1) 
|S|
 |SS|
C
CG ?=recall   (2) 
||||
||*2
CG
CG
SS
SS
fmeasure +
?=  (3) 
fmeasure
SS
SS
AER
CG
CG ?=+
??= 1
||||
||*2
1  (4) 
 
Method Precision Recall AER 
Ours 0.8363 0.7673 0.1997
Gen+Spec 0.8276 0.6758 0.2559
Gen 0.8668 0.6428 0.2618
Spec 0.8178 0.4769 0.3974
Table 1. Word Alignment Adaptation Results 
We get the alignment results shown in Table 1 by 
setting the translation probability threshold to 
1.01 =? , the co-occurring frequency threshold to 
52 =?  and log-likelihood ratio score to 503 =? . 
From the results, it can be seen that our approach 
performs the best among others, achieving much 
higher recall and comparable precision. It also 
achieves a 21.96% relative error rate reduction 
compared to the method ?Gen+Spec?. This indicates 
that separately modeling the general words and 
domain-specific words can effectively improve the 
word alignment in a specific domain. 
4 Computer Assisted Translation System  
A direct application of the word alignment result to 
the GTMS is to get translations for sub-sequences in 
the input sentence using the pre-translated examples. 
For each sentence, there are many sub-sequences. 
GTMS tries to find translation examples that match 
the longest sub-sequences so as to cover as much of 
the input sentence as possible without overlapping. 
Figure 3 shows a sentence translated on the 
sub-sentential level. The three panels display the 
input sentence, the example translations and the 
translation suggestion provided by the system, 
respectively. The input sentence is segmented to 
three parts. For each part, the GTMS finds one 
example to get a translation fragment according to 
the word alignment result. By combining the three 
translation fragments, the GTMS produces a correct 
translation suggestion ??????? CT ????? 
Without the word alignment information, the 
conventional TMS cannot find translations for the 
input sentence because there are no examples closely 
matched with it. Thus, word alignment information 
can improve the translation accuracy of the GTMS, 
which in turn reduces editing time of the translators 
and improves translation efficiency. 
 
Figure 3. A Snapshot of the Translation System 
5 Conclusion 
This paper proposes an approach to improve 
domain-specific word alignment through alignment 
adaptation. Our contribution is that our approach 
improves domain-specific word alignment by 
adapting word alignment information from the 
general domain to the specific domain. Our 
approach achieves it by training two alignment 
models with a large-scale general bilingual corpus 
and a small-scale domain-specific corpus. Moreover, 
with the training data, two translation dictionaries 
are built to select or modify the word alignment 
links and further improve the alignment results. 
Experimental results indicate that our approach 
achieves a precision of 83.63% and a recall of 
76.73% for word alignment on a user manual of a 
medical system, resulting in a relative error rate 
reduction of 21.96%. Furthermore, the alignment 
results are applied to a computer assisted translation 
system to improve translation efficiency.  
Our future work includes two aspects. First, we 
will seek other adaptation methods to further 
improve the domain-specific word alignment results. 
Second, we will use the alignment adaptation results 
in other applications. 
References 
Lars Ahrenberg, Magnus Merkel and Mikael 
Andersson. 1998. A Simple Hybrid Aligner for 
Generating Lexical Correspondences in Parallel 
Tests. In Proc. of the 36th Annual Meeting of the 
Association for Computational Linguistics and the 
17th International Conference on Computational 
Linguistics, pages 29-35. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 88-95. 
Ted Dunning. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, 19(1): 61-74. 
Sue J. Ker, Jason S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2): 313-343. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, pages 440-447. 
Michel Simard and Philippe Langlais. 2001. 
Sub-sentential Exploitation of Translation 
Memories. In Proc. of MT Summit VIII, pages 
335-339. 
Dan Tufis and Ana Maria Barbu. 2002. Lexical 
Token Alignment: Experiments, Results and 
Application. In Proc. of the Third International 
Conference on Language Resources and 
Evaluation, pages 458-465. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics, 23(3): 
377-403. 
Proceedings of the 43rd Annual Meeting of the ACL, pages 467?474,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Alignment Model Adaptation for Domain-Specific Word Alignment 
WU Hua, WANG Haifeng, LIU Zhanyi 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China  
{wuhua, wanghaifeng, liuzhanyi}@rdc.toshiba.com.cn   
 
Abstract 
This paper proposes an alignment 
adaptation approach to improve 
domain-specific (in-domain) word 
alignment. The basic idea of alignment 
adaptation is to use out-of-domain corpus 
to improve in-domain word alignment 
results. In this paper, we first train two 
statistical word alignment models with the 
large-scale out-of-domain corpus and the 
small-scale in-domain corpus respectively, 
and then interpolate these two models to 
improve the domain-specific word 
alignment. Experimental results show that 
our approach improves domain-specific 
word alignment in terms of both precision 
and recall, achieving a relative error rate 
reduction of 6.56% as compared with the 
state-of-the-art technologies. 
1 Introduction 
Word alignment was first proposed as an 
intermediate result of statistical machine 
translation (Brown et al, 1993). In recent years, 
many researchers have employed statistical models 
(Wu, 1997; Och and Ney, 2003; Cherry and Lin, 
2003) or association measures  (Smadja et al, 
1996; Ahrenberg et al, 1998; Tufis and Barbu, 
2002) to build alignment links. In order to achieve 
satisfactory results, all of these methods require a 
large-scale bilingual corpus for training. When the 
large-scale bilingual corpus is not available, some 
researchers use existing dictionaries to improve 
word alignment (Ker and Chang, 1997). However, 
only a few studies (Wu and Wang, 2004) directly 
address the problem of domain-specific word 
alignment when neither the large-scale 
domain-specific bilingual corpus nor the 
domain-specific translation dictionary is available. 
In this paper, we address the problem of word 
alignment in a specific domain, in which only a 
small-scale corpus is available. In the 
domain-specific (in-domain) corpus, there are two 
kinds of words: general words, which also 
frequently occur in the out-of-domain corpus, and 
domain-specific words, which only occur in the 
specific domain. Thus, we can use the 
out-of-domain bilingual corpus to improve the 
alignment for general words and use the in-domain 
bilingual corpus for domain-specific words. We 
implement this by using alignment model 
adaptation. 
Although the adaptation technology is widely 
used for other tasks such as language modeling 
(Iyer et al, 1997), only a few studies, to the best of 
our knowledge, directly address word alignment 
adaptation. Wu and Wang (2004) adapted the 
alignment results obtained with the out-of-domain 
corpus to the results obtained with the in-domain 
corpus. This method first trained two models and 
two translation dictionaries with the in-domain 
corpus and the out-of-domain corpus, respectively. 
Then these two models were applied to the 
in-domain corpus to get different results. The 
trained translation dictionaries were used to select 
alignment links from these different results. Thus, 
this method performed adaptation through result 
combination. The experimental results showed a 
significant error rate reduction as compared with 
the method directly combining the two corpora as 
training data.  
In this paper, we improve domain-specific word 
alignment through statistical alignment model 
adaptation instead of result adaptation. Our method 
includes the following steps: (1) two word 
alignment models are trained using a small-scale 
in-domain bilingual corpus and a large-scale 
467
out-of-domain bilingual corpus, respectively. (2) A 
new alignment model is built by interpolating the 
two trained models. (3) A translation dictionary is 
also built by interpolating the two dictionaries that 
are trained from the two training corpora. (4) The 
new alignment model and the translation dictionary 
are employed to improve domain-specific word 
alignment results. Experimental results show that 
our approach improves domain-specific word 
alignment in terms of both precision and recall, 
achieving a relative error rate reduction of 6.56% 
as compared with the state-of-the-art technologies. 
The remainder of the paper is organized as 
follows. Section 2 introduces the statistical word 
alignment model. Section 3 describes our 
alignment model adaptation method. Section 4 
describes the method used to build the translation 
dictionary. Section 5 describes the model 
adaptation algorithm. Section 6 presents the 
evaluation results. The last section concludes our 
approach. 
2 Statistical Word Alignment 
According to the IBM models (Brown et al, 1993), 
the statistical word alignment model can be 
generally represented as in Equation (1).  
?=
'
)|,'(
)|,(
),|(
a
ap
ap
ap
ef
efef  (1)
In this paper, we use a simplified IBM model 4 
(Al-Onaizan et al, 1999), which is shown in 
Equation (2). This simplified version does not take 
word classes into account as described in (Brown 
et al, 1993). 
))))(()](([                  
))()](([(                    
)|( )|(                     
                 
)|,Pr()|,(
0,1
1
0,1
1
11
1
2
0
0
0
),(
00
?
?
??
?
?=
>
?=
==
?
???
+??=
??
????
?
???
? ?=
=
m
aj
j
m
aj
j
m
j
aj
l
i
ii
m
j
j
ja
j
jpjdahj
cjdahj
eften
pp
m
ap
?
??
??
?
?
?
?? eef
 
(2)
ml,  are the lengths of the target sentence and the  
source sentence respectively. 
j  is the position index of the source word. 
ja  is the position of the target word aligned to 
    the jth source word. 
i?  is the fertility of . ie
1p  is the fertility probability for e , and 
. 
0
110 =+ pp
)
jaj|et(f  is the word translation probability. 
)|( ii en ?  is the fertility probability. 
)(1 jacjd ??  is the distortion probability for the  
head of each cept1. 
))((1 jpjd ?>  is the distortion probability for the  
remaining words of the cept. 
}:{min)( kk
aikih == is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
 
i?  is the first word before  with non-zero 
fertility. If , 
; else . 
ie
0?
}i
0|}0:{| '' ' ><<> iii i?
00 'i <<? 0=i?:max{ ' 'i ii >= ??
i
j j
i
jia
c ?
? ?== ][  is the center of cept i. 
During the training process, IBM model 3 is 
first trained, and then the parameters in model 3 
are employed to train model 4. During the testing 
process, the trained model 3 is also used to get an 
initial alignment result, and then the trained model 
4 is employed to improve this alignment result. For 
convenience, we describe model 3 in Equation (3). 
The main difference between model 3 and model 4 
lies in the calculation of distortion probability. 
??
??
?
?=
==
?
?
??
????
?
???
? ?=
=
m
aj
j
m
j
aj
l
i
i
l
i
ii
m
j
j
mlajdeft
en
pp
m
ap
0:1
11
1
2
0
0
0
),(
),,|()|(                     
!  )|(                     
                   
)|,Pr()|,(
00
??
?
?
??
??
??
eef
(3)
                                                           
1 A cept is defined as the set of target words connected to a source word 
(Brown et al, 1993).  
468
However, both model 3 and model 4 do not 
take the multiword cept into account. Only 
one-to-one and many-to-one word alignments are 
considered. Thus, some multi-word units in the 
domain-specific corpus cannot be correctly aligned. 
In order to deal with this problem, we perform 
word alignment in two directions (source to target, 
and target to source) as described in (Och and Ney, 
2000). The GIZA++ toolkit2 is used to perform 
statistical word alignment. 
We use  and  to represent the 
bi-directional alignment sets, which are shown in 
Equation (4) and (5). For alignment in both sets, 
we use j for source words and i for target words. If 
a target word in position i is connected to source 
words in positions  and , then . 
We call an element in the alignment set an 
alignment link. 
1SG 2SG
2j1j },{ 21 jjAi =
}}0 ,|{|),{(1 ?=== jjii aiajAiASG  (4)
}}0  ,|{|),{(2 ?=== jjjj aaiiAAjSG (5)
3 Word Alignment Model Adaptation 
In this paper, we first train two models using the 
out-of-domain training data and the in-domain 
training data, and then build a new alignment 
model through linear interpolation of the two 
trained models. In other words, we make use of the 
out-of-domain training data and the in-domain 
training data by interpolating the trained alignment 
models. One method to perform model adaptation 
is to directly interpolate the alignment models as 
shown in Equation (6).  
),|()1(),|(),|( efapefapefap OI ??+?= ??
 
(6)
),|( efapI  and  are the alignment 
model trained using the in-domain corpus and the 
out-of-domain corpus, respectively.
),|( efapO
?  is an 
interpolation weight. It can be a constant or a 
function of  and . f e
However, in both model 3 and model 4, there 
are mainly three kinds of parameters: translation 
probability, fertility probability and distortion 
probability. These three kinds of parameters have 
their own interpretation in these two models. In 
order to obtain fine-grained interpolation models, 
we separate the alignment model interpolation into 
three parts: translation probability interpolation, 
fertility probability interpolation and distortion 
probability interpolation. For these probabilities, 
we use different interpolation methods to calculate 
the interpolation weights. After interpolation, we 
replace the corresponding parameters in equation 
(2) and (3) with the interpolated probabilities to get 
new alignment models. 
                                                          
2 It is located at http://www.fjoch.com/GIZA++.html. 
In the following subsections, we will perform 
linear interpolation for word alignment in the 
source to target direction. For the word alignment 
in the target to source direction, we use the same 
interpolation method. 
3.1 Translation Probability Interpolation 
The word translation probability  is 
very important in translation models. The same 
word may have different distributions in the 
in-domain corpus and the out-of-domain corpus. 
Thus, the interpolation weight for the translation 
probability is taken as a variant. The interpolation 
model for  is described in Equation (7).  
)|(
jaj eft
)|(
jaj eft
)|())(1(                      
)|()()|(
jj
jjj
ajOat
ajIataj
efte
efteeft
??
+?=
?
?
 (7)
The interpolation weight  in (7) is a 
function of . It is calculated as shown in 
Equation (8).  
)(
jat e?
jae
?
? ???
?
???
?
+= )()(
)(
)(
jj
j
j
aOaI
aI
at epep
ep
e  (8)
)(
jaI ep  and  are the relative 
frequencies of  in the in-domain corpus and in 
the out-of-domain corpus, respectively. 
)(
jaO ep
jae
?  is an 
adaptation coefficient, such that 0?? . 
Equation (8) indicates that if a word occurs 
more frequently in a specific domain than in the 
general domain, it can usually be considered as a 
domain-specific word (Pe?as et al, 2001). For 
example, if  is much larger than , 
the word  is a domain-specific word and the 
interpolation weight approaches to 1. In this case, 
we trust more on the translation probability 
obtained from the in-domain corpus than that 
obtained from the out-of-domain corpus. 
)(
jaI ep
ja
)(
jaO ep
e
469
3.2 
3.3 
4 
Fertility Probability Interpolation 
The fertility probability describes the 
distribution of the number of words that  is 
aligned to. The interpolation model is shown in (9). 
)|( ii en ?
ie
)|()1()|()|( iiOniiInii enenen ????? ??+?= (9)
Where,  is a constant. This constant is obtained 
using a manually annotated held-out data set. In 
fact, we can also set the interpolation weight to be 
a function of the word . From the word 
alignment results on the held-out set, we conclude 
that these two weighting schemes do not perform 
quite differently. 
n?
ie
Distortion Probability Interpolation 
The distortion probability describes the distribution 
of alignment positions. We separate it into two 
parts: one is the distortion probability in model 3, 
and the other is the distortion probability in model 
4. The interpolation model for the distortion 
probability in model 3 is shown in (10). Since the 
distortion probability is irrelevant with any specific 
source or target words, we take  as a constant. 
This constant is obtained using the held-out set. 
d?
),,|()1(                          
),,|(),,|(
mlajd
mlajdmlajd
jOd
jIdj
??
+?=
?
?
 (10)
For the distortion probability in model 4, we 
use the same interpolation method and take the 
interpolation weight as a constant.  
Translation Dictionary Acquisition 
We use the translation dictionary trained from the 
training data to further improve the alignment 
results. When we train the bi-directional statistical 
word alignment models with the training data, we 
get two word alignment results for the training data. 
By taking the intersection of the two word 
alignment results, we build a new alignment set. 
The alignment links in this intersection set are 
extended by iteratively adding word alignment 
links into it as described in (Och and Ney, 2000). 
Based on the extended alignment links, we build a 
translation dictionary. In order to filter the noise 
caused by the error alignment links, we only retain 
those translation pairs whose log-likelihood ratio 
scores (Dunning, 1993) are above a threshold. 
Based on the alignment results on the 
out-of-domain corpus, we build a translation 
dictionary  filtered with a threshold . Based 
on the alignment results on a small-scale 
in-domain corpus, we build another translation 
dictionary  filtered with a threshold .  
1D
2D
1?
2?
After obtaining the two dictionaries, we 
combine two dictionaries through linearly 
interpolating the translation probabilities in the two 
dictionaries, which is shown in (11). The symbols f 
and e represent a single word or a phrase in the 
source and target languages. This differs from the 
translation probability in Equation (7), where these 
two symbols only represent single words. 
)|())(1()|()()|( efpeefpeefp OI ??+?= ?? (11)
The interpolation weight is also a function of e. It 
is calculated as shown in (12)3. 
)()(
)(
)(
epep
ep
e
OI
I
+=?  (12)
)(epI  and  represent the relative 
frequencies of e  in the in-domain corpus and 
out-of-domain corpus, respectively.  
)(epO
5 
6 Evaluation 
                                                          
Adaptation Algorithm 
The adaptation algorithms include two parts: a 
training algorithm and a testing algorithm. The 
training algorithm is shown in Figure 1.  
After getting the two adaptation models and the 
translation dictionary, we apply them to the 
in-domain corpus to perform word alignment. Here 
we call it testing algorithm. The detailed algorithm 
is shown in Figure 2. For each sentence pair, there 
are two different word alignment results, from 
which the final alignment links are selected 
according to their translation probabilities in the 
dictionary D. The selection order is similar to that 
in the competitive linking algorithm (Melamed, 
1997). The difference is that we allow many-to-one 
and one-to-many alignments. 
We compare our method with four other methods. 
The first method is descried in (Wu and Wang, 
2004). We call it ?Result Adaptation (ResAdapt)?. 
3 We also tried an adaptation coefficient to calculate the 
interpolation weight as in (8). However, the alignment results 
are not improved by using this coefficient for the dictionary. 
470
Input: In-domain training data 
      Out-of-domain training data 
(1) Train two alignment models 
(source to target) and  (target to 
source) using the in-domain corpus. 
st
IM
ts
IM
(2) Train the other two alignment models 
 and  using the out-of-domain 
corpus. 
st
OM
ts
OM
(3) Build an adaptation model stM  based on 
 and , and build the other 
adaptation model 
st
IM
st
OM
tsM  based on 
and  using the interpolation methods 
described in section 3. 
ts
IM
ts
OM
(4) Train a dictionary  using the 
alignment results on the in-domain 
training data. 
1D
(5) Train another dictionary  using the 
alignment results on the out-of-domain 
training data. 
2D
(6) Build an adaptation dictionary D  based 
on  and  using the interpolation 
method described in section 4. 
1D 2D
Output: Alignment models stM  and tsM  
       Translation dictionary D  
Figure 1. Training Algorithm 
Input: Alignment models stM  and tsM , 
translation dictionary D , and testing 
data 
(1) Apply the adaptation model stM and 
tsM  to the testing data to get two 
different alignment results. 
(2) Select the alignment links with higher 
translation probability in the translation 
dictionary D . 
Output: Alignment results on the testing data
Figure 2. Testing Algorithm 
The second method ?Gen+Spec? directly combines 
the out-of-domain corpus and the in-domain corpus 
as training data. The third method ?Gen? only uses 
the out-of-domain corpus as training data. The 
fourth method ?Spec? only uses the in-domain 
corpus as training data. For each of the last three 
methods, we first train bi-directional alignment 
models using the training data. Then we build a 
translation dictionary based on the alignment 
results on the training data and filter it using 
log-likelihood ratio as described in section 4. 
6.1 
6.2 
Training and Testing Data 
In this paper, we take English-Chinese word 
alignment as a case study. We use a sentence- 
aligned out-of-domain English-Chinese bilingual 
corpus, which includes 320,000 bilingual sentence 
pairs. The average length of the English sentences 
is 13.6 words while the average length of the 
Chinese sentences is 14.2 words. 
We also use a sentence-aligned in-domain 
English-Chinese bilingual corpus (operation 
manuals for diagnostic ultrasound systems), which 
includes 5,862 bilingual sentence pairs. The 
average length of the English sentences is 12.8 
words while the average length of the Chinese 
sentences is 11.8 words. From this domain-specific 
corpus, we randomly select 416 pairs as testing 
data. We also select 400 pairs to be manually 
annotated as held-out set (development set) to 
adjust parameters. The remained 5,046 pairs are 
used as domain-specific training data. 
The Chinese sentences in both the training set 
and the testing set are automatically segmented 
into words. In order to exclude the effect of the 
segmentation errors on our alignment results, the 
segmentation errors in our testing set are 
post-corrected. The alignments in the testing set 
are manually annotated, which includes 3,166 
alignment links. Among them, 504 alignment links 
include multiword units.  
Evaluation Metrics 
We use the same evaluation metrics as described in 
(Wu and Wang, 2004). If we use  to represent 
the set of alignment links identified by the 
proposed methods and  to denote the reference 
alignment set, the methods to calculate the 
precision, recall, f-measure, and alignment error 
rate (AER) are shown in Equation (13), (14), (15), 
and (16). It can be seen that the higher the 
f-measure is, the lower the alignment error rate is. 
Thus, we will only show precision, recall and AER 
scores in the evaluation results. 
GS
CS
|S|
|SS|
G
CG ?=precision      
(13)
471
|S|
 |SS|
C
CG ?=recall  
(14)
||||
||2
CG
CG
SS
SS
fmeasure +
??=  (15)
fmeasure
SS
SS
AER
CG
CG ?=+
???= 1
||||
||2
1 (16)
 
6.3 Evaluation Results 
We use the held-out set described in section 6.1 to 
set the interpolation weights. The coefficient ?  in 
Equation (8) is set to 0.8, the interpolation weight 
 in Equation (9) is set to 0.1, the interpolation 
weight  in model 3 in Equation (10) is set to 
0.1, and the interpolation weight  in model 4 is 
set to 1. In addition, log-likelihood ratio score 
thresholds are set to  and . With 
these parameters, we get the lowest alignment error 
rate on the held-out set. 
n?
d?
d?
301 =? 252 =?
Using these parameters, we build two 
adaptation models and a translation dictionary on 
the training data, which are applied to the testing 
set. The evaluation results on our testing set are 
shown in Table 1. From the results, it can be seen 
that our approach performs the best among all of 
the methods, achieving the lowest alignment error 
rate. Compared with the method ?ResAdapt?, our 
method achieves a higher precision without loss of 
recall, resulting in an error rate reduction of 6.56%. 
Compared with the method ?Gen+Spec?, our 
method gets a higher recall, resulting in an error 
rate reduction of 17.43%. This indicates that our 
model adaptation method is very effective to 
alleviate the data-sparseness problem of 
domain-specific word alignment. 
Method Precision Recall AER 
Ours 0.8490 0.7599 0.1980
ResAdapt 0.8198 0.7587 0.2119
Gen+Spec 0.8456 0.6905 0.2398
Gen 0.8589 0.6576 0.2551
Spec 0.8386 0.6731 0.2532
Table 1. Word Alignment Adaptation Results 
The method that only uses the large-scale 
out-of-domain corpus as training data does not 
produce good result. The alignment error rate is 
almost the same as that of the method only using 
the in-domain corpus. In order to further analyze 
the result, we classify the alignment links into two 
classes: single word alignment links (SWA) and 
multiword alignment links (MWA). Single word 
alignment links only include one-to-one 
alignments. The multiword alignment links include 
those links in which there are multiword units in 
the source language or/and the target language. 
The results are shown in Table 2. From the results, 
it can be seen that the method ?Spec? produces 
better results for multiword alignment while the 
method ?Gen? produces better results for single 
word alignment. This indicates that the multiword 
alignment links mainly include the domain-specific 
words. Among the 504 multiword alignment links, 
about 60% of the links include domain-specific 
words. In Table 2, we also present the results of 
our method. Our method achieves the lowest error 
rate results on both single word alignment and 
multiword alignment.  
Method Precision Recall AER 
Ours (SWA) 0.8703 0.8621 0.1338
Ours (MWA) 0.5635 0.2202 0.6833
Gen (SWA) 0.8816 0.7694 0.1783
Gen (MWA) 0.3366 0.0675 0.8876
Spec (SWA) 0.8710 0.7633 0.1864
Spec (MWA) 0.4760 0.1964 0.7219
Table 2. Single Word and Multiword Alignment 
Results 
In order to further compare our method with the 
method described in (Wu and Wang, 2004). We do 
another experiment using almost the same-scale 
in-domain training corpus as described in (Wu and 
Wang, 2004). From the in-domain training corpus, 
we randomly select about 500 sentence pairs to 
build the smaller training set. The testing data is 
the same as shown in section 6.1. The evaluation 
results are shown in Table 3. 
Method Precision Recall AER 
Ours 0.8424 0.7378 0.2134
ResAdapt 0.8027 0.7262 0.2375
Gen+Spec 0.8041 0.6857 0.2598
Table 3. Alignment Adaptation Results Using a 
Smaller In-Domain Corpus 
Compared with the method ?Gen+Spec?, our 
method achieves an error rate reduction of 17.86% 
472
while the method ?ResAdapt? described in (Wu 
and Wang, 2004) only achieves an error rate 
reduction of 8.59%. Compared with the method 
?ResAdapt?, our method achieves an error rate 
reduction of 10.15%. 
This result is different from that in (Wu and 
Wang, 2004), where their method achieved an 
error rate reduction of 21.96% as compared with 
the method ?Gen+Spec?. The main reason is that 
the in-domain training corpus and testing corpus in 
this paper are different from those in (Wu and 
Wang, 2004). The training data and the testing data 
described in (Wu and Wang, 2004) are from a 
single manual. The data in our corpus are from 
several manuals describing how to use the 
diagnostic ultrasound systems. 
In addition to the above evaluations, we also 
evaluate our model adaptation method using the 
"refined" combination in Och and Ney (2000) 
instead of the translation dictionary. Using the 
"refined" method to select the alignments produced 
by our model adaptation method (AER: 0.2371) 
still yields better result than directly combining 
out-of-domain and in-domain corpora as training 
data of the "refined" method (AER: 0.2290). 
6.4 The Effect of In-Domain Corpus 
In general, it is difficult to obtain large-scale 
in-domain bilingual corpus. For some domains, 
only a very small-scale bilingual sentence pairs are 
available. Thus, in order to analyze the effect of the 
size of in-domain corpus, we randomly select 
sentence pairs from the in-domain training corpus 
to generate five training sets. The numbers of 
sentence pairs in these five sets are 1,010, 2,020, 
3,030, 4,040 and 5,046. For each training set, we 
use model 4 in section 2 to train an in-domain 
model. The out-of-domain corpus for the 
adaptation experiments and the testing set are the 
same as described in section 6.1. 
# Sentence 
Pairs Precision Recall AER 
1010 0.8385 0.7394 0.2142
2020 0.8388 0.7514 0.2073
3030 0.8474 0.7558 0.2010
4040 0.8482 0.7555 0.2008
5046 0.8490 0.7599 0.1980
Table 4. Alignment Adaptation Results Using 
In-Domain Corpora of Different Sizes 
# Sentence 
Pairs Precision Recall AER 
1010 0.8737 0.6642 0.2453
2020 0.8502 0.6804 0.2442
3030 0.8473 0.6874 0.2410
4040 0.8430 0.6917 0.2401
5046 0.8456 0.6905 0.2398
Table 5. Alignment Results Directly Combining 
Out-of-Domain and In-Domain Corpora  
The results are shown in Table 4 and Table 5. 
Table 4 describes the alignment adaptation results 
using in-domain corpora of different sizes. Table 5 
describes the alignment results by directly 
combining the out-of-domain corpus and the 
in-domain corpus of different sizes.  From the 
results, it can be seen that the larger the size of 
in-domain corpus is, the smaller the alignment 
error rate is. However, when the number of the 
sentence pairs increase from 3030 to 5046, the 
error rate reduction in Table 4 is very small. This is 
because the contents in the specific domain are 
highly replicated. This also shows that increasing 
the domain-specific corpus does not obtain great 
improvement on the word alignment results. 
Comparing the results in Table 4 and Table 5, we 
find out that our adaptation method reduces the 
alignment error rate on all of the in-domain 
corpora of different sizes.  
6.5 The Effect of Out-of-Domain Corpus 
In order to further analyze the effect of the 
out-of-domain corpus on the adaptation results, we 
randomly select sentence pairs from the 
out-of-domain corpus to generate five sets. The 
numbers of sentence pairs in these five sets are 
65,000, 130,000, 195,000, 260,000, and 320,000 
(the entire out-of-domain corpus). In the adaptation 
experiments, we use the entire in-domain corpus 
(5046 sentence pairs). The adaptation results are 
shown in Table 6. 
From the results in Table 6, it can be seen that 
the larger the size of out-of-domain corpus is, the 
smaller the alignment error rate is. However, when 
the number of the sentence pairs is more than 
130,000, the error rate reduction is very small. This 
indicates that we do not need a very large bilingual 
out-of-domain corpus to improve domain-specific 
word alignment results. 
 
473
# Sentence 
Pairs (k) Precision Recall AER 
65 0.8441 0.7284 0.2180
130 0.8479 0.7413 0.2090
195 0.8454 0.7461 0.2073
260 0.8426 0.7508 0.2059
320 0.8490 0.7599 0.1980
Table 6. Adaptation Alignment Results Using 
Out-of-Domain Corpora of Different Sizes 
7 Conclusion 
This paper proposes an approach to improve 
domain-specific word alignment through alignment 
model adaptation. Our approach first trains two 
alignment models with a large-scale out-of-domain 
corpus and a small-scale domain-specific corpus. 
Second, we build a new adaptation model by 
linearly interpolating these two models. Third, we 
apply the new model to the domain-specific corpus 
and improve the word alignment results. In 
addition, with the training data, an interpolated 
translation dictionary is built to select the word 
alignment links from different alignment results. 
Experimental results indicate that our approach 
achieves a precision of 84.90% and a recall of 
75.99% for word alignment in a specific domain. 
Our method achieves a relative error rate reduction 
of 17.43% as compared with the method directly 
combining the out-of-domain corpus and the 
in-domain corpus as training data.  It also 
achieves a relative error rate reduction of 6.56% as 
compared with the previous work in (Wu and 
Wang, 2004). In addition, when we train the model 
with a smaller-scale in-domain corpus as described 
in (Wu and Wang, 2004), our method achieves an 
error rate reduction of 10.15% as compared with 
the method in (Wu and Wang, 2004). 
We also use in-domain corpora and 
out-of-domain corpora of different sizes to perform 
adaptation experiments. The experimental results 
show that our model adaptation method improves 
alignment results on in-domain corpora of different 
sizes.  The experimental results also show that 
even a not very large out-of-domain corpus can 
help to improve the domain-specific word 
alignment through alignment model adaptation. 
References 
L. Ahrenberg, M. Merkel, M. Andersson. 1998. A 
Simple Hybrid Aligner for Generating Lexical 
Correspondences in Parallel Tests. In Proc. of 
ACL/COLING-1998, pp. 29-35. 
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, 
D. Melamed, F. J. Och, D. Purdy, N. A. Smith, D. 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, R. 
Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. 
Computational Linguistics, 19(2): 263-311. 
C. Cherry and D. Lin. 2003. A Probability Model to 
Improve Word Alignment. In Proc. of ACL-2003, pp. 
88-95. 
T. Dunning. 1993. Accurate Methods for the Statistics of 
Surprise and Coincidence. Computational Linguistics, 
19(1): 61-74. 
R. Iyer,  M. Ostendorf,  H. Gish.  1997. Using 
Out-of-Domain Data to Improve In-Domain 
Language Models. IEEE Signal Processing Letters, 
221-223. 
S. J. Ker and J. S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2): 313-343. 
I. D. Melamed. 1997. A Word-to-Word Model of 
Translational Equivalence. In Proc. of ACL 1997, pp. 
490-497. 
F. J. Och and H. Ney. 2000. Improved Statistical 
Alignment Models. In Proc. of ACL-2000, pp. 
440-447. 
A. Pe?as, F. Verdejo, J. Gonzalo. 2001. Corpus-based 
Terminology Extraction Applied to Information 
Access. In Proc. of the Corpus Linguistics 2001, vol. 
13. 
F. Smadja, K. R. McKeown, V. Hatzivassiloglou. 1996. 
Translating Collocations for Bilingual Lexicons: a 
Statistical Approach. Computational Linguistics, 
22(1): 1-38. 
D. Tufis and A. M. Barbu. 2002. Lexical Token 
Alignment: Experiments, Results and Application. In 
Proc. of LREC-2002, pp. 458-465. 
D. Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics, 23(3): 377-403. 
H. Wu and H. Wang. 2004. Improving Domain-Specific 
Word Alignment with a General Bilingual Corpus. In 
R. E. Frederking and K. B. Taylor (Eds.), Machine 
Translation: From Real Users to Research: 6th 
conference of AMTA-2004, pp. 262-271. 
474
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 874?881,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Alignment for Languages with Scarce Resources 
Using Bilingual Corpora of Other Language Pairs 
 
Haifeng Wang      Hua Wu      Zhanyi Liu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wanghaifeng, wuhua, liuzhanyi}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper proposes an approach to im-
prove word alignment for languages with 
scarce resources using bilingual corpora 
of other language pairs. To perform word 
alignment between languages L1 and L2, 
we introduce a third language L3. Al-
though only small amounts of bilingual 
data are available for the desired lan-
guage pair L1-L2, large-scale bilingual 
corpora in L1-L3 and L2-L3 are available. 
Based on these two additional corpora 
and with L3 as the pivot language, we 
build a word alignment model for L1 and 
L2. This approach can build a word 
alignment model for two languages even 
if no bilingual corpus is available in this 
language pair. In addition, we build an-
other word alignment model for L1 and 
L2 using the small L1-L2 bilingual cor-
pus. Then we interpolate the above two 
models to further improve word align-
ment between L1 and L2. Experimental 
results indicate a relative error rate reduc-
tion of 21.30% as compared with the 
method only using the small bilingual 
corpus in L1 and L2. 
1 Introduction 
Word alignment was first proposed as an inter-
mediate result of statistical machine translation 
(Brown et al, 1993). Many researchers build 
alignment links with bilingual corpora (Wu, 
1997; Och and Ney, 2003; Cherry and Lin, 2003; 
Zhang and Gildea, 2005). In order to achieve 
satisfactory results, all of these methods require a 
large-scale bilingual corpus for training. When 
the large-scale bilingual corpus is unavailable, 
some researchers acquired class-based alignment 
rules with existing dictionaries to improve word 
alignment (Ker and Chang, 1997). Wu et al 
(2005) used a large-scale bilingual corpus in 
general domain to improve domain-specific word 
alignment when only a small-scale domain-
specific bilingual corpus is available. 
This paper proposes an approach to improve 
word alignment for languages with scarce re-
sources using bilingual corpora of other language 
pairs. To perform word alignment between lan-
guages L1 and L2, we introduce a third language 
L3 as the pivot language. Although only small 
amounts of bilingual data are available for the 
desired language pair L1-L2, large-scale bilin-
gual corpora in L1-L3 and L2-L3 are available. 
Using these two additional bilingual corpora, we 
train two word alignment models for language 
pairs L1-L3 and L2-L3, respectively. And then, 
with L3 as a pivot language, we can build a word 
alignment model for L1 and L2 based on the 
above two models. Here, we call this model an 
induced model. With this induced model, we per-
form word alignment between languages L1 and 
L2 even if no parallel corpus is available for this 
language pair. In addition, using the small bilin-
gual corpus in L1 and L2, we train another word 
alignment model for this language pair. Here, we 
call this model an original model. An interpo-
lated model can be built by interpolating the in-
duced model and the original model. 
As a case study, this paper uses English as the 
pivot language to improve word alignment be-
tween Chinese and Japanese. Experimental re-
sults show that the induced model performs bet-
ter than the original model trained on the small 
Chinese-Japanese corpus. And the interpolated 
model further improves the word alignment re-
sults, achieving a relative error rate reduction of 
874
21.30% as compared with results produced by 
the original model. 
The remainder of this paper is organized as 
follows. Section 2 discusses the related work. 
Section 3 introduces the statistical word align-
ment models. Section 4 describes the parameter 
estimation method using bilingual corpora of 
other language pairs. Section 5 presents the in-
terpolation model. Section 6 reports the experi-
mental results. Finally, we conclude and present 
the future work in section 7. 
2 Related Work 
A shared task on word alignment was organized 
as part of the ACL 2005 Workshop on Building 
and Using Parallel Texts (Martin et al, 2005). 
The focus of the task was on languages with 
scarce resources. Two different subtasks were 
defined: Limited resources and Unlimited re-
sources. The former subtask only allows partici-
pating systems to use the resources provided. 
The latter subtask allows participating systems to 
use any resources in addition to those provided. 
For the subtask of unlimited resources, As-
wani and Gaizauskas (2005) used a multi-feature 
approach for many-to-many word alignment on 
English-Hindi parallel corpora. This approach 
performed local word grouping on Hindi sen-
tences and used other methods such as dictionary 
lookup, transliteration similarity, expected Eng-
lish words, and nearest aligned neighbors. Martin 
et al (2005) reported that this method resulted in 
absolute improvements of up to 20% as com-
pared with the case of only using limited re-
sources. Tufis et al (2005) combined two word 
aligners: one is based on the limited resources 
and the other is based on the unlimited resources.  
The unlimited resource consists of a translation 
dictionary extracted from the alignment of Ro-
manian and English WordNet. Lopez and Resnik 
(2005) extended the HMM model by integrating 
a tree distortion model based on a dependency 
parser built on the English side of the parallel 
corpus. The latter two methods produced compa-
rable results with those methods using limited 
resources. All the above three methods use some 
language dependent resources such as dictionary, 
thesaurus, and dependency parser. And some 
methods, such as transliteration similarity, can 
only be used for very similar language pairs. 
In this paper, besides the limited resources for 
the given language pair, we make use of large 
amounts of resources available for other lan-
guage pairs to address the alignment problem for 
languages with scarce resources. Our method 
does not need language-dependent resources or 
deep linguistic processing. Thus, it is easy to 
adapt to any language pair where a pivot lan-
guage and corresponding large-scale bilingual 
corpora are available. 
3 Statistical Word Alignment 
According to the IBM models (Brown et al, 
1993), the statistical word alignment model can 
be generally represented as in equation (1).  
?=
a'
c|f,a'
c|fa,
c|fa,
)Pr(
)Pr(
)Pr(  
(1)
Where,  and  represent the source sentence 
and the target sentence, respectively
c f
1. 
In this paper, we use a simplified IBM model 
4 (Al-Onaizan et al, 1999), which is shown in 
equation (2). This version does not take into ac-
count word classes in Brown et al (1993). 
))))(()](([            
))()](([(           
)|( )|(             
   
         
)Pr(
0,1
1
0,1
11
11
1
2
0
0
0 00
?
?
??
?=
>
?=
?
==
?
???
+??=
??
????
?
???
? ?=
m
aj
j
m
aj
ij
m
j
aj
l
i
ii
m
j
j
j
jpjdahj
jdahj
cftcn
pp
m
?
?
?
? ??
c|fa,
 
(2)
ml,  are the lengths of the source sentence and 
the target sentence respectively. 
j  is the position index of the target word. 
ja  is the position of the source word aligned to 
the jth target word. 
i?  is the fertility of . ic
0p ,  are the fertility probabilities for , 
and 
1p 0c
110 =+ pp . 
)|
jaj
ct(f  is the word translation probability. 
)|( ii cn ?  is the fertility probability. 
)( 11 ?? ijd ?  is the distortion probability for the 
head word of the cept. 
))((1 jpjd ?>  is the distortion probability for 
the non-head words of the cept. 
                                                 
1 This paper uses c and f to represent a Chinese sentence 
and a Japanese sentence, respectively. And e represents an 
English sentence. 
875
}:{min)( k
k
aikih == is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
. 
i?  is the center of cept i. 
During the training process, IBM model 3 is 
first trained, and then the parameters in model 3 
are employed to train model 4. For convenience, 
we describe model 3 in equation (3). The main 
difference between model 3 and model 4 lies in 
the calculation of distortion probability. 
??
??
?==
==
?
?
??
????
?
???
? ?=
m
aj
j
m
j
aj
l
i
i
l
i
ii
m
j
j
mlajdcft
cn
pp
m
0,11
11
1
2
0
0
0
),,|()|(                   
!  )|(                   
   
)Pr( 00
??
?
? ??c|fa,
 
(3)
4 Parameter Estimation Using Bilingual 
Corpora of Other Language Pairs 
As shown in section 3, the word alignment 
model mainly has three kinds of parameters that 
must be specified, including the translation prob-
ability, the fertility probability, and the distortion 
probability. The parameters are usually estimated 
by using bilingual sentence pairs in the desired 
languages, namely Chinese and Japanese here. In 
this section, we describe how to estimate the pa-
rameters without using the Chinese-Japanese 
bilingual corpus. We introduce English as the 
pivot language, and use the Chinese-English and 
English-Japanese bilingual corpora to estimate 
the parameters of the Chinese-Japanese word 
alignment model. With these two corpora, we 
first build Chinese-English and English-Japanese 
word alignment models as described in section 3. 
Then, based on these two models, we estimate 
the parameters of Chinese-Japanese word align-
ment model. The estimated model is named in-
duced model. 
The following subsections describe the 
method to estimate the parameters of Chinese-
Japanese alignment model. For reversed Japa-
nese-Chinese word alignment, the parameters 
can be estimated with the same method. 
4.1  Translation Probability 
Basic Translation Probability  
We use the translation probabilities trained 
with Chinese-English and English-Japanese cor-
pora to estimate the Chinese-Japanese probabil-
ity as shown in equation (4). In (4), we assume 
that the translation probability  is 
independent of the Chinese word . 
),|(EJ ikj ceft
ic
)|()|(     
)|(),|(      
)|(
CEEJ
CEEJ
CJ
ik
e
kj
ik
e
ikj
ij
ceteft
cetceft
cft
k
k
?
?
?=
?=  
(4)
Where  is the translation probability 
for Chinese-Japanese word alignment. 
is the translation probability trained 
using the English-Japanese corpus.  is 
the translation probability trained using the Chi-
nese-English corpus. 
)|(CJ ij cft
)|(EJ kj eft
)|(CE ik cet
Cross-Language Word Similarity 
In any language, there are ambiguous words 
with more than one sense. Thus, some noise may 
be introduced by the ambiguous English word 
when we estimate the Chinese-Japanese transla-
tion probability using English as the pivot lan-
guage. For example, the English word "bank" has 
at least two senses, namely: 
bank1 - a financial organization 
bank2 - the border of a river 
Let us consider the Chinese word: 
?? - bank2 (the border of a river) 
And the Japanese word: 
?? - bank1 (a financial organization) 
In the Chinese-English corpus, there is high 
probability that the Chinese word "??(bank2)"  
would be translated into the English word "bank". 
And in the English-Japanese corpus, there is also 
high probability that the English word "bank" 
would be translated into the Japanese word "?
?(bank1)". 
As a result, when we estimate the translation 
probability using equation (4), the translation 
probability of "?? (bank1)" given "??
(bank2)" is high. Such a result is not what we 
expect. 
In order to alleviate this problem, we intro-
duce cross-language word similarity to improve 
translation probability estimation in equation (4). 
The cross-language word similarity describes 
how likely a Chinese word is to be translated into 
a Japanese word with an English word as the 
pivot. We make use of both the Chinese-English 
corpus and the English-Japanese corpus to calcu-
late the cross language word similarity between a 
Chinese word c and a Japanese word f given an 
876
Input: An English word e , a Chinese word , and a Japanese word ; c f
The Chinese-English corpus; The English-Japanese corpus. 
(1) Construct Set 1: identify those Chinese-English sentence pairs that include the given Chinese 
word  and English word , and put the English sentences in the pairs into Set 1. c e
(2) Construct Set 2: identify those English-Japanese sentence pairs that include the given English 
word  and Japanese word , and put the English sentences in the pairs into Set 2. e f
(3) Construct the feature vectors  and  of the given English word using all other words as 
context in Set 1 and Set 2, respectively. 
CEV EJV
>=< ),(, ... ),,(),,( 1122111CE nn ctectecteV  
>=< ),(, ... ),,(),,( 2222211EJ nn ctectecteV  
Where  is the frequency of the context word . ijct je 0=ijct  if  does not occur in Set i . je
(4) Given the English word e , calculate the cross-language word similarity between the Chinese 
word  and the Japanese word  as in equation (5) c f
??
?
?
?
==
j
j
j
j
j
jj
ctct
ctct
VVefcsim
2
2
2
1
21
EJCE
)()(
),cos();,(                                     (5) 
Output: The cross language word similarity  of the Chinese word c and the Japanese 
word given the English word  
);,( efcsim
f e
Figure 1. Similarity Calculation 
English word e. For the ambiguous English word 
e, both the Chinese word c and the Japanese 
word f can be translated into e. The sense of an 
instance of the ambiguous English word e can be 
determined by the context in which the instance 
appears. Thus, the cross-language word similar-
ity between the Chinese word c and the Japanese 
word f can be calculated according to the con-
texts of their English translation e. We use the 
feature vector constructed using the context 
words in the English sentence to represent the 
context. So we can calculate the cross-language 
word similarity using the feature vectors. The 
detailed algorithm is shown in figure 1. This idea 
is similar to translation lexicon extraction via a 
bridge language (Schafer and Yarowsky, 2002). 
For example, the Chinese word "??" and its 
English translation "bank" (the border of a river) 
appears in the following Chinese-English sen-
tence pair: 
(a) ?????????? 
(b) They walked home along the river bank. 
The Japanese word "??" and its English 
translation "bank" (a financial organization) ap-
pears in the following English-Japanese sentence 
pair: 
(c) He has plenty of money in the bank. 
(d) ???????????? 
The context words of the English word "bank" in 
sentences (b) and (c) are quite different. The dif-
ference indicates the cross language word simi-
larity of the Chinese word "??" and the Japa-
nese word "??" is low. So they tend to have 
different senses. 
Translation Probability Embedded with Cross 
Language Word Similarity 
Based on the cross language word similarity 
calculation in equation (5), we re-estimate the 
translation probability as shown in (6). Then we 
normalize it in equation (7). 
The word similarity of the Chinese word "?
? (bank2)" and the Japanese word " ? ?
(bank1)" given the word English word "bank" is 
low. Thus, using the updated estimation method, 
the translation probability of "?? (bank1)" 
given "??(bank2)" becomes low. 
));,()|()|((
)|('
CEEJ
CJ
kjiik
e
kj
ij
efcsimceteft
cft
k
??= ?
 
(6)
?=
'
CJ
CJ
CJ )|'('
)|('
)|(
f
i
ij
ij cft
cft
cft  (7)
4.2  Fertility Probability 
The induced fertility probability is calculated as 
shown in (8). Here, we assume that the probabil-
877
ity ),|(EJ iki cen ?  is independent of the Chinese 
word . ic
)|()|(
)|(),|(
)|(
CEEJ
CEEJ
CJ
ik
e
ki
ik
e
iki
ii
ceten
cetcen
cn
k
k
?=
?=
?
?
?
?
?
 
(8)
Where, )|(CJ ii cn ?  is the fertility probability for 
the Chinese-Japanese alignment. )|(EJ ki en ?  is 
the trained fertility probability for the English-
Japanese alignment. 
4.3  Distortion Probability in Model 3 
With the English language as a pivot language, 
we calculate the distortion probability of model 3. 
For this probability, we introduce two additional 
parameters: one is the position of English word 
and the other is the length of English sentence. 
The distortion probability is estimated as shown 
in (9). 
)),,|Pr(),,,|Pr(         
),,,,|(Pr(
),,|,Pr(),,,,|Pr(
),,|,,Pr(
),,|(
,
,
,
CJ
mlinmlink
mlinkj
mlinkmlinkj
mlinkj
mlijd
nk
nk
nk
?
?=
?=
=
?
?
?
 
(9)
Where, is the estimated distortion 
probability.  is the introduced position of an 
English word. n  is the introduced length of an 
English sentence.  
).,|(CJ mlijd
k
In the above equation, we assume that the po-
sition probability  is independent 
of the position of the Chinese word and the 
length of the Chinese sentence. And we assume 
that the position probability  is in-
dependent of the length of Japanese sentence. 
Thus, we rewrite these two probabilities as fol-
lows. 
),,,,|Pr( mlinkj
),,,|Pr( mlink
),,|(),,|Pr(),,,,|Pr( EJ mnkjdmnkjmlinkj =?  
),,|(),,|Pr(),,,|Pr( CE nlikdnliknmlik =?  
For the length probability, the English sen-
tence length n  is independent of the word posi-
tions i . And we assume that it is uniformly dis-
tributed. Thus, we take it as a constant, and re-
write it as follows.  
constant),|Pr(),,|Pr( == mlnmlin  
According to the above three assumptions, we 
ignore the length probability . Equa-
tion (9) is rewritten in (10).  
),|Pr( mln
? ?=
nk
nlikdmnkjd
mlijd
,
CEEJ
CJ
),,|(),,|(
).,|(
 (10)
4.4  Distortion Probability in Model 4 
In model 4, there are two parameters for the dis-
tortion probability: one for head words and the 
other for non-head words.  
Distortion Probability for Head Words 
The distortion probability for head 
words represents the relative position of the head 
word of the i
)( 11 ?? ijd ?
th cept and the center of the (i-1)th 
cept. Let 1??=? ijj ? , then  is independent of 
the absolute position. Thus, we estimate the dis-
tortion probability by introducing another rela-
tive position 
j?
'j? of English words, which is 
shown in (11).    
?
?
?
????=
?=?
'
EJCE,1
1CJ,1
)'|(Pr)'(
)(
j
i
jjjd
jjd ?
 (11)
Where, )( 1CJ1, ??=? ijjd ? is the estimated dis-
tortion probability for head words in Chinese-
Japanese alignment. is the distortion 
probability for head word in Chinese-English 
alignment. 
)'(CE1, jd ?
)'|(PrEJ jj ??  is the translation prob-
ability of relative Japanese position given rela-
tive English position.  
In order to simplify , we introduce 
and  and let 
)'|(PrEJ jj ??
'j 1'?i? 1''' ??=? ijj ? , where  and 
 are positions of English words. We rewrite 
'j
1'?i?
)'|(PrEJ jj ??  in (12).   
?
?=?
?=?
??
??
??
??
=
??=
??
'':,'
:,
1'1EJ
1'1EJ
EJ
1'1'
11
),'|,(Pr
)'|(Pr
)'|(Pr
jjj
jjj
ii
ii
ii
ii
jj
jj
jj
??
??
??
??  
(12)
The English word in position  is aligned to 
the Japanese word in position , and the English 
word in position  is aligned to the Japanese 
word in position . 
'j
j
1'?i?
1?i?
We assume that  and  are independent, 
 only depends on , and  only depends 
on . Then  can be esti-
mated as shown in (13). 
j 1?i?
j 'j 1?i?
1'?i? ),'|,(Pr 1'1EJ ?? ii jj ??
878
)|(Pr)'|(Pr
),'|,(Pr
1'1EJEJ
1'1EJ
??
??
?= ii
ii
jj
jj
??
??
 (13)
Both of the two parameters in (13) represent 
the position translation probabilities. Thus, we 
can estimate them from the distortion probability 
in model 3.  is estimated as shown in 
(14).  And  can be estimated in 
the same way. In (14), we also assume that the 
sentence length distribution  is inde-
pendent of the word position and that it is uni-
formly distributed. 
)'|(PrEJ jj
)|(Pr 1'1EJ ?? ii ??
)'|,Pr( jml
?
?
?
=
?=
=
ml
ml
ml
mljjd
jmlmljjd
jmljjj
,
EJ
,
EJ
,
EJEJ
),,'|(
)'|,Pr(),,'|(
)'|,,(Pr)'|(Pr
 (14)
Distortion Probability for Non-Head Words 
The distortion probability de-
scribes the distribution of the relative position of 
non-head words. In the same way, we introduce 
relative position of English words, and model 
the probability in (15). 
))((1 jpjd ?>
'j?
?
?
>
>
????=
?=?
'
EJCE,1
CJ,1
)'|(Pr)'(
))((
j
jjjd
jpjjd
 (15)
))((CJ1, jpjjd ?=?> is the estimated distortion 
probability for the non-head words in Chinese-
Japanese alignment.  is the distortion 
probability for non-head words in Chinese-
English alignment. 
)'(CE1, jd ?>
)'|(PrEJ jj ?? is the translation 
probability of the relative Japanese position 
given the relative English position.  
In fact,  has the same interpreta-
tion as in (12). Thus, we introduce two parame-
ters and  and let , where 
and  are positions of English words. The 
final distortion probability for non-head words 
can be estimated as shown in (16). 
)'|(PrEJ jj ??
'j )'( jp )'('' jpjj ?=?
'j )'( jp
)))'(|)((Pr)'|(Pr
)'(())((
')'(':)'(,'
)(:)(,
EJEJ
'
CE1,CJ1,
?
?
?=? ?=?
?
>>
?
??=?=?
jjpjjpj
jjpjjpj
j
jpjpjj
jdjpjjd
 (16)
5 Interpolation Model 
With the Chinese-English and English-Japanese 
corpora, we can build the induced model for Chi-
nese-Japanese word alignment as described in 
section 4. If we have small amounts of Chinese-
Japanese corpora, we can build another word 
alignment model using the method described in 
section 3, which is called the original model here. 
In order to further improve the performance of 
Chinese-Japanese word alignment, we build an 
interpolated model by interpolating the induced 
model and the original model.  
Generally, we can interpolate the induced 
model and the original model as shown in equa-
tion (17). 
)(Pr)1( )(Pr
)Pr(
IO c|fa,c|fa,
c|fa,
??+?= ??  (17)
Where is the original model trained 
from the Chinese-Japanese corpus, and 
 is the induced model trained from the 
Chinese-English and English-Japanese corpora. 
)(PrO c|fa,
)(PrI c|fa,
?  is an interpolation weight. It can be a constant 
or a function of f  and . c
 In both model 3 and model 4, there are mainly 
three kinds of parameters: translation probability, 
fertility probability and distortion probability. 
These three kinds of parameters have their own 
interpretation in these two models. In order to 
obtain fine-grained interpolation models, we in-
terpolate the three kinds of parameters using dif-
ferent weights, which are obtained in the same 
way as described in Wu et al (2005). t?  repre-
sents the weights for translation probability. n?  
represents the weights for fertility probability. 
d3?  and d4?  represent the weights for distortion 
probability in model 3 and in model 4, respec-
tively. d4?  is set as the interpolation weight for 
both the head words and the non-head words. 
The above four weights are obtained using a 
manually annotated held-out set. 
6 Experiments 
In this section, we compare different word 
alignment methods for Chinese-Japanese align-
ment. The "Original" method uses the original 
model trained with the small Chinese-Japanese 
corpus.  The "Basic Induced" method uses the 
induced model that employs the basic translation 
probability without introducing cross-language 
word similarity. The "Advanced Induced" 
method uses the induced model that introduces 
the cross-language word similarity into the calcu-
lation of the translation probability. The "Inter-
polated" method uses the interpolation of the 
word alignment models in the "Advanced In-
duced" and "Original" methods. 
879
6.1 Data 
There are three training corpora used in this pa-
per: Chinese-Japanese (CJ) corpus, Chinese-
English (CE) Corpus, and English-Japanese (EJ) 
Corpus. All of these tree corpora are from gen-
eral domain. The Chinese sentences and Japa-
nese sentences in the data are automatically seg-
mented into words. The statistics of these three 
corpora are shown in table 1. "# Source Words" 
and "# Target Words" mean the word number of 
the source and target sentences, respectively. 
Language 
Pairs 
#Sentence 
Pairs 
# Source 
Words 
# Target 
Words 
CJ 21,977 197,072 237,834 
CE 329,350 4,682,103 4,480,034
EJ 160,535 1,460,043 1,685,204
Table 1. Statistics for Training Data 
Besides the training data, we also have held-
out data and testing data. The held-out data in-
cludes 500 Chinese-Japanese sentence pairs, 
which is used to set the interpolated weights de-
scribed in section 5. We use another 1,000 Chi-
nese-Japanese sentence pairs as testing data, 
which is not included in the training data and the 
held-out data. The alignment links in the held-out 
data and the testing data are manually annotated. 
Testing data includes 4,926 alignment links2. 
6.2  Evaluation Metrics 
We use the same metrics as described in Wu et al 
(2005), which is similar to those in (Och and Ney, 
2000). The difference lies in that Wu et al (2005) 
took all alignment links as sure links. 
If we use  to represent the set of alignment 
links identified by the proposed methods and  
to denote the reference alignment set, the meth-
ods to calculate the precision, recall, f-measure, 
and alignment error rate (AER) are shown in 
equations (18), (19), (20), and (21), respectively. 
It can be seen that the higher the f-measure is, 
the lower the alignment error rate is. Thus, we 
will only show precision, recall and AER scores 
in the evaluation results. 
GS
CS
||
||
G
CG
S
SS
precision
?=      (18)
||
 ||
C
CG
S
SS
recall
?=  (19)
                                                 
2 For a non one-to-one link, if m source words are aligned to 
n target words, we take it as one alignment link instead of 
m?n alignment links. 
||||
||2
CG
CG
SS
SS
fmeasure +
?=  (20)
fmeasure
SS
SS
AER ?=+
??= 1
||||
||2
1
CG
CG  (21)
6.3 Experimental Results 
We use the held-out data described in section 6.1 
to set the interpolation weights in section 5. t?  is 
set to 0.3, n?  is set to 0.1, d3?  for model 3  is set 
to 0.5, and d4?  for model 4 is set to 0.1. With 
these parameters, we get the lowest alignment 
error rate on the held-out data. 
For each method described above, we perform 
bi-directional (source to target and target to 
source) word alignment and obtain two align-
ment results. Based on the two results, we get a 
result using "refined" combination as described 
in (Och and Ney, 2000). Thus, all of the results 
reported here describe the results of the "refined" 
combination. For model training, we use the 
GIZA++ toolkit3. 
Method Precision Recall AER 
Interpolated 0.6955 0.5802 0.3673
Advanced 
Induced 0.7382 0.4803 0.4181
Basic  
Induced 0.6787 0.4602 0.4515
Original 0.6026 0.4783 0.4667
Table 2. Word Alignment Results 
The evaluation results on the testing data are 
shown in table 2.  From the results, it can be seen 
that both of the two induced models perform bet-
ter than the "Original" method that only uses the 
limited Chinese-Japanese sentence pairs. The 
"Advanced Induced" method achieves a relative 
error rate reduction of 10.41% as compared with 
the "Original" method. Thus, with the Chinese-
English corpus and the English-Japanese corpus, 
we can achieve a good word alignment results 
even if no Chinese-Japanese parallel corpus is 
available. After introducing the cross-language 
word similarity into the translation probability, 
the "Advanced Induced" method achieves a rela-
tive error rate reduction of 7.40% as compared 
with the "Basic Induced" method. It indicates 
that cross-language word similarity is effective in 
the calculation of the translation probability. 
Moreover, the "interpolated" method further im-
proves the result, which achieves relative error 
                                                 
3 It is located at http://www.fjoch.com/ GIZA++.html. 
880
rate reductions of 12.51% and 21.30% as com-
pared with the "Advanced Induced" method and 
the "Original" method. 
7 Conclusion and Future Work 
This paper presented a word alignment approach 
for languages with scarce resources using bilin-
gual corpora of other language pairs. To perform 
word alignment between languages L1 and L2, 
we introduce a pivot language L3 and bilingual 
corpora in L1-L3 and L2-L3. Based on these two 
corpora and with the L3 as a pivot language, we 
proposed an approach to estimate the parameters 
of the statistical word alignment model. This ap-
proach can build a word alignment model for the 
desired language pair even if no bilingual corpus 
is available in this language pair. Experimental 
results indicate a relative error reduction of 
10.41% as compared with the method using the 
small bilingual corpus. 
In addition, we interpolated the above model 
with the model trained on the small L1-L2 bilin-
gual corpus to further improve word alignment 
between L1 and L2. This interpolated model fur-
ther improved the word alignment results by 
achieving a relative error rate reduction of 
12.51% as compared with the method using the 
two corpora in L1-L3 and L3-L2, and a relative 
error rate reduction of 21.30% as compared with 
the method using the small bilingual corpus in 
L1 and L2. 
In future work, we will perform more evalua-
tions. First, we will further investigate the effect 
of the size of corpora on the alignment results. 
Second, we will investigate different parameter 
combination of the induced model and the origi-
nal model. Third, we will also investigate how 
simpler IBM models 1 and 2 perform, in com-
parison with IBM models 3 and 4. Last, we will 
evaluate the word alignment results in a real ma-
chine translation system, to examine whether 
lower word alignment error rate will result in 
higher translation accuracy. 
References 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
Niraj Aswani and Robert Gaizauskas. 2005. Aligning 
Words in English-Hindi Parallel Corpora. In Proc. 
of the ACL 2005 Workshop on Building and Using 
Parallel Texts: Data-driven Machine Translation 
and Beyond, pages 115-118.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st  Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95. 
Sue J. Ker and Jason S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational Lin-
guistics, 23(2): 313-343. 
Adam Lopez and Philip Resnik. 2005. Improved 
HMM Alignment Models for Languages with 
Scarce Resources. In Proc. of the ACL-2005 Work-
shop on Building and Using Parallel Texts: Data-
driven Machine Translation and Beyond, pages 83-
86. 
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on 
Building and Using Parallel Texts: Data-driven 
Machine Translation and Beyond, pages 65-74. 
Charles Schafer and David Yarowsky. 2002. Inducing 
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of the 6th 
Conference on Natural Language Learning 2002 
(CoNLL-2002), pages 1-7. 
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan 
Stefanescu. 2005. Combined Word Alignments. In 
Proc. of the ACL-2005 Workshop on Building and 
Using Parallel Texts: Data-driven Machine Trans-
lation and Beyond, pages 107-110. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403. 
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. 
Alignment Model Adaptation for Domain-Specific 
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474. 
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for 
Alignment. In Proc. of the 43rd Annual Meeting of 
the Association for Computational Linguistics 
(ACL-2005), pages 475-482. 
881
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 913?920,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Boosting Statistical Word Alignment Using  
Labeled and Unlabeled Data 
 
Hua Wu      Haifeng Wang      Zhanyi Liu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wuhua, wanghaifeng, liuzhanyi}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper proposes a semi-supervised 
boosting approach to improve statistical 
word alignment with limited labeled data 
and large amounts of unlabeled data. The 
proposed approach modifies the super-
vised boosting algorithm to a semi-
supervised learning algorithm by incor-
porating the unlabeled data. In this algo-
rithm, we build a word aligner by using 
both the labeled data and the unlabeled 
data. Then we build a pseudo reference 
set for the unlabeled data, and calculate 
the error rate of each word aligner using 
only the labeled data. Based on this semi-
supervised boosting algorithm, we inves-
tigate two boosting methods for word 
alignment. In addition, we improve the 
word alignment results by combining the 
results of the two semi-supervised boost-
ing methods. Experimental results on 
word alignment indicate that semi-
supervised boosting achieves relative er-
ror reductions of 28.29% and 19.52% as 
compared with supervised boosting and 
unsupervised boosting, respectively. 
1 Introduction 
Word alignment was first proposed as an inter-
mediate result of statistical machine translation 
(Brown et al, 1993). In recent years, many re-
searchers build alignment links with bilingual 
corpora (Wu, 1997; Och and Ney, 2003; Cherry 
and Lin, 2003; Wu et al, 2005; Zhang and 
Gildea, 2005). These methods unsupervisedly 
train the alignment models with unlabeled data. 
A question about word alignment is whether 
we can further improve the performances of the 
word aligners with available data and available 
alignment models. One possible solution is to use 
the boosting method (Freund and Schapire, 
1996), which is one of the ensemble methods 
(Dietterich, 2000). The underlying idea of boost-
ing is to combine simple "rules" to form an en-
semble such that the performance of the single 
ensemble is improved. The AdaBoost (Adaptive 
Boosting) algorithm by Freund and Schapire 
(1996) was developed for supervised learning. 
When it is applied to word alignment, it should 
solve the problem of building a reference set for 
the unlabeled data. Wu and Wang (2005) devel-
oped an unsupervised AdaBoost algorithm by 
automatically building a pseudo reference set for 
the unlabeled data to improve alignment results. 
In fact, large amounts of unlabeled data are 
available without difficulty, while labeled data is 
costly to obtain. However, labeled data is valu-
able to improve performance of learners. Conse-
quently, semi-supervised learning, which com-
bines both labeled and unlabeled data, has been 
applied to some NLP tasks such as word sense 
disambiguation (Yarowsky, 1995; Pham et al, 
2005), classification (Blum and Mitchell, 1998; 
Thorsten, 1999), clustering (Basu et al, 2004), 
named entity classification (Collins and Singer, 
1999), and parsing (Sarkar, 2001). 
In this paper, we propose a semi-supervised 
boosting method to improve statistical word 
alignment with both limited labeled data and 
large amounts of unlabeled data. The proposed 
approach modifies the supervised AdaBoost al-
gorithm to a semi-supervised learning algorithm 
by incorporating the unlabeled data. Therefore, it 
should address the following three problems. The 
first is to build a word alignment model with 
both labeled and unlabeled data. In this paper, 
with the labeled data, we build a supervised 
model by directly estimating the parameters in 
913
the model instead of using the Expectation 
Maximization (EM) algorithm in Brown et al 
(1993). With the unlabeled data, we build an un-
supervised model by estimating the parameters 
with the EM algorithm. Based on these two word 
alignment models, an interpolated model is built 
through linear interpolation. This interpolated 
model is used as a learner in the semi-supervised 
AdaBoost algorithm. The second is to build a 
reference set for the unlabeled data. It is auto-
matically built with a modified "refined" combi-
nation method as described in Och and Ney 
(2000). The third is to calculate the error rate on 
each round. Although we build a reference set 
for the unlabeled data, it still contains alignment 
errors. Thus, we use the reference set of the la-
beled data instead of that of the entire training 
data to calculate the error rate on each round.  
With the interpolated model as a learner in the 
semi-supervised AdaBoost algorithm, we inves-
tigate two boosting methods in this paper to im-
prove statistical word alignment. The first 
method uses the unlabeled data only in the inter-
polated model. During training, it only changes 
the distribution of the labeled data. The second 
method changes the distribution of both the la-
beled data and the unlabeled data during training. 
Experimental results show that both of these two 
methods improve the performance of statistical 
word alignment. 
In addition, we combine the final results of the 
above two semi-supervised boosting methods. 
Experimental results indicate that this combina-
tion outperforms the unsupervised boosting 
method as described in Wu and Wang (2005), 
achieving a relative error rate reduction of 
19.52%. And it also achieves a reduction of 
28.29% as compared with the supervised boost-
ing method that only uses the labeled data. 
The remainder of this paper is organized as 
follows. Section 2 briefly introduces the statisti-
cal word alignment model. Section 3 describes 
parameter estimation method using the labeled 
data. Section 4 presents our semi-supervised 
boosting method. Section 5 reports the experi-
mental results. Finally, we conclude in section 6. 
2 Statistical Word Alignment Model 
According to the IBM models (Brown et al, 
1993), the statistical word alignment model can 
be generally represented as in equation (1).  
?=
a'
e|f,a'
e|fa,
e|fa,
)Pr(
)Pr(
)Pr(  
(1)
Where  and f  represent the source sentence 
and the target sentence, respectively. 
e
In this paper, we use a simplified IBM model 
4 (Al-Onaizan et al, 1999), which is shown in 
equation (2). This simplified version does not 
take into account word classes as described in 
Brown et al (1993). 
))))(()](([                
))()](([(                 
)|( )|(                 
   
 )Pr(
0,1
1
0,1
1
11
1
2
0
0
0 00
?
?
??
?=
>
?=
==
?
???
+??=
??
????
?
???
? ?=
m
aj
j
m
aj
j
m
j
aj
l
i
ii
m
j
j
ja
j
jpjdahj
cjdahj
eften
pp
m
?
??
?
?
?
e|fa,
(2)
ml,  are the lengths of the source sentence and 
the target sentence respectively. 
j  is the position index of the target word. 
ja  is the position of the source word aligned to 
the  target word. thj
i?  is the number of target words that  is 
aligned to. 
ie
0p ,  are the fertility probabilities for , and 1p 0e
110 =+ pp . 
)|
jaj
et(f  is the word translation probability. 
)|( ii en ?  is the fertility probability. 
)(1
ja
cjd ??  is the distortion probability for the 
head word of cept1 i. 
))((1 jpjd ?>  is the distortion probability for the 
non-head words of cept i. 
}:{min)( k
k
aikih ==  is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
. 
i?  is the first word before  with non-zero  ie
fertility.  
ic  is the center of cept i. 
3 Parameter Estimation with Labeled 
Data 
With the labeled data, instead of using EM algo-
rithm, we directly estimate the three main pa-
rameters in model 4: translation probability, fer-
tility probability, and distortion probability. 
                                                 
1 A cept is defined as the set of target words connected to a source word 
(Brown et al, 1993).  
914
3.1 Translation Probability Where 1),( =yx?  if yx = . Otherwise, 0),( =yx? .  
The translation probability is estimated from the 
labeled data as described in (3). 4 Boosting with Labeled Data and 
Unlabeled Data 
?=
'
)',(
),(
)|(
f
i
ji
ij
fecount
fecount
eft  
(3) In this section, we first propose a semi-
supervised AdaBoost algorithm for word align-
ment, which uses both the labeled data and the 
unlabeled data. Based on the semi-supervised 
algorithm, we describe two boosting methods for 
word alignment. And then we develop a method 
to combine the results of the two boosting meth-
ods. 
Where  is the occurring frequency of 
 aligned to  in the labeled data. 
),( ji fecount
ie jf
3.2 Fertility Probability 
The fertility probability )|( ii en ?  describes the 
distribution of the numbers of words that  is 
aligned to. It is estimated as described in (4).  
ie 4.1 Semi-Supervised AdaBoost Algorithm 
for Word Alignment 
?=
'
),'(
),(
)|(
?
?
??
i
ii
ii ecount
ecount
en  
(4)
Figure 1 shows the semi-supervised AdaBoost 
algorithm for word alignment by using labeled 
and unlabeled data. Compared with the super-
vised Adaboost algorithm, this semi-supervised 
AdaBoost algorithm mainly has five differences.  
Where ),( ii ecount ? describes the occurring fre-
quency of word  aligned to ie i?  target words in 
the labeled data.  Word Alignment Model  
0p  and   describe the fertility probabilities 
for .  And  and  sum to 1. We estimate 
 directly from the labeled data, which is 
shown in (5). 
1p
0e 0p 1p
0p
The first is the word alignment model, which 
is taken as a learner in the boosting algorithm. 
The word alignment model is built using both the 
labeled data and the unlabeled data. With the 
labeled data, we train a supervised model by di-
rectly estimating the parameters in the IBM 
model as described in section 3. With the unla-
beled data, we train an unsupervised model using 
the same EM algorithm in Brown et al (1993). 
Then we build an interpolation model by linearly 
interpolating these two word alignment models, 
which is shown in (8). This interpolated model is 
used as the model  described in figure 1. lM
 
Aligned
NullAligned
p
#
##
0
?=  (5)
Where  is the occurring frequency of 
the target words that have counterparts in the 
source language. is the occurring fre-
quency of the target words that have no counter-
parts in the source language. 
Aligned#
Null#
3.3 Distortion Probability 
)(Pr)1()(Pr
)Pr(
US e|fa,e|fa,
e|fa,
??+?= ??  (8)There are two kinds of distortion probability in 
model 4: one for head words and the other for 
non-head words. Both of the distortion probabili-
ties describe the distribution of relative positions 
Thus, if we let 
i
cjj ??=? 1  and )(1 jpjj ?=? > , 
the distortion probabilities for head words and 
non-head words are estimated in (6) and (7) with 
the labeled data, respectively. 
Where  and  are the 
trained supervised model and unsupervised 
model, respectively. 
)(PrS e|fa, )(PrU e|fa,
?  is an interpolation weight. 
We train the weight in equation (8) in the same 
way as described in Wu et al (2005).  
Pseudo Reference Set for Unlabeled Data 
??
?
?
??
??
=?
'
1 '
'
'
,
''
1
,
1
11
),(
),(
)(
j cj
cj
i
i
i
i
cjj
cjj
jd
?
?
?
?
?
?
 (6)
? ?
?
>?
>
>
>> ??
??
=?
'
1
'' )(,
'''
1
)(,
1
11
))(,(
))(,(
)(
j jpj
jpj
jpjj
jpjj
jd ?
?
 (7)
The second is the reference set for the unla-
beled data. For the unlabeled data, we automati-
cally build a pseudo reference set. In order to 
build a reliable pseudo reference set, we perform 
bi-directional word alignment on the training 
data using the interpolated model trained on the 
first round. Bi-directional word alignment in-
cludes alignment in two directions (source to 
915
Input: A training set  including m  bilingual sentence pairs;  TS
The reference set  for the training data; TR
The reference sets  and  ( ) for the labeled data  and the unlabeled 
data  respectively, where 
LR UR TUL , RRR ? LS
US LUT SSS ?=  and NULLLU =? SS ; 
A loop count L. 
(1) Initialize the weights: 
mimiw ,...,1,/1)(1 ==  
(2) For , execute steps (3) to (9).  L l to1=
(3) For each sentence pair i, normalize the 
weights on the training set: 
? ==
j
lll mijwiwip ,...,1),(/)()(  
(4) Update the word alignment model  
based on the weighted training data. 
lM
(5) Perform word alignment on the training set 
with the alignment model :  lM
)( lll pMh =  
(6) Calculate the error of  with the reference 
set : 
lh
LR ? ?=
i
ll iip )()( ??  
Where )(i?  is calculated as in equation (9). 
(7) If 2/1>l? , then let , and end the 
training process. 
1?= lL
(8) Let )1/( lll ??? ?= . 
(9) For all i, compute new weights: 
nknkiwiw lll /))(()()(1 ???+?=+  
where, n represents n alignment links in 
the ith sentence pair. k represents the num-
ber of error links as compared with . TR
Output: The final word alignment result for a source word e : 
?
=
??==
L
l
ll
lff
fehfeWTfeRSeh
1
F )),((),()
1
(logmaxarg),(maxarg)( ??  
Where 1),( =yx?  if yx = . Otherwise, 0),( =yx? .  is the weight of the alignment link 
 produced by the model , which is calculated as described in equation (10). 
),( feWTl
),( fe lM
Figure 1. The Semi-Supervised Adaboost Algorithm for Word Alignment 
target and target to source) as described in Och 
and Ney (2000). Thus, we get two sets of align-
ment results  and  on the unlabeled data. 
Based on these two sets, we use a modified "re-
fined" method (Och and Ney, 2000) to construct 
a pseudo reference set .  
1A 2A
UR
(1) The intersection  is added to the 
reference set . 
21 AAI ?=
UR
(2) We add  to  if a) is satis-
fied or both b) and c) are satisfied.  
21)  ,( AAfe ?? UR
a) Neither  nor  has an alignment in  
and  is greater than a threshold 
e f UR
)|( efp 1? . 
?=
'
)',(
),(
)|(
f
fecount
fecount
efp  
Where  is the occurring fre-
quency of the alignment link  in 
the bi-directional word alignment results. 
),( fecount
)  ,( fe
b)  has a horizontal or a vertical 
neighbor that is already in . 
)  ,( fe
UR
c) The set does not contain 
alignments with both horizontal and ver-
tical neighbors. 
),(U feR ?
 Error of Word Aligner 
The third is the calculation of the error of the 
individual word aligner on each round. For word 
alignment, a sentence pair is taken as a sample. 
Thus, we calculate the error rate of each sentence 
pair as described in (9), which is the same as de-
scribed in Wu and Wang (2005).  
 
||||
||2
1)(
RW
RW
SS
SS
i +
??=?  (9)
Where  represents the set of alignment 
links of a sentence pair i identified by the indi-
vidual interpolated model on each round.  is 
the reference alignment set for the sentence pair. 
WS
RS
With the error rate of each sentence pair, we 
calculate the error of the word aligner on each 
round. Although we build a pseudo reference set 
 for the unlabeled data, it contains alignment 
errors. Thus, the weighted sum of the error rates 
of sentence pairs in the labeled data instead of 
that in the entire training data is used as the error 
of the word aligner. 
UR
 
916
 Weights Update for Sentence Pairs  
The forth is the weight update for sentence 
pairs according to the error and the reference set. 
In a sentence pair, there are usually several word 
alignment links. Some are correct, and others 
may be incorrect. Thus, we update the weights 
according to the number of correct and incorrect 
alignment links as compared with the reference 
set, which is shown in step (9) in figure 1.  
 Weights for Word Alignment Links  
The fifth is the weights used when we con-
struct the final ensemble. Besides the weight 
)/1log( l? , which is the confidence measure of 
the  word aligner, we also use the weight 
 to measure the confidence of each 
alignment link produced by the model . The 
weight  is calculated as shown in (10). 
Wu and Wang (2005) proved that adding this 
weight improved the word alignment results. 
thl
),( feWTl
lM
),( feWTl
?? +
?=
''
),'()',(
),(2
),(
ef
l fecountfecount
fecount
feWT
(10) 
Where  is the occurring frequency 
of the alignment link  in the word align-
ment results of the training data produced by the 
model . 
),( fecount
)  ,( fe
lM
4.2 Method 1 
This method only uses the labeled data as train-
ing data. According to the algorithm in figure 1, 
we obtain  and . Thus, we only 
change the distribution of the labeled data. How-
ever, we build an unsupervised model using the 
unlabeled data. On each round, we keep this un-
supervised model unchanged, and we rebuild the 
supervised model by estimating the parameters 
as described in section 3 with the weighted train-
ing data. Then we interpolate the supervised 
model and the unsupervised model to obtain an 
interpolated model as described in section 4.1. 
The interpolated model is used as the alignment 
model  in figure 1. Thus, in this interpolated 
model, we use both the labeled and unlabeled 
data. On each round, we rebuild the interpolated 
model using the rebuilt supervised model and the 
unchanged unsupervised model. This interpo-
lated model is used to align the training data.  
LT SS = LT RR =
lM
According to the reference set of the labeled 
data, we calculate the error of the word aligner 
on each round. According to the error and the 
reference set, we update the weight of each sam-
ple in the labeled data. 
4.3 Method 2 
This method uses both the labeled data and the 
unlabeled data as training data. Thus, we set 
ULT SSS ?=  and ULT RRR ?=  as described in 
figure 1. With the labeled data, we build a super-
vised model, which is kept unchanged on each 
round.2 With the weighted samples in the train-
ing data, we rebuild the unsupervised model with 
EM algorithm on each round. Based on these two 
models, we built an interpolated model as de-
scribed in section 4.1. The interpolated model is 
used as the alignment model  in figure 1. On 
each round, we rebuild the interpolated model 
using the unchanged supervised model and the 
rebuilt unsupervised model. Then the interpo-
lated model is used to align the training data. 
lM
Since the training data includes both labeled 
and unlabeled data, we need to build a pseudo 
reference set  for the unlabeled data using the 
method described in section 4.1.  According to 
the reference set  of the labeled data, we cal-
culate the error of the word aligner on each 
round. Then, according to the pseudo reference 
set  and the reference set , we update the 
weight of each sentence pair in the unlabeled 
data and in the labeled data, respectively.  
UR
LR
UR LR
There are four main differences between 
Method 2 and Method 1.  
(1) On each round, Method 2 changes the distri-
bution of both the labeled data and the unla-
beled data, while Method 1 only changes the 
distribution of the labeled data. 
(2) Method 2 rebuilds the unsupervised model, 
while Method 1 rebuilds the supervised 
model.  
(3) Method 2 uses the labeled data instead of the 
entire training data to estimate the error of 
the word aligner on each round. 
(4) Method 2 uses an automatically built pseudo 
reference set to update the weights for the 
sentence pairs in the unlabeled data. 
4.4 Combination 
In the above two sections, we described two 
semi-supervised boosting methods for word 
alignment. Although we use interpolated models 
                                                 
2 In fact, we can also rebuild the supervised model accord-
ing to the weighted labeled data. In this case, as we know, 
the error of the supervised model increases. Thus, we keep 
the supervised model unchanged in this method. 
917
for word alignment in both Method 1 and 
Method 2, the interpolated models are trained 
with different weighted data. Thus, they perform 
differently on word alignment. In order to further 
improve the word alignment results, we combine 
the results of the above two methods as described 
in (11). 
  )),(),((maxarg
)(
2211
F3,
feRSfeRS
eh
f
?+?= ??
ods to calculate the precision, recall, f-measure, 
and alignment error rate (AER) are shown in 
equations (12), (13), (14), and (15). It can be 
seen that the higher the f-measure is, the lower 
the alignment error rate is.  
|S|
|SS|
G
CG ?=precision      (12)
|S|
 |SS|
C
CG ?=recall  (11) (13)
||||
||2
CG
CG
SS
SS
fmeasure +
??=  Where  is the combined hypothesis for 
word alignment.  and  are the 
two ensemble results as shown in figure 1 for 
Method 1 and Method 2, respectively. 
)(F3, eh
),(1 feRS ),(2 feRS
1?  and 2?  
are the constant weights. 
(14)
fmeasure
SS
SS
AER ?=+
???= 1
||||
||2
1
CG
CG  (15)
5.3 Experimental Results 
5 Experiments With the data in section 5.1, we get the word 
alignment results shown in table 2. For all of the 
methods in this table, we perform bi-directional 
(source to target and target to source) word 
alignment, and obtain two alignment results on 
the testing set. Based on the two results, we get 
the "refined" combination as described in Och 
and Ney (2000). Thus, the results in table 2 are 
those of the "refined" combination. For EM 
training, we use the GIZA++ toolkit4. 
In this paper, we take English to Chinese word 
alignment as a case study. 
5.1 Data 
We have two kinds of training data from general 
domain: Labeled Data (LD) and Unlabeled Data 
(UD). The Chinese sentences in the data are 
automatically segmented into words. The statis-
tics for the data is shown in Table 1. The labeled 
data is manually word aligned, including 156,421 
alignment links. 
Data # Sentence Pairs 
# English 
Words 
 Results of Supervised Methods  
Using the labeled data, we use two methods to 
estimate the parameters in IBM model 4: one is 
to use the EM algorithm, and the other is to esti-
mate the parameters directly from the labeled 
data as described in section 3.  In table 2, the 
method "Labeled+EM" estimates the parameters 
with the EM algorithm, which is an unsupervised 
method without boosting. And the method "La-
beled+Direct" estimates the parameters directly 
from the labeled data, which is a supervised 
method without boosting. "Labeled+EM+Boost" 
and "Labeled+Direct+Boost" represent the two 
supervised boosting methods for the above two 
parameter estimation methods.  
# Chinese 
Words 
LD 31,069 255,504 302,470 
UD 329,350 4,682,103 4,480,034
Table 1. Statistics for Training Data 
We use 1,000 sentence pairs as testing set, 
which are not included in LD or UD. The testing 
set is also manually word aligned, including 
8,634 alignment links in the testing set3.  
5.2 Evaluation Metrics 
We use the same evaluation metrics as described 
in Wu et al (2005), which is similar to those in 
(Och and Ney, 2000). The difference lies in that 
Wu et al (2005) take all alignment links as sure 
links. 
Our methods that directly estimate parameters 
in IBM model 4 are better than that using the EM 
algorithm.  "Labeled+Direct" is better than "La-
beled+EM", achieving a relative error rate reduc-
tion of 22.97%. And "Labeled+Direct+Boost" is 
better than "Labeled+EM+Boost", achieving a 
relative error rate reduction of 22.98%. In addi-
tion, the two boosting methods perform better 
than their corresponding methods without
 If we use  to represent the set of alignment 
links identified by the proposed method and  
to denote the reference alignment set, the meth-
GS
CS
                                                 
3 For a non one-to-one link, if m source words are aligned to 
n target words, we take it as one alignment link instead of 
m?n alignment links. 
                                                 
4 It is located at http://www.fjoch.com/ GIZA++.html. 
918
Method Precision Recall F-Measure AER 
Labeled+EM 0.6588 0.5210 0.5819 0.4181 
Labeled+Direct 0.7269 0.6609 0.6924 0.3076 
Labeled+EM+Boost 0.7384 0.5651 0.6402 0.3598 
Labeled+Direct+Boost 0.7771 0.6757 0.7229 0.2771 
Unlabeled+EM 0.7485 0.6667 0.7052 0.2948 
Unlabeled+EM+Boost 0.8056 0.7070 0.7531 0.2469 
Interpolated 0.7555 0.7084 0.7312 0.2688 
Method 1 0.7986 0.7197 0.7571 0.2429 
Method 2 0.8060 0.7388 0.7709 0.2291 
Combination 0.8175 0.7858 0.8013 0.1987 
Table 2. Word Alignment Results 
boosting. For example, "Labeled+Direct+Boost" 
achieves an error rate reduction of 9.92% as 
compared with "Labeled+Direct". 
Results of Unsupervised Methods   
With the unlabeled data, we use the EM algo-
rithm to estimate the parameters in the model. 
The method "Unlabeled+EM" represents an un-
supervised method without boosting. And the 
method "Unlabeled+EM+Boost" uses the same 
unsupervised Adaboost algorithm as described in 
Wu and Wang (2005). 
The boosting method "Unlabeled+EM+Boost" 
achieves a relative error rate reduction of 16.25% 
as compared with "Unlabeled+EM". In addition, 
the unsupervised boosting method "Unla-
beled+EM+Boost" performs better than the su-
pervised boosting method "Labeled+Direct+ 
Boost", achieving an error rate reduction of 
10.90%. This is because the size of labeled data 
is too small to subject to data sparseness problem.  
Results of Semi-Supervised Methods    
By using both the labeled and the unlabeled 
data, we interpolate the models trained by "La-
beled+Direct" and "Unlabeled+EM" to get an 
interpolated model. Here, we use "interpolated" 
to represent it. "Method 1" and  "Method 2" rep-
resent the semi-supervised boosting methods de-
scribed in section 4.2 and section 4.3, respec-
tively. "Combination" denotes the method de-
scribed in section 4.4, which combines "Method 
1" and "Method 2".  Both of the weights 1?  and 
2?  in equation (11) are set to 0.5. 
 "Interpolated" performs better than the meth-
ods using only labeled data or unlabeled data. It 
achieves relative error rate reductions of 12.61% 
and 8.82% as compared with "Labeled+Direct" 
and "Unlabeled+EM", respectively. 
Using an interpolation model, the two semi-
supervised boosting methods "Method 1" and 
"Method 2" outperform the supervised boosting 
method "Labeled+Direct+Boost", achieving a 
relative error rate reduction of 12.34% and 
17.32% respectively. In addition, the two semi-
supervised boosting methods perform better than 
the unsupervised boosting method "Unlabeled+ 
EM+Boost". "Method 1" performs slightly better 
than "Unlabeled+EM+Boost". This is because 
we only change the distribution of the labeled 
data in "Method 1". "Method 2" achieves an er-
ror rate reduction of 7.77% as compared with 
"Unlabeled+EM+Boost". This is because we use 
the interpolated model in our semi-supervised 
boosting method, while "Unlabeled+EM+Boost" 
only uses the unsupervised model. 
Moreover, the combination of the two semi-
supervised boosting methods further improves 
the results, achieving relative error rate reduc-
tions of 18.20% and 13.27% as compared with 
"Method 1" and "Method 2", respectively. It also 
outperforms both the supervised boosting 
method "Labeled+Direct+Boost" and the unsu-
pervised boosting method "Unlabeled+EM+ 
Boost", achieving relative error rate reductions of 
28.29% and 19.52% respectively.  
Summary of the Results    
From the above result, it can be seen that all 
boosting methods perform better than their corre-
sponding methods without boosting. The semi-
supervised boosting methods outperform the su-
pervised boosting method and the unsupervised 
boosting method. 
6 Conclusion and Future Work 
This paper proposed a semi-supervised boosting 
algorithm to improve statistical word alignment 
with limited labeled data and large amounts of 
unlabeled data. In this algorithm, we built an in-
terpolated model by using both the labeled data 
919
and the unlabeled data. This interpolated model 
was employed as a learner in the algorithm. Then, 
we automatically built a pseudo reference for the 
unlabeled data, and calculated the error rate of 
each word aligner with the labeled data.  Based 
on this algorithm, we investigated two methods 
for word alignment. In addition, we developed a 
method to combine the results of the above two 
semi-supervised boosting methods. 
Experimental results indicate that our semi-
supervised boosting method outperforms the un-
supervised boosting method as described in Wu 
and Wang (2005), achieving a relative error rate 
reduction of 19.52%. And it also outperforms the 
supervised boosting method that only uses the 
labeled data, achieving a relative error rate re-
duction of 28.29%. Experimental results also 
show that all boosting methods outperform their 
corresponding methods without boosting. 
In the future, we will evaluate our method 
with an available standard testing set. And we 
will also evaluate the word alignment results in a 
machine translation system, to examine whether 
lower word alignment error rate will result in 
higher translation accuracy. 
References 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
Sugato Basu, Mikhail Bilenko, and Raymond J. 
Mooney.  2004. Probabilistic Framework for Semi-
Supervised Clustering. In Proc. of the 10th ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), pages 
59-68.  
Avrim Blum and Tom Mitchell. 1998. Combing La-
beled and Unlabeled Data with Co-training. In 
Proc. of the 11th Conference on Computational 
Learning Theory (COLT-1998), pages1-10.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95. 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In 
Proc. of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and 
Very Large Corpora (EMNLP/VLC-1999), pages 
100-110. 
Thomas G. Dietterich. 2000. Ensemble Methods in 
Machine Learning. In Proc. of the First Interna-
tional Workshop on Multiple Classifier Systems 
(MCS-2000), pages 1-15. 
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a New Boosting Algorithm. In Proc. of 
the 13th International Conference on Machine 
Learning (ICML-1996), pages 148-156. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Thanh Phong Pham, Hwee Tou Ng, and Wee Sun Lee 
2005. Word Sense Disambiguation with Semi-
Supervised Learning. In Proc. of the 20th National 
Conference on Artificial Intelligence (AAAI 2005), 
pages 1093-1098. 
Anoop Sarkar. 2001. Applying Co-Training Methods 
to Statistical Parsing. In Proc. of the 2nd Meeting of 
the North American Association for Computational 
Linguistics( NAACL-2001), pages 175-182. 
Joachims Thorsten. 1999. Transductive Inference for 
Text Classification Using Support Vector Ma-
chines. In Proc. of the 16th International Confer-
ence on Machine Learning (ICML-1999), pages 
200-209. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403. 
Hua Wu and Haifeng Wang. 2005. Boosting Statisti-
cal Word Alignment. In Proc. of the 10th Machine 
Translation Summit, pages 313-320. 
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. 
Alignment Model Adaptation for Domain-Specific 
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196.  
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for 
Alignment. In Proc. of the 43rd Annual Meeting of 
the Association for Computational Linguistics 
(ACL-2005), pages 475-482. 
920
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 856?863,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Pivot Language Approach for Phrase-Based Statistical Machine 
Translation 
Hua Wu and Haifeng Wang 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wuhua,wanghaifeng}@rdc.toshiba.com.cn 
 
 
Abstract 
This paper proposes a novel method for 
phrase-based statistical machine translation 
by using pivot language. To conduct trans-
lation between languages Lf and Le with a 
small bilingual corpus, we bring in a third 
language Lp, which is named the pivot lan-
guage. For Lf-Lp and Lp-Le, there exist 
large bilingual corpora. Using only Lf-Lp 
and Lp-Le bilingual corpora, we can build a 
translation model for Lf-Le. The advantage 
of this method lies in that we can perform 
translation between Lf and Le even if there 
is no bilingual corpus available for this 
language pair. Using BLEU as a metric, 
our pivot language method achieves an ab-
solute improvement of 0.06 (22.13% rela-
tive) as compared with the model directly 
trained with 5,000 Lf-Le sentence pairs for 
French-Spanish translation. Moreover, with 
a small Lf-Le bilingual corpus available, 
our method can further improve the transla-
tion quality by using the additional Lf-Lp 
and Lp-Le bilingual corpora. 
1 Introduction 
For statistical machine translation (SMT), phrase-
based methods (Koehn et al, 2003; Och and Ney, 
2004) and syntax-based methods (Wu, 1997; Al-
shawi et al 2000; Yamada and Knignt, 2001; 
Melamed, 2004; Chiang, 2005; Quick et al, 2005; 
Mellebeek et al, 2006) outperform word-based 
methods (Brown et al, 1993). These methods need 
large bilingual corpora. However, for some lan-
guages pairs, only a small bilingual corpus is 
available, which will degrade the performance of 
statistical translation systems. 
To solve this problem, this paper proposes a 
novel method for phrase-based SMT by using a 
pivot language. To perform translation between 
languages Lf and Le, we bring in a pivot language 
Lp, for which there exist large bilingual corpora for 
language pairs Lf-Lp and Lp-Le. With the Lf-Lp and 
Lp-Le bilingual corpora, we can build a translation 
model for Lf-Le by using Lp as the pivot language. 
We name the translation model pivot model. The 
advantage of this method lies in that we can con-
duct translation between Lf and Le even if there is 
no bilingual corpus available for this language pair. 
Moreover, if a small corpus is available for Lf-Le, 
we build another translation model, which is 
named standard model. Then, we build an interpo-
lated model by performing linear interpolation on 
the standard model and the pivot model. Thus, the 
interpolated model can employ both the small Lf-
Le corpus and the large Lf-Lp and Lp-Le corpora. 
We perform experiments on the Europarl corpus 
(Koehn, 2005). Using BLEU (Papineni et al, 2002) 
as a metric, our method achieves an absolute im-
provement of 0.06 (22.13% relative) as compared 
with the standard model trained with 5,000 Lf-Le 
sentence pairs for French-Spanish translation. The 
translation quality is comparable with that of the 
model trained with a bilingual corpus of 30,000 Lf-
Le sentence pairs. Moreover, translation quality is 
further boosted by using both the small Lf-Le bilin-
gual corpus and the large Lf-Lp and Lp-Le corpora. 
Experimental results on Chinese-Japanese trans-
lation also indicate that our method achieves satis-
factory results using English as the pivot language.  
856
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
Section 3 briefly introduces phrase-based SMT. 
Section 4 and Section 5 describes our method for 
phrase-based SMT using pivot language. We de-
scribe the experimental results in sections 6 and 7. 
Lastly, we conclude in section 8. 
2 Related Work 
Our method is mainly related to two kinds of 
methods: those using pivot language and those 
using a small bilingual corpus or scarce resources.  
For the first kind, pivot languages are employed 
to translate queries in cross-language information 
retrieval (CLIR) (Gollins and Sanderson, 2001; 
Kishida and Kando, 2003). These methods only 
used the available dictionaries to perform word by 
word translation. In addition, NTCIR 4 workshop 
organized a shared task for CLIR using pivot lan-
guage. Machine translation systems are used to 
translate queries into pivot language sentences, and 
then into target sentences (Sakai et al, 2004). 
Callison-Burch et al (2006) used pivot lan-
guages for paraphrase extraction to handle the un-
seen phrases for phrase-based SMT. Borin (2000) 
and Wang et al (2006) used pivot languages to 
improve word alignment. Borin (2000) used multi-
lingual corpora to increase alignment coverage. 
Wang et al (2006) induced alignment models by 
using two additional bilingual corpora to improve 
word alignment quality. Pivot Language methods 
were also used for translation dictionary induction 
(Schafer and Yarowsky, 2002), word sense disam-
biguation (Diab and Resnik, 2002), and so on. 
For the second kind, Niessen and Ney (2004) 
used morpho-syntactic information for translation 
between language pairs with scarce resources. 
Vandeghinste et al (2006) used translation dic-
tionaries and shallow analysis tools for translation 
between the language pair with low resources. A 
shared task on word alignment was organized as 
part of the ACL 2005 Workshop on Building and 
Using Parallel Texts (Martin et al, 2005). This 
task focused on languages with scarce resources. 
For the subtask of unlimited resources, some re-
searchers (Aswani and Gaizauskas, 2005; Lopez 
and Resnik, 2005; Tufis et al, 2005) used lan-
guage-dependent resources such as dictionary, the-
saurus, and dependency parser to improve word 
alignment results. 
In this paper, we address the translation problem 
for language pairs with scarce resources by bring-
ing in a pivot language, via which we can make 
use of large bilingual corpora. Our method does 
not need language-dependent resources or deep 
linguistic processing. Thus, the method is easy to 
be adapted to any language pair where a pivot lan-
guage and corresponding large bilingual corpora 
are available. 
3 Phrase-Based SMT 
According to the translation model presented in 
(Koehn et al, 2003), given a source sentence f , 
the best target translation beste  can be obtained 
according to the following model 
)()()|(maxarg
)|(maxarg
e
e
e
eef
fee
length
LM
best
?pp
p
=
=
 (1)
Where the translation model )|( efp can be 
decomposed into  
?
=
??=
I
i
iiiiii
II
aefpbadef
efp
1
1
11
),|()()|(
)|(
?? w
(2)
Where )|( ii ef?  and )( 1?? ii bad  denote phrase 
translation probability and distortion probability, 
respectively. ),|( aefp iiw  is the lexical weight, 
and ?  is the strength of the lexical weight. 
4 Phrase-Based SMT Via Pivot Language 
This section will introduce the method that per-
forms phrase-based SMT for the language pair Lf-
Le by using the two bilingual corpora of Lf-Lp and 
Lp-Le. With the two additional bilingual corpora, 
we train two translation models for Lf-Lp and Lp-Le, 
respectively. Based on these two models, we build 
a pivot translation model for Lf-Le, with Lp as a 
pivot language. 
According to equation (2), the phrase translation 
probability and the lexical weight are language 
dependent. We will introduce them in sections 4.1 
and 4.2, respectively. 
4.1 Phrase Translation Probability 
Using the Lf-Lp and Lp-Le bilingual corpora, we 
train two phrase translation probabilities 
857
)|( ii pf?  and )|( ii ep? , where ip  is the phrase 
in the pivot language Lp. Given the phrase 
translation probabilities )|( ii pf?  and )|( ii ep? , 
we obtain the phrase translation probability 
)|( ii ef?  according to the following model. 
?=
ip
iiiiiii epepfef )|(),|()|( ???  (3)
The phrase translation probability ),|( iii epf?  
does not depend on the phase ie  in the language Le, 
since it is estimated from the Lf-Lp bilingual corpus. 
Thus, equation (3) can be rewritten as  
?=
ip
iiiiii eppfef )|()|()|( ???  (4)
4.2 Lexical Weight 
Given a phrase pair ),( ef  and a word alignment 
a  between the source word positions ni ,...,1=  
and the target word positions mj ,...,1= , the 
lexical weight can be estimated according to the 
following method (Koehn et al, 2003). 
? ?
= ???=
n
i aji
ji efwajij
aefp
1 ),(
)|(
),(|
1
),|(w
 (5)
In order to estimate the lexical weight, we first 
need to obtain the alignment information a  be-
tween the two phrases f  and e , and then estimate 
the lexical translation probability )|( efw  accord-
ing to the alignment information. The alignment 
information of the phrase pair ),( ef  can be in-
duced from the two phrase pairs ),( pf  and ),( ep . 
 
Figure 1. Alignment Information Induction 
Let 1a  and 2a  represent the word alignment in-
formation inside the phrase pairs ),( pf  and ),( ep  
respectively, then the alignment information a  
inside ),( ef  can be obtained as shown in (6). An 
example is shown in Figure 1. 
}),(&),(:|),{( 21 aepapfpefa ???=  (6)
With the induced alignment information, this 
paper proposes a method to estimate the probabil-
ity directly from the induced phrase pairs. We 
name this method phrase method. If we use K to 
denote the number of the induced phrase pairs, we 
estimate the co-occurring frequency of the word 
pair ),( ef  according to the following model. 
??
==
=
n
i
ai
K
k
k i
eeffef
efcount
11
),(),()|(
),(
???  (7)
Where )|( efk?  is the phrase translation probabil-
ity for phrase pair k . 1),( =yx?  if yx = ; other-
wise, 0),( =yx? . Thus, lexical translation prob-
ability can be estimated as in (8). 
?=
'
),'(
),(
)|(
f
efcount
efcount
efw  (8)
We also estimate the lexical translation prob-
ability )|( efw  using the method described in 
(Wang et al, 2006), which is shown in (9). We 
named it word method in this paper. 
);,()|()|()|( pefsimepwpfwefw
p
?= (9)
Where )|( pfw  and )|( epw  are two lexical 
probabilities, and );,( pefsim  is the cross-
language word similarity. 
5 Interpolated Model 
If we have a small Lf-Le bilingual corpus, we can 
employ this corpus to estimate a translation model 
as described in section 3. However, this model may 
perform poorly due to the sparseness of the data. In 
order to improve its performance, we can employ 
the additional Lf-Lp and Lp-Le bilingual corpora. 
Moreover, we can use more than one pivot lan-
guage to improve the translation performance if the 
corresponding bilingual corpora exist. Different 
pivot languages may catch different linguistic phe-
858
nomena, and improve translation quality for the 
desired language pair Lf-Le in different ways. 
If we include n  pivot languages, n  pivot mod-
els can be estimated using the method as described 
in section 4. In order to combine these n  pivot 
models with the standard model trained with the 
Lf-Le corpus, we use the linear interpolation 
method. The phrase translation probability and the 
lexical weight are estimated as shown in (10) and 
(11), respectively. 
?
=
=
n
i
ii efef
0
)|()|( ???  (10)
?
=
=
n
i
ii aefpaefp
0
),|(),|( w,w ?  (11)
Where )|(0 ef?  and ),|( aefpw,0  denote the 
phrase translation probability and lexical weight 
trained with the Lf-Le bilingual corpus, respec-
tively. )|( efi?  and ),|( aefp iw,  ( ni ,...,1= ) are 
the phrase translation probability and lexical 
weight estimated by using the pivot languages. i?  
and i?  are the interpolation coefficients. 
6 Experiments on the Europarl Corpus 
6.1 Data 
A shared task to evaluate machine translation per-
formance was organized as part of the 
NAACL/HLT 2006 Workshop on Statistical Ma-
chine Translation (Koehn and Monz, 2006). The 
shared task used the Europarl corpus (Koehn, 
2005), in which four languages are involved: Eng-
lish, French, Spanish, and German. The shared task 
performed translation between English and the 
other three languages. In our work, we perform 
translation from French to the other three lan-
guages. We select French to Spanish and French to 
German translation that are not in the shared task 
because we want to use English as the pivot lan-
guage. In general, for most of the languages, there 
exist bilingual corpora between these languages 
and English since English is an internationally 
used language. 
Table 1 shows the information about the bilin-
gual training data. In the table, "Fr", "En", "Es", 
and "De" denotes "French", "English", "Spanish", 
and "German", respectively. For the language pairs 
Lf-Le not including English, the bilingual corpus is 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words 
Target 
Words 
Fr-En 688,031 15,323,737 13,808,104
Fr-Es 640,661 14,148,926 13,134,411
Fr-De 639,693 14,215,058 12,155,876
Es-En 730,740 15,676,710 15,222,105
De-En 751,088 15,256,793 16,052,269
De-Es 672,813 13,246,255 14,362,615
Table 1. Training Corpus for European Languages 
extracted from Lf-English and English-Le since 
Europarl corpus is a multilingual corpus.  
For the language models, we use the same data 
provided in the shared task. We also use the same 
development set and test set provided by the shared 
task. The in-domain test set includes 2,000 sen-
tences and the out-of-domain test set includes 
1,064 sentences for each language. 
6.2 Translation System and Evaluation 
Method 
To perform phrase-based SMT, we use Koehn's 
training scripts1 and the Pharaoh decoder (Koehn, 
2004). We run the decoder with its default settings 
and then use Koehn's implementation of minimum 
error rate training (Och, 2003) to tune the feature 
weights on the development set. 
The translation quality was evaluated using a 
well-established automatic measure: BLEU score 
(Papineni et al, 2002). And we also use the tool 
provided in the NAACL/HLT 2006 shared task on 
SMT to calculate the BLEU scores. 
6.3 Comparison of Different Lexical Weights 
As described in section 4, we employ two methods 
to estimate the lexical weight in the translation 
model. In order to compare the two methods, we 
translate from French to Spanish, using English as 
the pivot language. We use the French-English and 
English-Spanish corpora described in Table 1 as 
training data.  During training, before estimating 
the Spanish to French phrase translation probabil-
ity, we filter those French-English and English-
Spanish phrase pairs whose translation probabili-
ties are below a fixed threshold 0.001.2 The trans-
lation results are shown in Table 2. 
                                                 
1  It is located at http://www.statmt.org/wmt06/shared-
task/baseline.htm  
2 In the following experiments using pivot languages, we use 
the same filtering threshold for all of the language pairs. 
859
The phrase method proposed in this paper per-
forms better than the word method proposed in 
(Wang et al, 2006). This is because our method 
uses phrase translation probability as a confidence 
weight to estimate the lexical translation probabil-
ity. It strengthens the frequently aligned pairs and 
weakens the infrequently aligned pairs. Thus, the 
following sections will use the phrase method to 
estimate the lexical weight. 
Method In-Domain Out-of-Domain
Phrase  0.3212 0.2098 
Word 0.2583 0.1672 
Table 2. Results with Different Lexical Weights 
6.4 Results of Using One Pivot Language 
This section describes the translation results by 
using only one pivot language. For the language 
pair French and Spanish, we use English as the 
pivot language. The entire French-English and 
English-Spanish corpora as described in section 4 
are used to train a pivot model for French-Spanish. 
As described in section 5, if we have a small Lf-
Le bilingual corpus and large Lf-Lp and Lp-Le bilin-
gual corpora, we can obtain interpolated models. 
In order to conduct the experiments, we ran-
domly select 5K, 10K, 20K, 30K, 40K, 50K, and 
100K sentence pairs from the French-Spanish cor-
pus. Using each of these corpora, we train a stan-
dard translation model.  
For each standard model, we interpolate it with 
the pivot model to get an interpolated model. The 
interpolation weights are tuned using the develop-
ment set. For all the interpolated models, we set 
9.00 =? , 1.01 =? , 9.00 =? , and 1.01 =? . We 
test the three kinds of models on both the in-
domain and out-of-domain test sets. The results are 
shown in Figures 2 and 3.  
The pivot model achieves BLEU scores of 
0.3212 and 0.2098 on the in-domain and out-of-
domain test set, respectively. It achieves an abso-
lute improvement of 0.05 on both test sets (16.92% 
and 35.35% relative) over the standard model 
trained with 5,000 French-Spanish sentence pairs. 
And the performance of the pivot models are com-
parable with that of the standard models trained 
with 20,000 and 30,000 sentence pairs on the in-
domain and out-of-domain test set, respectively. 
When the French-Spanish training corpus is in-
creased, the standard models quickly outperform 
the pivot model. 
25
27
29
31
33
35
37
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 2. In-Domain French-Spanish Results 
14
16
18
20
22
24
26
5 10 20 30 40 50 100
Fr-Es Data (K pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
 
Figure 3. Out-of-Domain French-Spanish Results 
18
20
22
24
26
28
30
5 10 20 30 40 50 100
Fr-En Data (k Pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 4. In-Domain French-English Results 
9
10
11
12
13
14
15
16
17
5 10 20 30 40 50 100
Fr-De Data (k Pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
 
Figure 5. In-Domain French-German Results 
When only a very small French-Spanish bilin-
gual corpus is available, the interpolated method 
can greatly improve the translation quality. For 
example, when only 5,000 French-Spanish sen-
tence pairs are available, the interpolated model 
outperforms the standard model by achieving a 
relative improvement of 17.55%, with the BLEU 
score improved from 0.2747 to 0.3229. With 
50,000 French-Spanish sentence pairs available, 
the interpolated model significantly3 improves the 
translation quality by achieving an absolute im-
                                                 
3 We conduct the significance test using the same method as 
described in (Koehn and Monz, 2006). 
860
provement of 0.01 BLEU. When the French-
Spanish training corpus increases to 100,000 sen-
tence pairs, the interpolated model achieves almost 
the same result as the standard model. This indi-
cates that our pivot language method is suitable for 
the language pairs with small quantities of training 
data available. 
Besides experiments on French-Spanish transla-
tion, we also conduct translation from French to 
English and French to German, using German and 
English as the pivot language, respectively. The 
results on the in-domain test set4 are shown in Fig-
ures 4 and 5. The tendency of the results is similar 
to that in Figure 2. 
6.5 Results of Using More Than One Pivot 
Language 
For French to Spanish translation, we also intro-
duce German as a pivot language besides English. 
Using these two pivot languages, we build two dif-
ferent pivot models, and then perform linear inter-
polation on them. The interpolation weights for the 
English pivot model and the German pivot model 
are set to 0.6 and 0.4 respectively5. The translation 
results on the in-domain test set are 0.3212, 0.3077, 
and 0.3355 for the pivot models using English, 
German, and both German and English as pivot 
languages, respectively. 
With the pivot model using both English and 
German as pivot languages, we interpolate it with 
the standard models trained with French-Spanish 
corpora of different sizes as described in the above 
section. The comparison of the translation results 
among the interpolated models, standard models, 
and the pivot model are shown in Figure 6. 
It can be seen that the translation results can be 
further improved by using more than one pivot 
language. The pivot model "Pivot-En+De" using 
two pivot languages achieves an absolute im-
provement of 0.06 (22.13% relative) as compared 
with the standard model trained with 5,000 sen-
tence pairs. And it achieves comparable translation 
result as compared with the standard model trained 
with 30,000 French-Spanish sentence pairs. 
The results in Figure 6 also indicate the interpo-
lated models using two pivot languages achieve the 
                                                 
4 The results on the out-of-domain test set are similar to that in 
Figure 3. We only show the in-domain translation results in all 
of the following experiments because of space limit. 
5 The weights are tuned on the development set. 
best results of all. Significance test shows that the 
interpolated models using two pivot languages sig-
nificantly outperform those using one pivot lan-
guage when less than 50,000 French-Spanish sen-
tence pairs are available. 
27
28
29
30
31
32
33
34
35
36
37
5 10 20 30 40 50 100
Fr-Es Data (k Pairs)
B
L
E
U
 (%
)
Interpolated-En+De
Interpolated-En
Interpolated-De
Standard
Pivot-En+De
 
Figure 6. In-Domain French-Spanish Translation 
Results by Using Two Pivot Languages 
6.6 Results by Using Pivot Language Related 
Corpora of Different Sizes 
In all of the above results, the corpora used to train 
the pivot models are not changed. In order to ex-
amine the effect of the size of the pivot corpora, 
we decrease the French-English and English-
French corpora. We randomly select 200,000 and 
400,000 sentence pairs from both of them to train 
two pivot models, respectively. The translation 
results on the in-domain test set are 0.2376, 0.2954, 
and 0.3212 for the pivot models trained with 
200,000, 400,000, and the entire French-English 
and English-Spanish corpora, respectively. The 
results of the interpolated models and the standard 
models are shown in Figure 7. The results indicate 
that the larger the training corpora used to train the 
pivot model are, the better the translation quality is. 
27
28
29
30
31
32
33
34
35
36
37
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
B
L
E
U
 (%
)
Interpolated-All
interpolated-400k
Interpolated-200k
Standard
 
Figure 7. In-Domain French-Spanish Results by 
Using Lf-Lp and Lp-Le Corpora of Different Sizes 
861
7 Experiments on Chinese to Japanese 
Translation 
In section 6, translation results on the Europarl 
multilingual corpus indicate the effectiveness of 
our method. To investigate the effectiveness of our 
method by using independently sourced parallel 
corpora, we conduct Chinese-Japanese translation 
using English as a pivot language in this section, 
where the training data are not limited to a specific 
domain. 
The data used for this experiment is the same as 
those used in (Wang et al, 2006). There are 21,977, 
329,350, and 160,535 sentence pairs for the lan-
guage pairs Chinese-Japanese, Chinese-English, 
and English-Japanese, respectively. The develop-
ment data and testing data include 500 and 1,000 
Chinese sentences respectively, with one reference 
for each sentence. For Japanese language model 
training, we use about 100M bytes Japanese corpus. 
The translation result is shown in Figure 8. The 
pivot model only outperforms the standard model 
trained with 2,500 sentence pairs. This is because 
(1) the corpora used to train the pivot model are 
smaller as compared with the Europarl corpus; (2) 
the training data and the testing data are not limited 
to a specific domain; (3) The languages are not 
closely related. 
6
8
10
12
14
16
18
2.5 5 10 21.9
Chinese-Japanese Data (k pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 8. Chinese-Japanese Translation Results 
The interpolated models significantly outper-
form the other models. When only 5,000 sentence 
pairs are available, the BLEU score increases rela-
tively by 20.53%. With the entire (21,977 pairs) 
Chinese-Japanese available, the interpolated model 
relatively increases the BLEU score by 5.62%, 
from 0.1708 to 0.1804. 
8 Conclusion 
This paper proposed a novel method for phrase-
based SMT on language pairs with a small bilin-
gual corpus by bringing in pivot languages. To per-
form translation between Lf and Le, we bring in a 
pivot language Lp, via which the large corpora of 
Lf-Lp and Lp-Le can be used to induce a translation 
model for Lf-Le. The advantage of this method is 
that it can perform translation between the lan-
guage pair Lf-Le even if no bilingual corpus for this 
pair is available. Using BLEU as a metric, our 
method achieves an absolute improvement of 0.06 
(22.13% relative) as compared with the model di-
rectly trained with 5,000 sentence pairs for French-
Spanish translation. And the translation quality is 
comparable with that of the model directly trained 
with 30,000 French-Spanish sentence pairs. The 
results also indicate that using more pivot lan-
guages leads to better translation quality. 
With a small bilingual corpus available for Lf-Le, 
we built a translation model, and interpolated it 
with the pivot model trained with the large Lf-Lp 
and Lp-Le bilingual corpora. The results on both 
the Europarl corpus and Chinese-Japanese transla-
tion indicate that the interpolated models achieve 
the best results. Results also indicate that our pivot 
language approach is suitable for translation on 
language pairs with a small bilingual corpus. The 
less the Lf-Le bilingual corpus is, the bigger the 
improvement is. 
We also performed experiments using Lf-Lp and 
Lp-Le corpora of different sizes. The results indi-
cate that using larger training corpora to train the 
pivot model leads to better translation quality. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning Dependency Translation Models as 
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1):45-60. 
Niraj Aswani and Robert Gaizauskas. 2005. Aligning 
Words in English-Hindi Parallel Corpora. In Proc. of 
the ACL 2005 Workshop on Building and Using Par-
allel Texts: Data-driven Machine Translation and 
Beyond, pages 115-118. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2): 
263-311. 
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
862
tion Using Paraphrases. In Proc. of NAACL-2006, 
pages 17-24. 
Lars Borin. 2000. You'll Take the High Road and I'll 
Take the Low Road: Using a Third Language to Im-
prove Bilingual Word Alignment. In Proc. of COL-
ING-2000, pages 97-103. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of ACL-2005, pages 263-270. 
Mona Diab  and  Philip Resnik. 2002. An Unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. In Proc. of ACL-2002, pages 255-262. 
Tim Gollins and Mark Sanderson. 2001. Improving 
Cross Language Information Retrieval with Triangu-
lated Translation. In Proc. of ACM SIGIR-2001, 
pages 90-95. 
Kazuaki Kishida and Noriko Kando.  2003. Two-Stage 
Refinement of Query Translation in a Pivot Lan-
guage Approach to Cross-Lingual Information Re-
trieval: An Experiment at CLEF 2003. In Proc. of 
CLEF-2003. pages 253-262. 
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder 
for Phrase-Based Statistical Machine Translation 
Models. In Proc. of AMTA-2004, pages 115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proc. of MT 
Summit X, pages 79-86. 
Philipp Koehn and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proc. of the 2006 
HLT-NAACL Workshop on Statistical Machine 
Translation, pages 102-121. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Proc. 
of HLT-NAAC- 2003, pages 127-133. 
Adam Lopez and Philip Resnik. 2005. Improved HMM 
Alignment Models for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Work-shop on 
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 83-86. 
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on 
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 65-74. 
Dan Melamed. 2004. Statistical Machine Translation by 
Parsing. In Proc. of ACL-2004, pages 653-660. 
Bart Mellebeek, Karolina Owczarzak, Declan Groves, 
Josef Van Genabith, and Andy Way. 2006. A Syntac-
tic Skeleton for Statistical Machine Translation. In 
Proc. of EAMT-2006, pages 195-202. 
Sonja Niessen and Hermann Ney. 2004. Statistical 
Machine Translation with Scarce Resources Using 
Morpho-Syntactic Information. Computational 
linguistics, 30(2): 181-204. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL-
2003, pages 160-167. 
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine 
Translation. Computational Linguistics, 30(4):417-
449. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proc. of ACL-
2002, pages 311-318. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. of ACL-2005, pages 
271-279. 
Tetsuya Sakai, Makoto Koyama, Akira Kumano, and 
Toshihiko Manabe. 2004. Toshiba BRIDJE at 
NTCIR-4 CLIR: Monolingual/Bilingual IR and 
Flexible Feedback. In Proc. of NTCIR 4. 
Charles Schafer and David Yarowsky. 2002. Inducing 
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of CoNLL-2002, 
pages 1-7. 
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word 
Alignment for Languages with Scarce Resources Us-
ing Bilingual Corpora of Other Language Pairs. In 
Proc. of COLING/ACL-2006 Main Conference 
Poster Sessions, pages 874-881. 
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Ste-
fanescu. 2005. Combined Word Alignments. In Proc. 
of the ACL-2005 Workshop on Building and Using 
Parallel Texts: Data-driven Machine Translation and 
Beyond, pages 107-110. 
Vincent Vandeghinste, Ineka Schuurman, Michael Carl, 
Stella Markantonatou, and Toni Badia. 2006. 
METIS-II: Machine Translation for Low-Resource 
Languages. In Proc. of LREC-2006, pages 1284-1289. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Kenji Yamada and Kevin Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proc. of ACL-2001, 
pages 523-530. 
863
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46?54,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Heterogeneous Treebanks for Parsing
Zheng-Yu Niu, Haifeng Wang, Hua Wu
Toshiba (China) Research and Development Center
5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
{niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn
Abstract
We address the issue of using heteroge-
neous treebanks for parsing by breaking
it down into two sub-problems, convert-
ing grammar formalisms of the treebanks
to the same one, and parsing on these
homogeneous treebanks. First we pro-
pose to employ an iteratively trained tar-
get grammar parser to perform grammar
formalism conversion, eliminating prede-
fined heuristic rules as required in previ-
ous methods. Then we provide two strate-
gies to refine conversion results, and adopt
a corpus weighting technique for parsing
on homogeneous treebanks. Results on the
Penn Treebank show that our conversion
method achieves 42% error reduction over
the previous best result. Evaluation on
the Penn Chinese Treebank indicates that a
converted dependency treebank helps con-
stituency parsing and the use of unlabeled
data by self-training further increases pars-
ing f-score to 85.2%, resulting in 6% error
reduction over the previous best result.
1 Introduction
The last few decades have seen the emergence of
multiple treebanks annotated with different gram-
mar formalisms, motivated by the diversity of lan-
guages and linguistic theories, which is crucial to
the success of statistical parsing (Abeille et al,
2000; Brants et al, 1999; Bohmova et al, 2003;
Han et al, 2002; Kurohashi and Nagao, 1998;
Marcus et al, 1993; Moreno et al, 2003; Xue et
al., 2005). Availability of multiple treebanks cre-
ates a scenario where we have a treebank anno-
tated with one grammar formalism, and another
treebank annotated with another grammar formal-
ism that we are interested in. We call the first
a source treebank, and the second a target tree-
bank. We thus encounter a problem of how to
use these heterogeneous treebanks for target gram-
mar parsing. Here heterogeneous treebanks refer
to two or more treebanks with different grammar
formalisms, e.g., one treebank annotated with de-
pendency structure (DS) and the other annotated
with phrase structure (PS).
It is important to acquire additional labeled data
for the target grammar parsing through exploita-
tion of existing source treebanks since there is of-
ten a shortage of labeled data. However, to our
knowledge, there is no previous study on this is-
sue.
Recently there have been some works on us-
ing multiple treebanks for domain adaptation of
parsers, where these treebanks have the same
grammar formalism (McClosky et al, 2006b;
Roark and Bacchiani, 2003). Other related works
focus on converting one grammar formalism of a
treebank to another and then conducting studies on
the converted treebank (Collins et al, 1999; Forst,
2003; Wang et al, 1994; Watkinson and Manand-
har, 2001). These works were done either on mul-
tiple treebanks with the same grammar formalism
or on only one converted treebank. We see that
their scenarios are different from ours as we work
with multiple heterogeneous treebanks.
For the use of heterogeneous treebanks1, we
propose a two-step solution: (1) converting the
grammar formalism of the source treebank to the
target one, (2) refining converted trees and using
them as additional training data to build a target
grammar parser.
For grammar formalism conversion, we choose
the DS to PS direction for the convenience of the
comparison with existing works (Xia and Palmer,
2001; Xia et al, 2008). Specifically, we assume
that the source grammar formalism is dependency
1Here we assume the existence of two treebanks.
46
grammar, and the target grammar formalism is
phrase structure grammar.
Previous methods for DS to PS conversion
(Collins et al, 1999; Covington, 1994; Xia and
Palmer, 2001; Xia et al, 2008) often rely on pre-
defined heuristic rules to eliminate converison am-
biguity, e.g., minimal projection for dependents,
lowest attachment position for dependents, and the
selection of conversion rules that add fewer num-
ber of nodes to the converted tree. In addition, the
validity of these heuristic rules often depends on
their target grammars. To eliminate the heuristic
rules as required in previous methods, we propose
to use an existing target grammar parser (trained
on the target treebank) to generate N-best parses
for each sentence in the source treebank as conver-
sion candidates, and then select the parse consis-
tent with the structure of the source tree as the con-
verted tree. Furthermore, we attempt to use con-
verted trees as additional training data to retrain
the parser for better conversion candidates. The
procedure of tree conversion and parser retraining
will be run iteratively until a stopping condition is
satisfied.
Since some converted trees might be imper-
fect from the perspective of the target grammar,
we provide two strategies to refine conversion re-
sults: (1) pruning low-quality trees from the con-
verted treebank, (2) interpolating the scores from
the source grammar and the target grammar to se-
lect better converted trees. Finally we adopt a cor-
pus weighting technique to get an optimal combi-
nation of the converted treebank and the existing
target treebank for parser training.
We have evaluated our conversion algorithm on
a dependency structure treebank (produced from
the Penn Treebank) for comparison with previous
work (Xia et al, 2008). We also have investi-
gated our two-step solution on two existing tree-
banks, the Penn Chinese Treebank (CTB) (Xue et
al., 2005) and the Chinese Dependency Treebank
(CDT)2 (Liu et al, 2006). Evaluation on WSJ data
demonstrates that it is feasible to use a parser for
grammar formalism conversion and the conversion
benefits from converted trees used for parser re-
training. Our conversion method achieves 93.8%
f-score on dependency trees produced from WSJ
section 22, resulting in 42% error reduction over
the previous best result for DS to PS conversion.
Results on CTB show that score interpolation is
2Available at http://ir.hit.edu.cn/.
more effective than instance pruning for the use
of converted treebanks for parsing and converted
CDT helps parsing on CTB. When coupled with
self-training technique, a reranking parser with
CTB and converted CDT as labeled data achieves
85.2% f-score on CTB test set, an absolute 1.0%
improvement (6% error reduction) over the previ-
ous best result for Chinese parsing.
The rest of this paper is organized as follows. In
Section 2, we first describe a parser based method
for DS to PS conversion, and then we discuss pos-
sible strategies to refine conversion results, and
finally we adopt the corpus weighting technique
for parsing on homogeneous treebanks. Section
3 provides experimental results of grammar for-
malism conversion on a dependency treebank pro-
duced from the Penn Treebank. In Section 4, we
evaluate our two-step solution on two existing het-
erogeneous Chinese treebanks. Section 5 reviews
related work and Section 6 concludes this work.
2 Our Two-Step Solution
2.1 Grammar Formalism Conversion
Previous DS to PS conversion methods built a
converted tree by iteratively attaching nodes and
edges to the tree with the help of conversion
rules and heuristic rules, based on current head-
dependent pair from a source dependency tree and
the structure of the built tree (Collins et al, 1999;
Covington, 1994; Xia and Palmer, 2001; Xia et
al., 2008). Some observations can be made on
these methods: (1) for each head-dependent pair,
only one locally optimal conversion was kept dur-
ing tree-building process, at the risk of pruning
globally optimal conversions, (2) heuristic rules
are required to deal with the problem that one
head-dependent pair might have multiple conver-
sion candidates, and these heuristic rules are usu-
ally hand-crafted to reflect the structural prefer-
ence in their target grammars. To overcome these
limitations, we propose to employ a parser to gen-
erate N-best parses as conversion candidates and
then use the structural information of source trees
to select the best parse as a converted tree.
We formulate our conversion method as fol-
lows.
Let CDS be a source treebank annotated with
DS and CPS be a target treebank annotated with
PS. Our goal is to convert the grammar formalism
of CDS to that of CPS .
We first train a constituency parser on CPS
47
Input: CPS , CDS , Q, and a constituency parser Output: Converted trees CDSPS
1. Initialize:
? Set CDS,0PS as null, DevScore=0, q=0;
? Split CPS into training set CPS,train and development set CPS,dev;
? Train the parser on CPS,train and denote it by Pq?1;
2. Repeat:
? Use Pq?1 to generate N-best PS parses for each sentence in CDS , and convert PS to DS for each parse;
? For each sentence in CDS Do
? t?=argmaxtScore(xi,t), and select the t?-th parse as a converted tree for this sentence;
? Let CDS,qPS represent these converted trees, and let Ctrain=CPS,train
?CDS,qPS ;
? Train the parser on Ctrain, and denote the updated parser by Pq;
? Let DevScoreq be the f-score of Pq on CPS,dev;
? If DevScoreq > DevScore Then DevScore=DevScoreq, and CDSPS =CDS,qPS ;
? Else break;
? q++;
Until q > Q
Table 1: Our algorithm for DS to PS conversion.
(90% trees in CPS as training set CPS,train, and
other trees as development set CPS,dev) and then
let the parser generate N-best parses for each sen-
tence in CDS .
Let n be the number of sentences (or trees) in
CDS and ni be the number of N-best parses gen-
erated by the parser for the i-th (1 ? i ? n) sen-
tence in CDS . Let xi,t be the t-th (1 ? t ? ni)
parse for the i-th sentence. Let yi be the tree of the
i-th (1 ? i ? n) sentence in CDS .
To evaluate the quality of xi,t as a conversion
candidate for yi, we convert xi,t to a dependency
tree (denoted as xDSi,t ) and then use unlabeled de-
pendency f-score to measure the similarity be-
tween xDSi,t and yi. Let Score(xi,t) denote the
unlabeled dependency f-score of xDSi,t against yi.
Then we determine the converted tree for yi by
maximizing Score(xi,t) over the N-best parses.
The conversion from PS to DS works as fol-
lows:
Step 1. Use a head percolation table to find the
head of each constituent in xi,t.
Step 2. Make the head of each non-head child
depend on the head of the head child for each con-
stituent.
Unlabeled dependency f-score is a harmonic
mean of unlabeled dependency precision and unla-
beled dependency recall. Precision measures how
many head-dependent word pairs found in xDSi,t
are correct and recall is the percentage of head-
dependent word pairs defined in the gold-standard
tree that are found in xDSi,t . Here we do not take
dependency tags into consideration for evaluation
since they cannot be obtained without more so-
phisticated rules.
To improve the quality of N-best parses, we at-
tempt to use the converted trees as additional train-
ing data to retrain the parser. The procedure of
tree conversion and parser retraining can be run it-
eratively until a termination condition is satisfied.
Here we use the parser?s f-score on CPS,dev as a
termination criterion. If the update of training data
hurts the performance on CPS,dev, then we stop
the iteration.
Table 1 shows this DS to PS conversion algo-
rithm. Q is an upper limit of the number of loops,
and Q ? 0.
2.2 Target Grammar Parsing
Through grammar formalism conversion, we have
successfully turned the problem of using hetero-
geneous treebanks for parsing into the problem of
parsing on homogeneous treebanks. Before using
converted source treebank for parsing, we present
two strategies to refine conversion results.
Instance Pruning For some sentences in
CDS , the parser might fail to generate high qual-
ity N-best parses, resulting in inferior converted
trees. To clean the converted treebank, we can re-
move the converted trees with low unlabeled de-
pendency f-scores (defined in Section 2.1) before
using the converted treebank for parser training
48
Figure 1: A parse tree in CTB for a sentence of
/?.<world> ?<every> I<country> <
?<people> ?<all> r<with> 81<eyes>
? ?<cast> ? l<Hong Kong>0with
/People from all over the world are cast-
ing their eyes on Hong Kong0as its English
translation.
because these trees are/misleading0training in-
stances. The number of removed trees will be de-
termined by cross validation on development set.
Score Interpolation Unlabeled dependency
f-scores used in Section 2.1 measure the quality of
converted trees from the perspective of the source
grammar only. In extreme cases, the top best
parses in the N-best list are good conversion can-
didates but we might select a parse ranked quite
low in the N-best list since there might be con-
flicts of syntactic structure definition between the
source grammar and the target grammar.
Figure 1 shows an example for illustration of
a conflict between the grammar of CDT and
that of CTB. According to Chinese head percola-
tion tables used in the PS to DS conversion tool
/Penn2Malt03 and Charniak?s parser4, the head
of VP-2 is the word /r0(a preposition, with
/BA0as its POS tag in CTB), and the head of
IP-OBJ is ??0. Therefore the word /?
?0depends on the word/r0. But according
to the annotation scheme in CDT (Liu et al, 2006),
the word/r0is a dependent of the word/?
?0. The conflicts between the two grammars
may lead to the problem that the selected parses
based on the information of the source grammar
might not be preferred from the perspective of the
3Available at http://w3.msi.vxu.se/?nivre/.
4Available at http://www.cs.brown.edu/?ec/.
target grammar.
Therefore we modified the selection metric in
Section 2.1 by interpolating two scores, the prob-
ability of a conversion candidate from the parser
and its unlabeled dependency f-score, shown as
follows:
S?core(xi,t) = ??Prob(xi,t)+(1??)?Score(xi,t). (1)
The intuition behind this equation is that converted
trees should be preferred from the perspective of
both the source grammar and the target grammar.
Here 0 ? ? ? 1. Prob(xi,t) is a probability pro-
duced by the parser for xi,t (0 ? Prob(xi,t) ? 1).
The value of ? will be tuned by cross validation on
development set.
After grammar formalism conversion, the prob-
lem now we face has been limited to how to build
parsing models on multiple homogeneous tree-
bank. A possible solution is to simply concate-
nate the two treebanks as training data. However
this method may lead to a problem that if the size
of CPS is significantly less than that of converted
CDS , converted CDS may weaken the effect CPS
might have. One possible solution is to reduce the
weight of examples from converted CDS in parser
training. Corpus weighting is exactly such an ap-
proach, with the weight tuned on development set,
that will be used for parsing on homogeneous tree-
banks in this paper.
3 Experiments of Grammar Formalism
Conversion
3.1 Evaluation on WSJ section 22
Xia et al (2008) used WSJ section 19 from the
Penn Treebank to extract DS to PS conversion
rules and then produced dependency trees from
WSJ section 22 for evaluation of their DS to PS
conversion algorithm. They showed that their
conversion algorithm outperformed existing meth-
ods on the WSJ data. For comparison with their
work, we conducted experiments in the same set-
ting as theirs: using WSJ section 19 (1844 sen-
tences) as CPS , producing dependency trees from
WSJ section 22 (1700 sentences) as CDS5, and
using labeled bracketing f-scores from the tool
/EVALB0on WSJ section 22 for performance
evaluation.
5We used the tool/Penn2Malt0to produce dependency
structures from the Penn Treebank, which was also used for
PS to DS conversion in our conversion algorithm.
49
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
The best result of
Xia et al (2008) - 90.7 88.1 89.4
Q-0-method 86.8 92.2 92.8 92.5
Q-10-method 88.0 93.4 94.1 93.8
Table 2: Comparison with the work of Xia et al
(2008) on WSJ section 22.
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
Q-0-method 91.0 91.6 92.5 92.1
Q-10-method 91.6 93.1 94.1 93.6
Table 3: Results of our algorithm on WSJ section
2?18 and 20?22.
We employed Charniak?s maximum entropy in-
spired parser (Charniak, 2000) to generate N-best
(N=200) parses. Xia et al (2008) used POS
tag information, dependency structures and depen-
dency tags in test set for conversion. Similarly, we
used POS tag information in the test set to restrict
search space of the parser for generation of better
N-best parses.
We evaluated two variants of our DS to PS con-
version algorithm:
Q-0-method: We set the value of Q as 0 for a
baseline method.
Q-10-method: We set the value of Q as 10 to
see whether it is helpful for conversion to retrain
the parser on converted trees.
Table 2 shows the results of our conversion al-
gorithm on WSJ section 22. In the experiment
of Q-10-method, DevScore reached the highest
value of 88.0% when q was 1. Then we used
CDS,1PS as the conversion result. Finally Q-10-
method achieved an f-score of 93.8% on WSJ sec-
tion 22, an absolute 4.4% improvement (42% er-
ror reduction) over the best result of Xia et al
(2008). Moreover, Q-10-method outperformed Q-
0-method on the same test set. These results indi-
cate that it is feasible to use a parser for DS to PS
conversion and the conversion benefits from the
use of converted trees for parser retraining.
3.2 Evaluation on WSJ section 2?18 and
20?22
In this experiment we evaluated our conversion al-
gorithm on a larger test set, WSJ section 2?18 and
20?22 (totally 39688 sentences). Here we also
used WSJ section 19 as CPS . Other settings for
All the sentences
LR LP F
Training data (%) (%) (%)
1? CTB + CDTPS 84.7 85.1 84.9
2? CTB + CDTPS 85.1 85.6 85.3
5? CTB + CDTPS 85.0 85.5 85.3
10? CTB + CDTPS 85.3 85.8 85.6
20? CTB + CDTPS 85.1 85.3 85.2
50? CTB + CDTPS 84.9 85.3 85.1
Table 4: Results of the generative parser on the de-
velopment set, when trained with various weight-
ing of CTB training set and CDTPS .
this experiment are as same as that in Section 3.1,
except that here we used a larger test set.
Table 3 provides the f-scores of our method with
Q equal to 0 or 10 on WSJ section 2?18 and
20?22.
With Q-10-method, DevScore reached the high-
est value of 91.6% when q was 1. Finally Q-
10-method achieved an f-score of 93.6% on WSJ
section 2?18 and 20?22, better than that of Q-0-
method and comparable with that of Q-10-method
in Section 3.1. It confirms our previous finding
that the conversion benefits from the use of con-
verted trees for parser retraining.
4 Experiments of Parsing
We investigated our two-step solution on two ex-
isting treebanks, CDT and CTB, and we used CDT
as the source treebank and CTB as the target tree-
bank.
CDT consists of 60k Chinese sentences, anno-
tated with POS tag information and dependency
structure information (including 28 POS tags, and
24 dependency tags) (Liu et al, 2006). We did not
use POS tag information as inputs to the parser in
our conversion method due to the difficulty of con-
version from CDT POS tags to CTB POS tags.
We used a standard split of CTB for perfor-
mance evaluation, articles 1-270 and 400-1151 as
training set, articles 301-325 as development set,
and articles 271-300 as test set.
We used Charniak?s maximum entropy inspired
parser and their reranker (Charniak and Johnson,
2005) for target grammar parsing, called a gener-
ative parser (GP) and a reranking parser (RP) re-
spectively. We reported ParseVal measures from
the EVALB tool.
50
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB 79.9 82.2 81.0
RP CTB 82.0 84.6 83.3
GP 10? CTB + CDTPS 80.4 82.7 81.5
RP 10? CTB + CDTPS 82.8 84.7 83.8
Table 5: Results of the generative parser (GP) and
the reranking parser (RP) on the test set, when
trained on only CTB training set or an optimal
combination of CTB training set and CDTPS .
4.1 Results of a Baseline Method to Use CDT
We used our conversion algorithm6 to convert the
grammar formalism of CDT to that of CTB. Let
CDTPS denote the converted CDT by our method.
The average unlabeled dependency f-score of trees
in CDTPS was 74.4%, and their average index in
200-best list was 48.
We tried the corpus weighting method when
combining CDTPS with CTB training set (abbre-
viated as CTB for simplicity) as training data, by
gradually increasing the weight (including 1, 2, 5,
10, 20, 50) of CTB to optimize parsing perfor-
mance on the development set. Table 4 presents
the results of the generative parser with various
weights of CTB on the development set. Consid-
ering the performance on the development set, we
decided to give CTB a relative weight of 10.
Finally we evaluated two parsing models, the
generative parser and the reranking parser, on the
test set, with results shown in Table 5. When
trained on CTB only, the generative parser and the
reranking parser achieved f-scores of 81.0% and
83.3%. The use of CDTPS as additional training
data increased f-scores of the two models to 81.5%
and 83.8%.
4.2 Results of Two Strategies for a Better Use
of CDT
4.2.1 Instance Pruning
We used unlabeled dependency f-score of each
converted tree as the criterion to rank trees in
CDTPS and then kept only the top M trees
with high f-scores as training data for pars-
ing, resulting in a corpus CDTPSM . M var-
ied from 100%?|CDTPS | to 10%?|CDTPS |
with 10%?|CDTPS | as the interval. |CDTPS |
6The setting for our conversion algorithm in this experi-
ment was as same as that in Section 3.1. In addition, we used
CTB training set as CPS,train, and CTB development set as
CPS,dev .
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB + CDTPS? 81.4 82.8 82.1
RP CTB + CDTPS? 83.0 85.4 84.2
Table 6: Results of the generative parser and the
reranking parser on the test set, when trained on
an optimal combination of CTB training set and
converted CDT.
is the number of trees in CDTPS . Then
we tuned the value of M by optimizing the
parser?s performance on the development set with
10?CTB+CDTPSM as training data. Finally the op-
timal value of M was 100%?|CDT|. It indicates
that even removing very few converted trees hurts
the parsing performance. A possible reason is that
most of non-perfect parses can provide useful syn-
tactic structure information for building parsing
models.
4.2.2 Score Interpolation
We used ?Score(xi,t)7 to replace Score(xi,t) in
our conversion algorithm and then ran the updated
algorithm on CDT. Let CDTPS? denote the con-
verted CDT by this updated conversion algorithm.
The values of ? (varying from 0.0 to 1.0 with 0.1
as the interval) and the CTB weight (including 1,
2, 5, 10, 20, 50) were simultaneously tuned on the
development set8. Finally we decided that the op-
timal value of ? was 0.4 and the optimal weight of
CTB was 1, which brought the best performance
on the development set (an f-score of 86.1%). In
comparison with the results in Section 4.1, the
average index of converted trees in 200-best list
increased to 2, and their average unlabeled depen-
dency f-score dropped to 65.4%. It indicates that
structures of converted trees become more consis-
tent with the target grammar, as indicated by the
increase of average index of converted trees, fur-
ther away from the source grammar.
Table 6 provides f-scores of the generative
parser and the reranker on the test set, when
trained on CTB and CDTPS? . We see that the
performance of the reranking parser increased to
7Before calculating S?core(xi,t), we normal-
ized the values of Prob(xi,t) for each N-best list
by (1) Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,?)),
(2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,?)), resulting
in that their maximum value was 1 and their minimum value
was 0.
8Due to space constraint, we do not show f-scores of the
parser with different values of ? and the CTB weight.
51
All the sentences
LR LP F
Models Training data (%) (%) (%)
Self-trained GP 10?T+10?D+P 83.0 84.5 83.7
Updated RP CTB+CDTPS? 84.3 86.1 85.2
Table 7: Results of the self-trained gen-
erative parser and updated reranking parser
on the test set. 10?T+10?D+P stands for
10?CTB+10?CDTPS? +PDC.
84.2% f-score, better than the result of the rerank-
ing parser with CTB and CDTPS as training data
(shown in Table 5). It indicates that the use of
probability information from the parser for tree
conversion helps target grammar parsing.
4.3 Using Unlabeled Data for Parsing
Recent studies on parsing indicate that the use of
unlabeled data by self-training can help parsing
on the WSJ data, even when labeled data is rel-
atively large (McClosky et al, 2006a; Reichart
and Rappoport, 2007). It motivates us to em-
ploy self-training technique for Chinese parsing.
We used the POS tagged People Daily corpus9
(Jan. 1998?Jun. 1998, and Jan. 2000?Dec.
2000) (PDC) as unlabeled data for parsing. First
we removed the sentences with less than 3 words
or more than 40 words from PDC to ease pars-
ing, resulting in 820k sentences. Then we ran the
reranking parser in Section 4.2.2 on PDC and used
the parses on PDC as additional training data for
the generative parser. Here we tried the corpus
weighting technique for an optimal combination
of CTB, CDTPS? and parsed PDC, and chose the
relative weight of both CTB and CDTPS? as 10
by cross validation on the development set. Fi-
nally we retrained the generative parser on CTB,
CDTPS? and parsed PDC. Furthermore, we used
this self-trained generative parser as a base parser
to retrain the reranker on CTB and CDTPS? .
Table 7 shows the performance of self-trained
generative parser and updated reranker on the test
set, with CTB and CDTPS? as labeled data. We see
that the use of unlabeled data by self-training fur-
ther increased the reranking parser?s performance
from 84.2% to 85.2%. Our results on Chinese data
confirm previous findings on English data shown
in (McClosky et al, 2006a; Reichart and Rap-
poport, 2007).
9Available at http://icl.pku.edu.cn/.
4.4 Comparison with Previous Studies for
Chinese Parsing
Table 8 and 9 present the results of previous stud-
ies on CTB. All the works in Table 8 used CTB
articles 1-270 as labeled data. In Table 9, Petrov
and Klein (2007) trained their model on CTB ar-
ticles 1-270 and 400-1151, and Burkett and Klein
(2008) used the same CTB articles and parse trees
of their English translation (from the English Chi-
nese Translation Treebank) as training data. Com-
paring our result in Table 6 with that of Petrov
and Klein (2007), we see that CDTPS? helps pars-
ing on CTB, which brought 0.9% f-score improve-
ment. Moreover, the use of unlabeled data further
boosted the parsing performance to 85.2%, an ab-
solute 1.0% improvement over the previous best
result presented in Burkett and Klein (2008).
5 Related Work
Recently there have been some studies address-
ing how to use treebanks with same grammar for-
malism for domain adaptation of parsers. Roark
and Bachiani (2003) presented count merging and
model interpolation techniques for domain adap-
tation of parsers. They showed that their sys-
tem with count merging achieved a higher perfor-
mance when in-domain data was weighted more
heavily than out-of-domain data. McClosky et al
(2006b) used self-training and corpus weighting to
adapt their parser trained on WSJ corpus to Brown
corpus. Their results indicated that both unla-
beled in-domain data and labeled out-of-domain
data can help domain adaptation. In comparison
with these works, we conduct our study in a dif-
ferent setting where we work with multiple het-
erogeneous treebanks.
Grammar formalism conversion makes it possi-
ble to reuse existing source treebanks for the study
of target grammar parsing. Wang et al (1994)
employed a parser to help conversion of a tree-
bank from a simple phrase structure to a more in-
formative phrase structure and then used this con-
verted treebank to train their parser. Collins et al
(1999) performed statistical constituency parsing
of Czech on a treebank that was converted from
the Prague Dependency Treebank under the guid-
ance of conversion rules and heuristic rules, e.g.,
one level of projection for any category, minimal
projection for any dependents, and fixed position
of attachment. Xia and Palmer (2001) adopted bet-
ter heuristic rules to build converted trees, which
52
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Bikel & Chiang (2000) 76.8 77.8 77.3 - - -
Chiang & Bikel (2002) 78.8 81.1 79.9 - - -
Levy & Manning (2003) 79.2 78.4 78.8 - - -
Bikel?s thesis (2004) 78.0 81.2 79.6 - - -
Xiong et. al. (2005) 78.7 80.1 79.4 - - -
Chen et. al. (2005) 81.0 81.7 81.2 76.3 79.2 77.7
Wang et. al. (2006) 79.2 81.1 80.1 76.2 78.0 77.1
Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Petrov & Klein (2007) 85.7 86.9 86.3 81.9 84.8 83.3
Burkett & Klein (2008) - - - - - 84.2
Table 9: Results of previous studies on CTB with more labeled data.
reflected the structural preference in their target
grammar. For acquisition of better conversion
rules, Xia et al (2008) proposed to automati-
cally extract conversion rules from a target tree-
bank. Moreover, they presented two strategies to
solve the problem that there might be multiple
conversion rules matching the same input depen-
dency tree pattern: (1) choosing the most frequent
rules, (2) preferring rules that add fewer number
of nodes and attach the subtree lower.
In comparison with the works of Wang et al
(1994) and Collins et al (1999), we went fur-
ther by combining the converted treebank with the
existing target treebank for parsing. In compar-
ison with previous conversion methods (Collins
et al, 1999; Covington, 1994; Xia and Palmer,
2001; Xia et al, 2008) in which for each head-
dependent pair, only one locally optimal conver-
sion was kept during tree-building process, we
employed a parser to generate globally optimal
syntactic structures, eliminating heuristic rules for
conversion. In addition, we used converted trees to
retrain the parser for better conversion candidates,
while Wang et al (1994) did not exploit the use of
converted trees for parser retraining.
6 Conclusion
We have proposed a two-step solution to deal with
the issue of using heterogeneous treebanks for
parsing. First we present a parser based method
to convert grammar formalisms of the treebanks to
the same one, without applying predefined heuris-
tic rules, thus turning the original problem into the
problem of parsing on homogeneous treebanks.
Then we present two strategies, instance pruning
and score interpolation, to refine conversion re-
sults. Finally we adopt the corpus weighting tech-
nique to combine the converted source treebank
with the existing target treebank for parser train-
ing.
The study on the WSJ data shows the benefits of
our parser based approach for grammar formalism
conversion. Moreover, experimental results on the
Penn Chinese Treebank indicate that a converted
dependency treebank helps constituency parsing,
and it is better to exploit probability information
produced by the parser through score interpolation
than to prune low quality trees for the use of the
converted treebank.
Future work includes further investigation of
our conversion method for other pairs of grammar
formalisms, e.g., from the grammar formalism of
the Penn Treebank to more deep linguistic formal-
ism like CCG, HPSG, or LFG.
References
Anne Abeille, Lionel Clement and Francois Toussenel. 2000.
Building a Treebank for French. In Proceedings of LREC
2000, pages 87-94.
Daniel Bikel and David Chiang. 2000. Two Statistical Pars-
ing Models Applied to the Chinese Treebank. In Proceed-
ings of the Second SIGHAN workshop, pages 1-6.
Daniel Bikel. 2004. On the Parameter Space of Generative
Lexicalized Statistical Parsing Models. Ph.D. thesis, Uni-
versity of Pennsylvania.
Alena Bohmova, Jan Hajic, Eva Hajicova and Barbora
Vidova-Hladka. 2003. The Prague Dependency Tree-
bank: A Three-Level Annotation Scenario. Treebanks:
53
Building and Using Annotated Corpora. Kluwer Aca-
demic Publishers, pages 103-127.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit. 1999.
Syntactic Annotation of a German Newspaper Corpus. In
Proceedings of the ATALA Treebank Workshop, pages 69-
76.
David Burkett and Dan Klein. 2008. Two Languages are
Better than One (for Syntactic Parsing). In Proceedings of
EMNLP 2008, pages 877-886.
Eugene Charniak. 2000. A Maximum Entropy Inspired
Parser. In Proceedings of NAACL 2000, pages 132-139.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
N-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL 2005, pages 173-180.
Ying Chen, Hongling Sun and Dan Jurafsky. 2005. A Cor-
rigendum to Sun and Jurafsky (2004) Shallow Semantic
Parsing of Chinese. University of Colorado at Boulder
CSLR Tech Report TR-CSLR-2005-01.
David Chiang and Daniel M. Bikel. 2002. Recovering La-
tent Information in Treebanks. In Proceedings of COL-
ING 2002, pages 1-7.
Micheal Collins, Lance Ramshaw, Jan Hajic and Christoph
Tillmann. 1999. A Statistical Parser for Czech. In Pro-
ceedings of ACL 1999, pages 505-512.
Micheal Covington. 1994. GB Theory as Dependency
Grammar. Research Report AI-1992-03.
Martin Forst. 2003. Treebank Conversion - Establishing
a Testsuite for a Broad-Coverage LFG from the TIGER
Treebank. In Proceedings of LINC at EACL 2003, pages
25-32.
Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer.
2002. Development and Evaluation of a Korean Treebank
and its Application to NLP. In Proceedings of LREC 2002,
pages 1635-1642.
Sadao Kurohashi and Makato Nagao. 1998. Building a
Japanese Parsed Corpus While Improving the Parsing Sys-
tem. In Proceedings of LREC 1998, pages 719-724.
Roger Levy and Christopher Manning. 2003. Is It Harder to
Parse Chinese, or the Chinese Treebank? In Proceedings
of ACL 2003, pages 439-446.
Ting Liu, Jinshan Ma and Sheng Li. 2006. Building a Depen-
dency Treebank for Improving Chinese Parser. Journal of
Chinese Language and Computing, 16(4):207-224.
Mitchell P. Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
David McClosky, Eugene Charniak and Mark Johnson.
2006a. Effective Self-Training for Parsing. In Proceed-
ings of NAACL 2006, pages 152-159.
David McClosky, Eugene Charniak and Mark Johnson.
2006b. Reranking and Self-Training for Parser Adapta-
tion. In Proceedings of COLING/ACL 2006, pages 337-
344.
Antonio Moreno, Susana Lopez, Fernando Sanchez and
Ralph Grishman. 2003. Developing a Syntactic Anno-
tation Scheme and Tools for a Spanish Treebank. Tree-
banks: Building and Using Annotated Corpora. Kluwer
Academic Publishers, pages 149-163.
Slav Petrov and Dan Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of HLT/NAACL
2007, pages 404-411.
Roi Reichart and Ari Rappoport. 2007. Self-Training for En-
hancement and Domain Adaptation of Statistical Parsers
Trained on Small Datasets. In Proceedings of ACL 2007,
pages 616-623.
Brian Roark and Michiel Bacchiani. 2003. Supervised and
Unsupervised PCFG Adaptation to Novel Domains. In
Proceedings of HLT/NAACL 2003, pages 126-133.
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su. 1994.
An Automatic Treebank Conversion Algorithm for Corpus
Sharing. In Proceedings of ACL 1994, pages 248-254.
Mengqiu Wang, Kenji Sagae and Teruko Mitamura. 2006. A
Fast, Accurate Deterministic Parser for Chinese. In Pro-
ceedings of COLING/ACL 2006, pages 425-432.
Stephen Watkinson and Suresh Manandhar. 2001. Translat-
ing Treebank Annotation for Evaluation. In Proceedings
of ACL Workshop on Evaluation Methodologies for Lan-
guage and Dialogue Systems, pages 1-8.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Proceedings of HLT
2001, pages 1-5.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer
and Dipti Misra. Sharma. 2008. Towards a Multi-
Representational Treebank. In Proceedings of the 7th In-
ternational Workshop on Treebanks and Linguistic Theo-
ries, pages 159-170.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with Semantic Knowledge. In Proceedings of IJC-
NLP 2005, pages 70-81.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
54
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 154?162,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Revisiting Pivot Language Approach for Machine Translation
Hua Wu and Haifeng Wang
Toshiba (China) Research and Development Center
5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
{wuhua, wanghaifeng}@rdc.toshiba.com.cn
Abstract
This paper revisits the pivot language ap-
proach for machine translation. First,
we investigate three different methods
for pivot translation. Then we employ
a hybrid method combining RBMT and
SMT systems to fill up the data gap
for pivot translation, where the source-
pivot and pivot-target corpora are inde-
pendent. Experimental results on spo-
ken language translation show that this
hybrid method significantly improves the
translation quality, which outperforms the
method using a source-target corpus of
the same size. In addition, we pro-
pose a system combination approach to
select better translations from those pro-
duced by various pivot translation meth-
ods. This method regards system com-
bination as a translation evaluation prob-
lem and formalizes it with a regression
learning model. Experimental results in-
dicate that our method achieves consistent
and significant improvement over individ-
ual translation outputs.
1 Introduction
Current statistical machine translation (SMT) sys-
tems rely on large parallel and monolingual train-
ing corpora to produce translations of relatively
higher quality. Unfortunately, large quantities of
parallel data are not readily available for some lan-
guages pairs, therefore limiting the potential use
of current SMT systems. In particular, for speech
translation, the translation task often focuses on a
specific domain such as the travel domain. It is es-
pecially difficult to obtain such a domain-specific
corpus for some language pairs such as Chinese to
Spanish translation.
To circumvent the data bottleneck, some re-
searchers have investigated to use a pivot language
approach (Cohn and Lapata, 2007; Utiyama and
Isahara, 2007; Wu and Wang 2007; Bertoldi et al,
2008). This approach introduces a third language,
named the pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. A pivot task was also designed for spoken
language translation in the evaluation campaign of
IWSLT 2008 (Paul, 2008), where English is used
as a pivot language for Chinese to Spanish trans-
lation.
Three different pivot strategies have been in-
vestigated in the literature. The first is based
on phrase table multiplication (Cohn and Lap-
ata 2007; Wu and Wang, 2007). It multiples
corresponding translation probabilities and lexical
weights in source-pivot and pivot-target transla-
tion models to induce a new source-target phrase
table. We name it the triangulation method. The
second is the sentence translation strategy, which
first translates the source sentence to the pivot sen-
tence, and then to the target sentence (Utiyama and
Isahara, 2007; Khalilov et al, 2008). We name it
the transfer method. The third is to use existing
models to build a synthetic source-target corpus,
from which a source-target model can be trained
(Bertoldi et al, 2008). For example, we can ob-
tain a source-pivot corpus by translating the pivot
sentence in the source-pivot corpus into the target
language with pivot-target translation models. We
name it the synthetic method.
The working condition with the pivot language
approach is that the source-pivot and pivot-target
parallel corpora are independent, in the sense that
they are not derived from the same set of sen-
tences, namely independently sourced corpora.
Thus, some linguistic phenomena in the source-
pivot corpus will lost if they do not exist in the
pivot-target corpus, and vice versa. In order to fill
up this data gap, we make use of rule-based ma-
chine translation (RBMT) systems to translate the
pivot sentences in the source-pivot or pivot-target
154
corpus into target or source sentences. As a re-
sult, we can build a synthetic multilingual corpus,
which can be used to improve the translation qual-
ity. The idea of using RBMT systems to improve
the translation quality of SMT sysems has been
explored in Hu et al (2007). Here, we re-examine
the hybrid method to fill up the data gap for pivot
translation.
Although previous studies proposed several
pivot translation methods, there are no studies to
combine different pivot methods for translation
quality improvement. In this paper, we first com-
pare the individual pivot methods and then in-
vestigate to improve pivot translation quality by
combining the outputs produced by different sys-
tems. We propose to regard system combination
as a translation evaluation problem. For transla-
tions from one of the systems, this method uses the
outputs from other translation systems as pseudo
references. A regression learning method is used
to infer a function that maps a feature vector
(which measures the similarity of a translation to
the pseudo references) to a score that indicates the
quality of the translation. Scores are first gener-
ated independently for each translation, then the
translations are ranked by their respective scores.
The candidate with the highest score is selected
as the final translation. This is achieved by opti-
mizing the regression learning model?s output to
correlate against a set of training examples, where
the source sentences are provided with several ref-
erence translations, instead of manually labeling
the translations produced by various systems with
quantitative assessments as described in (Albrecht
and Hwa, 2007; Duh, 2008). The advantage of
our method is that we do not need to manually la-
bel the translations produced by each translation
system, therefore enabling our method suitable for
translation selection among any systems without
additional manual work.
We conducted experiments for spoken language
translation on the pivot task in the IWSLT 2008
evaluation campaign, where Chinese sentences in
travel domain need to be translated into Spanish,
with English as the pivot language. Experimen-
tal results show that (1) the performances of the
three pivot methods are comparable when only
SMT systems are used. However, the triangulation
method and the transfer method significantly out-
perform the synthetic method when RBMT sys-
tems are used to improve the translation qual-
ity; (2) The hybrid method combining SMT and
RBMT system for pivot translation greatly im-
proves the translation quality. And this translation
quality is higher than that of those produced by the
system trained with a real Chinese-Spanish cor-
pus; (3) Our sentence-level translation selection
method consistently and significantly improves
the translation quality over individual translation
outputs in all of our experiments.
Section 2 briefly introduces the three pivot
translation methods. Section 3 presents the hy-
brid method combining SMT and RBMT sys-
tems. Section 4 describes the translation selec-
tion method. Experimental results are presented
in Section 5, followed by a discussion in Section
6. The last section draws conclusions.
2 Pivot Methods for Phrase-based SMT
2.1 Triangulation Method
Following the method described in Wu and Wang
(2007), we train the source-pivot and pivot-target
translation models using the source-pivot and
pivot-target corpora, respectively. Based on these
two models, we induce a source-target translation
model, in which two important elements need to
be induced: phrase translation probability and lex-
ical weight.
Phrase Translation Probability We induce the
phrase translation probability by assuming the in-
dependence between the source and target phrases
when given the pivot phrase.
?(s?|t?) =
?
p?
?(s?|p?)?(p?|t?) (1)
Where s?, p? and t? represent the phrases in the lan-
guages Ls, Lp and Lt, respectively.
Lexical Weight According to the method de-
scribed in Koehn et al (2003), there are two im-
portant elements in the lexical weight: word align-
ment information a in a phrase pair (s?, t?) and lex-
ical translation probability w(s|t).
Let a1 and a2 represent the word alignment in-
formation inside the phrase pairs (s?, p?) and (p?, t?)
respectively, then the alignment information inside
(s?, t?) can be obtained as shown in Eq. (2).
a = {(s, t)|?p : (s, p) ? a1 & (p, t) ? a2} (2)
Based on the the induced word alignment in-
formation, we estimate the co-occurring frequen-
cies of word pairs directly from the induced phrase
155
pairs. Then we estimate the lexical translation
probability as shown in Eq. (3).
w(s|t) = count(s, t)?
s? count(s?, t)
(3)
Where count(s, t) represents the co-occurring fre-
quency of the word pair (s, t).
2.2 Transfer Method
The transfer method first translates from the
source language to the pivot language using a
source-pivot model, and then from the pivot lan-
guage to the target language using a pivot-target
model. Given a source sentence s, we can trans-
late it into n pivot sentences p1, p2, ..., pn using a
source-pivot translation system. Each pi can be
translated into m target sentences ti1, ti2, ..., tim.
We rescore all the n ? m candidates using both
the source-pivot and pivot-target translation scores
following the method described in Utiyama and
Isahara (2007). If we use hfp and hpt to denote the
features in the source-pivot and pivot-target sys-
tems, respectively, we get the optimal target trans-
lation according to the following formula.
t? = argmax
t
L?
k=1
(?spk hspk (s, p)+?ptk hptk (p, t)) (4)
Where L is the number of features used in SMT
systems. ?sp and ?pt are feature weights set by
performing minimum error rate training as de-
scribed in Och (2003).
2.3 Synthetic Method
There are two possible methods to obtain a source-
target corpus using the source-pivot and pivot-
target corpora. One is to obtain target transla-
tions for the source sentences in the source-pivot
corpus. This can be achieved by translating the
pivot sentences in source-pivot corpus to target
sentences with the pivot-target SMT system. The
other is to obtain source translations for the tar-
get sentences in the pivot-target corpus using the
pivot-source SMT system. And we can combine
these two source-target corpora to produced a fi-
nal synthetic corpus.
Given a pivot sentence, we can translate it into
n source or target sentences. These n translations
together with their source or target sentences are
used to create a synthetic bilingual corpus. Then
we build a source-target translation model using
this corpus.
3 Using RBMT Systems for Pivot
Translation
Since the source-pivot and pivot-target parallel
corpora are independent, the pivot sentences in the
two corpora are distinct from each other. Thus,
some linguistic phenomena in the source-pivot
corpus will lost if they do not exist in the pivot-
target corpus, and vice versa. Here we use RBMT
systems to fill up this data gap. For many source-
target language pairs, the commercial pivot-source
and/or pivot-target RBMT systems are available
on markets. For example, for Chinese to Span-
ish translation, English to Chinese and English to
Spanish RBMT systems are available.
With the RBMT systems, we can create a syn-
thetic multilingual source-pivot-target corpus by
translating the pivot sentences in the pivot-source
or pivot-target corpus. The source-target pairs ex-
tracted from this synthetic multilingual corpus can
be used to build a source-target translation model.
Another way to use the synthetic multilingual cor-
pus is to add the source-pivot or pivot-target sen-
tence pairs in this corpus to the training data to re-
build the source-pivot or pivot-target SMT model.
The rebuilt models can be applied to the triangula-
tion method and the transfer method as described
in Section 2.
Moreover, the RBMT systems can also be used
to enlarge the size of bilingual training data. Since
it is easy to obtain monolingual corpora than bilin-
gual corpora, we use RBMT systems to translate
the available monolingual corpora to obtain syn-
thetic bilingual corpus, which are added to the
training data to improve the performance of SMT
systems. Even if no monolingual corpus is avail-
able, we can also use RBMT systems to translate
the sentences in the bilingual corpus to obtain al-
ternative translations. For example, we can use
source-pivot RBMT systems to provide alternative
translations for the source sentences in the source-
pivot corpus.
In addition to translating training data, the
source-pivot RBMT system can be used to trans-
late the test set into the pivot language, which
can be further translated into the target language
with the pivot-target RBMT system. The trans-
lated test set can be added to the training data to
further improve translation quality. The advantage
of this method is that the RBMT system can pro-
vide translations for sentences in the test set and
cover some out-of-vocabulary words in the test set
156
that are uncovered by the training data. It can also
change the distribution of some phrase pairs and
reinforce some phrase pairs relative to the test set.
4 Translation Selection
We propose a method to select the optimal trans-
lation from those produced by various translation
systems. We regard sentence-level translation se-
lection as a machine translation (MT) evaluation
problem and formalize this problem with a regres-
sion learning model. For each translation, this
method uses the outputs from other translation
systems as pseudo references. The regression ob-
jective is to infer a function that maps a feature
vector (which measures the similarity of a trans-
lation from one system to the pseudo references)
to a score that indicates the quality of the transla-
tion. Scores are first generated independently for
each translation, then the translations are ranked
by their respective scores. The candidate with the
highest score is selected.
The similar ideas have been explored in previ-
ous studies. Albrecht and Hwa (2007) proposed
a method to evaluate MT outputs with pseudo
references using support vector regression as the
learner to evaluate translations. Duh (2008) pro-
posed a ranking method to compare the transla-
tions proposed by several systems. These two
methods require quantitative quality assessments
by human judges for the translations produced by
various systems in the training set. When we apply
such methods to translation selection, the relative
values of the scores assigned by the subject sys-
tems are important. In different data conditions,
the relative values of the scores assigned by the
subject systems may change. In order to train a re-
liable learner, we need to prepare a balanced train-
ing set, where the translations produced by differ-
ent systems under different conditions are required
to be manually evaluated. In extreme cases, we
need to relabel the training data to obtain better
performance. In this paper, we modify the method
in Albrecht and Hwa (2007) to only prepare hu-
man reference translations for the training exam-
ples, and then evaluate the translations produced
by the subject systems against the references us-
ing BLEU score (Papineni et al, 2002). We use
smoothed sentence-level BLEU score to replace
the human assessments, where we use additive
smoothing to avoid zero BLEU scores when we
calculate the n-gram precisions. In this case, we
ID Description
1-4 n-gram precisions against pseudo refer-
ences (1 ? n ? 4)
5-6 PER and WER
7-8 precision, recall, fragmentation from
METEOR (Lavie and Agarwal, 2007)
9-12 precisions and recalls of non-
consecutive bigrams with a gap
size of m (1 ? m ? 2)
13-14 longest common subsequences
15-19 n-gram precision against a target cor-
pus (1 ? n ? 5)
Table 1: Feature sets for regression learning
can easily retrain the learner under different con-
ditions, therefore enabling our method to be ap-
plied to sentence-level translation selection from
any sets of translation systems without any addi-
tional human work.
In regression learning, we infer a function
f that maps a multi-dimensional input vec-
tor x to a continuous real value y, such that
the error over a set of m training examples,
(x1, y1), (x2, y2), ..., (xm, ym), is minimized ac-
cording to a loss function. In the context of trans-
lation selection, y is assigned as the smoothed
BLEU score. The function f represents a math-
ematic model of the automatic evaluation metrics.
The input sentence is represented as a feature vec-
tor x, which are extracted from the input sen-
tence and the comparisons against the pseudo ref-
erences. We use the features as shown in Table 1.
5 Experiments
5.1 Data
We performed experiments on spoken language
translation for the pivot task of IWSLT 2008. This
task translates Chinese to Spanish using English
as the pivot language. Table 2 describes the data
used for model training in this paper, including the
BTEC (Basic Travel Expression Corpus) Chinese-
English (CE) corpus and the BTEC English-
Spanish (ES) corpus provided by IWSLT 2008 or-
ganizers, the HIT olympic CE corpus (2004-863-
008)1 and the Europarl ES corpus2. There are
two kinds of BTEC CE corpus: BTEC CE1 and
1http://www.chineseldc.org/EN/purchasing.htm
2http://www.statmt.org/europarl/
157
Corpus Size SW TW
BTEC CE1 20,000 164K 182K
BTEC CE2 18,972 177K 182K
HIT CE 51,791 490K 502K
BTEC ES 19,972 182K 185K
Europarl ES 400,000 8,485K 8,219K
Table 2: Training data. SW and TW represent
source words and target words, respectively.
BTEC CE2. BTEC CE1 was distributed for the
pivot task in IWSLT 2008 while BTEC CE2 was
for the BTEC CE task, which is parallel to the
BTEC ES corpus. For Chinese-English transla-
tion, we mainly used BTEC CE1 corpus. We used
the BTEC CE2 corpus and the HIT Olympic cor-
pus for comparison experiments only. We used the
English parts of the BTEC CE1 corpus, the BTEC
ES corpus, and the HIT Olympic corpus (if in-
volved) to train a 5-gram English language model
(LM) with interpolated Kneser-Ney smoothing.
For English-Spanish translation, we selected 400k
sentence pairs from the Europarl corpus that are
close to the English parts of both the BTEC CE
corpus and the BTEC ES corpus. Then we built
a Spanish LM by interpolating an out-of-domain
LM trained on the Spanish part of this selected
corpus with the in-domain LM trained with the
BTEC corpus.
For Chinese-English-Spanish translation, we
used the development set (devset3) released for
the pivot task as the test set, which contains 506
source sentences, with 7 reference translations in
English and Spanish. To be capable of tuning pa-
rameters on our systems, we created a develop-
ment set of 1,000 sentences taken from the training
sets, with 3 reference translations in both English
and Spanish. This development set is also used to
train the regression learning model.
5.2 Systems and Evaluation Method
We used two commercial RBMT systems in our
experiments: System A for Chinese-English bidi-
rectional translation and System B for English-
Chinese and English-Spanish translation. For
phrase-based SMT translation, we used the Moses
decoder (Koehn et al, 2007) and its support train-
ing scripts. We ran the decoder with its default
settings and then used Moses? implementation of
minimum error rate training (Och, 2003) to tune
the feature weights on the development set.
To select translation among outputs produced
by different pivot translation systems, we used
SVM-light (Joachins, 1999) to perform support
vector regression with the linear kernel.
Translation quality was evaluated using both the
BLEU score proposed by Papineni et al (2002)
and also the modified BLEU (BLEU-Fix) score3
used in the IWSLT 2008 evaluation campaign,
where the brevity calculation is modified to use
closest reference length instead of shortest refer-
ence length.
5.3 Results by Using SMT Systems
We conducted the pivot translation experiments
using the BTEC CE1 and BTEC ES described
in Section 5.1. We used the three methods de-
scribed in Section 2 for pivot translation. For the
transfer method, we selected the optimal transla-
tions among 10? 10 candidates. For the synthetic
method, we used the ES translation model to trans-
late the English part of the CE corpus to Spanish to
construct a synthetic corpus. And we also used the
BTEC CE1 corpus to build a EC translation model
to translate the English part of ES corpus into Chi-
nese. Then we combined these two synthetic cor-
pora to build a Chinese-Spanish translation model.
In our experiments, only 1-best Chinese or Span-
ish translation was used since using n-best results
did not greatly improve the translation quality. We
used the method described in Section 4 to select
translations from the translations produced by the
three systems. For each system, we used three
different alignment heuristics (grow, grow-diag,
grow-diag-final4) to obtain the final alignment re-
sults, and then constructed three different phrase
tables. Thus, for each system, we can get three
different translations for each input. These differ-
ent translations can serve as pseudo references for
the outputs of other systems. In our case, for each
sentence, we have 6 pseudo reference translations.
In addition, we found out that the grow heuristic
performed the best for all the systems. Thus, for
an individual system, we used the translation re-
sults produced using the grow alignment heuristic.
The translation results are shown in Table 3.
ASR and CRR represent different input condi-
tions, namely the result of automatic speech recog-
3https://www.slc.atr.jp/Corpus/IWSLT08/eval/IWSLT08
auto eval.tgz
4A description of the alignment heuristics can be found at
http://www.statmt.org/jhuws/?n=FactoredTraining.Training
Parameters
158
Method BLEU BLEU-Fix
Triangulation 33.70/27.46 31.59/25.02
Transfer 33.52/28.34 31.36/26.20
Synthetic 34.35/27.21 32.00/26.07
Combination 38.14/29.32 34.76/27.39
Table 3: CRR/ASR translation results by using
SMT systems
nition and correct recognition result, respectively.
Here, we used the 1-best ASR result. From the
translation results, it can be seen that three meth-
ods achieved comparable translation quality on
both ASR and CRR inputs, with the translation re-
sults on CRR inputs are much better than those on
ASR inputs because of the errors in the ASR in-
puts. The results also show that our translation se-
lection method is very effective, which achieved
absolute improvements of about 4 and 1 BLEU
scores on CRR and ASR inputs, respectively.
5.4 Results by Using both RBMT and SMT
Systems
In order to fill up the data gap as discussed in Sec-
tion 3, we used the RBMT System A to translate
the English sentences in the ES corpus into Chi-
nese. As described in Section 3, this corpus can
be used by the three pivot translation methods.
First, the synthetic Chinese-Spanish corpus can be
combined with those produced by the EC and ES
SMT systems, which were used in the synthetic
method. Second, the synthetic Chinese-English
corpus can be added into the BTEC CE1 corpus to
build the CE translation model. In this way, the in-
tersected English phrases in the CE corpus and ES
corpus becomes more, which enables the Chinese-
Spanish translation model induced using the trian-
gulation method to cover more phrase pairs. For
the transfer method, the CE translation quality can
be also improved, which would result in the im-
provement of the Spanish translation quality.
The translation results are shown in the columns
under ?EC RBMT? in Table 4. As compared with
those in Table 3, the translation quality was greatly
improved, with absolute improvements of at least
5.1 and 3.9 BLEU scores on CRR and ASR inputs
for system combination results. The above results
indicate that RBMT systems indeed can be used to
fill up the data gap for pivot translation.
In our experiments, we also used a CE RBMT
system to enlarge the size of training data by pro-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7
Phrase length
Co
ve
rag
e
SMT (Triangulation)
+EC RBMT
+EC RBMT+CE RBMT
+EC RBMT+CE RBMT+Test Set
Figure 1: Coverage on test source phrases
viding alternative English translations for the Chi-
nese part of the CE corpus. The translation results
are shown in the columns under ?+CE RBMT? in
Table 4. From the translation results, it can be
seen that, enlarging the size of training data with
RBMT systems can further improve the translation
quality.
In addition to translating the training data, the
CE RBMT system can be also used to translate the
test set into English, which can be further trans-
lated into Spanish with the ES RBMT system B.56
The translated test set can be further added to the
training data to improve translation quality. The
columns under ?+Test Set? in Table 4 describes
the translation results. The results show that trans-
lating the test set using RBMT systems greatly im-
proved the translation result, with further improve-
ments of about 2 and 1.5 BLEU scores on CRR
and ASR inputs, respectively.
The results also indicate that both the triangula-
tion method and the transfer method greatly out-
performed the synthetic method when we com-
bined both RBMT and SMT systems in our exper-
iments. Further analysis shows that the synthetic
method contributed little to system combination.
The selection results are almost the same as those
selected from the translations produced by the tri-
angulation and transfer methods.
In order to further analyze the translation re-
sults, we evaluated the above systems by examin-
ing the coverage of the phrase tables over the test
phrases. We took the triangulation method as a
case study, the results of which are shown in Fig-
5Although using the ES RBMT system B to translate the
training data did not improve the translation quality, it im-
proved the translation quality by translating the test set.
6The RBMT systems achieved a BLEU score of 24.36 on
the test set.
159
EC RBMT + CE RBMT + Test Set
Method BLEU BLEU-Fix BLEU BLEU-Fix BLEU BLEU-Fix
Triangulation 40.69/31.02 37.99/29.15 41.59/31.43 39.39/29.95 44.71/32.60 42.37/31.14
Transfer 42.06/31.72 39.73/29.35 43.40/33.05 40.73/30.06 45.91/34.52 42.86/31.92
Synthetic 39.10/29.73 37.26/28.45 39.90/30.00 37.90/28.66 41.16/31.30 37.99/29.36
Combination 43.21/33.23 40.58/31.17 45.09/34.10 42.88/31.73 47.06/35.62 44.94/32.99
Table 4: CRR/ASR translation results by using RBMT and SMT systems
Method BLEU BLEU-Fix
Triangulation 45.64/33.15 42.11/31.11
Transfer 47.18/34.56 43.61/32.17
Combination 48.42/36.42 45.42/33.52
Table 5: CRR/ASR translation results by using ad-
ditional monolingual corpora
ure 1. It can be seen that using RBMT systems
to translate the training and/or test data can cover
more source phrases in the test set, which results
in translation quality improvement.
5.5 Results by Using Monolingual Corpus
In addition to translating the limited bilingual cor-
pus, we also translated additional monolingual
corpus to further enlarge the size of the training
data. We assume that it is easier to obtain a mono-
lingual pivot corpus than to obtain a monolingual
source or target corpus. Thus, we translated the
English part of the HIT Olympic corpus into Chi-
nese and Spanish using EC and ES RBMT sys-
tems. The generated synthetic corpus was added to
the training data to train EC and ES SMT systems.
Here, we used the synthetic CE Olympic corpus
to train a model, which was interpolated with the
CE model trained with both the BTEC CE1 cor-
pus and the synthetic BTEC corpus to obtain an
interpolated CE translation model. Similarly, we
obtained an interpolated ES translation model. Ta-
ble 5 describes the translation results.7 The results
indicate that translating monolingual corpus using
the RBMT system further improved the translation
quality as compared with those in Table 4.
6 Discussion
6.1 Effects of Different RBMT Systems
In this section, we compare the effects of two
commercial RBMT systems with different transla-
7Here we excluded the synthetic method since it greatly
falls behind the other two methods.
Method Sys. A Sys. B Sys. A+B
Triangulation 40.69 39.28 41.01
Transfer 42.06 39.57 43.03
Synthetic 39.10 38.24 39.26
Combination 43.21 40.59 44.27
Table 6: CRR translation results (BLEU scores)
by using different RBMT systems
tion accuracy on spoken language translation. The
goals are (1) to investigate whether a RBMT sys-
tem can improve pivot translation quality even if
its translation accuracy is not high, and (2) to com-
pare the effects of RBMT system with different
translation accuracy on pivot translation. Besides
the EC RBMT system A used in the above section,
we also used the EC RBMT system B for this ex-
periment.
We used the two systems to translate the test set
from English to Chinese, and then evaluated the
translation quality against Chinese references ob-
tained from the IWSLT 2008 evaluation campaign.
The BLEU scores are 43.90 and 29.77 for System
A and System B, respectively. This shows that
the translation quality of System B on spoken lan-
guage corpus is much lower than that of System A.
Then we applied these two different RBMT sys-
tems to translate the English part of the BTEC ES
corpus into Chinese as described in Section 5.4.
The translation results on CRR inputs are shown
in Table 6.8 We replicated some of the results in
Table 4 for the convenience of comparison. The
results indicate that the higher the translation ac-
curacy of the RBMT system is, the better the pivot
translation is. If we compare the results with those
only using SMT systems as described in Table 3,
the translation quality was greatly improved by at
least 3 BLEU scores, even if the translation ac-
8We omitted the ASR translation results since the trends
are the same as those for CRR inputs. And we only showed
BLEU scores since the trend for BLEU-Fix scores is similar.
160
Method Multilingual + BTEC CE1
Triangulation 41.86/39.55 42.41/39.55
Transfer 42.46/39.09 43.84/40.34
Standard 42.21/40.23 42.21/40.23
Combination 43.75/40.34 44.68/41.14
Table 7: CRR translation results by using multilin-
gual corpus. ?/? separates the BLEU and BLEU-
fix scores.
curacy of System B is not so high. Combining
two RBMT systems further improved the transla-
tion quality, which indicates that the two systems
complement each other.
6.2 Results by Using Multilingual Corpus
In this section, we compare the translation results
by using a multilingual corpus with those by us-
ing independently sourced corpora. BTEC CE2
and BTEC ES are from the same source sentences,
which can be taken as a multilingual corpus. The
two corpora were employed to build CE and ES
SMT models, which were used in the triangula-
tion method and the transfer method. We also ex-
tracted the Chinese-Spanish (CS) corpus to build a
standard CS translation system, which is denoted
as Standard. The comparison results are shown
in Table 7. The translation quality produced by
the systems using a multilingual corpus is much
higher than that produced by using independently
sourced corpora as described in Table 3, with an
absolute improvement of about 5.6 BLEU scores.
If we used the EC RBMT system, the translation
quality of those in Table 4 is comparable to that by
using the multilingual corpus, which indicates that
our method using RBMT systems to fill up the data
gap is effective. The results also indicate that our
translation selection method for pivot translation
outperforms the method using only a real source-
target corpus.
For comparison purpose, we added BTEC CE1
into the training data. The translation quality was
improved by only 1 BLEU score. This again
proves that our method to fill up the data gap is
more effective than that to increase the size of the
independently sourced corpus.
6.3 Comparison with Related Work
In IWSLT 2008, the best result for the pivot task
is achieved by Wang et al (2008). In order to
compare the results, we added the bilingual HIT
Ours Wang TSAL
BLEU 49.57 - 48.25
BLEU-Fix 46.74 45.10 45.27
Table 8: Comparison with related work
Olympic corpus into the CE training data.9 We
also compared our translation selection method
with that proposed in (Wang et al, 2008) that
is based on the target sentence average length
(TSAL). The translation results are shown in Ta-
ble 8. ?Wang? represents the results in Wang et al
(2008). ?TSAL? represents the translation selec-
tion method proposed in Wang et al (2008), which
is applied to our experiment. From the results, it
can be seen that our method outperforms the best
system in IWSLT 2008 and that our translation se-
lection method outperforms the method based on
target sentence average length.
7 Conclusion
In this paper, we have compared three differ-
ent pivot translation methods for spoken language
translation. Experimental results indicated that the
triangulation method and the transfer method gen-
erally outperform the synthetic method. Then we
showed that the hybrid method combining RBMT
and SMT systems can be used to fill up the data
gap between the source-pivot and pivot-target cor-
pora. By translating the pivot sentences in inde-
pendent corpora, the hybrid method can produce
translations whose quality is higher than those pro-
duced by the method using a source-target corpus
of the same size. We also showed that even if the
translation quality of the RBMT system is low, it
still greatly improved the translation quality.
In addition, we proposed a system combination
method to select better translations from outputs
produced by different pivot methods. This method
is developed through regression learning, where
only a small size of training examples with ref-
erence translations are required. Experimental re-
sults indicate that this method can consistently and
significantly improve translation quality over indi-
vidual translation outputs. And our system out-
performs the best system for the pivot task in the
IWSLT 2008 evaluation campaign.
9We used about 70k sentence pairs for CE model training,
while Wang et al (2008) used about 100k sentence pairs, a
CE translation dictionary and more monolingual corpora for
model training.
161
References
Joshua S. Albrecht and Rebecca Hwa. 2007. Regres-
sion for Sentence-Level MT Evaluation with Pseudo
References. In Proceedings of the 45th Annual
Meeting of the Accosiation of Computational Lin-
guistics, pages 296?303.
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-Based
Statistical Machine Translation with Pivot Lan-
guages. In Proceedings of the International Work-
shop on Spoken Language Translation, pages 143-
149.
Tevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Making Effective Use of
Multi-Parallel Corpora. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 348?355.
Kevin Duh. 2008. Ranking vs. Regression in Machine
Translation Evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
191?194.
Xiaoguang Hu, Haifeng Wang, and Hua Wu. 2007.
Using RBMT Systems to Produce Bilingual Corpus
for SMT. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 287?295.
Thorsten Joachims. 1999. Making Large-Scale
SVM Learning Practical. In Bernhard Scho?elkopf,
Christopher Burges, and Alexander Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
Maxim Khalilov, Marta R. Costa-Jussa`, Carlos A.
Henr??quez, Jose? A.R. Fonollosa, Adolfo Herna?ndez,
Jose? B. Marin?o, Rafael E. Banchs, Chen Boxing,
Min Zhang, Aiti Aw, and Haizhou Li. 2008. The
TALP & I2R SMT Systems for IWSLT 2008. In
Proceedings of the International Workshop on Spo-
ken Language Translation, pages 116?123.
Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL: Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Associa-tion for Computational Linguistics, demon-
stration session, pages 177?180.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of Workshop on Statistical Machine
Translation at the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 228?
231.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th annual meeting of the Association for
Computational Linguistics, pages 311?318.
Michael Paul. 2008. Overview of the IWSLT 2008
Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Trans-
lation, pages 1?17.
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-Based Statisti-
cal Machine Translation. In Proceedings of human
language technology: the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 484?491.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The TCH Machine Translation System for IWSLT
2008. In Proceedings of the International Workshop
on Spoken Language Translation, pages 124?131.
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 856?863.
162
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524?534,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation 
Using Random Walk 
 
Xiaoning Zhu1*
Conghui Zhu1, and Tiejun Zhao1 
, Zhongjun He2, Hua Wu2, Haifeng Wang2,  
Harbin Institute of Technology, Harbin, China1 
Baidu Inc., Beijing, China2 
{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
 
 
 
                                                          
* This work was done when the first author was visiting Baidu. 
Abstract 
This paper proposes a novel approach that uti-
lizes a machine learning method to improve 
pivot-based statistical machine translation 
(SMT). For language pairs with few bilingual 
data, a possible solution in pivot-based SMT 
using another language as a "bridge" to gen-
erate source-target translation. However, one 
of the weaknesses is that some useful source-
target translations cannot be generated if the 
corresponding source phrase and target phrase 
connect to different pivot phrases. To allevi-
ate the problem, we utilize Markov random 
walks to connect possible translation phrases 
between source and target language. Experi-
mental results on European Parliament data, 
spoken language data and web data show that 
our method leads to significant improvements 
on all the tasks over the baseline system. 
1 Introduction 
Statistical machine translation (SMT) uses bilin-
gual corpora to build translation models. The 
amount and the quality of the bilingual data 
strongly affect the performance of SMT systems. 
For resource-rich language pairs, such as Chinese-
English, it is easy to collect large amounts of bi-
lingual corpus. However, for resource-poor lan-
guage pairs, such as Chinese-Spanish, it is difficult 
to build a high-performance SMT system with the 
small scale bilingual data available.  
The pivot language approach, which performs 
translation through a third language, provides a 
possible solution to the problem. The triangulation 
method (Wu and Wang, 2007; Cohn and Lapata, 
2007) is a representative work for pivot-based ma-
chine translation. With a triangulation pivot ap-
proach, a source-target phrase table can be 
obtained by combining the source-pivot phrase 
table and the pivot-target phrase table. However, 
one of the weaknesses is that some corresponding 
source and target phrase pairs cannot be generated, 
because they are connected to different pivot 
phrases (Cui et al, 2013). As illustrated in Figure 
1, since there is no direct translation between ??
?? henkekou? and ?really delicious?, the trian-
gulation method is unable to establish a relation 
between ???? henkekou? and the two Spanish 
phrases. 
To solve this problem, we apply a Markov ran-
dom walk method to pivot-based SMT system. 
Random walk has been widely used. For example, 
Brin and Page (1998) used random walk to dis-
cover potential relations between queries and doc-
uments for link analysis in information retrieval. 
Analogous to link analysis, the aim of pivot-based 
translation is to discover potential translations be-
tween source and target language via the pivot 
language.  
524
The goal of this paper is to extend the previous 
triangulation approach by exploring implicit trans-
lation relations using random walk method. We 
evaluated our approach in several translation tasks, 
including translations between European lan-
guages; Chinese-Spanish spoken language transla-
tion and Chinese-Japanese translation with English 
as the pivot language. Experimental results show 
that our approach achieves significant improve-
ments over the conventional pivot-based method, 
triangulation method. 
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
We review the triangulation method for pivot-
based machine translation in section 3. Section 4 
describes the random walk models. In section 5 
and section 6, we describe the experiments and 
analyze the performance, respectively. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classified 
into 3 kinds of methods: 
Transfer Method: Within the transfer frame-
work (Utiyama and Isahara, 2007; Wang et al, 
2008; Costa-juss? et al, 2011), a source sentence 
is first translated to n pivot sentences via a source-
pivot translation system, and then each pivot sen-
tence is translated to m target sentences via a piv-
ot-target translation system. At each step (source 
to pivot and pivot to target), multiple translation 
outputs will be generated, thus a minimum Bayes-
risk system combination method is often used to 
select the optimal sentence (Gonz?lez-Rubio et al, 
2011; Duh et al, 2011). A problem with the trans-
fer method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot translation 
system will be transferred to the pivot-target trans-
lation. 
Synthetic Method: A synthetic method creates 
a synthetic source-target corpus using source-pivot 
translation model or pivot-target translation model 
(Utiyama et al, 2008; Wu and Wang, 2009). For 
example, we can translate each pivot sentence in 
the pivot-target corpus to source language with a 
pivot-source model, and then combine the translat-
ed source sentence with the target sentence to ob-
tain a synthetic source-target corpus, and vice 
versa. However, it is difficult to build a high quali-
ty translation system with a corpus created by a 
machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target model by combining 
source-pivot and pivot-target translation models 
(Wu and Wang, 2007; Cohn and Lapata 2007), 
which has been shown to work better than the oth-
er pivot approaches (Utiyama and Isahara, 2007). 
As we mentioned earlier, the weakness of triangu-
lation is that the corresponding source and target 
phrase pairs cannot be connected in the case that 
they connect to different pivot phrases. 
3 The Triangulation Method 
In this section, we review the triangulation method 
for pivot-based translation. 
With the two additional bilingual corpora, the 
source-pivot and pivot-target translation models 
can be trained. Thus, a pivot model can be ob-
tained by merging these two models. In the trans-
lation model, the phrase translation probability and 
the lexical weight are language dependent, which 
will be introduced in the next two sub-sections. 
Figure 1: An example of random walk on phrase table. The dashed line indicates an implicit relation 
in the phrase table. 
???? 
feichanghaochi 
really delicious 
very tasty 
 
???
henkekou 
realmente delicioso 
 
Chinese English Spanish 
muy delicioso 
 
525
3.1 Phrase Translation Probability 
The triangulation method assumes that there exist 
translations between phrases s  and phrase p  in 
source and pivot languages, and between phrase 
p  and phrase t  in pivot and target languages. 
The phrase translation probability ?  between 
source and target languages is determined by the 
following model: 
( | ) ( | , ) ( | )
          ( | ) ( | )
p
p
s t s p t p t
s p p t
? ? ?
? ?
=
=
?
?
       (1) 
3.2 Lexical Weight 
Given a phrase pair ( , )s t and a word alignment 
a  between the source word positions 1, ,i n= ?  
and the target word positions 0,1, ,j m= ? , the 
lexical weight of phrase pair ( , )s t  can be calcu-
lated with the following formula (Koehn et al 
2003) : 
( , )1
1
( | , ) ( | )
{ | ( , ) }
n
i j
i j ai
p s t a s t
j i j a?
?
? ?=
=
? ?? (2) 
In formula 2, the lexical translation probability 
distribution ( | )s t?  between source word s  and 
target word t  can be estimated with formula 3. 
'
'
( , )
( | )
( , )
s
count s t
s t
count s t
? =
?
            (3) 
Thus the alignment a  between the source 
phrase s  and target phrase t  via pivot phrase p  
is needed for computing the lexical weight. The 
alignment a  can be obtained as follows: 
1 2{( , ) | : ( , ) & ( , ) }a s t p s p a p t a= ? ? ?    (4) 
where 1a  and 2a  indicate the word alignment be-
tween the phrase pair ( , )s p  and ( , )p t , respec-
tively. 
The triangulation method requires that both the 
source and target phrases connect to the same piv-
ot phrase. Otherwise, the source-target phrase pair 
cannot be discovered. As a result, some useful 
translation relations will be lost. In order to allevi-
ate this problem, we propose a random walk model, 
to discover the implicit relations among the source, 
pivot and target phrases. 
4 Random Walks on Translation Graph 
For phrase-based SMT, all source-target phrase 
pairs are stored in a phrase table. In our random 
walk approach, we first build a translation graph 
according to the phrase table. A translation graph 
contains two types of nodes: source phrase and 
target phrase. A source phrase s  and a target 
phrase t  are connected if exists a phrase pair 
( , )s t  in the phrase table. The edge can be 
weighted according to translation probabilities or 
alignments in the phrase table. For the pivot-based 
translation, the translation graph can be derived 
from the source-pivot phrase table and pivot-target 
phrase table.  
Our random walk model is inspired by two 
works (Szummer and Jaakkola, 2002; Craswell 
and Szummer,2007). The general process of ran-
dom walk can be described as follows: 
Let ( , )G V E= be a directed graph with n  ver-
tices and m  edges. For a vertex v V? , ( )v?  de-
notes the set of neighbors of v  in G . A random 
walk on G  follows the following process: start at 
a vertex 0v , chose and walk along a random 
neighbor 1v , with 1 0( )v v?? . At the second step, 
start from 1v  and chose a random neighbor 2v , and 
so on. 
Let S be the set of source phrases, and P be the 
set of pivot phrases. Then the nodes V are the un-
ion of S and P. The edges E correspond to the rela-
tions between phrase pairs.  
Let R represent the binary relations between 
source phrases and pivot phrases. Then the 1-step 
translation ikR from node i to node k can be direct-
ly obtained in the phrase table. 
Define operator ?  to denote the calculation of 
relation R. Then 2-step translation ijR  from node i 
to node j can be obtained with the following for-
mula.  
ij ik kjR R R= ?                           (4) 
We use |0 ( | )tR k i  to denote a t-step translation 
relation from node i to node k. In order to calculate 
the translation relations efficiently, we use a ma-
trix A to represent the graph. A t step translation 
probability can be denoted with the following for-
mula. 
526
|0 ( | ) [ ]
t
t ikP k i A=                         (5) 
where A is a matrix whose i,k-th element is ikR . 
4.1 Framework of Random Walk Approach 
The overall framework of random walk for pivot-
based machine translation is shown in Figure 2. 
Before using random walk model, we have two 
phrase tables: source-pivot phrase table (SP phrase 
table) and pivot-target phrase table (PT phrase ta-
ble). After applying the random walk approach, we 
can achieve two extended phrase table: extended 
source-pivot phrase table (S?P? phrase table) and 
extended pivot-target phrase table (P?T? phrase 
table). The goal of pivot-based SMT is to get a 
source-target phrase table (ST phrase table) via SP 
phrase table and PT phrase table.  
Our random walk was applied on SP phrase ta-
ble or PT phrase table separately. In next 2 sub-
sections, we will explain how the phrase transla-
tion probabilities and lexical weight are obtained 
with random walk model on the phrase table. 
Figure 3 shows some possible decoding pro-
cesses of random walk based pivot approach. In 
figure 3-a, the possible source-target phrase pair 
can be obtained directly via a pivot phrase, so it 
does not need a random walk model. In figure 3-b 
and figure 3-c, one candidate source-target phrase 
pair can be obtained by random walks on source-
pivot side or pivot-target side. Figure 3-d shows 
that the possible source-target can only by ob-
tained by random walks on source-pivot side and 
pivot-target side. 
4.2 Phrase Translation Probabilities 
For the translation probabilities, the binary relation 
R is the translation probabilities in the phrase table. 
The operator ?  is multiplication. According to 
formula 5, the random walk sums up the probabili-
ties of all paths of length t between the node i and 
k. 
Figure 2: Framework of random walk based pivot translation. The ST phrase table was generated by combin-
ing SP and PT phrase table through triangulation method. The phrase table with superscript ??? means that it 
was enlarged by random walk. 
 
S?P?
Phrase Table
P?T? 
Phrase Table
 SP 
Phrase Table
PT 
Phrase Table
ST 
Phrase Table
S?T?
Phrase Table
Pivot without 
random walk
Pivot with 
random walkrandom walk
random walk
Figure 3: Some possible decoding processes of random walk based pivot approach. The ? stands for the 
source phrase (S); the ? represents the pivot phrase (P) and the ? stands for the target phrase (T). 
 
(a) Pivot without  
       random walk 
S P T 
(d) Random walk on   
     both sides 
S P T 
(b) Random walk on  
      source-pivot side 
S P T 
(c) Random walk on 
      pivot-target side 
S P T 
527
Take source-to-pivot phrase graph as an exam-
ple; denote matrix A contains s+p nodes (s source 
phrases and p pivot phrases) to represent the trans-
lation graph.  
( ) ( )ij s p s p
A g
+ ? +
? ?= ? ?                         (6) 
where ijg  is the i,j-th elements of matrix A. 
We can split the matrix A into 4 sub-matrixes: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
                      (7) 
where the sub-matrix [ ]sp ik s pA p ?=  represents the 
translation probabilities from source to pivot lan-
guage, and psA  represents the similar meaning. 
Take 3 steps walks as an example: 
Step1: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
 
Step2: 
2
0
0
sp ps s p
p s ps sp
A A
A
A A
?
?
?? ?
= ? ??? ?
 
Step3: 
3
0
0
s s sp ps sp
ps sp ps p p
A A A
A
A A A
?
?
? ?? ?
= ? ?? ?? ?
 
For the 3 steps example, each step performs a 
translation process in the form of matrix?s self-
multiplication.  
1. The first step means the translation from 
source language to pivot language. The matrix 
A is derived from the phrase table directly and 
each element in the graph indicates a transla-
tion rule in the phrase table.  
2. The second step demonstrates a procedure: S-
P-S?. With 2 steps random walks, we can find 
the synonymous phrases, and this procedure is 
analogous to paraphrasing (Bannard and 
Callison-Burch, 2005). For the example shown 
in  figure 1 as an example, the hidden relation 
between ???? henkekou? and ?????
feichanghaochi? can be found through Step 2. 
3. The third step describes the following proce-
dure: S-P-S?-P?. An extended source-pivot 
phrase table is generated by 3-step random 
walks. Compared with the initial phrase table 
in Step1, although the number of phrases is 
not increased, the relations between phrase 
pairs are increased and more translation rules 
can be obtained. Still for the example in Fig-
ure 1 , the hidden relation between ????
henkekou? and ?really delicious? can be gen-
erated in Step 3. 
4.3 Lexical Weights 
To build a translation graph, the two sets of phrase 
translation probabilities are represented in the 
phrase tables. However, the two lexical weights 
are not presented in the graph directly. To deal 
with this, we should conduct a word alignment 
random walk model to obtain a new alignment a 
after t steps. For the computation of lexical 
weights, the relation R can be expressed as the 
word alignment in the phrase table. The operator 
?  can be induced with the following formula. 
1 2{( , ) | : ( , ) & ( , ) }a x y p x z a z y a= ? ? ?         (8) 
where a1 and a2 represent the word alignment 
information inside the phrase pairs ( , )x y  and 
( , )y z respectively. An example of word 
alignment inducing is shown in Figure 4. With a 
new word alignment, the two lexical weights can 
be calculated by formula 2 and formula 3. 
Figure 4: An example of word alignment induction with 3 steps random walks 
?   ??   ?   ?   ? 
could   you   fill   out   this   form ?   ?   ??   ??   ?? 
please   fill   out   this   form 
?   ??   ?   ?   ? 
could   you   fill   out   this   form 
step 1 
step 2 
step 3 
528
5 Experiments 
5.1 Translation System and Evaluation Met-
ric 
In our experiments, the word alignment was ob-
tained by GIZA++ (Och and Ney, 2000) and the 
heuristics ?grow-diag-final? refinement rule. 
(Koehn et al, 2003). Our translation system is an 
in-house phrase-based system using a log-linear 
framework including a phrase translation model, a 
language model, a lexicalized reordering model, a 
word penalty model and a phrase penalty model, 
which is analogous to Moses (Koehn et al, 2007). 
The baseline system is the triangulation method 
based pivot approach (Wu and Wang, 2007).  
To evaluate the translation quality, we used 
BLEU (Papineni et al, 2002) as our evaluation 
metric. The statistical significance using 95% con-
fidence intervals were measured with paired boot-
strap resampling (Koehn, 2004). 
5.2 Experiments on Europarl 
5.2.1. Data sets 
We mainly test our approach on Europarl1
We perform our experiments on different trans-
lation directions and via different pivot languages. 
As a most widely used language in the world 
(Mydans, 2011), English was used as the pivot 
language for granted when carrying out experi-
ments on different translation directions. For trans-
lating Portuguese to Swedish, we also tried to 
perform our experiments via different pivot lan-
 corpus, 
which is a multi-lingual corpus including 21 Euro-
pean languages. Due to the size of the data, we 
only select 11 languages which were added to 
Europarl from 04/1996 or 01/1997, including Dan-
ish (da), German (de), Greek (el), English (en), 
Spanish (es), Finnish (fi), French (fr), Italian (it) 
Dutch (nl) Portuguese (pt) and Swedish (sv). In 
order to avoid a trilingual scenario, we split the 
training corpus into 2 parts by the year of the data: 
the data released in odd years were used for train-
ing source-pivot model and the data released in 
even years were used for training pivot-target 
model.  
                                                          
1 http://www.statmt.org/europarl/ 
guages. Table 1 and Table 2 summarized the train-
ing data. 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
da-en 974,189 en-da 953,002 
de-en 983,411 en-de 905,167 
el-en 609,315 en-el 596,331 
es-en 968,527 en-es 961,782 
fi-en 998,429 en-fi 903,689 
fr-en 989,652 en-fr 974,637 
it-en 934,448 en-it 938,573 
nl-en 982,696 en-nl 971,379 
pt-en 967,816 en-pt 960,214 
sv-en 960,631 en-sv 869,254 
 
Table1. Training data for experiments using English as 
the pivot language. For source-pivot (src-pvt; xx-en) 
model training, the data of odd years were used. Instead 
the data of even years were used for pivot-target (pvt-
src; en-xx) model training. 
 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
pt-da 941,876 da-sv 865,020 
pt-de 939,932 de-sv 814,678 
pt-el 591,429 el-sv 558,765 
pt-es 934,783 es-sv 827,964 
pt-fi 950,588 fi-sv 872,182 
pt-fr 954,637 fr-sv 860,272 
pt-it 900,185 it-sv 813,000 
pt-nl 945,997 nl-sv 864,675 
 
Table2. Training data for experiments via different piv-
ot languages. For source-pivot (src-pvt; pt-xx) model 
training, the data of odd years were used. Instead the 
data of even years were used for pivot-target (pvt-src; 
xx-sv) model training. 
 
Test Set Sentence # Reference # 
WMT06 2,000 1 
WMT07 2,000 1 
WMT08 2,000 1 
 
Table3. Statistics of test sets. 
529
 
Several test sets have been released for the 
Europarl corpus. In our experiments, we used 
WMT20062, WMT20073 and WMT20084 as our 
test data. The original test data includes 4 lan-
guages and extended versions with 11 languages 
of these test sets are available by the EuroMatrix5
5.2.2. Experiments on Different Translation 
Directions 
  
project. Table 3 shows the test sets. 
We build 180 pivot translation systems6
The baseline system was built following the tra-
ditional triangulation pivot approach. Table 4 lists 
the results on Europarl training data. Limited by 
 (including 
90 baseline systems and 90 random walk based 
systems) using 10 source/target languages and 1 
pivot language (English).  
                                                          
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.statmt.org/wmt07/shared-task.html 
4 http://www.statmt.org/wmt08/shared-task.html 
5 http://matrix.statmt.org/test_sets/list 
6 Given N languages, a total of N*(N-1) SMT systems should 
be build to cover the translation between each language.  
the length of the paper, we only show the results 
on WMT08, the tendency of the results on 
WMT06 and WMT07 is similar to WMT08. 
Several observations can be made from the table.  
1. In all 90 language pairs, our method achieves 
general improvements over the baseline system.  
2. Among 90 language pairs, random walk 
based approach is significantly better than the 
baseline system in 75 language pairs. 
3. The improvements of our approach are not 
equal in different translation directions. The im-
provement ranges from 0.06 (it-es) to 1.21 (pt-da). 
One possible reason is that the performance is re-
lated with the source and target language. For ex-
ample, when using Finnish as the target language, 
the improvement is significant over the baseline. 
This may be caused by the great divergence be-
tween Uralic language (Finnish) and Indo-
European language (the other European language 
in Table4). From the table we can find that the 
translation between languages in different lan-
guage family is worse than that in some language 
family. But our random walk approach can im-
 TGT 
SRC 
da de el es fi fr it nl pt sv 
Baseline 
RW 
da - 
19.83 
20.15* 
20.46 
21.02* 
27.59 
28.29* 
14.76 
15.63* 
24.11 
24.71* 
20.49 
20.82* 
22.26 
22.57* 
24.38 
24.88* 
28.33 
28.87* 
Baseline 
RW 
de 
23.35 
23.69* 
- 
19.83 
20.05 
26.21 
26.70* 
12.72 
13.57* 
22.43 
22.78* 
18.82 
19.32* 
23.74 
24.11* 
23.05 
23.35* 
21.17 
21.27 
Baseline 
RW 
el 
23.24 
23.82* 
18.12 
18.49* 
- 
32.28 
32.48 
13.31 
14.08* 
27.35 
27.67* 
23.19 
23.63* 
20.80 
21.26* 
27.62 
27.86 
22.70 
23.15* 
Baseline 
RW 
es 
25.34 
26.07* 
19.67 
20.17* 
27.24 
27.52 
- 
13.93 
14.61* 
32.91 
33.16 
27.67 
27.92 
22.37 
22.85* 
34.73 
34.93 
24.83 
25.50* 
Baseline 
RW 
fi 
18.29 
18.63* 
13.20 
13.40 
14.72 
15.00* 
20.17 
20.48* 
- 
17.52 
17.84* 
14.76 
15.01 
15.50 
16.04* 
17.30 
17.68* 
16.63 
16.79 
Baseline 
RW 
fr 
25.67 
26.51* 
20.02 
20.45* 
26.58 
26.75 
37.50 
37.80* 
13.90 
14.75* 
- 
28.51 
28.71 
22.65 
23.33* 
33.81 
33.93 
24.64 
25.59* 
Baseline 
RW 
it 
22.63 
23.27* 
17.81 
18.40* 
24.24 
24.66* 
34.36 
35.42* 
13.20 
14.11* 
30.16 
30.48* 
- 
21.37 
21.81* 
30.84 
30.92* 
22.12 
22.64* 
Baseline 
RW 
nl 
22.49 
22.76 
19.86 
20.45* 
18.56 
19.10* 
24.69 
25.19* 
11.96 
12.63* 
21.48 
22.05* 
18.36 
18.67* 
- 
21.71 
22.13* 
19.83 
22.17* 
Baseline 
RW 
pt 
24.08 
25.29* 
19.11 
19.83* 
25.30 
26.20* 
36.59 
37.13* 
13.33 
14.21* 
32.47 
32.78* 
28.08 
28.44* 
21.52 
22.46* 
- 
22.90 
23.90* 
Baseline 
RW 
sv 
31.24 
31.75* 
20.26 
20.74* 
22.06 
22.59* 
29.21 
29.87* 
15.39 
16.13* 
25.63 
26.18* 
21.25 
21.81* 
22.30 
22.62* 
25.60 
26.09* 
- 
Table4. Experimental results on Europarl with different translation directions (BLEU% on WMT08). 
RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05). 
530
prove the performance of translations between dif-
ferent language families. 
5.2.3. Experiments via Different Pivot Lan-
guages 
In addition to using English as the pivot language, 
we also try some other languages as the pivot 
language. In this sub-section, experiments were 
carried out from translating Portuguese to Swedish 
via different pivot languages.  
Table 5 summarizes the BLEU% scores of dif-
ferent pivot language when translating from Por-
tuguese to Swedish. Similar to Table 4, our 
approach still achieves general improvements over 
the baseline system even if the pivot language has 
been changed. From the table we can see that for 
most of the pivot language, the random walk based 
approach gains more than 1 BLEU score over the 
baseline. But when using Finnish as the pivot lan-
guage, the improvement is only 0.02 BLEU scores 
on WMT08. This phenomenon shows that the piv-
ot language can also influence the performance of 
random walk approach. One possible reason for 
the poor performance of using Finnish as the pivot 
language is that Finnish belongs to Uralic lan-
guage family, and the other languages belong to 
Indo-European family. The divergence between 
different language families led to a poor perfor-
mance. Thus how to select a best pivot language is 
our future work. 
The problem with random walk is that it will 
lead to a larger phrase table with noises. In this 
sub-section, a pre-pruning (before random walk) 
and a post-pruning (after random walk) method 
were introduced to deal with this problem.  
We used a naive pruning method which selects 
the top N phrase pairs in the phrase table. In our 
experiments, we set N to 20. For pre-pruning, we 
prune the SP phrase table and PT phrase table be-
fore applying random walks. Post-pruning means 
that we prune the ST phrase table after random 
walks. For the baseline system, we also apply a 
pruning method before combine the SP and PT 
phrase table. We test our pruning method on pt-en-
sv translation task. Table 6 shows the results. 
With a pre- and post-pruning method, the ran-
dom walk approach is able to achieve further im-
provements. Our approach achieved BLEU scores 
of 25.11, 24.69 and 24.34 on WMT06, WMT07 
and WMT08 respectively, which is much better 
than the baseline and the random walk approach 
with pruning.  Moreover, the size of the phrase 
table is about half of the no-pruning method. 
When adopting a post-pruning method, the per-
formance of translation did not improved signifi-
cantly over the pre-pruning, but the scale of the 
phrase table dropped to 69M, which is only about 
2 times larger than the triangulation method. 
Phrase table pruning is a key work to improve 
the performance of random walk. We plan to ex-
plore more approaches for phrase table pruning. 
E.g. using significance test (Johnson et al, 2007) 
or monolingual key phrases (He et al, 2009) to 
filter the phrase table. 
 
 
Table5. Experimental results on translating from Portu-
guese to Swedish via different pivot language. 
RW=Random Walk. * indicates the results are signifi-
cantly better than the baseline (p<0.05). 
 
 
Table6. Results of Phrase Table Filtering 
 
trans 
language 
WMT 
06 
WMT 
07 
WMT 
08 
Baseline 
RW 
pt-da-sv 
23.40 
24.47* 
22.80 
24.21* 
22.49 
23.75* 
Baseline 
RW 
pt-de-sv 
22.72 
23.12* 
22.21 
23.26* 
21.76 
22.35* 
Baseline 
RW 
pt-el-sv 
22.53 
23.75* 
22.19 
23.22* 
21.37 
22.40* 
Baseline 
RW 
pt-en-sv 
23.54 
24.66* 
23.24 
24.22* 
22.90 
23.90* 
Baseline 
RW 
pt-es-sv 
23.58 
24.65* 
23.37 
24.10* 
22.80 
23.77* 
Baseline 
RW 
pt-fi-sv 
21.06 
21.17 
20.06 
20.42* 
20.26 
20.28 
Baseline 
RW 
pt-fr-sv 
23.55 
24.75* 
23.09 
24.15* 
22.89 
23.96* 
Baseline 
RW 
pt-it-sv 
23.65 
24.74* 
22.96 
24.18* 
22.79 
24.02* 
Baseline 
RW 
pt-nl-sv 
21.87 
23.06* 
21.83 
22.76* 
21.36 
22.29* 
 WMT 
06 
WMT 
07 
WMT 
08 
Phrase 
Pairs # 
Baseline 
+pruning 
23.54 
24.05
* 
23.24 
23.70
* 
22.90 
23.59
* 
46M 
32M 
RW 
+pre-pruning 
+post-pruning 
24.66 
25.11 
25.19
* 
24.22 
24.69 
24.79
* 
23.90 
24.34 
24.41
* 
215M 
109M 
69M 
531
5.3 Experiments on Spoken Language 
The European languages show various degrees of 
similarity to one another. In this sub-section, we 
consider translation from Chinese to Spanish with 
English as the pivot language. Chinese belongs to 
Sino-Tibetan Languages and English/Spanish be-
longs to Indo-European Languages, the gap be-
tween two languages is wide. 
A pivot task was included in IWSLT 2008 in 
which the participants need to translate Chinese to 
Spanish via English. A Chinese-English and an 
English-Spanish data were supplied to carry out 
the experiments. The entire training corpus was 
tokenized and lowercased. Table 7 and Table 8 
summarize the training data and test data. 
Table 9 shows the similar tendency with Table 4. 
The random walk models achieved BLEU% scores 
32.09, which achieved an absolute improvement of 
2.08 percentages points on BLEU over the base-
line.   
 
Corpus 
Sentence 
pair # 
Source 
word # 
Target 
word # 
CE 20,000 135,518 182,793 
ES 19,972 153,178 147,560 
 
Table 7: Training Data of IWSLT2008 
 
Test Set Sentence # Reference # 
IWSLT08 507 16 
 
Table8. Test Data of IWSLT2008 
 
System BLEU% phrase pairs # 
Baseline 30.01 143,790 
+pruning 30.25 108,407 
RW 31.57 2,760,439 
+pre-pruning 31.99 1,845,648 
+post-pruning 32.09* 1,514,694 
 
Table9. Results on IWSLT2008 
5.4 Experiments on Web Data 
The setting with Europarl data is quite artificial as 
the training data for directly translating between 
source and target actually exists in the original 
data sets. The IWSLT data set is too small to rep-
resent the real scenario. Thus we try our experi-
ment on a more realistic scenario: translating from 
Chinese to Japanese via English with web crawled 
data. 
All the training data were crawled on the web. 
The scale of Chinese-English and English-
Japanese is 10 million respectively. The test set 
was built in house with 1,000 sentences and 4 ref-
erences. 
 
System BLEU% phrase pairs # 
Baseline 28.76 4.5G 
+pruning 28.90 273M 
RW 29.13 46G 
+pre-pruning 29.44 11G 
+post-pruning 29.51* 3.4G 
 
Table10. Results on Web Data 
 
Table 10 lists the results on web data. From the 
table we can find that the random walk model can 
achieve an absolute improvement of 0.75 percent-
ages points on BLEU over the baseline.  
In this subsection, the training data contains 
parallel sentences with different domains. And the 
two training corpora (Chinese-English and Eng-
lish-Japanese) are typically very different. It 
means that our random walk approach is robust in 
the realistic scenario. 
6 Discussions 
The random walk approach mainly improves the 
performance of pivot translation in two aspects: 
reduces the OOVs and provides more hypothesis 
phrases for decoding.  
6.1 OOV 
Out-of-vocabulary (OOV 7
We count the OOVs when decoding with trian-
gulation model and random walk model on 
IWSLT2008 data. The statistics shows that when 
using triangulation model, there are 11% OOVs 
when using triangulation model, compared with 
9.6% when using random walk model. Less OOV 
often lead to a better result. 
) terms cause serious 
problems for machine translation systems (Zhang 
et al, 2005). The random walk model can reduce 
the OOVs. As illustrated in Figure 1, the Chinese 
phrase ????henkekou? cannot be connected to 
any Spanish phrase, thus it is a OOV term.  
                                                          
7 OOV refer to phrases here. 
532
6.2 Hypothesis Phrases 
To illustrate how the random walk method helps 
improve the performance of machine translation, 
we illustrate an example as follows: 
 
- Source: ? ? ? ?? 
              wo xiang yao zhentou 
- Baseline trans: Quiero almohada 
- Random Walk trans: Quiero una almohada 
 
For translating a Chinese sentence ??????
wo xiang yao zhentou? to Spanish, we can get two 
candidate translations. In this case, the random 
walk translation is better than the baseline system. 
The key phrase in this sentence is ??? zhentou?, 
figure 5 shows the extension process. In this case, 
the article ?a? is hidden in the source-pivot phrase 
table. The same situation often occurs in articles 
and prepositions. Random walk is able to discover 
the hidden relations (hypothesis phrases) among 
source, pivot and target phrases. 
 
 
 
 
 
 
 
 
 
 
7 Conclusion and Future Work 
In this paper, we proposed a random walk method 
to improve pivot-based statistical machine transla-
tion. The random walk method can find implicit 
relations between phrases in the source and target 
languages. Therefore, more source-target phrase 
pairs can be obtained than conventional pivot-
based method. Experimental results show that our 
method achieves significant improvements over 
the baseline on Europarl corpus, spoken language 
data and the web data.  
A critical problem in the approach is the noise 
that may bring in. In this paper, we used a simple 
filtering to reduce the noise. Although the filtering 
method is effective, other method may work better. 
In the future, we plan to explore more approaches 
for phrase table pruning. 
Acknowledgments 
We would like to thank Jianyun Nie, Muyun Yang 
and Lemao Liu for insightful discussions, and 
three anonymous reviewers for many invaluable 
comments and suggestions to improve our paper. 
This work is supported by National Natural Sci-
ence Foundation of China (61100093), and the 
Key Project of the National High Technology Re-
search and Development Program of China 
(2011AA01A207). 
References  
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
597-604 
Sergey Brin and Lawrence Page. 1998. The Anatomy of 
a Large-Scale Hypertextual Web Search Engine. In 
Proceedings of the Seventh International World 
Wide Web Conference  
Trevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Make Effective Use of Mul-
ti-Parallel Corpora. In Proceedings of 45th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Rafael E. 
Banchs. 2011. Enhancing Scarce-Resource Language 
Translation through Pivot Combinations. In Proceed-
ings of the 5th International Joint Conference on 
Natural Language Processing, pages 1361-1365 
Nick Craswell and Martin Szummer. 2007. Random 
Walks on the Click Graph. In Proceedings of the 
30th annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 239-246 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao 
and Dequan Zheng. 2013. Phrase Table Combination 
Deficiency Analyses in Pivot-based SMT. In Pro-
ceedings of 18th International Conference on Appli-
cation of Natural Language to Information Systems, 
pages 355-358. 
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime 
Tsukada and Masaaki Nagata. 2011. Generalized 
Minimum Bayes Risk System Combination. In Pro-
ceedings of the 5th International Joint Conference 
on Natural Language Processing, pages 1356?1360 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francisco 
Casacuberta. 2011. Minimum Bayes-risk System 
Figure 5: Phrase extension process. The dotted line 
indicates an implicit relation in the phrase table. 
??? 
ge zhentou 
?? 
zhentou 
pillow 
a pillow 
almohada 
una 
almohada 
533
Combination. In Proceedings of the 49th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1268?1277 
Zhongjun He, Yao Meng, Yajuan L?, Hao Yu and Qun 
Liu. 2009. Reducing SMT Rule Table with Mono-
lingual Key Phrase. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 121-
124 
Howard Johnson, Joel Martin, George Foster, and Ro-
land Kuhn. 2007. Improving  translation quality by 
discarding most of the phrase table. In Proceedings 
of the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pages 967?975. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In HLT-NAACL: 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 127-133 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 388?395. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proceedings of 
MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics, demon-
stration session, pages 177?180. 
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine 
translation. In Proceedings of the 18th International 
Conference on Computational Linguistics, pages 
1086?1090 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computation Linguistics, pages 311-319 
Karl Pearson. 1905. The Problem of the Random Walk. 
Nature, 27(1865):294 
Mydans, Seth. 2011. Across cultures, English is the 
word. New York Times. 
Martin Szummer and Tommi Jaakkola. 2002. Partially 
Labeled Classification with Markov Random Walks. 
In Advances in Neural Information Processing Sys-
tems, pages 945-952 
Kristina Toutanova, Christopher D. Manning and An-
drew Y. Ng. 2004. Learning Random Walk Models 
for Inducting Word Dependency Distributions. In 
Proceedings of the 21st International Conference on 
Machine Learning.  
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical 
Machine Translation. In Proceedings of Human 
Language Technology: the Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 484-491 
Masao Utiyama, Andrew Finch, Hideo Okuma, Michael 
Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-
suda, and Eiichiro Sumita. 2008. The NICT/ATR 
speech Translation System for IWSLT 2008. In Pro-
ceedings of the International Workshop on Spoken 
Language Translation, pages 77-84 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, 
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. 
The TCH Machine Translation System for IWSLT 
2008. In Proceedings of the International Workshop 
on Spoken Language Translation, pages 124-131 
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proceedings of 45th Annual Meeting of the 
Association for Computational Linguistics, pages 
856-863.  
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. In 
Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th 
IJCNLP of the AFNLP, pages 154-162 
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proceedings of the 
27th ACM SIGIR. pages 524-525 
 
 
534
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57?67,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Policy Learning for Domain Selection in an Extensible Multi-domain
Spoken Dialogue System
Zhuoran Wang
Mathematical & Computer Sciences
Heriot-Watt University
Edinburgh, UK
zhuoran.wang@hw.ac.uk
Hongliang Chen, Guanchun Wang
Hao Tian, Hua Wu
?
, Haifeng Wang
Baidu Inc., Beijing, P. R. China
SurnameForename@baidu.com
?
wu hua@baidu.com
Abstract
This paper proposes a Markov Decision
Process and reinforcement learning based
approach for domain selection in a multi-
domain Spoken Dialogue System built on
a distributed architecture. In the proposed
framework, the domain selection prob-
lem is treated as sequential planning in-
stead of classification, such that confir-
mation and clarification interaction mech-
anisms are supported. In addition, it is
shown that by using a model parameter ty-
ing trick, the extensibility of the system
can be preserved, where dialogue com-
ponents in new domains can be easily
plugged in, without re-training the domain
selection policy. The experimental results
based on human subjects suggest that the
proposed model marginally outperforms a
non-trivial baseline.
1 Introduction
Due to growing demand for natural human-
machine interaction, over the last decade Spo-
ken Dialogue Systems (SDS) have been increas-
ingly deployed in various commercial applications
ranging from traditional call centre automation
(e.g. AT&T ?Lets Go!? bus information sys-
tem (Williams et al., 2010)) to mobile personal
assistants and knowledge navigators (e.g. Ap-
ple?s Siri
R
?
, Google Now
TM
, Microsoft Cortana,
etc.) or voice interaction for smart household ap-
pliance control (e.g. Samsung Evolution Kit for
Smart TVs). Furthermore, latest progress in open-
vocabulary Automatic Speech Recognition (ASR)
is pushing SDS from traditional single-domain in-
formation systems towards more complex multi-
domain speech applications, of which typical ex-
amples are those voice assistant mobile applica-
tions.
Recent advances in SDS have shown that sta-
tistical approaches to dialogue management can
result in marginal improvement in both the nat-
uralness and the task success rate for domain-
specific dialogues (Lemon and Pietquin, 2012;
Young et al., 2013). State-of-the-art statistical
SDS treat the dialogue problem as a sequential
decision making process, and employ established
planning models, such as Markov Decision Pro-
cesses (MDPs) (Singh et al., 2002) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Thomson and Young, 2010; Young et al., 2010;
Williams and Young, 2007), in conjunction with
reinforcement learning techniques (Jur?c???cek et al.,
2011; Jur?c???cek et al., 2012; Ga?si?c et al., 2013a)
to seek optimal dialogue policies that maximise
long-term expected (discounted) rewards and are
robust to ASR errors.
However, to the best of our knowledge, most of
the existing multi-domain SDS in public use are
rule-based (e.g. (Gruber et al., 2012; Mirkovic
and Cavedon, 2006)). The application of statistical
models in multi-domain dialogue systems is still
preliminary. Komatani et al. (2006) and Nakano
et al. (2011) utilised a distributed architecture (Lin
et al., 1999) to integrate expert dialogue systems in
different domains into a unified framework, where
a central controller trained as a data-driven clas-
sifier selects a domain expert at each turn to ad-
dress user?s query. Alternatively, Hakkani-T?ur et
al. (2012) adopted the well-known Information
State mechanism (Traum and Larsson, 2003) to
construct a multi-domain SDS and proposed a dis-
criminative classification model for more accurate
state updates. More recently, Ga?si?c et al. (2013b)
proposed that by a simple expansion of the kernel
function in Gaussian Process (GP) reinforcement
learning (Engel et al., 2005; Ga?si?c et al., 2013a),
one can adapt pre-trained dialogue policies to han-
dle unseen slots for SDS in extended domains.
In this paper, we use a voice assistant applica-
57
User Interface Manager
ASR
User Intention
Identier Central Controller
SLU NLG
Domain Expert
(Travel Info)
SLU NLG
Domain Expert
(Restaurant Search)
SLU NLG
Domain Expert
(Movie Search)
SLU NLG
Domain Expert
(etc.)
Web 
Search
Weather
Report
QA
etc.
Ou
t-o
f-d
om
ain
 Se
rvi
ce
s
Service
Ranker
Mobile Devices
Flight Ticket 
Booking
Train Ticket 
Booking
Hotel  
Booking
speech
text
text, clicks
query,
intention label,
condence
TTS Web PageRendering etc.
Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).
tion (similar to Apple?s Siri but in Chinese lan-
guage) as an example to demonstrate a novel
MDP-based approach for central interaction man-
agement in a complex multi-domain dialogue sys-
tem. The voice assistant employs a distributed ar-
chitecture similar to (Lin et al., 1999; Komatani et
al., 2006; Nakano et al., 2011), and handles mixed
interactions of multi-turn dialogues across differ-
ent domains and single-turn queries powered by
a collection of information access services (such
as web search, Question Answering (QA), etc.).
In our system, the dialogues in each domain are
managed by an individual domain expert SDS, and
the single-turn services are used to handle those
so-called out-of-domain requests. We use fea-
turised representations to summarise the current
dialogue states in each domain (see Section 3 for
more details), and let the central controller (the
MDP model) choose one of the following system
actions at each turn: (1) addressing user?s query
based on a domain expert, (2) treating it as an
out-of-domain request, (3) asking user to confirm
whether he/she wants to continue a domain ex-
pert?s dialogue or to switch to out-of-domain ser-
vices, and (4) clarifying user?s intention between
two domains. The Gaussian Process Temporal
Difference (GPTD) algorithm (Engel et al., 2005;
Ga?si?c et al., 2013a) is adopted here for policy op-
timisation based on human subjects, where a pa-
rameter tying trick is applied to preserve the ex-
tensibility of the system, such that new domain
experts (dialogue systems) can be flexibly plugged
in without the need of re-training the central con-
troller.
Comparing to the previous classification-based
methods (Komatani et al., 2006; Nakano et al.,
2011), the proposed approach not only has the
advantage of action selection in consideration of
long-term rewards, it can also yield more robust
policies that allow clarifications and confirmations
to mitigate ASR and Spoken Language Under-
standing (SLU) errors. Our human evaluation re-
sults show that the proposed system with a trained
MDP policy achieves significantly better natural-
ness in domain switching tasks than a non-trivial
baseline with a hand-crafted policy.
The remainder of this paper is organised as
follows. Section 2 defines the terminology used
throughout the paper. Section 3 briefly overviews
the distributed architecture of our system. The
MDP model and the policy optimisation algorithm
are introduced in Section 4 and Section 5, respec-
tively. After this, experimental settings and eval-
uation results are described in Section 6. Finally,
we discuss some possible improvements in Sec-
tion 7 and conclude ourselves in Section 8.
2 Terminology
A voice assistant application provides a unified
speech interface to a collection of individual infor-
mation access systems. It aims to collect and sat-
isfy user requests in an interactive manner, where
58
different types of interactions can be involved.
Here we focus ourselves on two interaction scenar-
ios, i.e. task-oriented (multi-turn) dialogues and
single-turn queries.
According to user intentions, the dialogue inter-
actions in our voice assistant system can further be
categorised into different domains, of which each
is handled by a separate dialogue manager, namely
a domain expert. Example domains include travel
information, restaurant search, etc. In addition,
some domains in our system can be further de-
composed into sub-domains, e.g. the travel in-
formation domain consists of three sub-domains:
flight ticket booking, train ticket booking and hotel
reservation. We use an integrated domain expert to
address queries in all its sub-domains, so that rel-
evant information can be shared across those sub-
domains to allow intelligent induction in the dia-
logue flow.
For convenience of future reference, we call
those single-turn information access systems out-
of-domain services or simply services for short.
The services integrated in our system include web
search, semantic search, QA, system command ex-
ecution, weather report, chat-bot, and many more.
3 System Architecture
The voice assistant system introduced in this pa-
per is built on a distributed architecture (Lin et al.,
1999), as shown in Figure 1, where the dialogue
flow is processed as follows. Firstly, a user?s query
(either an ASR utterance or directly typed in text)
is passed to a user intention identifier, which la-
bels the raw query with a list of intention hypothe-
ses with confidence scores. Here an intention label
could be either a domain name or a service name.
After this, the central controller distributes the raw
query together with its intention labels and confi-
dence scores to all the domain experts and the ser-
vice modules, which will attempt to process the
query and return their results to the central con-
troller.
The domain experts in the current implementa-
tion of our system are all rule-based SDS follow-
ing the RavenClaw framework proposed in (Bo-
hus and Rudnicky, 2009). When receiving a query,
a domain expert will use its own SLU module to
parse the utterance or text input and try to update
its dialogue state in consideration of both the SLU
output and the intention labels. If the dialogue
state in the domain expert can be updated given
the query, it will return its output, internal ses-
sion record and a confidence score to the central
controller, where the output can be either a natu-
ral language utterance realised by its Natural Lan-
guage Generation (NLG) module or a set of data
records obtained from its database (if a database
search operation is triggered), or both. If the do-
main expert cannot update its state using the cur-
rent query, it will just return an empty result with
a low confidence score. Similar procedures ap-
ply to those out-of-domain services as well, but
there are no session records or confidence scores
returned. Finally, given all the returned informa-
tion, the central controller chooses, according to
its policy, the module (either a domain expert or a
service) whose results will be provided to the user.
When the central controller decides to pass a
domain expert?s output to the user, we regard the
domain expert as being activated. Also note here,
the updated state of a domain expert in a turn will
not be physically stored, unless the domain expert
is activated in that turn. This is a necessary mech-
anism to prevent an inactive domain expert being
misled by ambiguous queries in other domains.
In addition, we use a well-engineered priority
ranker to rank the services based on the num-
bers of results they returned as well as some prior
knowledge about the quality of their data sources.
When the central controller decides to show user
the results from an out-of-domain service, it will
choose the top one from the ranked list.
4 MDP Modelling of the Central Control
Process
The main focus of this paper is to seek a policy for
robustly switching the control flow among those
domain experts and services (the service ranker in
practice) during a dialogue, where the user may
have multiple or compound goals (e.g. booking a
flight ticket, booking a restaurant in the destina-
tion city and checking the weather report of the
departure or destination city).
In order to make the system robust to ASR er-
rors or ambiguous queries, the central controller
should also have basic dialogue abilities for confir-
mation and clarification purposes. Here we define
the confirmation as an action of asking whether a
user wants to continue the dialogue in a certain do-
main. If the system receives a negative response at
this point, it will switch to out-of-domain services.
On the other hand, the clarification action is de-
59
fined between domains, in which case, the system
will explicitly ask the user to choose between two
domain candidates before continuing the dialogue.
Due to the confirmation and clarification mech-
anisms defined above, the central controller be-
comes a sequential decision maker that must take
the overall smoothness of the dialogue into ac-
count. Therefore, we propose an MDP-based ap-
proach for learning an optimal central control pol-
icy in this section.
The potential state space of our MDP is huge,
which in principle consists of the combinations of
all possible situations of the domain experts and
the out-of-domain services, therefore function ap-
proximation techniques must be employed to en-
able tractable computations. However, when de-
veloping such a complex application as the voice
assistant here, one also needs to take the extensi-
bility of the system into account, so that new do-
main experts can be easily integrated into the sys-
tem without major re-training or re-engineering of
the existing components. Essentially, it requires
the state featurisation and the central control pol-
icy learnt here to be independent of the number of
domain experts. In Section 4.3, we show that such
a property can be achieved by a parameter tying
trick in the definition of the MDP.
4.1 MDP Preliminaries
Let P
X
denote the set of probability distributions
over a set X . An MDP is defined as a five tuple
?S,A, T,R, ??, where the components are defined
as follows. S and A are the sets of system states
and actions, respectively. T : S ? A ? P
S
is the
transition function, and T (s
?
|s, a) defines the con-
ditional probability of the system transiting from
state s ? S to state s
?
? S after taking action
a ? A. R : S ? A ? P
R
is the reward function
with R(s, a) specifying the distribution of the im-
mediate rewards for the system taking action a at
state s. In addition, 0 ? ? ? 1 is the discount
factor on the summed sequence of rewards.
A finite-horizon MDP operates as follows. The
system occupies a state s and takes an action a,
which then will make it transit to a next state s
?
?
T (?|s, a) and receive a reward r ? R(s, a). This
process repeats until a terminal state is reached.
For a given policy pi : S ? A, the value
function V
pi
is defined to be the expected cumula-
tive reward, as V
pi
(s
0
) = E
[
?
n
t=0
?
t
r
t
|
s
t
,pi(s
t
)
]
,
where s
0
is the starting state and n is the plan-
ning horizon. The aim of policy optimisation is
to seek an optimal policy pi
?
that maximises the
value function. If T and R are given, in conjunc-
tion with a Q-function, the optimal value V
?
can
be expressed by recursive equations as Q(s, a) =
R(s, a) + ?
?
s
?
?S
T (s
?
|s, a)V
?
(s
?
) and V
?
(s) =
max
a?A
Q(s, a) (here we assume R(s, a) is de-
terministic), which can be solved by dynamic pro-
gramming (Bellman, 1957). For problems with
unknown T or R, such as dialogue systems, the
Q-values are usually estimated via reinforcement
learning (Sutton and Barto, 1998).
4.2 Problem Definition
Let D denote the set of the domain experts in our
voice assistant system, and s
d
be the current di-
alogue state of domain expert d ? D at a certain
timestamp. We also define s
o
as an abstract state to
describe the current status of those out-of-domain
services. Then mathematically we can represent
the central control process as an MDP, where its
state s is a joint set of the states of all the domain
experts and the services, as s = {s
d
}
d?D
? {s
o
}.
Four types of system actions are defined as fol-
lows.
? present(d): presenting the output of do-
main expert d to user;
? present ood(null): presenting the re-
sults of the top-ranked out-of-domain service
given by the service ranker;
? confirm(d): confirming whether user
wants to continue with domain expert d (or
to switch to out-of-domain services);
? clarify(d,d
?
): asking user to clarify
his/her intention between domains d and d
?
.
For convenience of notations, we use a(x) to
denote a system action of our MDP, where a ?
{present,present ood,confirm,clarify},
x ? {d,null, (d, d
?
)}
d,d
?
?D,d6=d
?
, x = null
only applies to present ood, and x = (d, d
?
)
only applies to clarify actions.
4.3 Function Approximation
Function approximation is a commonly used tech-
nique to estimate the Q-values when the state
space of the MDP is huge. Concretely, in our case,
we assume that:
Q(s, a(x)) = f(?(s, a(x)); ?) (1)
60
where ? : S ? A ? R
K
is a feature function
that maps a state-action pair to an K-dimensional
feature vector, and f : R
K
? R is a function of
?(s, a(x)) parameterised by ?. A frequent choice
of f is the linear function, as:
Q(s, a(x)) = ?
>
?(s, a(x)) (2)
After this, the policy optimisation problem be-
comes learning the parameter ? to approximate the
Q-values based on example dialogue trajectories.
However, a crucial problem with the standard
formulation in Eq. (2) is that the feature function
? is defined over the entire state and action spaces.
In this case, when a new domain expert is inte-
grated into the system, both the state space and the
action space will be changed, therefore one will
have to re-define the feature function and conse-
quentially re-train the model. In order to achieve
an extensible system, we make some simplifica-
tion assumptions and decompose the feature func-
tion as follows. Firstly, we let:
?(s, a(x)) = ?
a
(s
x
) (3)
=
?
?
?
?
?
?
?
?
pr
(s
d
) if a(x) =present(d)
?
ood
(s
o
) if a(x) =present ood()
?
cf
(s
d
) if a(x) =confirm(d)
?
cl
(s
d
, s
d
?
) if a(x) =clarify(d,d
?
)
where the feature function is reduced to only de-
pend on the state of the action?s operand, instead
of the entire system state. Then, we make those ac-
tions a(x) that have a same action type (a) but op-
erate different domain experts (x) share the same
parameter, i.e.:
Q(s, a(x)) = ?
>
a
?
a
(s
x
) (4)
This decomposition and parameter tying trick pre-
serves the extensibility of the system, because both
?
>
a
and ?
a
are independent of x, when there is a
new domain expert
?
d, we can directly substitute
its state s
?
d
into Eq. (3) and (4) to compute its cor-
responding Q-values.
4.4 Features
Based on the problem formulation in Eq. (3) and
(4), we shall only select high-level summary fea-
tures to sketch the dialogue state and dialogue his-
tory of each domain expert, which must be ap-
plicable to all domain experts, regardless of their
domain-specific characteristics or implementation
differences. Suppose that the dialogue states of the
# Feature Range
1
the number of unfilled
required slots of a domain
expert
{0, . . . ,M}
2
the number of filled required
slots of a domain expert
{0, . . . ,M}
3
the number of filled optional
slots of a domain expert
{0, . . . , L}
4
whether a domain expert has
executed a database search
{0, 1}
5
the confidence score
returned by a domain expert
[0, 1.2]
6
the total number of turns that
a domain expert has been
activated during a dialogue
Z
+
7
e
?t
a
where t
a
denotes the
relative turn of a domain
expert being last activated,
or 0 if not applicable
[0, 1]
8
e
?t
c
where t
c
denotes the
relative turn of a domain
expert being last confirmed,
or 0 if not applicable
[0, 1]
9
the summed confidence
score from the user intention
identifier of a query being
for out-of-domain services
[0, 1.2N ]
Table 1: A list of all features used in our model.
M and L respectively denote the maximum num-
bers of required and optional slots for the domain
experts. N is the maximum number of hypotheses
that the intention identifier can return. Z
+
stands
for the non-negative integer set.
domain experts can be represented as slot-value
pairs
1
, and for each domain there are required slots
and optional slots, where all required slots must
be filled before the domain expert can execute a
database search operation. The features investi-
gated in the proposed framework are listed in Ta-
ble 1.
Detailed featurisation in Eq. (3) is explained
as follows. For ?
pr
, we choose the first 8 fea-
tures plus a bias dimension that is always set to
1
This is a rather general assumption. Informally speak-
ing, for most task-oriented SDS, one can extract a slot-value
representation from their dialogue models, of which exam-
ples include the RavenClaw architecture (Bohus and Rud-
nicky, 2009), the Information State dialogue engine (Traum
and Larsson, 2003), MDP-SDS (Singh et al., 2002) or
POMDP-SDS (Thomson and Young, 2010; Young et al.,
2010; Williams and Young, 2007).
61
?1. Whilst, feature #9 plus a bias is used to de-
fine ?
ood
. All the features are used in ?
cf
, as to
do a confirmation, one needs to consider the joint
situation in and out of the domain. Finally, the
feature function for a clarification action between
two domains d and d
?
is defined as ?
cl
(s
d
, s
d
?
) =
exp{?|?
pr
(s
d
) ? ?
pr
(s
d
?
)|}, where we use | ? |
to denote the element-wise absolute of a vector
operand. The intuition here is that the more dis-
tinguishable the (featurised) states of two domain
experts are, the less we tend to clarify them.
For those domain experts that have multiple
sub-domains with different numbers of required
and optional slots, the feature extraction procedure
only applies to the latest active sub-domain.
In addition, note that, the confidence scores pro-
vided by the user intention identifier are only used
as features for out-of-domain services. This is be-
cause in the current version of our system, the con-
fidence estimation of the intention identifier for
domain-dependent dialogue queries is less reliable
due to the lack of context information. In contrast,
the confidence scores returned by the domain ex-
perts will be more informative at this point.
5 Policy Learning with GPTD
In traditional statistical SDS, dialogue policies are
usually trained using reinforcement learning based
on simulated dialogue trajectories (Schatzmann
et al., 2007; Keizer et al., 2010; Thomson and
Young, 2010; Young et al., 2010). Although the
evaluation of the simulators themselves could be
an arguable issue, there are various advantages,
e.g. hundreds of thousands of data examples can
be easily generated for training and initial policy
evaluation purposes, and different reinforcement
learning models can be compared without incur-
ring notable extra costs.
However, for more complex multi-domain SDS,
particularly a voice assistant application like ours
that aims at handling very complicated (ideally
open-domain) dialogue scenarios, it would be dif-
ficult to develop a proper simulator that can rea-
sonably mimic real human behaviours. There-
fore, in this work, we learn the central control
policy directly with human subjects, for which
the following properties of the learning algorithm
are required. Firstly and most importantly, the
learner must be sample-efficient as the data collec-
tion procedure is costly. Secondly, the algorithm
should support batch reinforcement learning. This
is because when using function approximation, the
learning process may not strictly converge, and the
quality of the sequence of generated policies tends
to oscillate after a certain number of improving
steps at the beginning (Bertsekas and Tsitsiklis,
1996). If online reinforcement learning is used,
we will be unable to evaluate the generated policy
after each update, and hence will not know which
policy to keep for the final evaluation. Therefore,
we do a batch policy update and iterate the learn-
ing process for a number of batches, such that the
data collection phase in a new iteration yields an
evaluation of the policy obtained from the previ-
ous iteration at the same time.
To fulfill the above two requirements, the Gaus-
sian Process Temporal Difference (GPTD) algo-
rithm (Engel et al., 2005) is a proper choice, due to
its sample efficiency (Fard et al., 2011) and batch
learning ability (Engel et al., 2005), as well as its
previous success in dialogue policy learning with
human subjects (Ga?si?c et al., 2013a). Note that,
GPTD can also admit recursive (online) compu-
tations, but here we focus ourselves on the batch
version.
A Gaussian Process (GP) is a generative model
of Bayesian inference that can be used for func-
tion regression, and has the superiority of obtain-
ing good posterior estimates with just a few obser-
vations (Rasmussen and Williams, 2006). GPTD
models the Q-function as a zero mean GP which
defines correlations in different parts of the fea-
turised state and action spaces through a kernel
function ?, as:
Q(s, a(x)) ? GP(0, ?((s
x
, a), (s
x
, a))) (5)
Given a sequence of t state-action pairs X
t
=
[(s
0
, a
0
(x
0
)), . . . , (s
t
, a
t
(x
t
))] from a collection
of dialogues and their corresponding immedi-
ate rewards r
t
= [r
0
, . . . , r
t
], the posterior of
Q(s, a(x)) for an arbitrary new state-action pair
(s, a(x)) can be computed as:
Q(s, a(x))|
X
t
,r
t
? N
(
?
Q(s, a(x)), cov (s, a(x))
)
(6)
?
Q(s, a(x)) = k
t
(s
x
, a)
>
H
>
t
G
?1
t
r
t
(7)
cov (s, a(x)) = ?((s
x
, a), (s
x
, a))
? k
t
(s
x
, a)
>
H
>
t
G
?1
t
H
t
k
t
(s
x
, a) (8)
G
t
= H
t
K
t
H
>
t
+ ?
2
H
t
H
>
t
(9)
62
Ht
=
?
?
?
?
?
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 ? ? ? 0 1 ??
?
?
?
?
?
(10)
where K
t
is the Gram matrix with elements
K
t
(i, j) = ?((s
i
x
i
, a
i
), (s
j
x
j
, a
j
)), k
t
(s
x
, a) =
[?((s
i
x
i
, a
i
), (s
x
, a))]
t
i=0
is a vector, and ? is a
hyperparameter specifying the diagonal covari-
ance values of the zero-mean Gaussian noise. In
addition, we use cov (s, a(x)) to denote (for short)
the self-covariance cov (s, a(x), s, a(x)).
In our case, as different feature functions ?
a
are
defined for different action types, the kernel func-
tion is defined to be:
?((s
x
, a), (s
?
x
?
, a
?
)) = [[a = a
?
]]?
a
(s
x
, s
?
x
?
) (11)
where [[?]] is an indicator function and ?
a
is the ker-
nel function defined corresponding to the feature
function ?
a
.
Given a state, a most straightforward policy is
to select the action that corresponds to the max-
imum mean Q-value estimated by the GP. How-
ever, since the objective is to learn the Q-function
associated with the optimal policy by interacting
directly with users, the policy must exhibit some
form of stochastic behaviour in order to explore
alternatives during the process of learning. In this
work, the strategy employed for the exploration-
exploitation trade-off is that, during exploration,
actions are chosen according to the variance of
the GP estimate for the Q-function, and during
exploitation, actions are chosen according to the
mean. That is:
pi(s) =
{
arg max
a(x)
?
Q(s, a(x)) : w.p. 1? 
arg max
a(x)
cov (s, a(x)) : w.p. 
(12)
where 0 <  < 1 is a pre-defined exploration rate,
and will be exponentially reduced at each batch
iteration during our learning process.
Note that, in practice, not all the actions are
valid at every possible state. For example, if a do-
main expert d has never been activated during a
dialogue and can neither process the user?s current
query, the actions with an operand d will be re-
garded as invalid at this state. When executing the
policy, we only consider those valid actions for a
given state.
Score Interpretation
5
The domain selections are totally
correct, and the entire dialogue flow
is fluent.
4
The domain selections are totally
correct, but the dialogue flow is
slightly redundant.
3
There are accidental domain
selections errors, or the dialogue
flow is perceptually redundant.
2
There are frequent domain selections
errors, or the dialogue flow is
intolerably redundant.
1
Most domain selections are
incorrect, or the dialogue is
incompletable.
Table 2: The scoring standard in our experiments.
6 Experimental Results
6.1 Training
We use the batch version of GPTD as described
in Section 5 to learn the central control policy
with human subjects. There are three domain ex-
perts available in our current system, but during
the training only two domains are used, which are
the travel information domain and the restaurant
search domain. We reserve a movie search domain
for evaluating the generalisation property of the
learnt policy (see Section 6.2). The learning pro-
cess started from a hand-crafted policy. Then 15
experienced users
2
volunteered to contribute dia-
logue examples with multiple or compound goals
(see Figure 4 for an instance), from whom we
collected around 50?70 dialogues per day for 5
days
3
. After each dialogue, the users were asked
to score the system from 5 to 1 according to a scor-
ing standard shown in Table 2. The scores were
taken as the (delayed) rewards to train the GPTD
model, where we set the rewards for intermediate
turns to 0. The working policy was updated daily
based on the data obtained up to that day. The
data collected on the first day was used for pre-
liminary experiments to choose the hyperparame-
2
Overall user satisfactions may rely on various aspects of
the entire system, e.g. the data source quality of the services,
the performance of each domain expert, etc. It will be diffi-
cult to make non-experienced users to score the central con-
troller isolatedly.
3
Not all the users participated the experiments everyday.
There were 311 valid dialogues received in total, with an av-
erage length of 9 turns.
63
2	 ?
3	 ?
4	 ?
5	 ?
Figure 2: Average scores and standard deviations
during policy iteration.
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
0.9	 ?
Figure 3: Domain selection accuracies during pol-
icy iteration.
ters of the model, such as the kernel function, the
kernel parameters (if applicable), and ? and ? in
the GPTD model. We initially experimented with
linear, polynomial and Gaussian kernels, with dif-
ferent configurations of ? and ? values, as well
as kernel parameter values. It was found that
the linear kernel in combination with ? = 5 and
? = 0.99 works more appropriate than the other
settings. This configuration was then fixed for the
rest iterations.
The learning process was iterated for 4 days af-
ter the first one. On each day, we computed the
mean and standard deviation of the user scores
as an evaluation of the policy learnt on the pre-
vious day. The learning curve is illustrated in Fig-
ure 2. Note here, as we were actually executing a
stochastic policy according to Eq. (12), to calcu-
late the values in Figure 2 we ignored those dia-
logues that contain any actions selected due to the
exploration. We also manually labelled the cor-
rectness of domain selection at every turn of the
dialogues. The domain selection accuracies of the
obtained policy sequence are shown in Figure 3,
where similarly, those exploration actions as well
Policy
Scenario
Baseline GPTD
p-value
(i) 4.5?0.8 4.2?0.8 0.387
(ii) 3.4?0.9 4.2?0.8 0.018
(iii) 4.1?1.0 4.3?1.0 0.0821
(iv) 3.9?1.1 4.5?0.8 0.0440
Table 3: Paired comparison experiments between
the system with a trained GPTD policy and the
rule-based baseline.
as the clarification and confirmation actions were
excluded from the calculations. Although the do-
main selection accuracy is not the target that our
learning algorithm aims to optimise, it reflects the
quality of the learnt policies from a different angle
of view.
It can be found in Figure 2 that the best policy
was obtained in the third iteration, and after that
the policy quality oscillated. The same finding is
indicated in Figure 3 as well, when we use the do-
main selection accuracy as the evaluation metric.
Therefore, we kept the policy corresponding to the
peak point of the learning curve for the compari-
son experiments below.
6.2 Comparison Experiments
We conducted paired comparison experiments in
four scenarios to compare between the system
with the GPTD-learnt central control policy and a
non-trivial baseline. The baseline is a publicly de-
ployed version of the voice assistant application.
The central control policy of the baseline system is
handcrafted, which has a separate list of semantic
matching rules for each domain to enable domain
switching.
The first two scenarios aim to test the perfor-
mance of the two systems on (i) switching between
a domain expert and out-of-domain services, and
(ii) switching between two domain experts, where
only the two training domains (travel information
and restaurant search) were considered. Scenar-
ios (iii) and (iv) are similar to scenarios (i) and (ii)
respectively, but at this time, the users were re-
quired to carry out the tests surrounding the movie
search domain (which is addressed by a new do-
main expert not used in the training phase). There
were 13 users who participated this experiment.
In each scenario, every user was required to test
the two systems with an identical goal and similar
queries. After each test, the users were asked to
64
score the two systems separately according to the
scoring standard in Table 2.
The average scores received by the two systems
are shown in Table 3, where we also compute the
statistical significance (the p-values) of the results
based on paired t-tests. It can be found that the
learnt policy works significantly better than the
rule-based policy in scenarios (ii) and (iv), but in
scenarios (i) and (iii) the differences between two
systems are statistically insignificant. Moreover,
the learnt policy preserves the extensibility of the
entire system as expected, of which strong evi-
dences are given by the results in scenarios (iii)
and (iv).
6.3 Policy Analysis
To better understand the policy learnt by the
GPTD model, we look into the obtained weight
vectors, as shown in Table 4. It can be found that
confidence score (#5) is an informative feature for
all the system actions, while the relative turn of a
domain being last activated (#7) is an additional
strong evidence for a confirmation decision. In
addition, the similarity between the dialogue com-
pletion status (#1 & #2) of two ambiguous domain
experts and the relative turns of them being last
confirmed (#8) tend to be extra dominating fea-
tures for clarification decisions, besides the close-
ness of the confidence scores returned by the two
domain experts.
A less noticeable but important phenomenon is
observed for feature #6, i.e. the total number of
active turns of a domain expert during a dialogue.
Concretely, feature #6 has a small negative effect
on presentation actions but a small positive con-
tribution to confirmation actions. Such weights
could correspond to the discount factor?s penalty
to long dialogues in the value function. How-
ever, it implies an unexpected effect in extreme
cases, which we explain in detail as follows. Al-
though the absolute weights for feature #6 are tiny
for both presentation and confirmation actions, the
feature value will grow linearly during a dialogue.
Therefore, when a dialogue in a certain domain
last rather long, there tend to be very frequent con-
firmations. A possible solution to this problem
could be either ignoring feature #6 or twisting it to
some nonlinear function, such that its value stops
increasing at a certain threshold point. In addition,
to cover sufficient amount of those ?extreme? ex-
amples in the training phase could also be an alter-
Feature Weights
#
present confirm clarify
1 0.09 0.02 0.60
p
r
e
s
e
n
t
o
o
d
2 0.20 0.29 0.53
3 0.18 0.29 0.16
4 -0.10 0.16 0.25
5 0.75 0.57 0.54
6 -0.02 0.11 0.13
7 0.25 1.19 0.36
8 -0.22 -0.19 0.69
9 ? 0.20 ? 0.47
Bias -1.79 ? ? -2.42
Table 4: Feature weights learnt by GPTD. See Ta-
ble 1 for the meanings of the features.
native solution, as our current training set contains
very few examples that exhibit extraordinary long
domain persistence.
7 Further Discussions
The proposed approach is a rather general frame-
work to learn extensible central control policies
for multi-domain SDS based on distributed archi-
tectures. It does not rely on any internal represen-
tations of those individual domain experts, as long
as a unified featurisation of their dialogue states
can be achieved.
However, from the entire system point of view,
the current implementation is still preliminary.
Particularly, the confirmation and clarification
mechanisms are isolated, for which the surface re-
alisations sometimes may sound stiff. This phe-
nomenon explains one of the reasons that make
the proposed system slightly less preferred by the
users than the baseline in scenario (i), when the
interaction flows are relative simple. A possi-
ble improvement here could be associating the
confirmation and clarification actions in the cen-
tral controller to the error handling mechanisms
within each domain expert, and letting domain ex-
perts generate their own utterances on receiving a
confirmation/clarification request from the central
controller.
Online reinforcement learning with real user
cases will be another undoubted direction of fur-
ther improvement of our system. The key chal-
lenge here is to automatically estimate user?s satis-
factions, which will be transformed to the rewards
for the reinforcement learners. Strong feedbacks
such as clicks or actual order placements can be
65
collected. But to regress user?s true satisfaction,
other environment features must also be taken into
account. Practical solutions are still an open issue
at this stage, and are left to our future work.
8 Conclusion
In this paper, we introduce an MDP framework
for learning domain selection policies in a com-
plex multi-domain SDS. Standard problem for-
mulation is modified with tied model parameters,
so that the entire system is extensible and new
domain experts can be easily integrated without
re-training the policy. This expectation is con-
firmed by empirical experiments with human sub-
jects, where the proposed system marginally beats
a non-trivial baseline and demonstrates proper ex-
tensibility. Several possible improvements are dis-
cussed, which will be the central arc of our future
research.
Acknowledgements
The research in this paper is supported by China?s
973 Programme no. 2014CB340505. The first au-
thor is partially funded by the EC FP7 programme
under grant agreement no. 287615 (PARLANCE)
and a SICSA PECE grant. The authors would also
like to thank Qiaoqiao She, Duo Cai and the HCI-
APP group at Baidu for volunteering to participate
in the human subject experiments.
References
Richard Ernest Bellman. 1957. Dynamic Program-
ming. Princeton University Press, Princeton, NJ.
Dimitri P. Bertsekas and John N. Tsitsiklis. 1996.
Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Dan Bohus and Alexander I. Rudnicky. 2009. The
RavenClaw dialog management framework: Archi-
tecture and systems. Computer Speech and Lan-
guage, 23(3):332?361.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference
on Machine Learning (ICML), pages 201?208.
Mahdi Milani Fard, Joelle Pineau, and Csaba
Szepesv?ari. 2011. PAC-Bayesian policy evaluation
for reinforcement learning. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelli-
gence (UAI), pages 195?202.
USER/ASR:      ?????? (Flight ticket to Qingdao.)
SYSTEM: ?????????????????????        ???????        (Okay, flight ticket to Qingdao. When are you        departing, and where are you departing from?)     [Travel Info. Domain Expert]
USER/ASR:  ???? (I?m leaving tomorrow.)
SYSTEM: ???????????????????????     (Okay, flight ticket to Qingdao, leaving         tomorrow. Where are you departing from?)      [Travel Info. Domain Expert]
USER/ASR:  ?? (Beijing)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER:    ??????????        (Are there any good Sichuan cuisines in Qingdao?)ASR:       ???????????        (ungrammatical utterance, partially with     ?delicious? and ?Sichuan food?.)
SYSTEM: ?????????    (Do you want to continue with flight ticket booking?)    [Confirmation - Central Controller]
USER:      ?? (No)
SYSTEM: [Web Search Service: ?????????????     ]
USER/ASR:  ????? (I?m going to Qingdao tomorrow.)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER/ASR:  ????????? (Restaurants near Qingdao Airport)
SYSTEM: ????????   (I found the following restaurants for you.)        [Results Displayed     - Restaurant Search Domain Expert]
USER/ASR:  ????????? (Will it rain in Qingdao tomorrow?)
SYSTEM: ?????????16??20????5-6?     (Qingdao is rainy tomorrow. The temperature is 16 to 20         degree centigrade with moderate east wind.)     [Results Displayed     - Weather Report Service]
Figure 4: An example dialogue containing multi-
ple user goals. The icons stand for graphical user
interface based nonverbal outputs.
Milica Ga?si?c, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013a. On-
line policy optimisation of Bayesian spoken dia-
logue systems via human interaction. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8367?8371.
Milica Ga?si?c, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013b.
POMDP-based dialogue manager adaptation to ex-
tended domains. In Proceedings of the 14th annual
SIGdial Meeting on Discourse and Dialogue, pages
214?222.
Thomas Robert Gruber, Adam John Cheyer, Dag
66
Kittlaus, Didier Rene Guzzoni, Christopher Dean
Brigham, Richard Donald Giuli, Marcello Bastea-
Forte, and Harry Joseph Saddler. 2012. Intelligent
automated assistant. United States Patent No. US
20120245944 A1.
Dilek Z. Hakkani-T?ur, Gokhan T?ur, Larry P. Heck,
Ashley Fidler, and Asli C?elikyilmaz. 2012. A dis-
criminative classification-based approach to infor-
mation state updates for a multi-domain dialog sys-
tem. In Proceedings of the 13th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH).
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2011. Natural actor and belief critic: Reinforcement
algorithm for learning parameters of dialogue sys-
tems modelled as POMDPs. ACM Transactions on
Speech and Language Processing, 7(3):6:1?6:25.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech & Language, 26(3):168?192.
Simon Keizer, Milica Ga?si?c, Filip Jur?c???cek, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Parameter estimation for agenda-
based user simulation. In Proceedings of the 11th
annual SIGdial Meeting on Discourse and Dialogue,
pages 116?123.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, pages 9?17.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-Driven Methods for Adaptive Spoken Dia-
logue Systems: Computational Learning for Conver-
sational Interfaces. Springer.
Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.
1999. A distributed architecture for cooperative
spoken dialogue agents with coherent dialogue state
and history. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop
(ASRU).
Danilo Mirkovic and Lawrence Cavedon. 2006. Di-
alogue management using scripts. United States
Patent No. US 20060271351 A1.
Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko
Matsuyama, Kotaro Funakoshi, and Hiroshi G.
Okuno. 2011. A two-stage domain selection frame-
work for extensible multi-domain spoken dialogue
systems. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue, pages 18?29.
Carl Edward Rasmussen and Christopher K. I.
Williams, editors. 2006. Gaussian Processes for
Machine Learning. MIT Press.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 149?152.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16(1):105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
David R. Traum and Staffan Larsson. 2003. The In-
formation State approach to dialogue management.
In Jan van Kuppevelt and Ronnie W. Smith, editors,
Current and New Directions in Discourse and Dia-
logue, pages 325?353. Springer.
Jason D. Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. 2010. Demonstration of AT&T ?Let?s Go?:
A production-grade statistical spoken dialog system.
In Proceedings of the 3rd IEEE Workshop on Spoken
Language Technology (SLT).
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1?20.
67
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
Haiyang Wu
1
Daxiang Dong
1
Wei He
1
Xiaoguang Hu
1
Dianhai Yu
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
Abstract
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding?s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
1 Introduction
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase??? jieguo? can be trans-
lated to either ?results?, ?eventually? or ?fruit?,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
2 Related Work
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words? semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
142
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad?o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli?c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
3 Context-Sensitive Bilingual Semantic
Embedding Model
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase?s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
3.1 Bilingual Word Embedding
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
f(x,y; W,U) = cos(W
T
x,U
T
y) (1)
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ? R
|X|?k
,U ? R
|Y|?k
are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X| and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X|+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x,y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from W
T
x and target phrase embed-
ding can be obtained through U
T
y.
3.2 Context Sensitive Features
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase?s con-
text. Features are then extracted from the focused
phrase?s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase?s context. The word fruit
Figure 1: Feature extraction and label generation
143
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of ??. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
3.3 Parameter Learning
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
I(?) =
1
m
m
?
i=1
(? ? f(x
i
, y
i
; ?)? f(x
i
, y
?
i
; ?))+
(2)
Where f(x
i
,y
i
) is previously defined, ? =
{W,U} and + means max-margin hinge loss. In
our implementation, a margin of ? = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
? := ?? ?
?l(?)
??
(3)
where ? = {W,U}. Given an instance with pos-
itive and negative label pair {x,y,y
?
}, gradients
of parameter W and U are as follows:
?l(W,U)
?W
= qsx(W
T
x)
T
? pqs
3
x(U
T
y) (4)
?l(W,U)
?U
= qsy(U
T
y)
T
? pqs
3
y(W
T
x) (5)
Where we set p = (W
T
x)
T
(U
T
y), q =
1
||W
T
x||
2
and s =
1
||U
T
y||
2
. To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x,y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
? Get the focused phrase as well as its context in the
source sentence.
? Extract features from the focused phrase?s context.
? Get translation candidate extracted from phrase pairs of
the focused phrase.
? Compute scores for any pair of the focused phrase and
a candidate phrase.
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
5 Experiment
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
5.1 Data set
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task
1
. The parallel training corpus
1
LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
144
Method
OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
Table 1: Results of lowercase BLEU on NIST08
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(? < 0.05 or
? < 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
5.2 Results and Analysis
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
Source Sentence
4 Nearest Neighbor from
bilingual embedding
??????????
?????????
?????(Investors
can only get down to
business in a stable so-
cial environment)
will be, can only, will, can
??????????
?????????
?????(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
skills, ability, abilities, tal-
ent
??????????
?????????
????(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
fruit, outcome of, the out-
come, result
? ? ??????
???????(As
a result, Eastern District
Council passed a pro-
posal)
in the end, eventually, as a
result, results
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
word senses. And we also observe from the word
??? jieguo? that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
6 Conlusion
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
Acknowledgments
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
145
References
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39?48, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921?929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520?527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106?116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
146
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147?152,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Transformation from Discontinuous to Continuous Word Alignment
Improves Translation Quality
Zhongjun He
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
{hezhongjun,wu hua,wanghaifeng}@baidu.com
tliu@ir.hit.edu.cn
Abstract
We present a novel approach to im-
prove word alignment for statistical ma-
chine translation (SMT). Conventional
word alignment methods allow discontin-
uous alignment, meaning that a source
(or target) word links to several target (or
source) words whose positions are dis-
continuous. However, we cannot extrac-
t phrase pairs from this kind of align-
ments as they break the alignment con-
sistency constraint. In this paper, we use
a weighted vote method to transform dis-
continuous word alignment to continuous
alignment, which enables SMT system-
s extract more phrase pairs. We carry
out experiments on large scale Chinese-
to-English and German-to-English trans-
lation tasks. Experimental results show
statistically significant improvements of
BLEU score in both cases over the base-
line systems. Our method produces a gain
of +1.68 BLEU on NIST OpenMT04 for
the phrase-based system, and a gain of
+1.28 BLEU on NIST OpenMT06 for the
hierarchical phrase-based system.
1 Introduction
Word alignment, indicating the correspondence
between the source and target words in bilingual
sentences, plays an important role in statistical
machine translation (SMT). Almost all of the SMT
models, not only phrase-based (Koehn et al.,
2003), but also syntax-based (Chiang, 2005; Liu
et al., 2006; Huang et al., 2006), derive translation
knowledge from large amount bilingual text anno-
tated with word alignment. Therefore, the quality
of the word alignment has big impact on the qual-
ity of translation output.
Word alignments are usually automatically ob-
tained from a large amount of bilingual training
corpus. The most widely used toolkit for word
alignment in SMT community is GIZA++ (Och
and Ney, 2004), which implements the well known
IBM models (Brown et al., 1993) and the HM-
M model (Vogel and Ney, 1996). Koehn et al.
(2003) proposed some heuristic methods (e.g. the
?grow-diag-final? method) to refine word align-
ments trained by GIZA++. Another group of word
alignment methods (Liu et al., 2005; Moore et
al., 2006; Riesa and Marcu, 2010) define feature
functions to describe word alignment. They need
manually aligned bilingual texts to train the mod-
el. However, the manually annotated data is too
expensive to be available for all languages. Al-
though these models reported high accuracy, the
GIZA++ and ?grow-diag-final? method are domi-
nant in practice.
However, automatic word alignments are usu-
ally very noisy. The example in Figure 1 shows
a Chinese and English sentence pair, with word
alignment automatically trained by GIZA++ and
the ?grow-diag-final? method. We find many er-
rors (dashed links) are caused by discontinuous
alignment (formal definition is described in Sec-
tion 2), a source (or target) word linking to sev-
eral discontinuous target (or source) words. This
kind of errors will result in the loss of many use-
ful phrase pairs that are learned based on bilingual
word alignment. Actually, according to the defini-
tion of phrases in a standard phrase-based model,
we cannot extract phrases from the discontinuous
alignment. The reason is that this kind of align-
ment break the alignment consistency constrain-
t (Koehn et al., 2003). For example, the Chi-
147
1{I
meiguo
2
?
shi
3
?
shaoshu
4
A?
jige
5
?
tou
6
e
xia
7
??
fandui
8
?
piao
9

de
10
I[
guojia
11
??
zhiyi
The
1
United
2
States
3
was
4
among
5
the
6
handful
7
of
8
nations
9
that
10
cast
11
a
12
nay
13
note
14
Figure 1: An example of word alignment between a Chinese and English sentence pair. The dashed links
are incorrect alignments.
nese word ?shi
2
?
1
is aligned to the English words
?was
4
? and ?that
10
?. However, these two English
words are discontinuous, and we cannot extract the
phrase pair ?(shi, was)?.
In this paper, we propose a simple weighed vote
method to deal with the discontinuous word align-
ment. Firstly, we split the discontinuous align-
ment into several continuous alignment group-
s, and consider each continuous alignment group
as a bucket. Secondly, we vote for each buck-
et with alignment score measured by word trans-
lation probabilities. Finally, we select the buck-
et with the highest score as the final alignment.
The strength of our method is that we refine word
alignment without using any external knowledge,
as the word translation probabilities can be esti-
mated from the bilingual corpus with the original
word alignment.
We notice that the discontinuous alignment is
helpful for hierarchical phrase-based model, as the
model allows discontinuous phrases. Thus, for
the hierarchical phrase-based model, our method
may lost some discontinuous phrases. To solve
the problem, we keep the original discontinuous
alignment in the training corpus.
We carry out experiment with the state-of-the-
art phrase-based and hierarchical phrase-based
(Chiang, 2005) SMT systems implemented in
Moses (Koehn et al., 2007). Experiments on large
scale Chinese-to-English and German-to-English
translation tasks demonstrate significant improve-
ments in both cases over the baseline systems.
2 The Weighted Vote Method
To refine the discontinuous alignment, we propose
a weighted vote method to transform discontinu-
ous alignment to continuous alignment by discard-
ing noisy links. We split discontinuous alignment
1
The subscript denotes the word position.
into several continuous groups, and select the best
group with the highest score computed by word
translation probabilities as the final alignment.
For further understanding, we first describe
some definitions. Given a word-aligned sentence
pair (F
I
1
, E
J
1
, A), an alignment set A
set
(i) is the
set of target word positions that aligned to the
source word F
i
i
:
A
set
(i) = {j|(i, j) ? A} (1)
For example, in Figure 1, the alignment set
for the Chinese word ?shaoshu
3
? is A
set
(3) =
{5, 7, 8, 10}. We define an alignment s-
pan A
span
(i) as [min(A
set
(i)),max(A
set
(i))].
Thus, the alignment span for the Chinese word
?shaoshu
3
? is A
span
(3) = [5, 10].
The alignment for F
i
i
is discontinuous if there
exist some target words in A
span
(i) linking to an-
other source word, i.e. ?(i
?
, j
?
) ? A, where i
?
6= i,
j
?
? A
span
(i). Otherwise, the alignment is contin-
uous. According to the definition, the alignment
for ?shaoshu
3
? is discontinuous. Because the tar-
get words ?the
6
? and ?nations
9
? in the alignmen-
t span link to another Chinese words ?de
9
? and
?guojia
10
?, respectively. For a target word E
j
j
, the
definition is similar.
If the alignment for F
i
i
is discontinuous, we
can split the alignment span A
span
(i) = [j
1
, j
2
]
into m continuous spans {[j
k
p
, j
k
q
]}, where k =
1, 2, ...,m, and j
k
p
, j
k
q
? [j
1
, j
2
]. Our goal is to se-
lect the best continuous span for the word F
i
i
. To
do this, we score each continuous span with word
translation probabilities:
S([j
k
p
, j
k
q
]) =
q
?
t=p
(Pr(E
j
k
t
|F
i
) + Pr(F
i
|E
j
k
t
))
(2)
where,
Pr(f |e) =
count(f, e)
?
f
?
count(f
?
, e)
(3)
148
am
o
n
g
t
h
e
h
a
n
d
f
u
l
o
f
n
a
t
i
o
n
s
t
h
a
t
?? shaoshu 0.1 0.5 0.2 0.1
Figure 2: An example of weighted voted method
for selecting the best continuous alignment from
the discontinuous alignment. The heavy shading
area is selected as the final alignment.
Pr(e|f) =
count(e, f)
?
e
?
count(f, e
?
)
(4)
The word translation probabilities can be comput-
ed from the bilingual corpus with the initial word
alignment. Finally, we select the span with the
highest score as the final alignment, and discard
all other alignments.
We illustrate our method in Figure 2, which
shows the source word ?shaoshu? and its align-
ment in Figure 1. We split the alignments into
three continuous alignment spans and compute s-
core for each span. Finally, the span with highest
score (heavy shading area) is selected as the final
alignment.
We conduct the procedure for each source and
target word, the improved alignment (solid links)
is shown in Figure 1.
3 Experiment
To demonstrate the effect of the proposed method,
we use the state-of-the-art phrase-based system
and hierarchical phrase-based system implement-
ed in Moses (Koehn et al., 2007). The phrase-
based system uses continuous phrase pair as the
main translation knowledge. While the hierarchi-
cal phrase-based system uses both continuous and
discontinuous phrase pairs, which has an ability to
capture long distance phrase reordering.
we carried out experiments on two translation
tasks: the Chinese-to-English task comes from the
NIST Open MT Evaluation, and the German-to-
English task comes from the Workshop on Ma-
chine Translation (WMT) shared task.
3.1 Training
The training data we used are listed in Table 1. For
the Chinese-English task, the bilingual data are s-
elected from LDC. We used NIST MT03 as the
development set and tested our system on NIST
MT evaluation sets from 2004 to 2008. For the
German-English task, the bilingual data are from
Task Src. Words Tgt. Words
Chinese-to-English 75M 78M
German-to- English 107M 113M
Table 1: Bilingual data for our experiments.
System N04 N05 N06 N08
Baseline 34.53 33.02 30.43 23.29
Refined 36.21 33.99 31.59 24.36
Table 2: Chinese-to-English translation quality of
the phrase-based system.
System W10 W11 W12 W13
Baseline 20.71 20.26 20.52 23.26
Refined 21.46 20.95 21.11 23.77
Table 3: German-to-English translation quality of
the phrase-based system.
the shared translation task 2013. We used WMT08
as the development set and tested our system on
WMT test sets from 2010 to 2013.
The baseline systems are trained on the training
corpus with initial word alignment, which was ob-
tained via GIZA++ and ?grow-diag-final? method.
Based on the initial word alignment, we comput-
ed word translation probabilities and used the pro-
posed method to obtain a refined word alignment.
Then we used the refined word alignment to train
our SMT systems.
The translation results are evaluated by case-
insensitive BLEU-4 (Papineni et al., 2002).
The feature weights of the translation system
are tuned with the standard minimum-error-rate-
training (Och, 2003) to maximize the systems
BLEU score on the development set.
3.2 Results
3.2.1 Phrase-based System
Table 2 shows Chinese-to-English translation
quality of the phrase-based system. We ob-
served that our refined method significantly out-
performed the baseline word alignment on all test
sets. The improvements are ranged from 0.97 to
1.68 BLEU%.
Table 3 shows German-to-English translation
quality of the phrase-based system. The improve-
ments are ranged from 0.51 to 0.75 BLEU%.
These results demonstrate that the proposed
method improves the translation quality for
149
System N04 N05 N06 N08
Baseline 37.33 34.81 32.20 25.33
Refined 37.91 35.36 32.75 25.40
Combined 38.13 35.63 33.48 25.66
Table 4: Chinese-to-English translation quality of
the hierarchical phrase-based system.
System W10 W11 W12 W13
Baseline 21.22 19.77 20.53 23.51
Refined 21.34 20.64 20.88 23.82
Combined 21.65 20.87 21.16 24.04
Table 5: German-to-English translation quality of
the hierarchical phrase-based system.
phrase-based system. The reason is that by dis-
carding noisy word alignments from the discon-
tinuous alignments, the phrase pairs constrained
by the noisy alignments can be extracted. Thus the
system utilized more phrase pairs than the baseline
did.
3.2.2 Hierarchical Phrase-based System
The hierarchical phrase-based system utilizes dis-
continuous phrase pairs for long distance phrase
reordering. Some of the discontinuous phrase
pairs are extracted from the discontinuous align-
ments. By transforming the discontinuous align-
ments to continuous alignments, on the one hand,
we may lost some discontinuous phrase pairs. On
the other hand, we may extract additional contin-
uous and discontinuous phrase pairs as the align-
ment restriction is loose.
See Figure 3 for illustration. From the initial
alignment, we can extract a hierarchical phrase
pair ?(dang X
1
shi, when X
1
)? from the discon-
tinuous alignment of the English word ?when?.
However, the hierarchical phrase pair cannot be
extracted from our refined alignment, because our
method discards the link between the Chinese
word ?dang? and the English word ?when?. In-
stead, we can extract another hierarchical phrase
pair ?(X
1
shi, when X
1
)?.
Does our method still obtain improvements on
the hierarchical phrase-based system? Table 4 and
Table 5 shows Chinese-to-English and German-
to-English translation quality of the hierarchical
phrase-based system, respectively. For Chinese-
to-English translation, the refined alignment ob-
tained improvements ranged from 0.07 to 0.58

dang
?
shigu
u)
fasheng
?
shi
when the accident
happend
Figure 3: Example of word alignment between a
Chinese and English sentence pair. The dashed
initial link is discarded by our method.
BLEU% on the test set ( the row ?Refined?).
While for German-to-English translation, the im-
provements ranged from 0.12 to 0.59 BLEU% on
the test set (the row ?Refined?).
We find that the improvements are less than
that of the phrase-based system. As discussed
above, our method may lost some hierarchical
phrase pairs that extracted from the discontinuous
alignments. To solve the problem, we combine
2
the initial alignments and the refined alignments
to train the SMT system. The results are shown
in the row ?Combined? in Table 4 and Table 5.
For Chinese-to-English translation, we obtained
an improvements of 1.28 BLEU% on NIST06 over
the baseline. While for German-to-English trans-
lation, the greatest improvements is 1.10 BLEU%
on WMT11.
4 Analyses
In order to further study the performance of the
proposed method, we analyze the word alignment
and the phrase table for Chinese-to-English trans-
lation. We find that our method improves the qual-
ity of word alignment. And as a result, more useful
phrase pairs are extracted from the refined word
alignment.
4.1 Word Alignment
The Chinese-to-English training corpus contains
4.5M sentence pairs. By applying GIZA++ and
the ?grow-diag-final? method, we obtained initial
alignments. We find that 4.0M (accounting for
89%) sentence pairs contain discontinuous align-
ments. We then used the proposed method to dis-
card noisy links. By doing this, the total links
between words in the training corpus are reduced
from 99.6M to 78.9M, indicating that 21% links
are discarded.
2
We do not perform combination for phrase-based sys-
tem, because the phrase table extracted from the initial align-
ment is a subset of that extracted from the refined alignment.
150
Alignment Precision Recall AER
Initial 62.94 89.55 26.07
Refined 73.43 87.82 20.01
Table 6: Precision, Recall and AER on Chinese-
to-English alignment.
Alignment StandPhr HierPhr
Initial 29M 86M
Refined 104M 436M
Table 7: The phrase number extracted from the
initial and refined alignment for the hierarchical
phrase-based system on Chinese-to-English trans-
lation. StandPhr is standard phrase, HierPhr is hi-
erarchical phrase.
We evaluated the alignment quality on 200 sen-
tence pairs. Results are shown in Table 6. It is
observed that our method improves the precision
and decreases the AER, while keeping a high re-
call. This means that our method effectively dis-
cards noisy links in the initial word alignments.
4.2 Phrase Table
According to the standard definition of phrase in
SMT, phrase pairs cannot be extracted from the
discontinuous alignments. By transforming dis-
continuous alignments into continuous alignmen-
t, we can extract more phrase pairs. Table 7
shows the number of standard phrases and hier-
archical phrases extracted from the initial and re-
fined word alignments. We find that the number of
both phrases and hierarchical phrases grows heav-
ily. This is because that the word alignment con-
straint for phrase extraction is loosed by removing
noisy links. Although the phrase table becomes
larger, fortunately, there are some methods (John-
son et al., 2007; He et al., 2009) to prune phrase
table without hurting translation quality.
For further illustration, we compare the phrase
pairs extracted from the initial alignment and re-
fined alignment in Figure 1. From the initial align-
ments, we extracted only 3 standard phrase pairs
and no hierarchical phrase pairs (Table 8). After
discarding noisy alignments (dashed links) by us-
ing the proposed method, we extracted 21 standard
phrase pairs and 36 hierarchical phrases. Table 9
and Table 10 show selected phrase pairs and hier-
archical phrase pairs, respectively.
Chinese English
meiguo The United States
guojia nations
piao note
Table 8: Phrase pairs extracted from the initial
alignment of Figure 1.
Chinese English
shi was
fandui piao a nay note
shaoshu jige the handful of
Table 9: Selected phrase pairs extracted from the
refined alignment of Figure 1.
Chinese English
X
1
zhiyi among X
1
X
1
de guojia nations that X
1
X
1
fandui piao X
2
X
2
X
1
a nay note
Table 10: Selected hierarchical phrase pairs ex-
tracted from the refined alignment of Figure 1.
5 Conclusion and Future Work
In this paper, we proposed a novel method to im-
prove word alignment for SMT. The method re-
fines initial word alignments by transforming dis-
continuous alignment to continuous alignment. As
a result, more useful phrase pairs are extracted
from the refined word alignment. Our method is
simple and efficient, since it uses only the word
translation probabilities obtained from the initial
alignments to discard noisy links. Our method
is independent of languages and can be applied
to most SMT models. Experimental results show
significantly improvements for the state-of-the-art
phrase-based and hierarchical phrase-based sys-
tems on all Chinese-to-English and German-to-
English translation tasks.
In the future, we will refine the method by con-
sidering neighbor words and alignments when dis-
carding noisy links.
Acknowlegement
This paper is supported by the 973 program No.
2014CB340505. We would like to thank Xuan Liu
and the anonymous reviewers for their insightful
comments.
151
References
Peter F. Brown, Stephen A. Della Pietra, Vincen-
t J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium, pages 25?29.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bienni-
al Conference of the Association for Machine Trans-
lation in the Americas.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quali-
ty by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 demonstration session.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglin-
ear models for word alignment. In Proceedings of
of ACL 2005, pages 459?466, Ann Arbor,Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 609?616.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In In Proceedings of COLING/ACL
2006, pages 513?520, Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search forword alignment. In In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166, Uppsala, Swe-
den, July.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
152
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation by Pivoting 
the Co-occurrence Count of Phrase Pairs 
 
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,  
Haifeng Wang2, and Tiejun Zhao1 
Harbin Institute of Technology, Harbin, China1 
{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cn 
Baidu Inc., Beijing, China2 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
                                                 
* This work was done when the first author was visiting Baidu. 
Abstract 
To overcome the scarceness of bilingual 
corpora for some language pairs in ma-
chine translation, pivot-based SMT uses 
pivot language as a "bridge" to generate 
source-target translation from source-
pivot and pivot-target translation. One of 
the key issues is to estimate the probabili-
ties for the generated phrase pairs. In this 
paper, we present a novel approach to 
calculate the translation probability by 
pivoting the co-occurrence count of 
source-pivot and pivot-target phrase pairs. 
Experimental results on Europarl data 
and web data show that our method leads 
to significant improvements over the 
baseline systems. 
1 Introduction 
Statistical Machine Translation (SMT) relies on 
large bilingual parallel data to produce high qual-
ity translation results. Unfortunately, for some 
language pairs, large bilingual corpora are not 
readily available. To alleviate the parallel data 
scarceness, a conventional solution is to intro-
duce a ?bridge? language (named pivot language) 
to connect the source and target language (de 
Gispert and Marino, 2006; Utiyama and Isahara, 
2007; Wu and Wang, 2007; Bertoldi et al., 2008; 
Paul et al., 2011; El Kholy et al., 2013; Zahabi et 
al., 2013), where there are large amounts of 
source-pivot and pivot-target parallel corpora. 
Among various pivot-based approaches, the 
triangulation method (Cohn and Lapata, 2007; 
Wu and Wang, 2007) is a representative work in 
pivot-based machine translation. The approach 
proposes to build a source-target phrase table by 
merging the source-pivot and pivot-target phrase 
table. One of the key issues in this method is to 
estimate the translation probabilities for the gen-
erated source-target phrase pairs. Conventionally, 
the probabilities are estimated by multiplying the 
posterior probabilities of source-pivot and pivot-
target phrase pairs. However, it has been shown 
that the generated probabilities are not accurate 
enough (Cui et al., 2013). One possible reason 
may lie in the non-uniformity of the probability 
space. Through Figure 1. (a), we can see that the 
probability distributions of source-pivot and piv-
ot-target language are calculated separately, and 
the source-target probability distributions are 
induced from the source-pivot and pivot-target 
probability distributions. Because of the absence 
of the pivot language (e.g., p2 is in source-pivot 
probability space but not in pivot-target one), the 
induced source-target probability distribution is 
not complete, which will result in inaccurate 
probabilities.  
To solve this problem, we propose a novel ap-
proach that utilizes the co-occurrence count of 
source-target phrase pairs to estimate phrase 
translation probabilities more precisely. Different 
from the triangulation method, which merges the 
source-pivot and pivot-target phrase pairs after 
training the translation model, we propose to 
merge the source-pivot and pivot-target phrase 
pairs immediately after the phrase extraction step, 
and estimate the co-occurrence count of the 
source-pivot-target phrase pairs. Finally, we 
compute the translation probabilities according 
to the estimated co-occurrence counts, using the 
standard training method in phrase-based SMT 
(Koehn et al., 2003). As Figure 1. (b) shows, the 
1665
source-target probability distributions are calcu-
lated in a complete probability space. Thus, it 
will be more accurate than the traditional trian-
gulation method. Figure 2. (a) and (b) show the 
difference between the triangulation method and 
our co-occurrence count method. 
Furthermore, it is common that a small stand-
ard bilingual corpus can be available between the 
source and target language. The direct translation 
model trained with the standard bilingual corpus 
exceeds in translation performance, but its weak-
ness lies in low phrase coverage. However, the 
pivot model has characteristics characters. Thus, 
it is important to combine the direct and pivot 
translation model to compensate mutually and 
further improve the translation performance. To 
deal with this problem, we propose a mixed 
model by merging the phrase pairs extracted by 
pivot-based method and the phrase pairs extract-
ed from the standard bilingual corpus. Note that, 
this is different from the conventional interpola-
tion method, which interpolates the direct and 
pivot translation model. See Figure 2. (b) and (c) 
for further illustration. 
(a) the triangulation method                         (b) the co-occurrence count method 
 
Figure 1: An example of probability space evolution in pivot translation. 
 
 
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
SP model PT model
ST pivot 
model
Phrase Extraction Phrase Extraction
Train Train
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
ST pivot 
model
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Train
PT phrase 
pairs
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
ST mixed 
pairs
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Merge
Standard 
ST corpus
ST phrase 
pairs
Phrase Extraction
Train
ST mixed 
model
Mix
        (a) the triangulation method        (b) the co-occurrence count method            (c) the mixed model 
 
Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed 
model. The shaded box in (b) denotes difference between the co-occurrence count method and the 
triangulation method. The shaded box in (c) denotes the difference between the interpolation model 
and the mixed model. 
1666
The remainder of this paper is organized as 
follows. In Section 2, we describe the related 
work. We introduce the co-occurrence count 
method in Section 3, and the mixed model in 
Section 4. In Section 5 and Section 6, we de-
scribe and analyze the experiments. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classi-
fied into 3 kinds as follows: 
Transfer Method: The transfer method 
(Utiyama and Isahara, 2007; Wang et al., 2008; 
Costa-juss? et al., 2011) connects two translation 
systems: a source-pivot MT system and a pivot-
target MT system. Given a source sentence, (1) 
the source-pivot MT system translates it into the 
pivot language, (2) and the pivot-target MT sys-
tem translates the pivot sentence into the target 
sentence. During each step (source to pivot and 
pivot to target), multiple translation outputs will 
be generated, thus a minimum Bayes-risk system 
combination method is often used to select the 
optimal sentence (Gonz?lez-Rubio et al., 2011; 
Duh et al., 2011). The problem with the transfer 
method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot transla-
tion system will be transferred to the pivot-target 
translation. 
Synthetic Method: It aims to create a synthet-
ic source-target corpus by: (1) translate the pivot 
part in source-pivot corpus into target language 
with a pivot-target model; (2) translate the pivot 
part in pivot-target corpus into source language 
with a pivot-source model; (3) combine the 
source sentences with translated target sentences 
or/and combine the target sentences with trans-
lated source sentences (Utiyama et al., 2008; Wu 
and Wang, 2009). However, it is difficult to 
build a high quality translation system with a 
corpus created by a machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target phrase table by 
merging source-pivot and pivot-target phrase 
table entries with identical pivot language 
phrases and multiplying corresponding posterior 
probabilities (Wu and Wang, 2007; Cohn and 
Lapata, 2007), which has been shown to work 
better than the other pivot approaches (Utiyama 
and Isahara, 2007). A problem of this approach is 
that the probability space of the source-target 
phrase pairs is non-uniformity due to the mis-
matching of the pivot phrase.  
3 Our Approach 
In this section, we will introduce our method for 
learning a source-target phrase translation model 
with a pivot language as a bridge. We extract the 
co-occurrence count of phrase pairs for each lan-
guage pair with a source-pivot and a pivot-target 
corpus. Then we generate the source-target 
phrase pairs with induced co-occurrence infor-
mation. Finally, we compute translation proba-
bilities using the standard phrase-based SMT 
training method. 
3.1 Phrase Translation Probabilities 
Following the standard phrase extraction method 
(Koehn et al., 2003), we can extract phrase pairs 
???, ???  and ???, ???  from the corresponding word-
aligned source-pivot and pivot-target training 
corpus, where ?? , ??  and ??  denotes the phrase in 
source, pivot and target language respectively. 
Formally, given the co-occurrence count 
????, ??? and ????, ???, we can estimate  ????, ???  by 
Equation 1: 
????, ??? ? ???????, ???, ????, ????
??
 (1) 
where ????  is a function to merge the co-
occurrences count ????, ???  and ????, ??? . We pro-
pose four calculation methods for function ????. 
Given the co-occurrence count ????, ???  and 
????, ???, we first need to induce the co-occurrence 
count ????, ?,? ??? . The ????, ?,? ???  is counted when 
the source phrase, pivot phrase and target phrase 
occurred together, thus we can infer that 
????, ?,? ???  is smaller than ????, ???  and ????, ??? . In 
this circumstance, we consider that ????, ?,? ???  is 
approximately equal to the minimum value of 
????, ??? and ????, ???, as shown in Equation 2. 
????, ??, ??? ? ?min?????, ???, ????, ????
??
 (2) 
Because the co-occurrence count of source-
target phrase pairs needs the existence of pivot 
phrase ?? , we intuitively believe that the co-
occurrence count ????, ???  is equal to the co-
occurrence count ????, ?,? ???. Under this assump-
tion, we can obtain the co-occurrence count 
????, ??? as shown in Equation 3. Furthermore, to 
testify our assumption, we also try the maximum 
value (Equation 4) to infer the co-occurrence 
count of ???, ???  phrase pair. 
1667
????, ??? ? ?min?????, ???, ????, ????
??
 (3) 
????, ??? ? ?max?????, ???, ????, ????
??
 (4) 
In addition, if source-pivot and pivot-target 
parallel corpus greatly differ in quantities, then 
the minimum function would likely just take the 
counts from the smaller corpus. To deal with the 
problem of the imbalance of the parallel corpora, 
we also try the arithmetic mean (Equation 5) and 
geometric mean (Equation 6) function to infer 
the co-occurrence count of source-target phrase 
pairs. 
????, ??? ? ??????, ??? ? ????, ????/2
??
 (5) 
????, ??? ? ??????, ??? ? ????, ???
??
 (6) 
When the co-occurrence count of source-target 
language is calculated, we can estimate the 
phrase translation probabilities with the follow-
ing Equation 7 and Equation 8. 
?????|?? ? ????, ???? ????, ?????  (7) 
????|??? ? ????, ???? ????, ?????  (8) 
3.2 Lexical Weight 
Given a phrase pair ???, ??? and a word alignment 
a between the source word positions ? ? 1,? , ? 
and the target word positions ? ? 0,? ,? , the 
lexical weight of phrase pair ???, ??? can be calcu-
lated by the following Equation 9 (Koehn et al., 
2003). 
??????|?, ?? ??
1
|??|??, ?? ? ??| ? ????|??????,????
?
???
(9) 
The lexical translation probability distribution 
???|?? between source word s and target word t 
can be estimated with Equation 10. 
???|?? ? ???, ??? ????, ????  (10)
To compute the lexical weight for a phrase 
pair ???, ??? generated by ???, ??? and ???, ???, we need 
the alignment information ?, which can be ob-
tained as Equation 11 shows. 
? ? ???, ??|??: ??, ?? ? ??&??, ?? ? ??? (11)
where ??  and ??  indicate the word alignment 
information in the phrase pair ???, ???  and ???, ??? 
respectively. 
4 Integrate with Direct Translation 
If a standard source-target bilingual corpus is 
available, we can train a direct translation model. 
Thus we can integrate the direct model and the 
pivot model to obtain further improvements. We 
propose a mixed model by merging the co-
occurrence count in direct translation and pivot 
translation. Besides, we also employ an interpo-
lated model (Wu and Wang, 2007) by merging 
the direct translation model and pivot translation 
model using a linear interpolation. 
4.1 Mixed Model 
Given ?  pivot languages, the co-occurrence 
count can be estimated using the method de-
scribed in Section 3.1. Then the co-occurrence 
count and the lexical weight of the mixed model 
can be estimated with the following Equation 12 
and 13. 
???, ?? ??????, ??
?
???
 (12)
??????|?, ?? ????
?
???
??,?????|?, ?? (13)
where ????, ??  and ??,?????|?, ??  are the co-
occurrence count and lexical weight in the direct 
translation model respectively. ????, ??  and 
??,?????|?, ?? denote the co-occurrence count and 
lexical weight in the pivot translation model. ?? 
is the interpolation coefficient, requiring 
? ?????? ? 1. 
4.2 Interpolated Model 
Following Wu and Wang (2007), the interpolated 
model can be modelled with Equation 14. 
?????|?? ? ?????????|??
?
???
 (14)
where ??????|?? is the phrase translation probabil-
ity in direct translation model; ??????|??  is the 
phrase translation probability in pivot translation 
model. The lexical weight is obtained with Equa-
tion 13. ?? is the interpolation coefficient, requir-
ing ? ?? ? 1???? . 
1668
5 Experiments on Europarl Corpus 
Our first experiment is carried out on Europarl1 
corpus, which is a multi-lingual corpus including 
21 European languages (Koehn, 2005). In our 
work, we perform translations among French (fr), 
German (de) and Spanish (es). Due to the rich-
ness of available language resources, we choose 
English (en) as the pivot language. Table 1 
summarized the statistics of training data. For the 
language model, the same monolingual data ex-
tracted from the Europarl are used. 
The word alignment is obtained by GIZA++ 
(Och and Ney, 2000) and the heuristics ?grow-
diag-final? refinement rule (Koehn et al., 2003). 
Our translation system is an in-house phrase-
based system analogous to Moses (Koehn et al., 
2007). The baseline system is the triangulation 
method (Wu and Wang, 2007), including an in-
terpolated model which linearly interpolate the 
direct and pivot translation model. 
                                                 
1 http://www.statmt.org/europarl 
We use WMT082  as our test data, which con-
tains 2000 in-domain sentences and 2051 out-of-
domain sentences with single reference. The 
translation results are evaluated by case-
insensitive BLEU-4 metric (Papineni et al., 
2002). The statistical significance tests using 
95% confidence interval are measured with 
paired bootstrap resampling (Koehn, 2004). 
5.1 Results 
We compare 4 merging methods with the base-
line system. The results are shown in Table 2 and 
Table 3. We find that the minimum method out-
performs the others, achieving significant im-
provements over the baseline on all translation 
directions. The absolute improvements range 
from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score 
on in-domain test data, and range from 0.36 (fr-
de) to 2.05 (fr-es) in BLEU% score on out-of-
domain test data. This indicates that our method 
is effective and robust in general. 
                                                 
2 http://www.statmt.org/wmt08/shared-task.html 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
de-en 1.9M 48.5M 50.9M
es-en 1.9M 54M 51.7M
fr-en 2M 58.1M 52.4M
 
Table 1: Training data of Europarl corpus 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 27.04 23.01 20.65 33.84 20.87 38.31 
Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62* 
Maximum 25.70 21.59 20.26 32.58 20.50 37.30 
Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37 
Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19* 
 
Table 2: Comparison of different merging methods on in-domain test set. * indicates the results are 
significantly better than the baseline (p<0.05). 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 15.34 13.52 11.47 21.99 12.19 25.00 
Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05* 
Maximum 13.41 11.83 10.17 20.48 10.83 22.75 
Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70 
Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22* 
 
Table 3: Comparison of different merging methods on out-of-domain test set. 
 
1669
The geometric mean method also achieves im-
provement, but not as significant as the minimum 
method. However, the maximum and the arith-
metic mean methods show a decrement in BLEU 
scores. This reminds us that how to choose a 
proper merging function for the co-occurrence 
count is a key problem.  In the future, we will 
explore more sophisticated method to merge co-
occurrence count. 
5.2 Analysis 
The pivot-based translation is suitable for the 
scenario that there exists large amount of source-
pivot and pivot-target bilingual corpora and only 
a little source-target bilingual data. Thus, we 
randomly select 10K, 50K, 100K, 200K, 500K, 
1M, 1.5M sentence pairs from the source-target 
bilingual corpora to simulate the lack of source-
target data. With these corpora, we train several 
direct translation models with different scales of 
bilingual data. We interpolate each direct transla-
tion model with the pivot model (both triangula-
tion method and co-occurrence count method) to 
obtain the interpolated model respectively. We 
also mix the direct model and pivot model using 
the method described in Section 4.1.  Following 
 
(a) German-English-Spanish                                        (b) German-English-French 
 
 
(c) Spanish-English-German                                        (d) Spanish-English-French 
 
 
(e) French-English-German                                         (f) French-English-Spanish 
 
Figure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora. 
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulation 
model interpolated with direct model ; co+inter: co-occurrence count model interpolated with direct 
model; co+mix: mixed model). X-axis represents the scale of the standard training data. 
22.5
23
23.5
24
24.5
25
25.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
26.5
27
27.5
28
28.5
29
29.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
33.5
34
34.5
35
35.5
36
36.5
37
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
37.5
38
38.5
39
39.5
40
40.5
41
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
1670
Wu and Wang (2007), we set ?? ? 0.9, ?? ? 0.1, 
?? ? 0.9  and ?? ? 0.1  empirically. The experi-
ments are carried out on 6 translation directions: 
German-Spanish, German-French, Spanish-
German, Spanish-French, French-German and 
French-Spanish. The results are shown in Figure 
3. We only list the results on in-domain test sets. 
The trend of the results on out-of domain test 
sets is similar with in-domain test sets. 
The results are explained as follows: 
(1) Comparison of Pivot Translation and Di-
rect Translation 
The pivot translation models are better than 
the direct translation models trained on a small 
source-target bilingual corpus. With the incre-
ment of source-target corpus, the direct model 
first outperforms the triangulation model and 
then outperforms the co-occurrence count model 
consecutively. 
Taking Spanish-English-French translation as 
an example, the co-occurrence count model 
achieves BLEU% scores of 35.38, which is close 
to the direct translation model trained with 200K 
source-target bilingual data. Compared with the 
co-occurrence count model, the triangulation 
model only achieves BLEU% scores of 33.84, 
which is close to the direct translation model 
trained with 50K source-target bilingual data. 
(2) Comparison of Different Interpolated 
Models 
For the pivot model trained by triangulation 
method and co-occurrence count method, we 
interpolate them with the direct translation model 
trained with different scales of bilingual data. 
Figure 3 shows the translation results of the dif-
ferent interpolated models. For all the translation 
directions, our co-occurrence count method in-
terpolated with the direct model is better than the 
triangulation model interpolated with the direct 
model.  
The two interpolated model are all better than 
the direct translation model. With the increment 
of the source-target training corpus, the gap be-
comes smaller. This indicates that the pivot mod-
el and its affiliated interpolated model are suita-
ble for language pairs with small bilingual data. 
Even if the scale of source-pivot and pivot-target 
corpora is close to the scale of source-target bi-
lingual corpora, the pivot translation model can 
help the direct translation model to improve the 
translation performance. Take Spanish-English-
French translation as an issue, when the scale of 
Spanish-French parallel data is 1.5M sentences 
pairs, which is close to the Spanish-English and 
English-French parallel data, the performance of 
co+mix model is still outperforms the direct 
translation model. 
(3) Comparison of Interpolated Model and 
Mixed Model 
When only a small source-target bilingual 
corpus is available, the mix model outperforms 
the interpolated model. With the increasing of 
source-target corpus, the mix model is close to 
the interpolated model or worse than the interpo-
lated model. This indicates that the mix model 
has a better performance when the source-target 
corpus is small which is close to the realistic sce-
nario. 
5.3 Integrate the Co-occurrence Count 
Model and Triangulation Model 
Experimental results in the previous section 
show that, our co-occurrence count models gen-
erally outperform the baseline system. In this 
section, we carry out experiments that integrates 
co-occurrence count model into the triangulation 
model. 
For French-English-German translation, we 
apply a linear interpolation method to integrate 
the co-occurrence count model into triangulation 
model following the method described in Section 
4.2.  We set ? as the interpolation coefficient of 
triangulation model and 1 ? ? as the interpola-
tion coefficient of co-occurrence count model 
respectively. The experiments take 9 values for 
interpolation coefficient, from 0.1 to 0.9. The 
results are shown in Figure 4. 
 
 
Figure 4: Results of integrating the co-
occurrence count model and the triangulation 
model. 
 
When using interpolation coefficient ranging 
from 0.2 to 0.7, the integrated models outperform 
the triangulation and the co-occurrence count 
model. However, for the other intervals, the inte-
20.4
20.6
20.8
21
21.2
21.4
21.6
21.8
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
BL
EU
%
Interpolation Coefficient 
integrated triangulation
co-occurrence
1671
grated models perform slightly lower than the 
co-occurrence count model, but still show better 
results than the triangulation model. The trend of 
the curve infers that the integrated model synthe-
sizes the contributions of co-occurrence count 
model and triangulation model. Additionally, it 
also indicates that, the choice of the interpolation 
coefficient affects the translation performances. 
6 Experiments on Web Data 
The experimental on Europarl is artificial, as the 
training data for directly translating between 
source and target language actually exists in the 
original data sets. Thus, we conducted several 
experiments on a more realistic scenario: trans-
lating Chinese (zh) to Japanese (jp) via English 
(en) with web crawled data. 
As mentioned in Section 3.1, the source-pivot 
and pivot-target parallel corpora can be imbal-
anced in quantities. If one parallel corpus was 
much larger than another, then minimum heuris-
tic function would likely just take the counts 
from the smaller corpus.  
In order to analyze this issue, we manually set 
up imbalanced corpora. For source-pivot parallel 
corpora, we randomly select 1M, 2M, 3M, 4M 
and 5M Chinese-English sentence pairs. On the 
other hand, we randomly select 1M English-
Japanese sentence pairs as pivot-target parallel 
corpora. The training data of Chinese-English 
and English-Japanese language pairs are summa-
rized in Table 4. For the Chinese-Japanese direct 
corpus, we randomly select 5K, 10K, 20K, 30K, 
40K, 50K, 60K, 70K, 80K, 90K and 100K sen-
tence pairs to simulate the lack of bilingual data. 
We built a 1K in-house test set with four refer-
ences. For Japanese language model training, we 
used the monolingual part of English-Japanese 
corpus. 
Table 5 shows the results of different co-
occurrence count merging methods. First, the 
minimum method and the geometric mean meth-
od outperform the other two merging methods 
and the baseline system with different training 
corpus. When the scale of source-pivot and piv-
ot-target corpus is roughly balanced (zh-en-jp-1), 
the minimum method achieves an absolute im-
provement of 2.06 percentages points on BLEU 
over the baseline, which is also better than the 
other merging methods. While, with the growth 
of source-pivot corpus, the gap between source-
pivot corpus and pivot-target corpus becomes 
bigger. In this circumstance, the geometric mean 
method becomes better than the minimum meth-
od. Compared to the minimum method, the geo-
metric mean method considers both the source-
pivot and the pivot-target corpus, which may 
lead to a better result in the case of imbalanced 
training corpus. 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
zh-en-1 1M 18.1M 17.7M
zh-en-2 2M 36.2M 35.5M
zh-en-3 3M 54.2M 53.2M
zh-en-4 4M 72.3M 70.9M
zh-en-5 5M 90.4M 88.6M
en-jp 1M 9.2M 11.1M
 
Table 4: Training data of web corpus 
 
System 
BLEU% 
zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5
Baseline 29.07 29.39 29.44 29.67 29.80 
Minimum 31.13* 31.28* 31.43* 31.62* 32.02* 
Maximum 28.88 29.01 29.12 29.37 29.59 
Arithmetic mean 29.08 29.36 29.51 29.79 30.01 
Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34* 
 
Table 5: Comparison of different merging methods on the imbalanced web data. ( zh-en-jp-1 means 
the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus, 
and so on. ) 
1672
Furthermore, with the imbalanced corpus zh-
en-jp-5, we compared the translation perfor-
mance of our co-occurrence count model (with 
geometric mean merging method), triangulation 
model, interpolated model, mixed model and the 
direct translation models. Figure 5 summarized 
the results. 
The co-occurrence count model can achieve an 
absolute improvement of 2.54 percentages points 
on BLEU over the baseline. The triangulation 
method outperforms the direct translation when 
only 5K sentence pairs are available. Meanwhile, 
the number is 10K when using the co-occurrence 
count method. The co-occurrence count models 
interpolated with the direct model significantly 
outperform the other models. 
 
 
Figure 5: Results on Chinese-Japanese Web Data. 
X-axis represents the scale of the standard train-
ing data. 
 
In this experiment, the training data contains 
parallel sentences on various domains. And the 
training corpora (Chinese-English and English-
Japanese) are typically very different, since they 
are obtained on the web. It indicates that our co-
occurrence count method is robust in the realistic 
scenario. 
7 Conclusion 
This paper proposed a novel approach for pivot-
based SMT by pivoting the co-occurrence count 
of phrase pairs. Different from the triangulation 
method merging the source-pivot and pivot-
target language after training the translation 
model, our method merges the source-pivot and 
pivot-target language after extracting the phrase 
pairs, thus the computing for phrase translation 
probabilities is under the uniform probability 
space. The experimental results on Europarl data 
and web data show significant improvements 
over the baseline systems. We also proposed a 
mixed model to combine the direct translation 
and pivot translation, and the experimental re-
sults show that the mixed model has a better per-
formance when the source-target corpus is small 
which is close to the realistic scenario. 
A key problem in the approach is how to learn 
the co-occurrence count. In this paper, we use the 
minimum function on balanced corpora and the 
geometric mean function on imbalanced corpora 
to estimate the co-occurrence count intuitively. 
In the future, we plan to explore more effective 
approaches. 
Acknowledgments 
We would like to thank Yiming Cui for insight-
ful discussions, and three anonymous reviewers 
for many invaluable comments and suggestions 
to improve our paper. This work is supported by 
National Natural Science Foundation of China 
(61100093), and the State Key Development 
Program for Basic Research of China (973 Pro-
gram, 2014CB340505). 
Reference 
Nicola Bertoldi, Madalina Barbaiani, Marcello 
Federico, and Roldano Cattoni. 2008. Phrase-
Based statistical machine translation with Piv-
ot Languages. In Proceedings of the 5th Inter-
national Workshop on Spoken Language 
Translation (IWSLT), pages 143-149. 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Make Effective 
Use of Multi-Parallel Corpora. In Proceedings 
of 45th Annual Meeting of the Association for 
Computational Linguistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Ra-
fael E. Banchs. 2011. Enhancing Scarce-
Resource Language Translation through Pivot 
Combinations. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing, pages 1361-1365. 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun 
Zhao and Dequan Zheng. 2013. Phrase Table 
Combination Deficiency Analyses in Pivot-
based SMT. In Proceedings of 18th Interna-
tional Conference on Application of Natural 
Language to Information Systems, pages 355-
358. 
Adria de Gispert and Jose B. Marino. 2006. 
Catalan-English statistical machine translation 
without parallel corpus: bridging through 
Spanish. In Proceedings of 5th International 
Conference on Language Resources and Eval-
uation (LREC), pages 65-68. 
29
31
33
35
37
39
5K 20K 40K 60K 80K 100K
BL
EU
%
direct
tri
co-occur
tri+inter
co+inter
co+mix
1673
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, 
Hajime Tsukada and Masaaki Nagata. 2011. 
Generalized Minimum Bayes Risk System 
Combination. In Proceedings of the 5th Inter-
national Joint Conference on Natural Lan-
guage Processing, pages 1356-1360. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. 
Language Independent Connectivity Strength 
Features for Phrase Pivot Statistical Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational 
Linguistics, pages 412-418. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. Se-
lective Combination of Pivot and Direct Sta-
tistical Machine Translation Models. In Pro-
ceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 
1174-1180. 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francis-
co Casacuberta. 2011. Minimum Bayes-risk 
System Combination. In Proceedings of the 
49th Annual Meeting of the Association for 
Computational Linguistics, pages 1268-1277. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
HLT-NAACL: Human Language Technology 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 127-133. 
Philipp Koehn. 2004. Statistical significance 
tests for machine translation evaluation. In 
Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Pro-
cessing (EMNLP), pages 388-395. 
Philipp Koehn. 2005. Europarl: A Parallel Cor-
pus for Statistical Machine Translation. In 
Proceedings of MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ondrej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceed-
ings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, demon-
stration session, pages 177-180. 
Philipp Koehn, Alexandra Birch, and Ralf Stein-
berger. 2009. 462 Machine Translation Sys-
tems for Europe. In Proceedings of the MT 
Summit XII. 
Gregor Leusch, Aur?lien Max, Josep Maria 
Crego and Hermann Ney. 2010. Multi-Pivot 
Translation by System Combination. In Pro-
ceedings of the 7th International Workshop on 
Spoken Language Translation, pages 299-306. 
Franz Josef Och and Hermann Ney. 2000. A 
comparison of alignment models for statistical 
machine translation. In Proceedings of the 
18th International Conference on Computa-
tional Linguistics, pages 1086-1090. 
Michael Paul, Andrew Finch, Paul R. Dixon and 
Eiichiro Sumita. 2011. Dialect Translation: In-
tegrating Bayesian Co-segmentation Models 
with Pivot-based SMT. In Proceedings of the 
2011 Conference on Empirical Methods in 
Natural Language Processing, pages 1-9. 
Michael Paul and Eiichiro Sumita. 2011. Trans-
lation Quality Indicators for Pivot-based Sta-
tistical MT. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language 
Processing, pages 811-818. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computation Linguistics, pag-
es 311-319. 
Rie Tanaka, Yohei Murakami and Toru Ishida. 
2009. Context-Based Approach for Pivot 
Translation Services. In the Twenty-first In-
ternational Conference on Artificial Intelli-
gence, pages 1555-1561. 
J?rg Tiedemann. 2012. Character-Based Pivot 
Translation for Under-Resourced Languages 
and Domains. In Proceedings of the 13th Con-
ference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 
141-151. 
Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-
kiWakita and Seiichi Nakagawa. 2007. Ex-
panding Indonesian-Japanese Small Transla-
tion Dictionary Using a Pivot Language. In 
Proceedings of the ACL 2007 Demo and Post-
er Sessions, pages 197-200. 
Takashi Tsunakawa, Naoaki Okazaki and 
Jun'ichi Tsujii. 2010. Building a Bilingual 
Lexicon Using Phrase-based Statistical Ma-
chine Translation via a Pivot Language. In 
1674
Proceedings of the 22th International Confer-
ence on Computational Linguistics (Coling), 
pages 127-130. 
Masao Utiyama and Hitoshi Isahara. 2007. A 
Comparison of Pivot Methods for Phrase-
Based Statistical Machine Translation. In Pro-
ceedings of Human Language Technology: the 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 484-491. 
Masao Utiyama, Andrew Finch, Hideo Okuma, 
Michael Paul, Hailong Cao, Hirofumi Yama-
moto, Keiji Yasuda,and Eiichiro Sumita. 2008. 
The NICT/ATR speech Translation System for 
IWSLT 2008. In Proceedings of the Interna-
tional Workshop on Spoken Language Trans-
lation, pages 77-84. 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi 
Liu, Jianfeng Li, Dengjun Ren, and Zhengyu 
Niu. 2008. The TCH Machine Translation 
System for IWSLT 2008. In Proceedings of 
the International Workshop on Spoken Lan-
guage Translation, pages 124-131. 
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical 
Machine Translation. In Proceedings of 45th 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 856-863. 
Hua Wu and Haifeng Wang. 2009. Revisiting 
Pivot Language Approach for Machine Trans-
lation. In Proceedings of the 47th Annual 
Meeting of the Association for Computational 
Linguistics and the 4th IJCNLP of the AFNLP, 
pages 154-162. 
Samira Tofighi Zahabi, Somayeh Bakhshaei and 
Shahram Khadivi. Using Context Vectors in 
Improving a Machine Translation System with 
Bridge Language. In Proceedings of the 51st 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 318-322. 
 
 
1675
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825?833,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving Statistical Machine Translation with 
Monolingual Collocation 
 
Zhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu.com Inc., Beijing, China 
zhanyiliu@gmail.com 
{wanghaifeng, wu_hua}@baidu.com 
lisheng@hit.edu.cn 
 
Abstract? 
This paper proposes to use monolingual 
collocations to improve Statistical Ma-
chine Translation (SMT). We make use 
of the collocation probabilities, which are 
estimated from monolingual corpora, in 
two aspects, namely improving word 
alignment for various kinds of SMT sys-
tems and improving phrase table for 
phrase-based SMT. The experimental re-
sults show that our method improves the 
performance of both word alignment and 
translation quality significantly. As com-
pared to baseline systems, we achieve ab-
solute improvements of 2.40 BLEU score 
on a phrase-based SMT system and 1.76 
BLEU score on a parsing-based SMT 
system. 
1 Introduction 
Statistical bilingual word alignment (Brown et al 
1993) is the base of most SMT systems. As com-
pared to single-word alignment, multi-word 
alignment is more difficult to be identified. Al-
though many methods were proposed to improve 
the quality of word alignments (Wu, 1997; Och 
and Ney, 2000; Marcu and Wong, 2002; Cherry 
and Lin, 2003; Liu et al, 2005; Huang, 2009), 
the correlation of the words in multi-word 
alignments is not fully considered. 
In phrase-based SMT (Koehn et al, 2003), the 
phrase boundary is usually determined based on 
the bi-directional word alignments. But as far as 
we know, few previous studies exploit the collo-
cation relations of the words in a phrase. Some 
                                                 
This work was partially done at Toshiba (China) Research 
and Development Center. 
researches used soft syntactic constraints to pre-
dict whether source phrase can be translated to-
gether (Marton and Resnik, 2008; Xiong et al, 
2009). However, the constraints were learned 
from the parsed corpus, which is not available 
for many languages.  
In this paper, we propose to use monolingual 
collocations to improve SMT. We first identify 
potentially collocated words and estimate collo-
cation probabilities from monolingual corpora 
using a Monolingual Word Alignment (MWA) 
method (Liu et al, 2009), which does not need 
any additional resource or linguistic preprocess-
ing, and which outperforms previous methods on 
the same experimental data. Then the collocation 
information is employed to improve Bilingual 
Word Alignment (BWA) for various kinds of 
SMT systems and to improve phrase table for 
phrase-based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. A cept is the 
set of source words that are connected to the 
same target word (Brown et al, 1993). An 
alignment between a source multi-word cept and 
a target word is a many-to-one multi-word 
alignment. 
To improve phrase table, we calculate phrase 
collocation probabilities based on word colloca-
tion probabilities. Then the phrase collocation 
probabilities are used as additional features in 
phrase-based SMT systems. 
The evaluation results show that the proposed 
method in this paper significantly improves mul-
ti-word alignment, achieving an absolute error 
rate reduction of 29%. The alignment improve-
ment results in an improvement of 2.16 BLEU 
score on phrase-based SMT system and an im-
provement of 1.76 BLEU score on parsing-based 
SMT system. If we use phrase collocation proba-
bilities as additional features, the phrase-based 
825
SMT performance is further improved by 0.24 
BLEU score. 
The paper is organized as follows: In section 2, 
we introduce the collocation model based on the 
MWA method. In section 3 and 4, we show how 
to improve the BWA method and the phrase ta-
ble using collocation models respectively. We 
describe the experimental results in section 5, 6 
and 7. Lastly, we conclude in section 8. 
2 Collocation Model 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). A colloca-
tion is composed of two words occurring as ei-
ther a consecutive word sequence or an inter-
rupted word sequence in sentences, such as "by 
accident" or "take ... advice". In this paper, we 
use the MWA method (Liu et al, 2009) for col-
location extraction. This method adapts the bi-
lingual word alignment algorithm to monolingual 
scenario to extract collocations only from mono-
lingual corpora. And the experimental results in 
(Liu et al, 2009) showed that this method 
achieved higher precision and recall than pre-
vious methods on the same experimental data. 
2.1 Monolingual word alignment 
The monolingual corpus is first replicated to 
generate a parallel corpus, where each sentence 
pair consists of two identical sentences in the 
same language. Then the monolingual word 
alignment algorithm is employed to align the 
potentially collocated words in the monolingual 
sentences. 
According to Liu et al (2009), we employ the 
MWA Model 3 (corresponding to IBM Model 3) 
to calculate the probability of the monolingual 
word alignment sequence, as shown in Eq. (1). 
? ?
???
?
?
l
j
jaj
l
i
ii
lajdwwt
wnSASp
j1
1
3 ModelMWA 
),|()|(
)|()|,( ?    (1) 
Where lwS 1?  is a monolingual sentence, i?  
denotes the number of words that are aligned 
with 
iw . Since a word never collocates with itself, 
the alignment set is denoted as 
}&],1[|),{( ialiaiA ii ??? . Three kinds of prob-
abilities are involved in this model: word collo-
cation probability 
)|( jaj wwt
, position colloca-
tion probability ),|( lajd j  and fertility probabili-
ty )|( ii wn ? . 
In the MWA method, the similar algorithm to 
bilingual word alignment is used to estimate the 
parameters of the models, except that a word 
cannot be aligned to itself.  
Figure 1 shows an example of the potentially 
collocated word pairs aligned by the MWA me-
thod. 
 
Figure 1. MWA Example 
2.2 Collocation probability 
Given the monolingual word aligned corpus, we 
calculate the frequency of two words aligned in 
the corpus, denoted as ),( ji wwfreq . We filtered 
the aligned words occurring only once. Then the 
probability for each aligned word pair is esti-
mated as follows: 
? ??
?w j
ji
ji wwfreq
wwfreqwwp ),(
),()|(
                 (2) 
? ??
?w i
ji
ij wwfreq
wwfreqwwp ),(
),()|(
                  (3) 
In this paper, the words of collocation are 
symmetric and we do not determine which word 
is the head and which word is the modifier. Thus, 
the collocation probability of two words is de-
fined as the average of both probabilities, as in 
Eq. (4). 
2
)|()|(),( ijjiji wwpwwpwwr ??
      (4) 
If we have multiple monolingual corpora to 
estimate the collocation probabilities, we interpo-
late the probabilities as shown in Eq. (5). 
),(),( jik kkji wwrwwr ?? ?
          (5) 
k?  denotes the interpolation coefficient for 
the probabilities estimated on the kth corpus. 
3 Improving Statistical Bilingual Word 
Alignment 
We use the collocation information to improve 
both one-directional and bi-directional bilingual 
word alignments. The alignment probabilities are 
re-estimated by using the collocation probabili-
ties of words in the same cept. 
The team leader plays a key role in the project undertaking. 
The team leader plays a key role in the project undertaking. 
 
826
3.1 Improving one-directional bilingual 
word alignment 
According to the BWA method, given a bilingual 
sentence pair leE 1?  and mfF 1? , the optimal 
alignment sequence A  between E and F can be 
obtained as in Eq. (6). 
)|,(maxarg* EAFpA A?
                   (6) 
The method is implemented in a series of five 
models (IBM Models). IBM Model 1 only em-
ploys the word translation model to calculate the 
probabilities of alignments. In IBM Model 2, 
both the word translation model and position dis-
tribution model are used. IBM Model 3, 4 and 5 
consider the fertility model in addition to the 
word translation model and position distribution 
model. And these three models are similar, ex-
cept for the word distortion models. 
One-to-one and many-to-one alignments could 
be produced by using IBM models. Although the 
fertility model is used to restrict the number of 
source words in a cept and the position distortion 
model is used to describe the correlation of the 
positions of the source words, the quality of 
many-to-one alignments is lower than that of 
one-to-one alignments. 
Intuitively, the probability of the source words 
aligned to a target word is not only related to the 
fertility ability and their relative positions, but 
also related to lexical tokens of words, such as 
common phrase or idiom. In this paper, we use 
the collocation probability of the source words in 
a cept to measure their correlation strength. Giv-
en source words }|{ iaf jj ?  aligned to ie , their 
collocation probability is calculated as in Eq. (7). 
)1(*
),(2
})|({
1
1 1
][][
?
? ?
??
?
? ??
ii
k kg
giki
jj
i i ffr
iafr ??
? ?
     (7) 
Here, 
kif ][
and 
gif ][
denote the thk  word and 
thg  word in }|{ iaf jj ? ; ),( ][][ giki ffr
 denotes 
the collocation probability of 
kif ][
and 
gif ][
, as 
shown in Eq. (4).  
Thus, the collocation probability of the align-
ment sequence of a sentence pair can be calcu-
lated according to Eq. (8). 
? ?? ?
l
i jj iafrEAFr 1 })|({)|,(
           (8) 
Based on maximum entropy framework, we 
combine the collocation model and the BWA 
model to calculate the word alignment probabili-
ty of a sentence pair, as shown in Eq. (9). 
? ? ?
?
?
'
)),,(exp(
)),,(exp(
)|,(
A i
ii
i
ii
r AEFh
AEFh
EAFp ?
?     (9) 
Here, ),,( AEFhi and i?  denote features and 
feature weights, respectively. We use two fea-
tures in this paper, namely alignment probabili-
ties and collocation probabilities. 
Thus, we obtain the decision rule: 
}),,({maxarg* ?? i iiA AEFhA ?
          (10) 
Based on the GIZA++ package 1 , we imple-
mented a tool for the improved BWA method. 
We first train IBM Model 4 and collocation 
model on bilingual corpus and monolingual cor-
pus respectively. Then we employ the hill-
climbing algorithm (Al-Onaizan et al, 1999) to 
search for the optimal alignment sequence of a 
given sentence pair, where the score of an align-
ment sequence is calculated as in Eq. (10). 
We note that Eq. (8) only deals with many-to-
one alignments, but the alignment sequence of a 
sentence pair also includes one-to-one align-
ments. To calculate the collocation probability of 
the alignment sequence, we should also consider 
the collocation probabilities of such one-to-one 
alignments. To solve this problem, we use the 
collocation probability of the whole source sen-
tence, )(Fr , as the collocation probability of 
one-word cept. 
3.2 Improving bi-directional bilingual word 
alignments 
In word alignment models implemented in GI-
ZA++, only one-to-one and many-to-one word 
alignment links can be found. Thus, some multi-
word units cannot be correctly aligned. The 
symmetrization method is used to effectively 
overcome this deficiency (Och and Ney, 2003). 
Bi-directional alignments are generally obtained 
from source-to-target algnments 
tsA 2  and target-
to-source alignments 
stA 2 , using some heuristic 
rules (Koehn et al, 2005). This method ignores 
the correlation of the words in the same align-
ment unit, so an alignment may include many 
unrelated words2 , which influences the perfor-
mances of SMT systems. 
                                                 
1 http://www.fjoch.com/GIZA++.html 
2 In our experiments, a multi-word unit may include up to 
40 words. 
827
In order to solve the above problem, we incor-
porate the collocation probabilities into the bi-
directional word alignment process. 
Given alignment sets 
tsA 2  and stA 2 . We can 
obtain the union 
sttsts AAA 22 ??? . The source 
sentence mf1  can be segmented into m?  cepts 
mf ?1 . The target sentence le1  can also be seg-
mented into l ?  cepts le ?1 . The words in the same 
cept can be a consecutive word sequence or an 
interrupted word sequence. 
Finally, the optimal alignments A  between 
mf ?1  and le ?1  can be obtained from tsA ?  using the 
following decision rule. 
})()(),({maxarg
),,(
321
),(
*'
1
'
1
????
??
???
? Afe
jiji
AA
ml
jits
frerfep
Afe     (11) 
Here, )( jfr  and )( ier  denote the collocation 
probabilities of the words in the source language 
and target language respectively, which are cal-
culated by using Eq. (7). ),( ji fep  denotes the 
word translation probability that is calculated 
according to Eq. (12). 
i?  denotes the weights of 
these probabilities. 
||*||
2/))|()|((
),(
ji
ee ff
ji fe
efpfep
fep i j
? ? ?
? ? ?
    (12) 
)|( fep  and )|( efp  are the source-to-target 
and target-to-source translation probabilities 
trained from the word aligned bilingual corpus. 
4 Improving Phrase Table 
Phrase-based SMT system automatically extracts 
bilingual phrase pairs from the word aligned bi-
lingual corpus. In such a system, an idiomatic 
expression may be split into several fragments, 
and the phrases may include irrelevant words. In 
this paper, we use the collocation probability to 
measure the possibility of words composing a 
phrase. 
For each bilingual phrase pair automatically 
extracted from word aligned corpus, we calculate 
the collocation probabilities of source phrase and 
target phrase respectively, according to Eq. (13). 
)1(*
),(2
)(
1
1 1
1 ?
? ?
?
?
? ??
nn
wwr
wr
n
i
n
ij
ji
n                  (13) 
Here, nw1  denotes a phrase with n words; 
),( ji wwr
 denotes the collocation probability of a 
Corpora 
Chinese 
words 
English 
words 
Bilingual corpus 6.3M 8.5M 
Additional monolingual 
corpora 
312M 203M 
Table 1. Statistics of training data 
word pair calculated according to Eq. (4). For the 
phrase only including one word, we set a fixed 
collocation probability that is the average of the 
collocation probabilities of the sentences on a 
development set. These collocation probabilities 
are incorporated into the phrase-based SMT sys-
tem as features.  
5 Experiments on Word Alignment 
5.1 Experimental settings 
We use a bilingual corpus, FBIS (LDC2003E14), 
to train the IBM models. To train the collocation 
models, besides the monolingual parts of FBIS, 
we also employ some other larger Chinese and 
English monolingual corpora, namely, Chinese 
Gigaword (LDC2007T38), English Gigaword 
(LDC2007T07), UN corpus (LDC2004E12), Si-
norama corpus (LDC2005T10), as shown in Ta-
ble 1. 
Using these corpora, we got three kinds of col-
location models: 
CM-1: the training data is the additional mo-
nolingual corpora; 
CM-2: the training data is either side of the bi-
lingual corpus; 
CM-3: the interpolation of CM-1 and CM-2. 
To investigate the quality of the generated 
word alignments, we randomly selected a subset 
from the bilingual corpus as test set, including 
500 sentence pairs. Then word alignments in the 
subset were manually labeled, referring to the 
guideline of the Chinese-to-English alignment 
(LDC2006E93), but we made some modifica-
tions for the guideline. For example, if a preposi-
tion appears after a verb as a phrase aligned to 
one single word in the corresponding sentence, 
then they are glued together. 
There are several different evaluation metrics 
for word alignment (Ahrenberg et al, 2000). We 
use precision (P), recall (R) and alignment error 
ratio (AER), which are similar to those in Och 
and Ney (2000), except that we consider each 
alignment as a sure link. 
828
Experiments 
Single word alignments Multi-word alignments 
P R AER P R AER 
Baseline 0.77 0.45 0.43 0.23 0.71 0.65 
Improved BWA methods 
CM-1 0.70 0.50 0.42 0.35 0.86 0.50 
CM-2 0.73 0.48 0.42 0.36 0.89 0.49 
CM-3 0.73 0.48 0.41 0.39 0.78 0.47 
Table 2. English-to-Chinese word alignment results 
 
Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and 
the improved BWA method using CM-3. " " denotes the alignments of our method; " " denotes 
the alignments of the baseline method. 
||
||
g
rg
S
SSP ??
                      (14) 
||
||
r
rg
S
SSR ??
                     (15) 
||||
||*21
rg
rg
SS
SSAER ???
?              (16) 
Where, 
gS
 and 
rS  denote the automatically 
generated alignments and the reference align-
ments. 
In order to tune the interpolation coefficients 
in Eq. (5) and the weights of the probabilities in 
Eq. (11), we also manually labeled a develop-
ment set including 100 sentence pairs, in the 
same manner as the test set. By minimizing the 
AER on the development set, the interpolation 
coefficients of the collocation probabilities on 
CM-1 and CM-2 were set to 0.1 and 0.9. And the 
weights of probabilities were set as 6.01 ?? , 
2.02 ?? and 2.03 ?? . 
5.2 Evaluation results 
One-directional alignment results 
To train a Chinese-to-English SMT system, 
we need to perform both Chinese-to-English and 
English-to-Chinese word alignment. We only 
evaluate the English-to-Chinese word alignment 
here. GIZA++ with the default settings is used as 
the baseline method. The evaluation results in 
Table 2 indicate that the performances of our 
methods on single word alignments are close to 
that of the baseline method. For multi-word 
alignments, our methods significantly outper-
form the baseline method in terms of both preci-
sion and recall, achieving up to 18% absolute 
error rate reduction. 
Although the size of the bilingual corpus is 
much smaller than that of additional monolingual 
corpora, our methods using CM-1 and CM-2 
achieve comparable performances. It is because 
CM-2 and the BWA model are derived from the 
same resource. By interpolating CM1 and CM2, 
i.e. CM-3, the error rate of multi-word alignment 
results is further reduced. 
Figure 2 shows an example of word alignment 
results generated by the baseline method and the 
improved method using CM-3. In this example, 
our method successfully identifies many-to-one 
alignments such as "the people of the world  
??". In our collocation model, the collocation 
probability of "the people of the world" is much 
higher than that of "people world". And our me-
thod is also effective to prevent the unrelated 
?? ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
China's science and technology research has made achievements which have gained the attention of the people of the world . 
??  ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
zhong-guo  de     ke-xue-ji-shu      yan-jiu      qu-de       le      xu-duo   ling   shi-ren     zhu-mu     de     cheng-jiu . 
china        DE    science and         research   obtain      LE      many     let    common    attract     DE  achievement . 
                             technology                                                                            people    attention   
829
Experiments 
Single word alignments Multi-word alignments All alignments 
P R AER P R AER P R AER 
Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51 
Our methods 
WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45 
WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44 
WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40 
Table 3. Bi-directional word alignment results 
words from being aligned. For example, in the 
baseline alignment "has made ... have ??", 
"have" and "has" are unrelated to the target word, 
while our method only generated "made  ?
?", this is because that the collocation probabili-
ties of "has/have" and "made" are much lower 
than that of the whole source sentence. 
Bi-directional alignment results 
We build a bi-directional alignment baseline 
in two steps: (1) GIZA++ is used to obtain the 
source-to-target and target-to-source alignments; 
(2) the bi-directional alignments are generated by 
using "grow-diag-final". We use the methods 
proposed in section 3 to replace the correspond-
ing steps in the baseline method. We evaluate 
three methods:  
WA-1: one-directional alignment method pro-
posed in section 3.1 and grow-diag-final; 
WA-2: GIZA++ and the bi-directional bilin-
gual word alignments method proposed in 
section 3.2; 
WA-3: both methods proposed in section 3. 
Here, CM-3 is used in our methods. The re-
sults are shown in Table 3. 
We can see that WA-1 achieves lower align-
ment error rate as compared to the baseline me-
thod, since the performance of the improved one-
directional alignment method is better than that 
of GIZA++. This result indicates that improving 
one-directional word alignment results in bi-
directional word alignment improvement. 
The results also show that the AER of WA-2 
is lower than that of the baseline. This is because 
the proposed bi-directional alignment method 
can effectively recognize the correct alignments 
from the alignment union, by leveraging colloca-
tion probabilities of the words in the same cept. 
Our method using both methods proposed in 
section 3 produces the best alignment perfor-
mance, achieving 11% absolute error rate reduc-
tion. 
Experiments BLEU (%) 
Baseline 29.62 
Our methods 
WA-1 
CM-1 30.85 
CM-2 31.28 
CM-3 31.48 
WA-2 
CM-1 31.00 
CM-2 31.33 
CM-3 31.51 
WA-3 
CM-1 31.43 
CM-2 31.62 
CM-3 31.78 
Table 4. Performances of Moses using the dif-
ferent bi-directional word alignments (Signifi-
cantly better than baseline with p < 0.01) 
6 Experiments on Phrase-Based SMT 
6.1 Experimental settings 
We use FBIS corpus to train the Chinese-to-
English SMT systems. Moses (Koehn et al, 2007) 
is used as the baseline phrase-based SMT system. 
We use SRI language modeling toolkit (Stolcke, 
2002) to train a 5-gram language model on the 
English sentences of FBIS corpus. We used the 
NIST MT-2002 set as the development set and 
the NIST MT-2004 test set as the test set. And 
Koehn's implementation of minimum error rate 
training (Och, 2003) is used to tune the feature 
weights on the development set. 
We use BLEU (Papineni et al, 2002) as eval-
uation metrics. We also calculate the statistical 
significance differences between our methods 
and the baseline method by using paired boot-
strap re-sample method (Koehn, 2004). 
6.2 Effect of improved word alignment on 
phrase-based SMT 
We investigate the effectiveness of the improved 
word alignments on the phrase-based SMT sys-
tem. The bi-directional alignments are obtained 
830
 Figure 3. Example of the translations generated by the baseline system and the system where the 
phrase collocation probabilities are added 
Experiments BLEU (%) 
Moses 29.62 
+ Phrase collocation probability 30.47 
+ Improved word alignments 
+ Phrase collocation probability 
32.02 
Table 5. Performances of Moses employing 
our proposed methods (Significantly better than 
baseline with p < 0.01) 
using the same methods as those shown in Table 
3. Here, we investigate three different collocation 
models for translation quality improvement. The 
results are shown in Table 4. 
From the results of Table 4, it can be seen that 
the systems using the improved bi-directional 
alignments achieve higher quality of translation 
than the baseline system. If the same alignment 
method is used, the systems using CM-3 got the 
highest BLEU scores. And if the same colloca-
tion model is used, the systems using WA-3 
achieved the higher scores. These results are 
consistent with the evaluations of word align-
ments as shown in Tables 2 and 3. 
6.3 Effect of phrase collocation probabili-
ties 
To investigate the effectiveness of the method 
proposed in section 4, we only use the colloca-
tion model CM-3 as described in section 5.1. The 
results are shown in Table 5. When the phrase 
collocation probabilities are incorporated into the 
SMT system, the translation quality is improved, 
achieving an absolute improvement of 0.85 
BLEU score. This result indicates that the collo-
cation probabilities of phrases are useful in de-
termining the boundary of phrase and predicting 
whether phrases should be translated together, 
which helps to improve the phrase-based SMT 
performance. 
Figure 3 shows an example: T1 is generated 
by the system where the phrase collocation prob-
abilities are used and T2 is generated by the 
baseline system. In this example, since the collo-
cation probability of "? ??" is much higher 
than that of "?? ?", our method tends to split 
"? ?? ?" into "(? ??) (?)", rather than 
"(?) (?? ?)". For the phrase "?? ??" in 
the source sentence, the collocation probability 
of the translation "in order to avoid" is higher 
than that of the translation "can we avoid". Thus, 
our method selects the former as the translation. 
Although the phrase "?? ?? ?? ?? ?
?" in the source sentence has the same transla-
tion "We must adopt effective measures", our 
method splits this phrase into two parts "?? ?
?" and "?? ?? ??", because two parts 
have higher collocation probabilities than the 
whole phrase. 
We also investigate the performance of the 
system employing both the word alignment im-
provement and phrase table improvement me-
thods. From the results in Table 5, it can be seen 
that the quality of translation is future improved. 
As compared with the baseline system, an abso-
lute improvement of 2.40 BLEU score is 
achieved. And this result is also better than  the 
results shown in Table 4. 
7 Experiments on Parsing-Based SMT 
We also investigate the effectiveness of the im-
proved word alignments on the parsing-based 
SMT system, Joshua (Li et al, 2009). In this sys-
tem, the Hiero-style SCFG model is used 
(Chiang, 2007), without syntactic information. 
The rules are extracted only based on the FBIS 
corpus, where words are aligned by "MW-3 & 
CM-3". And the language model is the same as 
that in Moses. The feature weights are tuned on 
the development set using the minimum error 
??  ??  ??  ??  ??  ??  ??  ?  ??  ? 
wo-men bi-xu      cai-qu   you-xiao  cuo-shi   cai-neng  bi-mian  chu      wen-ti      . 
we          must        use      effective   measure    can        avoid    out      problem  . 
We must  adopt effective measures  in order to avoid  problems  . 
 
 
We must adopt effective measures  can we avoid  out of the  question . 
T1: 
T2: 
831
Experiments BLEU (%) 
Joshua 30.05 
+ Improved word alignments 31.81 
Table 6. Performances of Joshua using the dif-
ferent word alignments (Significantly better than 
baseline with p < 0.01) 
rate training method. We use the same evaluation 
measure as described in section 6.1. 
The translation results on Joshua are shown in 
Table 6. The system using the improved word 
alignments achieves an absolute improvement of 
1.76 BLEU score, which indicates that the im-
provements of word alignments are also effective 
to improve the performance of the parsing-based 
SMT systems. 
8 Conclusion 
We presented a novel method to use monolingual 
collocations to improve SMT. We first used the 
MWA method to identify potentially collocated 
words and estimate collocation probabilities only 
from monolingual corpora, no additional re-
source or linguistic preprocessing is needed. 
Then the collocation information was employed 
to improve BWA for various kinds of SMT sys-
tems and to improve phrase table for phrase-
based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. To improve 
phrase table, we calculate phrase collocation 
probabilities based on word collocation probabil-
ities. Then the phrase collocation probabilities 
are used as additional features in phrase-based 
SMT systems. 
The evaluation results showed that the pro-
posed method significantly improved word 
alignment, achieving an absolute error rate re-
duction of 29% on multi-word alignment. The 
improved word alignment results in an improve-
ment of 2.16 BLEU score on a phrase-based 
SMT system and an improvement of 1.76 BLEU 
score on a parsing-based SMT system. When we 
also used phrase collocation probabilities as ad-
ditional features, the phrase-based SMT perfor-
mance is finally improved by 2.40 BLEU score 
as compared with the baseline system. 
Reference 
Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, 
and Jorg Tiedemann. 2000. Evaluation of Word 
Alignment Systems. In Proceedings of the Second 
International Conference on Language Resources 
and Evaluation, pp. 1255-1261. 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David Ya-
rowsky. 1999. Statistical Machine Translation. Fi-
nal Report. In Johns Hopkins University Workshop. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert. L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics, pp. 88-95. 
David Chiang. 2007. Hierarchical Phrase-Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
Fei Huang. 2009. Confidence Measure for Word 
Alignment. In Proceedings of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP, pp. 932-
940. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing, pp. 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In Processings of the International Workshop 
on Spoken Language Translation 2005. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In Proceed-
ings of the Human Language Technology Confe-
rence and the North American Association for 
Computational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
ACL, Poster and Demonstration Sessions, pp. 177-
180. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-
nitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren Thornton, Jonathan Weese, and Omar Zaidan. 
2009. Demonstration of Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In 
Proceedings of the 47th Annual Meeting of the As-
832
sociation for Computational Linguistics, Software 
Demonstrations, pp. 25-28. 
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear 
Models for Word Alignment. 2005. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 459-466. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pp. 487-495. 
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Lan-
guage Processing,  pp. 133-139. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics, pp. 
1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of 
the 38th Annual Meeting of the Association for 
Computational Linguistics, pp. 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1): 19-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Weijing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th annual meeting of the Association 
for Computational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403. 
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 
2009. A Syntax-Driven Bracketing Model for 
Phrase-Based Translation. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP, pp. 315-323. 
 
833
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036?1044,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering with Source Language Collocations 
 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu Inc., Beijing, China 
{liuzhanyi, wanghaifeng, wu_hua}@baidu.com  
{tliu, lisheng}@hit.edu.cn 
 
 
 
Abstract 
This paper proposes a novel reordering model 
for statistical machine translation (SMT) by 
means of modeling the translation orders of 
the source language collocations. The model 
is learned from a word-aligned bilingual cor-
pus where the collocated words in source sen-
tences are automatically detected. During 
decoding, the model is employed to softly 
constrain the translation orders of the source 
language collocations, so as to constrain the 
translation orders of those source phrases con-
taining these collocated words. The experi-
mental results show that the proposed method 
significantly improves the translation quality, 
achieving the absolute improvements of 
1.1~1.4 BLEU score over the baseline me-
thods. 
1 Introduction 
Reordering for SMT is first proposed in IBM mod-
els (Brown et al, 1993), usually called IBM con-
straint model, where the movement of words 
during translation is modeled. Soon after, Wu 
(1997) proposed an ITG (Inversion Transduction 
Grammar) model for SMT, called ITG constraint 
model, where the reordering of words or phrases is 
constrained to two kinds: straight and inverted. In 
order to further improve the reordering perfor-
mance, many structure-based methods are pro-
posed, including the reordering model in 
hierarchical phrase-based SMT systems (Chiang, 
2005) and syntax-based SMT systems (Zhang et al, 
2007; Marton and Resnik, 2008; Ge, 2010; Vis-
weswariah et al, 2010). Although the sentence 
structure has been taken into consideration, these 
methods don?t explicitly make use of the strong 
correlations between words, such as collocations, 
which can effectively indicate reordering in the 
target language. 
In this paper, we propose a novel method to im-
prove the reordering for SMT by estimating the 
reordering score of the source-language colloca-
tions (source collocations for short in this paper). 
Given a bilingual corpus, the collocations in the 
source sentence are first detected automatically 
using a monolingual word alignment (MWA) me-
thod without employing additional resources (Liu 
et al, 2009), and then the reordering model based 
on the detected collocations is learned from the 
word-aligned bilingual corpus. The source colloca-
tion based reordering model is integrated into SMT 
systems as an additional feature to softly constrain 
the translation orders of the source collocations in 
the sentence to be translated, so as to constrain the 
translation orders of those source phrases contain-
ing these collocated words. 
This method has two advantages: (1) it can au-
tomatically detect and leverage collocated words in 
a sentence, including long-distance collocated 
words; (2) such a reordering model can be inte-
grated into any SMT systems without resorting to 
any additional resources. 
We implemented the proposed reordering mod-
el in a phrase-based SMT system, and the evalua-
tion results show that our method significantly 
improves translation quality. As compared to the 
baseline systems, an absolute improvement of 
1.1~1.4 BLEU score is achieved.  
1036
The paper is organized as follows: In section 2, 
we describe the motivation to use source colloca-
tions for reordering, and briefly introduces the col-
location extraction method. In section 3, we 
present our reordering model. And then we de-
scribe the experimental results in section 4 and 5. 
In section 6, we describe the related work.  Lastly, 
we conclude in section 7. 
2 Collocation 
A collocation is generally composed of a group of 
words that occur together more often than by 
chance. Collocations effectively reveal the strong 
association among words in a sentence and are 
widely employed in a variety of NLP tasks 
(Mckeown and Radey, 2000).   
Given two words in a collocation, they can be 
translated in the same order as in the source lan-
guage, or in the inverted order. We name the first 
case as straight, and the second inverted. Based on 
the observation that some collocations tend to have 
fixed translation orders such as ??? jin-rong ?fi-
nancial? ??  wei-ji ?crisis?? (financial crisis) 
whose English translation order is usually straight, 
and  ???  fa-lv ?law? ??  fan-wei ?scope?? 
(scope of law) whose English translation order is 
generally inverted, some methods have been pro-
posed to improve the reordering model for SMT 
based on the collocated words crossing the neigh-
boring components (Xiong et al, 2006). We fur-
ther notice that some words are translated in 
different orders when they are collocated with dif-
ferent words. For instance, when ??? chao-liu 
?trend?? is collocated with ??? shi-dai ?times??, 
they are often translated into the ?trend of times?; 
when collocated with ??? li-shi ?history??, the 
translation usually becomes the ?historical trend?. 
Thus, if we can automatically detect the colloca-
tions in the sentence to be translated and their or-
ders in the target language, the reordering 
information of the collocations could be used to 
constrain the reordering of phrases during decod-
ing. Therefore, in this paper, we propose to im-
prove the reordering model for SMT by estimating 
the reordering score based on the translation orders 
of the source collocations. 
In general, the collocations can be automatically 
identified based on syntactic information such as 
dependency trees (Lin, 1998). However these me-
thods may suffer from parsing errors. Moreover, 
for many languages, no valid dependency parser 
exists. Liu et al (2009) proposed to automatically 
detect the collocated words in a sentence with the 
MWA method. The advantage of this method lies 
in that it can identify the collocated words in a sen-
tence without additional resources. In this paper, 
we employ MWA Model l~3 described in Liu et al 
(2009) to detect collocations in sentences, which 
are shown in Eq. (1)~(3). 
?
?
?
l
j
cj jwwtSAp 11 ModelMWA 
)|()|(
 (1) 
?
?
??
l
j
jcj lcjdwwtSAp j12 ModelMWA 
),|()|()|(
 (2) 
?
?
?
?
?
???
l
j
jcj
l
i
ii
lcjdwwt
wnSAp
j
1
1
3 ModelMWA 
),|()|(
)|()|(
 (3) 
Where lwS 1?  is a monolingual sentence; i?  de-
notes the number of words collocating with 
iw ; 
}&],1[|),{( icliciA ii ???  denotes the potentially 
collocated words in S. 
The MWA models measure the collocated 
words under different constraints. MWA Model 1 
only models word collocation probabilities 
)|( jcj wwt
. MWA Model 2 additionally employs 
position collocation probabilities 
),|( lcjd j
. Be-
sides the features in MWA Model 2, MWA Model 
3 also considers fertility probabilities )|( ii wn ? . 
Given a sentence, the optimal collocated words 
can be obtained according to Eq. (4). 
)|(maxarg*  ModelMWA SApA iA?
           (4) 
Given a monolingual word aligned corpus, the 
collocation probabilities can be estimated as fol-
lows. 
2
)|()|(),( ijjiji wwpwwpwwr ??           
(5) 
Where, 
?
?
??
w
j
ji
ji wwcount
wwcountwwp ),(
),()|(
; 
),( ji ww  
denotes the collocated words in the corpus and 
),( ji wwcount
 denotes the co-occurrence frequency. 
1037
3 Reordering Model with Source Lan-
guage Collocations 
In this section, we first describe how to estimate 
the orientation probabilities for a given collocation, 
and then describe the estimation of the reordering 
score during translation. Finally, we describe the 
integration of the reordering model into the SMT 
system. 
3.1 Reordering probability estimation 
Given a source collocation ),( ji ff
 and its corres-
ponding translations 
),( ji aa ee
 in a bilingual sen-
tence pair, the reordering orientation of the 
collocation can be defined as in Eq. (6).  
??
?
????
?????
jiji
jiji
aaji aajiaaji
aajiaajio ji &or& ifinverted
or ifstraight
,,,
(6) 
In our method, only those collocated words in 
source language that are aligned to different target 
words, are taken into consideration, and those be-
ing aligned to the same target word are ignored. 
Given a word-aligned bilingual corpus where 
the collocations in source sentences are detected, 
the probabilities of the translation orientation of 
collocations in the source language can be esti-
mated, as follows: 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,straight(),|straight(
   (7) 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,inverted(),|inverted(
   
(8) 
Here, ),,( ji ffocount
 is collected according to 
the algorithm in Figure 1. 
3.2 Reordering model 
Given a sentence lfF 1?  to be translated, the col-
locations are first detected using the algorithm de-
scribed in Eq. (4). Then the reordering score is 
estimated according to the reordering probability 
weighted by the collocation probability of the col-
located words. Formally, for a generated transla-
tion candidate T , the reordering score is calculated 
as follows. 
),|(log),(),( ,,,),( iiciii i ciaacici ciO ffopffrTFP ??
    (9) 
Input: A word-aligned bilingual corpus where 
the source collocations are detected 
Initialization: 
),,( ji ffocount
=0 
for each sentence pair <F, E> in the corpus do 
for each collocated word pair 
),( ici ff
in F do 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffstraightocount
 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffinvertedocount  
Output: ),,( ji ffocount
 
Figure 1. Algorithm of estimating  
reordering frequency 
Here, 
),( ici ffr
 denotes the collocation probabil-
ity of 
if  and icf
 as shown in Eq. (5). 
In addition to the detected collocated words in 
the sentence, we also consider other possible word 
pairs whose collocation probabilities are higher 
than a given threshold.  Thus, the reordering score 
is further improved according to Eq. (10). 
?
?
??
?
??
????
),(&
)},{(),(
,,,
,,,
),(
)},|(log),(
),|(log),(),(
ji
i
ji
iicii
i
i
ffr
ciji
jiaajiji
ciaaci
ci
ciO
ffopffr
ffopffrTFP
 
(10) 
Where ? and ?  are two interpolation weights. 
?  is the threshold of collocation probability. The 
weights and the threshold can be tuned using a de-
velopment set. 
3.3 Integrated into SMT system 
The SMT systems generally employ the log-linear 
model to integrate various features (Chiang, 2005; 
Koehn et al, 2007). Given an input sentence F, the 
final translation E* with the highest score is chosen 
from candidates, as in Eq. (11). 
}),({maxarg*
1
?
?
? M
m
mmE
FEhE ?
 (11) 
Where hm(E, F) (m=1,...,M) denotes fea-
tures.
m?  is a feature weight. 
Our reordering model can be integrated into the 
system as one feature as shown in (10). 
1038
 Figure 2. An example for reordering 
4 Evaluation of Our Method 
4.1 Implementation 
We implemented our method in a phrase-based 
SMT system (Koehn et al, 2007). Based on the 
GIZA++ package (Och and Ney, 2003), we im-
plemented a MWA tool for collocation detection. 
Thus, given a sentence to be translated, we first 
identify the collocations in the sentence, and then 
estimate the reordering score according to the 
translation hypothesis. For a translation option to 
be expanded, the reordering score inside this 
source phrase is calculated according to their trans-
lation orders of the collocations in the correspond-
ing target phrase. The reordering score crossing the 
current translation option and the covered parts can 
be calculated according to the relative position of 
the collocated words. If the source phrase matched 
by the current translation option is behind the cov-
ered parts in the source sentence, then 
...)|staight(log ?op  is used, otherwise 
...)|inverted(log ?op . For example, in Figure 2, the 
current translation option is (
4332 eeff ? ). The 
collocations related to this translation option are 
),( 31 ff , ),( 32 ff , ),( 53 ff . The reordering scores 
can be estimated as follows: 
),|straight(log),( 3131 ffopffr ? 
),|inverted(log),( 3232 ffopffr ? 
),|inverted(log),( 5353 ffopffr ? 
In order to improve the performance of the de-
coder, we design a heuristic function to estimate 
the future score, as shown in Figure 3. For any un-
covered word and its collocates in the input sen-
tence, if the collocate is uncovered, then the higher 
reordering probability is used. If the collocate has 
been covered, then the reordering orientation can 
Input: Input sentence LfF 1?  
Initialization: Score = 0 
for each uncovered word 
if  do 
for each word
jf
(
icj ?  
or 
??)( , ji ffr
) do 
if 
jf
 is covered then 
if i > j then 
Score+=
),|straight(log)( , jiji ffopffr ?
 
else 
Score+=
),|inverted(log)( , jiji ffopffr ? 
else 
 Score +=
),|(log)(maxarg , jijio ffopffr
 
Output: Score 
Figure 3. Heuristic function for estimating future 
score 
be determined according to the relative positions of 
the words and the corresponding reordering proba-
bility is employed. 
4.2 Settings 
We use the FBIS corpus (LDC2003E14) to train a 
Chinese-to-English phrase-based translation model. 
And the SRI language modeling toolkit (Stolcke, 
2002) is used to train a 5-gram language model on 
the English sentences of FBIS corpus.  
We used the NIST evaluation set of 2002 as the 
development set to tune the feature weights of the 
SMT system and the interpolation parameters, 
based on the minimum error rate training method 
(Och, 2003), and the NIST evaluation sets of 2004 
and 2008 (MT04 and MT08) as the test sets. 
We use BLEU (Papineni et al, 2002) as evalua-
tion metrics. We also calculate the statistical signi-
ficance differences between our methods and the 
baseline method by using the paired bootstrap re-
sample method (Koehn, 2004). 
4.3 Translation results 
We compare the proposed method with various 
reordering methods in previous work. 
Monotone model: no reordering model is used. 
Distortion based reordering (DBR) model: a 
distortion based reordering method (Al-
Onaizan & Papineni, 2006). In this method, the 
distortion cost is defined in terms of words, ra-
ther than phrases. This method considers out-
bound, inbound, and pairwise distortions that  
f1    f2     f3     f4      f5 
e4 
e3 
e2 
e1 
1039
Reorder models MT04 MT08 
Monotone model 26.99 18.30 
DBR model 26.64 17.83 
MSDR model (Baseline) 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
SCBR Model 1 29.21 19.28 
SCBR Model 2 29.44 19.36 
SCBR Model 3 29.50 19.44 
SCBR models (1+2) 29.65 19.57 
SCBR models (1+2+3) 29.75 19.61 
Table 1. Translation results on various reordering models 
 
T1: The two sides are also the basic stand of not relaxed. 
T2: The basic stance of the two sides have not relaxed. 
Reference: The basic stances of both sides did not move. 
Figure 4. Translation example.  (*/*) denotes (pstraight / pinverted)
 are directly estimated by simple counting over 
alignments in the word-aligned bilingual cor-
pus. This method is similar to our proposed 
method. But our method considers the transla-
tion order of the collocated words. 
msd-bidirectional-fe reordering (MSDR or 
Baseline) model: it is one of the reordering 
models in Moses. It considers three different 
orientation types (monotone, swap, and discon-
tinuous) on both source phrases and target 
phrases. And the translation orders of both the 
next phrase and the previous phrase in respect 
to the current phrase are modeled. 
Source collocation based reordering (SCBR) 
model: our proposed method. We investigate 
three reordering models based on the corres-
ponding MWA models and their combinations. 
In SCBR Model i (i=1~3), we use MWA Mod-
el i as described in section 2 to obtain the col-
located words and estimate the reordering 
probabilities according to section 3. 
The experiential results are shown in Table 1. 
The DBR model suffers from serious data sparse-
ness. For example, the reordering cases in the 
trained pairwise distortion model only covered 
32~38% of those in the test sets. So its perfor-
mance is worse than that of the monotone model. 
The MSDR model achieves higher BLEU scores 
than the monotone model and the DBR model. Our 
models further improve the translation quality, 
achieving better performance than the combination 
of MSDR model and DBR model. The results in 
Table 1 show that ?MSDR + SCBR Model 3? per-
forms the best among the SCBR models. This is 
because, as compared to MWA Model 1 and 2, 
MWA Model 3 takes more information into con-
sideration, including not only the co-occurrence 
information of lexical tokens and the position of 
words, but also the fertility of words in a sentence. 
And when the three SCBR models are combined, 
the performance of the SMT system is further im-
proved. As compared to other reordering models, 
our models achieve an absolute improvement of 
0.98~1.19 BLEU score on the test sets, which are 
statistically significant (p < 0.05).  
Figure 4 shows an example: T1 is generated by 
the baseline system and T2 is generated by the sys-
tem where the SCBR models (1+2+3)1 are used.  
                                                          
1 In the remainder of this paper, ?SCBR models? means the 
combination of the SCBR models (1+2+3) unless it is explicit-
ly explained.  
Input:  ??     ?   ??      ??  ?    ?  ??  ??   ? 
shuang-fang    DE    ji-ben       li-chang   ye      dou mei-you song-dong . 
(0.99/0.01) 
both-side       DE     basic          stance  also    both    not        loose     . 
(0.21/0.79) 
(0.95/0.05) 
1040
Reordering models MT04 MT08 
MSDR model 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
CBR model 28.96 18.77 
WCBR model 29.15 19.10 
WCBR+SCBR 
models 
29.87 19.83 
Table 2. Translation results of co-occurrence 
based reordering models 
 CBR model 
SCBR 
Model3 
Consecutive words 77.9% 73.5% 
Interrupted words 74.1% 87.8% 
Total 74.3% 84.9% 
Table 3. Precisions of the reordering models on 
the development set 
The input sentence contains three collocations. The 
collocation (??, ??) is included in the same 
phrase and translated together as a whole. Thus its 
translation is correct in both translations. For the 
other two long-distance collocations (??, ??) 
and (??, ??), their translation orders are not 
correctly handled by the reordering model in the 
baseline system. For the collocation (??, ??), 
since the SCBR models indicate p(o=straight|??, 
??) < p(o=inverted|??, ??), the system fi-
nally generates the translation T2 by constraining 
their translation order with the proposed model. 
5 Collocations vs. Co-occurring Words 
We compared our method with the method that 
models the reordering orientations based on co-
occurring words in the source sentences, rather 
than the collocations.  
5.1 Co-occurrence based reordering model 
We use the similar algorithm described in section 3 
to train the co-occurrence based reordering (CBR) 
model, except that the probability of the reordering 
orientation is estimated on the co-occurring words 
and the relative distance. Given an input sentence 
and a translation candidate, the reordering score is 
estimated as shown in Eq. (12). 
? ??? ),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji
        (12) 
Here, 
ji??
 is the relative distance of two words 
in the source sentence.  
We also construct the weighted co-occurrence 
based reordering (WCBR) model. In this model, 
the probability of the reordering orientation is ad-
ditionally weighted by the pointwise mutual infor-
mation 2  score of the two words (Manning and 
Sch?tze, 1999), which is estimated as shown in Eq. 
(13). 
? ???
),(
,,,MI ),,|(log),(
),(
ji
jijiaajiji
O
ffopffs
TFP
ji
   (13) 
5.2 Translation results 
Table 2 shows the translation results. It can be seen 
that the performance of the SMT system is im-
proved by integrating the CBR model. The perfor-
mance of the CBR model is also better than that of 
the DBR model. It is because the former is trained 
based on all co-occurring aligned words, while the 
latter only considers the adjacent aligned words. 
When the WCBR model is used, the translation 
quality is further improved. However, its perfor-
mance is still inferior to that of the SCBR models, 
indicating that our method (SCBR models) of 
modeling the translation orders of source colloca-
tions is more effective. Furthermore, we combine 
the weighted co-occurrence based model and our 
method, which outperform all the other models. 
5.3 Result analysis 
Precision of prediction 
First of all, we investigate the performance of 
the reordering models by calculating precisions of 
the translation orders predicted by the reordering 
models. Based on the source sentences and refer-
ence translations of the development set, where the 
source words and target words are automatically 
aligned by the bilingual word alignment method, 
we construct the reference translation orders for 
two words. Against the references, we calculate 
three kinds of precisions as follows: 
|}1|||{|
|}&1{|
,
,,,,
CW ??
???? jio
ooj||iP
ji
aajiji ji
 (14) 
                                                          
2 For occurring words extraction, the window size is set to [-6, 
+6]. 
1041
|}1|||{|
|}&1{|
,
,,,,
IW ??
???? jio
ooj||iP
ji
aajiji ji
 (15) 
 |}{|
|}{|
,
,,,,
total
ji
aajiji
o
ooP ji??
 (16) 
Here, 
jio ,
 denotes the translation order of (
ji ff ,
) 
predicted by the reordering models. If 
)|straight( , ji ffop ?
>
),inverted( ji f|fop ?
, then 
straight, ?jio
, else if 
)|straight( , ji ffop ?
< 
),inverted( ji f|fop ?
, then
inverted, ?jio
. 
ji aajio ,,,
 
denotes the translation order derived from the word 
alignments. If 
ji aajiji oo ,,,, ?
, then the predicted 
translation order is correct, otherwise wrong. 
CWP  
and 
IWP  denote the precisions calculated on the 
consecutive words and the interrupted words in the 
source sentences, respectively. 
totalP  denotes the 
precision on both cases. Here, the CBR model and 
SCBR Model 3 are compared. The results are 
shown in Table 3.  
From the results in Table 3, it can be seen that 
the CBR model has a higher precision on the con-
secutive words than the SCBR model, but lower 
precisions on the interrupted words. It is mainly 
because the CBR model introduces more noise 
when the relative distance of words is set to a large 
number, while the MWA method can effectively 
detect the long-distance collocations in sentences 
(Liu et al, 2009). This explains why the combina-
tion of the two models can obtain the highest 
BLEU score as shown in Table 2. On the whole, 
the SCBR Model 3 achieves higher precision than 
the CBR model. 
Effect of the reordering model 
Then we evaluate the reordering results of the 
generated translations in the test sets. Using the 
above method, we construct the reference transla-
tion orders of collocations in the test sets. For a 
given word pair in a source sentence, if the transla-
tion order in the generated translation is the same 
as that in the reference translations, then it is cor-
rect, otherwise wrong. 
We compare the translations of the baseline me-
thod, the co-occurrence based method, and our me-
thod (SCBR models). The precisions calculated on 
both kinds of words are shown in Table 4. From 
Test sets 
Baseline 
(MSDR) 
MSDR+ 
WCBR 
MSDR+ 
SCBR 
MT04 78.9% 80.8% 82.5% 
MT08 80.7% 83.8% 85.0% 
Table 4. Precisions (total) of the reordering 
models on the test sets 
the results, it can be seen that our method achieves 
higher precisions than both the baseline and the 
method modeling the translation orders of the co-
occurring words. It indicates that the proposed me-
thod effectively constrains the reordering of source 
words during decoding and improves the transla-
tion quality. 
6 Related Work 
Reordering was first proposed in the IBM models 
(Brown et al, 1993), later was named IBM con-
straint by Berger et al (1996). This model treats 
the source word sequence as a coverage set that is 
processed sequentially and a source token is cov-
ered when it is translated into a new target token. 
In 1997, another model called ITG constraint was 
presented, in which the reordering order can be 
hierarchically modeled as straight or inverted for 
two nodes in a binary branching structure (Wu, 
1997). Although the ITG constraint allows more 
flexible reordering during decoding, Zens and Ney 
(2003) showed that the IBM constraint results in 
higher BLEU scores. Our method models the reor-
dering of collocated words in sentences instead of 
all words in IBM models or two neighboring 
blocks in ITG models. 
For phrase-based SMT models, Koehn et al 
(2003) linearly modeled the distance of phrase 
movements, which results in poor global reorder-
ing. More methods are proposed to explicitly mod-
el the movements of phrases (Tillmann, 2004; 
Koehn et al, 2005) or to directly predict the orien-
tations of phrases (Tillmann and Zhang, 2005; 
Zens and Ney, 2006), conditioned on current 
source phrase or target phrase. Hierarchical phrase-
based SMT methods employ SCFG bilingual trans-
lation model and allow flexible reordering (Chiang, 
2005). However, these methods ignored the corre-
lations among words in the source language or in 
the target language. In our method, we automati-
cally detect the collocated words in sentences and 
1042
their translation orders in the target languages, 
which are used to constrain the ordering models 
with the estimated reordering (straight or inverted) 
score. Moreover, our method allows flexible reor-
dering by considering both consecutive words and 
interrupted words. 
In order to further improve translation results, 
many researchers employed syntax-based reorder-
ing methods (Zhang et al, 2007; Marton and Res-
nik, 2008; Ge, 2010; Visweswariah et al, 2010). 
However these methods are subject to parsing er-
rors to a large extent. Our method directly obtains 
collocation information without resorting to any 
linguistic knowledge or tools, therefore is suitable 
for any language pairs. 
In addition, a few models employed the collo-
cation information to improve the performance of 
the ITG constraints (Xiong et al, 2006). Xiong et 
al. used the consecutive co-occurring words as col-
location information to constrain the reordering, 
which did not lead to higher translation quality in 
their experiments. In our method, we first detect 
both consecutive and interrupted collocated words 
in the source sentence, and then estimated the 
reordering score of these collocated words, which 
are used to softly constrain the reordering of source 
phrases. 
7 Conclusions 
We presented a novel model to improve SMT by 
means of modeling the translation orders of source 
collocations. The model was learned from a word-
aligned bilingual corpus where the potentially col-
located words in source sentences were automati-
cally detected by the MWA method. During 
decoding, the model is employed to softly con-
strain the translation orders of the source language 
collocations. Since we only model the reordering 
of collocated words, our methods can partially al-
leviate the data sparseness encountered by other 
methods directly modeling the reordering based on 
source phrases or target phrases. In addition, this 
kind of reordering information can be integrated 
into any SMT systems without resorting to any 
additional resources. 
The experimental results show that the pro-
posed method significantly improves the transla-
tion quality of a phrase based SMT system, 
achieving an absolute improvement of 1.1~1.4 
BLEU score over the baseline methods. 
References 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL, pp. 529-536. 
Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-
tra, Vincent J. Della Pietra, Andrew S. Kehler, and 
Robert L. Mercer. 1996. Language Translation Appa-
ratus and Method of Using Context-Based Transla-
tion Models. United States Patent, Patent Number 
5510981, April.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Pietra, and Robert. L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter 
estimation. Computational Linguistics, 19(2): 263-
311. 
David Chiang. 2005. A Hierarchical Phrase-based Mod-
el for Statistical Machine Translation. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 263-270. 
Niyu Ge. 2010. A Direct Syntax-Driven Reordering 
Model for Phrase-Based Machine Translation. In 
Proceedings of Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the ACL, pp. 849-857. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 388-395. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proceedings of the 45th Annual Meeting of the ACL, 
Poster and Demonstration Sessions, pp. 177-180. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of International Workshop on Spoken 
Language Translation. 
1043
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 487-495. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language 
Processing, Cambridge, MA; London, U.K.: Brad-
ford Book & MIT Press. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1) : 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Weij-
ing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 101-104. 
Christoph Tillmann and Tong Zhang. 2005. A Localized 
Prediction Model for Statistical Machine Translation. 
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 557-564. 
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, 
Vijil Chenthamarakshan, and Nanda Kambhatla. 
2010. Syntax Based Reordering with Automatically 
Derived Rules for Improved Statistical Machine 
Translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pp. 1119-
1127. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
the 21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 521-528. 
Richard Zens and Herman Ney. 2003. A Comparative 
Study on Reordering Constraints in Statistical Ma-
chine Translation. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, pp. 192-202. 
Richard Zens and Herman Ney. 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical 
Machine Translation, pp. 55-63. 
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning, pp. 533-540. 
 
 
 
1044
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information?
Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,
Huailin Dong1, and Qun Liu2
Xiamen University, Xiamen, China1
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2
Baidu Inc., Beijing, China3
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
Abstract
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
1 Introduction
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al, 2003; Och and Ney, 2004;
Galley et al, 2006; Liu et al, 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
?Part of this work was done during the first author?s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al, 2008; Wu et al, 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al, 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
459
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al, 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, ?bank? often occurs in the sentences
related to the economy topic when translated into
?y?inha?ng?, and occurs in the sentences related to the
geography topic when translated to ?he?a`n?. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if ?bank? occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that ?bank?
is translated to ?y?inha?ng? than to ?he?a`n?. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
2 Background
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al, 2003), given a source sentence f = fJ1 =
f1, . . . , fj , . . . , fJ , a target sentence e = eI1 =
e1, . . . , ei, . . . , eI , and its word alignment a which
is a subset of the Cartesian product of word position-
s: a ? (j, i) : j = 1, . . . , J ; i = 1, . . . , I , the phrase
pair (f? , e?) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
?(e?|f?) =
count(f? , e?)
?
e??
count(f? , e??)
(1)
Here count(f? , e?) indicates how often the phrase pair
(f? , e?) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(e|f) by relative frequency from the train-
ing corpus:
w(e|f) =
count(f, e)
?
e?
count(f, e?)
(2)
Retaining the alignment a? between the phrase pair
(f? , e?), the corresponding lexical weight is calculated
as
pw(e?|f? , a?) =
|e?|?
i=1
1
|{j|(j, i) ? a?}|
?
?(j,i)?a?
w(ei|fj) (3)
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
460
3 Translation Model Adaptation via
Monolingual Topic Information
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
3.1 Hidden Topic Markov Model
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the ?bag-of-words? assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the ?bag-of-words? assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P (word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P (topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
3.2 Adapted Phrase Probability Estimation
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
? Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
? Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
? Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability ?(e?|f?)1 as
follows:
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|f? , tf ) ? P (tf |f?) (4)
where ?(e?|f? , tf ) indicates the probability of trans-
lating f? into e? given the source-side topic tf ,
P (tf |f?) denotes the phrase-topic distribution of f? .
To compute ?(e?|f?), we first apply HTMM to re-
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
?(e?|f? , tf ) and P (tf |f?) from these two corpora. To
avoid confusion, we further refine ?(e?|f? , tf ) and
P (tf |f?) with ?(e?|f? , tf out) and P (tf in|f?), respec-
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability ?(f? |e?), which can be
adjusted in a similar way to ?(e?|f?) with the help of in-domain
monolingual corpus in the target language.
461
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P (tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P (tf out|f?) as follows:
P (tf out|f?) =
?
tf in
P (tf out|tf in) ? P (tf in|f?) (5)
Thus formula (4) can be further refined as the fol-
lowing formula:
?(e?|f?) =
?
tf out
?
tf in
?(e?|f? , tf out)
?P (tf out|tf in) ? P (tf in|f?) (6)
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
3.2.1 Topic-Specific Phrase Translation
Probability ?(e?|f? , tf out)
We follow the common practice (Koehn et al,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
P (tf out|f) from the relevant topic model of Cf out,
the conditional probability ?(e?|f? , tf out) can be eas-
ily obtained by MLE method:
?(e?|f? , tf out)
=
?
?f ,e??Cout
count?f ,e?(f? , e?) ? P (tf out|f)
?
e??
?
?f ,e??Cout
count?f ,e?(f? , e??) ? P (tf out|f)
(7)
where Cout is the out-of-domain bilingual training
corpus, and count?f ,e?(f? , e?) denotes the number of
the phrase pair (f? , e?) in sentence pair ?f , e?.
3.2.2 Topic Mapping Probability P (tf out|tf in)
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
P (tf out|tf in) as follows:
P (tf out|tf in)
=
?
f?Cf out
?
Cf in
P (tf out|f) ? P (f |tf in) (8)
Here we first get P (f |tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P (tf out|f) as:
P (tf out|f)
=
?
f?Cf out
countf (f) ? P (tf out|f)
?
tf out
?
f?Cf out
countf (f) ? P (tf out|f)
(9)
where countf (f) denotes the number of the word f
in sentence f .
3.2.3 Phrase-Topic Distribution P (tf in|f? )
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
P (tf in|f?) = ? ? Pmle(tf in|f?) +
(1? ?) ? Pword(tf in|f?) (10)
where Pmle(tf in|f?) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in|f?) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
? is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f? in sentence f
denoted as countf (f?), we compute the in-domain
phrase-topic distribution in the following way:
Pmle(tf in|f?)
=
?
f?Cf in
countf (f?) ? P (tf in|f)
?
tf in
?
f?Cf in
countf (f?) ? P (tf in|f)
(11)
462
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in|f?). One is
a ?Noisy-OR? combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in|f?) is
defined as:
Pword(tf in|f?)
= 1? Pword(t?f in|f?)
? 1?
?
fj?f?
P (t?f in|fj)
= 1?
?
fj?f?
(1? P (tf in|fj)) (12)
where Pword(t?f in|f?) represents the probability that
tf in is not the topic of the phrase f? . Similarly,
P (t?f in|fj) indicates the probability that tf in is not
the topic of the word fj .
The other method is an ?Averaging? combination
one. With the assumption that tf in is the topic of f?
if at least one of the words in f? belongs to this topic,
we derive Pword(tf in|f?) as follows:
Pword(tf in|f?) ?
?
fj?f?
P (tf in|fj)/|f? | (13)
where |f? | denotes the number of words in phrase f? .
3.3 Adapted Lexical Probability Estimation
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
word, so
w(e|f) =
?
tf out
?
tf in
w(e|f, tf out)
?P (tf out|tf in) ? P (tf in|f) (14)
Here we obtain w(e|f, tf out) with a simi-
lar approach to ?(e?|f? , tf out), and calculate
P (tf out|tf in) and P (tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair (f? , e?).
4 Experiment
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
4.1 Experimental setup
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al, 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, ? = 1.5, ? = 1.01,
iters = 100. See (Gruber et al, 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight ? in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
463
phrase-based machine translation system (Koehn
et al, 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
?grow-diag-final-and? are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: ?Noisy-OR? and ?Averaging?.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
Adaptation
Method
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
Table 1: Experimental results using different smoothing
methods.
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both ?Noisy-OR? and ?Averaging? combination
methods improve the performance over the base-
line, and ?Averaging? method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the ?Noisy-OR? method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than ?Averaging?
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
?Averaging?method.
4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with ?OutBP? and ?AdapBP?, respec-
tively.
Used Phrase
Table
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both ?AdapBP? and ?OutBP + AdapBP?
consistently outperform the baseline, and the lat-
464
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
ter produces further improvements over the former.
Specifically, the BLEU scores of the ?OutBP +
AdapBP? method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
?AdapBP? method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the ?AdapBP?
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by ?AdapBP?,
therefore further improving the translation quality.
4.2.3 Effect of In-domain Monolingual Corpus
Size
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the ?OutBP + AdapBP? method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P < 0.01 using the significance test tool
developed by Zhang et al(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
5 Related work
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al, 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al, 2009) or the phrase pairs of phrase ta-
ble (Foster et al, 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
465
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al, 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al, 2007; Carpuat and Wu, 2007; He
et al, 2008; Liu et al, 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
? We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
? In addition to the utilization of in-domain
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al, 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model ? HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
? Different from trigger-based lexicon model and
context-dependent translation selection both of
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
6 Conclusion and future work
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
466
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
Acknowledgement
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. of ACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?s
Andre?s-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
467
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Sas?a
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Sas?a Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
468
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979?987,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improve SMT Quality with Automatically Extracted Paraphrase Rules 
 
 
Wei He1, Hua Wu2, Haifeng Wang2, Ting Liu1* 
1Research Center for Social Computing and Information 
Retrieval, Harbin Institute of Technology 
{whe,tliu}@ir.hit.edu.cn 
2Baidu 
{wu_hua,wanghaifeng}@baidu.com 
 
 
 
Abstract1 
We propose a novel approach to improve 
SMT via paraphrase rules which are 
automatically extracted from the bilingual 
training data. Without using extra 
paraphrase resources, we acquire the rules 
by comparing the source side of the parallel 
corpus with the target-to-source 
translations of the target side. Besides the 
word and phrase paraphrases, the acquired 
paraphrase rules mainly cover the 
structured paraphrases on the sentence 
level. These rules are employed to enrich 
the SMT inputs for translation quality 
improvement. The experimental results 
show that our proposed approach achieves 
significant improvements of 1.6~3.6 points 
of BLEU in the oral domain and 0.5~1 
points in the news domain. 
1 Introduction 
The translation quality of the SMT system is 
highly related to the coverage of translation models. 
However, no matter how much data is used for 
training, it is still impossible to completely cover 
the unlimited input sentences. This problem is 
more serious for online SMT systems in real-world 
applications. Naturally, a solution to the coverage 
problem is to bridge the gaps between the input 
sentences and the translation models, either from 
the input side, which targets on rewriting the input 
sentences to the MT-favored expressions, or from 
                                                          
This work was done when the first author was visiting Baidu. 
*Correspondence author: tliu@ir.hit.edu.cn 
the side of translation models, which tries to enrich 
the translation models to cover more expressions.  
In recent years, paraphrasing has been proven 
useful for improving SMT quality. The proposed 
methods can be classified into two categories 
according to the paraphrase targets: (1) enrich 
translation models to cover more bilingual 
expressions; (2) paraphrase the input sentences to 
reduce OOVs or generate multiple inputs. In the 
first category, He et al (2011), Bond et al (2008) 
and Nakov (2008) enriched the SMT models via 
paraphrasing the training corpora. Kuhn et al 
(2010) and Max (2010) used paraphrases to 
smooth translation models. For the second 
category, previous studies mainly focus on finding 
translations for unknown terms using phrasal 
paraphrases. Callison-Burch et al (2006) and 
Marton et al (2009) paraphrase unknown terms in 
the input sentences using phrasal paraphrases 
extracted from bilingual and monolingual corpora. 
Mirkin et al (2009) rewrite OOVs with 
entailments and paraphrases acquired from 
WordNet. Onishi et al (2010) and Du et al (2010) 
use phrasal paraphrases to build a word lattice to 
get multiple input candidates. In the above 
methods, only word or phrasal paraphrases are 
used for input sentence rewriting. No structured 
paraphrases on the sentence level have been 
investigated. However, the information in the 
sentence level is very important for disambiguation.  
For example, we can only substitute play with 
drama in a context related to stage or theatre. 
Phrasal paraphrase substitutions can hardly solve 
such kind of problems.  
In this paper, we propose a method that rewrites 
979
the input sentences of the SMT system using 
automatically extracted paraphrase rules which can 
capture structures on sentence level in addition to 
paraphrases on the word or phrase level. Without 
extra paraphrase resources, a novel approach is 
proposed to acquire paraphrase rules from the 
bilingual training corpus based on the results of 
Forward-Translation and Back-Translation. The 
rules target on rewriting the input sentences to an 
MT-favored expression to ensure a better 
translation. The paraphrase rules cover all kinds of 
paraphrases on the word, phrase and sentence 
levels, enabling structure reordering, word or 
phrase insertion, deletion and substitution. The 
experimental results show that our proposed 
approach achieves significant improvements of 
1.6~3.6 points of BLEU in the oral domain and 
0.5~1 points in the news domain. 
The remainder of the paper is organized as 
follows: Section 2 makes a comparison between 
the Forward-Translation and Back-Translation. 
Section 3 introduces our methods that extract 
paraphrase rules from the bilingual corpus of SMT. 
Section 4 describes the strategies for constructing 
word lattice with paraphrase rules. The 
experimental results and some discussions are 
presented in Section 5 and Section 6. Section 7 
compares our work to the previous researches. 
Finally, Section 8 concludes the paper and suggests 
directions for future work. 
2 Forward-Translation vs. Back-
Translation 
The Back-Translation method is mainly used for 
automatic MT evaluation (Rapp 2009). This 
approach is very helpful when no target language 
reference is available. The only requirement is that 
the MT system needs to be bidirectional. The 
procedure includes translating a text into certain 
foreign language with the MT system (Forward-
Translation), and translating it back into the 
original language with the same system (Back-
Translation). Finally the translation quality of 
Back-Translation is evaluated by using the original 
source texts as references. 
Sun et al (2010) reported an interesting 
phenomenon: given a bilingual text, the Back-
Translation results of the target sentences is better 
than the Forward-Translation results of the source 
sentences. Clearly, let (S0, T0) be the initial pair of 
bilingual text. A source-to-target translation system 
SYS_ST and a target-to-source translation system 
SYS_TS are trained using the bilingual corpus. 
?????  is a Forward-Translation function, and 
????? is a function of Back-Translation which can 
be deduced with two rounds of translations: 
????? ? ???_??????_??????. In the first round 
of translation, S0 and T0 are fed into SYS_ST and 
SYS_TS, and we get T1 and S1 as translation results. 
In the second round, we translate S1 back into the 
target side with SYS_ST, and get the translation T2. 
The procedure is illustrated in Figure 1, which can 
also formally be described as: 
1. T1 = FT(S0) = SYS_ST(S0). 
2. T2 = BT(T0), which can be decomposed into 
two steps: S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Using T0 as reference, an interesting result is 
reported in Sun et al (2010) that T2 achieves a 
higher score than T1 in automatic MT evaluation. 
This outcome is important because T2 is translated 
Figure 1: Procedure of Forward-Translation and Back-Translation. 
S0 T0 
S1 T1 
T2 
Source Language Target Language 
Initial Parallel Text 
1st Round Translation 
2nd Round Translation 
Forward- 
Translation
Back- 
Translation 
980
from a machine-generated text S1, but T1 is 
translated from a human-write text S0. Why the 
machine-generated text results in a better 
translation than the human-write text? Two 
possible reasons may explain this phenomenon: (1) 
in the first round of translation T0 ? S1, some 
target word orders are reserved due to the 
reordering failure, and these reserved orders lead to 
a better result in the second round of translation; (2) 
the text generated by an MT system is more likely 
to be matched by the reversed but homologous MT 
system.  
Note that all the texts of S0, S1, S2, T0 and T1 are 
sentence aligned because the initial parallel corpus 
(S0, T0) is aligned in the sentence level. The aligned 
sentence pairs in (S0, S1) can be considered as 
paraphrases. Since S1 has some MT-favored 
structures which may result in a better translation, 
an intuitive idea is whether we can learn these 
structures by comparing S1 with S0. This is the 
main assumption of this paper. Taking (S0, S1) as 
paraphrase resource, we propose a method that 
automatically extracts paraphrase rules to capture 
the MT-favored structures. 
3 Extraction of Paraphrase Rules 
3.1 Definition of Paraphrase Rules 
We define a paraphrase rule as follows: 
1. A paraphrase rule consists of two parts, left-
hand-side (LHS) and right-hand-side (RHS). 
Both of LHS and RHS consist of non-
terminals (slot) and terminals (words). 
2. LHS must start/end with a terminal. 
3. There must be at least one terminal between 
two non-terminals in LHS. 
A paraphrase rule in the format of:  
LHS ? RHS 
which means the words matched by LHS can be 
paraphrased to RHS. Taking Chinese as a case 
study, some examples of paraphrase rules are 
shown in Table 1. 
3.2  Selecting Paraphrase Sentence Pairs 
Following the methods in Section 2, the initial 
bilingual corpus is (S0, T0). We train a source-to-
target PBMT system (SYS_ST) and a target-to-
source PBMT system (SYS_TS) on the parallel 
corpus. Then a Forward-Translation is performed 
on S0 using SYS_ST, and a Back-Translation is 
performed on T0 using SYS_TS and SYS_ST. As 
mentioned above, the detailed procedure is: T1 = 
SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Finally we compute BLEU (Papineni et al 2002) 
score for every sentence in T2 and T1, using the 
corresponding sentence in T0 as reference. If the 
sentence in T2 has a higher BLEU score than the 
aligned sentence in T1, the corresponding sentences 
in S0 and S1 are selected as candidate paraphrase 
sentence pairs, which are used in the following 
steps of paraphrase extractions. 
3.3 Word Alignments Filtering 
We can construct word alignment between S0 and 
S1 through T0. On the initial corpus of (S0, T0), we 
conduct word alignment with Giza++ (Och and 
Ney, 2000) in both directions and then apply the 
grow-diag-final heuristic (Koehn et al, 2005) for 
symmetrization. Because S1 is generated by 
feeding T0 into the PBMT system SYS_TS, the 
word alignment between T0 and S1 can be acquired 
from the verbose information of the decoder. 
The word alignments of S0 and S1 contain noises 
which are produced by either wrong alignment of 
GIZA++ or translation errors of SYS_TS. To ensure 
the alignment quality, we use some heuristics to 
filter the alignment between S0 and S1: 
1. If two identical words are aligned in S0 and 
S1, then remove all the other links to the two 
words. 
No. LHS RHS 
1 ??/ride   X1   ????/bus ??/ride    X1   ??/bus 
2    ?/at   X1  ?/location   ???/turn left  ???/turn left   ?/at   X1  ?/location 
3 ?/NULL   X1    ?/give    ?/me ?/give    ?/me    X1 
4 ?/from  X1  ?/to  X2  ?/need ??/how long??/time 
?/need   ?/spend  ??/how long  ??/time 
?/from X1?/to X2 
Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word 
981
2. Stop words (including some function words 
and punctuations) can only be aligned to 
either stop words or null. 
Figure 2 illustrates an example of using the 
heuristics to filter alignment. 
3.4 Extracting Paraphrase Rules 
From the word-aligned sentence pairs, we then 
extract a set of rules that are consistent with the 
word alignments. We use the rule extracting 
methods of Chiang (2005). Take the sentence pair 
in Figure 2 as an example, two initial phrase pairs 
PP1 = ?? ? ?? ??? ||| ? ? ?? ????  
and  PP2 = ?? ? ? ?? ??? ? ?? ||| ? 
? ?? ? ? ?? ???? are identified, and 
PP1 is contained by PP2, then we could form the 
rule: 
? X1 ? ?? ? ? ? ?? X1
to  have interest  very feel interest  
4 Paraphrasing the Input Sentences 
The extracted paraphrase rules aim to rewrite the 
input sentences to an MT-favored form which may 
lead to a better translation. However, it is risky to 
directly replace the input sentence with a 
paraphrased sentence, since the errors in automatic 
paraphrase substitution may jeopardize the 
translation result seriously. To avoid such damage, 
for a given input sentence, we first transform all 
paraphrase rules that match the input sentences to 
phrasal paraphrases, and then build a word lattice 
for SMT decoder using the phrasal paraphrases. In 
this case, the decoder can search for the best result 
among all the possible paths. 
The input sentences are first segmented into sub-
sentences by punctuations. Then for each sub-
sentence, the matched paraphrase rules are ranked 
according to: (1) the number of matched words; (2) 
the frequency of the paraphrase rule in the training 
data. Actually, the ranking strategy tends to select 
paraphrase rules that have more matched words 
(therefore less ambiguity) and higher frequency 
(therefore more reliable). 
4.1 Applying Paraphrase Rules 
Given an input sentence S and a paraphrase rule R 
<RLHS, RRHS>, if S matches RLHS, then the matched 
part can be replaced by RRHS. An example for 
applying the paraphrase rules is illustrated in 
Figure 3.  
From Figure 3, we can see that the words of 
position 1~3 are replaced to ??? 10 ? ???. 
Actually, only the words at position 3 and 4 are 
paraphrased to the word ????, other words are 
left unchanged. Therefore, we can use a triple, 
<MIN_RP_TEXT, COVER_START, COVER_LEN> 
(<?? , 3, 1> in this example) to denote the 
paraphrase rule, which means the minimal text to 
replace is ????, and the paraphrasing starts at 
position 3 and covers 1 words. 
In this manner, all the paraphrase rules matched 
for a certain sentence can be converted to the 
format of <MIN_RP_TEXT, COVER_START, 
COVER_LEN>, which can also be considered as 
phrasal paraphrases. Then the methods of building 
phrasal paraphrases into word lattice for SMT 
inputs can be used in our approaches. 
??    ??     [10?]   ????
??     [10?]      ?? 
Rule 
LHS:??/ride  X1 ????/bus 
RHS:??/ride  X1  ??/bus 
Figure 3: Example for Applying Paraphrase Rules 
0         1            2                3
welcome  ride     No.10         bus
ride       No.10        bus 
I  very feel interest that N/A  blue   handbag  
I     to   that   N/A  blue  handbag have interest    
?   ?   ?    ??   ?    ?  ??   ???     ? 
?   ?    ?     ?    ??   ???  ?  ??     ? 
Figure 2: Example for Word Alignment 
Filtration 
I     to   that   N/A  blue  handbag have interest    
?   ?    ?     ?    ??   ???  ?  ??     ? 
I  very feel interest that N/A  blue   handbag  
?   ?   ?    ??   ?    ?  ??   ???      ? 
982
4.2 Construction of Paraphrase Lattice 
Given an input sentence, all the matched 
paraphrase rules are converted to phrasal 
paraphrases first. Then we build the phrasal 
paraphrases into word lattices using the methods 
proposed by Du et al (2010). The construction 
process takes advantage of the correspondence 
between detected phrasal paraphrases and positions 
of the original words in the input sentence, and 
then creates extra edges in the lattices to allow the 
decoder to consider paths involving the paraphrase 
words. An example is illustrated in Figure 4: given 
a sequence of words {w1,?,wN} as the input, two 
phrases ? ={?1,??p} and ? = {?1,?, ?q} are 
detected as paraphrases for P1 = {wx,?, wy} (1 ? x 
? y ? N) and P2 = {wm,?,wn} (1 ? m ? n ? N) 
respectively. The following steps are taken to 
transform them into word lattices: 
1. Transform the original source sentence into 
word lattice. N + 1 nodes (?k, 0 ? k ? N) are 
created, and N edges labeled with wi (1 ? i ? 
N) are generated to connect them 
sequentially. 
2. Generate extra nodes and edges for each of 
the paraphrases. Taking ? as an example, 
firstly, p ? 1 nodes are created, and then p 
edges labeled with ?j (1 ? j ? p) are 
generated to connect node ?x-1, p-1 nodes 
and ?y-1. 
Via step 2, word lattices are generated by adding 
new nodes and edges coming from paraphrases. 
4.3  Weight Estimation 
The weights of new edges in the lattices are 
estimated by an empirical method base on ranking 
positions. Following Du et al (2010), supposing 
that E = {e1,?,ek} are a set of new edges 
constructed from k paraphrase rules, which are 
sorted in a descending order. Then the weight for 
an edge ei is calculated as: 
??e?? ? 1? ? ? ???1 ? ? ? ?? where k is a predefined tradeoff parameter between 
decoding speed and the number of potential 
paraphrases being considered. 
5  Experiments 
5.1  Experimental Data 
In our experiments, we used Moses (Koehn et al, 
2007) as the baseline system which can support 
lattice decoding. The alignment was obtained using 
GIZA++ (Och and Ney, 2003) and then we 
symmetrized the word alignment using the grow-
diag-final heuristic. Parameters were tuned using 
Minimum Error Rate Training (Och, 2003). To 
comprehensively evaluate the proposed methods in 
different domains, two groups of experiments were 
carried out, namely, the oral group (Goral) and the 
news group (Gnews). The experiments were 
conducted in both Chinese-English and English-
Chinese directions for the oral group, and Chinese-
English direction for the news group. The English 
sentences were all tokenized and lowercased, and 
the Chinese sentences were segmented into words 
by Language Technology Platform (LTP) 1 . We 
used SRILM2 for the training of language models 
(5-gram in all the experiments). The metrics for 
automatic evaluation were BLEU 3  and TER 4 
(Snover et al, 2005). 
The detailed statistics of the training data in Goral 
are showed in Table 2. For the bilingual corpus, we 
used the BTEC and PIVOT data of IWSLT 2008, 
HIT corpus 5  and other Chinese LDC (CLDC) 
                                                          
1 http://ir.hit.edu.cn/ltp/ 
2 http://www.speech.sri.com/projects/srilm/ 
3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 
4 http://www.umiacs.umd.edu/~snover/terp/ 
5 The HIT corpus contains the CLDC Olympic corpus (2004-
863-008) and the other HIT corpora available at 
http://mitlab.hit.edu.cn/index.php/resources/29-the-
resource/111-share-bilingual-corpus.html. 
Figure 4: An example of lattice-based 
paraphrases for an input sentence. 
983
corpora, including the Chinese-English Sentence 
Aligned Bilingual Corpus (CLDC-LAC-2003-004) 
and the Chinese-English Parallel Corpora (CLDC-
LAC-2003-006). We trained a Chinese language 
model for the E-C translation on the Chinese part 
of the bi-text. For the English language model of 
C-E translation, an extra corpus named Tanaka was 
used besides the English part of the bilingual 
corpora. For testing and developing, we used six 
Chinese-English development corpora of IWSLT 
2008. The statistics are shown in Table 3.  
In detail, we chose CSTAR03-test and 
IWSLT06-dev as the development set; and used 
IWSLT04-test, IWSLT05-test, IWSLT06-dev and 
IWSLT07-test for testing. For English-Chinese 
evaluation, we used IWSLT English-Chinese MT 
evaluation 2005 as the test set. Due to the lacking 
of development set, we did not tune parameters on 
English-Chinese side, instead, we just used the 
default parameters of Moses. 
In the experiments of the news group, we used 
the Sinorama and FBIS corpora (LDC2005T10 and 
LDC2003E14) for bilingual corpus. After 
tokenization and filtering, this bilingual corpus 
contained 319,694 sentence pairs (7.9M tokens on 
Chinese side and 9.2M tokens on English side). 
We trained a 5-gram language model on the 
English side of the bi-text. The system was tested 
using the Chinese-English MT evaluation sets of 
NIST 2004, NIST 2006 and NIST 2008. For 
development, we used the Chinese-English MT 
evaluation sets of NIST 2002 and NIST 2005. 
Table 4 shows the statistics of test/development 
sets used in the news group. 
5.2 Results 
We extract both Chinese and English rules in Goral, 
and Chinese paraphrase rules in Gnews by 
comparing the results of Forward-Translation and 
Back-Translation as described in Section 3. During 
the extraction, some heuristics are used to ensure 
the quality of paraphrase rules. Take the extraction 
of Chinese paraphrase rules in Goral as a case study. 
Suppose (C0, E0) are the initial bilingual corpus of 
Goral. A Chinese-English and an English-Chinese 
MT system are trained on (C0, E0). We perform 
Back-Translation on E0 (?? ???????? ?? ?? ???????? ?? ??), and 
Forward-Translation on C0 (?? ???????? ?? ??). Suppose e1i and e2i are two aligned sentences in E1 and E2, 
c0i and c1i are the corresponding sentences in C0 
and C1. (c0i, c1i) are selected for the extraction of 
paraphrase rules if two conditions are satisfied: (1) 
BLEU(e2i) ? BLEU(e1i) > ?1, and (2) BLEU(e2i) > 
?2, where BLEU???  is a function for computing 
BLEU score; ?1 and ?2 are thresholds for balancing 
the rules number and the quality of paraphrase 
rules. In our experiment, ?1 and ?2 are empirically 
set to 0.1 and 0.3. 
As a result, we extract 912,625 Chinese and 
1,116,375 English paraphrase rules for Goral, and 
for Gnews the number of Chinese paraphrase rules is 
2,877,960. Then we use the extracted paraphrase 
rules to improve SMT by building word lattices for 
the input sentences. 
The Chinese-English experimental results of 
Goral and Gnews are shown in Table 5 and Table 6, 
respectively. It can be seen that our method 
outperforms the baselines in both oral and news 
domains. Our system gains significant 
improvements of 1.6~3.6 points of BLEU in the 
oral domain, and 0.5~1 points of BLEU in the 
news domain. Figure 5 shows the effect of 
considered paraphrases (k) in the step of building  
Corpus #Sen. pairs #Ch. words #En words
BETC 19,972 174k 190k 
PIVOT 20,000 162k 196k 
HIT 80,868 788k 850k 
CLDC 190,447 1,167k 1,898k 
Tanaka 149,207 - 1,375k 
Table 2: Statistics of training data in Goral 
 Corpus #Sen.  #Ref.  
develop CSTAR03 test set 506 16 IWSLT06 dev set 489 7 
test 
IWSLT04 test set 500 16 
IWSLT05 test set 506 16 
IWSLT06 test set 500 7 
IWSLT07 test set 489 6 
Table 3: Statistics of test/develop sets in Goral 
 Corpus #Sen.  #Ref.  
develop NIST 2002 878 10 NIST 2005 1,082 4 
test 
NIST 2004 1,788 5 
NIST 2006 1,664 4 
NIST 2008 1,357 4 
Table 4: Statistics of test/develop sets in Gnews 
984
word lattices. The result of English-Chinese 
experiments in Goral is shown in Table 7.  
6 Discussion 
We make a detailed analysis on the Chinese-
English translation results that are affected by our 
paraphrase rules. The aim is to investigate what 
kinds of paraphrases have been captured in the 
rules. Firstly the input path is recovered from the 
translation results according to the tracing 
information of the decoder, and therefore we can 
examine which path is selected by the SMT 
decoder from the paraphrase lattice. A human 
annotator is asked to judge whether the recovered 
paraphrase sentence keeps the same meaning as the 
original input. Then the annotator compares the 
baseline translation with the translations proposed 
by our approach. The analysis is carried out on the 
IWSLT 2007 Chinese-English test set, 84 out of 
489 input sentences have been affected by 
paraphrases, and the statistic of human evaluation 
is shown in Table 8.  
It can be seen in Table 8 that the paraphrases 
achieve a relatively high accuracy, 60 (71.4%) 
paraphrased sentences retain the same meaning, 
and the other 24 (28.6%) are incorrect. Among the 
60 correct paraphrases, 36 sentences finally result 
in an improved translation. We further analyze 
these paraphrases and the translation results to 
investigate what kinds of transformation finally 
lead to the translation quality improvement. The 
paraphrase variations can be classified into four 
categories: 
(1) Reordering: The original source sentences 
are reordered to be similar to the order of 
the target language. 
(2) Word substitution: A phrase with multi-
word translations is replaced by a phrase 
with a single-word translation.  
(3) Recovering omitted words: Ellipsis occurs 
frequently in spoken language. Recovering 
the omitted words often leads to a better 
translation. 
(4) Removing redundant words: Mostly, 
translating redundant words may confuse 
the SMT system and would be unnecessary. 
Removing redundant words can mitigate 
this problem. 
44.2?
44.4?
44.6?
44.8?
45.0?
45.2?
45.4?
0 10 20 30 40
BLE
U?s
cor
e?(
%)
Considered?paraprhases?(k)
Figure 5: Effect of considered paraphrases (k) 
on BLEU score
Model BLEU TER iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07
baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390 
para. improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217 
Model BLEU TER nist 04 nist 06 nist 08 nist 04 nist 06 nist 08 
baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652 
para. improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582 
 model IWSLT 2005  BLEU TER 
 baseline 0.4644 0.4164 
 para. improved  0.4853 0.3883 
trans. 
para. improve comparable worsen total
correct 36 20 4 60 
incorrect 1 9 14 24 
Table 8: Human analysis of the paraphrasing 
results in IWSLT 2007 CE translation 
Table 5: Experimental results of Goral in Chinese-English direction 
Table 6: Experimental results of Gnews in Chinese-English direction 
Table 7: Experimental results of Goral in 
English-Chinese direction 
985
Four examples for category (1), (2), (3) and (4) 
are shown in Table 9, respectively. The numbers in 
the second column indicates the number of the 
sentences affected by the rules, among the 36 
sentences with improved paraphrasing and 
translation. A sentence can be classified into 
multiple categories. Except category (2), the other 
three categories cannot be detected by the previous 
approaches, which verify our statement that our 
rules can capture structured paraphrases on the 
sentence level in addition to the paraphrases on the 
word or phrase level. 
Not all the paraphrased results are correct. 
Sometimes an ill paraphrased sentence can produce 
better translations. Take the first line of Table 9 as 
an example, the paraphrased sentence ???/How 
many ??/cigarettes ??/can ??/duty-free ?
/take ?/NULL? is not a fluent Chinese sentence, 
however, the rearranged word order is closer to 
English, which finally results in a much better 
translation. 
7 Related Work 
Previous studies on improving SMT using 
paraphrase rules focus on hand-crafted rules. 
Nakov (2008) employs six rules for paraphrasing 
the training corpus. Bond et al (2008) use 
grammars to paraphrase the source side of training 
data, covering aspects like word order and minor 
lexical variations (tenses etc.) but not content 
words. The paraphrases are added to the source 
side of the corpus and the corresponding target 
sentences are duplicated. 
A disadvantage for hand-crafted paraphrase 
rules is that it is language dependent. In contrast, 
our method that automatically extracted paraphrase 
rules from bilingual corpus is flexible and suitable 
for any language pairs. 
Our work is similar to Sun et al (2010). Both 
tried to capture the MT-favored structures from 
bilingual corpus. However, a clear difference is 
that Sun et al (2010) captures the structures 
implicitly by training an MT system on (S0, S1) and 
?translates? the SMT input to an MT-favored 
expression. Actually, the rewriting process is 
considered as a black box in Sun et al (2010). In 
this paper, the MT-favored expressions are 
captured explicitly by automatically extracted 
paraphrase rules. The advantages of the paraphrase 
rules are: (1) Our method can explicitly capture the 
structure information in the sentence level, 
enabling global reordering, which is impossible in 
Sun et al (2010). (2) For each rule, we can control 
its quality automatically or manually. 
8 Conclusion 
In this paper, we propose a novel method for 
extracting paraphrase rules by comparing the 
source side of bilingual corpus to the target-to-
source translation of the target side. The acquired 
paraphrase rules are employed to enrich the SMT 
inputs, which target on rewriting the input 
sentences to an MT-favored form. The paraphrase 
rules cover all kinds of paraphrases on the word, 
phrase and sentence levels, enabling structure 
reordering, word or phrase insertion, deletion and 
substitution. Experimental results show that the 
paraphrase rules can improve SMT quality in both 
the oral and news domains. The manual 
investigation on oral translation results indicate 
that the paraphrase rules capture four kinds of MT-
favored transformation to ensure translation quality 
improvement. 
Cate. Num Original Sentence/Translation Paraphrased Sentence/Translation 
(1) 11 
??/cigarette ??/can ??/duty-free ?
/take ??/how much ?/N/A ?  
??/how much ??/cigarettes ??/can ??
/duty-free ?/take ?/N/A ? 
what a cigarette can i take duty-free ? how many cigarettes can i take duty-free  one ? 
(2) 18 
?/you  ?/have  ??/how long  ?/N/A  
??/teaching ??/experience ? 
?/you  ?/have  ??/how much  ??/teaching  
??/experience ? 
you have how long teaching experience ? how much teaching experience you have ? 
(3) 10 ??/need  ??/deposit  ?/N/A ? ?/you  ??/need  ??/deposit  ?/N/A ? you need a deposit ? do you need a deposit ? 
(4) 4 
??/ring ?/fall ?/into ???/washbasin 
?/in ?/N/A ?  
ring off into the washbasin is in . 
??/ring  ?/fall  ?/into  ???/washbasin ?
/N/A ? 
ring off into the washbasin . 
Table 9: Examples for classification of paraphrase rules 
986
Acknowledgement 
This work was supported by National Natural 
Science Foundation of China (NSFC) (61073126, 
61133012), 863 High Technology Program 
(2011AA01A207). 
References  
Francis Bond, Eric Nichols, Darren Scott Appling, and 
Michael Paul. 2008. Improving Statistical Machine 
Translation by Paraphrasing the Training Data. In 
Proceedings of the IWSLT, pages 150?157. 
Chris Callison-Burch, Philipp Koehn, and Miles 
Osborne. 2006. Improved Statistical Machine 
Translation Using Paraphrases. In Proceedings of 
NAACL, pages 17-24. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL, pages 263?270. 
Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating 
Translation Using Source Language Paraphrase 
Lattices. In Proceedings of EMNLP, pages 420-429. 
Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. 
Enriching SMT Training Data via Paraphrasing. In 
Proceedings of IJCNLP, pages 803-810. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of HLT/NAACL, pages 48?54 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
EMNLP, pages 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of IWSLT. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of the ACL Demo and Poster Sessions, 
pages 177?180. 
Roland Kuhn, Boxing Chen, George Foster and Evan 
Stratford. 2010. Phrase Clustering for Smoothing TM 
Probabilities-or, How to Extract Paraphrases from 
Phrase Tables. In Proceedings of COLING, pages 
608?616. 
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 
2009. Improved Statistical Machine Translation 
Using Monolingually-Dervied Paraphrases. In 
Proceedings of EMNLP, pages 381-390. 
Aur?lien Max. 2010. Example-Based Paraphrasing for 
Improved Phrase-Based Statistical Machine 
TranslationIn Proceedings of EMNLP, pages 656-
666. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, Idan Szpektor. 2009. 
Source-Language Entailment Modeling for 
Translation Unknown Terms. In Proceedings of ACL, 
pages 791-799. 
Preslav Nakov. 2008. Improved Statistical Machine 
Translation Using Monolingual Paraphrases. In 
Proceedings of ECAI, pages 338-342. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of ACL, 
pages 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL, pages 160-167. 
Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. 
Paraphrase Lattice for Statistical Machine 
Translation. In Proceedings of ACL, pages 1-5. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
ACL, pages 311-318. 
Reinhard Rapp. 2009. The Back-translation Score: 
Automatic MT Evaluation at the Sentence Level 
without Reference Translations. In Proceedings of 
ACL-IJCNLP, pages 133-136. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
John Makhoul, Linnea Micciulla, and Ralph 
Weischedel. 2005. A study of translation error rate 
with targeted human annotation. Technical Report 
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005. 
Yanli Sun, Sharon O?Brien, Minako O?Hagan and Fred 
Hollowood. 2010. A Novel Statistical Pre-Processing 
Model for Rule-Based Machine Translation System. 
In Proceedings of EAMT, 8pp. 
987
