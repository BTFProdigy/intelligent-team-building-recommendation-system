Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070?1079,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
An Analysis of Active Learning Strategies for Sequence Labeling Tasks
Burr Settles??
?Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706, USA
bsettles@cs.wisc.edu
Mark Craven??
?Dept. of Biostatistics & Medical Informatics
University of Wisconsin
Madison, WI 53706, USA
craven@biostat.wisc.edu
Abstract
Active learning is well-suited to many prob-
lems in natural language processing, where
unlabeled data may be abundant but annota-
tion is slow and expensive. This paper aims
to shed light on the best active learning ap-
proaches for sequence labeling tasks such as
information extraction and document segmen-
tation. We survey previously used query selec-
tion strategies for sequence models, and pro-
pose several novel algorithms to address their
shortcomings. We also conduct a large-scale
empirical comparison using multiple corpora,
which demonstrates that our proposed meth-
ods advance the state of the art.
1 Introduction
Traditional supervised learning algorithms use
whatever labeled data is provided to induce a model.
By contrast, active learning gives the learner a de-
gree of control by allowing it to select which in-
stances are labeled and added to the training set. A
typical active learner begins with a small labeled set
L, selects one or more informative query instances
from a large unlabeled pool U , learns from these la-
beled queries (which are then added to L), and re-
peats. In this way, the learner aims to achieve high
accuracy with as little labeling effort as possible.
Thus, active learning can be valuable in domains
where unlabeled data are readily available, but ob-
taining training labels is expensive.
Such is the case with many sequence labeling
tasks in natural language domains. For example,
part-of-speech tagging (Seung et al, 1992; Lafferty
et al, 2001), information extraction (Scheffer et al,
2001; Sang and DeMeulder, 2003; Kim et al, 2004),
and document segmentation (Carvalho and Cohen,
2004) are all typically treated as sequence labeling
problems. The source data for these tasks (i.e., text
documents in electronic form) are often easily ob-
tained. However, due to the nature of sequence la-
beling tasks, annotating these texts can be rather te-
dious and time-consuming, making active learning
an attractive technique.
While there has been much work on active learn-
ing for classification (Cohn et al, 1994; McCallum
and Nigam, 1998; Zhang and Oles, 2000; Zhu et
al., 2003), active learning for sequence labeling has
received considerably less attention. A few meth-
ods have been proposed, based mostly on the con-
ventions of uncertainty sampling, where the learner
queries the instance about which it has the least cer-
tainty (Scheffer et al, 2001; Culotta and McCallum,
2005; Kim et al, 2006), or query-by-committee,
where a ?committee? of models selects the instance
about which its members most disagree (Dagan and
Engelson, 1995). We provide more detail on these
and the new strategies we propose in Section 3.
The comparative effectiveness of these ap-
proaches, however, has not been studied. Further-
more, it has been suggested that uncertainty sam-
pling and query-by-committee fail on occasion (Roy
and McCallum, 2001; Zhu et al, 2003) by query-
ing outliers, e.g., instances considered informative
in isolation by the learner, but containing little infor-
mation about the rest of the distribution of instances.
Proposed methods for dealing with these shortcom-
ings have so far only considered classification tasks.
1070
This paper presents two major contributions for
active learning and sequence labeling tasks. First,
we motivate and introduce several new query strate-
gies for probabilistic sequence models. Second, we
conduct a thorough empirical analysis of previously
proposed methods with our algorithms on a variety
of benchmark corpora. The remainder of this pa-
per is organized as follows. Section 2 provides a
brief introduction to sequence labeling and condi-
tional random fields (the sequence model used in
our experiments). Section 3 describes in detail all
the query selection strategies we consider. Section 4
presents the results of our empirical study. Section 5
concludes with a summary of our findings.
2 Sequence Labeling and CRFs
In this paper, we are concerned with active learn-
ing for sequence labeling. Figure 1 illustrates
how, for example, an information extraction prob-
lem can be viewed as a sequence labeling task.
Let x = ?x1, . . . , xT ? be an observation sequence
of length T with a corresponding label sequence
y = ?y1, . . . , yT ?. Words in a sentence corre-
spond to tokens in the input sequence x, which are
mapped to labels in y. These labels indicate whether
the word belongs to a particular entity class of inter-
est (in this case, org and loc) or not (null). These
labels can be assigned by a sequence model based
on a finite state machine, such as the one shown to
the right in Figure 1.
We focus our discussion of active learning for
sequence labeling on conditional random fields, or
CRFs (Lafferty et al, 2001). The rest of this sec-
tion serves as a brief introduction. CRFs are sta-
tistical graphical models which have demonstrated
state-of-the-art accuracy on virtually all of the se-
quence labeling tasks mentioned in Section 1. We
use linear-chain CRFs, which correspond to condi-
tionally trained probabilistic finite state machines.
A linear-chain CRF model with parameters ? de-
fines the posterior probability of y given x to be1:
P (y|x; ?) =
1
Z(x)
exp
(
T?
t=1
K?
k=1
?kfk(yt?1, yt,xt)
)
.
(1)
1Our discussion assumes, without loss of generality, that
each label is uniquely represented by one state, thus each label
sequence y corresponds to exactly one path through the model.
loc
orgnull
x:
y:
...the
null
ACME
org
Inc.
org
offices
null
in
null
Chicago
loc
Figure 1: An information extraction example treated as
a sequence labeling task. Also shown is a corresponding
sequence model represented as a finite state machine.
Here Z(x) is a normalization factor over all pos-
sible labelings of x, and ?k is one of K model
parameter weights corresponding to some feature
fk(yt?1, yt,xt). Each feature fk describes the se-
quence x at position t with label yt, observed along
a transition from label states yt?1 to yt in the finite
state machine. Consider the example text from Fig-
ure 1. Here, fk might be the feature WORD=ACME
and have the value fk = 1 along a transition from
the null state to the org state (and 0 elsewhere).
Other features set to 1 here might be ALLCAPS and
NEXTWORD=Inc. The weights in ? are set to max-
imize the conditional log likelihood ` of training se-
quences in the labeled data set L:
`(L; ?) =
L?
l=1
logP (y(l)|x(l); ?) ?
K?
k=1
?2k
2?2
,
where L is the size of the labeled set L, and the sec-
ond term is a Gaussian regularization penalty on ???
to prevent over-fitting. After training, labels can be
predicted for new sequences using the Viterbi algo-
rithm. For more details on CRFs and their training
procedures, see Sutton and McCallum (2006).
Note that, while we describe the active learning
algorithms in the next section in terms of linear-
chain CRFs, they have analogs for other kinds of
sequence models, such as hidden Markov models,
or HMMs (Rabiner, 1989), probabilistic context-
free grammars (Lari and Young, 1990), and general
CRFs (Sutton and McCallum, 2006).
3 Active Learning with Sequence Models
In order to select queries, an active learner must have
a way of assessing how informative each instance is.
Let x? be the most informative instance according to
some query strategy ?(x), which is a function used
to evaluate each instance x in the unlabeled pool U .
1071
Given: Labeled set L, unlabeled pool U , query
strategy ?(?), query batch size B
repeat
// learn a model using the current L
? = train(L) ;
for b = 1 to B do
// query the most informative instance
x?b = argmaxx?U ?(x) ;
// move the labeled query from U to L
L = L ? ?x?b , label(x
?
b)? ;
U = U ? x?b ;
end
until some stopping criterion ;
Algorithm 1: Pool-based active learning.
Algorithm 1 provides a sketch of the generic pool-
based active learning scenario.
In the remainder of this section, we describe var-
ious query strategy formulations of ?(?) that have
been used for active learning with sequence mod-
els. We also point out where we think these ap-
proaches may be flawed, and propose several novel
query strategies to address these issues.
3.1 Uncertainty Sampling
One of the most common general frameworks for
measuring informativeness is uncertainty sampling
(Lewis and Catlett, 1994), where a learner queries
the instance that it is most uncertain how to la-
bel. Culotta and McCallum (2005) employ a sim-
ple uncertainty-based strategy for sequence models
called least confidence (LC):
?LC(x) = 1 ? P (y?|x; ?).
Here, y? is the most likely label sequence, i.e., the
Viterbi parse. This approach queries the instance
for which the current model has the least confidence
in its most likely labeling. For CRFs, this confi-
dence can be calculated using the posterior proba-
bility given by Equation (1).
Scheffer et al (2001) propose another uncertainty
strategy, which queries the instance with the smallest
margin between the posteriors for its two most likely
labelings. We call this approach margin (M):
?M (x) = ?
(
P (y?1|x; ?) ? P (y
?
2|x; ?)
)
.
Here, y?1 and y
?
2 are the first and second best la-
bel sequences, respectively. These can be efficiently
computed using the N -best algorithm (Schwartz
and Chow, 1990), a beam-search generalization of
Viterbi, with N = 2. The minus sign in front is sim-
ply to ensure that ?M acts as a maximizer for use
with Algorithm 1.
Another uncertainty-based measure of informa-
tiveness is entropy (Shannon, 1948). For a dis-
crete random variable Y , the entropy is given by
H(Y ) = ?
?
i P (yi) logP (yi), and represents the
information needed to ?encode? the distribution of
outcomes for Y . As such, is it often thought of as
a measure of uncertainty in machine learning. In
active learning, we wish to use the entropy of our
model?s posteriors over its labelings. One way this
has been done with probabilistic sequence models is
by computing what we call token entropy (TE):
?TE(x) = ?
1
T
T?
t=1
M?
m=1
P?(yt = m) logP?(yt = m),
(2)
where T is the length of x, m ranges over all pos-
sible token labels, and P?(yt = m) is shorthand
for the marginal probability that m is the label at
position t in the sequence, according to the model.
For CRFs and HMMs, these marginals can be effi-
ciently computed using the forward and backward
algorithms (Rabiner, 1989). The summed token en-
tropies have typically been normalized by sequence
length T , to avoid simply querying longer sequences
(Baldridge and Osborne, 2004; Hwa, 2004). How-
ever, we argue that querying long sequences should
not be explicitly discouraged, if in fact they contain
more information. Thus, we also propose the total
token entropy (TTE) measure:
?TTE(x) = T ? ?TE(x).
For most sequence labeling tasks, however, it is
more appropriate to consider the entropy of the la-
bel sequence y as a whole, rather than some aggre-
gate of individual token entropies. Thus an alternate
query strategy is sequence entropy (SE):
?SE(x) = ?
?
y?
P (y?|x; ?) logP (y?|x; ?), (3)
where y? ranges over all possible label sequences for
input sequence x. Note, however, that the number
1072
of possible labelings grows exponentially with the
length of x. To make this feasible, previous work
(Kim et al, 2006) has employed an approximation
we call N-best sequence entropy (NSE):
?NSE(x) = ?
?
y??N
P (y?|x; ?) logP (y?|x; ?),
where N = {y?1, . . . ,y
?
N}, the set of the N most
likely parses, and the posteriors are re-normalized
(i.e., Z(x) in Equation (1) only ranges over N ). For
N = 2, this approximation is equivalent to ?M , thus
N -best sequence entropy can be thought of as a gen-
eralization of the margin approach.
Recently, an efficient entropy calculation via dy-
namic programming was proposed for CRFs in the
context of semi-supervised learning (Mann and Mc-
Callum, 2007). We use this algorithm to compute
the true sequence entropy (3) for active learning in
a constant-time factor of Viterbi?s complexity. Hwa
(2004) employed a similar approach for active learn-
ing with probabilistic context-free grammars.
3.2 Query-By-Committee
Another general active learning framework is the
query-by-committee (QBC) approach (Seung et al,
1992). In this setting, we use a committee of models
C = {?(1), . . . , ?(C)} to represent C different hy-
potheses that are consistent with the labeled set L.
The most informative query, then, is the instance
over which the committee is in most disagreement
about how to label.
In particular, we use the query-by-bagging ap-
proach (Abe and Mamitsuka, 1998) to learn a com-
mittee of CRFs. In each round of active learning,
L is sampled (with replacement) L times to create
a unique, modified labeled set L(c). Each model
?(c) ? C is then trained using its own corresponding
labeled set L(c). To measure disagreement among
committee members, we consider two alternatives.
Dagan and Engelson (1995) introduced QBC with
HMMs for part-of-speech tagging using a measure
called vote entropy (VE):
?V E(x) = ?
1
T
T?
t=1
M?
m=1
V (yt,m)
C
log
V (yt,m)
C
,
where V (yt,m) is the number of ?votes? labelm re-
ceives from all the committee member?s Viterbi la-
belings at sequence position t.
McCallum and Nigam (1998) propose a QBC
strategy for classification based on Kullback-Leibler
(KL) divergence, an information-theoretic measure
of the difference between two probability distribu-
tions. The most informative query is considered to
be the one with the largest average KL divergence
between a committee member?s posterior label dis-
tribution and the consensus. We modify this ap-
proach for sequence models by summing the average
KL scores using the marginals at each token position
and, as with vote entropy, normalizing for length.
We call this approach Kullback-Leibler (KL):
?KL(x) =
1
T
T?
t=1
1
C
C?
c=1
D(?(c)?C),
where (using shorthand again):
D(?(c)?C) =
M?
m=1
P?(c)(yt = m) log
P?(c)(yt = m)
PC(yt = m)
.
Here PC(yt = m) = 1C
?C
c=1 P?(c)(yt = m), or the
?consensus? marginal probability that m is the label
at position t in the sequence.
Both of these disagreement measures are normal-
ized for sequence length T . As with token en-
tropy (2), this may bias the learner toward query-
ing shorter sequences. To study the effects of nor-
malization, we also conduct experiments with non-
normalized variants ?TV E and ?TKL.
Additionally, we argue that these token-level dis-
agreement measures may be less appropriate for
most tasks than measuring the committee?s disagree-
ment about the label sequence y as a whole. There-
fore, we propose sequence vote entropy (SVE):
?SV E(x) = ?
?
y??NC
P (y?|x; C) logP (y?|x; C),
where N C is the union of the N -best parses from
all models in the committee C, and P (y?|x; C) =
1
C
?C
c=1 P (y?|x; ?
(c)), or the ?consensus? posterior
probability for some label sequence y?. This can be
thought of as a QBC generalization of N -best en-
tropy, where each committee member casts a vote
for the posterior label distribution. We also explore
a sequence Kullback-Leibler (SKL) variant:
?SKL(x) =
1
C
C?
c=1
?
y??NC
P (y?|x; ?(c)) log
P (y?|x; ?(c))
P (y?|x; C)
.
1073
3.3 Expected Gradient Length
A third general active learning framework we con-
sider is to query the instance that would impart the
greatest change to the current model if we knew its
label. Since we train discriminative models like
CRFs using gradient-based optimization, this in-
volves querying the instance which, if labeled and
added to the training set, would create the greatest
change in the gradient of the objective function (i.e.,
the largest gradient vector used to re-estimate pa-
rameter values).
Let ?`(L; ?) be the gradient of the log-
likelihood ` with respect to the model parameters ?,
as given by Sutton and McCallum (2006). Now let
?`(L+?x,y?; ?) be the new gradient that would be
obtained by adding the training tuple ?x,y? to L.
Since the query algorithm does not know the true la-
bel sequence y in advance, we instead calculate the
expected gradient length (EGL):
?EGL(x) =
?
y??N
P (y?|x; ?)
?
?
??`(L+?x,y??; ?)
?
?
? ,
approximated as an expectation over the N -best la-
belings, where ? ? ? is the Euclidean norm of each
resulting gradient vector. We first introduced this ap-
proach in previous work on multiple-instance active
learning (Settles et al, 2008), and adapt it to query
selection with sequences here. Note that, at query
time, ?`(L; ?) should be nearly zero since ` con-
verged at the previous round of training. Thus, we
can approximate ?`(L+?x,y??; ?) ? ?`(?x, y??; ?)
for computational efficiency, because the training in-
stances are assumed to be independent.
3.4 Information Density
It has been suggested that uncertainty sampling and
QBC are prone to querying outliers (Roy and Mc-
Callum, 2001; Zhu et al, 2003). Figure 2 illus-
trates this problem for a binary linear classifier us-
ing uncertainty sampling. The least certain instance
lies on the classification boundary, but is not ?rep-
resentative? of other instances in the distribution, so
knowing its label is unlikely to improve accuracy on
the data as a whole. QBC and EGL exhibit similar
behavior, by spending time querying possible out-
liers simply because they are controversial, or are
expected to impart significant change in the model.
A
B
Figure 2: An illustration of when uncertainty sampling
can be a poor strategy for classification. Shaded poly-
gons represent labeled instances (L), and circles repre-
sent unlabeled instances (U). Since A is on the decision
boundary, it will be queried as the most uncertain. How-
ever, querying B is likely to result in more information
about the data as a whole.
We argue that this phenomenon can occur with se-
quence labeling tasks as well as with classification.
To address this, we propose a new active learning
approach called information density (ID):
?ID(x) = ?SE(x) ?
(
1
U
U?
u=1
sim(x,x(u))
)?
.
That is, the informativeness of x is weighted by its
average similarity to all other sequences in U , sub-
ject to a parameter ? that controls the relative im-
portance of the density term. In the formulation pre-
sented above, sequence entropy ?SE measures the
?base? informativeness, but we could just as easily
use any of the instance-level strategies presented in
the previous sections.
This density measure requires us to compute the
similarity of two sequences. To do this, we first
transform each x, which is a sequence of feature
vectors (tokens), into a single kernel vector ~x:
~x =
[
T?
t=1
f1(xt), . . . ,
T?
t=1
fJ(xt)
]
,
where fj(xt) is the value of feature fj for token xt,
and J is the number of features in the input represen-
tation2. In other words, sequence x is compressed
into a fixed-length feature vector ~x, for which each
element is the sum of the corresponding feature?s
values across all tokens. We can then use cosine
2Note that J 6= K, and fj(xt) here differs slightly from the
feature definition given in Section 2. Since the labels yt?1 and
yt are unknown before querying, the K features used for model
training are reduced down to the J input features here, which
factor out any label dependencies.
1074
similarity on this simplified representation:
simcos(x,x(u)) =
~x ? ~x(u)
?~x? ? ?~x(u)?
.
We have also investigated similarity functions
based on exponentiated Euclidean distance and KL-
divergence, the latter of which was also employed by
McCallum and Nigam (1998) for density-weighting
QBC in text classification. However, these measures
show no improvement over cosine similarity, and re-
quire setting additional hyper-parameters.
One potential drawback of information density is
that the number of required similarity calculations
grows quadratically with the number of instances
in U . For pool-based active learning, we often as-
sume that the size of U is very large. However,
these densities only need to be computed once, and
are independent of the base information measure.
Thus, when employing information density in a real-
world interactive learning setting, the density scores
can simply be pre-computed and cached for efficient
lookup during the actual active learning process.
3.5 Fisher Information
We also introduce a query selection strategy for se-
quence models based on Fisher information, build-
ing on the theoretical framework of Zhang and Oles
(2000). Fisher information I(?) represents the over-
all uncertainty about the estimated model parame-
ters ?, as given by:
I(?) = ?
?
x
P (x)
?
y
P (y|x; ?)
?2
??2
logP (y|x; ?).
For a model with K parameters, the Fisher infor-
mation takes the form of a K ? K covariance ma-
trix. Our goal in active learning is to select the query
that most efficiently minimizes the model variance
reflected in I(?). This can be accomplished by op-
timizing the Fisher information ratio (FIR):
?FIR(x) = ?tr
(
Ix(?)
?1IU (?)
)
, (4)
where Ix(?) and IU (?) are Fisher information ma-
trices for sequence x and the unlabeled pool U , re-
spectively. The leading minus sign again ensures
that ?FIR is a maximizer for use with Algorithm 1.
Previously, Fisher information for active learning
has only been investigated in the context of simple
binary classification. When employing FIR with se-
quence models like CRFs, there are two additional
computational challenges. First, we must integrate
over all possible labelings y, which can, as we have
seen, be approximated as an expectation over theN -
best labelings. Second, the inner product in the ratio
calculation (4) requires inverting a K ? K matrix
for each x. In most interesting natural language ap-
plications, K is very large, making this algorithm
intractable. However, it is common in similar situ-
ations to approximate the Fisher information matrix
with its diagonal (Nyffenegger et al, 2006). Thus
we estimate Ix(?) using:
Ix(?) =
?
y??N
P (y?|x; ?)
[(
? logP (y?|x; ?)
??1
)2
+ ?, . . . ,
(
? logP (y?|x; ?)
??K
)2
+ ?
]
,
and IU (?) using:
IU (?) =
1
U
U?
u=1
Ix(u)(?).
For CRFs, the partial derivative at the root of each
element in the diagonal vector is given by:
? logP (y?|x; ?)
??k
=
T?
t=1
fk(y?t?1, y?t,xt)
?
T?
t=1
?
y,y?
P (y, y?|x)fk(y, y?,xt),
which is similar to the equation used to compute the
training gradient, but without a regularization term.
A smoothing parameter ?  1 is added to prevent
division by zero when computing the ratio.
Notice that this method implicitly selects repre-
sentative instances by favoring queries with Fisher
information Ix(?) that is not only high, but similar
to that of the overall data distribution IU (?). This
is in contrast to information density, which tries to
query representative instances by explicitly model-
ing the distribution with a density weight.
1075
Corpus Entities Features Instances
CoNLL-03 4 78,644 19,959
NLPBA 5 128,401 18,854
BioCreative 1 175,331 10,000
FlySlip 1 31,353 1,220
CORA:Headers 15 22,077 935
CORA:References 13 4,208 500
Sig+Reply 2 25 617
SigIE 12 10,600 250
Table 1: Properties of the different evaluation corpora.
4 Empirical Evaluation
In this section we present a large-scale empirical
analysis of the query strategies described in Sec-
tion 3 on eight benchmark information extraction
and document segmentation corpora. The data sets
are summarized in Table 1.
4.1 Data and Methodology
CoNLL-03 (Sang and DeMeulder, 2003) is a col-
lection of newswire articles annotated with four en-
tities: person, organization, location, and misc.
NLPBA (Kim et al, 2004) is a large collection
of biomedical abstracts annotated with five entities
of interest, such as protein, RNA, and cell-type.
BioCreative (Yeh et al, 2005) and FlySlip (Vla-
chos, 2007) also comprise texts in the biomedical
domain, annotated for gene entity mentions in arti-
cles from the human and fruit fly literature, respec-
tively. CORA (Peng and McCallum, 2004) consists
of two collections: a set of research paper headers
annotated for entities such as title, author, and insti-
tution; and a collection of references annotated with
BibTeX fields such as journal, year, and publisher.
The Sig+Reply corpus (Carvalho and Cohen, 2004)
is a set of email messages annotated for signature
and quoted reply line segments. SigIE is a subset of
the signature blocks from Sig+Reply which we have
enhanced with several address book fields such as
name, email, and phone. All corpora are format-
ted in the ?IOB? sequence representation (Ramshaw
and Marcus, 1995).
We implement all fifteen query selection strate-
gies described in Section 3 for use with CRFs, and
evaluate them on all eight data sets. We also com-
pare against two baseline strategies: random in-
stance selection (i.e., passive learning), and na??vely
querying the longest sequence in terms of tokens.
We use a typical feature set for each corpus based on
the cited literature (including words, orthographic
patterns, part-of-speech, lexicons, etc.). Where the
N -best approximation is used N = 15, and for all
QBC methods C = 3; these figures exhibited a good
balance of accuracy and training speed in prelimi-
nary work. For information density, we arbitrarily
set ? = 1 (i.e., the information and density terms
have equal weight). In each experiment, L is ini-
tialized with five random labeled instances, and up
to 150 queries are subsequently selected from U in
batches of size B = 5. All results are averaged
across five folds using cross-validation.
We evaluate each query strategy by constructing
learning curves that plot the overall F1 measure (for
all entities or segments) as a function of the num-
ber of instances queried. Due to lack of space, we
cannot show learning curves for every experiment.
Instead, Table 2 summarizes our results by reporting
the area under the learning curve for all strategies
on all data. Figure 3 presents a few representative
learning curves for six of the corpora.
4.2 Discussion of Learning Curves
The first conclusion we can draw from these results
is that there is no single clear winner. However, in-
formation density (ID), which we introduce in this
paper, stands out. It usually improves upon the base
sequence entropy measure, never performs poorly,
and has the highest average area under the learning
curve across all tasks. It seems particularly effective
on large corpora, which is a typical assumption for
the active learning setting. Sequence vote entropy
(SVE), a QBCmethod we propose here, is also note-
worthy in that it is fairly consistently among the top
three strategies, although never the best.
Second, the top uncertainty sampling strategies
are least confidence (LC) and sequence entropy
(SE), the latter being the dominant entropy-based
method. Among the QBC strategies, sequence vote
entropy (SVE) is the clear winner. We conclude that
these three methods are the best base information
measures for use with information density.
Third, query strategies that evaluate the en-
tire sequence (SE, SVE, SKL) are generally su-
perior to those which aggregate token-level infor-
mation. Furthermore, the total token-level strate-
gies (TTE, TVE, TKL) outperform their length-
1076
Baselines Uncertainty Sampling Query-By-Committee Other
Corpus Rand Long LC M TE TTE SE NSE VE KL TVE TKL SVE SKL EGL ID FIR
CoNLL-03 78.8 79.4 89.4 84.5 38.9 89.7 90.1 89.1 45.9 62.0 86.7 81.7 89.0 87.9 87.3 89.6 81.7
NLPBA 59.9 67.6 71.0 62.9 53.4 70.9 71.5 68.9 52.4 53.1 66.9 63.5 71.8 68.5 69.3 73.1 73.6
BioCreative 34.6 26.9 54.8 46.8 37.8 53.0 56.0 50.5 35.2 37.4 49.2 45.1 56.6 50.8 51.5 59.1 58.8
FlySlip 112.1 121.0 125.1 119.5 110.3 124.9 125.4 124.1 113.3 109.4 124.1 119.5 122.7 120.7 125.9 126.8 118.2
Headers 76.0 78.2 81.4 78.6 78.5 78.5 80.8 80.4 72.8 78.5 79.7 78.5 80.7 78.4 79.6 80.2 79.1
References 90.0 86.0 89.8 91.5 84.4 88.6 88.4 89.4 85.1 89.1 88.7 88.2 89.9 86.9 88.2 88.7 87.1
Sig+Reply 129.1 129.6 132.1 132.3 131.7 131.6 131.4 133.1 131.4 130.7 132.1 130.6 132.8 132.3 130.5 131.5 133.2
SigIE 84.3 82.7 88.8 87.3 89.3 88.3 87.6 89.1 89.8 85.5 89.7 85.1 89.5 89.7 87.7 88.5 88.5
Average 83.1 83.9 91.6 87.9 78.0 90.7 91.4 90.6 78.2 80.7 89.6 86.5 91.6 89.4 90.0 92.2 90.0
Table 2: Detailed results for all query strategies on all evaluation corpora. Reported is the area under the F1 learning
curve for each strategy after 150 queries (maximum possible score is 150). For each row, the best method is shown
boxed in bold, the second best is shown underlined in bold, and the third best is shown in bold. The last row summa-
rizes the results across all eight tasks by reporting the average area for each strategy. Query strategy formulations for
sequence models introduced in this paper are indicated with italics along the top.
normalized counterparts (TE, VE, KL) in nearly all
cases. In fact, the normalized variants are often in-
ferior even to the baselines. While an argument can
be made that these shorter sequences might be eas-
ier to label from a human annotator?s perspective,
our ongoing work indicates that the relationship be-
tween instance length and actual labeling costs (e.g.,
elapsed annotation time) is not a simple one. Anal-
ysis of our experiment logs also shows that length-
normalized methods are occasionally biased toward
short sequences with little intuitive value (e.g., sen-
tences with few or no entities to label). In addition,
vote entropy appears to be a better disagreement
measure for QBC strategies than KL divergence.
Finally, Fisher information (FIR), while theoreti-
cally sound, exhibits behavior that is difficult to in-
terpret. It is sometimes the winning strategy, but oc-
casionally only on par with the baselines. When it
does show significant gains over the other strategies,
these gains appear to be only for the first several
queries (e.g., NLPBA and BioCreative in Figure 3).
This inconsistent performance may be a result of the
approximations made for computational efficiency.
Expected gradient length (EGL) also appears to ex-
hibit mediocre performance, and is likely not worth
its additional computational expense.
4.3 Discussion of Run Times
Here we discuss the execution times for each query
strategy using current hardware. The uncertainty
sampling methods are roughly comparable in run
time (token-based methods run slightly faster), each
routinely evaluating tens of thousands of sequences
in under a minute. The QBC methods, on the other
hand, must re-train multiple models with each query,
resulting in a lag of three to four minutes per query
batch (and up to 20 minutes for corpora with more
entity labels).
The expected gradient length and Fisher informa-
tion methods are the most computationally expen-
sive, because they must first perform inference over
the possible labelings and then calculate gradients
for each candidate label sequence. As a result, they
take eight to ten minutes (upwards of a half hour on
the larger corpora) for each query. Unlike the other
strategies, their time complexities also scale linearly
with the number of model parameters K which, in
turn, increases as new sequences are added to L.
As noted in Section 3.4, information density in-
curs a large computational cost to estimate the den-
sity weights, but these can be pre-computed and
cached for efficient lookup. In our experiments, this
pre-processing step takes less than a minute for the
smaller corpora, about a half hour for CoNLL-03
and BioCreative, and under two hours for NLPBA.
The density lookup causes no significant change in
the run time of the base information measure. Given
these results, we advocate information density with
an uncertainty sampling base measure in practice,
particularly for active learning with large corpora.
5 Conclusion
In this paper, we have presented a detailed analy-
sis of active learning for sequence labeling tasks.
In particular, we have described and criticized the
query selection strategies used with probabilistic se-
1077
F1 m
eas
ure
F1 m
eas
ure
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
CoNLL-03
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
NLPBA
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
BioCreative
number of instances queried
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
FlySlip
number of instances queried
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
Sig+Reply
number of instances queried
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
SigIE
Figure 3: Learning curves for selected query strategies on six of the evaluation corpora.
quence models to date, and proposed several novel
strategies to address some of their shortcomings.
Our large-scale empirical evaluation demonstrates
that some of these newly proposed methods advance
the state of the art in active learning with sequence
models. These methods include information density
(which we recommend in practice), sequence vote
entropy, and sometimes Fisher information.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful feedback. This work was supported by
NIH grants T15-LM07359 and R01-LM07050.
References
N. Abe and H. Mamitsuka. 1998. Query learning strate-
gies using boosting and bagging. In Proceedings of
the International Conference on Machine Learning
(ICML), pages 1?9. Morgan Kaufmann.
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 9?16. ACL Press.
V.R. Carvalho and W. Cohen. 2004. Learning to extract
signature and reply lines from email. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201?221.
A. Culotta and A. McCallum. 2005. Reducing labeling
effort for stuctured prediction tasks. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI), pages 746?751. AAAI Press.
I. Dagan and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 150?157. Morgan Kaufmann.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):73?77.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA), pages 70?
75.
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee.
2006. MMR-based active machine learning for bio
named entity recognition. In Proceedings of Human
Language Technology and the North American Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 69?72. ACL Press.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 282?289. Morgan Kaufmann.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
1078
ceedings of the International Conference on Machine
Learning (ICML), pages 148?156. Morgan Kaufmann.
G. Mann and A. McCallum. 2007. Efficient computation
of entropy gradient for semi-supervised conditional
random fields. In Proceedings of the North American
Association for Computational Linguistics (NAACL),
pages 109?112. ACL Press.
A. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 359?367. Morgan
Kaufmann.
M. Nyffenegger, J.C. Chappelier, and E. Gaussier. 2006.
Revisiting Fisher kernels for document similarities. In
Proceedings of the European Conference on Machine
Learning (ECML), pages 727?734. Springer.
F. Peng and A. McCallum. 2004. Accurate information
extraction from research papers using conditional ran-
dom fields. In Proceedings of Human Language Tech-
nology and the North American Association for Com-
putational Linguistics (HLT-NAACL). ACL Press.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the ACL Workshop on Very Large Corpora.
N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In Proceedings of the International Conference
on Machine Learning (ICML), pages 441?448. Mor-
gan Kaufmann.
E.F.T.K. Sang and F. DeMeulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of the
Conference on Natural Language Learning (CoNLL),
pages 142?147.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extraction.
In Proceedings of the International Conference on Ad-
vances in Intelligent Data Analysis (CAIDA), pages
309?318. Springer-Verlag.
R. Schwartz and Y.-L. Chow. 1990. The N -best algo-
rithm: an efficient and exact procedure for finding the
N most likely sentence hypotheses. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 81?83. IEEE
Press.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. In Advances in Neural Infor-
mation Processing Systems (NIPS), volume 20, pages
1289?1296. MIT Press.
H.S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the ACM
Workshop on Computational Learning Theory, pages
287?294.
C. E. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379?
423,623?656.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
L. Getoor and B. Taskar, editors, Introduction to Sta-
tistical Relational Learning. MIT Press.
A. Vlachos. 2007. Evaluating and combining biomedical
named entity recognition systems. In BioNLP 2007:
Biological, translational, and clinical language pro-
cessing, pages 199?206.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl 1):S2.
T. Zhang and F.J. Oles. 2000. A probability analysis
on the value of unlabeled data for classification prob-
lems. In Proceedings of the International Conference
onMachine Learning (ICML), pages 1191?1198. Mor-
gan Kaufmann.
X. Zhu, J. Lafferty, and Z. Ghahramani. 2003. Combin-
ing active learning and semi-supervised learning using
gaussian fields and harmonic functions. In Proceed-
ings of the ICML Workshop on the Continuum from
Labeled to Unlabeled Data, pages 58?65.
1079
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81?90,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Active Learning by Labeling Features
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Burr Settles
Dept. of Biostatistics &
Medical Informatics
Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706
bsettles@cs.wisc.edu
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Methods that learn from prior informa-
tion about input features such as general-
ized expectation (GE) have been used to
train accurate models with very little ef-
fort. In this paper, we propose an ac-
tive learning approach in which the ma-
chine solicits ?labels? on features rather
than instances. In both simulated and real
user experiments on two sequence label-
ing tasks we show that our active learning
method outperforms passive learning with
features as well as traditional active learn-
ing with instances. Preliminary experi-
ments suggest that novel interfaces which
intelligently solicit labels on multiple fea-
tures facilitate more efficient annotation.
1 Introduction
The application of machine learning to new prob-
lems is slowed by the need for labeled training
data. When output variables are structured, an-
notation can be particularly difficult and time-
consuming. For example, when training a condi-
tional random field (Lafferty et al, 2001) to ex-
tract fields such as rent, contact, features, and utilities
from apartment classifieds, labeling 22 instances
(2,540 tokens) provides only 66.1% accuracy.
1
Recent work has used unlabeled data and lim-
ited prior information about input features to boot-
strap accurate structured output models. For ex-
ample, both Haghighi and Klein (2006) and Mann
and McCallum (2008) have demonstrated results
better than 66.1% on the apartments task de-
scribed above using only a list of 33 highly dis-
criminative features and the labels they indicate.
However, these methods have only been applied
in scenarios in which the user supplies such prior
knowledge before learning begins.
1
Averaged over 10 randomly selected sets of 22 instances.
In traditional active learning (Settles, 2009), the
machine queries the user for only the labels of in-
stances that would be most helpful to the machine.
This paper proposes an active learning approach in
which the user provides ?labels? for input features,
rather than instances. A labeled input feature de-
notes that a particular input feature, for example
the word call, is highly indicative of a particular
label, such as contact. Table 1 provides an excerpt
of a feature active learning session.
In this paper, we advocate using generalized
expectation (GE) criteria (Mann and McCallum,
2008) for learning with labeled features. We pro-
vide an alternate treatment of the GE objective
function used by Mann and McCallum (2008) and
a novel speedup to the gradient computation. We
then provide a pool-based feature active learning
algorithm that includes an option to skip queries,
for cases in which a feature has no clear label.
We propose and evaluate feature query selection
algorithms that aim to reduce model uncertainty,
and compare to several baselines. We evaluate
our method using both real and simulated user ex-
periments on two sequence labeling tasks. Com-
pared to previous approaches (Raghavan and Al-
lan, 2007), our method can be used for both classi-
fication and structured tasks, and the feature query
selection methods we propose perform better.
We use experiments with simulated labelers on
real data to extensively compare feature query se-
lection algorithms and evaluate on multiple ran-
dom splits. To make these simulations more re-
alistic, the effort required to perform different la-
beling actions is estimated from additional exper-
iments with real users. The results show that ac-
tive learning with features outperforms both pas-
sive learning with features and traditional active
learning with instances.
In the user experiments, each annotator actively
labels instances, actively labels features one at a
time, and actively labels batches of features orga-
81
accuracy 46.5? 60.5
feature label
PHONE* contact
call contact
deposit rent
month rent
pets restrict.
lease rent
appointment contact
parking features
EMAIL* contact
information contact
accuracy 60.5? 67.1
feature label
water utilities
close neighbor.
garbage utilities
included utilities
features
shopping neighbor.
bart neighbor.
downtown neighbor.
TIME* contact
bath size
Table 1: Two iterations of feature active learning.
Each table shows the features labeled, and the re-
sulting change in accuracy. Note that the word in-
cluded was labeled as both utilities and features, and
that ? denotes a regular expression feature.
nized using a ?grid? interface. The results support
the findings of the simulated experiments and pro-
vide evidence that the ?grid? interface can facili-
tate more efficient annotation.
2 Conditional Random Fields
In this section we describe the underlying proba-
bilistic model for all methods in this paper. We
focus on sequence labeling, though the described
methods could be applied to other structured out-
put or classification tasks. We model the proba-
bility of the label sequence y ? Y
n
conditioned
on the input sequence x ? X
n
, p(y|x; ?) using
first-order linear-chain conditional random fields
(CRFs) (Lafferty et al, 2001). This probability is
p(y|x; ?) =
1
Z
x
exp
(
?
i
?
j
?
j
f
j
(y
i
, y
i+1
,x, i)
)
,
where Z
x
is the partition function and feature
functions f
j
consider the entire input sequence
and at most two consecutive output variables.
The most probable output sequence and transition
marginal distributions can be computed using vari-
ants of Viterbi and forward-backward.
Provided a training data distribution p?, we es-
timate CRF parameters by maximizing the condi-
tional log likelihood of the training data.
L(?) = E
p?(x,y)
[log p(y|x; ?)]
We use numerical optimization to maximize L(?),
which requires the gradient of L(?) with respect
to the parameters. It can be shown that the par-
tial derivative with respect to parameter j is equal
to the difference between the empirical expecta-
tion of F
j
and the model expectation of F
j
, where
F
j
(y,x) =
?
i
f
j
(y
i
, y
i+1
,x, i).
?
??
j
L(?) = E
p?(x,y)
[F
j
(y,x)]
? E
p?(x)
[E
p(y|x;?)
[F
j
(y,x)]].
We also include a zero-mean variance ?
2
= 10
Gaussian prior on parameters in all experiments.
2
2.1 Learning with missing labels
The training set may contain partially labeled se-
quences. Let z denote missing labels. We esti-
mate parameters with this data by maximizing the
marginal log-likelihood of the observed labels.
L
MML
(?) = E
p?(x,y)
[log
?
z
p(y, z|x; ?)]
We refer to this training method as maximum
marginal likelihood (MML); it has also been ex-
plored by Quattoni et al (2007).
The gradient of L
MML
(?) can also be written
as the difference of two expectations. The first is
an expectation over the empirical distribution of x
and y, and the model distribution of z. The second
is a double expectation over the empirical distribu-
tion of x and the model distribution of y and z.
?
??
j
L
MML
(?) = E
p?(x,y)
[E
p(z|y,x;?)
[F
j
(y, z,x)]]
? E
p?(x)
[E
p(y,z|x;?)
[F
j
(y, z,x)]].
We train models using L
MML
(?) with expected
gradient (Salakhutdinov et al, 2003).
To additionally leverage unlabeled data, we
compare with entropy regularization (ER). ER
adds a term to the objective function that en-
courages confident predictions on unlabeled data.
Training of linear-chain CRFs with ER is de-
scribed by Jiao et al (2006).
3 Generalized Expectation Criteria
In this section, we give a brief overview of gen-
eralized expectation criteria (GE) (Mann and Mc-
Callum, 2008; Druck et al, 2008) and explain how
we can use GE to learn CRF parameters with esti-
mates of feature expectations and unlabeled data.
GE criteria are terms in a parameter estimation
objective function that express preferences on the
2
10 is a default value that works well in many settings.
82
value of a model expectation of some function.
Given a score function S, an empirical distribution
p?(x), a model distribution p(y|x; ?), and a con-
straint function G
k
(x,y), the value of a GE crite-
rion is G(?) = S(E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]]).
GE provides a flexible framework for parameter
estimation because each of these elements can take
an arbitrary form. The most important difference
between GE and other parameter estimation meth-
ods is that it does not require a one-to-one cor-
respondence between constraint functions G
k
and
model feature functions. We leverage this flexi-
bility to estimate parameters of feature-rich CRFs
with a very small set of expectation constraints.
Constraint functions G
k
can be normalized so
that the sum of the expectations of a set of func-
tions is 1. In this case, S may measure the di-
vergence between the expectation of the constraint
function and a target expectation
?
G
k
.
G(?) =
?
G
k
log(E[G
k
(x,y)]), (1)
where E[G
k
(x,y)] = E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]].
It can be shown that the partial derivative of
G(?) with respect to parameter j is proportional to
the predicted covariance between the model fea-
ture function F
j
and the constraint function G
k
.
3
?
??
j
G(?) =
?
G
k
E[G
k
(x,y)]
? (2)
(
E
p?(x)
[
E
p(y|x;?)
[F
j
(x,y)G
k
(x,y)]
? E
p(y|x;?)
[F
j
(x,y)]E
p(y|x;?)
[G
k
(x,y)]
]
)
The partial derivative shows that GE learns pa-
rameter values for model feature functions based
on their predicted covariance with the constraint
functions. GE can thus be interpreted as a boot-
strapping method that uses the limited training sig-
nal to learn about parameters for related model
feature functions.
3.1 Learning with feature-label distributions
Mann and McCallum (2008) apply GE to a linear-
chain, first-order CRF. In this section we provide
an alternate treatment that arrives at the same ob-
jective function from the general form described
in the previous section.
Often, feature functions in a first-order linear-
chain CRF f are binary, and are the conjunction
3
If we use squared error for S, the partial derivative is the
covariance multiplied by 2(
?
G
k
? E[G
k
(x,y)]).
of an observational test q(x, i) and a label pair test
1
{y
i
=y
?
,y
i+1
=y
??
}
.
4
f(y
i
, y
i+1
,x, i) = 1
{y
i
=y
?
,y
i+1
=y
??
}
q(x, i)
The constraint functions G
k
we use here decom-
pose and operate similarly, except that they only
include a test for a single label. Single label con-
straints are easier for users to estimate and make
GE training more efficient. Label transition struc-
ture can be learned automatically from single la-
bel constraints through the covariance-based pa-
rameter update of Equation 2. For convenience,
we can write G
yk
to denote the constraint func-
tion that combines observation test k with a test
for label y. We also add a normalization constant
C
k
= E
p?(x)
[
?
i
q
k
(x, i)],
G
yk
(x,y) =
?
i
1
C
k
1
{y
i
=y}
q
k
(x, i)
Under this construction the expectation of G
yk
is
the predicted conditional probability that the label
at some arbitrary position i is y when the observa-
tional test at i succeeds, p?(y
i
=y|q
k
(x, i)=1; ?).
If we have a set of constraint functions {G
yk
:
y ? Y}, and we use the score function in Equa-
tion 1, then the GE objective function specifies the
minimization of the KL divergence between the
model and target distributions over labels condi-
tioned on the success of the observational test. In
general the objective function will consist of many
such KL divergence penalties.
Computing the first term of the covariance in
Equation 2 requires a marginal distribution over
three labels, two of which will be consecutive, but
the other of which could appear anywhere in the
sequence. We can compute this marginal using
the algorithm of Mann and McCallum (2008). As
previously described, this algorithm is O(n|Y|
3
)
for a sequence of length n. However, we make
the following novel observation: we do not need
to compute the extra lattices for feature label pairs
with
?
G
yk
= 0, since this makes Equation 2 equal
to zero. In Mann and McCallum (2008), probabil-
ities were smoothed so that ?
y
?
G
yk
> 0. If we
assume that only a small number of labels m have
non-zero probability, then the time complexity of
the gradient computation is O(nm|Y|
2
). In this
paper typically 1 ?m? 4, while |Y| is 11 or 13.
4
We this notation for an indicator function that returns 1
if the condition in braces is satisfied, and 0 otherwise.
83
In experiments in this paper, using this optimiza-
tion does not significantly affect final accuracy.
We use numerical optimization to estimate
model parameters. In general GE objective func-
tions are not convex. Consequently, we initial-
ize 0th-order CRF parameters using a sliding win-
dow logistic regression model trained with GE.
We also include a Gaussian prior on parameters
with ?
2
= 10 in the objective function.
3.2 Learning with labeled features
The training procedure described above requires
a set of observational tests or input features with
target distributions over labels. Estimating a dis-
tribution could be a difficult task for an annotator.
Consequently, we abstract away from specifying
a distribution by allowing the user to assign labels
to features (c.f. Haghighi and Klein (2006) , Druck
et al (2008)). For example, we say that the word
feature call has label contact. A label for a feature
simply indicates that the feature is a good indicator
of the label. Note that features can have multiple
labels, as does included in the active learning ses-
sion shown in Table 1. We convert an input feature
with a set of labels L into a distribution by assign-
ing probability 1/|L| for each l ? L and probabil-
ity 0 for each l /? L. By assigning 0 probability to
labels l /? L, we can use the speed-up described in
the previous section.
3.3 Related Work
Other proposed learning methods use labeled fea-
tures to label unlabeled data. The resulting
partially-labeled corpus can be used to train a CRF
by maximizing MML. Similarly, prototype-driven
learning (PDL) (Haghighi and Klein, 2006) opti-
mizes the joint marginal likelihood of data labeled
with prototype input features for each label. Ad-
ditional features that indicate similarity to the pro-
totypes help the model to generalize. In a previ-
ous comparison between GE and PDL (Mann and
McCallum, 2008), GE outperformed PDL without
the extra similarity features, whose construction
may be problem-specific. GE also performed bet-
ter when supplied accurate label distributions.
Additionally, both MML and PDL do not natu-
rally generalize to learning with features that have
multiple labels or distributions over labels, as in
these scenarios labeling the unlabeled data is not
straightforward. In this paper, we attempt to ad-
dress this problem using a simple heuristic: when
there are multiple choices for a token?s label, sam-
ple a label. In Section 5 we use this heuristic with
MML, but in general obtain poor results.
Raghavan and Allan (2007) also propose sev-
eral methods for learning with labeled features,
but in a previous comparison GE gave better re-
sults (Druck et al, 2008). Additionally, the gen-
eralization of these methods to structured output
spaces is not straightforward. Chang et al (2007)
present an algorithm for learning with constraints,
but this method requires users to set weights by
hand. We plan to explore the use of the recently
developed related methods of Bellare et al (2009),
Grac?a et al (2008), and Liang et al (2009) in fu-
ture work. Druck et al (2008) provide a survey
of other related methods for learning with labeled
input features.
4 Active Learning by Labeling Features
Feature active learning, presented in Algorithm 1,
is a pool-based active learning algorithm (Lewis
and Gale, 1994) (with a pool of features rather
than instances). The novel components of the
algorithm are an option to skip a query and the
notion that skipping and labeling have different
costs. The option to skip is important when us-
ing feature queries because a user may not know
how to label some features. In each iteration the
model is retrained using the train procedure, which
takes as input a set of labeled features C and un-
labeled data distribution p?. For the reasons de-
scribed in Section 3.3, we advocate using GE for
the train procedure. Then, while the iteration cost
c is less than the maximum cost c
max
, the feature
query q that maximizes the query selection met-
ric ? is selected. The accept function determines
whether the labeler will label q. If q is labeled, it
is added to the set of labeled features C, and the
label cost c
label
is added to c. Otherwise, the skip
cost c
skip
is added to c. This process continues for
N iterations.
4.1 Feature query selection methods
In this section we propose feature query selection
methods ?. Queries with a higher scores are con-
sidered better candidates. Note again that by fea-
tures we mean observational tests q
k
(x, i). It is
also important to note these are not feature selec-
tion methods since we are determining the features
for which supervisory feedback will be most help-
ful to the model, rather than determining which
features will be part of the model.
84
Algorithm 1 Feature Active Learning
Input: empirical distribution p?, initial feature constraints
C, label cost c
label
, skip cost c
skip
, max cost per iteration
c
max
, max iterations N
Output: model parameters ?
for i = 1 to N do
? = train(p?, C)
c = 0
while c < c
max
do
q = argmax
q
k
?(q
k
)
if accept(q) then
C = C ? label(q)
c = c+ c
label
else
c = c+ c
skip
end if
end while
end for
? = train(p?, C)
We propose to select queries that provide the
largest reduction in model uncertainty. We notate
possible responses to a query q
k
as g?. The Ex-
pected Information Gain (EIG) of a query is the
expectation of the reduction in model uncertainty
over all possible responses. Mathematically, IG is
?
EIG
(q
k
) = E
p(g?|q
k
;?)
[E
p?(x)
[H(p(y|x; ?)?
H(p(y|x; ?
g?
)]],
where ?
g?
are the new model parameters if the re-
sponse to q
k
is g?. Unfortunately, this method is
computationally intractable. Re-estimating ?
g?
will
typically involve retraining the model, and do-
ing this for each possible query-response pair is
prohibitively expensive for structured output mod-
els. Computing the expectation over possible re-
sponses is also difficult, as in this paper users may
provide a set of labels for a query, and more gen-
erally g? could be a distribution over labels.
Instead, we propose a tractable strategy for re-
ducing model uncertainty, motivated by traditional
uncertainty sampling (Lewis and Gale, 1994). We
assume that when a user responds to a query, the
reduction in uncertainty will be equal to the To-
tal Uncertainty (TU), the sum of the marginal en-
tropies at the positions where the feature occurs.
?
TU
(q
k
) =
?
i
?
j
q
k
(x
i
, j)H(p(y
j
|x
i
; ?))
Total uncertainty, however, is highly biased to-
wards selecting frequent features. A mean un-
certainty variant, normalized by the feature?s
count, would tend to choose very infrequent fea-
tures. Consequently we propose a tradeoff be-
tween the two extremes, called weighted uncer-
tainty (WU), that scales the mean uncertainty by
the log count of the feature in the corpus.
?
WU
(q
k
) = log(C
k
)
?
TU
(q
k
)
C
k
.
Finally, we also suggest an uncertainty-based met-
ric called diverse uncertainty (DU) that encour-
ages diversity among queries by multiplying TU
by the mean dissimilarity between the feature and
previously labeled features. For sequence labeling
tasks, we can measure the relatedness of features
using distributional similarity.
5
?
DU
(q
k
) = ?
TU
(q
k
)
1
|C|
?
j?C
1?sim(q
k
, q
j
)
We contrast the notion of uncertainty described
above with another type of uncertainty: the en-
tropy of the predicted label distribution for the fea-
ture, or expectation uncertainty (EU). As above
we also multiply by the log feature count.
?
EU
(q
k
) = log(C
k
)H(p?(y
i
= y|q
k
(x, i)=1; ?))
EU is flawed because it will have a large value for
non-discriminative features.
The methods described above require the model
to be retrained between iterations. To verify that
this is necessary, we compare against query selec-
tion methods that only consider the previously la-
beled features. First, we consider a feature query
selection method called coverage (cov) that aims
to select features that are dissimilar from existing
labeled features, increasing the labeled features?
?coverage? of the feature space. In order to com-
pensate for choosing very infrequent features, we
multiply by the log count of the feature.
?
cov
(q
k
) = log(C
k
)
1
|C|
?
j?C
1? sim(q
k
, q
j
)
Motivated by the feature query selection method
of Tandem Learning (Raghavan and Allan, 2007)
(see Section 4.2 for further discussion), we con-
sider a feature selection metric similarity (sim)
that is the maximum similarity to a labeled fea-
ture, weighted by the log count of the feature.
?
sim
(q
k
) = log(C
k
)max
j?C
sim(q
k
, q
j
)
5
sim(q
k
, q
j
) returns the cosine similarity between context
vectors of words occurring in a window of ?3.
85
Features similar to those already labeled are likely
to be discriminative, and therefore likely to be la-
beled (rather than skipped). However, insufficient
diversity may also result in an inaccurate model,
suggesting that coverage should select more use-
ful queries than similarity.
Finally, we compare with several passive base-
lines. Random (rand) assigns scores to features
randomly. Frequency (freq) scores input features
using their frequency in the training data.
?
freq
(q
k
) =
?
i
?
j
q
k
(x
i
, j)
Top LDA (LDA) selects the top words from 50
topics learned from the unlabeled data using la-
tent Dirichlet alocation (LDA) (Blei et al, 2003).
More specifically, the words w generated by each
topic t are ranked using the conditional probability
p(w|t). The word feature is assigned its maximum
rank across all topics.
?
LDA
(q
k
) = max
t
rank
LDA
(q
k
, t)
This method will select useful features if the top-
ics discovered are relevant to the task. A similar
heuristic was used by Druck et al (2008).
4.2 Related Work
Tandem Learning (Raghavan and Allan, 2007) is
an algorithm that combines feature and instance
active learning for classification. The algorithm it-
eratively queries the user first for instance labels,
then for feature labels. Feature queries are selected
according to their co-occurrence with important
model features and previously labeled features. As
noted in Section 3.3, GE is preferable to the meth-
ods Tandem Learning uses to learn with labeled
features. We address the mixing of feature and in-
stance queries in Section 4.3.
In order to better understand differences in fea-
ture query selection methodology, we proposed a
feature query selection method motivated
6
by the
method used in Tandem Learning in Section 4.1.
However, this method performs poorly in the ex-
periments in Section 5.
Liang et al (2009) simultaneously developed
a method for learning with and actively selecting
6
The query selection method of Raghavan and Allan
(2007) requires a stack that is modified between queries
within each iteration. Here query scores are only updated
after each iteration of labeling.
measurements, or target expectations with associ-
ated noise. The measurement selection method
proposed by Liang et al (2009) is based on
Bayesian experimental design and is similar to
the expected information gain method described
above. Consequently this method is likely to be
intractable for real applications. Note that Liang
et al (2009) only use this method in synthetic ex-
periments, and instead use a method similar to to-
tal uncertainty for experiments in part-of-speech
tagging. Unlike the experiments presented in this
paper, Liang et al (2009) conduct only simulated
active learning experiments and do not consider
skipping queries.
Sindhwani (Sindhwani et al, 2009) simultane-
ously developed an active learning method that
queries for both instance and feature labels that
are then used in a graph-based learning algorithm.
They find that querying certain features outper-
forms querying uncertain features, but this is likely
because their query selection method is similar
to the expectation uncertainty method described
above, and consequently non-discriminative fea-
tures may be queried often (see also the discus-
sion in Section 4.1). It is also not clear how this
graph-based training method would generalize to
structured output spaces.
4.3 Expectation Constraint Active Learning
Throughout this paper, we have focussed on label-
ing input features. However, the proposed meth-
ods generalize to queries for expectation estimates
of arbitrary functions, for example queries for the
label distributions for input features, labels for in-
stances (using a function that is non-zero only for
a particular instance), partial labels for instances,
and class priors. The uncertainty-based query se-
lection methods described in Section 4.1 apply
naturally to these new query types. Importantly
this framework would allow principled mixing of
different query types, instead of alternating be-
tween them as in Tandem Learning (Raghavan and
Allan, 2007). When mixing queries, it will be
important to use different costs for different an-
notation types (Vijayanarasimhan and Grauman,
2008), and estimate the probability of obtaining a
useful response to a query. We plan to pursue these
directions in future work. This idea was also pro-
posed by Liang et al (2009), but no experiments
with mixed active learning were presented.
86
5 Simulated User Experiments
In this section we experiment with an automated
oracle labeler. When presented an instance query,
the oracle simply provides the true labels. When
presented a feature query, the oracle first decides
whether to skip the query. We have found that
users are more likely to label features that are rel-
evant for only a few labels. Therefore, the oracle
labels a feature if the entropy of its per occurrence
label expectation, H(p?(y
i
= y|q
k
(x, i) = 1; ?)) ?
0.7. The oracle then labels the feature using a
heuristic: label the feature with the label whose
expectation is highest, as well as any label whose
expectation is at least half as large.
We estimate the effort of different labeling ac-
tions with preliminary experiments in which we
observe users labeling data for ten minutes. Users
took an average of 4 seconds to label a feature, 2
seconds to skip a feature, and 0.7 seconds to la-
bel a token. We setup experiments such that each
iteration simulates one minute of labeling by set-
ting c
max
= 60, c
skip
= 2 and c
label
= 4. For
instance active learning, we use Algorithm 1 but
without the skip option, and set c
label
= 0.7. We
use N = 10 iterations, so the entire experiment
simulates 10 minutes of annotation time. For ef-
ficiency, we consider the 500 most frequent unla-
beled features in each iteration. To start, ten ran-
domly selected seed labeled features are provided.
We use random (rand) selection, uncertainty
sampling (US) (using sequence entropy, normal-
ized by sequence length) and information den-
sity (ID) (Settles and Craven, 2008) to select in-
stance queries. We use Entropy Regularization
(ER) (Jiao et al, 2006) to leverage unlabeled in-
stances.
7
We weight the ER term by choosing the
best
8
weight in {10
?3
, 10
?2
, 10
?1
, 1, 10} multi-
plied by
#labeled
#unlabeled
for each data set and query se-
lection method. Seed instances are provided such
that the simulated labeling time is equivalent to la-
beling 10 features.
We evaluate on two sequence labeling tasks.
The apartments task involves segmenting 300
apartment classified ads into 11 fields including
features, rent, neighborhood, and contact. We use
the same feature processing as Haghighi and Klein
(2006), with the addition of context features in a
window of ?3. The cora references task is to ex-
tract 13 BibTeX fields such as author and booktitle
7
Results using self-training instead of ER are similar.
8
As measured by test accuracy, giving ER an advantage.
method apartments cora
mean final mean final
ER rand 48.1 53.6 75.9 81.1
ER US 51.7 57.9 76.0 83.2
ER ID 51.4 56.9 75.9 83.1
MML rand 47.7 51.2 58.6 64.6
MML WU 57.6 60.8 61.0 66.2
GE rand 59.0 64.8
?
77.6 83.7
GE freq 66.5
?
71.6
?
68.6 79.8
GE LDA 65.7
?
71.4
?
74.9 85.0
GE cov 68.2
??
72.6
?
73.5 83.3
GE sim 57.8 65.9
?
67.1 79.2
GE EU 66.5
?
71.6
?
68.6 79.8
GE TU 70.1
??
73.6
??
76.9 88.2
??
GE WU 71.6
??
74.6
??
80.3
??
88.1
??
GE DU 70.5
??
74.4
??
78.4
?
87.5
??
Table 2: Mean and final token accuracy results.
A
?
or
?
denotes that a GE method significantly
outperforms all non-GE or passive GE methods,
respectively. Bold entries significantly outperform
all others. Methods in italics are passive.
from 500 research paper references. We use a stan-
dard set of word, regular expressions, and lexicon
features, as well as context features in a window
of ?3. All results are averaged over ten random
80:20 splits of the data.
5.1 Results
Table 2 presents mean (across all iterations) and
final token accuracy results. On the apartments
task, GE methods greatly outperform MML
9
and
ER methods. Each uncertainty-based GE method
also outperforms all passive GE methods. On the
cora task, only GE with weighted uncertainty sig-
nificantly outperforms ER and passive GE meth-
ods in terms of mean accuracy, but all uncertainty-
based GE methods provide higher final accuracy.
This suggests that on the cora task, active GE
methods are performing better in later iterations.
Figure 1, which compares the learning curves of
the best performing methods of each type, shows
this phenomenon. Further analysis reveals that the
uncertainty-based methods are choosing frequent
features that are more likely to be skipped than
those selected randomly in early iterations.
We next compare with the results of related
methods published elsewhere. We cannot make
claims about statistical significance, but the results
9
Only the best MML results are shown.
87
illustrate the competitiveness of our method. The
74.6% final accuracy on apartments is higher than
any result obtained by Haghighi and Klein (2006)
(the highest is 74.1%), higher than the supervised
HMM results reported by Grenager et al (2005)
(74.4%), and matches the results of Mann and Mc-
Callum (2008) with GE with more accurate sam-
pled label distributions and 10 labeled examples.
Chang et al (2007) only obtain better results than
88.2% on cora when using 300 labeled examples
(two hours of estimated annotation time), 5000 ad-
ditional unlabeled examples, and extra test time in-
ference constraints. Note that obtaining these re-
sults required only 10 simulated minutes of anno-
tation time, and that GE methods are provided no
information about the label transition matrix.
6 User Experiments
Another advantage of feature queries is that fea-
ture names are concise enough to be browsed,
rather than considered individually. This allows
the design of improved interfaces that can further
increase the speed of feature active learning. We
built a prototype interface that allows the user to
quickly browse many candidate features. The fea-
tures are split into groups of five features each.
Each group contains features that are related, as
measured by distributional similarity. The features
within each group are sorted according to the ac-
tive learning metric. This interface, displayed in
Figure 3, may be useful because features in the
same group are likely to have the same label.
We conduct three types of experiments. First, a
user labels instances selected by information den-
sity, and models are trained using ER. The in-
stance labeling interface allows the user to label
tokens quickly by extending the current selection
one token at a time and only requiring a single
keystroke to label an entire segment. Second,
the user labels features presented one-at-a-time by
weighted uncertainty, and models are trained us-
ing GE. To aid the user in understanding the func-
tion of the feature quickly, we provide several ex-
amples of the feature occurring in context and the
model?s current predicted label distribution for the
feature. Finally, the user labels features organized
using the grid interface described in the previous
paragraph. Weighted uncertainty is used to sort
feature queries within each group, and GE is used
to train models. Each iteration of labeling lasts
two minutes, and there are five iterations. Retrain-
ing with ER between iterations takes an average
of 5 minutes on cora and 3 minutes on apart-
ments. With GE, the retraining times are on av-
erage 6 minutes on cora and 4 minutes on apart-
ments. Consequently, even when viewed with to-
tal time, rather than annotation time, feature active
learning is beneficial. While waiting for models to
retrain, users can perform other tasks.
Figure 2 displays the results. User 1 labeled
apartments data, while Users 2 and 3 labeled cora
data. User 1 was able to obtain much better results
with feature labeling than with instance labeling,
but performed slightly worse with the grid inter-
face than with the serial interface. User 1 com-
mented that they found the label definitions for
apartments to be imprecise, so the other experi-
ments were conducted on the cora data. User 2
obtained better results with feature labeling than
instance labeling, and obtained higher mean ac-
curacy with the grid interface. User 3 was much
better at labeling features than instances, and per-
formed especially well using the grid interface.
7 Conclusion
We proposed an active learning approach in which
features, rather than instances, are labeled. We
presented an algorithm for active learning with
features and several feature query selection meth-
ods that approximate the expected reduction in
model uncertainty of a feature query. In simu-
lated experiments, active learning with features
outperformed passive learning with features, and
uncertainty-based feature query selection outper-
formed other baseline methods. In both simulated
and real user experiments, active learning with
features outperformed passive and active learning
with instances. Finally, we proposed a new label-
ing interface that leverages the conciseness of fea-
ture queries. User experiments suggested that this
grid interface can improve labeling efficiency.
Acknowledgments
We thank Kedar Bellare for helpful discussions and Gau-
rav Chandalia for providing code. This work was supported
in part by the Center for Intelligent Information Retrieval
and the Central Intelligence Agency, the National Security
Agency and National Science Foundation under NSF grant
#IIS-0326249. The second author was supported by a grant
from National Human Genome Research Institute. Any opin-
ions, findings and conclusions or recommendations are the
authors? and do not necessarily reflect those of the sponsor.
88
2 4 6 8 1035
4045
5055
6065
7075
80
simulated annotation time (minutes)
token
 accu
racy
apartments
 
 
ER + uncertaintyMML + weighted uncertaintyGE + frequencyGE + weighted uncertainty 2 4 6 8 1045
5055
6065
7075
8085
90
simulated annotation time (minutes)
token
 accu
racy
cora
 
 
ER + uncertaintyMML + weighted uncertaintyGE + randomGE + weighted uncertainty
Figure 1: Token accuracy vs. time for best performing ER, MML, passive GE, and active GE methods.
2 4 6 8 10510
1520
2530
3540
4550
5560
65
annotation time (minutes)
token 
accura
cy
user 1 ? apartments
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 1030
3540
4550
5560
6570
annotation time (minutes)
token 
accura
cy
user 2 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 103540
4550
5560
6570
7580
85
annotation time (minutes)
token 
accura
cy
user 3 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid)
Figure 2: User experiments with instance labeling and feature labeling with the serial and grid interfaces.
Figure 3: Grid feature labeling interface. Boxes on the left contain groups of features that appear in
similar contexts. Features in the same group often receive the same label. On the right, the model?s
current expectation and occurrences of the selected feature in context are displayed.
89
References
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning with
expectation constraints. In UAI.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation.
Journal of Machine Learning Research, 3:2003.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL, pages 280?287.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In SIGIR.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20. MIT Press.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In HTL-NAACL.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209?216.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
David D. Lewis and William A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR,
pages 3?12, New York, NY, USA. Springer-Verlag
New York, Inc.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In ICML.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In ACL.
A. Quattoni, S. Wang, L.-P Morency, M. Collins, and
T. Darrell. 2007. Hidden conditional random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29:1848?1852, October.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR,
pages 79?86.
Ruslan Salakhutdinov, Sam Roweis, and Zoubin
Ghahramani. 2003. Optimization with em and
expectation-conjugate-gradient. In ICML, pages
672?679.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP.
Burr Settles. 2009. Active learning literature survey.
Technical Report 1648, University of Wisconsin -
Madison.
Vikas Sindhwani, Prem Melville, and Richard D.
Lawrence. 2009. Uncertainty sampling and trans-
ductive experimental design for active dual supervi-
sion. In ICML.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful image
annotations for recognition. In NIPS.
90
Biomedical Named Entity Recognition Using
Conditional Random Fields and Rich Feature Sets
Burr Settles
Department of Computer Sciences
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
Madison, WI, USA
bsettles@cs.wisc.edu
1 Introduction
As the wealth of biomedical knowledge in the
form of literature increases, there is a rising need
for effective natural language processing tools
to assist in organizing, curating, and retrieving
this information. To that end, named entity
recognition (the task of identifying words and
phrases in free text that belong to certain classes
of interest) is an important first step for many
of these larger information management goals.
In recent years, much attention has been fo-
cused on the problem of recognizing gene and
protein mentions in biomedical abstracts. This
paper presents a framework for simultaneously
recognizing occurrences of PROTEIN, DNA, RNA,
CELL-LINE, and CELL-TYPE entity classes us-
ing Conditional Random Fields with a variety
of traditional and novel features. I show that
this approach can achieve an overall F1 mea-
sure around 70, which seems to be the current
state of the art.
The system described here was developed as
part of the BioNLP/NLPBA 2004 shared task.
Experiments were conducted on a training and
evaluation set provided by the task organizers.
2 Conditional Random Fields
Biomedical named entity recognition can be
thought of as a sequence segmentation prob-
lem: each word is a token in a sequence to
be assigned a label (e.g. PROTEIN, DNA, RNA,
CELL-LINE, CELL-TYPE, or OTHER1). Conditional
Random Fields (CRFs) are undirected statisti-
cal graphical models, a special case of which is a
linear chain that corresponds to a conditionally
trained finite-state machine. Such models are
well suited to sequence analysis, and CRFs in
1More accurately, the data is in IOB format. B-DNA
labels the first word of a DNA mention, I-DNA labels all
subsequent words (likewise for other entities), and O la-
bels non-entities. For simplicity, this paper only refers
to the entities, not all the IOB label variants.
particular have been shown to be useful in part-
of-speech tagging (Lafferty et al, 2001), shallow
parsing (Sha and Pereira, 2003), and named en-
tity recognition for newswire data (McCallum
and Li, 2003). They have also just recently been
applied to the more limited task of finding gene
and protein mentions (McDonald and Pereira,
2004), with promising early results.
Let o = ?o1, o2, . . . , on? be an sequence of
observed words of length n. Let S be a set
of states in a finite state machine, each corre-
sponding to a label l ? L (e.g. PROTEIN, DNA,
etc.). Let s = ?s1, s2, . . . , sn? be the sequence
of states in S that correspond to the labels as-
signed to words in the input sequence o. Linear-
chain CRFs define the conditional probability of
a state sequence given an input sequence to be:
P (s|o) =
1
Zo
exp
?
?
n?
i=1
m?
j=1
?jfj(si?1, si, o, i)
?
?
where Zo is a normalization factor of all state
sequences, fj(si?1, si, o, i) is one of m functions
that describes a feature, and ?j is a learned
weight for each such feature function. This pa-
per considers the case of CRFs that use a first-
order Markov independence assumption with
binary feature functions. For example, a fea-
ture may have a value of 0 in most cases, but
given the text ?the ATPase? it has the value 1
along the transition where si?1 corresponds to
a state with the label OTHER, si corresponds to a
state with the label PROTEIN, and fj is the fea-
ture function Word=ATPase ? o at position
i in the sequence. Other feature functions that
could have the value 1 along this transition are
Capitalized, MixedCase, and Suffix=ase.
Intuitively, the learned feature weight ?j
for each feature fj should be positive for fea-
tures that are correlated with the target label,
negative for features that are anti-correlated
with the label, and near zero for relatively
uninformative features. These weights are
104
set to maximize the conditional log likelihood
of labeled sequences in a training set D =
{?o, l?(1), . . . , ?o, l?(n)}:
LL(D) =
n?
i=1
log
(
P (l(i)|o(i))
)
?
m?
j=1
?2j
2?2
.
When the training state sequences are fully
labeled and unambiguous, the objective func-
tion is convex, thus the model is guaranteed
to find the optimal weight settings in terms of
LL(D). Once these settings are found, the la-
beling for an new, unlabeled sequence can be
done using a modified Viterbi algorithm. CRFs
are presented in more complete detail by Laf-
ferty et al (2001).
These experiments use the MALLET imple-
mentation of CRFs (McCallum, 2002), which
uses a quasi-Newton method called L-BFGS to
find these feature weights efficiently.
3 Feature Set
One property that makes feature based statisti-
cal models like CRFs so attractive is that they
reduce the problem to finding an appropriate
feature set. This section outlines the two main
types of features used in these experiments.
3.1 Orthographic Features
The simplest and most obvious feature set is the
vocabulary from the training data. Generaliza-
tions over how these words appear (e.g. capital-
ization, affixes, etc.) are also important. The
present model includes training vocabulary, 17
orthographic features based on regular expres-
sions (e.g. Alphanumeric, HasDash, Ro-
manNumeral) as well as prefixes and suffixes
in the character length range [3,5].
Words are also assigned a generalized ?word
class? similar to Collins (2002), which replaces
capital letters with ?A?, lowercase letters with
?a?, digits with ?0?, and all other characters
with ? ?. There is a similar ?brief word class?
feature which collapses consecutive identical
characters into one. Thus the words ?IL5?
and ?SH3? would both be given the features
WC=AA0 and BWC=A0, while ?F-actin? and
?T-cells? would both be assigned WC=A aaaaa
and BWC=A a.
To model local context simply, neighboring
words in the window [-1,1] are also added as
features. For instance, the middle token in the
sequence ?human UDG promoter? would have
features Word=UDG, Neighbor=human and
Neighbor=promoter.
3.2 Semantic Features
In addition to orthography, the model could also
benefit from generalized semantic word groups.
If training sequences contain ?PML/RAR al-
pha,? ?beta 2-M,? and ?kappa B-specific DNA
binding protein? all labeled with PROTEIN, the
model might learn that the words ?alpha,?
?beta,? and ?kappa? are indicative of pro-
teins, but cannot capture the fact that they
are all semantically related because they are
Greek letters. Similarly, words with the feature
WC=Aaa are often part of protein names, such
as ?Rab,? ?Alu,? and ?Gag.? But the model
may have a difficult time setting the weights
for this feature when confronted with words like
?Phe,? ?Arg,? and ?Cys,? which are amino acid
abbreviations and not often labeled as part of a
protein name.
This sort of semantic domain knowledge can
be provided in the form of lexicons. I pre-
pared a total of 17 such lexicons, which include
7 that were entered by hand (Greek letters,
amino acids, chemical elements, known viruses,
plus abbreviations of all these), and 4 corre-
sponding to genes, chromosome locations, pro-
teins, and cell lines, drawn from online public
databases (Cancer GeneticsWeb,2 BBID,3 Swis-
sProt,4 and the Cell Line Database5). Feature
functions for the lexicons are set to 1 if they
match words in the input sequence exactly. For
lexicon entries that are multi-word, all words
are required to match in the input sequence.
Since no suitable database of terms for the
CELL-TYPE class was found online, a lexicon was
constructed by utilizing Google Sets,6 an online
tool which takes a few seed examples and lever-
ages Google?s web index to return other terms
that appear in similar formatting and context
as the seeds on web pages across the Internet.
Several examples from the training data (e.g.
?lymphocyte? and ?neutrophil?) were used as
seeds and new cell types (e.g. ?chondroblast,?
which doesn?t even occur in the training data),
were returned. The process was repeated until
the lexicon grew to roughly 50 entries, though
it could probably be more complete.
With all this information at the model?s dis-
posal, it can still be difficult to properly dis-
ambiguate between these entities. For exam-
2http://www.cancerindex.org/geneweb/
3http://bbid.grc.nia.nih.gov/bbidgene.html
4http://us.expasy.org/sprot/
5http://www.biotech.ist.unige.it/interlab/cldb.html
6http://labs.google.com/sets
105
ple, the acronym ?EPC? appears in these static
lexicons both as a protein (?eosinophil cationic
protein? [sic]) and as a cell line (?epithelioma
papulosum cyprini?). Furthermore, a single
word like ?transcript? is sometimes all that
disambiguates between RNA and DNA mentions
(e.g. ?BMLF1 transcript?). The CRF can learn
weights for these individual words, but it may
help to build general, dynamic keyword lexi-
cons that are associated with each label to assist
in disambiguating between similar classes (and
perhaps boost performance on low-frequency la-
bels, such as RNA and CELL-LINE, for which
training data are sparse).
These keyword lexicons are generated auto-
matically as follows. All of the labeled terms are
extracted from the training set and separated
into five lists (one for each entity class). Stop
words, Greek letters, and digits are filtered, and
remaining words are tallied for raw frequency
counts under each entity class label. These fre-
quencies are then subjected to a ?2 test, where
the null hypothesis is that a word?s frequency is
the same for a given entity as it is for any other
entity of interest (i.e. PROTEIN vs. DNA + RNA
+ CELL-LINE + CELL-TYPE, such that there is
only one degree of freedom). All words for which
the null hypothesis is rejected with a p-value
< 0.005 are added to the keyword lexicon for
its majority class. Some example keywords are
listed in table 1.
Keyword ?2 value Lexicon
protein 1121.5 PROTEIN
gene 984.3 DNA
line 618.1 CELL-LINE
promoter 613.4 DNA
factor 563.2 PROTEIN
site 399.8 DNA
receptor 338.7 PROTEIN
complex 312.8 PROTEIN
mRNA 292.2 RNA
sequence 196.5 DNA
peripheral 57.8 CELL-TYPE
lineage 56.1 CELL-TYPE
jurkat 45.2 CELL-LINE
culture 41.3 CELL-LINE
transcript 40.9 RNA
clone 38.1 CELL-LINE
mononuclear 30.2 CELL-TYPE
messenger 12.3 RNA
Table 1: A sample of high-ranking semantic key-
words and the lexicons to which they belong.
Orthographic Features Only
Entity R P F1 L-F1 R-F1
PROTEIN 76.3 68.4 72.1 77.4 79.2
DNA 62.4 68.2 65.2 68.5 73.8
RNA 61.9 62.9 62.4 64.9 75.2
CELL-LINE 53.8 54.0 53.9 58.5 65.1
CELL-TYPE 63.6 78.5 70.3 72.6 80.4
Overall 70.3 69.3 69.8 74.2 77.9
Complete Feature Set
Entity R P F1 L-F1 R-F1
PROTEIN 76.1 68.2 72.0 77.3 79.2
DNA 62.1 67.9 64.9 67.7 74.1
RNA 65.3 64.2 64.7 66.4 73.9
CELL-LINE 57.4 54.1 55.7 59.2 64.2
CELL-TYPE 61.7 78.4 69.1 71.3 79.7
Overall 70.0 69.0 69.5 73.7 77.7
Table 2: Detailed performance of the two fea-
tures sets. Relaxed F1-scores using left- and
right-boundary matching are also reported.
4 Results and Discussion
Two experiments were completed in the time
allotted: one CRF model using only the ortho-
graphic features described in section 3.1, and a
second system using all the semantic lexicons
from 3.2 as well. Detailed results are presented
in table 2. The orthographic model achieves an
overall F1 measure of 69.8 on the evaluation set
(88.9 on the training set), converging after 230
training iterations and approximately 18 hours
of computation. The complete model, however,
only reached an overall F1 of 69.5 on the evalu-
ation set (86.7 on the training set), converging
after 152 iterations in approximately 9 hours.
The deleterious effect of the semantic lexi-
cons is surprising and puzzling.7 However, even
though semantic lexicons slightly decrease over-
all performance, it is worthwhile to note that
adding lexicons actually improves both recall
and precision for the RNA and CELL-LINE en-
tities. These happen to be the two lowest fre-
quency class labels in the data, together com-
prising less than 10% of the mentions in either
the training or evaluation set. Error analysis
shows that several of the orthographic model?s
false negatives for these entities are of the form
?messenger accumulation? (RNA) or ?nonadher-
ent culture? (CELL-LINE). It may be that key-
word lexicons contributed to the model identify-
ing these low frequency terms more accurately.
7Note, however, that these figures are on a single
training/evaluation split without cross-validation, so dif-
ferences are likely not statistically significant.
106
Also of note is that, in both experiments, the
CRF framework achieves somewhat comparable
performance across all entities. In a previous
attempt to use a Hidden Markov Model to si-
multaneously recognize multiple biomedical en-
tities (Collier et al, 2000), HMM performance
for a particular entity seemed more or less pro-
portional to its frequency in the data. The ad-
vantage of the CRF here may be due to the
fact that HMMs are generative models trained
to learn the joint probability P (o, l) ? where
data for l may be sparse ? and use Bayes rule
to predict the best label. CRFs are discrimina-
tive models trained to maximize P (l|o) directly.
5 Conclusions and Future Work
In short, I have presented in detail a frame-
work for recognizing multiple entity classes
in biomedical abstracts with Conditional Ran-
dom Fields. I have shown that a CRF-based
model with only simple orthographic features
can achieve performance near the current state
of the art, while using semantic lexicons (as
presented here) do not positively affect perfor-
mance.8
While the system presented here shows
promise, there is still much to be explored.
Richer syntactic information such as shallow
parsing may be useful. The method introduced
in section 3.2 to generate semantic keywords can
also be adapted to generate features for entity-
specific morphology (e.g. affixes) and context,
both linearly (e.g. neighboring words) and hi-
erarchically (e.g. from a parse).
Most interesting, though, might be to inves-
tigate why the lexicons do not generally help.
One explanation is simply an issue of tokeniza-
tion. While one abstract refers to ?IL12,? oth-
ers may write ?IL-12? or ?IL 12.? Similarly,
the generalization of entities to groups (e.g. ?x
antibody? vs. ?x antibodies?) can cause prob-
lems for these rigid lexicons that require exact
matching. Enumerating all such variants for ev-
ery entry in a lexicon is absurd. Perhaps relax-
ing the matching criteria and standardizing to-
kenization for both the input and lexicons will
improve their utility.
8More recent work (not submitted for evaluation) in-
dicates that lexicons are indeed useful, but mainly when
training data are limited. I have also found that using
orthographic features with part-of-speech tags and only
the RNA and CELL-LINE (rare class) lexicons can boost
overall F1 to 70.3 on the evaluation data, with particu-
lar improvements for the RNA and CELL-LINE entities.
Acknowledgements
I would like to thank my advisor Mark Craven for
his advice and guidance, as well as Andrew McCal-
lum and Aron Culotta for answering my questions
about the MALLET system. This work is supported
by NLM training grant 5T15LM007359-02 and NIH
grant R01 LM07050-01.
References
Nigel Collier, Chikashi Nobata, and Jun ichi Tsu-
jii. 2000. Extracting the names of genes and gene
products with a hidden markov model. In Pro-
ceedings of the International Conference on Com-
putational Linguistics, pages 201?207. Saarbru-
ucken, Germany.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proceedings of the Association
for Computational Linguistics Conference, pages
489?496. Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning. Williamstown,
MA, USA.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced
lexicons. In Proceedings of the Conference on Nat-
ural Language Learning, pages 188?191. Edmon-
ton, Canada.
Andrew McCallum. 2002. Mallet: A
machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Iden-
tifying gene and protein mentions in text us-
ing conditional random fields. In Proceedings of
BioCreative: Critical Assessment for Information
Extraction in Biology. Grenada, Spain.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference. Edmonton, Canada.
107
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1467?1478,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Closing the Loop: Fast, Interactive Semi-Supervised Annotation
With Queries on Features and Instances
Burr Settles
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213 USA
bsettles@cs.cmu.edu
Abstract
This paper describes DUALIST, an active
learning annotation paradigm which solicits
and learns from labels on both features (e.g.,
words) and instances (e.g., documents). We
present a novel semi-supervised training al-
gorithm developed for this setting, which is
(1) fast enough to support real-time interac-
tive speeds, and (2) at least as accurate as pre-
existing methods for learning with mixed fea-
ture and instance labels. Human annotators in
user studies were able to produce near-state-
of-the-art classifiers?on several corpora in a
variety of application domains?with only a
few minutes of effort.
1 Introduction
In active learning, a classifier participates in its own
training process by posing queries, such as request-
ing labels for documents in a text classification task.
The goal is to maximize the accuracy of the trained
system in the most economically efficient way. This
paradigm is well-motivated for natural language ap-
plications, where unlabeled data may be readily
available (e.g., text on the Internet), but the anno-
tation process can be slow and expensive.
Nearly all previous work in active learning, how-
ever, has focused on selecting queries from the
learner?s perspective. For example, experiments are
often run in simulation rather than with user stud-
ies, and results are routinely evaluated in terms of
training set size rather than human annotation time
or labor costs (which are more reasonable measures
of labeling effort). Many state-of-the-art algorithms
are also too slow to run or too tedious to implement
to be useful for real-time interaction with human an-
notators, and few analyses have taken these factors
into account. Furthermore, there is very little work
on actively soliciting domain knowledge from hu-
mans (e.g., information about features) and incorpo-
rating this into the learning process.
While selecting good queries is clearly important,
if our goal is to reduce actual annotation effort these
human factors must be taken into account. In this
work, we propose a new interactive annotation inter-
face which addresses some of these issues; in partic-
ular it has the ability to pose queries on both features
(e.g., words) and instances (e.g., documents). We
present a novel semi-supervised learning algorithm
that is fast, flexible, and accurate enough to support
these interface design constraints interactively.
2 DUALIST: Utility for Active Learning
with Instances and Semantic Terms
Figure 1 shows a screenshot of the DUALIST an-
notation tool, which is freely available as an open-
source software project1. On the left panel, users
are presented with unlabeled documents: in this case
Usenet messages that belong to one of two sports-
related topics: baseball and hockey. Users may label
documents by clicking on the class buttons listed be-
low each text. In cases of extreme ambiguity, users
may ignore a document by clicking the ?X? to re-
move it from the pool of possible queries.
On the right panel, users are given a list of fea-
ture queries organized into columns by class label.
1http://code.google.com/p/dualist/
1467
Figure 1: A screenshot of DUALIST.
The rationale for these columns is that they should
reduce cognitive load (i.e., once a user is in the base-
ball mindset, s/he can simply go down the list, label-
ing features in context: ?plate,? ?pitcher,? ?bases,?
etc.). Within each column, words are sorted by how
informative they are the to classifier, and users may
click on words to label them. Each column also con-
tains a text box, where users may ?inject? domain
knowledge by typing in arbitrary words (whether
they appear in any of the columns or not). The list
of previously labeled words appears at the bottom of
each list (highlighted), and can be unlabeled at any
time, if users later feel they made any errors.
Finally, a large submit button is located at the top
of the screen, which users must click to re-train the
classifier and receive a new set of queries. The learn-
ing algorithm is actually fast enough to do this au-
tomatically after each labeling action. However, we
found such a dynamically changing interface to be
frustrating for users (e.g., words they wanted to la-
bel would move or disappear).
2.1 A Generative Model for Learning from
Feature and Instance Labels
For the underlying model in this system, we use
multinomial na??ve Bayes (MNB) since it is sim-
ple, fast, and known to work well for several nat-
ural language applications?text classification in
particular?despite its simplistic and often violated
independence assumptions (McCallum and Nigam,
1998; Rennie et al, 2003).
MNB models the distribution of features as a
multinomial: documents are sequences of words,
with the ?na??ve? assumption that words in each
position are generated independently. Each docu-
ment is treated as a mixture of classes, which have
their own multinomial distributions over words. Let
the model be parameterized by the vector ?, with
?j = P (yj) denoting the probability of class yj , and
?jk = P (fk|yj) denoting the probability of generat-
ing word fk given class yj . Note that for class pri-
ors?j ?j = 1, and for per-class word multinomials?
k ?jk = 1. The likelihood of document x being
generated by class yj is given by:
P?(x|yj) = P (|x|)
?
k
(?jk)fk(x),
where fk(x) is the frequency count of word fk in
document x. If we assume P (|x|) is distributed in-
dependently of class, and since document length |x|
is fixed, we can drop the first term for classification
purposes. Then, we can use Bayes? rule to calculate
the posterior probability under the model of a label,
given the input document for classification:
P?(yj |x) =
P?(yj)P?(x|yj)
P?(x)
= ?j
?
k(?jk)fk(x)
Z(x) ,
(1)
where Z(x) is shorthand for a normalization con-
stant, summing over all possible class labels.
The task of training such a classifier involves es-
timating the parameters in ?, given a set of labeled
instances L = {?x(l), y(l)?}Ll=1. To do this, we use
a Dirichlet prior and take the expectation of each
parameter with respect to the posterior, which is a
simple way to estimate a multinomial (Heckerman,
1995). In other words, we count the fraction of times
the word fk occurs in the labeled set among doc-
uments of class yj , and the prior adds mjk ?hallu-
cinated? occurrences for a smoothed version of the
maximum likelihood estimate:
?jk =
mjk +
?
i P (yj |x(i))fk(x(i))
Z(fk)
. (2)
Here, mjk is the prior for word fk under class yj ,
P (yj |x(i)) ? {0, 1} indicates the true labeling of the
ith document in the training set, and Z(fk) is a nor-
malization constant summing over all words in the
vocabulary. Typically, a uniform prior is used, such
as the Laplacian (a value of 1 for all mjk). Class pa-
rameters ?j are estimated a similar way, by counting
1468
the fraction of documents that are labeled with that
class, subject to a prior mj . This prior is important
in the event that no documents are yet labeled with
yj , which can be quite common early on in the active
learning process.
Recall that our scenario lets human annotators
provide not only document labels, but feature labels
as well. To make use of this additional information,
we assume that labeling the word fk with a class yj
increases the probability P (fk|yj) of the word ap-
pearing in documents of that class. The natural in-
terpretation of this under our model is to increase the
prior mjk for the corresponding multinomial. To do
this we introduce a new parameter ?, and define the
elements of the Dirichlet prior as follows:
mjk =
{
1 + ? if fk is labeled with yj ,
1 otherwise.
This approach is extremely flexible, and offers three
particular advantages over the previous ?pooling
multinomials? approach for incorporating feature la-
bels into MNB (Melville et al, 2009). The pooling
multinomials algorithm averages together two sets
of ?jk parameters: one that is estimated from labeled
data, and another derived from feature labels under
the assumption of a boolean output variable (treating
labeled features are ?polarizing? factors). Therefore,
pooling multinomials can only be applied to binary
classification tasks, while our method works equally
well for problems with multiple classes. The second
advantage is that feature labels need not be mutu-
ally exclusive, so the word ?score? could be labeled
with both baseball and hockey, if necessary (e.g.,
if the task also includes several non-sports labels).
Finally, our framework allows users to conceivably
provide feature-specific priors ?jk to, for example,
imply that the word ?inning? is a stronger indicator
for baseball than the word ?score? (which is a more
general sports term). However, we leave this aspect
for future work and employ the fixed-? approach as
described above in this study.
2.2 Exploiting Unlabeled Data
In addition to document and feature labels, we usu-
ally have access to a large unlabeled corpus. In fact,
these texts form the pool of possible instance queries
in active learning. We can take advantage of this ad-
ditional data in generative models like MNB by em-
ploying the Expectation-Maximization (EM) algo-
rithm. Combining EM with pool-based active learn-
ing was previously studied in the context of instance
labeling (McCallum and Nigam, 1998), and we ex-
tend the method to our interactive scenario, which
supports feature labeling as well.
First, we estimate initial parameters ?? as in Sec-
tion 2.1, but using only the priors (and no instances).
Then, we apply the induced classifier on the unla-
beled pool U = {x(u)}Uu=1 (Eq. 1). This is the ?E?
step of EM. Next we re-estimate feature multino-
mials ?jk, using both labeled instances from L and
probabilistically-labeled instances from U (Eq. 2).
In other words, P (yj |x) ? {0, 1} for x ? L, and
P (yj |x) = P??(yj |x) for x ? U . We also weight
the data in U by a factor of 0.1, so as not to over-
whelm the training signal coming from true instance
labels in L. Class parameters ?j are re-estimated in
the analogous fashion. This is the ?M? step.
For speed and interactivity, we actually stop train-
ing after this first iteration. When feature labels are
available, we found that EM generally converges in
four to 10 iterations, requiring more training time
but rarely improving accuracy (the largest gains con-
sistently come in the first iteration). Also, we ignore
labeled data in the initial estimation of ?? because L
is too small early in active learning to yield good re-
sults with EM. Perhaps this can be improved by us-
ing an ensemble (McCallum and Nigam, 1998), but
that comes at further computational expense. Fea-
ture labels, on the other hand, seem generally more
reliable for probabilistically labeling U .
2.3 Selecting Instance and Feature Queries
The final algorithmic component to our system is
the selection of informative queries (i.e., unlabeled
words and documents) to present to the annotator.
Querying instances is the traditional mode of ac-
tive learning, and is well-studied in the literature;
see Settles (2009) for a review. In this work we use
entropy-based uncertainty sampling, which ranks all
instances in U by the posterior class entropy under
the modelH?(Y |x) = ??j P?(yj |x) logP?(yj |x),
and asks the user to label the top D unlabeled doc-
uments. This simple heuristic is an approximation
to querying the instance with the maximum infor-
mation gain (since the class entropy, once labeled,
is zero), under the assumption that each x is repre-
1469
sentative of the underlying natural data distribution.
Moreover, it is extremely fast to compute, which is
important for our interactive environment.
Querying features, though, is a newer idea with
significantly less research behind it. Previous work
has either assumed that (1) features are not assigned
to classes, but instead flagged for ?relevance? to the
task (Godbole et al, 2004; Raghavan et al, 2006),
or (2) feature queries are posed just like instance
queries: a word is presented to the annotator, who
must choose among the labels (Druck et al, 2009;
Attenberg et al, 2010). Recall from Figure 1 that
we want to organize feature queries into columns by
class label. This means our active learner must pro-
duce queries that are class-specific.
To select these feature queries, we first rank ele-
ments in the vocabulary by information gain (IG):
IG(fk) =
?
Ik
?
j
P (Ik, yj) log
P (Ik, yj)
P (Ik)P (yj)
,
where Ik ? {0, 1} is a variable indicating the pres-
ence or absence of a feature. This is essentially
the common feature-selection method for identify-
ing the most salient features in text classification
(Sebastiani, 2002). However, we use both L and
probabilistically-labeled instances from U to com-
pute IG(fk), to better reflect what the model be-
lieves it has learned. To organize queries into
classes, we take the top V unlabeled features and
pose fk for the class yj with which it occurs most
frequently, as well as any other class with which it
occurs at least 75% as often. Intuitively, this ap-
proach (1) queries features that the model believes
are most informative, and (2) automatically identi-
fies classes that seem most correlated. To our knowl-
edge, DUALIST is the first active learning environ-
ment with both of these properties.
3 Experiments
We conduct four sets of experiments to evaluate our
approach. The first two are ?offline? experiments,
designed to better understand (1) how our training
algorithm compares to existing methods for feature-
label learning, and (2) the effects of tuning the ?
parameter. The other experiments are user studies
designed to empirically gauge how well human an-
notators make use of DUALIST in practice.
We use a variety of benchmark corpora in the fol-
lowing evaluations. Reuters (Rose et al, 2002) is
a collection of news articles organized into topics,
such as acquisitions, corn, earnings, etc. As in pre-
vious work (Raghavan et al, 2006) we use the 10
most frequent topics, but further process the cor-
pus by removing ambiguous documents (i.e., that
belong to multiple topics) so that all articles have
a unique label, resulting in a corpus of 9,002 arti-
cles. WebKB (Craven et al, 1998) consists of 4,199
university web pages of four types: course, faculty,
project, and student. 20 Newsgroups (Lang, 1995)
is a set of 18,828 Usenet messages from 20 different
online discussion groups. For certain experiments
(such as the one shown in Figure 1), we also use
topical subsets. Movie Reviews (Pang et al, 2002)
is a set of 2,000 online movie reviews categorized as
positive or negative in sentiment. All data sets were
processed using lowercased unigram features, with
punctuation and common stop-words removed.
3.1 Comparison of Learning Algorithms
An important question is how well our learning al-
gorithm, ?MNB/Priors,? performs relative to exist-
ing baseline methods for learning with labeled fea-
tures. We compare against two such approaches
from the literature. ?MaxEnt/GE? is a maximum en-
tropy classifier trained using generalized expectation
(GE) criteria (Druck et al, 2008), which are con-
straints used in training discriminative linear mod-
els. For labeled features, these take the form of ex-
pected ?reference distributions? conditioned on the
presence of the feature (e.g., 95% of documents con-
taining the word ?inning? should be labeled base-
ball). For each constraint, a term is added to the
objective function to encourage parameter settings
that yield predictions conforming to the reference
distribution on unlabeled instances. ?MNB/Pool? is
na??ve Bayes trained using the pooling multinomials
approach (Melville et al, 2009) mentioned in Sec-
tion 2.1. We also expand upon MNB/Pool using an
EM variant to make it semi-supervised.
We use the implementation of GE training from
the open-source MALLET toolkit2, and implement
both MNB variants in the same data-processing
pipeline. Because the GE implementation available
2http://mallet.cs.umass.edu
1470
Corpus MaxEnt/GE MNB/Pool Pool+EM1 MNB/Priors Priors+EM1
Reuters 82.8 (22.9) ? ? ? ? 83.7 (?0.1) 86.6 (0.3)
WebKB 22.2 (4.9) ? ? ? ? 67.5 (?0.1) 67.8 (0.1)
20 Newsgroups 49.7 (326.6) ? ? ? ? 50.1 (0.2) 70.7 (6.9)
Science 86.9 (5.7) ? ? ? ? 71.4 (?0.1) 92.8 (0.1)
Autos/Motorcycles 90.8 (0.8) 90.1 (?0.1) 97.5 (?0.1) 89.9 (?0.1) 97.6 (?0.1)
Baseball/Hockey 49.9 (0.8) 90.7 (?0.1) 96.7 (?0.1) 90.5 (?0.1) 96.9 (?0.1)
Mac/PC 50.5 (0.6) 86.7 (?0.1) 91.2 (?0.1) 86.6 (?0.1) 90.2 (?0.1)
Movie Reviews 68.8 (1.8) 68.0 (?0.1) 73.4 (0.1) 67.7 (?0.1) 72.0 (0.1)
Table 1: Accuracies and training times for different feature-label learning algorithms on benchmark corpora. Classi-
fication accuracy is reported for each model, using only the top 10 oracle-ranked features per label (and no labeled
instances) for training. The best model for each corpus is highlighted in bold. Training time (in seconds) is shown in
parentheses on the right side of each column. All results are averaged across 10 folds using cross-validation.
to us only supports labeled features (and not labeled
instances as well), we limit the MNB methods to
features for a fair comparison. To obtain feature la-
bels in this experiment, we simulate a ?feature or-
acle? as in previous work (Druck et al, 2008; At-
tenberg et al, 2010), which is essentially the query
selection algorithm from Section 2.3, but using com-
plete labeled data to compute IG(fk). We con-
servatively use only the top 10 features per class,
which is meant to resemble a handful of very salient
features that a human might brainstorm to jump-
start the learning process. We experiment with
EM1 (one-step EM) variants of both MNB/Pool
and MNB/Priors, and set ? = 50 for the latter
(see Section 3.2 for details on tuning this parame-
ter). Results are averaged over 10 folds using cross-
validation, and all experiments are conducted on a
single 2.53GHz processor machine.
Results are shown in Table 1. As expected, adding
one iteration of EM for semi-supervised training im-
proves the accuracy of both MNB methods across all
data sets. These improvements come without signif-
icant overhead in terms of time: training still rou-
tinely finishes in a fraction of a second per fold.
MNB/Pool and MNB/Priors, where they can be
compared, perform virtually the same as each other
with or without EM, in terms of accuracy and speed
alike. However, MNB/Pool is only applicable to bi-
nary classification problems. As explained in Sec-
tion 2.1, MNB/Priors is more flexible, and prefer-
able for a more general-use interactive annotation
tool like DUALIST.
The semi-supervised MNB methods are also con-
sistently more accurate than GE training?and are
about 40 times faster as well. The gains of
Priors+EM1 over MaxEnt/GE are statistically sig-
nificant in all cases but two: Autos/Motorcycles and
Movie Reviews3. MNB is superior when using any-
where from five to 20 oracle-ranked features per
class, but as the number of feature labels increases
beyond 30, GE is often more accurate (results not
shown). If we think of MaxEnt/GE as a discrim-
inative analog of MNB/Priors+EM, this is consis-
tent with what is known about labeled set size in su-
pervised learning for generative/discriminative pairs
(Ng and Jordan, 2002). However, the time complex-
ity of GE training increases sharply with each new
labeled feature, since it adds a new constraint to the
objective function whose gradient must be computed
using all the unlabeled data. In short, GE train-
ing is too slow and too inaccurate early in the ac-
tive learning process (where labels are more scarce)
to be appropriate for our scenario. Thus, we select
MNB/Priors to power the DUALIST interface.
3.2 Tuning the Parameter ?
A second question is how sensitive the accuracy of
MNB/Priors is to the parameter ?. To study this,
we ran experiments varying ? from from one to 212,
using different combinations of labeled instances
and/or features (again using the simulated oracle and
10-fold cross-validation).
3Paired 2-tailed t-test, p < 0.05, correcting for multiple tests
using the Bonferroni method.
1471
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 1  10  100  1000
10 feat10 feat, 100 inst100 feat100 feat, 100 inst
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 1  10  100  1000
10 feat10 feat, 100 inst100 feat100 feat, 100 inst
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 1  10  100  1000
10 feat10 feat, 100 inst100 feat100 feat, 100 inst
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 1  10  100  1000
10 feat10 feat, 100 inst100 feat100 feat, 100 inst
alpha alpha
Reuters WebKB
Science Movie Reviews
Figure 2: The effects of varying ? on accuracy for four
corpora, using differing amounts of training data (labeled
features and/or instances). For clarity, vertical axes are
scaled differently for each data set, and horizontal axes
are plotted on a logarithmic scale. Classifier performance
remains generally stable across data sets for ? < 100.
Figure 2 plots these results for four of the corpora.
The first thing to note is that in all cases, accuracy is
relatively stable for ? < 100, so tuning this value
seems not to be a significant concern; we chose 50
for all other experiments in this paper. A second ob-
servation is that, for all but the Reuters corpus, label-
ing 90 additional features improves accuracy much
more than labeling 100 documents. This is encour-
aging, since labeling features (e.g., words) is known
to be generally faster and easier for humans than la-
beling entire instances (e.g., documents).
For Reuters, however, the additional feature la-
bels appear harmful. The anomaly can be explained
in part by previous work with this corpus, which
found that a few expertly-chosen keywords can
outperform machine learning methods (Cohen and
Singer, 1996), or that aggressive feature selection?
i.e., using only three or four features per class?
helps tremendously (Moulinier, 1996). Corpora like
Reuters may naturally lend themselves to feature se-
lection, which is (in some sense) what happens when
labeling features. The simulated oracle here was
forced to label 100 features, some with very low
information gain (e.g., ?south? for acquisitions, or
?proven? for gold); we would not expect humans an-
notators to provide such misleading information. In-
stead, we hypothesize that in practice there may be a
limited set of features with high enough information
content for humans to feel confident labeling, after
which they switch their attention to labeling instance
queries instead. This further indicates that the user-
guided flexibility of annotation in DUALIST is an
appropriate design choice.
3.3 User Experiments
To evaluate our system in practice, we conducted
a series of user experiments. This is in contrast to
most previous work, which simulates active learning
by using known document labels and feature labels
from a simulated oracle (which can be flawed, as we
saw in the previous section). We argue that this is an
important contribution, as it gives us a better sense
of how well the approach actually works in practice.
It also allows us to analyze behavioral results, which
in turn may help inform future protocols for human
interaction in active learning.
DUALIST is implemented as a web-based appli-
cation in Java and was deployed online. We used
three different configurations: active dual (as in Fig-
ure 1, implementing everything from Section 2), ac-
tive instance (instance queries only, no features), and
a passive instance baseline (instances only, but se-
lected at random). We also began by randomly se-
lecting instances in the active configurations, until
every class has at least one labeled instance or one
labeled feature. D = 2 documents and V = 100 fea-
tures were selected for each round of active learning.
We recruited five members of our research group
to label three data sets using each configuration, in
an order of their choosing. Users were first allowed
to spend a minute or two familiarizing themselves
with DUALIST, but received no training regarding
the interface or data sets. All experiments used a
fixed 90% train, 10% test split which was consistent
across all users, and annotators were not allowed to
see the accuracy of the classifier they were train-
ing at any time. Each annotation action was times-
tamped and logged for analysis, and each experi-
ment automatically terminated after six minutes.
Figure 3 shows learning curves, in terms of accu-
racy vs. annotation time, for each trial in the user
study. The first thing to note is that the active
1472
WebKB Science Movie Reviews
annotation time (sec) annotation time (sec) annotation time (sec)
us
er
 
1
us
er
 
2
us
er
 
3
us
er
 
4
us
er
 
5
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  60  120  180  240  300  360
active dual
active inst
passive inst
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  60  120  180  240  300  360
active dual
active inst
passive inst
Figure 3: User experiments involving human annotators for text classification. Each row plots accuracy vs. time
learning curves for a particular user (under all three experimental conditions) for each of the three corpora (one
column per data set). For clarity, vertical axes are scaled differently for each corpus, but held constant across all users.
The thin dashed lines at the top of each plot represents the idealized fully-supervised accuracy. Horizontal axes show
labeling cost in terms of actual elapsed annotation time (in seconds).
1473
dual configuration yields consistently better learn-
ing curves than either active or passive learning with
instances alone, often getting within 90% of fully-
supervised accuracy (in under six minutes). The
only two exceptions make interesting (and differ-
ent) case studies. User 4 only provided four la-
beled features in the Movie Review corpus, which
partially explains the similarity in performance to
the instance-only cases. Moreover, these were
manually-added features, i.e., he never answered
any of the classifier?s feature queries, thus depriving
the learner of the information it requested. User 5,
on the other hand, never manually added features
and only answered queries. With the WebKB cor-
pus, however, he apparently found feature queries
for the course label to be easier than the other
classes, and 71% of all his feature labels came
from that class (sometimes noisily, e.g., ?instructor?
might also indicate faculty pages). This imbalance
ultimately biased the learner toward the course la-
bel, which led to classification errors. These patho-
logical cases represent potential pitfalls that could
be alleviated with additional user studies and train-
ing. However, we note that the active dual interface
is not particularly worse in these cases, it is simply
not significantly better, as in the other 13 trials.
Feature queries were less costly than instances,
which is consistent with findings in previous work
(Raghavan et al, 2006; Druck et al, 2009). The
least expensive actions in these experiments were
labeling (mean 3.2 seconds) and unlabeling (1.8s)
features, while manually adding new features took
only slightly longer (5.9s). The most expensive ac-
tions were labeling (10.8s) and ignoring (9.9s) in-
stance queries. Interestingly, we observed that the
human annotators spent most of the first three min-
utes performing feature-labeling actions ( ), and
switched to more instance-labeling activity for the
final three minutes ( ). As hypothesized in Sec-
tion 3.2, it seems that the active learner is exhausting
the most salient feature queries early on, and users
begin to focus on more interpretable instance queries
over time. However, more study (and longer annota-
tion periods) are warranted to better understand this
phenomenon, which may suggest additional user in-
terface design improvements.
We also saw surprising trends in annotation qual-
ity. In active settings, users made an average of one
instance-labeling error per trial (relative to the gold-
standard labels), but in the passive case this rose to
1.6, suggesting they are more accurate on the active
queries. However, they also explicitly ignored more
instances in the active dual condition (7.7) than ei-
ther active instance (5.9) or passive (2.5), indicating
that they find these queries more ambiguous. This
seems reasonable, since these are the instances the
classifier is least certain about. But if we look at
the time users spent on these actions, they are much
faster to label/ignore (9.7s/7.5s) in the active dual
scenario than in the active instance (10.0s/10.7s) or
passive (12.3s/15.4s) cases, which means they are
being more efficient. The differences in time be-
tween dual and passive are statistically significant4.
3.4 Additional Use Cases
Here we discuss the application of DUALIST to a
few other natural language processing tasks. This
section is not meant to show its superiority relative
to other methods, but rather to demonstrate the flex-
ibility and potential of our approach in a variety of
problems in human language technology.
3.4.1 Word Sense Disambiguation
Word Sense Disambiguation (WSD) is the prob-
lem of determining which meaning of a word is be-
ing used in a particular context (e.g., ?hard? in the
sense of a challenging task vs. a marble floor). We
asked a user to employ DUALIST for 10 minutes
for each of three benchmark WSD corpora (Moham-
mad and Pedersen, 2004): Hard (3 senses), Line
(6 senses), and Serve (4 senses). Each instance rep-
resents a sentence using the ambiguous word, and
features are lowercased unigram and bigram terms
from the surrounding context in the sentence. The
learned models? prediction accuracies (on the sen-
tences not labeled by the user) were: 83.0%, 78.4%,
and 78.7% for Hard, Line, and Serve (respectively),
which appears to be comparable to recent supervised
learning results in the WSD literature on these data
sets. However, our results were achieved in less than
10 minutes of effort each, by labeling an average of
76 sentences and 32 words or phrases per task (com-
pared to the thousands of labeled training sentences
used in previous work).
4Kolmogorov-Smirnov test, p < 0.01.
1474
3.4.2 Information Extraction
DUALIST is also well-suited to a kind of large-
scale information extraction known as semantic
class learning: given a set of semantic categories
and a very large unlabeled text corpus, learn to pop-
ulate a knowledge base with words or phrases that
belong to each class (Riloff and Jones, 1999; Carl-
son et al, 2010). For this task, we first processed
500 million English Web pages from the ClueWeb09
corpus (Callan and Hoy, 2009) by using a shallow
parser. Then we represented noun phrases (e.g., ?Al
Gore,? ?World Trade Organization,? ?upholstery?)
as instances, using a vector of their co-occurrences
with heuristic contextual patterns (e.g., ?visit to X?
or ?X?s mission?) as well as a few orthographic pat-
terns (e.g., capitalization, head nouns, affixes) as
features. We filtered out instances or contexts that
occurred fewer than 200 times in the corpus, result-
ing in 49,923 noun phrases and 87,760 features.
We then had a user annotate phrases and patterns
into five semantic classes using DUALIST: person,
location, organization, date/time, and other (the
background or null class). The user began by insert-
ing simple hyponym patterns (Hearst, 1992) for their
corresponding classes (e.g., ?people such as X? for
person, or ?organizations like X? for organization)
and proceeded from there for 20 minutes. Since
there was no gold-standard for evaluation, we ran-
domly sampled 300 predicted extractions for each
of the four non-null classes, and hired human eval-
uators using the Amazon Mechanical Turk service5
to estimate precision. Each instance was assigned
to three evaluators, using majority vote to score for
correctness.
Table 2 shows the estimated precision, total ex-
tracted instances, and the number of user-labeled
features and instances for each class. While there
is room for improvement (published results for this
kind of task are often above 80% precision), it is
worth noting that in this experiment the user did not
provide any initial ?seed examples? for each class,
which is fairly common in semantic class learning.
In practice, such additional seeding should help, as
the active learner acquired 115 labeled instances for
the null class, but fewer than a dozen for each non-
null class (in the first 20 minutes).
5http://www.mturk.com
Class Prec. # Ext. # Feat. # Inst.
person 74.7 6,478 37 6
location 76.3 5,307 47 5
organization 59.7 4,613 51 7
date/time 85.7 494 51 12
other ? 32,882 13 115
Table 2: Summary of results using DUALIST for web-
scale information extraction.
3.4.3 Twitter Filtering and Sentiment Analysis
There is growing interest in language analysis
for online social media services such as Twitter6
(Petrovic? et al, 2010; Ritter et al, 2010), which al-
lows users to broadcast short messages limited to
140 characters. Two basic but interesting tasks in
this domain are (1) language filtering and (2) sen-
timent classification, both of which are difficult be-
cause of the extreme brevity and informal use of lan-
guage in the messages.
Even though Twitter attempts to provide language
metadata for its ?tweets,? English is the default set-
ting for most users, so about 35% of English-tagged
tweets are actually in a different language. Further-
more, the length constraints encourage acronyms,
emphatic misspellings, and orthographic shortcuts
even among English-speaking users, so many tweets
in English actually contain no proper English words
(e.g., ?OMG ur sooo gr8!! #luvya?). This may
render existing lexicon-based language filters?and
possibly character n-gram filters?ineffective.
To quickly build an English-language filter for
Twitter, we sampled 150,000 tweets from the Twit-
ter Streaming API and asked an annotator spend 10
minutes with DUALIST labeling English and non-
English messages and features. Features were rep-
resented as unigrams and bigrams without any stop-
word filtering, plus a few Twitter-specific features
such as emoticons (text-based representations of fa-
cial expressions such as :) or :( used to convey feel-
ing or tone), the presence of anonymized usernames
(preceded by ?@?) or URL links, and hashtags (com-
pound words preceded by ?#? and used to label mes-
sages, e.g., ?#loveit?). Following the same method-
ology as Section 3.4.2, we evaluated 300 random
predictions using the Mechanical Turk service. The
6http://twitter.com
1475
estimated accuracy of the trained language filter was
85.2% (inter-annotator agreement among the evalu-
ators was 94.3%).
We then took the 97,813 tweets predicted to be in
English and used them as the corpus for a sentiment
classifier, which attempts to predict the mood con-
veyed by the author of a piece of text (Liu, 2010).
Using the same feature representation as the lan-
guage filter, the annotator spent 20 minutes with
DUALIST, labeling tweets and features into three
mood classes: positive, negative, and neutral. The
annotator began by labeling emoticons, by which
the active learner was able to uncover some interest-
ing domain-specific salient terms, e.g., ?cant wait?
and ?#win? for positive tweets or ?#tiredofthat? for
negative tweets. Using a 300-instance Mechanical
Turk evaluation, the estimated accuracy of the sen-
timent classifier was 65.9% (inter-annotator agree-
ment among the evaluators was 77.4%).
4 Discussion and Future Work
We have presented DUALIST, a new type of dual-
strategy annotation interface for semi-supervised ac-
tive learning. To support this dual-query interface,
we developed a novel, fast, and practical semi-
supervised learning algorithm, and demonstrated
how users can employ it to rapidly develop use-
ful natural language systems for a variety of tasks.
For several of these applications, the interactively-
trained systems are able to achieve 90% of state-
of-the-art performance after only a few minutes of
labeling effort on the part of a human annotator.
By releasing DUALIST as an open-source tool, we
hope to facilitate language annotation projects and
encourage more user experiments in active learning.
This represents one of the first studies of an ac-
tive learning system designed to compliment the
strengths of both learner and annotator. Future di-
rections along these lines include user studies of effi-
cient annotation behaviors, which in turn might lead
to new types of queries or improvements to the user
interface design. An obvious extension in the natural
language domain is to go beyond classification tasks
and query domain knowledge for structured predic-
tion in this way. Another interesting potential appli-
cation is human-driven active feature induction and
engineering, after Della Pietra et al (1997).
From a machine learning perspective, there is an
open empirical question of how useful the labels
gathered by DUALIST?s internal na??ve Bayes model
might be in later training machine learning systems
with different inductive biases (e.g., MaxEnt models
or decision trees), since the data are not IID. So far,
attempts to ?reuse? active learning data have yielded
mixed results (Lewis and Catlett, 1994; Baldridge
and Osborne, 2004). Practically speaking, DUAL-
IST is designed to run on a single machine, and
supports a few hundred thousand instances and fea-
tures at interactive speeds on modern hardware. Dis-
tributed data storage (Chang et al, 2008) and paral-
lelized learning algorithms (Chu et al, 2007) may
help scale this approach into the millions.
Finally, modifying the learning algorithm to better
cope with violated independence assumptions may
be necessary for interesting language applications
beyond those presented here. TAN-Trees (Fried-
man et al, 1997), for example, might be able to ac-
complish this while retaining speed and interactiv-
ity. Alternatively, one could imagine online stochas-
tic learning algorithms for discriminatively-trained
classifiers, which are semi-supervised and can ex-
ploit feature labels. To our knowledge, such flexi-
ble and efficient learning algorithms do not currently
exist, but they could be easily incorporated into the
DUALIST framework in the future.
Acknowledgments
Thanks to members of Carnegie Mellon?s ?Read the
Web? research project for helpful discussions and
participation in the user studies. This work is sup-
ported in part by DARPA (under contracts FA8750-
08-1-0009 and AF8750-09-C-0179), the National
Science Foundation (IIS-0968487), and Google.
References
J. Attenberg, P. Melville, and F. Provost. 2010. A uni-
fied approach to active dual supervision for labeling
features and examples. In Proceedings of the Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD). Springer.
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 9?16. ACL Press.
1476
J. Callan and M. Hoy. 2009. The clueweb09 dataset.
http://lemurproject.org/clueweb09/.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306?1313. AAAI Press.
F. Chang, J. Dean, S. Ghemawat, W.C. Hsieh, D.A. Wal-
lach, M. Burrows, T. Chandra, A. Fikes, and R.E. Gru-
ber. 2008. Bigtable: A distributed storage system for
structured data. ACM Transactions on Computer Sys-
tems, 26(2):1?26.
C.T. Chu, S.K. Kim, Y.A. Lin, Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-reduce for machine
learning on multicore. In B. Scho?lkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Information
Processing Systems, volume 19, pages 281?288. MIT
Press.
W. Cohen and Y. Singer. 1996. Context-sensitive learn-
ing methods for text categorization. In Proceedings of
the ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 307?315. ACM
Press.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the world
wide web. In Proceedings of the National Confer-
ence on Artificial Intelligence (AAAI), pages 509?516.
AAAI Press.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595?602. ACM Press.
G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81?90. ACL Press.
N. Friedman, D. Geiger, and M. Goldszmidt. 1997.
Bayesian network classifiers. Machine learning,
29(2):131?163.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. In Proceed-
ings of the Conference on Principles and Practice of
Knowledge Discovery in Databases (PKDD), pages
185?196. Springer.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the Confer-
ence on Computational Linguistics (COLING), pages
539?545. ACL.
D. Heckerman. 1995. A tutorial on learning with
bayesian networks. Technical Report MSR-TR-95-06,
Microsoft Research.
K. Lang. 1995. Newsweeder: Learning to filter net-
news. In Proceedings of the International Conference
on Machine Learning (ICML), pages 331?339. Mor-
gan Kaufmann.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 148?156. Morgan Kaufmann.
B. Liu. 2010. Sentiment analysis and subjectivity. In
N. Indurkhya and F.J. Damerau, editors, Handbook of
Natural Language Processing,. CRC Press.
A. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 359?367. Morgan
Kaufmann.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275?1284. ACM Press.
S. Mohammad and T. Pedersen. 2004. Combining lex-
ical and syntactic features for supervised word sense
disambiguation. In Hwee Tou Ng and Ellen Riloff,
editors, Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL), pages
25?32. ACL Press.
I. Moulinier. 1996. A framework for comparing text cat-
egorization approaches. In Proceedings of the AAAI
Symposium on Machine Learning in Information Ac-
cess. AAAI Press.
A.Y. Ng and M. Jordan. 2002. On discriminative vs.
generative classifiers: A comparison of logistic regres-
sion and naive bayes. In Advances in Neural Infor-
mation Processing Systems (NIPS), volume 14, pages
841?848. MIT Press.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up: Sentiment classification using machine learning
techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 79?86. ACL Press.
S. Petrovic?, M. Osborne, and V. Lavrenko. 2010.
Streaming first story detection with application to
Twitter. In Proceedings of the North American Asso-
ciation for Computational Linguistics (NAACL), pages
181?189. ACL Press.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on both features and instances.
Journal of Machine Learning Research, 7:1655?1686.
1477
J.D. Rennie, L. Shih, J. Teevan, and D. Karger. 2003.
Tackling the poor assumptions of naive bayes text clas-
sifiers. In Proceedings of the International Conference
on Machine Learning (ICML), pages 285?295. Mor-
gan Kaufmann.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the Conference on Artificial Intel-
ligence (AAAI), pages 474?479. AAAI Press.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised
modeling of Twitter conversations. In Proceedings
of the North American Association for Computational
Linguistics (NAACL), pages 172?180. ACL Press.
T. Rose, M. Stevenson, and M. Whitehead. 2002. The
Reuters corpus vol. 1 - from yesterday?s news to to-
morrow?s language resources. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC), pages 29?31.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
B. Settles. 2009. Active learning literature survey. Com-
puter Sciences Technical Report 1648, University of
Wisconsin?Madison.
1478
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 563?567,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Behavioral Factors in Interactive Training of Text Classifiers
Burr Settles
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA 15213, USA
bsettles@cs.cmu.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin
Madison WI 53715, USA
jerryzhu@cs.wisc.edu
Abstract
This paper describes a user study where hu-
mans interactively train automatic text clas-
sifiers. We attempt to replicate previous re-
sults using multiple ?average? Internet users
instead of a few domain experts as annotators.
We also analyze user annotation behaviors to
find that certain labeling actions have an im-
pact on classifier accuracy, drawing attention
to the important role these behavioral factors
play in interactive learning systems.
1 Introduction
There is growing interest in methods that incorpo-
rate human domain knowledge in machine learning
algorithms, either as priors on model parameters or
as constraints in an objective function. Such ap-
proaches lend themselves well to natural language
tasks, where input features are often discrete vari-
ables that carry semantic meaning (e.g., words). A
feature label is a simple but expressive form of do-
main knowledge that has received considerable at-
tention recently (Druck et al, 2008; Melville et al,
2009). For example, a single feature (word) can be
used to indicate a particular label or set of labels,
such as ?excellent?? positive or ?terrible?? neg-
ative, which might be useful word-label rules for a
sentiment analysis task.
Contemporary work has also focused on mak-
ing such learning algorithms active, by enabling
them to pose ?queries? in the form of feature-based
rules to be labeled by annotators in addition to ?
and sometimes lieu of ? data instances such as
documents (Attenberg et al, 2010; Druck et al,
2009). These concepts were recently implemented
in a practical system for interactive training of text
classifiers called DUALIST1. Settles (2011) reports
that, in user experiments with real annotators, hu-
mans were able to train near state of the art classi-
fiers with only a few minutes of effort. However,
there were only five subjects, who were all com-
puter science researchers. It is possible that these
positive results can be attributed to the subjects? im-
plicit familiarity with machine learning and natural
language processing algorithms.
This short paper sheds more light on previous ex-
periments by replicating them with many more hu-
man subjects, and of a different type: non-experts
recruited through the Amazon Mechanical Turk ser-
vice2. We also analyze the impact of annotator be-
havior on the resulting classifiers, and suggest rela-
tionships to recent work in curriculum learning.
2 DUALIST
Figure 1 shows a screenshot of DUALIST, an inter-
active machine learning system for quickly build-
ing text classifiers. The annotator is allowed to
take three kinds of actions: ? label query docu-
ments (instances) by clicking class-label buttons in
the left panel, ? label query words (features) by
selecting them from the class-label columns in the
right panel, or ? ?volunteer? domain knowledge by
typing labeled words into a text box at the top of
each class column. The underlying classifier is a
na??ve Bayes variant combining informative priors,
1http://code.google.com/p/dualist/
2http://mturk.com
563
12
3
Figure 1: The DUALIST user interface.
maximum likelihood estimation, and the EM algo-
rithm for fast semi-supervised training. When a
user performs action ? or ?, she labels queries that
should help minimize the classifier?s uncertainty on
unlabeled documents (according to active learning
heuristics). For action ?, the user is free to volun-
teer any relevant word, whether or not it appears in
a document or word column. For example, the user
might volunteer the labeled word ?oscar? ? posi-
tive in a sentiment analysis task for movie reviews
(leveraging her knowledge of domain), even if the
word ?oscar? does not appear anywhere in the in-
terface. This flexibility goes beyond traditional ac-
tive learning, which restricts the user to feedback on
items queried by the learner (i.e., actions ? and ?).
After a few labeling actions, the user submits her
feedback and receives the next set of queries in real
time. For more details, see Settles (2011).
3 Experimental Setup
We recruited annotators through the crowdsourcing
marketplace Mechanical Turk. Subjects were shown
a tutorial page with a brief description of the clas-
sification task, as well as a cartoon of the interface
similar to Figure 1 explaining the various annotation
options. When they decided they were ready, users
followed a link to a web server running a customized
version of DUALIST, which is an open source web-
based application. At the end of each trial, subjects
were given a confirmation code to receive payment.
We conducted experiments using two corpora
from the original DUALIST study: Science (a subset
of the 20 Newsgroups benchmark: cryptography,
electronics, medicine, and space) and Movie Re-
views (a sentiment analysis collection). These are
not specialized domains, i.e., we could expect av-
erage Internet users to be knowledgable enough to
perform the annotations. While both are generally
accessible, these corpora represent different types of
tasks and vary both in number of categories (four
vs. two) and difficulty (Movie Reviews is known to
be harder for learning algorithms). We replicated
the same experimental conditions as previous work:
DUALIST (the full interface in Figure 1), active-doc
(the left-hand ? document panel only), and passive-
doc (the ? document panel only, but with texts se-
lected at random and not queried by active learning).
For each condition, we recruited 25 users for the
Science corpus (75 total) and 35 users for Movie Re-
views (105 total). We were careful to publish tasks
on MTurk in a way that no one user annotated more
than one condition. Some users experienced techni-
cal difficulties that nullified their work, and four ap-
peared to be spammers3. After removing these sub-
jects from the analysis, we were left with 23 users
for the Science DUALIST condition, 25 each for the
two document-only conditions (73 total), 32 users
for the Movie Reviews DUALIST condition, and
33 each for the document-only conditions (98 total).
DUALIST automatically logged data about user ac-
tions and model accuracies as training progressed,
although users could not see these statistics. Trials
lasted 6 minutes for the Science corpus and 10 min-
utes for Movie Reviews. We did advertise a ?bonus?
for the user who trained the best classifier to encour-
age correctness, but otherwise offered no guidance
on how subjects should prioritize their time.
4 Results
Figure 2(a) shows learning curves aggregated across
all users in each experimental condition. Curves are
LOESS fits to classifier accuracy over time: locally-
weighted polynomial regressions (Cleveland et al,
1992) ?1 standard error, with the actual user data
points omitted for clarity. For the Science task (top),
DUALIST users trained significantly better classi-
fiers after about four minutes of annotation time.
Document-only active learning also outperformed
3A spammer was ruled to be one whose document error rate
(vs. the gold standard) was more than double the chance error,
and whose feature labels appeared to be arbitrary clicks.
564
0.20
0.30
0.40
0.50
0.60
0.70
 0  60  120  180  240  300  360
Sc
ien
ce
 
DUALIST
active-doc
passive-doc
0.49
0.52
0.55
0.58
0.61
0.64
 0  120  240  360  480  600
M
ov
ie 
Re
vie
ws
annotation time (sec)
DUALIST
active-doc
passive-doc
(a) learning curves
DUALIST active-doc passive-doc
0.
3
0.
5
0.
7
DUALIST active-doc passive-doc
0.
50
0.
60
0.
70
(b) final classifier accuracies
0.20
0.30
0.40
0.50
0.60
0.70
 0  60  120  180  240  300  360
Sc
ien
ce
 
DV++ (5)
DV+ (9)
DV- (9)
0.49
0.52
0.55
0.58
0.61
0.64
 0  120  240  360  480  600
M
ov
ie 
Re
vie
ws
annotation time (sec)
DV++ (8)
DV+ (13)
DV- (11)
(c) behavioral subgroup curves
Figure 2: (a) Learning curves plotting accuracy vs. actual annotation time for the three conditions. Curves are LOESS
fits (?1 SE) to all classifier accuracies at that point in time. (b) Box plots showing the distribution of final accuracies
under each condition. (c) Learning curves for three behavioral subgroups found in the DUALIST condition. The
DV++ group volunteered many labeled words (action ?), DV+ volunteered some, and DV- volunteered none.
standard passive learning, which is consistent with
previous work. However, for Movie Reviews (bot-
tom), there is little difference among the three set-
tings, and in fact models trained with DUALIST ap-
pear to lag behind active learning with documents.
Figure 2(b) shows the distribution of final classi-
fier accuracies in each condition. For Science, the
DUALIST users are significantly better than either
of the baselines (two-sided KS test, p < 0.005).
While the differences in DUALIST accuracies are
not significantly different, we can see that the top
quartile does much better than the two baselines.
Clearly some DUALIST users are making better use
of the interface and training better classifiers. How?
It is important to note that users in the active-
doc and passive-doc conditions can only choose ac-
tion ? (label documents), whereas those in the DU-
ALIST condition must allocate their time among
three kinds of actions. It turns out that the anno-
tators exhibited very non-uniform behavior in this
respect. In particular, activity of action ? (volunteer
labeled words) follows a power law, and many sub-
jects volunteered no features at all. By inspecting
the distribution of these actions for natural break-
points, we identified three subgroups of DUALIST
users: DV++ (many volunteered words), DV+ (some
words), and DV- (none; labeled queries only). Note
Movie Reviews Science
Group # Words Users # Words Users
DV++ 21?62 8 24?42 5
DV+ 1?15 13 2?19 9
DV- 0 11 0 9
Table 1: The range of volunteered words and number of
users in each behavioral subgroup of DUALIST subjects.
that DV- is not functionally equivalent to the active-
doc condition, as users in the DV- group could still
view and label word queries. The three behavioral
subgroups are summarized in Table 1.
Figure 2(c) shows learning curves for these three
groups. We can see that the DV++ and DV+ groups
ultimately train better classifiers than the DV- group,
and DV++ also dominates both the active and pas-
sive baselines from Figure 2(a). The DV++ group is
particularly effective on the Movie Reviews corpus.
This suggests that a user?s choice to volunteer more
labeled features ? by occasionally side-stepping the
queries posed by the active learner and directly in-
jecting their domain knowledge ? is a good predic-
tor of classifier accuracy on this task.
To tease apart the relative impact of other behav-
iors, we conducted an ordinary least-squares regres-
sion to predict classifier accuracy at the end of a trial.
We included the number of user events for each ac-
565
tion as independent variables, plus two controls: the
subject?s document error rate in [0,1] with respect to
the gold standard, and class entropy in [0, logC] of
all labeled words (whereC is the number of classes).
The entropy variable is meant to capture how ?bal-
anced? a user?s word-labeling activity was for ac-
tions? and?, with the intuition that a skewed set of
words could confuse the learner, by biasing it away
from categories with fewer labeled words.
Table 2 summarizes these results. Surprisingly,
query-labeling actions (? and ?) have a relatively
small impact on accuracy. The number of volun-
teered words and entropy among word labels appear
to be the only two factors that are somewhat signif-
icant: the former is strongest in the Movie Reviews
corpus, the latter in Science4. Interestingly, there is a
strong positive correlation between these two factors
in the Movie Reviews corpus (Spearman?s ? = 0.51,
p = 0.02) but not in Science (? = 0.03). When we
consider change in word label entropy over time, the
Science DA++ group is balanced early on and be-
comes steadily more so on average , whereas
DA+ goes for several minutes before catching up
(and briefly overtaking) . This may account
for DA+?s early dip in accuracy in Figure 2(c). For
Movie Reviews, DA++ is more balanced than DA+
throughout the trial. DA++ labeled many words that
were also class-balanced, which may explain why
it is the best consistently-performing group. As is
common in behavior modeling with small samples,
the data are noisy and the regressions in Table 2 only
explain 33%?46% of the variance in accuracy.
5 Discussion
We were able to partially replicate the results from
Settles (2011). That is, for two of the same data sets,
some of the subjects using DUALIST significantly
outperformed those using traditional document-only
interfaces. However, our results show that the
gains come not merely from the interface itself, but
from which labeling actions the users chose to per-
form. As interactive learning systems continue to
expand the palette of interactive options (e.g., la-
4Science has four labels and a larger entropy range, which
might explain the importance of the entropy factor here. Also,
labels are more related to natural clusterings in this corpus
(Nigam et al, 2000), so class-balanced priors might be key for
DUALIST?s semi-supervised EM procedure to work well.
Movie Reviews Science
Action ? SE ? SE
(intercept) 0.505 0.038 *** 0.473 0.147 **
? label query docs 0.001 0.001 0.005 0.005
? label query words -0.001 0.001 0.000 0.001
? volunteer words 0.002 0.001 * 0.000 0.002
human error rate -0.036 0.109 -0.328 0.230
word label entropy 0.053 0.051 0.201 0.102 .
R2 = 0.4608 ** R2 = 0.3342
*** p < 0.001 ** p < 0.01 * p < 0.05 . p < 0.1
Table 2: Linear regressions estimating the accuracy of a
classifier as a function of annotator actions and behaviors.
beling and/or volunteering features), understanding
how these options impact learning becomes more
important. In particular, training a good classifier
in our experiments appears to be linked to (1) vol-
unteering more labeled words, and (2) maintaining
a class balance among them. Users who exhibited
both of these behaviors ? which are possibly arti-
facts of their good intuitions ? performed the best.
We posit that there is a conceptual connection be-
tween these insights and curriculum learning (Ben-
gio et al, 2009), the commonsense notion that learn-
ers perform better if they begin with clear and unam-
biguous examples before graduating to more com-
plex training data. A recent study found that some
humans use a curriculum strategy when teaching a
1D classification task to a robot (Khan et al, 2012).
About half of those subjects alternated between ex-
treme positive and negative instances in a relatively
class-balanced way. This behavior was explained by
showing that it is optimal under an assumption that,
in reality, the learning task has many input features
for which only one is relevant to the task.
Text classification exhibits similar properties:
there are many features (words), of which only a few
are relevant. We argue that labeling features can be
seen as a kind of training by curriculum. By volun-
teering labeled words in a class-balanced way (espe-
cially early on), a user provides clear, unambiguous
training signals that effectively perform feature se-
lection while biasing the classifier toward the user?s
hypothesis. Future research on mixed-initiative user
interfaces might try to detect and encourage these
kinds of annotator behaviors, and potentially im-
prove interactive machine learning outcomes.
566
Acknowledgments
This work was funded in part by DARPA, the
National Science Foundation (under grants IIS-
0953219 and IIS-0968487), and Google.
References
J. Attenberg, P. Melville, and F. Provost. 2010. A uni-
fied approach to active dual supervision for labeling
features and examples. In Proceedings of the Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 40?55. Springer.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 119?126. Omnipress.
W.S. Cleveland, E. Grosse, and W.M. Shyu. 1992. Lo-
cal regression models. In J.M. Chambers and T.J.
Hastie, editors, Statistical Models in S. Wadsworth &
Brooks/Cole.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595?602. ACM Press.
G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81?90. ACL Press.
F. Khan, X. Zhu, and B. Mutlu. 2012. How do humans
teach: On curriculum learning and teaching dimen-
sion. In Advances in Neural Information Processing
Systems (NIPS), volume 24, pages 1449?1457. Mor-
gan Kaufmann.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275?1284. ACM Press.
K. Nigam, A.K. Mccallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using em. Machine Learning, 39:103?134.
B. Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1467?1478. ACL Press.
567
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 49?57,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Computational Creativity Tools for Songwriters
Burr Settles
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213 USA
bsettles@cs.cmu.edu
Abstract
This paper describes two natural language
processing systems designed to assist song-
writers in obtaining and developing ideas for
their craft. Titular is a text synthesis algo-
rithm for automatically generating novel song
titles, which lyricists can use to back-form
concepts and narrative story arcs. LyriCloud
is a word-level language ?browser? or ?ex-
plorer,? which allows users to interactively se-
lect words and receive lyrical suggestions in
return. Two criteria for creativity tools are
also presented along with examples of how
they guided the development of these systems,
which were used by musicians during an inter-
national songwriting contest.
1 Introduction
Writing lyrics for popular music is challenging.
Even apart from musical considerations, well-
crafted lyrics should succinctly tell a story, express
emotion, or create an image in the listener?s mind
with vivid and original language. Finding the right
words, from an evocative title to a refrain, hook, or
narrative detail, is often difficult even for full-time
professional songwriters.
This paper considers the task of creating ?intel-
ligent? interactive lyric-writing tools for musicians.
As a preliminary case study, I discuss two systems
that (1) suggest novel song titles and (2) find inter-
esting words given a seed word as input, and de-
ployed them online for participants in an interna-
tional songwriting event called FAWM1 (February
1http://fawm.org
Album Writing Month). The goal of this event?
which attracts both professional and amateur musi-
cians worldwide?is to compose 14 new works of
music (roughly an album?s worth) during the month
of February. Working at a rate of one new song ev-
ery two days on average is taxing, described by some
participants as a ?musical marathon.? To maintain
pace, songwriters are constantly looking for new
ideas, which makes them good candidates for us-
ing computational creativity tools. Typical lyric-
writing tools consist of rhyme or phrase dictionaries
which are searchable but otherwise static, passive re-
sources. By contrast, we wish to develop advanced
software which uses learned linguistic knowledge to
actively help stimulate creative thought.
To formalize the task of developing computa-
tional creativity tools, let us first define creativity
as ?the ability to extrapolate beyond existing ideas,
rules, patterns, interpretations, etc., and to generate
meaningful new ones.? By this working definition,
which is similar to Zhu et al (2009), tools that assist
humans in creative endeavors should:
1. Suggest instances unlike the majority that ex-
ist. If one were to model instances statistically,
system proposals should be ?outliers.?
2. Suggest instances that are meaningful. Purely
random proposals might be outliers, but they
are not likely to be interesting or useful.
Previous approaches to linguistic lyric-modeling
have generally not focused on creativity, but rather
on quantifying ?hit potential? (Yang et al, 2007),
which is arguably the opposite, or classifying mu-
sical genre (Li and Ogihara, 2004; Neumayer and
49
Rauber, 2007). There has been some work on auto-
matically generating percussive lyrics to accompany
a given piece of musical input (Oliveira et al, 2007;
Ramakrishnan et al, 2009), and there exists a rich
body of related work on natural language generation
for fiction (Montfort, 2006; Solis et al, 2009), po-
etry (Gerva?s, 2001; Manurung, 2004; Netzer et al,
2009), and even jokes (Binstead, 1996). However,
the goal of these systems is to be an ?artificial artist?
which can create complete works of language au-
tonomously, rather than interactive tools for assist-
ing humans in their creative process.
A few computational lyric-writing tools have
been developed outside of academia, such as Ver-
basizer, which was famously co-created by rock star
David Bowie to help him brainstorm ideas (Thomp-
son, 2007). These types of systems take a small
amount of seed text as input, such as a newspa-
per article, and generate novel phrases by iterating
through random word permutations. However, these
approaches fail the second criterion for creativity
tools, since the majority of output is not meaning-
ful. Many other so-called lyric generators exist on
the Internet2, but these are by and large ?Mad-Lib?
style fill-in-the-blanks meant for amusement rather
than more serious artistic exploration.
The contribution of this work is to develop and
study methods that satisfy both criteria for compu-
tational creativity tools?that their suggestions are
both unlikely and meaningful?and to demonstrate
that these methods are useful to humans in prac-
tice. To do this, I employ statistical natural language
models induced from a large corpus of song lyrics to
produce real-time interactive programs for exploring
songwriting ideas. The rest of this paper is organized
as follows. Section 2 describes the overall approach
and implementation details for these tools. Section 3
presents some empirical and anecdotal system eval-
uations. Section 4 summarizes my findings, dis-
cusses limitations of the current tools, and proposes
future directions for work in this vein.
2 Methodology
Davis (1992) describes the discipline of songwriting
as a multi-step process, beginning with activation
and association. Activation, according to Davis, in-
2http://www.song-lyrics-generator.org.uk
volves becoming hyper-observant, embracing spon-
taneous ideas, and then choosing a title, theme, or
progression around which to build a song. Associa-
tion is the act of expanding on that initial theme or
idea as much as possible. Subsequent steps are to
develop, organize, and winnow down the ideas gen-
erated during the first two stages.
Following this philosophy, I decided to design
two natural language processing systems aimed at
stimulating songwriters during the early stages of
the process, while adhering to the criteria for cre-
ativity tools defined in Section 1. I call these tools
Titular, which suggests novel song titles as a starting
point, and LyriCloud, which expands lyrical ideas by
suggesting related words in the form of a ?cloud.?
To encourage songwriters to actually adopt these
tools, they were deployed online as part of the of-
ficial FAWM website for participants to use. This
posed some development constraints, namely that
the user interface be implemented in the PHP lan-
guage and run in a shared web-hosting environment.
Because of this setup, complex inference methods
based on a large number of statistics had to be
avoided in order to maintain speed and interactiv-
ity. Thus, I was limited to approaches where suffi-
cient statistics could be pre-computed and stored in
a database to be accessed quickly as needed.
To induce linguistic models for Titular and Lyri-
Cloud, existing song data were needed for training.
For this, I used a ?screen scraper? to extract song
title, artist, and lyric fields from ubiquitous online
lyrics websites. In this way, I collected a corpus of
137,787 songs by 15,940 unique artists. The collec-
tion spans multiple genres (e.g., pop/rock, hip-hop,
R&B, punk, folk, blues, gospel, showtunes), and has
good coverage of mainstream charting artists (e.g.,
Beyonce?, R.E.M., Van Halen) as well as more ob-
scure independent performers (e.g., Over the Rhine,
Dismemberment Plan). An estimated 85% of the
songs are primarily in English.
2.1 Titular
Figure 1 shows example output from Titular, which
presents the user with five suggested song titles at a
time. Suggestions often combine visceral and con-
tradictory images, like ?Bleeding in the Laughter,?
while others lend themselves to more metaphori-
cal interpretation, such as ?Heads of Trick? (which
50
Figure 1: A screenshot of the Titular user interface.
evokes some sort of executive committee for deceit).
Occasionally the output is syntactically valid, but
a little too awkward to interpret sensibly, such as
?Starting in the Sisters.?
To accomplish this, I adopted a template-based
approach (Deemter et al, 2005) to title synthesis. In-
stead of using hand-crafted templates, however, Tit-
ular learns its own using the following method:
1. Source titles in the training corpus are tok-
enized and tagged for part-of-speech (POS). In-
put is first lowercased to discourage the tagger
from labeling everything NNP (proper noun).
2. The open word classes (i.e., various forms of
adjectives, adverbs, nouns, and verbs) are sub-
stituted with their assigned POS tag, while
closed words classes (i.e., conjunctions, de-
terminers, prepositions, pronouns and punctu-
ation) remain intact. Titular also remembers
which words were substituted with which tag,
in order to use them in generating new titles in
the future. Vulgar and offensive words are fil-
tered out using regular expressions.
3. The induced templates and words are culled if
they occur in the corpus below a certain fre-
quency. This step helps to counterbalance tag-
ging errors, which is important because song ti-
tles are often too short to infer POS information
accurately. Thresholds are set to 3 for templates
and 2 for POS-word pairs.
Table 1 shows several example templates induced
using this method. To generate new titles, Titular
Learned Template Freq. Example Source Title
JJ NN 3531 Irish Rover
VB 783 Breathe
VBN NN 351 Born Yesterday
NN and NN 205 Gin And Juice
NNP POS NN 167 Tom?s Diner
FW NN 65 Voodoo Woman
VB and VB 57 Seek And Destroy
CD JJ NNS 49 99 Red Balloons
JJ , JJ NN 14 Bad, Bad Girl
you ?re so JJ 8 You?re So Vain
NN ( the NN ) 7 Silver (The Hunger)
VBG with a NN 4 Walking With A Zombie
Table 1: Example title templates induced by Titular.
first randomly selects a template in proportion to
its empirical frequency, and likewise selects words
to replace each POS tag in the template (drawn
from the set of words for the corresponding tag).
This model can be thought of as a simple stochastic
context-free grammar (Lari and Young, 1990) with
only a small set of production rules. Specifically, the
root nonterminal S goes to complete templates, e.g.,
S ? VBG , VBG | UH , JJ NN ! | . . . ,
and POS nonterminals go to words that have been
tagged accordingly in the corpus, e.g.,
VBG? waiting | catching | going | . . . ,
JJ? good | little | sacrificial | . . . ,
NN? time | life | dream | . . . ,
UH? hey | oh | yeah | . . . ,
all with corresponding probabilities based on fre-
quency. Using these production rules, Titular can
generate novel titles like ?Going, Waiting? or ?Oh,
Little Life!? The system learned 2,907 template pro-
duction rules and 11,247 word production rules from
the lyrics corpus, which were stored with frequency
information in a MySQL database for deployment.
Post-processing heuristics are implemented in the
user interface to strip white-spaces from punctuation
and contractions to improve readability. Output re-
mains lowercased as an aesthetic decision.
An alternative approach to title generation is
an n-gram model based on Markov chains, which
are common for both natural language generation
(Jurafsky and Martin, 2008) and music synthesis
(Farbood and Schoner, 2001). For titles, each
word wi is generated with conditional probability
51
P (wi|wi?n, . . . , wi?1) based on the n words gener-
ated previously, using statistics gathered from titles
in the lyrics corpus. However, this approach tends to
simply recreate titles in the training data. In a pre-
liminary study using n = {1, 2, 3, 4} and 200 titles
each, the proportion of suggestions that were verba-
tim recreations of existing titles ranged from 35%
to 97%: . When n-gram titles do manage to be
novel, they tend to be long and unintelligible, such
as ?Too Many A Woman First the Dark Clouds Will
It Up? or ?Born of Care 4 My Paradise.? These two
extremes satisfy one or the other, but not both of the
criteria for creativity tools. By contrast, none of the
200 template-generated titles existed in the source
database, and they tend to be more intelligible as
well (see results in Section 3.1).
The template grammar offers several other advan-
tages over the n-gram approach, including fewer
inference steps (which results in fewer database
queries) and helping to ensure that titles are well-
formed with respect to longer-range syntactic depen-
dencies (like matching parentheses). Constraining
words by part-of-speech rather than immediate con-
text also allows the system to create novel and unex-
pected word juxtapositions, which satisfies the first
criterion for creativity tools, while remaining rela-
tively coherent and meaningful, which helps satisfy
the second criterion.
2.2 LyriCloud
Figure 2 shows example output from LyriCloud,
which takes a seed (in this case, the word ?dream?
highlighted near the center) and suggests up to 25
related words arranged visually in a cloud (Bateman
et al, 2008). The size of each word is intended to
communicate its specificity to the cloud.
Notice that LyriCloud?s notion of related words
can be quite loose. Some suggestions are modi-
fiers of the seed, like ?broken dream? and ?deep
dream,? although these invoke different senses of
dream (metaphorical vs. literal). Some suggestions
are part of an idiom involving the seed, such as ?ful-
fill a dream,? while others are synonyms/antonyms,
or specializations/generalizations, such as ?night-
mare.? Still other words might be useable as rhymes
for the seed, like ?smithereen? and even ?thing?
(loosely). The goal is to help the user see the seed
word in a variety of senses and perspectives. Users
Figure 2: A screenshot of the LyriCloud user interface.
may also interact with the system by clicking with
a pointer device on words in the cloud that they find
interesting, and be presented with a new cloud based
on the new seed. In this way, LyriCloud is a sort of
?language browser? for lyrical ideas.
I liken the problem of generating interesting word
clouds to an information retrieval task: given a seed
word s, return a set of words that are related to s,
with the constraint that they not be overly general
terms. To do this, the corpus was pre-processed by
filtering out common stop-words and words that oc-
cur in only one song, or fewer than five times in
the entire corpus (this catches most typos and mis-
spellings). As with Titular, vulgar and offensive
words are filtered using regular expressions.
For a potential seed word s, we can compute a
similarity score with every other word w in the cor-
pus vocabulary using the following measure:
sim(s, w) =
(
1 + log c(s, w)
)
? log
N
u(w)
,
where c(s, w) is the number of times s and w co-
occur in the same line of a lyric in the corpus, u(w)
is the number of unique words with which w oc-
curs in any line of the corpus, and N is the size of
the overall vocabulary. This is essentially the well-
known log-tempered tf?idf measure from the infor-
mation retrieval literature, if we treat each seed s as
a ?document? by concatenating all the lines of text
in which s appears.
I also experimented with the co-occurence fre-
quency c(s, w) and point-wise mutual information
(Church and Hanks, 1989) as similarity functions.
52
The former still used overly common words (e.g.,
?love,? ?heart,? ?baby?), which fails the first crite-
rion for creativity tools, and the latter yielded overly
seed-specific results (and often typos not filtered by
the pre-processing step), which can fail the second
criterion. The log-tempered tf?idf metric provided a
reasonable balance between the two extremes.
To generate a new cloud from a given seed, up to
25 words are randomly sampled from the top 300
ranked by similarity, which are pre-computed and
stored in the database for efficient lookup. The sam-
pled words are then sorted alphabetically for display
and scaled using a polynomial equation:
size(w) = f0 + f1 ?
(
1?
rank(w)
K
)4
,
where f0 and f1 are constants that bound the min-
imum and maximum font size, and K is the num-
ber of words in the cloud (usually 25, unless there
were unusually few words co-occuring with s). This
choice of scaling equation is ad-hoc, but produces
aesthetically pleasing results.
As a point of comparison, the original approach
for LyriCloud was to employ a topic model such as
Latent Dirichlet Allocation (Blei et al, 2003). This
is an unsupervised machine learning algorithm that
attempts to automatically organize words according
to empirical regularities in how they are used in data.
Table 2 shows 15 example ?topics? induced on the
lyrics corpus. Intuitively, these models treat doc-
uments (song lyrics) as a probabilistic mixture of
topics, which in turn are probabilistic mixtures of
words. For example, a religious hymn that employs
nature imagery might be a mixture of topics #11 and
#13 with high probability, and low probability for
other topics (see Blei et al for more details).
To generate clouds, I trained a model of 200 top-
ics on the lyrics corpus, and let the system choose
the most probable topic for a user-provided seed. It
then randomly sampled 25 of the 300 most proba-
ble words for the corresponding topic, which were
scaled in proportion to topic probability. The intu-
ition was that clouds based on the top-ranked words
for a topic should be semantically coherent, but sam-
pling at random from a large window would also al-
low for more interesting and unusual words.
In a small pilot launch of the system, songwrit-
ers rejected the topic-based version of LyriCloud for
# Most Probable Words By Topic
1 love baby give true sweet song girl ...
2 la big black white hair wear hot ...
3 hold hand hands head free put back ...
4 love find nothing something everything wrong give ...
5 yeah baby hey man girl rock ooh ...
6 feel make eyes gonna cry makes good ...
7 fall turn light face sun again world ...
8 night day cold long sleep dream days ...
9 mind world lose nothing left gonna shake ...
10 time life long live day end die ...
11 god lord man hell king heaven jesus ...
12 hear play call people good talk heard ...
13 sky eyes fire fly blue high sea ...
14 heart inside pain soul break broken deep ...
15 go back home again time gonna coming ...
Table 2: Example words from a topic model induced by
Latent Dirichlet Allocation on the lyrics corpus.
two reasons. First, the top-ranked words within a
topic tend to be high frequency words in general
(consider Table 2). These were deemed by users to
be unoriginal and in violation the first criterion for
creativity tools. Tweaking various system param-
eters, such as the number of topics in the model,
did not seem to improve performance in this re-
spect. Second, while words were thought to be co-
herent overall, users had trouble seeing the connec-
tion between the chosen topic and the highlighted
seeds themselves (see also the results in Section 3.1).
Instead, users wanted to receive interesting sug-
gestions that were more specifically tailored to the
seeds they chose, which motivated the information-
retrieval view of the problem. While this is plau-
sible using topic models with a more sophisticated
sampling scheme, it was beyond the capabilities of a
simple PHP/MySQL deployment setup.
3 Results
This section discusses some results and observations
about Titular and LyriCloud.
3.1 Empirical Evaluation
I conducted an empirical study of these systems us-
ing Amazon Mechanical Turk3, which is being used
increasingly to evaluate several systems on open-
ended tasks for which gold-standard evaluation data
does not exist (Mintz et al, 2009; Carlson et al,
3http://www.mturk.com
53
template bigram real title
1
2
3
4
5
inspiration (Titular)
tfidf topic random
1
2
3
4
5
inspiration (LyriCloud)
tfidf topic random
1
2
3
4
5
relatedness (LyriCloud)
m
ea
n 
ev
alu
at
or
 ra
tin
g
Figure 3: Box plots illustrating the distribution of ?inspiration? and ?relatedness? ratings assigned by Mechanical Turk
evaluators to Titular and LyriCloud output. Boxes represent the middle 50% of ratings, with the medians indicated by
thick black lines. Whiskers on either side span the first and last quartiles of each distribution; circles indicate possible
outliers. The methods actually deployed for use online are are highlighted on the left of each plot.
Method Good Title? Grammatical? Offensive?
template 68% 67% 6%
bigram 62% 52% 7%
real title 93% 88% 9%
Table 3: Titular results from Mechanical Turk evaluators.
2010). This was done because (1) we would like
some quantification of how well these systems per-
form, and (2) using Mechanical Turk workers should
provide more stringent and objective feedback than
the songwriters for whom the tools were developed.
For each tool, I generated 200 instances (i.e., ti-
tles for Titular and word clouds for LyriCloud) and
had evaluators fill out a simple form for each, com-
posed of yes/no questions and ratings on a five-star
scale. Each instance was given to three unique eval-
uators in order to break ties in the event of disagree-
ment, and to smooth out variance in the ratings. Tit-
ular is compared against titles generated by a bi-
gram Markov chain model (n = 1), as well as ac-
tual song titles randomly selected from the lyrics
database. LyriCloud is compared against the topic
model method, and a baseline where both words and
their sizes are generated at random.
Table 3 summarizes Titular results for the yes/no
questions in the Mechanical Turk evaluation. At
least two of three evaluators agreed that 68% of
the suggested template-based phrases would make
good song titles, which is higher than the bigram
method, but lower than the 93% rate for real song
titles (as expected). Similarly, template-based sug-
gestions were deemed more grammatical than the
bigram approach and less grammatical than actual
songs. Somewhat surprisingly, 12% of all titles in
the evaluation were judged to be good titles despite
grammatical errors, including ?Dandelion Cheatin?
and ?Dressed in Fast.? This is an interesting ob-
servation, indicating that people sometimes enjoy
titles which they find syntactically awkward (per-
haps even because they are awkward). All methods
yielded a few potentially offensive titles, which were
mostly due to violent or sexual interpretations.
Evaluators were also asked to rate titles on a scale
from boring (1) to inspiring (5), the results of which
are shown on the left of Figure 3. The template ap-
proach does fairly well in this respect: half its sug-
gestions are rated 3 or higher, which is better than
the bigram method but slightly worse than real titles.
Interestingly, distributional differences between the
ratings for template-based output and actual song ti-
tles are not statistically significant, while all other
differences are significant4. This suggests that Titu-
lar?s titles are nearly as good as real song titles, from
an inspirational perspective. Unlike other methods,
no single template-based title received a unanimous
rating of 5 from all three evaluators (although ?The
Lungs Are Pumping? and ?Sure Spider Vengeance?
both received average scores of 4.7).
For LyriCloud, evaluators were asked to rate both
the level of inspiration and whether or not they
found the words in a cloud to be unrelated (1) or
4Kolmogorov-Smirnov tests with p < 0.05. Multiple tests
in this paper are corrected for using the Bonferroni method.
54
related (5). These results are shown in the center
and right plots of Figure 3. In both measures, the
tf?idf approach outperforms the topic model, which
in turn outperforms a random collection of words.
All differences are statistically significant. Interest-
ingly, the R2 coefficient between these two mea-
sures is only 0.25 for all word clouds in the evalu-
ation, meaning that relatedness ratings only explain
25% of the variance in inspiration ratings (and vice
versa). This curious result implies that there are
other, more subtle factors at play in how ?inspiring?
humans find a set of words like this to be. Addi-
tionally, only 36% of evaluators reported that they
could find meaning in the size/scale of words in tf?idf
clouds (compared to 9% for the topic model and 1%
for random), which is lower than anticipated.
3.2 Anecdotal Results
The implementations described in this paper were
developed over a four-day period, and made avail-
able to FAWM participants mid-way through the
songwriting challenge on February 16, 2010. They
were promoted as part of a suite of online tools
called The Muse5, which also includes two other
programs, Struxxure and Plot Spline, aimed at help-
ing songwriters consider various novel song struc-
tures and plot constraints. These other tools do
not use any language modeling, and a discussion of
them is beyond the scope of this paper.
Table 4 summarizes the internet traffic that The
Muse received between its launch and the end of the
FAWM challenge on March 1, 2010 (thirteen days).
Encouragingly, Titular and LyriCloud were the most
popular destinations on the website. These statis-
tics include visitors from 36 countries, mostly from
the United States (55%), Germany (15%) and the
United Kingdom (9%). News of these tools spread
well beyond the original FAWM community, as 29%
of website visitors were referred via hyperlinks from
unaffiliated music forums, songwriting blogs, and
social websites like Facebook and Twitter.
FAWM participants typically post their songs on-
line after completion to track their progress, so they
were asked to ?tag? the songs they wrote with help
from these creativity tools as a way to measure how
much the tools were being used in practice. By the
5http://muse.fawm.org
Tool Pageviews % Traffic
Titular 11,408 42.9%
LyriCloud 5,313 20.0%
Struxxure 4,371 16.5%
Plot Spline 3,219 12.1%
Home Page 2,248 8.5%
Total 26,559 100.0%
Table 4: Website activity for The Muse tools between
February 16 and March 1, 2010. The creativity tools dis-
cussed in this paper are italicized.
end of the challenge, 66 songs were tagged ?titular?
and 29 songs were tagged ?lyricloud.? Note that
these figures are conservative lower-bounds for ac-
tual usage, since not all participants tag their songs
and, as previously stated, a significant portion of the
internet traffic for these tools came from outside the
FAWM community.
The tools were published without detailed instruc-
tions, so songwriters were free to interpret them
however they saw fit. Several users found inspiration
in Titular titles that are interpretable as metaphor or
synecdoche. For example, Expendable Friend (the
stage name of British singer/songwriter Jacqui Car-
nall) wrote a song around the suggestion ?I Am Your
Adult,? which she interpreted this way:
So, this is a song about the little voice inside
you that stops you from doing fun things be-
cause you?re not a child any more and you
can?t really get away with doing them.
Surprisingly, even non-lyricists adopted Titular.
?Mexican of No Breakup? led guitarist Ryan Day
to compose a latin-style instrumental, and ?What a
Sunset!? became an ambient electronic piece by an
artist by the pseudonym of Vom Vorton.
While LyriCloud was about half as popular as
Titular, it was arguably open to a wider range of
interpretation. Some songwriters used it as origi-
nally envisioned, i.e., a ?browser? for interactively
exploring lyrical possibilities. New York lawyer and
folksinger Mike Skliar wrote two songs this way. He
said this about the process:
I had a few images in mind, and I would type
in the key word of the image, which generated
other words sometimes slightly related... then
kind of let my mind free associate on what
those words might mean juxtaposed against
55
the first image, and came up with about half
the song that way.
Unexpectedly, some took LyriCloud as a challenge
to write a song using all the words from a single
cloud (or as many as possible), since it chooses a
seed word at random if none is provided as input.
Songwriter James Currey, who actually used Lyri-
Cloud and Titular together to write ?For the Bethle-
hem of Manhattan,? described the process this way:
It was like doing a puzzle, and the result is ac-
tually quite surprisingly coherent AND good.
As a final anecdote, middle school English
teacher Keith Schumacher of California was a
FAWM 2010 participant. He shared these tools with
faculty members at his school, who designed an in-
class creative writing exercise for students involving
words generated by LyriCloud, projected overhead
in the classroom. This demonstrates the utility of
these and similar tools to a broad range of age groups
and writing styles.
4 Conclusions and Future Work
In this paper, I introduced the task of designing com-
putational creativity tools for songwriters. I de-
scribed two such tools, Titular and LyriCloud, and
presented an empirical evaluation as well as anecdo-
tal results based on actual usage during an interna-
tional songwriting event. I also presented two cri-
teria for creativity tools?that their suggestions be
both unlikely and meaningful to the human artists
interacting with them?and showed how these prin-
ciples guided the development of the tools.
This preliminary foray into creativity tools
based on language modeling shows the potential
for creativity-oriented human-computer interaction.
However, there is still much that can be improved
on. For example, Titular might benefit from train-
ing with a more traditional context-free grammar
(i.e., one with recursive production rules), which
might yield more complex and interesting possibili-
ties. LyriCloud could be extended to include bigram
and trigram phrases in addition to single words.
Also, the vocabularies of both systems might cur-
rently suffer due to the limited and domain-specific
training data (the lyrics corpus), which could be sup-
plemented with other sources.
Perhaps more importantly, neither of the current
tools incorporate any explicit notion of wordplay
(e.g., rhyme, alliteration, assonance, puns) or any
other device of creative writing (meter, repetition,
irony, etc.). The systems occasionally do suggest in-
stances that embody these properties, but they are
wholly by chance at this stage. One can, however,
imagine future versions that take preference param-
eters as input from the user, and try to suggest in-
stances based on these constraints.
Acknowledgments
Thanks to Jacob Eisenstein for helpful discussions
during the development of these tools, and to the
anonymous reviewers for valuable suggestions that
much improved the paper. To all the ?fawmers? who
inspired the project, used the tools, and provided
feedback?particularly those who gave permission
to be discussed in Section 3.2?you rock.
References
S. Bateman, C. Gutwin, and M. Nacenta. 2008. Seeing
things in the clouds: the effect of visual features on tag
cloud selections. In Proceedings of the ACM Confer-
ence on Hypertext and Hypermedia, pages 193?202.
ACM.
K. Binstead. 1996. Machine humour: An implemented
model of puns. Ph.D. thesis, University of Edinburgh.
D.M. Blei, A.Y. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
A. Carlson, J. Betteridge, R. Wang, E.R. Hruschka Jr, and
T. Mitchell. 2010. Coupled semi-supervised learning
for information extraction. In Proceedings of the In-
ternational Conference on Web Search and Data Min-
ing (WSDM), pages 101?110. ACM Press.
K. Church and P. Hanks. 1989. Word associate norms,
mutual information and lexicography. In Proceed-
ings of the Association for Computational Linguistics
(ACL), pages 76?83. ACL Press.
S. Davis. 1992. The Songwriters Idea Book. Writer?s
Digest Books.
K. Deemter, M. Theune, and E. Krahmer. 2005. Real
versus template-based natural language generation:
a false opposition? Computational Linguistics,
31(1):15?24.
M. Farbood and B. Schoner. 2001. Analysis and syn-
thesis of Palestrina-style counterpoint using Markov
chains. In Proceedings of the International Computer
56
Music Conference, pages 471?474. International Com-
puter Music Association.
P. Gerva?s. 2001. An expert system for the composition of
formal spanish poetry. Journal of Knowledge-Based
Systems, 14:181?188.
D. Jurafsky and J.H. Martin. 2008. Speech and Lan-
guage Processing. Prentice Hall.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
T. Li and M. Ogihara. 2004. Music artist style identi-
fication by semi-supervised learning from both lyrics
and content. In Proceedings of the ACM International
Conference on Multimedia, pages 364?367. ACM.
H. Manurung. 2004. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edinburgh.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without la-
beled data. In Proceedings of the Association for Com-
putational Linguistics (ACL), pages 1003?1011. ACL
Press.
N. Montfort. 2006. Natural language generation and nar-
rative variation in interactive fiction. In Proceedings of
the AAAI Workshop on Computational Aesthetics.
Y. Netzer, D. Gabay, Y. Goldberg, and M. Elhadad. 2009.
Gaiku : Generating haiku with word associations
norms. In Proceedings of the Workshop on Compu-
tational Approaches to Linguistic Creativity (CALC),
pages 32?39. ACL Press.
R. Neumayer and A. Rauber. 2007. Integration of text
and audio features for genre classification in music in-
formation retrieval. In Proceedings of the European
Conference on Information Retrieval (ECIR), pages
724?727. Springer.
H.R.G. Oliveira, F.A. Cardoso, and F.C. Pereira. 2007.
Tra-la-lyrics: An approach to generate text based on
rhythm. In Proceedings of the International Joint
Workshop on Computational Creativity, London.
A. Ramakrishnan, S. Kuppan, and S. Lalitha Devi. 2009.
Automatic generation of tamil lyrics for melodies. In
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity (CALC), pages 40?
46. ACL Press.
C. Solis, J.T. Siy, E. Tabirao, and E. Ong. 2009. Plan-
ning author and character goals for story generation.
In Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity (CALC), pages 63?
70. ACL Press.
D. Thompson. 2007. Hallo Spaceboy: The Rebirth of
David Bowie. ECW Press.
J.M. Yang, C.Y. Lai, and Y.H. Hsieh. 2007. A quantita-
tive measurement mechanism for lyrics evaluation: A
text mining approach. In Proceedings of the Interna-
tional Conference on Business and Information (BAI).
ATISR.
X. Zhu, Z. Xu, and T. Khot. 2009. How creative is your
writing? In Proceedings of the Workshop on Compu-
tational Approaches to Linguistic Creativity (CALC),
pages 87?93. ACL Press.
57
