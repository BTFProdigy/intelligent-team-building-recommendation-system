Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269?1280,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lateen EM: Unsupervised Training with Multiple Objectives,
Applied to Dependency Grammar Induction
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We present new training methods that aim to
mitigate local optima and slow convergence in
unsupervised training by using additional im-
perfect objectives. In its simplest form, lateen
EM alternates between the two objectives of
ordinary ?soft? and ?hard? expectation max-
imization (EM) algorithms. Switching objec-
tives when stuck can help escape local optima.
We find that applying a single such alternation
already yields state-of-the-art results for En-
glish dependency grammar induction. More
elaborate lateen strategies track both objec-
tives, with each validating the moves proposed
by the other. Disagreements can signal earlier
opportunities to switch or terminate, saving it-
erations. De-emphasizing fixed points in these
ways eliminates some guesswork from tuning
EM. An evaluation against a suite of unsu-
pervised dependency parsing tasks, for a vari-
ety of languages, showed that lateen strategies
significantly speed up training of both EM al-
gorithms, and improve accuracy for hard EM.
1 Introduction
Expectation maximization (EM) algorithms (Demp-
ster et al, 1977) play important roles in learning
latent linguistic structure. Unsupervised techniques
from this family excel at core natural language pro-
cessing (NLP) tasks, including segmentation, align-
ment, tagging and parsing. Typical implementations
specify a probabilistic framework, pick an initial
model instance, and iteratively improve parameters
using EM. A key guarantee is that subsequent model
instances are no worse than the previous, according
to training data likelihood in the given framework.
Another attractive feature that helped make EM
instrumental (Meng, 2007) is its initial efficiency:
Training tends to begin with large steps in a param-
eter space, sometimes bypassing many local optima
at once. After a modest number of such iterations,
however, EM lands close to an attractor. Next, its
convergence rate necessarily suffers: Disproportion-
ately many (and ever-smaller) steps are needed to
finally approach this fixed point, which is almost in-
variably a local optimum. Deciding when to termi-
nate EM often involves guesswork; and finding ways
out of local optima requires trial and error. We pro-
pose several strategies that address both limitations.
Unsupervised objectives are, at best, loosely cor-
related with extrinsic performance (Pereira and Sch-
abes, 1992; Merialdo, 1994; Liang and Klein, 2008,
inter alia). This fact justifies (occasionally) devi-
ating from a prescribed training course. For exam-
ple, since multiple equi-plausible objectives are usu-
ally available, a learner could cycle through them,
optimizing alternatives when the primary objective
function gets stuck; or, instead of trying to escape, it
could aim to avoid local optima in the first place, by
halting search early if an improvement to one objec-
tive would come at the expense of harming another.
We test these general ideas by focusing on non-
convex likelihood optimization using EM. This set-
ting is standard and has natural and well-understood
objectives: the classic, ?soft? EM; and Viterbi, or
?hard? EM (Kearns et al, 1997). The name ?la-
teen? comes from the sea ? triangular lateen sails
can take wind on either side, enabling sailing ves-
sels to tack (see Figure 1). As a captain can?t count
on favorable winds, so an unsupervised learner can?t
rely on co-operative gradients: soft EM maximizes
1269
Figure 1: A triangular sail atop a traditional Arab sail-
ing vessel, the dhow (right). Older square sails permitted
sailing only before the wind. But the efficient lateen sail
worked like a wing (with high pressure on one side and
low pressure on the other), allowing a ship to go almost
directly into a headwind. By tacking, in a zig-zag pattern,
it became possible to sail in any direction, provided there
was some wind at all (left). For centuries seafarers ex-
pertly combined both sails to traverse extensive distances,
greatly increasing the reach of medieval navigation.1
likelihoods of observed data across assignments to
hidden variables, whereas hard EM focuses on most
likely completions.2 These objectives are plausible,
yet both can be provably ?wrong? (Spitkovsky et al,
2010a, ?7.3). Thus, it is permissible for lateen EM
to maneuver between their gradients, for example by
tacking around local attractors, in a zig-zag fashion.
2 The Lateen Family of Algorithms
We propose several strategies that use a secondary
objective to improve over standard EM training. For
hard EM, the secondary objective is that of soft EM;
and vice versa if soft EM is the primary algorithm.
2.1 Algorithm #1: Simple Lateen EM
Simple lateen EM begins by running standard EM
to convergence, using a user-supplied initial model,
primary objective and definition of convergence.
Next, the algorithm alternates. A single lateen al-
ternation involves two phases: (i) retraining using
the secondary objective, starting from the previ-
ous converged solution (once again iterating until
convergence, but now of the secondary objective);
1Partially adapted from http://www.britannica.com/
EBchecked/topic/331395, http://allitera.tive.org/
archives/004922.html and http://landscapedvd.com/
desktops/images/ship1280x1024.jpg.
2See Brown et al?s (1993, ?6.2) definition of Viterbi train-
ing for a succinct justification of hard EM; in our case, the cor-
responding objective is Spitkovsky et al?s (2010a, ?7.1) ??VIT.
and (ii) retraining using the primary objective again,
starting from the latest converged solution (once
more to convergence of the primary objective). The
algorithm stops upon failing to sufficiently improve
the primary objective across alternations (applying
the standard convergence criterion end-to-end) and
returns the best of all models re-estimated during
training (as judged by the primary objective).
2.2 Algorithm #2: Shallow Lateen EM
Same as algorithm #1, but switches back to optimiz-
ing the primary objective after a single step with the
secondary, during phase (i) of all lateen alternations.
Thus, the algorithm alternates between optimizing
a primary objective to convergence, then stepping
away, using one iteration of the secondary optimizer.
2.3 Algorithm #3: Early-Stopping Lateen EM
This variant runs standard EM but quits early if
the secondary objective suffers. We redefine con-
vergence by ?or?-ing the user-supplied termination
criterion (i.e., a ?small-enough? change in the pri-
mary objective) with any adverse change of the sec-
ondary (i.e., an increase in its cross-entropy). Early-
stopping lateen EM does not alternate objectives.
2.4 Algorithm #4: Early-Switching Lateen EM
Same as algorithm #1, but with the new definition
of convergence, as in algorithm #3. Early-switching
lateen EM halts primary optimizers as soon as they
hurt the secondary objective and stops secondary op-
timizers once they harm the primary objective. This
algorithm terminates when it fails to sufficiently im-
prove the primary objective across a full alternation.
2.5 Algorithm #5: Partly-Switching Lateen EM
Same as algorithm #4, but again iterating primary
objectives to convergence, as in algorithm #1; sec-
ondary optimizers still continue to terminate early.
3 The Task and Study #1
We chose to test the impact of these five lateen al-
gorithms on unsupervised dependency parsing ? a
task in which EM plays an important role (Paskin,
2001; Klein and Manning, 2004; Gillenwater et al,
2010, inter alia). This entailed two sets of exper-
iments: In study #1, we tested whether single al-
ternations of simple lateen EM (as defined in ?2.1,
1270
System DDA (%)
(Blunsom and Cohn, 2010) 55.7
(Gillenwater et al, 2010) 53.3
(Spitkovsky et al, 2010b) 50.4
+ soft EM + hard EM 52.8 (+2.4)
lexicalized, using hard EM 54.3 (+1.5)
+ soft EM + hard EM 55.6 (+1.3)
Table 1: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences) for recent state-of-the-art
systems and our two experiments (one unlexicalized and
one lexicalized) with a single alternation of lateen EM.
Algorithm #1) improve our recent publicly-available
system for English dependency grammar induction.
In study #2, we introduced a more sophisticated
methodology that uses factorial designs and regres-
sions to evaluate lateen strategies with unsupervised
dependency parsing in many languages, after also
controlling for other important sources of variation.
For study #1, our base system (Spitkovsky et al,
2010b) is an instance of the popular (unlexicalized)
Dependency Model with Valence (Klein and Man-
ning, 2004). This model was trained using hard EM
on WSJ45 (WSJ sentences up to length 45) until suc-
cessive changes in per-token cross-entropy fell be-
low 2?20 bits (Spitkovsky et al, 2010b; 2010a, ?4).3
We confirmed that the base model had indeed con-
verged, by running 10 steps of hard EM on WSJ45
and verifying that its objective did not change much.
Next, we applied a single alternation of simple la-
teen EM: first running soft EM (this took 101 steps,
using the same termination criterion), followed by
hard EM (again to convergence ? another 23 it-
erations). The result was a decrease in hard EM?s
cross-entropy, from 3.69 to 3.59 bits per token (bpt),
accompanied by a 2.4% jump in accuracy, from 50.4
to 52.8%, on Section 23 of WSJ (see Table 1).4
Our first experiment showed that lateen EM holds
promise for simple models. Next, we tested it in
a more realistic setting, by re-estimating lexicalized
models,5 starting from the unlexicalized model?s
3http://nlp.stanford.edu/pubs/
markup-data.tar.bz2: dp.model.dmv
4It is standard practice to convert gold labeled constituents
from Penn English Treebank?s Wall Street Journal (WSJ) por-
tion (Marcus et al, 1993) into unlabeled reference dependency
parses using deterministic ?head-percolation? rules (Collins,
1999); sentence root symbols (but not punctuation) arcs count
towards accuracies (Paskin, 2001; Klein and Manning, 2004).
5We used Headden et al?s (2009) method (also the approach
parses; this took 24 steps with hard EM. We then
applied another single lateen alternation: This time,
soft EM ran for 37 steps, hard EM took another 14,
and the new model again improved, by 1.3%, from
54.3 to 55.6% (see Table 1); the corresponding drop
in (lexicalized) cross-entropy was from 6.10 to 6.09
bpt. This last model is competitive with the state-of-
the-art; moreover, gains from single applications of
simple lateen alternations (2.4 and 1.3%) are on par
with the increase due to lexicalization alone (1.5%).
4 Methodology for Study #2
Study #1 suggests that lateen EM can improve gram-
mar induction in English. To establish statistical sig-
nificance, however, it is important to test a hypothe-
sis in many settings (Ioannidis, 2005). We therefore
use a factorial experimental design and regression
analyses with a variety of lateen strategies. Two re-
gressions ? one predicting accuracy, the other, the
number of iterations ? capture the effects that la-
teen algorithms have on performance and efficiency,
relative to standard EM training. We controlled for
important dimensions of variation, such as the un-
derlying language: to make sure that our results are
not English-specific, we induced grammars in 19
languages. We also explored the impact from the
quality of an initial model (using both uniform and
ad hoc initializers), the choice of a primary objective
(i.e., soft or hard EM), and the quantity and com-
plexity of training data (shorter versus both short and
long sentences). Appendix A gives the full details.
4.1 Data Sets
We use all 23 train/test splits from the 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007),6 which cover 19 different lan-
guages.7 We splice out all punctuation labeled in the
data, as is standard practice (Paskin, 2001; Klein and
Manning, 2004), introducing new arcs from grand-
mothers to grand-daughters where necessary, both in
train- and test-sets. Evaluation is always against the
taken by the two stronger state-of-the-art systems): for words
seen at least 100 times in the training corpus, gold part-of-
speech tags are augmented with lexical items.
6These disjoint splits require smoothing; in the WSJ setting,
training and test sets overlapped (Klein and Manning, 2004).
7We down-weigh languages appearing in both years ? Ara-
bic, Chinese, Czech and Turkish ? by 50% in all our analyses.
1271
entire resulting test sets (i.e., all sentence lengths).8
4.2 Grammar Models
In all remaining experiments we model grammars
via the original DMV, which ignores punctuation; all
models are unlexicalized, with gold part-of-speech
tags for word classes (Klein and Manning, 2004).
4.3 Smoothing Mechanism
All unsmoothed models are smoothed immediately
prior to evaluation; some of the baseline models are
also smoothed during training. In both cases, we use
the ?add-one? (a.k.a. Laplace) smoothing algorithm.
4.4 Standard Convergence
We always halt an optimizer once a change in its ob-
jective?s consecutive cross-entropy values falls be-
low 2?20 bpt (at which point we consider it ?stuck?).
4.5 Scoring Function
We report directed accuracies ? fractions of cor-
rectly guessed (unlabeled) dependency arcs, includ-
ing arcs from sentence root symbols, as is standard
practice (Paskin, 2001; Klein and Manning, 2004).
Punctuation does not affect scoring, as it had been
removed from all parse trees in our data (see ?4.1).
5 Experiments
We now summarize our baseline models and briefly
review the proposed lateen algorithms. For details of
the default systems (standard soft and hard EM), all
control variables and both regressions (against final
accuracies and iteration counts) see Appendix A.
5.1 Baseline Models
We tested a total of six baseline models, experiment-
ing with two types of alternatives: (i) strategies that
perturb stuck models directly, by smoothing, ignor-
ing secondary objectives; and (ii) shallow applica-
tions of a single EM step, ignoring convergence.
Baseline B1 alternates running standard EM to
convergence and smoothing. A second baseline, B2,
smooths after every step of EM instead. Another
shallow baseline, B3, alternates single steps of soft
8With the exception of Arabic ?07, from which we discarded
a single sentence containing 145 non-punctuation tokens.
and hard EM.9 Three such baselines begin with hard
EM (marked with the subscript h); and three more
start with soft EM (marked with the subscript s).
5.2 Lateen Models
Ten models, A{1, 2, 3, 4, 5}{h,s}, correspond to our la-
teen algorithms #1?5 (?2), starting with either hard
or soft EM?s objective, to be used as the primary.
6 Results
Soft EM Hard EM
Model ?a ?i ?a ?i
Baselines B3 -2.7 ?0.2 -2.0 ?0.3
B2 +0.6 ?0.7 +0.6 ?1.2
B1 0.0 ?2.0 +0.8 ?3.7
Algorithms A1 0.0 ?1.3 +5.5 ?6.5
A2 -0.0 ?1.3 +1.5 ?3.6
A3 0.0 ?0.7 -0.1 ?0.7
A4 0.0 ?0.8 +3.0 ?2.1
A5 0.0 ?1.2 +2.9 ?3.8
Table 2: Estimated additive changes in directed depen-
dency accuracy (?a) and multiplicative changes in the
number of iterations before terminating (?i) for all base-
line models and lateen algorithms, relative to standard
training: soft EM (left) and hard EM (right). Bold en-
tries are statistically different (p < 0.01) from zero, for
?a, and one, for ?i (details in Table 4 and Appendix A).
Not one baseline attained a statistically significant
performance improvement. Shallow models B3{h,s},
in fact, significantly lowered accuracy: by 2.0%, on
average (p ? 7.8 ? 10?4), for B3h, which began with
hard EM; and down 2.7% on average (p ? 6.4?10?7),
for B3s, started with soft EM. They were, however,
3?5x faster than standard training, on average (see
Table 4 for all estimates and associated p-values;
above, Table 2 shows a preview of the full results).
6.1 A1{h,s} ? Simple Lateen EM
A1h runs 6.5x slower, but scores 5.5% higher, on av-
erage, compared to standard Viterbi training; A1s is
only 30% slower than standard soft EM, but does not
impact its accuracy at all, on average.
Figure 2 depicts a sample training run: Italian ?07
with A1h. Viterbi EM converges after 47 iterations,
9It approximates a mixture (the average of soft and hard
objectives) ? a natural comparison, computable via gradients
and standard optimization algorithms, such as L-BFGS (Liu and
Nocedal, 1989). We did not explore exact interpolations, how-
ever, because replacing EM is itself a significant confounder,
even with unchanged objectives (Berg-Kirkpatrick et al, 2010).
1272
50 100 150 200 250 300
3.0
3.5
4.0
4.5
3.39
3.26
(3.42)
(3.19)
3.33
3.23
(3.39)
(3.18)
3.29
3.21
(3.39)
(3.18)
3.29
3.22
bpt
iteration
cross-entropies (in bits per token)
Figure 2: Cross-entropies for Italian ?07, initialized uni-
formly and trained on sentences up to length 45. The two
curves are primary and secondary objectives (soft EM?s
lies below, as sentence yields are at least as likely as parse
trees): shaded regions indicate iterations of hard EM (pri-
mary); and annotated values are measurements upon each
optimizer?s convergence (soft EM?s are parenthesized).
reducing the primary objective to 3.39 bpt (the sec-
ondary is then at 3.26); accuracy on the held-out set
is 41.8%. Three alternations of lateen EM (totaling
265 iterations) further decrease the primary objec-
tive to 3.29 bpt (the secondary also declines, to 3.22)
and accuracy increases to 56.2% (14.4% higher).
6.2 A2{h,s} ? Shallow Lateen EM
A2h runs 3.6x slower, but scores only 1.5% higher,
on average, compared to standard Viterbi training;
A2s is again 30% slower than standard soft EM and
also has no measurable impact on parsing accuracy.
6.3 A3{h,s} ? Early-Stopping Lateen EM
Both A3h and A3s run 30% faster, on average, than
standard training with hard or soft EM; and neither
heuristic causes a statistical change to accuracy.
Table 3 shows accuracies and iteration counts for
10 (of 23) train/test splits that terminate early with
A3s (in one particular, example setting). These runs
are nearly twice as fast, and only two score (slightly)
lower, compared to standard training using soft EM.
6.4 A4{h,s} ? Early-Switching Lateen EM
A4h runs only 2.1x slower, but scores only 3.0%
higher, on average, compared to standard Viterbi
training; A4s is, in fact, 20% faster than standard soft
EM, but still has no measurable impact on accuracy.
6.5 A5{h,s} ? Partly-Switching Lateen EM
A5h runs 3.8x slower, scoring 2.9% higher, on av-
erage, compared to standard Viterbi training; A5s is
20% slower than soft EM, but, again, no more accu-
rate. Indeed, A4 strictly dominates both A5 variants.
CoNLL Year Soft EM A3s
& Language DDA iters DDA iters
Arabic 2006 28.4 180 28.4 118
Bulgarian ?06 39.1 253 39.6 131
Chinese ?06 49.4 268 49.4 204
Dutch ?06 21.3 246 27.8 35
Hungarian ?07 17.1 366 17.4 213
Italian ?07 39.6 194 39.6 164
Japanese ?06 56.6 113 56.6 93
Portuguese ?06 37.9 180 37.5 102
Slovenian ?06 30.8 234 31.1 118
Spanish ?06 33.3 125 33.1 73
Average: 35.4 216 36.1 125
Table 3: Directed dependency accuracies (DDA) and iter-
ation counts for the 10 (of 23) train/test splits affected by
early termination (setting: soft EM?s primary objective,
trained using shorter sentences and ad-hoc initialization).
7 Discussion
Lateen strategies improve dependency grammar in-
duction in several ways. Early stopping offers a
clear benefit: 30% higher efficiency yet same perfor-
mance as standard training. This technique could be
used to (more) fairly compare learners with radically
different objectives (e.g., lexicalized and unlexical-
ized), requiring quite different numbers of steps (or
magnitude changes in cross-entropy) to converge.
The second benefit is improved performance, but
only starting with hard EM. Initial local optima dis-
covered by soft EM are such that the impact on ac-
curacy of all subsequent heuristics is indistinguish-
able from noise (it?s not even negative). But for hard
EM, lateen strategies consistently improve accuracy
? by 1.5, 3.0 or 5.5% ? as an algorithm follows the
secondary objective longer (a single step, until the
primary objective gets worse, or to convergence).
Our results suggest that soft EM should use early
termination to improve efficiency. Hard EM, by con-
trast, could use any lateen strategy to improve either
efficiency or performance, or to strike a balance.
8 Related Work
8.1 Avoiding and/or Escaping Local Attractors
Simple lateen EM is similar to Dhillon et al?s (2002)
refinement algorithm for text clustering with spher-
ical k-means. Their ?ping-pong? strategy alternates
batch and incremental EM, exploits the strong points
of each, and improves a shared objective at every
1273
step. Unlike generalized (GEM) variants (Neal and
Hinton, 1999), lateen EM uses multiple objectives:
it sacrifices the primary in the short run, to escape
local optima; in the long run, it also does no harm,
by construction (as it returns the best model seen).
Of the meta-heuristics that use more than a stan-
dard, scalar objective, deterministic annealing (DA)
(Rose, 1998) is closest to lateen EM. DA perturbs
objective functions, instead of manipulating solu-
tions directly. As other continuation methods (All-
gower and Georg, 1990), it optimizes an easy (e.g.,
convex) function first, then ?rides? that optimum by
gradually morphing functions towards the difficult
objective; each step reoptimizes from the previous
approximate solution. Smith and Eisner (2004) em-
ployed DA to improve part-of-speech disambigua-
tion, but found that objectives had to be further
?skewed,? using domain knowledge, before it helped
(constituent) grammar induction. (For this reason,
we did not experiment with DA, despite its strong
similarities to lateen EM.) Smith and Eisner (2004)
used a ?temperature? ? to anneal a flat uniform dis-
tribution (? = 0) into soft EM?s non-convex objec-
tive (? = 1). In their framework, hard EM corre-
sponds to ? ?? ?, so the algorithms differ only in
their ?-schedule: DA?s is continuous, from 0 to 1; la-
teen EM?s is a discrete alternation, of 1 and +?.10
8.2 Terminating Early, Before Convergence
EM is rarely run to (even numerical) convergence.
Fixing a modest number of iterations a priori (Klein,
2005, ?5.3.4), running until successive likelihood ra-
tios become small (Spitkovsky et al, 2009, ?4.1) or
using a combination of the two (Ravi and Knight,
2009, ?4, Footnote 5) is standard practice in NLP.
Elworthy?s (1994, ?5, Figure 1) analysis of part-of-
speech tagging showed that, in most cases, a small
number of iterations is actually preferable to conver-
gence, in terms of final accuracies: ?regularization
by early termination? had been suggested for image
deblurring algorithms in statistical astronomy (Lucy,
1974, ?2); and validation against held-out data ? a
strategy proposed much earlier, in psychology (Lar-
son, 1931), has also been used as a halting crite-
rion in NLP (Yessenalina et al, 2010, ?4.2, 5.2).
10One can think of this as a kind of ?beam search? (Lowerre,
1976), with soft EM expanding and hard EM pruning a frontier.
Early-stopping lateen EM tethers termination to a
sign change in the direction of a secondary objective,
similarly to (cross-)validation (Stone, 1974; Geisser,
1975; Arlot and Celisse, 2010), but without splitting
data ? it trains using all examples, at all times.11,12
8.3 Training with Multiple Views
Lateen strategies may seem conceptually related to
co-training (Blum and Mitchell, 1998). However,
bootstrapping methods generally begin with some
labeled data and gradually label the rest (discrimina-
tively) as they grow more confident, but do not opti-
mize an explicit objective function; EM, on the other
hand, can be fully unsupervised, relabels all exam-
ples on each iteration (generatively), and guarantees
not to hurt a well-defined objective, at every step.13
Co-training classically relies on two views of the
data ? redundant feature sets that allow different al-
gorithms to label examples for each other, yielding
?probably approximately correct? (PAC)-style guar-
antees under certain (strong) assumptions. In con-
trast, lateen EM uses the same data, features, model
and essentially the same algorithms, changing only
their objective functions: it makes no assumptions,
but guarantees not to harm the primary objective.
Some of these distinctions have become blurred
with time: Collins and Singer (1999) introduced
an objective function (also based on agreement)
into co-training; Goldman and Zhou (2000), Ng
and Cardie (2003) and Chan et al (2004) made do
without redundant views; Balcan et al (2004) re-
laxed other strong assumptions; and Zhou and Gold-
man (2004) generalized co-training to accommodate
three and more algorithms. Several such methods
have been applied to dependency parsing (S?gaard
and Rish?j, 2010), constituent parsing (Sarkar,
11We see in it a milder contrastive estimation (Smith and Eis-
ner, 2005a; 2005b), agnostic to implicit negative evidence, but
caring whence learners push probability mass towards training
examples: when most likely parse trees begin to benefit at the
expense of their sentence yields (or vice versa), optimizers halt.
12For a recently proposed instance of EM that uses cross-
validation (CV) to optimize smoothed data likelihoods (in learn-
ing synchronous PCFGs, for phrase-based machine translation),
see Mylonakis and Sima?an?s (2010, ?3.1) CV-EM algorithm.
13Some authors (Nigam and Ghani, 2000; Ng and Cardie,
2003; Smith and Eisner, 2005a, ?5.2, 7; ?2; ?6) draw a hard line
between bootstrapping algorithms, such as self- and co-training,
and probabilistic modeling using EM; others (Dasgupta et al,
2001; Chang et al, 2007, ?1; ?5) tend to lump them together.
1274
2001) and parser reranking (Crim, 2002). Funda-
mentally, co-training exploits redundancies in unla-
beled data and/or learning algorithms. Lateen strate-
gies also exploit redundancies: in noisy objectives.
Both approaches use a second vantage point to im-
prove their perception of difficult training terrains.
9 Conclusions and Future Work
Lateen strategies can improve performance and effi-
ciency for dependency grammar induction with the
DMV. Early-stopping lateen EM is 30% faster than
standard training, without affecting accuracy ? it
reduces guesswork in terminating EM. At the other
extreme, simple lateen EM is slower, but signifi-
cantly improves accuracy ? by 5.5%, on average
? for hard EM, escaping some of its local optima.
It would be interesting to apply lateen algorithms
to advanced parsing models (Blunsom and Cohn,
2010; Headden et al, 2009, inter alia) and learn-
ing algorithms (Gillenwater et al, 2010; Cohen and
Smith, 2009, inter alia). Future work could explore
other NLP tasks ? such as clustering, sequence la-
beling, segmentation and alignment ? that often
employ EM. Our meta-heuristics are multi-faceted,
featuring aspects of iterated local search, determin-
istic annealing, cross-validation, contrastive estima-
tion and co-training. They may be generally useful
in machine learning and non-convex optimization.
Appendix A. Experimental Design
Statistical techniques are vital to many aspects of
computational linguistics (Johnson, 2009; Charniak,
1997; Abney, 1996, inter alia). We used factorial
designs,14 which are standard throughout the natu-
ral and social sciences, to assist with experimental
design and statistical analyses. Combined with or-
dinary regressions, these methods provide succinct
and interpretable summaries that explain which set-
tings meaningfully contribute to changes in depen-
dent variables, such as running time and accuracy.
14We used full factorial designs for clarity of exposition. But
many fewer experiments would suffice, especially in regression
models without interaction terms: for the more efficient frac-
tional factorial designs, as well as for randomized block designs
and full factorial designs, see Montgomery (2005, Ch. 4?9).
9.1 Dependent Variables
We constructed two regressions, for two types of de-
pendent variables: to summarize performance, we
predict accuracies; and to summarize efficiency, we
predict (logarithms of) iterations before termination.
In the performance regression, we used four dif-
ferent scores for the dependent variable. These in-
clude both directed accuracies and undirected accu-
racies, each computed in two ways: (i) using a best
parse tree; and (ii) using all parse trees. These four
types of scores provide different kinds of informa-
tion. Undirected scores ignore polarity of parent-
child relations (Paskin, 2001; Klein and Manning,
2004; Schwartz et al, 2011), partially correcting for
some effects of alternate analyses (e.g., systematic
choices between modals and main verbs for heads
of sentences, determiners for noun phrases, etc.).
And integrated scoring, using the inside-outside al-
gorithm (Baker, 1979) to compute expected accu-
racy across all ? not just best ? parse trees, has the
advantage of incorporating probabilities assigned to
individual arcs: This metric is more sensitive to the
margins that separate best from next-best parse trees,
and is not affected by tie-breaking. We tag scores
using two binary predictors in a simple (first order,
multi-linear) regression, where having multiple rel-
evant quality assessments improves goodness-of-fit.
In the efficiency regression, dependent variables
are logarithms of the numbers of iterations. Wrap-
ping EM in an inner loop of a heuristic has a mul-
tiplicative effect on the total number of models re-
estimated prior to termination. Consequently, loga-
rithms of the final counts better fit the observed data.
9.2 Independent Predictors
All of our predictors are binary indicators (a.k.a.
?dummy? variables). The undirected and integrated
factors only affect the regression for accuracies (see
Table 4, left); remaining factors participate also in
the running times regression (see Table 4, right). In a
default run, all factors are zero, corresponding to the
intercept estimated by a regression; other estimates
reflect changes in the dependent variable associated
with having that factor ?on? instead of ?off.?
? adhoc ? This setting controls initialization.
By default, we use the uninformed uniform ini-
tializer (Spitkovsky et al, 2010a); when it is
1275
Regression for Accuracies Regression for ln(Iterations)
Goodness-of-Fit: (R2adj ? 76.2%) (R2adj ? 82.4%)
Indicator Factors coeff. ?? adj. p-value
undirected 18.1 < 2.0 ? 10?16
integrated -0.9 ? 7.0 ? 10?7 coeff. ?? mult. e?? adj. p-value
(intercept) 30.9 < 2.0 ? 10?16 5.5 255.8 < 2.0 ? 10?16
adhoc 1.2 ? 3.1 ? 10?13 -0.0 1.0 ? 1.0
Model sweet 1.0 ? 3.1 ? 10?9 -0.2 0.8 < 2.0 ? 10?16
B3s shallow (soft-first) -2.7 ? 6.4 ? 10?7 -1.5 0.2 < 2.0 ? 10?16
B3h shallow (hard-first) -2.0 ? 7.8 ? 10?4 -1.2 0.3 < 2.0 ? 10?16
B2s shallow smooth 0.6 ? 1.0 -0.4 0.7 ? 1.4 ? 10?12
B1s smooth 0.0 ? 1.0 0.7 2.0 < 2.0 ? 10?16
A1s simple lateen 0.0 ? 1.0 0.2 1.3 ? 4.1 ? 10?4
A2s shallow lateen -0.0 ? 1.0 0.2 1.3 ? 5.8 ? 10?4
A3s early-stopping lateen 0.0 ? 1.0 -0.3 0.7 ? 2.6 ? 10?7
A4s early-switching lateen 0.0 ? 1.0 -0.3 0.8 ? 2.6 ? 10?7
A5s partly-switching lateen 0.0 ? 1.0 0.2 1.2 ? 4.2 ? 10?3
viterbi -4.0 ? 5.7 ? 10?16 -1.7 0.2 < 2.0 ? 10?16
B2h shallow smooth 0.6 ? 1.0 0.2 1.2 ? 5.6 ? 10?2
B1h smooth 0.8 ? 1.0 1.3 3.7 < 2.0 ? 10?16
A1h simple lateen 5.5 < 2.0 ? 10?16 1.9 6.5 < 2.0 ? 10?16
A2h shallow lateen 1.5 ? 5.0 ? 10?2 1.3 3.6 < 2.0 ? 10?16
A3h early-stopping lateen -0.1 ? 1.0 -0.4 0.7 ? 1.7 ? 10?11
A4h early-switching lateen 3.0 ? 1.0 ? 10?8 0.7 2.1 < 2.0 ? 10?16
A5h partly-switching lateen 2.9 ? 7.6 ? 10?8 1.3 3.8 < 2.0 ? 10?16
Table 4: Regressions for accuracies and natural-log-iterations, using 86 binary predictors (all p-values jointly adjusted
for simultaneous hypothesis testing; {langyear} indicators not shown). Accuracies? estimated coefficients ?? that are
statistically different from 0 ? and iteration counts? multipliers e?? significantly different from 1 ? are shown in bold.
on, we use Klein and Manning?s (2004) ?ad-
hoc? harmonic heuristic, bootstrapped using
sentences up to length 10, from the training set.
? sweet ? This setting controls the length cut-
off. By default, we train with all sentences con-
taining up to 45 tokens; when it is on, we use
Spitkovsky et al?s (2009) ?sweet spot? cutoff
of 15 tokens (recommended for English, WSJ).
? viterbi ? This setting controls the primary ob-
jective of the learning algorithm. By default,
we run soft EM; when it is on, we use hard EM.
? {langyeari}22i=1 ? This is a set of 22 mutually-
exclusive selectors for the language/year of a
train/test split; default (all zeros) is English ?07.
Due to space limitations, we exclude langyear pre-
dictors from Table 4. Further, we do not explore
(even two-way) interactions between predictors.15
15This approach may miss some interesting facts, e.g., that
the adhoc initializer is exceptionally good for English, with soft
9.3 Statistical Significance
Our statistical analyses relied on the R package (R
Development Core Team, 2011), which does not,
by default, adjust statistical significance (p-values)
for multiple hypotheses testing.16 We corrected
this using the Holm-Bonferroni method (Holm,
1979), which is uniformly more powerful than the
older (Dunn-)Bonferroni procedure; since we tested
many fewer hypotheses (44 + 42 ? one per inter-
cept/coefficient ??) than settings combinations, its ad-
justments to the p-values are small (see Table 4).17
EM. Instead it yields coarse summaries of regularities supported
by overwhelming evidence across data and training regimes.
16Since we would expect p% of randomly chosen hypotheses
to appear significant at the p% level simply by chance, we must
take precautions against these and other ?data-snooping? biases.
17We adjusted the p-values for all 86 hypotheses jointly, us-
ing http://rss.acs.unt.edu/Rdoc/library/multtest/
html/mt.rawp2adjp.html.
1276
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 28.4 118 28.4 162 21.6 19 21.6 21 32.1 200
?7 ? ? 26.9 171 24.7 17 24.8 24 22.0 239
Basque ?7 ? ? 39.9 180 32.0 16 32.2 20 43.6 128
Bulgarian ?6 39.6 131 39.1 253 41.6 22 41.5 25 44.3 140
Catalan ?7 ? ? 58.5 135 50.1 48 50.1 54 63.8 279
Chinese ?6 49.4 204 49.4 268 31.3 24 31.6 55 37.9 378
?7 ? ? 46.0 262 30.0 25 30.2 64 34.5 307
Czech ?6 ? ? 50.5 294 27.8 27 27.7 33 35.2 445
?7 ? ? 49.8 263 29.0 37 29.0 41 31.4 307
Danish ?6 ? ? 43.5 116 43.8 31 43.9 45 44.0 289
Dutch ?6 27.8 35 21.3 246 24.9 44 24.9 49 32.5 241
English ?7 ? ? 38.1 180 34.0 32 33.9 42 34.9 186
German ?6 ? ? 33.3 136 25.4 20 25.4 39 33.5 155
Greek ?7 ? ? 17.5 230 18.3 18 18.3 21 21.4 117
Hungarian ?7 17.4 213 17.1 366 12.3 26 12.4 36 23.0 246
Italian ?7 39.6 164 39.6 194 32.6 25 32.6 27 37.6 273
Japanese ?6 56.6 93 56.6 113 49.6 20 49.7 23 53.5 91
Portuguese ?6 37.5 102 37.9 180 28.6 27 28.9 41 34.4 134
Slovenian ?6 31.1 118 30.8 234 ? ? 23.4 22 33.6 255
Spanish ?6 33.1 73 33.3 125 18.2 29 18.4 36 33.3 235
Swedish ?6 ? ? 41.8 242 36.0 24 36.1 29 42.5 296
Turkish ?6 ? ? 29.8 303 17.8 19 22.2 38 31.9 134
?7 ? ? 28.3 227 14.0 9 10.7 31 33.4 242
Average: 37.4 162 37.0 206 30.0 26 30.0 35 37.1 221
Table 5: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with adhoc and sweet settings on.
9.4 Interpretation
Table 4 shows the estimated coefficients and their
(adjusted) p-values for both intercepts and most pre-
dictors (excluding the language/year of the data sets)
for all 1,840 experiments. The default (English) sys-
tem uses soft EM, trains with both short and long
sentences, and starts from an uninformed uniform
initializer. It is estimated to score 30.9%, converging
after approximately 256 iterations (both intercepts
are statistically different from zero: p < 2.0 ? 10?16).
As had to be the case, we detect a gain from undi-
rected scoring; integrated scoring is slightly (but
significantly: p ? 7.0 ? 10?7) negative, which is re-
assuring: best parses are scoring higher than the rest
and may be standing out by large margins. The ad-
hoc initializer boosts accuracy by 1.2%, overall (also
significant: p ? 3.1 ? 10?13), without a measurable
impact on running time (p ? 1.0). Training with
fewer, shorter sentences, at the sweet spot gradation,
adds 1.0% and shaves 20% off the total number of it-
erations, on average (both estimates are significant).
We find the viterbi objective harmful ? by 4.0%,
on average (p ? 5.7 ? 10?16) ? for the CoNLL sets.
Spitkovsky et al (2010a) reported that it helps on
WSJ, at least with long sentences and uniform ini-
tializers. Half of our experiments are with shorter
sentences, and half use ad hoc initializers (i.e., three
quarters of settings are not ideal for Viterbi EM),
which may have contributed to this negative result;
still, our estimates do confirm that hard EM is sig-
nificantly (80%, p < 2.0? 10?16) faster than soft EM.
9.5 More on Viterbi Training
The overall negative impact of Viterbi objectives is
a cause for concern: On average, A1h?s estimated
gain of 5.5% should more than offset the expected
4.0% loss from starting with hard EM. But it is, nev-
ertheless, important to make sure that simple lateen
EM with hard EM?s primary objective is in fact an
improvement over both standard EM algorithms.
Table 5 shows performance and efficiency num-
bers for A1h, A3{h,s}, as well as standard soft and
hard EM, using settings that are least favorable for
1277
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 ? ? 33.4 317 20.8 8 20.2 32 16.6 269
?7 18.6 60 8.7 252 26.5 9 26.4 14 49.5 171
Basque ?7 ? ? 18.3 245 23.2 16 23.0 23 24.0 162
Bulgarian ?6 27.0 242 27.1 293 40.6 33 40.5 34 43.9 276
Catalan ?7 15.0 74 13.8 159 53.2 30 53.1 31 59.8 176
Chinese ?6 63.5 131 63.6 261 36.8 45 36.8 47 44.5 213
?7 58.5 130 58.5 258 35.2 20 35.0 48 43.2 372
Czech ?6 29.5 125 29.7 224 23.6 18 23.8 41 27.7 179
?7 ? ? 25.9 215 27.1 37 27.2 64 28.4 767
Danish ?6 ? ? 16.6 155 28.7 30 28.7 30 38.3 241
Dutch ?6 20.4 51 21.2 174 25.5 30 25.6 38 27.8 243
English ?7 ? ? 18.0 162 ? ? 38.7 35 45.2 366
German ?6 ? ? 24.4 148 30.1 39 30.1 44 30.4 185
Greek ?7 25.5 133 25.3 156 ? ? 13.2 27 13.2 252
Hungarian ?7 ? ? 18.9 310 28.9 34 28.9 44 34.7 414
Italian ?7 25.4 127 25.3 165 ? ? 52.3 36 52.3 81
Japanese ?6 ? ? 39.3 143 42.2 38 42.4 48 50.2 199
Portuguese ?6 35.2 48 35.6 224 ? ? 34.5 21 36.7 143
Slovenian ?6 24.8 182 25.3 397 28.8 17 28.8 20 32.2 121
Spanish ?6 ? ? 27.7 252 ? ? 28.3 31 50.6 130
Swedish ?6 27.9 49 32.6 287 45.2 22 45.6 52 50.0 314
Turkish ?6 ? ? 30.5 239 30.2 16 30.6 24 29.0 138
?7 ? ? 48.8 254 34.3 24 33.1 34 35.9 269
Average: 27.3 161 27.3 225 33.2 28 33.2 35 38.2 236
Table 6: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with setting adhoc off and sweet on.
Viterbi training: adhoc and sweet on. Although A1h
scores 7.1% higher than hard EM, on average, it is
only slightly better than soft EM ? up 0.1% (and
worse than A1s). Without adhoc (i.e., using uniform
initializers ? see Table 6), however, hard EM still
improves, by 3.2%, on average, whereas soft EM
drops nearly 10%; here, A1h further improves over
hard EM, scoring 38.2% (up 5.0), higher than soft
EM?s accuracies from both settings (27.3 and 37.0).
This suggests that A1h is indeed better than both
standard EM algorithms. We suspect that our exper-
imental set-up may be disadvantageous for Viterbi
training, since half the settings use ad hoc initializ-
ers, and because CoNLL sets are small. (Viterbi EM
works best with more data and longer sentences.)
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Angel X. Chang, Spence Green,
David McClosky, Fernando Pereira, Slav Petrov and the anony-
mous reviewers, for many helpful comments on draft versions
of this paper, and Andrew Y. Ng, for a stimulating discussion.
First author is grateful to Lynda K. Dunnigan for first introduc-
ing him to lateen sails, among other connections, in Humanities.
1278
References
S. Abney. 1996. Statistical methods and linguistics. In
J. L. Klavans and P. Resnik, editors, The Balancing
Act: Combining Symbolic and Statistical Approaches
to Language. MIT Press.
E. L. Allgower and K. Georg. 1990. Numerical Contin-
uation Methods: An Introduction. Springer-Verlag.
S. Arlot and A. Celisse. 2010. A survey of cross-
validation procedures for model selection. Statistics
Surveys, 4.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M.-F. Balcan, A. Blum, and K. Yang. 2004. Co-training
and expansion: Towards bridging theory and practice.
In NIPS.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
J. Chan, I. Koprinska, and J. Poon. 2004. Co-training
with a single natural feature set applied to email clas-
sification. In WI.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 1997. Statistical techniques for natural lan-
guage parsing. AI Magazine, 18.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL-HLT.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Crim. 2002. Co-training re-rankers for improved
parser accuracy.
S. Dasgupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. In NIPS.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B, 39.
I. S. Dhillon, Y. Guan, and J. Kogan. 2002. Iterative
clustering of high dimensional text data augmented by
local search. In ICDM.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In ANLP.
S. Geisser. 1975. The predictive sample reuse method
with applications. Journal of the American Statistical
Association, 70.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. In ICML.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple
test procedure. Scandinavian Journal of Statistics, 6.
J. P. A. Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2.
M. Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In EACL: In-
teraction between Linguistics and Computational Lin-
guistics: Virtuous, Vicious or Vacuous?
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In UAI.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
S. C. Larson. 1931. The shrinkage of the coefficient of
multiple correlation. Journal of Educational Psychol-
ogy, 22.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In HLT-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming. Series B, 45.
B. T. Lowerre. 1976. The HARPY Speech Recognition
System. Ph.D. thesis, CMU.
L. B. Lucy. 1974. An iterative technique for the recti-
fication of observed distributions. The Astronomical
Journal, 79.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
1279
X.-L. Meng. 2007. EM and MCMC: Workhorses for sci-
entific computing (thirty years of EM and much more).
Statistica Sinica, 17.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
D. C. Montgomery. 2005. Design and Analysis of Exper-
iments. John Wiley & Sons, 6th edition.
M. Mylonakis and K. Sima?an. 2010. Learning prob-
abilistic synchronous CFGs for phrase-based transla-
tion. In CoNLL.
R. M. Neal and G. E. Hinton. 1999. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In M. I. Jordan, editor, Learning in Graphical
Models. MIT Press.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL.
K. Nigam and R. Ghani. 2000. Analyzing the effective-
ness and applicability of co-training. In CIKM.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing.
S. Ravi and K. Knight. 2009. Minimized models for un-
supervised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression and related opt-
mization problems. Proceedings of the IEEE, 86.
A. Sarkar. 2001. Applying co-training methods to statis-
tical parsing. In NAACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In IJ-
CAI: Grammatical Inference Applications.
A. S?gaard and C. Rish?j. 2010. Semi-supervised de-
pendency parsing using generalized tri-training. In
COLING.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. Journal of the Royal Statisti-
cal Society. Series B, 36.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
Y. Zhou and S. Goldman. 2004. Democratic co-learning.
In ICTAI.
1280
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281?1290,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Dependency Parsing without Gold Part-of-Speech Tags
Valentin I. Spitkovsky??
valentin@cs.stanford.edu
Hiyan Alshawi?
hiyan@google.com
Angel X. Chang??
angelx@cs.stanford.edu
Daniel Jurafsky??
jurafsky@stanford.edu
?Computer Science Department
Stanford University
Stanford, CA, 94305
?Google Research
Google Inc.
Mountain View, CA, 94043
?Department of Linguistics
Stanford University
Stanford, CA, 94305
Abstract
We show that categories induced by unsuper-
vised word clustering can surpass the perfor-
mance of gold part-of-speech tags in depen-
dency grammar induction. Unlike classic clus-
tering algorithms, our method allows a word
to have different tags in different contexts.
In an ablative analysis, we first demonstrate
that this context-dependence is crucial to the
superior performance of gold tags ? requir-
ing a word to always have the same part-of-
speech significantly degrades the performance
of manual tags in grammar induction, elim-
inating the advantage that human annotation
has over unsupervised tags. We then introduce
a sequence modeling technique that combines
the output of a word clustering algorithm with
context-colored noise, to allow words to be
tagged differently in different contexts. With
these new induced tags as input, our state-of-
the-art dependency grammar inducer achieves
59.1% directed accuracy on Section 23 (all
sentences) of the Wall Street Journal (WSJ)
corpus ? 0.7% higher than using gold tags.
1 Introduction
Unsupervised learning ? machine learning without
manually-labeled training examples ? is an active
area of scientific research. In natural language pro-
cessing, unsupervised techniques have been success-
fully applied to tasks such as word alignment for ma-
chine translation. And since the advent of the web,
algorithms that induce structure from unlabeled data
have continued to steadily gain importance. In this
paper we focus on unsupervised part-of-speech tag-
ging and dependency parsing ? two related prob-
lems of syntax discovery. Our methods are applica-
ble to vast quantities of unlabeled monolingual text.
Not all research on these problems has been fully
unsupervised. For example, to the best of our knowl-
edge, every new state-of-the-art dependency gram-
mar inducer since Klein and Manning (2004) relied
on gold part-of-speech tags. For some time, multi-
point performance degradations caused by switching
to automatically induced word categories have been
interpreted as indications that ?good enough? parts-
of-speech induction methods exist, justifying the fo-
cus on grammar induction with supervised part-of-
speech tags (Bod, 2006), pace (Cramer, 2007). One
of several drawbacks of this practice is that it weak-
ens any conclusions that could be drawn about how
computers (and possibly humans) learn in the ab-
sence of explicit feedback (McDonald et al, 2011).
In turn, not all unsupervised taggers actually in-
duce word categories: Many systems ? known as
part-of-speech disambiguators (Merialdo, 1994) ?
rely on external dictionaries of possible tags. Our
work builds on two older part-of-speech inducers
? word clustering algorithms of Clark (2000) and
Brown et al (1992) ? that were recently shown to
be more robust than other well-known fully unsuper-
vised techniques (Christodoulopoulos et al, 2010).
We investigate which properties of gold part-of-
speech tags are useful in grammar induction and
parsing, and how these properties could be intro-
duced into induced tags. We also explore the number
of word classes that is good for grammar induction:
in particular, whether categorization is needed at all.
By removing the ?unrealistic simplification? of us-
ing gold tags (Petrov et al, 2011, ?3.2, Footnote 4),
we will go on to demonstrate why grammar induc-
tion from plain text is no longer ?still too difficult.?
1281
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0? ?? ?
PSTOP(, L, T)) ? PATTACH(, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(, L, F)? ?? ?
1
? PSTOP(, R, T)? ?? ?
1
.
Figure 1: A dependency structure for a short WSJ sen-
tence and its probability, factored by the DMV, using gold
tags, after summing out PORDER (Spitkovsky et al, 2009).
2 Methodology
In all experiments, we model the English grammar
via Klein and Manning?s (2004) Dependency Model
with Valence (DMV), induced from subsets of not-
too-long sentences of the Wall Street Journal (WSJ).
2.1 The Model
The original DMV is a single-state head automata
model (Alshawi, 1996) over lexical word classes
{cw} ? gold part-of-speech tags. Its generative story
for a sub-tree rooted at a head (of class ch) rests on
three types of independent decisions: (i) initial di-
rection dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e., ad-
jacent, child); and (iii) attachments (of class ca), ac-
cording to PATTACH(ch, dir, ca). This recursive process
produces only projective trees. A root token ? gen-
erates the head of the sentence as its left (and only)
child (see Figure 1 for a simple, concrete example).
2.2 Learning Algorithms
The DMV lends itself to unsupervised learning via
inside-outside re-estimation (Baker, 1979). Klein
and Manning (2004) initialized their system using an
?ad-hoc harmonic? completion, followed by training
using 40 steps of EM (Klein, 2005). We reproduce
this set-up, iterating without actually verifying con-
vergence, in most of our experiments (#1?4, ?3?4).
Experiments #5?6 (?5) employ our new state-of-
the-art grammar inducer (Spitkovsky et al, 2011),
which uses constrained Viterbi EM (details in ?5).
2.3 Training Data
The DMV is usually trained on a customized sub-
set of Penn English Treebank?s Wall Street Jour-
nal portion (Marcus et al, 1993). Following Klein
and Manning (2004), we begin with reference con-
stituent parses, prune out all empty sub-trees and
remove punctuation and terminals (tagged # and $)
that are not pronounced where they appear. We then
train only on the remaining sentence yields consist-
ing of no more than fifteen tokens (WSJ15), in most
of our experiments (#1?4, ?3?4); by contrast, Klein
and Manning?s (2004) original system was trained
using less data: sentences up to length ten (WSJ10).1
Our final experiments (#5?6, ?5) employ a simple
scaffolding strategy (Spitkovsky et al, 2010a) that
follows up initial training at WSJ15 (?less is more?)
with an additional training run (?leapfrog?) that in-
corporates most sentences of the data set, at WSJ45.
2.4 Evaluation Methods
Evaluation is against the training set, as is standard
practice in unsupervised learning, in part because
Klein and Manning (2004, ?3) did not smooth the
DMV (Klein, 2005, ?6.2). For most of our experi-
ments (#1?4, ?3?4), this entails starting with the ref-
erence trees from WSJ15 (as modified in ?2.3), au-
tomatically converting their labeled constituents into
unlabeled dependencies using deterministic ?head-
percolation? rules (Collins, 1999), and then com-
puting (directed) dependency accuracy scores of the
corresponding induced trees. We report overall per-
centages of correctly guessed arcs, including the
arcs from sentence root symbols, as is standard prac-
tice (Paskin, 2001; Klein and Manning, 2004).
For a meaningful comparison with previous work,
we also test some of the models from our earlier ex-
periments (#1,3) ? and both models from final ex-
periments (#5,6) ? against Section 23 of WSJ?, af-
ter applying Laplace (a.k.a. ?add one?) smoothing.
1WSJ15 contains 15,922 sentences up to length fifteen (a to-
tal of 163,715 tokens, not counting punctuation) ? versus 7,422
sentences of at most ten words (only 52,248 tokens) comprising
WSJ10 ? and is a better trade-off between the quantity and
complexity of training data in WSJ (Spitkovsky et al, 2009).
1282
Accuracy Viable
1. manual tags Unsupervised Sky Groups
gold 50.7 78.0 36
mfc 47.2 74.5 34
mfp 40.4 76.4 160
ua 44.3 78.4 328
2. tagless lexicalized models
full 25.8 97.3 49,180
partial 29.3 60.5 176
none 30.7 24.5 1
3. tags from a flat (Clark, 2000) clustering
47.8 83.8 197
4. prefixes of a hierarchical (Brown et al, 1992) clustering
first 7 bits 46.4 73.9 96
8 bits 48.0 77.8 165
9 bits 46.8 82.3 262
Table 1: Directed accuracies for the ?less is more? DMV,
trained on WSJ15 (after 40 steps of EM) and evaluated
also against WSJ15, using various lexical categories in
place of gold part-of-speech tags. For each tag-set, we
include its effective number of (non-empty) categories in
WSJ15 and the oracle skylines (supervised performance).
3 Motivation and Ablative Analyses
The concepts of polysemy and synonymy are of fun-
damental importance in linguistics. For words that
can take on multiple parts of speech, knowing the
gold tag can reduce ambiguity, improving parsing by
limiting the search space. Furthermore, pooling the
statistics of words that play similar syntactic roles,
as signaled by shared gold part-of-speech tags, can
simplify the learning task, improving generalization
by reducing sparsity. We begin with two sets of ex-
periments that explore the impact that each of these
factors has on grammar induction with the DMV.
3.1 Experiment #1: Human-Annotated Tags
Our first set of experiments attempts to isolate the
effect that replacing gold part-of-speech tags with
deterministic one class per word mappings has on
performance, quantifying the cost of switching to a
monosemous clustering (see Table 1: manual; and
Table 4). Grammar induction with gold tags scores
50.7%, while the oracle skyline (an ideal, supervised
instance of the DMV) could attain 78.0% accuracy.
It may be worth noting that only 6,620 (13.5%) of
49,180 unique tokens in WSJ appear with multiple
part-of-speech tags. Most words, like it, are always
tagged the same way (5,768 times PRP). Some words,
token mfc mfp ua
it {PRP} {PRP} {PRP}
gains {NNS} {VBZ, NNS} {VBZ, NNS}
the {DT} {JJ, DT} {VBP, NNP, NN, JJ, DT, CD}
Table 2: Example most frequent class, most frequent pair
and union all reassignments for tokens it, the and gains.
like gains, usually serve as one part of speech (227
times NNS, as in the gains) but are occasionally used
differently (5 times VBZ, as in he gains). Only 1,322
tokens (2.7%) appear with three or more different
gold tags. However, this minority includes the most
frequent word ? the (50,959 times DT, 7 times JJ,
6 times NNP and once as each of CD, NN and VBP).2
We experimented with three natural reassign-
ments of part-of-speech categories (see Table 2).
The first, most frequent class (mfc), simply maps
each token to its most common gold tag in the entire
WSJ (with ties resolved lexicographically). This ap-
proach discards two gold tags (types PDT and RBR are
not most common for any of the tokens in WSJ15)
and costs about three-and-a-half points of accuracy,
in both supervised and unsupervised regimes.
Another reassignment, union all (ua), maps each
token to the set of all of its observed gold tags, again
in the entire WSJ. This inflates the number of group-
ings by nearly a factor of ten (effectively lexicaliz-
ing the most ambiguous words),3 yet improves the
oracle skyline by half-a-point over actual gold tags;
however, learning is harder with this tag-set, losing
more than six points in unsupervised training.
Our last reassignment, most frequent pair (mfp),
allows up to two of the most common tags into
a token?s label set (with ties, once again, resolved
lexicographically). This intermediate approach per-
forms strictly worse than union all, in both regimes.
3.2 Experiment #2: Lexicalization Baselines
Our next set of experiments assesses the benefits of
categorization, turning to lexicalized baselines that
avoid grouping words altogether. All three models
discussed below estimated the DMV without using
the gold tags in any way (see Table 1: lexicalized).
2Some of these are annotation errors in the treebank (Banko
and Moore, 2004, Figure 2): such (mis)taggings can severely
degrade the accuracy of part-of-speech disambiguators, without
additional supervision (Banko and Moore, 2004, ?5, Table 1).
3Kupiec (1992) found that the 50,000-word vocabulary of
the Brown corpus similarly reduces to ?400 ambiguity classes.
1283
First, not surprisingly, a fully-lexicalized model
over nearly 50,000 unique words is able to essen-
tially memorize the training set, supervised. (With-
out smoothing, it is possible to deterministically at-
tach most rare words in a dependency tree correctly,
etc.) Of course, local search is unlikely to find good
instantiations for so many parameters, causing unsu-
pervised accuracy for this model to drop in half.
For our next experiment, we tried an intermediate,
partially-lexicalized approach. We mapped frequent
words ? those seen at least 100 times in the training
corpus (Headden et al, 2009) ? to their own indi-
vidual categories, lumping the rest into a single ?un-
known? cluster, for a total of under 200 groups. This
model is significantly worse for supervised learn-
ing, compared even with the monosemous clusters
derived from gold tags; yet it is only slightly more
learnable than the broken fully-lexicalized variant.
Finally, for completeness, we trained a model that
maps every token to the same one ?unknown? cat-
egory. As expected, such a trivial ?clustering? is
ineffective in supervised training; however, it out-
performs both lexicalized variants unsupervised,4
strongly suggesting that lexicalization alone may be
insufficient for the DMV and hinting that some de-
gree of categorization is essential to its learnability.
Cluster #173 Cluster #188
1. open 1. get
2. free 2. make
3. further 3. take
4. higher 4. find
5. lower 5. give
6. similar 6. keep
7. leading 7. pay
8. present 8. buy
9. growing 9. win
10. increased 10. sell
.
.
.
.
.
.
37. cool 42. improve
.
.
.
.
.
.
1,688. up-wind 2,105. zero-out
Table 3: Representative members for two of the flat word
groupings: cluster #173 (left) contains adjectives, espe-
cially ones that take comparative (or other) complements;
cluster #188 comprises bare-stem verbs (infinitive stems).
(Of course, many of the words have other syntactic uses.)
4Note that it also beats supervised training. That isn?t a bug:
Spitkovsky et al (2010b, ?7.2) explain this paradox in the DMV.
4 Grammars over Induced Word Clusters
We have demonstrated the need for grouping simi-
lar words, estimated a bound on performance losses
due to monosemous clusterings and are now ready
to experiment with induced part-of-speech tags. We
use two sets of established, publicly-available hard
clustering assignments, each computed from a much
larger data set than WSJ (approximately a million
words). The first is a flat mapping (200 clusters)
constructed by training Clark?s (2000) distributional
similarity model over several hundred million words
from the British National and the English Gigaword
corpora.5 The second is a hierarchical clustering ?
binary strings up to eighteen bits long ? constructed
by running Brown et al?s (1992) algorithm over 43
million words from the BLLIP corpus, minus WSJ.6
4.1 Experiment #3: A Flat Word Clustering
Our main purely unsupervised results are with a flat
clustering (Clark, 2000) that groups words having
similar context distributions, according to Kullback-
Leibler divergence. (A word?s context is an ordered
pair: its left- and right-adjacent neighboring words.)
To avoid overfitting, we employed an implemen-
tation from previous literature (Finkel and Manning,
2009). The number of clusters (200) and the suf-
ficient amount of training data (several hundred-
million words) were tuned to a task (NER) that is
not directly related to dependency parsing. (Table 3
shows representative entries for two of the clusters.)
We added one more category (#0) for unknown
words. Now every token in WSJ could again be re-
placed by a coarse identifier (one of at most 201,
instead of just 36), in both supervised and unsuper-
vised training. (Our training code did not change.)
The resulting supervised model, though not as
good as the fully-lexicalized DMV, was more than
five points more accurate than with gold part-of-
speech tags (see Table 1: flat). Unsupervised accu-
racy was lower than with gold tags (see also Table 4)
but higher than with all three derived hard assign-
ments. This suggests that polysemy (i.e., ability to
5http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
6http://people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
1284
1 4 16 64 256 1,024 (# of clusters) 49,180
20
40
60
80
%
gold
mfc mfp ua
full
partial
none
flat
gold
mfc
mfp
ua
full
partial
none
flat
k = 1 2 3 4 5 6 7 8 9 10 11 12 ? 18 bits
Figure 2: Parsing performance (accuracy on WSJ15) as a ?function? of the number of syntactic categories, for all prefix
lengths ? k ? {1, . . . , 18} ? of a hierarchical (Brown et al, 1992) clustering, connected by solid lines (dependency
grammar induction in blue; supervised oracle skylines in red, above). Tagless lexicalized models (full, partial and
none) connected by dashed lines. Models based on gold part-of-speech tags, and derived monosemous clusters (mfc,
mfp and ua), shown as vertices of gold polygons. Models based on a flat (Clark, 2000) clustering indicated by squares.
tag a word differently in context) may be the primary
advantage of manually constructed categorizations.
4.2 Experiment #4: A Hierarchical Clustering
The purpose of this batch of experiments is to show
that Clark?s (2000) algorithm isn?t unique in its suit-
ability for grammar induction. We found that Brown
et al?s (1992) older information-theoretic approach,
which does not explicitly address the problems of
rare and ambiguous words (Clark, 2000) and was de-
signed to induce large numbers of plausible syntac-
tic and semantic clusters, can perform just as well.
Once again, the sufficient amount of data (43 mil-
lion words) was tuned in earlier work (Koo, 2010).
His task of interest was, in fact, dependency parsing.
But since this algorithm is hierarchical (i.e., there
isn?t a parameter for the number of categories), we
doubt that there was a strong enough risk of overfit-
ting to question the clustering?s unsupervised nature.
As there isn?t a set number of categories, we used
binary prefixes of length k from each word?s address
in the computed hierarchy as cluster labels. Results
for 7 ? k ? 9 bits (approximately 100?250 non-
empty clusters, close to the 200 we used before) are
similar to those of flat clusters (see Table 1: hierar-
chical). Outside of this range, however, performance
can be substantially worse (see Figure 2), consistent
with earlier findings: Headden et al (2008) demon-
strated that (constituent) grammar induction, using
the singular-value decomposition (SVD-based) tag-
ger of Schu?tze (1995), also works best with 100?200
clusters. Important future research directions may
include learning to automatically select a good num-
ber of word categories (in the case of flat clusterings)
and ways of using multiple clustering assignments,
perhaps of different granularities/resolutions, in tan-
dem (e.g., in the case of a hierarchical clustering).
4.3 Further Evaluation
It is important to enable easy comparison with pre-
vious and future work. Since WSJ15 is not a stan-
dard test set, we evaluated two key experiments ?
?less is more? with gold part-of-speech tags (#1, Ta-
ble 1: gold) and with Clark?s (2000) clusters (#3, Ta-
ble 1: flat) ? on all sentences (not just length fifteen
and shorter), in Section 23 of WSJ (see Table 4).
This required smoothing both final models (?2.4).
We showed that two classic unsupervised word
1285
System Description Accuracy
#1 (?3.1) ?less is more? (Spitkovsky et al, 2009) 44.0
#3 (?4.1) ?less is more? with monosemous induced tags 41.4 (-2.6)
Table 4: Directed accuracies on Section 23 of WSJ (all sentences) for two experiments with the base system.
clusterings ? one flat and one hierarchical ? can
be better for dependency grammar induction than
monosemous syntactic categories derived from gold
part-of-speech tags. And we confirmed that the un-
supervised tags are worse than the actual gold tags,
in a simple dependency grammar induction system.
5 State-of-the-Art without Gold Tags
Until now, we have deliberately kept our experimen-
tal methods simple and nearly identical to Klein and
Manning?s (2004), for clarity. Next, we will explore
how our main findings generalize beyond this toy
setting. A preliminary test will simply quantify the
effect of replacing gold part-of-speech tags with the
monosemous flat clustering (as in experiment #3,
?4.1) on a modern grammar inducer. And our last
experiment will gauge the impact of using a polyse-
mous (but still unsupervised) clustering instead, ob-
tained by executing standard sequence labeling tech-
niques to introduce context-sensitivity into the origi-
nal (independent) assignment or words to categories.
These final experiments are with our latest state-
of-the-art system (Spitkovsky et al, 2011) ? a par-
tially lexicalized extension of the DMV that uses
constrained Viterbi EM to train on nearly all of the
data available in WSJ, at WSJ45 (48,418 sentences;
986,830 non-punctuation tokens). The key contribu-
tion that differentiates this model from its predeces-
sors is that it incorporates punctuation into grammar
induction (by turning it into parsing constraints, in-
stead of ignoring punctuation marks altogether). In
training, the model makes a simplifying assumption
? that sentences can be split at punctuation and that
the resulting fragments of text could be parsed inde-
pendently of one another (these parsed fragments are
then reassembled into full sentence trees, by pars-
ing the sequence of their own head words). Fur-
thermore, the model continues to take punctuation
marks into account in inference (using weaker, more
accurate constraints, than in training). This system
scores 58.4% on Section 23 of WSJ? (see Table 5).
5.1 Experiment #5: A Monosemous Clustering
As in experiment #3 (?4.1), we modified the base
system in exactly one way: we swapped out gold
part-of-speech tags and replaced them with a flat dis-
tributional similarity clustering. In contrast to sim-
pler models, which suffer multi-point drops in ac-
curacy from switching to unsupervised tags (e.g.,
2.6%), our new system?s performance degrades only
slightly, by 0.2% (see Tables 4 and 5). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).7
One risk that arises from using gold tags is that
newer systems could be finding cleverer ways to ex-
ploit manual labels (i.e., developing an over-reliance
on gold tags) instead of actually learning to acquire
language. Part-of-speech tags are known to contain
significant amounts of information for unlabeled de-
pendency parsing (McDonald et al, 2011, ?3.1), so
we find it reassuring that our latest grammar inducer
is less dependent on gold tags than its predecessors.
5.2 Experiment #6: A Polysemous Clustering
Results of experiments #1 and 3 (?3.1, 4.1) suggest
that grammar induction stands to gain from relaxing
the one class per word assumption. We next test this
conjecture by inducing a polysemous unsupervised
word clustering, then using it to induce a grammar.
Previous work (Headden et al, 2008, ?4) found
that simple bitag hidden Markov models, classically
trained using the Baum-Welch (Baum, 1972) variant
of EM (HMM-EM), perform quite well,8 on aver-
age, across different grammar induction tasks. Such
sequence models incorporate a sensitivity to context
via state transition probabilities PTRAN(ti | ti?1), cap-
turing the likelihood that a tag ti immediately fol-
lows the tag ti?1; emission probabilities PEMIT(wi | ti)
capture the likelihood that a word of type ti is wi.
7We also briefly comment on this result in the ?punctuation?
paper (Spitkovsky et al, 2011, ?7), published concurrently.
8They are also competitive with Bayesian estimators, on
larger data sets, with cross-validation (Gao and Johnson, 2008).
1286
System Description Accuracy
(?5) ?punctuation? (Spitkovsky et al, 2011) 58.4
#5 (?5.1) ?punctuation? with monosemous induced tags 58.2 (-0.2)
#6 (?5.2) ?punctuation? with context-sensitive induced tags 59.1 (+0.7)
Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system.
We need a context-sensitive tagger, and HMM
models are good ? relative to other tag-inducers.
However, they are not better than gold tags, at least
when trained using a modest amount of data.9 For
this reason, we decided to relax the monosemous
flat clustering, plugging it in as an initializer for the
HMM. The main problem with this approach is that,
at least without smoothing, every monosemous la-
beling is trivially at a local optimum, since P(ti | wi)
is deterministic. To escape the initial assignment,
we used a ?noise injection? technique (Selman et
al., 1994), inspired by the contexts of Clark (2000).
First, we collected the MLE statistics for PR(ti+1 | ti)
and PL(ti | ti+1) in WSJ, using the flat monosemous
tags. Next, we replicated the text of WSJ 100-fold.
Finally, we retagged this larger data set, as follows:
with probability 80%, a word kept its monosemous
tag; with probability 10%, we sampled a new tag
from the left context (PL) associated with the origi-
nal (monosemous) tag of its rightmost neighbor; and
with probability 10%, we drew a tag from the right
context (PR) of its leftmost neighbor.10 Given that
our initializer ? and later the input to the grammar
inducer ? are hard assignments of tags to words, we
opted for (the faster and simpler) Viterbi training.
In the spirit of reproducibility, we again used an
off-the-shelf component for tagging-related work.11
Viterbi training converged after just 17 steps, re-
placing the original monosemous tags for 22,280 (of
1,028,348 non-punctuation) tokens in WSJ. For ex-
9All of Headden et al?s (2008) grammar induction experi-
ments with induced parts-of-speech were worse than their best
results using gold part-of-speech tags, most likely because they
used a very small corpus (half of WSJ10) to cluster words.
10We chose the sampling split (80:10:10) and replication pa-
rameter (100) somewhat arbitrarily, so better results could likely
be obtained with tuning. However, we suspect that the real gains
would come from using soft clustering techniques (Hinton and
Roweis, 2003; Pereira et al, 1993, inter alia) and propagating
(joint) estimates of tag distributions into a parser. Our ad-hoc
approach is intended to serve solely as a proof of concept.
11David Elworthy?s C+ tagger, with options -i t -G -l,
available from http://friendly-moose.appspot.com/
code/NewCpTag.zip.
ample, the first changed sentence is #3 (of 49,208):
Some ?circuit breakers? installed after
the October 1987 crash failed their first
test, traders say, unable to cool the selling
panic in both stocks and futures.
Above, the word cool gets relabeled as #188 (from
#173 ? see Table 3), since its context is more
suggestive of an infinitive verb than of its usual
grouping with adjectives. (A proper analysis of all
changes, however, is beyond the scope of this work.)
Using this new context-sensitive hard assignment
of tokens to unsupervised categories our gram-
mar inducer attained a directed accuracy of 59.1%,
nearly a full point better than with the monosemous
hard assignment (see Table 5). To the best of our
knowledge it is also the first state-of-the-art unsuper-
vised dependency parser to perform better with in-
duced categories than with gold part-of-speech tags.
6 Related Work
Early work in dependency grammar induction al-
ready relied on gold part-of-speech tags (Carroll and
Charniak, 1992). Some later models (Yuret, 1998;
Paskin, 2001, inter alia) attempted full lexicaliza-
tion. However, Klein and Manning (2004) demon-
strated that effort to be worse at recovering depen-
dency arcs than choosing parse structures at random,
leading them to incorporate gold tags into the DMV.
Klein and Manning (2004, ?5, Figure 6) had also
tested their own models with induced word classes,
constructed using a distributional similarity cluster-
ing method (Schu?tze, 1995). Without gold part-of-
speech tags, their combined DMV+CCM model was
about five points worse, both in (directed) unlabeled
dependency accuracy (42.3% vs. 47.5%)12 and unla-
beled bracketing F1 (72.9% vs. 77.6%), on WSJ10.
In constituent parsing, earlier Seginer (2007a, ?6,
Table 1) built a fully-lexicalized grammar inducer
12On the same evaluation set (WSJ10), our context-sensitive
system without gold tags (Experiment #6, ?5.2) scores 66.8%.
1287
that was competitive with DMV+CCM despite not
using gold tags. His CCL parser has since been
improved via a ?zoomed learning? technique (Re-
ichart and Rappoport, 2010). Moreover, Abend et
al. (2010) reused CCL?s internal distributional rep-
resentation of words in a cognitively-motivated part-
of-speech inducer. Unfortunately their tagger did
not make it into Christodoulopoulos et al?s (2010)
excellent and otherwise comprehensive evaluation.
Outside monolingual grammar induction, fully-
lexicalized statistical dependency transduction mod-
els have been trained from unannotated parallel bi-
texts for machine translation (Alshawi et al, 2000).
More recently, McDonald et al (2011) demonstrated
an impressive alternative to grammar induction by
projecting reference parse trees from languages that
have annotations to ones that are resource-poor.13 It
uses graph-based label propagation over a bilingual
similarity graph for a sentence-aligned parallel cor-
pus (Das and Petrov, 2011), inducing part-of-speech
tags from a universal tag-set (Petrov et al, 2011).
Even in supervised parsing we are starting to see
a shift away from using gold tags. For example,
Alshawi et al (2011) demonstrated good results for
mapping text to underspecified semantics via depen-
dencies without resorting to gold tags. And Petrov et
al. (2010, ?4.4, Table 4) observed only a small per-
formance loss ?going POS-less? in question parsing.
We are not aware of any systems that induce both
syntactic trees and their part-of-speech categories.
However, aside from the many systems that induce
trees from gold tags, there are also unsupervised
methods for inducing syntactic categories from gold
trees (Finkel et al, 2007; Pereira et al, 1993), as
well as for inducing dependencies from gold con-
stituent annotations (Sangati and Zuidema, 2009;
Chiang and Bikel, 2002). Considering that Headden
et al?s (2008) study of part-of-speech taggers found
no correlation between standard tagging metrics and
the quality of induced grammars, it may be time for
a unified treatment of these very related syntax tasks.
13When the target language is English, however, their best ac-
curacy (projected from Greek) is low: 45.7% (McDonald et al,
2011, ?4, Table 2); tested on the same CoNLL 2007 evaluation
set (Nivre et al, 2007), our ?punctuation? system with context-
sensitive induced tags (trained on WSJ45, without gold tags)
performs substantially better, scoring 51.6%. Note that this is
also an improvement over our system trained on the CoNLL set
using gold tags: 50.3% (Spitkovsky et al, 2011, ?8, Table 6).
7 Discussion and Conclusions
Unsupervised word clustering techniques of Brown
et al (1992) and Clark (2000) are well-suited to de-
pendency parsing with the DMV. Both methods out-
perform gold parts-of-speech in supervised modes.
And both can do better than monosemous clusters
derived from gold tags in unsupervised training. We
showed how Clark?s (2000) flat tags can be relaxed,
using context, with the resulting polysemous cluster-
ing outperforming gold part-of-speech tags for the
English dependency grammar induction task.
Monolingual evaluation is a significant flaw in our
methodology, however. One (of many) take-home
points made in Christodoulopoulos et al?s (2010)
study is that results on one language do not neces-
sarily correlate with other languages.14 Assuming
that our results do generalize, it will still remain to
remove the present reliance on gold tokenization and
sentence boundary labels. Nevertheless, we feel that
eliminating gold tags is an important step towards
the goal of fully-unsupervised dependency parsing.
We have cast the utility of a categorization scheme
as a combination of two effects on parsing accuracy:
a synonymy effect and a polysemy effect. Results
of our experiments with both full and partial lexi-
calization suggest that grouping similar words (i.e.,
synonymy) is vital to grammar induction with the
DMV. This is consistent with an established view-
point, that simple tabulation of frequencies of words
participating in certain configurations cannot be reli-
ably used for comparing their likelihoods (Pereira et
al., 1993, ?4.2): ?The statistics of natural languages
is inherently ill defined. Because of Zipf?s law, there
is never enough data for a reasonable estimation of
joint object distributions.? Seginer?s (2007b, ?1.4.4)
argument, however, is that the Zipfian distribution
? a property of words, not parts-of-speech ?
should allow frequent words to successfully guide
14Furthermore, it would be interesting to know how sensitive
different head-percolation schemes (Yamada and Matsumoto,
2003; Johansson and Nugues, 2007) would be to gold versus
unsupervised tags, since the Magerman-Collins rules (Mager-
man, 1995; Collins, 1999) agree with gold dependency annota-
tions only 85% of the time, even for WSJ (Sangati and Zuidema,
2009). Proper intrinsic evaluation of dependency grammar in-
ducers is not yet a solved problem (Schwartz et al, 2011).
1288
parsing and learning: ?A relatively small number of
frequent words appears almost everywhere and most
words are never too far from such a frequent word
(this is also the principle behind successful part-of-
speech induction).? We believe that it is important to
thoroughly understand how to reconcile these only
seemingly conflicting insights, balancing them both
in theory and in practice. A useful starting point may
be to incorporate frequency information in the pars-
ing models directly ? in particular, capturing the
relationships between words of various frequencies.
The polysemy effect appears smaller but is less
controversial: Our experiments suggest that the pri-
mary drawback of the classic clustering schemes
stems from their one class per word nature ? and
not a lack of supervision, as may be widely believed.
Monosemous groupings, even if they are themselves
derived from human-annotated syntactic categories,
simply cannot disambiguate words the way gold tags
can. By relaxing Clark?s (2000) flat clustering, us-
ing contextual cues, we improved dependency gram-
mar induction: directed accuracy on Section 23 (all
sentences) of the WSJ benchmark increased from
58.2% to 59.1% ? from slightly worse to better than
with gold tags (58.4%, previous state-of-the-art).
Since Clark?s (2000) word clustering algorithm is
already context-sensitive in training, we suspect that
one could do better simply by preserving the polyse-
mous nature of its internal representation. Importing
the relevant distributions into a sequence tagger di-
rectly would make more sense than going through an
intermediate monosemous summary. And exploring
other uses of soft clustering algorithms ? perhaps as
inputs to part-of-speech disambiguators ? may be
another fruitful research direction. We believe that
a joint treatment of grammar and parts-of-speech in-
duction could fuel major advances in both tasks.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Spence Green,
David McClosky and the anonymous reviewers for many help-
ful comments on draft versions of this paper.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In ACL.
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26.
H. Alshawi, P.-C. Chang, and M. Ringgaard. 2011. De-
terministic statistical mapping of sentences to under-
specied semantics. In IWCS.
H. Alshawi. 1996. Head automata for speech translation.
In ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M. Banko and R. C. Moore. 2004. Part of speech tagging
in context. In COLING.
L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. In Inequalities.
R. Bod. 2006. An all-subtrees approach to unsupervised
parsing. In COLING-ACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
D. Chiang and D. M. Bikel. 2002. Recovering latent
information in treebanks. In COLING.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL: Student Research.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In ACL.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In EMNLP.
1289
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor
embedding. In NIPS.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
T. Koo. 2010. Advances in Discriminative Dependency
Parsing. Ph.D. thesis, MIT.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden Markov model. Computer Speech and Lan-
guage, 6.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In ACL.
S. Petrov, P.-C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv.
R. Reichart and A. Rappoport. 2010. Improved fully un-
supervised parsing with zoomed learning. In EMNLP.
F. Sangati and W. Zuidema. 2009. Unsupervised meth-
ods for head assignments. In EACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
Y. Seginer. 2007a. Fast unsupervised incremental pars-
ing. In ACL.
Y. Seginer. 2007b. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise
strategies for improving local search. In AAAI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More? in
unsupervised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011.
Punctuation: Making a point in unsupervised depen-
dency parsing. In CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
D. Yuret. 1998. Discovery of Linguistic Relations Using
Lexical Attraction. Ph.D. thesis, MIT.
1290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 688?698, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Three Dependency-and-Boundary Models for Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present a new family of models for unsu-
pervised parsing, Dependency and Boundary
models, that use cues at constituent bound-
aries to inform head-outward dependency tree
generation. We build on three intuitions that
are explicit in phrase-structure grammars but
only implicit in standard dependency formu-
lations: (i) Distributions of words that oc-
cur at sentence boundaries ? such as English
determiners ? resemble constituent edges.
(ii) Punctuation at sentence boundaries fur-
ther helps distinguish full sentences from
fragments like headlines and titles, allow-
ing us to model grammatical differences be-
tween complete and incomplete sentences.
(iii) Sentence-internal punctuation boundaries
help with longer-distance dependencies, since
punctuation correlates with constituent edges.
Our models induce state-of-the-art depen-
dency grammars for many languages without
special knowledge of optimal input sentence
lengths or biased, manually-tuned initializers.
1 Introduction
Natural language is ripe with all manner of bound-
aries at the surface level that align with hierarchical
syntactic structure. From the significance of func-
tion words (Berant et al2006) and punctuation
marks (Seginer, 2007; Ponvert et al2010) as sepa-
rators between constituents in longer sentences ? to
the importance of isolated words in children?s early
vocabulary acquisition (Brent and Siskind, 2001)
? word boundaries play a crucial role in language
learning. We will show that boundary information
can also be useful in dependency grammar induc-
tion models, which traditionally focus on head rather
than fringe words (Carroll and Charniak, 1992).
DT NN VBZ IN DT NN
[The check] is in [the mail].
? ?? ?
Subject
? ?? ?
Object
Figure 1: A partial analysis of our running example.
Consider the example in Figure 1. Because the
determiner (DT) appears at the left edge of the sen-
tence, it should be possible to learn that determiners
may generally be present at left edges of phrases.
This information could then be used to correctly
parse the sentence-internal determiner in the mail.
Similarly, the fact that the noun head (NN) of the ob-
ject the mail appears at the right edge of the sentence
could help identify the noun check as the right edge
of the subject NP. As with jigsaw puzzles, working
inwards from boundaries helps determine sentence-
internal structures of both noun phrases, neither of
which would be quite so clear if viewed separately.
Furthermore, properties of noun-phrase edges are
partially shared with prepositional- and verb-phrase
units that contain these nouns. Because typical head-
driven grammars model valence separately for each
class of head, however, they cannot see that the left
fringe boundary, The check, of the verb-phrase is
shared with its daughter?s, check. Neither of these
insights is available to traditional dependency for-
mulations, which could learn from the boundaries
of this sentence only that determiners might have no
left- and that nouns might have no right-dependents.
We propose a family of dependency parsing mod-
els that are capable of inducing longer-range im-
plications from sentence edges than just fertilities
of their fringe words. Our ideas conveniently lend
themselves to implementations that can reuse much
of the standard grammar induction machinery, in-
cluding efficient dynamic programming routines for
the relevant expectation-maximization algorithms.
688
2 The Dependency and Boundary Models
Our models follow a standard generative story for
head-outward automata (Alshawi, 1996a), restricted
to the split-head case (see below),1 over lexical word
classes {cw}: first, a sentence root cr is chosen, with
probability PATTACH(cr | ?; L); ? is a special start
symbol that, by convention (Klein and Manning,
2004; Eisner, 1996), produces exactly one child, to
its left. Next, the process recurses. Each (head)
word ch generates a left-dependent with probability
1 ? PSTOP( ? | L; ? ? ? ), where dots represent additional
parameterization on which it may be conditioned. If
the child is indeed generated, its identity cd is cho-
sen with probability PATTACH(cd | ch; ? ? ? ), influenced
by the identity of the parent ch and possibly other pa-
rameters (again represented by dots). The child then
generates its own subtree recursively and the whole
process continues, moving away from the head, un-
til ch fails to generate a left-dependent. At that point,
an analogous procedure is repeated to ch?s right, this
time using stopping factors PSTOP( ? | R; ? ? ? ). All parse
trees derived in this way are guaranteed to be projec-
tive and can be described by split-head grammars.
Instances of these split-head automata have been
heavily used in grammar induction (Paskin, 2001b;
Klein and Manning, 2004; Headden et al2009,
inter alia), in part because they allow for efficient
implementations (Eisner and Satta, 1999, ?8) of
the inside-outside re-estimation algorithm (Baker,
1979). The basic tenet of split-head grammars is
that every head word generates its left-dependents
independently of its right-dependents. This as-
sumption implies, for instance, that words? left-
and right-valences ? their numbers of children
to each side ? are also independent. But it does
not imply that descendants that are closer to the
head cannot influence the generation of farther
dependents on the same side. Nevertheless, many
popular grammars for unsupervised parsing behave
as if a word had to generate all of its children
(to one side) ? or at least their count ? before
allowing any of these children themselves to recurse.
For example, Klein and Manning?s (2004) depen-
dency model with valence (DMV) could be imple-
1Unrestricted head-outward automata are strictly more pow-
erful (e.g., they recognize the language anbn in finite state) than
the split-head variants, which process one side before the other.
mented as both head-outward and head-inward au-
tomata. (In fact, arbitrary permutations of siblings
to a given side of their parent would not affect the
likelihood of the modified tree, with the DMV.) We
propose to make fuller use of split-head automata?s
head-outward nature by drawing on information in
partially-generated parses, which contain useful pre-
dictors that, until now, had not been exploited even
in featurized systems for grammar induction (Cohen
and Smith, 2009; Berg-Kirkpatrick et al2010).
Some of these predictors, including the identity
? or even number (McClosky, 2008) ? of already-
generated siblings, can be prohibitively expensive in
sentences above a short length k. For example, they
break certain modularity constraints imposed by the
charts used in O(k3)-optimized algorithms (Paskin,
2001a; Eisner, 2000). However, in bottom-up pars-
ing and training from text, everything about the yield
? i.e., the ordered sequence of all already-generated
descendants, on the side of the head that is in the
process of spawning off an additional child ? is not
only known but also readily accessible. Taking ad-
vantage of this availability, we designed three new
models for dependency grammar induction.
2.1 Dependency and Boundary Model One
DBM-1 conditions all stopping decisions on adja-
cency and the identity of the fringe word ce ? the
currently-farthest descendant (edge) derived by head
ch in the given head-outward direction (dir ? {L, R}):
PSTOP( ? | dir; adj, ce).
In the adjacent case (adj = T), ch is deciding whether
to have any children on a given side: a first child?s
subtree would be right next to the head, so the head
and the fringe words coincide (ch = ce). In the non-
adjacent case (adj = F), these will be different words
and their classes will, in general, not be the same.2
Thus, non-adjacent stopping decisions will be made
independently of a head word?s identity. Therefore,
all word classes will be equally likely to continue to
grow or not, for a specific proposed fringe boundary.
For example, production of The check is involves
two non-adjacent stopping decisions on the left: one
by the noun check and one by the verb is, both of
which stop after generating a first child. In DBM-1,
2Fringe words differ also from other standard dependency
features (Eisner, 1996, ?2.3): parse siblings and adjacent words.
689
DT NN VBZ IN DT NN ?
The check is in the mail .
P = (1?
0
? ?? ?
PSTOP(? | L; T)) ? PATTACH(VBZ | ?; L)
? (1? PSTOP( ? | L; T, VBZ)) ? PATTACH(NN | VBZ; L)
? (1? PSTOP( ? | R; T, VBZ)) ? PATTACH(IN | VBZ; R)
? PSTOP( ? | L; F, DT) // VBZ ? PSTOP( ? | R; F, NN) // VBZ
? (1? PSTOP( ? | L; T, NN))2 ? P2ATTACH(DT | NN; L)
? (1? PSTOP( ? | R; T, IN)) ? PATTACH(NN | IN; R)
? P2STOP( ? | R; T, NN) ? P2STOP( ? | L; F, DT) // NN
? PSTOP( ? | L; T, IN) ? PSTOP( ? | R; F, NN) // IN
? P2STOP( ? | L; T, DT) ? P2STOP( ? | R; T, DT)
? PSTOP(? | L; F)
? ?? ?
1
? PSTOP(? | R; T)
? ?? ?
1
.
Figure 2: Our running example ? a simple sentence and
its unlabeled dependency parse structure?s probability, as
factored by DBM-1; highlighted comments specify heads
associated to non-adjacent stopping probability factors.
this outcome is captured by squaring a shared pa-
rameter belonging to the left-fringe determiner The:
PSTOP( ? | L; F, DT)2 ? instead of by a product of two
factors, such as PSTOP( ? | L; F, NN) ? PSTOP( ? | L; F, VBZ).
In these grammars, dependents? attachment prob-
abilities, given heads, are additionally conditioned
only on their relative positions ? as in traditional
models (Klein and Manning, 2004; Paskin, 2001b):
PATTACH(cd | ch; dir).
Figure 2 shows a completely factored example.
2.2 Dependency and Boundary Model Two
DBM-2 allows different but related grammars to co-
exist in a single model. Specifically, we presuppose
that all sentences are assigned to one of two classes:
complete and incomplete (comp ? {T, F}, for now
taken as exogenous). This model assumes that word-
word (i.e., head-dependent) interactions in the two
domains are the same. However, sentence lengths
? for which stopping probabilities are responsible
? and distributions of root words may be different.
Consequently, an additional comp parameter is
added to the context of two relevant types of factors:
PSTOP( ? | dir; adj, ce, comp);
and PATTACH(cr | ?; L, comp).
For example, the new stopping factors could capture
the fact that incomplete fragments ? such as the
noun-phrases George Morton, headlines Energy and
Odds and Ends, a line item c - Domestic car, dollar
quantity Revenue: $3.57 billion, the time 1:11am,
and the like ? tend to be much shorter than com-
plete sentences. The new root-attachment factors
could further track that incomplete sentences gener-
ally lack verbs, in contrast to other short sentences,
e.g., Excerpts follow:, Are you kidding?, Yes, he
did., It?s huge., Indeed it is., I said, ?NOW??, ?Ab-
solutely,? he said., I am waiting., Mrs. Yeargin de-
clined., McGraw-Hill was outraged., ?It happens.?,
I?m OK, Jack., Who cares?, Never mind. and so on.
All other attachment probabilities PATTACH(cd | ch; dir)
remain unchanged, as in DBM-1. In practice, comp
can indicate presence of sentence-final punctuation.
2.3 Dependency and Boundary Model Three
DBM-3 adds further conditioning on punctuation
context. We introduce another boolean parameter,
cross, which indicates the presence of intervening
punctuation between a proposed head word ch and
its dependent cd. Using this information, longer-
distance punctuation-crossing arcs can be modeled
separately from other, lower-level dependencies, via
PATTACH(cd | ch; dir, cross).
For instance, in Continentals believe that the
strongest growth area will be southern Europe., four
words appear between that and will. Conditioning
on (the absence of) intervening punctuation could
help tell true long-distance relations from impostors.
All other probabilities, PSTOP( ? | dir; adj, ce, comp) and
PATTACH(cr | ?; L, comp), remain the same as in DBM-2.
2.4 Summary of DBMs and Related Models
Head-outward automata (Alshawi, 1996a; Alshawi,
1996b; Alshawi et al2000) played a central part as
generative models for probabilistic grammars, start-
ing with their early adoption in supervised split-head
constituent parsers (Collins, 1997; Collins, 2003).
Table 1 lists some parameterizations that have since
been used by unsupervised dependency grammar in-
ducers sharing their backbone split-head process.
3 Experimental Set-Up and Methodology
We first motivate each model by analyzing the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al1993),3 before delving into
3We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
690
Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not)
GB (Paskin, 2001b) 1 / |{w}| d | h; dir 1 / 2
DMV (Klein and Manning, 2004) cr | ?; L cd | ch; dir ? | dir; adj, ch
EVG (Headden et al2009) cr | ?; L cd | ch; dir, adj ? | dir; adj, ch
DBM-1 (?2.1) cr | ?; L cd | ch; dir ? | dir; adj, ce
DBM-2 (?2.2) cr | ?; L, comp cd | ch; dir ? | dir; adj, ce, comp
DBM-3 (?2.3) cr | ?; L, comp cd | ch; dir, cross ? | dir; adj, ce, comp
Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.
grammar induction experiments. Although motivat-
ing solely from this treebank biases our discussion
towards a very specific genre of just one language,
it has the advantage of allowing us to make concrete
claims that are backed up by significant statistics.
In the grammar induction experiments that follow,
we will test each model?s incremental contribution
to accuracies empirically, across many disparate lan-
guages. We worked with all 23 (disjoint) train/test
splits from the 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al2007), span-
ning 19 languages.4 For each data set, we induced
a baseline grammar using the DMV. We excluded
all training sentences with more than 15 tokens to
create a conservative bias, because in this set-up the
baseline is known to excel (Spitkovsky et al2009).
Grammar inducers were initialized using (the same)
uniformly-at-random chosen parse trees of training
sentences (Cohen and Smith, 2010); thereafter, we
applied ?add one? smoothing at every training step.
To fairly compare the models under considera-
tion ? which could have quite different starting
perplexities and ensuing consecutive relative like-
lihoods ? we experimented with two termination
strategies. In one case, we blindly ran each learner
through 40 steps of inside-outside re-estimation, ig-
noring any convergence criteria; in the other case,
we ran until numerical convergence of soft EM?s ob-
jective function or until the likelihood of resulting
Viterbi parse trees suffered ? an ?early-stopping la-
teen EM? strategy (Spitkovsky et al2011a, ?2.3).
We evaluated against all sentences of the blind test
sets (except one 145-token item in Arabic ?07 data).
Table 2 shows experimental results, averaged over
1999), discarding any empty nodes, etc., as is standard practice.
4We did not test on WSJ data because such evaluation would
not be blind, as parse trees from the PTB are our motivating ex-
amples; instead, performance on WSJ serves as a strong base-
line in a separate study (Spitkovsky et al2012a): bootstrapping
of DBMs from mostly incomplete inter-punctuation fragments.
all 19 languages, for the DMV baselines and DBM-1
and 2. We did not test DBM-3 in this set-up because
most sentence-internal punctuation occurs in longer
sentences; instead, DBM-3 will be tested later (see
?7), using most sentences,5 in the final training step
of a curriculum strategy (Bengio et al2009) that we
will propose for DBMs. For the three models tested
on shorter inputs (up to 15 tokens) both terminating
criteria exhibited the same trend; lateen EM consis-
tently scored slightly higher than 40 EM iterations.
Termination Criterion DMV DBM-1 DBM-2
40 steps of EM 33.5 38.8 40.7
early-stopping lateen EM 34.0 39.0 40.9
Table 2: Directed dependency accuracies, averaged over
all 2006/7 CoNLL evaluation sets (all sentences), for the
DMV and two new dependency-and-boundary grammar
inducers (DBM-1,2) ? using two termination strategies.6
4 Dependency and Boundary Model One
The primary difference between DBM-1 and tradi-
tional models, such as the DMV, is that DBM-1 con-
ditions non-adjacent stopping decisions on the iden-
tities of fringe words in partial yields (see ?2.1).
4.1 Analytical Motivation
Treebank data suggests that the class of the fringe
word ? its part-of-speech, ce ? is a better predic-
tor of (non-adjacent) stopping decisions, in a given
direction dir, than the head?s own class ch. A statis-
tical analysis of logistic regressions fitted to the data
shows that the (ch, dir) predictor explains only about
7% of the total variation (see Table 3). This seems
low, although it is much better compared to direction
alone (which explains less than 2%) and slightly bet-
ter than using the (current) number of the head?s de-
5Results for DBM-3 ? given only standard input sentences,
up to length fifteen ? would be nearly identical to DBM-2?s.
6We down-weighed the four languages appearing in both
CoNLL years (see Table 8) by 50% in all reported averages.
691
Non-Adjacent Stop Predictor R2adj AICc
(dir) 0.0149 1,120,200
(n, dir) 0.0726 1,049,175
(ch, dir) 0.0728 1,047,157
(ce, dir) 0.2361 904,102.4
(ch, ce, dir) 0.3320 789,594.3
Table 3: Coefficients of determination (R2) and Akaike
information criteria (AIC), both adjusted for the number
of parameters, for several single-predictor logistic models
of non-adjacent stops, given direction dir; ch is the class
of the head, n is its number of descendants (so far) to that
side, and ce represents the farthest descendant (the edge).
scendants on that side, n, instead of the head?s class.
In contrast, using ce in place of ch boosts explanatory
power to 24%, keeping the number of parameters the
same. If one were willing to roughly square the size
of the model, explanatory power could be improved
further, to 33% (see Table 3), using both ce and ch.
Fringe boundaries thus appear to be informative
even in the supervised case, which is not surprising,
since using just one probability factor (and its com-
plement) to generate very short (geometric coin-flip)
sequences is a recipe for high entropy. But as sug-
gested earlier, fringes should be extra attractive in
unsupervised settings because yields are observable,
whereas heads almost always remain hidden. More-
over, every sentence exposes two true edges (Ha?nig,
2010): integrated over many sample sentence begin-
nings and ends, cumulative knowledge about such
markers can guide a grammar inducer inside long in-
puts, where structure is murky. Table 4 shows distri-
butions of all part-of-speech (POS) tags in the tree-
bank versus in sentence-initial, sentence-final and
sentence-root positions. WSJ often leads with deter-
miners, proper nouns, prepositions and pronouns ?
all good candidates for starting English phrases; and
its sentences usually end with various noun types,
again consistent with our running example.
4.2 Experimental Results
Table 2 shows DBM-1 to be substantially more ac-
curate than the DMV, on average: 38.8 versus 33.5%
after 40 steps of EM.7 Lateen termination improved
both models? accuracies slightly, to 39.0 and 34.0%,
respectively, with DBM-1 scoring five points higher.
7DBM-1?s 39% average accuracy with uniform-at-random
initialization is two points above DMV?s scores with the ?ad-
hoc harmonic? strategy, 37% (Spitkovsky et al2011a, Table 5).
% of All First Last Sent. Frag.
POS Tokens Tokens Tokens Roots Roots
NN 15.94 4.31 36.67 0.10 23.40
IN 11.85 13.54 0.57 0.24 4.33
NNP 11.09 20.49 12.85 0.02 32.02
DT 9.84 23.34 0.34 0.00 0.04
JJ 7.32 4.33 3.74 0.01 1.15
NNS 7.19 4.49 20.64 0.15 17.12
CD 4.37 1.29 6.92 0.00 3.27
RB 3.71 5.96 3.88 0.00 1.50
VBD 3.65 0.09 3.52 46.65 0.93
VB 3.17 0.44 1.67 0.48 6.81
CC 2.86 5.93 0.00 0.00 0.00
TO 2.67 0.37 0.05 0.02 0.44
VBZ 2.57 0.17 1.65 28.31 0.93
VBN 2.42 0.61 2.57 0.65 1.28
PRP 2.08 9.04 1.34 0.00 0.00
VBG 1.77 1.26 0.64 0.10 0.97
VBP 1.50 0.05 0.61 14.33 0.71
MD 1.17 0.07 0.05 8.88 0.57
POS 1.05 0.00 0.11 0.01 0.04
PRP$ 1.00 0.90 0.00 0.00 0.00
WDT 0.52 0.08 0.00 0.01 0.13
JJR 0.39 0.18 0.43 0.00 0.09
RP 0.32 0.00 0.42 0.00 0.00
NNPS 0.30 0.20 0.56 0.00 2.96
WP 0.28 0.42 0.01 0.01 0.04
WRB 0.26 0.78 0.02 0.01 0.31
JJS 0.23 0.27 0.06 0.00 0.00
RBR 0.21 0.20 0.54 0.00 0.04
EX 0.10 0.75 0.00 0.00 0.00
RBS 0.05 0.06 0.01 0.00 0.00
PDT 0.04 0.08 0.00 0.00 0.00
FW 0.03 0.01 0.05 0.00 0.09
WP$ 0.02 0.00 0.00 0.00 0.00
UH 0.01 0.08 0.05 0.00 0.62
SYM 0.01 0.11 0.01 0.00 0.18
LS 0.01 0.09 0.00 0.00 0.00
Table 4: Empirical distributions for non-punctuation part-
of-speech tags in WSJ, ordered by overall frequency, as
well as distributions for sentence boundaries and for the
roots of complete and incomplete sentences. (A uniform
distribution would have 1/36 = 2.7% for all POS-tags.)
?
1?
?
x
?pxqx All First Last Sent. Frag.
Uniform 0.48 0.58 0.64 0.79 0.65
All 0.35 0.40 0.79 0.42
First 0.59 0.94 0.57
Last 0.83 0.29
Sent. 0.86
Table 5: A distance matrix for all pairs of probability dis-
tributions over POS-tags shown in Table 4 and the uni-
form distribution; the BC- (or Hellinger) distance (Bhat-
tacharyya, 1943; Nikulin, 2002) between discrete distri-
butions p and q (over x ? X ) ranges from zero (iff p = q)
to one (iff p ? q = 0, i.e., when they do not overlap at all).
692
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
250
500
750
1,000
1,250
1,500
1,750
2,000
(Box-and-whiskers quartile diagrams.)
1
3
7
17
76
1
14
20
27
171
l
Distributions of Sentence Lengths (l) in WSJ
Figure 3: Histograms of lengths (in tokens) for 2,261 non-clausal fragments (red) and other sentences (blue) in WSJ.
5 Dependency and Boundary Model Two
DBM-2 adapts DBM-1 grammars to two classes
of inputs (complete sentences and incomplete frag-
ments) by forking off new, separate multinomials for
stopping decisions and root-distributions (see ?2.2).
5.1 Analytical Motivation
Unrepresentative short sentences ? such as head-
lines and titles ? are common in news-style data
and pose a known nuisance to grammar inducers.
Previous research sometimes took radical measures
to combat the problem: for example, Gillenwater
et al2009) excluded all sentences with three or
fewer tokens from their experiments; and Marec?ek
and Zabokrtsky? (2011) enforced an ?anti-noun-root?
policy to steer their Gibbs sampler away from the
undercurrents caused by the many short noun-phrase
fragments (among sentences up to length 15, in
Czech data). We refer to such snippets of text as
?incomplete sentences? and focus our study of WSJ
on non-clausal data (as signaled by top-level con-
stituent annotations whose first character is not S).8
Table 4 shows that roots of incomplete sentences,
which are dominated by nouns, barely resemble the
other roots, drawn from more traditional verb and
modal types. In fact, these two empirical root dis-
tributions are more distant from one another than ei-
ther is from the uniform distribution, in the space of
discrete probability distributions over POS-tags (see
Table 5). Of the distributions we considered, only
sentence boundaries are as or more different from
8I.e., separating top-level types {S, SINV, SBARQ, SQ, SBAR}
from the rest (ordered by frequency): {NP, FRAG, X, PP, . . .}.
(complete) roots, suggesting that heads of fragments
too may warrant their own multinomial in the model.
Further, incomplete sentences are uncharacteris-
tically short (see Figure 3). It is this property that
makes them particularly treacherous to grammar in-
ducers, since by offering few options of root posi-
tions they increase the chances that a learner will
incorrectly induce nouns to be heads. Given that ex-
pected lengths are directly related to stopping deci-
sions, it could make sense to also model the stopping
probabilities of incomplete sentences separately.
5.2 Experimental Results
Since it is not possible to consult parse trees during
grammar induction (to check whether an input sen-
tence is clausal), we opted for a proxy: presence of
sentence-final punctuation. Using punctuation to di-
vide input sentences into two groups, DBM-2 scored
higher: 40.9, up from 39.0% accuracy (see Table 2).
After evaluating these multi-lingual experiments,
we checked how well our proxy corresponds to ac-
tual clausal sentences in WSJ. Table 6 shows the bi-
nary confusion matrix having a fairly low (but posi-
tive) Pearson correlation coefficient. False positives
r? ? 0.31 Clausal non-Clausal Total
Punctuation 46,829 1,936 48,765
no Punctuation 118 325 443
Total 46,947 2,261 49,208
Table 6: A contingency table for clausal sentences and
trailing punctuation in WSJ; the mean square contingency
coefficient r? signifies a low degree of correlation. (For
two binary variables, r? is equivalent to Karl Pearson?s
better-known product-moment correlation coefficient, ?.)
693
include parenthesized expressions that are marked
as noun-phrases, such as (See related story: ?Fed
Ready to Inject Big Funds?: WSJ Oct. 16, 1989);
false negatives can be headlines having a main verb,
e.g., Population Drain Ends For Midwestern States.
Thus, our proxy is not perfect but seems to be toler-
able in practice. We suspect that identities of punc-
tuation marks (Collins, 2003, Footnote 13) ? both
sentence-final and sentence-initial ? could be of ex-
tra assistance in grammar induction, specifically for
grouping imperatives, questions, and so forth.
6 Dependency and Boundary Model Three
DBM-3 exploits sentence-internal punctuation con-
texts by modeling punctuation-crossing dependency
arcs separately from other attachments (see ?2.3).
6.1 Analytical Motivation
Many common syntactic relations, such as between
a determiner and a noun, are unlikely to hold over
long distances. (In fact, 45% of all head-percolated
dependencies in WSJ are between adjacent words.)
However, some common constructions are more re-
mote: e.g., subordinating conjunctions are, on av-
erage, 4.8 tokens away from their dependent modal
verbs. Sometimes longer-distance dependencies can
be vetted using sentence-internal punctuation marks.
It happens that the presence of punctuation be-
tween such conjunction (IN) and verb (MD) types
serves as a clue that they are not connected (see Ta-
ble 7a); by contrast, a simpler cue ? whether these
words are adjacent ? is, in this case, hardly of any
use (see Table 7b). Conditioning on crossing punc-
tuation could be of help then, playing a role simi-
lar to that of comma-counting (Collins, 1997, ?2.1)
? and ?verb intervening? (Bikel, 2004, ?5.1) ? in
early head-outward models for supervised parsing.
a) r? ? ?0.40 Attached not Attached Total
Punctuation 337 7,645 7,982
no Punctuation 2,144 4,040 6,184
Total 2,481 11,685 14,166
non-Adjacent 2,478 11,673 14,151
Adjacent 3 12 15
b) r? ? +0.00 Attached not Attached Total
Table 7: Contingency tables for IN right-attaching MD,
among closest ordered pairs of these tokens in WSJ sen-
tences with punctuation, versus: (a) presence of interven-
ing punctuation; and (b) presence of intermediate words.
6.2 Experimental Results Postponed
As we mentioned earlier (see ?3), there is little point
in testing DBM-3 with shorter sentences, since most
sentence-internal punctuation occurs in longer in-
puts. Instead, we will test this model in a final step of
a staged training strategy, with more data (see ?7.3).
7 A Curriculum Strategy for DBMs
We propose to train up to DBM-3 iteratively ?
by beginning with DBM-1 and gradually increasing
model complexity through DBM-2, drawing on the
intuitions of IBM translation models 1?4 (Brown et
al., 1993). Instead of using sentences of up to 15 to-
kens, as in all previous experiments (?4?5), we will
now make use of nearly all available training data:
up to length 45 (out of concern for efficiency), dur-
ing later stages. In the first stage, however, we will
use only a subset of the data with DBM-1, in a pro-
cess sometimes called curriculum learning (Bengio
et al2009; Krueger and Dayan, 2009, inter alia).
Our grammar inducers will thus be ?starting small?
in both senses suggested by Elman (1993): simulta-
neously scaffolding on model- and data-complexity.
7.1 Scaffolding Stage #1: DBM-1
We begin by training DBM-1 on sentences with-
out sentence-internal punctuation but with at least
one trailing punctuation mark. Our goal is to avoid,
when possible, overly specific arbitrary parameters
like the ?15 tokens or less? threshold used to select
training sentences. Unlike DBM-2 and 3, DBM-1
does not model punctuation or sentence fragments,
so we instead explicitly restrict its attention to this
cleaner subset of the training data, which takes ad-
vantage of the fact that punctuation may generally
correlate with sentence complexity (Frank, 2000).9
Aside from input sentence selection, our exper-
imental set-up here remained identical to previous
training of DBMs (?4?5). Using this new input data,
DBM-1 averaged 40.7% accuracy (see Table 8).
This is slightly higher than the 39.0% when using
sentences up to length 15, suggesting that our heuris-
tic for clean, simple sentences may be a useful one.
9More incremental training strategies are the subject of an
unpublished companion manuscript (Spitkovsky et al2012a).
694
Directed Dependency Accuracies for: Best of State-of-the-Art Systems
CoNLL Year this Work (@10) Monolingual; POS- Cross-Lingual
& Language DMV DBM-1 DBM-2 DBM-3 +inference (i) Agnostic (ii) Identified (iii) Transfer
Arabic 2006 12.9 10.6 11.0 11.1 10.9 (34.5) 33.4 SCAJ6 ? 50.2 Sbg
?7 36.6 43.9 44.0 44.4 44.9 (48.8) 55.6 RF 54.6 RFH1 ?
Basque ?7 32.7 34.1 33.0 32.7 33.3 (36.5) 43.6 SCAJ5 34.7 MZNR ?
Bulgarian ?7 24.7 59.4 63.6 64.6 65.2 (70.4) 44.3 SCAJ5 53.9 RFH1&2 70.3 Spt
Catalan ?7 41.1 61.3 61.1 61.1 62.1 (78.1) 63.8 SCAJ5 56.3 MZNR ?
Chinese ?6 50.4 63.1 63.0 63.2 63.2 (65.7) 63.6 SCAJ6 ? ?
?7 55.3 56.8 57.0 57.1 57.0 (59.8) 58.5 SCAJ6 34.6 MZNR ?
Czech ?6 31.5 51.3 52.8 53.0 55.1 (61.8) 50.5 SCAJ5 ? ?
?7 34.5 50.5 51.2 53.3 54.2 (67.3) 49.8 SCAJ5 42.4 RFH1&2 ?
Danish ?6 22.4 21.3 19.9 21.8 22.2 (27.4) 46.0 RF 53.1 RFH1&2 56.5 Sar
Dutch ?6 44.9 45.9 46.5 46.0 46.6 (48.6) 32.5 SCAJ5 48.8 RFH1&2 65.7 MPHm:p
English ?7 32.3 29.2 28.6 29.0 29.6 (51.4) 50.3 SAJ 23.8 MZNR 45.7 MPHel
German ?6 27.7 36.3 37.9 38.4 39.1 (52.1) 33.5 SCAJ5 21.8 MZNR 56.7 MPHm:d
Greek ?6 36.3 28.1 26.1 26.1 26.9 (36.8) 39.0 MZ 33.4 MZNR 65.1 MPHm:p
Hungarian ?7 23.6 43.2 52.1 57.4 58.2 (68.4) 48.0 MZ 48.1 MZNR ?
Italian ?7 25.5 41.7 39.8 39.9 40.7 (41.8) 57.5 MZ 60.6 MZNR 69.1 MPHpt
Japanese ?6 42.2 22.8 22.7 22.7 22.7 (32.5) 56.6 SCAJ5 53.5 MZNR ?
Portuguese ?6 37.1 68.9 72.3 71.1 72.4 (80.6) 43.2 MZ 55.8 RFH1&2 76.9 Sbg
Slovenian ?6 33.4 30.4 33.0 34.1 35.2 (36.8) 33.6 SCAJ5 34.6 MZNR ?
Spanish ?6 22.0 25.0 26.7 27.1 28.2 (51.8) 53.0 MZ 54.6 MZNR 68.4 MPHit
Swedish ?6 30.7 48.6 50.3 50.0 50.7 (63.2) 50.0 SCAJ6 34.3 RFH1&2 68.0 MPHm:p
Turkish ?6 43.4 32.9 33.7 33.4 34.4 (38.1) 40.9 SAJ 61.3 RFH1 ?
?7 58.5 44.6 44.2 43.7 44.8 (44.4) 48.8 SCAJ6 ? ?
Average: 33.6 40.7 41.7 42.2 42.9 (51.9) 38.2 SCAJ6 (best average, not an average of bests)
Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1?3 trained
with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al2011a, Tables 5?6) and SAJ (Spitkovsky et al2011b); (ii) rely on gold POS-tag
identities to discourage noun roots (Marec?ek and Zabokrtsky?, 2011, MZ) or to encourage verbs (Rasooli and Faili,
2012, RF); and (iii) transfer delexicalized parsers (S?gaard, 2011a, S) from resource-rich languages with transla-
tions (McDonald et al2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3
trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints.
7.2 Scaffolding Stage #2: DBM-2? DBM-1
Next, we trained on all sentences up to length 45.
Since these inputs are punctuation-rich, in both re-
maining stages we used the constrained Viterbi EM
set-up suggested by Spitkovsky et al2011b) in-
stead of plain soft EM; we employ an early termina-
tion strategy, quitting hard EM as soon as soft EM?s
objective suffers (Spitkovsky et al2011a). Punc-
tuation was converted into Viterbi-decoding con-
straints during training using the so-called loose
method, which stipulates that all words in an inter-
punctuation fragment must be dominated by a single
(head) word, also from that fragment ? with only
these head words allowed to attach the head words
of other fragments, across punctuation boundaries.
To adapt to full data, we initialized DBM-2 using
Viterbi parses from the previous stage (?7.1), plus
uniformly-at-random chosen dependency trees for
the new complex and incomplete sentences, subject
to punctuation-induced constraints. This approach
improved parsing accuracies to 41.7% (see Table 8).
7.3 Scaffolding Stage #3: DBM-3? DBM-2
Next, we repeated the training process of the pre-
vious stage (?7.2) using DBM-3. To initialize this
model, we combined the final instance of DBM-2
with uniform multinomials for punctuation-crossing
attachment probabilities (see ?2.3). As a result, av-
erage performance improved to 42.2% (see Table 8).
Lastly, we applied punctuation constraints also in
inference. Here we used the sprawl method ? a
more relaxed approach than in training, allowing ar-
bitrary words to attach inter-punctuation fragments
(provided that each entire fragment still be derived
695
by one of its words) ? as suggested by Spitkovsky
et al2011b). This technique increased DBM-3?s
average accuracy to 42.9% (see Table 8). Our fi-
nal result substantially improves over the baseline?s
33.6% and compares favorably to previous work.10
8 Discussion and the State-of-the-Art
DBMs come from a long line of head-outward mod-
els for dependency grammar induction yet their gen-
erative processes feature important novelties. One
is conditioning on more observable state ? specifi-
cally, the left and right end words of a phrase being
constructed ? than in previous work. Another is al-
lowing multiple grammars ? e.g., of complete and
incomplete sentences ? to coexist in a single model.
These improvements could make DBMs quick-and-
easy to bootstrap directly from any available partial
bracketings (Pereira and Schabes, 1992), for exam-
ple capitalized phrases (Spitkovsky et al2012b).
The second part of our work ? the use of a cur-
riculum strategy to train DBM-1 through 3 ? elim-
inates having to know tuned cut-offs, such as sen-
tences with up to a predetermined number of tokens.
Although this approach adds some complexity, we
chose conservatively, to avoid overfitting settings
of sentence length, convergence criteria, etc.: stage
one?s data is dictated by DBM-1 (which ignores
punctuation); subsequent stages initialize additional
pieces uniformly: uniform-at-random parses for new
data and uniform multinomials for new parameters.
Even without curriculum learning ? trained with
vanilla EM ? DBM-2 and 1 are already strong.
Further boosts to accuracy could come from em-
ploying more sophisticated optimization algorithms,
e.g., better EM (Samdani et al2012), constrained
Gibbs sampling (Marec?ek and Zabokrtsky?, 2011) or
locally-normalized features (Berg-Kirkpatrick et al
2010). Other orthogonal dependency grammar in-
duction techniques ? including ones based on uni-
versal rules (Naseem et al2010) ? may also ben-
efit in combination with DBMs. Direct comparisons
to previous work require some care, however, as
there are several classes of systems that make dif-
ferent assumptions about training data (see Table 8).
10Note that DBM-1?s 39% average accuracy with standard
training (see Table 2) was already nearly a full point higher than
that of any single previous best system (SCAJ6 ? see Table 8).
8.1 Monolingual POS-Agnostic Inducers
The first type of grammar inducers, including our
own approach, uses standard training and test data
sets for each language, with gold part-of-speech tags
as anonymized word classes. For the purposes of
this discussion, we also include in this group trans-
ductive learners that may train on data from the test
sets. Our DBM-3 (decoded with punctuation con-
straints) does well among such systems ? for which
accuracies on all sentence lengths of the evaluation
sets are reported ? attaining highest scores for 8 of
19 languages; the DMV baseline is still state-of-the-
art for one language; and the remaining 10 bests are
split among five other recent systems (see Table 8).11
Half of the five came from various lateen EM strate-
gies (Spitkovsky et al2011a) for escaping and/or
avoiding local optima. These heuristics are compat-
ible with how we trained our DBMs and could po-
tentially provide further improvement to accuracies.
Overall, the final scores of DBM-3 were better, on
average, than those of any other single system: 42.9
versus 38.2% (Spitkovsky et al2011a, Table 6).
The progression of scores for DBM-1 through 3
without using punctuation constraints in inference
? 40.7, 41.7 and 42.2% ? fell entirely above this
previous state-of-the-art result as well; the DMV
baseline ? also trained on sentences without inter-
nal but with final punctuation ? averaged 33.6%.
8.2 Monolingual POS-Identified Inducers
The second class of techniques assumes knowledge
about identities of part-of-speech tags (Naseem et
al., 2010), i.e., which word tokens are verbs, which
ones are nouns, etc. Such grammar inducers gener-
ally do better than the first kind ? e.g., by encour-
aging verbocentricity (Gimpel and Smith, 2011) ?
though even here our results appear to be compet-
itive. In fact, to our surprise, only in 5 of 19 lan-
guages a ?POS-identified? system performed better
than all of the ?POS-agnostic? ones (see Table 8).
8.3 Multi-Lingual Semi-Supervised Parsers
The final broad class of related algorithms we con-
sidered extends beyond monolingual data and uses
11For Turkish ?06, the ?right-attach? baseline outperforms
even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an
important difference between 2006 and 2007 CoNLL data sets
has to do with segmentation of morphologically-rich languages.
696
both identities of POS-tags and/or parallel bitexts
to transfer (supervised) delexicalized parsers across
languages. Parser projection is by far the most suc-
cessful approach to date and we hope that it too
may stand to gain from our modeling improvements.
Of the 10 languages for which we found results
in the literature, transferred parsers underperformed
the grammar inducers in only one case: on En-
glish (see Table 8). The unsupervised system that
performed better used a special ?weighted? initial-
izer (Spitkovsky et al2011b, ?3.1) that worked well
for English (but less so for many other languages).
DBMs may be able to improve initialization. For
example, modeling of incomplete sentences could
help in incremental initialization strategies like baby
steps (Spitkovsky et al2009), which are likely sen-
sitive to the proverbial ?bum steer? from unrepresen-
tative short fragments, pace Tu and Honavar (2011).
8.4 Miscellaneous Systems on Short Sentences
Several recent systems (Cohen et al2011; S?gaard,
2011b; Naseem et al2010; Gillenwater et al2010;
Berg-Kirkpatrick and Klein, 2010, inter alia) are ab-
sent from Table 8 because they do not report perfor-
mance for all sentence lengths. To facilitate com-
parison with this body of important previous work,
we also tabulated final accuracies for the ?up-to-ten
words? task under heading @10: 51.9%, on average.
9 Conclusion
Although a dependency parse for a sentence can be
mapped to a constituency parse (Xia and Palmer,
2001), the probabilistic models generating them use
different conditioning: dependency grammars focus
on the relationship between arguments and heads,
constituency grammars on the coherence of chunks
covered by non-terminals. Since redundant views of
data can make learning easier (Blum and Mitchell,
1998), integrating aspects of both constituency and
dependency ought to be able to help grammar in-
duction. We have shown that this insight is correct:
dependency grammar inducers can gain from mod-
eling boundary information that is fundamental to
constituency (i.e., phrase-structure) formalisms.
DBMs are a step in the direction towards mod-
eling constituent boundaries jointly with head de-
pendencies. Further steps must involve more tightly
coupling the two frameworks, as well as showing
ways to incorporate both kinds of information in
other state-of-the art grammar induction paradigms.
Acknowledgments
We thank Roi Reichart and Marta Recasens, for many helpful
comments on draft versions of this paper, and Marie-Catherine
de Marneffe, Roy Schwartz, Mengqiu Wang and the anonymous
reviewers, for their apt recommendations. Funded, in part, by
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program, under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of the DARPA, AFRL, or the US government.
First author is grateful to Cindy Chan for her friendship and
support over many long months leading up to this publication.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite-state
head transducers. Computational Linguistics, 26.
H. Alshawi. 1996a. Head automata for speech translation. In
ICSLP.
H. Alshawi. 1996b. Method and apparatus for an improved
language recognition system. US Patent 1999/5870706.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin, and
S. Edelman. 2006. Boosting unsupervised grammar induc-
tion by splitting complex sentences on function words. In
BUCLD.
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic gram-
mar induction. In ACL.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with fea-
tures. In NAACL-HLT.
A. Bhattacharyya. 1943. On a measure of divergence between
two statistical populations defined by their probability distri-
butions. BCMS, 35.
D. M. Bikel. 2004. Intricacies of Collins? parsing model. Com-
putational Linguistics, 30.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
M. R. Brent and J. M. Siskind. 2001. The role of exposure to
isolated words in early vocabulary development. Cognition,
81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
697
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. Technical
report, Brown University.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2003. Head-driven statistical models for natural
language parsing. Computational Linguistics, 29.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. M. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. Technical report, IRCS.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H. C. Bunt and A. Nijholt, editors,
Advances in Probabilistic and Other Parsing Technologies.
Kluwer Academic Publishers.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and F. Pereira.
2009. Sparsity in grammar induction. In NIPS: Gram-
mar Induction, Representation of Language and Language
Learning.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
C. Ha?nig. 2010. Improvements in unsupervised co-occurrence
based parsing. In CoNLL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. McClosky. 2008. Modeling valence effects in unsupervised
grammar induction. Technical report, Brown University.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
M. S. Nikulin. 2002. Hellinger distance. In M. Hazewinkel,
editor, Encyclopaedia of Mathematics. Kluwer Academic
Publishers.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
R. Samdani, M.-W. Chang, and D. Roth. 2012. Unified expec-
tation maximization. In NAACL-HLT.
Y. Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis,
University of Amsterdam.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models ? the ?wabi-sabi? of
unsupervised parsing. In submission.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Capi-
talization cues improve dependency grammar induction. In
WILS.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT.
698
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1983?1995,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Breaking Out of Local Optima with Count Transforms
and Model Recombination: A Study in Grammar Induction
Valentin I. Spitkovsky
valentin@cs.stanford.edu
Hiyan Alshawi
hiyan@google.com
Daniel Jurafsky
jurafsky@stanford.edu
Abstract
Many statistical learning problems in NLP call
for local model search methods. But accu-
racy tends to suffer with current techniques,
which often explore either too narrowly or too
broadly: hill-climbers can get stuck in local
optima, whereas samplers may be inefficient.
We propose to arrange individual local opti-
mizers into organized networks. Our building
blocks are operators of two types: (i) trans-
form, which suggests new places to search, via
non-random restarts from already-found local
optima; and (ii) join, which merges candidate
solutions to find better optima. Experiments
on grammar induction show that pursuing dif-
ferent transforms (e.g., discarding parts of a
learned model or ignoring portions of train-
ing data) results in improvements. Groups of
locally-optimal solutions can be further per-
turbed jointly, by constructing mixtures. Us-
ing these tools, we designed several modu-
lar dependency grammar induction networks
of increasing complexity. Our complete sys-
tem achieves 48.6% accuracy (directed depen-
dency macro-average over all 19 languages in
the 2006/7 CoNLL data) ? more than 5%
higher than the previous state-of-the-art.
1 Introduction
Statistical methods for grammar induction often boil
down to solving non-convex optimization problems.
Early work attempted to locally maximize the likeli-
hood of a corpus, using EM to estimate probabilities
of dependency arcs between word bigrams (Paskin
2001a; 2001b). That parsing model has since been
extended to make unsupervised learning more feasi-
ble (Klein and Manning, 2004; Headden et al, 2009;
Spitkovsky et al, 2012b). But even the latest tech-
niques can be quite error-prone and sensitive to ini-
tialization, because of approximate, local search.
In theory, global optima can be found by enumer-
ating all parse forests that derive a corpus, though
this is usually prohibitively expensive in practice. A
preferable brute force approach is sampling, as in
Markov-chain Monte Carlo (MCMC) and random
restarts (Hu et al, 1994), which hit exact solutions
eventually. Restarts can be giant steps in a parameter
space that undo all previous work. At the other ex-
treme, MCMC may cling to a neighborhood, reject-
ing most proposed moves that would escape a local
attractor. Sampling methods thus take unbounded
time to solve a problem (and can?t certify optimal-
ity) but are useful for finding approximate solutions
to grammar induction (Cohn et al, 2011; Marec?ek
and ?Zabokrtsky?, 2011; Naseem and Barzilay, 2011).
We propose an alternative (deterministic) search
heuristic that combines local optimization via EM
with non-random restarts. Its new starting places are
informed by previously found solutions, unlike con-
ventional restarts, but may not resemble their prede-
cessors, unlike typical MCMC moves. We show that
one good way to construct such steps in a parame-
ter space is by forgetting some aspects of a learned
model. Another is by merging promising solutions,
since even simple interpolation (Jelinek and Mercer,
1980) of local optima may be superior to all of the
originals. Informed restarts can make it possible to
explore a combinatorial search space more rapidly
and thoroughly than with traditional methods alone.
2 Abstract Operators
Let C be a collection of counts ? the sufficient
statistics from which a candidate solution to an
optimization problem could be computed, e.g., by
smoothing and normalizing to yield probabilities.
The counts may be fractional and solutions could
take the form of multinomial distributions. A local
optimizer L will convert C into C? = LD(C) ? an
updated collection of counts, resulting in a proba-
bilistic model that is no less (and hopefully more)
consistent with a data set D than the original C:
(1)
LDC C?
1983
Unless C? is a global optimum, we should be able
to make further improvements. But if L is idempo-
tent (and ran to convergence) then L(L(C)) = L(C).
Given only C and LD, the single-node optimization
network above would be the minimal search pattern
worth considering. However, if we had another opti-
mizer L? ? or a fresh starting point C? ? then more
complicated networks could become useful.
2.1 Transforms (Unary)
New starts could be chosen by perturbing an existing
solution, as in MCMC, or independently of previous
results, as in random restarts. We focus on interme-
diate changes to C, without injecting randomness.
All of our transforms involve selective forgetting
or filtering. For example, if the probabilistic model
that is being estimated decomposes into independent
constituents (e.g., several multinomials) then a sub-
set of them can be reset to uniform distributions, by
discarding associated counts from C. In text classifi-
cation, this could correspond to eliminating frequent
or rare tokens from bags-of-words. We use circular
shapes to represent such model ablation operators:
(2)C
An orthogonal approach might separate out vari-
ous counts in C by their provenance. For instance,
if D consisted of several heterogeneous data sources,
then the counts from some of them could be ignored:
a classifier might be estimated from just news text.
We will use squares to represent data-set filtering:
(3)C
Finally, if C represents a mixture of possible inter-
pretations over D ? e.g., because it captures the out-
put of a ?soft? EM algorithm ? contributions from
less likely, noisier completions could also be sup-
pressed (and their weights redistributed to the more
likely ones), as in ?hard? EM. Diamonds will repre-
sent plain (single) steps of Viterbi training:
(4)C
2.2 Joins (Binary)
Starting from different initializers, say C1 and C2,
it may be possible for L to arrive at distinct local
optima, C?1 6= C?2 . The better of the two solutions,
according to likelihood LD of D, could then be se-
lected ? as is standard practice when sampling.
Our joining technique could do better than either
C?1 or C
?
2 , by entertaining also a third possibility,
which combines the two candidates. We construct
a mixture model by adding together all counts from
C?1 and C?2 into C+ = C?1 + C?2 . Original initializers
C1, C2 will, this way, have equal pull on the merged
model,1 regardless of nominal size (because C?1 , C?2
will have converged using a shared training set, D).
We return the best of C?1 , C?2 and C?+ = L(C+). This
approach may uncover more (and never returns less)
likely solutions than choosing among C?1 , C?2 alone:
(5)
LD
LD
LD
+
arg
M
A
X
L
D
C1
C?1 = L(C1)
C2
C?2 = L(C2)
C?1 + C?2 = C+
We will use a short-hand notation to represent the
combiner network diagrammed above, less clutter:
(6)
LDC2
C1
3 The Task and Methodology
We apply transform and join paradigms to grammar
induction, an important problem of computational
linguistics that involves notoriously difficult objec-
tives (Pereira and Schabes, 1992; de Marcken, 1995;
Gimpel and Smith, 2012, inter alia). The goal is to
induce grammars capable of parsing unseen text. In-
put, in both training and testing, is a sequence of to-
kens labeled as: (i) a lexical item and its category,
(w, cw); (ii) a punctuation mark; or (iii) a sentence
boundary. Output is unlabeled dependency trees.
3.1 Models and Data
We constrain all parse structures to be projective, via
dependency-and-boundary grammars (Spitkovsky et
al., 2012a; 2012b): DBMs 0?3 are head-outward
generative parsing models (Alshawi, 1996) that dis-
tinguish complete sentences from incomplete frag-
ments in a corpus D: Dcomp comprises inputs ending
with punctuation; Dfrag = D ? Dcomp is everything
1If desired, a scaling factor could be used to bias C+ towards
either C?1 or C?2 , for example based on their likelihood ratio.
1984
else. The ?complete? subset is further partitioned
into simple sentences, Dsimp ? Dcomp, with no inter-
nal punctuation, and others, which may be complex.
As an example, consider the beginning of an arti-
cle from (simple) Wikipedia: (i) Linguistics (ii) Lin-
guistics (sometimes called philology) is the science
that studies language. (iii) Scientists who study lan-
guage are called linguists. Since the title does not
end with punctuation, it would be relegated to Dfrag.
But two complete sentences would be in Dcomp, with
the last also filed under Dsimp, as it has only a trail-
ing punctuation mark. Spitkovsky et al suggested
two curriculum learning strategies: (i) one in which
induction begins with clean, simple data, Dsimp, and
a basic model, DBM-1 (2012b); and (ii) an alterna-
tive bootstrapping approach: starting with still more,
simpler data ? namely, short inter-punctuation frag-
ments up to length l = 15, Dlsplit ? Dlsimp ? and a
bare-bones model, DBM-0 (2012a). In our example,
Dsplit would hold five text snippets: (i) Linguistics;
(ii) Linguistics; (iii) sometimes called philology;
(iv) is the science that studies language; and (v) Sci-
entists who study language are called linguists.
Only the last piece of text would still be considered
complete, isolating its contribution to sentence root
and boundary word distributions from those of in-
complete fragments. The sparse model, DBM-0, as-
sumes a uniform distribution for roots of incomplete
inputs and reduces conditioning contexts of stopping
probabilities, which works well with split data. We
will exploit both DBM-0 and the full DBM,2 draw-
ing also on split, simple and raw views of input text.
All experiments prior to final multi-lingual eval-
uation will use the Penn English Treebank?s Wall
Street Journal (WSJ) portion (Marcus et al, 1993) as
the underlying tokenized and sentence-broken cor-
pus D. Instead of gold parts-of-speech, we plugged
in 200 context-sensitive unsupervised tags, from
Spitkovsky et al (2011c),3 for the word categories.
3.2 Smoothing and Lexicalization
All unlexicalized instances of DBMs will be esti-
mated with ?add one? (a.k.a. Laplace) smoothing,
2We use the short-hand DBM to refer to DBM-3, which is
equivalent to DBM-2 if D has no internally-punctuated sen-
tences (D=Dsplit), and DBM-1 if all inputs also have trailing
punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0.
3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2
using only the word category cw to represent a token.
Fully-lexicalized grammars (L-DBM) are left un-
smoothed, and represent each token as both a word
and its category, i.e., the whole pair (w, cw). To eval-
uate a lexicalized parsing model, we will always ob-
tain a delexicalized-and-smoothed instance first.
3.3 Optimization and Viterbi Decoding
We use ?early-switching lateen? EM (Spitkovsky et
al., 2011a, ?2.4) to train unlexicalized models, alter-
nating between the objectives of ordinary (soft) and
hard EM algorithms, until neither can improve its
own objective without harming the other?s. This ap-
proach does not require tuning termination thresh-
olds, allowing optimizers to run to numerical con-
vergence if necessary, and handles only our shorter
inputs (l ? 15), starting with soft EM (L = SL, for
?soft lateen?). Lexicalized models will cover full
data (l ? 45) and employ ?early-stopping lateen?
EM (2011a, ?2.3), re-estimating via hard EM until
soft EM?s objective suffers. Alternating EMs would
be expensive here, since updates take (at least) O(l3)
time, and hard EM?s objective (L = H) is the one
better suited to long inputs (Spitkovsky et al, 2010).
Our decoders always force an inter-punctuation
fragment to derive itself (Spitkovsky et al, 2011b,
?2.2).4 In evaluation, such (loose) constraints may
help attach sometimes and philology to called (and
the science... to is). In training, stronger (strict)
constraints also disallow attachment of fragments?
heads by non-heads, to connect Linguistics, called
and is (assuming each piece got parsed correctly).
3.4 Final Evaluation and Metrics
Evaluation is against held-out CoNLL shared task
data (Buchholz and Marsi, 2006; Nivre et al, 2007),
spanning 19 languages. We compute performance
as directed dependency accuracies (DDA), fractions
of correct unlabeled arcs in parsed output (an extrin-
sic metric).5 For most WSJ experiments we include
also sentence and parse tree cross-entropies (soft and
hard EMs? intrinsic metrics), in bits per token (bpt).
4But these constraints do not impact training with shorter
inputs, since there is no internal punctuation in Dsplit or Dsimp.
5We converted gold labeled constituents in WSJ to unlabeled
reference dependencies using deterministic ?head-percolation?
rules (Collins, 1999); sentence root symbols, though not punc-
tuation arcs, contribute to scores, as is standard (Paskin, 2001b).
1985
4 Concrete Operators
We will now instantiate the operators sketched out
in ?2 specifically for the grammar induction task.
Throughout, we repeatedly employ single steps of
Viterbi training to transfer information between sub-
networks in a model-independent way: when a mod-
ule?s output is a set of (Viterbi) parse trees, it neces-
sarily contains sufficient information required to es-
timate an arbitrarily-factored model down-stream.6
4.1 Transform #1: A Simple Filter
Given a model that was estimated from (and there-
fore parses) a data set D, the simple filter (F ) at-
tempts to extract a cleaner model, based on the sim-
pler complete sentences of Dsimp. It is implemented
as a single (unlexicalized) step of Viterbi training:
(7)C F
The idea here is to focus on sentences that are not
too complicated yet grammatical. This punctuation-
sensitive heuristic may steer a learner towards easy
but representative training text and, we showed, aids
grammar induction (Spitkovsky et al, 2012b, ?7.1).
4.2 Transform #2: A Symmetrizer
The symmetrizer (S) reduces input models to sets of
word association scores. It blurs all details of in-
duced parses in a data set D, except the number of
times each (ordered) word pair participates in a de-
pendency relation. We implemented symmetrization
also as a single unlexicalized Viterbi training step,
but now with proposed parse trees? scores, for a sen-
tence in D, proportional to a product over non-root
dependency arcs of one plus how often the left and
right tokens (are expected to) appear connected:
(8)C S
The idea behind the symmetrizer is to glean infor-
mation from skeleton parses. Grammar inducers can
sometimes make good progress in resolving undi-
rected parse structures despite being wrong about
the polarities of most arcs (Spitkovsky et al, 2009,
Figure 3: Uninformed). Symmetrization offers an
extra chance to make heads or tails of syntactic rela-
tions, after learning which words tend to go together.
6A related approach ? initializing EM training with an
M-step ? was advocated by Klein and Manning (2004, ?3).
At each instance where a word a? attaches z? on
(say) the right, our implementation attributes half its
weight to the intended construction, ya? z?, reserving
the other half for the symmetric structure, z? attach-
ing a? to its left: xa? z?. For the desired effect, these
aggregated counts are left unnormalized, while all
other counts (of word fertilities and sentence roots)
get discarded. To see why we don?t turn word attach-
ment scores into probabilities, consider sentences
a? z? and c? z?. The fact that z? co-occurs with a?
introduces an asymmetry into z??s relation with c?:
P( z? | c?) = 1 differs from P( c? | z?) = 1/2. Normal-
izing might force the interpretation yc? z? (and also
y
a? z?), not because there is evidence in the data, but
as a side-effect of a model?s head-driven nature (i.e.,
factored with dependents conditioned on heads). Al-
ways branching right would be a mistake, however,
for example if z? is a noun, since either of a? or c?
could be a determiner, with the other a verb.
4.3 Join: A Combiner
The combiner must admit arbitrary inputs, includ-
ing models not estimated from D, unlike the trans-
forms. Consequently, as a preliminary step, we con-
vert each input Ci into parse trees of D, with counts
C?i, via Viterbi-decoding with a smoothed, unlexical-
ized version of the corresponding incoming model.
Actual combination is then performed in a more pre-
cise (unsmoothed) fashion: C?i are the (lexicalized)
solutions starting from C?i; and C?+ is initialized with
their sum,
?
iC
?
i . Counts of the lexicalized model
with lowest cross-entropy on D become the output:7
(9)
LDC2
C1
5 Basic Networks
We are ready to propose a non-trivial subnetwork for
grammar induction, based on the transform and join
operators, which we will reuse in larger networks.
5.1 Fork/Join (FJ)
Given a model that parses a base data set D0, the
fork/join subnetwork will output an adaptation of
that model for D. It could facilitate a grammar in-
duction process, e.g., by advancing it from smaller
7In our diagrams, lexicalized modules are shaded black.
1986
to larger ? or possibly more complex ? data sets.
We first fork off two variations of the incoming
model based on D0: (i) a filtered view, which fo-
cuses on cleaner, simpler data (transform #1); and
(ii) a symmetrized view that backs off to word asso-
ciations (transform #2). Next is grammar induction
over D. We optimize a full DBM instance starting
from the first fork, and bootstrap a reduced DBM0
from the second. Finally, the two new induced sets
of parse trees, for D, are merged (lexicalized join):
(10)
HL?DBMD
SLDBMD
SLDBM0D
C
F
S
D0
C1
C2
C?1
C?2
The idea here is to prepare for two scenarios: an
incoming grammar that is either good or bad for D.
If the model is good, DBM should be able to hang
on to it and make improvements. But if it is bad,
DBM could get stuck fitting noise, whereas DBM0
might be more likely to ramp up to a good alterna-
tive. Since we can?t know ahead of time which is the
true case, we pursue both optimization paths simul-
taneously and let a combiner later decide for us.
Note that the forks start (and end) optimizing with
soft EM. This is because soft EM integrates previ-
ously unseen tokens into new grammars better than
hard EM, as evidenced by our failed attempt to re-
produce the ?baby steps? strategy with Viterbi train-
ing (Spitkovsky et al, 2010, Figure 4). A combiner
then executes hard EM, and since outputs of trans-
forms are trees, the end-to-end process is a chain of
lateen alternations that starts and ends with hard EM.
We will use a ?grammar inductor? to represent
subnetworks that transition from Dlsplit to Dl+1split, by
taking transformed parse trees of inter-punctuation
fragments up to length l (base data set, D0) to ini-
tialize training over fragments up to length l + 1:
(11)C l+1
The FJ network instantiates a grammar inductor
with l = 14, thus training on inter-punctuation frag-
ments up to length 15, as in previous work, starting
from an empty set of counts, C = ?. Smoothing
causes initial parse trees to be chosen uniformly at
random, as suggested by Cohen and Smith (2010):
(12)? 15
5.2 Iterated Fork/Join (IFJ)
Our second network daisy-chains grammar induc-
tors, starting from the single-word inter-punctuation
fragments in D1split, then retraining on D2split, and so
forth, until finally stopping at D15split, as before:
(13)1 2 14 15
We diagrammed this system as not taking an input,
since the first inductor?s output is fully determined
by unique parse trees of single-token strings. This
iterative approach to optimization is akin to deter-
ministic annealing (Rose, 1998), and is patterned af-
ter ?baby steps? (Spitkovsky et al, 2009, ?4.2).
Unlike the basic FJ, where symmetrization was a
no-op (since there were no counts in C = ?), IFJ
makes use of symmetrizers ? e.g., in the third in-
ductor, whose input is based on strings with up to
two tokens. Although it should be easy to learn
words that go together from very short fragments,
extracting correct polarities of their relations could
be a challenge: to a large extent, outputs of early in-
ductors may be artifacts of how our generative mod-
els factor (see ?4.2) or how ties are broken in opti-
mization (Spitkovsky et al, 2012a, Appendix B). We
therefore expect symmetrization to be crucial in ear-
lier stages but to weaken any high quality grammars,
nearer the end; it will be up to combiners to handle
such phase transitions correctly (or gracefully).
5.3 Grounded Iterated Fork/Join (GIFJ)
So far, our networks have been either purely itera-
tive (IFJ) or static (FJ). These two approaches can
also be combined, by injecting FJ?s solutions into
IFJ?s more dynamic stream. Our new transition sub-
network will join outputs of grammar inductors that
either (i) continue a previous solution (as in IFJ); or
(ii) start over from scratch (?grounding? to an FJ):
(14)
HL?DBMDl+1split?
Cl Cl+1l+1
l+1
The full GIFJ network can then be obtained by un-
rolling the above template from l = 14 back to one.
1987
WSJ15split WSJ
15
simp
Instance Label Model hsents htrees DDA hsents htrees DDA TA Description
DBM 6.54 6.75 83.7 6.05 6.21 85.1 42.7 Supervised (MLE of WSJ45)
? = C ? 8.76 10.46 21.4 8.58 10.52 20.7 3.9 Random Projective Parses
SL(S(C)) = C2 DBM0 6.18 6.39 57.0 5.90 6.11 57.5 10.4 B
A
}
Unlexicalized
BaselinesSL(F (C)) = C1 DBM 5.89 5.99 62.2 5.79 5.90 60.9 12.0
H(C?2) = C?2 L-DBM 7.28 7.30 59.2 6.87 6.88 58.6 10.4
Fork/Join
?
?
?
?
?
Baseline
Combination
H(C?1) = C?1 L-DBM 7.07 7.08 62.3 6.72 6.73 60.8 12.0
C?1 + C?2 = C+ L-DBM 7.20 7.27 64.0 6.82 6.88 62.5 12.3
H(C+) = C?+ L-DBM 7.02 7.04 64.2 6.64 6.65 62.7 12.8
L-DBM 6.95 6.96 70.5 6.55 6.56 68.2 14.9 Iterated Fork/Join (IFJ)
L-DBM 6.91 6.92 71.4 6.52 6.52 69.2 15.6 Grounded Iterated Fork/Join
L-DBM 6.83 6.83 72.3 6.41 6.41 70.2 17.9 Grammar Transformer (GT)
L-DBM 6.92 6.93 71.9 6.53 6.53 69.8 16.7 IFJ
GT
}
w/Iterated
CombinersL-DBM 6.83 6.83 72.9 6.41 6.41 70.6 18.0
Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments
up to length 15 (WSJ15split) and its subset of simple, complete sentences (WSJ15simp, with exact tree accuracies ? TA).
6 Performance of Basic Networks
We compared our three networks? performance on
their final training sets, WSJ15split (see Table 1, which
also tabulates results for a cleaner subset, WSJ15simp).
The first network starts from C = ?, helping us es-
tablish several straw-man baselines. Its empty ini-
tializer corresponds to guessing (projective) parse
trees uniformly at random, which has 21.4% accu-
racy and sentence string cross-entropy of 8.76bpt.
6.1 Fork/Join (FJ)
FJ?s symmetrizer yields random parses of WSJ14split,
which initialize training of DBM0. This baseline (B)
lowers cross-entropy to 6.18bpt and scores 57.0%.
FJ?s filter starts from parse trees of WSJ14simp only, and
trains up a full DBM. This choice makes a stronger
baseline (A), with 5.89bpt cross-entropy, at 62.2%.
The join operator uses counts from A and B, C1
and C2, to obtain parse trees whose own counts C?1
and C?2 initialize lexicalized training. From each C?i,
an optimizer arrives at C?i . Grammars corresponding
to these counts have higher cross-entropies, because
of vastly larger vocabularies, but also better accura-
cies: 59.2 and 62.3%. Their mixture C+ is a simple
sum of counts in C?1 and C?2 : it is not expected to be
an improvement but happens to be a good move, re-
sulting in a grammar with higher accuracy (64.0%),
though not better Viterbi cross-entropy (7.27 falls
between 7.08 and 7.30bpt) than both sources. The
combiner?s third alternative, a locally optimal C?+, is
then obtained by re-optimizing from C+. This so-
lution performs slightly better (64.2%) and will be
the local optimum returned by FJ?s join operator, be-
cause it attains the lowest cross-entropy (7.04bpt).
6.2 Iterated Fork/Join (IFJ)
IFJ?s iterative approach results in an improvement:
70.5% accuracy and 6.96bpt cross-entropy. To test
how much of this performance could be obtained by
a simpler iterated network, we experimented with
ablated systems that don?t fork or join, i.e., our clas-
sic ?baby steps? schema (chaining together 15 op-
timizers), using both DBM and DBM0, with and
without a transform in-between. However, all such
?linear? networks scored well below 50%. We con-
clude from these results that an ability to branch out
into different promising regions of a solution space,
and to merge solutions of varying quality into better
models, are important properties of FJ subnetworks.
6.3 Grounded Iterated Fork/Join (GIFJ)
Grounding improves GIFJ?s performance further, to
71.4% accuracy and 6.92bpt cross-entropy. This re-
sult shows that fresh perspectives from optimizers
that start over can make search efforts more fruitful.
7 Enhanced Subnetworks
Modularity and abstraction allow for compact repre-
sentations of complex systems. Another key benefit
is that individual components can be understood and
improved in isolation, as we will demonstrate next.
1988
7.1 An Iterative Combiner (IC)
Our basic combiner introduced a third option, C?+,
into a pool of candidate solutions, {C?1 , C?2}. This
new entry may not be a simple mixture of the orig-
inals, because of non-linear effects from applying L
to C?1 + C?2 , but could most likely still be improved.
Rather than stop at C?+, when it is better than both
originals, we could recombine it with a next best so-
lution, continuing until no further improvement is
made. Iterating can?t harm a given combiner?s cross-
entropy (e.g., it lowers FJ?s from 7.04 to 7.00bpt),
and its advantages can be realized more fully in the
larger networks (albeit without any end-to-end guar-
antees): upgrading all 15 combiners in IFJ would
improve performance (slightly) more than ground-
ing (71.5 vs. 71.4%), and lower cross-entropy (from
6.96 to 6.93bpt). But this approach is still a bit timid.
A more greedy way is to proceed so long as C?+
is not worse than both predecessors. We shall now
state our most general iterative combiner (IC) algo-
rithm: Start with a solution pool p = {C?i }ni=1. Next,
construct p? by adding C?+ = L(
?n
i=1 C
?
i ) to p and re-
moving the worst of n+ 1 candidates in the new set.
Finally, if p = p?, return the best of the solutions in p;
otherwise, repeat from p := p?. At n = 2, one could
think of taking L(C?1 + C?2 ) as performing a kind of
bisection search in some (strange) space. With these
new and improved combiners, the IFJ network per-
forms better: 71.9% (up from 70.5 ? see Table 1),
lowering cross-entropy (down from 6.96 to 6.93bpt).
We propose a distinguished notation for the ICs:
(15)
*C2
C1
7.2 A Grammar Transformer (GT)
The levels of our systems? performance at grammar
induction thus far suggest that the space of possible
networks (say, with up to k components) may itself
be worth exploring more thoroughly. We leave this
exercise to future work, ending with two relatively
straight-forward extensions for grounded systems.
Our static bootstrapping mechanism (?ground? of
GIFJ) can be improved by pretraining with simple
sentences first ? as in the curriculum for learning
DBM-1 (Spitkovsky et al, 2012b, ?7.1), but now
with a variable length cut-off l (much lower than the
original 45) ? instead of starting from ? directly:
(16)
SDBMDlsimp?
l+1
?
?
?
l
The output of this subnetwork can then be refined,
by reconciling it with a previous dynamic solution.
We perform a mini-join of a new ground?s counts
with Cl, using the filter transform (single steps of
lexicalized Viterbi training on clean, simple data),
ahead of the main join (over more training data):
(17)
HL?DBMDl+1splitCl Cl+1
l+1
F
l
This template can be unrolled, as before, to obtain
our last network (GT), which achieves 72.9% accu-
racy and 6.83bpt cross-entropy (slightly less accu-
rate with basic combiners, at 72.3% ? see Table 1).
8 Full Training and System Combination
All systems that we described so far stop training at
D15split. We will use a two-stage adaptor network to
transition their grammars to a full data set, D45:
(18)
HL?DBMD45split H
L?DBM
D45C
The first stage exposes grammar inducers to longer
inputs (inter-punctuation fragments with up to 45
tokens); the second stage, at last, reassembles text
snippets into actual sentences (also up to l = 45).8
After full training, our IFJ and GT systems parse
Section 23 of WSJ at 62.7 and 63.4% accuracy, bet-
ter than the previous state-of-the-art (61.2% ? see
Table 2). To test the generalized IC algorithm, we
merged our implementations of these three strong
grammar induction pipelines into a combined sys-
tem (CS). It scored highest: 64.4%.
(19)
HL?DBMD45(GT) #1
(IFJ) #2
#3
CS
The quality of bracketings corresponding to (non-
trivial) spans derived by heads of our dependency
structures is competitive with the state-of-the-art in
unsupervised constituent parsing. On the WSJ sen-
tences up to length 40 in Section 23, CS attains sim-
ilar F1-measure (54.2 vs. 54.6, with higher recall) to
8Note that smoothing in the final (unlexicalized) Viterbi step
masks the fact that model parts that could not be properly es-
timated in the first stage (e.g., probabilities of punctuation-
crossing arcs) are being initialized to uniform multinomials.
1989
System DDA (@10)
(Gimpel and Smith, 2012) 53.1 (64.3)
(Gillenwater et al, 2010) 53.3 (64.3)
(Bisk and Hockenmaier, 2012) 53.3 (71.5)
(Blunsom and Cohn, 2010) 55.7 (67.7)
(Tu and Honavar, 2012) 57.0 (71.4)
(Spitkovsky et al, 2011b) 58.4 (71.4)
(Spitkovsky et al, 2011c) 59.1 (71.4)
#3 (Spitkovsky et al, 2012a) 61.2 (71.4)
#2
w/Full Training
{
IFJ
GT
62.7 (70.3)
#1 63.4 (70.3)
#1 + #2 + #3 System Combination CS 64.4 (72.0)
Supervised DBM (also with loose decoding) 76.3 (85.4)
Table 2: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences and up to length ten) for
recent systems, our full networks (IFJ and GT), and three-
way combination (CS) with the previous state-of-the-art.
PRLG (Ponvert et al, 2011), which is the strongest
system of which we are aware (see Table 3).9
9 Multi-Lingual Evaluation
Last, we checked how our algorithms generalize out-
side English WSJ, by testing in 23 more set-ups: all
2006/7 CoNLL test sets (Buchholz and Marsi, 2006;
Nivre et al, 2007), spanning 19 languages. Most re-
cent work evaluates against this multi-lingual data,
with the unrealistic assumption of part-of-speech
tags. But since inducing high quality word clusters
for many languages would be beyond the scope of
our paper, here we too plugged in gold tags for word
categories (instead of unsupervised tags, as in ?3?8).
We compared to the two strongest systems we
knew:10 MZ (Marec?ek and ?Zabokrtsky?, 2012) and
SAJ (Spitkovsky et al, 2012b), which report average
accuracies of 40.0 and 42.9% for CoNLL data (see
Table 4). Our fully-trained IFJ and GT systems score
40.0 and 47.6%. As before, combining these net-
works with our own implementation of the best pre-
vious state-of-the-art system (SAJ) yields a further
improvement, increasing final accuracy to 48.6%.
9These numbers differ from Ponvert et al?s (2011, Table 6)
for the full Section 23 because we restricted their eval-ps.py
script to a maximum length of 40 words, in our evaluation, to
match other previous work: Golland et al?s (2012, Figure 1) for
CCM and LLCCM; Huang et al?s (2012, Table 2) for the rest.
10During review, another strong system (Marec?ek and Straka,
2013, scoring 48.7%) of possible interest to the reader came out,
exploiting prior knowledge of stopping probabilities (estimated
from large POS-tagged corpora, via reducibility principles).
System F1
Binary-Branching Upper Bound 85.7
Left-Branching Baseline 12.0
CCM (Klein and Manning, 2002) 33.7
Right-Branching Baseline 40.7
F-CCM (Huang et al, 2012) 45.1
HMM (Ponvert et al, 2011) 46.3
LLCCM (Golland et al, 2012) 47.6 P R
CCL (Seginer, 2007) 52.8 54.6 51.1
PRLG (Ponvert et al, 2011) 54.6 60.4 49.8
CS System Combination 54.2 55.6 52.8
Supervised DBM Skyline 59.3 65.7 54.1
Dependency-Based Upper Bound 87.2 100 77.3
Table 3: Harmonic mean (F1) of precision (P) and re-
call (R) for unlabeled constituent bracketings on Section
23 of WSJ (sentences up to length 40) for our combined
system (CS), recent state-of-the-art and the baselines.
10 Discussion
CoNLL training sets were intended for comparing
supervised systems, and aren?t all suitable for unsu-
pervised learning: 12 languages have under 10,000
sentences (with Arabic, Basque, Danish, Greek, Ital-
ian, Slovenian, Spanish and Turkish particularly
small), compared to WSJ?s nearly 50,000. In some
treebanks sentences are very short (e.g., Chinese and
Japanese, which appear to have been split on punc-
tuation), and in others extremely long (e.g., Arabic).
Even gold tags aren?t always helpful, as their num-
ber is rarely ideal for grammar induction (e.g., 42 vs.
200 for English). These factors contribute to high
variances of our (and previous) results (see Table 4).
Nevertheless, if we look at the more stable aver-
age accuracies, we see a positive trend as we move
from a simpler fully-trained system (IFJ, 40.0%),
to a more complex system (GT, 47.6%), to system
combination (CS, 48.6%). Grounding seems to be
more important for the CoNLL sets, possibly be-
cause of data sparsity or availability of gold tags.
11 Related Work
The surest way to avoid local optima is to craft
an objective that doesn?t have them. For example,
Wang et al (2008) demonstrated a convex train-
ing method for semi-supervised dependency pars-
ing; Lashkari and Golland (2008) introduced a con-
vex reformulation of likelihood functions for clus-
tering tasks; and Corlett and Penn (2010) designed
1990
Directed Dependency Accuracies (DDA) (@10)
CoNLL Data MZ SAJ IFJ GT CS
Arabic 2006 26.5 10.9 33.3 8.3 9.3 (30.2)
?7 27.9 44.9 26.1 25.6 26.8 (45.6)
Basque ?7 26.8 33.3 23.5 24.2 24.4 (32.8)
Bulgarian ?7 46.0 65.2 35.8 64.2 63.4 (69.1)
Catalan ?7 47.0 62.1 65.0 68.4 68.0 (79.2)
Chinese ?6 ? 63.2 56.0 55.8 58.4 (60.8)
?7 ? 57.0 49.0 48.6 52.5 (56.0)
Czech ?6 49.5 55.1 44.5 43.9 44.0 (52.3)
?7 48.0 54.2 42.9 24.5 34.3 (51.1)
Danish ?6 38.6 22.2 37.8 17.1 21.4 (29.8)
Dutch ?6 44.2 46.6 40.8 51.3 48.0 (48.7)
English ?7 49.2 29.6 39.3 57.6 58.2 (75.0)
German ?6 44.8 39.1 34.1 54.5 56.2 (71.2)
Greek ?6 20.2 26.9 23.7 45.0 45.4 (52.2)
Hungarian ?7 51.8 58.2 24.8 52.9 58.3 (67.6)
Italian ?7 43.3 40.7 56.8 31.1 34.9 (44.9)
Japanese ?6 50.8 22.7 32.6 63.7 63.0 (68.9)
Portuguese ?6 50.6 72.4 38.0 72.7 74.5 (81.1)
Slovenian ?6 18.1 35.2 42.1 50.8 50.9 (57.3)
Spanish ?6 51.9 28.2 57.0 61.7 61.4 (73.2)
Swedish ?6 48.2 50.7 46.6 48.6 49.7 (62.1)
Turkish ?6 ? 34.4 28.0 32.9 29.2 (33.2)
?7 15.7 44.8 42.1 41.7 37.9 (42.4)
Average: 40.0 42.9 40.0 47.6 48.6 (57.8)
Table 4: Blind evaluation on 2006/7 CoNLL test sets (all
sentences) for our full networks (IFJ and GT), previous
state-of-the-art systems of Spitkovsky et al (2012b) and
Marec?ek and ?Zabokrtsky? (2012), and three-way combi-
nation with SAJ (CS, including results up to length ten).
a search algorithm for encoding decipherment prob-
lems that guarantees to quickly converge on optimal
solutions. Convexity can be ideal for comparative
analyses, by eliminating dependence on initial con-
ditions. But for many NLP tasks, including grammar
induction, the most relevant known objective func-
tions are still riddled with local optima. Renewed ef-
forts to find exact solutions (Eisner, 2012; Gormley
and Eisner, 2013) may be a good fit for the smaller
and simpler, earlier stages of our iterative networks.
Multi-start methods (Solis and Wets, 1981) can
recover certain global extrema almost surely (i.e.,
with probability approaching one). Moreover, ran-
dom restarts via uniform probability measures can
be optimal, in a worst-case-analysis sense, with par-
allel processing sometimes leading to exponential
speed-ups (Hu et al, 1994). This approach is rarely
emphasized in NLP literature. For instance, Moore
and Quirk (2008) demonstrated consistent, substan-
tial gains from random restarts in statistical machine
translation (but also suggested better and faster re-
placements ? see below); Ravi and Knight (2009,
?5, Figure 8) found random restarts for EM to be
crucial in parts-of-speech disambiguation. However,
other reviews are few and generally negative (Kim
and Mooney, 2010; Martin-Brualla et al, 2010).
Iterated local search methods (Hoos and Stu?tzle,
2004; Johnson et al, 1988, inter alia) escape lo-
cal basins of attraction by perturbing candidate so-
lutions, without undoing all previous work. ?Large-
step? moves can come from jittering (Hinton and
Roweis, 2003), dithering (Price et al, 2005, Ch. 2)
or smoothing (Bhargava and Kondrak, 2009). Non-
improving ?sideways? moves offer substantial help
with hard satisfiability problems (Selman et al,
1992); and injecting non-random noise (Selman et
al., 1994), by introducing ?uphill? moves via mix-
tures of random walks and greedy search strate-
gies, does better than random noise alone or simu-
lated annealing (Kirkpatrick et al, 1983). In NLP,
Moore and Quirk?s (2008) random walks from pre-
vious local optima were faster than uniform sam-
pling and also increased BLEU scores; Elsner and
Schudy (2009) showed that local search can outper-
form greedy solutions for document clustering and
chat disentanglement tasks; and Mei et al (2001)
incorporated tabu search (Glover, 1989; Glover and
Laguna, 1993, Ch. 3) into HMM training for ASR.
Genetic algorithms are a fusion of what?s best in
local search and multi-start methods (Houck et al,
1996), exploiting a problem?s structure to combine
valid parts of any partial solutions (Holland, 1975;
Goldberg, 1989). Evolutionary heuristics proved
useful in the induction of phonotactics (Belz, 1998),
text planning (Mellish et al, 1998), factored mod-
eling of morphologically-rich languages (Duh and
Kirchhoff, 2004) and plot induction for story gener-
ation (McIntyre and Lapata, 2010). Multi-objective
genetic algorithms (Fonseca and Fleming, 1993) can
handle problems with equally important but con-
flicting criteria (Stadler, 1988), using Pareto-optimal
ensembles. They are especially well-suited to lan-
guage, which evolves under pressures from compet-
ing (e.g., speaker, listener and learner) constraints,
and have been used to model configurations of vow-
els and tone systems (Ke et al, 2003). Our transform
and join mechanisms also exhibit some features of
genetic search, and make use of competing objec-
1991
tives: good sets of parse trees must make sense both
lexicalized and with word categories, to rich and im-
poverished models of grammar, and for both long,
complex sentences and short, simple text fragments.
This selection of text filters is a specialized case
of more general ?data perturbation? techniques ?
even cycling over randomly chosen mini-batches
that partition a data set helps avoid some local op-
tima (Liang and Klein, 2009). Elidan et al (2002)
suggested how example-reweighing could cause ?in-
formed? changes, rather than arbitrary damage, to
a hypothesis. Their (adversarial) training scheme
guided learning toward improved generalizations,
robust against input fluctuations. Language learn-
ing has a rich history of reweighing data via (co-
operative) ?starting small? strategies (Elman, 1993),
beginning from simpler or more certain cases. This
family of techniques has met with success in semi-
supervised named entity classification (Collins and
Singer, 1999; Yarowsky, 1995),11 parts-of-speech
induction (Clark, 2000; 2003), and language model-
ing (Krueger and Dayan, 2009; Bengio et al, 2009),
in addition to unsupervised parsing (Spitkovsky et
al., 2009; Tu and Honavar, 2011; Cohn et al, 2011).
12 Conclusion
We proposed several simple algorithms for combin-
ing grammars and showed their usefulness in merg-
ing the outputs of iterative and static grammar in-
duction systems. Unlike conventional system com-
bination methods, e.g., in machine translation (Xiao
et al, 2010), ours do not require incoming mod-
els to be of similar quality to make improvements.
We exploited these properties of the combiners to
reconcile grammars induced by different views of
data (Blum and Mitchell, 1998). One such view re-
tains just the simple sentences, making it easier to
recognize root words. Another splits text into many
inter-punctuation fragments, helping learn word as-
sociations. The induced dependency trees can them-
selves also be viewed not only as directed structures
but also as skeleton parses, facilitating the recovery
of correct polarities for unlabeled dependency arcs.
By reusing templates, as in dynamic Bayesian
network (DBN) frameworks (Koller and Friedman,
11The so-called Yarowsky-cautious modification of the orig-
inal algorithm for unsupervised word-sense disambiguation.
2009, ?6.2.2), we managed to specify relatively
?deep? learning architectures without sacrificing
(too much) clarity or simplicity. On a still more
speculative note, we see two (admittedly, tenuous)
connections to human cognition. First, the benefits
of not normalizing probabilities, when symmetriz-
ing, might be related to human language process-
ing through the base-rate fallacy (Bar-Hillel, 1980;
Kahneman and Tversky, 1982) and the availability
heuristic (Chapman, 1967; Tversky and Kahneman,
1973), since people are notoriously bad at probabil-
ity (Attneave, 1953; Kahneman and Tversky, 1972;
Kahneman and Tversky, 1973). And second, inter-
mittent ?unlearning? ? though perhaps not of the
kind that takes place inside of our transforms ?
is an adaptation that can be essential to cognitive
development in general, as evidenced by neuronal
pruning in mammals (Craik and Bialystok, 2006;
Low and Cheng, 2006). ?Forgetful EM? strategies
that reset subsets of parameters may thus, possibly,
be no less relevant to unsupervised learning than is
?partial EM,? which only suppresses updates, other
EM variants (Neal and Hinton, 1999), or ?dropout
training? (Hinton et al, 2012; Wang and Manning,
2013), which is important in supervised settings.
Future parsing models, in grammar induction,
may benefit by modeling head-dependent relations
separately from direction. As frequently employed
in tasks like semantic role labeling (Carreras and
Ma`rquez, 2005) and relation extraction (Sun et al,
2011), it may be easier to first establish existence,
before trying to understand its nature. Other key
next steps may include exploring more intelligent
ways of combining systems (Surdeanu and Man-
ning, 2010; Petrov, 2010) and automating the op-
erator discovery process. Furthermore, we are opti-
mistic that both count transforms and model recom-
bination could be usefully incorporated into sam-
pling methods: although symmetrized models may
have higher cross-entropies, hence prone to rejection
in vanilla MCMC, they could work well as seeds
in multi-chain designs; existing algorithms, such as
MCMCMC (Geyer, 1991), which switch contents
of adjacent chains running at different temperatures,
may also benefit from introducing the option to com-
bine solutions, in addition to just swapping them.
1992
Acknowledgments
We thank Yun-Hsuan Sung, for early-stage discussions
on ways of extending ?baby steps,? Elias Ponvert, for
sharing all of the relevant experimental results and eval-
uation scripts from his work with Jason Baldridge and
Katrin Erk, and the anonymous reviewers, for their
helpful comments on the draft version of this paper.
Funded, in part, by Defense Advanced Research Projects
Agency (DARPA) Deep Exploration and Filtering of
Text (DEFT) Program, under Air Force Research Lab-
oratory (AFRL) prime contract no. FA8750-13-2-0040.
Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. Once again, the
first author thanks Moofus.
References
H. Alshawi. 1996. Head automata for speech translation. In
ICSLP.
F. Attneave. 1953. Psychological probability as a function of
experienced frequency. Experimental Psychology, 46.
M. Bar-Hillel. 1980. The base-rate fallacy in probability judg-
ments. Acta Psychologica, 44.
A. Belz. 1998. Discovering phonotactic finite-state automata
by genetic search. In COLING-ACL.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
A. Bhargava and G. Kondrak. 2009. Multiple word alignment
with profile hidden Markov models. In NAACL-HLT: Stu-
dent Research and Doctoral Consortium.
Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar
induction with combinatory categorial grammars. In AAAI.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree
substitution grammars for dependency parsing. In EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. In CoNLL.
L. J. Chapman. 1967. Illusory correlation in observational re-
port. Verbal Learning and Verbal Behavior, 6.
A. Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In CoNLL-LLL.
A. Clark. 2003. Combining distributional and morphological
information for part of speech induction. In EACL.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. JMLR.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
E. Corlett and G. Penn. 2010. An exact A? method for deci-
phering letter-substitution ciphers. In ACL.
F. I. M. Craik and E. Bialystok. 2006. Cognition through the
lifespan: mechanisms of change. TRENDS in Cognitive Sci-
ences, 10.
C. de Marcken. 1995. Lexical heads, phrase structure and the
induction of grammar. In WVLC.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In COLING.
J. Eisner. 2012. Grammar induction: Beyond local search. In
ICGI.
G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002.
Data perturbation for escaping local maxima in learning. In
AAAI.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
M. Elsner and W. Schudy. 2009. Bounding and comparing
methods for correlation clustering beyond ILP. In NAACL-
HLT: Integer Linear Programming for NLP.
C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for
multiobjective optimization: Formulation, discussion and
generalization. In ICGA.
C. J. Geyer. 1991. Markov chain Monte Carlo maximum like-
lihood. In Interface Symposium.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2012. Concavity and initialization
for unsupervised dependency parsing. In NAACL-HLT.
F. Glover and M. Laguna. 1993. Tabu search. In C. R.
Reeves, editor, Modern Heuristic Techniques for Combina-
torial Problems. Blackwell Scientific Publications.
F. Glover. 1989. Tabu search ? Part I. ORSA Journal on
Computing, 1.
D. E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization & Machine Learning. Addison-Wesley.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature-
rich constituent context model for grammar induction. In
EMNLP-CoNLL.
M. R. Gormley and J. Eisner. 2013. Nonconvex global opti-
mization for latent-variable models. In ACL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor embed-
ding. In NIPS.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. In ArXiv.
J. H. Holland. 1975. Adaptation in Natural and Artificial Sys-
tems: An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence. University of Michigan
Press.
H. H. Hoos and T. Stu?tzle. 2004. Stochastic Local Search:
Foundations and Applications. Morgan Kaufmann.
1993
C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison
of genetic algorithms, random restart, and two-opt switching
for solving large location-allocation problems. Computers
& Operations Research, 23.
X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random
restarts in global optimization. Technical report, GT.
Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved con-
stituent context model with features. In PACLIC.
F. Jelinek and R. L. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pattern
Recognition in Practice.
D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988.
How easy is local search? Journal of Computer and System
Sciences, 37.
D. Kahneman and A. Tversky. 1972. Subjective probability: A
judgment of representativeness. Cognitive Psychology, 3.
D. Kahneman and A. Tversky. 1973. On the psychology of
prediction. Psychological Review, 80.
D. Kahneman and A. Tversky. 1982. Evidential impact of base
rates. In D. Kahneman, P. Slovic, and A. Tversky, editors,
Judgment under uncertainty: Heuristics and biases. Cam-
bridge University Press.
J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization mod-
els of sound systems using genetic algorithms. Computa-
tional Linguistics, 29.
J. Kim and R. J. Mooney. 2010. Generative alignment and
semantic parsing for learning from ambiguous supervision.
In COLING.
S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Opti-
mization by simulated annealing. Science, 220.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphical
Models: Principles and Techniques. MIT Press.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
D. Lashkari and P. Golland. 2008. Convex clustering with
exemplar-based models. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsupervised
models. In NAACL-HLT.
L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essen-
tial step underlying the developmental plasticity of neuronal
connections. Royal Society of London Philosophical Trans-
actions Series B, 361.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and M. Straka. 2013. Stop-probability estimates
computed on a large corpus improve unsupervised depen-
dency parsing. In ACL.
D. Marec?ek and Z. ?Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. Marec?ek and Z. ?Zabokrtsky?. 2012. Exploiting reducibility
in unsupervised dependency parsing. In EMNLP-CoNLL.
R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo-
Arnuncio, and M. Ciaramita. 2010. Instance sense induction
from attribute sets. In COLING.
N. McIntyre and M. Lapata. 2010. Plot induction and evolu-
tionary search for story generation. In ACL.
X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Op-
timization of HMM by the tabu search algorithm. In RO-
CLING.
C. Mellish, A. Knott, J. Oberlander, and M. O?Donnell. 1998.
Experiments using stochastic search for text planning. In
INLG.
R. C. Moore and C. Quirk. 2008. Random restarts in min-
imum error rate training for statistical machine translation.
In COLING.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
R. M. Neal and G. E. Hinton. 1999. A view of the EM al-
gorithm that justifies incremental, sparse, and other variants.
In M. I. Jordan, editor, Learning in Graphical Models. MIT
Press.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
S. Petrov. 2010. Products of random latent variable grammars.
In NAACL-HLT.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differ-
ential Evolution: A Practical Approach to Global Optimiza-
tion. Springer.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression and related optmization
problems. Proceedings of the IEEE, 86.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
B. Selman, H. Levesque, and D. Mitchell. 1992. A new method
for solving hard satisfiability problems. In AAAI.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies
for improving local search. In AAAI.
F. J. Solis and R. J.-B. Wets. 1981. Minimization by random
search techniques. Mathematics of Operations Research, 6.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In GRLL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning.
2010. Viterbi training improves unsupervised dependency
parsing. In CoNLL.
1994
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models. In ICGI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three
dependency-and-boundary models for grammar induction.
In EMNLP-CoNLL.
W. Stadler, editor. 1988. Multicriteria Optimization in Engi-
neering and in the Sciences. Plenum Press.
A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised
relation extraction with large-scale word clustering. In ACL.
M. Surdeanu and C. D. Manning. 2010. Ensemble models for
dependency parsing: Cheap and good? In NAACL-HLT.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
K. Tu and V. Honavar. 2012. Unambiguity regularization
for unsupervised learning of probabilistic grammars. In
EMNLP-CoNLL.
A. Tversky and D. Kahneman. 1973. Availability: A heuristic
for judging frequency and probability. Cognitive Psychol-
ogy, 5.
S. I. Wang and C. D. Manning. 2013. Fast dropout training. In
ICML.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised convex training for dependency parsing. In HLT-
ACL.
T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based
system combination for machine translation. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In ACL.
1995
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 751?759,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
From Baby Steps to Leapfrog: How ?Less is More?
in Unsupervised Dependency Parsing?
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present three approaches for unsupervised
grammar induction that are sensitive to data
complexity and apply them to Klein and Man-
ning?s Dependency Model with Valence. The
first, Baby Steps, bootstraps itself via iterated
learning of increasingly longer sentences and
requires no initialization. This method sub-
stantially exceeds Klein and Manning?s pub-
lished scores and achieves 39.4% accuracy on
Section 23 (all sentences) of the Wall Street
Journal corpus. The second, Less is More,
uses a low-complexity subset of the avail-
able data: sentences up to length 15. Focus-
ing on fewer but simpler examples trades off
quantity against ambiguity; it attains 44.1%
accuracy, using the standard linguistically-
informed prior and batch training, beating
state-of-the-art. Leapfrog, our third heuristic,
combines Less is More with Baby Steps by
mixing their models of shorter sentences, then
rapidly ramping up exposure to the full train-
ing set, driving up accuracy to 45.0%. These
trends generalize to the Brown corpus; aware-
ness of data complexity may improve other
parsing models and unsupervised algorithms.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is a
hard problem whose eventual solution promises to
benefit applications ranging from question answer-
ing to speech recognition and machine translation.
A restricted version that targets dependencies and
?Partially funded by NSF award IIS-0811974; first author
supported by the Fannie & John Hertz Foundation Fellowship.
assumes partial annotation, e.g., sentence bound-
aries, tokenization and typically even part-of-speech
(POS) tagging, has received much attention, elicit-
ing a diverse array of techniques (Smith and Eis-
ner, 2005; Seginer, 2007; Cohen et al, 2008). Klein
and Manning?s (2004) Dependency Model with Va-
lence (DMV) was the first to beat a simple parsing
heuristic ? the right-branching baseline. Today?s
state-of-the-art systems (Headden et al, 2009; Co-
hen and Smith, 2009) are still rooted in the DMV.
Despite recent advances, unsupervised parsers lag
far behind their supervised counterparts. Although
large amounts of unlabeled data are known to im-
prove semi-supervised parsing (Suzuki et al, 2009),
the best unsupervised systems use less data than is
available for supervised training, relying on complex
models instead: Headden et al?s (2009) Extended
Valence Grammar (EVG) combats data sparsity with
smoothing alone, training on the same small subset
of the tree-bank as the classic implementation of the
DMV; Cohen and Smith (2009) use more compli-
cated algorithms (variational EM and MBR decod-
ing) and stronger linguistic hints (tying related parts
of speech and syntactically similar bilingual data).
We explore what can be achieved through judi-
cious use of data and simple, scalable techniques.
Our first approach iterates over a series of training
sets that gradually increase in size and complex-
ity, forming an initialization-independent scaffold-
ing for learning a grammar. It works with Klein and
Manning?s simple model (the original DMV) and
training algorithm (classic EM) but eliminates their
crucial dependence on manually-tuned priors. The
second technique is consistent with the intuition that
learning is most successful within a band of the size-
complexity spectrum. Both could be applied to more
751
intricate models and advanced learning algorithms.
We combine them in a third, efficient hybrid method.
2 Intuition
Focusing on simple examples helps guide unsuper-
vised learning,1 as blindly added confusing data can
easily mislead training. We suggest that unless it is
increased gradually, unbridled, complexity can over-
whelm a system. How to grade an example?s diffi-
culty? The cardinality of its solution space presents
a natural proxy. In the case of parsing, the num-
ber of possible syntactic trees grows exponentially
with sentence length. For longer sentences, the un-
supervised optimization problem becomes severely
under-constrained, whereas for shorter sentences,
learning is tightly reined in by data. In the extreme
case of a single-word sentence, there is no choice
but to parse it correctly. At two words, a raw 50%
chance of telling the head from its dependent is still
high, but as length increases, the accuracy of even
educated guessing rapidly plummets. In model re-
estimation, long sentences amplify ambiguity and
pollute fractional counts with noise. At times, batch
systems are better off using less data.
Baby Steps: Global non-convex optimization is
hard. We propose a meta-heuristic that takes the
guesswork out of initializing local search. Begin-
ning with an easy (convex) case, it slowly extends it
to the fully complex target task by taking tiny steps
in the problem space, trying not to stray far from
the relevant neighborhoods of the solution space. A
series of nested subsets of increasingly longer sen-
tences that culminates in the complete data set offers
a natural progression. Its base case ? sentences of
length one ? has a trivial solution that requires nei-
ther initialization nor search yet reveals something
of sentence heads. The next step ? sentences of
length one and two ? refines initial impressions
of heads, introduces dependents, and exposes their
identities and relative positions. Although not rep-
resentative of the full grammar, short sentences cap-
ture enough information to paint most of the picture
needed by slightly longer sentences. They set up an
easier, incremental subsequent learning task. Step
k + 1 augments training input to include lengths
1It mirrors the effect that boosting hard examples has for
supervised training (Freund and Schapire, 1997).
1, 2, . . . , k, k + 1 of the full data set and executes
local search starting from the (smoothed) model es-
timated by step k. This truly is grammar induction.
Less is More: For standard batch training, just us-
ing simple, short sentences is not enough. They are
rare and do not reveal the full grammar. We find a
?sweet spot? ? sentence lengths that are neither too
long (excluding the truly daunting examples) nor too
few (supplying enough accessible information), us-
ing Baby Steps? learning curve as a guide. We train
where it flattens out, since remaining sentences con-
tribute little (incremental) educational value.2
Leapfrog: As an alternative to discarding data, a
better use of resources is to combine the results of
batch and iterative training up to the sweet spot data
gradation, then iterate with a large step size.
3 Related Work
Two types of scaffolding for guiding language learn-
ing debuted in Elman?s (1993) experiments with
?starting small?: data complexity (restricting input)
and model complexity (restricting memory). In both
cases, gradually increasing complexity allowed ar-
tificial neural networks to master a pseudo-natural
grammar they otherwise failed to learn. Initially-
limited capacity resembled maturational changes in
working memory and attention span that occur over
time in children (Kail, 1984), in line with the ?less
is more? proposal (Newport, 1988; 1990). Although
Rohde and Plaut (1999) failed to replicate this3 re-
sult with simple recurrent networks, other machine
learning techniques reliably benefit from scaffolded
model complexity on a variety of language tasks.
In word-alignment, Brown et al (1993) used IBM
Models 1-4 as ?stepping stones? to training Model 5.
Other prominent examples include ?coarse-to-fine?
2This is akin to McClosky et al?s (2006) ?Goldilocks effect.?
3Worse, they found that limiting input hindered language
acquisition. And making the grammar more English-like (by
introducing and strengthening semantic constraints), increased
the already significant advantage for ?starting large!? With it-
erative training invoking the optimizer multiple times, creating
extra opportunities to converge, Rohde and Plaut (1999) sus-
pected that Elman?s (1993) simulations simply did not allow
networks exposed exclusively to complex inputs sufficient train-
ing time. Our extremely generous, low termination threshold
for EM (see ?5.1) addresses this concern. However, given the
DMV?s purely syntactic POS tag-based approach (see ?5), it
would be prudent to re-test Baby Steps with a lexicalized model.
752
approaches to parsing, translation and speech recog-
nition (Charniak and Johnson, 2005; Charniak et al,
2006; Petrov et al, 2008; Petrov, 2009), and re-
cently unsupervised POS tagging (Ravi and Knight,
2009). Initial models tend to be particularly simple,4
and each refinement towards a full model introduces
only limited complexity, supporting incrementality.
Filtering complex data, the focus of our work,
is unconventional in natural language processing.
Such scaffolding qualifies as shaping ? a method
of instruction (routinely exploited in animal train-
ing) in which the teacher decomposes a complete
task into sub-components, providing an easier path
to learning. When Skinner (1938) coined the term,
he described it as a ?method of successive approx-
imations.? Ideas that gradually make a task more
difficult have been explored in robotics (typically,
for navigation), with reinforcement learning (Singh,
1992; Sanger, 1994; Saksida et al, 1997; Dorigo
and Colombetti, 1998; Savage, 1998; Savage, 2001).
Recently, Krueger and Dayan (2009) showed that
shaping speeds up language acquisition and leads
to better generalization in abstract neural networks.
Bengio et al (2009) confirmed this for deep de-
terministic and stochastic networks, using simple
multi-stage curriculum strategies. They conjectured
that a well-chosen sequence of training criteria ?
different sets of weights on the examples ? could
act as a continuation method (Allgower and Georg,
1990), helping find better local optima for non-
convex objectives. Elman?s learners constrained the
peaky solution space by focusing on just the right
data (simple sentences that introduced basic repre-
sentational categories) at just the right time (early
on, when their plasticity was greatest). Self-shaping,
they simplified tasks through deliberate omission (or
misunderstanding). Analogously, Baby Steps in-
duces an early structural locality bias (Smith and
Eisner, 2006), then relaxes it, as if annealing (Smith
and Eisner, 2004). Its curriculum of binary weights
initially discards complex examples responsible for
?high-frequency noise,? with earlier, ?smoothed?
objectives revealing more of the global picture.
There are important differences between our re-
sults and prior work. In contrast to Elman, we use a
4Brown et al?s (1993) Model 1 (and, similarly, the first baby
step) has a global optimum that can be computed exactly, so that
no initial or subsequent parameters depend on initialization.
large data set (WSJ) of real English. Unlike Bengio
et al and Krueger and Dayan, we shape a parser, not
a language model. Baby Steps is similar, in spirit, to
Smith and Eisner?s methods. Deterministic anneal-
ing (DA) shares nice properties with Baby Steps,
but performs worse than EM for (constituent) pars-
ing; Baby Steps handedly defeats standard training.
Structural annealing works well, but requires a hand-
tuned annealing schedule and direct manipulation of
the objective function; Baby Steps works ?out of the
box,? its locality biases a natural consequence of a
complexity/data-guided tour of optimization prob-
lems. Skewed DA incorporates a good initializer
by interpolating between two probability distribu-
tions, whereas our hybrid, Leapfrog, admits multi-
ple initializers by mixing structures instead. ?Less
is More? is novel and confirms the tacit consensus
implicit in training on small data sets (e.g., WSJ10).
4 Data Sets and Metrics
Klein and Manning (2004) both trained and tested
the DMV on the same customized subset (WSJ10)
of Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993). Its 49,208 annotated
parse trees were pruned5 down to 7,422 sentences
of at most 10 terminals, spanning 35 unique POS
tags. Following standard practice, automatic ?head-
percolation? rules (Collins, 1999) were used to con-
vert the remaining trees into dependencies. Forced
to produce a single ?best? parse, their algorithm
was judged on accuracy: its directed score was the
fraction of correct dependencies; a more flattering6
undirected score was also used. We employ the
same metrics, emphasizing directed scores, and gen-
eralize WSJk to be the subset of pre-processed sen-
tences with at most k terminals. Our experiments fo-
cus on k ? {1, . . . , 45}, but we also test on WSJ100
and Section 23 of WSJ? (the entire WSJ), as well as
the held-out Brown100 (similarly derived from the
Brown corpus (Francis and Kucera, 1979)). See Fig-
ure 1 for these corpora?s sentence and token counts.
5Stripped of all empty sub-trees, punctuation, and terminals
(tagged # and $) not pronounced where they appear, those sen-
tences still containing more than ten tokens were thrown out.
6Ignoring polarity of parent-child relations partially ob-
scured effects of alternate analyses (systematic choices between
modals and main verbs for heads of sentences, determiners for
noun phrases, etc.) and facilitated comparison with prior work.
753
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100.
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A simple dependency structure for a short sen-
tence and its probability, as factored by the DMV.
5 New Algorithms for the Classic Model
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over lex-
ical word classes {cw} ? POS tags. Its generative
story for a sub-tree rooted at a head (of class ch) rests
on three types of independent decisions: (i) initial
direction dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e.,
adjacent, child); and (iii) attachments (of class ca),
according to PATTACH(ch, dir, ca). This produces only
projective trees.7 A root token ? generates the head
of a sentence as its left (and only) child. Figure 2
displays an example that ignores (sums out) PORDER.
The DMV lends itself to unsupervised learn-
7Unlike spanning tree algorithms (McDonald et al, 2005),
DMV?s chart-based method disallows crossing dependencies.
ing via inside-outside re-estimation (Baker, 1979).
Klein and Manning did not use smoothing and
started with an ?ad-hoc harmonic? completion: aim-
ing for balanced trees, non-root heads attached de-
pendents in inverse proportion to (a constant plus)
their distance; ? generated heads uniformly at ran-
dom. This non-distributional heuristic created favor-
able initial conditions that nudged EM towards typi-
cal linguistic dependency structures.
5.1 Algorithm #0: Ad-Hoc?
? A Variation on Original Ad-Hoc Initialization
Since some of the important implementation details
are not available in the literature (Klein and Man-
ning, 2004; Klein, 2005), we had to improvise ini-
tialization and terminating conditions. We suspect
that our choices throughout this section do not match
Klein and Manning?s actual training of the DMV.
We use the following ad-hoc harmonic scores (for
all tokens other than ?): P?ORDER ? 1/2;
P?STOP ? (ds + ?s)?1 = (ds + 3)?1, ds ? 0;
P?ATTACH ? (da + ?a)?1 = (da + 2)?1, da ? 1.
Integers d{s,a} are distances from heads to stopping
boundaries and dependents.8 We initialize train-
ing by producing best-scoring parses of all input
sentences and converting them into proper proba-
bility distributions PSTOP and PATTACH via maximum-
likelihood estimation (a single step of Viterbi train-
ing (Brown et al, 1993)). Since left and right chil-
dren are independent, we drop PORDER altogether, mak-
8Constants ?{s,a} come from personal communication.
Note that ?s is one higher than is strictly necessary to avoid both
division by zero and determinism; ?a could have been safely ze-
roed out, since we never compute 1 ? PATTACH (see Figure 2).
754
ing ?headedness? deterministic. Our parser care-
fully randomizes tie-breaking, so that all parse trees
having the same score get an equal shot at being
selected (both during initialization and evaluation).
We terminate EM when a successive change in over-
all per-token cross-entropy drops below 2?20 bits.
5.2 Algorithm #1: Baby Steps
? An Initialization-Independent Scaffolding
We eliminate the need for initialization by first train-
ing on a trivial subset of the data ? WSJ1; this
works, since there is only one (the correct) way to
parse a single-token sentence. We plug the resulting
model into training on WSJ2 (sentences of up to two
tokens), and so forth, building up to WSJ45.9 This
algorithm is otherwise identical to Ad-Hoc?, with
the exception that it re-estimates each model using
Laplace smoothing, so that earlier solutions could
be passed to next levels, which sometimes contain
previously unseen dependent and head POS tags.
5.3 Algorithm #2: Less is More
? Ad-Hoc? where Baby Steps Flatlines
We jettison long, complex sentences and deploy Ad-
Hoc??s initializer and batch training at WSJk?? ? an
estimate of the sweet spot data gradation. To find
it, we track Baby Steps? successive models? cross-
entropies on the complete data set, WSJ45. An ini-
tial segment of rapid improvement is separated from
the final region of convergence by a knee ? points
of maximum curvature (see Figure 3). We use an
improved10 L method (Salvador and Chan, 2004) to
automatically locate this area of diminishing returns.
Specifically, we determine its end-points [k0, k?] by
minimizing squared error, estimating k?0 = 7 and
k?? = 15. Training at WSJ15 just misses the plateau.
5.4 Algorithm #3: Leapfrog
? A Practical and Efficient Hybrid Mixture
Cherry-picking the best features of ?Less is More?
and Baby Steps, we begin by combining their mod-
9Its 48,418 sentences (see Figure 1) cover 94.4% of all sen-
tences in WSJ; the longest of the missing 790 has length 171.
10Instead of iteratively fitting a two-segment form and adap-
tively discarding its tail, we use three line segments, applying
ordinary least squares to the first two, but requiring the third to
be horizontal and tangent to a minimum. The result is a batch
optimization routine that returns an interval for the knee, rather
than a point estimate (see Figure 3 for details).
5 10 15 20 25 30 35 40 45
3.0
3.5
4.0
4.5
5.0
WSJk
bpt
Cross-entropy h (in bits per token) on WSJ45
Knee
[7, 15] Tight, Flat, Asymptotic Bound
min
b0,m0,b1,m1
2<k0<k?<45
8
>>
>>
>
>
<
>
>
>
>>
>:
k0?1X
k=1
(hk ? b0 ? m0k)2 +
k?X
k=k0
(hk ? b1 ? m1k)2 +
45X
k=k?+1
?
hk ?
45
min
j=k?+1
hj
?2
Figure 3: Cross-entropy on WSJ45 after each baby step, a
piece-wise linear fit, and an estimated region for the knee.
els at WSJk??. Using one best parse from each,
for every sentence in WSJk??, the base case re-
estimates a new model from a mixture of twice the
normal number of trees; inductive steps leap over k??
lengths, conveniently ending at WSJ45, and estimate
their initial models by applying a previous solution
to a new input set. Both follow up the single step of
Viterbi training with at most five iterations of EM.
Our hybrid makes use of two good (condition-
ally) independent initialization strategies and exe-
cutes many iterations of EM where that is cheap ?
at shorter sentences (WSJ15 and below). It then in-
creases the step size, training just three more times
(at WSJ{15, 30, 45}) and allowing only a few (more
expensive) iterations of EM. Early termination im-
proves efficiency and regularizes these final models.
5.5 Reference Algorithms
? Baselines, a Skyline and Published Art
We carve out the problem space using two extreme
initialization strategies: (i) the uninformed uniform
prior, which serves as a fair ?zero-knowledge? base-
line for comparing uninitialized models; and (ii) the
maximum-likelihood ?oracle? prior, computed from
reference parses, which yields a skyline (a reverse
baseline) ? how well any algorithm that stumbled
on the true solution would fare at EM?s convergence.
In addition to citing Klein and Manning?s (2004)
results, we compare our accuracies on Section 23
of WSJ? to two state-of-the-art systems and past
baselines (see Table 2). Headden et al?s (2009)
lexicalized EVG is the best on short sentences, but
755
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
90
Oracle
Baby StepsAd-Hoc
Uninformed
WSJk
(a) Directed Accuracy (%) on WSJk
5 10 15 20 25 30 35 40 45
(b) Undirected Accuracy (%) on WSJk
Oracle
Baby Steps
Ad-Hoc
Uninformed
Figure 4: Directed and undirected accuracy scores attained by the DMV, when trained and tested on the same gradation
of WSJ, for several different initialization strategies. Green circles mark Klein and Manning?s (2004) published scores;
red, violet and blue curves represent the supervised (maximum-likelihood oracle) initialization, Baby Steps, and the
uninformed uniform prior. Dotted curves reflect starting performance, solid curves register performance at EM?s
convergence, and the arrows connecting them emphasize the impact of learning.
5 10 15 20 25 30 35 40 45
20
30
40
50
60
WSJk
Oracle
Leapfrog
Baby Steps
Ad-Hoc?
Uninformed
Ad-Hoc
Directed Accuracy (%) on WSJk
Figure 5: Directed accuracies for Ad-Hoc? (shown in
green) and Leapfrog (in gold); all else as in Figure 4(a).
its performance is unreported for longer sentences,
for which Cohen and Smith?s (2009) seem to be
the highest published scores; we include their in-
termediate results that preceded parameter-tying ?
Bayesian models with Dirichlet and log-normal pri-
ors, coupled with both Viterbi and minimum Bayes-
risk (MBR) decoding (Cohen et al, 2008).
6 Experimental Results
We packed thousands of empirical outcomes into the
space of several graphs (Figures 4, 5 and 6). The col-
ors (also in Tables 1 and 2) correspond to different
initialization strategies ? to a first approximation,
the learning algorithm was held constant (see ?5).
Figures 4 and 5 tell one part of our story. As data
sets increase in size, training algorithms gain access
to more information; however, since in this unsu-
pervised setting training and test sets are the same,
additional longer sentences make for substantially
more challenging evaluation. To control for these
dynamics, we applied Laplace smoothing to all (oth-
erwise unsmoothed) models and re-plotted their per-
formance, holding several test sets fixed, in Figure 6.
We report undirected accuracies parenthetically.
6.1 Result #1: Baby Steps
Figure 4 traces out performance on the training set.
Klein and Manning?s (2004) published scores ap-
pear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%).
Baby Steps achieves 53.0% (65.7%) by WSJ10;
trained and tested on WSJ45, it gets 39.7% (54.3%).
Uninformed, classic EM learns little about directed
dependencies: it improves only slightly, e.g., from
17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learn-
ing some of the structure, as evidenced by its undi-
rected scores), but degrades with shorter sentences,
where its initial guessing rate is high. In the case
of oracle training, we expected EM to walk away
from supervised solutions (Elworthy, 1994; Meri-
756
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
(a) Directed Accuracy (%) on WSJ10
WSJk
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Ad-Hoc
Uninformed
5 10 15 20 25 30 35 40 45
(b) Directed Accuracy (%) on WSJ40
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Uninformed
Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested
against fixed evaluation sets ? WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40.
aldo, 1994; Liang and Klein, 2008), but the ex-
tent of its drops is alarming, e.g., from the super-
vised 69.8% (72.2%) to the skyline?s 50.6% (59.5%)
on WSJ45. In contrast, Baby Steps? scores usu-
ally do not change much from one step to the
next, and where its impact of learning is big (at
WSJ{4, 5, 14}), it is invariably positive.
6.2 Result #2: Less is More
Ad-Hoc??s curve (see Figure 5) suggests how Klein
and Manning?s Ad-Hoc initializer may have scaled
with different gradations of WSJ. Strangely, our im-
plementation performs significantly above their re-
ported numbers at WSJ10: 54.5% (68.3%) is even
slightly higher than Baby Steps; nevertheless, given
enough data (from WSJ22 onwards), Baby Steps
overtakes Ad-Hoc?, whose ability to learn takes a se-
rious dive once the inputs become sufficiently com-
plex (at WSJ23), and never recovers. Note that Ad-
Hoc??s biased prior peaks early (at WSJ6), eventu-
ally falls below the guessing rate (by WSJ24), yet
still remains well-positioned to climb, outperform-
ing uninformed learning.
Figure 6 shows that Baby Steps scales better with
more (complex) data ? its curves do not trend
downwards. However, a good initializer induces a
sweet spot at WSJ15, where the DMV is learned
best using Ad-Hoc?. This mode is ?Less is More,?
scoring 44.1% (58.9%) on WSJ45. Curiously, even
oracle training exhibits a bump at WSJ15: once sen-
tences get long enough (at WSJ36), its performance
degrades below that of oracle training with virtually
no supervision (at the hardly representative WSJ3).
6.3 Result #3: Leapfrog
Mixing Ad-Hoc? with Baby Steps at WSJ15 yields
a model whose performance initially falls between
its two parents but surpasses both with a little train-
ing (see Figure 5). Leaping to WSJ45, via WSJ30,
results in our strongest model: its 45.0% (58.4%) ac-
curacy bridges half of the gap between Baby Steps
and the skyline, and at a tiny fraction of the cost.
6.4 Result #4: Generalization
Our models carry over to the larger WSJ100, Section
23 of WSJ?, and the independent Brown100 (see
Table 1). Baby Steps improves out of domain, con-
firming that shaping generalizes well (Krueger and
Dayan, 2009; Bengio et al, 2009). Leapfrog does
best across the board but dips on Brown100, despite
its safe-guards against over-fitting.
Section 23 (see Table 2) reveals, unexpectedly,
that Baby Steps would have been state-of-the-art in
2008, whereas ?Less is More? outperforms all prior
work on longer sentences. Baby Steps is competi-
tive with log-normal families (Cohen et al, 2008),
scoring slightly better on longer sentences against
Viterbi decoding, though worse against MBR. ?Less
is More? beats state-of-the-art on longer sentences
by close to 2%; Leapfrog gains another 1%.
757
Ad-Hoc? Baby Steps Leapfrog Ad-Hoc? Baby Steps Leapfrog
Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4)
WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45
Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1)
Table 1: Directed and undirected accuracies on Section 23 of WSJ?, WSJ100 and Brown100 for Ad-Hoc?, Baby
Steps and Leapfrog, trained at WSJ15 and WSJ45.
Decoding WSJ10 WSJ20 WSJ?
Attach-Right (Klein and Manning, 2004) ? 38.4 33.4 31.7
DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2
Dirichlet (Cohen et al, 2008) Viterbi 45.9 39.4 34.9
Ad-Hoc (Cohen et al, 2008) MBR 46.1 39.9 35.9
Dirichlet (Cohen et al, 2008) MBR 46.1 40.6 36.9
Log-Normal Families (Cohen et al, 2008) Viterbi 59.3 45.1 39.0
Baby Steps (@15) Viterbi 55.5 44.3 39.2
Baby Steps (@45) Viterbi 55.1 44.4 39.4
Log-Normal Families (Cohen et al, 2008) MBR 59.4 45.9 40.5
Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4
Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2
Less is More (Ad-Hoc? @15) Viterbi 56.2 48.2 44.1
Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0
EVG Smoothed (skip-val) (Headden et al, 2009) Viterbi 62.1
Smoothed (skip-head) (Headden et al, 2009) Viterbi 65.0
Smoothed (skip-head), Lexicalized (Headden et al, 2009) Viterbi 68.8
Table 2: Directed accuracies on Section 23 of WSJ{10, 20,? } for several baselines and recent state-of-the-art systems.
7 Conclusion
We explored three simple ideas for unsupervised de-
pendency parsing. Pace Halevy et al (2009), we
find ?Less is More? ? the paradoxical result that
better performance can be attained by training with
less data, even when removing samples from the true
(test) distribution. Our small tweaks to Klein and
Manning?s approach of 2004 break through the 2009
state-of-the-art on longer sentences, when trained at
WSJ15 (the auto-detected sweet spot gradation).
The second, Baby Steps, is an elegant meta-
heuristic for optimizing non-convex training crite-
ria. It eliminates the need for linguistically-biased
manually-tuned initializers, particularly if the loca-
tion of the sweet spot is not known. This tech-
nique scales gracefully with more (complex) data
and should easily carry over to more powerful pars-
ing models and learning algorithms.
Finally, Leapfrog forgoes the elegance and metic-
ulousness of Baby Steps in favor of pragmatism.
Employing both good initialization strategies at
its disposal, and spending CPU cycles wisely, it
achieves better performance than both ?Less is
More? and Baby Steps.
Future work could explore unifying these tech-
niques with other state-of-the-art approaches. It may
be useful to scaffold on both data and model com-
plexity, e.g., by increasing head automata?s number
of states (Alshawi and Douglas, 2000). We see many
opportunities for improvement, considering the poor
performance of oracle training relative to the super-
vised state-of-the-art, and in turn the poor perfor-
mance of unsupervised state-of-the-art relative to the
oracle models.11 To this end, it would be instructive
to understand both the linguistic and statistical na-
ture of the sweet spot, and to test its universality.
Acknowledgments
We thank Angel X. Chang, Pi-Chuan Chang, David L.W. Hall,
Christopher D. Manning, David McClosky, Daniel Ramage and
the anonymous reviewers for many helpful comments on draft
versions of this paper.
References
E. L. Allgower and K. Georg. 1990. Numerical Continuation
Methods: An Introduction. Springer-Verlag.
11To facilitate future work, all of our models are publicly
available at http://cs.stanford.edu/?valentin/.
758
H. Alshawi and S. Douglas. 2000. Learning dependency trans-
duction models from unannotated examples. In Royal Soci-
ety of London Philosophical Transactions Series A, volume
358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proc. of ACL.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,
I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and
T. Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic nor-
mal priors for unsupervised probabilistic grammar induction.
In NIPS.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Dorigo and M. Colombetti. 1998. Robot Shaping: An
Experiment in Behavior Engineering. MIT Press/Bradford
Books.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information to
Accompany a Standard Corpus of Present-Day Edited Amer-
ican English, for use with Digital Computers. Department of
Linguistic, Brown University.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1).
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24(2).
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In Proc. of NAACL-HLT.
R. Kail. 1984. The development of memory in children. W. H.
Freeman and Company, 2nd edition.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In Proc. of ACL.
D. Klein. 2005. The Unsupervised Learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
P. Liang and D. Klein. 2008. Analyzing the errors of unsuper-
vised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proc. of NAACL-HLT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of HLT-EMNLP.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
E. L. Newport. 1988. Constraints on learning and their role in
language acquisition: Studies of the acquisition of American
Sign Language. Language Sciences, 10(1).
E. L. Newport. 1990. Maturational constraints on language
learning. Cognitive Science, 14(1).
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-to-fine
syntactic machine translation using language projections. In
Proc. of EMNLP.
S. O. Petrov. 2009. Coarse-to-Fine Natural Language Process-
ing. Ph.D. thesis, University of California, Berkeley.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In Proc. of ACL-IJCNLP.
D. L. T. Rohde and D. C. Plaut. 1999. Language acquisition in
the absence of explicit negative evidence: How important is
starting small? Cognition, 72(1).
L. M. Saksida, S. M. Raymond, and D. S. Touretzky. 1997.
Shaping robot behavior using principles from instrumental
conditioning. Robotics and Autonomous Systems, 22(3).
S. Salvador and P. Chan. 2004. Determining the number of
clusters/segments in hierarchical clustering/segmentation al-
gorithms. In Proc. of ICTAI.
T. D. Sanger. 1994. Neural network learning control of
robot manipulators using gradually increasing task difficulty.
IEEE Trans. on Robotics and Automation, 10.
T. Savage. 1998. Shaping: The link between rats and robots.
Connection Science, 10(3).
T. Savage. 2001. Shaping: A multiple contingencies analysis
and its relevance to behaviour-based robotics. Connection
Science, 13(3).
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
Proc. of ACL.
S. P. Singh. 1992. Transfer of learning by composing solutions
of elemental squential tasks. Machine Learning, 8.
B. F. Skinner. 1938. The behavior of organisms: An experi-
mental analysis. Appleton-Century-Crofts.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of the
IJCAI Workshop on Grammatical Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An
empirical study of semi-supervised structured conditional
models for dependency parsing. In Proc. of EMNLP.
759
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@google.com
Daniel Jurafsky
Departments of Linguistics and
Computer Science, Stanford University
jurafsky@stanford.edu
Hiyan Alshawi
Google Inc.
hiyan@google.com
Abstract
We show how web mark-up can be used
to improve unsupervised dependency pars-
ing. Starting from raw bracketings of four
common HTML tags (anchors, bold, ital-
ics and underlines), we refine approximate
partial phrase boundaries to yield accurate
parsing constraints. Conversion proce-
dures fall out of our linguistic analysis of
a newly available million-word hyper-text
corpus. We demonstrate that derived con-
straints aid grammar induction by training
Klein and Manning?s Dependency Model
with Valence (DMV) on this data set: pars-
ing accuracy on Section 23 (all sentences)
of the Wall Street Journal corpus jumps
to 50.4%, beating previous state-of-the-
art by more than 5%. Web-scale exper-
iments show that the DMV, perhaps be-
cause it is unlexicalized, does not benefit
from orders of magnitude more annotated
but noisier data. Our model, trained on a
single blog, generalizes to 53.3% accuracy
out-of-domain, against the Brown corpus
? nearly 10% higher than the previous
published best. The fact that web mark-up
strongly correlates with syntactic structure
may have broad applicability in NLP.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is
a hard problem whose eventual solution promises
to benefit applications ranging from question an-
swering to speech recognition and machine trans-
lation. A restricted version of this problem that tar-
gets dependencies and assumes partial annotation
? sentence boundaries and part-of-speech (POS)
tagging ? has received much attention. Klein
and Manning (2004) were the first to beat a sim-
ple parsing heuristic, the right-branching baseline;
today?s state-of-the-art systems (Headden et al,
2009; Cohen and Smith, 2009; Spitkovsky et al,
2010a) are rooted in their Dependency Model with
Valence (DMV), still trained using variants of EM.
Pereira and Schabes (1992) outlined three ma-
jor problems with classic EM, applied to a related
problem, constituent parsing. They extended clas-
sic inside-outside re-estimation (Baker, 1979) to
respect any bracketing constraints included with
a training corpus. This conditioning on partial
parses addressed all three problems, leading to:
(i) linguistically reasonable constituent boundaries
and induced grammars more likely to agree with
qualitative judgments of sentence structure, which
is underdetermined by unannotated text; (ii) fewer
iterations needed to reach a good grammar, coun-
tering convergence properties that sharply deterio-
rate with the number of non-terminal symbols, due
to a proliferation of local maxima; and (iii) better
(in the best case, linear) time complexity per it-
eration, versus running time that is ordinarily cu-
bic in both sentence length and the total num-
ber of non-terminals, rendering sufficiently large
grammars computationally impractical. Their al-
gorithm sometimes found good solutions from
bracketed corpora but not from raw text, sup-
porting the view that purely unsupervised, self-
organizing inference methods can miss the trees
for the forest of distributional regularities. This
was a promising break-through, but the problem
of whence to get partial bracketings was left open.
We suggest mining partial bracketings from a
cheap and abundant natural language resource: the
hyper-text mark-up that annotates web-pages. For
example, consider that anchor text can match lin-
guistic constituents, such as verb phrases, exactly:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
To validate this idea, we created a new data set,
novel in combining a real blog?s raw HTML with
tree-bank-like constituent structure parses, gener-
1278
ated automatically. Our linguistic analysis of the
most prevalent tags (anchors, bold, italics and un-
derlines) over its 1M+ words reveals a strong con-
nection between syntax and mark-up (all of our
examples draw from this corpus), inspiring several
simple techniques for automatically deriving pars-
ing constraints. Experiments with both hard and
more flexible constraints, as well as with different
styles and quantities of annotated training data ?
the blog, web news and the web itself, confirm that
mark-up-induced constraints consistently improve
(otherwise unsupervised) dependency parsing.
2 Intuition and Motivating Examples
It is natural to expect hidden structure to seep
through when a person annotates a sentence. As it
happens, a non-trivial fraction of the world?s pop-
ulation routinely annotates text diligently, if only
partially and informally.1 They inject hyper-links,
vary font sizes, and toggle colors and styles, using
mark-up technologies such as HTML and XML.
As noted, web annotations can be indicative of
phrase boundaries, e.g., in a complicated sentence:
In 1998, however, as I <a>[VP established in
<i>[NP The New Republic]</i>]</a> and Bill
Clinton just <a>[VP confirmed in his memoirs]</a>,
Netanyahu changed his mind and ...
In doing so, mark-up sometimes offers useful cues
even for low-level tokenization decisions:
[NP [NP Libyan ruler]
<a>[NP Mu?ammar al-Qaddafi]</a>] referred to ...
(NP (ADJP (NP (JJ Libyan) (NN ruler))
(JJ Mu))
(?? ?) (NN ammar) (NNS al-Qaddafi))
Above, a backward quote in an Arabic name con-
fuses the Stanford parser.2 Yet mark-up lines up
with the broken noun phrase, signals cohesion, and
moreover sheds light on the internal structure of
a compound. As Vadas and Curran (2007) point
out, such details are frequently omitted even from
manually compiled tree-banks that err on the side
of flat annotations of base-NPs.
Admittedly, not all boundaries between HTML
tags and syntactic constituents match up nicely:
..., but [S [NP the <a><i>Toronto
Star</i>][VP reports [NP this][PP in the
softest possible way]</a>,[S stating only that ...]]]
Combining parsing with mark-up may not be
straight-forward, but there is hope: even above,
1Even when (American) grammar schools lived up to their
name, they only taught dependencies. This was back in the
days before constituent grammars were invented.
2http://nlp.stanford.edu:8080/parser/
one of each nested tag?s boundaries aligns; and
Toronto Star?s neglected determiner could be for-
given, certainly within a dependency formulation.
3 A High-Level Outline of Our Approach
Our idea is to implement the DMV (Klein and
Manning, 2004) ? a standard unsupervised gram-
mar inducer. But instead of learning the unan-
notated test set, we train with text that contains
web mark-up, using various ways of converting
HTML into parsing constraints. We still test on
WSJ (Marcus et al, 1993), in the standard way,
and also check generalization against a hidden
data set ? the Brown corpus (Francis and Kucera,
1979). Our parsing constraints come from a blog
? a new corpus we created, the web and news (see
Table 1 for corpora?s sentence and token counts).
To facilitate future work, we make the final
models and our manually-constructed blog data
publicly available.3 Although we are unable
to share larger-scale resources, our main results
should be reproducible, as both linguistic analysis
and our best model rely exclusively on the blog.
Corpus Sentences POS Tokens
WSJ? 49,208 1,028,347
Section 23 2,353 48,201
WSJ45 48,418 986,830
WSJ15 15,922 163,715
Brown100 24,208 391,796
BLOGp 57,809 1,136,659
BLOGt45 56,191 1,048,404
BLOGt15 23,214 212,872
NEWS45 2,263,563,078 32,119,123,561
NEWS15 1,433,779,438 11,786,164,503
WEB45 8,903,458,234 87,269,385,640
WEB15 7,488,669,239 55,014,582,024
Table 1: Sizes of corpora derived from WSJ and
Brown, as well as those we collected from the web.
4 Data Sets for Evaluation and Training
The appeal of unsupervised parsing lies in its abil-
ity to learn from surface text alone; but (intrinsic)
evaluation still requires parsed sentences. Follow-
ing Klein and Manning (2004), we begin with ref-
erence constituent parses and compare against de-
terministically derived dependencies: after prun-
ing out all empty subtrees, punctuation and ter-
minals (tagged # and $) not pronounced where
they appear, we drop all sentences with more
than a prescribed number of tokens remaining and
use automatic ?head-percolation? rules (Collins,
1999) to convert the rest, as is standard practice.
3http://cs.stanford.edu/?valentin/
1279
Length Marked POS Bracketings Length Marked POS Bracketings
Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token
0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684
1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479
2 4,934 124,527 6,482 6,015 10 245 7,887 365 352
3 3,295 85,423 4,476 4,212 15 42 1,519 65 63
4 2,103 56,390 2,952 2,789 20 13 466 20 20
5 1,402 38,265 1,988 1,874 25 6 235 10 10
6 960 27,285 1,365 1,302 30 3 136 6 6
7 692 19,894 992 952 40 0 0 0 0
Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those
sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).
Our primary reference sets are derived from the
Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993): WSJ45 (sentences with
fewer than 46 tokens) and Section 23 of WSJ? (all
sentence lengths). We also evaluate on Brown100,
similarly derived from the parsed portion of the
Brown corpus (Francis and Kucera, 1979). While
we use WSJ45 and WSJ15 to train baseline mod-
els, the bulk of our experiments is with web data.
4.1 A News-Style Blog: Daniel Pipes
Since there was no corpus overlaying syntactic
structure with mark-up, we began constructing a
new one by downloading articles4 from a news-
style blog. Although limited to a single genre ?
political opinion, danielpipes.org is clean, consis-
tently formatted, carefully edited and larger than
WSJ (see Table 1). Spanning decades, Pipes?
editorials are mostly in-domain for POS taggers
and tree-bank-trained parsers; his recent (internet-
era) entries are thoroughly cross-referenced, con-
veniently providing just the mark-up we hoped to
study via uncluttered (printer-friendly) HTML.5
After extracting moderately clean text and
mark-up locations, we used MxTerminator (Rey-
nar and Ratnaparkhi, 1997) to detect sentence
boundaries. This initial automated pass begot mul-
tiple rounds of various semi-automated clean-ups
that involved fixing sentence breaking, modifying
parser-unfriendly tokens, converting HTML enti-
ties and non-ASCII text, correcting typos, and so
on. After throwing away annotations of fractional
words (e.g., <i>basmachi</i>s) and tokens (e.g.,
<i>Sesame Street</i>-like), we broke up all mark-
up that crossed sentence boundaries (i.e., loosely
speaking, replaced constructs like <u>...][S...</u>
with <u>...</u> ][S <u>...</u>) and discarded any
4http://danielpipes.org/art/year/all
5http://danielpipes.org/article print.php?
id=. . .
tags left covering entire sentences.
We finalized two versions of the data: BLOGt,
tagged with the Stanford tagger (Toutanova and
Manning, 2000; Toutanova et al, 2003),6 and
BLOGp, parsed with Charniak?s parser (Charniak,
2001; Charniak and Johnson, 2005).7 The rea-
son for this dichotomy was to use state-of-the-art
parses to analyze the relationship between syntax
and mark-up, yet to prevent jointly tagged (and
non-standard AUX[G]) POS sequences from interfer-
ing with our (otherwise unsupervised) training.8
4.2 Scaled up Quantity: The (English) Web
We built a large (see Table 1) but messy data set,
WEB ? English-looking web-pages, pre-crawled
by a search engine. To avoid machine-generated
spam, we excluded low quality sites flagged by the
indexing system. We kept only sentence-like runs
of words (satisfying punctuation and capitalization
constraints), POS-tagged with TnT (Brants, 2000).
4.3 Scaled up Quality: (English) Web News
In an effort to trade quantity for quality, we con-
structed a smaller, potentially cleaner data set,
NEWS. We reckoned editorialized content would
lead to fewer extracted non-sentences. Perhaps
surprisingly, NEWS is less than an order of magni-
tude smaller than WEB (see Table 1); in part, this
is due to less aggressive filtering ? we trust sites
approved by the human editors at Google News.9
In all other respects, our pre-processing of NEWS
pages was identical to our handling of WEB data.
6http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz
7ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz
8However, since many taggers are themselves trained on
manually parsed corpora, such as WSJ, no parser that relies
on external POS tags could be considered truly unsupervised;
for a fully unsupervised example, see Seginer?s (2007) CCL
parser, available at http://www.seggu.net/ccl/
9http://news.google.com/
1280
5 Linguistic Analysis of Mark-Up
Is there a connection between mark-up and syn-
tactic structure? Previous work (Barr et al, 2008)
has only examined search engine queries, show-
ing that they consist predominantly of short noun
phrases. If web mark-up shared a similar char-
acteristic, it might not provide sufficiently dis-
ambiguating cues to syntactic structure: HTML
tags could be too short (e.g., singletons like
?click <a>here</a>?) or otherwise unhelpful in re-
solving truly difficult ambiguities (such as PP-
attachment). We began simply by counting vari-
ous basic events in BLOGp.
Count POS Sequence Frac Sum
1 1,242 NNP NNP 16.1%
2 643 NNP 8.3 24.4
3 419 NNP NNP NNP 5.4 29.8
4 414 NN 5.4 35.2
5 201 JJ NN 2.6 37.8
6 138 DT NNP NNP 1.8 39.5
7 138 NNS 1.8 41.3
8 112 JJ 1.5 42.8
9 102 VBD 1.3 44.1
10 92 DT NNP NNP NNP 1.2 45.3
11 85 JJ NNS 1.1 46.4
12 79 NNP NN 1.0 47.4
13 76 NN NN 1.0 48.4
14 61 VBN 0.8 49.2
15 60 NNP NNP NNP NNP 0.8 50.0
BLOGp +3,869 more with Count ? 49 50.0%
Table 3: Top 50% of marked POS tag sequences.
Count Non-Terminal Frac Sum
1 5,759 NP 74.5%
2 997 VP 12.9 87.4
3 524 S 6.8 94.2
4 120 PP 1.6 95.7
5 72 ADJP 0.9 96.7
6 61 FRAG 0.8 97.4
7 41 ADVP 0.5 98.0
8 39 SBAR 0.5 98.5
9 19 PRN 0.2 98.7
10 18 NX 0.2 99.0
BLOGp +81 more with Count ? 16 1.0%
Table 4: Top 99% of dominating non-terminals.
5.1 Surface Text Statistics
Out of 57,809 sentences, 6,047 (10.5%) are anno-
tated (see Table 2); and 4,934 (8.5%) have multi-
token bracketings. We do not distinguish HTML
tags and track only unique bracketing end-points
within a sentence. Of these, 6,015 are multi-token
? an average per-sentence yield of 10.4%.10
10A non-trivial fraction of our corpus is older (pre-internet)
unannotated articles, so this estimate may be conservative.
As expected, many of the annotated words are
nouns, but there are adjectives, verbs and other
parts of speech too (see Table 3). Mark-up is short,
typically under five words, yet (by far) the most
frequently marked sequence of POS tags is a pair.
5.2 Common Syntactic Subtrees
For three-quarters of all mark-up, the lowest domi-
nating non-terminal is a noun phrase (see Table 4);
there are also non-trace quantities of verb phrases
(12.9%) and other phrases, clauses and fragments.
Of the top fifteen ? 35.2% of all ? annotated
productions, only one is not a noun phrase (see Ta-
ble 5, left). Four of the fifteen lowest dominating
non-terminals do not match the entire bracketing
? all four miss the leading determiner, as we saw
earlier. In such cases, we recursively split internal
nodes until the bracketing aligned, as follows:
[S [NP the <a>Toronto Star][VP reports [NP this]
[PP in the softest possible way]</a>,[S stating ...]]]
S? NP VP? DT NNP NNP VBZ NP PP S
We can summarize productions more compactly
by using a dependency framework and clipping
off any dependents whose subtrees do not cross a
bracketing boundary, relative to the parent. Thus,
DT NNP NNP VBZ DT IN DT JJS JJ NN
becomes DT NNP VBZ, ?the <a>Star reports</a>.?
Viewed this way, the top fifteen (now collapsed)
productions cover 59.4% of all cases and include
four verb heads, in addition to a preposition and
an adjective (see Table 5, right). This exposes five
cases of inexact matches, three of which involve
neglected determiners or adjectives to the left of
the head. In fact, the only case that cannot be ex-
plained by dropped dependents is #8, where the
daughters are marked but the parent is left out.
Most instances contributing to this pattern are flat
NPs that end with a noun, incorrectly assumed to
be the head of all other words in the phrase, e.g.,
... [NP a 1994 <i>New Yorker</i> article] ...
As this example shows, disagreements (as well
as agreements) between mark-up and machine-
generated parse trees with automatically perco-
lated heads should be taken with a grain of salt.11
11In a relatively recent study, Ravi et al (2008) report
that Charniak?s re-ranking parser (Charniak and Johnson,
2005) ? reranking-parserAug06.tar.gz, also available
from ftp://ftp.cs.brown.edu/pub/nlparser/ ? at-
tains 86.3% accuracy when trained on WSJ and tested against
Brown; its nearly 5% performance loss out-of-domain is con-
sistent with the numbers originally reported by Gildea (2001).
1281
Count Constituent Production Frac Sum
1 746 NP? NNP NNP 9.6%
2 357 NP? NNP 4.6 14.3
3 266 NP? NP PP 3.4 17.7
4 183 NP? NNP NNP NNP 2.4 20.1
5 165 NP? DT NNP NNP 2.1 22.2
6 140 NP? NN 1.8 24.0
7 131 NP? DT NNP NNP NNP 1.7 25.7
8 130 NP? DT NN 1.7 27.4
9 127 NP? DT NNP NNP 1.6 29.0
10 109 S ? NP VP 1.4 30.4
11 91 NP? DT NNP NNP NNP 1.2 31.6
12 82 NP? DT JJ NN 1.1 32.7
13 79 NP? NNS 1.0 33.7
14 65 NP? JJ NN 0.8 34.5
15 60 NP? NP NP 0.8 35.3
BLOGp +5,000 more with Count ? 60 64.7%
Count Head-Outward Spawn Frac Sum
1 1,889 NNP 24.4%
2 623 NN 8.1 32.5
3 470 DT NNP 6.1 38.6
4 458 DT NN 5.9 44.5
5 345 NNS 4.5 49.0
6 109 NNPS 1.4 50.4
7 98 VBG 1.3 51.6
8 96 NNP NNP NN 1.2 52.9
9 80 VBD 1.0 53.9
10 77 IN 1.0 54.9
11 74 VBN 1.0 55.9
12 73 DT JJ NN 0.9 56.8
13 71 VBZ 0.9 57.7
14 69 POS NNP 0.9 58.6
15 63 JJ 0.8 59.4
BLOGp +3,136 more with Count ? 62 40.6%
Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), after
recursively expanding any internal nodes that did not align with the bracketing (underlined). Tabulated
dependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent
(i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.
5.3 Proposed Parsing Constraints
The straight-forward approach ? forcing mark-up
to correspond to constituents ? agrees with Char-
niak?s parse trees only 48.0% of the time, e.g.,
... in [NP<a>[NP an analysis]</a>[PP of perhaps the
most astonishing PC item I have yet stumbled upon]].
This number should be higher, as the vast major-
ity of disagreements are due to tree-bank idiosyn-
crasies (e.g., bare NPs). Earlier examples of in-
complete constituents (e.g., legitimately missing
determiners) would also be fine in many linguistic
theories (e.g., as N-bars). A dependency formula-
tion is less sensitive to such stylistic differences.
We begin with the hardest possible constraint on
dependencies, then slowly relax it. Every example
used to demonstrate a softer constraint doubles
as a counter-example against all previous versions.
? strict ? seals mark-up into attachments, i.e.,
inside a bracketing, enforces exactly one external
arc ? into the overall head. This agrees with
head-percolated trees just 35.6% of the time, e.g.,
As author of <i>The Satanic Verses</i>, I ...
? loose ? same as strict, but allows the bracket-
ing?s head word to have external dependents. This
relaxation already agrees with head-percolated de-
pendencies 87.5% of the time, catching many
(though far from all) dropped dependents, e.g.,
. . . the <i>Toronto Star</i> reports . . .
? sprawl ? same as loose, but now allows all
words inside a bracketing to attach external de-
pendents.12 This boosts agreement with head-
percolated trees to 95.1%, handling new cases,
e.g., where ?Toronto Star? is embedded in longer
mark-up that includes its own parent ? a verb:
. . . the <a>Toronto Star reports . . .</a> . . .
? tear ? allows mark-up to fracture after all,
requiring only that the external heads attaching the
pieces lie to the same side of the bracketing. This
propels agreement with percolated dependencies
to 98.9%, fixing previously broken PP-attachment
ambiguities, e.g., a fused phrase like ?Fox News in
Canada? that detached a preposition from its verb:
... concession ... has raised eyebrows among those
waiting [PP for <a>Fox News][PP in Canada]</a>.
Most of the remaining 1.1% of disagreements are
due to parser errors. Nevertheless, it is possible for
mark-up to be torn apart by external heads from
both sides. We leave this section with a (very rare)
true negative example. Below, ?CSA? modifies
?authority? (to its left), appositively, while ?Al-
Manar? modifies ?television? (to its right):13
The French broadcasting authority, <a>CSA, banned
... Al-Manar</a> satellite television from ...
12This view evokes the trapezoids of the O(n3) recognizer
for split head automaton grammars (Eisner and Satta, 1999).
13But this is a stretch, since the comma after ?CSA? ren-
ders the marked phrase ungrammatical even out of context.
1282
6 Experimental Methods and Metrics
We implemented the DMV (Klein and Manning,
2004), consulting the details of (Spitkovsky et al,
2010a). Crucially, we swapped out inside-outside
re-estimation in favor of Viterbi training. Not only
is it better-suited to the general problem (see ?7.1),
but it also admits a trivial implementation of (most
of) the dependency constraints we proposed.14
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 1: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
Six settings parameterized each run:
? INIT: 0? default, uniform initialization; or
1 ? a high quality initializer, pre-trained using
Ad-Hoc? (Spitkovsky et al, 2010a): we chose the
Laplace-smoothed model trained at WSJ15 (the
?sweet spot? data gradation) but initialized off
WSJ8, since that ad-hoc harmonic initializer has
the best cross-entropy on WSJ15 (see Figure 1).
? GENRE: 0? default, baseline training on WSJ;
else, uses 1? BLOGt; 2? NEWS; or 3? WEB.
? SCOPE: 0 ? default, uses all sentences up to
length 45; if 1, trains using sentences up to length
15; if 2, re-trains on sentences up to length 45,
starting from the solution to sentences up to length
15, as recommended by Spitkovsky et al (2010a).
? CONSTR: if 4, strict; if 3, loose; and if 2,
sprawl. We did not implement level 1, tear. Over-
constrained sentences are re-attempted at succes-
sively lower levels until they become possible to
parse, if necessary at the lowest (default) level 0.15
? TRIM: if 1, discards any sentence without a sin-
gle multi-token mark-up (shorter than its length).
? ADAPT: if 1, upon convergence, initializes re-
training on WSJ45 using the solution to <GENRE>,
attempting domain adaptation (Lee et al, 1991).
These make for 294 meaningful combinations. We
judged each one by its accuracy on WSJ45, using
standard directed scoring ? the fraction of correct
dependencies over randomized ?best? parse trees.
14We analyze the benefits of Viterbi training in a compan-
ion paper (Spitkovsky et al, 2010b), which dedicates more
space to implementation and to the WSJ baselines used here.
15At level 4, <b> X<u> Y</b> Z</u> is over-constrained.
7 Discussion of Experimental Results
Evaluation on Section 23 of WSJ and Brown re-
veals that blog-training beats all published state-
of-the-art numbers in every traditionally-reported
length cutoff category, with news-training not far
behind. Here is a mini-preview of these results, for
Section 23 of WSJ10 and WSJ? (from Table 8):
WSJ10 WSJ?
(Cohen and Smith, 2009) 62.0 42.2
(Spitkovsky et al, 2010a) 57.1 45.0
NEWS-best 67.3 50.1
BLOGt-best 69.3 50.4
(Headden et al, 2009) 68.8
Table 6: Directed accuracies on Section 23 of
WSJ{10,? } for three recent state-of-the-art sys-
tems and our best runs (as judged against WSJ45)
for NEWS and BLOGt (more details in Table 8).
Since our experimental setup involved testing
nearly three hundred models simultaneously, we
must take extreme care in analyzing and interpret-
ing these results, to avoid falling prey to any loom-
ing ?data-snooping? biases.16 In a sufficiently
large pool of models, where each is trained using
a randomized and/or chaotic procedure (such as
ours), the best may look good due to pure chance.
We appealed to three separate diagnostics to con-
vince ourselves that our best results are not noise.
The most radical approach would be to write off
WSJ as a development set and to focus only on the
results from the held-out Brown corpus. It was ini-
tially intended as a test of out-of-domain general-
ization, but since Brown was in no way involved
in selecting the best models, it also qualifies as
a blind evaluation set. We observe that our best
models perform even better (and gain more ? see
Table 8) on Brown than on WSJ ? a strong indi-
cation that our selection process has not overfitted.
Our second diagnostic is a closer look at WSJ.
Since we cannot graph the full (six-dimensional)
set of results, we begin with a simple linear re-
gression, using accuracy on WSJ45 as the depen-
dent variable. We prefer this full factorial design
to the more traditional ablation studies because it
allows us to account for and to incorporate every
single experimental data point incurred along the
16In the standard statistical hypothesis testing setting, it
is reasonable to expect that p% of randomly chosen hy-
potheses will appear significant at the p% level simply by
chance. Consequently, multiple hypothesis testing requires
re-evaluating significance levels ? adjusting raw p-values,
e.g., using the Holm-Bonferroni method (Holm, 1979).
1283
Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token Bracketings
BLOGt45 5,641 56,191 1,048,404 7,021 5,346
BLOG?t45 4,516 4,516 104,267 5,771 5,346
BLOGt15 1,562 23,214 212,872 1,714 1,240
BLOG?t15 1,171 1,171 11,954 1,288 1,240
NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150
NEWS?45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070
NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675
NEWS?15 147,848,358 147,848,358 1,397,562,474 272,223,918 231,029,921
WEB45 1,577,208,680 8,903,458,234 87,269,385,640 3,309,897,461 2,459,337,571
WEB?45 933,115,032 933,115,032 11,552,983,379 2,084,359,555 1,793,238,913
WEB15 1,181,696,194 7,488,669,239 55,014,582,024 2,071,743,595 1,494,675,520
WEB?15 681,087,020 681,087,020 5,813,555,341 1,200,980,738 1,072,910,682
Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,
restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime (?).
way. Its output is a coarse, high-level summary of
our runs, showing which factors significantly con-
tribute to changes in error rate on WSJ45:
Parameter (Indicator) Setting ?? p-value
INIT 1 ad-hoc @WSJ8,15 11.8 ***
GENRE 1 BLOGt -3.7 0.06
2 NEWS -5.3 **
3 WEB -7.7 ***
SCOPE 1 @15 -0.5 0.40
2 @15?45 -0.4 0.53
CONSTR 2 sprawl 0.9 0.23
3 loose 1.0 0.15
4 strict 1.8 *
TRIM 1 drop unmarked -7.4 ***
ADAPT 1 WSJ re-training 1.5 **
Intercept (R2Adjusted = 73.6%) 39.9 ***
We use a standard convention: *** for p < 0.001;
** for p < 0.01 (very signif.); and * for p < 0.05 (signif.).
The default training mode (all parameters zero) is
estimated to score 39.9%. A good initializer gives
the biggest (double-digit) gain; both domain adap-
tation and constraints also make a positive impact.
Throwing away unannotated data hurts, as does
training out-of-domain (the blog is least bad; the
web is worst). Of course, this overview should not
be taken too seriously. Overly simplistic, a first
order model ignores interactions between parame-
ters. Furthermore, a least squares fit aims to cap-
ture central tendencies, whereas we are more in-
terested in outliers ? the best-performing runs.
A major imperfection of the simple regression
model is that helpful factors that require an in-
teraction to ?kick in? may not, on their own, ap-
pear statistically significant. Our third diagnostic
is to examine parameter settings that give rise to
the best-performing models, looking out for com-
binations that consistently deliver superior results.
7.1 WSJ Baselines
Just two parameters apply to learning from WSJ.
Five of their six combinations are state-of-the-art,
demonstrating the power of Viterbi training; only
the default run scores worse than 45.0%, attained
by Leapfrog (Spitkovsky et al, 2010a), on WSJ45:
Settings SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 41.3 45.0 45.2
1 46.6 47.5 47.6
@45 @15 @15?45
7.2 Blog
Simply training on BLOGt instead of WSJ hurts:
GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 39.6 36.9 36.9
1 46.5 46.3 46.4
@45 @15 @15?45
The best runs use a good initializer, discard unan-
notated sentences, enforce the loose constraint on
the rest, follow up with domain adaptation and
benefit from re-training ? GENRE=TRIM=ADAPT=1:
INIT=1 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 45.8 48.3 49.6
(sprawl) 2 46.3 49.2 49.2
(loose) 3 41.3 50.2 50.4
(strict) 4 40.7 49.9 48.7
@45 @15 @15?45
The contrast between unconstrained learning and
annotation-guided parsing is higher for the default
initializer, still using trimmed data sets (just over a
thousand sentences for BLOG?t15 ? see Table 7):
INIT=0 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 25.6 19.4 19.3
(sprawl) 2 25.2 22.7 22.5
(loose) 3 32.4 26.3 27.3
(strict) 4 36.2 38.7 40.1
@45 @15 @15?45
Above, we see a clearer benefit to our constraints.
1284
7.3 News
Training on WSJ is also better than using NEWS:
GENRE=2 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 40.2 38.8 38.7
1 43.4 44.0 43.8
@45 @15 @15?45
As with the blog, the best runs use the good initial-
izer, discard unannotated sentences, enforce the
loose constraint and follow up with domain adap-
tation ? GENRE=2; INIT=TRIM=ADAPT=1:
Settings SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 46.6 45.4 45.2
(sprawl) 2 46.1 44.9 44.9
(loose) 3 49.5 48.1 48.3
(strict) 4 37.7 36.8 37.6
@45 @15 @15?45
With all the extra training data, the best new score
is just 49.5%. On the one hand, we are disap-
pointed by the lack of dividends to orders of mag-
nitude more data. On the other, we are comforted
that the system arrives within 1% of its best result
? 50.4%, obtained with a manually cleaned up
corpus ? now using an auto-generated data set.
7.4 Web
The WEB-side story is more discouraging:
GENRE=3 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 38.3 35.1 35.2
1 42.8 43.6 43.4
@45 @15 @15?45
Our best run again uses a good initializer, keeps
all sentences, still enforces the loose constraint
and follows up with domain adaptation, but per-
forms worse than all well-initialized WSJ base-
lines, scoring only 45.9% (trained at WEB15).
We suspect that the web is just too messy for
us. On top of the challenges of language iden-
tification and sentence-breaking, there is a lot of
boiler-plate; furthermore, web text can be difficult
for news-trained POS taggers. For example, note
that the verb ?sign? is twice mistagged as a noun
and that ?YouTube? is classified as a verb, in the
top four POS sequences of web sentences:17
POS Sequence WEB Count
Sample web sentence, chosen uniformly at random.
1 DT NNS VBN 82,858,487
All rights reserved.
2 NNP NNP NNP 65,889,181
Yuasa et al
3 NN IN TO VB RB 31,007,783
Sign in to YouTube now!
4 NN IN IN PRP$ JJ NN 31,007,471
Sign in with your Google Account!
17Further evidence: TnT tags the ubiquitous but ambigu-
ous fragments ?click here? and ?print post? as noun phrases.
7.5 The State of the Art
Our best model gains more than 5% over previ-
ous state-of-the-art accuracy across all sentences
of WSJ?s Section 23, more than 8% on WSJ20 and
rivals the oracle skyline (Spitkovsky et al, 2010a)
on WSJ10; these gains generalize to Brown100,
where it improves by nearly 10% (see Table 8).
We take solace in the fact that our best mod-
els agree in using loose constraints. Of these,
the models trained with less data perform better,
with the best two using trimmed data sets, echo-
ing that ?less is more? (Spitkovsky et al, 2010a),
pace Halevy et al (2009). We note that orders of
magnitude more data did not improve parsing per-
formance further and suspect a different outcome
from lexicalized models: The primary benefit of
additional lower-quality data is in improved cover-
age. But with only 35 unique POS tags, data spar-
sity is hardly an issue. Extra examples of lexical
items help little and hurt when they are mistagged.
8 Related Work
The wealth of new annotations produced in many
languages every day already fuels a number of
NLP applications. Following their early and
wide-spread use by search engines, in service of
spam-fighting and retrieval, anchor text and link
data enhanced a variety of traditional NLP tech-
niques: cross-lingual information retrieval (Nie
and Chen, 2002), translation (Lu et al, 2004), both
named-entity recognition (Mihalcea and Csomai,
2007) and categorization (Watanabe et al, 2007),
query segmentation (Tan and Peng, 2008), plus
semantic relatedness and word-sense disambigua-
tion (Gabrilovich and Markovitch, 2007; Yeh et
al., 2009). Yet several, seemingly natural, can-
didate core NLP tasks ? tokenization, CJK seg-
mentation, noun-phrase chunking, and (until now)
parsing ? remained conspicuously uninvolved.
Approaches related to ours arise in applications
that combine parsing with named-entity recogni-
tion (NER). For example, constraining a parser to
respect the boundaries of known entities is stan-
dard practice not only in joint modeling of (con-
stituent) parsing and NER (Finkel and Manning,
2009), but also in higher-level NLP tasks, such as
relation extraction (Mintz et al, 2009), that couple
chunking with (dependency) parsing. Although
restricted to proper noun phrases, dates, times and
quantities, we suspect that constituents identified
by trained (supervised) NER systems would also
1285
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Leapfrog (Spitkovsky et al, 2010a) 57.1 48.7 45.0 43.6
default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 45.8 41.6 40.5
WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 53.8 47.9 50.8
BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIM=1,ADAPT=1 69.3 56.8 50.4 53.3
NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIM=1,ADAPT=1 67.3 56.2 50.1 51.6
WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIM=0,ADAPT=1 64.1 52.7 46.3 46.9
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 8: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.
be helpful in constraining grammar induction.
Following Pereira and Schabes? (1992) success
with partial annotations in training a model of
(English) constituents generatively, their idea has
been extended to discriminative estimation (Rie-
zler et al, 2002) and also proved useful in mod-
eling (Japanese) dependencies (Sassano, 2005).
There was demand for partially bracketed corpora.
Chen and Lee (1995) constructed one such corpus
by learning to partition (English) POS sequences
into chunks (Abney, 1991); Inui and Kotani (2001)
used n-gram statistics to split (Japanese) clauses.
We combine the two intuitions, using the web
to build a partially parsed corpus. Our approach
could be called lightly-supervised, since it does
not require manual annotation of a single complete
parse tree. In contrast, traditional semi-supervised
methods rely on fully-annotated seed corpora.18
9 Conclusion
We explored novel ways of training dependency
parsing models, the best of which attains 50.4%
accuracy on Section 23 (all sentences) of WSJ,
beating all previous unsupervised state-of-the-art
by more than 5%. Extra gains stem from guid-
ing Viterbi training with web mark-up, the loose
constraint consistently delivering best results. Our
linguistic analysis of a blog reveals that web an-
notations can be converted into accurate parsing
constraints (loose: 88%; sprawl: 95%; tear: 99%)
that could be helpful to supervised methods, e.g.,
by boosting an initial parser via self-training (Mc-
Closky et al, 2006) on sentences with mark-up.
Similar techniques may apply to standard word-
processing annotations, such as font changes, and
to certain (balanced) punctuation (Briscoe, 1994).
We make our blog data set, overlaying mark-up
and syntax, publicly available. Its annotations are
18A significant effort expended in building a tree-bank
comes with the first batch of sentences (Druck et al, 2009).
75% noun phrases, 13% verb phrases, 7% simple
declarative clauses and 2% prepositional phrases,
with traces of other phrases, clauses and frag-
ments. The type of mark-up, combined with POS
tags, could make for valuable features in discrimi-
native models of parsing (Ratnaparkhi, 1999).
A logical next step would be to explore the con-
nection between syntax and mark-up for genres
other than a news-style blog and for languages
other than English. We are excited by the possi-
bilities, as unsupervised parsers are on the cusp
of becoming useful in their own right ? re-
cently, Davidov et al (2009) successfully applied
Seginer?s (2007) fully unsupervised grammar in-
ducer to the problems of pattern-acquisition and
extraction of semantic data. If the strength of the
connection between web mark-up and syntactic
structure is universal across languages and genres,
this fact could have broad implications for NLP,
with applications extending well beyond parsing.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fannie
& John Hertz Foundation Fellowship. We thank Angel X.
Chang, Spence Green, Christopher D. Manning, Richard
Socher, Mihai Surdeanu and the anonymous reviewers for
many helpful suggestions, and we are especially grateful to
Andy Golding, for pointing us to his sample Map-Reduce
over the Google News crawl, and to Daniel Pipes, for allow-
ing us to distribute the data set derived from his blog entries.
References
S. Abney. 1991. Parsing by chunks. Principle-Based Pars-
ing: Computation and Psycholinguistics.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
C. Barr, R. Jones, and M. Regelson. 2008. The linguistic
structure of English web-search queries. In EMNLP.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP.
1286
T. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
H.-H. Chen and Y.-S. Lee. 1995. Development of a partially
bracketed corpus with part-of-speech information only. In
WVLC.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In NAACL-HLT.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
D. Davidov, R. Reichart, and A. Rappoport. 2009. Supe-
rior and efficient fully unsupervised pattern-based concept
acquisition using an unsupervised parser. In CoNLL.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using general-
ized expectation criteria. In ACL-IJCNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
E. Gabrilovich and S. Markovitch. 2007. Computing seman-
tic relatedness using Wikipedia-based Explicit Semantic
Analysis. In IJCAI.
D. Gildea. 2001. Corpus variation and parser performance.
In EMNLP.
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple test
procedure. Scandinavian Journal of Statistics, 6.
N. Inui and Y. Kotani. 2001. Robust N -gram based syntactic
analysis using segmentation words. In PACLIC.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
C.-H. Lee, C.-H. Lin, and B.-H. Juang. 1991. A study on
speaker adaptation of the parameters of continuous den-
sity Hidden Markov Models. IEEE Trans. on Signal Pro-
cessing, 39.
W.-H. Lu, L.-F. Chien, and H.-J. Lee. 2004. Anchor text
mining for translation of Web queries: A transitive trans-
lation approach. ACM Trans. on Information Systems, 22.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In NAACL-HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: Linking docu-
ments to encyclopedic knowledge. In CIKM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant
supervision for relation extraction without labeled data. In
ACL-IJCNLP.
J.-Y. Nie and J. Chen. 2002. Exploiting the Web as paral-
lel corpora for cross-language information retrieval. Web
Intelligence.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
A. Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34.
S. Ravi, K. Knight, and R. Soricut. 2008. Automatic predic-
tion of parser accuracy. In EMNLP.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In ANLP.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In ACL.
M. Sassano. 2005. Using a partially annotated corpus to
build a dependency parser for Japanese. In IJCNLP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised de-
pendency parsing. In CoNLL.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia. In
WWW.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP-VLC.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In HLT-NAACL.
D. Vadas and J. R. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL.
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A
graph-based approach to named entity categorization in
Wikipedia using conditional random fields. In EMNLP-
CoNLL.
E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa.
2009. WikiWalk: Random walks on Wikipedia for se-
mantic relatedness. In TextGraphs.
1287
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?16,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparison of Chinese Parsers for Stanford Dependencies
Wanxiang Che?
car@ir.hit.edu.cn
Valentin I. Spitkovsky?
vals@stanford.edu
Ting Liu?
tliu@ir.hit.edu.cn
?School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Stanford dependencies are widely used in nat-
ural language processing as a semantically-
oriented representation, commonly generated
either by (i) converting the output of a con-
stituent parser, or (ii) predicting dependencies
directly. Previous comparisons of the two ap-
proaches for English suggest that starting from
constituents yields higher accuracies. In this
paper, we re-evaluate both methods for Chi-
nese, using more accurate dependency parsers
than in previous work. Our comparison of per-
formance and efficiency across seven popular
open source parsers (four constituent and three
dependency) shows, by contrast, that recent
higher-order graph-based techniques can be
more accurate, though somewhat slower, than
constituent parsers. We demonstrate also that
n-way jackknifing is a useful technique for
producing automatic (rather than gold) part-
of-speech tags to train Chinese dependency
parsers. Finally, we analyze the relations pro-
duced by both kinds of parsing and suggest
which specific parsers to use in practice.
1 Introduction
Stanford dependencies (de Marneffe and Man-
ning, 2008) provide a simple description of rela-
tions between pairs of words in a sentence. This
semantically-oriented representation is intuitive and
easy to apply, requiring little linguistic expertise.
Consequently, Stanford dependencies are widely
used: in biomedical text mining (Kim et al, 2009),
as well as in textual entailment (Androutsopou-
los and Malakasiotis, 2010), information extrac-
tion (Wu and Weld, 2010; Banko et al, 2007) and
sentiment analysis (Meena and Prabhakar, 2007).
In addition to English, there is a Chinese ver-
sion of Stanford dependencies (Chang et al, 2009),
(a) A constituent parse tree.
(b) Stanford dependencies.
Figure 1: A sample Chinese constituent parse tree and its
corresponding Stanford dependencies for the sentence
China (??) encourages (??) private (??)
entrepreneurs (???) to invest (??) in
national (??) infrastructure (??) construction (??).
which is also useful for many applications, such as
Chinese sentiment analysis (Wu et al, 2011; Wu et
al., 2009; Zhuang et al, 2006) and relation extrac-
tion (Huang et al, 2008). Figure 1 shows a sample
constituent parse tree and the corresponding Stan-
ford dependencies for a sentence in Chinese. Al-
though there are several variants of Stanford depen-
dencies for English,1 so far only a basic version (i.e,
dependency tree structures) is available for Chinese.
Stanford dependencies were originally obtained
from constituent trees, using rules (de Marneffe et
al., 2006). But as dependency parsing technolo-
gies mature (Ku?bler et al, 2009), they offer increas-
ingly attractive alternatives that eliminate the need
for an intermediate representation. Cer et al (2010)
reported that Stanford?s implementation (Klein and
Manning, 2003) underperforms other constituent
1nlp.stanford.edu/software/dependencies_manual.pdf
11
Type Parser Version Algorithm URL
Constituent Berkeley 1.1 PCFG code.google.com/p/berkeleyparser
Bikel 1.2 PCFG www.cis.upenn.edu/?dbikel/download.html
Charniak Nov. 2009 PCFG www.cog.brown.edu/?mj/Software.htm
Stanford 2.0 Factored nlp.stanford.edu/software/lex-parser.shtml
Dependency MaltParser 1.6.1 Arc-Eager maltparser.org
Mate 2.0 2nd-order MST code.google.com/p/mate-tools
MSTParser 0.5 MST sourceforge.net/projects/mstparser
Table 1: Basic information for the seven parsers included in our experiments.
parsers, for English, on both accuracy and speed.
Their thorough investigation also showed that con-
stituent parsers systematically outperform parsing
directly to Stanford dependencies. Nevertheless, rel-
ative standings could have changed in recent years:
dependency parsers are now significantly more ac-
curate, thanks to advances like the high-order maxi-
mum spanning tree (MST) model (Koo and Collins,
2010) for graph-based dependency parsing (McDon-
ald and Pereira, 2006). Therefore, we deemed it im-
portant to re-evaluate the performance of constituent
and dependency parsers. But the main purpose of
our work is to apply the more sophisticated depen-
dency parsing algorithms specifically to Chinese.
Number of \in Train Dev Test Total
files 2,083 160 205 2,448
sentences 46,572 2,079 2,796 51,447
tokens 1,039,942 59,955 81,578 1,181,475
Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data.
2 Methodology
We compared seven popular open source constituent
and dependency parsers, focusing on both accuracy
and parsing speed. We hope that our analysis will
help end-users select a suitable method for parsing
to Stanford dependencies in their own applications.
2.1 Parsers
We considered four constituent parsers. They are:
Berkeley (Petrov et al, 2006), Bikel (2004), Char-
niak (2000) and Stanford (Klein and Manning,
2003) chineseFactored, which is also the default
used by Stanford dependencies. The three depen-
dency parsers are: MaltParser (Nivre et al, 2006),
Mate (Bohnet, 2010)2 and MSTParser (McDonald
and Pereira, 2006). Table 1 has more information.
2A second-order MST parser (with the speed optimization).
2.2 Corpus
We used the latest Chinese TreeBank (CTB) 7.0 in
all experiments.3 CTB 7.0 is larger and has more
sources (e.g., web text), compared to previous ver-
sions. We split the data into train/development/test
sets (see Table 2), with gold word segmentation, fol-
lowing the guidelines suggested in documentation.
2.3 Settings
Every parser was run with its own default options.
However, since the default classifier used by Malt-
Parser is libsvm (Chang and Lin, 2011) with a poly-
nomial kernel, it may be too slow for training models
on all of CTB 7.0 training data in acceptable time.
Therefore, we also tested this particular parser with
the faster liblinear (Fan et al, 2008) classifier. All
experiments were performed on a machine with In-
tel?s Xeon E5620 2.40GHz CPU and 24GB RAM.
2.4 Features
Unlike constituent parsers, dependency models re-
quire exogenous part-of-speech (POS) tags, both in
training and in inference. We used the Stanford tag-
ger (Toutanova et al, 2003) v3.1, with the MEMM
model,4 in combination with 10-way jackknifing.5
Word lemmas ? which are generalizations of
words ? are another feature known to be useful
for dependency parsing. Here we lemmatized each
Chinese word down to its last character, since ? in
contrast to English ? a Chinese word?s suffix often
carries that word?s core sense (Tseng et al, 2005).
For example, bicycle (???), car (??) and
train (??) are all various kinds of vehicle (?).
3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2010T07
4nlp.stanford.edu/software/tagger.shtml
5Training sentences in each fold were tagged using a model
based on the other nine folds; development and test sentences
were tagged using a model based on all ten of the training folds.
12
Dev Test
Type Parser UAS LAS UAS LAS Parsing Time
Constituent Berkeley 82.0 77.0 82.9 77.8 45:56
Bikel 79.4 74.1 80.0 74.3 6,861:31
Charniak 77.8 71.7 78.3 72.3 128:04
Stanford 76.9 71.2 77.3 71.4 330:50
Dependency MaltParser (liblinear) 76.0 71.2 76.3 71.2 0:11
MaltParser (libsvm) 77.3 72.7 78.0 73.1 556:51
Mate (2nd-order) 82.8 78.2 83.1 78.1 87:19
MSTParser (1st-order) 78.8 73.4 78.9 73.1 12:17
Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)
are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen-
eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
3 Results
Table 3 tabulates efficiency and performance for all
parsers; UAS and LAS are unlabeled and labeled at-
tachment scores, respectively ? the standard crite-
ria for evaluating dependencies. They can be com-
puted via a CoNLL-X shared task dependency pars-
ing evaluation tool (without scoring punctuation).6
3.1 Chinese
Mate scored highest, and Berkeley was the most ac-
curate of constituent parsers, slightly behind Mate,
using half of the time. MaltParser (liblinear) was by
far the most efficient but also the least performant; it
scored higher with libsvm but took much more time.
The 1st-order MSTParser was more accurate than
MaltParser (libsvm) ? a result that differs from that
of Cer et al (2010) for English (see ?3.2). The Stan-
ford parser (the default for Stanford dependencies)
was only slightly more accurate than MaltParser (li-
blinear). Bikel?s parser was too slow to be used in
practice; and Charniak?s parser ? which performs
best for English ? did not work well for Chinese.
3.2 English
Our replication of Cer et al?s (2010, Table 1) evalua-
tion revealed a bug: MSTParser normalized all num-
bers to a <num> symbol, which decreased its scores
in the evaluation tool used with Stanford dependen-
cies. After fixing this glitch, MSTParser?s perfor-
mance improved from 78.8 (reported) to 82.5%, thus
making it more accurate than MaltParser (81.1%)
and hence the better dependency parser for English,
consistent with our results for Chinese (see Table 3).
6ilk.uvt.nl/conll/software/eval.pl
Our finding does not contradict the main qualita-
tive result of Cer et al (2010), however, since the
constituent parser of Charniak and Johnson (2005)
still scores substantially higher (89.1%), for English,
compared to all dependency parsers.7 In a separate
experiment (parsing web data),8 we found Mate to
be less accurate than Charniak-Johnson ? and im-
provement from jackknifing smaller ? on English.
4 Analysis
To further compare the constituent and dependency
approaches to generating Stanford dependencies, we
focused on Mate and Berkeley parsers ? the best
of each type. Overall, the difference between their
accuracies is not statistically significant (p > 0.05).9
Table 4 highlights performance (F1 scores) for the
most frequent relation labels. Mate does better on
most relations, noun compound modifiers (nn) and
adjectival modifiers (amod) in particular; and the
Berkeley parser is better at root and dep.10 Mate
seems to excel at short-distance dependencies, pos-
sibly because it uses more local features (even with
a second-order model) than the Berkeley parser,
whose PCFG can capture longer-distance rules.
Since POS-tags are especially informative of Chi-
nese dependencies (Li et al, 2011), we harmonized
training and test data, using 10-way jackknifing (see
?2.4). This method is more robust than training a
7One (small) factor contributing to the difference between
the two languages is that in the Chinese setup we stop with basic
Stanford dependencies ? there is no penalty for further conver-
sion; another is not using discriminative reranking for Chinese.
8sites.google.com/site/sancl2012/home/shared-task
9For LAS, p ? 0.11; and for UAS, p ? 0.25, according to
www.cis.upenn.edu/?dbikel/download/compare.pl
10An unmatched (default) relation (Chang et al, 2009, ?3.1).
13
Relation Count Mate Berkeley
nn 7,783 91.3 89.3
dep 4,651 69.4 70.3
nsubj 4,531 87.1 85.5
advmod 4,028 94.3 93.8
dobj 3,990 86.0 85.0
conj 2,159 76.0 75.8
prep 2,091 94.3 94.1
root 2,079 81.2 82.3
nummod 1,614 97.4 96.7
assmod 1,593 86.3 84.1
assm 1,590 88.9 87.2
pobj 1,532 84.2 82.9
amod 1,440 85.6 81.1
rcmod 1,433 74.0 70.6
cpm 1,371 84.4 83.2
Table 4: Performance (F1 scores) for the fifteen most-
frequent dependency relations in the CTB 7.0 develop-
ment data set attained by both Mate and Berkeley parsers.
parser with gold tags because it improves consis-
tency, particularly for Chinese, where tagging accu-
racies are lower than in English. On development
data, Mate scored worse given gold tags (75.4 versus
78.2%).11 Lemmatization offered additional useful
cues for overcoming data sparseness (77.8 without,
versus 78.2% with lemma features). Unsupervised
word clusters could thus also help (Koo et al, 2008).
5 Discussion
Our results suggest that if accuracy is of primary
concern, then Mate should be preferred;12 however,
Berkeley parser offers a trade-off between accuracy
and speed. If neither parser satisfies the demands
of a practical application (e.g., real-time processing
or bulk-parsing the web), then MaltParser (liblinear)
may be the only viable option. Fortunately, it comes
with much headroom for improving accuracy, in-
cluding a tunable margin parameter C for the classi-
fier, richer feature sets (Zhang and Nivre, 2011) and
ensemble models (Surdeanu and Manning, 2010).
Stanford dependencies are not the only popular
dependency representation. We also considered the
11Berkeley?s performance suffered with jackknifed tags (76.5
versus 77.0%), possibly because it parses and tags better jointly.
12Although Mate?s performance was not significantly better
than Berkeley?s in our setting, it has the potential to tap richer
features and other advantages of dependency parsers (Nivre and
McDonald, 2008) to further boost accuracy, which may be diffi-
cult in the generative framework of a typical constituent parser.
conversion scheme of the Penn2Malt tool,13 used
in a series of CoNLL shared tasks (Buchholz and
Marsi, 2006; Nivre et al, 2007; Surdeanu et al,
2008; Hajic? et al, 2009). However, this tool relies
on function tag information from the CTB in deter-
mining dependency relations. Since these tags usu-
ally cannot be produced by constituent parsers, we
could not, in turn, obtain CoNLL-style dependency
trees from their output. This points to another advan-
tage of dependency parsers: they need only the de-
pendency tree corpus to train and can conveniently
make use of native (unconverted) corpora, such as
the Chinese Dependency Treebank (Liu et al, 2006).
Lastly, we must note that although the Berkeley
parser is on par with Charniak?s (2000) system for
English (Cer et al, 2010, Table 1), its scores for Chi-
nese are substantially higher. There may be subtle
biases in Charniak?s approach (e.g., the conditioning
hierarchy used in smoothing) that could turn out to
be language-specific. The Berkeley parser appears
more general ? without quite as many parameters
or idiosyncratic design decisions ? as evidenced by
a recent application to French (Candito et al, 2010).
6 Conclusion
We compared seven popular open source parsers ?
four constituent and three dependency ? for gen-
erating Stanford dependencies in Chinese. Mate, a
high-order MST dependency parser, with lemmati-
zation and jackknifed POS-tags, appears most accu-
rate; but Berkeley?s faster constituent parser, with
jointly-inferred tags, is statistically no worse. This
outcome is different from English, where constituent
parsers systematically outperform direct methods.
Though Mate scored higher overall, Berkeley?s
parser was better at recovering longer-distance re-
lations, suggesting that a combined approach could
perhaps work better still (Rush et al, 2010, ?4.2).
Acknowledgments
We thank Daniel Cer, for helping us replicate the English ex-
perimental setup and for suggesting that we explore jackknifing
methods, and the anonymous reviewers, for valuable comments.
Supported in part by the National Natural Science Founda-
tion of China (NSFC) via grant 61133012, the National ?863?
Major Project grant 2011AA01A207, and the National ?863?
Leading Technology Research Project grant 2012AA011102.
13w3.msi.vxu.se/?nivre/research/Penn2Malt.html
14
Second author gratefully acknowledges the continued help
and support of his advisor, Dan Jurafsky, and of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program, under the Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions, findings,
and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the views
of DARPA, AFRL, or the US government.
References
Ion Androutsopoulos and Prodromos Malakasiotis. 2010.
A survey of paraphrasing and textual entailment methods.
Journal of Artificial Intelligence Research, 38(1):135?187,
May.
Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt
Broadhead, and Oren Etzioni. 2007. Open information ex-
traction from the web. In Proceedings of the 20th interna-
tional joint conference on Artifical intelligence, IJCAI?07,
pages 2670?2676, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Daniel M. Bikel. 2004. A distributional analysis of a lexi-
calized statistical parsing model. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 182?189,
Barcelona, Spain, July. Association for Computational Lin-
guistics.
Bernd Bohnet. 2010. Top accuracy and fast dependency pars-
ing is not a contradiction. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics (Coling
2010), pages 89?97, Beijing, China, August. Coling 2010
Organizing Committee.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Language
Learning (CoNLL-X), pages 149?164, New York City, June.
Association for Computational Linguistics.
Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Hene-
stroza Anguiano. 2010. Benchmarking of statistical depen-
dency parsers for French. In Coling 2010: Posters, pages
108?116, Beijing, China, August. Coling 2010 Organizing
Committee.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and
Christopher D. Manning. 2010. Parsing to Stanford depen-
dencies: Trade-offs between speed and accuracy. In Pro-
ceedings of the 7th International Conference on Language
Resources and Evaluation (LREC 2010).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A li-
brary for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27:1?27:27, May.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Discriminative reordering with
Chinese grammatical relations features. In Proceedings of
the Third Workshop on Syntax and Structure in Statistical
Translation, Boulder, Colorado, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 173?180, Ann
Arbor, Michigan, June. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference,
NAACL 2000, pages 132?139, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
COLING Workshop on Cross-framework and Cross-domain
Parser Evaluation.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
Fifth International Conference on Language Resources and
Evaluation (LREC?06).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning
Research, 9:1871?1874, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek,
Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages. In
Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Ruihong Huang, Le Sun, and Yuanyong Feng. 2008. Study
of kernel-based methods for Chinese relation extraction. In
Proceedings of the 4th Asia information retrieval conference
on Information retrieval technology, AIRS?08, pages 598?
604, Berlin, Heidelberg. Springer-Verlag.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
shared task on event extraction. In Proceedings of the Work-
shop on Current Trends in Biomedical Natural Language
Processing: Shared Task, BioNLP ?09, pages 1?9, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics - Volume
1, ACL ?03, pages 423?430, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 1?11, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple semi-supervised dependency parsing. In Proceedings of
ACL-08: HLT, pages 595?603, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre. 2009.
Dependency Parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
15
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang
Chen, and Haizhou Li. 2011. Joint models for Chinese POS
tagging and dependency parsing. In Proceedings of the 2011
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 1180?1191, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building a de-
pendency treebank for improving Chinese parser. Journal of
Chinese Language and Computing, 16(4).
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of the 11th Conference of the European Chapter of the
ACL (EACL 2006), pages 81?88.
Arun Meena and T. V. Prabhakar. 2007. Sentence level sen-
timent analysis in the presence of conjuncts using linguistic
analysis. In Proceedings of the 29th European conference on
IR research, ECIR?07, pages 573?580, Berlin, Heidelberg.
Springer-Verlag.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In Proceed-
ings of ACL-08: HLT, pages 950?958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages 2216?
2219.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932, Prague, Czech Republic, June.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics, pages
433?440, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and linear
programming relaxations for natural language processing. In
Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1?11, Cambridge,
MA, October. Association for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble
models for dependency parsing: cheap and good? In Hu-
man Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 649?652, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic dependen-
cies. In CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning, pages
159?177, Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 173?180,
Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.
2005. Morphological features help POS tagging of un-
known words across language varieties. In Proceedings of
the fourth SIGHAN bakeoff.
Fei Wu and Daniel S. Weld. 2010. Open information extraction
using Wikipedia. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 118?127, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009.
Phrase dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1533?1541, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2011.
Structural opinion mining for graph-based sentiment rep-
resentation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP ?11,
pages 1332?1341, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceedings
of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 188?193, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of the 15th
ACM international conference on Information and knowl-
edge management, CIKM ?06, pages 43?50, New York, NY,
USA. ACM.
16
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Viterbi Training Improves Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky and Christopher D. Manning
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu and manning@cs.stanford.edu
Abstract
We show that Viterbi (or ?hard?) EM is
well-suited to unsupervised grammar in-
duction. It is more accurate than standard
inside-outside re-estimation (classic EM),
significantly faster, and simpler. Our ex-
periments with Klein and Manning?s De-
pendency Model with Valence (DMV) at-
tain state-of-the-art performance ? 44.8%
accuracy on Section 23 (all sentences) of
the Wall Street Journal corpus ? without
clever initialization; with a good initial-
izer, Viterbi training improves to 47.9%.
This generalizes to the Brown corpus,
our held-out set, where accuracy reaches
50.8% ? a 7.5% gain over previous best
results. We find that classic EM learns bet-
ter from short sentences but cannot cope
with longer ones, where Viterbi thrives.
However, we explain that both algorithms
optimize the wrong objectives and prove
that there are fundamental disconnects be-
tween the likelihoods of sentences, best
parses, and true parses, beyond the well-
established discrepancies between likeli-
hood, accuracy and extrinsic performance.
1 Introduction
Unsupervised learning is hard, often involving dif-
ficult objective functions. A typical approach is
to attempt maximizing the likelihood of unlabeled
data, in accordance with a probabilistic model.
Sadly, such functions are riddled with local op-
tima (Charniak, 1993, Ch. 7, inter alia), since their
number of peaks grows exponentially with in-
stances of hidden variables. Furthermore, a higher
likelihood does not always translate into superior
task-specific accuracy (Elworthy, 1994; Merialdo,
1994). Both complications are real, but we will
discuss perhaps more significant shortcomings.
We prove that learning can be error-prone even
in cases when likelihood is an appropriate mea-
sure of extrinsic performance and where global
optimization is feasible. This is because a key
challenge in unsupervised learning is that the de-
sired likelihood is unknown. Its absence renders
tasks like structure discovery inherently under-
constrained. Search-based algorithms adopt sur-
rogate metrics, gambling on convergence to the
?right? regularities in data. Their wrong objec-
tives create cases in which both efficiency and per-
formance improve when expensive exact learning
techniques are replaced by cheap approximations.
We propose using Viterbi training (Brown
et al, 1993), instead of inside-outside re-
estimation (Baker, 1979), to induce hierarchical
syntactic structure from natural language text. Our
experiments with Klein and Manning?s (2004) De-
pendency Model with Valence (DMV), a popular
state-of-the-art model (Headden et al, 2009; Co-
hen and Smith, 2009; Spitkovsky et al, 2009),
beat previous benchmark accuracies by 3.8% (on
Section 23 of WSJ) and 7.5% (on parsed Brown).
Since objective functions used in unsupervised
grammar induction are provably wrong, advan-
tages of exact inference may not apply. It makes
sense to try the Viterbi approximation ? it is also
wrong, only simpler and cheaper than classic EM.
As it turns out, Viterbi EM is not only faster but
also more accurate, consistent with hypotheses of
de Marcken (1995) and Spitkovsky et al (2009).
We begin by reviewing the model, standard data
sets and metrics, and our experimental results. Af-
ter relating our contributions to prior work, we
delve into proofs by construction, using the DMV.
9
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100 (Spitkovsky et al, 2009).
NNS VBD IN NN ?
Payrolls fell in September .
P = (1?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A dependency structure for a short sen-
tence and its probability, as factored by the DMV,
after summing out PORDER (Spitkovsky et al, 2009).
2 Dependency Model with Valence
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over
lexical word classes {cw} ? POS tags. Its gener-
ative story for a sub-tree rooted at a head (of class
ch) rests on three types of independent decisions:
(i) initial direction dir ? {L, R} in which to attach
children, via probability PORDER(ch); (ii) whether to
seal dir, stopping with probability PSTOP(ch, dir, adj),
conditioned on adj ? {T, F} (true iff considering
dir?s first, i.e., adjacent, child); and (iii) attach-
ments (of class ca), according to PATTACH(ch, dir, ca).
This produces only projective trees. A root token
? generates the head of a sentence as its left (and
only) child. Figure 2 displays a simple example.
The DMV lends itself to unsupervised learn-
ing via inside-outside re-estimation (Baker, 1979).
Viterbi training (Brown et al, 1993) re-estimates
each next model as if supervised by the previous
best parse trees. And supervised learning from
reference parse trees is straight-forward, since
maximum-likelihood estimation reduces to count-
ing: P?ATTACH(ch, dir, ca) is the fraction of children ?
those of class ca ? attached on the dir side of a
head of class ch; P?STOP(ch, dir, adj = T), the frac-
tion of words of class ch with no children on the
dir side; and P?STOP(ch, dir, adj = F), the ratio1 of the
number of words of class ch having a child on the
dir side to their total number of such children.
3 Standard Data Sets and Evaluation
The DMV is traditionally trained and tested on
customized subsets of Penn English Treebank?s
Wall Street Journal portion (Marcus et al, 1993).
Following Klein and Manning (2004), we be-
gin with reference constituent parses and com-
pare against deterministically derived dependen-
cies: after pruning out all empty sub-trees, punc-
tuation and terminals (tagged # and $) not pro-
nounced where they appear, we drop all sentences
with more than a prescribed number of tokens
remaining and use automatic ?head-percolation?
rules (Collins, 1999) to convert the rest, as is stan-
dard practice. We experiment with WSJk (sen-
tences with at most k tokens), for 1 ? k ? 45, and
Section 23 of WSJ? (all sentence lengths). We
also evaluate on Brown100, similarly derived from
the parsed portion of the Brown corpus (Francis
and Kucera, 1979), as our held-out set. Figure 1
shows these corpora?s sentence and token counts.
Proposed parse trees are judged on accuracy: a
directed score is simply the overall fraction of cor-
rectly guessed dependencies. Let S be a set of
sentences, with |s| the number of terminals (to-
1The expected number of trials needed to get one
Bernoulli(p) success is n ? Geometric(p), with n ? Z+,
P(n) = (1 ? p)n?1p and E(n) = p?1; MoM and MLE
agree, p? = (# of successes)/(# of trials).
10
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
WSJk
(training on all WSJ sentences up to k tokens in length)
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
5 10 15 20 25 30 35 40
50
100
150
200
350
400
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(c) Iterations for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
50
100
150
200
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(d) Iterations for Viterbi (Hard EM)
Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, then
tested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al,
2009). Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization,
a linguistically-biased initializer (Ad-Hoc?) and the uninformed (uniform) prior. Panel (b) shows results
obtained with Viterbi training instead of classic EM ? Panel (a), but is otherwise identical (in both, each
of the 45 vertical slices captures five new experimental results and arrows connect starting performance
with final accuracy, emphasizing the impact of learning). Panels (c) and (d) show the corresponding
numbers of iterations until EM?s convergence.
kens) for each s ? S. Denote by T (s) the set
of all dependency parse trees of s, and let ti(s)
stand for the parent of token i, 1 ? i ? |s|, in
t(s) ? T (s). Call the gold reference t?(s) ? T (s).
For a given model of grammar, parameterized by
?, let t??(s) ? T (s) be a (not necessarily unique)
likeliest (also known as Viterbi) parse of s:
t??(s) ?
{
arg max
t?T (s)
P?(t)
}
;
then ??s directed accuracy on a reference set R is
100% ?
?
s?R
?|s|
i=1 1{t??i (s)=t?i (s)}?
s?R |s|
.
4 Experimental Setup and Results
Following Spitkovsky et al (2009), we trained the
DMV on data sets WSJ{1, . . . , 45} using three ini-
tialization strategies: (i) the uninformed uniform
prior; (ii) a linguistically-biased initializer, Ad-
Hoc?;2 and (iii) an oracle ? the supervised MLE
solution. Standard training is without smoothing,
iterating each run until successive changes in over-
all per-token cross-entropy drop below 2?20 bits.
We re-trained all models using Viterbi EM
instead of inside-outside re-estimation, explored
Laplace (add-one) smoothing during training, and
experimented with hybrid initialization strategies.
2Ad-Hoc? is Spitkovsky et al?s (2009) variation on Klein
and Manning?s (2004) ?ad-hoc harmonic? completion.
11
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing
(brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al, 2009).
4.1 Result #1: Viterbi-Trained Models
The results of Spitkovsky et al (2009), tested
against WSJ40, are re-printed in Figure 3(a); our
corresponding Viterbi runs appear in Figure 3(b).
We observe crucial differences between the two
training modes for each of the three initialization
strategies. Both algorithms walk away from the
supervised maximum-likelihood solution; how-
ever, Viterbi EM loses at most a few points of
accuracy (3.7% at WSJ40), whereas classic EM
drops nearly twenty points (19.1% at WSJ45). In
both cases, the single best unsupervised result is
with good initialization, although Viterbi peaks
earlier (45.9% at WSJ8) and in a narrower range
(WSJ8-9) than classic EM (44.3% at WSJ15;
WSJ13-20). The uniform prior never quite gets off
the ground with classic EM but manages quite well
under Viterbi training,3 given sufficient data ? it
even beats the ?clever? initializer everywhere past
WSJ10. The ?sweet spot? at WSJ15 ? a neigh-
borhood where both Ad-Hoc? and the oracle ex-
cel under classic EM ? disappears with Viterbi.
Furthermore, Viterbi does not degrade with more
(complex) data, except with a biased initializer.
More than a simple efficiency hack, Viterbi EM
actually improves performance. And its benefits to
running times are also non-trivial: it not only skips
computing the outside charts in every iteration but
also converges (sometimes an order of magnitude)
3In a concurrently published related work, Cohen and
Smith (2010) prove that the uniform-at-random initializer is a
competitive starting M-step for Viterbi EM; our uninformed
prior consists of uniform multinomials, seeding the E-step.
faster than classic EM (see Figure 3(c,d)).4
4.2 Result #2: Smoothed Models
Smoothing rarely helps classic EM and hurts in
the case of oracle training (see Figure 4(a)). With
Viterbi, supervised initialization suffers much less,
the biased initializer is a wash, and the uninformed
uniform prior generally gains a few points of ac-
curacy, e.g., up 2.9% (from 42.4% to 45.2%, eval-
uated against WSJ40) at WSJ15 (see Figure 4(b)).
Baby Steps (Spitkovsky et al, 2009) ? iterative
re-training with increasingly more complex data
sets, WSJ1, . . . ,WSJ45 ? using smoothed Viterbi
training fails miserably (see Figure 4(b)), due to
Viterbi?s poor initial performance at short sen-
tences (possibly because of data sparsity and sen-
sitivity to non-sentences ? see examples in ?7.3).
4.3 Result #3: State-of-the-Art Models
Simply training up smoothed Viterbi at WSJ15,
using the uninformed uniform prior, yields 44.8%
accuracy on Section 23 of WSJ?, already beating
previous state-of-the-art by 0.7% (see Table 1(A)).
Since both classic EM and Ad-Hoc? initializers
work well with short sentences (see Figure 3(a)),
it makes sense to use their pre-trained models to
initialize Viterbi training, mixing the two strate-
gies. We judged all Ad-Hoc? initializers against
WSJ15 and found that the one for WSJ8 mini-
mizes sentence-level cross-entropy (see Figure 5).
This approach does not involve reference parse
4For classic EM, the number of iterations to convergence
appears sometimes inversely related to performance, giving
credence to the notion of early termination as a regularizer.
12
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Less is More (Ad-Hoc? @15) (Spitkovsky et al, 2009) 56.2 48.2 44.1 43.3
A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1
B. A Good Initializer (Ad-Hoc??s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3
C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5
D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 1: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 5: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
trees and is therefore still unsupervised. Using the
Ad-Hoc? initializer based on WSJ8 to seed classic
training at WSJ15 yields a further 1.4% gain in ac-
curacy, scoring 46.2% on WSJ? (see Table 1(B)).
This good initializer boosts accuracy attained
by smoothed Viterbi at WSJ15 to 47.8% (see Ta-
ble 1(C)). Using its solution to re-initialize train-
ing at WSJ45 gives a tiny further improvement
(0.1%) on Section 23 of WSJ? but bigger gains
on WSJ10 (0.9%) and WSJ20 (see Table 1(D)).
Our results generalize. Gains due to smoothed
Viterbi training and favorable initialization carry
over to Brown100 ? accuracy improves by 7.5%
over previous published numbers (see Table 1).5
5 Discussion of Experimental Results
The DMV has no parameters to capture syntactic
relationships beyond local trees, e.g., agreement.
Spitkovsky et al (2009) suggest that classic EM
breaks down as sentences get longer precisely be-
cause the model makes unwarranted independence
assumptions. They hypothesize that the DMV re-
serves too much probability mass for what should
be unlikely productions. Since EM faithfully al-
locates such re-distributions across the possible
parse trees, once sentences grow sufficiently long,
this process begins to deplete what began as like-
lier structures. But medium lengths avoid a flood
of exponentially-confusing longer sentences (and
5In a sister paper, Spitkovsky et al (2010) improve perfor-
mance by incorporating parsing constraints harvested from
the web into Viterbi training; nevertheless, results presented
in this paper remain the best of models trained purely on WSJ.
the sparseness of unrepresentative shorter ones).6
Our experiments corroborate this hypothesis.
First of all, Viterbi manages to hang on to su-
pervised solutions much better than classic EM.
Second, Viterbi does not universally degrade with
more (complex) training sets, except with a biased
initializer. And third, Viterbi learns poorly from
small data sets of short sentences (WSJk, k < 5).
Viterbi may be better suited to unsupervised
grammar induction compared with classic EM, but
neither is sufficient, by itself. Both algorithms
abandon good solutions and make no guarantees
with respect to extrinsic performance. Unfortu-
nately, these two approaches share a deep flaw.
6 Related Work on Improper Objectives
It is well-known that maximizing likelihood may,
in fact, degrade accuracy (Pereira and Schabes,
1992; Elworthy, 1994; Merialdo, 1994). de Mar-
cken (1995) showed that classic EM suffers from
a fatal attraction towards deterministic grammars
and suggested a Viterbi training scheme as a rem-
edy. Liang and Klein?s (2008) analysis of errors
in unsupervised learning began with the inappro-
priateness of the likelihood objective (approxima-
tion), explored problems of data sparsity (estima-
tion) and focused on EM-specific issues related to
non-convexity (identifiability and optimization).
Previous literature primarily relied on experi-
mental evidence. de Marcken?s analytical result is
an exception but pertains only to EM-specific lo-
cal attractors. Our analysis confirms his intuitions
and moreover shows that there can be global pref-
erences for deterministic grammars ? problems
that would persist with tractable optimization. We
prove that there is a fundamental disconnect be-
tween objective functions even when likelihood is
a reasonable metric and training data are infinite.
6Klein and Manning (2004) originally trained the DMV
on WSJ10 and Gillenwater et al (2009) found it useful to dis-
card data from WSJ3, which is mostly incomplete sentences.
13
7 Proofs (by Construction)
There is a subtle distinction between three differ-
ent probability distributions that arise in parsing,
each of which can be legitimately termed ?likeli-
hood? ? the mass that a particular model assigns
to (i) highest-scoring (Viterbi) parse trees; (ii) the
correct (gold) reference trees; and (iii) the sen-
tence strings (sums over all derivations). A classic
unsupervised parser trains to optimize the third,
makes actual parsing decisions according to the
first, and is evaluated against the second. There
are several potential disconnects here. First of all,
the true generative model ?? may not yield the
largest margin separations for discriminating be-
tween gold parse trees and next best alternatives;
and second, ?? may assign sub-optimal mass to
string probabilities. There is no reason why an op-
timal estimate ?? should make the best parser or
coincide with a peak of an unsupervised objective.
7.1 The Three Likelihood Objectives
A supervised parser finds the ?best? parameters
?? by maximizing the likelihood of all reference
structures t?(s) ? the product, over all sentences,
of the probabilities that it assigns to each such tree:
??SUP = arg max
?
L(?) = arg max
?
?
s
P?(t?(s)).
For the DMV, this objective function is convex ?
its unique peak is easy to find and should match
the true distribution ?? given enough data, barring
practical problems caused by numerical instability
and inappropriate independence assumptions. It is
often easier to work in log-probability space:
??SUP = arg max? logL(?)
= arg max?
?
s log P?(t?(s)).
Cross-entropy, measured in bits per token (bpt),
offers an interpretable proxy for a model?s quality:
h(?) = ?
?
s lg P?(t?(s))?
s |s|
.
Clearly, arg max? L(?) = ??SUP = arg min? h(?).
Unsupervised parsers cannot rely on references
and attempt to jointly maximize the probability of
each sentence instead, summing over the probabil-
ities of all possible trees, according to a model ?:
??UNS = arg max
?
?
s
log
?
t?T (s)
P?(t)
? ?? ?
P?(s)
.
This objective function is not convex and in gen-
eral does not have a unique peak, so in practice one
usually settles for ??UNS ? a fixed point. There is no
reason why ??SUP should agree with ??UNS, which is
in turn (often badly) approximated by ??UNS, in our
case using EM. A logical alternative to maximiz-
ing the probability of sentences is to maximize the
probability of the most likely parse trees instead:7
??VIT = arg max
?
?
s
log P?(t??(s)).
This 1-best approximation similarly arrives at ??VIT,
with no claims of optimality. Each next model is
re-estimated as if supervised by reference parses.
7.2 A Warm-Up Case: Accuracy vs. ??SUP 6= ??
A simple way to derail accuracy is to maximize
the likelihood of an incorrect model, e.g., one that
makes false independence assumptions. Consider
fitting the DMV to a contrived distribution ? two
equiprobable structures over identical three-token
sentences from a unary vocabulary { a?}:
(i) x xa? a? a?; (ii) y ya? a? a?.
There are six tokens and only two have children
on any given side, so adjacent stopping MLEs are:
P?STOP( a?, L, T) = P?STOP( a?, R, T) = 1 ?
2
6 =
2
3 .
The rest of the estimated model is deterministic:
P?ATTACH(?, L, a?) = P?ATTACH( a?, ?, a?) = 1
and P?STOP( a?, ?, F) = 1,
since all dependents are a? and every one is an
only child. But the DMV generates left- and right-
attachments independently, allowing a third parse:
(iii) x ya? a? a?.
It also cannot capture the fact that all structures are
local (or that all dependency arcs point in the same
direction), admitting two additional parse trees:
(iv) a? xa? a?; (v) ya? a? a?.
Each possible structure must make four (out of six)
adjacent stops, incurring identical probabilities:
P?STOP( a?, ?, T)4 ? (1 ? P?STOP( a?, ?, T))2 =
24
36 .
7It is also possible to use k-best Viterbi, with k > 1.
14
Thus, the MLE model does not break symmetry
and rates each of the five parse trees as equally
likely. Therefore, its expected per-token accuracy
is 40%. Average overlaps between structures (i-v)
and answers (i,ii) are (i) 100% or 0; (ii) 0 or 100%;
and (iii,iv,v) 33.3%: (3+3)/(5?3) = 2/5 = 0.4.
A decoy model without left- or right-branching,
i.e., P?STOP( a?, L, T) = 1 or P?STOP( a?, R, T) = 1,
would assign zero probability to some of the train-
ing data. It would be forced to parse every instance
of a? a? a? either as (i) or as (ii), deterministically.
Nevertheless, it would attain a higher per-token ac-
curacy of 50%. (Judged on exact matches, at the
granularity of whole trees, the decoy?s guaranteed
50% accuracy clobbers the MLE?s expected 20%.)
Our toy data set could be replicated n-fold with-
out changing the analysis. This confirms that, even
in the absence of estimation errors or data sparsity,
there can be a fundamental disconnect between
likelihood and accuracy, if the model is wrong.8
7.3 A Subtler Case: ?? = ??SUP vs. ??UNS vs. ??VIT
We now prove that, even with the right model,
mismatches between the different objective like-
lihoods can also handicap the truth. Our calcula-
tions are again exact, so there are no issues with
numerical stability. We work with a set of param-
eters ?? already factored by the DMV, so that its
problems could not be blamed on invalid indepen-
dence assumptions. Yet we are able to find another
impostor distribution ?? that outshines ??SUP = ?? on
both unsupervised metrics, which proves that the
true models ??SUP and ?? are not globally optimal,
as judged by the two surrogate objective functions.
This next example is organic. We began with
WSJ10 and confirmed that classic EM abandons
the supervised solution. We then iteratively dis-
carded large portions of the data set, so long as
the remainder maintained the (un)desired effect ?
EM walking away from its ??SUP. This procedure
isolated such behavior, arriving at a minimal set:
NP : NNP NNP ?
? Marvin Alisky.
S : NNP VBD ?
(Braniff declined).
NP-LOC : NNP NNP ?
Victoria, Texas
8And as George Box quipped, ?Essentially, all models are
wrong, but some are useful? (Box and Draper, 1987, p. 424).
This kernel is tiny, but, as before, our analysis is
invariant to n-fold replication: the problem cannot
be explained away by a small training size ? it
persists even in infinitely large data sets. And so,
we consider three reference parse trees for two-
token sentences over a binary vocabulary { a?, z?}:
(i) xa? a?; (ii) xa? z?; (iii) ya? a?.
One third of the time, z? is the head; only a? can
be a child; and only a? has right-dependents. Trees
(i)-(iii) are the only two-terminal parses generated
by the model and are equiprobable. Thus, these
sentences are representative of a length-two re-
striction of everything generated by the true ??:
PATTACH(?, L, a?) =
2
3 and PSTOP( a?, ?, T) =
4
5 ,
since a? is the head two out of three times, and
since only one out of five a??s attaches a child on
either side. Elsewhere, the model is deterministic:
PSTOP( z?, L, T) = 0;
PSTOP(?, ?, F) = PSTOP( z?, R, T) = 1;
PATTACH( a?, ?, a?) = PATTACH( z?, L, a?) = 1.
Contrast the optimal estimate ??SUP = ?? with the
decoy fixed point9 ?? that is identical to ??, except
P?STOP( a?, L, T) =
3
5 and P?STOP( a?, R, T) = 1.
The probability of stopping is now 3/5 on the left
and 1 on the right, instead of 4/5 on both sides ?
?? disallows a??s right-dependents but preserves its
overall fertility. The probabilities of leaves a? (no
children), under the models ??SUP and ??, are:
P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
(4
5
)2
and P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
3
5 .
And the probabilities of, e.g., structure
x
a? z?, are:
P?ATTACH(?, L, z?) ? P?STOP( z?, R, T)
? (1 ? P?STOP( z?, L, T)) ? P?STOP( z?, L, F)
? P?ATTACH( z?, L, a?) ? P?( a?)
9The model estimated from the parse trees induced by ??
over the three sentences is again ??, for both soft and hard EM.
15
= P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
16
25
and P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
3
5 .
Similarly, the probabilities of all four possible
parse trees for the two distinct sentences, a? a? and
a? z?, under the two models, ??SUP = ?? and ??, are:
??SUP = ?? ??
x
a? z? 13
` 16
25
?
= 13
` 3
5
?
=
16
75 = 0.213 15 = 0.2y
a? z? 0 0
x
a? a? 23
` 4
5
? `
1? 45
? ` 16
25
?
= 23
`
1 ? 35
? ` 3
5
?
=
128
1875 = 0.06826 425 = 0.16y
a? a? 0.06826 0
To the three true parses, ??SUP assigns probability
(16
75
) ( 128
1875
)2 ? 0.0009942 ? about 1.66bpt; ??
leaves zero mass for (iii), corresponding to a larger
(infinite) cross-entropy, consistent with theory.
So far so good, but if asked for best (Viterbi)
parses, ??SUP could still produce the actual trees,
whereas ?? would happily parse sentences of (iii)
and (i) the same, perceiving a joint probability of
(0.2)(0.16)2 = 0.00512 ? just 1.27bpt, appear-
ing to outperform ??SUP = ??! Asked for sentence
probabilities, ?? would remain unchanged (it parses
each sentence unambiguously), but ??SUP would ag-
gregate to
(16
75
) (
2 ? 1281875
)2 ? 0.003977, improv-
ing to 1.33bpt, but still noticeably ?worse? than ??.
Despite leaving zero probability to the truth, ??
beats ?? on both surrogate metrics, globally. This
seems like an egregious error. Judged by (extrin-
sic) accuracy, ?? still holds its own: it gets four
directed edges from predicting parse trees (i) and
(ii) completely right, but none of (iii) ? a solid
66.7%. Subject to tie-breaking, ?? is equally likely
to get (i) and/or (iii) entirely right or totally wrong
(they are indistinguishable): it could earn a perfect
100%, tie ??, or score a low 33.3%, at 1:2:1 odds,
respectively ? same as ???s deterministic 66.7%
accuracy, in expectation, but with higher variance.
8 Discussion of Theoretical Results
Daume? et al (2009) questioned the benefits of us-
ing exact models in approximate inference. In our
case, the model already makes strong simplifying
assumptions and the objective is also incorrect. It
makes sense that Viterbi EM sometimes works,
since an approximate wrong ?solution? could, by
chance, be better than one that is exactly wrong.
One reason why Viterbi EM may work well is
that its score is used in selecting actual output
parse trees. Wainwright (2006) provided strong
theoretical and empirical arguments for using the
same approximate inference method in training
as in performing predictions for a learned model.
He showed that if inference involves an approxi-
mation, then using the same approximate method
to train the model gives even better performance
guarantees than exact training methods. If our task
were not parsing but language modeling, where
the relevant score is the sum of the probabilities
over individual derivations, perhaps classic EM
would not be doing as badly, compared to Viterbi.
Viterbi training is not only faster and more accu-
rate but also free of inside-outside?s recursion con-
straints. It therefore invites more flexible model-
ing techniques, including discriminative, feature-
rich approaches that target conditional likelihoods,
essentially via (unsupervised) self-training (Clark
et al, 2003; Ng and Cardie, 2003; McClosky et
al., 2006a; McClosky et al, 2006b, inter alia).
Such ?learning by doing? approaches may be
relevant to understanding human language ac-
quisition, as children frequently find themselves
forced to interpret a sentence in order to inter-
act with the world. Since most models of human
probabilistic parsing are massively pruned (Juraf-
sky, 1996; Chater et al, 1998; Lewis and Vasishth,
2005, inter alia), the serial nature of Viterbi EM
? or the very limited parallelism of k-best Viterbi
? may be more appropriate in modeling this task
than the fully-integrated inside-outside solution.
9 Conclusion
Without a known objective, as in unsupervised
learning, correct exact optimization becomes im-
possible. In such cases, approximations, although
liable to pass over a true optimum, may achieve
faster convergence and still improve performance.
We showed that this is the case with Viterbi
training, a cheap alternative to inside-outside re-
estimation, for unsupervised dependency parsing.
We explained why Viterbi EM may be partic-
ularly well-suited to learning from longer sen-
tences, in addition to any general benefits to syn-
chronizing approximation methods across learn-
ing and inference. Our best algorithm is sim-
pler and an order of magnitude faster than clas-
sic EM. It achieves state-of-the-art performance:
3.8% higher accuracy than previous published best
16
results on Section 23 (all sentences) of the Wall
Street Journal corpus. This improvement general-
izes to the Brown corpus, our held-out evaluation
set, where the same model registers a 7.5% gain.
Unfortunately, approximations alone do not
bridge the real gap between objective functions.
This deeper issue could be addressed by drawing
parsing constraints (Pereira and Schabes, 1992)
from specific applications. One example of such
an approach, tied to machine translation, is syn-
chronous grammars (Alshawi and Douglas, 2000).
An alternative ? observing constraints induced by
hyper-text mark-up, harvested from the web ? is
explored in a sister paper (Spitkovsky et al, 2010),
published concurrently.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fan-
nie & John Hertz Foundation Fellowship. We thank An-
gel X. Chang, Mengqiu Wang and the anonymous reviewers
for many helpful comments on draft versions of this paper.
References
H. Alshawi and S. Douglas. 2000. Learning dependency
transduction models from unannotated examples. In
Royal Society of London Philosophical Transactions Se-
ries A, volume 358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
G. E. P. Box and N. R. Draper. 1987. Empirical Model-
Building and Response Surfaces. John Wiley.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
N. Chater, M. J. Crocker, and M. J. Pickering. 1998. The
rational analysis of inquiry: The case of parsing. In
M. Oaksford and N. Chater, editors, Rational Models of
Cognition. Oxford University Press.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrapping
POS-taggers using unlabelled data. In Proc. of CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for
PCFGs: Hardness results and competitiveness of uniform
initialization. In Proc. of ACL.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
H. Daume?, III, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine Learning, 75(3).
C. de Marcken. 1995. Lexical heads, phrase structure and
the induction of grammar. In WVLC.
D. Elworthy. 1994. Does Baum-Welch re-estimation help
taggers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and
F. Pereira. 2009. Sparsity in grammar induction. In
NIPS: Grammar Induction, Representation of Language
and Language Learning.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In Proc. of NAACL-HLT.
D. Jurafsky. 1996. A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory retrieval.
Cognitive Science, 29.
P. Liang and D. Klein. 2008. Analyzing the errors of unsu-
pervised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006a. Effec-
tive self-training for parsing. In Proc. of NAACL-HLT.
D. McClosky, E. Charniak, and M. Johnson. 2006b. Rerank-
ing and self-training for parser adaptation. In Proc. of
COLING-ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2).
V. Ng and C. Cardie. 2003. Weakly supervised natural lan-
guage learning without redundant views. In Proc. of HLT-
NAACL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010. Profit-
ing from mark-up: Hyper-text annotations for guided pars-
ing. In Proc. of ACL.
M. J. Wainwright. 2006. Estimating the ?wrong? graphical
model: Benefits in the computation-limited setting. Jour-
nal of Machine Learning Research, 7.
17
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 19?28,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Punctuation: Making a Point in Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We show how punctuation can be used to im-
prove unsupervised dependency parsing. Our
linguistic analysis confirms the strong connec-
tion between English punctuation and phrase
boundaries in the Penn Treebank. However,
approaches that naively include punctuation
marks in the grammar (as if they were words)
do not perform well with Klein and Manning?s
Dependency Model with Valence (DMV). In-
stead, we split a sentence at punctuation and
impose parsing restrictions over its fragments.
Our grammar inducer is trained on the Wall
Street Journal (WSJ) and achieves 59.5% ac-
curacy out-of-domain (Brown sentences with
100 or fewer words), more than 6% higher
than the previous best results. Further evalu-
ation, using the 2006/7 CoNLL sets, reveals
that punctuation aids grammar induction in
17 of 18 languages, for an overall average
net gain of 1.3%. Some of this improvement
is from training, but more than half is from
parsing with induced constraints, in inference.
Punctuation-aware decoding works with exist-
ing (even already-trained) parsing models and
always increased accuracy in our experiments.
1 Introduction
Unsupervised dependency parsing is a type of gram-
mar induction ? a central problem in computational
linguistics. It aims to uncover hidden relations be-
tween head words and their dependents in free-form
text. Despite decades of significant research efforts,
the task still poses a challenge, as sentence structure
is underdetermined by only raw, unannotated words.
Structure can be clearer in formatted text, which
typically includes proper capitalization and punctua-
tion (Gravano et al, 2009). Raw word streams, such
as utterances transcribed by speech recognizers, are
often difficult even for humans (Kim and Woodland,
2002). Therefore, one would expect grammar induc-
ers to exploit any available linguistic meta-data. And
yet in unsupervised dependency parsing, sentence-
internal punctuation has long been ignored (Carroll
and Charniak, 1992; Paskin, 2001; Klein and Man-
ning, 2004; Blunsom and Cohn, 2010, inter alia).
HTML is another kind of meta-data that is ordi-
narily stripped out in pre-processing. However, re-
cently Spitkovsky et al (2010b) demonstrated that
web markup can successfully guide hierarchical
syntactic structure discovery, observing, for exam-
ple, that anchors often match linguistic constituents:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
We propose exploring punctuation?s potential to
aid grammar induction. Consider a motivating ex-
ample (all of our examples are from WSJ), in which
all (six) marks align with constituent boundaries:
[SBAR Although it probably has reduced the level of
expenditures for some purchasers], [NP utilization man-
agement] ? [PP like most other cost containment strate-
gies] ? [VP doesn?t appear to have altered the long-term
rate of increase in health-care costs], [NP the Institute of
Medicine], [NP an affiliate of the National Academy of
Sciences], [VP concluded after a two-year study].
This link between punctuation and constituent
boundaries suggests that we could approximate
parsing by treating inter-punctuation fragments in-
dependently. In training, our algorithm first parses
each fragment separately, then parses the sequence
of the resulting head words. In inference, we use a
better approximation that allows heads of fragments
to be attached by arbitrary external words, e.g.:
The Soviets complicated the issue by offering to
[VP include light tanks], [SBAR which are as light as ... ].
19
Count POS Sequence Frac Cum
1 3,492 NNP 2.8%
2 2,716 CD CD 2.2 5.0
3 2,519 NNP NNP 2.0 7.1
4 2,512 RB 2.0 9.1
5 1,495 CD 1.2 10.3
6 1,025 NN 0.8 11.1
7 1,023 NNP NNP NNP 0.8 11.9
8 916 IN NN 0.7 12.7
9 795 VBZ NNP NNP 0.6 13.3
10 748 CC 0.6 13.9
11 730 CD DT NN 0.6 14.5
12 705 PRP VBD 0.6 15.1
13 652 JJ NN 0.5 15.6
14 648 DT NN 0.5 16.1
15 627 IN DT NN 0.5 16.6
WSJ +103,148 more with Count ? 621 83.4%
Table 1: Top 15 fragments of POS tag sequences in WSJ.
Count Non-Terminal Frac Cum
1 40,223 S 32.5%
2 33,607 NP 27.2 59.7
3 16,413 VP 13.3 72.9
4 12,441 PP 10.1 83.0
5 8,350 SBAR 6.7 89.7
6 4,085 ADVP 3.3 93.0
7 3,080 QP 2.5 95.5
8 2,480 SINV 2.0 97.5
9 1,257 ADJP 1.0 98.5
10 369 PRN 0.3 98.8
WSJ +1,446 more with Count ? 356 1.2%
Table 2: Top 99% of the lowest dominating non-terminals
deriving complete inter-punctuation fragments in WSJ.
2 Definitions, Analyses and Constraints
Punctuation and syntax are related (Nunberg, 1990;
Briscoe, 1994; Jones, 1994; Doran, 1998, inter alia).
But are there simple enough connections between
the two to aid in grammar induction? This section
explores the regularities. Our study of punctuation
in WSJ (Marcus et al, 1993) parallels Spitkovsky
et al?s (2010b, ?5) analysis of markup from a web-
log, since their proposed constraints turn out to be
useful. Throughout, we define an inter-punctuation
fragment as a maximal (non-empty) consecutive se-
quence of words that does not cross punctuation
boundaries and is shorter than its source sentence.
2.1 A Linguistic Analysis
Out of 51,558 sentences, most ? 37,076 (71.9%) ?
contain sentence-internal punctuation. These punc-
tuated sentences contain 123,751 fragments, nearly
all ? 111,774 (90.3%) ? of them multi-token.
Common part-of-speech (POS) sequences compris-
ing fragments are diverse (note also their flat distri-
bution ? see Table 1). The plurality of fragments
are dominated by a clause, but most are dominated
by one of several kinds of phrases (see Table 2).
As expected, punctuation does not occur at all con-
stituent boundaries: Of the top 15 productions that
yield fragments, five do not match the exact brack-
eting of their lowest dominating non-terminal (see
ranks 6, 11, 12, 14 and 15 in Table 3, left). Four of
them miss a left-adjacent clause, e.g., S? S NP VP:
[S [S It?s an overwhelming job], [NP she] [VP says.]]
This production is flagged because the fragment
NP VP is not a constituent ? it is two; still, 49.4%
of all fragments do align with whole constituents.
Inter-punctuation fragments correspond more
strongly to dependencies (see Table 3, right). Only
one production (rank 14) shows a daughter outside
her mother?s fragment. Some number of such pro-
ductions is inevitable and expected, since fragments
must coalesce (i.e., the root of at least one fragment
? in every sentence with sentence-internal punc-
tuation ? must be attached by some word from a
different, external fragment). We find it noteworthy
that in 14 of the 15 most common cases, a word in
an inter-punctuation fragment derives precisely the
rest of that fragment, attaching none of the other,
external words. This is true for 39.2% of all frag-
ments, and if we include fragments whose heads at-
tach other fragments? heads, agreement increases to
74.0% (see strict and loose constraints in ?2.2, next).
2.2 Five Parsing Constraints
Spitkovsky et al (2010b, ?5.3) showed how to ex-
press similar correspondences with markup as pars-
ing constraints. They proposed four constraints but
employed only the strictest three, omitting imple-
mentation details. We revisit their constraints, speci-
fying precise logical formulations that we use in our
code, and introduce a fifth (most relaxed) constraint.
Let [x, y] be a fragment (or markup) spanning po-
sitions x through y (inclusive, with 1 ? x < y ? l), in
a sentence of length l. And let [i, j]h be a sealed span
headed by h (1 ? i ? h ? j ? l), i.e., the word at po-
sition h dominates precisely i . . . j (but none other):
i h j
20
Count Constituent Production Frac Cum
1 7,115 PP? IN NP 5.7%
2 5,950 S? NP VP 4.8 10.6
3 3,450 NP? NP PP 2.8 13.3
4 2,799 SBAR? WHNP S 2.3 15.6
5 2,695 NP? NNP 2.2 17.8
6 2,615 S? S NP VP 2.1 19.9
7 2,480 SBAR? IN S 2.0 21.9
8 2,392 NP? NNP NNP 1.9 23.8
9 2,354 ADVP? RB 1.9 25.7
10 2,334 QP? CD CD 1.9 27.6
11 2,213 S? PP NP VP 1.8 29.4
12 1,441 S? S CC S 1.2 30.6
13 1,317 NP? NP NP 1.1 31.6
14 1,314 S? SBAR NP VP 1.1 32.7
15 1,172 SINV? S VP NP NP 0.9 33.6
WSJ +82,110 more with Count ? 976 66.4%
Count Head-Outward Spawn Frac Cum
1 11,928 IN 9.6%
2 8,852 NN 7.2 16.8
3 7,802 NNP 6.3 23.1
4 4,750 CD 3.8 26.9
5 3,914 VBD 3.2 30.1
6 3,672 VBZ 3.0 33.1
7 3,436 RB 2.8 35.8
8 2,691 VBG 2.2 38.0
9 2,304 VBP 1.9 39.9
10 2,251 NNS 1.8 41.7
11 1,955 WDT 1.6 43.3
12 1,409 MD 1.1 44.4
13 1,377 VBN 1.1 45.5
14 1,204 IN VBD 1.0 46.5
15 927 JJ 0.7 47.3
WSJ +65,279 more with Count ? 846 52.8%
Table 3: Top 15 productions yielding punctuation-induced fragments in WSJ, viewed as constituents (left) and as de-
pendencies (right). For constituents, we recursively expanded any internal nodes that did not align with the associated
fragmentation (underlined). For dependencies we dropped all daughters that fell entirely in the same region as their
mother (i.e., both inside a fragment, both to its left or both to its right), keeping only crossing attachments (just one).
Define inside(h, x, y) as true iff x ? h ? y; and let
cross(i, j, x, y) be true iff (i < x ? j ? x ? j < y) ?
(i > x ? i ? y ? j > y). The three tightest constraints
impose conditions which, when satisfied, disallow
sealing [i, j]h in the presence of an annotation [x, y]:
strict ? requires [x, y] itself to be sealed in the
parse tree, voiding all seals that straddle exactly one
of {x, y} or protrude beyond [x, y] if their head is in-
side. This constraint holds for 39.2% of fragments.
By contrast, only 35.6% of HTML annotations, such
as anchor texts and italics, agree with it (Spitkovsky
et al, 2010b). This necessarily fails in every sen-
tence with internal punctuation (since there, some
fragment must take charge and attach another), when
cross(i, j, x, y) ? (inside(h, x, y) ? (i < x ? j > y)).
... the British daily newspaper, The Financial Times .
x = i h = j = y
loose ? if h ? [x, y], requires that everything in
x . . . y fall under h, with only h allowed external at-
tachments. This holds for 74.0% of fragments ?
87.5% of markup, failing when cross(i, j, x, y).
... arrests followed a ? Snake Day ? at Utrecht ...
i x h = j = y
sprawl ? still requires that h derive x . . . y but
lifts restrictions on external attachments. Holding
for 92.9% of fragments (95.1% of markup), it fails
when cross(i, j, x, y) ? ?inside(h, x, y).
Maryland Club also distributes tea , which ...
x = i h y j
These three strictest constraints lend themselves to a
straight-forward implementation as an O(l5) chart-
based decoder. Ordinarily, the probability of [i, j]h
is computed by multiplying the probability of the as-
sociated unsealed span by two stopping probabilities
? that of the word at h on the left (adjacent if i = h;
non-adjacent if i < h) and on the right (adjacent if
h = j; non-adjacent if h < j). To impose a con-
straint, we ran through all of the annotations [x, y]
associated with a sentence and zeroed out this prob-
ability if any of them satisfied disallowed conditions.
There are faster ? e.g., O(l4), and even O(l3) ?
recognizers for split head automaton grammars (Eis-
ner and Satta, 1999). Perhaps a more practical, but
still clear, approach would be to generate n-best lists
using a more efficient unconstrained algorithm, then
apply the constraints as a post-filtering step.
Relaxed constraints disallow joining adjacent
subtrees, e.g., preventing the seal [i, j]h from merg-
ing below the unsealed span [j +1, J ]H , on the left:
i h j j + 1 H J
21
tear ? prevents x . . . y from being torn apart by
external heads from opposite sides. It holds for
94.7% of fragments (97.9% of markup), and is vi-
olated when (x ? j ? y > j ? h < x), in this case.
... they ?were not consulted about the [Ridley decision]
in advance and were surprised at the action taken .
thread ? requires only that no path from the root
to a leaf enter [x, y] twice. This holds for 95.0% of
all fragments (98.5% of markup); it is violated when
(x ? j ? y > j ? h < x) ? (H ? y), again, in this
case. Example that satisfies thread but violates tear:
The ... changes ?all make a lot of sense to me,? he added.
The case when [i, j]h is to the right is entirely sym-
metric, and these constraints could be incorporated
in a more sophisticated decoder (since i and J do
not appear in the formulae, above). We implemented
them by zeroing out the probability of the word at H
attaching that at h (to its left), in case of a violation.
Note that all five constraints are nested. In partic-
ular, this means that it does not make sense to com-
bine them, for a given annotation [x, y], since the re-
sult would just match the strictest one. Our markup
number for tear is lower (97.9 versus 98.9%) than
Spitkovsky et al?s (2010b), because theirs allowed
cases where markup was neither torn nor threaded.
Common structures that violate thread (and, con-
sequently, all five of the constraints) include, e.g.,
?seamless? quotations and even ordinary lists:
Her recent report classifies the stock as a ?hold.?
The company said its directors, management and
subsidiaries will remain long-term investors and ...
2.3 Comparison with Markup
Most punctuation-induced constraints are less ac-
curate than the corresponding markup-induced con-
straints (e.g., sprawl: 92.9 vs. 95.1%; loose: 74.0
vs. 87.5%; but not strict: 39.2 vs. 35.6%). However,
markup is rare: Spitkovsky et al (2010b, ?5.1) ob-
served that only 10% of the sentences in their blog
were annotated; in contrast, over 70% of the sen-
tences in WSJ are fragmented by punctuation.
Fragments are more than 40% likely to be dom-
inated by a clause; for markup, this number is be-
low 10% ? nearly 75% of it covered by noun
phrases. Further, inter-punctuation fragments are
spread more evenly under noun, verb, prepositional,
adverbial and adjectival phrases (approximately
27:13:10:3:1 versus 75:13:2:1:1) than markup.1
3 The Model, Methods and Metrics
We model grammar via Klein and Manning?s (2004)
Dependency Model with Valence (DMV), which
ordinarily strips out punctuation. Since this step
already requires identification of marks, our tech-
niques are just as ?unsupervised.? We would have
preferred to test punctuation in their original set-up,
but this approach wasn?t optimal, for several rea-
sons. First, Klein and Manning (2004) trained with
short sentences (up to only ten words, on WSJ10),
whereas most punctuation appears in longer sen-
tences. And second, although we could augment
the training data (say, to WSJ45), Spitkovsky et
al. (2010a) showed that classic EM struggles with
longer sentences. For this reason, we use Viterbi
EM and the scaffolding suggested by Spitkovsky et
al. (2010a) ? also the setting in which Spitkovsky et
al. (2010b) tested their markup-induced constraints.
3.1 A Basic System
Our system is based on Laplace-smoothed Viterbi
EM, following Spitkovsky et al?s (2010a) two-stage
scaffolding: the first stage trains with just the sen-
tences up to length 15; the second stage then retrains
on nearly all sentences ? those with up to 45 words.
Initialization
Klein and Manning?s (2004) ?ad-hoc harmonic? ini-
tializer does not work very well for longer sentences,
particularly with Viterbi training (Spitkovsky et al,
2010a, Figure 3). Instead, we use an improved ini-
tializer that approximates the attachment probability
between two words as an average, over all sentences,
of their normalized aggregate weighted distances.
Our weighting function is w(d) = 1+1/ lg(1+d).2
Termination
Spitkovsky et al (2010a) iterated until successive
changes in overall (best parse) per-token cross-
entropy dropped below 2?20 bits. Since smoothing
can (and does, at times) increase the objective, we
found it more efficient to terminate early, after ten
1Markup and fragments are as likely to be in verb phrases.
2Integer d ? 1 is a distance between two tokens; lg is log2.
22
steps of suboptimal models. We used the lowest-
perplexity (not necessarily the last) model found, as
measured by the cross-entropy of the training data.
Constrained Training
Training with punctuation replaces ordinary Viterbi
parse trees, at every iteration of EM, with the out-
put of a constrained decoder. In all experiments
other than #2 (?5) we train with the loose constraint.
Spitkovsky et al (2010b) found this setting to be
best for markup-induced constraints. We apply it to
constraints induced by inter-punctuation fragments.
Constrained Inference
Spitkovsky et al (2010b) recommended using the
sprawl constraint in inference. Once again, we fol-
low their advice in all experiments except #2 (?5).
3.2 Data Sets and Scoring
We trained on the Penn English Treebank?s Wall
Street Journal portion (Marcus et al, 1993). To eval-
uate, we automatically converted its labeled con-
stituents into unlabeled dependencies, using deter-
ministic ?head-percolation? rules (Collins, 1999),
discarding punctuation, any empty nodes, etc., as is
standard practice (Paskin, 2001; Klein and Manning,
2004). We also evaluated against the parsed portion
of the Brown corpus (Francis and Kuc?era, 1979),
used as a blind, out-of-domain evaluation set,3 sim-
ilarly derived from labeled constituent parse trees.
We report directed accuracies ? fractions of cor-
rectly guessed arcs, including the root, in unlabeled
reference dependency parse trees, as is also standard
practice (Paskin, 2001; Klein and Manning, 2004).
One of our baseline systems (?3.3) produces depen-
dency trees containing punctuation. In this case we
do not score the heads assigned to punctuation and
use forgiving scoring for regular words: crediting
correct heads separated from their children by punc-
tuation alone (from the point of view of the child,
looking up to the nearest non-punctuation ancestor).
3.3 Baseline Systems
Our primary baseline is the basic system without
constraints (standard training). It ignores punctu-
ation, as is standard, scoring 52.0% against WSJ45.
A secondary (punctuation as words) baseline in-
3Note that WSJ{15, 45} overlap with Section 23 ? training
on the test set is standard practice in unsupervised learning.
corporates punctuation into the grammar as if it were
words, as in supervised dependency parsing (Nivre
et al, 2007b; Lin, 1998; Sleator and Temperley,
1993, inter alia). It is worse, scoring only 41.0%.4,5
4 Experiment #1: Default Constraints
Our first experiment compares ?punctuation as con-
straints? to the baseline systems. We use default set-
tings, as recommended by Spitkovsky et al (2010b):
loose in training; and sprawl in inference. Evalua-
tion is on Section 23 of WSJ (all sentence lengths).
To facilitate comparison with prior work, we also re-
port accuracies against shorter sentences, with up to
ten non-punctuation tokens (WSJ10 ? see Table 4).
We find that both constrained regimes improve
performance. Constrained decoding alone increases
the accuracy of a standardly-trained system from
52.0% to 54.0%. And constrained training yields
55.6% ? 57.4% in combination with inference.
4We were careful to use exactly the same data sets in both
cases, not counting punctuation towards sentence lengths. And
we used forgiving scoring (?3.2) when evaluating these trees.
5To get this particular number we forced punctuation to be
tacked on, as a layer below the tree of words, to fairly compare
systems (using the same initializer). Since improved initializa-
tion strategies ? both ours and Klein and Manning?s (2004)
?ad-hoc harmonic? initializer ? rely on distances between to-
kens, they could be unfairly biased towards one approach or the
other, if punctuation counted towards length. We also trained
similar baselines without restrictions, allowing punctuation to
appear anywhere in the tree (still with forgiving scoring ? see
?3.2), using the uninformed uniform initializer (Spitkovsky et
al., 2010a). Disallowing punctuation as a parent of a real word
made things worse, suggesting that not all marks belong near
the leaves (sentence stops, semicolons, colons, etc. make more
sense as roots and heads). We tried the weighted initializer also
without restrictions and repeated all experiments without scaf-
folding, on WSJ15 and WSJ45 alone, but treating punctuation
as words never came within even 5% of (comparable) standard
training. Punctuation, as words, reliably disrupted learning.
WSJ? WSJ10
Supervised DMV 69.8 83.6
w/Constrained Inference 73.0 84.3
Punctuation as Words 41.7 54.8
Standard Training 52.0 63.2
w/Constrained Inference 54.0 63.6
Constrained Training 55.6 67.0
w/Constrained Inference 57.4 67.5
Table 4: Directed accuracies on Section 23 of WSJ? and
WSJ10 for the supervised DMV, our baseline systems and
the punctuation runs (all using the weighted initializer).
23
These are multi-point increases, but they could dis-
appear in a more accurate state-of-the-art system.
To test this hypothesis, we applied constrained de-
coding to a supervised system. We found that this
(ideal) instantiation of the DMV benefits as much or
more than the unsupervised systems: accuracy in-
creases from 69.8% to 73.0%. Punctuation seems
to capture the kinds of, perhaps long-distance, regu-
larities that are not accessible to the model, possibly
because of its unrealistic independence assumptions.
5 Experiment #2: Optimal Settings
Spitkovsky et al (2010b) recommended training
with loose and decoding with sprawl based on their
experiments with markup. But are these the right
settings for punctuation? Inter-punctuation frag-
ments are quite different from markup ? they are
more prevalent but less accurate. Furthermore, we
introduced a new constraint, thread, that Spitkovsky
et al (2010b) had not considered (along with tear).
We next re-examined the choices of constraints.
Our full factorial analysis was similar, but signifi-
cantly smaller, than Spitkovsky et al?s (2010b): we
excluded their larger-scale news and web data sets
that are not publicly available. Nevertheless, we
still tried every meaningful combination of settings,
testing both thread and tear (instead of strict, since
it can?t work with sentences containing sentence-
internal punctuation), in both training and inference.
We did not find better settings than loose for train-
ing, and sprawl for decoding, among our options.
A full analysis is omitted due to space constraints.
Our first observation is that constrained inference,
using punctuation, is helpful and robust. It boosted
accuracy (on WSJ45) by approximately 1.5%, on
average, with all settings. Indeed, sprawl was con-
sistently (but only slightly, at 1.6%, on average) bet-
ter than the rest. Second, constrained training hurt
more often than it helped. It degraded accuracy in all
but one case, loose, where it gained approximately
0.4%, on average. Both improvements are statisti-
cally significant: p ? 0.036 for training with loose;
and p ? 5.6? 10?12 for decoding with sprawl.
6 More Advanced Methods
So far, punctuation has improved grammar induction
in a toy setting. But would it help a modern system?
Our next two experiments employ a slightly more
complicated set-up, compared with the one used up
until now (?3.1). The key difference is that this sys-
tem is lexicalized, as is standard among the more ac-
curate grammar inducers (Blunsom and Cohn, 2010;
Gillenwater et al, 2010; Headden et al, 2009).
Lexicalization
We lexicalize only in the second (full data) stage, us-
ing the method of Headden et al (2009). For words
seen at least 100 times in the training corpus, we
augment their gold POS tag with the lexical item.
The first (data poor) stage remains entirely unlexi-
calized, with gold POS tags for word classes, as in
the earlier systems (Klein and Manning, 2004).
Smoothing
We do not use smoothing in the second stage except
at the end, for the final lexicalized model. Stage one
still applies ?add-one? smoothing at every iteration.
7 Experiment #3: State-of-the-Art
The purpose of these experiments is to compare the
punctuation-enhanced DMV with other, recent state-
of-the-art systems. We find that, lexicalized (?6), our
approach performs better, by a wide margin; without
lexicalization (?3.1), it was already better for longer,
but not for shorter, sentences (see Tables 5 and 4).
We trained a variant of our system without gold
part-of-speech tags, using the unsupervised word
clusters (Clark, 2000) computed by Finkel and Man-
ning (2009).6 Accuracy decreased slightly, to 58.2%
on Section 23 of WSJ (down only 0.2%). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).
6Available from http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
Brown WSJ? WSJ10
(Headden et al, 2009) ? ? 68.8
(Spitkovsky et al, 2010b) 53.3 50.4 69.3
(Gillenwater et al, 2010) ? 53.3 64.3
(Blunsom and Cohn, 2010) ? 55.7 67.7
Constrained Training 58.4 58.0 69.3
w/Constrained Inference 59.5 58.4 69.5
Table 5: Accuracies on the out-of-domain Brown100 set
and Section 23 of WSJ? and WSJ10, for the lexicalized
punctuation run and other recent state-of-the-art systems.
24
Unlexicalized, Unpunctuated Lexicalized ... and Punctuated
CoNLL Year Initialization @15 Training @15 Retraining @45 Retraining @45 Net
& Language 1. w/Inference 2. w/Inference 3. w/Inference 3?. w/Inference Gain
Arabic 2006 23.3 23.6 (+0.3) 32.8 33.1 (+0.4) 31.5 31.6 (+0.1) 32.1 32.6 (+0.5) +1.1
?7 25.6 26.4 (+0.8) 33.7 34.2 (+0.5) 32.7 33.6 (+0.9) 34.9 35.3 (+0.4) +2.6
Basque ?7 19.3 20.8 (+1.5) 29.9 30.9 (+1.0) 29.3 30.1 (+0.8) 29.3 29.9 (+0.6) +0.6
Bulgarian ?6 23.7 24.7 (+1.0) 39.3 40.7 (+1.4) 38.8 39.9 (+1.1) 39.9 40.5 (+0.6) +1.6
Catalan ?7 33.2 34.1 (+0.8) 54.8 55.5 (+0.7) 54.3 55.1 (+0.8) 54.3 55.2 (+0.9) +0.9
Czech ?6 18.6 19.6 (+1.0) 34.6 35.8 (+1.2) 34.8 35.7 (+0.9) 37.0 37.8 (+0.8) +3.0
?7 17.6 18.4 (+0.8) 33.5 35.4 (+1.9) 33.4 34.4 (+1.0) 35.2 36.2 (+1.0) +2.7
Danish ?6 22.9 24.0 (+1.1) 35.6 36.7 (+1.2) 36.9 37.8 (+0.9) 36.5 37.1 (+0.6) +0.2
Dutch ?6 15.8 16.5 (+0.7) 11.2 12.5 (+1.3) 11.0 11.9 (+1.0) 13.7 14.0 (+0.3) +3.0
English ?7 25.0 25.4 (+0.5) 47.2 49.5 (+2.3) 47.5 48.8 (+1.3) 49.3 50.3 (+0.9) +2.8
German ?6 19.2 19.6 (+0.4) 27.4 28.0 (+0.7) 27.0 27.8 (+0.8) 28.2 28.6 (+0.4) +1.6
Greek ?7 18.5 18.8 (+0.3) 20.7 21.4 (+0.7) 20.5 21.0 (+0.5) 20.9 21.2 (+0.3) +0.7
Hungarian ?7 17.4 17.7 (+0.3) 6.7 7.2 (+0.5) 6.6 7.0 (+0.4) 7.8 8.0 (+0.2) +1.4
Italian ?7 25.0 26.3 (+1.2) 29.6 29.9 (+0.3) 29.7 29.7 (+0.1) 28.3 28.8 (+0.5) -0.8
Japanese ?6 30.0 30.0 (+0.0) 27.3 27.3 (+0.0) 27.4 27.4 (+0.0) 27.5 27.5 (+0.0) +0.1
Portuguese ?6 27.3 27.5 (+0.2) 32.8 33.7 (+0.9) 32.7 33.4 (+0.7) 33.3 33.5 (+0.3) +0.8
Slovenian ?6 21.8 21.9 (+0.2) 28.3 30.4 (+2.1) 28.4 30.4 (+2.0) 29.8 31.2 (+1.4) +2.8
Spanish ?6 25.3 26.2 (+0.9) 31.7 32.4 (+0.7) 31.6 32.3 (+0.8) 31.9 32.3 (+0.5) +0.8
Swedish ?6 31.0 31.5 (+0.6) 44.1 45.2 (+1.1) 45.6 46.1 (+0.5) 46.1 46.4 (+0.3) +0.8
Turkish ?6 22.3 22.9 (+0.6) 39.1 39.5 (+0.4) 39.9 39.9 (+0.1) 40.6 40.9 (+0.3) +1.0
?7 22.7 23.3 (+0.6) 41.7 42.3 (+0.6) 41.9 42.1 (+0.2) 41.6 42.0 (+0.4) +0.1
Average: 23.4 24.0 (+0.7) 31.9 32.9 (+1.0) 31.9 32.6 (+0.7) 32.6 33.2 (+0.5) +1.3
Table 6: Multi-lingual evaluation for CoNLL sets, measured at all three stages of training, with and without constraints.
8 Experiment #4: Multi-Lingual Testing
This final batch of experiments probes the general-
ization of our approach (?6) across languages. The
data are from 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al, 2007a), where
punctuation was identified by the organizers, who
also furnished disjoint train/test splits. We tested
against all sentences in their evaluation sets.7,8
The gains are not English-specific (see Table 6).
Every language improves with constrained decod-
ing (more so without constrained training); and all
but Italian benefit in combination. Averaged across
all eighteen languages, the net change in accuracy is
1.3%. After standard training, constrained decoding
alone delivers a 0.7% gain, on average, never caus-
ing harm in any of our experiments. These gains are
statistically significant: p ? 1.59 ? 10?5 for con-
strained training; and p ? 4.27?10?7 for inference.
7With the exception of Arabic ?07, from which we discarded
one sentence with 145 tokens. We down-weighed languages
appearing in both years by 50% in our analyses, and excluded
Chinese entirely, since it had already been cut up at punctuation.
8Note that punctuation was treated differently in the two
years: in ?06, it was always at the leaves of the dependency
trees; in ?07, it matched original annotations of the source tree-
banks. For both, we used punctuation-insensitive scoring (?3.2).
We did not detect synergy between the two im-
provements. However, note that without constrained
training, ?full? data sets do not help, on average, de-
spite having more data and lexicalization. Further-
more, after constrained training, we detected no ev-
idence of benefits to additional retraining: not with
the relaxed sprawl constraint, nor unconstrained.
9 Related Work
Punctuation has been used to improve parsing since
rule-based systems (Jones, 1994). Statistical parsers
reap dramatic gains from punctuation (Engel et al,
2002; Roark, 2001; Charniak, 2000; Johnson, 1998;
Collins, 1997, inter alia). And it is even known to
help in unsupervised constituent parsing (Seginer,
2007). But for dependency grammar induction, until
now, punctuation remained unexploited.
Parsing Techniques Most-Similar to Constraints
A ?divide-and-rule? strategy that relies on punctua-
tion has been used in supervised constituent parsing
of long Chinese sentences (Li et al, 2005). For En-
glish, there has been interest in balanced punctua-
tion (Briscoe, 1994), more recently using rule-based
filters (White and Rajkumar, 2008) in a combinatory
categorial grammar (CCG). Our focus is specifically
25
on unsupervised learning of dependency grammars
and is similar, in spirit, to Eisner and Smith?s (2005)
?vine grammar? formalism. An important difference
is that instead of imposing static limits on allowed
dependency lengths, our restrictions are dynamic ?
they disallow some long (and some short) arcs that
would have otherwise crossed nearby punctuation.
Incorporating partial bracketings into grammar
induction is an idea tracing back to Pereira and Sch-
abes (1992). It inspired Spitkovsky et al (2010b) to
mine parsing constraints from the web. In that same
vein, we prospected a more abundant and natural
language-resource ? punctuation, using constraint-
based techniques they developed for web markup.
Modern Unsupervised Dependency Parsing
State-of-the-art in unsupervised dependency pars-
ing (Blunsom and Cohn, 2010) uses tree substitu-
tion grammars. These are powerful models, capa-
ble of learning large dependency fragments. To help
prevent overfitting, a non-parametric Bayesian prior,
defined by a hierarchical Pitman-Yor process (Pit-
man and Yor, 1997), is trusted to nudge training to-
wards fewer and smaller grammatical productions.
We pursued a complementary strategy: using
Klein and Manning?s (2004) much simpler Depen-
dency Model with Valence (DMV), but persistently
steering training away from certain constructions, as
guided by punctuation, to help prevent underfitting.
Various Other Uses of Punctuation in NLP
Punctuation is hard to predict,9 partly because it
can signal long-range dependences (Lu and Ng,
2010). It often provides valuable cues to NLP tasks
such as part-of-speech tagging and named-entity
recognition (Hillard et al, 2006), information ex-
traction (Favre et al, 2008) and machine transla-
tion (Lee et al, 2006; Matusov et al, 2006). Other
applications have included Japanese sentence anal-
ysis (Ohyama et al, 1986), genre detection (Sta-
matatos et al, 2000), bilingual sentence align-
ment (Yeh, 2003), semantic role labeling (Pradhan et
al., 2005), Chinese creation-title recognition (Chen
and Chen, 2005) and word segmentation (Li and
Sun, 2009), plus, recently, automatic vandalism de-
9Punctuation has high semantic entropy (Melamed, 1997);
for an analysis of the many roles played in the WSJ by the
comma ? the most frequent and unpredictable punctuation
mark in that data set ? see Beeferman et al (1998, Table 2).
tection in Wikipedia (Wang and McKeown, 2010).
10 Conclusions and Future Work
Punctuation improves dependency grammar induc-
tion. Many unsupervised (and supervised) parsers
could be easily modified to use sprawl-constrained
decoding in inference. It applies to pre-trained mod-
els and, so far, helped every data set and language.
Tightly interwoven into the fabric of writing sys-
tems, punctuation frames most unannotated plain-
text. We showed that rules for converting markup
into accurate parsing constraints are still optimal for
inter-punctuation fragments. Punctuation marks are
more ubiquitous and natural than web markup: what
little punctuation-induced constraints lack in preci-
sion, they more than make up in recall ? perhaps
both types of constraints would work better yet in
tandem. For language acquisition, a natural ques-
tion is whether prosody could similarly aid grammar
induction from speech (Kahn et al, 2005).
Our results underscore the power of simple mod-
els and algorithms, combined with common-sense
constraints. They reinforce insights from joint mod-
eling in supervised learning, where simplified, in-
dependent models, Viterbi decoding and expressive
constraints excel at sequence labeling tasks (Roth
and Yih, 2005). Such evidence is particularly wel-
come in unsupervised settings (Punyakanok et al,
2005), where it is crucial that systems scale grace-
fully to volumes of data, on top of the usual desider-
ata ? ease of implementation, extension, under-
standing and debugging. Future work could explore
softening constraints (Hayes and Mouradian, 1980;
Chang et al, 2007), perhaps using features (Eisner
and Smith, 2005; Berg-Kirkpatrick et al, 2010) or
by learning to associate different settings with var-
ious marks: Simply adding a hidden tag for ?ordi-
nary? versus ?divide? types of punctuation (Li et al,
2005) may already usefully extend our model.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Slav Petrov and
anonymous reviewers for many helpful suggestions, and we are
especially grateful to Jenny R. Finkel for shaming us into using
punctuation, to Christopher D. Manning for reminding us to ex-
plore ?punctuation as words? baselines, and to Noah A. Smith
for encouraging us to test against languages other than English.
26
References
D. Beeferman, A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In ICASSP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc.
Technical report, Xerox European Research Labora-
tory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL.
C. Chen and H.-H. Chen. 2005. Integrating punctuation
rules and na??ve Bayesian model for Chinese creation
title recognition. In IJCNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
C. D. Doran. 1998. Incorporating Punctuation into
the Sentence Grammar: A Lexicalized Tree Adjoin-
ing Grammar Perspective. Ph.D. thesis, University of
Pennsylvania.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In IWPT.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In EMNLP.
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tu?r, and M. Ostendorf. 2008. Punctuating speech for
information extraction. In ICASSP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kuc?era, 1979. Manual of Informa-
tion to Accompany a Standard Corpus of Present-Day
Edited American English, for use with Digital Com-
puters. Department of Linguistics, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In ICASSP.
P. J. Hayes and G. V. Mouradian. 1980. Flexible parsing.
In ACL.
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tu?r, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of automatic comma prediction on POS/name
tagging of speech. In IEEE/ACL: SLT.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24.
B. E. M. Jones. 1994. Exploring the role of punctuation
in parsing natural text. COLING.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in pars-
ing conversational speech. In HLT-EMNLP.
J.-H. Kim and P. C. Woodland. 2002. Implementation of
automatic capitalisation generation systems for speech
input. In ICASSP.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
Y.-S. Lee, S. Roukos, Y. Al-Onaizan, and K. Papineni.
2006. IBM spoken language translation system. In
TC-STAR: Speech-to-Speech Translation.
Z. Li and M. Sun. 2009. Punctuation as implicit annota-
tions for Chinese word segmentation. Computational
Linguistics, 35.
X. Li, C. Zong, and R. Hu. 2005. A hierarchical parsing
approach with punctuation processing for long Chi-
nese sentences. In IJCNLP.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Evaluation of Parsing Systems.
W. Lu and H. T. Ng. 2010. Better punctuation prediction
with dynamic conditional random fields. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic
sentence segmentation and punctuation prediction for
spoken language translation. In IWSLT.
I. D. Melamed. 1997. Measuring semantic entropy. In
ACL-SIGLEX: Tagging Text with Lexical Semantics.
27
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007a. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13.
G. Nunberg. 1990. The Linguistics of Punctuation.
CSLI Publications.
Y. Ohyama, T. Fukushima, T. Shutoh, and M. Shutoh.
1986. A sentence analysis method for a Japanese book
reading machine for the blind. In ACL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic role chunking combin-
ing complementary syntactic views. In CoNLL.
V. Punyakanok, D. Roth, W.-t. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
B. E. Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
D. Roth and W.-t. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a link grammar. In IWPT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2000.
Text genre detection using common word frequencies.
In COLING.
W. Y. Wang and K. R. McKeown. 2010. ?Got you!?: Au-
tomatic vandalism detection in Wikipedia with web-
based shallow syntactic-semantic modeling. In COL-
ING.
M. White and R. Rajkumar. 2008. A more precise analy-
sis of punctuation for broad-coverage surface realiza-
tion with CCG. In GEAF.
K. C. Yeh. 2003. Bilingual sentence alignment based on
punctuation marks. In ROCLING: Student.
28
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 16?22,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Capitalization Cues Improve Dependency Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We show that orthographic cues can be helpful
for unsupervised parsing. In the Penn Tree-
bank, transitions between upper- and lower-
case tokens tend to align with the boundaries
of base (English) noun phrases. Such signals
can be used as partial bracketing constraints to
train a grammar inducer: in our experiments,
directed dependency accuracy increased by
2.2% (average over 14 languages having case
information). Combining capitalization with
punctuation-induced constraints in inference
further improved parsing performance, attain-
ing state-of-the-art levels for many languages.
1 Introduction
Dependency grammar induction and related prob-
lems of unsupervised syntactic structure discovery
are attracting increasing attention (Rasooli and Faili,
2012; Marec?ek and Zabokrtsky?, 2011, inter alia).
Since sentence structure is underdetermined by raw
text, there have been efforts to simplify the task, via
(i) pooling features of syntax across languages (Co-
hen et al, 2011; McDonald et al, 2011; Cohen
and Smith, 2009); as well as (ii) identifying uni-
versal rules (Naseem et al, 2010) ? such as verbo-
centricity (Gimpel and Smith, 2011) ? that need not
be learned at all. Unfortunately most of these tech-
niques do not apply to plain text, because they re-
quire knowing, for example, which words are verbs.
As standard practice shifts away from relying on
gold part-of-speech (POS) tags (Seginer, 2007; Pon-
vert et al, 2010; S?gaard, 2011b; Spitkovsky et al,
2011c, inter alia), lighter cues to inducing linguistic
structure become more important. Examples of use-
ful POS-agnostic clues include punctuation bound-
aries (Ponvert et al, 2011; Spitkovsky et al, 2011b;
Briscoe, 1994) and various kinds of bracketing con-
straints (Naseem and Barzilay, 2011; Spitkovsky et
al., 2010b; Pereira and Schabes, 1992). We propose
adding capitalization to this growing list of sources
of partial bracketings. Our intuition stems from En-
glish, where (maximal) spans of capitalized words
? such as Apple II, World War I, Mayor William H.
Hudnut III, International Business Machines Corp. and
Alexandria, Va ? tend to demarcate proper nouns.
Consider a motivating example (all of our exam-
ples are from WSJ) without punctuation, in which all
(eight) capitalized word clumps and uncased numer-
als match base noun phrase constituent boundaries:
[NP Jay Stevens] of [NP Dean Witter] actually cut his
per-share earnings estimate to [NP $9] from [NP $9.50]
for [NP 1989] and to [NP $9.50] from [NP $10.35]
in [NP 1990] because he decided sales would be even
weaker than he had expected.
and another (whose first word happens to be a leaf),
where capitalization complements punctuation cues:
[NP Jurors] in [NP U.S. District Court] in [NP Miami]
cleared [NP Harold Hershhenson], a former executive
vice president; [NP John Pagones], a former vice presi-
dent; and [NP Stephen Vadas] and [NP Dean Ciporkin],
who had been engineers with [NP Cordis].
Could such chunks help bootstrap grammar induc-
tion and/or improve the accuracy of already-trained
unsupervised parsers? In answering these questions,
we will focus predominantly on sentence-internal
capitalization. But we will also show that first words
? those capitalized by convention ? and uncased
segments ? whose characters are not even drawn
from an alphabet ? could play a useful role as well.
2 English Capitalization from a Treebank
We began our study by consulting the 51,558 parsed
sentences of the WSJ corpus (Marcus et al, 1993):
30,691 (59.5%) of them contain non-trivially capi-
talized fragments ? maximal (non-empty and not
16
Count POS Sequence Frac Cum
1 27,524 NNP 44.6%
2 17,222 NNP NNP 27.9 72.5
3 4,598 NNP NNP NNP 7.5 79.9
4 2,973 JJ 4.8 84.8
5 1,716 NNP NNP NNP NNP 2.8 87.5
6 1,037 NN 1.7 89.2
7 932 PRP 1.5 90.7
8 846 NNPS 1.4 92.1
9 604 NNP NNPS 1.0 93.1
10 526 NNP NNP NNP NNP NNP 0.9 93.9
WSJ +3,753 more with Count ? 498 6.1%
Table 1: Top 10 fragments of POS tag sequences in WSJ.
sentence-initial) consecutive sequences of words
that each differs from its own lower-cased form.
Nearly all ? 59,388 (96.2%) ? of the 61,731 frag-
ments are dominated by noun phrases; slightly less
than half ? 27,005 (43.8%) ? perfectly align with
constituent boundaries in the treebank; and about as
many ? 27,230 (44.1%) are multi-token. Table 1
shows the top POS sequences comprising fragments.
3 Analytical Experiments with Gold Trees
We gauged the suitability of capitalization-induced
fragments for guiding dependency grammar induc-
tion by assessing accuracy, in WSJ,1 of parsing con-
straints derived from their end-points. Following the
suite of increasingly-restrictive constraints on how
dependencies may interact with fragments, intro-
duced by Spitkovsky et al (2011b, ?2.2), we tested
several such heuristics. The most lenient constraint,
thread, only asks that no dependency path from the
root to a leaf enter the fragment twice; tear requires
any incoming arcs to come from the same side of
the fragment; sprawl demands that there be exactly
one incoming arc; loose further constrains any out-
going arcs to be from the fragment?s head; and strict
? the most stringent constraint ? bans external
dependents. Since only strict is binding for single
words, we experimented also with strict?: applying
strict solely to multi-token fragments (ignoring sin-
gletons). In sum, we explored six ways in which
dependency parse trees can be constrained by frag-
ments whose end-points could be defined by capital-
ization (or in other various ways, e.g., semantic an-
1We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
1999), discarding any empty nodes, etc., as is standard practice.
markup punct. capital initial uncased
thread 98.5 95.0 99.5 98.4 99.2
tear 97.9 94.7 98.6 98.4 98.5
sprawl 95.1 92.9 98.2 97.9 96.4
loose 87.5 74.0 97.9 96.9 96.4
strict? 32.7 35.6 38.7 40.3 55.6
strict 35.6 39.2 59.3 66.9 61.1
Table 2: Several sources of fragments? end-points and
%-correctness of their derived constraints (for English).
notations, punctuation or HTML tags in web pages).
For example, in the sentence about Cordis, the
strict hypothesis would be wrong about five of the
eight fragments: Jurors attaches in; Court takes the
second in; Hershhenson and Pagones derive their ti-
tles, president; and (at least in our reference) Vadas
attaches and, Ciporkin and who. Based on this, we
would consider strict to be 37.5%-accurate. But
loose ? and the rest of the more relaxed constraints
? would get perfect scores. (And strict? would re-
tract the mistake about Jurors but also the correct
guesses about Miami and Cordis, scoring only 20%.)
Table 2 (capital) shows scores averaged over the
entire treebank. Columns markup (Spitkovsky et al,
2010b) and punct (Spitkovsky et al, 2011b) indicate
that capitalization yields across-the-board more ac-
curate constraints (for English) compared with frag-
ments derived from punctuation or markup (i.e., an-
chor text, bold, italics and underline tags in HTML),
for which such constraints were originally intended.
4 Pilot Experiments on Supervised Parsing
To further test the potential of capitalization-induced
constraints, we applied them in the Viterbi-decoding
phase of a simple (unlexicalized) supervised depen-
dency parser ? an instance of DBM-1 (Spitkovsky
et al, 2012, ?2.1), trained on WSJ sentences with up
punct.: thread tear sprawl loose
none: 71.8 74.3 74.4 74.5 73.3
capital:thread 72.3 74.6 74.7 74.9 73.6
tear 72.4 74.7 74.7 74.9 73.6
sprawl 72.4 74.7 74.7 74.9 73.4
loose 72.4 74.8 74.7 74.9 73.3
strict? 71.4 73.7 73.7 73.9 72.7
strict 71.0 73.1 73.1 73.2 72.1
Table 3: Supervised (directed) accuracy on Section 23
of WSJ using capitalization-induced constraints (vertical)
jointly with punctuation (horizontal) in Viterbi-decoding.
17
CoNLL Year Filtered Training Directed Accuracies with Initial Constraints Fragments
& Language Tokens / Sentences none thread tear sprawl loose strict? strict Multi Single
German 2006 139,333 12,296 36.3 36.3 36.3 39.1 36.2 36.3 30.1 3,287 30,435
Czech ?6 187,505 20,378 51.3 51.3 51.3 51.3 52.5 52.5 51.4 1,831 6,722
English ?7 74,023 5,087 29.2 28.5 28.3 29.0 29.3 28.3 27.7 1,135 2,218
Bulgarian ?6 46,599 5,241 59.4 59.3 59.3 59.4 59.1 59.3 59.5 184 1,506
Danish ?6 14,150 1,599 21.3 17.7 22.7 21.5 21.4 31.4 27.9 113 317
Greek ?7 11,943 842 28.1 46.1 46.3 46.3 46.4 31.1 31.0 113 456
Dutch ?6 72,043 7,107 45.9 45.8 45.9 45.8 45.8 45.7 29.6 89 4,335
Italian ?7 9,142 921 41.7 52.6 52.7 52.6 44.2 52.6 45.8 41 296
Catalan ?7 62,811 4,082 61.3 61.3 61.3 61.3 61.3 61.3 36.5 28 2,828
Turkish ?6 17,610 2,835 32.9 32.9 32.2 33.0 33.0 33.6 33.9 27 590
Portuguese ?6 24,494 2,042 68.9 67.1 69.1 69.2 68.9 68.9 38.5 9 953
Hungarian ?7 10,343 1,258 43.2 43.2 43.1 43.2 43.2 43.7 25.5 7 277
Swedish ?6 41,918 4,105 48.6 48.6 48.6 48.5 48.5 48.5 48.8 3 296
Slovenian ?6 3,627 477 30.4 30.5 30.5 30.4 30.5 30.5 30.8 1 63
Median: 42.5 46.0 46.1 46.0 45.0 44.7 32.5
Mean: 42.8 44.4 44.8 45.0 44.3 44.6 36.9
Table 4: Parsing performance for grammar inducers trained with capitalization-based initial constraints, tested against
14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.
to 45 words (excluding Section 23). Table 3 shows
evaluation results on held-out data (all sentences),
using ?add-one? smoothing. All constraints other
than strict improve accuracy by about a half-a-point,
from 71.8 to 72.4%, suggesting that capitalization
is informative of certain regularities not captured by
DBM grammars; moreover, it still continues to be
useful when punctuation-based constraints are also
enforced, boosting accuracy from 74.5 to 74.9%.
5 Multi-Lingual Grammar Induction
So far, we showed only that capitalization informa-
tion can be helpful in parsing a very specific genre
of English. Next, we tested its ability to generally
aid dependency grammar induction, focusing on sit-
uations when other bracketing cues are unavailable.
We experimented with 14 languages from 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007), excluding Arabic, Chinese and
Japanese (which lack case), as well as Basque and
Spanish (which are pre-processed in a way that loses
relevant capitalization information). For all remain-
ing languages we trained only on simple sentences
? those lacking sentence-internal punctuation ?
from the relevant training sets (for blind evaluation).
Restricting our attention to a subset of the avail-
able training data serves a dual purpose. First, it al-
lows us to estimate capitalization?s impact where no
other (known or obvious) cues could also be used.
Otherwise, unconstrained baselines would not yield
the strongest possible alternative, and hence not the
most interesting comparison. Second, to the extent
that presence of punctuation may correlate with sen-
tence complexity (Frank, 2000), there are benefits to
?starting small? (Elman, 1993): e.g., relegating full
data to later stages helps training (Spitkovsky et al,
2010a; Cohn et al, 2011; Tu and Honavar, 2011).
Our base systems induced DBM-1, starting from
uniformly-at-random chosen parse trees (Cohen and
Smith, 2010) of each sentence, followed by inside-
outside re-estimation (Baker, 1979) with ?add-one?
smoothing.2 Capitalization-constrained systems dif-
fered from controls in exactly one way: each learner
got a slight nudge towards more promising struc-
tures by choosing initial seed trees satisfying an ap-
propriate constraint (but otherwise still uniformly).
Table 4 contains the stats for all 14 training sets,
ordered by number of multi-token fragments. Fi-
nal accuracies on respective (disjoint, full) evalua-
tion sets are improved by all constraints other than
strict, with the highest average performance result-
ing from sprawl: 45.0% directed dependency accu-
racy,3 on average. This increase of about two points
over the base system?s 42.8% is driven primarily by
improvements in two languages (Greek and Italian).
2We used ?early-stopping lateen EM? (Spitkovsky et al,
2011a, ?2.3) instead of thresholding or waiting for convergence.
3Starting from five parse trees for each sentence (using con-
straints thread through strict?) was no better, at 44.8% accuracy.
18
6 Capitalizing on Punctuation in Inference
Until now we avoided using punctuation in grammar
induction, except to filter data. Yet our pilot exper-
iments indicated that both kinds of information are
helpful in the decoding stage of a supervised system.
We took trained models obtained using the sprawl
nudge (from ?5) and proceeded to again apply con-
straints in inference (as in ?4). Capitalization alone
increased parsing accuracy only slightly, from 45.0
to 45.1%, on average. Using punctuation constraints
instead led to more improved performance: 46.5%.
Combining both types of constraints again resulted
in slightly higher accuracies: 46.7%. Table 5 breaks
down our last average performance number by lan-
guage and shows the combined approach to be com-
petitive with state-of-the-art. We suspect that further
improvements could be attained by also incorporat-
ing both constraints in training and with full data.
7 Discussion and A Few Post-Hoc Analyses
Our discussion, thus far, has been English-centric.
Nevertheless, languages differ in how they use capi-
talization (and even the rules governing a given lan-
guage tend to change over time ? generally towards
having fewer capitalized terms). For instance, adjec-
tives derived from proper nouns are not capitalized
in French, German, Polish, Spanish or Swedish, un-
like in English (see Table 1: JJ). And while English
forces capitalization of the first-person pronoun in
the nominative case, I (see Table 1: PRP), in Danish
it is the plural second-person pronoun (also I) that
is capitalized; further, formal pronouns (and their
case-forms) are capitalized in German (Sie and Ihre,
Ihres...), Italian, Slovenian, Russian and Bulgarian.
In contrast to pronouns, single-word proper nouns
? including personal names ? are capitalized in
nearly all European languages. Such shortest brack-
etings are not particularly useful for constraining
sets of possible parse trees in grammar induction,
however, compared to multi-word expressions; from
this perspective, German appears less helpful than
most cased languages, because of noun compound-
ing, despite prescribing capitalization of all nouns.
Another problem with longer word-strings in many
languages is that, e.g., in French (as in English)
lower-case prepositions may be mixed in with con-
tiguous groups of proper nouns: even in surnames,
CoNLL Year this State-of-the-Art Systems: POS-
& Language Work (i) Agnostic (ii) Identified
Bulgarian 2006 64.5 44.3 SCAJ5 70.3 Spt
Catalan ?7 61.5 63.8 SCAJ5 56.3 MZNR
Czech ?6 53.5 50.5 SCAJ5 33.3? MZNR
Danish ?6 20.6 46.0 RF 56.5 Sar
Dutch ?6 46.7 32.5 SCAJ5 62.1 MPHel
English ?7 29.2 50.3 SAJ 45.7 MPHel
German ?6 42.6 33.5 SCAJ5 55.8 MPHnl
Greek ?7 49.3 39.0 MZ 63.9 MPHen
Hungarian ?7 53.7 48.0 MZ 48.1 MZNR
Italian ?7 50.5 57.5 MZ 69.1 MPHpt
Portuguese ?6 72.4 43.2 MZ 76.9 Sbg
Slovenian ?6 34.8 33.6 SCAJ5 34.6 MZNR
Swedish ?6 50.5 50.0 SCAJ6 66.8 MPHpt
Turkish ?6 34.4 40.9 SAJ 61.3 RFH1
Median: 48.5 45.2 58.9
Mean: 46.7 45.2 57.2?
Table 5: Unsupervised parsing with both capitalization-
and punctuation-induced constraints in inference, tested
against the 14 held-out sets from 2006/7 CoNLL shared
tasks, and state-of-the-art results (all sentence lengths) for
systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al, 2011a, Tables 5?6)
and SAJ (Spitkovsky et al, 2011b); and (ii) rely on gold
POS-tag identities to (a) discourage noun roots (Marec?ek
and Zabokrtsky?, 2011, MZ), (b) encourage verbs (Ra-
sooli and Faili, 2012, RF), or (c) transfer delexicalized
parsers (S?gaard, 2011a, S) from resource-rich languages
with parallel translations (McDonald et al, 2011, MPH).
the German particle von is not capitalized, although
the Dutch van is, unless preceded by a given name or
initial ? hence Van Gogh, yet Vincent van Gogh.
7.1 Constraint Accuracies Across Languages
Since even related languages (e.g., Flemish, Dutch,
German and English) can have quite different con-
ventions regarding capitalization, one would not ex-
pect the same simple strategy to be uniformly useful
? or useful in the same way ? across disparate lan-
guages. To get a better sense of how universal our
constraints may be, we tabulated their accuracies for
the full training sets of the CoNLL data, after all
grammar induction experiments had been executed.
Table 6 shows that the less-strict capitalization-
induced constraints all fall within narrow (yet high)
bands of accuracies of just a few percentage points:
99?100% in the case of thread, 98?100% for tear,
95?99% for sprawl and 94?99% for loose. By con-
trast, the ranges for punctuation-induced constraints
are all at least 10%. We do not see anything partic-
19
CoNLL Year Total Training Capitalization-Induced Constraints Punctuation-Induced Constraints
& Language Tokens / Sentences thr-d tear spr-l loose str.? strict thr-d tear spr-l loose str.? strict
Arabic 2006 52,752 1,460 ? ? ? ? ? ? 89.6 89.5 81.9 61.2 29.7 33.4
?7 102,375 2,912 ? ? ? ? ? ? 90.9 90.6 83.1 61.2 29.5 35.2
Basque ?7 41,013 3,190 ? ? ? ? ? ? 96.2 95.7 92.3 81.9 42.8 50.6
Bulgarian ?6 162,985 12,823 99.8 99.5 96.6 96.4 51.8 81.0 97.6 97.2 96.1 74.7 36.7 41.2
Catalan ?7 380,525 14,958 100 99.5 95.0 94.6 15.8 57.9 96.1 95.5 94.6 73.7 36.0 42.6
Chinese ?6 337,162 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
?7 337,175 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
Czech ?6 1,063,413 72,703 99.7 98.3 96.2 95.4 42.4 68.0 89.4 89.2 87.7 68.9 37.2 41.7
?7 368,624 25,364 99.7 98.3 96.1 95.4 42.6 67.6 89.5 89.3 87.8 69.3 37.4 41.9
Danish ?6 80,743 5,190 99.9 99.4 98.3 97.0 59.0 69.7 96.9 96.9 95.2 68.3 39.6 40.9
Dutch ?6 172,958 13,349 99.9 99.1 98.4 96.6 16.6 46.3 89.6 89.5 86.4 69.6 42.5 46.2
English ?7 395,139 18,577 99.3 98.7 98.0 96.0 17.5 24.8 91.5 91.4 90.6 76.5 39.6 42.3
German ?6 605,337 39,216 99.6 98.0 96.7 96.4 41.7 57.1 94.5 93.9 90.7 71.1 37.2 40.7
Greek ?7 58,766 2,705 99.9 99.3 98.5 96.6 13.6 50.1 91.3 91.0 89.8 75.7 43.7 47.0
Hungarian ?7 111,464 6,034 99.9 98.1 95.7 94.4 46.6 62.0 96.1 94.0 89.0 77.1 28.9 32.6
Italian ?7 60,653 3,110 99.9 99.6 99.0 98.8 12.8 68.2 97.1 96.8 96.0 77.8 44.7 47.9
Japanese ?6 133,927 17,044 ? ? ? ? ? ? 100 100 95.4 89.0 48.9 63.5
Portuguese ?6 177,581 9,071 100 99.0 97.6 97.0 14.4 37.7 96.0 95.8 94.9 74.5 40.3 45.0
Slovenian ?6 23,779 1,534 100 99.8 98.9 98.9 52.0 84.7 93.3 93.3 92.6 72.7 42.7 45.8
Spanish ?6 78,068 3,306 ? ? ? ? ? ? 96.5 96.0 95.2 75.4 33.4 40.9
Swedish ?6 163,301 11,042 99.8 99.6 99.0 97.0 24.7 58.4 90.8 90.4 87.4 66.8 31.1 33.9
Turkish ?6 48,373 4,997 100 99.8 96.2 94.0 22.8 42.8 99.8 99.7 95.1 76.9 37.7 42.0
?7 54,761 5,635 100 99.9 96.1 94.2 21.6 42.9 99.8 99.7 94.6 76.7 38.2 42.8
Max: 100 99.9 99.0 98.9 59.0 84.7 100 100 96.1 89.0 48.9 63.5
Mean: 99.8 99.1 97.4 96.4 30.8 57.7 94.6 94.2 91.7 74.0 38.5 43.3
Min: 99.3 98.0 95.0 94.0 12.8 24.8 89.4 89.2 81.9 61.2 28.9 32.6
Table 6: Accuracies for capitalization- and punctuation-induced constraints on all (full) 2006/7 CoNLL training sets.
ularly special about Greek or Italian in these sum-
maries that could explain their substantial improve-
ments (18 and 11%, respectively ? see Table 4),
though Italian does appear to mesh best with the
sprawl constraint (not by much, closely followed by
Swedish). And English ? the language from which
we drew our inspiration ? barely improved with
capitalization-induced constraints (see Table 4) and
caused the lowest accuracies of thread and strict.
These outcomes are not entirely surprising: some
best- and worst-performing results are due to noise,
since learning via non-convex optimization can be
chaotic: e.g., in the case of Greek, applying 113 con-
straints to initial parse trees could have a significant
impact on the first grammar estimated in training ?
and consequently also on a learner?s final, converged
model instance. We expect the averages (i.e., means
and medians) ? computed over many data sets ?
to be more stable and meaningful than the outliers.
7.2 Immediate Impact from Capitalization
Next, we considered two settings that are less af-
fected by training noise: grammar inducers immedi-
ately after an initial step of constrained Viterbi EM
and supervised DBM parsers (trained on sentences
with up to 45 words), for various languages in the
CoNLL sets. Table 7 shows effects of capitalization
to be exceedingly mild, both if applied alone and in
tandem with punctuation. Exploring better ways of
incorporating this informative resource ? perhaps
as soft features, rather than as hard constraints ?
and in combination with punctuation- and markup-
induced bracketings could be a fruitful direction.
7.3 Odds and Ends
Our earlier analysis excluded sentence-initial words
because their capitalization is, in a way, trivial. But
for completeness, we also tested constraints derived
from this source, separately (see Table 2: initials).
As expected, the new constraints scored worse (de-
spite many automatically-correct single-word frag-
ments) except for strict, whose binding constraints
over singletons drove up accuracy. It turns out, most
first words in WSJ are leaves ? possibly due to a
dearth of imperatives (or just English?s determiners).
We broadened our investigation of the ?first leaf?
20
CoNLL Year Evaluation Bracketings Unsupervised Training Supervised Parsing
& Language Tokens / Sents capital. punct. init. 1-step constrained none capital. punct. both
Arabic 2006 5,215 146 ? 101 18.4 20.6 ? ? 59.8 ? ? ?
?7 4,537 130 ? 311 19.0 23.5 ? ? 63.5 ? ? ?
Basque ?7 4,511 334 ? 547 17.4 22.4 ? ? 58.4 ? ? ?
Bulgarian ?6 5,032 398 44 552 19.4 28.9 28.4 -0.5 76.7 76.8 78.1 78.2
Catalan ?7 4,478 167 24 398 18.0 25.1 25.4 +0.3 78.1 78.3 78.6 78.9
Chinese ?6 5,012 867 ? ? 23.5 27.2 ? ? 83.7 ? ? ?
?7 5,161 690 ? ? 19.4 25.0 ? ? 81.0 ? ? ?
Czech ?6 5,000 365 48 549 18.6 19.7 19.8 +0.1 64.9 64.8 67.0 66.9
?7 4,029 286 57 466 18.0 21.7 ? ? 62.8 ? ? ?
Danish ?6 4,978 322 85 590 19.5 27.4 26.0 -1.3 71.9 72.0 74.2 74.3
Dutch ?6 4,989 386 28 318 18.7 17.9 17.7 -0.1 60.9 60.9 62.7 62.8
English ?7 4,386 214 151 423 17.6 24.0 21.9 -2.1 65.2 65.6 68.5 68.4
German ?6 4,886 357 135 523 16.4 23.0 23.7 +0.7 70.7 70.7 71.5 71.4
Greek ?7 4,307 197 47 372 17.1 17.1 16.6 -0.5 71.3 71.6 73.5 73.7
Hungarian ?7 6,090 390 28 893 17.1 18.5 18.6 +0.1 67.3 67.2 69.8 69.6
Italian ?7 4,360 249 71 505 18.6 32.5 34.2 +1.7 66.0 65.9 67.0 66.8
Japanese ?6 5,005 709 ? 0 26.5 36.8 ? ? 85.1 ? ? ?
Portuguese ?6 5,009 288 29 559 19.3 24.2 24.0 -0.1 80.5 80.5 81.6 81.6
Slovenian ?6 5,004 402 7 785 18.3 22.5 22.4 -0.1 67.5 67.4 70.9 70.9
Spanish ?6 4,991 206 ? 453 18.0 19.3 ? ? 69.5 ? ? ?
Swedish ?6 4,873 389 14 417 20.2 31.4 31.4 +0.0 74.9 74.9 74.7 74.6
Turkish ?6 6,288 623 18 683 20.4 26.4 26.7 +0.3 66.1 66.0 66.9 66.7
?7 3,983 300 4 305 20.3 24.8 ? ? 67.3 ? ? ?
Max: 20.4 32.5 34.2 +1.7 80.5 80.5 81.6 81.6
(aggregated as in Tables 4 and 5) Mean: 18.5 24.2 24.1 -0.1 70.1 70.2 71.8 71.8
Min: 16.4 17.1 16.6 -2.1 60.9 60.9 62.7 62.8
Table 7: Unsupervised accuracies for uniform-at-random projective parse trees (init), also after a step of Viterbi EM,
and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).
phenomenon and found that in 16 of the 19 CoNLL
languages first words are more likely to be leaves
than other words without dependents on the left;4
last words, by contrast, are more likely to take de-
pendents than expected. These propensities may be
related to the functional tendency of languages to
place old information before new (Ward and Birner,
2001) and could also help bias grammar induction.
Lastly, capitalization points to yet another class of
words: those with identical upper- and lower-case
forms. Their constraints too tend to be accurate (see
Table 2: uncased), but the underlying text is not par-
ticularly interesting. In WSJ, caseless multi-token
fragments are almost exclusively percentages (e.g.,
the two tokens of 10%), fractions (e.g., 1 1/4) or both.
Such boundaries could be useful in dealing with fi-
nancial data, as well as for breaking up text in lan-
guages without capitalization (e.g., Arabic, Chinese
4Arabic, Basque, Bulgarian, Catalan, Chinese, Danish,
Dutch, English, German, Greek, Hungarian, Italian, Japanese,
Portuguese, Spanish, Swedish vs. Czech, Slovenian, Turkish.
and Japanese). More generally, transitions between
different fonts and scripts should be informative too.
8 Conclusion
Orthography provides valuable syntactic cues. We
showed that bounding boxes signaled by capitaliza-
tion changes can help guide grammar induction and
boost unsupervised parsing performance. As with
punctuation-delimited segments and tags from web
markup, it is profitable to assume only that a single
word derives the rest, in such text fragments, without
further restricting relations to external words ? pos-
sibly a useful feature for supervised parsing models.
Our results should be regarded with some cau-
tion, however, since improvements due to capitaliza-
tion in grammar induction experiments came mainly
from two languages, Greek and Italian. Further re-
search is clearly needed to understand the ways that
capitalization can continue to improve parsing.
21
Acknowledgments
Funded, in part, by Defense Advanced Research Projects Age-
ncy (DARPA) Machine Reading Program under Air Force Re-
search Laboratory (AFRL) prime contract FA8750-09-C-0181.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of DARPA, AFRL, or the US gov-
ernment. We also thank Ryan McDonald and the anonymous
reviewers for helpful comments on draft versions of this paper.
References
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. Journal of Machine Learning Re-
search.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL-HLT.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b. Profiting
from mark-up: Hyper-text annotations for guided parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012. Three
dependency-and-boundary models for grammar induction.
In submission to EMNLP.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
G. Ward and B. J. Birner. 2001. Discourse and information
structure. In D. Schiffrin, D. Tannen, and H. Hamilton, edi-
tors, Handbook of Discourse Analysis. Oxford: Basil Black-
well.
22
