Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 905?912
Manchester, August 2008
A Uniform Approach to Analogies, Synonyms, Antonyms,
and Associations
Peter D. Turney
National Research Council of Canada
Institute for Information Technology
M50 Montreal Road
Ottawa, Ontario, Canada
K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Abstract
Recognizing analogies, synonyms, anto-
nyms, and associations appear to be four
distinct tasks, requiring distinct NLP al-
gorithms. In the past, the four tasks have
been treated independently, using a wide
variety of algorithms. These four seman-
tic classes, however, are a tiny sample of
the full range of semantic phenomena, and
we cannot afford to create ad hoc algo-
rithms for each semantic phenomenon; we
need to seek a unified approach. We pro-
pose to subsume a broad range of phenom-
ena under analogies. To limit the scope of
this paper, we restrict our attention to the
subsumption of synonyms, antonyms, and
associations. We introduce a supervised
corpus-based machine learning algorithm
for classifying analogous word pairs, and
we show that it can solve multiple-choice
SAT analogy questions, TOEFL synonym
questions, ESL synonym-antonym ques-
tions, and similar-associated-both ques-
tions from cognitive psychology.
1 Introduction
A pair of words (petrify:stone) is analogous to an-
other pair (vaporize:gas) when the semantic re-
lations between the words in the first pair are
highly similar to the relations in the second pair.
Two words (levied and imposed) are synonymous
in a context (levied a tax) when they can be in-
terchanged (imposed a tax), they are are antony-
mous when they have opposite meanings (black
c
? 2008, National Research Council of Canada (NRC).
Licensed to the Coling 2008 Organizing Committee for pub-
lication in Coling 2008 and for re-publishing in any form or
medium.
and white), and they are associated when they tend
to co-occur (doctor and hospital).
On the surface, it appears that these are four dis-
tinct semantic classes, requiring distinct NLP al-
gorithms, but we propose a uniform approach to
all four. We subsume synonyms, antonyms, and
associations under analogies. In essence, we say
that X and Y are antonyms when the pair X:Y
is analogous to the pair black:white, X and Y
are synonyms when they are analogous to the pair
levied:imposed, and X and Y are associated when
they are analogous to the pair doctor:hospital.
There is past work on recognizing analogies
(Reitman, 1965), synonyms (Landauer and Du-
mais, 1997), antonyms (Lin et al, 2003), and asso-
ciations (Lesk, 1969), but each of these four tasks
has been examined separately, in isolation from the
others. As far as we know, the algorithm proposed
here is the first attempt to deal with all four tasks
using a uniform approach. We believe that it is
important to seek NLP algorithms that can han-
dle a broad range of semantic phenomena, because
developing a specialized algorithm for each phe-
nomenon is a very inefficient research strategy.
It might seem that a lexicon, such as Word-
Net (Fellbaum, 1998), contains all the information
we need to handle these four tasks. However, we
prefer to take a corpus-based approach to seman-
tics. Veale (2004) used WordNet to answer 374
multiple-choice SAT analogy questions, achieving
an accuracy of 43%, but the best corpus-based ap-
proach attains an accuracy of 56% (Turney, 2006).
Another reason to prefer a corpus-based approach
to a lexicon-based approach is that the former re-
quires less human labour, and thus it is easier to
extend to other languages.
In Section 2, we describe our algorithm for rec-
ognizing analogies. We use a standard supervised
905
machine learning approach, with feature vectors
based on the frequencies of patterns in a large cor-
pus. We use a support vector machine (SVM)
to learn how to classify the feature vectors (Platt,
1998; Witten and Frank, 1999).
Section 3 presents four sets of experiments. We
apply our algorithm for recognizing analogies to
multiple-choice analogy questions from the SAT
college entrance test, multiple-choice synonym
questions from the TOEFL (test of English as a
foreign language), ESL (English as a second lan-
guage) practice questions for distinguishing syn-
onyms and antonyms, and a set of word pairs that
are labeled similar, associated, and both, devel-
oped for experiments in cognitive psychology.
We discuss the results of the experiments in Sec-
tion 4. The accuracy of the algorithm is competi-
tive with other systems, but the strength of the al-
gorithm is that it is able to handle all four tasks,
with no tuning of the learning parameters to the
particular task. It performs well, although it is
competing against specialized algorithms, devel-
oped for single tasks.
Related work is examined in Section 5 and lim-
itations and future work are considered in Sec-
tion 6. We conclude in Section 7.
2 Classifying Analogous Word Pairs
An analogy, A:B::C:D, asserts that A is to B as C
is to D; for example, traffic:street::water:riverbed
asserts that traffic is to street as water is to riverbed;
that is, the semantic relations between traffic and
street are highly similar to the semantic relations
between water and riverbed. We may view the
task of recognizing word analogies as a problem
of classifying word pairs (see Table 1).
Word pair Class label
carpenter:wood artisan:material
mason:stone artisan:material
potter:clay artisan:material
glassblower:glass artisan:material
traffic:street entity:carrier
water:riverbed entity:carrier
packets:network entity:carrier
gossip:grapevine entity:carrier
Table 1: Examples of how the task of recogniz-
ing word analogies may be viewed as a problem of
classifying word pairs.
We approach this as a standard classification
problem for supervised machine learning. The al-
gorithm takes as input a training set of word pairs
with class labels and a testing set of word pairs
without labels. Each word pair is represented as a
vector in a feature space and a supervised learning
algorithm is used to classify the feature vectors.
The elements in the feature vectors are based on
the frequencies of automatically defined patterns
in a large corpus. The output of the algorithm is an
assignment of labels to the word pairs in the test-
ing set. For some of the experiments, we select
a unique label for each word pair; for other ex-
periments, we assign probabilities to each possible
label for each word pair.
For a given word pair, such as mason:stone, the
first step is to generate morphological variations,
such as masons:stones. In the following experi-
ments, we use morpha (morphological analyzer)
and morphg (morphological generator) for mor-
phological processing (Minnen et al, 2001).1
The second step is to search in a large corpus for
all phrases of the following form:
?[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]?
In this template, X:Y consists of morphological
variations of the given word pair, in either or-
der; for example, mason:stone, stone:mason, ma-
sons:stones, and so on. A typical phrase for ma-
son:stone would be ?the mason cut the stone with?.
We then normalize all of the phrases that are found,
by using morpha to remove suffixes.
The template we use here is similar to Turney
(2006), but we have added extra context words
before the X and after the Y . Our morpholog-
ical processing also differs from Turney (2006).
In the following experiments, we search in a cor-
pus of 5 ? 1010 words (about 280 GB of plain
text), consisting of web pages gathered by a web
crawler.2 To retrieve phrases from the corpus, we
use Wumpus (Bu?ttcher and Clarke, 2005), an effi-
cient search engine for passage retrieval from large
corpora.3
The next step is to generate patterns from all
of the phrases that were found for all of the in-
put word pairs (from both the training and testing
sets). To generate patterns from a phrase, we re-
place the given word pairs with variables, X and
Y , and we replace the remaining words with a wild
card symbol (an asterisk) or leave them as they are.
1http://www.informatics.susx.ac.uk/research/groups/nlp/
carroll/morph.html.
2The corpus was collected by Charles Clarke, University
of Waterloo. We can provide copies on request.
3http://www.wumpus-search.org/.
906
For example, the phrase ?the mason cut the stone
with? yields the patterns ?the X cut * Y with?, ?*
X * the Y *?, and so on. If a phrase contains n
words, then it yields 2(n?2) patterns.
Each pattern corresponds to a feature in the fea-
ture vectors that we will generate. Since a typi-
cal input set of word pairs yields millions of pat-
terns, we need to use feature selection, to reduce
the number of patterns to a manageable quantity.
For each pattern, we count the number of input
word pairs that generated the pattern. For example,
?* X cut * Y *? is generated by both mason:stone
and carpenter:wood. We then sort the patterns in
descending order of the number of word pairs that
generated them. If there are N input word pairs
(and thus N feature vectors, including both the
training and testing sets), then we select the top
kN patterns and drop the remainder. In the fol-
lowing experiments, k is set to 20. The algorithm
is not sensitive to the precise value of k.
The reasoning behind the feature selection al-
gorithm is that shared patterns make more useful
features than rare patterns. The number of features
(kN ) depends on the number of word pairs (N ),
because, if we have more feature vectors, then we
need more features to distinguish them. Turney
(2006) also selects patterns based on the number
of pairs that generate them, but the number of se-
lected patterns is a constant (8000), independent of
the number of input word pairs.
The next step is to generate feature vectors, one
vector for each input word pair. Each of the N
feature vectors has kN elements, one element for
each selected pattern. The value of an element in
a vector is given by the logarithm of the frequency
in the corpus of the corresponding pattern for the
given word pair. For example, suppose the given
pair is mason:stone and the pattern is ?* X cut
* Y *?. We look at the normalized phrases that
we collected for mason:stone and we count how
many match this pattern. If f phrases match the
pattern, then the value of this element in the fea-
ture vector is log(f +1) (we add 1 because log(0)
is undefined). Each feature vector is then normal-
ized to unit length. The normalization ensures that
features in vectors for high-frequency word pairs
(traffic:street) are comparable to features in vectors
for low-frequency word pairs (water:riverbed).
Now that we have a feature vector for each in-
put word pair, we can apply a standard supervised
learning algorithm. In the following experiments,
we use a sequential minimal optimization (SMO)
support vector machine (SVM) with a radial ba-
sis function (RBF) kernel (Platt, 1998), as imple-
mented in Weka (Waikato Environment for Knowl-
edge Analysis) (Witten and Frank, 1999).4 The
algorithm generates probability estimates for each
class by fitting logistic regression models to the
outputs of the SVM. We disable the normalization
option in Weka, since the vectors are already nor-
malized to unit length. We chose the SMO RBF
algorithm because it is fast, robust, and it easily
handles large numbers of features.
For convenience, we will refer to the above algo-
rithm as PairClass. In the following experiments,
PairClass is applied to each of the four problems
with no adjustments or tuning to the specific prob-
lems. Some work is required to fit each problem
into the general framework of PairClass (super-
vised classification of word pairs) but the core al-
gorithm is the same in each case.
3 Experiments
This section presents four sets of experiments, with
analogies, synonyms, antonyms, and associations.
We explain how each task is treated as a problem
of classifying analogous word pairs, we give the
experimental results, and we discuss past work on
each of the four tasks.
3.1 SAT Analogies
In this section, we apply PairClass to the task
of recognizing analogies. To evaluate the perfor-
mance, we use a set of 374 multiple-choice ques-
tions from the SAT college entrance exam. Table 2
shows a typical question. The target pair is called
the stem. The task is to select the choice pair that
is most analogous to the stem pair.
Stem: mason:stone
Choices: (a) teacher:chalk
(b) carpenter:wood
(c) soldier:gun
(d) photograph:camera
(e) book:word
Solution: (b) carpenter:wood
Table 2: An example of a question from the 374
SAT analogy questions.
The problem of recognizing word analogies was
first attempted with a system called Argus (Reit-
4http://www.cs.waikato.ac.nz/ml/weka/.
907
man, 1965), using a small hand-built semantic net-
work with a spreading activation algorithm. Tur-
ney et al (2003) used a combination of 13 in-
dependent modules. Veale (2004) used a spread-
ing activation algorithm with WordNet (in effect,
treating WordNet as a semantic network). Turney
(2006) used a corpus-based algorithm.
We may view Table 2 as a binary classifica-
tion problem, in which mason:stone and carpen-
ter:wood are positive examples and the remaining
word pairs are negative examples. The difficulty is
that the labels of the choice pairs must be hidden
from the learning algorithm. That is, the training
set consists of one positive example (the stem pair)
and the testing set consists of five unlabeled exam-
ples (the five choice pairs). To make this task more
tractable, we randomly choose a stem pair from
one of the 373 other SAT analogy questions, and
we assume that this new stem pair is a negative ex-
ample, as shown in Table 3.
Word pair Train or test Class label
mason:stone train positive
tutor:pupil train negative
teacher:chalk test hidden
carpenter:wood test hidden
soldier:gun test hidden
photograph:camera test hidden
book:word test hidden
Table 3: How to fit a SAT analogy question into
the framework of supervised pair classification.
To answer the SAT question, we use PairClass to
estimate the probability that each testing example
is positive, and we guess the testing example with
the highest probability. Learning from a training
set with only one positive example and one nega-
tive example is difficult, since the learned model
can be highly unstable. To increase the stability,
we repeat the learning process 10 times, using a
different randomly chosen negative training exam-
ple each time. For each testing word pair, the 10
probability estimates are averaged together. This
is a form of bagging (Breiman, 1996).
PairClass attains an accuracy of 52.1%. For
comparison, the ACL Wiki lists 12 previously pub-
lished results with the 374 SAT analogy ques-
tions.5 Only 2 of the 12 algorithms have higher
accuracy. The best previous result is an accuracy
of 56.1% (Turney, 2006). Random guessing would
5For more information, see SAT Analogy Questions (State
of the art) at http://aclweb.org/aclwiki/.
yield an accuracy of 20%. The average senior
high school student achieves 57% correct (Turney,
2006).
3.2 TOEFL Synonyms
Now we apply PairClass to the task of recogniz-
ing synonyms, using a set of 80 multiple-choice
synonym questions from the TOEFL (test of En-
glish as a foreign language). A sample question is
shown in Table 4. The task is to select the choice
word that is most similar in meaning to the stem
word.
Stem: levied
Choices: (a) imposed
(b) believed
(c) requested
(d) correlated
Solution: (a) imposed
Table 4: An example of a question from the 80
TOEFL questions.
Synonymy can be viewed as a high degree of
semantic similarity. The most common way to
measure semantic similarity is to measure the dis-
tance between words in WordNet (Resnik, 1995;
Jiang and Conrath, 1997; Hirst and St-Onge,
1998). Corpus-based measures of word similarity
are also common (Lesk, 1969; Landauer and Du-
mais, 1997; Turney, 2001).
We may view Table 4 as a binary classifica-
tion problem, in which the pair levied:imposed is a
positive example of the class synonymous and the
other possible pairings are negative examples, as
shown in Table 5.
Word pair Class label
levied:imposed positive
levied:believed negative
levied:requested negative
levied:correlated negative
Table 5: How to fit a TOEFL question into the
framework of supervised pair classification.
The 80 TOEFL questions yield 320 (80 ? 4)
word pairs, 80 labeled positive and 240 labeled
negative. We apply PairClass to the word pairs us-
ing ten-fold cross-validation. In each random fold,
90% of the pairs are used for training and 10%
are used for testing. For each fold, the model that
is learned from the training set is used to assign
probabilities to the pairs in the testing set. With
908
ten separate folds, the ten non-overlapping test-
ing sets cover the whole dataset. Our guess for
each TOEFL question is the choice with the high-
est probability of being positive, when paired with
the corresponding stem.
PairClass attains an accuracy of 76.2%. For
comparison, the ACL Wiki lists 15 previously pub-
lished results with the 80 TOEFL synonym ques-
tions.6 Of the 15 algorithms, 8 have higher accu-
racy and 7 have lower. The best previous result
is an accuracy of 97.5% (Turney et al, 2003), ob-
tained using a hybrid of four different algorithms.
Random guessing would yield an accuracy of 25%.
The average foreign applicant to a US university
achieves 64.5% correct (Landauer and Dumais,
1997).
3.3 Synonyms and Antonyms
The task of classifying word pairs as either syn-
onyms or antonyms readily fits into the framework
of supervised classification of word pairs. Table 6
shows some examples from a set of 136 ESL (En-
glish as a second language) practice questions that
we collected from various ESL websites.
Word pair Class label
galling:irksome synonyms
yield:bend synonyms
naive:callow synonyms
advise:suggest synonyms
dissimilarity:resemblance antonyms
commend:denounce antonyms
expose:camouflage antonyms
unveil:veil antonyms
Table 6: Examples of synonyms and antonyms
from 136 ESL practice questions.
Lin et al (2003) distinguish synonyms from
antonyms using two patterns, ?from X to Y ? and
?either X or Y ?. When X and Y are antonyms,
they occasionally appear in a large corpus in one
of these two patterns, but it is very rare for syn-
onyms to appear in these patterns. Our approach
is similar to Lin et al (2003), but we do not rely
on hand-coded patterns; instead, PairClass patterns
are generated automatically.
Using ten-fold cross-validation, PairClass at-
tains an accuracy of 75.0%. Always guessing
the majority class would result in an accuracy of
65.4%. The average human score is unknown and
6For more information, see TOEFL Synonym Questions
(State of the art) at http://aclweb.org/aclwiki/.
there are no previous results for comparison.
3.4 Similar, Associated, and Both
A common criticism of corpus-based measures of
word similarity (as opposed to lexicon-based mea-
sures) is that they are merely detecting associations
(co-occurrences), rather than actual semantic sim-
ilarity (Lund et al, 1995). To address this criti-
cism, Lund et al (1995) evaluated their algorithm
for measuring word similarity with word pairs that
were labeled similar, associated, or both. These
labeled pairs were originally created for cogni-
tive psychology experiments with human subjects
(Chiarello et al, 1990). Table 7 shows some ex-
amples from this collection of 144 word pairs (48
pairs in each of the three classes).
Word pair Class label
table:bed similar
music:art similar
hair:fur similar
house:cabin similar
cradle:baby associated
mug:beer associated
camel:hump associated
cheese:mouse associated
ale:beer both
uncle:aunt both
pepper:salt both
frown:smile both
Table 7: Examples of word pairs labeled similar,
associated, or both.
Lund et al (1995) did not measure the accuracy
of their algorithm on this three-class classification
problem. Instead, following standard practice in
cognitive psychology, they showed that their al-
gorithm?s similarity scores for the 144 word pairs
were correlated with the response times of human
subjects in priming tests. In a typical priming test,
a human subject reads a priming word (cradle) and
is then asked to complete a partial word (complete
bab as baby). The time required to perform the
task is taken to indicate the strength of the cogni-
tive link between the two words (cradle and baby).
Using ten-fold cross-validation, PairClass at-
tains an accuracy of 77.1% on the 144 word pairs.
Since the three classes are of equal size, guessing
the majority class and random guessing both yield
an accuracy of 33.3%. The average human score
is unknown and there are no previous results for
comparison.
909
4 Discussion
The four experiments are summarized in Tables 8
and 9. For the first two experiments, where there
are previous results, PairClass is not the best, but
it performs competitively. For the second two ex-
periments, PairClass performs significantly above
the baselines. However, the strength of this ap-
proach is not its performance on any one task, but
the range of tasks it can handle.
As far as we know, this is the first time a stan-
dard supervised learning algorithm has been ap-
plied to any of these four problems. The advantage
of being able to cast these problems in the frame-
work of standard supervised learning problems is
that we can now exploit the huge literature on su-
pervised learning. Past work on these problems
has required implicitly coding our knowledge of
the nature of the task into the structure of the algo-
rithm. For example, the structure of the algorithm
for latent semantic analysis (LSA) implicitly con-
tains a theory of synonymy (Landauer and Dumais,
1997). The problem with this approach is that it
can be very difficult to work out how to modify the
algorithm if it does not behave the way we want.
On the other hand, with a supervised learning algo-
rithm, we can put our knowledge into the labeling
of the feature vectors, instead of putting it directly
into the algorithm. This makes it easier to guide
the system to the desired behaviour.
With our approach to the SAT analogy ques-
tions, we are blurring the line between supervised
and unsupervised learning, since the training set
for a given SAT question consists of a single real
positive example (and a single ?virtual? or ?simu-
lated? negative example). In effect, a single exam-
ple (mason:stone) becomes a sui generis; it con-
stitutes a class of its own. It may be possible
to apply the machinery of supervised learning to
other problems that apparently call for unsuper-
vised learning (for example, clustering or measur-
ing similarity), by using this sui generis device.
5 Related Work
One of the first papers using supervised ma-
chine learning to classify word pairs was Rosario
and Hearst?s (2001) paper on classifying noun-
modifier pairs in the medical domain. For ex-
ample, the noun-modifier expression brain biopsy
was classified as Procedure. Rosario and Hearst
(2001) constructed feature vectors for each noun-
modifier pair using MeSH (Medical Subject Head-
ings) and UMLS (Unified Medical Language Sys-
tem) as lexical resources. They then trained a neu-
ral network to distinguish 13 classes of semantic
relations, such as Cause, Location, Measure, and
Instrument. Nastase and Szpakowicz (2003) ex-
plored a similar approach to classifying general-
domain noun-modifier pairs, using WordNet and
Roget?s Thesaurus as lexical resources.
Turney and Littman (2005) used corpus-based
features for classifying noun-modifier pairs. Their
features were based on 128 hand-coded patterns.
They used a nearest-neighbour learning algorithm
to classify general-domain noun-modifier pairs
into 30 different classes of semantic relations. Tur-
ney (2006) later addressed the same problem using
8000 automatically generated patterns.
One of the tasks in SemEval 2007 was the clas-
sification of semantic relations between nominals
(Girju et al, 2007). The problem is to classify
semantic relations between nouns and noun com-
pounds in the context of a sentence. The task
attracted 14 teams who created 15 systems, all
of which used supervised machine learning with
features that were lexicon-based, corpus-based, or
both.
PairClass is most similar to the algorithm of Tur-
ney (2006), but it differs in the following ways:
? PairClass does not use a lexicon to find syn-
onyms for the input word pairs. One of our
goals in this paper is to show that a pure
corpus-based algorithm can handle synonyms
without a lexicon. This considerably simpli-
fies the algorithm.
? PairClass uses a support vector machine
(SVM) instead of a nearest neighbour (NN)
learning algorithm.
? PairClass does not use the singular value
decomposition (SVD) to smooth the feature
vectors. It has been our experience that SVD
is not necessary with SVMs.
? PairClass generates probability estimates,
whereas Turney (2006) uses a cosine mea-
sure of similarity. Probability estimates can
be readily used in further downstream pro-
cessing, but cosines are less useful.
? The automatically generated patterns in Pair-
Class are slightly more general than the pat-
terns of Turney (2006).
? The morphological processing in PairClass
(Minnen et al, 2001) is more sophisticated
than in Turney (2006).
910
Experiment Number of vectors Number of features Number of classes
SAT Analogies 2,244 (374 ? 6) 44,880 (2, 244 ? 20) 374
TOEFL Synonyms 320 (80 ? 4) 6,400 (320 ? 20) 2
Synonyms and Antonyms 136 2,720 (136 ? 20) 2
Similar, Associated, and Both 144 2,880 (144 ? 20) 3
Table 8: Summary of the four tasks. See Section 3 for explanations.
Experiment Accuracy Best previous Human Baseline Rank
SAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12
TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15
Synonyms and Antonyms 75.0% none unknown 65.4% none
Similar, Associated, and Both 77.1% none unknown 33.3% none
Table 9: Summary of experimental results. See Section 3 for explanations.
However, we believe that the main contribution of
this paper is not PairClass itself, but the extension
of supervised word pair classification beyond the
classification of noun-modifier pairs and seman-
tic relations between nominals, to analogies, syn-
onyms, antonyms, and associations. As far as we
know, this has not been done before.
6 Limitations and Future Work
The main limitation of PairClass is the need for a
large corpus. Phrases that contain a pair of words
tend to be more rare than phrases that contain ei-
ther of the members of the pair, thus a large cor-
pus is needed to ensure that sufficient numbers of
phrases are found for each input word pair. The
size of the corpus has a cost in terms of disk space
and processing time. In the future, as hardware im-
proves, this will become less of an issue, but there
may be ways to improve the algorithm, so that a
smaller corpus is sufficient.
Another area for future work is to apply Pair-
Class to more tasks. WordNet includes more than
a dozen semantic relations (e.g., synonyms, hy-
ponyms, hypernyms, meronyms, holonyms, and
antonyms). PairClass should be applicable to all
of these relations. Other potential applications in-
clude any task that involves semantic relations,
such as word sense disambiguation, information
retrieval, information extraction, and metaphor in-
terpretation.
7 Conclusion
In this paper, we have described a uniform ap-
proach to analogies, synonyms, antonyms, and as-
sociations, in which all of these phenomena are
subsumed by analogies. We view the problem of
recognizing analogies as the classification of se-
mantic relations between words.
We believe that most of our lexical knowledge
is relational, not attributional. That is, meaning is
largely about relations among words, rather than
properties of individual words, considered in iso-
lation. For example, consider the knowledge en-
coded in WordNet: much of the knowledge in
WordNet is embedded in the graph structure that
connects words.
Analogies of the form A:B::C:D are called
proportional analogies. These types of lower-
level analogies may be contrasted with higher-
level analogies, such as the analogy between the
solar system and Rutherford?s model of the atom
(Falkenhainer et al, 1989), which are sometimes
called conceptual analogies. We believe that the
difference between these two types is largely a
matter of complexity. A higher-level analogy is
composed of many lower-level analogies. Progress
with algorithms for processing lower-level analo-
gies will eventually contribute to algorithms for
higher-level analogies.
The idea of subsuming a broad range of se-
mantic phenomena under analogies has been sug-
gested by many researchers. Minsky (1986) wrote,
?How do we ever understand anything? Almost
always, I think, by using one or another kind of
analogy.? Hofstadter (2007) claimed, ?all meaning
comes from analogies.? In NLP, analogical algo-
rithms have been applied to machine translation
(Lepage and Denoual, 2005), morphology (Lep-
age, 1998), and semantic relations (Turney and
Littman, 2005). Analogy provides a framework
that has the potential to unify the field of seman-
tics. This paper is a small step towards that goal.
Acknowledgements
Thanks to Joel Martin and the anonymous review-
ers of Coling 2008 for their helpful comments.
911
References
Breiman, Leo. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Bu?ttcher, Stefan and Charles Clarke. 2005. Efficiency
vs. effectiveness in terabyte-scale information re-
trieval. In Proceedings of the 14th Text REtrieval
Conference (TREC 2005), Gaithersburg, MD.
Chiarello, Christine, Curt Burgess, Lorie Richards, and
Alma Pollock. 1990. Semantic and associative
priming in the cerebral hemispheres: Some words
do, some words don?t ... sometimes, some places.
Brain and Language, 38:75?104.
Falkenhainer, Brian, Kenneth D. Forbus, and Dedre
Gentner. 1989. The structure-mapping engine:
Algorithm and examples. Artificial Intelligence,
41(1):1?63.
Fellbaum, Christiane, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In SemEval 2007, pages
13?18, Prague, Czech Republic.
Hirst, Graeme and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Fellbaum,
Christiane, editor, WordNet: An Electronic Lexical
Database, pages 305?332. MIT Press.
Hofstadter, Douglas. 2007. I Am a Srange Loop. Basic
Books.
Jiang, Jay J. and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
taxonomy. In ROCLING X, pages 19?33, Tapei, Tai-
wan.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104(2):211?240.
Lepage, Yves and Etienne Denoual. 2005. Purest
ever example-based machine translation: Detailed
presentation and assessment. Machine Translation,
19(3):251?282.
Lepage, Yves. 1998. Solving analogies on words: An
algorithm. In Proceedings of the 36th Annual Con-
ference of the Association for Computational Lin-
guistics, pages 728?735.
Lesk, Michael E. 1969. Word-word associations in
document retrieval systems. American Documenta-
tion, 20(1):27?38.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In IJCAI-03, pages 1492?
1493.
Lund, Kevin, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th Annual Conference of the Cognitive Science So-
ciety, pages 660?665.
Minnen, Guido, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Minsky, Marvin. 1986. The Society of Mind. Simon &
Schuster, New York, NY.
Nastase, Vivi and Stan Szpakowicz. 2003. Explor-
ing noun-modifier semantic relations. In Fifth In-
ternational Workshop on Computational Semantics
(IWCS-5), pages 285?301, Tilburg, The Netherlands.
Platt, John C. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Advances in Kernel Methods: Support Vector Learn-
ing, pages 185?208. MIT Press Cambridge, MA,
USA.
Reitman, Walter R. 1965. Cognition and Thought: An
Information Processing Approach. John Wiley and
Sons, New York, NY.
Resnik, Philip. 1995. Using information content
to evaluate semantic similarity in a taxonomy. In
IJCAI-95, pages 448?453, San Mateo, CA. Morgan
Kaufmann.
Rosario, Barbara and Marti Hearst. 2001. Classify-
ing the semantic relations in noun-compounds via
a domain-specific lexical hierarchy. In EMNLP-01,
pages 82?90.
Turney, Peter D. and Michael L. Littman. 2005.
Corpus-based learning of analogies and semantic re-
lations. Machine Learning, 60(1?3):251?278.
Turney, Peter D., Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining indepen-
dent modules to solve multiple-choice synonym and
analogy problems. In RANLP-03, pages 482?489,
Borovets, Bulgaria.
Turney, Peter D. 2001. Mining the Web for syn-
onyms: PMI-IR versus LSA on TOEFL. In Proceed-
ings of the Twelfth European Conference on Machine
Learning, pages 491?502, Berlin. Springer.
Turney, Peter D. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Veale, Tony. 2004. WordNet sits the SAT: A
knowledge-based approach to lexical analogy. In
Proceedings of the 16th European Conference on
Artificial Intelligence (ECAI 2004), pages 606?612,
Valencia, Spain.
Witten, Ian H. and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Francisco.
912
Similarity of Semantic Relations
Peter D. Turney?
National Research Council Canada
There are at least two kinds of similarity. Relational similarity is correspondence between re-
lations, in contrast with attributional similarity, which is correspondence between attributes.
When two words have a high degree of attributional similarity, we call them synonyms. When
two pairs of words have a high degree of relational similarity, we say that their relations are
analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood.
This article introduces Latent Relational Analysis (LRA), a method for measuring relational
similarity. LRA has potential applications in many areas, including information extraction,
word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM)
of information retrieval has been adapted to measuring relational similarity, achieving a score
of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the
VSM approach, the relation between a pair of words is characterized by a vector of frequencies
of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1)
The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition
(SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are
used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions,
statistically equivalent to the average human score of 57%. On the related problem of classifying
semantic relations, LRA achieves similar gains over the VSM.
1. Introduction
There are at least two kinds of similarity. Attributional similarity is correspondence be-
tween attributes and relational similarity is correspondence between relations (Medin,
Goldstone, and Gentner 1990). When two words have a high degree of attributional
similarity, we call them synonyms. When two word pairs have a high degree of relational
similarity, we say they are analogous.
Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to
D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over
a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of
relational similarity between the word pair traffic:street and the word pair water:riverbed.
In fact, this analogy is the basis of several mathematical theories of traffic flow (Daganzo
1994).
In Section 2, we look more closely at the connections between attributional and
relational similarity. In analogies such as mason:stone::carpenter:wood, it seems that
? Institute for Information Technology, National Research Council Canada, M-50 Montreal Road, Ottawa,
Ontario, Canada K1A 0R6. E-mail: peter.turney@nrc-cnrc.gc.ca.
Submission received: 30 March 2005; revised submission received: 10 November 2005; accepted for
publication: 27 February 2006.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
relational similarity can be reduced to attributional similarity, since mason and carpen-
ter are attributionally similar, as are stone and wood. In general, this reduction fails.
Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally
similar. Street and riverbed are only moderately attributionally similar.
Many algorithms have been proposed for measuring the attributional similar-
ity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang
and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and
Pedersen 2003). Measures of attributional similarity have been studied extensively, due
to their applications in problems such as recognizing synonyms (Landauer and Dumais
1997), information retrieval (Deerwester et al 1990), determining semantic orientation
(Turney 2002), grading student essays (Rehder et al 1998), measuring textual cohesion
(Morris and Hirst 1991), and word sense disambiguation (Lesk 1986).
On the other hand, since measures of relational similarity are not as well developed
as measures of attributional similarity, the potential applications of relational similarity
are not as well known. Many problems that involve semantic relations would benefit
from an algorithm for measuring relational similarity. We discuss related problems in
natural language processing, information retrieval, and information extraction in more
detail in Section 3.
This article builds on the Vector Space Model (VSM) of information retrieval. Given
a query, a search engine produces a ranked list of documents. The documents are
ranked in order of decreasing attributional similarity between the query and each
document. Almost all modern search engines measure attributional similarity using
the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the
VSM approach to measuring relational similarity. They used a vector of frequencies of
patterns in a corpus to represent the relation between a pair of words. Section 4 presents
the VSM approach to measuring similarity.
In Section 5, we present an algorithm for measuring relational similarity, which
we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus
of unlabeled, unstructured text, without supervision. LRA extends the VSM approach
of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived
automatically from the corpus, instead of using a fixed set of patterns. (2) Singular Value
Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such
as traffic:street, LRA considers transformations of the word pair, generated by replacing
one of the words by synonyms, such as traffic:road or traffic:highway.
Section 6 presents our experimental evaluation of LRA with a collection of 374
multiple-choice word analogy questions from the SAT college entrance exam.1 An ex-
ample of a typical SAT question appears in Table 1. In the educational testing literature,
the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the
solution and the incorrect choices are distractors. We evaluate LRA by testing its ability
to select the solution and avoid the distractors. The average performance of college-
bound senior high school students on verbal SAT questions corresponds to an accuracy
of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the
VSM attained 47%.
1 The College Board eliminated analogies from the SAT in 2005, apparently because it was believed that
analogy questions discriminate against minorities, although it has been argued by liberals (Goldenberg
2005) that dropping analogy questions has increased discrimination against minorities and by
conservatives (Kurtz 2002) that it has decreased academic standards. Analogy questions remain an
important component in many other tests, such as the GRE.
380
Turney Similarity of Semantic Relations
Table 1
An example of a typical SAT question, from the collection of 374 questions.
Stem: mason:stone
Choices: (a) teacher:chalk
(b) carpenter:wood
(c) soldier:gun
(d) photograph:camera
(e) book:word
Solution: (b) carpenter:wood
One application for relational similarity is classifying semantic relations in noun-
modifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance
of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003).
The problem is to classify a noun-modifier pair, such as ?laser printer,? according to
the semantic relation between the head noun (printer) and the modifier (laser). The
600 pairs have been manually labeled with 30 classes of semantic relations. For example,
?laser printer? is classified as instrument; the printer uses the laser as an instrument for
printing.
We approach the task of classifying semantic relations in noun-modifier pairs as a
supervised learning problem. The 600 pairs are divided into training and testing sets
and a testing pair is classified according to the label of its single nearest neighbor in the
training set. LRA is used to measure distance (i.e., similarity, nearness). LRA achieves
an accuracy of 39.8% on the 30-class problem and 58.0% on the 5-class problem. On the
same 600 noun-modifier pairs, the VSM had accuracies of 27.8% (30-class) and 45.7%
(5-class) (Turney and Littman 2005).
We discuss the experimental results, limitations of LRA, and future work in Sec-
tion 8 and we conclude in Section 9.
2. Attributional and Relational Similarity
In this section, we explore connections between attributional and relational similarity.
2.1 Types of Similarity
Medin, Goldstone, and Gentner (1990) distinguish attributes and relations as follows:
Attributes are predicates taking one argument (e.g., X is red, X is large), whereas
relations are predicates taking two or more arguments (e.g., X collides with Y, X is
larger than Y). Attributes are used to state properties of objects; relations express
relations between objects or propositions.
Gentner (1983) notes that what counts as an attribute or a relation can depend on the
context. For example, large can be viewed as an attribute of X, LARGE(X), or a relation
between X and some standard Y, LARGER THAN(X, Y).
The amount of attributional similarity between two words, A and B, depends
on the degree of correspondence between the properties of A and B. A measure of
attributional similarity is a function that maps two words, A and B, to a real number,
381
Computational Linguistics Volume 32, Number 3
sima(A, B) ? . The more correspondence there is between the properties of A and B,
the greater their attributional similarity. For example, dog and wolf have a relatively
high degree of attributional similarity.
The amount of relational similarity between two pairs of words, A:B and C:D,
depends on the degree of correspondence between the relations between A and B
and the relations between C and D. A measure of relational similarity is a function
that maps two pairs, A:B and C:D, to a real number, simr(A :B, C :D) ? . The more
correspondence there is between the relations of A:B and C:D, the greater their relational
similarity. For example, dog:bark and cat:meow have a relatively high degree of relational
similarity.
Cognitive scientists distinguish words that are semantically associated (bee?honey)
from words that are semantically similar (deer?pony), although they recognize that some
words are both associated and similar (doctor?nurse) (Chiarello et al 1990). Both of these
are types of attributional similarity, since they are based on correspondence between
attributes (e.g., bees and honey are both found in hives; deer and ponies are both
mammals).
Budanitsky and Hirst (2001) describe semantic relatedness as follows:
Recent research on the topic in computational linguistics has emphasized the
perspective of semantic relatedness of two lexemes in a lexical resource, or its inverse,
semantic distance. It?s important to note that semantic relatedness is a more general
concept than similarity; similar entities are usually assumed to be related by virtue of
their likeness (bank?trust company), but dissimilar entities may also be semantically
related by lexical relationships such as meronymy (car?wheel) and antonymy (hot?cold),
or just by any kind of functional relationship or frequent association (pencil?paper,
penguin?Antarctica).
As these examples show, semantic relatedness is the same as attributional similarity
(e.g., hot and cold are both kinds of temperature, pencil and paper are both used for
writing). Here we prefer to use the term attributional similarity because it emphasizes the
contrast with relational similarity. The term semantic relatedness may lead to confusion
when the term relational similarity is also under discussion.
Resnik (1995) describes semantic similarity as follows:
Semantic similarity represents a special case of semantic relatedness: for example, cars
and gasoline would seem to be more closely related than, say, cars and bicycles, but the
latter pair are certainly more similar. Rada et al (1989) suggest that the assessment of
similarity in semantic networks can in fact be thought of as involving just taxonomic
(IS-A) links, to the exclusion of other link types; that view will also be taken here,
although admittedly it excludes some potentially useful information.
Thus semantic similarity is a specific type of attributional similarity. The term semantic
similarity is misleading, because it refers to a type of attributional similarity, yet rela-
tional similarity is not any less semantic than attributional similarity.
To avoid confusion, we will use the terms attributional similarity and relational
similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic sim-
ilarity (Resnik 1995) or semantically similar (Chiarello et al 1990), we prefer the term
taxonomical similarity, which we take to be a specific type of attributional similarity. We
interpret synonymy as a high degree of attributional similarity. Analogy is a high degree
of relational similarity.
382
Turney Similarity of Semantic Relations
2.2 Measuring Attributional Similarity
Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986;
Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969;
Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik
1995; Jiang and Conrath 1997; Turney et al 2003). Intuitively, we might expect
that lexicon-based algorithms would be better at capturing synonymy than corpus-
based algorithms, since lexicons, such as WordNet, explicitly provide synonymy in-
formation that is only implicit in a corpus. However, experiments do not support this
intuition.
Several algorithms have been evaluated using 80 multiple-choice synonym ques-
tions taken from the Test of English as a Foreign Language (TOEFL). An example of
one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance
on the TOEFL questions for each type of attributional similarity algorithm. The results
support the claim that lexicon-based algorithms have no advantage over corpus-based
algorithms for recognizing synonymy.
2.3 Using Attributional Similarity to Solve Analogies
We may distinguish near analogies (mason:stone::carpenter:wood) from far anal-
ogies (traffic:street::water:riverbed) (Gentner 1983; Medin, Goldstone, and Gentner 1990).
In an analogy A:B::C:D, where there is a high degree of relational similarity between
A:B and C:D, if there is also a high degree of attributional similarity between A
and C, and between B and D, then A:B::C:D is a near analogy; otherwise, it is a far
analogy.
It seems possible that SAT analogy questions might consist largely of near analogies,
in which case they can be solved using attributional similarity measures. We could score
each candidate analogy by the average of the attributional similarity, sima, between A
and C and between B and D:
score(A :B ::C :D) = 12 (sima(A, C) + sima(B, D)) (1)
This kind of approach was used in two of the thirteen modules in Turney et al (2003)
(see Section 3.1).
Table 2
An example of a typical TOEFL question, from the collection of 80 questions.
Stem: Levied
Choices: (a) imposed
(b) believed
(c) requested
(d) correlated
Solution: (a) imposed
383
Computational Linguistics Volume 32, Number 3
Table 3
Performance of attributional similarity measures on the 80 TOEFL questions. (The average
non-English US college applicant?s performance is included in the bottom row, for comparison.)
Reference Description Percent correct
Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75
Terra and Clarke (2003) Best corpus-based algorithm 81.25
Turney et al (2003) Best hybrid algorithm 97.50
Landauer and Dumais (1997) Average human score 64.50
To evaluate this approach, we applied several measures of attributional similarity to
our collection of 374 SAT questions. The performance of the algorithms was measured
by precision, recall, and F, defined as follows:
precision =
number of correct guesses
total number of guesses made
(2)
recall =
number of correct guesses
maximum possible number correct
(3)
F =
2 ? precision ? recall
precision + recall
(4)
Note that recall is the same as percent correct (for multiple-choice questions, with only
zero or one guesses allowed per question, but not in general).
Table 4 shows the experimental results for our set of 374 analogy questions. For
example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered
correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned
the same similarity to all of the choices for a given question, that question was skipped.
The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30).
The first five algorithms in Table 4 are implemented in Pedersen?s WordNet-
Similarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText
System (WMTS), as described in Terra and Clarke (2003).
The difference between the lowest performance (Jiang and Conrath 1997) and ran-
dom guessing is statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990). However, the difference between the highest performance
(Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically
significant with 95% confidence. We conclude that there are enough near analogies
in the 374 SAT questions for attributional similarity to perform better than random
guessing, but not enough near analogies for attributional similarity to perform as well
as relational similarity.
2 See http://www.d.umn.edu/?tpederse/similarity.html.
384
Turney Similarity of Semantic Relations
3. Related Work
This section is a brief survey of the many problems that involve semantic relations and
could potentially make use of an algorithm for measuring relational similarity.
3.1 Recognizing Word Analogies
The problem of recognizing word analogies is, given a stem word pair and a finite list of
choice word pairs, selecting the choice that is most analogous to the stem. This problem
was first attempted by a system called Argus (Reitman 1965), using a small hand-built
semantic network. Argus could only solve the limited set of analogy questions that its
programmer had anticipated. Argus was based on a spreading activation model and
did not explicitly attempt to measure relational similarity.
Turney et al (2003) combined 13 independent modules to answer SAT questions.
The final output of the system was based on a weighted combination of the outputs of
each individual module. The best of the 13 modules was the VSM, which is described
in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT
questions, achieving a score of 47%.
In contrast with the corpus-based approach of Turney and Littman (2005), Veale
(2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score
of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths
in WordNet, joining A to B and C to D. The quality measure was based on the similarity
between the A:B paths and the C:D paths.
Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version
of the VSM approach, which reached 56% on the 374 SAT questions. Here we go
beyond Turney (2005) by describing LRA in more detail, performing more extensive
experiments, and analyzing the algorithm and related work in more depth.
3.2 Structure Mapping Theory
French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its imple-
mentation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner
1989) as the most influential work on modeling of analogy making. The goal of com-
putational modeling of analogy making is to understand how people form complex,
Table 4
Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and
F are reported as percentages. (The bottom two rows are not attributional similarity measures.
They are included for comparison.)
Algorithm Type Precision Recall F
Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4
Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5
Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0
Lin (1998b) Hybrid 31.2 27.3 29.1
Resnik (1995) Hybrid 35.7 33.2 34.4
Turney (2001) Corpus-based 35.0 35.0 35.0
Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4
Random Random 20.0 20.0 20.0
385
Computational Linguistics Volume 32, Number 3
structured analogies. SME takes representations of a source domain and a target domain
and produces an analogical mapping between the source and target. The domains
are given structured propositional representations, using predicate logic. These de-
scriptions include attributes, relations, and higher-order relations (expressing relations
between relations). The analogical mapping connects source domain relations to target
domain relations.
For example, there is an analogy between the solar system and Rutherford?s model
of the atom (Falkenhainer, Forbus, and Gentner 1989). The solar system is the source
domain and Rutherford?s model of the atom is the target domain. The basic objects in
the source model are the planets and the sun. The basic objects in the target model are
the electrons and the nucleus. The planets and the sun have various attributes, such
as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and
attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as
charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus)
and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nu-
cleus) and attracts(sun, planet) to attracts(nucleus, electron).
Each individual connection (e.g., from revolve(planet, sun) to revolve(electron, nu-
cleus)) in an analogical mapping implies that the connected relations are similar; thus,
SMT requires a measure of relational similarity in order to form maps. Early versions
of SME only mapped identical relations, but later versions of SME allowed similar,
nonidentical relations to match (Falkenhainer 1990). However, the focus of research in
analogy making has been on the mapping process as a whole, rather than measuring
the similarity between any two particular relations; hence, the similarity measures used
in SME at the level of individual connections are somewhat rudimentary.
We believe that a more sophisticated measure of relational similarity, such as LRA,
may enhance the performance of SME. Likewise, the focus of our work here is on the
similarity between particular relations, and we ignore systematic mapping between sets
of relations, so LRA may also be enhanced by integration with SME.
3.3 Metaphor
Metaphorical language is very common in our daily life, so common that we are usually
unaware of it (Lakoff and Johnson 1980). Gentner et al (2001) argue that novel metaphors
are understood using analogy, but conventional metaphors are simply recalled from
memory. A conventional metaphor is a metaphor that has become entrenched in our
language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can
recognize conventional metaphors, but is not suited to novel metaphors. This suggests
that it may be fruitful to combine Dolan?s (1995) algorithm for handling conventional
metaphorical language with LRA and SME for handling novel metaphors.
Lakoff and Johnson (1980) give many examples of sentences in support of their
claim that metaphorical language is ubiquitous. The metaphors in their sample sen-
tences can be expressed using SAT-style verbal analogies of the form A:B::C:D. The first
column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and the second
column shows how the metaphor that is implicit in each sentence may be made explicit
as a verbal analogy.
3.4 Classifying Semantic Relations
The task of classifying semantic relations is to identify the relation between a pair
of words. Often the pairs are restricted to noun-modifier pairs, but there are many
386
Turney Similarity of Semantic Relations
Table 5
Metaphorical sentences from Lakoff and Johnson (1980), rendered as SAT-style verbal analogies.
Metaphorical sentence SAT-style verbal analogy
He shot down all of my arguments. aircraft:shoot down::argument:refute
I demolished his argument. building:demolish::argument:refute
You need to budget your time. money:budget::time:schedule
I?ve invested a lot of time in her. money:invest::time:allocate
My mind just isn?t operating today. machine:operate::mind:think
Life has cheated me. charlatan:cheat::life:disappoint
Inflation is eating up our profits. animal:eat::inflation:reduce
interesting relations, such as antonymy, that do not occur in noun-modifier pairs. How-
ever, noun-modifier pairs are interesting due to their high frequency in English. For
instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many
common noun-modifiers are not in WordNet, especially technical terms.
Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify noun-
modifier relations in the medical domain, using Medical Subject Headings (MeSH) and
Unified Medical Language System (UMLS) as lexical resources for representing each
noun-modifier pair with a feature vector. They trained a neural network to distinguish
13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar ap-
proach to classifying general noun-modifier pairs (i.e., not restricted to a particular
domain, such as medicine), using WordNet and Roget?s Thesaurus as lexical resources.
Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to
classify noun-modifier pairs.
None of these approaches explicitly involved measuring relational similarity, but
any classification of semantic relations necessarily employs some implicit notion of re-
lational similarity since members of the same class must be relationally similar to some
extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used
a measure of relational similarity, but their measure was based on literal matching,
which limited its ability to generalize. Moldovan et al (2004) also used a measure of
relational similarity based on mapping each noun and modifier into semantic classes
in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding
context in the corpus was used in a word sense disambiguation algorithm to improve
the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used
the VSM (as a component in a single nearest neighbor learning algorithm) to measure
relational similarity. We take the same approach here, substituting LRA for the VSM, in
Section 7.
Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun?
modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about. For
example, reptile haven was paraphrased as haven for reptiles. Lapata and Keller (2004)
achieved improved results on this task by using the database of AltaVista?s search
engine as a corpus.
3.5 Word Sense Disambiguation
We believe that the intended sense of a polysemous word is determined by its semantic
relations with the other words in the surrounding text. If we can identify the semantic
387
Computational Linguistics Volume 32, Number 3
relations between the given word and its context, then we can disambiguate the given
word. Yarowsky?s (1993) observation that collocations are almost always monosemous
is evidence for this view. Federici, Montemagni, and Pirrelli (1997) present an analogy-
based approach to word sense disambiguation.
For example, consider the word plant. Out of context, plant could refer to an in-
dustrial plant or a living organism. Suppose plant appears in some text near food. A
typical approach to disambiguating plant would compare the attributional similarity
of food and industrial plant to the attributional similarity of food and living organism
(Lesk 1986; Banerjee and Pedersen 2003). In this case, the decision may not be clear,
since industrial plants often produce food and living organisms often serve as food. It
would be very helpful to know the relation between food and plant in this example. In
the phrase ?food for the plant,? the relation between food and plant strongly suggests
that the plant is a living organism, since industrial plants do not need food. In the
text ?food at the plant,? the relation strongly suggests that the plant is an industrial
plant, since living organisms are not usually considered as locations. Thus, an algorithm
for classifying semantic relations (as in Section 7) should be helpful for word sense
disambiguation.
3.6 Information Extraction
The problem of relation extraction is, given an input document and a specific relation R,
to extract all pairs of entities (if any) that have the relation R in the document. The prob-
lem was introduced as part of the Message Understanding Conferences (MUC) in 1998.
Zelenko, Aone, and Richardella (2003) present a kernel method for extracting the
relations person?affiliation and organization?location. For example, in the sentence John
Smith is the chief scientist of the Hardcom Corporation, there is a person?affiliation relation
between John Smith and Hardcom Corporation (Zelenko, Aone, and Richardella 2003).
This is similar to the problem of classifying semantic relations (Section 3.4), except
that information extraction focuses on the relation between a specific pair of entities
in a specific document, rather than a general pair of words in general text. There-
fore an algorithm for classifying semantic relations should be useful for information
extraction.
In the VSM approach to classifying semantic relations (Turney and Littman 2005),
we would have a training set of labeled examples of the relation person?affiliation,
for instance. Each example would be represented by a vector of pattern frequencies.
Given a specific document discussing John Smith and Hardcom Corporation, we could
construct a vector representing the relation between these two entities and then mea-
sure the relational similarity between this unlabeled vector and each of our labeled
training vectors. It would seem that there is a problem here because the training
vectors would be relatively dense, since they would presumably be derived from
a large corpus, but the new unlabeled vector for John Smith and Hardcom Corpora-
tion would be very sparse, since these entities might be mentioned only once in the
given document. However, this is not a new problem for the VSM; it is the standard
situation when the VSM is used for information retrieval. A query to a search en-
gine is represented by a very sparse vector, whereas a document is represented by
a relatively dense vector. There are well-known techniques in information retrieval
for coping with this disparity, such as weighting schemes for query vectors that
are different from the weighting schemes for document vectors (Salton and Buckley
1988).
388
Turney Similarity of Semantic Relations
3.7 Question Answering
In their article on classifying semantic relations, Moldovan et al (2004) suggest that an
important application of their work is question answering (QA). As defined in the Text
Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as
?Where have nuclear incidents occurred??, by retrieving a relevant document from a
large corpus and then extracting a short string from the document, such as The Three
Mile Island nuclear incident caused a DOE policy crisis. Moldovan et al (2004) propose to
map a given question to a semantic relation and then search for that relation in a corpus
of semantically tagged text. They argue that the desired semantic relation can easily be
inferred from the surface form of the question. A question of the form ?Where. . . ?? is
likely to be looking for entities with a location relation and a question of the form ?What
did ... make?? is likely to be looking for entities with a product relation. In Section 7, we
show how LRA can recognize relations such as location and product (see Table 19).
3.8 Automatic Thesaurus Generation
Hearst (1992) presents an algorithm for learning hyponym (type of ) relations from a
corpus and Berland and Charniak (1999) describe how to learn meronym (part of )
relations from a corpus. These algorithms could be used to automatically generate a
thesaurus or dictionary, but we would like to handle more relations than hyponymy
and meronymy. WordNet distinguishes more than a dozen semantic relations between
words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for
noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated
rules to mine text for semantic relations. Turney and Littman (2005) also use a manually
generated set of 64 patterns.
LRA does not use a predefined set of patterns; it learns patterns from a large corpus.
Instead of manually generating new rules or patterns for each new semantic relation,
it is possible to automatically learn a measure of relational similarity that can handle
arbitrary semantic relations. A nearest neighbor algorithm can then use this relational
similarity measure to learn to classify according to any set of classes of relations, given
the appropriate labeled training data.
Girju, Badulescu, and Moldovan (2003) present an algorithm for learning meronym
relations from a corpus. Like Hearst (1992) and Berland and Charniak (1999), they
use manually generated rules to mine text for their desired relation. However, they
supplement their manual rules with automatically learned constraints, to increase the
precision of the rules.
3.9 Information Retrieval
Veale (2003) has developed an algorithm for recognizing certain types of word analo-
gies, based on information in WordNet. He proposes to use the algorithm for analog-
ical information retrieval. For example, the query Muslim church should return
mosque and the query Hindu bible should return the Vedas. The algorithm was de-
signed with a focus on analogies of the form adjective:noun::adjective:noun, such as
Christian:church::Muslim:mosque.
A measure of relational similarity is applicable to this task. Given a pair of words,
A and B, the task is to return another pair of words, X and Y, such that there is
high relational similarity between the pair A:X and the pair Y:B. For example, given
389
Computational Linguistics Volume 32, Number 3
A = Muslim and B = church, return X = mosque and Y = Christian. (The pair Muslim:mosque
has a high relational similarity to the pair Christian:church.)
Marx et al (2002) developed an unsupervised algorithm for discovering analogies
by clustering words from two different corpora. Each cluster of words in one corpus
is coupled one-to-one with a cluster in the other corpus. For example, one experiment
used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of
words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with
a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the
algorithm appears to have discovered an analogical mapping between Buddhist schools
and traditions and Christian schools and traditions. This is interesting work, but it is not
directly applicable to SAT analogies, because it discovers analogies between clusters of
words rather than individual words.
3.10 Identifying Semantic Roles
A semantic frame for an event such as judgement contains semantic roles such as judge,
evaluee, and reason, whereas an event such as statement contains roles such as speaker,
addressee, and message (Gildea and Jurafsky 2002). The task of identifying semantic roles
is to label the parts of a sentence according to their semantic roles. We believe that it
may be helpful to view semantic frames and their semantic roles as sets of semantic
relations; thus, a measure of relational similarity should help us to identify semantic
roles. Moldovan et al (2004) argue that semantic roles are merely a special case of
semantic relations (Section 3.4), since semantic roles always involve verbs or predicates,
but semantic relations can involve words of any part of speech.
4. The Vector Space Model
This section examines past work on measuring attributional and relational similarity
using the VSM.
4.1 Measuring Attributional Similarity with the Vector Space Model
The VSM was first developed for information retrieval (Salton and McGill 1983; Salton
and Buckley 1988; Salton 1989) and it is at the core of most modern search engines
(Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval,
queries and documents are represented by vectors. Elements in these vectors are based
on the frequencies of words in the corresponding queries and documents. The frequen-
cies are usually transformed by various formulas and weights, tailored to improve the
effectiveness of the search engine (Salton 1989). The attributional similarity between a
query and a document is measured by the cosine of the angle between their correspond-
ing vectors. For a given query, the search engine sorts the matching documents in order
of decreasing cosine.
The VSM approach has also been used to measure the attributional similarity of
words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered
words according to their attributional similarity, as measured by a VSM. Their algo-
rithm is able to discover the different senses of polysemous words, using unsupervised
learning.
Latent Semantic Analysis enhances the VSM approach to information retrieval by
using the Singular Value Decomposition (SVD) to smooth the vectors, which helps
390
Turney Similarity of Semantic Relations
to handle noise and sparseness in the data (Deerwester et al 1990; Dumais 1993;
Landauer and Dumais 1997). SVD improves both document-query attributional sim-
ilarity measures (Deerwester et al 1990; Dumais 1993) and word?word attributional
similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vec-
tors, as we discuss in Section 5.
4.2 Measuring Relational Similarity with the Vector Space Model
Let R1 be the semantic relation (or set of relations) between a pair of words, A and B,
and let R2 be the semantic relation (or set of relations) between another pair, C and D.
We wish to measure the relational similarity between R1 and R2. The relations R1 and R2
are not given to us; our task is to infer these hidden (latent) relations and then compare
them.
In the VSM approach to relational similarity (Turney and Littman 2005), we create
vectors, r1 and r2, that represent features of R1 and R2, and then measure the similarity
of R1 and R2 by the cosine of the angle ? between r1 and r2:
r1 = ?r1,1, . . . , r1,n? (5)
r2 = ?r2,1, . . . r2,n? (6)
cosine(?) =
n
?
i=1
r1,i ? r2,i
?
n
?
i=1
(r1,i)2 ?
n
?
i=1
(r2,i)2
=
r1 ? r2?
r1 ? r1 ?
?
r2 ? r2
=
r1 ? r2
?r1? ? ?r2?
(7)
We create a vector, r, to characterize the relationship between two words, X and Y,
by counting the frequencies of various short phrases containing X and Y. Turney and
Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases
that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These
phrases are then used as queries for a search engine and the number of hits (matching
documents) is recorded for each query. This process yields a vector of 128 numbers.
If the number of hits for a query is x, then the corresponding element in the vector r
is log(x + 1). Several authors report that the logarithmic transformation of frequencies
improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin
1998b).
Turney and Littman (2005) evaluated the VSM approach by its performance on 374
SAT analogy questions, achieving a score of 47%. Since there are five choices for each
question, the expected score for random guessing is 20%. To answer a multiple-choice
analogy question, vectors are created for the stem pair and each choice pair, and then
cosines are calculated for the angles between the stem pair and each choice pair. The
best guess is the choice pair with the highest cosine. We use the same set of analogy
questions to evaluate LRA in Section 6.
391
Computational Linguistics Volume 32, Number 3
The VSM was also evaluated by its performance as a distance (nearness) measure
in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney
and Littman 2005). The evaluation used 600 hand-labeled noun-modifier pairs from
Nastase and Szpakowicz (2003). A testing pair is classified by searching for its single
nearest neighbor in the labeled training data. The best guess is the label for the training
pair with the highest cosine. LRA is evaluated with the same set of noun-modifier pairs
in Section 7.
Turney and Littman (2005) used the AltaVista search engine to obtain the frequency
information required to build vectors for the VSM. Thus their corpus was the set of all
Web pages indexed by AltaVista. At the time, the English subset of this corpus consisted
of about 5 ? 1011 words. Around April 2004, AltaVista made substantial changes to
their search engine, removing their advanced search operators. Their search engine no
longer supports the asterisk operator, which was used by Turney and Littman (2005)
for stemming and wild-card searching. AltaVista also changed their policy toward
automated searching, which is now forbidden.3
Turney and Littman (2005) used AltaVista?s hit count, which is the number of
documents (Web pages) matching a given query, but LRA uses the number of passages
(strings) matching a query. In our experiments with LRA (Sections 6 and 7), we use a lo-
cal copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998;
Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about
5 ? 1010 English words. The WMTS is a distributed (multiprocessor) search engine,
designed primarily for passage retrieval (although document retrieval is possible, as a
special case of passage retrieval). The text and index require approximately one terabyte
of disk space. Although AltaVista only gives a rough estimate of the number of match-
ing documents, the WMTS gives exact counts of the number of matching passages.
Turney et al (2003) combine 13 independent modules to answer SAT questions. The
performance of LRA significantly surpasses this combined system, but there is no real
contest between these approaches, because we can simply add LRA to the combination,
as a fourteenth module. Since the VSM module had the best performance of the 13
modules (Turney et al 2003), the following experiments focus on comparing VSM and
LRA.
5. Latent Relational Analysis
LRA takes as input a set of word pairs and produces as output a measure of the
relational similarity between any two of the input pairs. LRA relies on three resources, a
search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms,
and an efficient implementation of SVD.
We first present a short description of the core algorithm. Later, in the following
subsections, we will give a detailed description of the algorithm, as it is applied in the
experiments in Sections 6 and 7.
 Given a set of word pairs as input, look in a thesaurus for synonyms for
each word in each word pair. For each input pair, make alternate pairs by
replacing the original words with their synonyms. The alternate pairs are
3 See http://www.altavista.com/robots.txt for AltaVista?s current policy toward ?robots? (software for
automatically gathering Web pages or issuing batches of queries). The protocol of the ?robots.txt? file is
explained in http://www.robotstxt.org/wc/robots.html.
392
Turney Similarity of Semantic Relations
intended to form near analogies with the corresponding original pairs (see
Section 2.3).
 Filter out alternate pairs that do not form near analogies by dropping
alternate pairs that co-occur rarely in the corpus. In the preceding step, if a
synonym replaced an ambiguous original word, but the synonym captures
the wrong sense of the original word, it is likely that there is no significant
relation between the words in the alternate pair, so they will rarely
co-occur.
 For each original and alternate pair, search in the corpus for short phrases
that begin with one member of the pair and end with the other. These
phrases characterize the relation between the words in each pair.
 For each phrase from the previous step, create several patterns, by
replacing words in the phrase with wild cards.
 Build a pair?pattern frequency matrix, in which each cell represents the
number of times that the corresponding pair (row) appears in the corpus
with the corresponding pattern (column). The number will usually be
zero, resulting in a sparse matrix.
 Apply the SVD to the matrix. This reduces noise in the matrix and helps
with sparse data.
 Suppose that we wish to calculate the relational similarity between any
two of the original pairs. Start by looking for the two row vectors in the
pair?pattern frequency matrix that correspond to the two original pairs.
Calculate the cosine of the angle between these two row vectors. Then
merge the cosine of the two original pairs with the cosines of their
corresponding alternate pairs, as follows. If an analogy formed with
alternate pairs has a higher cosine than the original pairs, we assume that
we have found a better way to express the analogy, but we have not
significantly changed its meaning. If the cosine is lower, we assume that
we may have changed the meaning by inappropriately replacing words
with synonyms. Filter out inappropriate alternates by dropping all
analogies formed of alternates, such that the cosines are less than the
cosine for the original pairs. The relational similarity between the two
original pairs is then calculated as the average of all of the remaining
cosines.
The motivation for the alternate pairs is to handle cases where the original pairs co-
occur rarely in the corpus. The hope is that we can find near analogies for the original
pairs, such that the near analogies co-occur more frequently in the corpus. The danger
is that the alternates may have different relations from the originals. The filtering steps
above aim to reduce this risk.
5.1 Input and Output
In our experiments, the input set contains from 600 to 2,244 word pairs. The output
similarity measure is based on cosines, so the degree of similarity can range from ?1
(dissimilar; ? = 180?) to +1 (similar; ? = 0?). Before applying SVD, the vectors are
393
Computational Linguistics Volume 32, Number 3
completely non-negative, which implies that the cosine can only range from 0 to+1, but
SVD introduces negative values, so it is possible for the cosine to be negative, although
we have never observed this in our experiments.
5.2 Search Engine and Corpus
In the following experiments, we use a local copy of the WMTS (Clarke, Cormack, and
Palmer 1998; Terra and Clarke 2003).4 The corpus consists of about 5 ? 1010 English
words, gathered by a Web crawler, mainly from US academic Web sites. The Web pages
cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is
well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our
case), it gives exact frequency counts (unlike most Web search engines), it is designed for
passage retrieval (rather than document retrieval), and it has a powerful query syntax.
5.3 Thesaurus
As a source of synonyms, we use Lin?s (1998a) automatically generated thesaurus. This
thesaurus is available through an on-line interactive demonstration or it can be down-
loaded.5 We used the on-line demonstration, since the downloadable version seems to
contain fewer words. For each word in the input set of word pairs, we automatically
query the on-line demonstration and fetch the resulting list of synonyms. As a cour-
tesy to other users of Lin?s on-line system, we insert a 20-second delay between each
two queries.
Lin?s thesaurus was generated by parsing a corpus of about 5 ? 107 English words,
consisting of text from the Wall Street Journal, San Jose Mercury, and AP Newswire (Lin
1998a). The parser was used to extract pairs of words and their grammatical relations.
Words were then clustered into synonym sets, based on the similarity of their grammat-
ical relations. Two words were judged to be highly similar when they tended to have
the same kinds of grammatical relations with the same sets of words. Given a word and
its part of speech, Lin?s thesaurus provides a list of words, sorted in order of decreasing
attributional similarity. This sorting is convenient for LRA, since it makes it possible
to focus on words with higher attributional similarity and ignore the rest. WordNet, in
contrast, given a word and its part of speech, provides a list of words grouped by the
possible senses of the given word, with groups sorted by the frequencies of the senses.
WordNet?s sorting does not directly correspond to sorting by degree of attributional
similarity, although various algorithms have been proposed for deriving attributional
similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst
2001; Banerjee and Pedersen 2003).
5.4 Singular Value Decomposition
We use Rohde?s SVDLIBC implementation of the SVD, which is based on SVDPACKC
(Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness.
4 See http://multitext.uwaterloo.ca/.
5 The online demonstration is at http://www.cs.ualberta.ca/?lindek/demos/depsim.htm and the
downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz.
6 SVDLIBC is available at http://tedlab.mit.edu/?dr/SVDLIBC/ and SVDPACKC is available at
http://www.netlib.org/svdpack/.
394
Turney Similarity of Semantic Relations
5.5 The Algorithm
We will go through each step of LRA, using an example to illustrate the steps. Assume
that the input to LRA is the 374 multiple-choice SAT word analogy questions of Turney
and Littman (2005). Since there are six word pairs per question (the stem and five
choices), the input consists of 2,244 word pairs. Let?s suppose that we wish to calculate
the relational similarity between the pair quart:volume and the pair mile:distance, taken
from the SAT question in Table 6. The LRA algorithm consists of the following 12 steps:
1. Find alternates: For each word pair A:B in the input set, look in Lin?s
(1998a) thesaurus for the top num sim words (in the following experiments,
num sim is 10) that are most similar to A. For each A? that is similar to A,
make a new word pair A?:B. Likewise, look for the top num sim words that
are most similar to B, and for each B?, make a new word pair A:B?. A:B is
called the original pair and each A?:B or A:B? is an alternate pair. The intent
is that alternates should have almost the same semantic relations as the
original. For each input pair, there will now be 2 ? num sim alternate pairs.
When looking for similar words in Lin?s (1998a) thesaurus, avoid words
that seem unusual (e.g., hyphenated words, words with three characters or
less, words with non-alphabetical characters, multiword phrases, and
capitalized words). The first column in Table 7 shows the alternate pairs
that are generated for the original pair quart:volume.
2. Filter alternates: For each original pair A:B, filter the 2 ? num sim
alternates as follows. For each alternate pair, send a query to the WMTS, to
find the frequency of phrases that begin with one member of the pair and
end with the other. The phrases cannot have more than max phrase words
(we use max phrase = 5). Sort the alternate pairs by the frequency of their
phrases. Select the top num filter most frequent alternates and discard the
remainder (we use num filter = 3, so 17 alternates are dropped). This step
tends to eliminate alternates that have no clear semantic relation. The third
column in Table 7 shows the frequency with which each pair co-occurs in a
window of max phrase words. The last column in Table 7 shows the pairs
that are selected.
3. Find phrases: For each pair (originals and alternates), make a list of
phrases in the corpus that contain the pair. Query the WMTS for all
phrases that begin with one member of the pair and end with the other
(in either order). We ignore suffixes when searching for phrases that match
Table 6
This SAT question, from Claman (2000), is used to illustrate the steps in the LRA algorithm.
Stem: quart:volume
Choices: (a) day:night
(b) mile:distance
(c) decade:century
(d) friction:heat
(e) part:whole
Solution: (b) mile:distance
395
Computational Linguistics Volume 32, Number 3
Table 7
Alternate forms of the original pair quart:volume. The first column shows the original pair and
the alternate pairs. The second column shows Lin?s similarity score for the alternate word
compared to the original word. For example, the similarity between quart and pint is 0.210. The
third column shows the frequency of the pair in the WMTS corpus. The fourth column shows the
pairs that pass the filtering step (i.e., step 2).
Word pair Similarity Frequency Filtering step
quart:volume NA 632 Accept (original pair)
pint:volume 0.210 372
gallon:volume 0.159 1500 Accept (top alternate)
liter:volume 0.122 3323 Accept (top alternate)
squirt:volume 0.084 54
pail:volume 0.084 28
vial:volume 0.084 373
pumping:volume 0.073 1386 Accept (top alternate)
ounce:volume 0.071 430
spoonful:volume 0.070 42
tablespoon:volume 0.069 96
quart:turnover 0.229 0
quart:output 0.225 34
quart:export 0.206 7
quart:value 0.203 266
quart:import 0.186 16
quart:revenue 0.185 0
quart:sale 0.169 119
quart:investment 0.161 11
quart:earnings 0.156 0
quart:profit 0.156 24
a given pair. The phrases cannot have more than max phrase words and
there must be at least one word between the two members of the word
pair. These phrases give us information about the semantic relations
between the words in each pair. A phrase with no words between the two
members of the word pair would give us very little information about the
semantic relations (other than that the words occur together with a certain
frequency in a certain order). Table 8 gives some examples of phrases in
the corpus that match the pair quart:volume.
4. Find patterns: For each phrase found in the previous step, build patterns
from the intervening words. A pattern is constructed by replacing any or
all or none of the intervening words with wild cards (one wild card can
Table 8
Some examples of phrases that contain quart:volume. Suffixes are ignored when searching for
matching phrases in the WMTS corpus. At least one word must occur between quart and
volume. At most max phrase words can appear in a phrase.
quarts liquid volume volume in quarts
quarts of volume volume capacity quarts
quarts in volume volume being about two quarts
quart total volume volume of milk in quarts
quart of spray volume volume include measures like quart
396
Turney Similarity of Semantic Relations
replace only one word). If a phrase is n words long, there are n ? 2
intervening words between the members of the given word pair (e.g.,
between quart and volume). Thus a phrase with n words generates 2(n?2)
patterns. (We use max phrase = 5, so a phrase generates at most eight
patterns.) For each pattern, count the number of pairs (originals and
alternates) with phrases that match the pattern (a wild card must match
exactly one word). Keep the top num patterns most frequent patterns and
discard the rest (we use num patterns = 4, 000). Typically there will be
millions of patterns, so it is not feasible to keep them all.
5. Map pairs to rows: In preparation for building the matrix X, create a
mapping of word pairs to row numbers. For each pair A:B, create a row for
A:B and another row for B:A. This will make the matrix more symmetrical,
reflecting our knowledge that the relational similarity between A:B and
C:D should be the same as the relational similarity between B:A and D:C.
This duplication of rows is examined in Section 6.6.
6. Map patterns to columns: Create a mapping of the top num patterns
patterns to column numbers. For each pattern P, create a column for
?word1 P word2? and another column for ?word2 P word1.? Thus there will
be 2 ? num patterns columns in X. This duplication of columns is
examined in Section 6.6.
7. Generate a sparse matrix: Generate a matrix X in sparse matrix format,
suitable for input to SVDLIBC. The value for the cell in row i and column j
is the frequency of the jth pattern (see step 6) in phrases that contain the
ith word pair (see step 5). Table 9 gives some examples of pattern
frequencies for quart:volume.
8. Calculate entropy: Apply log and entropy transformations to the sparse
matrix (Landauer and Dumais 1997). These transformations have been
found to be very helpful for information retrieval (Harman 1986; Dumais
1990). Let xi,j be the cell in row i and column j of the matrix X from step 7.
Let m be the number of rows in X and let n be the number of columns. We
wish to weight the cell xi,j by the entropy of the jth column. To calculate
the entropy of the column, we need to convert the column into a vector of
probabilities. Let pi,j be the probability of xi,j, calculated by normalizing the
column vector so that the sum of the elements is one, pi,j = xi,j/
?m
k=1 xk,j.
The entropy of the jth column is then Hj = ?
?m
k=1 pk,j log(pk,j). Entropy is
at its maximum when pi,j is a uniform distribution, pi,j = 1/m, in which
case Hj = log(m). Entropy is at its minimum when pi,j is 1 for some value
of i and 0 for all other values of i, in which case Hj = 0. We want to give
Table 9
Frequencies of various patterns for quart:volume. The asterisk ?*? represents the wildcard.
Suffixes are ignored, so quart matches quarts. For example, quarts in volume is one
of the four phrases that match quart P volume when P is in.
P = ?in? P = ?* of? P = ?of *? P = ?* *?
freq(?quart P volume?) 4 1 5 19
freq(?volume P quart?) 10 0 2 16
397
Computational Linguistics Volume 32, Number 3
more weight to columns (patterns) with frequencies that vary substantially
from one row (word pair) to the next, and less weight to columns that
are uniform. Therefore we weight the cell xi,j by wj = 1 ? Hj/ log(m),
which varies from 0 when pi,j is uniform to 1 when entropy is minimal.
We also apply the log transformation to frequencies, log(xi,j + 1).
(Entropy is calculated with the original frequency values, before the
log transformation is applied.) For all i and all j, replace the original
value xi,j in X by the new value wj log(xi,j + 1). This is an instance of the
Term Frequency-Inverse Document Frequency (TF-IDF) family of
transformations, which is familiar in information retrieval (Salton and
Buckley 1988; Baeza-Yates and Ribeiro-Neto 1999): log(xi,j + 1) is the TF
term and wj is the IDF term.
9. Apply SVD: After the log and entropy transformations have been applied
to the matrix X, run SVDLIBC. SVD decomposes a matrix X into a product
of three matrices U?VT, where U and V are in column orthonormal form
(i.e., the columns are orthogonal and have unit length: UTU = VTV = I)
and ? is a diagonal matrix of singular values (hence SVD) (Golub and Van
Loan 1996). If X is of rank r, then ? is also of rank r. Let ?k, where k < r,
be the diagonal matrix formed from the top k singular values, and let Uk
and Vk be the matrices produced by selecting the corresponding columns
from U and V. The matrix Uk?kVTk is the matrix of rank k that best
approximates the original matrix X, in the sense that it minimizes the
approximation errors. That is, X? = Uk?kVTk minimizes
?
?X? ? X
?
?
F over all
matrices X? of rank k, where ?. . .?F denotes the Frobenius norm (Golub and
Van Loan 1996). We may think of this matrix Uk?kVTk as a ?smoothed? or
?compressed? version of the original matrix. In the subsequent steps, we
will be calculating cosines for row vectors. For this purpose, we can
simplify calculations by dropping V. The cosine of two vectors is
their dot product, after they have been normalized to unit length. The
matrix XXT contains the dot products of all of the row vectors. We
can find the dot product of the ith and jth row vectors by looking at
the cell in row i, column j of the matrix XXT. Since VTV = I, we have
XXT = U?VT(U?VT )T = U?VTV?TUT = U?(U?)T, which means that
we can calculate cosines with the smaller matrix U?, instead of using
X = U?VT (Deerwester et al 1990).
10. Projection: Calculate Uk?k (we use k = 300). This matrix has the same
number of rows as X, but only k columns (instead of 2 ? num patterns
columns; in our experiments, that is 300 columns instead of 8,000). We can
compare two word pairs by calculating the cosine of the corresponding
row vectors in Uk?k. The row vector for each word pair has been projected
from the original 8,000 dimensional space into a new 300 dimensional
space. The value k = 300 is recommended by Landauer and Dumais (1997)
for measuring the attributional similarity between words. We investigate
other values in Section 6.4.
11. Evaluate alternates: Let A:B and C:D be any two word pairs in the input
set. From step 2, we have (num filter + 1) versions of A:B, the original and
num filter alternates. Likewise, we have (num filter + 1) versions of C:D.
398
Turney Similarity of Semantic Relations
Therefore we have (num filter + 1)2 ways to compare a version of A:B with
a version of C:D. Look for the row vectors in Uk?k that correspond to the
versions of A:B and the versions of C:D and calculate the (num filter + 1)2
cosines (in our experiments, there are 16 cosines). For example, suppose
A:B is quart:volume and C:D is mile:distance. Table 10 gives the cosines for
the sixteen combinations.
12. Calculate relational similarity: The relational similarity between A:B and
C:D is the average of the cosines, among the (num filter + 1)2 cosines from
step 11, that are greater than or equal to the cosine of the original pairs,
A:B and C:D. The requirement that the cosine must be greater than or
equal to the original cosine is a way of filtering out poor analogies, which
may be introduced in step 1 and may have slipped through the filtering in
step 2. Averaging the cosines, as opposed to taking their maximum, is
intended to provide some resistance to noise. For quart:volume and
mile:distance, the third column in Table 10 shows which alternates are
used to calculate the average. For these two pairs, the average of the
selected cosines is 0.677. In Table 7, we see that pumping:volume has
slipped through the filtering in step 2, although it is not a good alternate
for quart:volume. However, Table 10 shows that all four analogies that
involve pumping:volume are dropped here, in step 12.
Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This
completes the description of LRA.
Table 11 gives the cosines for the sample SAT question. The choice pair with the
highest average cosine (the choice with the largest value in column 1), choice (b), is
the solution for this question; LRA answers the question correctly. For comparison,
column 2 gives the cosines for the original pairs and column 3 gives the highest cosine.
Table 10
The 16 combinations and their cosines. A:B::C:D expresses the analogy A is to B as C is to D. The
third column indicates those combinations for which the cosine is greater than or equal
to the cosine of the original analogy, quart:volume::mile:distance.
Word pairs Cosine Cosine ? original pairs
quart:volume::mile:distance 0.525 Yes (original pairs)
quart:volume::feet:distance 0.464
quart:volume::mile:length 0.634 Yes
quart:volume::length:distance 0.499
liter:volume::mile:distance 0.736 Yes
liter:volume::feet:distance 0.687 Yes
liter:volume::mile:length 0.745 Yes
liter:volume::length:distance 0.576 Yes
gallon:volume::mile:distance 0.763 Yes
gallon:volume::feet:distance 0.710 Yes
gallon:volume::mile:length 0.781 Yes (highest cosine)
gallon:volume::length:distance 0.615 Yes
pumping:volume::mile:distance 0.412
pumping:volume::feet:distance 0.439
pumping:volume::mile:length 0.446
pumping:volume::length:distance 0.491
399
Computational Linguistics Volume 32, Number 3
For this particular SAT question, there is one choice that has the highest cosine for all
three columns, choice (b), although this is not true in general. Note that the gap between
the first choice (b) and the second choice (d) is largest for the average cosines (column
1). This suggests that the average of the cosines (column 1) is better at discriminating the
correct choice than either the original cosine (column 2) or the highest cosine (column 3).
6. Experiments with Word Analogy Questions
This section presents various experiments with 374 multiple-choice SAT word analogy
questions.
6.1 Baseline LRA System
Table 12 shows the performance of the baseline LRA system on the 374 SAT questions,
using the parameter settings and configuration described in Section 5. LRA correctly
answered 210 of the 374 questions; 160 questions were answered incorrectly and 4
questions were skipped, because the stem pair and its alternates were represented by
zero vectors. The performance of LRA is significantly better than the lexicon-based
approach of Veale (2004) (see Section 3.1) and the best performance using attributional
similarity (see Section 2.3), with 95% confidence, according to the Fisher Exact Test
(Agresti 1990).
As another point of reference, consider the simple strategy of always guessing the
choice with the highest co-occurrence frequency. The idea here is that the words in
the solution pair may occur together frequently, because there is presumably a clear
and meaningful relation between the solution words, whereas the distractors may only
occur together rarely because they have no meaningful relation. This strategy is signif-
cantly worse than random guessing. The opposite strategy, always guessing the choice
pair with the lowest co-occurrence frequency, is also worse than random guessing (but
not significantly). It appears that the designers of the SAT questions deliberately chose
distractors that would thwart these two strategies.
Table 11
Cosines for the sample SAT question given in Table 6. Column 1 gives the averages of the cosines
that are greater than or equal to the original cosines (e.g., the average of the cosines that are
marked Yes in Table 10 is 0.677; see choice (b) in column 1). Column 2 gives the cosine for the
original pairs (e.g., the cosine for the first pair in Table 10 is 0.525; see choice (b) in column 2).
Column 3 gives the maximum cosine for the 16 possible analogies (e.g., the maximum cosine in
Table 10 is 0.781; see choice (b) in column 3).
Stem: quart:volume Average Original Highest
cosines cosines cosines
1 2 3
Choices: (a) day:night 0.374 0.327 0.443
(b) mile:distance 0.677 0.525 0.781
(c) decade:century 0.389 0.327 0.470
(d) friction:heat 0.428 0.336 0.552
(e) part:whole 0.370 0.330 0.408
Solution: (b) mile:distance 0.677 0.525 0.781
Gap: (b)?(d) 0.249 0.189 0.229
400
Turney Similarity of Semantic Relations
Table 12
Performance of LRA on the 374 SAT questions. Precision, recall, and F are reported as
percentages. (The bottom five rows are included for comparison.)
Algorithm Precision Recall F
LRA 56.8 56.1 56.5
Veale (2004) 42.8 42.8 42.8
Best attributional similarity 35.0 35.0 35.0
Random guessing 20.0 20.0 20.0
Lowest co-occurrence frequency 16.8 16.8 16.8
Highest co-occurrence frequency 11.8 11.8 11.8
With 374 questions and six word pairs per question (one stem and five choices),
there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies
the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add
B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond
to zero vectors (they do not appear together in a window of five words in the WMTS
corpus). Also, a few words do not appear in Lin?s thesaurus, and some word pairs
appear twice in the SAT questions (e.g., lion:cat). The sparse matrix (step 7) has 17,232
rows (word pairs) and 8,000 columns (patterns), with a density of 5.8% (percentage of
nonzero values).
Table 13 gives the time required for each step of LRA, a total of almost 9 days. All of
the steps used a single CPU on a desktop computer, except step 3, finding the phrases
for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are
parallelizable; with a bit of programming effort, they could also be executed on the
Beowulf cluster. All CPUs (both desktop and cluster) were 2.4 GHz Intel Xeons. The
desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM.
6.2 LRA versus VSM
Table 14 compares LRA to the VSM with the 374 analogy questions. VSM-AV refers
to the VSM using AltaVista?s database as a corpus. The VSM-AV results are taken
Table 13
LRA elapsed run time.
Step Description Time H:M:S Hardware
1 Find alternates 24:56:00 1 CPU
2 Filter alternates 0:00:02 1 CPU
3 Find phrases 109:52:00 16 CPUs
4 Find patterns 33:41:00 1 CPU
5 Map pairs to rows 0:00:02 1 CPU
6 Map patterns to columns 0:00:02 1 CPU
7 Generate a sparse matrix 38:07:00 1 CPU
8 Calculate entropy 0:11:00 1 CPU
9 Apply SVD 0:43:28 1 CPU
10 Projection 0:08:00 1 CPU
11 Evaluate alternates 2:11:00 1 CPU
12 Calculate relational similarity 0:00:02 1 CPU
Total 209:49:36
401
Computational Linguistics Volume 32, Number 3
from Turney and Littman (2005). As mentioned in Section 4.2, we estimate this corpus
contained about 5 ? 1011 English words at the time the VSM-AV experiments took place.
VSM-WMTS refers to the VSM using the WMTS, which contains about 5 ? 1010 English
words. We generated the VSM-WMTS results by adapting the VSM to the WMTS.
The algorithm is slightly different from Turney and Littman?s (2005), because we used
passage frequencies instead of document frequencies.
All three pairwise differences in recall in Table 14 are statistically significant with
95% confidence, using the Fisher Exact Test (Agresti 1990). The pairwise differences in
precision between LRA and the two VSM variations are also significant, but the differ-
ence in precision between the two VSM variations (42.4% vs. 47.7%) is not significant.
Although VSM-AV has a corpus 10 times larger than LRA?s, LRA still performs better
than VSM-AV.
Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of
the VSM, but much of the drop is due to the larger number of questions that were
skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more
of the input word pairs simply do not appear together in short phrases in the corpus.
LRA is able to answer as many questions as VSM-AV, although it uses the same corpus
as VSM-WMTS, because Lin?s thesaurus allows LRA to substitute synonyms for words
that are not in the corpus.
VSM-AV required 17 days to process the 374 analogy questions (Turney and Littman
2005), compared to 9 days for LRA. As a courtesy to AltaVista, Turney and Littman
(2005) inserted a 5-second delay between each two queries. Since the WMTS is running
locally, there is no need for delays. VSM-WMTS processed the questions in only one day.
6.3 Human Performance
The average performance of college-bound senior high school students on verbal SAT
questions corresponds to a recall (percent correct) of about 57% (Turney and Littman
2005). The SAT I test consists of 78 verbal questions and 60 math questions (there is
also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are
only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374
analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then
we can estimate that the average college-bound senior would correctly answer about
57% of the 374 analogy questions.
Of our 374 SAT questions, 190 are from a collection of ten official SAT tests (Claman
2000). On this subset of the questions, LRA has a recall of 61.1%, compared to a recall
of 51.1% on the other 184 questions. The 184 questions that are not from Claman (2000)
seem to be more difficult. This indicates that we may be underestimating how well
LRA performs, relative to college-bound senior high school students. Claman (2000)
suggests that the analogy questions may be somewhat harder than other verbal SAT
Table 14
LRA versus VSM with 374 SAT analogy questions.
Algorithm Correct Incorrect Skipped Precision Recall F
VSM-AV 176 193 5 47.7 47.1 47.4
VSM-WMTS 144 196 34 42.4 38.5 40.3
LRA 210 160 4 56.8 56.1 56.5
402
Turney Similarity of Semantic Relations
questions, so we may be slightly overestimating the mean human score on the analogy
questions.
Table 15 gives the 95% confidence intervals for LRA, VSM-AV, and VSM-WMTS,
calculated by the Binomial Exact Test (Agresti 1990). There is no significant difference
between LRA and human performance, but VSM-AV and VSM-WMTS are significantly
below human-level performance.
6.4 Varying the Parameters in LRA
There are several parameters in the LRA algorithm (see Section 5.5). The parameter
values were determined by trying a small number of possible values on a small set of
questions that were set aside. Since LRA is intended to be an unsupervised learning
algorithm, we did not attempt to tune the parameter values to maximize the precision
and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive
to the values of the parameters.
Table 16 shows the variation in the performance of LRA as the parameter values
are adjusted. We take the baseline parameter settings (given in Section 5.5) and vary
each parameter, one at a time, while holding the remaining parameters fixed at their
baseline values. None of the precision and recall values are significantly different from
the baseline, according to the Fisher Exact Test (Agresti 1990), at the 95% confidence
level. This supports the hypothesis that the algorithm is not sensitive to the parameter
values.
Although a full run of LRA on the 374 SAT questions takes 9 days, for some of
the parameters it is possible to reuse cached data from previous runs. We limited the
experiments with num sim and max phrase because caching was not as helpful for these
parameters, so experimenting with them required several weeks.
6.5 Ablation Experiments
As mentioned in the introduction, LRA extends the VSM approach of Turney and
Littman (2005) by (1) exploring variations on the analogies by replacing words with
synonyms (step 1), (2) automatically generating connecting patterns (step 4), and (3)
smoothing the data with SVD (step 9). In this subsection, we ablate each of these three
components to assess their contribution to the performance of LRA. Table 17 shows the
results.
Table 15
Comparison with human SAT performance. The last column in the table indicates whether (YES)
or not (NO) the average human performance (57%) falls within the 95% confidence interval of
the corresponding algorithm?s performance. The confidence intervals are calculated using the
Binomial Exact Test (Agresti 1990).
System Recall 95% confidence Human-level
(% correct) interval for recall (57%)
VSM-AV 47.1 42.2?52.5 NO
VSM-WMTS 38.5 33.5?43.6 NO
LRA 56.1 51.0?61.2 YES
403
Computational Linguistics Volume 32, Number 3
Table 16
Variation in performance with different parameter values. The Baseline column marks the
baseline parameter values. The Step column gives the step number in Section 5.5 where each
parameter is discussed.
Parameter Baseline Value Step Precision Recall F
num sim 5 1 54.2 53.5 53.8
num sim ? 10 1 56.8 56.1 56.5
num sim 15 1 54.1 53.5 53.8
max phrase 4 2 55.8 55.1 55.5
max phrase ? 5 2 56.8 56.1 56.5
max phrase 6 2 56.2 55.6 55.9
num filter 1 2 54.3 53.7 54.0
num filter 2 2 55.7 55.1 55.4
num filter ? 3 2 56.8 56.1 56.5
num filter 4 2 55.7 55.1 55.4
num filter 5 2 54.3 53.7 54.0
num patterns 1000 4 55.9 55.3 55.6
num patterns 2000 4 57.6 57.0 57.3
num patterns 3000 4 58.4 57.8 58.1
num patterns ? 4000 4 56.8 56.1 56.5
num patterns 5000 4 57.0 56.4 56.7
num patterns 6000 4 57.0 56.4 56.7
num patterns 7000 4 58.1 57.5 57.8
k 100 10 55.7 55.1 55.4
k ? 300 10 56.8 56.1 56.5
k 500 10 57.6 57.0 57.3
k 700 10 56.5 55.9 56.2
k 900 10 56.2 55.6 55.9
Without SVD (compare column 1 to 2 in Table 17), performance drops, but the
drop is not statistically significant with 95% confidence, according to the Fisher Exact
Test (Agresti 1990). However, we hypothesize that the drop in performance would be
significant with a larger set of word pairs. More word pairs would increase the sample
size, which would decrease the 95% confidence interval, which would likely show that
SVD is making a significant contribution. Furthermore, more word pairs would increase
the matrix size, which would give SVD more leverage. For example, Landauer and
Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix
Table 17
Results of ablation experiments.
LRA LRA
Baseline LRA LRA No SVD,
system No SVD No synonyms no synonyms VSM-WMTS
1 2 3 4 5
Correct 210 198 185 178 144
Incorrect 160 172 167 173 196
Skipped 4 4 22 23 34
Precision 56.8 53.5 52.6 50.7 42.4
Recall 56.1 52.9 49.5 47.6 38.5
F 56.5 53.2 51.0 49.1 40.3
404
Turney Similarity of Semantic Relations
here is 8,000 columns by 17,232 rows. We are currently gathering more SAT questions
to test this hypothesis.
Without synonyms (compare column 1 to 3 in Table 17), recall drops significantly
(from 56.1% to 49.5%), but the drop in precision is not significant. When the synonym
component is dropped, the number of skipped questions rises from 4 to 22, which
demonstrates the value of the synonym component of LRA for compensating for sparse
data.
When both SVD and synonyms are dropped (compare column 1 to 4 in Table 17),
the decrease in recall is significant, but the decrease in precision is not significant.
Again, we believe that a larger sample size would show that the drop in precision is
significant.
If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from
VSM-WMTS is the patterns (step 4). The VSM approach uses a fixed list of 64 patterns
to generate 128 dimensional vectors (Turney and Littman 2005), whereas LRA uses a
dynamically generated set of 4,000 patterns, resulting in 8,000 dimensional vectors. We
can see the value of the automatically generated patterns by comparing LRA without
synonyms and SVD (column 4) to VSM-WMTS (column 5). The difference in both
precision and recall is statistically significant with 95% confidence, according to the
Fisher Exact Test (Agresti 1990).
The ablation experiments support the value of the patterns (step 4) and synonyms
(step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although
we believe more data will support its effectiveness. Nonetheless, the three components
together result in a 16% increase in F (compare column 1 to 5).
6.6 Matrix Symmetry
We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as
carpenter is to wood implies stone is to mason as wood is to carpenter. Therefore, a good
measure of relational similarity, simr, should obey the following equation:
simr(A :B, C :D) = simr(B :A, D :C) (8)
In steps 5 and 6 of the LRA algorithm (Section 5.5), we ensure that the matrix X is
symmetrical, so that equation (8) is necessarily true for LRA. The matrix is designed so
that the row vector for A:B is different from the row vector for B:A only by a permutation
of the elements. The same permutation distinguishes the row vectors for C:D and D:C.
Therefore the cosine of the angle between A:B and C:D must be identical to the cosine
of the angle between B:A and D:C (see equation (7)).
To discover the consequences of this design decision, we altered steps 5 and 6 so
that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in
the input set, we only have one row. There is no row for B:A unless B:A also appears in
the input set. Thus the number of rows in the matrix dropped from 17,232 to 8,616.
In step 6, we no longer have two columns for each pattern P, one for ?word1 P word2?
and another for ?word2 P word1.? However, to be fair, we kept the total number of
columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000),
distinguishing the pattern ?word1 P word2? from the pattern ?word2 P word1? (instead of
considering them equivalent). Thus a pattern P with a high frequency is likely to appear
in two columns, in both possible orders, but a lower frequency pattern might appear in
only one column, in only one possible order.
405
Computational Linguistics Volume 32, Number 3
These changes resulted in a slight decrease in performance. Recall dropped from
56.1% to 55.3% and precision dropped from 56.8% to 55.9%. The decrease is not sta-
tistically significant. However, the modified algorithm no longer obeys equation (8).
Although dropping symmetry appears to cause no significant harm to the performance
of the algorithm on the SAT questions, we prefer to retain symmetry, to ensure that
equation (8) is satisfied.
Note that, if A:B::C:D, it does not follow that B:A::C:D. For example, it is false that
?stone is to mason as carpenter is to wood.? In general (except when the semantic
relations between A and B are symmetrical), we have the following inequality:
simr(A :B, C :D) = simr(B :A, C :D) (9)
Therefore we do not want A:B and B:A to be represented by identical row vectors,
although it would ensure that equation (8) is satisfied.
6.7 All Alternates versus Better Alternates
In step 12 of LRA, the relational similarity between A:B and C:D is the average of the
cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal
to the cosine of the original pairs, A:B and C:D. That is, the average includes only those
alternates that are ?better? than the originals. Taking all alternates instead of the better
alternates, recall drops from 56.1% to 40.4% and precision drops from 56.8% to 40.8%.
Both decreases are statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990).
6.8 Interpreting Vectors
Suppose a word pair A:B corresponds to a vector r in the matrix X. It would be con-
venient if inspection of r gave us a simple explanation or description of the relation
between A and B. For example, suppose the word pair ostrich:bird maps to the row
vector r. It would be pleasing to look in r and find that the largest element corresponds
to the pattern ?is the largest? (i.e., ?ostrich is the largest bird?). Unfortunately, inspection
of r reveals no such convenient patterns.
We hypothesize that the semantic content of a vector is distributed over the whole
vector; it is not concentrated in a few elements. To test this hypothesis, we modified
step 10 of LRA. Instead of projecting the 8,000 dimensional vectors into the 300 dimen-
sional space Uk?k, we use the matrix Uk?kVTk . This matrix yields the same cosines as
Uk?k, but preserves the original 8,000 dimensions, making it easier to interpret the row
vectors. For each row vector in Uk?kVTk , we select the N largest values and set al other
values to zero. The idea here is that we will only pay attention to the N most important
patterns in r; the remaining patterns will be ignored. This reduces the length of the
row vectors, but the cosine is the dot product of normalized vectors (all vectors are
normalized to unit length; see equation (7)), so the change to the vector lengths has no
impact; only the angle of the vectors is important. If most of the semantic content is in
the N largest elements of r, then setting the remaining elements to zero should have
relatively little impact.
Table 18 shows the performance as N varies from 1 to 3,000. The precision and recall
are significantly below the baseline LRA until N ? 300 (95% confidence, Fisher Exact
406
Turney Similarity of Semantic Relations
Test). In other words, for a typical SAT analogy question, we need to examine the top
300 patterns to explain why LRA selected one choice instead of another.
We are currently working on an extension of LRA that will explain with a single
pattern why one choice is better than another. We have had some promising results, but
this work is not yet mature. However, we can confidently claim that interpreting the
vectors is not trivial.
6.9 Manual Patterns versus Automatic Patterns
Turney and Littman (2005) used 64 manually generated patterns, whereas LRA uses
4,000 automatically generated patterns. We know from Section 6.5 that the automatically
generated patterns are significantly better than the manually generated patterns. It may
be interesting to see how many of the manually generated patterns appear within the
automatically generated patterns. If we require an exact match, 50 of the 64 manual
patterns can be found in the automatic patterns. If we are lenient about wildcards, and
count the pattern not the as matching * not the (for example), then 60 of the 64 manual
patterns appear within the automatic patterns. This suggests that the improvement in
performance with the automatic patterns is due to the increased quantity of patterns,
rather than a qualitative difference in the patterns.
Turney and Littman (2005) point out that some of their 64 patterns have been used
by other researchers. For example, Hearst (1992) used the pattern such as to discover hy-
ponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms.
Both of these patterns are included in the 4,000 patterns automatically generated by
LRA.
The novelty in Turney and Littman (2005) is that their patterns are not used to
mine text for instances of word pairs that fit the patterns (Hearst 1992; Berland and
Charniak 1999); instead, they are used to gather frequency data for building vectors
that represent the relation between a given pair of words. The results in Section 6.8
show that a vector contains more information than any single pattern or small set of
patterns; a vector is a distributed representation. LRA is distinct from Hearst (1992) and
Berland and Charniak (1999) in its focus on distributed representations, which it shares
with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by
finding patterns automatically.
Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but
their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and
Table 18
Performance as a function of N.
N Correct Incorrect Skipped Precision Recall F
1 114 179 81 38.9 30.5 34.2
3 146 206 22 41.5 39.0 40.2
10 167 201 6 45.4 44.7 45.0
30 174 196 4 47.0 46.5 46.8
100 178 192 4 48.1 47.6 47.8
300 192 178 4 51.9 51.3 51.6
1000 198 172 4 53.5 52.9 53.2
3000 207 163 4 55.9 55.3 55.6
407
Computational Linguistics Volume 32, Number 3
Berland and Charniak (1999). Because LRA uses patterns to build distributed vector
representations, it can exploit patterns that would be much too noisy and unreliable for
the kind of text mining instance extraction that is the objective of Hearst (1992), Berland
and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can
simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the
more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003).
7. Experiments with Noun-Modifier Relations
This section describes experiments with 600 noun-modifier pairs, hand-labeled with
30 classes of semantic relations (Nastase and Szpakowicz 2003). In the following ex-
periments, LRA is used with the baseline parameter values, exactly as described in
Section 5.5. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is
used as a distance (nearness) measure in a single nearest neighbor supervised learning
algorithm.
7.1 Classes of Relations
The following experiments use the 600 labeled noun-modifier pairs of Nastase and
Szpakowicz (2003). This data set includes information about the part of speech and
WordNet synset (synonym set; i.e., word sense tag) of each word, but our algorithm
does not use this information.
Table 19 lists the 30 classes of semantic relations. The table is based on Appendix A
of Nastase and Szpakowicz (2003), with some simplifications. The original table listed
several semantic relations for which there were no instances in the data set. These were
relations that are typically expressed with longer phrases (three or more words), rather
than noun-modifier word pairs. For clarity, we decided not to include these relations in
Table 19.
In this table, H represents the head noun and M represents the modifier. For exam-
ple, in flu virus, the head noun (H) is virus and the modifier (M) is flu (*). In English,
the modifier (typically a noun or adjective) usually precedes the head noun. In the
description of purpose, V represents an arbitrary verb. In concert hall, the hall is for
presenting concerts (V is present) or holding concerts (V is hold) (?).
Nastase and Szpakowicz (2003) organized the relations into groups. The five capi-
talized terms in the Relation column of Table 19 are the names of five groups of semantic
relations. (The original table had a sixth group, but there are no examples of this group
in the data set.) We make use of this grouping in the following experiments.
7.2 Baseline LRA with Single Nearest Neighbor
The following experiments use single nearest neighbor classification with leave-one-out
cross-validation. For leave-one-out cross-validation, the testing set consists of a single
noun-modifier pair and the training set consists of the 599 remaining noun-modifiers.
The data set is split 600 times, so that each noun-modifier gets a turn as the testing word
pair. The predicted class of the testing pair is the class of the single nearest neighbor
in the training set. As the measure of nearness, we use LRA to calculate the relational
similarity between the testing pair and the training pairs. The single nearest neighbor
algorithm is a supervised learning algorithm (i.e., it requires a training set of labeled
408
Turney Similarity of Semantic Relations
Table 19
Classes of semantic relations, from Nastase and Szpakowicz (2003).
Relation Abbr. Example phrase Description
CAUSALITY
cause cs flu virus (*) H makes M occur or exist, H is
necessary and sufficient
effect eff exam anxiety M makes H occur or exist, M is
necessary and sufficient
purpose prp concert hall (?) H is for V-ing M, M does not
necessarily occur or exist
detraction detr headache pill H opposes M, H is not sufficient
to prevent M
TEMPORALITY
frequency freq daily exercise H occurs every time M occurs
time at tat morning exercise H occurs when M occurs
time through tthr six-hour meeting H existed while M existed, M is
an interval of time
SPATIAL
direction dir outgoing mail H is directed towards M, M is
not the final point
location loc home town H is the location of M
location at lat desert storm H is located at M
location from lfr foreign capital H originates at M
PARTICIPANT
agent ag student protest M performs H, M is animate or
natural phenomenon
beneficiary ben student discount M benefits from H
instrument inst laser printer H uses M
object obj metal separator M is acted upon by H
object property obj prop sunken ship H underwent M
part part printer tray H is part of M
possessor posr national debt M has H
property prop blue book H is M
product prod plum tree H produces M
source src olive oil M is the source of H
stative st sleeping dog H is in a state of M
whole whl daisy chain M is part of H
QUALITY
container cntr film music M contains H
content cont apple cake M is contained in H
equative eq player coach H is also M
material mat brick house H is made of M
measure meas expensive book M is a measure of H
topic top weather report H is concerned with M
type type oak tree M is a type of H
409
Computational Linguistics Volume 32, Number 3
data), but we are using LRA to measure the distance between a pair and its potential
neighbors, and LRA is itself determined in an unsupervised fashion (i.e., LRA does not
need labeled data).
Each SAT question has five choices, so answering 374 SAT questions required cal-
culating 374 ? 5 ? 16 = 29, 920 cosines. The factor of 16 comes from the alternate pairs,
step 11 in LRA. With the noun-modifier pairs, using leave-one-out cross-validation,
each test pair has 599 choices, so an exhaustive application of LRA would require
calculating 600 ? 599 ? 16 = 5, 750, 400 cosines. To reduce the amount of computation
required, we first find the 30 nearest neighbors for each pair, ignoring the alternate pairs
(600 ? 599 = 359, 400 cosines), and then apply the full LRA, including the alternates, to
just those 30 neighbors (600 ? 30 ? 16 = 288, 000 cosines), which requires calculating
only 359, 400 + 288, 000 = 647, 400 cosines.
There are 600 word pairs in the input set for LRA. In step 2, introducing alternate
pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each
pair A:B, we add B:A, yielding 4,800 pairs. However, some pairs are dropped because
they correspond to zero vectors and a few words do not appear in Lin?s thesaurus. The
sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8.4%.
Following Turney and Littman (2005), we evaluate the performance by accuracy
and also by the macroaveraged F measure (Lewis 1991). Macroaveraging calculates
the precision, recall, and F for each class separately, and then calculates the average
across all classes. Microaveraging combines the true positive, false positive, and false
negative counts for all of the classes, and then calculates precision, recall, and F from the
combined counts. Macroaveraging gives equal weight to all classes, but microaveraging
gives more weight to larger classes. We use macroaveraging (giving equal weight to all
classes), because we have no reason to believe that the class sizes in the data set reflect
the actual distribution of the classes in a real corpus.
Classification with 30 distinct classes is a hard problem. To make the task easier, we
can collapse the 30 classes to 5 classes, using the grouping that is given in Table 19. For
example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA
with the single nearest neighbor algorithm achieves an accuracy of 39.8% (239/600)
and a macroaveraged F of 36.6%. Always guessing the majority class would result in
an accuracy of 8.2% (49/600). On the 5 class problem, the accuracy is 58.0% (348/600)
and the macroaveraged F is 54.6%. Always guessing the majority class would give an
accuracy of 43.3% (260/600). For both the 30 class and 5 class problems, LRA?s accuracy
is significantly higher than guessing the majority class, with 95% confidence, according
to the Fisher Exact Test (Agresti 1990).
7.3 LRA versus VSM
Table 20 shows the performance of LRA and VSM on the 30 class problem. VSM-AV
is VSM with the AltaVista corpus and VSM-WMTS is VSM with the WMTS corpus.
The results for VSM-AV are taken from Turney and Littman (2005). All three pairwise
differences in the three F measures are statistically significant at the 95% level, according
to the Paired t-Test (Feelders and Verkooijen 1995). The accuracy of LRA is signifi-
cantly higher than the accuracies of VSM-AV and VSM-WMTS, according to the Fisher
Exact Test (Agresti 1990), but the difference between the two VSM accuracies is not
significant.
Table 21 compares the performance of LRA and VSM on the 5 class problem.
The accuracy and F measure of LRA are significantly higher than the accuracies and
410
Turney Similarity of Semantic Relations
Table 20
Comparison of LRA and VSM on the 30 class problem.
VSM-AV VSM-WMTS LRA
Correct 167 148 239
Incorrect 433 452 361
Total 600 600 600
Accuracy 27.8 24.7 39.8
Precision 27.9 24.0 41.0
Recall 26.8 20.9 35.9
F 26.5 20.3 36.6
Table 21
Comparison of LRA and VSM on the 5 class problem.
VSM-AV VSM-WMTS LRA
Correct 274 264 348
Incorrect 326 336 252
Total 600 600 600
Accuracy 45.7 44.0 58.0
Precision 43.4 40.2 55.9
Recall 43.1 41.4 53.6
F 43.2 40.6 54.6
F measures of VSM-AV and VSM-WMTS, but the differences between the two VSM
accuracies and F measures are not significant.
8. Discussion
The experimental results in Sections 6 and 7 demonstrate that LRA performs signifi-
cantly better than the VSM, but it is also clear that there is room for improvement. The
accuracy might not yet be adequate for practical applications, although past work has
shown that it is possible to adjust the trade-off of precision versus recall (Turney and
Littman 2005). For some of the applications, such as information extraction, LRA might
be suitable if it is adjusted for high precision, at the expense of low recall.
Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy
questions. However, with progress in computer hardware, speed will gradually become
less of a concern. Also, the software has not been optimized for speed; there are several
places where the efficiency could be increased and many operations are parallelizable.
It may also be possible to precompute much of the information for LRA, although this
would require substantial changes to the algorithm.
The difference in performance between VSM-AV and VSM-WMTS shows that VSM
is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the
WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA
would perform better with a larger corpus. The WMTS corpus requires one terabyte of
hard disk space, but progress in hardware will likely make 10 or even 100 terabytes
affordable in the relatively near future.
For noun-modifier classification, more labeled data should yield performance im-
provements. With 600 noun-modifier pairs and 30 classes, the average class has only
411
Computational Linguistics Volume 32, Number 3
20 examples. We expect that the accuracy would improve substantially with 5 or
10 times more examples. Unfortunately, it is time consuming and expensive to acquire
hand-labeled data.
Another issue with noun-modifier classification is the choice of classification
scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003)
might not be the best scheme. Other researchers have proposed different schemes
(Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario,
Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine
learning than others. For some applications, 30 classes may not be necessary; the 5 class
scheme may be sufficient.
LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past
work suggests that a hybrid approach, combining multiple modules, some corpus-
based, some lexicon-based, will surpass any purebred approach (Turney et al 2003).
In future work, it would be natural to combine the corpus-based approach of LRA with
the lexicon-based approach of Veale (2004), perhaps using the combination method of
Turney et al (2003).
SVD is only one of many methods for handling sparse, noisy data. We have also
experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999),
Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Com-
ponents Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling
(IS) (Ando 2000). We had some interesting results with small matrices (around 2,000
rows by 1,000 columns), but none of these methods seemed substantially better than
SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows
and 8,000 columns; see Section 6.1).
In step 4 of LRA, we simply select the top num patterns most frequent patterns
and discard the remaining patterns. Perhaps a more sophisticated selection algorithm
would improve the performance of LRA. We have tried a variety of ways of selecting
patterns, but it seems that the method of selection has little impact on performance. We
hypothesize that the distributed vector representation is not sensitive to the selection
method, but it is possible that future work will find a method that yields significant
improvement in performance.
9. Conclusion
This article has introduced a new method for calculating relational similarity, Latent
Relational Analysis. The experiments demonstrate that LRA performs better than the
VSM approach, when evaluated with SAT word analogy questions and with the task
of classifying noun-modifier expressions. The VSM approach represents the relation be-
tween a pair of words with a vector, in which the elements are based on the frequencies
of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways:
(1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth
the data, and (3) a thesaurus is used to explore variations of the word pairs. With the
WMTS corpus (about 5 ? 1010 English words), LRA achieves an F of 56.5%, whereas the
F of VSM is 40.3%.
We have presented several examples of the many potential applications for mea-
sures of relational similarity. Just as attributional similarity measures have proven
to have many practical uses, we expect that relational similarity measures will soon
become widely used. Gentner et al (2001) argue that relational similarity is essential
to understanding novel metaphors (as opposed to conventional metaphors). Many
412
Turney Similarity of Semantic Relations
researchers have argued that metaphor is the heart of human thinking (Lakoff and
Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner
et al 2001; French 2002). We believe that relational similarity plays a fundamental role
in the mind and therefore relational similarity measures could be crucial for artificial
intelligence.
In future work, we plan to investigate some potential applications for LRA. It
is possible that the error rate of LRA is still too high for practical applications, but
the fact that LRA matches average human performance on SAT analogy questions is
encouraging.
Acknowledgments
Thanks to Michael Littman for sharing the
374 SAT analogy questions and for inspiring
me to tackle them. Thanks to Vivi Nastase
and Stan Szpakowicz for sharing their 600
classified noun-modifier phrases. Thanks to
Egidio Terra, Charlie Clarke, and the School
of Computer Science of the University of
Waterloo, for giving us a copy of the
Waterloo MultiText System and their
Terabyte Corpus. Thanks to Dekang Lin for
making his Dependency-Based Word
Similarity lexicon available online. Thanks to
Doug Rohde for SVDLIBC and Michael
Berry for SVDPACK. Thanks to Ted Pedersen
for making his WordNet::Similarity package
available. Thanks to Joel Martin for
comments on the article. Thanks to the
anonymous reviewers of Computational
Linguistics for their very helpful comments
and suggestions.
References
Agresti, Alan. 1990. Categorical Data Analysis.
Wiley, Hoboken, NJ.
Ando, Rie Kubota. 2000. Latent semantic
space: Iterative scaling improves precision
of inter-document similarity measurement.
In Proceedings of the 23rd Annual ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR-2000),
pages 216?223, Athens, Greece.
Baeza-Yates, Ricardo A. and Berthier A.
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York, NY.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure of
semantic relatedness. In Proceedings of the
Eighteenth International Joint Conference on
Artificial Intelligence (IJCAI-03),
pages 805?810, Acapulco, Mexico.
Barker, Ken and Stan Szpakowicz. 1998.
Semi-automatic recognition of noun
modifier relationships. In Christian Boitet
and Pete Whitelock, editors, Proceedings of
the Thirty-Sixth Annual Meeting of the
Association for Computational Linguistics and
Seventeenth International Conference on
Computational Linguistics
(COLING-ACL?98), pages 96?102, San
Francisco, CA. Morgan Kaufmann
Publishers.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL ?99), pages 57?64, New Brunswick,
NJ.
Berry, Michael W. 1992. Large scale singular
value computations. International Journal of
Supercomputer Applications, 6(1):13?49.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in wordnet: An
experimental, application-oriented
evaluation of five measures. In Proceedings
of the Workshop on WordNet and Other
Lexical Resources, Second Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL-2001),
pages 29?34, Pittsburgh, PA.
Chiarello, Christine, Curt Burgess, Lorie
Richards, and Alma Pollock. 1990.
Semantic and associative priming in the
cerebral hemispheres: Some words do,
some words don?t ... sometimes, some
places. Brain and Language, 38:75?104.
Claman, Cathy. 2000. 10 Real SATs. College
Entrance Examination Board.
Clarke, Charles L. A., Gordon V. Cormack,
and Christopher R. Palmer. 1998. An
overview of multitext. ACM SIGIR Forum,
32(2):14?15.
Daganzo, Carlos F. 1994. The cell
transmission model: A dynamic
representation of highway traffic
consistent with the hydrodynamic theory.
Transportation Research Part B:
Methodological, 28(4):269?287.
Deerwester, Scott C., Susan T. Dumais,
Thomas K. Landauer, George W. Furnas,
and Richard A. Harshman. 1990. Indexing
413
Computational Linguistics Volume 32, Number 3
by latent semantic analysis. Journal of the
American Society for Information Science
(JASIS), 41(6):391?407.
Dolan, William B. 1995. Metaphor as an
emergent property of machine-readable
dictionaries. In Proceedings of the AAAI
1995 Spring Symposium Series: Representation
and Acquisition of Lexical Knowledge:
Polysemy, Ambiguity and Generativity,
pages 27?32, Menlo Park, CA.
Dumais, Susan T. 1990. Enhancing
performance in latent semantic indexing
(LSI) retrieval. Technical Report TM-ARH-
017527, Bellcore, Morristown, NJ.
Dumais, Susan T. 1993. Latent semantic
indexing (LSI) and TREC-2. In D. K.
Harman, editor, Proceedings of the
Second Text REtrieval Conference (TREC-2),
pages 105?115. National Institute
of Standards and Technology,
Gaithersburg, MD.
Falkenhainer, Brian. 1990. Analogical
interpretation in context. In Proceedings of
the Twelfth Annual Conference of the Cognitive
Science Society, pages 69?76. Lawrence
Erlbaum Associates, Mahwah, NJ.
Falkenhainer, Brian, Kenneth D. Forbus,
and Dedre Gentner. 1989. The
structure-mapping engine: Algorithm and
examples. Artificial Intelligence, 41(1):1?63.
Federici, Stefano, Simonetta Montemagni,
and Vito Pirrelli. 1997. Inferring semantic
similarity from distributional evidence: An
analogy-based approach to word sense
disambiguation. In Proceedings of the
ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 90?97, Madrid, Spain.
Feelders, Ad and William Verkooijen. 1995.
Which method learns the most from
data? Methodological issues in the
analysis of comparative studies. In Fifth
International Workshop on Artificial
Intelligence and Statistics, pages 219?225,
Ft. Lauderdale, FL.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
French, Robert M. 2002. The computational
modeling of analogy-making. Trends in
Cognitive Sciences, 6(5):200?205.
Gentner, Dedre. 1983. Structure-mapping: A
theoretical framework for analogy.
Cognitive Science, 7(2):155?170.
Gentner, Dedre, Brian Bowdle, Phillip Wolff,
and Consuelo Boronat. 2001. Metaphor is
like analogy. In Dedre Gentner, Keith J.
Holyoak, and Boicho N. Kokinov, editors,
The Analogical Mind: Perspectives from
Cognitive Science. MIT Press, Cambridge,
MA, pages 199?253.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Adriana Badulescu, and
Dan I. Moldovan. 2003. Learning semantic
constraints for the automatic discovery of
part-whole relations. In Proceedings of the
Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2003), pages 80?87, Edmonton, Canada.
Goldenberg, David. 2005. The emperor?s
new clothes: Undressing the new and
unimproved SAT. Gelf Magazine, March.
http://www.gelf-magazine.com/mt/
archives/the emperors new clothes.html.
Golub, Gene H. and Charles F. Van Loan.
1996. Matrix Computations. 3rd ed.
Johns Hopkins University Press,
Baltimore, MD.
Harman, Donna. 1986. An experimental
study of factors important in document
ranking. In Proceedings of the Ninth Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval (SIGIR?86), pages 186?193, Pisa,
Italy.
Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 539?545, Nantes, France.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of context
for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, pages 305?332. MIT Press,
Cambridge, MA.
Hofmann, Thomas. 1999. Probabilistic latent
semantic indexing. In Proceedings of the
22nd Annual ACM Conference on Research
and Development in Information Retrieval
(SIGIR ?99), pages 50?57, Berkeley, CA,
August.
Hofstadter, Douglas and the Fluid Analogies
Research Group. 1995. Fluid Concepts and
Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought. Basic
Books, New York.
Jarmasz, Mario and Stan Szpakowicz.
2003. Roget?s thesaurus and semantic
similarity. In Proceedings of the International
Conference on Recent Advances in Natural
Language Processing (RANLP-03),
pages 212?219, Borovets, Bulgaria.
414
Turney Similarity of Semantic Relations
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference on
Research in Computational Linguistics
(ROCLING X), pages 19?33, Tapei, Taiwan.
Kurtz, Stanley. 2002. Testing debate.
National Review Magazine, August.
http://www.nationalreview.com/
kurtz/kurtz082102.asp.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem: The
latent semantic analysis theory of the
acquisition, induction, and representation
of knowledge. Psychological Review,
104(2):211?240.
Lapata, Mirella and Frank Keller. 2004. The
web as a baseline: Evaluating the
performance of unsupervised web-based
models for a range of NLP tasks. In
Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 121?128, Boston, MA.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. thesis, Macquarie
University, Sydney.
Leacock, Claudia and Martin Chodorow.
1998. Combining local context and
WordNet similarity for word sense
identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, pages 265?283. MIT Press,
Cambridge, MA.
Lee, Daniel D. and H. Sebastian Seung. 1999.
Learning the parts of objects by
nonnegative matrix factorization. Nature,
401:788?791.
Lesk, Michael E. 1969. Word-word
associations in document retrieval
systems. American Documentation,
20(1):27?38.
Lesk, Michael E. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of ACM
SIGDOC ?86, pages 24?26, New York, NY.
Lewis, David D. 1991. Evaluating text
categorization. In Proceedings of the
Speech and Natural Language Workshop,
pages 312?318, Asilomar, CA. Morgan
Kaufmann, San Francisco, CA.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th
International Conference on Computational
Linguistics (COLING-ACL ?98),
pages 768?774, Montreal, Canada.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of the
15th International Conference on Machine
Learning (ICML ?98), pages 296?304.
Morgan Kaufmann, San Francisco, CA.
Marx, Zvika, Ido Dagan, Joachim Buhmann,
and Eli Shamir. 2002. Coupled clustering:
A method for detecting structural
correspondence. Journal of Machine
Learning Research, 3:747?780.
Medin, Douglas L., Robert L. Goldstone, and
Dedre Gentner. 1990. Similarity involving
attributes and relations: Judgments of
similarity and difference are not inverses.
Psychological Science, 1(1):64?69.
Moldovan, Dan, Adriana Badulescu, Marta
Tatu, Daniel Antohe, and Roxana Girju.
2004. Models for the semantic
classification of noun phrases. In
Proceedings of the Computational Lexical
Semantics Workshop at HLT-NAACL 2004,
pages 60?67, Boston, MA.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21?48.
Nastase, Vivi and Stan Szpakowicz. 2003.
Exploring noun?modifier semantic
relations. In Fifth International Workshop on
Computational Semantics (IWCS-5),
pages 285?301, Tilburg, the Netherlands.
Pantel, Patrick and Dekang Lin. 2002.
Discovering word senses from text. In
Proceedings of ACM SIGKDD Conference on
Knowledge Discovery and Data Mining,
pages 613?619, New York, NY.
Rada, Roy, Hafedh Mili, Ellen Bicknell, and
Maria Blettner. 1989. Development and
application of a metric on semantic nets.
IEEE Transactions on Systems, Man, and
Cybernetics, 19(1):17?30.
Rehder, Bob, M. E. Schreiner, Michael B. W.
Wolfe, Darrell Laham, Thomas K.
Landauer, and Walter Kintsch. 1998. Using
latent semantic analysis to assess
knowledge: Some technical considerations.
Discourse Processes, 25:337?354.
Reitman, Walter R. 1965. Cognition and
Thought: An Information Processing
Approach. John Wiley and Sons,
New York, NY.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In Proceedings of the 14th
415
Computational Linguistics Volume 32, Number 3
International Joint Conference on Artificial
Intelligence (IJCAI-95), pages 448?453, San
Mateo, CA. Morgan Kaufmann, San
Francisco, CA.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
the Sixteenth National Conference on Artificial
Intelligence (AAAI-99), pages 474?479,
Menlo Park, CA.
Rosario, Barbara and Marti Hearst. 2001.
Classifying the semantic relations in
noun-compounds via a domain-specific
lexical hierarchy. In Proceedings of the 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP-01),
pages 82?90, Pittsburgh, PA.
Rosario, Barbara, Marti Hearst, and Charles
Fillmore. 2002. The descent of hierarchy,
and selection in relational semantics.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL ?02), pages 417?424,
Philadelphia, PA.
Ruge, Gerda. 1992. Experiments on
linguistically-based term associations.
Information Processing and Management,
28(3):317?332.
Salton, Gerard. 1989. Automatic Text
Processing: The Transformation, Analysis, and
Retrieval of Information by Computer.
Addison-Wesley, Reading, MA.
Salton, Gerard and Chris Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5):513?523.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York, NY.
Scholkopf, Bernhard, Alexander J. Smola,
and Klaus-Robert Muller. 1997. Kernel
principal component analysis. In
Proceedings of the International Conference on
Artificial Neural Networks (ICANN-1997),
pages 583?588, Berlin.
Terra, Egidio and Charles L. A. Clarke. 2003.
Frequency estimates for statistical word
similarity measures. In Proceedings of the
Human Language Technology and North
American Chapter of Association of
Computational Linguistics Conference 2003
(HLT/NAACL 2003), pages 244?251,
Edmonton, Canada.
Turney, Peter D. 2001. Mining the Web for
synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of the Twelfth European
Conference on Machine Learning,
pages 491?502, Springer, Berlin.
Turney, Peter D. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL?02), pages 417?424,
Philadelphia, PA.
Turney, Peter D. 2005. Measuring semantic
similarity by latent relational analysis. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI-05), pages 1136?1141, Edinburgh,
Scotland.
Turney, Peter D. and Michael L. Littman.
2005. Corpus-based learning of analogies
and semantic relations. Machine Learning,
60(1?3):251?278.
Turney, Peter D., Michael L. Littman,
Jeffrey Bigham, and Victor Shnayder.
2003. Combining independent modules
to solve multiple-choice synonym and
analogy problems. In Proceedings of
the International Conference on Recent
Advances in Natural Language Processing
(RANLP-03), pages 482?489, Borovets,
Bulgaria.
Vanderwende, Lucy. 1994. Algorithm for
automatic interpretation of noun
sequences. In Proceedings of the Fifteenth
International Conference on Computational
Linguistics, pages 782?788, Kyoto,
Japan.
Veale, Tony. 2003. The analogical thesaurus.
In Proceedings of the 15th Innovative
Applications of Artificial Intelligence
Conference (IAAI 2003), pages 137?142,
Acapulco, Mexico.
Veale, Tony. 2004. WordNet sits the SAT:
A knowledge-based approach to
lexical analogy. In Proceedings of the 16th
European Conference on Artificial Intelligence
(ECAI 2004), pages 606?612, Valencia,
Spain.
Yangarber, Roman. 2003. Counter-training in
discovery of semantic patterns. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL-2003), pages 343?350, Sapporo,
Japan.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
Zelenko, Dmitry, Chinatsu Aone,
and Anthony Richardella. 2003. Kernel
methods for relation extraction.
Journal of Machine Learning Research,
3:1083?1106.
416
Thumbs Up or Thumbs Down? Semantic Orientation Applied to  
Unsupervised Classification of Reviews 
Peter D. Turney 
Institute for Information Technology 
National Research Council of Canada 
Ottawa, Ontario, Canada, K1A 0R6 
peter.turney@nrc.ca 
 
Abstract 
This paper presents a simple unsupervised 
learning algorithm for classifying reviews 
as recommended (thumbs up) or not rec-
ommended (thumbs down). The classifi-
cation of a review is predicted by the 
average semantic orientation of the 
phrases in the review that contain adjec-
tives or adverbs. A phrase has a positive 
semantic orientation when it has good as-
sociations (e.g., ?subtle nuances?) and a 
negative semantic orientation when it has 
bad associations (e.g., ?very cavalier?). In 
this paper, the semantic orientation of a 
phrase is calculated as the mutual infor-
mation between the given phrase and the 
word ?excellent? minus the mutual 
information between the given phrase and 
the word ?poor?. A review is classified as 
recommended if the average semantic ori-
entation of its phrases is positive. The al-
gorithm achieves an average accuracy of 
74% when evaluated on 410 reviews from 
Epinions, sampled from four different 
domains (reviews of automobiles, banks, 
movies, and travel destinations). The ac-
curacy ranges from 84% for automobile 
reviews to 66% for movie reviews.  
1 Introduction 
If you are considering a vacation in Akumal, Mex-
ico, you might go to a search engine and enter the 
query ?Akumal travel review?. However, in this 
case, Google1 reports about 5,000 matches. It 
would be useful to know what fraction of these 
matches recommend Akumal as a travel destina-
tion. With an algorithm for automatically classify-
ing a review as ?thumbs up? or ?thumbs down?, it 
would be possible for a search engine to report 
such summary statistics. This is the motivation for 
the research described here. Other potential appli-
cations include recognizing ?flames? (abusive 
newsgroup messages) (Spertus, 1997) and develop-
ing new kinds of search tools (Hearst, 1992).  
In this paper, I present a simple unsupervised 
learning algorithm for classifying a review as rec-
ommended or not recommended. The algorithm 
takes a written review as input and produces a 
classification as output. The first step is to use a 
part-of-speech tagger to identify phrases in the in-
put text that contain adjectives or adverbs (Brill, 
1994). The second step is to estimate the semantic 
orientation of each extracted phrase (Hatzivassi-
loglou & McKeown, 1997). A phrase has a posi-
tive semantic orientation when it has good 
associations (e.g., ?romantic ambience?) and a 
negative semantic orientation when it has bad as-
sociations (e.g., ?horrific events?). The third step is 
to assign the given review to a class, recommended 
or not recommended, based on the average seman-
tic orientation of the phrases extracted from the re-
view. If the average is positive, the prediction is 
that the review recommends the item it discusses. 
Otherwise, the prediction is that the item is not 
recommended.  
The PMI-IR algorithm is employed to estimate 
the semantic orientation of a phrase (Turney, 
2001). PMI-IR uses Pointwise Mutual Information 
(PMI) and Information Retrieval (IR) to measure 
the similarity of pairs of words or phrases. The se-
                                                          
1
 http://www.google.com 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 417-424.
                         Proceedings of the 40th Annual Meeting of the Association for
mantic orientation of a given phrase is calculated 
by comparing its similarity to a positive reference 
word (?excellent?) with its similarity to a negative 
reference word (?poor?).   More specifically, a 
phrase is assigned a numerical rating by taking the 
mutual information between the given phrase and 
the word ?excellent? and subtracting the mutual 
information between the given phrase and the word 
?poor?. In addition to determining the direction of 
the phrase?s semantic orientation (positive or nega-
tive, based on the sign of the rating), this numerical 
rating also indicates the strength of the semantic 
orientation (based on the magnitude of the num-
ber). The algorithm is presented in Section 2. 
Hatzivassiloglou and McKeown (1997) have 
also developed an algorithm for predicting seman-
tic orientation. Their algorithm performs well, but 
it is designed for isolated adjectives, rather than 
phrases containing adjectives or adverbs. This is 
discussed in more detail in Section 3, along with 
other related work. 
The classification algorithm is evaluated on 410 
reviews from Epinions2, randomly sampled from 
four different domains: reviews of automobiles, 
banks, movies, and travel destinations. Reviews at 
Epinions are not written by professional writers; 
any person with a Web browser can become a 
member of Epinions and contribute a review. Each 
of these 410 reviews was written by a different au-
thor. Of these reviews, 170 are not recommended 
and the remaining 240 are recommended (these 
classifications are given by the authors). Always 
guessing the majority class would yield an accu-
racy of 59%. The algorithm achieves an average 
accuracy of 74%, ranging from 84% for automo-
bile reviews to 66% for movie reviews. The ex-
perimental results are given in Section 4. 
The interpretation of the experimental results, 
the limitations of this work, and future work are 
discussed in Section 5. Potential applications are 
outlined in Section 6. Finally, conclusions are pre-
sented in Section 7. 
2 Classifying Reviews 
The first step of the algorithm is to extract phrases 
containing adjectives or adverbs. Past work has 
demonstrated that adjectives are good indicators of 
subjective, evaluative sentences (Hatzivassiloglou 
                                                          
2
 http://www.epinions.com 
& Wiebe, 2000; Wiebe, 2000; Wiebe et al, 2001). 
However, although an isolated adjective may indi-
cate subjectivity, there may be insufficient context 
to determine semantic orientation. For example, 
the adjective ?unpredictable? may have a negative 
orientation in an automotive review, in a phrase 
such as ?unpredictable steering?, but it could have 
a positive orientation in a movie review, in a 
phrase such as ?unpredictable plot?. Therefore the 
algorithm extracts two consecutive words, where 
one member of the pair is an adjective or an adverb 
and the second provides context. 
First a part-of-speech tagger is applied to the 
review (Brill, 1994).3 Two consecutive words are 
extracted from the review if their tags conform to 
any of the patterns in Table 1. The JJ tags indicate 
adjectives, the NN tags are nouns, the RB tags are 
adverbs, and the VB tags are verbs.4 The second 
pattern, for example, means that two consecutive 
words are extracted if the first word is an adverb 
and the second word is an adjective, but the third 
word (which is not extracted) cannot be a noun. 
NNP and NNPS (singular and plural proper nouns) 
are avoided, so that the names of the items in the 
review cannot influence the classification. 
Table 1. Patterns of tags for extracting two-word 
phrases from reviews.  
 First Word Second Word Third Word  
(Not Extracted) 
1. JJ NN or NNS anything 
2. RB, RBR, or 
RBS 
JJ not NN nor NNS 
3. JJ JJ not NN nor NNS 
4. NN or NNS JJ not NN nor NNS 
5. RB, RBR, or 
RBS 
VB, VBD, 
VBN, or VBG 
anything 
The second step is to estimate the semantic ori-
entation of the extracted phrases, using the PMI-IR 
algorithm. This algorithm uses mutual information 
as a measure of the strength of semantic associa-
tion between two words (Church & Hanks, 1989). 
PMI-IR has been empirically evaluated using 80 
synonym test questions from the Test of English as 
a Foreign Language (TOEFL), obtaining a score of 
74% (Turney, 2001). For comparison, Latent Se-
mantic Analysis (LSA), another statistical measure 
of word association, attains a score of 64% on the 
                                                          
3
 http://www.cs.jhu.edu/~brill/RBT1_14.tar.Z 
4
 See Santorini (1995) for a complete description of the tags. 
same 80 TOEFL questions (Landauer & Dumais, 
1997).  
The Pointwise Mutual Information (PMI) be-
tween two words, word1 and word2, is defined as 
follows (Church & Hanks, 1989): 
                                             p(word1 & word2) 
PMI(word1, word2) = log2 
                                             p(word1) p(word2) 
 
(1) 
Here, p(word1 & word2) is the probability that 
word1 and word2 co-occur. If the words are statisti-
cally independent, then the probability that they 
co-occur is given by the product p(word1) 
p(word2). The ratio between p(word1 & word2) and 
p(word1) p(word2) is thus a measure of the degree 
of statistical dependence between the words. The 
log of this ratio is the amount of information that 
we acquire about the presence of one of the words 
when we observe the other.  
The Semantic Orientation (SO) of a phrase, 
phrase, is calculated here as follows: 
     SO(phrase) = PMI(phrase, ?excellent?)  
                          - PMI(phrase, ?poor?) (2) 
The reference words ?excellent? and ?poor? were 
chosen because, in the five star review rating sys-
tem, it is common to define one star as ?poor? and 
five stars as ?excellent?. SO is positive when 
phrase is more strongly associated with ?excellent? 
and negative when phrase is more strongly associ-
ated with ?poor?.   
PMI-IR estimates PMI by issuing queries to a 
search engine (hence the IR in PMI-IR) and noting 
the number of hits (matching documents). The fol-
lowing experiments use the AltaVista Advanced 
Search engine5, which indexes approximately 350 
million web pages (counting only those pages that 
are in English). I chose AltaVista because it has a 
NEAR operator. The AltaVista NEAR operator 
constrains the search to documents that contain the 
words within ten words of one another, in either 
order. Previous work has shown that NEAR per-
forms better than AND when measuring the 
strength of semantic association between words 
(Turney, 2001). 
Let hits(query) be the number of hits returned, 
given the query query. The following estimate of 
SO can be derived from equations (1) and (2) with 
                                                          
5
 http://www.altavista.com/sites/search/adv 
some minor algebraic manipulation, if co-
occurrence is interpreted as NEAR: 
SO(phrase) = 
          hits(phrase NEAR ?excellent?) hits(?poor?) 
log2 
          hits(phrase NEAR ?poor?) hits(?excellent?) 
 
 
(3) 
Equation (3) is a log-odds ratio (Agresti, 1996). 
To avoid division by zero, I added 0.01 to the hits. 
I also skipped phrase when both hits(phrase 
NEAR ?excellent?) and  hits(phrase NEAR 
?poor?) were (simultaneously) less than four. 
These numbers (0.01 and 4) were arbitrarily cho-
sen. To eliminate any possible influence from the 
testing data, I added ?AND (NOT host:epinions)? 
to every query, which tells AltaVista not to include 
the Epinions Web site in its searches. 
The third step is to calculate the average seman-
tic orientation of the phrases in the given review 
and classify the review as recommended if the av-
erage is positive and otherwise not recommended.  
Table 2 shows an example for a recommended 
review and Table 3 shows an example for a not 
recommended review. Both are reviews of the 
Bank of America. Both are in the collection of 410 
reviews from Epinions that are used in the experi-
ments in Section 4. 
Table 2. An example of the processing of a review that 
the author has classified as recommended.6 
Extracted Phrase Part-of-Speech 
Tags 
Semantic 
Orientation 
online experience  JJ NN  2.253 
low fees  JJ NNS  0.333 
local branch  JJ NN  0.421 
small part  JJ NN  0.053 
online service  JJ NN  2.780 
printable version  JJ NN -0.705 
direct deposit  JJ NN  1.288 
well other  RB JJ  0.237 
inconveniently  
located  
RB VBN -1.541 
other bank  JJ NN -0.850 
true service  JJ NN -0.732 
Average Semantic Orientation  0.322 
 
                                                          
6
 The semantic orientation in the following tables is calculated 
using the natural logarithm (base e), rather than base 2. The 
natural log is more common in the literature on log-odds ratio. 
Since all logs are equivalent up to a constant factor, it makes 
no difference for the algorithm. 
Table 3. An example of the processing of a review that 
the author has classified as not recommended. 
Extracted Phrase Part-of-Speech 
Tags 
Semantic 
Orientation 
little difference  JJ NN -1.615 
clever tricks  JJ NNS -0.040 
programs such  NNS JJ  0.117 
possible moment  JJ NN -0.668 
unethical practices  JJ NNS -8.484 
low funds  JJ NNS -6.843 
old man  JJ NN -2.566 
other problems  JJ NNS -2.748 
probably wondering  RB VBG -1.830 
virtual monopoly  JJ NN -2.050 
other bank  JJ NN -0.850 
extra day  JJ NN -0.286 
direct deposits  JJ NNS  5.771 
online web  JJ NN  1.936 
cool thing  JJ NN  0.395 
very handy  RB JJ  1.349 
lesser evil  RBR JJ -2.288 
Average Semantic Orientation -1.218 
3 Related Work 
This work is most closely related to Hatzivassi-
loglou and McKeown?s (1997) work on predicting 
the semantic orientation of adjectives. They note 
that there are linguistic constraints on the semantic 
orientations of adjectives in conjunctions. As an 
example, they present the following three sen-
tences (Hatzivassiloglou &  McKeown, 1997): 
1. The tax proposal was simple and well-
received by the public. 
2. The tax proposal was simplistic but well-
received by the public. 
3. (*) The tax proposal  was simplistic and 
well-received by the public. 
The third sentence is incorrect, because we use 
?and? with adjectives that have the same semantic 
orientation (?simple? and ?well-received? are both 
positive), but we use ?but? with adjectives that 
have different semantic orientations (?simplistic? 
is negative).  
Hatzivassiloglou and McKeown (1997) use a 
four-step supervised learning algorithm to infer the 
semantic orientation of adjectives from constraints 
on conjunctions: 
1. All conjunctions of adjectives are extracted 
from the given corpus. 
2. A supervised learning algorithm combines 
multiple sources of evidence to label pairs of 
adjectives as having the same semantic orienta-
tion or different semantic orientations. The re-
sult is a graph where the nodes are adjectives 
and links indicate sameness or difference of 
semantic orientation.  
3. A clustering algorithm processes the graph 
structure to produce two subsets of adjectives, 
such that links across the two subsets are 
mainly different-orientation links, and links in-
side a subset are mainly same-orientation links. 
4. Since it is known that positive adjectives 
tend to be used more frequently than negative 
adjectives, the cluster with the higher average 
frequency is classified as having positive se-
mantic orientation. 
This algorithm classifies adjectives with accuracies 
ranging from 78% to 92%, depending on the 
amount of training data that is available. The algo-
rithm can go beyond a binary positive-negative dis-
tinction, because the clustering algorithm (step 3 
above) can produce a ?goodness-of-fit? measure 
that indicates how well an adjective fits in its as-
signed cluster.  
Although they do not consider the task of clas-
sifying reviews, it seems their algorithm could be 
plugged into the classification algorithm presented 
in Section 2, where it would replace PMI-IR and 
equation (3) in the second step. However, PMI-IR 
is conceptually simpler, easier to implement, and it 
can handle phrases and adverbs, in addition to iso-
lated adjectives. 
As far as I know, the only prior published work 
on the task of classifying reviews as thumbs up or 
down is Tong?s (2001) system for generating sen-
timent timelines. This system tracks online discus-
sions about movies and displays a plot of the 
number of positive sentiment and negative senti-
ment messages over time. Messages are classified 
by looking for specific phrases that indicate the 
sentiment of the author towards the movie (e.g., 
?great acting?, ?wonderful visuals?, ?terrible 
score?, ?uneven editing?).  Each phrase must be 
manually added to a special lexicon and manually 
tagged as indicating positive or negative sentiment. 
The lexicon is specific to the domain (e.g., movies) 
and must be built anew for each new domain. The 
company Mindfuleye7 offers a technology called 
Lexant? that appears similar to Tong?s (2001) 
system.  
Other related work is concerned with determin-
ing subjectivity (Hatzivassiloglou & Wiebe, 2000; 
Wiebe, 2000; Wiebe et al, 2001). The task is to 
distinguish sentences that present opinions and 
evaluations from sentences that objectively present 
factual information (Wiebe, 2000). Wiebe et al 
(2001) list a variety of potential applications for 
automated subjectivity tagging, such as recogniz-
ing ?flames? (Spertus, 1997), classifying email, 
recognizing speaker role in radio broadcasts, and 
mining reviews. In several of these applications, 
the first step is to recognize that the text is subjec-
tive and then the natural second step is to deter-
mine the semantic orientation of the subjective 
text. For example, a flame detector cannot merely 
detect that a newsgroup message is subjective, it 
must further detect that the message has a negative 
semantic orientation; otherwise a message of praise 
could be classified as a flame.  
Hearst (1992) observes that most search en-
gines focus on finding documents on a given topic, 
but do not allow the user to specify the directional-
ity of the documents (e.g., is the author in favor of, 
neutral, or opposed to the event or item discussed 
in the document?). The directionality of a docu-
ment is determined by its deep argumentative 
structure, rather than a shallow analysis of its ad-
jectives. Sentences are interpreted metaphorically 
in terms of agents exerting force, resisting force, 
and overcoming resistance. It seems likely that 
there could be some benefit to combining shallow 
and deep analysis of the text.  
4 Experiments 
Table 4 describes the 410 reviews from Epinions 
that were used in the experiments. 170 (41%) of 
the reviews are not recommended and the remain-
ing 240 (59%) are recommended. Always guessing 
the majority class would yield an accuracy of 59%. 
The third column shows the average number of 
phrases that were extracted from the reviews. 
Table 5 shows the experimental results. Except 
for the travel reviews, there is surprisingly little 
variation in the accuracy within a domain. In addi-
                                                          
7
 http://www.mindfuleye.com/ 
tion to recommended and not recommended, Epin-
ions reviews are classified using the five star rating 
system. The third column shows the correlation be-
tween the average semantic orientation and the 
number of stars assigned by the author of the re-
view. The results show a strong positive correla-
tion between the average semantic orientation and 
the author?s rating out of five stars. 
Table 4. A summary of the corpus of reviews. 
Domain of Review Number of 
Reviews 
Average 
Phrases per 
Review 
Automobiles  75 20.87 
Honda  Accord         37        18.78 
Volkswagen Jetta        38        22.89 
Banks 120 18.52 
Bank of America        60        22.02 
Washington Mutual        60        15.02 
Movies 120 29.13 
The Matrix       60        19.08 
Pearl Harbor       60        39.17 
Travel Destinations  95 35.54 
Cancun       59        30.02 
Puerto Vallarta       36        44.58 
All 410 26.00 
Table 5. The accuracy of the classification and the cor-
relation of the semantic orientation with the star rating. 
Domain of Review Accuracy Correlation 
Automobiles 84.00 % 0.4618 
Honda Accord      83.78 %      0.2721 
Volkswagen Jetta      84.21 %      0.6299 
Banks 80.00 % 0.6167 
Bank of America      78.33 %      0.6423 
Washington Mutual      81.67 %      0.5896 
Movies 65.83 % 0.3608 
The Matrix      66.67 %      0.3811 
Pearl Harbor      65.00 %      0.2907 
Travel Destinations 70.53 % 0.4155 
Cancun      64.41 %      0.4194 
Puerto Vallarta      80.56 %      0.1447 
All 74.39 % 0.5174 
5 Discussion of Results 
A natural question, given the preceding results, is 
what makes movie reviews hard to classify? Table 
6 shows that classification by the average SO tends 
to err on the side of guessing that a review is not 
recommended, when it is actually recommended. 
This suggests the hypothesis that a good movie 
will often contain unpleasant scenes (e.g., violence, 
death, mayhem), and a recommended movie re-
view may thus have its average semantic orienta-
tion reduced if it contains descriptions of these un-
pleasant scenes. However, if we add a constant 
value to the average SO of the movie reviews, to 
compensate for this bias, the accuracy does not 
improve. This suggests that, just as positive re-
views mention unpleasant things, so negative re-
views often mention pleasant scenes. 
Table 6. The confusion matrix for movie classifications. 
 Author?s Classification 
Average  
Semantic 
Orientation 
Thumbs 
Up 
Thumbs 
Down 
Sum 
Positive  28.33 %  12.50 %  40.83 % 
Negative  21.67 %  37.50 %  59.17 % 
Sum  50.00 %  50.00 % 100.00 % 
Table 7 shows some examples that lend support 
to this hypothesis. For example, the phrase ?more 
evil? does have negative connotations, thus an SO 
of -4.384 is appropriate, but an evil character does 
not make a bad movie. The difficulty with movie 
reviews is that there are two aspects to a movie, the 
events and actors in the movie (the elements of the 
movie), and the style and art of the movie (the 
movie as a gestalt; a unified whole). This is likely 
also the explanation for the lower accuracy of the 
Cancun reviews: good beaches do not necessarily 
add up to a good vacation. On the other hand, good 
automotive parts usually do add up to a good 
automobile and good banking services add up to a 
good bank. It is not clear how to address this issue. 
Future work might look at whether it is possible to 
tag sentences as discussing elements or wholes. 
Another area for future work is to empirically 
compare PMI-IR and the algorithm of Hatzivassi-
loglou and McKeown (1997). Although their algo-
rithm does not readily extend to two-word phrases, 
I have not yet demonstrated that two-word phrases 
are necessary for accurate classification of reviews. 
On the other hand, it would be interesting to evalu-
ate PMI-IR on the collection of 1,336 hand-labeled 
adjectives that were used in the experiments of 
Hatzivassiloglou and McKeown (1997). A related 
question for future work is the relationship of  ac-
curacy of the estimation of semantic orientation at 
the level of individual phrases to accuracy of re-
view classification. Since the review classification 
is based on an average, it might be quite resistant 
to noise in the SO estimate for individual phrases. 
But it is possible that a better SO estimator could 
produce significantly better classifications. 
Table 7. Sample phrases from misclassified reviews. 
Movie:  The Matrix 
Author?s Rating: recommended (5 stars) 
Average SO: -0.219 (not recommended) 
Sample Phrase:  more evil    [RBR JJ] 
SO of Sample 
Phrase:  
-4.384 
Context of Sample 
Phrase: 
The slow, methodical way he 
spoke. I loved it! It made him 
seem more arrogant and even 
more evil. 
Movie: Pearl Harbor 
Author?s Rating: recommended (5 stars) 
Average SO: -0.378 (not recommended) 
Sample Phrase:  sick feeling    [JJ NN] 
SO of Sample 
Phrase:  
-8.308 
Context of Sample 
Phrase: 
During this period I had a sick 
feeling, knowing what was 
coming, knowing what was 
part of our history. 
Movie: The Matrix 
Author?s Rating: not recommended (2 stars) 
Average SO: 0.177 (recommended) 
Sample Phrase:  very talented    [RB JJ] 
SO of Sample 
Phrase:  
1.992 
Context of Sample 
Phrase: 
Well as usual Keanu Reeves is 
nothing special, but surpris-
ingly, the very talented Laur-
ence Fishbourne is not so good 
either, I was surprised. 
Movie: Pearl Harbor 
Author?s Rating: not recommended (3 stars) 
Average SO: 0.015 (recommended) 
Sample Phrase:  blue skies    [JJ NNS] 
SO of Sample 
Phrase:  
1.263 
Context of Sample 
Phrase: 
Anyone who saw the trailer in 
the theater over the course of 
the last year will never forget 
the images of Japanese war 
planes swooping out of the 
blue skies, flying past the 
children playing baseball, or 
the truly remarkable shot of a 
bomb falling from an enemy 
plane into the deck of the USS 
Arizona. 
Equation (3) is a very simple estimator of se-
mantic orientation. It might benefit from more so-
phisticated statistical analysis  (Agresti, 1996). One 
possibility is to apply a statistical significance test 
to each estimated SO. There is a large statistical 
literature on the log-odds ratio, which might lead 
to improved results on this task. 
This paper has focused on unsupervised classi-
fication, but average semantic orientation could be 
supplemented by other features, in a supervised 
classification system. The other features could be 
based on the presence or absence of specific 
words, as is common in most text classification 
work. This could yield higher accuracies, but the 
intent here was to study this one feature in isola-
tion, to simplify the analysis, before combining it 
with other features. 
Table 5 shows a high correlation between the 
average semantic orientation and the star rating of 
a review. I plan to experiment with ordinal classi-
fication of reviews in the five star rating system, 
using the algorithm of Frank and Hall (2001). For 
ordinal classification, the average semantic orienta-
tion would be supplemented with other features in 
a supervised classification system. 
A limitation of PMI-IR is the time required to 
send queries to AltaVista. Inspection of Equation 
(3) shows that it takes four queries to calculate the 
semantic orientation of a phrase. However, I 
cached all query results, and since there is no need 
to recalculate hits(?poor?) and hits(?excellent?) for 
every phrase, each phrase requires an average of 
slightly less than two queries. As a courtesy to 
AltaVista, I used a five second delay between que-
ries.8 The 410 reviews yielded 10,658 phrases, so 
the total time required to process the corpus was 
roughly 106,580 seconds, or about 30 hours. 
This might appear to be a significant limitation, 
but extrapolation of current trends in computer 
memory capacity suggests that, in about ten years, 
the average desktop computer will be able to easily 
store and search AltaVista?s 350 million Web 
pages. This will reduce the processing time to less 
than one second per review. 
6 Applications 
There are a variety of potential applications for 
automated review rating. As mentioned in the in-
                                                          
8
 This line of research depends on the good will of the major 
search engines. For a discussion of the ethics of Web robots, 
see http://www.robotstxt.org/wc/robots.html. For query robots, 
the proposed extended standard for robot exclusion would be 
useful. See http://www.conman.org/people/spc/robots2.html.  
troduction, one application is to provide summary 
statistics for search engines. Given the query 
?Akumal travel review?, a search engine could re-
port, ?There are 5,000 hits, of which 80% are 
thumbs up and 20% are thumbs down.? The search 
results could be sorted by average semantic orien-
tation, so that the user could easily sample the most 
extreme reviews. Similarly, a search engine could 
allow the user to specify the topic and the rating of 
the desired reviews (Hearst, 1992).  
Preliminary experiments indicate that semantic 
orientation is also useful for summarization of re-
views. A positive review could be summarized by 
picking out the sentence with the highest positive 
semantic orientation and a negative review could 
be summarized by extracting the sentence with the 
lowest negative semantic orientation.  
Epinions asks its reviewers to provide a short 
description of pros and cons for the reviewed item. 
A pro/con summarizer could be evaluated by 
measuring the overlap between the reviewer?s pros 
and cons and the phrases in the review that have 
the most extreme semantic orientation. 
Another potential application is filtering 
?flames? for newsgroups (Spertus, 1997). There 
could be a threshold, such that a newsgroup mes-
sage is held for verification by the human modera-
tor when the semantic orientation of a phrase drops 
below the threshold. A related use might be a tool 
for helping academic referees when reviewing 
journal and conference papers. Ideally, referees are 
unbiased and objective, but sometimes their criti-
cism can be unintentionally harsh. It might be pos-
sible to highlight passages in a draft referee?s 
report, where the choice of words should be modi-
fied towards a more neutral tone. 
Tong?s (2001) system for detecting and track-
ing opinions in on-line discussions could benefit 
from the use of a learning algorithm, instead of (or 
in addition to) a hand-built lexicon. With auto-
mated review rating (opinion rating), advertisers 
could track advertising campaigns, politicians 
could track public opinion, reporters could track 
public response to current events, stock traders 
could track financial opinions, and trend analyzers 
could track entertainment and technology trends.  
7 Conclusions 
This paper introduces a simple unsupervised learn-
ing algorithm for rating a review as thumbs up or 
down. The algorithm has three steps: (1) extract 
phrases containing adjectives or adverbs, (2) esti-
mate the semantic orientation of each phrase, and 
(3) classify the review based on the average se-
mantic orientation of the phrases. The core of the 
algorithm is the second step, which uses PMI-IR to 
calculate semantic orientation (Turney, 2001).  
In experiments with 410 reviews from Epin-
ions, the algorithm attains an average accuracy of 
74%. It appears that movie reviews are difficult to 
classify, because the whole is not necessarily the 
sum of the parts; thus the accuracy on movie re-
views is about 66%. On the other hand, for banks 
and automobiles, it seems that the whole is the sum 
of the parts, and the accuracy is 80% to 84%. 
Travel reviews are an intermediate case. 
Previous work on determining the semantic ori-
entation of adjectives has used a complex algo-
rithm that does not readily extend beyond isolated 
adjectives to adverbs or longer phrases (Hatzivassi-
loglou and McKeown, 1997). The simplicity of 
PMI-IR may encourage further work with semantic 
orientation. 
The limitations of this work include the time 
required for queries and, for some applications, the 
level of accuracy that was achieved. The former 
difficulty will be eliminated by progress in hard-
ware. The latter difficulty might be addressed by 
using semantic orientation combined with other 
features in a supervised classification algorithm. 
Acknowledgements 
Thanks to Joel Martin and Michael Littman for 
helpful comments. 
References 
Agresti, A. 1996. An introduction to categorical data 
analysis. New York: Wiley.  
Brill, E. 1994. Some advances in transformation-based 
part of speech tagging. Proceedings of the Twelfth 
National Conference on Artificial Intelligence (pp. 
722-727). Menlo Park, CA: AAAI Press. 
Church, K.W., & Hanks, P. 1989. Word association 
norms, mutual information and lexicography. Pro-
ceedings of the 27th Annual Conference of the ACL 
(pp. 76-83). New Brunswick, NJ: ACL. 
Frank, E., & Hall, M. 2001. A simple approach to ordi-
nal classification. Proceedings of the Twelfth Euro-
pean Conference on Machine Learning (pp. 145-
156). Berlin: Springer-Verlag. 
Hatzivassiloglou, V., & McKeown, K.R. 1997. Predict-
ing the semantic orientation of adjectives. Proceed-
ings of the 35th Annual Meeting of the ACL and the 
8th Conference of the European Chapter of the ACL 
(pp. 174-181). New Brunswick, NJ: ACL. 
Hatzivassiloglou, V., & Wiebe, J.M. 2000. Effects of 
adjective orientation and gradability on sentence sub-
jectivity. Proceedings of 18th International Confer-
ence on Computational Linguistics. New Brunswick, 
NJ: ACL. 
Hearst, M.A. 1992. Direction-based text interpretation 
as an information access refinement. In P. Jacobs 
(Ed.), Text-Based Intelligent Systems: Current Re-
search and Practice in Information Extraction and 
Retrieval. Mahwah, NJ: Lawrence Erlbaum Associ-
ates. 
Landauer, T.K., & Dumais, S.T. 1997. A solution to 
Plato?s problem: The latent semantic analysis theory 
of the acquisition, induction, and representation of 
knowledge. Psychological Review, 104, 211-240.  
Santorini, B. 1995. Part-of-Speech Tagging Guidelines 
for the Penn Treebank Project (3rd revision, 2nd 
printing). Technical Report, Department of Computer 
and Information Science, University of Pennsylvania. 
Spertus, E. 1997. Smokey: Automatic recognition of 
hostile messages. Proceedings of the Conference on 
Innovative Applications of Artificial Intelligence (pp. 
1058-1065). Menlo Park, CA: AAAI Press. 
Tong, R.M. 2001. An operational system for detecting 
and tracking opinions in on-line discussions. Working 
Notes of the ACM SIGIR 2001 Workshop on Opera-
tional Text Classification (pp. 1-6). New York, NY: 
ACM. 
Turney, P.D. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. Proceedings of the 
Twelfth European Conference on Machine Learning 
(pp. 491-502). Berlin: Springer-Verlag. 
Wiebe, J.M. 2000. Learning subjective adjectives from 
corpora. Proceedings of the 17th National Confer-
ence on Artificial Intelligence. Menlo Park, CA: 
AAAI Press. 
Wiebe, J.M., Bruce, R., Bell, M., Martin, M., & Wilson, 
T. 2001. A corpus study of evaluative and specula-
tive language. Proceedings of the Second ACL SIG 
on Dialogue Workshop on Discourse and Dialogue. 
Aalborg, Denmark.  
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 313?320,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Expressing Implicit Semantic Relations without Supervision 
 
Peter D. Turney 
Institute for Information Technology 
National Research Council Canada 
M-50 Montreal Road 
Ottawa, Ontario, Canada, K1A 0R6 
peter.turney@nrc-cnrc.gc.ca 
 
 
Abstract 
We present an unsupervised learning al-
gorithm that mines large text corpora for 
patterns that express implicit semantic re-
lations. For a given input word pair 
YX :  with some unspecified semantic 
relations, the corresponding output list of 
patterns mPP ,,1   is ranked according 
to how well each pattern iP  expresses the 
relations between X  and Y . For exam-
ple, given ostrich=X  and bird=Y , the 
two highest ranking output patterns are 
?X is the largest Y? and ?Y such as the 
X?. The output patterns are intended to 
be useful for finding further pairs with 
the same relations, to support the con-
struction of lexicons, ontologies, and se-
mantic networks. The patterns are sorted 
by pertinence, where the pertinence of a 
pattern iP  for a word pair YX :  is the 
expected relational similarity between the 
given pair and typical pairs for iP . The 
algorithm is empirically evaluated on two 
tasks, solving multiple-choice SAT word 
analogy questions and classifying seman-
tic relations in noun-modifier pairs. On 
both tasks, the algorithm achieves state-
of-the-art results, performing signifi-
cantly better than several alternative pat-
tern ranking algorithms, based on tf-idf.  
1 Introduction 
In a widely cited paper, Hearst (1992) showed 
that the lexico-syntactic pattern ?Y such as the 
X? can be used to mine large text corpora for 
word pairs YX :  in which X is a hyponym (type) 
of Y. For example, if we search in a large corpus 
using the pattern ?Y such as the X? and we find 
the string ?bird such as the ostrich?, then we can 
infer that ?ostrich? is a hyponym of ?bird?. Ber-
land and Charniak (1999) demonstrated that the 
patterns ?Y?s X? and ?X of the Y? can be used to 
mine corpora for pairs YX :  in which X is a 
meronym (part) of Y (e.g., ?wheel of the car?). 
Here we consider the inverse of this problem: 
Given a word pair YX :  with some unspecified 
semantic relations, can we mine a large text cor-
pus for lexico-syntactic patterns that express the 
implicit relations between X  and Y ? For exam-
ple, if we are given the pair ostrich:bird, can we 
discover the pattern ?Y such as the X?? We are 
particularly interested in discovering high quality 
patterns that are reliable for mining further word 
pairs with the same semantic relations. 
In our experiments, we use a corpus of web 
pages containing about 10105 ?  English words 
(Terra and Clarke, 2003). From co-occurrences 
of the pair ostrich:bird in this corpus, we can 
generate 516 patterns of the form ?X ... Y? and 
452 patterns of the form ?Y ... X?. Most of these 
patterns are not very useful for text mining. The 
main challenge is to find a way of ranking the 
patterns, so that patterns like ?Y such as the X? 
are highly ranked. Another challenge is to find a 
way to empirically evaluate the performance of 
any such pattern ranking algorithm. 
For a given input word pair YX :  with some 
unspecified semantic relations, we rank the cor-
responding output list of patterns mPP ,,1   in 
order of decreasing pertinence. The pertinence of 
a pattern iP  for a word pair YX :  is the expected 
relational similarity between the given pair and 
typical pairs that fit iP . We define pertinence 
more precisely in Section 2.  
Hearst (1992) suggests that her work may be 
useful for building a thesaurus. Berland and 
Charniak (1999) suggest their work may be use-
ful for building a lexicon or ontology, like 
WordNet. Our algorithm is also applicable to 
these tasks. Other potential applications and re-
lated problems are discussed in Section 3. 
To calculate pertinence, we must be able to 
measure relational similarity. Our measure is 
based on Latent Relational Analysis (Turney, 
2005). The details are given in Section 4. 
Given a word pair YX : , we want our algo-
rithm to rank the corresponding list of patterns 
313
mPP ,,1   according to their value for mining 
text, in support of semantic network construction 
and similar tasks. Unfortunately, it is difficult to 
measure performance on such tasks. Therefore 
our experiments are based on two tasks that pro-
vide objective performance measures.  
In Section 5, ranking algorithms are compared 
by their performance on solving multiple-choice 
SAT word analogy questions. In Section 6, they 
are compared by their performance on classify-
ing semantic relations in noun-modifier pairs. 
The experiments demonstrate that ranking by 
pertinence is significantly better than several al-
ternative pattern ranking algorithms, based on 
tf-idf. The performance of pertinence on these 
two tasks is slightly below the best performance 
that has been reported so far (Turney, 2005), but 
the difference is not statistically significant. 
We discuss the results in Section 7 and con-
clude in Section 8.  
2 Pertinence 
The relational similarity between two pairs of 
words, 11 :YX  and 22 :YX , is the degree to 
which their semantic relations are analogous. For 
example, mason:stone and carpenter:wood have 
a high degree of relational similarity. Measuring 
relational similarity will be discussed in Sec-
tion 4. For now, assume that we have a measure 
of the relational similarity between pairs of 
words, ??):,:(sim 2211r YXYX .  
Let }:,,:{ 11 nn YXYXW =  be a set of word 
pairs and let },,{ 1 mPPP =  be a set of patterns. 
The pertinence of pattern iP  to a word pair 
jj YX :  is the expected relational similarity be-
tween a word pair kk YX : , randomly selected 
from W  according to the probability distribution 
):(p ikk PYX , and the word pair jj YX : : 
),:(pertinence ijj PYX  

=
?=
n
k
kkjjikk YXYXPYX
1
r ):,:(sim):(p  
The conditional probability ):(p ikk PYX  can be 
interpreted as the degree to which the pair 
kk YX :  is representative (i.e., typical) of pairs 
that fit the pattern iP . That is, iP  is pertinent to 
jj YX :  if highly typical word pairs kk YX :  for 
the pattern iP  tend to be relationally similar to 
jj YX : .  
Pertinence tends to be highest with patterns 
that are unambiguous. The maximum value of 
),:(pertinence ijj PYX  is attained when the pair 
jj YX :  belongs to a cluster of highly similar 
pairs and the conditional probability distribution 
):(p ikk PYX  is concentrated on the cluster. An 
ambiguous pattern, with its probability spread 
over multiple clusters, will have less pertinence. 
If a pattern with high pertinence is used for 
text mining, it will tend to produce word pairs 
that are very similar to the given word pair; this 
follows from the definition of pertinence. We 
believe this definition is the first formal measure 
of quality for text mining patterns. 
Let ikf ,  be the number of occurrences in a 
corpus of the word pair kk YX :  with the pattern 
iP . We could estimate ):(p ikk PYX  as follows: 

=
=
n
j
ijikikk ffPYX
1
,,):(p  
Instead, we first estimate ):(p kki YXP : 

=
=
m
j
jkikkki ffYXP
1
,,):(p  
Then we apply Bayes? Theorem: 

=
?
?
=
n
j
jjijj
kkikk
ikk
YXPYX
YXPYX
PYX
1
):p():p(
):p():p():p(  
We assume nYX jj 1):p( =  for all pairs in W : 

=
=
n
j
jjikkiikk YXPYXPPYX
1
):p():p():p(  
The use of Bayes? Theorem and the assumption 
that nYX jj 1):p( =  for all word pairs is a way 
of smoothing the probability ):(p ikk PYX , simi-
lar to Laplace smoothing. 
3 Related Work 
Hearst (1992) describes a method for finding 
patterns like ?Y such as the X?, but her method 
requires human judgement. Berland and 
Charniak (1999) use Hearst?s manual procedure.  
Riloff and Jones (1999) use a mutual boot-
strapping technique that can find patterns auto-
matically, but the bootstrapping requires an ini-
tial seed of manually chosen examples for each 
class of words. Miller et al (2000) propose an 
approach to relation extraction that was evalu-
ated in the Seventh Message Understanding Con-
ference (MUC7). Their algorithm requires la-
beled examples of each relation. Similarly, Ze-
lenko et al (2003) use a supervised kernel 
method that requires labeled training examples. 
Agichtein and Gravano (2000) also require train-
ing examples for each relation. Brin (1998) uses 
bootstrapping from seed examples of author:title 
pairs to discover patterns for mining further pairs. 
Yangarber et al (2000) and Yangarber (2003) 
present an algorithm that can find patterns auto-
matically, but it requires an initial seed of manu-
ally designed patterns for each semantic relation. 
Stevenson (2004) uses WordNet to extract rela-
tions from text, but also requires initial seed pat-
terns for each relation.  
314
Lapata (2002) examines the task of expressing 
the implicit relations in nominalizations, which 
are noun compounds whose head noun is derived 
from a verb and whose modifier can be inter-
preted as an argument of the verb. In contrast 
with this work, our algorithm is not restricted to 
nominalizations. Section 6 shows that our algo-
rithm works with arbitrary noun compounds and 
the SAT questions in Section 5 include all nine 
possible pairings of nouns, verbs, and adjectives. 
As far as we know, our algorithm is the first 
unsupervised learning algorithm that can find 
patterns for semantic relations, given only a large 
corpus (e.g., in our experiments, about 10105 ?  
words) and a moderately sized set of word pairs 
(e.g., 600 or more pairs in the experiments), such 
that the members of each pair appear together 
frequently in short phrases in the corpus. These 
word pairs are not seeds, since the algorithm 
does not require the pairs to be labeled or 
grouped; we do not assume they are homogenous.   
The word pairs that we need could be gener-
ated automatically, by searching for word pairs 
that co-occur frequently in the corpus. However, 
our evaluation methods (Sections 5 and 6) both 
involve a predetermined list of word pairs. If our 
algorithm were allowed to generate its own word 
pairs, the overlap with the predetermined lists 
would likely be small. This is a limitation of our 
evaluation methods rather than the algorithm. 
Since any two word pairs may have some rela-
tions in common and some that are not shared, 
our algorithm generates a unique list of patterns 
for each input word pair. For example, ma-
son:stone and carpenter:wood share the pattern 
?X carves Y?, but the patterns ?X nails Y? and 
?X bends Y? are unique to carpenter:wood. The 
ranked list of patterns for a word pair YX :  
gives the relations between X and Y in the corpus, 
sorted with the most pertinent (i.e., characteristic, 
distinctive, unambiguous) relations first. 
Turney (2005) gives an algorithm for measur-
ing the relational similarity between two pairs of 
words, called Latent Relational Analysis (LRA). 
This algorithm can be used to solve multiple-
choice word analogy questions and to classify 
noun-modifier pairs (Turney, 2005), but it does 
not attempt to express the implicit semantic rela-
tions. Turney (2005) maps each pair YX :  to a 
high-dimensional vector v . The value of each 
element iv  in v

 is based on the frequency, for 
the pair YX : , of a corresponding pattern iP . 
The relational similarity between two pairs, 
11 :YX  and 22 :YX , is derived from the cosine of 
the angle between their two vectors. A limitation 
of this approach is that the semantic content of 
the vectors is difficult to interpret; the magnitude 
of an element iv  is not a good indicator of how 
well the corresponding pattern iP  expresses a 
relation of YX : . This claim is supported by the 
experiments in Sections 5 and 6. 
Pertinence (as defined in Section 2) builds on 
the measure of relational similarity in Turney 
(2005), but it has the advantage that the semantic 
content can be interpreted; we can point to spe-
cific patterns and say that they express the im-
plicit relations. Furthermore, we can use the pat-
terns to find other pairs with the same relations. 
Hearst (1992) processed her text with a part-
of-speech tagger and a unification-based con-
stituent analyzer. This makes it possible to use 
more general patterns. For example, instead of 
the literal string pattern ?Y such as the X?, where 
X and Y are words, Hearst (1992) used the more 
abstract pattern ? 0NP  such as 1NP ?, where iNP  
represents a noun phrase. For the sake of sim-
plicity, we have avoided part-of-speech tagging, 
which limits us to literal patterns. We plan to 
experiment with tagging in future work. 
4 The Algorithm 
The algorithm takes as input a set of word pairs 
}:,,:{ 11 nn YXYXW =  and produces as output 
ranked lists of patterns mPP ,,1   for each input 
pair. The following steps are similar to the algo-
rithm of Turney (2005), with several changes to 
support the calculation of pertinence. 
1. Find phrases: For each pair ii YX : , make a 
list of phrases in the corpus that contain the pair. 
We use the Waterloo MultiText System (Clarke 
et al, 1998) to search in a corpus of about 
10105 ?  English words (Terra and Clarke, 2003). 
Make one list of phrases that begin with iX  and 
end with iY  and a second list for the opposite 
order. Each phrase must have one to three inter-
vening words between iX  and iY . The first and 
last words in the phrase do not need to exactly 
match iX  and iY . The MultiText query language 
allows different suffixes. Veale (2004) has ob-
served that it is easier to identify semantic rela-
tions between nouns than between other parts of 
speech. Therefore we use WordNet 2.0 (Miller, 
1995) to guess whether iX  and iY  are likely to 
be nouns. When they are nouns, we are relatively 
strict about suffixes; we only allow variation in 
pluralization. For all other parts of speech, we 
are liberal about suffixes. For example, we allow 
an adjective such as ?inflated? to match a noun 
such as ?inflation?. With MultiText, the query 
?inflat*? matches both ?inflated? and ?inflation?. 
2. Generate patterns: For each list of phrases, 
generate a list of patterns, based on the phrases. 
Replace the first word in each phrase with the 
generic marker ?X? and replace the last word 
with ?Y?. The intervening words in each phrase 
315
may be either left as they are or replaced with the 
wildcard ?*?. For example, the phrase ?carpenter 
nails the wood? yields the patterns ?X nails the 
Y?, ?X nails * Y?, ?X * the Y?, and ?X * * Y?. 
Do not allow duplicate patterns in a list, but note 
the number of times a pattern is generated for 
each word pair ii YX :  in each order ( iX  first and 
iY  last or vice versa). We call this the pattern 
frequency. It is a local frequency count, analo-
gous to term frequency in information retrieval. 
3. Count pair frequency: The pair frequency 
for a pattern is the number of lists from the pre-
ceding step that contain the given pattern. It is a 
global frequency count, analogous to document 
frequency in information retrieval. Note that a 
pair ii YX :  yields two lists of phrases and hence 
two lists of patterns. A given pattern might ap-
pear in zero, one, or two of the lists for ii YX : . 
4. Map pairs to rows: In preparation for build-
ing a matrix X , create a mapping of word pairs 
to row numbers. For each pair ii YX : , create a 
row for ii YX :  and another row for ii XY : . If W 
does not already contain }:,,:{ 11 nn XYXY  , 
then we have effectively doubled the number of 
word pairs, which increases the sample size for 
calculating pertinence. 
5. Map patterns to columns: Create a mapping 
of patterns to column numbers. For each unique 
pattern of the form ?X ... Y? from Step 2, create 
a column for the original pattern ?X ... Y? and 
another column for the same pattern with X and 
Y swapped, ?Y ... X?. Step 2 can generate mil-
lions of distinct patterns. The experiment in Sec-
tion 5 results in 1,706,845 distinct patterns, 
yielding 3,413,690 columns. This is too many 
columns for matrix operations with today?s stan-
dard desktop computer. Most of the patterns have 
a very low pair frequency. For the experiment in 
Section 5, 1,371,702 of the patterns have a pair 
frequency of one. To keep the matrix X  man-
ageable, we drop all patterns with a pair fre-
quency less than ten. For Section 5, this leaves 
42,032 patterns, yielding 84,064 columns. Tur-
ney (2005) limited the matrix to 8,000 columns, 
but a larger pool of patterns is better for our pur-
poses, since it increases the likelihood of finding 
good patterns for expressing the semantic rela-
tions of a given word pair. 
6. Build a sparse matrix: Build a matrix X  in 
sparse matrix format. The value for the cell in 
row i and column j is the pattern frequency of the 
j-th pattern for the the i-th word pair.  
7. Calculate entropy: Apply log and entropy 
transformations to the sparse matrix X  (Lan-
dauer and Dumais, 1997). Each cell is replaced 
with its logarithm, multiplied by a weight based 
on the negative entropy of the corresponding 
column vector in the matrix. This gives more 
weight to patterns that vary substantially in fre-
quency for each pair. 
8. Apply SVD: After log and entropy transforms, 
apply the Singular Value Decomposition (SVD) 
to X  (Golub and Van Loan, 1996). SVD de-
composes X  into a product of three matrices 
TVU? , where U  and V  are in column or-
thonormal form (i.e., the columns are orthogonal 
and have unit length) and ?  is a diagonal matrix 
of singular values (hence SVD). If X  is of rank 
r , then ?  is also of rank r . Let k? , where 
rk < , be the diagonal matrix formed from the 
top k  singular values, and let kU  and kV  be the 
matrices produced by selecting the correspond-
ing columns from U  and V . The matrix 
T
kkk VU ?  is the matrix of rank k  that best ap-
proximates the original matrix X , in the sense 
that it minimizes the approximation errors 
(Golub and Van Loan, 1996). Following Lan-
dauer and Dumais (1997), we use 300=k . We 
may think of this matrix Tkkk VU ?  as a smoothed 
version of the original matrix. SVD is used to 
reduce noise and compensate for sparseness 
(Landauer and Dumais, 1997). 
9. Calculate cosines: The relational similarity 
between two pairs, ):,:(sim 2211r YXYX , is 
given by the cosine of the angle between their 
corresponding row vectors in the matrix 
T
kkk VU ?  (Turney, 2005). To calculate perti-
nence, we will need the relational similarity be-
tween all possible pairs of pairs. All of the co-
sines can be efficiently derived from the matrix 
T
kkkk )( ?? UU  (Landauer and Dumais, 1997). 
10. Calculate conditional probabilities: Using 
Bayes? Theorem (see Section 2) and the raw fre-
quency data in the matrix X  from Step 6, before 
log and entropy transforms, calculate the condi-
tional probability ):(p jii PYX  for every row 
(word pair) and every column (pattern). 
11. Calculate pertinence: With the cosines from 
Step 9 and the conditional probabilities from 
Step 10, calculate ),:(pertinence jii PYX  for 
every row ii YX :  and every column jP  for 
which 0):(p >jii PYX . When 0):(p =jii PYX , 
it is possible that 0),:(pertinence >jii PYX , but 
we avoid calculating pertinence in these cases for 
two reasons. First, it speeds computation, be-
cause X  is sparse, so 0):(p =jii PYX  for most 
rows and columns. Second, 0):(p =jii PYX  im-
plies that the pattern jP  does not actually appear 
with the word pair ii YX :  in the corpus; we are 
only guessing that the pattern is appropriate for 
the word pair, and we could be wrong. Therefore 
we prefer to limit ourselves to patterns and word 
pairs that have actually been observed in the cor-
pus. For each pair ii YX :  in W, output two sepa-
rate ranked lists, one for patterns of the form 
?X ? Y? and another for patterns of the form 
316
?Y ? X?, where the patterns in both lists are 
sorted in order of decreasing pertinence to ii YX : . 
Ranking serves as a kind of normalization. We 
have found that the relative rank of a pattern is 
more reliable as an indicator of its importance 
than the absolute pertinence. This is analogous to 
information retrieval, where documents are 
ranked in order of their relevance to a query. The 
relative rank of a document is more important 
than its actual numerical score (which is usually 
hidden from the user of a search engine). Having 
two separate ranked lists helps to avoid bias. For 
example, ostrich:bird generates 516 patterns of 
the form ?X ... Y? and 452 patterns of the form 
?Y ... X?. Since there are more patterns of the 
form ?X ... Y?, there is a slight bias towards 
these patterns. If the two lists were merged, the 
?Y ... X? patterns would be at a disadvantage. 
5 Experiments with Word Analogies 
In these experiments, we evaluate pertinence us-
ing 374 college-level multiple-choice word 
analogies, taken from the SAT test. For each 
question, there is a target word pair, called the 
stem pair, and five choice pairs. The task is to 
find the choice that is most analogous (i.e., has 
the highest relational similarity) to the stem. This 
choice pair is called the solution and the other 
choices are distractors. Since there are six word 
pairs per question (the stem and the five choices), 
there are 22446374 =?  pairs in the input set W. 
In Step 4 of the algorithm, we double the pairs, 
but we also drop some pairs because they do not 
co-occur in the corpus. This leaves us with 4194 
rows in the matrix. As mentioned in Step 5, the 
matrix has 84,064 columns (patterns). The sparse 
matrix density is 0.91%. 
To answer a SAT question, we generate 
ranked lists of patterns for each of the six word 
pairs. Each choice is evaluated by taking the in-
tersection of its patterns with the stem?s patterns. 
The shared patterns are scored by the average of 
their rank in the stem?s lists and the choice?s lists. 
Since the lists are sorted in order of decreasing 
pertinence, a low score means a high pertinence. 
Our guess is the choice with the lowest scoring 
shared pattern. 
Table 1 shows three examples, two questions 
that are answered correctly followed by one that 
is answered incorrectly. The correct answers are 
in bold font. For the first question, the stem is 
ostrich:bird and the best choice is (a) lion:cat. 
The highest ranking pattern that is shared by both 
of these pairs is ?Y such as the X?. The third 
question illustrates that, even when the answer is 
incorrect, the best shared pattern (?Y powered * 
* X?) may be plausible. 
 Word pair Best shared pattern Score 
1. ostrich:bird   
(a) lion:cat ?Y such as the X? 1.0 
(b) goose:flock ?X * * breeding Y? 43.5 
(c) ewe:sheep ?X are the only Y? 13.5 
(d) cub:bear ?Y are called X? 29.0 
(e) primate:monkey ?Y is the * X? 80.0 
2. traffic:street   
(a) ship:gangplank ?X * down the Y? 53.0 
(b) crop:harvest ?X * adjacent * Y? 248.0 
(c) car:garage ?X * a residential Y? 63.0 
(d) pedestrians:feet ?Y * accommodate X? 23.0 
(e) water:riverbed ?Y that carry X? 17.0 
3. locomotive:train   
(a) horse:saddle ?X carrying * Y? 82.0 
(b) tractor:plow ?X pulled * Y? 7.0 
(c) rudder:rowboat ?Y * X? 319.0 
(d) camel:desert ?Y with two X? 43.0 
(e) gasoline:automobile ?Y powered * * X? 5.0 
Table 1. Three examples of SAT questions. 
Table 2 shows the four highest ranking pat-
terns for the stem and solution for the first exam-
ple. The pattern ?X lion Y? is anomalous, but the 
other patterns seem reasonable. The shared pat-
tern ?Y such as the X? is ranked 1 for both pairs, 
hence the average score for this pattern is 1.0, as 
shown in Table 1. Note that the ?ostrich is the 
largest bird? and ?lions are large cats?, but the 
largest cat is the Siberian tiger. 
Word pair ?X ... Y? ?Y ... X? 
ostrich:bird ?X is the largest Y? ?Y such as the X? 
 ?X is * largest Y? ?Y such * the X? 
lion:cat ?X lion Y? ?Y such as the X? 
 ?X are large Y? ?Y and mountain X? 
Table 2. The highest ranking patterns. 
Table 3 lists the top five pairs in W that match 
the pattern ?Y such as the X?. The pairs are 
sorted by ):(p PYX . The pattern ?Y such as the 
X? is one of 146 patterns that are shared by os-
trich:bird and lion:cat. Most of these shared pat-
terns are not very informative. 
Word pair Conditional probability 
heart:organ 0.49342 
dodo:bird 0.08888 
elbow:joint 0.06385 
ostrich:bird 0.05774 
semaphore:signal 0.03741 
Table 3. The top five pairs for ?Y such as the X?. 
In Table 4, we compare ranking patterns by 
pertinence to ranking by various other measures, 
mostly based on varieties of tf-idf (term fre-
quency times inverse document frequency, a 
common way to rank documents in information 
retrieval). The tf-idf measures are taken from 
Salton and Buckley (1988). For comparison, we 
also include three algorithms that do not rank 
317
patterns (the bottom three rows in the table). 
These three algorithms can answer the SAT 
questions, but they do not provide any kind of 
explanation for their answers. 
 Algorithm Prec. Rec. F 
1 pertinence (Step 11) 55.7 53.5 54.6 
2 log and entropy matrix  
(Step 7) 
43.5 41.7 42.6 
3 TF = f, IDF = log((N-n)/n) 43.2 41.4 42.3 
4 TF = log(f+1), IDF = log(N/n) 42.9 41.2 42.0 
5 TF = f, IDF = log(N/n) 42.9 41.2 42.0 
6 TF = log(f+1), 
IDF = log((N-n)/n) 
42.3 40.6 41.4 
7 TF = 1.0, IDF = 1/n 41.5 39.8 40.6 
8 TF = f, IDF = 1/n 41.5 39.8 40.6 
9 TF = 0.5 + 0.5 * (f/F), 
IDF = log(N/n) 
41.5 39.8 40.6 
10 TF = log(f+1), IDF = 1/n 41.2 39.6 40.4 
11 p(X:Y|P) (Step 10) 39.8 38.2 39.0 
12 SVD matrix (Step 8) 35.9 34.5 35.2 
13 random 27.0 25.9 26.4 
14 TF = 1/f, IDF = 1.0 26.7 25.7 26.2 
15 TF = f, IDF = 1.0 (Step 6) 18.1 17.4 17.7 
16 Turney (2005) 56.8 56.1 56.4 
17 Turney and Littman (2005) 47.7 47.1 47.4 
18 Veale (2004) 42.8 42.8 42.8 
Table 4. Performance of various algorithms on SAT. 
All of the pattern ranking algorithms are given 
exactly the same sets of patterns to rank. Any 
differences in performance are due to the ranking 
method alone. The algorithms may skip ques-
tions when the word pairs do not co-occur in the 
corpus. All of the ranking algorithms skip the 
same set of 15 of the 374 SAT questions. Preci-
sion is defined as the percentage of correct an-
swers out of the questions that were answered 
(not skipped). Recall is the percentage of correct 
answers out of the maximum possible number 
correct (374). The F measure is the harmonic 
mean of precision and recall. 
For the tf-idf methods in Table 4, f is the pat-
tern frequency, n is the pair frequency, F is the 
maximum f for all patterns for the given word 
pair, and N is the total number of word pairs. By 
?TF = f, IDF = n/1 ?, for example (row 8), we 
mean that f plays a role that is analogous to term 
frequency and n/1  plays a role that is analogous 
to inverse document frequency. That is, in row 8, 
the patterns are ranked in decreasing order of 
pattern frequency divided by pair frequency. 
Table 4 also shows some ranking methods 
based on intermediate calculations in the algo-
rithm in Section 4. For example, row 2 in Table 4 
gives the results when patterns are ranked in or-
der of decreasing values in the corresponding 
cells of the matrix X  from Step 7.  
Row 12 in Table 4 shows the results we would 
get using Latent Relational Analysis (Turney, 
2005) to rank patterns. The results in row 12 
support the claim made in Section 3, that LRA is 
not suitable for ranking patterns, although it 
works well for answering the SAT questions (as 
we see in row 16). The vectors in LRA yield a 
good measure of relational similarity, but the 
magnitude of the value of a specific element in a 
vector is not a good indicator of the quality of the 
corresponding pattern.  
The best method for ranking patterns is perti-
nence (row 1 in Table 4). As a point of compari-
son, the performance of the average senior 
highschool student on the SAT analogies is about 
57% (Turney and Littman, 2005). The second 
best method is to use the values in the matrix X  
after the log and entropy transformations in 
Step 7 (row 2). The difference between these two 
methods is statistically significant with 95% con-
fidence. Pertinence (row 1) performs slightly 
below Latent Relational Analysis (row 16; Tur-
ney, 2005), but the difference is not significant.  
Randomly guessing answers should yield an F 
of 20% (1 out of 5 choices), but ranking patterns 
randomly (row 13) results in an F of 26.4%. This 
is because the stem pair tends to share more pat-
terns with the solution pair than with the distrac-
tors. The minimum of a large set of random 
numbers is likely to be lower than the minimum 
of a small set of random numbers. 
6 Experiments with Noun-Modifiers 
In these experiments, we evaluate pertinence on 
the task of classifying noun-modifier pairs. The 
problem is to classify a noun-modifier pair, such 
as ?flu virus?, according to the semantic relation 
between the head noun (virus) and the modifier 
(flu). For example, ?flu virus? is classified as a 
causality relation (the flu is caused by a virus). 
For these experiments, we use a set of 600 
manually labeled noun-modifier pairs (Nastase 
and Szpakowicz, 2003). There are five general 
classes of labels with thirty subclasses. We pre-
sent here the results with five classes; the results 
with thirty subclasses follow the same trends 
(that is, pertinence performs significantly better 
than the other ranking methods). The five classes 
are causality (storm cloud), temporality (daily 
exercise), spatial (desert storm), participant 
(student protest), and quality (expensive book).  
The input set W consists of the 600 noun-
modifier pairs. This set is doubled in Step 4, but 
we drop some pairs because they do not co-occur 
in the corpus, leaving us with 1184 rows in the 
matrix. There are 16,849 distinct patterns with a 
pair frequency of ten or more, resulting in 33,698 
columns. The matrix density is 2.57%. 
318
To classify a noun-modifier pair, we use a sin-
gle nearest neighbour algorithm with leave-one-
out cross-validation. We split the set 600 times. 
Each pair gets a turn as the single testing exam-
ple, while the other 599 pairs serve as training 
examples. The testing example is classified ac-
cording to the label of its nearest neighbour in 
the training set. The distance between two noun-
modifier pairs is measured by the average rank of 
their best shared pattern. Table 5 shows the re-
sulting precision, recall, and F, when ranking 
patterns by pertinence. 
Class name Prec. Rec. F Class size 
causality 37.3 36.0 36.7 86 
participant 61.1 64.4 62.7 260 
quality 49.3 50.7 50.0 146 
spatial 43.9 32.7 37.5 56 
temporality 64.7 63.5 64.1 52 
all 51.3 49.5 50.2 600 
Table 5. Performance on noun-modifiers. 
To gain some insight into the algorithm, we 
examined the 600 best shared patterns for each 
pair and its single nearest neighbour. For each of 
the five classes, Table 6 lists the most frequent 
pattern among the best shared patterns for the 
given class. All of these patterns seem appropri-
ate for their respective classes. 
Class Most frequent pattern Example pair 
causality ?Y * causes X? ?cold virus? 
participant ?Y of his X? ?dream analysis? 
quality ?Y made of X? ?copper coin? 
spatial ?X * * terrestrial Y? ?aquatic mammal? 
temporality ?Y in * early X? ?morning frost? 
Table 6. Most frequent of the best shared patterns. 
Table 7 gives the performance of pertinence 
on the noun-modifier problem, compared to 
various other pattern ranking methods. The bot-
tom two rows are included for comparison; they 
are not pattern ranking algorithms. The best 
method for ranking patterns is pertinence (row 1 
in Table 7). The difference between pertinence 
and the second best ranking method (row 2) is 
statistically significant with 95% confidence. 
Latent Relational Analysis (row 16) performs 
slightly better than pertinence (row 1), but the 
difference is not statistically significant. 
Row 6 in Table 7 shows the results we would 
get using Latent Relational Analysis (Turney, 
2005) to rank patterns. Again, the results support 
the claim in Section 3, that LRA is not suitable 
for ranking patterns. LRA can classify the noun-
modifiers (as we see in row 16), but it cannot 
express the implicit semantic relations that make 
an unlabeled noun-modifier in the testing set 
similar to its nearest neighbour in the training set. 
 Algorithm Prec. Rec. F 
1 pertinence (Step 11) 51.3 49.5 50.2 
2 TF = log(f+1), IDF = 1/n 37.4 36.5 36.9 
3 TF = log(f+1), IDF = log(N/n) 36.5 36.0 36.2 
4 TF = log(f+1),  
IDF = log((N-n)/n) 
36.0 35.4 35.7 
5 TF = f, IDF = log((N-n)/n) 36.0 35.3 35.6 
6 SVD matrix (Step 8) 43.9 33.4 34.8 
7 TF = f, IDF = 1/n 35.4 33.6 34.3 
8 log and entropy matrix  
(Step 7) 
35.6 33.3 34.1 
9 TF = f, IDF = log(N/n) 34.1 31.4 32.2 
10 TF = 0.5 + 0.5 * (f/F),  
IDF = log(N/n) 
31.9 31.7 31.6 
11 p(X:Y|P) (Step 10) 31.8 30.8 31.2 
12 TF = 1.0, IDF = 1/n 29.2 28.8 28.7 
13 random 19.4 19.3 19.2 
14 TF = 1/f, IDF = 1.0 20.3 20.7 19.2 
15 TF = f, IDF = 1.0 (Step 6) 12.8 19.7 8.0 
16 Turney (2005) 55.9 53.6 54.6 
17 Turney and Littman (2005) 43.4 43.1 43.2 
Table 7. Performance on noun-modifiers. 
7 Discussion 
Computing pertinence took about 18 hours for 
the experiments in Section 5 and 9 hours for Sec-
tion 6. In both cases, the majority of the time was 
spent in Step 1, using MultiText (Clarke et al, 
1998) to search through the corpus of 10105 ?  
words. MultiText was running on a Beowulf 
cluster with sixteen 2.4 GHz Intel Xeon CPUs. 
The corpus and the search index require about 
one terabyte of disk space. This may seem com-
putationally demanding by today?s standards, but 
progress in hardware will soon allow an average 
desktop computer to handle corpora of this size. 
Although the performance on the SAT anal-
ogy questions (54.6%) is near the level of the 
average senior highschool student (57%), there is 
room for improvement. For applications such as 
building a thesaurus, lexicon, or ontology, this 
level of performance suggests that our algorithm 
could assist, but not replace, a human expert. 
One possible improvement would be to add 
part-of-speech tagging or parsing. We have done 
some preliminary experiments with parsing and 
plan to explore tagging as well. A difficulty is 
that much of the text in our corpus does not con-
sist of properly formed sentences, since the text 
comes from web pages. This poses problems for 
most part-of-speech taggers and parsers. 
8 Conclusion 
Latent Relational Analysis (Turney, 2005) pro-
vides a way to measure the relational similarity 
between two word pairs, but it gives us little in-
sight into how the two pairs are similar. In effect, 
319
LRA is a black box. The main contribution of 
this paper is the idea of pertinence, which allows 
us to take an opaque measure of relational simi-
larity and use it to find patterns that express the 
implicit semantic relations between two words. 
The experiments in Sections 5 and 6 show that 
ranking patterns by pertinence is superior to 
ranking them by a variety of tf-idf methods. On 
the word analogy and noun-modifier tasks, perti-
nence performs as well as the state-of-the-art, 
LRA, but pertinence goes beyond LRA by mak-
ing relations explicit.  
Acknowledgements 
Thanks to Joel Martin, David Nadeau, and Deniz 
Yuret for helpful comments and suggestions. 
References  
Eugene Agichtein and Luis Gravano. 2000. Snowball: 
Extracting relations from large plain-text collec-
tions. In Proceedings of the Fifth ACM Conference 
on Digital Libraries (ACM DL 2000), pages 85-94. 
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics (ACL-99), pages 57-64. 
Sergey Brin. 1998. Extracting patterns and relations 
from the World Wide Web. In WebDB Workshop 
at the 6th International Conference on Extending 
Database Technology (EDBT-98), pages 172-183. 
Charles L.A. Clarke, Gordon V. Cormack, and Chris-
topher R. Palmer. 1998. An overview of MultiText. 
ACM SIGIR Forum, 32(2):14-15. 
Gene H. Golub and Charles F. Van Loan. 1996. Ma-
trix Computations. Third edition. Johns Hopkins 
University Press, Baltimore, MD. 
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of 
the 14th International Conference on Computa-
tional Linguistics (COLING-92), pages 539-545. 
Thomas K. Landauer and Susan T. Dumais. 1997. A 
solution to Plato?s problem: The latent semantic 
analysis theory of the acquisition, induction, and 
representation of knowledge. Psychological Review, 
104(2):211-240. 
Maria Lapata. 2002. The disambiguation of nominali-
sations. Computational Linguistics, 28(3):357-388. 
George A. Miller. 1995. WordNet: A lexical database 
for English. Communications of the ACM, 
38(11):39-41. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical pars-
ing to extract information from text. In Proceed-
ings of the Sixth Applied Natural Language Proc-
essing Conference (ANLP 2000), pages 226-233. 
Vivi Nastase and Stan Szpakowicz. 2003. Exploring 
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics 
(IWCS-5), pages 285-301. 
Ellen Riloff and Rosie Jones. 1999. Learning diction-
aries for information extraction by multi-level 
bootstrapping. In Proceedings of the 16th National 
Conference on Artificial Intelligence (AAAI-99), 
pages 474-479. 
Gerard Salton and Chris Buckley. 1988. Term weight-
ing approaches in automatic text retrieval. Informa-
tion Processing and Management, 24(5):513-523. 
Mark Stevenson. 2004. An unsupervised WordNet-
based algorithm for relation extraction. In Proceed-
ings of the Fourth International Conference on 
Language Resources and Evaluation (LREC) 
Workshop, Beyond Named Entity Recognition: Se-
mantic Labelling for NLP Tasks, Lisbon, Portugal. 
Egidio Terra and Charles L.A. Clarke. 2003. Fre-
quency estimates for statistical word similarity 
measures. In Proceedings of the Human Language 
Technology and North American Chapter of Asso-
ciation of Computational Linguistics Conference 
(HLT/NAACL-03), pages 244-251. 
Peter D. Turney. 2005. Measuring semantic similarity 
by latent relational analysis. In Proceedings of the 
Nineteenth International Joint Conference on Arti-
ficial Intelligence (IJCAI-05), pages 1136-1141. 
Peter D. Turney and Michael L. Littman. 2005. Cor-
pus-based learning of analogies and semantic rela-
tions. Machine Learning, 60(1-3):251-278. 
Tony Veale. 2004. WordNet sits the SAT: A knowl-
edge-based approach to lexical analogy. In Pro-
ceedings of the 16th European Conference on Arti-
ficial Intelligence (ECAI 2004), pages 606-612. 
Roman Yangarber, Ralph Grishman, Pasi Tapanainen, 
and Silja Huttunen. 2000. Unsupervised discovery 
of scenario-level patterns for information extrac-
tion. In Proceedings of the Sixth Applied Natural 
Language Processing Conference (ANLP 2000), 
pages 282-289. 
Roman Yangarber. 2003. Counter-training in discov-
ery of semantic patterns. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-03), pages 343-350. 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extrac-
tion. Journal of Machine Learning Research, 
3:1083-1106. 
320
Word Sense Disambiguation by Web Mining
for Word Co-occurrence Probabilities
Peter D. TURNEY
Institute for Information Technology
National Research Council of Canada
Ottawa, Ontario, Canada, K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Abstract
This paper describes the National Research Coun-
cil (NRC) Word Sense Disambiguation (WSD) sys-
tem, as applied to the English Lexical Sample (ELS)
task in Senseval-3. The NRC system approach-
es WSD as a classical supervised machine learn-
ing problem, using familiar tools such as the Weka
machine learning software and Brill?s rule-based
part-of-speech tagger. Head words are represent-
ed as feature vectors with several hundred features.
Approximately half of the features are syntactic and
the other half are semantic. The main novelty in the
system is the method for generating the semantic
features, based on word co-occurrence probabilities.
The probabilities are estimated using the Waterloo
MultiText System with a corpus of about one ter-
abyte of unlabeled text, collected by a web crawler.
1 Introduction
The Senseval-3 English Lexical Sample (ELS) task
requires disambiguating 57 words, with an average
of roughly 140 training examples and 70 testing
examples of each word. Each example is about a
paragraph of text, in which the word that is to be dis-
ambiguated is marked as the head word. The aver-
age head word has around six senses. The training
examples are manually classified according to the
intended sense of the head word, inferred from the
surrounding context. The task is to use the training
data and any other relevant information to automat-
ically assign classes to the testing examples.
This paper presents the National Research Coun-
cil (NRC) Word Sense Disambiguation (WSD)
system, which generated our four entries for
the Senseval-3 ELS task (NRC-Fine, NRC-Fine2,
NRC-Coarse, and NRC-Coarse2). Our approach to
the ELS task is to treat it as a classical supervised
machine learning problem. Each example is repre-
sented as a feature vector with several hundred fea-
tures. Each of the 57 ambiguous words is represent-
ed with a different set of features. Typically, around
half of the features are syntactic and the other half
are semantic. After the raw examples are converted
to feature vectors, the Weka machine learning soft-
ware is used to induce a model of the training data
and predict the classes of the testing examples (Wit-
ten and Frank, 1999).
The syntactic features are based on part-of-
speech tags, assigned by a rule-based tagger (Brill,
1994). The main innovation of the NRC WSD sys-
tem is the method for generating the semantic fea-
tures, which are derived from word co-occurrence
probabilities. We estimated these probabilities
using the Waterloo MultiText System with a corpus
of about one terabyte of unlabeled text, collected by
a web crawler (Clarke et al, 1995; Clarke and Cor-
mack, 2000; Terra and Clarke, 2003).
In Section 2, we describe the NRC WSD system.
Our experimental results are presented in Section 3
and we conclude in Section 4.
2 System Description
This section presents various aspects of the system
in roughly the order in which they are executed. The
following definitions will simplify the description.
Head Word: One of the 57 words that are to be
disambiguated.
Example: One or more contiguous sentences, illus-
trating the usage of a head word.
Context: The non-head words in an example.
Feature: A property of a head word in a context.
For instance, the feature tag hp1 NNP is the prop-
erty of having (or not having) a proper noun (NNP
is the part-of-speech tag for a proper noun) immedi-
ately following the head word (hp1 represents the
location head plus one).
Feature Value: Features have values, which
depend on the specific example. For instance,
tag hp1 NNP is a binary feature that has the value
1 (true: the following word is a proper noun) or 0
(false: the following word is not a proper noun).
Feature Vector: Each example is represented by
a vector. Features are the dimensions of the vector
space and a vector of feature values specifies a point
in the feature space.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.1 Preprocessing
The NRC WSD system first assigns part-of-speech
tags to the words in a given example (Brill, 1994),
and then extracts a nine-word window of tagged
text, centered on the head word (i.e., four words
before and after the head word). Any remaining
words in the example are ignored (usually most of
the example is ignored). The window is not allowed
to cross sentence boundaries. If the head word
appears near the beginning or end of the sentence,
where the window may overlap with adjacent sen-
tences, special null characters fill the positions of
any missing words in the window.
In rare cases, a head word appears more than once
in an example. In such cases, the system selects
a single window, giving preference to the earliest
occurring window with the least nulls. Thus each
example is converted into one nine-word window of
tagged text. Windows from the training examples
for a given head word are then used to build the fea-
ture set for that head word.
2.2 Syntactic Features
Each head word has a unique set of feature names,
describing how the feature values are calculated.
Feature Names: Every syntactic feature has a name
of the form matchtype position model. There are
three matchtypes, ptag, tag, and word, in order
of increasingly strict matching. A ptag match is
a partial tag match, which counts similar part-of-
speech tags, such as NN (singular noun), NNS (plu-
ral noun), NNP (singular proper noun), and NNPS
(plural proper noun), as equivalent. A tag match
requires exact matching in the part-of-speech tags
for the word and the model. A wordmatch requires
that the word and the model are exactly the same,
letter-for-letter, including upper and lower case.
There are five positions, hm2 (head minus two),
hm1 (head minus one), hd0 (head), hp1 (head plus
one), and hp2 (head plus two). Thus syntactic fea-
tures use only a five-word sub-window of the nine-
word window.
The syntactic feature names for a head word
are generated by all of the possible legal combina-
tions of matchtype, position, and model. For ptag
names, the model can be any partial tag. For tag
names, the model can be any tag. For word names,
the model names are not predetermined; they are
extracted from the training windows for the given
head word. For instance, if a training window con-
tains the head word followed by ?of?, then one of
the features will be word hp1 of.
For word names, the model names are not
allowed to be words that are tagged as nouns, verbs,
or adjectives. These words are reserved for use in
building the semantic features.
Feature Values: The syntactic features are all
binary-valued. Given a feature with a name of the
form matchtype position model, the feature value
for a given window depends on whether there is a
match of matchtype between the word in the posi-
tion position and the model model. For instance,
the value of tag hp1 NNP depends on whether
the given window has a word in the position hp1
(head plus one) with a tag (part-of-speech tag) that
matches NNP (proper noun). Similarly, the feature
word hp1 of has the value 1 (true) if the given
window contains the head word followed by ?of?;
otherwise, it has the value 0 (false).
2.3 Semantic Features
Each head word has a unique set of feature names,
describing how the feature values are calculated.
Feature Names: Most of the semantic features have
names of the form position model. The position
names can be pre (preceding) or fol (following).
They refer to the nearest noun, verb, or adjective
that precedes or follows the head word in the nine-
word window.
The model names are extracted from the training
windows for the head word. For instance, if a train-
ing window contains the word ?compelling?, and
this word is the nearest noun, verb, or adjective that
precedes the head word, then one of the features will
be pre compelling.
A few of the semantic features have a different
form of name, avg position sense. In names of this
form, position can be pre (preceding) or fol (fol-
lowing), and sense can be any of the possible senses
(i.e., classes, labels) of the head word.
Feature Values: The semantic features are all
real-valued. For feature names of the form posi-
tion model, the feature value depends on the seman-
tic similarity between the word in position position
and the model word model.
The semantic similarity between two words is
estimated by their Pointwise Mutual Information,
   
     
, using Information Retrieval (Turney,
2001; Terra and Clarke, 2003):
   


 

 	 

 

    


 


    
We estimate the probabilities in this equation by
issuing queries to the Waterloo MultiText System
(Clarke et al, 1995; Clarke and Cormack, 2000;
Terra and Clarke, 2003). Laplace smoothing is
applied to the PMI estimates, to avoid division by
zero.
weka.classifiers.meta.Bagging
-W weka.classifiers.meta.MultiClassClassifier
-W weka.classifiers.meta.Vote
-B weka.classifiers.functions.supportVector.SMO
-B weka.classifiers.meta.LogitBoost -W weka.classifiers.trees.DecisionStump
-B weka.classifiers.meta.LogitBoost -W weka.classifiers.functions.SimpleLinearRegression
-B weka.classifiers.trees.adtree.ADTree
-B weka.classifiers.rules.JRip
Table 1: Weka (version 3.4) commands for processing the feature vectors.
  
      has a value of zero when the two
words are statistically independent. A high posi-
tive value indicates that the two words tend to co-
occur, and hence are likely to be semantically relat-
ed. A negative value indicates that the presence of
one of the words suggests the absence of the other.
Past work demonstrates that PMI is a good estima-
tor of semantic similarity (Turney, 2001; Terra and
Clarke, 2003) and that features based on PMI can be
useful for supervised learning (Turney, 2003).
The Waterloo MultiText System allows us to set
the neighbourhood size for co-occurrence (i.e., the
meaning of    ). In preliminary experiments
with the ELS data from Senseval-2, we got good
results with a neighbourhood size of 20 words.
For instance, if  is the noun, verb, or adjec-
tive that precedes the head word and is nearest to
the head word in a given window, then the value
of pre compelling is
   
    


. If
there is no preceding noun, verb, or adjective within
the window, the value is set to zero.
In names of the form avg position sense, the
feature value is the average of the feature values of
the corresponding features. For instance, the val-
ue of avg pre argument 1 10 02 is the aver-
age of the values of all of the pre model features,
such that model was extracted from a training win-
dow in which the head word was labeled with the
sense argument 1 10 02.
The idea here is that, if a testing example should
be labeled, say, argument 1 10 02, and   is a
noun, verb, or adjective that is close to the head
word in the testing example, then        
should be relatively high when  is extract-
ed from a training window with the same sense,
argument 1 10 02, but relatively low when 
is extracted from a training window with a different
sense. Thus avg position argument 1 10 02
is likely to be relatively high, compared to other
avg position sense features.
All semantic features with names of the form
position model are normalized by converting them
to percentiles. The percentiles are calculated sepa-
rately for each feature vector; that is, each feature
vector is normalized internally, with respect to its
own values, not externally, with respect to the oth-
er feature vectors. The pre features are normalized
independently from the fol features. The semantic
features with names of the form avg position sense
are calculated after the other features are normal-
ized, so they do not need any further normalization.
Preliminary experiments with the ELS data from
Senseval-2 supported the merit of percentile nor-
malization, which was also found useful in another
application where features based on PMI were used
for supervised learning (Turney, 2003).
2.4 Weka Configuration
Table 1 shows the commands that were used to exe-
cute Weka (Witten and Frank, 1999). The default
parameters were used for all of the classifiers. Five
base classifiers (-B) were combined by voting. Mul-
tiple classes were handled by treating them as mul-
tiple two-class problems, using a 1-against-all strat-
egy. Finally, the variance of the system was reduced
with bagging.
We designed the Weka configuration by evalu-
ating many different Weka base classifiers on the
Senseval-2 ELS data, until we had identified five
good base classifiers. We then experimented with
combining the base classifiers, using a variety of
meta-learning algorithms. The resulting system is
somewhat similar to the JHU system, which had
the best ELS scores in Senseval-2 (Yarowsky et al,
2001). The JHU system combined four base clas-
sifiers using a form of voting, called Thresholded
Model Voting (Yarowsky et al, 2001).
2.5 Postprocessing
The output of Weka includes an estimate of the
probability for each prediction. When the head
word is frequently labeled U (unassignable) in the
training examples, we ignore U examples during
training, and then, after running Weka, relabel the
lowest probability testing examples as U.
3 Results
A total of 26 teams entered 47 systems (both
supervised and unsupervised) in the Senseval-3
ELS task. Table 2 compares the fine-grained and
System Fine-Grained Recall Coarse-Grained Recall
Best Senseval-3 System 72.9% 79.5%
NRC-Fine 69.4% 75.9%
NRC-Fine2 69.1% 75.6%
NRC-Coarse NA 75.8%
NRC-Coarse2 NA 75.7%
Median Senseval-3 System 65.1% 73.7%
Most Frequent Sense 55.2% 64.5%
Table 2: Comparison of NRC-Fine with other Senseval-3 ELS systems.
coarse-grained scores of our four entries with other
Senseval-3 systems.
With NRC-Fine and NRC-Coarse, each seman-
tic feature was scored by calculating its PMI with
the head word, and then low scoring semantic fea-
tures were dropped. With NRC-Fine2 and NRC-
Coarse2, the threshold for dropping features was
changed, so that many more features were retained.
The Senseval-3 results suggest that it is better to
drop more features.
NRC-Coarse and NRC-Coarse2 were designed to
maximize the coarse score, by training them with
data in which the senses were relabeled by their
coarse sense equivalence classes. The fine scores
for these two systems are meaningless and should be
ignored. The Senseval-3 results indicate that there
is no advantage to relabeling.
The NRC systems scored roughly midway
between the best and median systems. This per-
formance supports the hypothesis that corpus-based
semantic features can be useful for WSD. In future
work, we plan to design a system that combines
corpus-based semantic features with the most effec-
tive elements of the other Senseval-3 systems.
For reasons of computational efficiency, we chose
a relatively narrow window of nine-words around
the head word. We intend to investigate whether a
larger window would bring the system performance
up to the level of the best Senseval-3 system.
4 Conclusion
This paper has sketched the NRC WSD system for
the ELS task in Senseval-3. Due to space limita-
tions, many details were omitted, but it is likely that
their impact on the performance is relatively small.
The system design is relatively straightforward
and classical. The most innovative aspect of the sys-
tem is the set of semantic features, which are purely
corpus-based; no lexicon was used.
Acknowledgements
We are very grateful to Egidio Terra, Charlie Clarke,
and the School of Computer Science of the Univer-
sity of Waterloo, for giving us a copy of the Water-
loo MultiText System. Thanks to Diana Inkpen,
Joel Martin, and Mario Jarmasz for helpful discus-
sions. Thanks to the organizers of Senseval for their
service to the WSD research community. Thanks to
Eric Brill and the developers of Weka, for making
their software available.
References
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In Proceedings of
the 12th National Conference on Artificial Intel-
ligence (AAAI-94), pages 722?727.
Charles L.A. Clarke and Gordon V. Cormack. 2000.
Shortest substring retrieval and ranking. ACM
Transactions on Information Systems (TOIS),
18(1):44?78.
Charles L.A. Clarke, G.V. Cormack, and F.J.
Burkowski. 1995. An algebra for structured text
search and a framework for its implementation.
The Computer Journal, 38(1):43?56.
Egidio L. Terra and Charles L.A. Clarke. 2003.
Frequency estimates for statistical word similari-
ty measures. In Proceedings of the Human Lan-
guage Technology and North American Chapter
of Association of Computational Linguistics Con-
ference 2003 (HLT/NAACL 2003), pages 244?
251.
Peter D. Turney. 2001. Mining the Web for syn-
onyms: PMI-IR versus LSA on TOEFL. In Pro-
ceedings of the Twelfth European Conference
on Machine Learning (ECML-2001), pages 491?
502.
Peter D. Turney. 2003. Coherent keyphrase extrac-
tion via Web mining. In Proceedings of the Eigh-
teenth International Joint Conference on Artifi-
cial Intelligence (IJCAI-03), pages 434?439.
Ian H. Witten and Eibe Frank. 1999. Data Min-
ing: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan
Kaufmann, San Mateo, CA.
D. Yarowsky, S. Cucerzan, R. Florian, C. Schafer,
and R. Wicentowski. 2001. The Johns Hopkins
SENSEVAL2 system descriptions. In Proceed-
ings of SENSEVAL2, pages 163?166.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals
Roxana Girju
Univ. of Illinois
at Urbana-Champaign
Urbana, IL 61801
girju@uiuc.edu
Preslav Nakov
Univ. of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany 69118
nastase@eml-research.de
Stan Szpakowicz
University of Ottawa
Ottawa, ON K1N 6N5
szpak@site.uottawa.ca
Peter Turney
National Research Council of Canada
Ottawa, ON K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Deniz Yuret
Koc? University
Istanbul, Turkey 34450
dyuret@ku.edu.tr
Abstract
The NLP community has shown a renewed
interest in deeper semantic analyses, among
them automatic recognition of relations be-
tween pairs of words in a text. We present an
evaluation task designed to provide a frame-
work for comparing different approaches to
classifying semantic relations between nom-
inals in a sentence. This is part of SemEval,
the 4th edition of the semantic evaluation
event previously known as SensEval. We de-
fine the task, describe the training/test data
and their creation, list the participating sys-
tems and discuss their results. There were
14 teams who submitted 15 systems.
1 Task Description and Related Work
The theme of Task 4 is the classification of semantic
relations between simple nominals (nouns or base
noun phrases) other than named entities ? honey
bee, for example, shows an instance of the Product-
Producer relation. The classification occurs in the
context of a sentence in a written English text. Al-
gorithms for classifying semantic relations can be
applied in information retrieval, information extrac-
tion, text summarization, question answering and so
on. The recognition of textual entailment (Tatu and
Moldovan, 2005) is an example of successful use of
this type of deeper analysis in high-end NLP appli-
cations.
The literature shows a wide variety of methods
of nominal relation classification. They depend as
much on the training data as on the domain of ap-
plication and the available resources. Rosario and
Hearst (2001) classify noun compounds from the
domain of medicine, using 13 classes that describe
the semantic relation between the head noun and
the modifier in a given noun compound. Rosario
et al (2002) classify noun compounds using the
MeSH hierarchy and a multi-level hierarchy of se-
mantic relations, with 15 classes at the top level.
Nastase and Szpakowicz (2003) present a two-level
hierarchy for classifying noun-modifier relations in
base noun phrases from general text, with 5 classes
at the top and 30 classes at the bottom; other re-
searchers (Turney and Littman, 2005; Turney, 2005;
Nastase et al, 2006) have used their class scheme
and data set. Moldovan et al (2004) propose a 35-
class scheme to classify relations in various phrases;
the same scheme has been applied to noun com-
pounds and other noun phrases (Girju et al, 2005).
Chklovski and Pantel (2004) introduce a 5-class set,
designed specifically for characterizing verb-verb
semantic relations. Stephens et al (2001) propose
17 classes targeted to relations between genes. La-
pata (2002) presents a binary classification of rela-
tions in nominalizations.
There is little consensus on the relation sets and
algorithms for analyzing semantic relations, and it
seems unlikely that any single scheme could work
for all applications. For example, the gene-gene re-
lation scheme of Stephens et al (2001), with rela-
tions like X phosphorylates Y, is unlikely to be trans-
ferred easily to general text.
We have created a benchmark data set to allow the
evaluation of different semantic relation classifica-
tion algorithms. We do not presume to propose a sin-
gle classification scheme, however alluring it would
13
Relation Training data Test data Agreement Example
positive set size positive set size (independent tagging)
Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)
Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)
Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)
Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)
Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)
Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)
Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)
Table 1: Data set statistics
be to try to design a unified standard ? it would be
likely to have shortcomings just as any of the others
we have just reviewed. Instead, we have decided to
focus on separate semantic relations that many re-
searchers list in their relation sets. We have built an-
notated data sets for seven such relations. Every data
set supports a separate binary classification task.
2 Building the Annotated Data Sets
Ours is a new evaluation task, so we began with data
set creation and annotation guidelines. The data set
that Nastase and Szpakowicz (2003) created had re-
lation labels and part-of-speech and WordNet sense
annotations, to facilitate classification. (Moldovan
et al, 2004; Girju et al, 2005) gave the annotators
an example of each phrase in a sentence along with
WordNet senses and position of arguments. Our
annotations include all these, to support a variety
of methods (since we work with relations between
nominals, the part of speech is always noun). We
have used WordNet 3.0 on the Web and sense index
tags.
We chose the following semantic relations:
Cause-Effect, Content-Container, Instrument-
Agency, Origin-Entity, Part-Whole, Product-
Producer and Theme-Tool. We wrote seven detailed
definitions, including restrictions and conventions,
plus prototypical positive and near-miss negative
examples. For each relation separately, we based
data collection on wild-card search patterns that
Google allows. We built the patterns manually,
following Hearst (1992) and Nakov and Hearst
(2006). Instances of the relation Content-Container,
for example, come up in response to queries such as
?* contains *?, ?* holds *?, ?the * in the *?. Fol-
lowing the model of the Senseval-3 English Lexical
Sample Task, we set out to collect 140 training and
at least 70 test examples per relation, so we had a
number of different patterns to ensure variety. We
also aimed to collect a balanced number of positive
and negative examples. The use of heuristic patterns
to search for both positive and negative examples
should naturally result in negative examples that
are near misses. We believe that near misses are
more useful for supervised learning than negative
examples that are generated randomly.
?Among the contents of the <e1>vessel</e1>
were a set of carpenter?s <e2>tools</e2>, sev-
eral large storage jars, ceramic utensils, ropes and
remnants of food, as well as a heavy load of ballast
stones.?
WordNet(e1) = ?vessel%1:06:00::?,
WordNet(e2) = ?tool%1:06:00::?,
Content-Container(e2, e1) = ?true?,
Query = ?contents of the * were a?
Figure 1: Annotations illustrated
Figure 1 illustrates the annotations. We tag the
nominals, so parsing or chunking is not necessary.
For Task 4, we define a nominal as a noun or base
noun phrase, excluding names entities. A base noun
phrase, e.g., lawn or lawn mower, is a noun with pre-
modifiers. We also exclude complex noun phrases
(e.g., with attached prepositional phrases ? the en-
gine of the lawn mower).
The procedure was the same for each relation.
One person gathered the sample sentences (aim-
ing approximately for a similar number of positive
and negative examples) and tagged the entities; two
other people annotated the sentences with WordNet
senses and classified the relations. The detailed re-
lation definitions and the preliminary discussions of
positive and negative examples served to maximize
the agreement between the annotators. They first
classified the data independently, then discussed ev-
ery disagreement and looked for consensus. Only
the agreed-upon examples went into the data sets.
Next, we split each data set into 140 training and
no fewer than 70 test examples. (We published the
training set for the Content-Container relation as de-
velopment data two months before the test set.) Ta-
ble 1 shows the number of positive and negative ex-
14
amples for each relation.1
The average inter-annotator agreement on rela-
tions (true/false) after the independent annotation
step was 70.3%, and the average agreement on
WordNet sense labels was 71.9%. In the process of
arriving at a consensus between annotators, the def-
inition of each relation was revised to cover explic-
itly cases where there had been disagreement. We
expect that these revised definitions would lead to
much higher levels of agreement than the original
definitions did.
3 The Participants
The task of classifying semantic relations between
nominals has attracted the participation of 14 teams
who submitted 15 systems. Table 4 lists the sys-
tems, the authors and their affiliations, and brief de-
scriptions. The systems? performance information
in terms of precision, recall, F -measure and accu-
racy, macroaveraged over all relations, appears in
Table 3. We computed these measures as described
in Lewis (1991).
We distinguish four categories of systems based
on the type of information used ? WordNet senses
and/or Google queries:
A ? WordNet = NO & Query = NO;
B ? WordNet = YES & Query = NO;
C ? WordNet = NO & Query = YES;
D ? WordNet = YES & Query = YES.
WordNet = ?YES? or WordNet = ?NO? tells us
only whether a system uses the WordNet sense la-
bels in the data sets. A system may use WordNet
internally for varied purposes, but ignore our sense
labels; such a system would be in category A or C .
Based on the input variation, each submitted system
may have up to 4 variations ? A,B,C,D.
Table 2 presents three baselines for a relation.
Majority always guesses either ?true? or ?false?,
whichever is the majority in the test set (maximizes
accuracy). Alltrue always guesses ?true? (maxi-
mizes recall). Probmatch randomly guesses ?true?
(?false?) with the probability matching the distribu-
tion of ?true? (?false?) in the test dataset (balances
precision and recall).
We present the results in Table 3 grouped by cat-
egory, to facilitate system comparison.
1As this paper serves also as a documentation of the data set,
the order of relations in the table is the same as in the data set.
Type P R F Acc
majority 81.3 42.9 30.8 57.0
alltrue 48.5 100.0 64.8 48.5
probmatch 48.5 48.5 48.5 51.7
Table 2: Baselines: precision, recall, F -measure and
accuracy averaged over the 7 binary classifications.
Team P R F Acc
A ? WordNet = NO & Query = NO
UCD-FC 66.1 66.7 64.8 66.0
ILK 60.5 69.5 63.8 63.5
UCB? 62.7 63.0 62.7 65.4
UMELB-B 61.5 55.7 57.8 62.7
UTH 56.1 57.1 55.9 58.8
UC3M 48.2 40.3 43.1 49.9
avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0
B ? WordNet = YES & Query = NO
UIUC? 79.7 69.8 72.4 76.3
FBK-IRST 70.9 73.4 71.8 72.9
ILK 72.8 70.6 71.5 73.2
UCD-S1 69.9 64.6 66.8 71.4
UCD-PN 62.0 71.7 65.4 67.0
UC3M 66.7 62.8 64.3 67.2
CMU-AT 55.7 66.7 60.4 59.1
UCD-FC 66.4 58.1 60.3 63.6
UMELB-A 61.7 56.8 58.7 62.5
UVAVU 56.8 56.3 56.1 57.7
LCC-SRN 55.9 57.8 51.4 53.7
avg ? stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2
C ? WordNet = NO & Query = YES
UCB? 64.2 66.5 65.1 67.0
UCD-FC 66.1 66.7 64.8 66.0
UC3M 49.4 43.9 45.3 50.1
avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5
D ? WordNet = YES & Query = YES
UTD-HLT-CG 67.3 65.3 62.6 67.2
UCD-FC 66.4 58.1 60.3 63.6
UC3M 60.9 57.8 58.8 62.3
avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 3: System performance grouped by category.
Precision, recall, F -measure and accuracy macro-
averaged over each system?s performance on all 7
relations.
4 Discussion
The highest average accuracy on Task 4 was 76.3%.
Therefore, the average initial agreement between an-
notators (70.3%), before revising the definitions, is
not an upper bound on the accuracy that can be
achieved. That the initial agreement between anno-
tators is not a good indicator of the accuracy that can
be achieved is also supported by the low correlation
15
System Institution Team Description System Type
UVAVU Univ. of Amsterdam
TNO Science & Industry
Free Univ. Amsterdam
Sophia Katrenko
Willem Robert van
Hage
similarity measures in WordNet; syn-
tactic dependencies; lexical patterns;
logical combination of attributes
B
CMU -AT Carnegie Mellon Univ. Alicia Tribble
Scott E. Fahlman
WordNet; manually-built ontologies;
Scone Knowledge Representation Lan-
guage; semantic distance
B
ILK Tilburg University Caroline Sporleder
Roser Morante
Antal van den Bosch
semantic clusters based on noun simi-
larity; WordNet supersenses; grammat-
ical relation between entities; head of
sentence; WEKA
A, B
FBK-IRST Fondazione Bruno
Kessler - IRST
Claudio Giuliano
Alberto Lavelli
Daniele Pighin
Lorenza Romano
shallow and deep syntactic information;
WordNet synsets and hypernyms; ker-
nel methods; SVM
B
LCC-SRN Language Computer
Corp.
Adriana Badulescu named entity recognition; lexical, se-
mantic, syntactic features; decision tree
and semantic scattering
B
UMELB-A Univ. of Melbourne Su Kim
Timothy Baldwin
sense collocations; similarity of con-
stituents; extending training and testing
data using similar words
B
UMELB-B Univ. of Melbourne Su Kim
Timothy Baldwin
similarity of nearest-neighbor matching
over the union of senses for the two
nominals; cascaded tagging with de-
creasing thresholds
A
UCB? Univ. of California at
Berkeley
Preslav Nakov
Marti Hearst
VSM; joining terms; KNN-1 A, C
UC3M Univ. Carlos III of Madrid Isabel Segura Bedmar
Doaa Sammy
Jose? Luis Mart??nez
Ferna?ndez
WordNet path; syntactic features; SVM A, B, C, D
UCD-S1 Univ. College Dublin Cristina Butnariu
Tony Veale
lexical-semantic categories from Word-
Net; syntactic patterns from corpora,
SVM
B
UCD-FC Univ. College Dublin Fintan Costello WordNet; additional noun compounds
tagged corpus; Naive Bayes
A, B, C, D
UCD-PN Univ. College Dublin Paul Nulty WordNet supersenses; web-based fre-
quency counts for specific joining
terms; WEKA (SMO)
B
UIUC? Univ. of Illinois at Urbana
Champaign
Roxana Girju
Brandon Beamer
Suma Bhat
Brant Chee
Andrew Fister
Alla Rozovskaya
features based on WordNet, NomLex-
PLUS, grammatical roles, lexico-
syntactic patterns, semantic parses
B
UTD-HLT-CG Univ. of Texas at Dallas Cristina Nicolae
Garbiel Nicolae
Sanda Harabagiu
lexico-semantic features from Word-
Net, VerbNet; semantic features from a
PropBank parser; dependency features
D
UTH Univ. of Tokio Eiji Aramaki
Takeshi Imai
Kengo Miyo
Kazuhiko Ohe
joining phrases; physical size for enti-
ties; web-mining; SVM
A
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 4: Short description of the teams and the participating systems.
16
Relation Team Type P R F Acc Test size Base-F Base-Acc Avg. rank
Cause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4
Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4
Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7
Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0
Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0
Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5
Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1
Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each
system?s performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc ? the
baseline accuracy score (majority). The last column shows the average rank for each relation.
of 0.15 between the Acc column in Table 5 and the
Agreement column in Table 1.
We performed various analyses of the results,
which we summarize here in four questions. We
write Xi to refer to four possible system categories
(Ai, Bi, Ci, and Di) with four possible amounts of
training data (X1 for training examples 1 to 35, X2
for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).
Does more training data help?
Overall, the results suggest that more training data
improves the performance. There were 17 cases in
which we had results for all four possible amounts
of training data. All average F -measure differences,
F (X4)?F (Xi) where X = A to D, i = 1 to 3, for
these 17 sets of results are statistically significant:
F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =
1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.
F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =
?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.
F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =
?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.
Does WordNet help?
The statistics show that WordNet is important, al-
though the contribution varies across systems. Three
teams submitted altogether 12 results both for A1?
A4 and B1?B4. The average F -measure difference,
F (Bi)?F (Ai), i = 1 to 4, is significant:
F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =
?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.
The results of the UCD-FC system actually went
down when WordNet was used. The statistics for the
remaining two teams, however, are a bit better:
F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =
?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.
Does knowing the query help?
Overall, knowing the query did not seem to improve
the results. Three teams submitted 12 results both
for A1?A4 and C1?C4. The average F -measure dif-
ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:
F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =
?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.
Again, the UCD-FC system differed from the
other systems in that the A and C scores were iden-
tical, but even averaging over the remaining two sys-
tems and 8 cases does not show a statistically signif-
icant advantage:
F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =
?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.
Are some relations harder to classify?
Table 5 shows the best results for each relation in
terms of precision, recall, and F -measure, per team
and system category. Column Base-F presents the
baseline F -measure (alltrue), while Base-Acc the
baseline accuracy score (majority). For all seven re-
lations, the best team significantly outperforms the
baseline. The category of the best-scoring system
in almost every case is B4 (only the ILK B4 system
scored second on the Origin-Entity relation).
Table 5 suggests that some relations are more dif-
ficult to classify than others. The best F -measure
ranges from 83.7 for Product?Producer to 68.6 for
Origin?Entity. The difference between the best F -
measure and the baseline F -measure ranges from
23.3 for Part-Whole to 3.7 for Product-Producer.
The difference between the best accuracy and the
baseline accuracy ranges from 31.0 for Content-
Container to 10.7 for Product-Producer.
The F column shows the best result for each rela-
tion, but similar differences among the relations may
be observed when all results are pooled. The Avg.
rank column computes the average rank of each re-
lation in the ordered list of relations generated by
each system. For example, Product?Producer is of-
ten listed as the first or the second easiest relation
(with an average rank of 1.7), while Origin?Entity
and Theme?Tool are identified as the most difficult
17
relations to classify (with average ranks of 6.0).
5 Conclusion
This paper describes a new semantic evaluation task,
Classification of Semantic Relations between Nom-
inals. We have accomplished our goal of providing
a framework and a benchmark data set to allow for
comparisons of methods for this task. The data in-
cluded different types of information ? lexical se-
mantic information, context, query used ? meant to
facilitate the analysis of useful sources of informa-
tion for determining the semantic relation between
nominals. The results that the participating systems
have reported show successful approaches to this
difficult task, and the advantages of using lexical se-
mantic information.
The success of the task ? measured in the inter-
est of the community and the results of the partici-
pating systems ? shows that the framework and the
data are useful resources. By making this collection
freely accessible, we encourage further research into
this domain and integration of semantic relation al-
gorithms in high-end applications.
Acknowledgments
We thank Eneko Agirre, Llu??s Ma`rquez and Richard
Wicentowski, the organizers of SemEval 2007, for
their guidance and prompt support in all organiza-
tional matters. We thank Marti Hearst for valu-
able advice throughout the task description and de-
bates on semantic relation definitions. We thank the
anonymous reviewers for their helpful comments.
References
T. Chklovski and P. Pantel. 2004. Verbocean: Mining the
web for fine-grained semantic verb relations. In Proc.
Conf. on Empirical Methods in Natural Language Pro-
cessing, EMNLP-04, pages 33?40, Barcelona, Spain.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19:479?496.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. 14th International
Conf. on Computational Linguistics (COLING-92),
pages 539?545.
M. Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
D.D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the Speech and Natural Language
Workshop, pages 312?318, Asilomar.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classification
of noun phrases. In Proc. Computational Lexical Se-
mantics Workshop at HLT-NAACL 2004, pages 60?67,
Boston, MA.
P. Nakov and M. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. Twelfth Inter-
national Conf. in Artificial Intelligence (AIMSA-06),
pages 233?244, Varna,Bulgaria.
V. Nastase and S. Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
V. Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-
pakowicz. 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proc. 21st National Conf. on Artificial Intel-
ligence (AAAI 2006), pages 781?787, Boston, MA.
B. Rosario and M. Hearst. 2001. Classifying the seman-
tic relations in noun-compounds via domain-specific
lexical hierarchy. In Proc. 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), pages 82?90.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The de-
scent of hierarchy, and selection in relational seman-
tics. In Proc. 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages 417?
424, Philadelphia, PA.
M. Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.
2001. Detecting gene relations from MEDLINE ab-
stracts. In Proc. Sixth Annual Pacific Symposium on
Biocomputing, pages 483?496.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. Human Lan-
guage Technology Conf. and Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver, Canada.
P.D. Turney and M.L. Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60(1-3):251?278.
P.D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. Nineteenth Interna-
tional Joint Conf. on Artificial Intelligence (IJCAI-05),
pages 1136?1141, Edinburgh, Scotland.
18
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680?690,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Literal and Metaphorical Sense Identification
through Concrete and Abstract Context
Peter D. Turney
Inst. for Info. Tech.
NRC Canada
Ottawa, Canada
peter.turney@nrc-cnrc.gc.ca
Yair Neuman
Dept. of Education
Ben-Gurion Univ.
Beer-Sheva, Israel
yneuman@bgu.ac.il
Dan Assaf
Dept. of Education
Ben-Gurion Univ.
Beer-Sheva, Israel
dan.assaf4@googlemail.com
Yohai Cohen
Gilasio Coding
Tel-Aviv, Israel
yohai@gilasio.com
Abstract
Metaphor is ubiquitous in text, even in highly
technical text. Correct inference about tex-
tual entailment requires computers to distin-
guish the literal and metaphorical senses of
a word. Past work has treated this problem
as a classical word sense disambiguation task.
In this paper, we take a new approach, based
on research in cognitive linguistics that views
metaphor as a method for transferring knowl-
edge from a familiar, well-understood, or con-
crete domain to an unfamiliar, less understood,
or more abstract domain. This view leads to
the hypothesis that metaphorical word usage
is correlated with the degree of abstractness of
the word?s context. We introduce an algorithm
that uses this hypothesis to classify a word
sense in a given context as either literal (de-
notative) or metaphorical (connotative). We
evaluate this algorithm with a set of adjective-
noun phrases (e.g., in dark comedy, the adjec-
tive dark is used metaphorically; in dark hair,
it is used literally) and with the TroFi (Trope
Finder) Example Base of literal and nonliteral
usage for fifty verbs. We achieve state-of-the-
art performance on both datasets.
1 Introduction
Metaphor is a natural consequence of our ability
to reason by analogy (Gentner et al, 2001). It is
so common in our daily language that we rarely
notice it (Lakoff and Johnson, 1980). Identifying
metaphorical word usage is important for reasoning
about the implications of text.
Past work on the problem of distinguishing lit-
eral and metaphorical senses has approached it as
a classical word sense disambiguation (WSD) task
(Birke and Sarkar, 2006). Here, we take a differ-
ent approach to the problem. Lakoff and Johnson
(1980) argue that metaphor is a method for trans-
ferring knowledge from a concrete domain to an ab-
stract domain. Therefore we hypothesize that the de-
gree of abstractness in a word?s context is correlated
with the likelihood that the word is used metaphori-
cally. This hypothesis is the basis for our algorithm
for distinguishing literal and metaphorical senses.
Consider the following sentences:
L: He shot down my plane.
? C1: He fired at my plane.
9 A1: He refuted my plane.
M : He shot down my argument.
9 C2: He fired at my argument.
? A2: He refuted my argument.
The literal sense of shot down in L invokes knowl-
edge from the domain of war. The metaphorical us-
age of shot down in M transfers knowledge from
the concrete domain of war to the abstract domain
of debate (Lakoff and Johnson, 1980).
The entailments of L and M depend on the in-
tended senses of shot down. L entails the concrete
fired at in C1 (because, in order to literally shoot
something down, you must first fire at it) but not the
abstract refuted in A1 (except perhaps as a joke). On
the other hand, M entails refuted in A2 but not fired
at in C2 (except perhaps as a novel metaphor).
In semiotics, Danesi (2003) argues that metaphor
transfers associations from the source domain to the
target domain. The metaphorical usage of shot down
in M carries associations of violence and destruc-
680
tion that are not conveyed by A2.
To make correct inferences about textual entail-
ment, computers must be able to distinguish the lit-
eral and metaphorical senses of a word. Since rec-
ognizing textual entailment (RTE) is a core problem
for NLP, with applications in Question Answering,
Information Retrieval, Information Extraction, and
Text Summarization, it follows that distinguishing
literal and metaphorical senses is a problem for a
wide variety of NLP tasks. The ability to recognize
metaphorical word usage is a core requirement in
the Intelligence Advanced Research Projects Activ-
ity (IARPA) Metaphor Program (Madrigal, 2011).1
Our approach to the problem of distinguishing lit-
eral and metaphorical senses is based on an algo-
rithm for calculating the degree of abstractness of
words. For instance, plane in L is rated 0.36396 (rel-
atively concrete), whereas argument in M is rated
0.64617 (relatively abstract), which suggests that the
verb shot down is used literally in L, whereas it is
used metaphorically in M . Our abstractness rating
algorithm is similar to Turney and Littman?s (2003)
algorithm for rating words according to their seman-
tic orientation.
To classify a word usage as literal or metaphori-
cal, based on the context, we use supervised learning
with logistic regression. The abstractness rating al-
gorithm is used to generate feature vectors from a
word?s context and training data is used to learn a
logistic regression model that relates degrees of ab-
stractness to the classes literal and metaphorical.
We evaluate our algorithm with three experi-
ments. The first experiment involves one hundred
adjective-noun phrases labeled denotative (literal) or
connotative (metaphorical or nonliteral) by five an-
notators, according to the sense of the adjective.2
For instance, deep snow is labeled denotative and
deep appreciation is labeled connotative. The algo-
rithm is able to predict the labels of the annotators
with an average accuracy of 79%.
The next two experiments use the TroFi (Trope
Finder) Example Base of literal and nonliteral usage
for fifty verbs.3 The fifty verbs occur in 3,737 sen-
tences from The 1987-89 Wall Street Journal (WSJ)
Corpus Release 1. In each sentence, the target verb
1See http://www.iarpa.gov/solicitations metaphor.html.
2The labeled phrases are available from Yair Neuman.
3Available at http://www.cs.sfu.ca/ anoop/students/jbirke/.
is labeled L (literal) or N (nonliteral), according to
the sense of the verb that is invoked by the sentence.
A subset of twenty-five of the fifty verbs was used
by Birke and Sarkar (2006).
In our second experiment, we duplicate the setup
of Birke and Sarkar (2006) so that we can com-
pare our results with theirs. In particular, a sepa-
rate model is learned for each individual verb. We
achieve an average f-score of 63.9%, compared to
Birke and Sarkar?s (2006) 64.9%.
In the third experiment, we train the algorithm
on the twenty-five new verbs that were not used by
Birke and Sarkar (2006) and then we test it on the
old verbs. That is, the algorithm is tested with verbs
that it has never seen before. The training verbs are
merged to build a single model, instead of building
a separate model for each individual verb. In this
experiment, the average f-score is 68.1%.
The next section presents our algorithm for calcu-
lating the degree of abstractness of words. In Sec-
tion 3, we review related work. The experiments are
described in Section 4. We discuss the results of the
experiments in Section 5 and conclude in Section 6.
2 Abstractness and Concreteness
Concrete words refer to things, events, and proper-
ties that we can perceive directly with our senses,
such as trees, walking, and red.4 Abstract words re-
fer to ideas and concepts that are distant from im-
mediate perception, such as economics, calculating,
and disputable. In this section, we describe an algo-
rithm that can automatically calculate a numerical
rating of the degree of abstractness of a word on a
scale from 0 (highly concrete) to 1 (highly abstract).
For example, the algorithm rates purvey as 1, donut
as 0, and immodestly as 0.5.
The algorithm is a variation of Turney and
Littman?s (2003) algorithm that rates words accord-
ing to their semantic orientation. Positive seman-
tic orientation indicates praise (honest, intrepid)
and negative semantic orientation indicates criticism
(disturbing, superfluous). The algorithm calculates
the semantic orientation of a given word by com-
paring it to seven positive words and seven nega-
4The word red has an abstract political sense, but our ab-
stractness rating algorithm does not distinguish word senses.
The more frequent concrete sense of red dominates, resulting
in an abstractness rating of 0.24984 (highly concrete).
681
tive words that are used as paradigms of positive and
negative semantic orientation:
Positive paradigm words: good, nice, excellent,
positive, fortunate, correct, and superior.
Negative paradigm words: bad, nasty, poor, nega-
tive, unfortunate, wrong, and inferior.
Likewise, here we calculate the abstractness of
a given word by comparing it to twenty abstract
words and twenty concrete words that are used as
paradigms of abstractness and concreteness.
Turney and Littman (2003) experimented with
two measures of semantic similarity, pointwise mu-
tual information (PMI) (Church and Hanks, 1989)
and latent semantic analysis (LSA) (Landauer and
Dumais, 1997). These measures take a pair of words
as input and generate a numerical similarity rating as
output. The semantic orientation of a given word is
calculated as the sum of its similarity with the posi-
tive paradigm words minus the sum of its similarity
with the negative paradigm words. Likewise, here
we calculate the abstractness of a given word by the
sum of its similarity with twenty abstract paradigm
words minus the sum of its similarity with twenty
concrete paradigm words. We then use a linear nor-
malization to map the calculated abstractness value
to range from 0 to 1.
Our algorithm for calculating abstractness uses a
form of LSA to measure semantic similarity. This is
described in detail in Section 2.1. Although Turney
and Littman (2003) manually selected their fourteen
paradigm words, here we use a supervised learning
algorithm to choose our forty paradigm words, as
explained in Section 2.2.
The MRC Psycholinguistic Database Machine
Usable Dictionary (Coltheart, 1981) includes 4,295
words rated with degrees of abstractness by human
subjects in psycholinguistic experiments.5 The rat-
ings range from 158 (highly abstract) to 670 (highly
concrete). Table 1 gives some examples.
We used half of the 4,295 MRC words to train our
supervised learning algorithm and the other half to
validate the algorithm. On the testing set, the algo-
rithm attains a correlation of 0.81 with the dictionary
ratings. This indicates that the algorithm agrees well
with human judgements of the degrees of abstract-
ness of words.
5Available at http://ota.oucs.ox.ac.uk/headers/1054.xml.
Abstract Words Rating Concrete Words Rating
as 158 ape 654
of 180 grasshopper 660
apt 183 tomato 662
however 186 milk 670
Table 1: Examples of abstract and concrete words from
the MRC Dictionary (Coltheart, 1981).
2.1 Measuring Semantic Similarity
The variation of LSA that we use here is similar
to Rapp?s (2003) work. We modeled our similarity
measure on Rapp?s due to the high score of 92.5%
that he achieved on a set of 80 multiple-choice syn-
onym questions from the Test of English as a For-
eign Language (TOEFL). The core idea is to repre-
sent words with vectors and calculate the similarity
of two words by the cosine of the angle between the
two corresponding vectors. The values of the ele-
ments in the vectors are derived from the frequencies
of the words in a large corpus of text. This general
approach is known as a Vector Space Model (VSM)
of semantics (Salton et al, 1975).
We began with a corpus of 5?1010 words (280 gi-
gabytes of plain text) gathered from university web-
sites by a webcrawler.6 We then indexed this cor-
pus with the Wumpus search engine (Bu?ttcher and
Clarke, 2005).7 We selected our vocabulary from the
terms (words and phrases) in the WordNet lexicon.8
By querying Wumpus, we obtained the frequency of
each WordNet term in our corpus. We selected all
WordNet terms with a frequency of 100 or more in
our corpus. This resulted in a set of 114,501 terms.
Next we used Wumpus to search for up to 10,000
phrases per term, where a phrase consists of the
given term plus four words to the left of the term and
four words to the right of the term. These phrases
were used to build a word?context frequency matrix
F with 114,501 rows and 139,246 columns. A row
vector in F corresponds to a term in WordNet and
the columns in F correspond to contexts (the words
to the left and right of a given term in a given phrase)
in which the term appeared.
The columns in F are unigrams (single words)
in WordNet with a frequency of 100 or more in
the corpus. A given unigram is represented by two
6Collected by Charles Clarke at the University of Waterloo.
7Wumpus is available at http://www.wumpus-search.org/.
8WordNet is available at http://wordnet.princeton.edu/.
682
columns, one marked left and one marked right.
Suppose r is the term corresponding to the i-th row
in F and c is the term corresponding to the j-th col-
umn in F. Let c be marked left. Let fij be the cell
in the i-th row and j-th column of F. The numerical
value in the cell fij is the number of phrases found
by Wumpus in which the center term was r and c
was the unigram closest to r on the left side of r.
That is, fij is the frequency with which r was found
in the context c in our corpus.
A new matrix X, with the same number of rows
and columns as in F, was formed by calculating
the Positive Pointwise Mutual Information (PPMI)
of each cell in F (Turney and Pantel, 2010). The
function of PPMI is to emphasize cells in which
the frequency fij is statistically surprising, and
hence particularly informative. This matrix was then
smoothed with a truncated Singular Value Decom-
position (SVD), which decomposes X into the prod-
uct of three matrices Uk?kVTk . Finally, the terms
were represented by the matrix Uk?pk, which has
114,501 rows (one for each term) and k columns
(one for each latent contextual factor). The semantic
similarity of two terms is given by the cosine of the
two corresponding rows in Uk?pk. For more detail,
see Turney and Pantel (2010).
There are two parameters in Uk?pk that need to
be set. The parameter k controls the number of la-
tent factors and the parameter p adjusts the weights
of the factors, by raising the corresponding singu-
lar values in ?pk to the power p. The parameter k is
well-known in the literature on LSA, but p is less fa-
miliar. The use of p was suggested by Caron (2001).
Based on our past experience, we set k to 1000 and
p to 0.5. We did not explore any alternative settings
of these parameters for measuring abstractness.
2.2 Measuring Abstractness
Now that we have Uk?pk, all we need in order
to measure abstractness is some paradigm words.
We used the MRC Psycholinguistic Database Ma-
chine Usable Dictionary (Coltheart, 1981) to guide
our search for paradigm words. We split the 4,295
MRC words into 2,148 for training (searching for
paradigm words) and 2,147 for testing (evaluation
of the final set of paradigm words). We began
with an empty set of paradigm words and added
words from the 114,501 rows of Uk?pk, one word
at a time, alternating between adding a word to the
concrete paradigm words and then adding a word
to the abstract paradigm words. At each step, we
added the paradigm word that resulted in the high-
est Pearson correlation with the ratings of the train-
ing words. This is a form of greedy forward search
without backtracking. We stopped the search after
forty paradigm words were found, in order to pre-
vent overfitting of the training data.
Table 2 shows the forty paradigm words and the
order in which they were selected. At each step, the
correlation increases on the training set, but even-
tually it must decrease on the testing set. After
forty steps, the training set Pearson correlation was
0.8600. At this point, we stopped the search for
paradigm words and calculated the testing set Pear-
son correlation, which was 0.8064. This shows a
small amount of overfitting of the training data. The
testing set Spearman correlation was 0.8216.
For another perspective on the performance of the
algorithm, we measured its accuracy on the testing
set, by creating a binary classification task from the
testing data. We calculated the median of the rat-
ings of the 2,147 words in the test set. Every word
with an abstractness above the median was assigned
to the class 1 and every word with an abstractness
below the median was assigned to the class 0. We
then used the algorithm to guess the rating of each
word in the test set, calculated the median guess, and
likewise assigned the guesses to classes 1 and 0. The
guesses were 84.65% accurate.
After generating the paradigm words with the
training set and evaluating them with the testing
set, we then used them to assign abstractness rat-
ings to every term in the matrix. The result of this
is that we now have a set of 114,501 terms (words
and phrases) with abstractness ratings ranging from
0 to 1.9 Based on the testing set performance, we
estimate these 114,501 ratings would have a Pearson
correlation of 0.81 with human ratings and an accu-
racy of 85% on binary (abstract or concrete) classi-
fication.
We chose to limit the search to forty paradigm
words based on our past experience with semantic
orientation (Turney and Littman, 2003). To validate
this choice, we allowed the algorithm to continue
9The 114,501 rated terms are available from Peter Turney.
683
Concrete Paradigm Words Abstract Paradigm Words
Order Word Correlation Order Word Correlation
1 donut 0.4447 2 sense 0.6165
3 antlers 0.6582 4 indulgent 0.6973
5 aquarium 0.7150 6 bedevil 0.7383
7 nursemaid 0.7476 8 improbable 0.7590
9 pyrethrum 0.7658 10 purvey 0.7762
11 swallowwort 0.7815 12 pigheadedness 0.7884
13 strongbox 0.7920 14 ranging 0.7973
15 sixth-former 0.8009 16 quietus 0.8067
17 restharrow 0.8089 18 regularisation 0.8123
19 recorder 0.8148 20 creditably 0.8188
21 sawmill 0.8212 22 arcella 0.8248
23 vulval 0.8270 24 nonproductive 0.8299
25 tenrecidae 0.8316 26 couth 0.8340
27 hairpiece 0.8363 28 repulsion 0.8400
29 sturnus 0.8414 30 palsgrave 0.8438
31 gadiformes 0.8451 32 goof-proof 0.8469
33 cobbler 0.8481 34 meshuga 0.8503
35 bullet 0.8521 36 dillydally 0.8538
37 dioxin 0.8550 38 reliance 0.8570
39 usa 0.8585 40 lumbus 0.8600
Table 2: The forty paradigm words and the Pearson correlation on the training set.
searching until one hundred paradigm words were
found. This resulted in a training set Pearson corre-
lation of 0.8963, but the testing set correlation was
only 0.8097, which shows a significant amount of
overfitting of the training data. Although the test-
ing set correlation is slightly higher with one hun-
dred paradigm words, we chose to base the follow-
ing experiments on the forty paradigm words, be-
cause the difference between 0.8064 and 0.8097 is
not significant, and the gap between the training and
testing correlation (0.8963 versus 0.8097) indicates
a problematic amount of overfitting. Furthermore,
the execution time of the algorithm increases as the
paradigm set increases.
We generated abstractness ratings for a large vo-
cabulary of 114,501 words in order to maximize the
variety of text genres and the range of applications
for which our list of abstractness ratings would be
useful. As a consequence of this large vocabulary,
many of the words in Table 2 are rare and obscure;
however, the measure of quality of the algorithm is
the correlation with the testing set (0.81), not the
familiarity of the words in the table. We include
the table here so that other researchers can exper-
iment with these paragidm words. The table may
give some insight into the internal functioning of the
algorithm, but the main output of the algorithm is
the list of 114,501 words with abstractness ratings,
not the list of paradigm words in Table 2.
3 Related Work
Here we discuss related work on metaphor and then
work on measuring abstractness. As far as we know,
our approach is the first in computational linguis-
tics to bring these two themes together, although
the connection is well-known in cognitive linguistics
(Lakoff and Johnson, 1980) and cognitive psychol-
ogy (Gentner et al, 2001).
3.1 Metaphor
The most closely related work is Birke and Sarkar?s
(2006) research on distinguishing literal and nonlit-
eral usage of verbs. A later paper (Birke and Sarkar,
2007) provides more detail on their active learn-
ing system, briefly mentioned in the earlier paper.
Birke and Sarkar (2006; 2007) treat the problem as
a classical word sense disambiguation task (Navigli,
2009). A model is learned for each verb indepen-
684
dently from the other verbs. This approach cannot
handle a new verb without additional training.
Hashimoto and Kawahara (2009) discuss work
on a similar problem, distinguishing idiomatic us-
age from literal usage. They also approach this as
a classical word sense disambiguation task. Idioms
are somewhat different from metaphors, in that the
meaning of an idiom (e.g., kick the bucket) is often
difficult to derive from the meanings of the compo-
nent words, unlike most metaphors.
Nissim and Markert (2003) use supervised learn-
ing to distinguish metonymic usage from literal us-
age. They take a classical WSD approach, learn-
ing a separate model for each target word. As with
Birke and Sarkar (2006; 2007) and Hashimoto and
Kawahara (2009), the core idea is to learn to clas-
sify word usage from similarity of context. Unlike
these approaches, our algorithm generalizes beyond
the specific semantic content of the context, paying
attention only to the degrees of abstractness of the
context.
Martin (1992) presents a knowledge-based ap-
proach to interpreting metaphors. This approach re-
quires complex hand-coded rules, which are specific
to a given domain (e.g., interpreting metaphorical
questions from computer users, such as, ?How can
I kill a process??, in an online help system). The
knowledge base cannot handle words that are not
hand-coded in its rules and a new set of rules must
be constructed for each new application domain.
Dolan (1995) describes an algorithm for extract-
ing metaphors from a dictionary. Some suggestive
examples are given, but the algorithm is not evalu-
ated in any systematic way.
Mason (2004) takes a corpus-based approach to
metaphor. His algorithm is based on a statistical
approach to discovering the selectional restrictions
of verbs. It then uses these restrictions to discover
metaphorical mappings, such as, ?Money flows like
a liquid.? Although the system can discover some
metaphorical mappings, it was not designed to dis-
tinguish literal and metaphorical usages of words.
3.2 Abstractness
Changizi (2008) uses the hypernym hierarchy in
WordNet to calculate the abstractness of a word.
A word near the top of the hierarchy is consid-
ered abstract and a word near the bottom is con-
sidered concrete. It seems to us that the WordNet
hypernym hierarchy captures the general?specific
continuum, which might not be the same as the
abstract?concrete continuum. It would be interest-
ing to see how much correspondence there is be-
tween Changizi?s measure of abstractness and the
ratings in the MRC Psycholinguistic Database Ma-
chine Usable Dictionary (Coltheart, 1981). Also,
note that adjectives and adverbs are outside of Word-
Net?s hypernym hierarchy, and thus cannot be rated
by Changizi?s algorithm.
Xing et al (2010) also use WordNet, but in a dif-
ferent way. They define the concreteness of a word
sense (a WordNet synset) to be 1 if the given word
sense is a hyponym of physical entity in the Word-
Net hypernym hierarchy; otherwise the concreteness
is 0. We believe that, although physical entities are
concrete, so are redness and walking, which are not
hyponyms of physical entity. The category physical
entity only partially captures concreteness.
4 Experiments
In the following experiments, we use the abstract-
ness ratings of Section 2.2 to generate features for
supervised machine learning. The learning algo-
rithm we apply is logistic regression (Le Cessie and
Van Houwelingen, 1992), as implemented in Weka
(Witten and Frank, 2005).10 In all experiments, we
used the Weka parameter settings R = 0.2 (for ro-
bust ridge regression) and M = ?1 (for unlimited
iterations).
4.1 Adjectives
For this experiment, we selected five adjectives,
dark, deep, hard, sweet, and warm. For each of
the five adjectives, we identified twenty word pairs
in which the first word is the adjective and the
second word is a noun. These pairs were identi-
fied through the Corpus of Contemporary American
English (COCA)11 (Davies, 2009) by seeking the
nouns that follow each adjective in the corpus and
sorting the candidate adjective-noun pairs by fre-
quency. We required a minimum pointwise mutual
information (PMI) of 3 between the adjective and
the noun. In some of the pairs, the adjective was
10Weka is available at http://www.cs.waikato.ac.nz/ml/weka/.
11Available at http://www.americancorpus.org/.
685
used in a denotative (literal) sense (dark hair) and in
others it was used in a connotative (nonliteral) sense
(dark humor). Table 3 gives some examples.
Adjective-Noun Pairs Noun Abstractness
dark glasses 0.26826
dark chocolate 0.28211
dark energy 0.66207
dark mood 0.61858
Table 3: Some examples of adjective-noun pairs and the
abstractness rating of the noun.
In this experiment, we used the abstractness rat-
ing of the noun (the context) to predict whether the
adjective (the target) was used in a metaphorical or
literal sense. Table 3 supports this idea, but it is easy
to find counterexamples. Although dark mood is
metaphorical, bad mood is literal. The difference is
that dark has an abstractness rating of 0.43356 (rel-
atively concrete), whereas bad has an abstractness
rating of 0.63326 (relatively abstract). Metaphor re-
sults when a concrete word is imported into an ab-
stract context (Lakoff and Johnson, 1980). Ideally,
we should be comparing the abstractness of the tar-
get to the abstractness of the context. However, in
our data, the target words are mostly concrete; thus
we can focus on the context and ignore the target.
We discuss this point further in Section 5.
Five judges, undergraduate students in psychol-
ogy, were asked to judge whether the use of the ad-
jective is a denotation or a connotation. The instruc-
tions were as follows:
Denotation is the most direct or specific
meaning of a word or expression while
connotation is the meaning suggested by
the word that goes beyond its literal mean-
ing. For instance, the meaning of bitter is
denotative in bitter lemon and connotative
in bitter relations. In each of the following
pairs, you will be asked to judge whether
(1) the meaning of the first word is denota-
tive or connotative and (2) to what extent
it is denotative or connotative on a scale
ranging from 1 to 4.
The judges were blind to the research hypothe-
sis. Each judge received a booklet with the items
organized by the groups of adjectives and presented
in a random order. Overall, each subject was asked
to evaluate one hundred pairs. Interjudge reliability
was high, with Cronbach?s Alpha equal to 0.95.
Our feature vectors for each pair contained only
one element, the abstractness rating of the noun in
the pair. We used logistic regression with ten-fold
cross-validation to predict each judge?s denotative
and connotative labels. The results are summarized
in Table 4. On average, we were able to predict a
judge?s labels with 79% accuracy.
Judge Accuracy Majority
1 0.730 0.590
2 0.810 0.570
3 0.840 0.560
4 0.790 0.510
5 0.780 0.520
Average 0.790 0.550
Table 4: The accuracy of logistic regression at predicting
the labels of each judge.
Table 4 also shows the size of the majority class
(the most common label) for each judge. For all
of the judges, the accuracy was significantly greater
than the size of the majority class (Fisher Exact test,
95% confidence level). The results support our hy-
pothesis that the abstractness of the context is pre-
dictive of whether an adjective is used in a literal or
metaphorical sense.
4.2 Known Verbs
For this experiment, we used the TroFi (Trope
Finder) Example Base of literal and nonliteral usage
for fifty verbs.12 To compare our results with Birke
and Sarkar?s (2006) results, we use the same subset
of twenty-five of the fifty verbs. These twenty-five
verbs appear in 1,965 sentences, manually labeled
L (literal) or N (nonliteral), according to the sense
of the target verb. The verbs also appeared in some
sentences labeled U (unannotated), but we ignored
these sentences (although they could be useful for
semi-supervised learning).
The label nonliteral is intended to be a broad cat-
egory that includes metaphorical as a special case.
Other types of nonliteral usage include idiomatic
and metonymical, but it seems that most of the non-
literal cases in TroFi are in fact metaphorical, and
12Available at http://www.cs.sfu.ca/ anoop/students/jbirke/.
686
hence our hypothesis about the correlation of ab-
stract context with metaphorical sense is appropriate
for classifying the TroFi sentences.
Two examples of sentences from TroFi follow.
Both contain the target verb absorb. The first sen-
tence is literal and the second is nonliteral.
L: An Energy Department spokesman says the sul-
fur dioxide might be simultaneously recover-
able through the use of powdered limestone,
which tends to absorb the sulfur.
N: He said that MMWEC will have to absorb only
$4 million in additional annual costs now paid
by the Vermont utilities.
To generate feature vectors for the sentences, we
first applied the OpenNLP part-of-speech tagger to
the sentences.13 We then looked for each word in
our list of 114,501 abstractness ratings (Section 2.2).
If the word was not found in the list, we applied the
Morpha morphological analyzer to identify the stem
of the word (e.g., the stem of managing is manage)
(Minnen et al, 2001).14 We then looked for the stem
in our list. If it was still not found, we skipped it.
For each sentence, we created a vector with five
features:
1. the average abstractness ratings of all nouns,
excluding proper nouns
2. the average abstractness ratings of all proper
nouns
3. the average abstractness ratings of all verbs, ex-
cluding the target verb
4. the average abstractness ratings of all adjectives
5. the average abstractness ratings of all adverbs
When there were no words for a given part of
speech, we set the average to a default value of 0.5.
Two examples of feature vectors follow, correspond-
ing to the two TroFi sentences above.
L: ?0.3873, 0.5397, 0.6375, 0.2641, 0.5835?
N: ?0.6120, 0.3726, 0.6699, 0.5612, 0.5000?
The intuition here is that the weight of each con-
text word, in predicting the class of the target verb,
may depend on the part of speech of the context
13Available at http://incubator.apache.org/opennlp/.
14Available at http://www.informatics.susx.ac.uk/research/
groups/nlp/carroll/morph.html.
word. We leave it to the logistic regression algo-
rithm to determine the appropriate weighting, based
on the training data. (See Table 7 in the next sec-
tion.)
Following Birke and Sarkar?s (2006) approach,
we treated each group of sentences for a given target
verb as a separate learning problem. For each verb,
we used ten-fold cross-validation to learn and test
logistic regression models. To measure the perfor-
mance of the models, we used three different scores,
macro-averaged accuracy and two forms of macro-
averaged f-score.
Birke and Sarkar (2006) explain their scoring as
follows:
Literal recall is defined as (correct literals
in literal cluster / total correct literals).
Literal precision is defined as (correct lit-
erals in literal cluster / size of literal clus-
ter). If there are no literals, literal recall
is 100%; literal precision is 100% if there
are no nonliterals in the literal cluster and
0% otherwise. The f-score is defined as
(2 ? precision ? recall) / (precision + re-
call). Nonliteral precision and recall are
defined similarly. Average precision is the
average of literal and nonliteral precision;
similarly for average recall. For overall
performance, we take the f-score of aver-
age precision and average recall.
The overall score is a macro-average, in which each
verb has equal weight, regardless of how many sen-
tences it appears in.
Every verb in TroFi has at least one literal usage
and one nonliteral usage, so there is no issue with
the definition of recall as 100% when there are no lit-
erals or no nonliterals. However, we believe that the
definition of precision as 100% when no sentence is
assigned to the literal or nonliteral cluster gives too
high a score to the trivial algorithm of always guess-
ing the majority class. The minority class will then
always have a precision of 100%. Therefore we use
a modified f-score in which the precision of a class
is 0% if the algorithm never guesses that class. We
refer to Birke and Sarkar?s (2006) score as f-score
(0/0 = 1) and to our own score as f-score (0/0 = 0).
Table 5 summarizes our results. Concrete-
Abstract refers to our own algorithm. Birke-Sarkar
687
refers to the best result reported by Birke and Sarkar
(2006), using a form of active learning. Majority
Class is the simple strategy of always guessing the
majority class. Probability Matching is the strategy
of randomly guessing each class with a probability
equal to the size of the class.
Algorithm Accuracy F-score F-score
(0/0=0) (0/0=1)
Concrete-Abstract 0.734 0.631 0.639
Birke-Sarkar NA NA 0.649
Majority Class 0.697 0.408 0.629
Probability Matching 0.605 0.500 0.500
Table 5: The performance with known verbs.
We used a paired t-test to evaluate the statistical
significance of the results in Table 5. The num-
bers are in bold font when the performance of an
algorithm is significantly below the performance of
Concrete-Abstract. In no case is any score signifi-
cantly above the performance of Concrete-Abstract,
at the 95% confidence level. NA indicates scores that
were not calculated by Birke and Sarkar (2006).
4.3 Unknown Verbs
For the final experiment, we again used the TroFi
Example Base, but with a different experimental
setup. Instead of ten-fold cross-validation, we used
the twenty-five verbs in Birke and Sarkar (2006) for
testing (we call these the old verbs) and the other
twenty-five verbs (the new verbs) for training. The
twenty-five old (testing) verbs appear in 1,965 sen-
tences and the twenty-five new (training) verbs ap-
pear in 1,772 sentences. For this experiment, we
no longer learn a separate logistic regression model
for each verb. All of the 1,772 training sentences
are used together to learn a single logistic regression
model, which is then evaluated on the testing sen-
tences.
Table 6 summarizes our results. Since the testing
set is exactly the same as in Section 4.2, we can com-
pare the performance directly with the performance
in the preceding section and with Birke and Sarkar?s
(2006) results.
Again, we used a paired t-test to evaluate the sta-
tistical significance of the results in Table 6. The
numbers are in bold font when the performance of
an algorithm is significantly below the performance
Algorithm Accuracy F-score F-score
(0/0=0) (0/0=1)
Concrete-Abstract 0.686 0.673 0.681
Birke-Sarkar NA NA 0.649
Majority Class 0.697 0.408 0.629
Probability Matching 0.605 0.500 0.500
Table 6: The performance with unknown verbs.
of Concrete-Abstract. In no case is any score signifi-
cantly above the performance of Concrete-Abstract,
at the 95% confidence level.
Table 7 shows the coefficients in the logistic re-
gression model that was learned on the training data.
The items numbered from 1 to 5 are the five features
described in Section 4.2. The sixth item is the con-
stant term in the regression equation. We see that the
abstractness of the nouns (excluding proper nouns)
has the largest weight in predicting whether the tar-
get verb is in class N.
Feature Coefficient
1 AvgNounAbs 11.4117
2 AvgPropAbs 0.7250
3 AvgVerbAbs -0.5528
4 AvgAdjAbs 1.1478
5 AvgAdvAbs -0.2013
6 Intercept -5.9436
Table 7: The logistic regression coefficients for class N.
5 Discussion
It is a strength of our approach that it can classify
verbs that it has never seen before, as we see in Sec-
tion 4.3. The feature vectors in all three experiments
are based only on the context; the target adjective or
verb is not used in the vectors. This avoids the need
for gathering training data on every verb or adjective
for which we want to determine whether it is being
used metaphorically or literally, since the algorithm
is not sensitive to the specific target word.
On the other hand, the performance might im-
prove if the target word were included in the fea-
ture vectors. If metaphor is a method for transfer-
ring knowledge from concrete domains to abstract
domains, then it follows that highly abstract target
words will tend to be used literally in most con-
texts. For instance, the highly abstract verb epito-
mize (with an abstractness rating of 0.85861) is per-
haps almost always used in a literal sense. There-
688
fore it would seem that the abstractness rating of the
target word could be a useful clue for determining
whether the sense is literal or metaphorical.
We experimented with including the abstractness
rating of the target word as a feature, but the im-
pact on performance was not significant for either
the adjectives or the verbs. We hypothesize that this
may be due to the relatively narrow range in the ab-
stractness of the adjectives and verbs in our data.
The abstractness ratings of the adjectives vary from
0.43356 for dark to 0.56637 for hard. The abstract-
ness ratings of the fifty verbs range from 0.28756 for
plant to 0.71628 for lend, but 80% of the verbs lie
in the range from 0.41879 for fly to 0.59912 for rest.
It seems possible that the abstractness rating of the
target word would be useful with a dataset in which
the target?s abstractness varied substantially.
In future work, we would like to gather data for
target words with a wider range of abstractness. We
expect that such data would show some benefit to in-
cluding information on the abstractness of the target
word in the feature vector.
We also expect that a hybrid of classical word
sense disambiguation, such as Birke and Sarkar?s
(2006) algorithm, with abstractness ratings would
perform better than either approach alone. Abstract-
ness may provide a good rough estimate of whether
a word usage is literal or metaphorical, but it seems
likely that knowledge of the specific target word in
question will be required for a highly precise answer.
This is another worthwhile topic for future research.
Currently there is no algorithm that identifies
what kind of concepts and relations are grafted from
the source domain to the target domain by metaphor-
ical inference. The algorithm presented in this pa-
per may be used within a constraints-based model
of metaphor (Neuman and Nave, 2009) to address
this challenge.
Recently there has been some interest in visual-
ness, picturability, and imagability, the degree to
which a word is associated with visual imagery (De-
schacht and Moens, 2007). Although Xing et al
(2010) use the term concreteness in their work, their
research is concerned with predicting the difficulty
of queries for image retrieval. It could be argued that
Xing et al should be trying to capture imagability,
not concreteness.
The MRC Psycholinguistic Database (Coltheart,
1981) includes words rated for imagability. Our al-
gorithm for rating the abstractness of words (Sec-
tion 2) could easily be trained with the MRC imaga-
bility ratings instead of the abstractness ratings. In
future work, it would be interesting to evaluate
imagability ratings on the TroFi Example Base. It
would also be worthwhile to see whether our algo-
rithm can be adapted for image retrieval (Xing et al,
2010) and image annotation (Deschacht and Moens,
2007).
6 Conclusion
Metaphor is ubiquitous, yet recognizing textual
entailment is a challenge when words are used
metaphorically. An algorithm for distinguishing
metaphorical and literal senses of a word will facil-
itate correct textual inference, which will improve
the many NLP applications that depend on textual
inference.
We have introduced a new algorithm for measur-
ing the degree of abstractness of a word. Inspired by
research in cognitive linguistics (Lakoff and John-
son, 1980), we hypothesize that the degree of ab-
stractness of the context in which a given word ap-
pears is predictive of whether the word is used in
a metaphorical or literal sense. This hypothesis is
supported by three experiments.
A strength of this approach to the problem of dis-
tinguishing metaphorical and literal senses is that
it readily generalizes to new words, outside of the
training data. We do not claim that abstractness is
a complete solution to the problem, but it may be a
valuable component in any practical system for pro-
cessing metaphorical text.
Acknowledgments
Part of the work of Yair Neuman, Dan Assaf, and
Yohai Cohen has been supported by a grant from the
Israel Ministry of Defense. Thanks to the EMNLP
reviewers for their helpful comments.
References
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
689
Computational Linguistics (EACL 2006), pages 329?
336.
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches to
Figurative Language at HLT/NAACL-07, pages 21?28.
Stefan Bu?ttcher and Charles Clarke. 2005. Efficiency vs.
effectiveness in terabyte-scale information retrieval.
In Proceedings of the 14th Text REtrieval Conference
(TREC 2005), Gaithersburg, MD.
John Caron. 2001. Experiments with LSA scor-
ing: Optimal rank and basis. In Proceedings of
the SIAM Computational Information Retrieval Work-
shop, pages 157?169, Raleigh, NC.
Mark Changizi. 2008. Economically organized hierar-
chies in WordNet and the Oxford English Dictionary.
Cognitive Systems Research, 9(3):214?228.
Kenneth Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In
Proceedings of the 27th Annual Conference of the As-
sociation of Computational Linguistics, pages 76?83,
Vancouver, British Columbia.
Max Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psychol-
ogy, 33A(4):497?505.
Marcel Danesi. 2003. Metaphorical ?networks? and ver-
bal communication: A semiotic perspective on human
discourse. Sign Systems Studies, 31:341?363.
Mark Davies. 2009. The 385+ million word Corpus of
Contemporary American English (1990?2008+): De-
sign, architecture, and linguistic insights. Interna-
tional Journal of Corpus Linguistics, 14(2):159?190.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 1000?1007.
William B. Dolan. 1995. Metaphor as an emergent prop-
erty of machine-readable dictionaries. In Proceedings
of the AAAI 1995 Spring Symposium Series: Repre-
sentation and Acquisition of Lexical Knowledge: Pol-
ysemy, Ambiguity and Generativity, pages 27?32.
Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and Con-
suelo Boronat. 2001. Metaphor is like analogy. In
D. Gentner, K. J. Holyoak, and B. N. Kokinov, editors,
The analogical mind: Perspectives from Cognitive Sci-
ence, pages 199?253. MIT Press, Cambridge, MA.
Chikara Hashimoto and Daisuke Kawahara. 2009. Com-
pilation of an idiom example database for supervised
idiom identification. Language Resources and Evalu-
ation, 43(4):355?384.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University Of Chicago Press, Chicago, IL.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Saskia Le Cessie and J.C. Van Houwelingen. 1992.
Ridge estimators in logistic regression. Applied Statis-
tics, 41(1):191?201.
Alexis Madrigal. 2011. Why are spy researchers build-
ing a ?Metaphor Program?? The Atlantic, May 25.
James H. Martin. 1992. Computer understanding of
conventional metaphoric language. Cognitive Science,
16(2):233?270.
Zachary Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Yair Neuman and Ophir Nave. 2009. Metaphor-based
meaning excavation. Information Sciences, 179:2719?
2728.
Malvina Nissim and Katja Markert. 2003. Syntactic
features and word similarity for supervised metonymy
resolution. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), pages 56?63, Sapporo, Japan.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit, pages 315?322.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector space model for automatic indexing. Com-
munications of the ACM, 18(11):613?620.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San Fran-
cisco.
Xing Xing, Yi Zhang, and Mei Han. 2010. Query dif-
ficulty prediction for contextual image retrieval. In
Advances in Information Retrieval, volume 5993 of
Lecture Notes in Computer Science, pages 581?585.
Springer.
690
Computing Lexical Contrast
Saif M. Mohammad?
National Research Council Canada
Bonnie J. Dorr??
University of Maryland
Graeme Hirst?
University of Toronto
Peter D. Turney?
National Research Council Canada
Knowing the degree of semantic contrast between words has widespread application in natural
language processing, including machine translation, information retrieval, and dialogue sys-
tems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of
many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not
classify opposites into the different kinds, however. They also do not explicitly list word pairs
that are not opposites but yet have some degree of contrast in meaning, such as warm and cold
or tropical and freezing. We propose an automatic method to identify contrasting word pairs
that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is
a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly
related. (For example, there exists the pair of opposites hot and cold such that tropical is related
to hot, and freezing is related to cold.) We will call this the contrast hypothesis.
We begin with a large crowdsourcing experiment to determine the amount of human
agreement on the concept of oppositeness and its different kinds. In the process, we flesh out
key features of different kinds of opposites. We then present an automatic and empirical measure
of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of
a Roget-like thesaurus. We show how, using four different data sets, we evaluated our approach
on two different tasks, solving ?most contrasting word? questions and distinguishing synonyms
from opposites. The results are analyzed across four parts of speech and across five different kinds
of opposites. We show that the proposed measure of lexical contrast obtains high precision and
large coverage, outperforming existing methods.
? National Research Council Canada. E-mail: saif.mohammad@nrc-cnrc.gc.ca.
?? Department of Computer Science and Institute of Advanced Computer Studies, University of Maryland.
E-mail: bonnie@umiacs.umd.edu.
? Department of Computer Science, University of Toronto. E-mail: gh@cs.toronto.edu.
? National Research Council Canada. E-mail: peter.turney@nrc-cnrc.gc.ca.
Submission received: 14 January 2010; revised submission received: 26 June 2012; accepted for publication:
16 July 2012.
doi:10.1162/COLI a 00143
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Native speakers of a language intuitively recognize different degrees of lexical contrast?
for example, most people will agree that hot and cold have a higher degree of contrast
than cold and lukewarm, and cold and lukewarm have a higher degree of contrast than
penguin and clown. Automatically determining the degree of contrast between words
has many uses, including:
 Detecting and generating paraphrases (Marton, El Kholy, and Habash
2011) (The dementors caught Sirius Black / Black could not escape the
dementors).
 Detecting certain types of contradictions (de Marneffe, Rafferty, and
Manning 2008; Voorhees 2008) (Kyoto has a predominantly wet climate /
It is mostly dry in Kyoto). This is in turn useful in effectively reranking
target language hypotheses in machine translation, and for reranking
query responses in information retrieval.
 Understanding discourse structure and improving dialogue systems.
Opposites often indicate the discourse relation of contrast (Marcu and
Echihabi 2002).
 Detecting humor (Mihalcea and Strapparava 2005). Satire and jokes tend
to have contradictions and oxymorons.
 Distinguishing near-synonyms from word pairs that are semantically
contrasting in automatically created distributional thesauri. Measures
of distributional similarity typically fail to do so.
Detecting lexical contrast is not sufficient by itself to solve most of these problems, but
it is a crucial component.
Lexicons of pairs of words that native speakers consider opposites have been
created for certain languages, but their coverage is limited. Opposites are of many kinds,
such as antipodals, complementaries, and gradable (summarized in Section 3). Existing
lexicons often do not classify opposites into the different kinds, however. Further, the
terminology is inconsistent across different sources. For example, Cruse (1986) defines
antonyms as gradable adjectives that are opposite in meaning, whereas the WordNet
antonymy link connects some verb pairs, noun pairs, and adverb pairs too. In this
article, we will follow Cruse?s terminology, and we will refer to word pairs connected
by WordNet?s antonymy link as opposites, unless referring specifically to gradable
adjectival pairs.
Manually created lexicons also do not explicitly list word pairs that are not
opposites but yet have some degree of contrast in meaning, such as warm and cold or
tropical and cold. Further, contrasting word pairs far outnumber those that are commonly
considered opposites. In our own experiments described later in this article, we find that
more than 90% of the contrasting pairs in GRE ?most contrasting word? questions are
not listed as antonyms in WordNet. We should not infer from this that WordNet or any
other lexicographic resource is a poor source for detecting opposites, but rather that
identifying the large number of contrasting word pairs requires further computation,
possibly relying on other semantic relations stored in the lexicographic resource.
Even though a number of computational approaches have been proposed for se-
mantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy?
556
Mohammad et al Computing Lexical Contrast
hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some
extent, this is because lexical contrast is not as well understood as other classical lexical?
semantic relations.
Over the years, many definitions of semantic contrast and opposites have been
proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan
1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from
each other in various respects. Cruse (1986, page 197) observes that even though people
have a robust intuition of opposites, ?the overall class is not a well-defined one.? He
points out that a defining feature of opposites is that they tend to have many common
properties, but differ saliently along one dimension of meaning. We will refer to this
semantic dimension as the dimension of opposition. For example, giant and dwarf are
both living beings; they both eat, they both walk, they are both capable of thinking, and
so on. They are most saliently different, however, along the dimension of height. Cruse
also points out that sometimes it is difficult to identify or articulate the dimension of
opposition (for example, city?farm).
Another way to define opposites is that they are word pairs with a ?binary incom-
patible relation? (Kempson 1977, page 84). That is to say that one member entails the
absence of the other, and given one member, the identity of the other member is obvious.
Thus, night and day are good examples of opposites because night is best paraphrased
by not day, rather than the negation of any other term. On the other hand, blue and yellow
make poor opposites because even though they are incompatible, they do not have an
obvious binary relation such that blue is understood to be a negation of yellow. It should
be noted that there is a relation between binary incompatibility and difference along just
one dimension of meaning.
For this article, we define opposites to be term pairs that clearly satisfy either
the property of binary incompatibility or the property of salient difference across a
dimension of meaning. Word pairs may satisfy the two properties to different degrees,
however. We will refer to all word pairs that satisfy either of the two properties to some
degree as contrasting. For example, daylight and darkness are very different along the
dimension of light, and they satisfy the binary incompatibity property to some degree,
but not as strongly as day and night. Thus we will consider both daylight and darkness as
well as day and night as semantically contrasting pairs (the former pair less so than the
latter), but only day and night as opposites. Even though there are subtle differences in
the meanings of the terms contrasting, opposite, and antonym, they have often been used
interchangeably in the literature, dictionaries, and common parlance. Thus, we list here
what we use these terms to mean in this article:
 Opposites are word pairs that have a strong binary incompatibility
relation with each other and/or are saliently different across a dimension
of meaning.
 Contrasting word pairs are word pairs that have some non-zero degree
of binary incompatibility and/or have some non-zero difference across
a dimension of meaning. Thus, all opposites are contrasting, but not all
contrasting pairs are opposites.
 Antonyms are opposites that are also gradable adjectives.1
1 We follow Cruse?s (1986) definition for antonyms. The WordNet antonymy link, however, also connects
some verb pairs, noun pairs, and adverb pairs.
557
Computational Linguistics Volume 39, Number 3
In this article, we present an automatic method to identify contrasting word pairs
that is based on the following hypothesis:
Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair of
opposites, C and D, such that A and C are strongly related and B and D are strongly
related.
For example, there exists the pair of opposites night and day such that darkness is related
to night, and daylight is related to day. We then determine the degree of contrast between
two words using this hypothesis:
Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then their
degree of contrast is proportional to their tendency to co-occur in a large corpus.
For example, consider the contrasting word pairs top?low and top?down; because top and
down occur together much more often than top and low, our method concludes that the
pair top?down has a higher degree of lexical contrast than the pair top?low. The degree
of contrast hypothesis is inspired by the idea that opposites tend to co-occur more often
than chance (Charles and Miller 1989; Fellbaum 1995). Murphy and Andrew (1993)
claim that this is because together opposites convey contrast well, which is rhetorically
useful. Thus we hypothesize that the higher the degree of contrast between two words,
the higher the tendency of people to use them together.
Because opposites are a key component of our method, we begin by first under-
standing different kinds of opposites (Section 3). Then we describe a crowdsourced
project on the annotation of opposites into different kinds (Section 4). In Section 5.1,
we examine whether opposites and other highly contrasting word pairs occur together
in text more often than randomly chosen word pairs. This experiment is crucial to
the degree of contrast hypothesis because if our assumption is true, then we should
find that highly contrasting pairs are used together much more often than randomly
chosen word pairs. Section 5.2 examines this question. Section 6 presents our method
to automatically compute the degree of contrast between word pairs by relying on
the contrast hypothesis, the degree of contrast hypothesis, seed opposites, and the
structure of a Roget-like thesaurus. (This method was first described in Mohammad,
Dorr, and Hirst [2008].) Finally we present experiments that evaluate various aspects of
the automatic method (Section 7). Following is a summary of the key research questions
addressed by this article:
(1) On the kinds of opposites:
Research questions: How good are humans at identifying different kinds
of opposites? Can certain term pairs belong to more than one kind of
opposite?
Experiment: In Sections 3 and 4, we describe how we designed a ques-
tionnaire to acquire annotations about opposites. Because the annotations
are done by crowdsourcing, and there is no control over the educational
background of the annotators, we devote extra effort to make sure that the
questions are phrased in a simple, yet clear, manner. We deploy a quality
control method that uses a word-choice question to automatically identify
and discard dubious and outlier annotations.
Findings: We find that humans agree markedly in identifying opposites;
there is significant variation in the agreement for different kinds of
558
Mohammad et al Computing Lexical Contrast
opposites, however. We find that a large number of opposing word pairs
have properties pertaining to more than one kind of opposite.
(2) On the manifestation of opposites and other highly contrasting pairs in text:
Research questions: How often do highly contrasting word pairs co-occur
in text? How strong is this tendency compared with random word pairs,
and compared with near-synonym word pairs?
Experiment: Section 5 describes how we compiled sets of highly contrast-
ing word pairs (including opposites), near-synonym pairs, and random
word pairs, and determine the tendency for pairs in each set to co-occur in
a corpus.
Findings: Highly contrasting word pairs co-occur significantly more often
than both the random word pairs set and also the near-synonyms set.
We also find that the average distributional similarity of highly contrasting
word pairs is higher than that of synonymous words. The standard de-
viations of the distributions for the high-contrast set and the synonyms
set are large, however, and so the tendency to co-occur is not sufficient to
distinguish highly contrasting word pairs from near-synonymous pairs.
(3) On an automatic method for computing lexical contrast:
Research questions: How can the contrast hypothesis and the degree
of contrast hypothesis be used to develop an automatic method for
identifying contrasting word pairs? How can we automatically generate
the list of opposites, which are needed as input for a method relying on
the contrast hypothesis?
Proposed Method: Section 6 describes an empirical method for deter-
mining the degree of contrast between two words by using the contrast
hypothesis, the degree of contrast hypothesis, the structure of a thesaurus,
and seed opposite pairs. The use of affixes to generate seed opposite pairs
is also described. (This method was first proposed in Mohammad, Dorr,
and Hirst [2008].)
(4) On the evaluation of automatic methods of contrast:
Research questions: How accurate are automatic methods at identifying
whether one word pair has a higher degree of contrast than another? What
is the accuracy of this method in detecting opposites (a notable subset of
the contrasting pairs)? How does this accuracy vary for different kinds of
opposites?2 How easy is it for automatic methods to distinguish between
opposites and synonyms? How does the proposed method perform when
compared with other automatic methods?
Experiments: We conduct three experiments (described in Sections 7.1,
7.2, and 7.3) involving three different data sets and two tasks to answer
2 Note that though linguists have classified opposites into different kinds, we know of no work doing so
for contrasts more generally. Thus this particular analysis must be restricted to opposites alone.
559
Computational Linguistics Volume 39, Number 3
these questions. We compare performance of our method with methods
proposed by Lin et al (2003) and Turney (2008). We automatically
generate a new set of 1,296 ?most contrasting word? questions to evaluate
performance of our method on five different kinds of opposites and across
four parts of speech. (The evaluation described in Section 7.1 was first
described in Mohammad, Dorr, and Hirst [2008].)
Findings: We find that the proposed measure of lexical contrast obtains
high precision and large coverage, outperforming existing methods.
Our method performs best on gradable pairs, antipodal pairs, and com-
plementary pairs, but poorly on disjoint opposite pairs. Among different
parts of speech, the method performs best on noun pairs, and relatively
worse on verb pairs.
All of the data created and compiled as part of this research are summarized in Table 18
(Section 8), and is available for download.3
2. Related Work
Charles and Miller (1989) proposed that opposites occur together in a sentence more
often than chance. This is known as the co-occurrence hypothesis. Paradis, Willners,
and Jones (2009) describe further experiments to show how canonical opposites tend
to have high textual co-occurrence. Justeson and Katz (1991) gave evidence in support
of the hypothesis using 35 prototypical opposites (from an original set of 39 opposites
compiled by Deese [1965]) and also with an additional 22 frequent opposites. They also
showed that opposites tend to occur in parallel syntactic constructions. All of these
pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb,
adjective, and adverb pairs (noun?noun, noun?verb, noun?adjective, verb?adverb, etc.)
pertaining to 18 concepts (for example, lose(v)?gain(n) and loss(n)?gain(n), where lose(v)
and loss(n) pertain to the concept of ?failing to have/maintain?). Non-opposite semanti-
cally related words also tend to occur together more often than chance, however. Thus,
separating opposites from these other classes has proven to be difficult.
Some automatic methods of lexical contrast rely on lexical patterns in text. For
example, Lin et al (2003) used patterns such as ?from X to Y ? and ?either X or Y ?
to separate opposites from distributionally similar pairs. They evaluated their method
on 80 pairs of opposites and 80 pairs of synonyms taken from the Webster?s Collegiate
Thesaurus (Kay 1988). The evaluation set of 160 word pairs was chosen such that it
included only high-frequency terms. This was necessary to increase the probability
of finding sentences in a corpus where the target pair occurred in one of the chosen
patterns. Lobanova, van der Kleij, and Spenader (2010) used a set of Dutch adjective
seed pairs to learn lexical patterns commonly containing opposites. The patterns were
in turn used to create a larger list of Dutch opposites. The method was evaluated by
comparing entries to Dutch lexical resources and by asking human judges to determine
whether an automatically found pair is indeed an opposite. Turney (2008) proposed a
supervised method for identifying synonyms, opposites, hypernyms, and other lexical-
semantic relations between word pairs. The approach learns patterns corresponding to
different relations.
3 http://www.purl.org/net/saif.mohammad/research.
560
Mohammad et al Computing Lexical Contrast
Harabagiu, Hickl, and Lacatusu (2006) detected contrasting word pairs for the
purpose of identifying contradictions by using WordNet chains?synsets connected by
the hypernymy?hyponymy links and exactly one antonymy link. Lucerto, Pinto, and
Jimen?ez-Salazar (2002) proposed detecting contrasting word pairs using the number
of tokens between two words in text and also cue words such as but, from, and and.
Unfortunately, they evaluated their method on only 18 word pairs. Neither Harabagiu,
Hickl, and Lacatusu nor Lucerto, Pinto, and Jime?nez-Salazar determined the degree of
contrast between words, and their methods have not been shown to have substantial
coverage.
Schwab, Lafourcade, and Prince (2002) created an oppositeness vector for a target
word. The closer this vector is to the context vector of the other target word, the
more opposite the two target words are. The oppositeness vectors were created by first
manually identifying possible opposites and then generating suitable vectors for each
using dictionary definitions. The approach was evaluated on only a handful of word
pairs.
There is a large amount of work on sentiment analysis and opinion mining aimed
at determining the polarity of words (Pang and Lee 2008). For example, Pang, Lee, and
Vaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping cast
their qualifying nouns positively whereas adjectives such as bad, cliched, and boring
portray the noun negatively. Many of these gradable adjectives have opposites, but
these approaches, with the exception of that of Hatzivassiloglou and McKeown (1997),
did not attempt to determine pairs of positive and negative polarity words that are
opposites. Hatzivassiloglou and McKeown proposed a supervised algorithm that uses
word usage patterns to generate a graph with adjectives as nodes. An edge between
two nodes indicates either that the two adjectives have the same or opposite polarity. A
clustering algorithm then partitions the graph into two subgraphs such that the nodes in
a subgraph have the same polarity. They used this method to create a lexicon of positive
and negative words, and argued that the method could also be used to detect opposites.
3. The Heterogeneous Nature of Opposites
Opposites, unlike synonyms, can be of different kinds. Many different classifications
have been proposed, one of which is given by Cruse (1986) (Chapters 9, 10, and 11).
It consists of complementaries (open?shut, dead?alive), antonyms (long?short, slow?
fast) (further classified into polar, overlapping, and equipollent opposites), directional
opposites (up?down, north?south) (further classified into antipodals, counterparts, and
reversives), relational opposites (husband?wife, predator?prey), indirect converses (give?
receive, buy?pay), congruence variants (huge?little, doctor?patient), and pseudo opposites
(black?white).
Various lexical relations have also received attention at the Educational Testing
Services, as analogies and ?most contrasting word? questions are part of the tests they
conduct. They classify opposites into contradictories (alive?dead, masculine?feminine),
contraries (old?young, happy-sad), reverses (attack?defend, buy?sell), directionals ( front?
back, left?right), incompatibles (happy?morbid, frank?hypocritical), asymmetric contraries
(hot?cool, dry?moist), pseudo-opposites (popular?shy, right?bad), and defectives (default?
payment, limp?walk) (Bejar, Chaffin, and Embretson 1991).
Keeping in mind the meanings and subtle distinctions between each of these kinds
of opposites is not easy even if we provide extensive training to annotators. Because
we crowdsource the annotations, and we know that Turkers prefer to spend their
time doing the task (and making money) rather than reading lengthy descriptions, we
561
Computational Linguistics Volume 39, Number 3
focused only on five kinds of opposites that we believed would be easiest to annotate,
and which still captured a majority of the opposites:
 Antipodals (top?bottom, start?finish): Antipodals are opposites in which
?one term represents an extreme in one direction along some salient axis,
while the other term denotes the corresponding extreme in the other
direction? (Cruse 1986, page 225).
 Complementaries (open?shut, dead?alive): The essential characteristic of a
pair of complementaries is that ?between them they exhaustively divide
the conceptual domain into two mutually exclusive compartments, so that
what does not fall into one of the compartments must necessarily fall into
the other? (Cruse 1986, page 198).
 Disjoint (hot?cold, like?dislike): Disjoint opposites are word pairs that
occupy non-overlapping regions in the semantic dimension such that there
are regions not covered by either term. This set of opposites includes
equipollent adjective pairs (for example, hot?cold) and stative verb pairs
(for example, like?dislike). We refer the reader to Sections 9.4 and 9.7 of
Cruse (1986) for details about these sub-kinds of opposites.
 Gradable opposites (long?short, slow?fast): are adjective-pair or
adverb-pair opposites that are gradable, that is, ?members of the pair
denote degrees of some variable property such as length, speed, weight,
accuracy, etc.? (Cruse 1986, page 204).
 Reversibles (rise?fall, enter?exit): Reversibles are opposite verb pairs such
that ?if one member denotes a change from A to B, its reversive partner
denotes a change from B to A? (Cruse 1986, page 226).
It should be noted that there is no agreed-upon number of kinds of opposites. Different
researchers have proposed various classifications that overlap to a greater or lesser
degree. It is possible that for a certain application or study one may be interested in
a kind of opposite not listed here.
4. Crowdsourcing
We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different
kinds of opposites. We broke the task into small independently solvable units called
HITs (Human Intelligence Tasks) and uploaded them on the AMT Web site.4 Each HIT
had a set of questions, all of which were to be answered by the same person (a Turker, in
AMT parlance). We created HITs for word pairs, taken from WordNet, that we expected
to have some degree of contrast in meaning.
In WordNet, words that are close in meaning are grouped together in a set called a
synset. If one of the words in a synset is an opposite of another word in a different synset,
then the two synsets are called head synsets and WordNet records the two words as
direct antonyms (Gross, Fischer, and Miller 1989)?WordNet regards the terms opposite
and antonym as synonyms. Other word pairs across the two head synsets are called
indirect antonyms. Because we follow Cruse?s definition of antonyms, which requires
4 https://www.mturk.com/mturk/welcome.
562
Mohammad et al Computing Lexical Contrast
Table 1
Target word pairs chosen for annotation. Each term was annotated about eight times.
part of speech # of word pairs
adverbs 185
adjectives 646
nouns 416
verbs 309
all 1,556
antonyms to be gradable adjectives, and because WordNet?s direct antonyms include
noun, verb, and adverb pairs too, for the rest of the article we will refer to WordNet
direct antonyms as direct opposites and WordNet indirect antonyms as indirect op-
posites. We will refer to the union of both the direct and indirect opposites simply as
WordNet opposites. Note that the WordNet opposites are highly contrasting term pairs.
We chose as target pairs all the direct or indirect opposites from WordNet that were
also listed in the Macquarie Thesaurus. This condition was a mechanism to ignore less-
frequent and obscure words, and apply our resources on words that are more common.
Additionally, as we will describe subsequently, we use the presence of the words in
the thesaurus to help generate Question 1, which we use for quality control of the
annotations. Table 1 gives a breakdown of the 1,556 pairs chosen by part of speech.
Because we do not have any control over the educational background of the
annotators, we made efforts to phrase questions about the kinds of opposites in a simple
and clear manner. Therefore we avoided definitions and long instructions in favor of
examples and short questions. We believe this strategy is beneficial even in traditional
annotation scenarios.
We created separate questionnaires (HITs) for adjectives, adverbs, nouns, and
verbs. A complete example adjective HIT with directions and questions is shown
in Figure 1. The adverb, noun, and verb questionnaires had similar questions, but
were phrased slightly differently to accommodate differences in part of speech. These
questionnaires are not shown here due to lack of space, but all four questionnaires are
available for download.5 The verb questionnaire had an additional question, shown
in Figure 2. Because nouns and verbs are not considered gradable, the corresponding
questionnaires did not have Q8 and Q9. We requested annotations from eight different
Turkers for each HIT.
4.1 The Word Choice Question: Q1
Q1 is an automatically generated word choice question that has a clear correct answer. It
helps identify outlier and malicious annotations. If this question is answered incorrectly,
then we assume that the annotator does not know the meanings of the target words, and
we ignore responses to the remaining questions. Further, as this question makes the
annotator think about the meanings of the words and about the relationship between
them, we believe it improves the responses for subsequent questions.
The options for Q1 were generated automatically. Each option is a set of four
comma-separated words. The words in the answer are close in meaning to both of the
target words. In order to create the answer option, we first generated a much larger
5 http://www.purl.org/net/saif.mohammad/research.
563
Computational Linguistics Volume 39, Number 3
Word-pair: musical ? dissonant
Q1. Which set of words is most related to the word pair musical:dissonant?
 useless, surgery, ineffectual, institution
 sequence, episode, opus, composition
 youngest, young, youthful, immature
 consequential, important, importance, heavy
Q2. Do musical and dissonant have some contrast in meaning?
 yes  no
For example, up?down, lukewarm?cold, teacher?student, attack?defend, all have at least
some degree of contrast in meaning. On the other hand, clown?down, chilly?cold,
teacher?doctor, and attack?rush DO NOT have contrasting meanings.
Q3. Some contrasting words are paired together so often that given one we naturally
think of the other. If one of the words in such a pair were replaced with another word of
almost the same meaning, it would sound odd. Are musical:dissonant such a pair?
 yes  no
Examples for ?yes?: tall?short, attack?defend, honest?dishonest, happy?sad.
Examples for ?no?: tall?stocky, attack?protect, honest?liar, happy?morbid.
Q5. Do musical and dissonant represent two ends or extremes?
 yes  no
Examples for ?yes?: top?bottom, basement?attic, always?never, all?none, start?finish.
Examples for ?no?: hot?cold (boiling refers to more warmth than hot and freezing refers
to less warmth than cold), teacher?student (there is no such thing as more or less teacher
and more or less student), always?sometimes (never is fewer times than sometimes).
Q6. If something is musical, would you assume it is not dissonant, and vice versa?
In other words, would it be unusual for something to be both musical and dissonant?
 yes  no
Examples for ?yes?: happy?sad, happy?morbid, vigilant?careless, slow?stationary.
Examples for ?no?: happy?calm, stationary?still, vigilant?careful, honest?truthful.
Q7. If something or someone could possibly be either musical or dissonant, is it
necessary that it must be either musical or dissonant? In other words, is it true that for
things that can be musical or dissonant, there is no third possible state, except perhaps
under highly unusual circumstances?
 yes  no
Examples for ?yes?: partial?impartial, true?false, mortal?immortal.
Examples for ?no?: hot?cold (an object can be at room temperature is neither hot nor cold),
tall?short (a person can be of medium or average height).
Q8. In a typical situation, if two things or two people are musical, then can one be more
musical than the other?
 yes  no
Examples for ?yes?: quick, exhausting, loving, costly.
Examples for ?no?: dead, pregnant, unique, existent.
Q9. In a typical situation, if two things or two people are dissonant, can one be more
dissonant than the other?
 yes  no
Examples for ?yes?: quick, exhausting, loving, costly, beautiful.
Examples for ?no?: dead, pregnant, unique, existent, perfect, absolute.
Figure 1
Example HIT: Adjective pairs questionnaire.
Note: Perhaps ?musical ? dissonant? might be better written as ?musical versus dissonant,? but we have
kept ??? here to show the reader exactly what the Turkers were given.
Note: Q4 is not shown here, but can be seen in the on-line version of the questionnaire. It was an exploratory
question, and it was not multiple choice. Q4?s responses have not been analyzed.
564
Mohammad et al Computing Lexical Contrast
Word-pair: enabling ? disabling
Q10. In a typical situation, do the sequence of actions disabling and then enabling bring
someone or something back to the original state, AND do the sequence of actions enabling
and disabling also bring someone or something back to the original state?
 yes, both ways: the transition back to the initial state makes much sense in both
sequences.
 yes, but only one way: the transition back to the original state makes much more sense
one way than the other way.
 none of the above
Examples for ?yes, both ways?: enter?exit, dress?undress, tie?untie, appear?disappear.
Examples for ?yes, but only one way?: live?die, create?destroy, damage?repair, kill?resurrect.
Examples for ?none of the above?: leave?exit, teach?learn, attack?defend (attacking and then
defending does not bring one back to the original state).
Figure 2
Additional question in the questionnaire for verbs.
source pool of all the words that were in the same thesaurus category as any of the two
target words. (Words in the same category are closely related.) Words that had the same
stem as either of the target words were discarded. For each of the remaining words, we
added their Lesk similarities with the two target words (Banerjee and Pedersen 2003).
The four words with the highest sum were chosen to form the answer option.
The three distractor options were randomly selected from the pool of correct an-
swers for all other word choice questions. Finally, the answer and distractor options
were presented to the Turkers in random order.
4.2 Post-Processing
The response to a HIT by a Turker is called an assignment. We obtained about 12,448
assignments in all (1,556 pairs ? 8 assignments each). About 7% of the adjective, adverb,
and noun assignments and about 13% of the verb assignments had an incorrect answer
to Q1. These assignments were discarded, leaving 1,506 target pairs with three or more
valid assignments. We will refer to this set of assignments as the master set, and all
further analysis in this article is based on this set. Table 2 gives a breakdown of the
average number of annotations for each of the target pairs in the master set.
4.3 Prevalence of Different Kinds of Contrasting Pairs
For each question pertaining to every word pair in the master set, we determined the
most frequent response by the annotators. Table 3 gives the percentage of word pairs in
Table 2
Number of word pairs and average number of annotations per word pair in the master set.
part of # of average # of
speech word pairs annotations
adverbs 182 7.80
adjectives 631 8.32
nouns 405 8.44
verbs 288 7.58
all 1,506 8.04
565
Computational Linguistics Volume 39, Number 3
Table 3
Percentage of word pairs that received a response of ?yes? for the questions in the questionnaire.
adj. = adjectives; adv. = adverbs.
% of word pairs
Question answer adj. adv. nouns verbs
Q2. Do X and Y have some contrast? yes 99.5 96.8 97.6 99.3
Q3. Are X and Y opposites? yes 91.2 68.6 65.8 88.8
Q5. Are X and Y at two ends of a dimension? yes 81.8 73.5 81.1 94.4
Q6. Does X imply not Y? yes 98.3 92.3 89.4 97.5
Q7. Are X and Y mutually exhaustive? yes 85.1 69.7 74.1 89.5
Q8. Does X represent a point on some scale? yes 78.5 77.3 ? ?
Q9. Does Y represent a point on some scale? yes 78.5 70.8 ? ?
Q10. Does X undo Y OR does Y undo X? one way ? ? ? 3.8
both ways ? ? ? 90.9
Table 4
Percentage of WordNet source pairs that are contrasting, opposite, and ?contrasting but not
opposite.?
category basis adj. adv. nouns verbs
contrasting Q2 yes 99.5 96.8 97.6 99.3
opposites Q2 yes and Q3 yes 91.2 68.6 60.2 88.9
contrasting, but not opposite Q2 yes and Q3 no 8.2 28.2 37.4 10.4
the master set that received a most frequent response of ?yes.? The first column in the
table lists the question number followed by a brief description of question. (Note that
the Turkers saw only the full forms of the questions, as shown in the example HIT.)
Observe that most of the word pairs are considered to have at least some contrast
in meaning. This is not surprising because the master set was constructed using words
connected through WordNet?s antonymy relation.6 Responses to Q3 show that not all
contrasting pairs are considered opposite, and this is especially the case for adverb
pairs and noun pairs. The rows in Table 4 show the percentage of words in the master
set that are contrasting (row 1), opposite (row 2), and contrasting but not opposite
(row 3).
Responses to Q5, Q6, Q7, Q8, and Q9 (Table 3) show the prevalence of different
kinds of relations and properties of the target pairs.
Table 5 shows the percentage of contrasting word pairs that may be classified into
the different types discussed in Section 3. Observe that rows for all categories other
than the disjoints have percentages greater than 60%. This means that a number of
contrasting word pairs can be classified into more than one kind. Complementaries
are the most common kind in case of adverbs, nouns, and verbs, whereas antipodals
are most common among adjectives. A majority of the adjective and adverb contrasting
pairs are gradable, but more than 30% of the pairs are not. Most of the verb pairs are
reversives (91.6%). Disjoint pairs are much less common than all the other categories
6 All of the direct antonyms were marked as contrasting by the Turkers. Only a few indirect antonyms
were marked as not contrasting.
566
Mohammad et al Computing Lexical Contrast
Table 5
Percentage of contrasting word pairs belonging to various subtypes. The subtype ?reversives?
applies only to verbs. The subtype ?gradable? applies only to adjectives and adverbs.
subtype basis adv. adj. nouns verbs
Antipodals Q2 yes, Q5 yes 82.3 75.9 82.5 95.1
Complementaries Q2 yes, Q7 yes 85.6 72.0 84.8 98.3
Disjoint Q2 yes, Q7 no 14.4 28.0 15.2 1.7
Gradable Q2 yes, Q8 yes, Q9 yes 69.6 66.4 - -
Reversives Q2 yes, Q10 both ways - - - 91.6
considered, and they are most prominent among adjectives (28%), and least among verb
pairs (1.7%).
4.4 Agreement
People do not always agree on linguistic classifications of terms, and one of the goals of
this work was to determine how much people agree on properties relevant to different
kinds of opposites. Table 6 lists the breakdown of agreement by target-pair part of
speech and question, where agreement is the average percentage of the number of
Turkers giving the most-frequent response to a question?the higher the number
of Turkers that vote for the majority answer, the higher is the agreement.
Observe that agreement is highest when asked whether a word pair has some
degree of contrast in meaning (Q2), and that there is a marked drop when asked if the
two words are opposites (Q3). This is true for each of the parts of speech, although the
drop is highest for verbs (94.7% to 75.2%).
For Questions 5 through 9, we see varying degrees of agreement?Q6 obtaining the
highest agreement and Q5 the lowest. There is marked difference across parts of speech
for certain questions. For example, verbs are the easiest to identify (highest agreement
for Q5, Q7, and Q8). For Q6, nouns have markedly lower agreement than all other parts
of speech?not surprising considering that the set of disjoint opposites is traditionally
associated with equipollent adjectives and stative verbs. Adverbs and adjectives have
markedly lower agreement scores for Q7 than nouns and verbs.
Table 6
Breakdown of answer agreement by target-pair part of speech and question: For every target
pair, a question is answered by about eight annotators. The majority response is chosen as the
answer. The ratio of the size of the majority and the number of annotators is indicative of the
amount of agreement. The table shows the average percentage of this ratio.
question adj. adv. nouns verbs average
Q2. Do X and Y have some contrast? 90.7 92.1 92.0 94.7 92.4
Q3. Are X and Y opposites? 79.0 80.9 76.4 75.2 77.9
Q5. Are X and Y at two ends of a dimension? 70.3 66.5 73.0 78.6 72.1
Q6. Does X imply not Y? 89.0 90.2 81.8 88.4 87.4
Q7. Are X and Y mutually exhaustive? 70.4 69.2 78.2 88.3 76.5
average (Q2, Q3, Q5, Q6, and Q7) 82.3 79.8 80.3 85.0 81.3
Q8. Does X represent a point on some scale? 77.9 71.5 ? ? 74.7
Q9. Does Y represent a point on some scale? 75.2 72.0 ? ? 73.6
Q10. Does X undo Y OR does Y undo X? ? ? ? 73.0 73.0
567
Computational Linguistics Volume 39, Number 3
5. Manifestation of Highly Contrasting Word Pairs in Text
As pointed out earlier, there is work on a small set of opposites showing that opposites
co-occur more often than chance (Charles and Miller 1989; Fellbaum 1995). Section 5.1
describes experiments on a larger scale to determine whether highly contrasting word
pairs (including opposites) occur together more often than randomly chosen word
pairs of similar frequency. The section also compares co-occurrence associations with
synonyms.
Research in distributional similarity has found that entries in distributional thesauri
tend to also contain terms that are opposite in meaning (Lin 1998; Lin et al 2003).
Section 5.2 describes experiments to determine whether highly contrasting word pairs
(including opposites) occur in similar contexts as often as randomly chosen pairs of
words with similar frequencies, and whether highly contrasting words occur in similar
contexts as often as synonyms.
5.1 Co-Occurrence
In order to compare the tendencies of highly contrasting word pairs, synonyms, and
random word pairs to co-occur in text, we created three sets of word pairs: the high-
contrast set, the synonyms set, and the control set of random word pairs. The high-contrast set
was created from a pool of direct and indirect opposites (nouns, verbs, and adjectives)
from WordNet. We discarded pairs that did not meet the following conditions: (1) both
members of the pair must be unigrams, (2) both members of the pair must occur in
the British National Corpus (BNC) (Burnard 2000), and (3) at least one member of the
pair must have a synonym in WordNet. A total of 1,358 word pairs remained, and these
form the high-contrast set.
Each of the pairs in the high-contrast set was used to create a synonym pair by
choosing a WordNet synonym of exactly one member of the pair.7 If a word has more
than one synonym, then the most frequent synonym is chosen.8 These 1,358 word pairs
form the synonyms set. Note that for each of the pairs in the high-contrast set, there is a
corresponding pair in the synonyms set, such that the two pairs have a common term.
For example, the pair agitation and calmness in the high-contrast set has a corresponding
pair agitation and ferment in the synonyms set. We will refer to the common terms
(agitation in this example) as the focus words. Because we also wanted to compare
occurrence statistics of the high-contrast set with the random pairs set, we created the
control set of random pairs by taking each of the focus words and pairing them with
another word in WordNet that has a frequency of occurrence in BNC closest to the term
contrasting with the focus word. This is to ensure that members of the pairs across the
high-contrast set and the control set have similar unigram frequencies.
We calculated the pointwise mutual information (PMI) (Church and Hanks 1990)
for each of the word pairs in the high-contrast set, the random pairs set, and the
synonyms set using unigram and co-occurrence frequencies in the BNC. If two words
occurred within a window of five adjacent words in a sentence, they were marked as
co-occurring (same window as Church and Hanks [1990] used in their seminal work on
word?word associations). Table 7 shows the average and standard deviation in each set.
7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym is
taken.
8 WordNet lists synonyms in order of decreasing frequency in the SemCor corpus.
568
Mohammad et al Computing Lexical Contrast
Table 7
Pointwise mutual information (PMI) of word pairs. High positive values imply a tendency to
co-occur in text more often than random chance.
average PMI standard deviation
high-contrast set 1.471 2.255
random pairs set 0.032 0.236
synonyms set 0.412 1.110
Observe that the high-contrast pairs have a much higher tendency to co-occur than the
random pairs control set, and also the synonyms set. The high-contrast set has a large
standard deviation, however. A two-sample t-test revealed that the high-contrast set
is significantly different from the random set (p < 0.05), and also that the high-contrast
set is significantly different from the synonyms set (p < 0.05).
On average, however, the PMI between a focus word and its contrasting term was
lower than the PMI between the focus word and 3,559 other words in the BNC. These
were often words related to the focus words, but neither contrasting nor synonymous.
Thus, even though a high tendency to co-occur is a feature of highly contrasting pairs,
it is not a sufficient condition for detecting them. We use PMI as part of our method
for determining the degree of lexical contrast (described in Section 6).
5.2 Distributional Similarity
Charles and Miller (1989) proposed that in most contexts, opposites may be inter-
changed. The meaning of the utterance will be inverted, of course, but the sentence
will remain grammatical and linguistically plausible. This came to be known as the
substitutability hypothesis. Their experiments did not support this claim, however. They
found that given a sentence with the target adjective removed, most people did not
confound the missing word with its opposite. Justeson and Katz (1991) later showed
that in sentences that contain both members of an adjectival opposite pair, the target
adjectives do indeed occur in similar syntactic structures at the phrasal level. Jones et al
(2007) show how the tendency to appear in certain textual constructions such as ?from
X to Y? and ?either X or Y? are indicative of prototypicalness of opposites. Thus, we can
formulate the distributional hypothesis of highly contrasting pairs: highly contrasting
pairs occur in similar contexts more often than non-contrasting word pairs.
We used the same sets of high-contrast pairs, synonyms, and random pairs de-
scribed in the previous section to gather empirical proof of the distributional hypothesis.
We calculated the distributional similarity between each pair in the three sets using
Lin?s (1998) measure. Table 8 shows the average and standard deviation in each set.
Observe that the high-contrast set has a much higher average distributional similarity
Table 8
Distributional similarity of word pairs. The measure proposed in Lin (1998) was used.
average distributional similarity standard deviation
opposites set 0.064 0.071
random pairs set 0.036 0.034
synonyms set 0.056 0.057
569
Computational Linguistics Volume 39, Number 3
than the random pairs control set, and interestingly it is also higher than the synonyms
set. Once again, the high-contrast set has a large standard deviation. A two-sample
t-test revealed that the high-contrast set is significantly different from both the random
set and the synonyms set with a confidence interval of 0.05. This demonstrates that
relative to other word pairs, high-contrast pairs tend to occur in similar contexts. We
also find that the synonyms set has a significantly higher distributional similarity than
the random pairs set (p< 0.05). This shows that near-synonymous word pairs also occur
in similar contexts (the distributional hypothesis of similarity). Further, a consequence
of the large standard deviations in the cases of both high-contrast pairs and synonyms
means that distributional similarity alone is not sufficient to determine whether two
words are contrasting or synonymous. An automatic method for recognizing contrast
will require additional cues. Our method uses PMI and other sources of information
described in the next section.
6. Computing Lexical Contrast
In this section, we recapitulate the automatic method for determining lexical con-
trast that we first proposed in Mohammad, Dorr, and Hirst (2008). Additional details
are provided regarding the lexical resources used (Section 6.1) and the method itself
(Section 6.2).
6.1 Lexical Resources
Our method makes use of a published thesaurus and co-occurrence information from
text. Optionally, it can use opposites listed in WordNet if available. We briefly describe
these resources here.
6.1.1 Published Thesauri. Published thesauri, such as Roget?s and Macquarie, divide the
vocabulary of a language into about a thousand categories. Words within a category are
semantically related to each other, and they tend to pertain to a coarse concept. Each
category is represented by a category number (unique ID) and a head word?a word that
best represents the meanings of the words in the category. One may also find opposites
in the same category, but this is rare. Words with more than one meaning may be found
in more than one category; these represent its coarse senses.
Within a category, the words are grouped into finer units called paragraphs. Words
in the same paragraph are closer in meaning than those in differing paragraphs. Each
paragraph has a paragraph head?a word that best represents the meaning of the words
in the paragraph. Words in a thesaurus paragraph belong to the same part of speech. A
thesaurus category may have multiple paragraphs belonging to the same part of speech.
For example, a category may have three noun paragraphs, four verb paragraphs, and
one adjective paragraph. We will take advantage of the structure of the thesaurus in
our approach.
6.1.2 WordNet. As mentioned earlier, WordNet encodes certain opposites. We found
in our experiments (Section 7, subsequently) that more than 90% of contrasting pairs
included in Graduate Record Examination (GRE) ?most contrasting word? questions
are not encoded in WordNet, however. Also, neither WordNet nor any other manually
created repository of opposites provides the degree of contrast between word pairs.
Nevertheless, we investigate the usefulness of WordNet as a source of seed opposites
for our approach.
570
Mohammad et al Computing Lexical Contrast
6.2 Proposed Measure of Lexical Contrast
Our method for determining lexical contrast has two parts: (1) determining whether
the target word pair is contrasting or not, and (2) determining the degree of contrast
between the words.
6.2.1 Detecting Whether a Target Word Pair is Contrasting. We use the contrast hypothesis
to determine whether two words are contrasting. The hypothesis is repeated here:
Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair of
opposites, C and D, such that A and C are strongly related and B and D are strongly
related.
Even if a few exceptions to this hypothesis are found (we are not aware of any), the
hypothesis would remain useful for practical applications. We first determine pairs of
thesaurus categories that have at least one word in each category that are opposites of
each other. We will refer to these categories as contrasting categories and the opposite
connecting the two categories as the seed opposite. Because each thesaurus category
is a collection of closely related terms, all of the word pairs across two contrasting
categories satisfy the contrast hypothesis, and they are considered to be contrasting
word pairs. Note also that words within a thesaurus category may belong to different
parts of speech, and they may be related to the seed opposite word through any of
the many possible semantic relations. Thus a small number of seed opposites can
help identify a large number of contrasting word pairs. We determine whether two
categories are contrasting using the three methods described here, which may be used
alone or in combination with each other:
Method 1: Using word pairs generated from affix patterns.
Opposites such as hot?cold and dark?light occur frequently in text, but in terms of type-
pairs they are outnumbered by those created using affixes, such as un- (clear?unclear)
and dis- (honest?dishonest). Further, this phenomenon is observed in most languages
(Lyons 1977).
Table 9 lists 15 affix patterns that tend to generate opposites in English. They
were compiled by the first author by examining a small list of affixes for the English
language.9 These patterns were applied to all words in the thesaurus that are at least
three characters long. If the resulting term was also a valid word in the thesaurus, then
the word pair was added to the affix-generated seed set. These fifteen rules generated 2,682
word pairs when applied to the words in the Macquarie Thesaurus. Category pairs that
had these opposites were marked as contrasting. Of course, not all of the word pairs
generated through affixes are truly opposites, for example sect?insect and part?impart.
For now, such pairs are sources of error in the system. Manual analysis of these 2,682
word pairs can help determine whether this error is large or small. (We have released
the full set of word pairs.) Evaluation results (Section 7) indicate that these seed pairs
improve the overall accuracy of the system, however.
Figure 3 presents such an example pair. Observe that categories 360 and 361 have
the words cover and uncover, respectively. Affix pattern 8 from Table 9 produces seed
9 http://www.englishclub.com/vocabulary/prefixes.htm.
571
Computational Linguistics Volume 39, Number 3
Table 9
Fifteen affix patterns used to generate opposites. Here ?X? stands for any sequence of letters
common to both words w1 and w2.
affix pattern
pattern # word 1 word 2 # word pairs example pair
1 X antiX 41 clockwise?anticlockwise
2 X disX 379 interest?disinterest
3 X imX 193 possible?impossible
4 X inX 690 consistent?inconsistent
5 X malX 25 adroit?maladroit
6 X misX 142 fortune?misfortune
7 X nonX 72 aligned?nonaligned
8 X unX 833 biased?unbiased
9 lX illX 25 legal?illegal
10 rX irrX 48 regular?irregular
11 imX exX 35 implicit?explicit
12 inX exX 74 introvert?extrovert
13 upX downX 22 uphill?downhill
14 overX underX 52 overdone?underdone
15 Xless Xful 51 harmless?harmful
Total: 2,682
pair cover?uncover, and so the system concludes that the two categories have contrasting
meaning. The contrast in meaning is especially strong for the paragraphs cover and
expose because words within these paragraphs are very close in meaning to cover and
uncover, respectively. We will refer to such thesaurus paragraph pairs that have one
word each of a seed pair as prime contrasting paragraphs. We expect the words across
prime contrasting paragraphs to have a high degree of antonymy (for example, mask
and bare), whereas words across other contrasting category paragraphs may have a
smaller degree of antonymy as the meaning of these words may diverge significantly
from the meanings of the words in the prime contrasting paragraphs (for example,
white lie and disclosure).
Method 2: Using opposites from WordNet.
We compiled a list of 20,611 pairs that WordNet records as direct and indirect opposites.
(Recall discussion in Section 4 about direct and indirect opposites.) A large number of
these pairs include multiword expressions. Only 10,807 of the 20,611 pairs have both
words in the Macquarie Thesaurus?the vocabulary used for our experiments. We will
refer to them as the WordNet seed set. Category pairs that had these opposites were
marked as contrasting.
Method 3: Using word pairs in adjacent thesaurus categories.
Most published thesauri, such as Roget?s, are organized such that categories correspond-
ing to opposing concepts are placed adjacent to each other. For example, in the Macquarie
Thesaurus: category 369 is about honesty and category 370 is about dishonesty; as shown
in Figure 3, category 360 is about hiding and category 361 is about revealing. There are
a number of exceptions to this rule, and often a category may be contrasting in meaning
to several other categories. Because this was an easy enough heuristic to implement,
however, we investigated the usefulness of considering adjacent thesaurus categories
572
Mohammad et al Computing Lexical Contrast
Figure 3
Example contrasting category pair. The system identifies the pair to be contrasting through the
affix-based seed pair cover?uncover. The paragraphs of cover and expose are referred to as prime
contrasting paragraphs. Paragraph heads are shown in bold italic.
as contrasting. We will refer to this as the adjacency heuristic. Note that this method of
determining contrasting categories does not explicitly identify a seed opposite, but one
can assume the head words of these category pairs as the seed opposites.
To determine how accurate the adjacency heuristic is, the first author manually
inspected adjacent thesaurus categories in the Macquarie Thesaurus to determine which
of them were indeed contrasting. Because a category, on average, has about a hundred
words, the task was made less arduous by representing each category by just the first
ten words listed in it. This way it took only about five hours to manually determine that
209 pairs of the 811 adjacent Macquarie category pairs were contrasting. Twice, it was
found that category number X was contrasting not just to category number X+1 but also
to category number X+2: category 40 (ARISTOCRACY) has a meaning that contrasts that
of category 41 (MIDDLE CLASS) as well as category 42 (WORKING CLASS); category 542
(PAST) contrasts with category 543 (PRESENT) as well as category 544 (FUTURE). Both
these X ? (X+2) pairs are also added to the list of manually annotated contrasting
categories.
6.2.2 Computing the Degree of Contrast Between Two Words. Charles and Miller (1989)
and Fellbaum (1995) argued that opposites tend to co-occur more often than random
chance. Murphy and Andrew (1993) claimed that the greater-than-chance co-occurrence
of opposites is because together they convey contrast well, which is rhetorically useful.
We showed earlier in Section 5.1 that highly contrasting pairs (including opposites)
co-occur more often than randomly chosen pairs. All of these support the degree of
contrast hypothesis stated earlier in the introduction:
Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then their
degree of contrast is proportional to their tendency to co-occur in a large corpus.
573
Computational Linguistics Volume 39, Number 3
We used PMI to capture the tendency of word?word co-occurrence. We collected
these co-occurrence statistics from the Google n-gram corpus (Brants and Franz 2006),
which was created from a text collection of over one trillion words. Words that occurred
within a window of five words were considered to be co-occurring.
We expected that some features may be more accurate than others. If multiple
features give evidence towards opposing information, then it is useful for the system
to know which feature is more reliable. Therefore, we held out some data from the
evaluation data described in Section 7.1 as the development set. Experiments on the
development set showed that contrasting words may be placed in three bins corre-
sponding to the amount of reliability of the source feature: high, medium, or acceptable.
 High reliability (Class I): target words that belong to adjacent thesaurus
categories. For example, all the word pairs across categories 360 and 361,
shown in Figure 3. Examples of Class I contrasting word pairs from the
development set include graceful?ungainly, fortunate?hapless, obese?slim,
and effeminate?virile. (Note, there need not be any affix or WordNet seed
pairs across adjacent thesaurus categories for these word pairs to be
marked Class I.) As expected, if we use only those adjacent categories that
were manually identified to be contrasting (as described in Section 6.2.1,
Method 3), then the system obtains even better results than those obtained
using all adjacent thesaurus categories. (Experiments and results shown
in Section 7.1).
 Medium reliability (Class II): target words that are not Class I contrasting
pairs, but belong to one paragraph each of a prime contrasting paragraph.
For example, all the word pairs across the paragraphs of sympathetic and
indifferent. See Figure 4. Examples of Class II contrasting word pairs
from the development set include altruism?avarice, miserly?munificent,
accept?repudiate, and improper?prim.
 Acceptable reliability (Class III): target words that are not Class I or
Class II contrasting pairs, but occur across contrasting category pairs. For
example, all word pairs across categories 423 and 230 except those that
have one word each from the paragraphs of sympathetic and indifferent.
See Figure 4. Examples of Class III contrasting word pairs from the
development set include pandemonium?calm, probity?error, artifice?sincerity,
and hapless?wealthy.
Even with access to very large textual data sets, there is always a long tail of words
that occur so few times that there is not enough co-occurrence information for them.
Thus we assume that all word pairs in Class I have a higher degree of contrast than
all word pairs in Class II, and that all word pairs in Class II have a higher degree of
contrast than the pairs in Class III. If two word pairs belong to the same class, then we
calculate their tendency to co-occur with each other in text to determine which pair is
more contrasting. All experiments in the evaluation section ahead follow this method.
6.2.3 Lexicon of Contrasting Word Pairs. Using the method described in the previous
sections, we generated a lexicon of word pairs pertaining to Class I and Class II. The
lexicon has 6.3 million contrasting word pairs, about 3.5 million of which belong to
Class I and about 2.8 million to Class II. Class III pairs are even more numerous and,
given a word pair, our algorithm checked whether it is a Class III pair, but we did not
574
Mohammad et al Computing Lexical Contrast
1. nouns:
Category number: 423
Category head: KINDNESS
1. nouns:
Category number: 230
Category head: APATHY
kindness
considerateness
niceness
goodness
...
2. adjectives:
sympathetic
consolatory
caring
involved...
3. adverbs:
benevolent
beneficiently
graciously
kindheartedly...
...
apathy
acedia
depression
moppishness...
2. nouns:
nonchalance
insouciance
carelessness
casualness
3. adjectives:
...
indifferent
detached
irresponsive
uncaring...
...
Figure 4
Example contrasting category pair that has Class II and Class III contrasting pairs. The system
identifies the pair to be contrasting through the affix-based seed pair caring (second word in
paragraph 2 or category 423) and uncaring (fourth word in paragraph 3 or category 230). The
paragraphs of sympathetic and indifferent are therefore the prime contrasting paragraphs and so
all word pairs that have one word each from these two paragraphs are Class II contrasting pairs.
All other pairs formed by taking one word each from the two contrasting categories are the
Class III contrasting pairs. Paragraph heads are shown in bold italic.
create a complete set of all Class III contrasting pairs. Class I and II lexicons are available
for download and summarized in Table 18.
7. Evaluation
We evaluate our algorithm on two different tasks and four data sets. Section 7.1 de-
scribes experiments on solving existing GRE ?choose the most contrasting word? ques-
tions (a recapitulation of the evaluation reported in Mohammad, Dorr, and Hirst [2008]).
Section 7.2 describes experiments on solving newly created ?choose the most contrast-
ing word? questions specifically designed to determine performance on different kinds
of opposites. And lastly, Section 7.3 describes experiments on two different data sets
where the goal is to identify whether a given word pair is synonymous or antonymous.
7.1 Solving GRE?s ?Choose the Most Contrasting Word? Questions
The GRE is a test taken by thousands of North American graduate school applicants.
The test is administered by Educational Testing Service (ETS). The Verbal Reasoning
section of GRE is designed to test verbal skills. Until August 2011, one of its sections had
a set of questions pertaining to word-pair contrast. Each question had a target word and
four or five alternatives, or option words. The objective was to identify the alternative
which was most contrasting with respect to the target. For example, consider:
adulterate: a. renounce b. forbid c. purify d. criticize e. correct
575
Computational Linguistics Volume 39, Number 3
Here the target word is adulterate. One of the alternatives provided is correct, which
as a verb has a meaning that contrasts with that of adulterate; purify, however, has
a greater degree of contrast with adulterate than correct does and must be chosen
in order for the instance to be marked as correctly answered. ETS referred to these
questions as ?antonym questions,? where the examinees had to ?choose the word
most nearly opposite? to the target. Most of the target?answer pairs are not gradable
adjectives, however, and because most of them are not opposites either, we will refer
to these questions as ?choose the most contrasting word? questions or contrast questions
for short.
Evaluation on this data set tests whether the automatic method is able to identify
not just opposites but also those pairs that are not opposites but that have some degree
of semantic contrast. Notably, for these questions, the method must be able to identify
that one word pair has a higher degree of contrast than all others, even though that
word pair may not necessarily be an opposite.
7.1.1 Data. A Web search for large sets of contrast questions yielded two independent
sets of questions designed to prepare students for the GRE. The first set consists of
162 questions. We used this set while we were developing our lexical contrast algo-
rithm described in Section 4. Therefore, we will refer to it as the development set. The
development set helped determine which features of lexical contrast were more reliable
than others. The second set has 1,208 contrast questions. We discarded questions that
had a multiword target or alternative. After removing duplicates we were left with
790 questions, which we used as the unseen test set. This data set was used (and seen)
only after our algorithm for determining lexical contrast was frozen.
Interestingly, the data contains many instances that have the same target word used
in different senses. For example:
1. obdurate: a. meager b. unsusceptible c. right d. tender e. intelligent
2. obdurate: a. yielding b. motivated c. moribund d. azure e. hard
3. obdurate: a. transitory b. commensurate c. complaisant d. similar e. laconic
In (1), obdurate is used in the sense of HARDENED IN FEELINGS and is most contrasting
with tender. In (2), it is used in the sense of RESISTANT TO PERSUASION and is most
contrasting with yielding. In (3), it is used in the sense of PERSISTENT and is most
contrasting with transitory.
The data sets also contain questions in which one or more of the alternatives is a
near-synonym of the target word. For example:
astute: a. shrewd b. foolish c. callow d. winning e. debating
Observe that shrewd is a near-synonym of astute. The word most contrasting with astute
is foolish. A manual check of a randomly selected set of 100 test-set questions revealed
that, on average, one in four had a near-synonym as one of the alternatives.
7.1.2 Results. Table 10 presents results obtained on the development and test data
using two baselines, a re-implementation of the method described in Lin et al (2003),
and variations of our method. Some of the results are for systems that refrain from
576
Mohammad et al Computing Lexical Contrast
Table 10
Results obtained on contrast questions. The best performing system and configuration are
shown in bold.
development data test data
P R F P R F
Baselines:
a. random baseline 0.20 0.20 0.20 0.20 0.20 0.20
b. WordNet antonyms 0.23 0.23 0.23 0.23 0.23 0.23
Related work:
a. Lin et al (2003) 0.23 0.23 0.23 0.24 0.24 0.24
Our method:
a. affix-generated pairs as seeds 0.72 0.53 0.61 0.71 0.51 0.59
b. WordNet antonyms as seeds 0.79 0.52 0.63 0.72 0.49 0.58
c. both seed sets (a + b) 0.77 0.65 0.70 0.72 0.58 0.64
d. adjacency heuristic only 0.81 0.43 0.56 0.83 0.44 0.57
e. manual annotation of adjacent categories 0.88 0.41 0.56 0.87 0.41 0.55
f. affix seed set and adjacency heuristic (a + d) 0.75 0.60 0.67 0.76 0.60 0.67
g. both seed sets and adjacency heuristic (a + b + d) 0.76 0.66 0.70 0.76 0.63 0.69
h. affix seed set and annotation of adjacent 0.79 0.63 0.70 0.78 0.60 0.68
categories (a + e)
i. both seed sets and annotation of adjacent 0.79 0.66 0.72 0.77 0.63 0.69
categories (a + b + e)
attempting questions for which they do not have sufficient information. We therefore
report precision (P), recall (R), and balanced F-score (F).
P =
# of questions answered correctly
# of questions attempted
(1)
R =
# of questions answered correctly
# of questions
(2)
F =
2 ? P ? R
P+ R
(3)
Baselines. If a system randomly guesses one of the five alternatives with equal probabil-
ity (random baseline), then it obtains an accuracy of 0.2. A system that looks up the list of
WordNet antonyms (10,807 pairs) to solve the contrast questions is our second baseline.
That obtained the correct answer in only 5 instances of the development set (3.09% of
the 162 instances) and 25 instances of the test set (3.17% of the 790 instances), however.
Even if the system guesses at random for all other instances, it attains only a modest
improvement over the random baseline (see row b, under ?Baselines,? in Table 10).
Re-implementation of related work. In order to estimate how well the method of Lin
et al (2003) performs on this task, we re-implemented their method. For each closest-
antonym question, we determined frequency counts in the Google n-gram corpus for
the phrases ?from ?target word? to ?known correct answer?,? ?from ?known correct
answer? to ?target word?,? ?either ?target word? or ?known correct answer?,? and
?either ?known correct answer? or ?target word?.? We then summed up the four counts
for each contrast question. This resulted in non-zero counts for only 5 of the 162 in-
stances in the development set (3.09%), and 35 of the 790 instances in the test set (4.43%).
Thus, these patterns fail to cover a vast majority of closest-antonyms, and even if the
577
Computational Linguistics Volume 39, Number 3
system guesses at random for all other instances, it attains only a modest improvement
over the baseline (see row a, under ?Related work,? in Table 10).
Our method. Table 10 presents results obtained on the development and test data using
different combinations of the seed sets and the adjacency heuristic. The best performing
system is marked in bold. It has significantly higher precision and recall than that of the
method proposed by Lin et al (2003), with 95% confidence according to the Fisher Exact
Test (Agresti 1990).
We performed experiments on the development set first, using our method with
configurations described in rows a, b, and d. These results showed that marking
adjacent categories as contrasting has the highest precision (0.81), followed by using
WordNet seeds (0.79), followed by the use of affix rules to generate seeds (0.72). This
allowed us to determine the relative reliability of the three features as described in
Section 6.2.2. We then froze all system development and ran the remaining experiments,
including those on the test data.
Observe that all of the results shown in Table 10 are well above the random baseline
of 0.20. Using only the small set of 15 affix rules, the system performs almost as well as
when it uses 10,807 WordNet opposites. Using both the affix-generated and the Word-
Net seed sets, the system obtains markedly improved precision and coverage. Using
only the adjacency heuristic gave precision values (upwards of 0.8) with substantial
coverage (attempting more than half of the questions). Using the manually identified
contrasting adjacent thesaurus categories gave precision values just short of 0.9. The
best results were obtained using both seed sets and the contrasting adjacent thesaurus
categories (F-scores of 0.72 and 0.69 on the development and test set, respectively).
In order to determine whether our method works well with thesauri other than the
Macquarie Thesaurus, we determined performance of configurations a, b, c, d, f, and h
using the 1911 U.S. edition of the Roget?s Thesaurus, which is available freely in the public
domain.10 The results were similar to those obtained using the Macquarie Thesaurus. For
example, configuration g obtained a precision of 0.81, recall of 0.58, and F-score of 0.68
on the test set. It may be possible to obtain even better results by combining multiple
lexical resources; that is left for future work. The remainder of this article reports results
obtained with the Macquarie Thesaurus; the 1911 vocabulary is less suited for practical
use in the 21st century.
7.1.3 Discussion. These results show that our method performs well on questions de-
signed to be challenging for humans. In tasks that require higher precision, using only
the contrasting adjacent categories is best, whereas in tasks that require both precision
and coverage, the seed sets may be included. Even when both seed sets were included,
only four instances in the development set and twenty in the test set had target?answer
pairs that matched a seed opposite pair. For all remaining instances, the approach had
to generalize to determine the most contrasting word. This also shows that even the
seemingly large number of direct and indirect antonyms from WordNet (more than
10,000) are by themselves insufficient.
The comparable performance obtained using the affix rules alone suggests that even
in languages that do not have a WordNet-like resource, substantial accuracies may be
obtained. Of course, improved results when using WordNet antonyms as well suggests
that the information they provide is complementary.
10 http://www.gutenberg.org/ebooks/10681.
578
Mohammad et al Computing Lexical Contrast
Error analysis revealed that at times the system failed to identify that a category
pertaining to the target word contrasted with a category pertaining to the answer.
Additional methods to identify seed opposite pairs will help in such cases. Certain
other errors occurred because one or more alternatives other than the official answer
were also contrasting with the target. For example, one of the questions has chasten as
the target word. One of the alternatives is accept, which has some degree of contrast in
meaning to the target. Another alternative, reward, has an even higher degree of contrast
with the target, however. In this instance, the system erred by choosing accept as the
answer.
7.2 Determining Performance of Automatic Method on Different Kinds of Opposites
The previous section showed the overall performance of our method. The performance
of a method may vary significantly on different subsets of data, however. In order to
determine performance on different kinds of opposites, we generated new contrast
questions from the crowdsourced term pairs described in Section 4. Note that for solving
contrast questions with this data set, again the method must be able to identify that
one word pair has a higher degree of contrast than the other pairs; unlike the previous
section, however, here the correct answer is often an opposite of the target.
7.2.1 Generating Contrast Questions. For each word pair from the list of WordNet oppo-
sites, we chose one word randomly to be the target word, and the other as one of its
candidate options. Four other candidate options were chosen from Lin?s distributional
thesaurus (Lin 1998).11 An entry in the distributional thesaurus has a focus word and
a number of other words that are distributionally similar to the focus word. The words
are listed in decreasing order of similarity. Note that these entries include not just
near-synonymous words but also at times contrasting words because contrasting
words tend to be distributionally similar (Lin et al 2003).
For each of the target words in our contrast questions, we chose the four distribu-
tionally closest words from Lin?s thesaurus to be the distractors. If a distractor had the
same first three letters as the target word or the correct answer, then it was replaced
with another word from the distributional thesaurus. This ad hoc filtering criterion is
effective at discarding distractors that are morphological variants of the target or the
answer. For example, if the target word is adulterate, then words such as adulterated and
adulterates will not be included as distractors even if they are listed as closely similar
terms in the distributional thesaurus.
We place the four distractors and the correct answer in random order. Some of the
WordNet opposites were not listed in Lin?s thesaurus, and the corresponding question
was not generated. In all, 1,269 questions were generated. We created subsets of these
questions corresponding to the different kinds of opposites and also corresponding to
different parts of speech. Because a word pair may be classified as more than one kind
of opposite, the corresponding question may be part of more than one subset.
7.2.2 Experiments and Results. We applied our method of lexical contrast to solve the
complete set of 1,269 questions and also the various subsets. Because this test set is
11 http://webdocs.cs.ualberta.ca/?lindek/downloads.htm.
579
Computational Linguistics Volume 39, Number 3
Table 11
Percentage of contrast questions correctly answered by the automatic method, where different
question sets correspond to target?answer pairs of different kinds. The automatic method did not
use WordNet seeds for this task. The results shown for ?ALL? are micro-averages, that is, they are
the results for the master set of 1,269 contrast questions.
# instances P R F
Antipodals 1,044 0.95 0.84 0.89
Complementaries 1,042 0.95 0.83 0.89
Disjoint 228 0.81 0.59 0.69
Gradable 488 0.95 0.85 0.90
Reversives 203 0.93 0.74 0.82
ALL 1,269 0.93 0.79 0.85
created from WordNet opposites, we applied the algorithm without the use of WordNet
seeds (no WordNet information was used by the method).
Table 11 shows the precision (P), recall (R), and F-score (F) obtained by the method
on the data sets corresponding to different kinds of opposites. The column ?# instances?
shows the number of questions in each of the data sets. The performance of our method
on the complete data set is shown in the last row ALL. Observe that the F-score of
0.85 is markedly higher than the score obtained on the GRE-preparatory questions.
This is expected because the GRE questions involved vocabulary from a higher reading
level, and included carefully chosen distractors to confuse the examinee. The automatic
method obtains highest F-score on the data sets of gradable adjectives (0.90), antipodals
(0.89), and complementaries (0.89). The precisions and recalls for these opposites are
significantly higher than those of disjoint opposites. The recall for reversives is also sig-
nificantly lower than that for the gradable adjectives, antipodals, and complementaries,
but precision on reversives is quite good (0.93).
Table 12 shows the precision, recall, and F-score obtained by the method on the the
data sets corresponding to different parts of speech. Observe that performances on all
parts of speech are fairly high. The method deals with adverb pairs best (F-score of 0.89),
and the lowest performance is for verbs (F-score of 0.80). The differences in precision
values between various parts of speech are not significant. The recall obtained on the
adverbs is significantly higher than that obtained on adjectives, however, and the recall
on adjectives is significantly higher than that obtained on verbs. The difference between
the recalls on adverbs and nouns is not significant. We used the Fisher Exact Test and a
confidence interval of 95% for all significance testing reported in this section.
Table 12
Percentage of contrast questions correctly answered by the automatic method, where different
question sets correspond to different parts-of-speech.
# instances P R F
Adjectives 551 0.92 0.79 0.85
Adverbs 165 0.95 0.84 0.89
Nouns 330 0.93 0.81 0.87
Verbs 226 0.93 0.71 0.80
ALL 1,269 0.93 0.79 0.85
580
Mohammad et al Computing Lexical Contrast
7.3 Distinguishing Synonyms from Opposites
Our third evaluation follows that of Lin et al (2003) and Turney (2008). We developed
a system for automatically distinguishing synonyms from opposites, and applied it to
two data sets. The approach and experiments are described herein.
7.3.1 Data. Lin et al (2003) compiled 80 pairs of synonyms and 80 pairs of opposites
from the Webster?s Collegiate Thesaurus (Kay 1988) such that each word in a pair is also
in their list of the 50 distributionally most similar words of the other. (Distributional
similarity was calculated using the algorithm proposed by Lin et al [1998].) Turney
(2008) compiled 136 pairs of words (89 opposites and 47 synonyms) from various Web
sites for learners of English as a second language; the objective for the learners is to
identify whether the words in a pair are opposites or synonyms of each other. The
goals of this evaluation are to determine whether our automatic method can distinguish
opposites from near-synonyms, and to compare our method with the closest related
work on an evaluation task for which published results are already available.
7.3.2 Method. The core of our method is this:
1. Word pairs that occur in the same thesaurus category are close in meaning
and so are marked as synonyms.
2. Word pairs that occur in contrasting thesaurus categories or paragraphs
(as described in Section 6.2.1 above) are marked as opposites.
Even though opposites often occur in different thesaurus categories, they can sometimes
also be found in the same category, however. For example, the word ascent is listed in
the Macquarie Thesaurus categories of 49 (CLIMBING) and 694 (SLOPE), whereas the word
descent is listed in the categories 40 (ARISTOCRACY), 50 (DROPPING), 538 (PARENTAGE),
and 694 (SLOPE). Observe that ascent and descent are both listed in the same category
694 (SLOPE), which makes sense here because both words are pertinent to the concept of
slope. On the other hand, two separate clues independently inform our system that the
words are opposites of each other: (1) Category 49 has the word upwardness in the same
paragraph as ascent, and category 50 has the word downwardness in the same paragraph
as descent. The 13th affix pattern from Table 9 (upX and downX) indicates that the two
thesaurus paragraphs have contrasting meaning. Thus, ascent and descent occur in prime
contrasting thesaurus paragraphs. (2) One of the ascent categories (49) is adjacent to one
of the descent categories (50), and further this adjacent category pair has been manually
marked as contrasting.
Thus the words in a pair may be deemed both synonyms and opposites simultane-
ously by our methods of determining synonyms and opposites, respectively. Some of
the features we use to determine opposites were found to be more precise (e.g., words
listed in adjacent categories) than others (e.g., categories identified as contrasting based
on affix and WordNet seeds), however. Thus we apply the following rules as a decision
list: If one rule fires, then the subsequent rules are ignored.
1. Rule 1 (high confidence for opposites): If the words in a pair occur in
adjacent thesaurus categories, then they are marked as opposites.
2. Rule 2 (high confidence for synonyms): If both the words in a pair occur
in the same thesaurus category, then they are marked as synonyms.
581
Computational Linguistics Volume 39, Number 3
3. Rule 3 (medium confidence for opposites): If the words in a pair occur in
prime contrasting thesaurus paragraphs, as determined by an affix-based
or WordNet seed set, then they are marked as opposites.
If a word pair is not tagged as synonym or opposite: (a) the system can refrain
from attempting an answer (this will attain high precision), or (b) the system can
randomly guess the lexical relation (this will obtain 50% accuracy for the pairs), or (c) it
could mark all remaining word pairs with the predominant lexical relation in the data
(this will obtain an accuracy proportional to the skew in distribution of opposites and
synonyms). For example, if after step 3, the system finds that 70% of the marked word
pairs were tagged opposites, and 30% as synonyms, then it could mark every hitherto
untagged word pair (word pair for which it has insufficient information) as opposites.
We implemented all three variants. Note that option (b) is indeed expected to perform
poorly compared to option (c), but we include it as part of our evaluation to measure
usefulness of option (c).
7.3.3 Results and Discussion. Table 13 shows the precision (P), recall (R), and balanced
F-score (F) of various systems and baselines in identifying synonyms and opposites
from the data set described in Lin et al (2003). We will refer to this data set as LZQZ
(the first letters of the authors? last names).
If a system guesses at random (random baseline) it will obtain an accuracy of 50%.
Choosing opposites (or synonyms) as the predominant class also obtains an accuracy of
50% because the data set has an equal number of opposites and synonyms. Published
results on LZQZ (Lin et al 2003) are shown here again for convenience. The results
obtained with our system and the three variations on handling word pairs for which
it does not have enough information are shown in the last three rows. The precision of
our method in configuration (a) is significantly higher than that of Lin et al (2003), with
95% confidence according to the Fisher Exact Test (Agresti 1990). Because precision and
recall are the same for configuration (b) and (c), as well as for the methods described in
Lin et al (2003) and Turney (2011), we can also refer to these results simply as accuracy.
Table 13
Results obtained on the synonym-or-opposite questions in LZQZ. The best performing systems
are marked in bold. The difference in precision and recall of method by Lin et al (2003) and our
method in configurations (b) and (c) is not statistically significant.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.50 0.50 0.50
Related work:
a. Lin et al (2003) 0.90 0.90 0.90
b. Turney (2011) 0.82 0.82 0.82
Our method: if no information
a. refrain from guessing 0.98 0.78 0.87
b. make random guess 0.88 0.88 0.88
c. mark the predominant class? 0.87 0.87 0.87
?This data set has an equal number of opposites and synonyms. Results reported are when
choosing opposites as the predominant class.
?The system concluded that opposites were slightly more frequent than synonyms.
582
Mohammad et al Computing Lexical Contrast
Table 14
Results obtained on the synonym-or-opposite questions in TURN. The best performing systems
are marked in bold.
P R F
Baselines
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.65 0.65 0.65
Related work
a. Turney (2008) 0.75 0.75 0.75
b. Lin et al (2003) 0.35 0.35 0.35
Our method: if no information
a. refrain from guessing 0.97 0.69 0.81
b. make random guess 0.84 0.84 0.84
c. mark the predominant class? 0.90 0.90 0.90
?About 65.4% of the pairs in this data set are opposites. So this row reports baseline results when
choosing opposites as the predominant class.
?The system concluded that opposites were much more frequent than synonyms.
We found that the differences in accuracies between the method of Lin et al (2003) and
our method in configurations (b) and (c) are not statistically significant. The method by
Lin et al (2003) and our method in configuration (b) have significantly higher accuracy
than the method described in Turney (2011), however. The lexical contrast features used
in configurations (a), (b), and (c) correspond to row i in Table 10. The next subsection
presents an analysis of the usefulness of the different features listed in Table 10.
Observe that when our method refrains from guessing in case of insufficient infor-
mation, it obtains excellent precision (0.98), while still providing very good coverage
(0.78). As expected, the results obtained with (b) and (c) do not differ much from each
other because the data set has an equal number of synonyms and opposites. (Note
that the system was not privy to this information.) After step 3 of the algorithm,
however, the system had marked 65 pairs as opposites and 63 pairs as synonyms,
and so it concluded that opposites are slightly more dominant in this data set and
therefore the guess-predominant-class variant marked all previously unmarked pairs as
opposites.
It should be noted that the LZQZ data set was chosen from a list of high-frequency
terms. This was necessary to increase the probability of finding sentences in a corpus
where the target pair occurred in one of the chosen patterns proposed by Lin et al
(2003). As shown in Table 10, the Lin et al (2003) patterns have a very low coverage
otherwise. Further, the test data compiled by Lin et al only had opposites whereas the
contrast questions had many contrasting word pairs that were not opposites.
Table 14 shows results on the data set described in Turney (2008). We will refer to
this data set as TURN. The supervised baseline of always guessing the most frequent
class (in this case, opposites), will obtain an accuracy of 65.4% (P = R = F = 0.654).
Turney (2008) obtains an accuracy of 75% using a supervised method and 10-fold
cross-validation. A re-implementation of the method proposed by Lin et al (2003) as
described in Section 7.1.3 did not recognize any of the word pairs in TURN as opposites;
that is, none of the word pairs in TURN occurred in the Google n-gram corpus in
patterns used by Lin et al (2003). Thus it marked all words in TURN as synonyms.
The results obtained with our method are shown in the last three rows. The precision
and recall of our method in configurations (b) and (c) are significantly higher than those
583
Computational Linguistics Volume 39, Number 3
obtained by the methods by Turney (2008) and Lin et al (2003), with 95% confidence
according to the Fisher Exact Test (Agresti 1990).
Observe that once again our method, especially the variant that refrains from
guessing in case of insufficient information, obtains excellent precision (0.97), while
still providing good coverage (0.69). Also observe that results obtained by guessing the
predominant class (method (c)) are markedly better than those obtained by randomly
guessing in case of insufficient information (method (b)). This is because, as mentioned
earlier, the distribution of opposites and synonyms is somewhat skewed in this data
set (65.4% of the pairs are opposites). Of course, again the system was not privy to this
information, but method (a) marked 58 pairs as opposites and 39 pairs as synonyms.
Therefore, the system concluded that opposites are more dominant and method (c)
marked all previously unmarked pairs as opposites, obtaining an accuracy of 90%.
Recall that in Section 7.3.2 we described how opposite pairs may occasionally be
listed in the same thesaurus category because the category may be pertinent to both
words. For 12 of the word pairs in the Lin et al data and 3 of the word pairs in the
Turney data, both words occurred together in the same thesaurus category, and yet
the system marked them as opposites because they occurred in adjacent thesaurus
categories (Class I). For 11 of the 12 pairs from LZQZ and for all 3 of the TURN pairs,
this resulted in the correct answer. These pairs are shown in Table 15. By contrast, only
one of the term pairs in this table occurred in one of Lin?s patterns of oppositeness, and
was thus the only one correctly identified by their method as a pair of opposites.
It should also be noted that a word may have multiple meanings such that it may be
synonymous to a word in one sense and opposite to it in another sense. Such pairs are
also expected to be marked as opposites by our system. Two such pairs in the Turney
(2008) data are: fantastic?awful and terrific?terrible. The word awful can mean INSPIRING
AWE (and so close to the meaning of fantastic in some contexts), and also EXTREMELY
DISAGREEABLE (and so opposite to fantastic). The word terrific can mean FRIGHTFUL
(and so close to the meaning of terrible), and also UNUSUALLY FINE (and so opposite
to terrible). Such pairs are probably not the best synonym-or-opposite questions. Faced
with these questions, however, humans probably home in on the dominant senses of
Table 15
Pairs from LZQZ and TURN that have at least one category in common but are still marked as
opposites by our method.
LZQZ TURN
word 1 word 2 official solution word 1 word 2 official solution
amateur professional opposite fantastic awful opposite
ascent descent opposite dry wet opposite
back front opposite terrific terrible opposite
bottom top opposite
broadside salvo synonym
entrance exit opposite
heaven hell opposite
inside outside opposite
junior senior opposite
lie truth opposite
majority minority opposite
nadir zenith opposite
strength weakness opposite
584
Mohammad et al Computing Lexical Contrast
Table 16
Results for individual components as well as certain combinations of components on the
synonym-or-opposite questions in LZQZ. The best performing configuration is shown in bold.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.50 0.50 0.50
Our methods:
a. affix-generated seeds only 0.86 0.54 0.66
b. WordNet seeds only 0.88 0.65 0.75
c. both seed sets (a + b) 0.88 0.65 0.75
d. adjacency heuristic only 0.95 0.74 0.83
e. manual annotation of adjacent categories 0.98 0.74 0.84
f. affix seed set and adjacency heuristic (a + d) 0.95 0.75 0.84
g. both seed sets and adjacency heuristic (a + b + d) 0.95 0.78 0.86
h. affix seed set and annotation of adjacent categories 0.98 0.77 0.86
(a + e)
i. both seed sets and annotation of adjacent categories 0.98 0.78 0.87
(a + b + e)
?This data set has equal number of opposites and synonyms, so either class can be chosen to be
predominant. Baseline results shown here are for choosing opposites as the predominant class.
the target words to determine an answer. For example, in modern-day English terrific
is used more frequently in the sense of UNUSUALLY FINE than the sense of FRIGHTFUL,
and so most people will say that terrific and terrible are opposites (in fact that is the
solution provided with these data).
7.3.4 Analysis. We carried out additional experiments to determine how useful individ-
ual components of our method were in solving the synonym-or-opposite questions.
The results on LZQZ are shown in Table 16 and the results on TURN are shown in
Table 17. These results are for the case when the system refrains from guessing in case
of insufficient information. The rows in the tables correspond to the rows in Table 10
shown earlier that gave results on the contrast questions.
Observe that the affix-generated seeds give a marked improvement over the base-
lines, and that knowing which categories are contrasting (either from the adjacency
heuristic or manual annotation of adjacent categories) proves to be the most useful
feature. Also note that even though manual annotation and WordNet seeds eventually
lead to the best results (F = 0.87 for LZQZ and F = 0.81 for TURN), using only the
adjacency heuristic and the affix-generated seeds gives competitive results (F = 0.84 for
the Lin set and F = 0.78 for the Turney set). We are interested in developing methods
to make the approach cross-lingual, so that we can use a thesaurus from one language
(say, English) to compute lexical contrast in a resource-poor target language.
The precision of our method is very good (>0.95). Thus future work will be aimed
at improving recall. This can be achieved by developing methods to generate more seed
opposites. This is also an avenue through which some of the pattern-based approaches
(such as the methods described by Lin et al [2003] and Turney [2008]) can be incorpo-
rated into our method. For instance, we could use n-gram patterns such as ?either X or
Y? and ?from X to Y? to identify pairs of opposites that can be used as additional seeds
in our method.
585
Computational Linguistics Volume 39, Number 3
Recall can also be improved by using affix patterns in other languages to identify
contrasting thesaurus paragraphs in the target language. Thus, constructing a cross-
lingual framework in which words from one language will be connected to thesaurus
categories in another language will be useful not only in computing lexical contrast in a
resource-poor language, but also in using affix information from different languages to
improve results in the target, possibly even resource-rich, language.
8. Conclusions and Future Work
Detecting semantically contrasting word pairs has many applications in natural lan-
guage processing. In this article, we proposed a method for computing lexical contrast
that is based on the hypothesis that if a pair of words, A and B, are contrasting, then
there is a pair of opposites, C and D, such that A and C are strongly related and B and
D are strongly related?the contrast hypothesis. We used pointwise mutual information
to determine the degree of contrast between two contrasting words. The method outper-
formed others on the task of solving a large set of ?choose the most contrasting word?
questions wherein the system not only identified whether two words are contrasting
but also distinguished between pairs of contrasting words with differing degrees of
contrast. We further determined performance of the method on five different kinds of
opposites and across four parts of speech. We used our approach to solve synonym-or-
opposite questions described in Turney (2008) and Lin et al (2003).
Because opposites were central to our methodology, we designed a questionnaire to
better understand different kinds of opposites, which we crowdsourced with Amazon
Mechanical Turk. We devoted extra effort to making sure the questions are phrased
in a simple, yet clear manner. Additionally, a quality control method was developed,
using a word-choice question, to automatically identify and discard dubious and outlier
annotations. From these data, we created a data set of different kinds of opposites that
Table 17
Results for individual components as well as certain combinations of components on the
synonym-or-opposite questions in TURN. The best performing configuration is shown in bold.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.65 0.65 0.65
Our methods:
a. affix-generated seeds only 0.92 0.54 0.68
b. WordNet seeds only 0.93 0.61 0.74
c. both seed sets (a + b) 0.93 0.61 0.74
d. adjacency heuristic only 0.94 0.60 0.74
e. manual annotation of adjacent categories 0.96 0.60 0.74
f. affix seed set and adjacency heuristic (a + d) 0.95 0.67 0.78
g. both seed sets and adjacency heuristic (a + b + d) 0.95 0.68 0.79
h. affix seeds and annotation of adjacent categories 0.97 0.68 0.80
(a + e)
i. both seed sets and annotation of adjacent categories 0.97 0.69 0.81
(a + b + e)
?About 65.4% of the pairs in this data set are opposites. So this row reports baseline results when
choosing opposites as the predominant class.
586
Mohammad et al Computing Lexical Contrast
Table 18
A summary of the data created as part of this research on lexical contrast. Available for
download at: http://www.purl.org/net/saif.mohammad/research.
Name # of items
Affix patterns that tend to generate opposites 15 rules
Contrast questions:
GRE preparatory questions:
Development set 162 questions
Test set 790 questions
Newly created questions: 1,269 questions
Data from work on types of opposites:
Crowdsourced questionnaires 4 sets (one for every pos)
Responses to questionnaires 12,448 assignments (in four files)
Lexicon of opposites generated by the
Mohammad et al method:
Class I opposites 3.5 million word pairs
Class II opposites 2.5 million word pairs
Manually identified contrasting categories
in the Macquarie Thesaurus 209 category pairs
Word-pairs used in Section 5 experiments:
WordNet opposites set 1,358 word pairs
WordNet random word pairs set 1,358 word pairs
WordNet synonyms set 1,358 word pairs
we have made available. We determined the amount of agreement among humans in
identifying lexical contrast, and also in identifying different kinds of opposites. We also
showed that a large number of opposing word pairs have properties pertaining to more
than one kind. Table 18 summarizes the data created as part of this research on lexical
contrast, all of which is available for download.
New questions that target other types of lexical contrast not addressed in this paper
may be added in the future. It may be desirable to break a complex question into two
or more simpler questions. For example, if a word pair is considered to be a certain
kind of opposite when it has both properties M and N, then it is best to have two
separate questions asking whether the word pair has properties M and N, respectively.
The crowdsourcing study can be replicated for other languages by asking the same
questions in the target language for words in the target language. Note, however, that
as of February 2012, most of the Mechanical Turk participants are native speakers of
English, certain Indian languages, and some European languages.
Our future goals include porting this approach to a cross-lingual framework to
determine lexical contrast in a resource-poor language by using a bilingual lexicon to
connect the words in that language with words in another resource-rich language. We
can then use the structure of the thesaurus from the resource-rich language as described
in this article to detect contrasting categories of terms. This is similar to the approach
described by Mohammad et al (2007), who compute semantic distance in a resource-
poor language by using a bilingual lexicon and a sense disambiguation algorithm to
connect text in the resource-poor language with a thesaurus in a different language. This
enables automatic discovery of lexical contrast in a language even if it does not have a
Roget-like thesaurus. The cross-lingual method still requires a bilingual lexicon to map
words between the target language and the language with the thesaurus, however.
587
Computational Linguistics Volume 39, Number 3
Our method used only one Roget-like published thesaurus, but even more gains
may be obtained by combining many dictionaries and thesauri using methods proposed
by Ploux and Victorri (1998) and others.
We modified our algorithm to create lexicons of words associated with positive
and negative sentiment (Mohammad, Dunne, and Dorr 2009). We also used the lexi-
cal contrast algorithm in some preliminary experiments to identify contrast between
sentences and use that information to improve cohesion in automatic summarization
(Mohammad et al 2008). Since its release, the lexicon of contrasting word pairs was used
to improve textual paraphrasing and in turn help improve machine translation (Marton,
El Kholy, and Habash 2011). We are interested in using contrasting word pairs as seeds
to identify phrases that convey contrasting meaning. These will be especially helpful
in machine translation where current systems have difficulty separating translation
hypotheses that convey the same meaning as the source sentences, and those that
do not.
Given a particular word, our method computes a single score as the degree of
contrast with another. A word may be more or less contrasting with another word when
used in different contexts (Murphy 2003), however. Just as in the lexical substitution task
(McCarthy and Navigli 2009), where a system has to find the word that can best replace
a target word in context to preserve meaning, one can imagine a lexical substitution
task to generate contradictions where the objective is to replace a given target word
with one that is contrasting so as to generate a contradiction. Our future work includes
developing a context-sensitive measure of lexical contrast that can be used for exactly
such a task.
There is considerable evidence that children are aware of lexical contrast at a
very early age (Murphy and Jones 2008). They rely on it to better understand var-
ious concepts and in order to communicate effectively. Thus we believe that com-
puter algorithms that deal with language can also obtain significant gains through the
ability to detect contrast and the ability to distinguish between differing degrees of
contrast.
Acknowledgments
We thank Tara Small, Smaranda Muresan,
and Siddharth Patwardhan for their valuable
feedback. This work was supported in part
by the National Research Council Canada;
in part by the National Science Foundation
under grant no. IIS-0705832; in part by the
Human Language Technology Center of
Excellence; and in part by the Natural
Sciences and Engineering Research Council
of Canada. Any opinions, findings, and
conclusions or recommendations expressed
in this material are those of the authors and
do not necessarily reflect the views of the
sponsors.
References
Agresti, Alan. 1990. Categorical Data Analysis.
Wiley, New York, NY.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure
of semantic relatedness. In International
Joint Conference on Artificial Intelligence,
pages 805?810, Acapulco, Mexico.
Bejar, Isaac I., Roger Chaffin, and Susan
Embretson. 1991. Cognitive and
Psychometric Analysis of Analogical Problem
Solving. Springer-Verlag, New York.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram version 1. Linguistic Data
Consortium, Philadelphia, PA.
Budanitsky, Alexander and Graeme Hirst.
2006. Evaluating WordNet-based
measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition).
Oxford University Computing Services,
Oxford.
Charles, Walter G. and George A. Miller.
1989. Contexts of antonymous adjectives.
Applied Psychology, 10:357?375.
Church, Kenneth and Patrick Hanks.
1990. Word association norms, mutual
588
Mohammad et al Computing Lexical Contrast
information and lexicography.
Computational Linguistics, 16(1):22?29.
Cruse, David A. 1986. Lexical Semantics.
Cambridge University Press, Cambridge.
Curran, James R. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, School of
Informatics, University of Edinburgh,
Edinburgh, UK.
de Marneffe, Marie-Catherine, Anna
Rafferty, and Christopher D. Manning.
2008. Finding contradictions in text. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL-08), pages 1,039?1,047, Columbus, OH.
Deese, James. 1965. The Structure of
Associations in Language and Thought.
The Johns Hopkins University Press,
Baltimore, MD.
Egan, Rose F. 1984. Survey of the history
of English synonymy. Webster?s New
Dictionary of Synonyms, pages 5a?25a.
Fellbaum, Christiane. 1995. Co-occurrence
and antonymy. International Journal of
Lexicography, 8:281?303.
Gross, Derek, Ute Fischer, and George A.
Miller. 1989. Antonymy and the
representation of adjectival meanings.
Memory and Language, 28(1):92?106.
Harabagiu, Sanda M., Andrew Hickl,
and Finley Lacatusu. 2006. Lacatusu:
Negation, contrast and contradiction in
text processing. In Proceedings of the 23rd
National Conference on Artificial Intelligence
(AAAI-06), pages 755?762, Boston, MA.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the Eighth Conference on European Chapter of
the Association for Computational Linguistics,
pages 174?181, Madrid.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 539?546, Nantes.
Jones, Steven, Carita Paradis, M. Lynne
Murphy, and Caroline Willners. 2007.
Googling for ?opposites?: A Web-based
study of antonym canonicity. Corpora,
2(2):129?154.
Justeson, John S. and Slava M. Katz.
1991. Co-occurrences of antonymous
adjectives and their contexts.
Computational Linguistics, 17:1?19.
Kagan, Jerome. 1984. The Nature of the Child.
Basic Books, New York.
Kay, Maire Weir, editor. 1988. Webster?s
Collegiate Thesaurus. Merriam-Webster,
Springfield, MA.
Kempson, Ruth M. 1977. Semantic Theory.
Cambridge University Press, Cambridge.
Lehrer, Adrienne and K. Lehrer. 1982.
Antonymy. Linguistics and Philosophy,
5:483?501.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 768?773,
Montreal.
Lin, Dekang, Shaojun Zhao, Lijuan Qin,
and Ming Zhou. 2003. Identifying
synonyms among distributionally
similar words. In Proceedings of the 18th
International Joint Conference on Artificial
Intelligence (IJCAI-03), pages 1,492?1,493,
Acapulco.
Lobanova, Anna, Tom van der Kleij, and
Jennifer Spenader. 2010. Defining
antonymy: A corpus-based study of
opposites by lexico-syntactic patterns.
International Journal of Lexicography,
23(1):19?53.
Lucerto, Cupertino, David Pinto, and
He?ctor Jime?nez-Salazar. 2002. An
automatic method to identify antonymy.
In Workshop on Lexical Resources and the
Web for Word Sense Disambiguation,
pages 105?111, Puebla.
Lyons, John. 1977. Semantics, volume 1.
Cambridge University Press, Cambridge.
Marcu, Daniel and Abdesammad Echihabi.
2002. An unsupervised approach
to recognizing discourse relations.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL-02), pages 368?375,
Philadelphia, PA.
Marton, Yuval, Ahmed El Kholy, and
Nizar Habash. 2011. Filtering antonymous,
trend-contrasting, and polarity-dissimilar
distributional paraphrases for improving
statistical machine translation. In
Proceedings of the Sixth Workshop on
Statistical Machine Translation,
pages 237?249, Edinburgh.
McCarthy, Diana and Roberto Navigli.
2009. The English lexical substitution
task. Language Resources And Evaluation,
43(2):139?159.
Mihalcea, Rada and Carlo Strapparava.
2005. Making computers laugh:
Investigations in automatic humor
recognition. In Proceedings of the
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing, pages 531?538, Vancouver.
Mohammad, Saif, Bonnie Dorr, and
Graeme Hirst. 2008. Computing word-pair
589
Computational Linguistics Volume 39, Number 3
antonymy. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2008), pages 982?991,
Waikiki, HI.
Mohammad, Saif, Bonnie J. Dorr,
Melissa Egan, Nitin Madnani, David Zajic,
and Jimmy Lin. 2008. Multiple alternative
sentence compressions and word-pair
antonymy for automatic text
summarization and recognizing textual
entailment. In Text Analysis Conference
(TAC), Gaithersburg, MD.
Mohammad, Saif, Cody Dunne, and
Bonnie Dorr. 2009. Generating
high-coverage semantic orientation
lexicons from overtly marked words
and a thesaurus. In Proceedings of
Empirical Methods in Natural Language
Processing (EMNLP-2009), pages 599?608,
Singapore.
Mohammad, Saif, Iryna Gurevych,
Graeme Hirst, and Torsten Zesch. 2007.
Cross-lingual distributional profiles of
concepts for measuring semantic distance.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL-2007),
pages 571?580, Prague.
Murphy, Gregory L. and Jane M. Andrew.
1993. The conceptual basis of antonymy
and synonymy in adjectives. Journal of
Memory and Language, 32(3):1?19.
Murphy, Lynne M. 2003. Semantic Relations
and the Lexicon: Antonymy, Synonymy, and
Other Paradigms. Cambridge University
Press, Cambridge.
Murphy, Lynne M. and Steven Jones.
2008. Antonyms in children?s and
child-directed speech. First Language,
28(4):403?430.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?:
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 79?86,
Philadelphia, PA.
Paradis, Carita, Caroline Willners,
and Steven Jones. 2009. Good and
bad opposites using textual and
experimental techniques to measure
antonym canonicity. The Mental Lexicon,
4(3):380?429.
Ploux, Sabine and Bernard Victorri. 1998.
Construction d?espaces se?mantiques a`
l?aide de dictionnaires de synonymes.
TAL, 39(1):161?182.
Schwab, Didier, Mathieu Lafourcade, and
Violaine Prince. 2002. Antonymy and
conceptual vectors. In Proceedings of
the 19th International Conference on
Computational Linguistics (COLING-02),
pages 904?910, Taipei, Taiwan.
Turney, Peter D. 2008. A uniform approach
to analogies, synonyms, antonyms, and
associations. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING-08), pages 905?912,
Manchester.
Turney, Peter D. 2011. Analogy perception
applied to seven tests of word
comprehension. Journal of Experimental
and Theoretical Artificial Intelligence -
Psychometric Artificial Intelligence,
23(3):343?362.
Voorhees, Ellen M. 2008. Contradictions and
justifications: Extensions to the textual
entailment task. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics (ACL-08), 63?71,
Columbus, OH.
590
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356?364,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 2: Measuring Degrees of Relational Similarity
David A. Jurgens
Department of Computer Science
University of California, Los Angeles
jurgens@cs.ucla.edu
Saif M. Mohammad
Emerging Technologies
National Research Council Canada
saif.mohammad@nrc-cnrc.gc.ca
Peter D. Turney
Emerging Technologies
National Research Council Canada
peter.turney@nrc-cnrc.gc.ca
Keith J. Holyoak
Department of Psychology
University of California, Los Angeles
holyoak@lifesci.ucla.edu
Abstract
Up to now, work on semantic relations has fo-
cused on relation classification: recognizing
whether a given instance (a word pair such as
virus:flu) belongs to a specific relation class
(such as CAUSE:EFFECT). However, instances
of a single relation class may still have signif-
icant variability in how characteristic they are
of that class. We present a new SemEval task
based on identifying the degree of prototypi-
cality for instances within a given class. As
a part of the task, we have assembled the first
dataset of graded relational similarity ratings
across 79 relation categories. Three teams
submitted six systems, which were evaluated
using two methods.
1 Introduction
Relational similarity measures the degree of corre-
spondence between two relations, where instance
pairs that have high relational similarity are said to
be analogous, i.e., to express the same relation (Tur-
ney, 2006). However, a class of analogous relations
may still have significant variability in the degree of
relational similarity of its members. Consider the
four word pairs dog:bark, cat:meow, floor:squeak,
and car:honk. We could say that these four X:Y
pairs are all instances of the semantic relation EN-
TITY:SOUND; that is, X is an entity that character-
istically makes the sound Y . Within a class of anal-
ogous pairs, certain pairs are more characteristic of
the relation. For example, many would agree that
dog:bark and cat:meow are better prototypes of the
ENTITY:SOUND relation than floor:squeak. Our task
requires automatic systems to quantify the degree of
prototypicality of a target pair by measuring the re-
lational similarity between it and pairs that are given
as defining examples of a particular relation.
So far, most work in semantic relations has fo-
cused on differences between relation categories for
classifying new relation instances. Past SemEval
tasks that use relations have focused largely on dis-
crete classification (Girju et al, 2007; Hendrickx et
al., 2010) and paraphrasing the relations connecting
noun compounds with a verb (Butnariu et al, 2010),
which is also a form of discrete classification due to
the lack of continuous degrees. However, there is
some loss of information in any discrete classifica-
tion of semantic relations. Furthermore, while some
discrete classifiers provide a degree of confidence or
probability for a relation classification, there is no
a priori reason that such values would correspond
to human prototypicality judgments. Our proposed
task is distinct from these past tasks in that we fo-
cus on measuring the degree of relational similarity.1
A graded measure of the degree of relational simi-
larity would tell us that dog:bark is more similar to
cat:meow than to floor:squeak. The discrete classifi-
cation ENTITY:SOUND drops this information.
Systems that are successful at identifying degrees
of relation similarity can have a significant impact
where an application must choose between multi-
ple instances of the same relation. We illustrate
this with two examples. First, consider a rela-
tional search task (Cafarella et al, 2006). A user
of a relational search engine might give the query,
1Task details and data are available at
https://sites.google.com/site/semeval2012task2/ .
356
Subcategory Relation name Relation schema Paradigms Responses
8(e) AGENT:GOAL ?Y is the goal of X? pilgrim:shrine patient:health
assassin:death runner:finish
climber:peak astronaut:space
5(e) OBJECT:TYPICAL ACTION ?an X will typically Y ? glass:break ice:melt
soldier:fight lion:roar
juggernaut:crush knife:stab
4(h) DEFECTIVE ?an X is is a defect in Y ? fallacy:logic pimple:skin
astigmatism:sight ignorance:learning
limp:walk tumor:body
Table 1: Examples of the three manually selected paradigms and the corresponding pairs generated by Turkers.
?List all things that are part of a car.? SemEval-
2007 Task 4 proposed that a relational search engine
would use semantic relation classification to answer
queries like this one. For this query, a classifier that
was trained with the relation PART:WHOLE would be
used. However, a system for measuring degrees of
relational similarity would be better suited to rela-
tional search than a discrete classifier, because the
relational search engine could then rank the output
list in order of applicability. For the same query, the
search engine could rank each item X in descending
order of the degree of relational similarity between
X:car and a training set of prototypical examples of
the relation PART:WHOLE. This would be analogous
to how standard search engines rank documents or
web pages in descending order of relevance to the
user?s query.
As a second example, consider the role of rela-
tional similarity in analogical transfer. When faced
with a new situation, we look for an analogous sit-
uation in our past experience, and we use analogi-
cal inference to transfer information from the past
experience (the source domain) to the new situation
(the target domain) (Gentner, 1983; Holyoak, 2012).
Analogy is based on relational similarity (Gentner,
1983; Turney, 2008). The degree of relational sim-
ilarity in an analogy is indicative of the likelihood
that transferred knowledge will be applicable in the
target domain. For example, past experience tells us
that a dog barks to send a signal to other creatures. If
we transfer this knowledge to a new experience with
a cat meowing, we can predict that the cat is sending
a signal, and we can act appropriately with that pre-
diction. If we transfer this knowledge to a new expe-
rience with a floor squeaking, we might predict that
the floor is sending a signal, which might lead us to
act inappropriately. If we have a choice among sev-
eral source analogies, usually the source pair with
the highest degree of relational similarity to the tar-
get pair will prove to be the most useful analogy in
the target domain, providing practical benefits be-
yond discrete relational classification.
2 Task Description
Here, we describe our task and the two-level hierar-
chy of semantic relation classes used for the task.
2.1 Objective
Our task is to rate word pairs by the degree to
which they are prototypical members of a given re-
lation class. The relation class is specified by a
few paradigmatic (highly prototypical) examples of
word pairs that belong to the class and also by a
schematic representation of the relation class. The
task requires comparing a word pair to the paradig-
matic examples and/or the schematic representation.
For example, suppose the relation class is REVERSE.
We may specify this class by the paradigmatic ex-
amples attack:defend, buy:sell, love:hate, and the
schematic representation ?X is the reverse act of
Y ? or ?X may be undone by Y .? Given a pair
such as repair:break, we compare this pair to the
paradigmatic examples and/or the schematic repre-
sentation, in order to estimate its degree of prototyp-
icality. The challenges are (1) to infer the relation
from the paradigmatic examples and identify what
relational or featural attributes best characterize that
relation, and (2) to identify the relation of the given
pair and rate how similar it is to that shared by the
paradigmatic examples.
357
2.2 Relation Categories
Researchers in psychology and linguistics have con-
sidered many different categorizations of semantic
relations. The particular relation categorization is
often driven by both the type of data and the in-
tended application. Nastase and Szpakowicz (2003)
propose a two-level hierarchy for noun-modifier re-
lations, which has been widely used (Nakov and
Hearst, 2008; Nastase et al, 2006; Turney and
Littman, 2005; Turney, 2005). Others have used
classifications based on the requirements for a spe-
cific task, such as Information Extraction (Pantel
and Pennacchiotti, 2006) or biomedical applications
(Stephens et al, 2001).
We adopt the relation classification scheme of Be-
jar et al (1991), which includes ten high-level cat-
egories (e.g., CAUSE-PURPOSE and SPACE-TIME).
Each category has between five and ten more re-
fined subcategories (e.g., CAUSE-PURPOSE includes
CAUSE:EFFECT and ACTION:GOAL), for a total of
79 distinct subcategories. Although these cate-
gories do not reflect all possible semantic rela-
tions, they greatly expand the coverage of rela-
tion types from those used in past relation-based
SemEval tasks (Girju et al, 2007; Hendrickx et
al., 2010), which used only seven and nine re-
lation types, respectively. Furthermore, the clas-
sification includes many of the fundamental rela-
tions, e.g., TAXONOMIC and PART:WHOLE, while
also including relations between a variety of parts
of speech and less common relations, such as REF-
ERENCE (e.g., SIGN:SIGNIFICANT) and NONAT-
TRIBUTE (e.g., AGENT:ATYPICAL ACTION). Using
such a large relation class inventory enables evalu-
ating the generality of an approach, while still mea-
suring performance on commonly used relations.
3 Task Data
We constructed a new data set for the task, in which
word pairs are manually classified into relation cat-
egories. Word pairs within a category are manually
distinguished according to how well they represent
the category; that is, the degree to which they are
relationally similar to paradigmatic members of the
given semantic relation class. Paradigmatic mem-
bers of a class were taken from examples provided
by Bejar et al (1991). Due to the large number of
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, ?X R Y ?. Give four ad-
ditional word pairs that illustrate the same relation, in the
same order (X on the left, Y on the right). Please do not
use phrases composed of two or more words in your ex-
amples (e.g., ?racing car?). Please do not use names of
people, places, or things in your examples (e.g., ?Europe?,
?Kleenex?).
(1) :
(2) :
(3) :
(4) :
Figure 1: An example of the two questions for Phase 1.
annotations needed, we used Amazon Mechanical
Turk (MTurk),2 which is a popular choice in com-
putational linguistics for gathering large numbers of
human responses to linguistic questions (Snow et al,
2008; Mohammad and Turney, 2010). We refer to
the MTurk workers as Turkers.
The data set was built in two phases. In the first
phase, Turkers were given three paradigmatic exam-
ples of a subcategory and asked to create new pairs
that instantiate the same relation as the paradigms.
In the second phase, people were asked to distin-
guish the new pairs from the first phase according to
the degree to which they are good representatives of
the given subcategory.
Phase 1 In the first phase, we built upon the
paradigmatic examples of Bejar et al (1991), who
provided one to ten examples for each subcategory.
From these examples, we manually selected three
instances to use as seeds for generating new exam-
ples, adding examples when a subcategory had less
than three. The examples were selected to be bal-
anced across topic domains so as not to bias the
Turkers. For each subcategory, we manually created
a schematic representation of the relation for the ex-
amples. Table 1 gives three examples.
2https://www.mturk.com/
358
To gather new examples of each subcategory,
a two-part questionnaire was presented to Turk-
ers (see Figure 1). In the first part, Turkers were
shown the three paradigm word pairs for a sub-
category along with a list of four relation descrip-
tions (schematic representations of possible rela-
tions). One of the four schematic representations
accurately described the three paradigm pairs and
the other three schematics were distractors (con-
founding descriptions). Turkers were asked to se-
lect which of the four schematic representations best
matched the paradigms. The first part of the ques-
tionnaire serves as quality control by ensuring that
the Turker is capable of recognizing the relation. An
incorrect answer to the question is used to recog-
nize and eliminate confused or negligent responses,
which were approximately 7% of the responses.
In the second part of the Phase 1 questionnaire,
Turkers were shown the three prototypes again and
asked to generate four word pairs that expressed the
same relation. Turkers were directed to be mindful
of the order of the words in each pair, as reversed
orderings can have very different degrees of proto-
typicality in the case of directional relations.
The Turkers provided a total of 3160 additional
examples for the 79 subcategories, 2905 of which
were unique. We applied minor manual correction
to remove spelling errors, which reduced the total
number of examples to 2823. A median of 38 exam-
ples were found per subcategory with a maximum of
40 and minimum of 23. We note that Phase 1 gathers
both high and low quality examples of the relation,
which were all included to capture different degrees
of prototypicality.
We included an additional 395 pairs by randomly
sampling five instances of each subcategory and
creating a new pair from the reversed arguments,
i.e., adding pair Y :X to the subcategory contain-
ing X:Y . Adding reversals was inspired by an ob-
servation during Phase 1 that reversed pairs would
occasionally be added by the Turkers themselves.
We were curious to see what impact reversals would
have on Turker responses and on the output of au-
tomatic systems. Reversals should reveal order sen-
sitivity with a strongly directional relation, such as
PART:WHOLE, but also perhaps there is order sensi-
tivity with more symmetric relations, such as SYN-
ONYMY. Phase 1 produced a total of 3218 pairs.
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, ?X R Y ?. Now consider
the following word pairs:
(1) pig:mud
(2) politician:votes
(3) dog:bone
(4) bird:worm
Which of the above numbered word pairs is the MOST illus-
trative example of the same relation ?X R Y ??
Which of the above numbered word pairs is the LEAST illus-
trative example of the same relation ?X R Y ??
Note: In some cases, a word pair might be in reverse order.
For example, tree:forest is in reverse order for the relation
?X is made from a collection of Y ?. The correct order would
be forest:tree; a forest is made from a collection of trees.
You should treat reversed pairs as BAD examples of the given
relation.
Figure 2: An example of the two questions for Phase 2.
Phase 2 In the second phase, the response pairs
from Phase 1 were ranked according to their pro-
totypicality. We opted to create a ranking using
MaxDiff questions (Louviere, 1991). MaxDiff is a
choice procedure consisting of a question about a
target concept and four or five alternatives. A partic-
ipant must choose both the best and worse answers
from the given alternatives.
MaxDiff is a strong alternative to creating a rank-
ing from standard rating scales, such as the Likert
scale, because it avoids scale biases. Furthermore
MaxDiff is more efficient than other choice proce-
dures such as pairwise comparison, because it does
not require comparing all pairs.
Like Phase 1, Phase 2 was performed using a two-
part questionnaire. The first question was identical
to that of Phase 1: four examples of the same re-
lation subcategory generated in Phase 1 were pre-
sented and the Turker was asked to select the cor-
rect relation from a list of four options. This first
question served as a quality control measure for en-
suring the Turker could properly identify the rela-
tion in question and it also served as a hint, guiding
359
the Turker toward the intended understanding of the
shared relation underlying the three paradigms. In
the second part, the Turker selects the most and least
illustrative example of that relation from among
the four examples of pairs generated by Turkers in
Phase 1.
We aimed for five Turker responses for each
MaxDiff question but averaged 4.73 responses for
each MaxDiff question in a subcategory, with a
minimum of 3.45 responses per MaxDiff question.
Turkers answered a total of 48,846 questions over a
period of five months, of which 6,536 (13%) were
rejected due to a missing answer or an incorrect re-
sponse to the first question.
3.1 Measuring Prototypicality
The MaxDiff responses were converted into the
prototypicality scores using a counting procedure
(Orme, 2009). For each word pair, the prototyp-
icality is scored as the percentage of times it is
chosen as most illustrative minus the percentage of
times it is chosen as least illustrative (see Figure 2).
While methods such as hierarchical Bayes models
can be used to compute a numerical rank from the
responses, we found the counting method to produce
very reasonable results.
3.2 Data Sets
The 79 subcategories were divided into training
and testing segments. Ten subcategories were pro-
vided as training with both the Turkers? MaxDiff
responses and the computed prototypicality ratings.
The ten training subcategories were randomly se-
lected. The remaining 69 subcategories were used
for testing. All data sets are now released on the task
website under the Creative Commons 3.0 license.3
Participants were given the list of all pairs gath-
ered in Phase 1 and the Phase 2 responses for the 10
training subcategories. Phase 2 responses for the 69
test categories were not made available. Participants
also had access to the set of questionnaire materials
provided to the Turkers, the full list of paradigmatic
examples provided by Bejar et al (1991), and the
confounding schema relations from the initial ques-
tions in Phase 1 and Phase 2, which might serve as
negative training examples.
3http://creativecommons.org/licenses/by/3.0/
4 Evaluation
Systems are given examples of pairs from a single
category and asked to provide numeric ratings of the
degree of relational similarity for each pair relative
to the relation expressed in that category.
4.1 Scoring
Spearman?s rank correlation coefficient, ?, and a
MaxDiff score were used to evaluate the systems.
For Spearman?s ?, the prototypicality rating of each
pair is used to build a ranking of all pairs in a sub-
category. Spearman?s ? is then computed between
the pair rankings of a system and the gold standard
ranking. This evaluation abstracts away from com-
paring the numeric values so that only their relative
ordering in prototypicality is measured.
In the second scoring procedure, we measure the
accuracy of a system at answering the same set of
MaxDiff questions as answered by the Turkers in
Phase 2 (see Figure 2). Given the four word pairs,
the system selects the pair with the lowest numeri-
cal rating as least illustrative and the pair with the
highest numerical rating as most illustrative. Ties
in prototypicality are broken arbitrarily. Accuracy is
measured as the percentage of questions answered
correctly. An answer is considered correct when it
agrees with the majority of the Turkers. In some
cases, two answers may be considered correct. For
example, when five Turkers answer a given MaxD-
iff question, two Turkers might choose one pair as
the most illustrative and two other Turkers might
choose another pair as the most illustrative. In this
case, both pairs would count as correct choices for
the most illustrative pair.
4.2 Baselines
We consider two baselines for evaluation: Random
and PMI. The Random baseline rates each pair in a
subcategory randomly. The expected Spearman cor-
relation for Random ratings is zero. The expected
MaxDiff score for Random ratings would be 25%
(because there are four word pairs to choose from
in Phase 2) if there were always a unique majority,
but it is actually about 31%, due to cases where two
pairs both get two votes from the Turkers.
Given a MaxDiff question, a Turker might select
the pair whose words are most strongly associated
360
Team Members System Description
Beneme?rita
Universidad
Auto?noma de
Puebla (Me?xico)
(BUAP)
Mireya T. Vidal,
Darnes V. Ayala,
Jose A.R. Ortiz,
Azucena M.
Rendon,
David Pinto, and
Saul L. Silverio
BUAP Each pair is represented as a vector over multiple features: lexical,
intervening words, WordNet relations between the pair, and syntactic
features such as part of speech and morphology. Prototypicality is
based on cosine similarity with the class?s pairs.
University of Texas at
Dallas (UTD)
Bryan Rink and
Sanda Harabagiu
NB Unsupervised learning identifies intervening patterns between all word
pairs. Each pattern is then ranked according to its subcategory
specificity by learning a generative model from patterns to word pairs.
Prototypicality ratings are based on confidence that the highest scoring
pattern found for a pair belongs to the subcategory.
SVM Intervening patterns are found using the same method as UTD-NB.
Word pairs are then represented as feature vectors of matching
patterns. An SVM classifier is trained using a subcategory?s pairs as
positive training data and all other pairs as negative. Prototypicality
ratings are based on SVM confidence of class inclusion.
University of
Minnesota, Duluth
(Duluth)
Ted Pedersen V0 WordNet is used to build the set of concepts connected by WordNet
relations to the pairs? words. Prototypicality is estimated using the
vector similarity of the concatenated glosses.
V1 Same procedure as V0, with one further expansion to related concepts.
V2 Same procedure as V0, with two further expansions to related concepts.
Table 2: Descriptions of the participating teams and systems.
as the most illustrative and the least associated as
the least illustrative. Therefore, we propose a sec-
ond baseline where pairs are rated according to their
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), which measures the statistical asso-
ciation between two words. For this baseline, the
prototypicality rating given to a word pair is simply
the PMI score for the pair. For two terms x and y,
PMI(x, y) is defined as log2
(
p(x,y)
p(x)p(y)
)
where p(?)
denotes the probability of a term or pair of terms.
The PMI score was calculated using the method of
Turney (2001) on a corpus of approximately 50 bil-
lion tokens, indexed by the Wumpus search engine.4
To calculate p(x, y), we recorded all co-occurrences
of both terms within a ten-word window.
5 Systems
Three teams submitted six systems for evaluation.
Table 2 summarizes the teams and systems. Two
teams (BUAP and UTD) based their approaches on
discovering relation-specific patterns for each cat-
egory, while the third team (Duluth) used vector
space comparisons of the glosses related to the pairs.
4http://www.wumpus-search.org/
No single system was able to achieve superior per-
formance on all subcategories. Table 3 reports the
averages across all subcategories for Spearman?s ?
and MaxDiff accuracy. Five systems were able to
perform above the Random baseline, while only one
system, UTD-NB, consistently performed above the
PMI baseline.
However, the average performance masks supe-
rior performance on individual subcategories. Ta-
ble 3 also reports the number of subcategories in
which a system obtained a statistically significant
Spearman?s ? with the gold standard ranking. De-
spite the low average performance, most models
were able to obtain significant correlation in multi-
ple subcategories. Furthermore, the significant cor-
relations for different systems were not always ob-
tained in the same subcategories. Across all subcat-
egories, 43 had a significant correlation at p < 0.05
and 27 at p < 0.01. The broad coverage of signifi-
cantly correlated subcategories spanned by the com-
bination of all systems and the PMI baseline sug-
gests that high performance on this task may be pos-
sible, but that adapting to each of the specific rela-
tion types may be very beneficial.
361
Team System Spearman?s ? # of Subcategories MaxDiff
p < 0.05 p < 0.01
BUAP BUAP 0.014 2 0 31.7
UTD NB 0.229 22 16 39.4
SVM 0.116 11 5 34.7
Duluth V0 0.050 9 3 32.4
V1 0.039 10 4 31.5
V2 0.038 7 3 31.1
Baselines Random 0.018 4 0 31.2
PMI 0.112 15 7 33.9
Table 3: Average Spearman?s ? and MaxDiff scores for all system across all 69 test subcategories. Columns 4 and 5
denote the number of subcategories with a Spearman?s ? that is statistically significant at the noted level of confidence.
Relation Class Random PMI BUAP UTD-NB UTD-SVM Duluth-V0 Duluth-V1 Duluth-V2
Class-Inclusion 0.057 0.221 0.064 0.233 0.093 0.045 0.178 0.168
Part-Whole 0.012 0.144 0.066 0.252 0.142 -0.061 -0.084 -0.054
Similar 0.026 0.094 -0.036 0.214 0.131 0.183 0.208 0.198
Contrast -0.049 0.032 0.000 0.206 0.162 0.142 0.120 0.051
Attribute 0.037 -0.032 -0.095 0.158 0.052 0.044 -0.003 0.008
Non-Attribute -0.070 0.191 0.009 0.098 0.094 0.079 0.066 0.074
Case Relations 0.090 0.168 -0.037 0.241 0.187 -0.011 -0.068 -0.115
Cause-Purpose -0.011 0.130 0.114 0.183 0.060 0.021 0.022 0.042
Space-Time 0.013 0.084 0.035 0.375 0.139 0.055 -0.004 0.040
Reference 0.142 0.125 -0.001 0.346 0.082 0.028 0.074 0.067
Table 4: Average Spearman?s ? correlation with the Turker rankings in each of the high-level relation categories, with
the highest average correlation for each subcategory shown in bold.
6 Discussion
Sensitivity to Pair Association The PMI base-
line performed much better than anticipated, outper-
forming all systems but UTD-NB on many of the
subcategories, despite treating all relations as direc-
tionless. Performance was highest in subcategories
where the X:Y pair might reasonably be expected
to occur together, e.g., FUNCTIONAL or CONTRA-
DICTORY. However, PMI benefits from the design
of our task, which focuses on rating pairs within a
given subcategory. In a different task that mixed
pairs from a variety of subcategories, PMI would
perform poorly, because it would assign high scores
to pairs of strongly associated words, regardless of
whether they belong to a given subcategory.
Difficulty of Specific Subcategories Performance
across the high-level categories was highly varied
between approaches. The category-level summary
shown in Table 4 reveals high-level trends in diffi-
culty across all submitted systems. The submitted
systems performed best for subcategories under the
Similar category, while the systems performed worst
for Non-Attribute subcategories.
As a further possibility of explaining performance
differences between subcategories, we considered
the hypothesis that the difficulty of a subcategory is
inversely proportional to the range of prototypicality
scores, i.e., subcategories with restricted ranges are
more difficult. However, we found that the difficulty
was uncorrelated with both the size of the interval
spanned by prototypicality scores and the standard
deviation of the scores.
Sensitivity to Argument Reversal The direction-
ality of a relation can significantly impact the rated
prototypicality of a pair whose arguments have been
reversed. As an approximate measure of the ef-
fect on prototypicality when a pairs? arguments are
reversed, we calculated the expected drop in rank
362
Spearman?s ?
Team System No Reversals With Reversals
BUAP BUAP -0.003 0.014
UTD NB 0.190 0.229
SVM 0.104 0.116
Duluth V0 0.062 0.050
V1 0.040 0.039
V2 0.046 0.038
Baselines Random 0.004 0.018
PMI 0.143 0.112
Table 5: Average pair ranking correlation for all subcate-
gories when reversed pairs are included and excluded.
between a pair and its reversed form. Based on
the Turker rankings, the SEQUENCE (e.g., preg-
nancy:birth) and FUNCTIONAL (e.g., weapon:knife)
subcategories exhibited the strongest sensitivity to
argument reversal, while ATTRIBUTE SIMILARITY
(e.g., rake:fork) and CONTRARY (e.g., happy:sad)
exhibited the least.
The inclusion of reversed pairs potentially adds
a small amount of noise to the relation identifica-
tion process for subcategories with directional rela-
tions. Two teams, BUAP and UTD, accounted for
relation directionality, while Duluth did not, which
resulted in the Duluth systems ranking reversed pairs
the same. Therefore, we conducted a post-hoc anal-
ysis of the impact of reversals by removing the re-
versed pairs from the computed prototypicality rank-
ings. Table 5 reports the resulting Spearman?s ?.
With Spearman?s ?, we can easily evaluate the im-
pact of the reversals, because we can delete a re-
versed pair without affecting anything else. For the
MaxDiff questions, if there is one reversal in a group
of four choices, then we need to delete the whole
MaxDiff question. Therefore we do not include the
MaxDiff score in Table 5.
Removing reversals decreased performance in the
three systems that were sensitive to pair order-
ing (BUAP, UTD-NB, and UTD-SVM), while only
marginally increasing performance in the three sys-
tems that ignored the ordering. The performance de-
crease in systems that use ordering suggests that the
reversed pairs are easily identified and ranked ap-
propriately low. As a further estimate of the models?
ability to correctly order reversals, we compared the
difference in a reversal?s rank for both a system?s
Team System RMSE
BUAP BUAP 256.07
UT Dallas NB 257.15
SVM 209.95
Baseline Random 227.25
Table 6: RMSE in estimating the difference in rank be-
tween a pair and its reversal in the gold standard.
ranking and the ranking computed from Turker Re-
sponses. Table 6 reports the Root Mean Squared
Error (RMSE) in ranking difference for the three
systems that took argument order into account. Al-
though not the best performing system, Table 6 indi-
cates that the UTD-SVM system was most able to
appropriately weight reversals? prototypicality. In
contrast, the UTD-NB system often had many pairs
tied for the lowest rank, which either resulted in pair
and its reversal being tied or having a much smaller
rank difference, thereby increasing its RMSE.
7 Conclusions
We have introduced a new task focused on rating the
degrees of prototypicality for word pairs sharing the
same relation. Participants first identify the relation
shared between example pairs and then rate the de-
gree to which each pair expresses that relation. As
a part of the task, we constructed a dataset of proto-
typicality ratings for 3218 word pairs in 79 different
relation categories.
Participating systems used combinations of
corpus-based, syntactic, and WordNet features, with
varying degrees of success. The task also included a
competitive baseline, PMI, which surpassed all but
one system. Several models obtained moderate per-
formance in select relation subcategories, but no one
approach succeeded in general, which introduces
much opportunity for future improvement. We also
hope that both the example pairs and their prototyp-
icality ratings will be a valuable data set for future
research in Linguistics as well as Cognitive Psychol-
ogy. All data sets for this task have been made pub-
licly available on the task website.
Acknowledgements
This research was supported by ONR grant
N000140810186.
363
References
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Dair-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
39?44. Association for Computational Linguistics.
Michael J. Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational web search. In WWW Conference.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Dedre Gentner. 1983. Structure-mapping: A theoretical
framework for analogy. Cognitive science, 7(2):155?
170.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the
4th International Workshop on Semantic Evaluation
(SemEval-2007).
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
33?38. Association for Computational Linguistics.
Keith J. Holyoak. 2012. Analogy and relational reason-
ing. In Oxford handbook of thinking and reasoning,
pages 234?259. Oxford University Press.
Jordan J. Louviere. 1991. Best-worst scaling: A model
for the largest difference judgments. Working Paper.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL-HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34. Association for Com-
putational Linguistics.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of ACL, volume 8.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301. ACL Press Tilburg,, The Nether-
lands.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and wordnet-
based features. In Proceedings of AAAI, volume 21,
page 781.
Bryan Orme. 2009. Maxdiff analysis: Simple counting,
individual-level logit, and hb.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113?120. Associa-
tion for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
M. Stephens, M. Palakal, S. Mukhopadhyay, R. Raje,
J. Mostafa, et al 2001. Detecting gene relations from
medline abstracts. In Pacific Symposium on Biocom-
puting, volume 6, pages 483?495. Citeseer.
Peter D. Turney and Michael L Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1?3):251?278.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502.
Peter D. Turney. 2005. Measuring semantic similarity
by latent relational analysis. In Proceedings of IJCAI,
pages 1136?1141.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2008. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33(1):615?655.
364
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26?34,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Emotions Evoked by Common Words and Phrases:
Using Mechanical Turk to Create an Emotion Lexicon
Saif M. Mohammad and Peter D. Turney
Institute for Information Technology,
National Research Council Canada.
Ottawa, Ontario, Canada, K1A 0R6
{saif.mohammad,peter.turney}@nrc-cnrc.gc.ca
Abstract
Even though considerable attention has been
given to semantic orientation of words and the
creation of large polarity lexicons, research
in emotion analysis has had to rely on lim-
ited and small emotion lexicons. In this pa-
per, we show how we create a high-quality,
moderate-sized emotion lexicon using Me-
chanical Turk. In addition to questions about
emotions evoked by terms, we show how the
inclusion of a word choice question can dis-
courage malicious data entry, help identify in-
stances where the annotator may not be famil-
iar with the target term (allowing us to reject
such annotations), and help obtain annotations
at sense level (rather than at word level). We
perform an extensive analysis of the annota-
tions to better understand the distribution of
emotions evoked by terms of different parts of
speech. We identify which emotions tend to be
evoked simultaneously by the same term and
show that certain emotions indeed go hand in
hand.
1 Introduction
When analyzing text, automatically detecting emo-
tions such as joy, sadness, fear, anger, and surprise is
useful for a number of purposes, including identify-
ing blogs that express specific emotions towards the
topic of interest, identifying what emotion a news-
paper headline is trying to evoke, and devising auto-
matic dialogue systems that respond appropriately to
different emotional states of the user. Often different
emotions are expressed through different words. For
example, delightful and yummy indicate the emo-
tion of joy, gloomy and cry are indicative of sadness,
shout and boiling are indicative of anger, and so on.
Therefore an emotion lexicon?a list of emotions
and words that are indicative of each emotion?is
likely to be useful in identifying emotions in text.
Words may evoke different emotions in different
contexts, and the emotion evoked by a phrase or a
sentence is not simply the sum of emotions conveyed
by the words in it, but the emotion lexicon will be a
useful component for any sophisticated emotion de-
tecting algorithm. The lexicon will also be useful for
evaluating automatic methods that identify the emo-
tions evoked by a word. Such algorithms may then
be used to automatically generate emotion lexicons
in languages where no such lexicons exist. As of
now, high-quality high-coverage emotion lexicons
do not exist for any language, although there are a
few limited-coverage lexicons for a handful of lan-
guages, for example, the WordNet Affect Lexicon
(WAL) (Strapparava and Valitutti, 2004) for six ba-
sic emotions and the General Inquirer (GI) (Stone et
al., 1966), which categorizes words into a number of
categories, including positive and negative semantic
orientation.
Amazon has an online service called Mechani-
cal Turk that can be used to obtain a large amount
of human annotation in an efficient and inexpensive
manner (Snow et al, 2008; Callison-Burch, 2009).1
However, one must define the task carefully to ob-
tain annotations of high quality. Several checks must
be placed to ensure that random and erroneous anno-
tations are discouraged, rejected, and re-annotated.
In this paper, we show how we compiled a
moderate-sized English emotion lexicon by manual
1https://www.mturk.com/mturk/welcome
26
annotation through Amazon?s Mechanical Turk ser-
vice. This dataset, which we will call EmoLex, is
many times as large as the only other known emo-
tion lexicon, WordNet Affect Lexicon. More impor-
tantly, the terms in this lexicon are carefully chosen
to include some of the most frequent nouns, verbs,
adjectives, and adverbs. Beyond unigrams, it has
a large number of commonly used bigrams. We
also include some words from the General Inquirer
and some from WordNet Affect Lexicon, to allow
comparison of annotations between the various re-
sources.
We perform an extensive analysis of the annota-
tions to answer several questions that have not been
properly addressed so far. For instance, how hard is
it for humans to annotate words with the emotions
they evoke? What percentage of commonly used
terms, in each part of speech, evoke an emotion? Are
emotions more commonly evoked by nouns, verbs,
adjectives, or adverbs? Is there a correlation be-
tween the semantic orientation of a word and the
emotion it evokes? Which emotions tend to go to-
gether; that is, which emotions are evoked simulta-
neously by the same term? This work is intended
to be a pilot study before we create a much larger
emotion lexicon with tens of thousands of terms.
We focus on the emotions of joy, sadness, anger,
fear, trust, disgust, surprise, and anticipation?
argued by many to be the basic and prototypical
emotions (Plutchik, 1980). Complex emotions can
be viewed as combinations of these basic emotions.
2 Related work
WordNet Affect Lexicon (Strapparava and Valitutti,
2004) has a few hundred words annotated with the
emotions they evoke.2 It was created by manually
identifying the emotions of a few seed words and
then marking all their WordNet synonyms as having
the same emotion. The General Inquirer (Stone et
al., 1966) has 11,788 words labeled with 182 cat-
egories of word tags, including positive and nega-
tive semantic orientation.3 It also has certain other
affect categories, such as pleasure, arousal, feeling,
and pain but these have not been exploited to a sig-
nificant degree by the natural language processing
2http://wndomains.fbk.eu/wnaffect.html
3http://www.wjh.harvard.edu/?inquirer
community.
Work in emotion detection can be roughly classi-
fied into that which looks for specific emotion denot-
ing words (Elliott, 1992), that which determines ten-
dency of terms to co-occur with seed words whose
emotions are known (Read, 2004), that which uses
hand-coded rules (Neviarouskaya et al, 2009), and
that which uses machine learning and a number of
emotion features, including emotion denoting words
(Alm et al, 2005).
Much of this recent work focuses on six emo-
tions studied by Ekman (1992). These emotions?
joy, sadness, anger, fear, disgust, and surprise? are
a subset of the eight proposed in Plutchik (1980).
We focus on the Plutchik emotions because the emo-
tions can be naturally paired into opposites?joy?
sadness, anger?fear, trust?disgust, and anticipation?
surprise. Natural symmetry apart, we believe that
prior work on automatically computing word?pair
antonymy (Lin et al, 2003; Mohammad et al, 2008;
Lobanova et al, 2010) can now be leveraged in au-
tomatic emotion detection.
3 Emotion annotation
In the subsections below we present the challenges
in obtaining high-quality emotion annotation, how
we address those challenges, how we select the tar-
get terms, and the questionnaire we created for the
annotators.
3.1 Key challenges
Words used in different senses can evoke different
emotions. For example, the word shout evokes a
different emotion when used in the context of ad-
monishment, than when used in ?Give me a shout if
you need any help.? Getting human annotations on
word senses is made complicated by decisions about
which sense-inventory to use and what level of gran-
ularity the senses must have. On the one hand, we
do not want to choose a fine-grained sense-inventory
because then the number of word?sense combina-
tions will become too large and difficult to easily
distinguish, and on the other hand we do not want
to work only at the word level because when used
in different senses a word may evoke different emo-
tions.
Yet another challenge is how best to convey a
27
word sense to the annotator. Long definitions will
take time to read and limit the number of annotations
we can obtain for the same amount of resources.
Further, we do not want to bias the annotator to-
wards an emotion through the definition. We want
the users to annotate a word only if they are already
familiar with it and know its meanings. And lastly,
we must ensure that malicious and erroneous anno-
tations are rejected.
3.2 Our solution
In order to overcome the challenges described
above, before asking the annotators questions about
what emotions are evoked by a target term, we first
present them with a word choice problem pertaining
to the target. They are provided with four different
words and asked which word is closest in meaning
to the target. This single question serves many pur-
poses. Through this question we convey the word
sense for which annotations are to be provided, with-
out actually providing annotators with long defini-
tions. If an annotator is not familiar with the target
word and still attempts to answer questions pertain-
ing to the target, or is randomly clicking options in
our questionnaire, then there is a 75% chance that
they will get the answer to this question wrong, and
we can discard all responses pertaining to this target
term by the annotator (that is, we discard answers to
the emotion questions provided by the annotator for
this target term).
We generated these word choice problems auto-
matically using the Macquarie Thesaurus (Bernard,
1986). Published thesauri, such as Roget?s and Mac-
quarie, divide the vocabulary into about a thou-
sand categories, which may be interpreted as coarse
senses. If a word has more than one sense, then it
can be found in more than one thesaurus category.
Each category also has a head word which best cap-
tures the meaning of the category.
Most of the target terms chosen for annotation are
restricted to those that are listed in exactly one the-
saurus category. The word choice question for a
target term is automatically generated by selecting
the following four alternatives (choices): the head
word of the thesaurus category pertaining to the tar-
get term (the correct answer); and three other head
words of randomly selected categories (the distrac-
tors). The four alternatives are presented to the an-
notator in random order.
Only a small number of the words in the WordNet
Affect Lexicon are listed in exactly one thesaurus
category (have one sense), and so we included tar-
get terms that occurred in two thesaurus categories
as well. For these questions, we listed head words
from both the senses (categories) as two of the alter-
natives (probability of a random choice being cor-
rect is 50%). Depending on the alternative chosen,
we can thus determine the sense for which the sub-
sequent emotion responses are provided by the an-
notator.
3.3 Target terms
In order to generate an emotion lexicon, we first
identify a list of words and phrases for which we
want human annotations. We chose the Macquarie
Thesaurus as our source pool for unigrams and bi-
grams. Any other published dictionary would have
worked well too. However, apart from over 57,000
commonly used English word types, the Macquarie
Thesaurus also has entries for more than 40,000
commonly used phrases. From this list of unigrams
and bigrams we chose those that occur frequently in
the Google n-gram corpus (Brants and Franz, 2006).
Specifically we chose the 200 most frequent n-grams
in the following categories: noun unigrams, noun
bigrams, verb unigrams, verb bigrams, adverb un-
igrams, adverb bigrams, adjective unigrams, adjec-
tive bigrams, words in the General Inquirer that are
marked as having a negative semantic orientation,
words in General Inquirer that are marked as hav-
ing a positive semantic orientation. When selecting
these sets, we ignored terms that occurred in more
than one Macquarie Thesaurus category. Lastly, we
chose all words from each of the six emotion cat-
egories in the WordNet Affect Lexicon that had at
most two senses in the thesaurus (occurred in at
most two thesaurus categories). The first and sec-
ond column of Table 1 list the various sets of tar-
get terms as well as the number of terms in each set
for which annotations were requested. EmoLexUni
stands for all the unigrams taken from the thesaurus.
EmoLexBi refers to all the bigrams. EmoLexGI
are all the words taken from the General Inquirer.
EmoLexWAL are all the words taken from the Word-
Net Affect Lexicon.
28
3.4 Mechanical Turk HITs
An entity submitting a task to Mechanical Turk is
called the requester. A requester first breaks the
task into small independently solvable units called
HITs (Human Intelligence Tasks) and uploads
them on the Mechanical Turk website. The requester
specifies the compensation that will be paid for solv-
ing each HIT. The people who provide responses to
these HITs are called Turkers. The requester also
specifies the number of different Turkers that are
to annotate each HIT. The annotation provided by
a Turker for a HIT is called an assignment.
We created Mechanical Turk HITs for each of the
terms specified in Table 1. Each HIT has a set of
questions, all of which are to be answered by the
same person. We requested five different assign-
ments for each HIT (each HIT is to be annotated
by five different Turkers). Different HITS may be
attempted by different Turkers, and a Turker may
attempt as many HITs as they wish. Below is an
example HIT for the target word ?startle?.
Title: Emotions evoked by words
Reward per HIT: $0.04
Directions: Return HIT if you are not familiar
with the prompt word.
Prompt word: startle
1. Which word is closest in meaning (most
related) to startle?
? automobile
? shake
? honesty
? entertain
2. How positive (good, praising) is the word
startle?
? startle is not positive
? startle is weakly positive
? startle is moderately positive
? startle is strongly positive
3. How negative (bad, criticizing) is the word
startle?
? startle is not negative
? startle is weakly negative
? startle is moderately negative
? startle is strongly negative
4. How much does the word startle evoke or
produce the emotion joy (for example, happy
and fun may strongly evoke joy)?
# of terms Annotns.
EmoLex Initial Master per word
EmoLexUni:
adjectives 200 196 4.7
adverbs 200 192 4.7
nouns 200 187 4.6
verbs 200 197 4.7
EmoLexBi:
adjectives 200 182 4.7
adverbs 187 171 4.7
nouns 200 193 4.7
verbs 200 186 4.7
EmoLexGI:
negatives in GI 200 196 4.7
positives in GI 200 194 4.8
EmoLexWAL:
anger terms in WAL 107 84 4.8
disgust terms in WAL 25 25 4.8
fear terms in WAL 58 58 4.8
joy terms in WAL 109 92 4.8
sadness terms in WAL 86 73 4.7
surprise terms in WAL 39 38 4.7
Union 2176 2081 4.75
Table 1: Break down of target terms into various cate-
gories. Initial refers to terms chosen for annotation. Mas-
ter refers to terms for which three or more valid assign-
ments were obtained using Mechanical Turk.
? startle does not evoke joy
? startle weakly evokes joy
? startle moderately evokes joy
? startle strongly evokes joy
[Questions 5 to 11 are similar to 4, except that
joy is replaced with one of the other seven
emotions: sadness (failure and heart-break);
fear (horror and scary); anger (rage and shout-
ing); trust (faith and integrity); disgust (gross
and cruelty); surprise (startle and sudden); an-
ticipation (expect and eager).]
Before going live, the survey was approved by the
ethics committee at the National Research Council
Canada.
4 Annotation analysis
The first set of emotion annotations on Mechanical
Turk were completed in about nine days. The Turk-
ers spent a minute on average to answer the ques-
tions in a HIT. This resulted in an hourly pay of
slightly more than $2.
29
Once the assignments were collected, we used au-
tomatic scripts to validate the annotations. Some as-
signments were discarded because they failed cer-
tain tests (described below). A subset of the dis-
carded assignments were officially rejected (the
Turkers were not paid for these assignments) be-
cause instructions were not followed. About 500 of
the 10,880 assignments (2,176 ? 5) included at least
one unanswered question. These assignments were
discarded and rejected. More than 85% of the re-
maining assignments had the correct answer for the
word choice question. This was a welcome result
showing that, largely, the annotations were done in
a responsible manner. We discarded all assignments
that had the wrong answer for the word choice ques-
tion. If an annotator obtained an overall score that
is less than 66.67% on the word choice questions
(that is, got more than one out of three wrong), then
we assumed that, contrary to instructions, HITs for
words not familiar to the annotator were attempted.
We discarded and rejected all assignments by such
annotators (not just the assignments for which they
got the word choice question wrong).
HITs pertaining to all the discarded assignments
were uploaded for a second time on Mechanical
Turk and the validation process was repeated. Af-
ter the second round, we had three or more valid as-
signments for 2081 of the 2176 target terms. We will
refer to this set of assignments as the master set. We
create the emotion lexicon from this master set con-
taining 9892 assignments from about 1000 Turkers
who attempted 1 to 450 assignments each. About
100 of them provided 20 or more assignments each
(more than 7000 assignments in all). The master set
has, on average, about 4.75 assignments for each of
the 2081 target terms. (See Table 1 for more details.)
4.1 Emotions evoked by words
The different emotion annotations for a target term
were consolidated by determining the majority
class of emotion intensities. For a given term?
emotion pair, the majority class is that intensity level
that is chosen most often by the Turkers to represent
the degree of emotion evoked by the word. Ties are
broken by choosing the stronger intensity level. Ta-
ble 2 lists the percent of 2081 target terms assigned
a majority class of no, weak, moderate, and strong
emotion. For example, it tells us that 7.6% of the tar-
Intensity
Emotion no weak moderate strong
anger 78.8 9.4 6.2 5.4
anticipation 71.4 13.6 9.4 5.3
disgust 82.6 8.8 4.9 3.5
fear 76.5 11.3 7.3 4.7
joy 72.6 9.6 10.0 7.6
sadness 76.0 12.4 5.8 5.6
surprise 84.8 7.9 4.1 3.0
trust 73.3 12.0 9.8 4.7
micro average 77.0 10.6 7.2 5.0
any emotion 17.9 23.4 28.3 30.1
Table 2: Percent of 2081 terms assigned a majority class
of no, weak, moderate, and strong emotion.
Emotion % of terms
anger 15.4
anticipation 20.9
disgust 11.0
fear 14.5
joy 21.9
sadness 14.4
surprise 9.8
trust 20.6
micro average 16.1
any emotion 67.9
Table 3: Percent of 2081 target terms that are evocative.
get terms strongly evoke joy. The table also presents
an average of the numbers in each column (micro av-
erage). Observe that the percentages for individual
emotions do not vary greatly from the average. The
last row lists the percent of target terms that evoke
some emotion (any of the eight) at the various in-
tensity levels. We calculated this using the intensity
level of the strongest emotion expressed by each tar-
get. Observe that 30.1% of the target terms strongly
evoke at least one of the eight basic emotions.
Even though we asked Turkers to annotate emo-
tions at four levels of intensity, practical NLP appli-
cations often require only two levels?evoking par-
ticular emotion (evocative) or not (non-evocative).
For each target term?emotion pair, we convert the
four-level annotations into two-level annotations by
placing all no- and weak-intensity assignments in
the non-evocative bin, all moderate- and strong-
intensity assignments in the evocative bin, and then
choosing the bin with the majority assignments. Ta-
ble 3 gives percent of target terms considered to be
30
EmoLex anger anticipation disgust fear joy sadness surprise trust any
EmoLexUni:
adjectives 12 21 8 11 30 13 10 19 72
adverbs 12 16 7 8 21 6 11 25 65
nouns 4 21 2 9 16 3 3 21 47
verbs 12 21 7 11 15 12 11 17 56
EmoLexBi:
adjectives 12 24 8 10 26 14 7 18 64
adverbs 3 26 1 5 15 4 8 25 54
nouns 9 30 6 12 15 6 2 24 56
verbs 8 34 2 5 29 6 9 28 67
EmoLexGI:
negatives in GI 45 5 34 35 1 37 11 2 78
positives in GI 0 23 0 0 48 0 6 47 77
EmoLexWAL:
anger terms in WAL 90 2 54 41 0 32 2 0 91
disgust terms in WAL 40 4 92 36 0 20 8 0 96
fear terms in WAL 25 17 31 79 0 36 34 0 87
joy terms in WAL 3 32 3 1 89 1 18 38 95
sadness terms in WAL 17 0 9 15 0 93 1 1 94
surprise terms in WAL 7 23 0 21 52 10 76 7 86
Table 4: Percent of terms, in each target set, that are evocative. Highest individual emotion scores for EmoLexWAL are
shown bold. Observe that WAL fear terms are marked most as fear evocative, joy terms as joy evocative, and so on.
evocative. The last row in the table gives the per-
centage of terms evocative of some emotion (any of
the eight). Table 4 shows how many terms in each
category are evocative of the different emotions.
4.1.1 Analysis and discussion
Table 4 shows that a sizable percent of nouns, verbs,
adjectives, and adverbs are evocative. Adverbs and
adjectives are some of the most emotion inspiring
terms and this is not surprising considering that they
are used to qualify a noun or a verb. Anticipation,
trust, and joy come through as the most common
emotions evoked by terms of all four parts of speech.
The EmoLexWAL rows are particularly interest-
ing because they serve to determine how much
the Turker annotations match annotations in the
Wordnet Affect Lexicon (WAL). The most common
Turker-determined emotion for each of these rows is
marked in bold. Observe that WAL anger terms are
mostly marked as anger evocative, joy terms as joy
evocative, and so on. The EmoLexWAL rows also
indicate which emotions get confused for which, or
which emotions tend to be evoked simultaneously
by a term. Observe that anger terms tend also to be
evocative of disgust. Similarly, fear and sadness go
together, as do joy, trust, and anticipation.
The EmoLexGI rows rightly show that words
marked as negative in the General Inquirer, mostly
evoke negative emotions (anger, fear, disgust, and
sadness). Observe that the percentages for trust and
joy are much lower. On the other hand, positive
words evoke anticipation, joy, and trust.
4.1.2 Agreement
In order to analyze how often the annotators agreed
with each other, for each term?emotion pair, we cal-
culated the percentage of times the majority class
has size 5 (all Turkers agree), size 4 (all but one
agree), size 3, and size 2. Observe that for more than
50% of the terms, at least four annotators agree with
each other. Table 5 presents these agreement values.
Since many NLP systems may rely only on two in-
tensity values (evocative or non-evocative), we also
calculate agreement at that level (Table 6). Observe
that for more than 50% of the terms, all five annota-
tors agree with each other, and for more than 80%
of the terms, at least four annotators agree. This
shows a high degree of agreement on emotion anno-
tations despite no real control over the educational
background and qualifications of the annotators.
31
Majority class size
Emotion two three four five
anger 13.1 25.6 27.4 33.7
anticipation 31.6 35.2 20.7 12.3
disgust 14.0 21.6 29.0 35.1
fear 15.0 29.9 28.6 26.2
joy 17.6 26.4 23.0 32.7
sadness 14.2 24.6 28.1 32.8
surprise 17.0 29.3 32.3 21.2
trust 22.4 27.8 22.4 27.2
micro average 18.1 27.6 26.4 27.7
Table 5: Agreement at four intensity levels for emotion
(no, weak, moderate, and strong): Percent of 2081 terms
for which the majority class size was 2, 3, 4, and 5.
Majority class size
Emotion three four five
anger 15.0 25.9 58.9
anticipation 32.3 33.7 33.8
disgust 12.8 24.6 62.4
fear 14.9 25.6 59.4
joy 18.4 27.0 54.5
sadness 13.6 22.0 64.2
surprise 17.5 31.4 50.9
trust 23.9 29.3 46.6
micro average 18.6 27.4 53.8
Table 6: Agreement at two intensity levels for emotion
(evocative and non-evocative): Percent of 2081 terms for
which the majority class size was 3, 4, and 5.
4.2 Semantic orientation of words
We consolidate the semantic orientation (polarity)
annotations in a manner identical to the process for
emotion annotations. Table 7 lists the percent of
2081 target terms assigned a majority class of no,
weak, moderate, and strong semantic orientation.
For example, it tells us that 16% of the target terms
are strongly negative. The last row in the table lists
the percent of target terms that have some semantic
orientation (positive or negative) at the various in-
tensity levels. Observe that 35% of the target terms
are strongly evaluative (positively or negatively).
Just as in the case for emotions, practical NLP ap-
plications often require only two levels of seman-
tic orientation?having particular semantic orienta-
tion or not (evaluative) or not (non-evaluative). For
each target term?emotion pair, we convert the four-
level semantic orientation annotations into two-level
ones, just as we did for the emotions. Table 8 gives
Intensity
Polarity no weak moderate strong
negative 60.8 10.8 12.3 16.0
positive 48.3 11.7 20.7 19.0
micro average 54.6 11.3 16.5 17.5
any polarity 14.7 17.4 32.7 35.0
Table 7: Percent of 2081 terms assigned a majority class
of no, weak, moderate, and strong polarity.
Polarity % of terms
negative 31.3
positive 45.5
micro average 38.4
any polarity 76.1
Table 8: Percent of 2081 target terms that are evaluative.
percent of target terms considered to be evaluative.
The last row in the table gives the percentage of
terms evaluative with respect to some semantic ori-
entation (positive or negative). Table 9 shows how
many terms in each category are positively and neg-
atively evaluative.
4.2.1 Analysis and discussion
Observe in Table 9 that, across the board, a sizable
number of terms are evaluative with respect to some
semantic orientation. Interestingly unigram nouns
have a markedly lower proportion of negative terms,
and a much higher proportion of positive terms. It
may be argued that the default semantic orientation
of noun concepts is positive, and that usually it takes
a negative adjective to make the phrase negative.
The EmoLexGI rows in the two tables show that
words marked as having a negative semantic orien-
tation in the General Inquirer are mostly marked as
negative by the Turkers. And similarly, the positives
in GI are annotated as positive. Again, this is con-
firmation that the quality of annotation obtained is
high. The EmoLexWAL rows show that anger, dis-
gust, fear, and sadness terms tend not to have a posi-
tive semantic orientation and are mostly negative. In
contrast, and expectedly, the joy terms are positive.
The surprise terms are more than twice as likely to
be positive than negative.
4.2.2 Agreement
In order to analyze how often the annotators agreed
with each other, for each term?emotion pair, we cal-
32
EmoLex negative positive any
EmoLexUni:
adjectives 33 55 87
adverbs 29 54 82
nouns 6 44 51
verbs 22 41 62
EmoLexBi:
adjectives 30 48 78
adverbs 10 52 61
nouns 13 49 61
verbs 12 57 68
EmoLexGI:
negatives in GI 90 2 92
positives in GI 2 91 91
EmoLexWAL:
anger terms in WAL 96 0 96
disgust terms in WAL 96 0 96
fear terms in WAL 87 3 89
joy terms in WAL 4 92 96
sadness terms in WAL 90 1 91
surprise terms in WAL 23 57 81
Table 9: Percent of terms, in each target set, that are eval-
uative. The highest individual polarity EmoLexGI row
scores are shown bold. Observe that the positive GI terms
are marked mostly as positively evaluative and the nega-
tive terms are marked mostly as negatively evaluative.
culated the percentage of times the majority class
has size 5 (all Turkers agree), size 4 (all but one
agree), size 3, and size 2. Table 10 presents these
agreement values. Observe that for more than 50%
of the terms, at least four annotators agree with each
other. Table 11 gives agreement values at the two-
intensity level. Observe that for more than 50% of
the terms, all five annotators agree with each other,
and for more than 80% of the terms, at least four
annotators agree.
5 Conclusions
We showed how Mechanical Turk can be used to
create a high-quality, moderate-sized, emotion lex-
icon for a very small cost (less than US$500). No-
tably, we used automatically generated word choice
questions to detect and reject erroneous annotations
and to reject all annotations by unqualified Turkers
and those who indulge in malicious data entry. We
compared a subset of our lexicon with existing gold
standard data to show that the annotations obtained
are indeed of high quality. A detailed analysis of the
Majority class size
Polarity two three four five
negative 11.8 28.7 29.4 29.8
positive 21.2 30.7 19.0 28.8
micro average 16.5 29.7 24.2 29.3
Table 10: Agreement at four intensity levels for polarity
(no, weak, moderate, and strong): Percent of 2081 terms
for which the majority class size was 2, 3, 4, and 5.
Majority class size
Polarity three four five
negative 11.8 21.2 66.9
positive 23.1 26.3 50.5
micro average 17.5 23.8 58.7
Table 11: Agreement at two intensity levels for polarity
(evaluative and non-evaluative): Percent of 2081 terms
for which the majority class size was 3, 4, and 5.
lexicon revealed insights into how prevalent emotion
bearing terms are among common unigrams and bi-
grams. We also identified which emotions tend to be
evoked simultaneously by the same term. The lexi-
con is available for free download.4
Since this pilot experiment with about 2000 target
terms was successful, we will now obtain emotion
annotations for tens of thousands of English terms.
We will use the emotion lexicon to identify emo-
tional tone of larger units of text, such as newspaper
headlines and blog posts. We will also use it to eval-
uate automatically generated lexicons, such as the
polarity lexicons by Turney and Littman (2003) and
Mohammad et al (2009). We will explore the vari-
ance in emotion evoked by near-synonyms, and also
how common it is for words with many meanings to
evoke different emotions in different senses.
Acknowledgments
This research was funded by the National research
Council Canada (NRC). Thanks to Diana Inkpen
and Diman Ghazi for early discussions on emotion.
Thanks to Joel Martin for encouragement and sup-
port. Thanks to Norm Vinson and the Ethics Com-
mittee at NRC for examining, guiding, and approv-
ing the survey. And last but not least, thanks to the
more than 1000 anonymous people who answered
the emotion survey with diligence and care.
4http://www.purl.org/net/emolex
33
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
2005. Emotions from text: Machine learning for
text-based emotion prediction. In Proceedings of the
Joint Conference on Human Language Technology
/ Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 579?586, Vancouver,
Canada.
J.R.L. Bernard, editor. 1986. The Macquarie Thesaurus.
Macquarie Library, Sydney, Australia.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium.
Chris Callison-Burch. 2009. Fast, cheap and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009), pages 286?295, Singapore.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169?200.
Clark Elliott. 1992. The affective reasoner: A process
model of emotions in a multi-agent system. Ph.D. the-
sis, Institute for the Learning Sciences, Northwestern
University.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493, Acapulco, Mexico.
A. Lobanova, T. van der Kleij, and J. Spenader. 2010.
Defining antonymy: A corpus-based study of oppo-
sites by lexico-syntactic patterns. International Jour-
nal of Lexicography (in press), 23:19?53.
Saif Mohammad, Bonnie Dorr, and Codie Dunn. 2008.
Computing word-pair antonymy. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2008), pages 982?991,
Waikiki, Hawaii.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP-2009), pages 599?608,
Singapore.
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru
Ishizuka. 2009. Compositionality principle in recog-
nition of fine-grained emotions from text. In Proceed-
ings of the Proceedings of the Third International Con-
ference on Weblogs and Social Media (ICWSM-09),
pages 278?281, San Jose, California.
R Plutchik. 1980. A general psychoevolutionary theory
of emotion. Emotion: Theory, research, and experi-
ence, 1(3):3?33.
Jonathon Read. 2004. Recognising affect in text using
pointwise-mutual information. Ph.D. thesis, Depart-
ment of Informatics, University of Sussex.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast - but is it good? Evalu-
ating nonexpert annotations for natural language tasks.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-2008),
pages 254?263, Waikiki, Hawaii.
Philip Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and associates. 1966. The General
Inquirer: A Computer Approach to Content Analysis.
The MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-Affect: An affective extension of WordNet.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (LREC-2004),
pages 1083?1086, Lisbon, Portugal.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21(4):315?346.
34
