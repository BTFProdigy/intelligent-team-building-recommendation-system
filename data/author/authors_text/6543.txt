Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 858?867, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Language Models in Machine Translation
Thorsten Brants Ashok C. Popat Peng Xu Franz J. Och Jeffrey Dean
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{brants,popat,xp,och,jeff}@google.com
Abstract
This paper reports on the benefits of large-
scale statistical language modeling in ma-
chine translation. A distributed infrastruc-
ture is proposed which we use to train on
up to 2 trillion tokens, resulting in language
models having up to 300 billion n-grams. It
is capable of providing smoothed probabil-
ities for fast, single-pass decoding. We in-
troduce a new smoothing method, dubbed
Stupid Backoff, that is inexpensive to train
on large data sets and approaches the quality
of Kneser-Ney Smoothing as the amount of
training data increases.
1 Introduction
Given a source-language (e.g., French) sentence f ,
the problem of machine translation is to automati-
cally produce a target-language (e.g., English) trans-
lation e?. The mathematics of the problem were for-
malized by (Brown et al, 1993), and re-formulated
by (Och and Ney, 2004) in terms of the optimization
e? = arg max
e
M
?
m=1
?mhm(e, f) (1)
where {hm(e, f)} is a set of M feature functions and
{?m} a set of weights. One or more feature func-
tions may be of the form h(e, f) = h(e), in which
case it is referred to as a language model.
We focus on n-gram language models, which are
trained on unlabeled monolingual text. As a general
rule, more data tends to yield better language mod-
els. Questions that arise in this context include: (1)
How might one build a language model that allows
scaling to very large amounts of training data? (2)
How much does translation performance improve as
the size of the language model increases? (3) Is there
a point of diminishing returns in performance as a
function of language model size?
This paper proposes one possible answer to the
first question, explores the second by providing
learning curves in the context of a particular statis-
tical machine translation system, and hints that the
third may yet be some time in answering. In particu-
lar, it proposes a distributed language model training
and deployment infrastructure, which allows direct
and efficient integration into the hypothesis-search
algorithm rather than a follow-on re-scoring phase.
While it is generally recognized that two-pass de-
coding can be very effective in practice, single-pass
decoding remains conceptually attractive because it
eliminates a source of potential information loss.
2 N -gram Language Models
Traditionally, statistical language models have been
designed to assign probabilities to strings of words
(or tokens, which may include punctuation, etc.).
Let wL1 = (w1, . . . , wL) denote a string of L tokens
over a fixed vocabulary. An n-gram language model
assigns a probability to wL1 according to
P (wL1 ) =
L
?
i=1
P (wi|wi?11 ) ?
L
?
i=1
P? (wi|wi?1i?n+1)
(2)
where the approximation reflects a Markov assump-
tion that only the most recent n ? 1 tokens are rele-
vant when predicting the next word.
858
For any substring wji of wL1 , let f(w
j
i ) denote the
frequency of occurrence of that substring in another
given, fixed, usually very long target-language string
called the training data. The maximum-likelihood
(ML) probability estimates for the n-grams are given
by their relative frequencies
r(wi|wi?1i?n+1) =
f(wii?n+1)
f(wi?1i?n+1)
. (3)
While intuitively appealing, Eq. (3) is problematic
because the denominator and / or numerator might
be zero, leading to inaccurate or undefined probabil-
ity estimates. This is termed the sparse data prob-
lem. For this reason, the ML estimate must be mod-
ified for use in practice; see (Goodman, 2001) for a
discussion of n-gram models and smoothing.
In principle, the predictive accuracy of the lan-
guage model can be improved by increasing the or-
der of the n-gram. However, doing so further exac-
erbates the sparse data problem. The present work
addresses the challenges of processing an amount
of training data sufficient for higher-order n-gram
models and of storing and managing the resulting
values for efficient use by the decoder.
3 Related Work on Distributed Language
Models
The topic of large, distributed language models is
relatively new. Recently a two-pass approach has
been proposed (Zhang et al, 2006), wherein a lower-
order n-gram is used in a hypothesis-generation
phase, then later the K-best of these hypotheses are
re-scored using a large-scale distributed language
model. The resulting translation performance was
shown to improve appreciably over the hypothesis
deemed best by the first-stage system. The amount
of data used was 3 billion words.
More recently, a large-scale distributed language
model has been proposed in the contexts of speech
recognition and machine translation (Emami et al,
2007). The underlying architecture is similar to
(Zhang et al, 2006). The difference is that they in-
tegrate the distributed language model into their ma-
chine translation decoder. However, they don?t re-
port details of the integration or the efficiency of the
approach. The largest amount of data used in the
experiments is 4 billion words.
Both approaches differ from ours in that they store
corpora in suffix arrays, one sub-corpus per worker,
and serve raw counts. This implies that all work-
ers need to be contacted for each n-gram request.
In our approach, smoothed probabilities are stored
and served, resulting in exactly one worker being
contacted per n-gram for simple smoothing tech-
niques, and in exactly two workers for smoothing
techniques that require context-dependent backoff.
Furthermore, suffix arrays require on the order of 8
bytes per token. Directly storing 5-grams is more
efficient (see Section 7.2) and allows applying count
cutoffs, further reducing the size of the model.
4 Stupid Backoff
State-of-the-art smoothing uses variations of con-
text-dependent backoff with the following scheme:
P (wi|wi?1i?k+1) =
{
?(wii?k+1) if (wii?k+1) is found
?(wi?1i?k+1)P (wii?k+2) otherwise
(4)
where ?(?) are pre-computed and stored probabili-
ties, and ?(?) are back-off weights. As examples,
Kneser-Ney Smoothing (Kneser and Ney, 1995),
Katz Backoff (Katz, 1987) and linear interpola-
tion (Jelinek and Mercer, 1980) can be expressed in
this scheme (Chen and Goodman, 1998). The recur-
sion ends at either unigrams or at the uniform distri-
bution for zero-grams.
We introduce a similar but simpler scheme,
named Stupid Backoff 1 , that does not generate nor-
malized probabilities. The main difference is that
we don?t apply any discounting and instead directly
use the relative frequencies (S is used instead of
P to emphasize that these are not probabilities but
scores):
S(wi|wi?1i?k+1) =
?
?
?
?
?
f(wii?k+1)
f(wi?1i?k+1)
if f(wii?k+1) > 0
?S(wi|wi?1i?k+2) otherwise
(5)
1The name originated at a time when we thought that such
a simple scheme cannot possibly be good. Our view of the
scheme changed, but the name stuck.
859
In general, the backoff factor ? may be made to de-
pend on k. Here, a single value is used and heuris-
tically set to ? = 0.4 in all our experiments2 . The
recursion ends at unigrams:
S(wi) =
f(wi)
N (6)
with N being the size of the training corpus.
Stupid Backoff is inexpensive to calculate in a dis-
tributed environment while approaching the quality
of Kneser-Ney smoothing for large amounts of data.
The lack of normalization in Eq. (5) does not affect
the functioning of the language model in the present
setting, as Eq. (1) depends on relative rather than ab-
solute feature-function values.
5 Distributed Training
We use the MapReduce programming model (Dean
and Ghemawat, 2004) to train on terabytes of data
and to generate terabytes of language models. In this
programming model, a user-specified map function
processes an input key/value pair to generate a set of
intermediate key/value pairs, and a reduce function
aggregates all intermediate values associated with
the same key. Typically, multiple map tasks oper-
ate independently on different machines and on dif-
ferent parts of the input data. Similarly, multiple re-
duce tasks operate independently on a fraction of the
intermediate data, which is partitioned according to
the intermediate keys to ensure that the same reducer
sees all values for a given key. For additional details,
such as communication among machines, data struc-
tures and application examples, the reader is referred
to (Dean and Ghemawat, 2004).
Our system generates language models in three
main steps, as described in the following sections.
5.1 Vocabulary Generation
Vocabulary generation determines a mapping of
terms to integer IDs, so n-grams can be stored us-
ing IDs. This allows better compression than the
original terms. We assign IDs according to term fre-
quency, with frequent terms receiving small IDs for
efficient variable-length encoding. All words that
2The value of 0.4 was chosen empirically based on good
results in earlier experiments. Using multiple values depending
on the n-gram order slightly improves results.
occur less often than a pre-determined threshold are
mapped to a special id marking the unknown word.
The vocabulary generation map function reads
training text as input. Keys are irrelevant; values are
text. It emits intermediate data where keys are terms
and values are their counts in the current section
of the text. A sharding function determines which
shard (chunk of data in the MapReduce framework)
the pair is sent to. This ensures that all pairs with
the same key are sent to the same shard. The re-
duce function receives all pairs that share the same
key and sums up the counts. Simplified, the map,
sharding and reduce functions do the following:
Map(string key, string value) {
// key=docid, ignored; value=document
array words = Tokenize(value);
hash_map<string, int> histo;
for i = 1 .. #words
histo[words[i]]++;
for iter in histo
Emit(iter.first, iter.second);
}
int ShardForKey(string key, int nshards) {
return Hash(key) % nshards;
}
Reduce(string key, iterator values) {
// key=term; values=counts
int sum = 0;
for each v in values
sum += ParseInt(v);
Emit(AsString(sum));
}
Note that the Reduce function emits only the aggre-
gated value. The output key is the same as the inter-
mediate key and automatically written by MapRe-
duce. The computation of counts in the map func-
tion is a minor optimization over the alternative of
simply emitting a count of one for each tokenized
word in the array. Figure 1 shows an example for
3 input documents and 2 reduce shards. Which re-
ducer a particular term is sent to is determined by a
hash function, indicated by text color. The exact par-
titioning of the keys is irrelevant; important is that all
pairs with the same key are sent to the same reducer.
5.2 Generation of n-Grams
The process of n-gram generation is similar to vo-
cabulary generation. The main differences are that
now words are converted to IDs, and we emit n-
grams up to some maximum order instead of single
860
Figure 1: Distributed vocabulary generation.
words. A simplified map function does the follow-
ing:
Map(string key, string value) {
// key=docid, ignored; value=document
array ids = ToIds(Tokenize(value));
for i = 1 .. #ids
for j = 0 .. maxorder-1
Emit(ids[i-j .. i], "1");
}
Again, one may optimize the Map function by first
aggregating counts over some section of the data and
then emit the aggregated counts instead of emitting
?1? each time an n-gram is encountered.
The reduce function is the same as for vocabu-
lary generation. The subsequent step of language
model generation will calculate relative frequencies
r(wi|wi?1i?k+1) (see Eq. 3). In order to make that step
efficient we use a sharding function that places the
values needed for the numerator and denominator
into the same shard.
Computing a hash function on just the first words
of n-grams achieves this goal. The required n-
grams wii?n+1 and wi?1i?n+1 always share the same
first word wi?n+1, except for unigrams. For that we
need to communicate the total count N to all shards.
Unfortunately, sharding based on the first word
only may make the shards very imbalanced. Some
terms can be found at the beginning of a huge num-
ber of n-grams, e.g. stopwords, some punctuation
marks, or the beginning-of-sentence marker. As an
example, the shard receiving n-grams starting with
the beginning-of-sentence marker tends to be several
times the average size. Making the shards evenly
sized is desirable because the total runtime of the
process is determined by the largest shard.
The shards are made more balanced by hashing
based on the first two words:
int ShardForKey(string key, int nshards) {
string prefix = FirstTwoWords(key);
return Hash(prefix) % nshards;
}
This requires redundantly storing unigram counts in
all shards in order to be able to calculate relative fre-
quencies within shards. That is a relatively small
amount of information (a few million entries, com-
pared to up to hundreds of billions of n-grams).
5.3 Language Model Generation
The input to the language model generation step is
the output of the n-gram generation step: n-grams
and their counts. All information necessary to calcu-
late relative frequencies is available within individ-
ual shards because of the sharding function. That is
everything we need to generate models with Stupid
Backoff. More complex smoothing methods require
additional steps (see below).
Backoff operations are needed when the full n-
gram is not found. If r(wi|wi?1i?n+1) is not found,
then we will successively look for r(wi|wi?1i?n+2),
r(wi|wi?1i?n+3), etc. The language model generation
step shards n-grams on their last two words (with
unigrams duplicated), so all backoff operations can
be done within the same shard (note that the required
n-grams all share the same last word wi).
5.4 Other Smoothing Methods
State-of-the-art techniques like Kneser-Ney
Smoothing or Katz Backoff require additional,
more expensive steps. At runtime, the client needs
to additionally request up to 4 backoff factors for
each 5-gram requested from the servers, thereby
multiplying network traffic. We are not aware of
a method that always stores the history backoff
factors on the same shard as the longer n-gram
without duplicating a large fraction of the entries.
This means one needs to contact two shards per
n-gram instead of just one for Stupid Backoff.
Training requires additional iterations over the data.
861
Step 0 Step 1 Step 2
context counting unsmoothed probs and interpol. weights interpolated probabilities
Input key wii?n+1 (same as Step 0 output) (same as Step 1 output)
Input value f(wii?n+1) (same as Step 0 output) (same as Step 1 output)
Intermediate key wii?n+1 wi?1i?n+1 wi?n+1i
Sharding wii?n+1 wi?1i?n+1 w
i?n+2
i?n+1 , unigrams duplicated
Intermediate value fKN (wii?n+1) wi,fKN (wii?n+1)
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1)
Output value fKN (wii?n+1) wi,
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1) PKN (wi|wi?1i?n+1), ?(wi?1i?n+1)
Table 1: Extra steps needed for training Interpolated Kneser-Ney Smoothing
Kneser-Ney Smoothing counts lower-order n-
grams differently. Instead of the frequency of the
(n? 1)-gram, it uses the number of unique single
word contexts the (n?1)-gram appears in. We use
fKN(?) to jointly denote original frequencies for the
highest order and context counts for lower orders.
After the n-gram counting step, we process the n-
grams again to produce these quantities. This can
be done similarly to the n-gram counting using a
MapReduce (Step 0 in Table 1).
The most commonly used variant of Kneser-Ney
smoothing is interpolated Kneser-Ney smoothing,
defined recursively as (Chen and Goodman, 1998):
PKN (wi|wi?1i?n+1) =
max(fKN(wii?n+1) ? D, 0)
fKN(wi?1i?n+1)
+ ?(wi?1i?n+1)PKN (wi|wi?1i?n+2),
where D is a discount constant and {?(wi?1i?n+1)} are
interpolation weights that ensure probabilities sum
to one. Two additional major MapReduces are re-
quired to compute these values efficiently. Table 1
describes their input, intermediate and output keys
and values. Note that output keys are always the
same as intermediate keys.
The map function of MapReduce 1 emits n-gram
histories as intermediate keys, so the reduce func-
tion gets all n-grams with the same history at the
same time, generating unsmoothed probabilities and
interpolation weights. MapReduce 2 computes the
interpolation. Its map function emits reversed n-
grams as intermediate keys (hence we use wi?n+1i
in the table). All unigrams are duplicated in ev-
ery reduce shard. Because the reducer function re-
ceives intermediate keys in sorted order it can com-
pute smoothed probabilities for all n-gram orders
with simple book-keeping.
Katz Backoff requires similar additional steps.
The largest models reported here with Kneser-Ney
Smoothing were trained on 31 billion tokens. For
Stupid Backoff, we were able to use more than 60
times of that amount.
6 Distributed Application
Our goal is to use distributed language models in-
tegrated into the first pass of a decoder. This may
yield better results than n-best list or lattice rescor-
ing (Ney and Ortmanns, 1999). Doing that for lan-
guage models that reside in the same machine as the
decoder is straight-forward. The decoder accesses
n-grams whenever necessary. This is inefficient in a
distributed system because network latency causes a
constant overhead on the order of milliseconds. On-
board memory is around 10,000 times faster.
We therefore implemented a new decoder archi-
tecture. The decoder first queues some number of
requests, e.g. 1,000 or 10,000 n-grams, and then
sends them together to the servers, thereby exploit-
ing the fact that network requests with large numbers
of n-grams take roughly the same time to complete
as requests with single n-grams.
The n-best search of our machine translation de-
coder proceeds as follows. It maintains a graph of
the search space up to some point. It then extends
each hypothesis by advancing one word position in
the source language, resulting in a candidate exten-
sion of the hypothesis of zero, one, or more addi-
tional target-language words (accounting for the fact
that variable-length source-language fragments can
correspond to variable-length target-language frag-
ments). In a traditional setting with a local language
model, the decoder immediately obtains the nec-
essary probabilities and then (together with scores
862
Figure 2: Illustration of decoder graph and batch-
querying of the language model.
from other features) decides which hypotheses to
keep in the search graph. When using a distributed
language model, the decoder first tentatively extends
all current hypotheses, taking note of which n-grams
are required to score them. These are queued up for
transmission as a batch request. When the scores are
returned, the decoder re-visits all of these tentative
hypotheses, assigns scores, and re-prunes the search
graph. It is then ready for the next round of exten-
sions, again involving queuing the n-grams, waiting
for the servers, and pruning.
The process is illustrated in Figure 2 assuming a
trigram model and a decoder policy of pruning to
the four most promising hypotheses. The four ac-
tive hypotheses (indicated by black disks) at time t
are: There is, There may, There are, and There were.
The decoder extends these to form eight new nodes
at time t + 1. Note that one of the arcs is labeled ,
indicating that no target-language word was gener-
ated when the source-language word was consumed.
The n-grams necessary to score these eight hypothe-
ses are There is lots, There is many, There may be,
There are lots, are lots of, etc. These are queued up
and their language-model scores requested in a batch
manner. After scoring, the decoder prunes this set as
indicated by the four black disks at time t + 1, then
extends these to form five new nodes (one is shared)
at time t + 2. The n-grams necessary to score these
hypotheses are lots of people, lots of reasons, There
are onlookers, etc. Again, these are sent to the server
together, and again after scoring the graph is pruned
to four active (most promising) hypotheses.
The alternating processes of queuing, waiting and
scoring/pruning are done once per word position in
a source sentence. The average sentence length in
our test data is 22 words (see section 7.1), thus we
have 23 rounds3 per sentence on average. The num-
ber of n-grams requested per sentence depends on
the decoder settings for beam size, re-ordering win-
dow, etc. As an example for larger runs reported in
the experiments section, we typically request around
150,000 n-grams per sentence. The average net-
work latency per batch is 35 milliseconds, yield-
ing a total latency of 0.8 seconds caused by the dis-
tributed language model for an average sentence of
22 words. If a slight reduction in translation qual-
ity is allowed, then the average network latency per
batch can be brought down to 7 milliseconds by re-
ducing the number of n-grams requested per sen-
tence to around 10,000. As a result, our system can
efficiently use the large distributed language model
at decoding time. There is no need for a second pass
nor for n-best list rescoring.
We focused on machine translation when describ-
ing the queued language model access. However,
it is general enough that it may also be applicable
to speech decoders and optical character recognition
systems.
7 Experiments
We trained 5-gram language models on amounts of
text varying from 13 million to 2 trillion tokens.
The data is divided into four sets; language mod-
els are trained for each set separately4 . For each
training data size, we report the size of the result-
ing language model, the fraction of 5-grams from
the test data that is present in the language model,
and the BLEU score (Papineni et al, 2002) obtained
by the machine translation system. For smaller train-
ing sizes, we have also computed test-set perplexity
using Kneser-Ney Smoothing, and report it for com-
parison.
7.1 Data Sets
We compiled four language model training data sets,
listed in order of increasing size:
3One additional round for the sentence end marker.
4Experience has shown that using multiple, separately
trained language models as feature functions in Eq (1) yields
better results than using a single model trained on all data.
863
 1e+07
 1e+08
 1e+09
 1e+10
 1e+11
 1e+12
 10  100  1000  10000  100000  1e+06
 0.1
 1
 10
 100
 1000
N
um
be
r o
f n
-g
ra
m
s
Ap
pr
ox
. L
M
 s
ize
 in
 G
B
LM training data size in million tokens
x1.8/x2
x1.8/x2
x1.8/x2
x1.6/x2
target
+ldcnews
+webnews
+web
Figure 3: Number of n-grams (sum of unigrams to
5-grams) for varying amounts of training data.
target: The English side of Arabic-English parallel
data provided by LDC5 (237 million tokens).
ldcnews: This is a concatenation of several English
news data sets provided by LDC6 (5 billion tokens).
webnews: Data collected over several years, up to
December 2005, from web pages containing pre-
dominantly English news articles (31 billion to-
kens).
web: General web data, which was collected in Jan-
uary 2006 (2 trillion tokens).
For testing we use the ?NIST? part of the 2006
Arabic-English NIST MT evaluation set, which is
not included in the training data listed above7. It
consists of 1797 sentences of newswire, broadcast
news and newsgroup texts with 4 reference transla-
tions each. The test set is used to calculate transla-
tion BLEU scores. The English side of the set is also
used to calculate perplexities and n-gram coverage.
7.2 Size of the Language Models
We measure the size of language models in total
number of n-grams, summed over all orders from
1 to 5. There is no frequency cutoff on the n-grams.
5http://www.nist.gov/speech/tests/mt/doc/
LDCLicense-mt06.pdf contains a list of parallel resources
provided by LDC.
6The bigger sets included are LDC2005T12 (Gigaword,
2.5B tokens), LDC93T3A (Tipster, 500M tokens) and
LDC2002T31 (Acquaint, 400M tokens), plus many smaller
sets.
7The test data was generated after 1-Feb-2006; all training
data was generated before that date.
target webnews web
# tokens 237M 31G 1.8T
vocab size 200k 5M 16M
# n-grams 257M 21G 300G
LM size (SB) 2G 89G 1.8T
time (SB) 20 min 8 hours 1 day
time (KN) 2.5 hours 2 days ?
# machines 100 400 1500
Table 2: Sizes and approximate training times for
3 language models with Stupid Backoff (SB) and
Kneser-Ney Smoothing (KN).
There is, however, a frequency cutoff on the vocab-
ulary. The minimum frequency for a term to be in-
cluded in the vocabulary is 2 for the target, ldcnews
and webnews data sets, and 200 for the web data set.
All terms below the threshold are mapped to a spe-
cial term UNK, representing the unknown word.
Figure 3 shows the number of n-grams for lan-
guage models trained on 13 million to 2 trillion to-
kens. Both axes are on a logarithmic scale. The
right scale shows the approximate size of the served
language models in gigabytes. The numbers above
the lines indicate the relative increase in language
model size: x1.8/x2 means that the number of n-
grams grows by a factor of 1.8 each time we double
the amount of training data. The values are simi-
lar across all data sets and data sizes, ranging from
1.6 to 1.8. The plots are very close to straight lines
in the log/log space; linear least-squares regression
finds r2 > 0.99 for all four data sets.
The web data set has the smallest relative increase.
This can be at least partially explained by the higher
vocabulary cutoff. The largest language model gen-
erated contains approx. 300 billion n-grams.
Table 2 shows sizes and approximate training
times when training on the full target, webnews, and
web data sets. The processes run on standard current
hardware with the Linux operating system. Gen-
erating models with Kneser-Ney Smoothing takes
6 ? 7 times longer than generating models with
Stupid Backoff. We deemed generation of Kneser-
Ney models on the web data as too expensive and
therefore excluded it from our experiments. The es-
timated runtime for that is approximately one week
on 1500 machines.
864
 50
 100
 150
 200
 250
 300
 350
 10  100  1000  10000  100000  1e+06
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
Pe
rp
le
xit
y
Fr
ac
tio
n 
of
 c
ov
er
ed
 5
-g
ra
m
s
LM training data size in million tokens
+.022/x2
+.035/x2
+.038/x2
+.026/x2
target KN PP
ldcnews KN PP
webnews KN PP
target C5
+ldcnews C5
+webnews C5
+web C5
Figure 4: Perplexities with Kneser-Ney Smoothing
(KN PP) and fraction of covered 5-grams (C5).
7.3 Perplexity and n-Gram Coverage
A standard measure for language model quality is
perplexity. It is measured on test data T = w|T |1 :
PP (T ) = e
? 1|T |
|T |
 
i=1
log p(wi|wi?1i?n+1) (7)
This is the inverse of the average conditional prob-
ability of a next word; lower perplexities are bet-
ter. Figure 4 shows perplexities for models with
Kneser-Ney smoothing. Values range from 280.96
for 13 million to 222.98 for 237 million tokens tar-
get data and drop nearly linearly with data size (r2 =
0.998). Perplexities for ldcnews range from 351.97
to 210.93 and are also close to linear (r2 = 0.987),
while those for webnews data range from 221.85 to
164.15 and flatten out near the end. Perplexities are
generally high and may be explained by the mix-
ture of genres in the test data (newswire, broadcast
news, newsgroups) while our training data is pre-
dominantly written news articles. Other held-out
sets consisting predominantly of newswire texts re-
ceive lower perplexities by the same language mod-
els, e.g., using the full ldcnews model we find per-
plexities of 143.91 for the NIST MT 2005 evaluation
set, and 149.95 for the NIST MT 2004 set.
Note that the perplexities of the different language
models are not directly comparable because they use
different vocabularies. We used a fixed frequency
cutoff, which leads to larger vocabularies as the
training data grows. Perplexities tend to be higher
with larger vocabularies.
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 10  100  1000  10000  100000  1e+06
Te
st
 d
at
a 
BL
EU
LM training data size in million tokens
+0.62BP/x2
+0.56BP/x2
+0.51BP/x2
+0.66BP/x2
+0.70BP/x2
+0.39BP/x2
+0.15BP/x2
target KN
+ldcnews KN
+webnews KN
target SB
+ldcnews SB
+webnews SB
+web SB
Figure 5: BLEU scores for varying amounts of data
using Kneser-Ney (KN) and Stupid Backoff (SB).
Perplexities cannot be calculated for language
models with Stupid Backoff because their scores are
not normalized probabilities. In order to neverthe-
less get an indication of potential quality improve-
ments with increased training sizes we looked at the
5-gram coverage instead. This is the fraction of 5-
grams in the test data set that can be found in the
language model training data. A higher coverage
will result in a better language model if (as we hy-
pothesize) estimates for seen events tend to be bet-
ter than estimates for unseen events. This fraction
grows from 0.06 for 13 million tokens to 0.56 for 2
trillion tokens, meaning 56% of all 5-grams in the
test data are known to the language model.
Increase in coverage depends on the training data
set. Within each set, we observe an almost constant
growth (correlation r2 ? 0.989 for all sets) with
each doubling of the training data as indicated by
numbers next to the lines. The fastest growth oc-
curs for webnews data (+0.038 for each doubling),
the slowest growth for target data (+0.022/x2).
7.4 Machine Translation Results
We use a state-of-the-art machine translation system
for translating from Arabic to English that achieved
a competitive BLEU score of 0.4535 on the Arabic-
English NIST subset in the 2006 NIST machine
translation evaluation8 . Beam size and re-ordering
window were reduced in order to facilitate a large
8See http://www.nist.gov/speech/tests/mt/
mt06eval official results.html for more results.
865
number of experiments. Additionally, our NIST
evaluation system used a mixture of 5, 6, and 7-gram
models with optimized stupid backoff factors for
each order, while the learning curve presented here
uses a fixed order of 5 and a single fixed backoff fac-
tor. Together, these modifications reduce the BLEU
score by 1.49 BLEU points (BP)9 at the largest train-
ing size. We then varied the amount of language
model training data from 13 million to 2 trillion to-
kens. All other parts of the system are kept the same.
Results are shown in Figure 5. The first part
of the curve uses target data for training the lan-
guage model. With Kneser-Ney smoothing (KN),
the BLEU score improves from 0.3559 for 13 mil-
lion tokens to 0.3832 for 237 million tokens. At
such data sizes, Stupid Backoff (SB) with a constant
backoff parameter ? = 0.4 is around 1 BP worse
than KN. On average, one gains 0.62 BP for each
doubling of the training data with KN, and 0.66 BP
per doubling with SB. Differences of more than 0.51
BP are statistically significant at the 0.05 level using
bootstrap resampling (Noreen, 1989; Koehn, 2004).
We then add a second language model using ldc-
news data. The first point for ldcnews shows a large
improvement of around 1.4 BP over the last point
for target for both KN and SB, which is approxi-
mately twice the improvement expected from dou-
bling the amount of data. This seems to be caused
by adding a new domain and combining two models.
After that, we find an improvement of 0.56?0.70 BP
for each doubling of the ldcnews data. The gap be-
tween Kneser-Ney Smoothing and Stupid Backoff
narrows, starting with a difference of 0.85 BP and
ending with a not significant difference of 0.24 BP.
Adding a third language models based on web-
news data does not show a jump at the start of the
curve. We see, however, steady increases of 0.39?
0.51 BP per doubling. The gap between Kneser-Ney
and Stupid Backoff is gone, all results with Stupid
Backoff are actually better than Kneser-Ney, but the
differences are not significant.
We then add a fourth language model based on
web data and Stupid Backoff. Generating Kneser-
Ney models for these data sizes is extremely ex-
pensive and is therefore omitted. The fourth model
91 BP = 0.01 BLEU. We show system scores as BLEU, dif-
ferences as BP.
shows a small but steady increase of 0.15 BP per
doubling, surpassing the best Kneser-Ney model
(trained on less data) by 0.82 BP at the largest
size. Goodman (2001) observed that Kneser-Ney
Smoothing dominates other schemes over a broad
range of conditions. Our experiments confirm this
advantage at smaller language model sizes, but show
the advantage disappears at larger data sizes.
The amount of benefit from doubling the training
size is partly determined by the domains of the data
sets10. The improvements are almost linear on the
log scale within the sets. Linear least-squares regres-
sion shows correlations r2 > 0.96 for all sets and
both smoothing methods, thus we expect to see sim-
ilar improvements when further increasing the sizes.
8 Conclusion
A distributed infrastructure has been described to
train and apply large-scale language models to ma-
chine translation. Experimental results were pre-
sented showing the effect of increasing the amount
of training data to up to 2 trillion tokens, resulting
in a 5-gram language model size of up to 300 billion
n-grams. This represents a gain of about two orders
of magnitude in the amount of training data that can
be handled over that reported previously in the liter-
ature (or three-to-four orders of magnitude, if one
considers only single-pass decoding). The infra-
structure is capable of scaling to larger amounts of
training data and higher n-gram orders.
The technique is made efficient by judicious
batching of score requests by the decoder in a server-
client architecture. A new, simple smoothing tech-
nique well-suited to distributed computation was
proposed, and shown to perform as well as more
sophisticated methods as the size of the language
model increases.
Significantly, we found that translation quality as
indicated by BLEU score continues to improve with
increasing language model size, at even the largest
sizes considered. This finding underscores the value
of being able to train and apply very large language
models, and suggests that further performance gains
may be had by pursuing this direction further.
10There is also an effect of the order in which we add the
models. As an example, web data yields +0.43 BP/x2 when
added as the second model. A discussion of this effect is omit-
ted due to space limitations.
866
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard, Cambridge,
MA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation (OSDI-04), San Francisco, CA, USA.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP-2007, Honolulu, HI, USA.
Joshua Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research, Redmond, WA, USA.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Pattern Recognition in Practice, pages
381?397. North Holland.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP-04, Barcelona, Spain.
Hermann Ney and Stefan Ortmanns. 1999. Dynamic
programming search for continuous speech recogni-
tion. IEEE Signal Processing Magazine, 16(5):64?83.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02, pages 311?318, Philadelphia, PA, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of EMNLP-2006, pages
216?223, Sydney, Australia.
867
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1101?1109,
Beijing, August 2010
Large Scale Parallel Document Mining for Machine Translation
Jakob Uszkoreit Jay M. Ponte Ashok C. Popat Moshe Dubiner
Google, Inc.
{uszkoreit,ponte,popat,moshe}@google.com
Abstract
A distributed system is described that re-
liably mines parallel text from large cor-
pora. The approach can be regarded
as cross-language near-duplicate detec-
tion, enabled by an initial, low-quality
batch translation. In contrast to other ap-
proaches which require specialized meta-
data, the system uses only the textual con-
tent of the documents. Results are pre-
sented for a corpus of over two billion web
pages and for a large collection of digi-
tized public-domain books.
1 Introduction
While the World Wide Web provides an abun-
dance of readily available monolingual text, par-
allel data is still a comparatively scarce resource,
yet plays a crucially important role in training sta-
tistical machine translation systems.
We describe an approach to mining document-
aligned parallel text to be used as training data
for a statistical machine translation system. Pre-
vious approaches have focused on rather homo-
geneous corpora and relied on metadata such as
publication dates (Munteanu and Marcu, 2005;
Munteanu and Marcu, 2006; Udupa et al, 2009;
Do et al, 2009; Abdul-Rauf and Schwenk, 2009)
or information about document structure (Resnik
and Smith, 2003; Chen and Nie, 2000). In large
and unstructured collections of documents such as
the Web, however, metadata is often sparse or un-
reliable. Our approach, in contrast, scales com-
putationally to very large and diverse collections
of documents and does not require metadata. It is
based solely on the textual contents of the input
documents.
Casting the problem as one of cross-language
near duplicate detection, we use a baseline ma-
chine translation system to translate all input doc-
uments into a single language. However, the
words and phrases that are most discriminatory
for the purposes of information retrieval and du-
plicate detection are the relatively rare ones, pre-
cisely those that are less likely to be translated
well by the baseline translation system.
Our approach to circumvent this problem and
to avoid the prohibitive quadratic computational
complexity of the naive approach of performing a
comparison of every possible pair of input docu-
ments is similar to previous work in near duplicate
detection (Broder, 2000; Henzinger, 2006; Man-
ber, 1994) and noisy data retrieval (Harding et al,
1997).
We use shingles consisting of word n-grams to
construct relatively rare features from more com-
mon, in-vocabulary words. For each input doc-
ument, we identify a comparatively small set of
candidate pairings with documents sharing at least
a certain number of such features. We then per-
form a more expensive comparison between each
document and all documents in its candidate set
using lower order n-gram features that would typ-
ically be too frequent to be used efficiently in
forming candidate pairings, but provide a higher
coverage of the scored document pairs. Another
important aspect of our approach is that it can be
implemented in a highly parallel way, as we de-
scribe in the following section.
1101
2 System Description
The input is a set of documents from diverse
sources such as web pages and digitized books.
In a first stage, all documents are independently
translated into English using a baseline statistical
machine translation system.
We then extract two different sets of n-grams
from the translated documents: matching n-grams
that are used to construct the candidate sets as well
as scoring n-grams used only in the computation
of a score for a given pair of documents. This
stage generates two indexes: a forward index list-
ing all extracted scoring n-grams, indexed by doc-
Machine translate input data
Extract n-grams
Filter inverted index
by document frequency and 
number of original languages
Generate all pairs of documents 
sharing matching n-grams
Score unique document pairs,
querying the forward Index
Discard non-symmetric pairs
Join with original input data
Evaluate on reference document 
alignments
Fold global, per-scoring n-gram 
information from inverted index 
into forward index
Documents 
in Multiple 
Languages
English 
Translations
Forward Index
Inverted Index
Per-document n-best lists
Figure 1: Architecture of the Parallel Text Mining
System.
ument; and an inverted index referencing all doc-
uments from which we extracted a given match-
ing n-gram, indexed by n-grams. The inverted
index is also used to accumulate global informa-
tion about scoring n-grams, such as their docu-
ment frequency, yet for scoring n-grams we do
not accumulate a posting list of all documents in
which they occur.
In the next step, the system generates all possi-
ble pairs of documents for each matching n-gram
posting list in the inverted index. Since we keep
only those pairs of documents that originated in
different languages, we can discard posting lists
from the inverted index that contain only a single
document, i.e. those of singleton n-grams, or only
documents in a single language.
Crucially, we further discard posting lists for
matching n-grams whose frequency exceeds a
certain threshold. When choosing a sufficiently
large order for the matching n-grams, their long-
tailed distribution causes only a small fraction of
matching n-grams to be filtered out due to fre-
quency, as we show empirically in Section 5. It
is this filtering step that causes the overall runtime
of the system to be linear in the size of the input
data and allows the system to scale to very large
document collections.
In parallel, global information about scoring n-
grams accumulated in the inverted index that is
required for pairwise scoring, such as their doc-
ument frequency, is folded into the forward in-
dex by iterating over all forward index entries, re-
questing the respective per-feature quantities from
the inverted index and storing them with each oc-
currence of a scoring n-gram in an updated for-
ward index.
In the next stage, we compute pairwise scores
for all candidate document pairs, accessing the
forward index entry of each of the two scored doc-
uments to obtain the respective scoring n-grams.
Document pairs with a score below a given thresh-
old are discarded. For each input document, this
results in one n-best list per language. In the last
step we retain only those document pairs where
each document is contained in the n-best list of
the other document for its original language. Fi-
nally we perform a join of our identified transla-
tion pairs with the original text by making another
1102
pass over the original, untranslated input data
where the contents of document pairs with suffi-
ciently high scores are then aggregated and out-
put. Document pairings involving all languages
are identified simultaneously. Each stage of the
system fits well into the MapReduce program-
ming model (Dean and Ghemawat, 2004). The
general architecture is shown in Figure 1.
2.1 Pairwise Scoring
For scoring a pair of documents d and d?, the
forward index is queried for the entries for both
documents. Let Fd = {f1, f2, ...fn} and Fd? =
{f ?1, f ?2, ...f ?n?} be the sets of scoring n-grams in
the forward index entries of d and d?, respectively.
Let idf(f) = log |D|df(f) be the inverse document
frequency of a scoring n-gram f , where |D| is
the number of documents in the input corpus and
df(f) is the number documents from which we
extracted the feature f . Interpreting Fd and Fd? as
incidence vectors in the vector space of n-grams
and replacing each non-zero component f with
idf(f), we compute the score of the document pair
as the inverse document frequency weighted co-
sine similarity of Fd and Fd?
score(d, d?) = Fd ? Fd?||Fd|| ? ||Fd? || (1)
The per-document n-best lists are sorted ac-
cording to this score and document pairs for which
the score is below a threshold are discarded com-
pletely.
We do not use term frequency in the scoring
metric. In preliminary experiments, incorporat-
ing the term frequency to yield basic tf/idf as
well as using other information retrieval ranking
functions incorporating term frequencies such as
BM25 (Robertson et al, 1995) resulted in a degra-
dation of performance compared to the simpler
scoring function described above. We believe this
is due to the fact that, in contrast to the standard
information retrieval setting, the overall length of
our queries is on par with that of the documents in
the collection.
The scoring is completely agnostic regarding
the scoring n-grams? positions in the documents.
Since especially for long documents such as
books this may produce spurious matches, we ap-
ply an additional filter to remove document pairs
for which the relative ordering of the matching
scoring n-grams is very different. Together with
each scoring n-gram we also extract its relative
position in each document and store it in the for-
ward index. When scoring a document pair, we
compute the normalized permutation edit distance
(Cormode et al, 2001) between the two sequences
of overlapping n-grams sorted by their position in
the respective document. If this distance exceeds
a certain threshold, we discard the document pair.
2.2 Computational Complexity
By limiting the frequency of matching n-grams,
the complexity becomes linear. Let the tunable
parameter c be the maximum occurrence count for
matching n-grams to be kept in the inverted in-
dex. Let m be the average number of matching
n-grams extracted from a single document whose
count is below c and D be the set of documents
in the input corpus. Then the system generates up
to |D| ?m ? c candidate pairings. Scoring a given
candidate document pair according to cosine sim-
ilarity involves computing three dot-products be-
tween sparse vectors with one non-zero compo-
nent per scoring n-gram extracted and not filtered
from the respective document. Let s be the av-
erage number of such scoring n-grams per docu-
ment, which is bounded by the average document
length. Then the time complexity of the entire
document alignment is in
O(|D| ?m ? c ? s) (2)
and therefore linear in the number of input doc-
uments in the corpus and the average document
size.
The space complexity is dominated by the size
of the inverted and forward indexes, both of which
are linear in the size of the input corpus.
2.3 Sentence-Level Alignment
Further filtering is performed on a per-sentence
basis during per-document-pair sentence align-
ment of the mined text with a standard dynamic
programming sentence alignment algorithm using
sentence length and multilingual probabilistic dic-
tionaries as features. Afterwards we crudely align
1103
words within each pair of aligned source and tar-
get sentences. This crude alignment is used only
to filter nonparallel sentences. Let S be the set
of source words, T the set of target words and
S ? T the set of ordered pairs. Let the source
sentence contain words S0 ? S and the target
sentence contain words T0 ? T . An alignment
A0 ? S0 ? T0 will be scored by
score(A0) =
?
(s,t)?A0
ln p(s, t)p(s) p(t) (3)
where the joint probabilities p(s, t) and marginal
probabilities p(s), p(t) are taken to be the respec-
tive empirical distributions (without smoothing)
in an existing word aligned corpus. This is greed-
ily maximized and the result is divided by its ap-
proximate expected value
?
(s,t)?S0?T
p(s, t)
p(s) ln
p(s, t)
p(s) p(t) (4)
We discard sentence pairs for which the ratio be-
tween the actual and the expected score is less
than 1/3. We also drop sentence pairs for which
both sides are identical, or a language detector de-
clares them to be in the wrong language.
2.4 Baseline Translation System
To translate the input documents into English we
use phrase-based statistical machine translation
systems based on the log-linear formulation of the
problem (Och and Ney, 2002).
We train the systems on the Europarl Cor-
pus (Koehn, 2002), the DGT Multilingual
Translation Memory (European Commission
Directorate-General for Translation, 2007) and
the United Nations ODS corpus (United Nations,
2006). Minimum error rate training (Macherey
et al, 2008) under the BLEU criterion is used
to optimize the feature function weights on de-
velopment data consisting of the nv-dev2007 and
news-dev2009 data sets provided by the organiz-
ers of the 2007 and 2009 WMT shared translation
tasks1. We use a 4-gram language model trained
on a variety of large monolingual corpora. The
BLEU scores of our baseline translation system
1available at http://statmt.org
on the test sets from various WMT shared trans-
lation tasks are listed in Table 5. An empirical
analysis of the impact of the baseline translation
system quality on the data mining system is given
in Section 6.3.
3 Input Document Collections
We evaluate the parallel text mining system on
two input data sets:
web A collection of 2.5 Billion general pages
crawled from the Web, containing only pages
in Czech, English, French, German, Hungar-
ian and Spanish
books A collection of 1.5 Million public domain
books digitized using an optical character
recognition system. The collection consists
primarily of English, French and fewer Span-
ish volumes
3.1 Reference Sets
We created reference sets of groups of docu-
ments in multiple languages which are true trans-
lations of one another for both the web and the
books data set. Due to the presence of duplicates,
each reference pairing can contain more than a
single alternative translation per language. The
web reference set was constructed by exploiting
the systematic hyperlink structure of the web-site
http://america.gov/, that links pages in
one language to their respective translations into
one or more other languages. The resulting refer-
ence set contains documents in Arabic, Chinese,
English, French, Russian and Spanish, however,
for most English pages there is only one transla-
tion into one of the other languages. Overall, the
reference set contains 6,818 documents and 7,286
translation pairs.
The books reference set contains 30 manually
aligned groups of translations covering a total of
103 volumes in English and French.
4 Evaluation Metrics
The fact that the system outputs pairs of docu-
ments and the presence of duplicate documents in
the corpus motivate the use of modified versions
of precision and recall.
1104
Let C be a set of candidate parallel document
pairs and let R be a possibly incomplete reference
set of groups of parallel documents known to exist
in the corpus. Consider the following two subsets
of C:
? Matching pairs which are in some reference
cluster.
? Touching pairs which are non-matching but
have at least one document in some reference
cluster.
We define
Precision = |CMatching||CMatching|+ |CTouching|
and
Recall = |CMatching||R| (5)
5 Parameter Selection
We conducted a series of small-scale experiments
on only those documents contained in the web ref-
erence data set to empirically determine good set-
tings for the tunable parameters of the text min-
ing system. Among the most important parame-
ters are the orders of the n-grams used for pair-
ing documents as well as scoring them. Aside
from the obvious impact on the quality of the out-
put, these parameters have a very large influence
on the overall computational performance of the
system. The choice of the order of the extracted
matching n-grams is mainly a trade-off between
recall and efficiency. If the order is too large
the system will miss valid pairs; if too small the
the threshold on matching n-gram frequency will
need to be increased.
Figure 2 shows the F1-scores obtained run-
ning only on the documents contained in the web
reference set with different orders of matching
and scoring n-grams. Figure 3 shows the corre-
sponding number of pairwise comparisons made
when using different orders of matching n-grams.
While there is a drop of 0.01 in F1 score between
using 2-grams and 5-grams as matching n-grams,
this drop in quality seems to be well worth the 42-
fold reduction in resulting pairwise comparisons.
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
2 3 4 5
F1
Sc
ore
on
we
bT
est
Da
ta
Se
t
Scoring n-gram Order
2-gram matching
3-gram matching
4-gram matching
5-gram matching
Figure 2: F1 scores on the web reference set for
different scoring and matching n-gram orders.
105
106
107
2 3 4 5
Nu
mb
er
of
Pa
irw
ise
Co
mp
ari
son
s
Matching n-gram Order
Figure 3: Number of pairwise comparisons made
when using matching n-grams of different orders.
The largest portion of the loss in F1 score is in-
curred when increasing the matching n-gram or-
der from 4 to 5, the reduction in pairwise compar-
isons, however, is still more than twofold.
Table 1 shows the precision and recall on the
web reference set when running only on docu-
ments in the reference set using 5-grams as match-
ing n-grams and bigrams for scoring for differ-
ent values of the threshold on the cosine similar-
ity score. In this setting as well as in large-scale
experiments on both complete data sets described
in section 6.1, a threshold of 0.1 yields the highest
F1 score.
1105
score threshold 0.06 0.10 0.12 0.16 0.20
precision 0.92 0.97 0.98 0.99 0.99
recall 0.91 0.91 0.90 0.89 0.83
Table 1: Precision and recall on the web reference
set when running only on documents contained in
the reference set.
6 Evaluation
We run the parallel text mining system on the web
and books data sets using 5-grams for matching
and bigrams for scoring. In both cases we discard
matching n-grams which occurred in more than
50 documents and output only the highest scoring
candidate for each document.
In case of the web data set, we extract every 5-
gram as potential matching feature. For the books
data set, however, we downsample the number
of candidate matching 5-grams by extracting only
those whose integer fingerprints under some hash
function have four specific bits set, thus keeping
on average only 1/16 of the matching n-grams.
Here, we also restrict the total number of match-
ing n-grams extracted from any given document
to 20,000. Scoring bigrams are dropped from
the forward index if their document frequency ex-
ceeds 100,000, at which point their influence on
the pairwise score would be negligible.
Running on the web data set, the system on
average extracts 250 matching 5-grams per doc-
ument, extracting a total of approximately 430
Billion distinct 5-grams. Of those, 78% are
singletons and 21% only occur in a single lan-
guage. Only approximately 0.8% of all match-
ing n-grams are filtered due to having a docu-
ment frequency higher than 50. The forward in-
dex initially contains more than 500 Billion bi-
gram occurrences; after pruning out singletons
and bigrams with a document frequency larger
than 100,000, the number of indexed scoring fea-
ture occurrences is reduced to 40%. During scor-
ing, approximately 50 Billion pairwise compar-
isons are performed.
In total the n-gram extraction, document scor-
ing and subsequent filtering takes less than 24
hours on a cluster of 2,000 state-of-the-art CPUs.
The number of words after sentence-level fil-
tering and alignment that the parallel text mining
baseline books web
Czech 27.5 M 0 271.9 M
French 479.8 M 228.5 M 4,914.3 M
German 54.2 M 0 3,787.6 M
Hungarian 26.9 M 0 198.9 M
Spanish 441.0 M 15.0 M 4,846.8 M
Table 2: The number of words per language in the
baseline training corpora and extracted from the
two different data sets.
system extracted for the different languages from
each dataset are listed in Table 2.
score threshold 0.06 0.10 0.12 0.16 0.20
precision 0.88 0.93 0.95 0.97 0.97
recall 0.68 0.65 0.63 0.52 0.38
Table 3: Precision and recall on the reference set
when running on the complete web data set with
different score thresholds.
score threshold 0.06 0.10 0.12 0.16 0.20
precision 0.95 1.00 1.00 1.00 1.00
recall 0.71 0.71 0.71 0.48 0.38
Table 4: Precision and recall on the reference set
when running on the complete books data set with
different score thresholds.
6.1 Precision and Recall
Tables 3 and 4 show precision and recall on the re-
spective reference sets for the web and the books
input data sets. While the text mining system
maintains a very high precision, recall drops sig-
nificantly compared to running only on the doc-
uments in the reference set. One reason for this
behavior is that the number of n-grams in the test
data set which are sufficiently rare to be used as
queries drops with increasing amounts of input
data and in particular short documents which only
share a small number of matching n-grams any-
way, may happen to only share matching n-grams
with a too high document frequency. Further anal-
ysis shows that another, more significant factor is
the existence of multiple, possibly partial transla-
tions and near-duplicate documents which cause
symmetrization to discard valid document pairs
because each document in the pair is determined
by the document pair score to be more similar to
a different translation of a near-duplicate or sub-
1106
Language Pair Training Data WMT 2007 news commentary WMT 2008 news WMT 2009 news
Czech English baseline 21.59 14.59 16.46web 29.26 (+7.67) 20.16 (+5.57) 23.25 (+6.76)
German English baseline 27.99 20.34 20.03web 32.35 (+4.36) 23.22 (+2.88) 23.35 (+3.32)
Hungarian English baseline - 10.21 11.02web - 12.92 (+2.71) 14.68 (+3.66)
French English
baseline 34.26 22.14 26.39
books 34.73 (+0.47) 22.39 (+0.25) 27.15 (+0.76)
web 36.65 (+2.39) 23.22 (+1.08) 28.34 (+1.95)
Spanish English
baseline 43.67 24.15 26.88
books 44.07 (+0.40) 24.32 (+0.17) 27.16 (+0.28)
web 46.21 (+2.54) 25.52 (+1.37) 28.50 (+1.62)
English Czech baseline 14.78 12.45 11.62web 20.65 (+5.86) 18.70 (+6.25) 16.60 (+4.98)
English German baseline 19.89 14.67 14.31web 23.49 (+3.60) 16.78 (+2.11) 16.96 (+2.65)
English Hungarian baseline - 07.93 08.52web - 10.16 (+2.23) 11.42 (+2.90)
English French
baseline 31.59 22.29 25.14
books 31.92 (+0.33) 22.42 (+0.13) 25.46 (+0.32)
web 34.35 (+2.76) 23.56 (+1.27) 27.05 (+1.91)
English Spanish
baseline 42.05 24.65 25.85
books 42.05 24.79 (+0.14) 26.07 (+0.22)
web 45.21 (+3.16) 26.46 (+1.81) 27.79 (+1.94)
Table 5: BLEU scores of the translation systems trained on the automatically mined parallel corpora
and the baseline training data.
set of the document. This problem seems to affect
news articles in particular where there are often
multiple different translations of large subsets of
the same or slightly changed versions of the arti-
cle.
6.2 Translation Quality
Arabic English NIST 2006 NIST 2008
Baseline (UN ODS) 44.31 42.79
Munteanu and Marcu 45.13 43.86
Present work 44.72 43.64
Chinese English NIST 2006 NIST 2008
Baseline (UN ODS) 25.71 19.79
Munteanu and Marcu 28.11 21.69
Present work 28.08 22.02
Table 6: BLEU scores of the Chinese and Arabic
to English translation systems trained on the base-
line UN ODS corpus and after adding either the
Munteanu and Marcu corpora or the training data
mined using the presented approach.
We trained a phrase-based translation system
on the mined parallel data sets and evaluated it
on translation tasks for the language pairs Czech,
French, German, Hungarian and Spanish to and
from English, measuring translation quality with
the BLEU score (Papineni et al, 2002). The trans-
lation tasks evaluated are the WMT 2007 news
commentary test set as well the WMT 2008 and
2009 news test sets.
The parallel data for this experiment was mined
using the general settings described in the previ-
ous section and a threshold of 0.1 on the pairwise
score. We ensure that the test data is not included
in the training data by filtering out all sentences
from the training data that share more than 30%
of their 6-grams with any sentence from one of
the test corpora.
Table 5 shows the BLEU scores of the differ-
ent translation systems. The consistent and signif-
icant improvements in BLEU score demonstrate
the usefulness of the mined document pairs in
training a translation system.
Even though the presented approach works
on a less granular level than the sentence-level
approach of Munteanu and Marcu (2005), we
compare results on the same input data2 used
by those authors to automatically generate the
2LDC corpora LDC2005T12, LDC2005T14 and
LDC2006T02, the second editions of the Arabic, Chinese
and English Gigaword corpora.
1107
Sampling Rate WMT 2007 news commentary WMT 2008 news WMT 2009 news
degraded Cz?En En?Cz degraded Cz?En En?Cz degraded Cz?En En?Cz
1.0 21.59 29.26 20.65 14.59 20.16 18.70 16.46 23.25 16.60
0.5 20.12 29.16 20.55 13.65 20.16 18.71 15.44 23.16 16.56
0.25 18.59 29.09 20.61 12.79 20.09 18.58 14.35 23.18 16.50
0.125 16.69 29.10 20.39 11.87 20.07 18.48 13.05 23.06 16.53
0.0625 14.72 29.04 20.44 10.87 20.06 18.49 11.62 23.11 16.44
0.0312 12.60 28.75 20.28 09.71 19.97 18.45 10.43 23.04 16.41
Table 7: BLEU scores of the degraded Czech to English baseline systems used for translating Czech
documents from the web data set as well as those of Czech to and from English systems trained on data
mined using translations of varying quality created by sampling from the training data.
Arabic English and Chinese English sentence-
aligned parallel LDC corpora LDC2007T08 and
LDC2007T09. We trained Arabic and Chinese
English baseline systems on the United Nations
ODS corpus (United Nations, 2006); we also use
these to translate the non-English portions of the
input data to English. We then evaluate the effects
of also training on either the LDC2007T08 and
LDC2007T09 corpora or the parallel documents
mined by our approach in addition to the United
Nations ODS corpus on the NIST 2006 and 2008
MT evaluation test sets. The results are presented
in Table 6.
The approach proposed in (Munteanu and
Marcu, 2005) relies critically on the existence
of publication dates in order to be computation-
ally feasible, yet it still scales superlinearly in the
amount of input data. It could therefore not easily
be applied to much larger and less structured input
data collections. While our approach neither uses
metadata nor operates on the sentence level, in all
but one of the tasks, the system trained on the data
mined using our approach performs similarly or
slightly better.
6.3 Impact of Baseline Translation Quality
In order to evaluate the impact of the translation
quality of the baseline system on the quality of
the mined document pairs, we trained artificially
degraded Czech to English translation systems by
sampling from the baseline training data at de-
creasing rates. We translate the Czech subset of
the web document collection into English with
each of the degraded systems and apply the paral-
lel data mining system in the same configuration.
Table 7 shows the BLEU scores of the degraded
baseline systems and those resulting from adding
the different mined data sets to the non-degraded
Czech English and English Czech systems. De-
grading the input data translation quality by up to
8.9% BLEU results in a consistent but only com-
paratively small decrease of less than 0.6% BLEU
in the scores obtained when training on the mined
document pairs. This does not only show that the
impact of variations of the baseline system quality
on the data mining system is limited, but also that
the data mining system will already work with a
rather low quality baseline system.
7 Conclusion
We presented a scalable approach to mining paral-
lel text from collections of billions of documents
with high precision. The system makes few as-
sumptions about the input documents. We demon-
strated that it works well on different types of
data: a large collection of web pages and a col-
lection of digitized books. We further showed that
the produced parallel corpora can significantly im-
prove the quality of a state-of-the-art statistical
machine translation system.
8 Acknowledgments
We thank the anonymous reviewers for their in-
sightful comments.
References
Abdul-Rauf, Sadaf and Holger Schwenk. 2009. On
the use of comparable corpora to improve SMT per-
formance. In EACL, pages 16?23.
Broder, Andrei Z. 2000. Identifying and filtering near-
duplicate documents. In COM ?00: Proceedings of
the 11th Annual Symposium on Combinatorial Pat-
1108
tern Matching, pages 1?10, London, UK. Springer-
Verlag.
Chen, Jiang and Jian-Yun Nie. 2000. Parallel web
text mining for cross-language IR. In In In Proc. of
RIAO, pages 62?77.
Cormode, Graham, S. Muthukrishnan, and
Su?leyman Cenk Sahinalp. 2001. Permutation
editing and matching via embeddings. In ICALP
?01: Proceedings of the 28th International Collo-
quium on Automata, Languages and Programming,,
pages 481?492, London, UK. Springer-Verlag.
Dean, Jeffrey and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In Proceedings of the Sixth Symposium on Operat-
ing System Design and Implementation (OSDI-04),
San Francisco, CA, USA.
Do, Thi-Ngoc-Diep, Viet-Bac Le, Brigitte Bigi, Lau-
rent Besacier Eric, and Castelli. 2009. Mining a
comparable text corpus for a Vietnamese - French
statistical machine translation system. In Proceed-
ings of the 4th EACL Workshop on Statistical Ma-
chine Translation, pages 165?172, Athens, Greece,
March.
European Commission Directorate-General for Trans-
lation. 2007. DGT-TM parallel corpus.
http://langtech.jrc.it/DGT-TM.html.
Harding, Stephen M., W. Bruce Croft, and C. Weir.
1997. Probabilistic retrieval of OCR degraded text
using n-grams. In ECDL ?97: Proceedings of
the First European Conference on Research and
Advanced Technology for Digital Libraries, pages
345?359, London, UK. Springer-Verlag.
Henzinger, Monika. 2006. Finding near-duplicate
web pages: a large-scale evaluation of algorithms.
In SIGIR ?06: Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 284?
291, New York, NY, USA. ACM.
Koehn, Philipp. 2002. Europarl: A multilingual cor-
pus for evaluation of machine translation. Draft.
Macherey, Wolfgang, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hi, October. Association for
Computational Linguistics.
Manber, Udi. 1994. Finding similar files in a large file
system. In Proceedings of the USENIX Winter 1994
Technical Conferenc.
Munteanu, Dragos Stefan and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477?504.
Munteanu, Dragos Stefan and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In ACL.
Och, Franz Josef and Hermann Ney. 2002. Dis-
criminative training and maximum entropy models
for statistical machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 295?302,
Philadelphia, PA, USA.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
311?318, Philadelphia, PA, USA.
Resnik, Philip and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29:349?380.
Robertson, S E, S Walker, S Jones, M M Hancock-
Beaulieu, and M Gatford. 1995. Okapi at TREC?3.
In Proceedings of the Third Text REtrieval Confer-
ence (TREC-3).
Udupa, Raghavendra, K. Saravanan, A. Kumaran, and
Jagadeesh Jagarlamudi. 2009. Mint: A method
for effective and scalable mining of named entity
transliterations from large comparable corpora. In
EACL, pages 799?807.
United Nations. 2006. ODS UN parallel corpus.
http://ods.un.org/.
1109
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1395?1404,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-independent Compound Splitting with Morphological Operations
Klaus Macherey1 Andrew M. Dai2 David Talbot1 Ashok C. Popat1 Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{kmach,talbot,popat,och}@google.com
2University of Edinburgh
10 Crichton Street
Edinburgh, UK EH8 9AB
a.dai@ed.ac.uk
Abstract
Translating compounds is an important prob-
lem in machine translation. Since many com-
pounds have not been observed during train-
ing, they pose a challenge for translation sys-
tems. Previous decompounding methods have
often been restricted to a small set of lan-
guages as they cannot deal with more complex
compound forming processes. We present a
novel and unsupervised method to learn the
compound parts and morphological operations
needed to split compounds into their com-
pound parts. The method uses a bilingual
corpus to learn the morphological operations
required to split a compound into its parts.
Furthermore, monolingual corpora are used to
learn and filter the set of compound part can-
didates. We evaluate our method within a ma-
chine translation task and show significant im-
provements for various languages to show the
versatility of the approach.
1 Introduction
A compound is a lexeme that consists of more than
one stem. Informally, a compound is a combina-
tion of two or more words that function as a single
unit of meaning. Some compounds are written as
space-separated words, which are called open com-
pounds (e.g. hard drive), while others are written
as single words, which are called closed compounds
(e.g. wallpaper). In this paper, we shall focus only
on closed compounds because open compounds do
not require further splitting.
The objective of compound splitting is to split a
compound into its corresponding sequence of con-
stituents. If we look at how compounds are created
from lexemes in the first place, we find that for some
languages, compounds are formed by concatenating
existing words, while in other languages compound-
ing additionally involves certain morphological op-
erations. These morphological operations can be-
come very complex as we illustrate in the following
case studies.
1.1 Case Studies
Below, we look at splitting compounds from 3 differ-
ent languages. The examples introduce in part the
notation used for the decision rule outlined in Sec-
tion 3.1.
1.1.1 English Compound Splitting
The word flowerpot can appear as a closed or open
compound in English texts. To automatically split
the closed form we have to try out every split point
and choose the split with minimal costs according to
a cost function. Let's assume that we already know
that flowerpot must be split into two parts. Then we
have to position two split points that mark the end of
each part (one is always reserved for the last charac-
ter position). The number of split points is denoted
by K (i.e. K = 2), while the position of split points
is denoted by n1 and n2. Since flowerpot consists of
9 characters, we have 8 possibilities to position split
point n1 within the characters c1, . . . , c8. The final
split point corresponds with the last character, that is,
n2 = 9. Trying out all possible single splits results
in the following candidates:
flowerpot ? f+ lowerpot
flowerpot ? fl+ owerpot
...
flowerpot ? flower+ pot
...
flowerpot ? flowerpo+ t
1395
If we associate each compound part candidate with
a cost that reflects how frequent this part occurs in a
large collection of English texts, we expect that the
correct split flower + pot will have the lowest cost.
1.1.2 German Compound Splitting
The previous example covered a casewhere the com-
pound is constructed by directly concatenating the
compound parts. While this works well for En-
glish, other languages require additional morpholog-
ical operations. To demonstrate, we look at the Ger-
man compound Verkehrszeichen (traffic sign) which
consists of the two nouns Verkehr (traffic) and Zei-
chen (sign). Let's assume that we want to split this
word into 3 parts, that is, K = 3. Then, we get the
following candidates.
Verkehrszeichen ? V+ e+ rkehrszeichen
Verkehrszeichen ? V+ er+ kehrszeichen
...
Verkehrszeichen ? Verkehr+ s+ zeichen
...
Verkehrszeichen ? Verkehrszeich+ e+ n
Using the same procedure as described before, we
can lookup the compound parts in a dictionary or de-
termine their frequency from large text collections.
This yields the optimal split points n1 = 7, n2 =
8, n3 = 15. The interesting part here is the addi-
tional s morpheme, which is called a linking mor-
pheme, because it combines the two compound parts
to form the compound Verkehrszeichen. If we have
a list of all possible linking morphemes, we can
hypothesize them between two ordinary compound
parts.
1.1.3 Greek Compound Splitting
The previous example required the insertion of a
linking morpheme between two compound parts.
We shall now look at a more complicated mor-
phological operation. The Greek compound
?????????? (cardboard box) consists of the two
parts ????? (paper) and ????? (box). Here, the
problem is that the parts ????? and ????? are not
valid words in Greek. To lookup the correct words,
we must substitute the suffix of the compound part
candidates with some other morphemes. If we allow
the compound part candidates to be transformed by
some morphological operation, we can lookup the
transformed compound parts in a dictionary or de-
termine their frequencies in some large collection of
Greek texts. Let's assume that we need only one split
point. Then this yields the following compound part
candidates:
?????????? ? ? + ?????????
?????????? ? ? + ????????? g2 : ? / ?
?????????? ? ? + ????????? g2 : ? / ?
...
?????????? ? ????? + ????? g1 : ? / ? ,
g2 : ? / ?...
?????????? ? ????????? + ? g1 : ? / ?
?????????? ? ????????? + ? g2 : ? / ?
Here, gk : s/t denotes the kth compound part which
is obtained by replacing string s with string t in the
original string, resulting in the transformed part gk.
1.2 Problems and Objectives
Our goal is to design a language-independent com-
pound splitter that is useful for machine translation.
The previous examples addressed the importance of
a cost function that favors valid compound parts ver-
sus invalid ones. In addition, the examples have
shown that, depending on the language, the morpho-
logical operations can become very complex. For
most Germanic languages like Danish, German, or
Swedish, the list of possible linking morphemes is
rather small and can be provided manually. How-
ever, in general, these lists can become very large,
and language experts who could provide such lists
might not be at our disposal. Because it seems in-
feasible to list the morphological operations explic-
itly, we want to find and extract those operations
automatically in an unsupervised way and provide
them as an additional knowledge source to the de-
compounding algorithm.
Another problem is how to evaluate the quality
of the compound splitter. One way is to compile
for every language a large collection of compounds
together with their valid splits and to measure the
proportion of correctly split compounds. Unfortu-
nately, such lists do not exist for many languages.
1396
While the training algorithm for our compound split-
ter shall be unsupervised, the evaluation data needs
to be verified by human experts. Since we are in-
terested in improving machine translation and to cir-
cumvent the problem of explicitly annotating com-
pounds, we evaluate the compound splitter within a
machine translation task. By decompounding train-
ing and test data of a machine translation system, we
expect an increase in the number of matching phrase
table entries, resulting in better translation quality
measured in BLEU score (Papineni et al, 2002).
If BLEU score is sensitive enough to measure the
quality improvements obtained from decompound-
ing, there is no need to generate a separate gold stan-
dard for compounds.
Finally, we do not want to split non-compounds
and named entities because we expect them to be
translated non-compositionally. For example, the
German wordDeutschland (Germany) could be split
into two parts Deutsch (German) + Land (coun-
try). Although this is a valid split, named entities
should be kept as single units. An example for a
non-compound is the German participle vereinbart
(agreed) which could be wrongly split into the parts
Verein (club) + Bart (beard). To avoid overly eager
splitting, we will compile a list of non-compounds in
an unsupervised way that serves as an exception list
for the compound splitter. To summarize, we aim to
solve the following problems:
? Define a cost function that favors valid com-
pound parts and rejects invalid ones.
? Learn morphological operations, which is im-
portant for languages that have complex com-
pound forming processes.
? Apply compound splitting to machine transla-
tion to aid in translation of compounds that have
not been seen in the bilingual training data.
? Avoid splitting non-compounds and named en-
tities as this may result in wrong translations.
2 Related work
Previous work concerning decompounding can be
divided into two categories: monolingual and bilin-
gual approaches.
Brown (2002) describes a corpus-driven approach
for splitting compounds in a German-English trans-
lation task derived from a medical domain. A large
proportion of the tokens in both texts are cognates
with a Latin or Greek etymological origin. While the
English text keeps the cognates as separate tokens,
they are combined into compounds in the German
text. To split these compounds, the author compares
both the German and the English cognates on a char-
acter level to find reasonable split points. The algo-
rithm described by the author consists of a sequence
of if-then-else conditions that are applied on the two
cognates to find the split points. Furthermore, since
the method relies on finding similar character se-
quences between both the source and the target to-
kens, the approach is restricted to cognates and can-
not be applied to split more complex compounds.
Koehn and Knight (2003) present a frequency-
based approach to compound splitting for German.
The compound parts and their frequencies are es-
timated from a monolingual corpus. As an exten-
sion to the frequency approach, the authors describe
a bilingual approach where they use a dictionary ex-
tracted from parallel data to find better split options.
The authors allow only two linking morphemes be-
tween compound parts and a few letters that can be
dropped. In contrast to our approach, those opera-
tions are not learned automatically, but must be pro-
vided explicitly.
Garera and Yarowsky (2008) propose an approach
to translate compounds without the need for bilin-
gual training texts. The compound splitting pro-
cedure mainly follows the approach from (Brown,
2002) and (Koehn and Knight, 2003), so the em-
phasis is put on finding correct translations for com-
pounds. To accomplish this, the authors use cross-
language compound evidence obtained from bilin-
gual dictionaries. In addition, the authors describe a
simple way to learn glue characters by allowing the
deletion of up to two middle and two end charac-
ters.1 More complex morphological operations are
not taken into account.
Alfonseca et al (2008b) describe a state-of-the-
art German compound splitter that is particularly ro-
bust with respect to noise and spelling errors. The
compound splitter is trained on monolingual data.
Besides applying frequency and probability-based
methods, the authors also take the mutual informa-
tion of compound parts into account. In addition, the
1However, the glue characters found by this procedure seem
to be biased for at least German and Albanian. A very frequent
glue morpheme like -es- is not listed, while glue morphemes
like -k- and -h- rank very high, although they are invalid glue
morphemes for German. Albanian shows similar problems.
1397
authors look for compound parts that occur in dif-
ferent anchor texts pointing to the same document.
All these signals are combined and the weights are
trained using a support vector machine classifier. Al-
fonseca et al (2008a) apply this compound splitter
on various other Germanic languages.
Dyer (2009) applies a maximum entropy model
of compound splitting to generate segmentation lat-
tices that serve as input to a translation system.
To train the model, reference segmentations are re-
quired. Here, we produce only single best segmen-
tations, but otherwise do not rely on reference seg-
mentations.
3 Compound Splitting Algorithm
In this section, we describe the underlying optimiza-
tion problem and the algorithm used to split a token
into its compound parts. Starting from Bayes' de-
cision rule, we develop the Bellman equation and
formulate a dynamic programming-based algorithm
that takes a word as input and outputs the constituent
compound parts. We discuss the procedure used to
extract compound parts from monolingual texts and
to learn themorphological operations using bilingual
corpora.
3.1 Decision Rule for Compound Splitting
Given a token w = c1, . . . , cN = cN1 consisting of a
sequence of N characters ci, the objective function
is to find the optimal number K? and sequence of split
points n?K?0 such that the subwords are the constituents
of the token, where2 n0 := 0 and nK := N :
w = cN1 ? (K?, n?K?0 ) =
= argmax
K,nK0
{
Pr(cN1 ,K, nK0 )
}
(1)
= argmax
K,nK0
{
Pr(K) ? Pr(cN1 , nK0 |K)
}
u argmax
K,nK0
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1|K)?
?p(nk|nk?1,K)} (2)
with p(n0) = p(nK |?) ? 1. Equation 2 requires that
token w can be fully decomposed into a sequence
2For algorithmic reasons, we use the start position 0 to rep-
resent a fictitious start symbol before the first character of the
word.
of lexemes, the compound parts. Thus, determin-
ing the optimal segmentation is sufficient for finding
the constituents. While this may work for some lan-
guages, the subwords are not valid words in general
as discussed in Section 1.1.3. Therefore, we allow
the lexemes to be the result of a transformation pro-
cess, where the transformed lexemes are denoted by
gK1 . This leads to the following refined decision rule:
w = cN1 ? (K?, n?K?0 , g?K?1 ) =
= argmax
K,nK0 ,gK1
{
Pr(cN1 ,K, nK0 , gK1 )
}
(3)
= argmax
K,nK0 ,gK1
{
Pr(K) ? Pr(cN1 , nK0 , gK1 |K)
}
(4)
u argmax
K,nK0 ,gK1
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1, gk|K)
? ?? ?
compound part probability
?
? p(nk|nk?1,K)
}
(5)
The compound part probability is a zero-order
model. If we penalize each split with a constant split
penalty ?, and make the probability independent of
the number of splits K, we arrive at the following
decision rule:
w = cN1 ? (K?, n?K?1 , g?K?1 )
= argmax
K,nK0 ,gK1
{
?K ?
K
?
k=1
p(cnknk?1+1, nk?1, gk)
}
(6)
3.2 Dynamic Programming
We use dynamic programming to find the optimal
split sequence. Each split infers certain costs that
are determined by a cost function. The total costs of
a decomposed word can be computed from the in-
dividual costs of the component parts. For the dy-
namic programming approach, we define the follow-
ing auxiliary function Q with nk = j:
Q(cj1) = max
nk0 ,gk1
{
?k ?
k
?
?=1
p(cn?n??1+1, n??1, g?)
}
that is, Q(cj1) is equal to the minimal costs (maxi-
mum probability) that we assign to the prefix string
cj1 where we have used k split points at positions nk1 .
This yields the following recursive equation:
Q(cj1) = maxnk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(7)
1398
Algorithm 1 Compound splitting
Input: input word w = cN1
Output: compound parts
Q(0) = 0
Q(1) = ? ? ? = Q(N) = ?
for i = 0, . . . , N ? 1 do
for j = i + 1, . . . , N do
split-costs = Q(i) + cost(cji+1, i, gj) +
split-penalty
if split-costs < Q(j) then
Q(j) = split-costs
B(j) = (i, gj)
end if
end for
end for
with backpointer
B(j) = argmax
nk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(8)
Using logarithms in Equations 7 and 8, we can inter-
pret the quantities as additive costs rather than proba-
bilities. This yields Algorithm 1, which is quadratic
in the length of the input string. By enforcing that
each compound part does not exceed a predefined
constant length `, we can change the second for loop
as follows:
for j = i + 1, . . . ,min(i + `,N) do
With this change, Algorithm 1 becomes linear in the
length of the input word, O(|w|).
4 Cost Function and Knowledge Sources
The performance of Algorithm 1 depends on
the cost function cost(?), that is, the probability
p(cnknk?1+1, nk?1, gk). This cost function incorpo-
rates knowledge about morpheme transformations,
morpheme positionswithin a compound part, and the
compound parts themselves.
4.1 Learning Morphological Operations using
Phrase Tables
Let s and t be strings of the (source) language al-
phabet A. A morphological operation s/t is a pair
of strings s, t ? A?, where s is replaced by t. With
the usual definition of the Kleene operator ?, s and
t can be empty, denoted by ?. An example for such
a pair is ?/es, which models the linking morpheme
es in the German compound Bundesagentur (federal
agency):
Bundesagentur ? Bund+ es+ Agentur .
Note that by replacing either s or t with ?, we can
model insertions or deletions of morphemes. The
explicit dependence on position nk?1 in Equation 6
allows us to determine if we are at the beginning,
in the middle, or at the end of a token. Thus, we
can distinguish between start, middle, or end mor-
phemes and hypothesize them during search.3 Al-
though not explicitly listed in Algorithm 1, we dis-
allow sequences of linking morphemes. This can
be achieved by setting the costs to infinity for those
morpheme hypotheses, which directly succeed an-
other morpheme hypothesis.
To learn the morphological operations involved
in compounding, we determine the differences be-
tween a compound and its compound parts. This can
be done by computing the Levenshtein distance be-
tween the compound and its compound parts, with
the allowable edit operations being insertion, dele-
tion, or substitution of one or more characters. If we
store the current and previous characters, edit opera-
tion and the location (prefix, infix or suffix) at each
position during calculation of the Levenshtein dis-
tance then we can obtain the morphological opera-
tions required for compounding. Applying the in-
verse operations, that is, replacing twith s yields the
operation required for decompounding.
4.1.1 Finding Compounds and their Parts
To learn the morphological operations, we need
compounds together with their compound parts. The
basic idea of finding compound candidates and their
compound parts in a bilingual setting are related to
the ideas presented in (Garera and Yarowsky, 2008).
Here, we use phrase tables rather than dictionaries.
Although phrase tablesmight containmore noise, we
believe that overall phrase tables cover more phe-
nomena of translations thanwhat can be found in dic-
tionaries. The procedure is as follows. We are given
a phrase table that provides translations for phrases
from a source language l into English and from En-
glish into l. Under the assumption that English does
not contain many closed compounds, we can search
3We jointly optimize over K and the split points nk, so we
know that cnKnK?1 is a suffix of w.
1399
the phrase table for those single-token source words
f in language l, which translate into multi-token En-
glish phrases e1, . . . , en for n > 1. This results
in a list of (f ; e1, . . . , en) pairs, which are poten-
tial compound candidates together with their English
translations. If for each pair, we take each token ei
from the English (multi-token) phrase and lookup
the corresponding translation for language l to get
gi, we should find entries that have at least some
partial match with the original source word f , if f
is a true compound. Because the translation phrase
table was generated automatically during the train-
ing of a multi-language translation system, there is
no guarantee that the original translations are cor-
rect. Thus, the bilingual extraction procedure is
subject to introduce a certain amount of noise. To
mitigate this, thresholds such as minimum edit dis-
tance between the potential compound and its parts,
minimum co-occurrence frequencies for the selected
bilingual phrase pairs and minimum source and tar-
get word lengths are used to reduce the noise at the
expense of finding fewer compounds. Those entries
that obey these constraints are output as triples of
form:
(f ; e1, . . . , en; g1, . . . , gn) (9)
where
? f is likely to be a compound,
? e1, . . . , en is the English translation, and
? g1, . . . , gn are the compound parts of f .
The following example for German illustrates the
process. Suppose that the most probable translation
for?berweisungsbetrag is transfer amount using the
phrase table. We then look up the translation back to
German for each translated token: transfer translates
to?berweisung and amount translates toBetrag. We
then calculate the distance between all permutations
of the parts and the original compound and choose
the one with the lowest distance and highest transla-
tion probability: ?berweisung Betrag.
4.2 Monolingual Extraction of Compound
Parts
The most important knowledge source required for
Algorithm 1 is a word-frequency list of compound
parts that is used to compute the split costs. The
procedure described in Section 4.1.1 is useful for
learning morphological operations, but it is not suffi-
cient to extract an exhaustive list of compound parts.
Such lists can be extracted frommonolingual data for
which we use language model (LM) word frequency
lists in combination with some filter steps. The ex-
traction process is subdivided into 2 passes, one over
a high-quality news LM to extract the parts and the
other over a web LM to filter the parts.
4.2.1 Phase 1: Bootstrapping pass
In the first pass, we generate word frequency lists de-
rived from news articles for multiple languages. The
motivation for using news articles rather than arbi-
trary web texts is that news articles are in general
less noisy and contain fewer spelling mistakes. The
language-dependent word frequency lists are filtered
according to a sequence of filter steps. These filter
steps include discarding all words that contain digits
or punctuations other than hyphen, minimum occur-
rence frequency, and a minimum length which we
set to 4. The output is a table that contains prelim-
inary compound parts together with their respective
counts for each language.
4.2.2 Phase 2: Filtering pass
In the second pass, the compound part vocabulary
is further reduced and filtered. We generate a LM
vocabulary based on arbitrary web texts for each lan-
guage and build a compound splitter based on the vo-
cabulary list that was generated in phase 1. We now
try to split every word of the web LM vocabulary
based on the compound splitter model from phase
1. For the compound parts that occur in the com-
pound splitter output, we determine how often each
compound part was used and output only those com-
pound parts whose frequency exceed a predefined
threshold n.
4.3 Example
Suppose we have the following word frequencies
output from pass 1:
floor 10k poll 4k
flow 9k pot 5k
flower 15k potter 20k
In pass 2, we observe the word flowerpot. With the
above list, the only compound parts used are flower
and pot. If we did not split any other words and
threshold at n = 1, our final list would consist of
flower and pot. This filtering pass has the advantage
of outputting only those compound part candidates
1400
which were actually used to split words from web
texts. The thresholding also further reduces the risk
of introducing noise. Another advantage is that since
the set of parts output in the first pass may contain a
high number of compounds, the filter is able to re-
move a large number of these compounds by exam-
ining relative frequencies. In our experiments, we
have assumed that compound part frequencies are
higher than the compound frequency and so remove
words from the part list that can themselves be split
and have a relatively high frequency. Finally, after
removing the low frequency compound parts, we ob-
tain the final compound splitter vocabulary.
4.4 Generating Exception Lists
To avoid eager splitting of non-compounds and
named entities, we use a variant of the procedure de-
scribed in Section 4.1.1. By emitting all those source
words that translate with high probability into single-
token English words, we obtain a list of words that
should not be split.4
4.5 Final Cost Function
The final cost function is defined by the following
components which are combined log-linearly.
? The split penalty ? penalizes each compound
part to avoid eager splitting.
? The cost for each compound part gk is com-
puted as ? logC(gk), where C(gk) is the un-
igram count for gk obtained from the news LM
word frequency list. Since we use a zero-order
model, we can ignore the normalization and
work with unigram counts rather than unigram
probabilities.
? Because Algorithm 1 iterates over the charac-
ters of the input token w, we can infer from the
boundaries (i, j) if we are at the start, in the
middle, or at the end of the token. Applying
a morphological operation adds costs 1 to the
overall costs.
Although the cost function is language dependent,
we use the same split penalty weight ? = 20 for all
languages except for German, where the split penalty
weight is set to 13.5.
5 Results
To show the language independence of the approach
within a machine translation task, we translate from
languages belonging to different language families
into English. The publicly available Europarl corpus
is not suitable for demonstrating the utility of com-
pound splitting because there are few unseen com-
pounds in the test section of the Europarl corpus.
The WMT shared translation task has a broader do-
main compared to Europarl but covers only a few
languages. Hence, we present results for German-
English using the WMT-07 data and cover other lan-
guages using non-public corporawhich contain news
as well as open-domain web texts. Table 1 lists the
various corpus statistics. The source languages are
grouped according to their language family.
For learning the morphological operations, we al-
lowed the substitution of at most 2 consecutive char-
acters. Furthermore, we only allowed at most one
morphological substitution to avoid introducing too
much noise. The found morphological operations
were sorted according to their frequencies. Those
which occurred less than 100 times were discarded.
Examples of extracted morphological operations are
given in Table 2. Because the extraction procedure
described in Section 4.1 is not purely restricted to the
case of decompounding, we found that many mor-
phological operations emitted by this procedure re-
flect morphological variations that are not directly
linked to compounding, but caused by inflections.
To generate the language-dependent lists of com-
pound parts, we used language model vocabulary
lists5 generated from news texts for different lan-
guages as seeds for the first pass. These lists were
filtered by discarding all entries that either con-
tained digits, punctuations other than hyphens, or se-
quences of the same characters. In addition, the in-
frequent entries were discarded as well to further re-
duce noise. For the second pass, we used the lists
generated in the first pass together with the learned
morphological operations to construct a preliminary
compound splitter. We then generated vocabulary
lists for monolingual web texts and applied the pre-
liminary compound splitter onto this list. The used
4Because we will translate only into English, this is not an
issue for the introductory example flowerpot.
5The vocabulary lists also contain the word frequencies. We
use the term vocabulary list synonymously for a word frequency
list.
1401
Family Src Language #Tokens Train src/trg #Tokens Dev src/trg #Tokens Tst src/trg
Germanic Danish 196M 201M 43, 475 44, 479 72, 275 74, 504
German 43M 45M 23, 151 22, 646 45, 077 43, 777
Norwegian 251M 255M 42, 096 43, 824 70, 257 73, 556
Swedish 201M 213M 42, 365 44, 559 70, 666 74, 547
Hellenic Greek 153M 148M 47, 576 44, 658 79, 501 74, 776
Uralic Estonian 199M 244M 34, 987 44, 658 57, 916 74, 765
Finnish 205M 246M 32, 119 44, 658 53, 365 74, 771
Table 1: Corpus statistics for various language pairs. The target language is always English. The source languages are
grouped according to their language family.
Language morpholog. operations
Danish -/?, s/?
German -/?, s/?, es/?, n/?, e/?, en/?
Norwegian -/?, s/?, e/?
Swedish -/?, s/?
Greek ?/?, ?/?, ?/?, ?/?, ?/?, ?/?
Estonian -/?, e/?, se/?
Finnish ?/n, n/?, ?/en
Table 2: Examples of morphological operations that were
extracted from bilingual corpora.
compound parts were collected and sorted according
to their frequencies. Those which were used at least
2 times were kept in the final compound parts lists.
Table 3 reports the number of compound parts kept
after each pass. For example, the Finnish news vo-
cabulary list initially contained 1.7M entries. After
removing non-alpha and infrequent words in the first
filter step, we obtained 190K entries. Using the pre-
liminary compound splitter in the second filter step
resulted in 73K compound part entries.
The finally obtained compound splitter was in-
tegrated into the preprocessing pipeline of a state-
of-the-art statistical phrase-based machine transla-
tion system that works similar to the Moses de-
coder (Koehn et al, 2007). By applying the com-
pound splitter during both training and decoding we
ensured that source language tokens were split in
the same way. Table 4 presents results for vari-
ous language-pairs with and without decompound-
ing. Both the Germanic and the Uralic languages
show significant BLEU score improvements of 1.3
BLEU points on average. The confidence inter-
vals were computed using the bootstrap resampling
normal approximation method described in (Noreen,
1989). While the compounding process for Ger-
manic languages is rather simple and requires only a
few linking morphemes, compounds used in Uralic
languages have a richer morphology. In contrast to
the Germanic and Uralic languages, we did not ob-
serve improvements for Greek. To investigate this
lack of performance, we turned off transliteration
and kept unknown source words in their original
script. We analyzed the number of remaining source
characters in the baseline system and the system us-
ing compound splitting by counting the number of
Greek characters in the translation output. The num-
ber of remaining Greek characters in the translation
output was reduced from 6, 715 in the baseline sys-
tem to 3, 624 in the systemwhich used decompound-
ing. In addition, a few other metrics like the number
of source words that consisted of more than 15 char-
acters decreased as well. Because we do not know
how many compounds are actually contained in the
Greek source sentences6 and because the frequency
of using compounds might vary across languages,
we cannot expect the same performance gains across
languages belonging to different language families.
An interesting observation is, however, that if one
language from a language family shows performance
gains, then there are performance gains for all the
languages in that family. We also investigated the ef-
fect of not using any morphological operations. Dis-
allowing all morphological operations accounts for
a loss of 0.1 - 0.2 BLEU points across translation
systems and increases the compound parts vocabu-
lary lists by up to 20%, which means that most of the
gains can be achieved with simple concatenation.
The exception lists were generated according to
the procedure described in Section 4.4. Since we
aimed for precision rather than recall when con-
structing these lists, we inserted only those source
6Quite a few of the remaining Greek characters belong to
rare named entities.
1402
Language initial vocab size #parts after 1st pass #parts after 2nd pass
Danish 918, 708 132, 247 49, 592
German 7, 908, 927 247, 606 45, 059
Norwegian 1, 417, 129 237, 099 62, 107
Swedish 1, 907, 632 284, 660 82, 120
Greek 877, 313 136, 436 33, 130
Estonian 742, 185 81, 132 36, 629
Finnish 1, 704, 415 190, 507 73, 568
Table 3: Number of remaining compound parts for various languages after the first and second filter step.
System BLEU[%] w/o splitting BLEU[%] w splitting ? CI 95%
Danish 42.55 44.39 1.84 (? 0.65)
German WMT-07 25.76 26.60 0.84 (? 0.70)
Norwegian 42.77 44.58 1.81 (? 0.64)
Swedish 36.28 38.04 1.76 (? 0.62)
Greek 31.85 31.91 0.06 (? 0.61)
Estonian 20.52 21.20 0.68 (? 0.50)
Finnish 25.24 26.64 1.40 (? 0.57)
Table 4: BLEU score results for various languages translated into English with and without compound splitting.
Language Split source translation
German no Die EU ist nicht einfach ein Freundschaftsclub. The EU is not just a Freundschaftsclub.
yes Die EU ist nicht einfach ein Freundschaft Club The EU is not simply a friendship club.
Greek no ?? ????? ??????????? ??????????; What ??????????? configuration?
yes ?? ????? ????? ?????? ??????????; What is pulse code modulation?
Finnish no Lis?vuodevaatteet ja pyyheliinat ovat kaapissa. Lis?vuodevaatteet and towels are in the closet.
yes Lis? Vuode Vaatteet ja pyyheliinat ovat kaapissa. Extra bed linen and towels are in the closet.
Table 5: Examples of translations into English with and without compound splitting.
words whose co-occurrence count with a unigram
translation was at least 1, 000 and whose translation
probability was larger than 0.1. Furthermore, we re-
quired that at least 70%of all target phrase entries for
a given source word had to be unigrams. All decom-
pounding results reported in Table 4 were generated
using these exception lists, which prevented wrong
splits caused by otherwise overly eager splitting.
6 Conclusion and Outlook
We have presented a language-independent method
for decompounding that improves translations for
compounds that otherwise rarely occur in the bilin-
gual training data. We learned a set of morpholog-
ical operations from a translation phrase table and
determined suitable compound part candidates from
monolingual data in a two pass process. This al-
lowed us to learn morphemes and operations for lan-
guages where these lists are not available. In addi-
tion, we have used the bilingual information stored
in the phrase table to avoid splitting non-compounds
as well as frequent named entities. All knowledge
sources were combined in a cost function that was
applied in a compound splitter based on dynamic
programming. Finally, we have shown this improves
translation performance on languages from different
language families.
The weights were not optimized in a systematic
way but set manually to their respective values. In
the future, the weights of the cost function should be
learned automatically by optimizing an appropriate
error function. Instead of using gold data, the devel-
opment data for optimizing the error function could
be collected without supervision using the methods
proposed in this paper.
1403
References
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008a. Decompounding query keywords from com-
pounding languages. In Proc. of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL): Human Language Technologies (HLT), pages
253--256, Columbus, Ohio, USA, June.
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008b. German decompounding in a difficult corpus.
In A. Gelbukh, editor, Lecture Notes in Computer Sci-
ence (LNCS): Proc. of the 9th Int. Conf. on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), volume 4919, pages 128--139. Springer Ver-
lag, February.
Ralf D. Brown. 2002. Corpus-Driven Splitting of Com-
poundWords. In Proc. of the 9th Int. Conf. on Theoret-
ical andMethodological Issues inMachine Translation
(TMI), pages 12--21, Keihanna, Japan, March.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc. of
the Human Language Technologies (HLT): The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL), pages
406--414, Boulder, Colorado, June.
Nikesh Garera and David Yarowsky. 2008. Translating
Compounds by Learning Component Gloss Transla-
tion Models via Multiple Languages. In Proc. of the
3rd Internation Conference on Natural Language Pro-
cessing (IJCNLP), pages 403--410, Hyderabad, India,
January.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proc. of the 10th
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL), volume 1, pages
187--193, Budapest, Hungary, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 44th
Annual Meeting of the Association for Computational
Linguistics (ACL), volume 1, pages 177--180, Prague,
Czech Republic, June.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311--318, Philadel-
phia, Pennsylvania, July.
1404
