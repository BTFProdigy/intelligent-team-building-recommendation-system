Identification of Patients with Congestive Heart Failure using a
binary classifier: a case study.
Serguei V. Pakhomov
Division of Medical
Informatics Research
Mayo Foundation
pakhomov@mayo.edu
James Buntrock
Division of Medical
Informatics Research
Mayo Foundation
buntrock@mayo.edu
Christopher G. Chute
Division of Medical
Informatics Research
Mayo Foundation
chute@mayo.edu
Abstract
This paper addresses a very specific
problem that happens to be  common in
health science research. We present a
machine learning based method for
identifying patients diagnosed with
congestive heart failure and other related
conditions by automatically classifying
clinical notes. This method relies on a
Perceptron neural network classifier
trained on comparable amounts of
positive and negative samples of clinical
notes previously categorized by human
experts. The documents are represented as
feature vectors where features are a mix
of single words and concept mappings to
MeSH and HICDA ontologies. The
method is designed and implemented to
support a particular epidemiological study
but has broader implications for clinical
research. In this paper, we describe the
method and present experimental
classification results based on
classification accuracy and positive
predictive value.
1 Introduction
Epidemiological research frequently has to deal
with collecting a comprehensive set of human
subjects that are deemed relevant for a particular
study. For example, the research focused on
patients with congestive heart failure needs to
identify all possible candidates for the study so that
the candidates could be asked to participate. One
of the requirements of a study like that is the
completeness of the subject pool. In many cases,
such as disease incidence or prevalence studies, it
is not acceptable for the investigator to miss any of
the candidates. The identification of the candidates
relies on a large number of sources some of which
do not exist in an electronic format, but it may start
with the clinical notes dictated by the treating
physician.   
Another aspect of candidate identification is
prospective patient recruitment. Prospective
recruitment is based on inclusion or exclusion
criteria and is of great interest to physicians for
enabling just-in-time treatment, clinic trial
enrollment, or research study options for patients.
At Mayo Clinic most clinical documents are
transcribed within 24 hours of patient consultation.
This electronic narration serves as resource for
enabling prospective recruitment based on criteria
present in clinical document.
Probably the most basic approach to
identification of candidates for recruitment is to
develop a set of terms whose presence in the note
may be indicative of the diagnoses of interest.
This term set can be used as a filtering mechanism
by either searching on an indexed collection of
clinical notes or simply by doing term spotting if
the size of the collection would allow it. For
example, in case of congestive heart failure, one
could define the following set of search terms:
?CHF?, ?heart failure?, ?cardiomyopathy?,
?volume overload?, ?fluid overload?, ?pulmonary
edema?, etc. The number of possible variants is
virtually unlimited, which is the inherent problem
with this approach. It would be hard to guarantee
the completeness of this set to begin with, which is
further complicated by morphological and spelling
variants. This problem is serious because it affects
the recall, which is especially important in
epidemiological studies.
Another problem is that such term spotting or
indexing approach would have to be intelligent
enough to identify the search terms in negated and
other contexts that would render documents
containing these terms irrelevant. A note
containing ?no evidence of heart failure? should
not be retrieved, for example. Identifying negation
reliably and, more importantly, its scope is far
from trivial and is in fact a notoriously difficult
problem in Linguistics [1]. This problem is slightly
less serious than the completeness problem since it
only affects precision which is less important in the
given context than recall.
In order to be able to correctly identify whether
a given patient note contains evidence that the
patient is relevant to a congestive heart failure
study, one has to ?understand? the note. Currently,
there are no systems capable of human-like
?understanding? of natural language; however,
there are methods that allow at least partial
solutions to the language understanding problem
once the problem is constrained in very specific
ways.  One such constraint is to treat language
understanding as a classification problem and to
use available machine learning approaches to
automatic classification to solve the problem.
Clearly, this is a very limited view of language
understanding but we hypothesize that it is
sufficient for the purposes referred to in this paper.
2 Previous work
The classification problems that have been
investigated in the past are just as varied as the
machine learning algorithms that have been used to
solve these problems. Linear Least Squares Fit [2],
Support Vector Machines, Decision trees,
Bayesean learning [3], symbolic rule induction [4],
maximum entropy [5], expert networks [6] are just
a few that have been applied to classifying e-mail,
Web pages, newswire articles, medical reports
among other documents.
Aronow et al [7] have investigated a problem
very similar to the one described in this papers.
They developed an ad hoc classifier based on a
variation of relevance feedback technique for
mammogram reports where the reports were
classified into three ?bins?: relevant, irrelevant and
unsure. One of the features of the text processing
system they used had to do with the ability to
detect and take into account negated elements of
the reports.
Wilcox et al [8] have experimented with a
number of classification algorithms for identifying
clinical conditions such as congestive heart failure,
chronic obstructive pulmonary disease, etc. in
raidograph reports. They found that using an NLP
system such as MedLEE (Medical Language
Extraction and Encoding System) and domain
knowledge sources such as UMLS? [9] for feature
extraction can significantly improve classification
accuracy over the baseline where single words are
used to represent training samples.
Jain and Friedman [10] have demonstrated the
feasibility of using MedLEE for classifying
mammogram reports. Unlike Wilcox  [8], this
work does not use an automatic classifier, instead,
it uses the NLP system to identify findings that are
considered suspicious for breast cancer.
3 NaiveBayes vs. Perceptron
We experimented with two widely used
machine learning algorithms, Perceptron and Na?ve
Bayes, in order to train models capable of
distinguishing between clinical notes that contain
sufficient evidence of the patient having the
diagnosis of congestive heart failure (positive
examples) from notes that do not contain such
evidence (negative examples). The choice of the
problem was dictated by a specific grant aimed at
studying patients with congestive heart failure.
The choice of the algorithms was largely
dictated by efficiency considerations. Both
Perceptron and Na?ve Bayes belong to a family of
linear classifiers which tend to be computationally
more manageable on large feature sets like the one
we are addressing than other algorithms. Damerau
et al [11] show on the Reuters corpus that sparse
feature implementations of linear algorithms are
capable of handling large feature sets. We used a
sparse feature implementation of these two
algorithms available in the SNoW (Sparse
Networks of Winnows) Version 2.1.2 package
[12]. Perceptron and Na?ve Bayes classifiers.
Perceptron is a simple iterative learning
algorithm that represents in its simplest form a
two-layer (input/output) neural network where
each node in the input layer is connected to each
node in the output layer. A detailed description can
be found in [13] and [14]. There are several well
known limitations of this algorithm. The most
significant is that the simple Perceptron is unable
to learn non-linearly separable problems. In order
for this algorithm to work, one should be able to
draw a hyperplane in the training data feature
space that will linearly separate positive examples
from negative. With large multidimensional feature
spaces, it is hard to know a priori whether the
space is linearly separable; however, a good
indication of that can be gleaned from the
classification accuracy testing on several folds of
training/testing data. If the accuracy results show
large fluctuations between folds, then that would
be a good indication that the space is not linearly
separable. On the other hand if the standard
deviation on such a cross-validation task is
relatively small, then one could be reasonably
certain that Perceptron is a usable technique for the
problem.
The other less serious limitation is that there is
a chance that the algorithm will falsely conclude
convergence in a local minimum on the error
function curve without reaching the global
minimum, which could also account for low or
inconsistent accuracy results. This limitation is less
serious because it can be controlled to some extent
with the learning rate parameter, which sets the
amount by which the weights are adjusted each
time Perceptron makes a classification error during
training [14].
Na?ve Bayes does not have the limitations of
Perceptron, but does have limitations of its own.
The Bayes decision rule chooses the class that
maximizes the conditional probability of the class
given the context in which it occurs:
(1) C` = argmax  )|()(
1
CVPCP
n
j
j?
=
Here, C` is the chosen category, C is the set of
all categories and Vj is the context. Na?ve Bayes
decision algorithm makes a simplifying
assumption that the words in Vj are independent of
each other. A particular implementation of the
Na?ve Bayes decision rule based on the
independence assumption to text categorization
and word sense disambiguation problems is also
known as ?bag of words? approach [13]. This
approach does not attempt to take into account any
sort of possible dependency between the individual
words in any given context, in fact it assumes that
the word ?heart? and the word ?failure?, for
example, occur completely independently of each
other. Theoretically, such assumption makes Na?ve
Bayes classifiers very unappealing for text
categorization problems, but in practice it has been
shown to perform well on a much greater range of
domains than the theory would support.
The common feature between the two
techniques is that both are linear classifiers and are
relatively efficient which makes them attractive for
learning from large feature sets with lots of
training samples.
4 CHF pilot study
As part of preliminary grant work to investigate
and evaluate incidence, outcome, and etiology
trends of heart failure, a pilot study for prospective
recruitment using term spotting techniques was
tested.  Prospective recruitment was needed for
rapid case identification with 24 hours of newly
diagnosed heart failure patients.
Within Mayo Clinic approximately 75% of
clinical dictations are electronically transcribed on
the date of diagnosis allowing them to be
processed using natural language techniques.
Using the terms ?cardiomyopathy, heart failure,
congestive heart failure, pulmonary edema,
decompensated heart failure, volume overload, and
fluid overload? all electronic outpatient,
emergency department, and hospital dismissal
notes were processed.  These results were reviewed
by trained nurse abstractors to determine if this
technique could provide identification of patients
with clinically active heart failure.  Using the term
spotting technique no cases were omitted as
compared to standard human diagnostic coding
methods of final diagnosis.  This pilot provided a
valid basis for using term spotting for prospective
recruitment; however, the nurse abstractors
reported filtering out a large number of documents
that were irrelevant to the query, thus indicating
that there was room for improvement especially in
precision. These were not quantified at the time.
The results derived from the test sets used for the
study described in this paper display similar
tendencies.
5 Human Expert Agreement
For testing a classifier, it is important to have a test
bed that contains positive as well as negative
examples that have been annotated by human
experts. It is also important to establish  some sort
of an agreement between annotators. For this study
we used a test bed created with a specific focus on
the diagnosis regarding the patient described
within the medical document for a separate pilot
study of agreement between annotators (de Groen
et al, p. c.).
One of the topics selected for this test bed
creation study included congestive heart failure.
For each topic, 90 documents were selected for
evaluation. Seventy of the 90 documents were
chosen from documents with a high likelihood of
containing diagnostic information regarding the
topic of inquiry. Specifically, thirty-five
documents were randomly selected from a pool of
documents based on a coded final diagnosis; thirty-
five documents were randomly selected from a
pool of documents based on a textual retrieval of
lexical surface forms (term spotting). The final
twenty documents were randomly selected from
the remaining documents, not originally included
in the coded or text identified collections. A group
of Emeritus physicians acted as the human experts
for this annotation task. The experts were
instructed to determine whether the information
contained in the clinical note could support
inclusion of the patient in a clinical/research
investigation, if such investigation was centered on
patients having - at the time the note was created -
the topic of inquiry.
Each document was judged by three physicians
on the following scale: (confirmed-probable-
indeterminate-probably not-definitely not). For the
purposes of our study we collapsed ?confirmed?
and ?probable? categories into one ?positive?
category. We also collapsed ?probably not? and
?definitely not? into a ?negative? category. The
?indeterminate? category happened to include such
artifacts as differential diagnosis as well as
uncertain judgements and therefore was ignored
for our purposes. The agreement on this particular
topic happened to be low: only 31% of the
instances were agreed upon by all three experts;
therefore, we decided to use the agreed upon
subset of the notes only for testing our approach.
The low level of agreement was partly attributable
to the breadth of the topic and, partly, to how the
instructions were interpreted by the experts.
Despite the low level of agreement, we were able
to select a subset of 26 documents where all three
annotators agreed. These were the documents
where all three annotators assigned either the
?positive? or the ?negative? category. 7 documents
were judged as ?positive? and 19 were judged  as
?negative? by all three experts.
6 Feature extraction
Arguably, the most important part of training any
text document classifier is extracting relevant
features from the training data. The resulting data
set looks like a set of feature vectors where each
vector should represent all the relevant information
encoded in the document and as little as possible of
the irrelevant information. To capture the relevant
information and give it more weight, we used two
classification schemes: MeSH (Medical Subjects
Headings) [15]and HICDA (Hospital International
Classification of Diseases Adaptation) [16]. The
MeSH classification is available as part of the
UMLS (Unified Medical Language System)
compiled and distributed by the National Library
of Medicine (NLM) [9]. HICDA is a hierarchical
classification with 19 root nodes and 4,334 leaf
nodes. Since 1975, it has been loosely expanded  to
comprise 35,676 rubrics or leaf nodes. It is an
adaptations of ICD-8, which is the 8th edition of the
International Classification of Diseases. HICDA
contains primarily diagnostic statements, whereas
MeSH is not limited to diagnostic statements and
therefore the two complement each other. It should
also be noted that, for mapping the text of clinical
notes to these two ontologies, in addition to the
text phrases present in HICDA and MeSH, some
lexical and syntactic variants found empirically in
medical texts were also added. For MeSH, these
variants were derived from MEDLINE articles by
UMLS developers and for HICDA, the variants
came from coded diagnoses. Having these lexical
and syntactic variants in conjunction with text
lemmatization made the job of mapping relatively
easy. Text lemmatization was done using the
Lexical Variant Generator?s (lvg1) ?norm? function
also developed at NLM.
For the purposes of this experiment, we
represented each document as a mixed set of
features of the following types: (MeSH code
mappings, HICDA code mapping, Single word
tokens, Demographic data). First, MeSH and
HICDA mappings were identified by stemming
and lowercasing all words in the notes and finding
                                                          
1 umlslex.nlm.nih.gov
their matches in the two ontologies. Next, stop
words were deleted from the text that remained
unmapped. The remaining words were treated as
single word token features. In addition to these
lexical features, we used a set of demographic
features such as age, gender, service code (the type
of specialty provider where the patient was seen (e.
g. ?cardiology?)) and death indicator (whether the
patient was alive at the time the note was created).
Since age is a continuous feature, we had to
discretize it by introducing ranges A-N arbitrarily
distributed across 5 year intervals from 0 to over
70 years old. For this experiment, features that
occurred less than 2 times were ignored. The
extracted feature ?vocabulary? consists of 11,118
unique features. Table 1 shows the breakdown of
the feature vocabulary by type.
Feature type N features Proportion
MeSH headings 6631 60 %
HICDA categories 2721 24 %
Single words 1635 15 %
Demographic features 131 01 %
Totals 11,118 100 %
Table 1 Breakdown of training features by type.
7 Experimental Setup
Both Na?ve Bayes and Perceptron were trained on
the same data and tested using a 10-fold cross-
validation technique as well as a held-out test set
of 26 notes mentioned in section 4.
7.1 Data
Two types of annotated testing/training data were
used in this study. The first type (Type I) is the
data generated by medical coders for the purpose
of conceptual indexing of the clinical notes. The
second type (Type II) is the data annotated by
Emeritus physicians (experts).
For Type I data, a set of clinical notes for 6
months of year 2001 was collected resulting in a
corpus of 1,117,284 notes. Most of these notes
contain a set of final diagnoses established by the
physician and coded using the HICDA
classification by specially trained staff. The coding
makes it easy to extract a set of notes whose final
diagnoses suggests that the patient has congestive
heart failure or a closely related condition or
symptom like pulmonary edema. Once this
positive set was extracted (2945 notes), the
remainder was randomized and a similar set of
negative samples was extracted (4675 notes). The
total size of the corpus is 7620 notes. Each note
was then run through feature extraction and the
resulting set was split into 10 train/test folds by
randomly selecting 20% of the 7620 notes to set
aside for testing for each fold.
Type II data set was split into two subsets: a
complete agreement (TypeII-CA) set and a partial
agreement set (TypeII-PA). The complete
agreement set was created by taking 26 notes that
were reliably categorized by the experts with
respect to congestive heart failure specifically.
These 26 notes represent a set where all three
annotators agreed at least to a large extent on the
categorization. ?A large extent? here means that all
three annotators labeled the positive samples as
either ?confirmed? or ?probable? and the negative
samples as either ?probably not? or ?definitely
not?. The set contains 7 positive and 19 negative
samples. The partial agreement set was created by
labeling all samples for which at least one expert
made a positive judgement and no experts made a
?negative? judgement as ?positive? and then
labeling all samples for which at least one expert
made a negative judgement and no experts made a
positive judgements as ?negative?. This procedure
resulted in reducing the initial set of 90 samples to
74 of which 21 were positive and 53 were negative
for congestive heart failure. This partial agreement
set is obviously weaker in its reliability but it does
provide substantially more data to test on and
would enable us to judge, at the very least, the
consistency of the automatic classifiers being
tested.   
7.2 Training
The following parameters were used for training
the classifiers. Na?ve Bayes was used with the
default smoothing parameter of 15. For Perceptron,
the most optimal combination of parameters was to
have the learning rate set at 0.0001 (very small
increments in weights), the error threshold was set
at 15. The algorithm with these settings was run for
1000 iterations.
7.3 Results
Standard classifier accuracy computation [13] for
binary classifiers was used.
(2) FNFPTNTP
TNTPAcc
+++
+
?= 100
Where TP represents the number of times the
classifier guessed a correct positive value (true
positives), TN is the number of times the classifier
correctly guessed a negative value (true negatives),
FP is the number of times the classifier predicted a
positive value but the correct value was negative
(false positives) and the FN (false negatives) is the
inverse of FP.
In addition to standard accuracy, positive
predictive value was also used. It is defined as:
(3) FPTP
TPPPV
+
= *100
Where TP+FP constitute all positive samples in
the test data set. We are interested in positive
predictive value because of the strong preference
towards perfect recall in document retrieval for
epidemiological studies, even if it comes at the
expense of precision. The rule is that it is better to
identify irrelevant data that can be discarded upon
review than to miss any of the relevant patients.
First, we established a baseline by running a a
very simple term spotter that looked for the CHF-
related terms mentioned in Section 2 (and their
normalized variants) in the collection of
normalized2 documents from the Type II data set.
The accuracy of the term spotter is 56% on Type
II-CA set and 54% on Type II-PA set. Positive
predictive value on Type II-CA set is 85% and on
Type II-PA set ? 71%. The positive predictive
value on Type II-CA set reflects the spotter
missing only 1 document out of 7 identified as
positive by the experts. The results are summarized
in Tables 3 and 4.
The results of testing the two classifiers are
presented in Table 2. Na?ve Bayes algorithm
achieves 82.2% accuracy, whereas Perceptron gets
86.5%. The standard deviation on the Perceptron
classifier results appears to be relatively small,
which leads us to believe that this particular
classification problem is linearly separable. The
difference of 4.3% happens to be statistically
significant as evidenced by a t-test at 0.01
                                                          
2 normalization was done with the lvg stemmer
(umlslex.nlm.nih.gov)
Fold Na?ve Bayes Perceptron Delta
PPV (%) Acc (%) PPV (%) Acc (%) PPV (%) Acc (%)
1 89.21 84.06 78.42 88.39 -10.79 4.33
2 88.16 82.41 74.88 85.30 -13.28 2.89
3 89.34 82.74 75.74 86.09 -13.61 3.35
4 90.77 82.02 79.62 87.07 -11.15 5.05
5 90.54 82.07 76.51 86.54 -14.03 4.47
6 89.55 82.74 80.27 87.40 -9.29 4.66
7 88.16 82.41 74.88 85.30 -13.28 2.89
8 88.10 81.16 78.62 86.28 -9.48 5.12
9 89.26 81.69 79.36 86.68 -9.90 4.99
10 88.12 80.45 76.59 85.89 -11.53 5.44
Mean 89.12 82.18 77.49 86.49 -11.63 4.32
Stdev 0.99 0.009 2.01 0.02
Table 2. Classification test results illustrating the differences between Perceptron and Na?ve Bayes.
confidence level. The difference in the positive
predictive value is also significant, however, is it
inversely related to the difference in accuracy.
Perceptron models perform on average 11 absolute
percentage points worse than Na?ve Bayes models.
Table 1 shows results that represent the
accuracy of the classifiers on classifying the Type I
test data that has been generated by medical
coders. Clearly, Type I data is not generated in
exactly the same way as Type II. Although Type I
data is captured reliably and is highly accurate,
Type II data is classified specifically with respect
to congestive heart failure only, by expert
physicians and, we believe, reflects the nature of
the task at hand a little better.
In order to test the classifiers on Type II data,
we re-trained them on the full set of 7620 notes of
Type I data using the same parameters as were
used for the 10-fold cross-validation test. The
results of testing the classifiers on Type II-CA data
(complete agreement) are presented in Table 3.
Classifier PPV (%) Acc (%)
NaiveBayes 100 69.2
Perceptron 85 76.92
TermSpotter 85 56
 Table 3. Test results for Type II-CA data
(annotated by retired physicians with complete
agreement).
These results are consistent with the ones
displayed in Table 2 in that Perceptron tends to be
more accurate overall but less so in predicting
positive samples. Table 4 summarizes the same
results for Type II-PA test set and the results
appear to be oriented in the same general direction
as the ones reported in Table 2 and 3.
Classifier PPV (%) Acc (%)
NaiveBayes 95 57
Perceptron 86 65
TermSpotter 71 54
 Table 4. Test results for Type II-PA data
(annotated by retired physicians with partial
agreement).
From a practical standpoint, the results
presented here are interesting in that they suggest
that the most accurate classifier may not be the
most useful for a given task. In our case, if we
were to use these classifiers for routing a stream of
electronic clinical notes, the gains in precision that
would be attained with the more accurate classifier
would most likely be wiped out by the losses in
recall since recall is more important for our
particular task than precision. However, for a
different task that may be more focused on
precision, obviously, Perceptron would be a better
choice.
Finally, both Perceptron and Na?ve Bayes
performance appears to be superior to the baseline
performance of the term spotter. Clearly such
comparison is only an indicator because the term
spotter is very simple. It is possible that a more
sophisticated term spotting algorithm may be able
to infer semantic relations between various terms
and be able to compensate for misspellings and
carry out other functions resulting possibly in
better performance. However, even the most
sophisticated term spotter will only be as good as
the initial list of terms supplied to it. The
advantage of automatic classification lies in the
fact that classifiers encode the terminological
information implicitly which alleviates the need to
rely on managing lists of terms and the risk of such
lists being incomplete. The disadvantage of
automatic classification is that the classifier?s
performance is heavily data dependent, which
raises the need for sufficient amounts of annotated
training data and limits this methodology to
environments where such data is available.
The error analysis of the misclassified notes
shows that a more intelligent feature selection
process that takes into account discourse
characteristics and semantics of negation in the
clinical notes is required. For example, one of the
misclassified notes contained ?no evidence of
CHF? as part of the History of Present Illness
(HPI) section. Clearly, the presence of a particular
concept in a clinical note is not always relevant.
For example, various terms and concepts may
appear in the Review of Systems (ROS) section of
the note; however, the ROS section is often used as
a preset template and may have little to do with the
present condition. Same is true for other sections
such as Family History, Surgical History, etc. It is
not clear at this point which sections are to be
included in the feature selection process. The
choice will most likely be task specific.
The current study did not use any negation
identification, which we think accounted for some
of the errors. As one of the future steps, we are
planning to implement a negation detector such as
the NegExpander used by Aronow et al[7].
8 Conclusion
In this paper, we have presented a methodology for
generating on-demand binary classifiers for
filtering clinical patient notes with respect to a
particular condition of interest to a clinical
investigator. Implementation of this approach is
feasible in environments where some quantity of
coded clinical notes can be used as training data.
We have experimented with HICDA codes;
however, other coding schemes may be usable or
even more usable as well.
We do not claim that either Na?ve Bayes or the
Perceptron are the best possible classifiers that
could be used for the task of identifying patients
with certain conditions. All we show is that either
one of these two classifiers is reasonably suitable
for the task and has the benefits of computational
efficiency and simplicity. The results of the
experiments with the classifiers suggest that
although Perceptron has higher accuracy than the
Na?ve Bayes classifier overall, its positive
predictive value is significantly lower.  The latter
result makes it less usable for a practical binary
classification task focused on identifying patient
records that have evidence of congestive heart
failure. It may be worth while pursuing an
approach that would use the two classifiers in
tandem. The classifier with the highest PPV would
be used to make the first cut to maximize recall
and the more accurate classifier would be used to
rank the output for subsequent review.
Acknowledgements
We are thankful to the investigators working on the
?Heart Failure? grant RO1-HL-72435 who have
provided valuable input and recommendations for
this research.
References
1. Horn, L., A Natural History of Negation. 1989,
Chicago: University Of Chicago Press.
2. Yang, Y. and C. Chute. A linear least squares fit
mapping method for information retrieval from
natural language texts. in 14th International
Conference on Computational Linguistics
(COLING). 1992.
3. Lewis, D. Naive (Bayes) at forty: The
independence assumption in information
retrieval. in ECML-98. 1998.
4. Johnson, D., et al, A deci-sion-tree-based
symbolic rule induction system for text
categorization. IBM Systems Journal, 2002.
41(3).
5. Nigam, K., J. Lafferty, and A. McCullum. Using
Maximum Entropy for Text Classification. in
IJCAI-99 Workshop on Machine Learning for
Information Filtering. 1999.
6. Yang, Y. Expert Network: Combining Word-
based Matching and Human Experiences in
Text Categorization and Retrieval. in SIGIR.
1994.
7. Aronow, D., F. Fangfang, and, and B. Croft, Ad
Hoc Classification of Radiology Reports.
Journal of Medical Informatics Association,
1999. 6(5).
8. Wilcox, A., et al Using Knowledge Sources to
Improve Classification of Medical Text
Reports. in KDD-2000. 2000.
9. NLM, UMLS. 2001, National Library of
Medicine.
10. Jain, N. and C. Friedman. Identification of
finding suspiciois for breast cancer based on
natural language processing of mammogram
reports. in AMIA. 1997.
11. Damerau, F., et al Experiments in High
Dimensional Text Categorization. in ACM
SIGIR International Conference on Information
Re-trieval. 2002.
12. Carlson, A.J., et al, SNoW User's Guide,
Cognitive Computations Group - University of
Illinois at Urbana/Champaign.
13. Manning, C. and H. Shutze, Foundations of
Statistical Natural Language Processing. 1999,
Cambridge, MA: MIT Press.
14. Anderson, J., Introduction to Neural Networks.
1995, Boston: MIT Press.
15. NLM, Fact sheet Medical Subject Headings
(MeSH?). 2000.
16. Commission on Professional and Hospital
Activities, Hospital Adaptation of ICDA. 2nd
ed. Vol. 1. 1973, Ann Arbor, MI: Commission
on Professional and Hospital Activities.
Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech 
Information 
Serguei  PAKHOMOV 
Division of Medical Informatics 
Research, Mayo Clinic  
Rochester, MN 
Pakhomov.Serguei@mayo.edu 
Anni  CODEN 
IBM, T.J. Watson Research 
Center, 
Hawthorne, NY 10532 
anni@us.ibm.com  
Christopher   CHUTE 
Division of Medical 
Informatics Research, Mayo 
Clinic Rochester, MN 
Chute@mayo.edu 
 
Abstract 
This paper presents a project whose main goal 
is to construct a corpus of clinical text 
manually annotated for part-of-speech 
information. We describe and discuss the 
process of training three domain experts to 
perform linguistic annotation. We list some of 
the challenges as well as encouraging results 
pertaining to inter-rater agreement and 
consistency of annotation. We also present 
preliminary experimental results indicating the 
necessity for adapting state-of-the-art POS 
taggers to the sublanguage domain of medical 
text. 
1 Introduction 
Having reliable part-of-speech (POS) 
information is critical to successful implementation 
of Natural Language Processing (NLP) techniques 
for processing unrestricted text in the biomedical 
domain. State-of-the-art automated POS taggers 
achieve accuracy of 93% - 98% and the most 
successful implementations are based on statistical 
approaches to POS tagging. Taggers based on 
Hidden Markoff Model (HMM) technology 
currently appear to be in the lead. The prime public 
domain examples of such implementations include 
the Trigrams?n?Tags tagger (Brandts 2000), Xerox 
tagger (Cutting et al 1992) and LT POS tagger 
(Mikheev 1997). Maximum Entropy (MaxEnt) 
based taggers also seem to perform very well        
(Ratnaparkhi 1996, Jason Baldridge, Tom Morton, 
and Gann Bierner  http://maxent.sourceforge.net ).  
One of the issues with statistical POS taggers is 
that most of them need a representative amount of 
hand-labeled training data either in the form of a 
comprehensive lexicon and a corpus of untagged 
data or a large corpus of text annotated for POS or 
a combination of the two. Currently, most of the 
POS tagger accuracy reports are based on the 
experiments involving Penn Treebank data 
(Marcus, 1993). The texts in Treebank represent 
the general English domain. It is not entirely clear 
how representative the general English language 
vocabulary and structure are of a specialized sub-
domain such as clinical reports.  
A well-recognized problem is that the accuracy 
of all current POS taggers drops dramatically on 
unknown words. For example, while the TnT 
tagger performs at 97% accuracy on known words 
in the Treebank, the accuracy drops to 89% on 
unknown words (Brandts, 2000). The LT POS 
tagger is reported to perform at 93.6-94.3% 
accuracy on known words and at 87.7-88.7% on 
unknown words using a cascading unknown word 
?guesser? (Mikheev, 1997). The overall results for 
both of  these taggers are much closer to the high 
end of the spectrum because the rate of the 
unknown words in the tests performed on the Penn 
Treebank corpus is generally relatively low ? 2.9% 
(Brandts, 2000). From these results, we can 
conclude that the higher the rate of unknown 
vocabulary, the lower the overall accuracy will be, 
necessitating the adaptation of the taggers trained 
on Penn Treebank  to sublanguage domains with 
vocabulary that is substantially different from the 
one represented by the Penn Treebank corpus.  
Based on the observable differences between 
the clinical and the general English  discourse and 
POS tagging accuracy results on unknown 
vocabulary, it is reasonable to assume that a tagger 
trained on general English may not perform as well 
on clinical notes, where the percentage of unknown 
words will increase. To test this assumption, a 
?gold standard? corpus of clinical notes needs to be 
manually annotated for POS information. The 
issues with the annotation process constitute the 
primary focus of this paper. 
We describe an effort to train three medical 
coding experts to mark the text of clinical notes for 
part-of-speech information. The motivation for 
using medical coders rather than trained linguists is 
threefold. First of all, due to confidentiality 
restrictions, in order to develop a corpus of hand 
labeled data from clinical notes one can only use 
personnel authorized to access patient information. 
The only way to avoid it, is to anonymize the notes 
prior to POS tagging which in itself is a difficult 
and expensive process (Ruch et al 2000). Second, 
medical coding experts are well familiar with 
62
clinical discourse, which helps especially with 
annotating medicine specific vocabulary. Third, 
the fact that POS tagging can be viewed as a 
classification task makes the medical coding 
experts highly suitable because their primary 
occupation and expertise is in classifying patient 
records for subsequent retrieval.    
We show that, given a good set of guidelines, 
medical coding experts can be trained in a limited 
amount of time to perform a linguistic task such as 
POS annotation at a high level of agreement on 
both clinical notes and Penn Treebank data. 
Finally, we report on a set of training experiments 
performed with the TnT tagger (Brandts, 2000) 
using the Penn Treebank as well as the newly 
developed medical corpus.. 
2 Annotation  
Prior to this study, the three annotators who 
participated in it had a substantial experience in 
coding clinical diagnoses but virtually no 
experience in POS markup. The training process 
consisted of a general and rather superficial 
introduction to the issues in linguistics as well as 
some formal training using the POS tagging 
guidelines developed by Santoriny (1991) for 
tagging Penn Treebank data. The formal training 
was followed by informal discussions of the data 
and difficult cases pertinent to the clinical notes 
domain which often resulted in slight 
modifications to the Penn Treebank guidelines. 
The annotation process consisted of 
preprocessing and editing. The pre-processing 
includes sentence boundary detection, tokenization 
and priming with part-of-speech tags generated by 
a MaxEnt tagger (Maxent 1.2.4 package (Baldridge 
et al)) trained on Penn Treebank data. 
Automatically annotated notes were then presented 
to the domain experts for editing. 
3 Annotator agreement 
In order to establish reliability of the data, we 
need to ensure internal as well as external 
consistency of the annotation. First of all, we need 
to make sure that the annotators agree amongst 
themselves (internal consistency) on how they 
mark up text for part-of-speech information. 
Second, we need to find out how closely the 
annotators generating data for this study agree with 
the annotators of an established project such as 
Penn Treebank (external consistency). If both tests 
show relatively high levels of agreement, then we 
can safely assume that the annotators in this study 
are able to generate part-of-speech tags for 
biomedical data that will be consistent with a 
widely recognized standard and can work 
independently of each other thus tripling the 
amount of manually annotated data.  
3.1 Methods 
Two types of measures of consistency were 
computed ? absolute agreement and Kappa 
coefficient. The absolute agreement (Abs Agr) was 
calculated by dividing the total number of times all 
annotators agreed on  a tag over the total number 
of tags. 
Kappa coefficient is given in (1) (Carletta 1996) 
(1) 
)(1
)()(
EP
EPAPKappa ?
?=  
 
where P(A) is the proportion of times the 
annotators actually agree and P(E) is the 
proportion of times the annotators are expected to 
agree due to chance3. 
The Absolute Agreement is most informative 
when computed over several sets of labels and 
where one of the sets represents the ?authoritative? 
set. In this case, the ratio of matches among all the 
sets including the ?authoritative? set to the total 
number of labels shows how close the other sets 
are to the ?authoritative? one. The Kappa statistic 
is useful in measuring how consistent the 
annotators are compared to each other as opposed 
to an authority standard.   
3.2 Annotator consistency 
In order to test for internal consistency, we 
analyzed inter-annotator agreement where the three 
annotators tagged the same small corpus of clinical 
dictations.  
 
File ID Abs agr. Kappa N Samples 
1137689 93.24% 0.9527 755 
1165875 94.59% 0.9622 795 
1283904 89.79% 0.9302 392 
1284881 90.42% 0.9328 397 
1307526 84.43% 0.8943 347 
Total   2686 
Average 90.49% 0.9344  
Table 1. Annotator agreement results based on 5 
clinical notes 
                                                     
3 A  very detailed explanation of the terms used in the formula for 
Kappa computation as well as concrete examples of how it is 
computed are provided in Poessio and Vieira (1988). 
63
The results were compared and the Kappa-
statistic was used to calculate the inter-annotator 
agreement. The results of this experiment are 
summarized in Table 1. For the absolute 
agreement, we computed the ratio of how many 
times all three annotators agreed on a tag for a 
given token to the total number of tags. 
Based on the small pilot sample of 5 clinical 
notes (2686 words), the Kappa test showed a very 
high agreement coefficient ? 0.93. An acceptable 
agreement for most NLP classification tasks lies 
between 0.7 and 0.8 (Carletta 1996, Poessio and 
Vieira 1988). Absolute agreement numbers are 
consistent with high Kappa as they show an 
average of 90% of all tags in the test documents 
assigned exactly the same way by all three 
annotators. 
The external consistency with the Penn Treebank 
annotation was computed using a small random 
sample of 939 words from the Penn Treebank 
Corpus annotated for POS information.  
 
Annotator Abs agr 
A1 88.17% 
A2 87.85% 
A3 87.85% 
Average 87.95% 
Table 2. Absolute agreement results based on 5 
clinical notes with an ?authority? label set. 
The results in Table 2 show that the three 
annotators are on average 88% consistent with the 
annotators of the Penn Treebank corpus.  
3.3 Descriptive statistics for the corpus of 
clinical notes 
  The annotation process resulted in a corpus of 
273 clinical notes annotated with POS tags. The 
corpus contains 100650 tokens from 8702 types 
distributed across 7299 sentences. Table 3 displays 
frequency counts for the top most frequent 
syntactic categories. 
Category Count % total 
NN 18372 18% 
IN 8963 9% 
JJ 8851 9% 
DT 6796 7% 
NNP 4794 5% 
Table 3 Syntactic category distribution in the 
corpus of clinical notes. 
The distribution of syntactic categories suggests 
the predominance of nominal categories, which is 
consistent with the nature of clinical notes 
reporting on various patient characteristics such as 
disorders, signs and symptoms. 
Another important descriptive characteristic of 
this corpus is that the average sentence length is 
13.79 tokens per sentence, which is relatively short 
as compared to the Treebank corpus where the 
average sentence length is 24.16 tokens per 
sentence. This supports our informal observation 
of the clinical notes data containing multiple 
sentence fragments and short diagnostic 
statements. Shorter sentence length implies greater 
number of inter-sentential transitions and therefore 
is likely to present a challenge for a stochastic 
process.   
4 Training a POS tagger on medical data 
In order to test some of our assumptions 
regarding how the differences between general 
English language and the language of clinical notes 
may affect POS tagging, we have trained the 
HMM-based TnT tagger (Brandts, 2000) with 
default parameters at the tri-gram level both on 
Penn Treebank and the clinical notes data. We 
should also note that the tagger relies on a 
sophisticated ?unknown? word guessing algorithm 
which computes the likelihood of a tag based on 
the N last letters of the word, which is meant to 
leverage the word?s morphology in a purely 
statistical manner.  
The clinical notes data was split at random 10 
times in 80/20 fashion where 80% of the sentences 
were used for training and 20% were used for 
testing. This technique is a variation on the classic 
10-fold validation and appears to be more suitable 
for smaller amounts of data.  
We conducted two experiments. First, we 
computed the correctness of the Treebank model 
on each fold of the clinical notes data. We tested 
the Treebank model on the 10 folds rather than the 
whole corpus of clinical notes in order to produce 
correctness results on exactly the same test data as 
would be used for validation tests of models build 
from the clinical notes data. Then, we computed 
the correctness of each of the 10 models trained on 
each training fold of the clinical notes data using 
the corresponding testing fold of the same data for 
testing. 
 
Table 4 Correctness results for the Treebank 
model. 
Correctness was computed simply as the 
percentage of correct tag assignments of the POS 
tagger (hits) to the total number of tokens in the 
test set. Table 4 summarizes the results of testing 
the Treebank model, while Table 5 summarizes the 
Split Hits Total Correctness 
Average 21826.3 24309 89.79% 
64
testing results for the models trained on the clinical 
notes. 
 
The average correctness of the Treebank model 
tested on clinical notes is ~88%, which is 
considerably lower than the state-of-the-art 
performance of the TnT tagger - ~96%. Training 
the tagger on a relatively small amount of clinical 
notes data brings the performance much closer to 
the state-of-the-art ? ~95%. 
 
Table 5 Correctness results for the clinical notes 
model. 
5 Discussion 
The results of this pilot project are encouraging. 
It is clear that with appropriate supervision, people 
who are well familiar with medical content can be 
reliably trained to carry out some of the tasks 
traditionally done by trained linguists.  
This study also indicates that an automatic POS 
tagger trained on data that does not include clinical 
documents may not perform as well as a tagger 
trained on data from the same domain. A 
comparison between the Treebank and the clinical 
notes data shows that the clinical notes corpus 
contains 3,239 lexical items that are not found in 
Treebank. The Treebank corpus contains over 
40,000 lexical items that are not found in the 
corpus of clinical notes. 5,463 lexical items are 
found in both corpora.  In addition to this 37% out-
of-vocabulary rate (words in clinical notes but not 
the Treebank corpus), the picture is further 
complicated by the differences between the n-gram 
tag transitions within the two corpora. For 
example, the likelihood of a DT ? NN bigram is 1 
in Treebank and 0.75 in the clinical notes corpus. 
On the other hand, JJ ? NN transition in the 
clinical notes is 1 but in the Treebank corpus it has 
a likelihood of 0.73. This is just to illustrate the 
fact that not only the ?unknown? out-of-vocabulary 
items may be responsible for the decreased 
accuracy of POS taggers trained on general 
English domain and tested on the clinical notes 
domain, but the actual n-gram statistics may be a 
major contributing factor.    
6 Conclusion 
Several questions remain unresolved. First of all, 
it is unclear how much domain specific data is 
enough to achieve state-of-the-art performance on 
POS tagging. Second, given that it is somewhat 
easier to develop lexicons for POS tagging than to 
annotate corpora, we need to find out how 
important the corpus statistics are as opposed to a 
domain specific lexicon. In other words, can we 
achieve state-of-the-art performance in a 
specialized domain by simply adding the 
vocabulary from the domain to the POS tagger?s 
lexicon? We intend to address both of these 
questions with further experimentation. 
7 Acknowledgements 
Our thanks go to Barbara Abbot, Pauline Funk 
and Debora Albrecht for their persistent efforts in 
the difficult task of corpus annotation. This work 
has been carried out under the NLM Training 
Grant # T15  LM07041-19.  
References 
Baldridge, J., Morton, T., and Bierner, G URL: 
http://maxent.sourceforge.net 
Brandts, T (2000) ?TnT ? A Statistical Part-of-Speech 
Tagger.? In Proc. NAACL/ANLP-2000. 
Carletta,  J.  (1996). Assiessing agreement on 
classification tasks: The Kappa statistic. 
Computational Linguistics, 22(2) pp. 249-254.  
Cutting, D., Kupiec, J., Pedersen, J, and Sibun, P. A 
(1992). Practical POS Tagger. In Proc. ANLP?92. 
Jurafski D. and Martin J. (2000). Speech and Language 
Processing. Prentice Hall, NJ. 
Manning, C. and Shutze H. (1999). Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge, MA. 
Marcus, M., B. Santorini, and M. A. Marcinkiewicz 
(1993). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics 19, 
297-352. 
Mikheev, A. (1997). Automatic Rule Induction for 
Unknown-Word Guessing. Computational Linguistics 
23(3): 405-423  
Poessio, M. and Vieira, R. (1988). ?A corpus based 
investigation of definite description use? 
Computational Linguistics, pp 186-215. 
Ratnaparkhi A. (1996). A maximum entropy part of 
speech tagger. In Proceedings of the conference on 
empirical methods in natural language processing, 
May 1996, University of Pennsylvania 
Ruch P, Baud RH, Rassinoux AM, Bouillon P, Robert 
G. Medical document anonymization with a semantic 
lexicon. Proc AMIA Symp. 2000; 729-33. 
Santorini B. (1991). Part-of-Speech Tagging Guidelines 
for the Penn Treebank Project. Technical Report. 
Department of Computer and Information Science, 
University of Pennsylvania. 
UMLS. (2001). UMLS Knowledge Sources (12th ed.). 
Bethesda (MD): National Library of Medicine.   
 
 
Split Hits Total Correctness 
Average 23018.4 24309 94.69% 
65
Cluster Stopping Rules for Word Sense Discrimination 
Guergana Savova, Terry Therneau and Christopher Chute, 
Mayo Clinic, Rochester, MN, USA 
[Savova.Guergana;therneau;chute]@mayo.edu
Abstract
As text data becomes plentiful, unsuper-
vised methods for Word Sense Disam-
biguation (WSD) become more viable. A 
problem encountered in applying WSD 
methods is finding the exact number of 
senses an ambiguity has in a training cor-
pus collected in an automated manner. 
That number is not known a priori; rather 
it needs to be determined based on the 
data itself. We address that problem us-
ing cluster stopping methods. Such tech-
niques have not previously applied to 
WSD. We implement the methods of 
Calinski and Harabasz (1975) and Harti-
gan (1975) and our adaptation of the Gap 
statistic (Tibshirani, Walter and Hastie, 
2001). For evaluation, we use the WSD 
Test Set from the National Library of 
Medicine, whose sense inventory is the 
Unified Medical Language System. The 
best accuracy for selecting the correct 
number of clusters is 0.60 with the C&H 
method. Our error analysis shows that the 
cluster stopping methods make finer-
grained sense distinctions by creating ad-
ditional clusters. The highest F-scores 
(82.89), indicative of the quality of clus-
ter membership assignment, are compa-
rable to the baseline majority sense 
(82.63) and point to a path towards accu-
racy improvement via additional cluster 
pruning. The importance and significance 
of the current work is in applying cluster 
stopping rules to WSD. 
1 Introduction 
The dominant approach in word sense disam-
biguation (WSD) is based on supervised learning 
from manually sense-tagged text. While this is 
effective, it is quite difficult to get a sufficient 
number of manually sense-tagged examples to 
train a system. Mihalcea (2003) estimates that 
80-person years of annotation would be needed 
to create training corpora for 20,000 ambiguous 
English words, given 500 instances per word. 
For that reason, we are developing unsupervised 
knowledge-lean methods that avoid the bottle-
necks created by sense-tagged text. Unsupervised 
clustering methods utilize only raw corpora as 
their source of information, and there are grow-
ing amounts of general and specialized domain 
corpora available, e.g. biomedical domain cor-
pora.
Improvements in WSD methods would be of 
immediate value in indexing and retrievals of 
biomedical text given the explosion of biomedi-
cal literature as well as the rapid deployment of 
electronic medical records. Semantic/conceptual 
indexing and retrieval in that domain is often 
done in regard to the Unified Medical Language 
System (UMLS) developed at the National Li-
brary of Medicine (NLM) at the United States 
National Institutes of Health (NIH)1. It is impor-
tant to understand that the UMLS is significantly 
different than a dictionary, which is often the 
source of the sense inventory. Rather, the UMLS 
integrates more than 100 medical domain con-
trolled vocabularies such as SNOMED-CT2 and 
the International Classification of Diseases 
(ICD)3. UMLS has three main components. The 
first component, the Metathesaurus, includes all 
terms from the controlled vocabularies and is 
organized by concept, which is a cluster of terms 
representing the same meaning. Each concept is 
assigned a concept unique identifier (CUI), 
which is inherited by each term in the cluster. 
UMLS-based semantic indexing is based on CUI 
assignments. The second component, the Seman-
tic Network, groups the concepts into 134 types 
of categories and indicates the relationships be-
tween them. The Semantic Network is a coarse 
ontology of the concepts. The third component, 
the SPECIALIST lexicon, contains syntactic in-
formation for the Metathesaurus terms. 
1 http://www.nlm.nih.gov/pubs/factsheets/umls.html
2 http://www.snomed.org/ 
3 http://www.who.int/classifications/help/icdfaq/en/
9
MeSH, an ontology within UMLS, is heavily 
used for indexing biomedical scientific publica-
tions, e.g. Medline4. Hospitals, medical practices 
and biomedical research increasingly rely on the 
UMLS, or a subset ontology within it, to index 
and retrieve relevant information. It is estimated 
that approximately 7400 UMLS terms map to 
multiple concepts which creates ambiguity 
(Weeber, Mork and Aronson., 2001). Term am-
biguity has been pointed out to be one of the ma-
jor challenges for UMLS-based semantic index-
ing and retrieval (Weeber et al, 2001). For ex-
ample, ?cold? has the following six UMLS 
meanings, each with its own UMLS CUI: cold 
temperature, common cold, cold sensation, cold 
therapy, chronic obstructive lung disease 
(COLD), and Cold brand of chlorpheniramine-
phenylpropanolamine.  
The problem we are addressing in this paper is 
discovering the number of senses an ambiguous 
word has in a given corpus, which is a compo-
nent within a completely unsupervised WSD sys-
tem. For example, if a corpus of 1000 instances 
containing the word ?cold? has been compiled 
from patients medical records, how many ?cold? 
senses are in that corpus? This is a challenge any 
NLP system implementing WSD faces. To ad-
dress this problem, we apply cluster stopping 
rules in an automated way. 
The paper is organized as follows. Section 2 
overviews the related work on cluster stopping 
rules. Section 3 outlines our methods, tools, fea-
tures selection, test set and evaluation metrics. 
Section 4 presents the results and discusses them. 
Section 5 is the conclusions. 
2 Background and Related Work 
Our work is based on cluster analysis. Cluster 
analysis is often performed to discover the 
groups that the data naturally fall into. The num-
ber of groups is not known a priori; rather, it 
needs to be determined based on the data itself. 
Such methods or ?cluster stopping rules? usually 
rely on within-cluster dissimilarity/error (W(k))
metrics which in general exhibit a decline when 
the number of clusters increases. Splitting a natu-
ral group into subgroups reduces the criterion 
less than when well-separated clusters are dis-
covered. In those cases, the W(k) will not have a 
sharp decline as the instances are close. This 
phenomenon has been described in statistical 
literature as the ?elbow? effect as illustrated in 
4 http://www.nlm.nih.gov/pubs/factsheets/medline.html
Figure 1. Methods for locating the ?elbow have 
been the goal of many research studies (Hartigan, 
1975; Calinski and Harabasz, 1975; Milligan and 
Cooper, 1987; Tibshirani, Walter and Hastie, 
2001 among many). 
Milligan and Cooper (1985) offer the most 
comprehensive comparative study of the per-
formance of 30 stopping rules. They carry out 
their study on ?mildly truncated data from multi-
variate normal distributions, and one would not 
expect their ranking of the set of stopping rules 
to be reproduced exactly if a different cluster-
generating strategy were adopted.? (Gordon, 
1999, p. 61). The five rules which were the top 
performance in the Milligan and Cooper study  
are Calinski and Harabasz (1974) a.k.a. C&H, 
Goodman and Kruskal (1954), C index (Hubert 
and Schultz, 1976), Duda and Hart (1973) and 
Beale (1969). Tibshirani et al (2001) introduce 
the Gap statistic and compare its performance to 
the methods of Calinski and Harabasz (1974), 
Krzanowski and Lai (1985), Hartigan (1975), 
and the Silhouette method (Kaufman and 
Rousseeuw, 1990). On the simulated and DNA 
microarray data Tibshirani and colleagues used 
for their experiments, the Gap statistic yields the 
best result. 
In general, stopping rules fall into two catego-
ries ? global and local (Gordon, 1999; Tibshirani 
et al, 2001). Global rules take into account a 
combination of within-cluster and between-
cluster similarity measures over the entire data. 
Global rules choose such k where that combined 
metric is optimal. Global rules, however, in most 
cases do not work for k=1, that is they do not 
make predictions of when the data should not be 
partitioned at all. Global rules look at the entire 
data over k number of clusters. Local rules, on 
the other hand, are based only on a given k solu-
tion or individual pairs of clusters and test 
whether they should be grouped together. They 
need a threshold value or a significance level, 
which depends on the specific data and in most 
cases have to be empirically determined. 
3 Methodology
3.1 Overview
In this study, we explore three cluster stopping 
methods as applied to unsupervised WSD ? Har-
tigan (1975), Calinski and Harabasz (1974), and 
the Gap statistic (Tibshirani et al, 2001). The 
data to be clustered is instances of context sur-
rounding each ambiguity. Each instance is con-
verted into a feature vector where the features are 
10
ngrams (unigrams or bigrams) and each cell is 
the frequency of occurrence of a unigram or bi-
gram or the log-likelihood of a bigram occurring
in that particular instance after applying a feature
selection method. The clustering algorithm for 
this set of experiments is agglomerative cluster-
ing (see Section 3.5 for a more detailed descrip-
tion).
Our goal is to group contexts into separate
clusters based on the underlying sense of the am-
biguous word. Thus, the observations are con-
texts and the features are the identified lexical 
features (i.e. significant word(s)) that represent
the contexts. Our observed data matrix generally
shows the following characteristics ?
1) it is discrete
2) it is high dimensional/multivariate
3) it can be real valued or integer, or binary
4) it is sparse; while the number of features
can be in few hundreds, contexts have a length 
limit (ignoring the commonly occurring ?closed
class words? like ?the?, ?an?, ?on? etc.) 
5) it represents a distribution of contexts that
is generally skewed.
Following is our motivation for choosing the 
three cluster stopping rules. Hartigan (1975) and
Calinski and Harabasz (1974) have been consis-
tently used as baselines in a number of studies, 
e.g. Tibshirani et al (2001). The Hartigan
method is computationally simple and efficient
and unlike C&H, it is defined for k=1. The C&H 
method was ranked the top among 30 stopping
rules in the comprehensive study conducted by
Milligan and Cooper (1975). The Gap statistic 
(Tibshirani et al, 2001) is a fairly recent method
that has gained popularity by showing excellent
results when applied to the bio domain, e.g. clus-
tering DNA mircoarray data. None of the meth-
ods, however, have been applied or adapted to
WSD.
3.2 Calinski and Harabasz (1975) Method
The C&H method is reported to perform the best 
among 30 stopping rules (Milligan and Cooper, 
1985). C&H is a global method. The Variance
Ratio Criteria C&H uses is 
kn
kWGSS
k
kBGSS
kVRC

 )(1
)(
)(
where BGSS (between group sum of squares) is
the sum of the dispersions between the k cluster
centroids and the general centroid; WGSS
(within-group sum of squares) is the sum of each
cluster?s dispersion of its cluster members
(measured by the sum of squared distances be-
tween each member and the cluster centroid) 
weighed by the number of cluster members; k is 
the number of clusters and n is the number of 
instances. The distance used is the Euclidean dis-
tance. As Calinski and Harabasz point out, VRC
?is analogous to the F-statistic in univariate 
analysis? (Calinski and Harabasz, 1975, p. 10).
C&H seeks to maximize VRC.
3.3 Hartigan (1975) Method 
Hartigan (1975) proposes a cluster stopping rule: 
)1(1
)1(
)()( ??
???
?  knkWGSS
kWGSSkH
where n is the total number of instances to be 
clustered, k is the number of clusters and
WGSS(k) is the total sum of squared distances of
cluster members from their cluster centroid in all 
clusters when clustered in k clusters. 
H(k) is used to decide when k+1 clusters are
needed rather than k clusters. Its distribution ap-
proximates the F distribution. A large value of 
H(k) would indicate that the addition of a cluster 
is warranted. Hartigan suggests that as a crude 
rule of thumb, values exceeding 10 justify in-
creasing the number of clusters from k to k+1 
(Hartigan, 1975, p. 91). Thus, a solution is the 
smallest k ? 1 such that H(k) ? 10. The method
can return 1 cluster as the optimal solution. Har-
tigan (1975) is a local method.
3.4 Gap Statistic Method 
In general, the ?gap? method compares the dif-
ference/gap between the within-cluster disper-
sion measure for the observed distribution and
that for an appropriate null distribution of the
data. Tibshirani and colleagues (Tibshirani et al, 
2001) start with the assumption of a single clus-
ter null model which is to be rejected in favor of 
a k-component model (k>1) if the observed data 
supports it. Tibshirani and colleagues use a uni-
form distribution as the null distribution of the
data to standardize the comparison between all
the W(k) over the various values of k where W(k)
is the pooled within cluster sum of squares 
around the cluster means (distance is squared 
Euclidean distance).  The uniform distribution is
the least favorable distribution and the most
likely to produce spurious clusters. However,
11
Tibshirani and colleagues also point out that the 
choice of an appropriate null distribution de-
pends on the data. Tibshirani and colleagues
compare the curve of log(W(k)) to the log(W*(k))
curve obtained from the reference uniformly dis-
tributed over the data. The estimated optimal
number of clusters is the k value where the gap 
between the two curves is the largest. Figure 1 is
an example of log(W(k)) to the log(W*(k)) curves
used in the computation of the Gap statistic. 
The two main advantages of the Gap statistic 
over various previously proposed ?stopping
rules? are its ability to work with data created by
almost any type of clustering and its ability to 
accurately estimate the optimal number of clus-
ters even for data that naturally falls into just one 
cluster. The Gap statistic is an application of pa-
rametric bootstrap methods to the clustering
problem. Unlike non-parametric methods, para-
metric techniques represent the observed data 
distribution. The basic strategy is to create multi-
ple random data sets over the observed distribu-
tion for which there are no clusters, apply the 
chosen clustering method to them, and tabulate 
the apparent decrease in within-cluster variation 
that ensues.  This gives a measure of optimism
with which to compare the clustering of the ob-
served data. 
The complete methodology can be broadly
classified into two important components namely
the reference distribution and the algorithm
which uses the reference distribution. We de-
scribe each of the two components below.
Figure 1: The functions log(W(k)) (observed) 
and log(W*(k)) (reference) used for computing 
the Gap statistic 
Reference Distribution Generation for an 
NLP Task
Here, we describe how we extend the generation
of the reference distribution over the observed
data to retain the characteristics mentioned at end
of section 3.1. We will use the observed data
shown in Table 1 as a running example. To simu-
late the structure of the observed data, the fol-
lowing features are to be emulated:
(a) Context length is the number of features 
that can occur in a context. Contexts can be sen-
tences, paragraphs, entire documents or just any
specified window size. In general, the number of
available features will be at least in the hundreds,
however, only a few might occur in a given con-
text, especially if the context is limited to the 
sentence the target ambiguity occurs in. Addi-
tionally, context length is influenced by the fea-
ture selection method ? if only very frequent 
lexical units are retained as features, then only 
those units will represent the context. Thus, a
context length could be very small compared to
the size of the feature set. In the example from
Table 1, context length is captured by the row 
marginals, e.g. the context length for Context1 is 
3, which means that overall there are only three
features for that context. 
(b) Sparsity is a consequence of relatively
small context length. Currently, our assumption
is that contexts are derived from small discourse 
units (sentences or abstracts at the most). For 
bigger discourse units, e.g. several paragraphs or
entire documents, our proposed generation of the 
reference distribution should be modified to re-
flect feature occurrences over those units. In the 
example from Table 1, for instance in Context1, 
there are 3 features that are present ? Feature1,
Feature4 and Feature5 ? the rest are absent.
Sparsity can be viewed as the number of ab-
sent/zero-valued features for each row. 
(c) Feature distribution is the frequency of oc-
currence of each feature across all contexts. It is 
captured by the column marginals of the ob-
served data matrix. For example, in Table 1 Fea-
ture1 occurs twice over the entire data; similarly
Feature2 occurs twice and so on. Feature distri-
bution can be viewed as the number of occur-
rences of each feature in the entire corpus. 
Now we describe how we do the reference
generation to stay faithful to the characteristics
described above. We use the uniform and the 
proportional methods. The uniform method gen-
erates data that realizes (a) and (b) characteristics
of the data and is the used originally in Tibshi-
rani et al (2001). The proportional method cap-
tures (a), (b), and (c) and is our adaptation of the 
Gap method.
The data is constructed as follows. To retain
the context lengths of the observed data in the
12
Feature1 Feature2 Feature3 Feature4 Feature5 ?FeatureP Total number of
non-zero value cells
Context1 1 0 0 1 1 ??? 3
Context2 0 1 1 0 1 ??? 3
Context3 1 0 0 0 1 ??? 2
Context4 0 1 1 1 1 ??? 4
?ContextN ??? ??? ??? ??? ??? ??? ???
2 2 2 2 4 ??? 12
Table 1: Observed data (sample)
reference data, the row marginals of the refer-
ence data are fixed to be equal to those of the 
observed data. In Table 1, the row marginals for 
the reference data will be {3, 3, 2, 4}. Carrying
the observed marginals to the reference data ap-
plies to both the uniform and proportional meth-
ods. Note that currently we fix only the row mar-
ginals. Due to the current assumption of binary
feature frequency, the generated reference data is 
binary too and this is true for both methods.
The main difference between the uniform
and proportional methods lies in whether the fea-
ture distribution is maintained in the simulation.
The uniform method does not weigh the features; 
rather, all features are given equal probability of 
occurring in the generated data. A uniform ran-
dom number r over the range [1, featureSetSize]
is drawn. The cell corresponding to the rth col-
umn (i.e. feature) in the current row under con-
sideration (i.e. context) is assigned ?1?. For ex-
ample, in our running example let?s say we are 
generating reference data for the 3rd row from
Table 1. We first generate a random number over 
the range [1, p]. Let?s assume that the generated
number is 4. Then, the cell [3, 4] is assigned
value ?1?. This procedure is repeated twice since
the row marginal for this row of the reference
data is 2. The proportional method factors in the
distribution of the column marginals of the ob-
served data while generating the random data. 
Unlike the uniform method, it takes into account 
the weight of each feature. In other words, the 
features by their frequency assign themselves a
range. For example, the features in the Table 1
will be assigned the following ranges: Feature1 -
[1, 2]; Feature2 - [3, 4]; Feature3 ? [5, 6]; Fea-
ture4 ? [7, 8]; Feature5 ? [9, 12]. A random
number is generated over the range [1, total
number of feature occurrences]. For the data in
Table 1, a random number is generated over the 
range [1, 12]. The feature corresponding to the 
range in which the random number falls is as-
signed ?1?. For example, if we are generating the 
reference for Context3 and the generated random
number over the range [1, 12] is 5, then a look-
up determines that 5 falls in the range for Fea-
ture3. Hence, the cell in Context3 corresponding
to Feature3 is assigned ?1?. Similar to the uni-
form method we would repeat this procedure 
twice to achieve the row marginal total of 2. 
Currently we proceed with the binary refer-
ence data created by the procedure described
above. Note that this binary reference matrix can 
be converted to a strength-of-association matrix
by multiplying it with a diagonal matrix that con-
tains the strength-of-association scores, e.g. log 
likelihood ratio, Mutual Information, Pointwise
mutual information, Chi-squared to name a few. 
Algorithm
The complete algorithm of the Gap Statistics 
which the reference distribution is a part of is: 
1.Cluster the observed data, varying the total 
number of clusters from k = 1, 2, ?., K, giving
within dispersion measures W(k), k = 1, 2, 
?.K.
2.Generate B reference datasets using the uni-
form or the proportional methods as described
above, and cluster each one giving within dis-
persion measures W*(kb), b = 1, 2, ? B, k = 
1, 2,? K. Compute the estimated Gap statistic:
))(log())(*log()/1()( kWkbWBkGap
b
6 
3. Let ? 
B
kbWBl ))(*log()/1( , compute the stan-
dard deviation
2/12]))(*(log()/1[()( lkbWBksd
B
 ?  and define
Bksdks /11)()(  . Finally choose the number of
clusters via 
)1()1()(_? t kskGapkGapuchThatsmallestKsk
The final step is the criterion for selecting the 
optimal k value. It says to choose the smallest k
value for which the gap is greater than the gap
for the earlier k value by the significance test of
?one standard error?. The ?one standard error?
calculations are modified to account for the
simulation error. Tibshirani and colleagues also
advise to use a multiplier to the s(k) for better 
rejection of the null hypothesis.
13
3.5 Tools, Feature Selection and Method
Parameters
rus strings (Weeber et al, 2001). Each ambiguity
has 100 manually sense-tagged instances. All
instances were randomly chosen from Medline 
abstracts. Each ambiguity instance is provided
with the sentence it occurred in and the Medline 
abstract text it was derived from. The senses for
every ambiguity are the UMLS senses plus a
?none of the above? category which captures all 
instances not fitting the available UMLS senses. 
For feature representation, selection, context rep-
resentation and clustering, we used Sense-
Clusters0.69 (http://senseclusters.sourceforge.net). It 
offers a variety of lexical features (ngrams, col-
locations, etc.) and feature selection methods 
(frequency, log likelihood, etc.). The contexts 
can then be represented with those features in
vector space using first or second order vectors 
which are then clustered. A detailed description
can be found in Purandare and Pedersen (2004)
and http://www.d.umn.edu/~tpederse/senseclusters.html.
SenseClusters links to CLUTO for the clustering
part (http://www-users.cs.umn.edu/~karypis/
cluto/download.html). CLUTO implements in a
fast and efficient way the main clustering algo-
rithms ? agglomerative, partitional and repeated
bisections.
For the current study, we modified the NLM
WSD by excluding instances sense-tagged with 
the ?none of the above? category. This is moti-
vated by the fact that that category is a catch-all
category for all senses that do not fit the current
UMLS inventory. First, we excluded words 
whose majority category was ?none of the 
above?. Secondly, from the instances of the re-
maining words, we removed those marked with 
?none of the above?. That subset of the original 
NLM WSD set we refer to as the ?modified
NLM WSD set? (Table 2).We chose the following methods for featurerepresentation and selection. Method1 uses bi-
grams as features, average link clustering in 
similarity space and the abstract as the context to 
derive the features from. The method is de-
scribed in Purandare and Pedersen (2004). It is 
based on first order context vectors, which repre-
sent features that occur in that context. A similar-
ity matrix is clustered using the average link ag-
glomerative method. Purandare and Pedersen
(2004) report that this method generally per-
formed better where there was a reasonably large
amount of data available (i.e., several thousand 
contexts). The application of that method to the 
biomedical domain is described in a technical
report (Savova, Pedersen, Kulkarni and Puran-
dare, 2005). Method2 uses unigrams which occur 
at least 5 times in the corpus. The context is the 
abstract. The choice of those features is moti-
vated by Joshi, Pedersen and Maclin (2005)
study which achieves best results with unigram
features.
3.7 Evaluation
Our evaluation of the performance of the cluster
stopping rules is two-fold. Accuracy is a direct
evaluation measuring the correctly recognized
number of senses:
words with correctly predicted number of senses
    all words 
Accuracy evaluates how well the methods dis-
cover the exact number of senses in the test cor-
pus. The F-score of the WSD is an indirect
evaluation for the quality of the cluster assign-
ment:
callecision?
callecision)(?scoreF
RePr2
RePr12_

 
Precision is the number of correctly clustered
instances divided by the number of clustered in-
stances; Recall is the number of correctly clus-
tered instances divided by all instances. There
may be some number of contexts that the cluster-
ing algorithm declines to process, which leads to 
the difference in precision and recall. 
For the Hartigan cluster stopping method, the
threshold is set to 10 which is the recommenda-
tion in the original algorithm. For the Gap cluster 
stopping method, we experiment with B=100, 
and the uniform and proportional reference gen-
eration methods. Our baseline is a simple clustering algorithm that
assigns all instances of a target word to a single
cluster.
3.6 Test Set
Our test set is the NLM WSD5 set which com-
prises 5000 disambiguated instances for 50 
highly frequent ambiguous UMLS Metathesau-
4 Results and Discussion
Table 3 presents the results for the three methods
5http://wsd.nlm.nih.gov/Restricted/Reviewed_Results/index.
shtml
14
Word, instances, senses after removal 
of ?none of the above? sense 
Word, instances, senses after removal 
of ?none of the above? sense 
Word, instances, senses after re-
moval of ?none of the above? sense 
Adjustment, 93, 3 Frequency, 94, 1 Radiation, 98, 2 
Blood pressure, 100, 3 Growth, 100, 2 Repair, 68, 2 
Cold, 95, 4 Immunosuppression, 100, 2 Scale, 65, 1 
Condition, 92, 2 Implantation, 98, 2 Secretion, 100, 1 
Culture, 100, 2 Inhibition, 99, 2 Sex, 100, 3 
Degree, 65, 2 Japanese, 79, 2 Single, 100, 2 
Depression, 85, 1 Ganglion, 100, 2 Strains, 93, 2 
Determination, 79, 1 Glucose, 100, 2 Surgery, 100, 2 
Discharge, 75, 2 Man, 92, 4 Transient, 100, 2 
Energy, 100, 2 Mole, 84, 2 Transport, 94, 2 
Evaluation, 100, 2 Mosaic, 97, 2 Ultrasound, 100, 2 
Extraction, 87, 2 Nutrition, 89, 4 Variation, 100, 2 
Fat, 73, 2 Pathology, 99, 2 White, 90, 2 
Fluid, 100, 1 Pressure, 96, 1 
Table 2: Modified NLM WSD set 
In terms of accuracy (Table 3, column 3), the 
C&H method has the best results (p<0.01 with t-
test). Note that the modified NLM WSD set con-
tains seven words with one sense ? depression, 
pressure, determination, fluid, frequency, scale, 
secretion ? for which the C&H method is at a 
disadvantage as it cannot return one cluster solu-
tion.
In terms of predicted number of senses (Table 
3, column 5), the Hartigan method tends to un-
derestimate the number of senses (overcluster), 
thus making coarser sense distinctions. The 
adapted Gap and C&H methods tend to overes-
timate them (undercluster), thus making finer 
grained sense distinctions. 
In terms of cluster member assignment as 
demonstrated by the F-scores (Table 3, column 
4), our adapted Gap method and the Hartigan 
method perform better than the C&H method 
(p<0.05 with t-test). The Hartigan method F-
scores  along with Gap uniform with Method 1 
feature selection are not significantly different 
from the baseline (p>0.05 with t-test); the rest 
are significantly lower than the majority sense 
baseline (p<0.05 with t-test). 
The high F-scores point to a path for improv-
ing accuracy results. Singleton clusters could be 
pruned as they could be insignificant to sense 
discrimination. As it was pointed out, the best 
performing algorithms (C&H and Gap propor-
tional) tend to create too many clusters (Table 3, 
column 5). Another way of dealing with single-
ton or smaller clusters is to present them for hu-
man review as they might represent new sense 
distinctions not included in the sense inventory. 
One explanation for the performance of the 
stopping rules (overclustering in particular) 
might be that some senses are very similar, e.g. 
?cold temperature? and ?cold sensation? for the 
?cold? ambiguity in instances like ?Her feet are 
cold.? Another explanation is that the stopping 
rules rely on the clustering algorithm used. In our 
current study, the experiments were run with 
only agglomerative clustering as implemented in 
CLUTO. The distance measure that we used is 
Euclidean distance, which is only one of many 
choices. Yet another explanation is in the feature 
sets we experimented with. They performed very 
similarly on both the accuracy and F-scores. Fu-
ture work we plan to do is aimed at experiment-
ing with different features, clustering algorithms, 
distance measures as well as applying Singular 
Value Decomposition (SVD) to the reference 
distribution matrix for our adapted Gap method. 
We are actively pursuing reference generation 
with fixed column and row marginals. The work 
of Pedersen, Kayaalp and Bruce (1996) uses this 
technique to find significant lexical relationships. 
They use the CoCo (Badsberg, 1995) package 
which implements the Patefield (1981) algorithm 
for I x J tables. Another venue is in the combina-
tion of several stopping rules which will take 
advantage of each rule?s strengths. Yet another 
component that needs to be addressed towards 
the path of completely automated WSD is cluster 
labeling.
5 Conclusions
In this work, we explored the problem of discov-
ering the number of the senses in a given target 
ambiguity corpus by studying three cluster stop-
ping rules. We implemented the original algo-
rithms of Calinski and Harabasz (1975) and Har-
tigan (1975) and adapted the reference genera-
tion of the Gap algorithm (Tibshirani et al, 
2001) to our task. The best accuracy for selecting 
the correct number of clusters is 0.60 with the
15
Feature 
Selection 
Stopping Rule Accuracy F-score                                  
(baseline majority sense = 82.63) 
Average number of senses (true 
average number of senses = 2.19) 
Method1 C&H 0.49 80.71 2.90 (overestimates) 
Hartigan 0.10 82.15 1.27 (underestimates) 
Gap (uniform) 0.02 82.00 1.49 (underestimates)     
Gap (proportional) 0.24 81.31 2.51 (overestimates) 
Method2 C&H 0.60 80.27 3.36 (overestimates) 
Hartigan 0.02 82.89 1.10 (underestimates) 
Gap (uniform) 0.05 81.63 2.44 (overestimates) 
Gap (proportional) 0.12 81.15 2.59 (overestimates) 
Table 3: Results ? accuracy, F-score and predicted average number of sense 
C&H method. Our error analysis shows that the 
cluster stopping methods make finer-grained 
sense distinctions by creating additional single-
ton clusters. The F-scores, indicative of the qual-
ity of cluster membership assignment, are in the 
80?s and point to a path towards accuracy im-
provement via additional cluster pruning. 
Acknowledgements 
The Perl modules of our implementations of the 
algorithms can be downloaded from 
http://search.cpan.org/dist/Statistics-CalinskiHarabasz/,
http://search.cpan.org/dist/Statistics-Hartigan/,
http://search.cpan.org/dist/Statistics-Gap/. We are 
greatly indebted to Anagha Kulkarni and Ted 
Pedersen for their participation in this research. 
We would also like to thank Patrick Duffy, 
James Buntrock and Philip Ogren for their sup-
port and collegial feedback, and the Mayo Clinic 
for funding the work.  
References 
A. D. Gordon. 1999. Classification (Second Edition). 
Chapman & Hall, London 
A. Purandare and T. Pedersen. 2004. Word Sense 
Discrimination by Clustering Contexts in Vector 
and Similarity Spaces. Proceedings of the Confer-
ence on Computational Natural Language Learning 
(CoNLL): 41-48, May 6-7, 2004, Boston, MA 
E. M.L. Beale.  1969. Euclidean cluster analysis. Bul-
letin of the International Statistical Institute:92?94, 
1969 
G. Savova, T. Pedersen, A. Kulkarni and A. Puran-
dare. 2005. Resolving Ambiguities in the Biomedi-
cal Domain. Technical Report. Minnesota Super-
computing Institute. 
G. W. Milligan and M.C. Cooper. 1985. An examina-
tion of procedures for determining the number of 
clusters in a data set. Psychometrika 50:159-179. 
J. H. Badsberg. 1995. An environment for graphical 
models. PhD dissertation, Aalborg University. 
J. Hartigan. 1975. Clustering Algorithms, Wiley, New 
York. 
L. A. Goodman and W.H. Kruskal. 1954. Measures of 
association for cross classifications. J. of Amer. 
Stat. Assoc., 49:732--764, 1954.
L. Hubert and J. Schultz. 1976. Quadratic assignment 
as a general data-analysis strategy. British Journal 
of Mathematical and Statistical Psychologie. 
29:190-241 
L. Kaufman and P. Rowsseeuw. 1990. Finding groups 
in data: an introduction to cluster analysis. New 
York. Wiley. 
M. Joshi, T. Pedersen and R. Maclin. 2005. A com-
parative study of support vector machines applied 
to the supervised word sense disambiguation prob-
lem in the medical domain. IICAI. India. 
M. Weeber, J. Mork and A. Aronson. 2001. Develop-
ing a test collection for biomedical word sense dis-
ambiguation. Proc. AMIA 
R. B. Calinski and J. Harabasz. 1974. A dendrite 
method for cluster analysis. Communications in 
statistics 3:1-27. 
R. Mihalcea. 2003. The role of non-ambiguous words 
in natural language disambiguation. RANLP-2003, 
Borovetz, Bulgaria 
R. O. Duda and P. E. Hart. 1973. Pattern Classifica-
tion and Scene Analysis. Wiley, New York, 1973
R. Tibshirani, G. Walther and T. Hastie. 2001. Esti-
mating the number of clusters in a dataset via the 
Gap statistic. Journal of the Royal Statistics Soci-
ety (Series B). 
T. Pedersen, M. Kayaalp and R. Bruce. 1996. Signifi-
cant lexical relationships. Proc. of the 13th National 
Conference on Artificial Intelligence, August 1996, 
Portland, Oregon. 
W. J. Krzanowski and Y. T. Lai. 1985. A criterion for 
determining the number of groups in a data set us-
ing the sum of squares clustering. Biometrics 
44:23-34. 
W. Patefield. 1981. An efficient method of generating 
random R x C tables with given row and column 
totals. Applied Statistics 30:91-97. 
16
