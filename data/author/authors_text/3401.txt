Spoken Interactive ODQA System: SPIQA
Chiori Hori, Takaaki Hori, Hajime Tsukada,
Hideki Isozaki, Yutaka Sasaki and Eisaku Maeda
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We have been investigating an interactive
approach for Open-domain QA (ODQA)
and have constructed a spoken interactive
ODQA system, SPIQA. The system de-
rives disambiguating queries (DQs) that
draw out additional information. To test
the efficiency of additional information re-
quested by the DQs, the system recon-
structs the user?s initial question by com-
bining the addition information with ques-
tion. The combination is then used for an-
swer extraction. Experimental results re-
vealed the potential of the generated DQs.
1 Introduction
Open-domain QA (ODQA), which extracts answers
from large text corpora, such as newspaper texts, has
been intensively investigated in the Text REtrieval
Conference (TREC). ODQA systems return an ac-
tual answer in response to a question written in a
natural language. However, the information in the
first question input by a user is not usually sufficient
to yield the desired answer. Interactions for col-
lecting additional information to accomplish QA are
needed. To construct more precise and user-friendly
ODQA systems, a speech interface is used for the
interaction between human beings and machines.
Our goal is to construct a spoken interactive
ODQA system that includes an automatic speech
recognition (ASR) system and an ODQA system.
To clarify the problems presented in building such
a system, the QA systems constructed so far have
been classified into a number of groups, depending
on their target domains, interfaces, and interactions
to draw out additional information from users to ac-
complish set tasks, as is shown in Table 1. In this
table, text and speech denote text input and speech
input, respectively. The term ?addition? represents
additional information queried by the QA systems.
This additional information is separate to that de-
rived from the user?s initial questions.
Table 1: Domain and data structure for QA systems
target domain specific open
data structure knowledge DB unstructured text
without addition CHAT-80 SAIQAtext
with addition MYCIN (SPIQA?)
without addition Harpy VAQA
speech
with addition JUPITER (SPIQA?)
? SPIQA is our system.
To construct spoken interactive ODQA systems,
the following problems must be overcome: 1. Sys-
tem queries for additional information to extract an-
swers and effective interaction strategies using such
queries cannot be prepared before the user inputs the
question. 2. Recognition errors degrade the perfor-
mance of QA systems. Some information indispens-
able for extracting answers is deleted or substituted
with other words.
Our spoken interactive ODQA system, SPIQA,
copes with the first problem by adopting disam-
biguating users? questions using system queries. In
addition, a speech summarization technique is ap-
plied to handle recognition errors.
2 Spoken Interactive QA system: SPIQA
Figure 1 shows the components of our system, and
the data that flows through it. This system com-
prises an ASR system (SOLON), a screening filter
that uses a summarization method, and ODQA en-
gine (SAIQA) for a Japanese newspaper text corpus,
a Deriving Disambiguating Queries (DDQ) module,
and a Text-to-Speech Synthesis (TTS) engine (Fi-
nalFluet).
ASR
TTS
Screening
filter
ODQA engine
(SAIQA)
DDQ
module
Answer
derived?
Answer
sentence generator
Question
reconstructor
No
Yes
Additional
info. New question
First
question
Question/
Additional info.
User Answer/
DDQ speech
Answer
sentence
DDQ
sentence
Recognition
result
Answer
Figure 1: Components and data flow in SPIQA.
ASR system
Our ASR system is based on the Weighted Finite-
State Transducers (WFST) approach that is becom-
ing a promising alternative formulation for the tra-
ditional decoding approach. The WFST approach
offers a unified framework representing various
knowledge sources in addition to producing an op-
timized search network of HMM states. We com-
bined cross-word triphones and trigrams into a sin-
gle WFST and applied a one-pass search algorithm
to it.
Screening filter
To alleviate degradation of the QA?s perfor-
mance by recognition errors, fillers, word fragments,
and other distractors in the transcribed question, a
screening filter that removes these redundant and
irrelevant information and extracts meaningful in-
formation is required. The speech summarization
approach (C. Hori et. al., 2003) is applied to the
screening process, wherein a set of words maximiz-
ing a summarization score that indicates the appro-
priateness of summarization is extracted automati-
cally from a transcribed question, and these words
are then concatenated together. The extraction pro-
cess is performed using a Dynamic Programming
(DP) technique.
ODQA engine
The ODQA engine, SAIQA, has four compo-
nents: question analysis, text retrieval, answer hy-
pothesis extraction, and answer selection.
DDQ module
When the ODQA engine cannot extract an appro-
priate answer to a user?s question, the question is
considered to be ?ambiguous.? To disambiguate the
initial questions, the DDQ module automatically de-
rives disambiguating queries (DQs) that require in-
formation indispensable for answer extraction. The
situations in which a question is considered ambigu-
ous are those when users? questions exclude indis-
pensable information or indispensable information
is lost through ASR errors. These instances of miss-
ing information should be compensated for by the
users.
To disambiguate a question, ambiguous phrases
within it should be identified. The ambiguity of
each phrase can be measured by using the struc-
tural ambiguity and generality score for the phrase.
The structural ambiguity is based on the dependency
structure of the sentence; phrase that is not modified
by other phrases is considered to be highly ambigu-
ous. Figure 2 has an example of a dependency struc-
ture, where the question is separated into phrases.
Each arrow represents the dependency between two
phrases. In this example, ?the World Cup? has no
Which  country won the  world  cupin Southeast Asia ?
Figure 2: Example of dependency structure.
modifiers and needs more information to be identi-
fied. ?Southeast Asia? also has no modifiers. How-
ever, since ?the World Cup?appears more frequently
than ?Southeast Asia? in the retrieved corpus, ?the
World Cup? is more difficult to identify. In other
words, words that frequently occur in a corpus rarely
help to extract answers in ODQA systems. There-
fore, it is adequate for the DDQ module to generate
questions relating to ?World Cup? in this example,
such as ?What kind of World Cup?? , ?What year
was the World Cup held??.
The structural ambiguity of the n-th phrase is de-
fined as
A
D
(P
n
) = log
{
1 ?
?
N
i=1:i=n
D(P
i
, P
n
)
}
,
where the complete question is separated into N
phrases, and D(P
i
, P
n
) is the probability that phrase
P
n
will be modified by phrase P
i
, which can be cal-
culated using Stochastic Dependency Context-Free
Grammar (SDCFG) (C. Hori et. al., 2003).
Using this SDCFG, only the number of non-
terminal symbols is determined and all combina-
tions of rules are applied recursively. The non-
terminal symbol has no specific function, such as
a noun phrase. All the probabilities of rules are
stochastically estimated based on data. Probabilities
for frequently used rules become greater, and those
for rarely used rules become smaller. Even though
transcription results given by a speech recognizer are
ill-formed, the dependency structure can be robustly
estimated by our SDCFG.
The generality score is defined as
A
G
(P
n
) =
?
w?P
n
:w=cont log P (w),
where P (w) is the unigram probability of w based
on the corpus to be retrieved. Thus, ?w = cont?
means that w is a content word such as a noun, verb
or adjective.
We generate the DQs using templates of interrog-
ative sentences. These templates contain an inter-
rogative and a phrase taken from the user?s question,
i.e., ?What kind of * ??, ?What year was * held??
and ?Where is * ??.
The DDQ module selects the best DQ based on its
linguistic appropriateness and the ambiguity of the
phrase. The linguistic appropriateness of DQs can
be measured by using a language model, N-gram.
Let S
mn
be a DQ generated by inserting the n-th
phrase into the m-th template. The DDQ module
selects the DQ that maximizes the DQ score:
H(S
mn
) = ?
L
L(S
mn
)+?
D
A
D
(P
n
)+?
G
A
G
(P
n
),
where L(?) is a linguistic score such as the loga-
rithm for trigram probability, and ?
L
, ?
D
, and ?
G
are weighting factors to balance the scores.
Hence, the module can generate a sentence that
is linguistically appropriate and asks the user to dis-
ambiguate the most ambiguous phrase in his or her
question.
3 Evaluation Experiments
Questions consisting of 69 sentences read aloud by
seven male speakers were transcribed by our ASR
system. The question transcriptions were processed
with a screening filter and input into the ODQA
engine. Each question consisted of about 19 mor-
phemes on average. The sentences were grammat-
ically correct, formally structured, and had enough
information for the ODQA engine to extract the cor-
rect answers. The mean word recognition accuracy
obtained by the ASR system was 76%.
3.1 Screening filter
Screening was performed by removing recognition
errors using a confidence measure as a threshold and
then summarizing it within an 80% to 100% com-
paction ratio. In this summarization technique, the
word significance and linguistic score for summa-
rization were calculated using text from Mainichi
newspapers published from 1994 to 2001, compris-
ing 13.6M sentences with 232M words. The SD-
CFG for the word concatenation score was calcu-
lated using the manually parsed corpus of Mainichi
newspapers published from 1996 to 1998, consist-
ing of approximately 4M sentences with 68M words.
The number of non-terminal symbols was 100. The
posterior probability of each transcribed word in a
word graph obtained by ASR was used as the confi-
dence score.
3.2 DDQ module
The word generality score A
G
was computed using
the same Mainichi newspaper text described above,
while the SDCFG for the dependency ambiguity
score A
D
for each phrase was the same as that used
in (C. Hori et. al., 2003). Eighty-two types of inter-
rogative sentences were created as disambiguating
queries for each noun and noun-phrase in each ques-
tion and evaluated by the DDQ module. The linguis-
tic score L indicating the appropriateness of inter-
rogative sentences was calculated using 1000 ques-
tions and newspaper text extracted for three years.
The structural ambiguity score A
D
was calculated
based on the SDCFG, which was used for the screen-
ing filter.
3.3 Evaluation method
The DQs generated by the DDQ module were eval-
uated in comparison with manual disambiguation
queries. Although the questions read by the seven
speakers had sufficient information to extract ex-
act answers, some recognition errors resulted in a
loss of information that was indispensable for ob-
taining the correct answers. The manual DQs were
made by five subjects based on a comparison of
the original written questions and the transcription
results given by the ASR system. The automatic
DQs were categorized into two classes: APPRO-
PRIATE when they had the same meaning as at
least one of the five manual DQs, and INAPPRO-
PRIATE when there was no match. The QA per-
formance in using recognized (REC) and screened
questions (SCRN) were evaluated by MRR (Mean
Reciprocal Rank) (http://trec.nist.gov/data/qa.html).
SCRN was compared with the transcribed question
that just had recognition errors removed (DEL). In
addition, the questions reconstructed manually by
merging these questions and additional information
requested the DQs generated by using SCRN, (DQ)
were also evaluated. The additional information was
extracted from the original users? question without
recognition errors. In this study, adding information
by using the DQs was performed only once.
3.4 Evaluation results
Table 2 shows the evaluation results in terms of
the appropriateness of the DQs and the QA-system
MRRs. The results indicate that roughly 50% of the
DQs generated by the DDQ module based on the
screened results were APPROPRIATE. The MRR
for manual transcription (TRS) with no recognition
errors was 0.43. In addition, we could improve the
MRR from 0.25 (REC) to 0.28 (DQ) by using the
DQs only once. Experimental results revealed the
potential of the generated DQs in compensating for
the degradation of the QA performance due to recog-
nition errors.
4 Conclusion
The proposed spoken interactive ODQA system,
SPIQA copes with missing information by adopt-
ing disambiguation of users? questions by system
queries. In addition, a speech summarization tech-
nique was applied for handling recognition errors.
Although adding information was performed using
DQs only once, experimental results revealed the
potential of the generated DQs to acquire indispens-
able information that was lacking for extracting an-
swers. In addition, the screening filter helped to gen-
erate the appropriate DQs. Future research will in-
Table 2: Evaluation results of disambiguating
queries generated by the DDQ module.
Word MRR w/o IN-SPK
acc. REC DEL SCRN DQ errors APP APP
A 70% 0.19 0.16 0.17 0.23 4 32 33
B 76% 0.31 0.24 0.29 0.31 8 36 25
C 79% 0.26 0.18 0.26 0.30 10 34 25
D 73% 0.27 0.21 0.24 0.30 4 35 30
E 78% 0.24 0.21 0.24 0.27 7 31 31
F 80% 0.28 0.25 0.30 0.33 8 34 27
G 74% 0.22 0.19 0.19 0.22 3 35 31
AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%
An integer without a % other than MRRs indicates number of
sentences. Word acc.:word accuracy, SPK:speaker, AVG: aver-
aged values, w/o errors: transcribed sentences without recog-
nition errors, APP: appropriate DQs and InAPP: inappropriate
DQs.
clude an evaluation of the appropriateness of DQs
derived repeatedly to obtain the final answers. In
addition, the interaction strategy automatically gen-
erated by the DDQ module should be evaluated in
terms of how much the DQs improve QA?s total per-
formance.
References
F. Pereira et. al., ?Definite Clause Grammars for Language
Analysis ?a Survey of the Formalism and a Comparison with
Augmented Transition Networks,? Artificial Intelligence, 13:
231-278, 1980.
E. H. Shortliffe, ?Computer-Based Medical Consultations:
MYCIN,? Elsevier/North Holland, New York NY, 1976.
B. Lowerre et. al., ?The Harpy speech understanding system,?
W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Pren-
tice Hall.
L. D. Erman et. al., ?The Hearsay-II Speech-Understanding
System: Integrating Knowledge to Resolve Uncertainty,?
ACM computing Survays, Vol. 12, No. 2, pp. 213 ? 253,
1980.
V. Zue, et al, ?JUPITER: A Telephone-Based Conversational
Interface for Weather Information,? IEEE Transactions on
Speech and Audio Processing, Vol. 8, No. 1, 2000.
S. Harabagiu et. al., ?Open-Domain Voice-Activated Ques-
tion Answering,? COLING2002, Vol.I, pp. 321?327, Taipei,
2002.
C. Hori et. al., ?A Statistical Approach for Automatic Speech
Summarization,? EURASIP Journal on Applied Signal Pro-
cessing (EURASIP), pp128?139, 2003.
Y. Sasaki et. al., ?NTT?s QA Systems for NTCIR QAC-1,?
Working Notes of the Third NTCIR Workshop Meeting,
pp.63?70, 2002.
Evaluation Measures Considering Sentence Concatenation for
Automatic Summarization by Sentence or Word Extraction
Chiori Hori, Tsutomu Hirao and Hideki Isozaki
NTT Communication Science Laboratories
{chiori, hirao, isozaki}@cslab.kecl.ntt.co.jp
Abstract
Automatic summaries of text generated through
sentence or word extraction has been evaluated by
comparing them with manual summaries generated
by humans by using numerical evaluation measures
based on precision or accuracy. Although sentence
extraction has previously been evaluated based only
on precision of a single sentence, sentence concate-
nations in the summaries should be evaluated as
well. We have evaluated the appropriateness of sen-
tence concatenations in summaries by using eval-
uation measures used for evaluating word concate-
nations in summaries through word extraction. We
determined that measures considering sentence con-
catenation much better reflect the human judgment
rather than those based only on the precision of a
single sentence.
1 Introduction
Summarization Target and Approach
The amount of text is explosively increasing day by
day, and it is becoming very difficult to manage in-
formation by reading all the text. To manage infor-
mation easily and find target information quickly,
we need technologies for summarizing text. Al-
though research into text summarization started in
the 1950?s, it is still largely in the research phase
(Mani and Maybury, 1999). Several projects on
text summarization have been carried out. 1 In
these project, text summarization has so far focused
on summarizing single documents through sentence
extraction. Recently, summarizing multiple docu-
ments with the same topic has been made a tar-
get. The major approach to extracting sentences that
have significant information is statistical, i.e., su-
pervised learning from parallel corpora consisting
of original texts and their summarization (Kupiec et
1SUMMAC in the Tipster project by DARPA (http://www-
nlpir.nist.gov/related projects/tipster summac) and DUC in
the TIDES project (http://duc.nist.gov/) in the U.S. TSC
(http://research.nii.ac.jp/ntcir/) in the NTCIR by NII (The Na-
tional Institute of Informatica) in Japan.
al., 1995) (Aone et al, 1998) (Mani and Bloedorn,
1998).
Several summarization techniques for multime-
dia including image, speech, and text have been re-
searched. Manually transcribed newswire speech
(TDT data) and meeting speech (Zechner, 2003)
have been set as summarization targets. The need
to automatically generate summaries from speech
has led to research on summarizing transcription re-
sults obtained by automatic speech recognition in-
stead of manually transcribed speech (Hori and Fu-
rui, 2000a). This summarization approach is word
extraction (sentence compaction) that attempts to
extract significant information, exclude acoustically
and linguistically unreliable words, and maintain
the meanings of the original speech.
The summarization approaches that have been
mainly researched so far are extracting sentences
or words from original text or transcribed speech.
There has also been research on generating an ?ab-
stract? like the much higher level summarization
composed freely by human experts (Jing, 2002).
This approach includes not only extracting sen-
tences but also combining sentences to generate new
sentences, replacing words, reconstructing syntactic
structure, and so on.
Evaluation Measures for Summarization
Metrics that can be used to accurately evaluate
the various appropriateness to summarization are
needed.The simplest and probably the ideal way of
evaluating automatic summarization is to have hu-
man subjects read the summaries and evaluate them
in terms of the appropriateness of summarization.
However, this type of evaluation is too expensive
for comparing the efficiencies of many different ap-
proaches precisely and repeatedly. We thus need au-
tomatic evaluation metrics to numerically validate
the efficiency of various approaches repeatedly and
consistently.
Automatic summaries can be evaluated by com-
paring them with manual summaries generated by
humans. The similarities between the targets and
the automatically processed results provide metrics
indicating the extent to which the task was accom-
plished. The similarity that can better reflect sub-
jective judgments is a better metric.
To create correct answers for automatic sum-
marization, humans generate manual summaries
through sentence or word extraction. However,
references consisting of manual summaries vary
among humans. The problems in validating auto-
matic summaries by comparing them with various
references are as follows:
? correct answers for automatic results cannot be
unified because of subjective variation,
? the coverage of correct answers in the collected
manual summaries is unknown, and
? the reliability of references in the collected
manual summaries is not always guaranteed.
When the similarity between automatic results
and references is used for the evaluation metrics,
the similarity determination function counts over-
lapping of each component or sequence of com-
ponents in the automatic results. If concatenations
between components in a summary had no mean-
ing, the overlap of a single component between the
automatic results and the references can represent
the extent of summarization. However, concatena-
tions between sentences or words have meanings,
so some concatenations of sentences or words in the
automatic summaries sometimes generate meanings
different from the original. The evaluation metrics
for summarization should thus consider each con-
catenation between components in the automatic re-
sults.
To evaluate sentence automatically generated
with taking consideration word concatenation into
by using references varied among humans, vari-
ous metrics using n-gram precision and word ac-
curacy have been proposed: word string preci-
sion (Hori and Furui, 2000b) for summarization
through word extraction, ROUGE (Lin and Hovy,
2003) for abstracts, and BLEU (Papineni et al,
2002) for machine translation. Evaluation metrics
based on word accuracy, summarization accuracy
(SumACCY), using a word network made by merg-
ing manual summaries has been proposed (Hori and
Furui, 2001). In addition, to solve the problems for
the coverage of correct answers and the reliability
of manual summaries as correct answers, weighted
summarization accuracy (WSumACCY) in which
SumACCY is weighted by the majority of the hu-
mans? selections, has been proposed (Hori and Fu-
rui, 2003a).
In contrast, summarization through sentence ex-
traction has been evaluated using only single sen-
tence precision. Sentence extraction should also be
evaluated using measures that take into account sen-
tence concatenations, the coverage of correct an-
swers, and the reliability of manual summaries.
This paper presents evaluation results of auto-
matic summarization through sentence or word ex-
traction using the above mentioned metrics based on
n-gram precision and sentence/word accuracy and
examines how well these measures reflect the judg-
ments of humans as well.
2 Evaluation Metrics for Extraction
In summarization through sentence or word extrac-
tion under a specific summarization ratio, the order
of the sentences or words and the length of the sum-
maries are restricted by the original documents or
sentences. Metrics based on the accuracy of the
components in the summary is a straight-forward
approach to measuring similarities between the tar-
get and automatic summaries.
2.1 Accuracy
In the field of speech recognition, automatic recog-
nition results are compared with manual transcrip-
tion results. The conventional metric for speech
recognition is recognition accuracy calculated based
on word accuracy:
ACCY
= Len ? (Sub + Ins + Del)Len ? 100[%], (1)
where Sub, Ins, Del, and Len are the numbers
of substitutions, insertions, deletions, and words in
the manual transcription, respectively. Although
word accuracy cannot be used to directly evaluate
the meanings of sentences, higher accuracy indi-
cates that more of the original information has been
preserved. Since the meaning of the original doc-
uments is generated by combining sentences, this
metric can be applied to the evaluation for sentence
extraction. Sentence accuracy defined by eq. (1)
with words replaced by sentences represents how
much the automatic result is similar to the answer
and how well it preserves the original meaning.
Accuracy is the simplest and most efficient metric
when the target for the automatic summaries can be
set as only one answer. However, there are usually
multiple targets for each automatic summary due to
the variation in manual summarization among hu-
mans. Therefore, it is not easy to use accuracy to
evaluate automatic summaries. Subjective variation
results into two problems:
? how to consider all possible correct answers in
the manual summaries, and
? how to measure the similarity between the
evaluation sentence and multiple manual sum-
maries.
If we could collect all possible manual sum-
maries, the one most similar to the automatic re-
sult could be chosen as the correct answer and used
for the evaluation. The sentence or word accuracy
compared with the most similar manual summary is
denoted as NrstACCY. However, in real situations,
the number of manual summaries that could be col-
lected is limited. The coverage of correct answers in
the collected manual summaries is unknown. When
the coverage is low, the summaries are compared
with inappropriate targets, and the NrstACCY ob-
tained by such comparison does not provide an effi-
cient measure.
2.2 N-gram Precision
One way to cope with the coverage problem is to
use local matching of components or component
strings with all the manual summaries instead of
using a measure comparing a word sequence as a
whole sentence, such as NrstACCY. The similar-
ity can be measured by counting the precision, i.e.,
the number of sentence or word n-gram overlapping
between the automatic result and all the references.
Even if there are multiple targets for an automatic
summary, the precision of components in each orig-
inal can be used to evaluate the similarity between
the automatic result and the multiple references.
Precision is an efficient way of evaluating the sim-
ilarity of component occurrence between automatic
results and targets with a different order of compo-
nents and different lengths.
In the evaluation of summarization through ex-
traction, a component occurring in a different loca-
tion in the original is considered to be a different
component even if it is the same component as one
in the result. When an answer for the automatic re-
sult can be unified and the lengths of the automatic
result and its answer are the same, accuracy counts
insertion errors and deletion errors and thus has both
the precision and recall characteristics.
Since meanings are basically conveyed by word
strings rather than single words, word string preci-
sion (Hori and Furui, 2000b) can be used to evalu-
ate linguistic precision and the maintenance of the
original meanings of an utterance. In this method,
word strings of various lengths, that is n-grams, are
used as components for measuring precision. The
extraction ratio, pn, of each word string consist-
ing of n words in a summarized sentence, V =
v1, v2, . . . , vM , is given by
pn =
M
?
m=n
?(vm?n+1, . . . , vm?1, vm)
M ? n + 1 , (2)
where
?(un) =
{ 1 if un ? Un
0 if un /? Un , (3)
un: each word string consisting of n words
Un: a set of word strings consisting of n words
in all manual summarizations.
When n is 1, pn corresponds to the precision of
each word, and when n is the same length as a
summarized sentence (n = M ), pn indicates the
precision of the summarized sentence itself.
2.3 Summarization Accuracy: SumACCY
Summarization accuracy (SumACCY) was pro-
posed to cope with the problem of correct answer
coverage and various references among humans
(Hori and Furui, 2001). To cover all possible correct
answers for summarization using a limited number
of manual summaries, all the manual summaries
are merged into a word network. In this evaluation
method, the word sequence in the network closest to
the evaluation word sequence is considered to be the
target answer. The word accuracy of the automatic
result is calculated in comparison with the target an-
swer extracted from the network.
Since summarization is processed by extracting
words from an original; the words cannot be re-
placed by other words, and the order of words can-
not be changed. Multiple manual summaries can
be combined into a network that represents the vari-
ations. Each set of words that could be extracted
from the network consists of words and word strings
occurring at least once in all the manual summaries.
The network made by the manual summaries can
be considered to represent all possible variations of
correct summaries.
SUB The beautiful cherry blossoms in Japan bloom in spring
A The cherry blossoms in Japan
B cherry blossoms in Japan bloom
C beautiful cherry bloom in spring
D beautiful cherry blossoms in spring
E The beautiful cherry blossoms bloom
Table 1: Example of manual summarization by sen-
tence compaction
<s> </s> The beautiful cherry blossoms inJapan bloomin spring
Figure 1: Word network made by merging manual
summaries
The sentence ?The beautiful cherry blossoms in
Japan bloom in spring.? is assumed to be manually
summarized as shown in Table 1. In this example,
five words are extracted from the nine words. There-
fore, the summarization ratio is 56%. The variations
of manual summaries are merged into a word net-
work, as shown in Fig. 1. We use <s> and </s>
as the beginning and ending symbols of a sentence.
Although ?Cherry blossoms bloom in spring? is not
among the manual answers in Table 1, this sentence,
which could be extracted from the network, is con-
sidered a correct answer.
When references consisting of manual sum-
maries cannot cover all possible answers and lack
the appropriate answer for an automatic summary,
SumACCY calculated using such a network is bet-
ter than NrstACCY for evaluating the automatic re-
sult. This evaluation method gives a penalty for
each word concatenation in the automatic results
that is excluded in the network, so it can be used
to evaluate the sentence-level appropriateness more
precisely than matching each word in all the refer-
ences.
2.4 Weighted SumACCY: WSumACCY
In SumACCY, all possible sets of words extracted
from the network of manually summarized sen-
tences are equally used as target answers. How-
ever, the set of words containing word strings se-
lected by many humans would presumably be better
and give more reliable answers. To obtain reliability
that reflects the majority of selections by humans,
the summarization accuracy is weighted by a pos-
terior probability based on the manual summariza-
tion network. The reliability of a sentence extracted
from the network is defined as the product of the
ratios of the number of subjects who selected each
word to the total number of subjects. The weighted
summarization accuracy is given by
WSumACCY
= P? (v1 . . . vM |R) ? SumACCY
P? (v?1 . . . v?M? |R)
, (4)
where P? (v1 . . . vM |R) is the reliability score of a
set of words v1 . . . vM in the manual summariza-
tion network, R, and M represents the total num-
ber of words in the target answer. The set of words
v?1 . . . v?M? represents the word sequence that maxi-
mizes the reliability score, P? (?|R), given by
P? (v1 . . . vM |R)
=
( M
?
m=2
C(vm?1, vm|R)
HR
)
1
M?1
, (5)
where vm is the m-th word in the sentence ex-
tracted from the network as the target answer, and
C(x, y|R) indicates the number of subjects who se-
lected the word connection of x and y. Here, ?word
connection? means an arc in the manual summariza-
tion network. HR is the number of subjects.
2.5 Evaluation Experiments
Newspaper articles and broadcast news speech were
automatically summarized through sentence extrac-
tion and word extraction respectively under the
given summarization ratio, which is the ratio of the
numbers of sentences or words in the summary to
that in the original.
The automatic summarization results were sub-
jectively evaluated by ten human subjects. The sub-
jects read these summaries and rated each one from
1 (incorrect) to 5 (perfect). The automatic sum-
maries were also evaluated by using the numerical
metrics SumACCY, WSumACCY, NrstACCY,
and n-gram precision (1 ? n ? 5) in compari-
son with reference summaries generated by humans.
The precisions of 1-gram, . . ., 5-gram are denoted
PREC1, . . ., PREC5. The numerical evaluation re-
sults were averaged over the number of automatic
summaries.
Note that the subjects who judged the automatic
summaries did not include anyone who generated
the references. To examine the similarity of the hu-
man judgments and that of the manual summaries,
the kappa statistics, ?, was calculated using eq. (A-
1) in the Appendix.
Finally, to examine how much the evaluation
measures reflected the human judgment, the correla-
tion coefficients between the human judgments and
the numerical evaluation results were calculated.
Sentence extraction
Sixty articles in Japanese newspaper published in
94, 95, and 98 were automatically summarized with
a 30% summarization ratio. Half the articles were
general news report (NEWS), and other half were
columns (EDIT).
The automatic summarization was performed us-
ing a Support Vector Machine (SVM) (Hirao et al,
2003), random extraction (RDM), the lead method
(LEAD) extracting sentences from the head of ar-
ticles. In comparison with these automatic sum-
maries, manual summaries (TSC) was also evalu-
ated.
These 4 types of summaries, SVM, RDM, LEAD,
and TSC were read and rated 1 to 5 by 10 humans.
The summaries were evaluated in terms of extrac-
tion of significance information (SIG), coherence
of sentences (COH), maintenance of original mean-
ings (SEM), and appropriateness of summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 20 other human subjects gener-
ated manual summaries through sentence extrac-
tion. These manual summaries were set as the target
set for the automatic summaries.
Word extraction
Japanese TV news broadcasts aired in 1996 were
automatically recognized and summarized sentence
by sentence (Hori and Furui, 2003b). They con-
sisted of 50 utterances by a female announcer. The
out-of-vocabulary (OOV) rate for the 20k word vo-
cabulary was 2.5%, and the test-set perplexity was
54.5. Fifty utterances with word recognition accu-
racy above 90%, which was the average rate over the
50 utterances, were selected and used for the evalu-
ation. The summarization ratio was set to 40%.
Nine automatic summaries with various summa-
rization accuracies from 40% to 70% and a manual
summary (SUB) were selected as a test set. These
ten summaries for each utterance were judged in
terms of the appropriateness of the summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 25 humans generated manual sum-
maries through word extraction. These manual
summaries were set as a target set for the automatic
summaries, and merged into a network. Note that a
set of 24 manual summaries made by other subjects
was used as the target for SUB.
2.6 Evaluation Results
Figures 2 and 3 show the correlation coefficients
between the judgments of the subjects and the nu-
merical evaluation results for EDIT and NEWS.
They show that the measures based on accuracy
much better reflected human judgments than those
of the n-gram precisions for evaluating SIG and
WHOLE for both EDIT and NEWS. On the other
hand, PREC2 better reflected the human judgments
for evaluating COH and SEM. These results show
that measures taking into account sentence concate-
nations better reflected human judgments than sin-
gle component precision. The precisions of longer
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 2: Correlation coefficients between human
judgment and numerical evaluation results for EDIT
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 3: Correlation coefficients between hu-
man judgment and numerical evaluation results for
NEWS
sentence strings (PREC3 to PREC5) didn?t reflect
the human judgments for all the conditions. These
results show that meanings of the original article can
maintain by the concatenations of only a few sen-
tences in summarization through sentence extrac-
tion.
Table 2 lists the kappa statistics for the manual
summaries and the human judgments for EDIT and
NEWS. The manual results varied among humans
DATA SUMMARIES ?
EDIT manual summaries 0.35
NEWS manual summaries 0.39
Table 2: Kappa statistics for manual summaries and
human judgments for sentence extraction.
and the similarity among humans was low. The
kappa statistics for NEWS is slightly higher than
that for EDIT. The difference of similarities among
manual summaries is due to the difference in struc-
tures of information in each article. Although the
articles in EDIT had a discourse structure, NEWS
had isolated and stereotyped information scattered
throughout the articles.
While the human judgments for NEWS were sim-
ilar, those for EDIT varied.The difficulty in evaluat-
ing COH and SEM in EDIT is due to the variation
in both manual summaries and human judgment.
Figure 4 shows the correlation coefficients be-
tween the judgments of the subjects and the numer-
ical evaluation results for summaries of broadcast
news speech through word extraction. Table 3 lists
0
0.2
0.4
0.6
0.8
1
WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
s
Evaluation point
PREC1
PREC2
PREC3
PREC4
PREC5
SumACCY
WSumACC
NrstACCY  
Figure 4: Correlation coefficients between human
judgment and numerical evaluation results for sum-
maries through word extraction
the kappa statistics for the manual summaries and
the human judgments for summaries through word
extraction. In word extraction, the human judg-
DATA SUMMARIES ?
Broadcast news manual summaries 0.47
Table 3: Kappa statistics for manual summaries and
human judgments for word extraction
ments and the manual summaries were very similar
among the subjects.
As shown in figure 4, WSumACCY yielded the
best correlation to the human judgments. This
means that the correctness as a sentence and the
weight (that is how many subjects support the ex-
tracted phrases in summarized sentences) are im-
portant in summarization through word extraction.
In comparison with the results of sentence extrac-
tion in Figures 2 and 3, PREC1 effectively reflected
the human judgments for word extraction. Since in
the manual summarized sentences through word ex-
traction under the low summarization ratio, the sen-
tences were summarized based on significance word
extraction rather than syntactic structure mainte-
nance to generate grammatically correct sentences.
3 Conclusion
We have presented the results of evaluating
the appropriateness of the sentence concatena-
tions in summaries generated using SumACCY,
WSumACCY, NrstACCY and n-gram precision.
We found that the measures taking into account sen-
tence concatenation much better reflected the judg-
ments of humans than did the single sentence pre-
cision, so the concatenation of sentences in sum-
maries should be evaluated.
Although the human judgments and the man-
ual summaries for word extraction did not vary
much among the subjects, those for sentence extrac-
tion for single article summarization greatly varied
among the subjects. As a result, it is very difficult to
set correct answers for single article summarization
through sentence extraction.
Future works involves experiments to examine
the efficiency of each numerical measures in re-
sponse to the coverage of correct answers.
4 Acknowledgments
We thank NHK (Japan Broadcasting Corporation)
for providing the broadcast news database. We
also thank Prof. Sadaoki Furui at Tokyo Institute
of Technology for providing the summaries of the
broadcast news speech.
References
C. Aone, M. Okurowski, and J. Gorlinsky. 1998.
Trainable scalable summarization using robust
NLP and machine learning. In Proceedings ACL,
pages 62?66.
J. Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
T. Hirao, K. Takeuchi, H. Isozaki, Y. Sasaki, and
E. Maeda. 2003. SVM-based multi-document
summarization integrating sentence extraction
with bunsetsu elimination. IEICE Trans. Inf. &
Syst., E86-D(9):1702?1709.
C. Hori and S. Furui. 2000a. Automatic speech
summarization based on word significance and
linguistic likelihood. In Proceedings ICASSP,
volume 3, pages 1579?1582.
C. Hori and S. Furui. 2000b. Improvements in
automatic speech summarization and evaluation
methods. In Proceedings ICSLP, volume 4,
pages 326?329.
C. Hori and S. Furui. 2001. Advances in auto-
matic speech summarization. In Proceedings Eu-
rospeech, volume 3, pages 1771?1774.
C. Hori and S. Furui. 2003a. Evaluation methods
for automatic speech summarization. In Proceed-
ings Eurospeech, pages 2825?2828.
C. Hori and S. Furui. 2003b. A new approach to
automatic speech summarization. IEEE Transac-
tions on Multimedia, 3:368?378.
H. Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Compu-
tational Linguistics, 28(4):527?543.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of
the 18th ACM-SIGIR, pages 68?73.
Chin-Yew Lin and E. H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram
co-occurrence statistics. In Proceedings HLT-
NAACL.
I. Mani and E. Bloedorn. 1998. Machine learning
of general and user-focused summarization. In
Proceedings of the 15th National Conference on
Artificial Intelligence, pages 821?826.
I. Mani and M. Maybury. 1999. Advances in Auto-
matic Text Summarization. The MIT Press.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings ACL.
K. Takeuchi and Y. Matsumoto. 2001. Relation be-
tween text structure and linguistic clues: An in-
vestigation on text structure of newspaper arti-
cles. Mathematical Linguistics, 22(8).
K. Zechner. 2003. Automatic summarization of
open-domain multiparty dialogues in diverse
genres. Computational Linguistics, 28(4):447?
485.
Appendix
? is given by
? = P (A) ? P (E)1 ? P (E) , (A-1)
where P (A) and P (E) are the probabilities of hu-
man agreement and chance agreement, respectively,
so ? is adjusted by the possibility of chance agree-
ment. This measure was used to assess agreement of
human selections for discourse segmentation (Car-
letta, 1996).
In this study, kappa was calculated using a table
of objects and categories (Takeuchi and Matsumoto,
2001). P (A) was calculated using
P (A) = 1N
N
?
i=1
Si, (A-2)
where N is the number of trials to select one class
among all classes, and Si is the probability that two
humans at least agree at the i-th selection:
Si =
m
?
j=1
nij C2
kC2
, (A-3)
where k and m are the number of subjects and
classes, respectively. When the task is sentence or
word extraction, the number of classes is two, i.e.,
extract/not extract. The numerator of eq. (A-3)
shows the sum of the combinations that two humans
at least agree for each class; nij is the number of hu-
mans who select the j-th class at the i-th selection.
P (E) is the probability of chance agreement by
at least two humans:
P (E) =
m
?
j=1
pj2, (A-4)
where pj is the probability of selecting the j-th class
given by
Pj =
N
?
i=1
nij
Nk , (A-5)
where the total number of humans who select the j-
th class for each trial is divided by the total number
of trials performed by all humans.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
Abstract
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
1 Introduction
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al, 2001) and train information e.g., ARISE
(Bouwman et al, 1999) and MASK (Lamel et al,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al, 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user?s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al, 2008). Hori et al
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
32
Table 2: Overview of Kyoto tour guide dialogue
corpus
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5/ dialogue (guide)
avg. # of utterance 301.7 112.9 373.5/ dialogue (tourist)
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-
lore et al, 2006; Rodriguez et al, 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
2 Kyoto Tour Guide Dialogue Corpus
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al, 2002) or (Thomson et al, 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column ?Time? in the table, it is easy to
see that there are many overlaps.
33
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
56 76669?78819 User
Ato (And,)
WH?Question Where
null
Ohara ga (Ohara is) (activity),location
dono henni (where) (activity),(demonstrative),interr
narimasuka (I?d like to know) (activity),predicate
57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun
58 81358?81841 Guide Ohara ha (Ohara) State Inversion location
59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116?83316 Guide A (Yeah,) Pause Grabber null
61 83136?85023 User
Kore demo (it)
Y/N?Question
null
ichinichi dewa (in a day) (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386?84396 Guide Soudesune (right.) State Acknowledgment?59 null
63 85206?87076 Guide
Ichinichi (One day)
State AffirmativeAnswer?61
(activity),(planning),(entity),day-window
areba (is) (activity),(planning),predicate
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
64 88392?90072 Guide
Oharamo (Ohara is)
State Opinion
(activity),location
sugoku (very) (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
65 89889?90759 User Iidesune (that would be nice.) State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64
* Tags are concatenated using a delimiter ? ? and omitting null values.
The number following the ??? symbol denotes the target utterance of the function.
3 Annotation of Communicative
Function and Semantic Content in DA
We annotate DAs in the corpus in order to de-
scribe a user?s intention and a system?s (or the tour
guide?s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al, 2002; Bangalore et al, 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
4 Speech Act Tags
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, ?ut-
terance? denotes a clause.
4.1 Tag Specifications
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus1). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al, 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer?s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
General: It is used to represent the basic form
1http://corpus.amiproject.org
34
Table 3: List of speech act tags and their occurrence in the experiment
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: ?Question,? ?Fragment,? and ?State-
ment.? ?Statement==? denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:? (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
4.2 Evaluation
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
4.2.1 Distributional Statistics
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al, 2004) and SWBD-DAMSL(Jurafsky et al,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
4.2.2 Reliability
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
35
Table 4: Agreement among labellers
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al, 2004) etc.)
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter?s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2 , which was calcu-
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.







    
	






	



	



	

	
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1908?1917, Dublin, Ireland, August 23-29 2014.
Recurrent Neural Network-based Tuple Sequence Model
for Machine Translation
Youzheng Wu, Taro Watanabe, Chiori Hori
National Institute of Information and Communications Technology (NICT), Japan
erzhengcn@gmail.com
{taro.watanabe, chiori.hori}@nict.go.jp
Abstract
In this paper, we propose a recurrent neural network-based tuple sequence model (RNNTSM)
that can help phrase-based translation model overcome the phrasal independence assumption.
Our RNNTSM can potentially capture arbitrary long contextual information during estimating
probabilities of tuples in continuous space. It, however, has severe data sparsity problem due
to the large tuple vocabulary coupled with the limited bilingual training data. To tackle this
problem, we propose two improvements. The first is to factorize bilingual tuples of RNNTSM
into source and target sides, we call factorized RNNTSM. The second is to decompose phrasal
bilingual tuples to word bilingual tuples for providing fine-grained tuple model. Our extensive
experimental results on the IWSLT2012 test sets
1
showed that the proposed approach essentially
improved the translation quality over state-of-the-art phrase-based translation systems (baselines)
and recurrent neural network language models (RNNLMs). Compared with the baselines, the
BLEU scores on English-French and English-German tasks were greatly enhanced by 2.1%-
2.6% and 1.8%-2.1%, respectively.
1 Introduction
The phrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized re-
ordering model to capture lexical dependencies that span phrase boundaries. Their translation models,
however, do not explicitly model context dependencies between translation units. To address this limi-
tation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to
capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be
effective in helping the phrase-based translation models overcome the phrasal independence assumption
(Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006;
Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual
tuples also known as minimal translation units (MTUs).
Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural
networks with factorizations to model bilingual tuples in a continuous space. Although the authors
reported some gains over the n-gram model in machine translation tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In language modeling, experimental
results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent
neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate
in speech recognition even though it is harder to train properly.
Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose re-
current neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation
system. Our RNNTSMs are capable of modeling long-span context and have better generalization. Com-
pared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions can
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures cov-
ering a variety of topics.
1908
be summarized as: (i) our models can be regarded as deep neural network translation models because
they can capture arbitrary-length context potentially, which are proven to estimate more accurate proba-
bilities of bilingual tuples; (ii) we extend the conventional RNNTSM to factorized RNNTSMs that can
significantly overcome the data sparseness problem caused by the large vocabularies of bilingual tuples
by incorporating the factors from the source and the target sides in addition to bilingual tuples; (iii) we
investigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducing
the out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate the
proposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement of
the work in (Son et al., 2012) that is a complete n-gram translation system.
2 Related Work
The n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples and
can improve the phrase-based translation system (Koehn et al., 2003) by providing contextual depen-
dencies between phrase pairs. To further improve the n-gram translation model, Crego and Yvon (2010)
explored factored bilingual n-gram language models. Durrani et al. (2011) proposed a joint sequence
model for the translation and reordering probabilities. Zhang et al. (2013) explored multiple decomposi-
tion structures as well as dynamic bidirectional decomposition. Since neural networks advance the state
of the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling
(Bengio et al., 2003), natural language processing (Collobert et al., 2011; Socher et al., 2013), machine
transliteration (Deselaers et al., 2009), etc, some prior studies have been done on neural network-based
translation models (NNTMs).
One kind of the NNTMs relies on word-to-word alignment information or phrasal bilingual tuples. For
example, Schwenk et al. (2007) investigated feed-forward neural networks to model bilingual tuples in
continuous space. Son et al. (2012) improved this idea by decomposing tuple units, i.e., distinguishing the
source and target sides of the tuple units, to address data sparsity issues. Although the authors reported
some gains over the n-gram model in the BLEU scores on some tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In addition, a feed-forward neural
network independent from bilingual tuples was proposed (Schwenk, 2012), which can infer meaningful
translation probabilities for phrase pairs not seen in the training data.
Another kind of the NNTMs do not rely on alignment. Auli et al. (2013) and Kalchbrenner and
Blunsom (2013) proposed joint language and translation model with recurrent neural networks, in which
latent semantic analysis and convolutional sentence model were used to model source-side sentence.
Potentially, they can exploit an unbounded history of both source and target words thanks to recurrent
connections. However, they only modestly observed gains over the recurrent neural network language
model. Previous studies (Wu and Wang, 2007; Yang et al., 2013) showed that the performance of word
alignment (alignment error rate) is nearly 80%. That means explicit word alignment may be more reliable
as a way to represent the corresponding bilingual sentences compared with an implicit compressed vector
representation (Auli et al., 2013).
Our RNNTSM takes the advantages of the above NNTMs, that is, RNN enables our model to cap-
ture long-span contextual information, while tuple sequence model uses word alignment without much
information loss. Furthermore, factorized RNN and word bilingual tuples are proposed to address data
sparsity issue. To the best of our knowledge, few studies have been done on this aspect.
3 Tuple Sequence Model
In tuple sequence model, bilingual tuples are translation units extracted from word-to-word alignment.
They are composed of source phrases and their aligned target phrases that are also known as minimal
translation units (MTUs) and thus cannot be broken down any further without violating the constrains
of the translation rules. This condition results in a unique segmentation of the bilingual sentence pair
given its alignment. In our implementation, GIZA++ with grow-diag-final-and setting is used
to conduct word-to-word alignments in both directions, source-to-target and target-to-source (Och and
1909
DZRUGWRZRUGDOLJQHGUHVXOWIURP*,=$ZLWKJURZGLDJILQDODQG
FRPSRVHUVDQG
RQW ?W? LQWHUURJ?V
DIULFDQ DPHULFDQ PXVLFLDQV
OHV musiciens DIURDP?ULFDL
LQWHUYLHZHGZHUH
HW FRPSRVLWHXUV
X
PXVLFLDQV
OHVmusiciensW
V
EVHTXHQFHRISKUDVDOELOLQJXDOWXSOHV
DIULFDQDPHULFDQ
DIURDP?ULFDL
V
X
W
X
DQG
HW
V
W
X
FRPSRVHUV
FRPSRVLWHXUV
V
W
X
ZHUHLQWHUYLHZHG
RQW?W?LQWHUURJ?V
V
W
DIULFDQ
DIURDP?ULFDL
V
X X X X XProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221?224,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura
MASTAR Project, NICT
Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.
1 Introduction
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user?s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.
This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.
We propose a model of dialogue state that consid-
ers the user?s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.
2 Spoken decision making dialogue
We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented
Choose the optimal spot
1. Cherry 
Blossoms
2. Japanese
Garden
3. Easy
Access
Kinkakuji-
Temple
Ryoanji-
Temple
Nanzenji-
Temple
?
?????
Goal
Criteria
Alternatives
(choices)
?????
p1 p2 p3
v11 v12 v13
? ?
Figure 1: Hierarchy structure for sightseeing guidance
dialogue
by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user?s evalu-
ation. We thus focus on these interactions.
Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.
For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector P
user
=
(p
1
, p
2
, . . . , p
M
) for criteria and local weight matrix
V
user
= (v
11
, v
12
, . . . , v
1M
, . . . , v
NM
) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of
?
M
m=1
p
m
v
km
. In typical AHP meth-
ods, the procedure of fixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system?s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.
We thus consider a spoken dialogue framework that
estimates the weights for the user?s preference (po-
tential preferences) as well as the user?s knowledge
221
about the domain through interactions of information
retrieval and navigation.
3 Decision support system with spoken
dialogue interface
The dialogue system we built has two functions: an-
swering users? information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user?s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).
3.1 Knowledge base
Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al, 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation e
nm
is ?1? when the spot n applies to the
determinant m and ?0? when it does not.
3.2 System initiative recommendation
The content of the recommendation is determined
based on one of the following six methods:
1. Recommendation of determinants based on the
currently focused spot (Method 1)
This method is structured on the basis of the user?s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is ?1? and presents
them to the user.
2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
This method functions on the basis of the focus on
a certain specific determinant.
3. Open prompt (Method 3)
The system does not make a recommendation, and
presents an open prompt.
4. Listing of determinants 1 (Method 4)
This method lists several determinants to the user in
ascending order from the low level user knowledge
K
sys
(that the system estimates). (K
sys
, P
sys
, p
m
and Pr(p
m
= 1) are defined and explained in Sec-
tion 4.2.)
5. Listing of determinants 2 (Method 5)
This method also lists the determinants, but the or-
der is based on the user?s high preference P
sys
(that
the system estimates).
6. Recommendation of user?s possibly preferred
spot (Method 6)
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference P
sys
. The sys-
tem selects one spot k with a maximum value of
?
M
m=1
Pr(p
m
= 1) ? e
k,m
. This idea is based
on collaborative filtering which is often used for
recommender systems (Breese et al, 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user?s preference; however,
it will be irrelevant if the system does not.
We will represent these recommendations
through a dialogue act expression, (ca
sys
{sc
sys
}),
which consists of a communicative act ca
sys
and the semantic content sc
sys
. (For exam-
ple Method1{(Spot
5
), (Det
3
,Det
4
,Det5)},
Method3{NULL,NULL}, etc.)
4 Optimization of dialogue strategy
4.1 Models for simulating a user
We introduce a user model that consists of a tuple of
knowledge vector K
user
, preference vector P
user
, and
local weight matrix V
user
. In this paper, for simplic-
ity, a user?s preference vector or weight for determi-
nants P
user
= (p
1
, p
2
, . . . , p
M
) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference p
m
is set to ?1?. Otherwise, it is set
to ?0?. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter K
user
= (k
1
, k
2
, . . . , k
M
) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. k
m
is set to
?1? if the user knows (or is listed by system?s recom-
mendations) that the system can handle determinant m
and ?0? when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(k
m
= 0, p
m
= 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al, 2007). A user?s local weight v
nm
for spot
n in terms of determinant m is set to ?1?, when the
system lets the user know that the evaluation of spots is
?1? through recommendation Methods 1, 2 and 6.
We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al, 2010) as well as the knowl-
edge and preference of the user. That is, the user?s com-
municative act cat
user
and the semantic content sct
user
for the system?s recommendation at
sys
are generated
based on the following equation:
Pr(cat
user
, sct
user
|cat
sys
, sct
sys
,K
user
,P
user
)
= Pr(cat
user
|cat
sys
)
?Pr(sct
user
|K
user
,P
user
, cat
user
, cat
sys
, sct
sys
)
This means that the user?s communicative act ca
user
is sampled based on the conditional probability of
Pr(cat
user
|cat
sys
) in (Misu et al, 2010). The seman-
tic content sc
user
is selected based on the user?s pref-
erence P
user
under current knowledge about the de-
terminants K
user
. That is, the sc is sampled from the
determinants within the user?s knowledge (k
m
= 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.
4.2 Dialogue state expression
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (P
user
,K
user
,V
user
) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP
222
 
Priors of the estimated state:
- Knowledge: K
sys
= (0.22, 0.01, 0.02, 0.18, . . . )
- Preference: P
sys
= (0.37, 0.19, 0.48, 0.38, . . . )
Interactions (observation):
- System recommendation:
a
sys
= Method1{(Spot
5
), (Det
1
, Det
3
, Det4)}
- User query:
a
user
= Accept{(Spot
5
), (Det
3
)}
Posterior of the estimated state:
- Knowledge: K
sys
= (1.00, 0.01, 1.00, 1.00, . . . )
- Preference: P
sys
= (0.26, 0.19, 0.65, 0.22, . . . )
User?s knowledge acquisition:
- Knowledge: K
user
? {k
1
= 1, k
3
= 1, k
4
= 1}
- Local weight: V
user
? {v
51
= 1, v
53
= 1, v
54
=
1}
 
Figure 2: Example of state update
and handle the problem as an MDP, we introduce
the system?s inferential user knowledge vector K
sys
or probability distribution (estimate value) K
sys
=
(Pr(k
1
= 1), P r(k
2
= 1), . . . , P r(k
M
= 1)) and
that of preference P
sys
= (Pr(p
1
= 1), P r(p
2
=
1), . . . , P r(p
M
= 1)).
The dialogue state DSt+1 or estimated user?s dia-
logue state of the step t+1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (at
sys
, at
user
).
The estimated user?s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al, 2005) classifies users to several discrete
user types. The estimated user?s preference P
sys
is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes? the-
orem using the previous state DSt as a prior.
Pr(p
m
= 1|It) =
Pr(I
t
|p
m
=1)Pr(p
m
=1)
Pr(I
t
|p
m
=1)Pr(p
m
=1)+Pr(I
t
|(p
m
=0))Pr(1?Pr(p
m
=1))
Here, Pr(It|p
m
= 1), P r(It|(p
m
= 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.
4.3 Reward function
The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference P
user
under their
knowledge K
user
(and local weight for spots V
user
)
at that time, and select the spot k with the maximum
priority of
?
m
k
k
? p
k
? v
km
. The reward R is then
calculated based on the improvement in the number of
agreed attributes between the user?s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.
R =
M
?
m=1
p
m
? e
k,m
?
1
N
N
?
n=1
M
?
m=1
p
m
? e
n,m
For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.
4.4 Optimization by reinforcement learning
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s
1
, s
2
, . . . , s
I
) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User?s
previous communicative act (1 if at?1
user
= x
i
, other-
wise 0). 3. System?s previous communicative act (1 if
at?1
sys
= y
j
, otherwise 0). 4. Sum of the estimated user
knowledge about determinants (?N
n=1
Pr(k
n
= 1)).
5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(k
n
= 1)? Pr(p
n
=
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.
A system action a
sys
(ca
sys
) is sampled based on the
following soft-max (Boltzmann) policy.
?(a
sys
= k|S) = Pr(a
sys
= k|S,?)
=
exp(
?
I
i=1
s
i
? ?
ki
)
?
J
j=1
exp(
?
I
i=1
s
i
? ?
ji
)
Here, ? = (?
11
, ?
12
, . . . ?
1I
, . . . , ?
JI
) consists of J (#
actions) ? I (# features) parameters. The parameter
?
ji
works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This ? is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.
4.5 Experiment by dialogue simulation
For each simulated dialogue session, a simulated user
(P
user
,K
user
,V
user
) is sampled. A preference vector
P
user
of the user is generated so that he/she has four
preferences. As a result, four parameters in P
user
are
?1? and the others are ?0?. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al,
2010). The user?s knowledge K
user
is also set based
on the statistics of the ?percentage of users who stated
the determinants before system recommendation?. For
each determinant, we sample a random valuable r that
ranges from ?0? to ?1?, and k
m
is set to ?1? if r is
smaller than the percentage. All the parameters of
local weights V
user
are initialized to ?0?, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user?s preference P
sys
and knowledge K
sys
are ini-
tialized based on the statistics of our trial system (Misu
et al, 2010).
We assumed that the system does not misunderstand
the user?s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy ? at that time and the user simulator
1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.
2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in
223
Table 1: Comparison of reward with baseline methods
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)
Table 2: Comparison of reward with discrete dialogue
state expression
Reward (?std)
State T = 5 T = 10 T = 15 T = 20
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)
Table 3: Effect of estimated preference and knowledge
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or
Know
0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)
of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter ?) is updated using NAC in every 2,000
dialogues.
4.6 Experimental result
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter ?. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user?s preference, and then, rec-
ommends specific information that would be useful to
the user.
Next, we compared the trained policy with the fol-
lowing baseline methods.
1. No recommendation (B1)
The system only provides the requested informa-
tion and does not generate any recommendations.
2. Random recommendation (B2)
The system randomly chooses a recommendation
from six methods.
The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p < .01).
We then compared the proposed method with the
case where estimated user?s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user?s preference p
m
of determinant m is set to ?1?
when the user requested the determinant, otherwise it
is ?0?. The estimated user?s knowledge k
m
is set to
the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.
3The parameters can be interpreted as the size of the con-
tribution for selecting the action.
?1? when the system lets the user know the determi-
nant, otherwise it is ?0?. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p < .01 (T=15,20)) the method using a
discrete state expression.
We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(k
n
=
1) ? Pr(p
n
= 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(k
n
= 1) or estimated
preference Pr(p
n
= 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p < .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.
5 Conclusion
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.
Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.
References
J. Breese, D. Heckerman, and C. Kadie. 1998. ?empirical
analysis of predictive algorithms for collaborative filter-
ing?. In ?Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence?, pages 43?52.
K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169?183.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.
K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32?39.
J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180?1190.
J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479?487.
T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.
224
