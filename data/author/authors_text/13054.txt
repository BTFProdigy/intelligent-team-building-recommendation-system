Coling 2010: Poster Volume, pages 81?89,
Beijing, August 2010
Improved Unsupervised Sentence Alignment for Symmetrical and
Asymmetrical Parallel Corpora
Fabienne Braune Alexander Fraser
Institute for Natural Language Processing
Universita?t Stuttgart
{braunefe,fraser}@ims.uni-stuttgart.de
Abstract
We address the problem of unsupervised
and language-pair independent alignment
of symmetrical and asymmetrical parallel
corpora. Asymmetrical parallel corpora
contain a large proportion of 1-to-0/0-to-1
and 1-to-many/many-to-1 sentence corre-
spondences. We have developed a novel
approach which is fast and allows us to
achieve high accuracy in terms of F1 for
the alignment of both asymmetrical and
symmetrical parallel corpora. The source
code of our aligner and the test sets are
freely available.
1 Introduction
Sentence alignment is the problem of, given a par-
allel text, finding a bipartite graph matching min-
imal groups of sentences in one language to their
translated counterparts. Because sentences do not
always align 1-to-1, the sentence alignment task is
non-trivial.
The achievement of high accuracy with mini-
mal consumption of computational resources is a
common requirement for sentence alignment ap-
proaches. However, in order to be applicable to
parallel corpora in any language without requir-
ing a separate training set, a method for sentence-
alignment should also work in an unsupervised
fashion and be language pair independent. By
?unsupervised?, we denote methods that infer the
alignment model directly from the data set to be
aligned. Language pair independence refers to ap-
proaches that require no specific knowledge about
the languages of the parallel texts to align.
We have developed an approach to unsuper-
vised and language-pair independent sentence
alignment which allows us to achieve high accu-
racy in terms of F1 for the alignment of both sym-
metrical and asymmetrical parallel corpora. Due
to the incorporation of a novel two-pass search
procedure with pruning, our approach is accept-
ably fast. Compared with Moore?s bilingual sen-
tence aligner (Moore, 2002), we obtain an average
F1 of 98.38 on symmetrical parallel documents,
while Moore?s aligner achieves 94.06. On asym-
metrical documents, our approach achieves 97.67
F1 while Moore?s aligner obtains 88.70. On av-
erage, our sentence aligner is only about 4 times
slower than Moore?s aligner.
This paper is organized as follows: previous
work is described in section 2. In section 3, we
present our approach. Finally, in section 4, we
conduct an extensive evaluation, including a brief
insight into the impact of our aligner on the over-
all performance of an MT system.
2 Related Work
Among approaches that are unsupervised and lan-
guage independent, (Brown et al, 1991) and (Gale
and Church, 1993) use sentence-length statistics
in order to model the relationship between groups
of sentences that are translations of each other. As
shown in (Chen, 1993) the accuracy of sentence-
length based methods decreases drastically when
aligning texts containing small deletions or free
translations. In contrast, our approach augments a
sentence-length based model with lexical statistics
and hence constantly provides high quality align-
ments.
(Moore, 2002) proposes a multi-pass search
81
procedure where sentence-length based statistics
are used in order to extract the training data for
the IBM Model-1 translation tables. The ac-
quired lexical statistics are then combined with
the sentence-length based model in order to ex-
tract 1-to-1 correspondences with high accuracy1.
Moore?s approach constantly achieves high preci-
sion, is robust to sequences of inserted and deleted
text, and is fast. However, the obtained recall is
at most equal to the proportion of 1-to-1 corre-
spondences contained in the parallel text to align.
This point is especially problematic when align-
ing asymmetrical parallel corpora. In contrast,
our approach allows to extract 1-to-many/many-
to-1 correspondences. Hence, we achieve high
accuracy in terms of precision and recall on both
symmetrical and asymmetrical documents. More-
over, because we use, in the last pass of our multi-
pass method, a novel two-stage search procedure,
our aligner also requires acceptably low computa-
tional resources.
(Deng et al, 2006) have developed a multi-
pass method similar to (Moore, 2002) but where
the last pass is composed of two alignment pro-
cedures: a standard dynamic programming (DP)
search that allows one to find many-to-many
alignments containing a large amount of sentences
in each language and a divisive clustering al-
gorithm that optimally refines those alignments
through iterative binary splitting. This alignment
method allows one to find, in addition to 1-to-
1 correspondences, high quality 1-to-many/many-
to-1 alignments. However, 1-to-0 and 0-to-1 cor-
respondences are not modeled in this approach2.
This leads to poor performance on parallel texts
containing that type of correspondence. Further-
more performing an exhaustive DP search in or-
der to find large size many-to-many alignments
involves high computational costs. In comparison
to (Deng et al, 2006), our approach works in the
opposite way. Our two-step search procedure first
1The used search heuristic is a forward-backward compu-
tation with a pruned dynamic programming procedure as the
forward pass.
2In (Deng et al, 2006), p. 5, the p(ak) = p(x, y) which
determines the prior probability of having an alignment con-
taining x source and y target sentences, is equal to 0 if x < 1
or y < 1. As p(ak) is a multiplicative factor of the model,
the probability of having an insertion or a deletion is always
equal to 0.
finds a model-optimal alignment composed of the
smallest possible correspondences, namely 1-to-
0/0-to-1 and 1-to-1, and then merges those cor-
respondences into larger alignments. This allows
the finding of 1-to-0/0-to-1 alignments as well
as high quality 1-to-many/many-to-1 alignments,
leading to high accuracy on parallel texts but also
on corpora containing large blocs of inserted or
deleted text. Furthermore, our approach keeps the
computational costs of the alignment procedure
low: our aligner is, on average, about 550 times
faster than our implementation3 of (Deng et al,
2006).
Many other approaches to sentence-alignment
are either supervised or language dependent. The
approaches by (Chen, 1993), (Ceausu et al, 2006)
or (Fattah et al, 2007) need manually aligned
pairs of sentences in order to train the used align-
ment models. The approaches by (Wu, 1994),
(Haruno and Yamazaki, 1996), (Ma, 2006) and
(Gautam and Sinha, 2007) require an externally
supplied bilingual lexicon. Similarly, the ap-
proaches by (Simard and Plamondon, 1998) or
(Melamed, 2000) are language pair dependent in-
sofar as they are based on cognates.
3 Two-Step Clustering Approach
We present here our two-step clustering approach
to sentence alignment4 which is the main contri-
bution of this paper. We begin by giving the main
ideas of our approach using an introductory exam-
ple (section 3.1). Then we show to which extent
computational costs are reduced in comparison to
a standard DP search (section 3.2) before present-
ing the theoretical background of our approach
(section 3.3). We further discuss a novel prun-
ing strategy used within our approach (section
3.4). This pruning technique is another important
contribution of this paper. Next, we present the
alignment model (section 3.5) which is a slightly
modified version of the alignment model used in
(Moore, 2002). Finally, we describe the overall
3In order to provide a precise comparison between our
aligner and (Deng et al, 2006), we have implemented their
model into our optimized framework.
4Note that our approach does not aim to find many-to-
many alignments. None of the unsupervised sentence align-
ment approaches discussed in section 2 are able to correctly
find that type of correspondence.
82
procedure required to align a parallel text with our
method (section 3.6).
3.1 Sketch of Approach
Consider a parallel text composed of six source
language sentences Fi and four target language
sentences Ej . Further assume that the correct
alignment between the given texts is composed of
four correspondences: three 1-to-1 alignments be-
tween F1, E1; F2, E2 and F6, E4 as well as a 3-to-
1 alignment between F3, F4, F5 and E3. Figure 1
illustrates this alignment.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 1: Correct Alignment between Fi and Ej
In the perspective of a statistical approach to
sentence alignment, the alignment in figure 1 is
found by computing the model-optimal alignment
A? for the bitext considered:
A? = argmax
A
?
ak?A
SCORE(ak) (1)
where SCORE(ak) denotes the score attributed
by the alignment model5 to a minimal alignment
ak composing A?. The optimization given in
equation 1 relies on two commonly made assump-
tions: (c1) a model-optimal alignment A? can
be decomposed into k minimal and independent
alignments ak; (c2) each alignment ak depends
only on local portions of text in both languages.
The search for A? is generally performed us-
ing a dynamic programming (DP) procedure over
the space formed by the l source and m target
sentences. The computation of A? using a DP
search relies on the assumption (c3) that sentence
alignment is a monotonic and continuous process.
The DP procedure recursively computes the opti-
mal score D(l,m)? for a sequence of alignments
covering the whole parallel corpus. The optimal
score D(l,m)? is given by the following recur-
5The alignment model will be presented in section 3.5.
sion:
D(l,m)? = min
0?x,y?R , x=1?y=1D(l ? x,m? y)
?
? logSCORE(ak)
(2)
where x denotes the number of sentences on the
source language side of ak and y the number of
sentences on the target language side of ak.
The constant R constitutes an upper bound to
the number of sentences that are allowed on each
side of a minimal alignment ak. This constant has
an important impact on the computational costs
of the DP procedure insofar as it determines the
number of minimal alignments that have to be
compared and scored at each step of the recursion
given in equation 2. As will be shown in section
3.2, the number of comparisons increases depend-
ing on R.
The solution we propose to the combinatorial
growth of the number of performed operations
consists of dividing the search for A? into two
steps. First, a model-optimal alignment A?1, in
which the value of R is fixed to 1, is found. Sec-
ond, the alignments a?k composing A?1 are merged
into clusters mr containing up to R sentences on
either the source or target language side. The
alignment composed of these clusters is A?R.
The search for the first alignment A?1 is per-
formed using a standard DP procedure as given in
equation 2 but withR = 1. This first alignment is,
hence, only composed of 0-to-1, 1-to-0 and 1-to-1
correspondences. Using our example, we show, in
figure 2, the alignment A?1 found in the first step
of our approach. The neighbors of F4, that is F3
and F5, are aligned as 1-to-0 correspondences.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 2: A?1 in our Approach (first step)
The search for A?R is performed using a DP
search over the alignments a?k composingA?1. The
score D(AR)? obtained when all alignments a?k ?
A?1 have been optimally clustered can be written
83
recursively as:
D(AR)? = min0?r?RD(AR ? r)
?
? logSCORE(mr)
(3)
whereD(AR?r)? denotes the best score obtained
for the prefix covering all minimal alignments in
A?1 except the last r minimal alignments consid-
ered for composing the last cluster mr.
The application of the second step of our ap-
proach is illustrated in figure 3. The first align-
ment, between F1 and E1, cannot be merged to be
part of a 1-to-many or many-to-1 cluster because
the following alignment in A?1 is also 1-to-1. So
it must be retained as given in A?1. The five last
alignments are, however, candidates for compos-
ing clusters. For instance, the alignment F2-E2
and F3-, where  denotes the empty string, could
be merged in order to compose the 2-to-1 cluster
F2,F3-E2. However, in our example, the align-
ment model chooses to merge the alignments F3-
, F4-E3 and F5- in order to compose the 3-to-1
cluster F3,F4,F5-E3.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 3: A?R in our Approach (second step)
3.2 Computational Gains
The aim of this section is to give an idea about
why our method is faster than the standard DP
approach. Let C denote the number of compar-
isons performed at each step of the recursion of
the standard DP procedure, as given in equation
2. This amount is equivalent to the number of
possible combinations of x source sentences with
y target sentences. Hence, for an approach find-
ing all types of correspondences except many-to-
many, we have:
C = 2R+ 1 (4)
In terms of lookups in the word-correspondence
tables of a model including lexical statistics, the
number of operations Cl performed at each step
of the recursion is given by:
Cl = R? ? w2 (5)
where R? denotes the number of scored sen-
tences6. w denotes the average length of each
sentence in terms of words. The total number of
lookups performed in order to align a parallel text
containing l source and m target sentences using
a standard DP procedure is hence given by:
L = R? ? w2 ? l ?m (6)
In the perspective of our two-step search proce-
dure, the computational costs of the search for the
initial alignment A?1 is given by:
L?1 = w2 ? l ?m (7)
For the second step of our approach, because A?R
is a cluster of A?1, the dynamic programming pro-
cedure used to find this alignment is no longer
over the l ? m space formed by the source and
target sentences but instead over the space formed
by the minimal alignments a?k in A?1. The aver-
age number of those alignments is approximately
l+m
2 .7 The number of lookups performed at eachstep of our DP procedure is given by:
L?2 = R? ? w2 ?
l +m
2 (8)
where R? and w are defined as in equation 6.
The total number of lookups for our clustering ap-
proach is hence given by:
L?1+2 = (w2 ? l ?m) + (R? ? w2 ?
l +m
2 ) (9)
In order to compare the costs of our approach and
a standard DP search over the l ?m space formed
by the source and target sentences, we re-write
equation 6 as:
L = (w2 ? l ?m) + ((R?? 1) ?w2 ? l ?m) (10)
The comparison of equation 9 with equation 10
shows that the computational gains obtained using
our two-step approach reside in the reduction of
the search space from l ?m to l+m2 .8
6In a framework where no caching of scores is performed,
we have R? = R2 +R+1 compared sentences while score-
caching allows one to reduce R? to R.
7Note that this amount tends to l +m when A?1 contains
a large number of 0-to-1/1-to-0 correspondences.
8It should be noted that through efficient pruning, the
search space of the standard (DP) procedure can be further
reduced, see section 3.4.
84
3.3 Theoretical Background
We now present the theoretical foundation of our
approach. First, we rewrite equation 1 in a more
detailed fashion as:
A?R = argmax
A
?
ak(xk,yk)?AR
P (ak(xk, yk), sqi , trj)
(11)
with 0 ? xk, yk ? R, where R denotes the max-
imal amounts x and y of source and target lan-
guage sentences composing a minimal alignment
ak(xk, yk). The distribution P (ak(xk, yk), sqi , trj)
specifies the alignment model presented in section
3.5.
As seen in section 3.1, the formulation of the
alignment problem as given in equation 11 and the
use of a DP search in order to solve this equation
rely on the assumptions (c1) to (c3). Following
these assumptions, a model-optimal alignmentA?1
can be defined as an ordered set of minimal align-
ments a?k(xk, yk), with 0 ? xk, yk ? 1, where the
aligned portions of text are sequential. In other
words, if the k ? th alignment a?k(xk, yk) con-
tains the sequences sqi and trj of source and tar-
get language sentences, then the next alignment
a?k+1(xk+1, yk+1) is composed of the sequences
suq+1 and tvr+1. Hence, each alignment composing
AR, with R > 1, can be obtained through sequen-
tial merging of a series of alignments a?k(xk, yk) ?
A?1.9 Accordingly, the sequences of sentences su1
and tv1 are obtained by merging sq1 and tr1 with
suq+1 and tvr+1. It can then be assumed that (c4) the
ordered set of minimal alignments composing A?R
under equation 11 is equivalent to the set of clus-
ters obtained by sequentially merging the minimal
alignments composing A?1. Following assump-
tion (c4), the optimization over ak(xk, yk) ? AR
is equivalent to an optimization over the merged
alignmentsmr(xr, yr) ? AR. Hence, equation 11
is equivalent to:
A?R = argmax
AR
?
mr(xr,yr)?AR
P (mr(xr, yr), sui , tvj )
(12)
where each mr(xr, yr) is obtained by merging r
minimal alignments a?k(xk, yk) ? A?1.
9Alignments of type 1-to-0/0-to-1 and 1-to-1 are assumed
to be clusters where a minimal alignment a?k(xk, yk) ? A?1has been merged with the empty alignment e0(0, 0)(, ).
The computation of A?R is done in two
steps. First, a model-optimal alignment A?1 is
found using a standard DP procedure as de-
fined in equation 2 but with R = 1 and where
SCORE(ak) is given by the alignment model
? logP (ak, sll?x+1, tmm?y+1). In the second step,
the search procedure used to find the optimal
clusters is defined as in equation 3 but where
SCORE(mr) is given by the alignment model
? logP (mr, sui , tvj ).
3.4 Search Space Pruning
In order to further reduce the costs of finding A?1,
we initially pruned the search space in the same
fashion as (Moore, 2002). We explored a nar-
row band around the main diagonal of the bi-
text to align. Each time the approximated align-
ment came close to the boundaries of the band,
the search was reiterated with a larger band size.
However, the computational costs for alignments
that were not along the diagonal quickly increased
with this pruning strategy. A high loss of effi-
ciency was hence observed when aligning asym-
metrical documents with this technique. Inciden-
tally, Moore reports, in his experiments, that for
the alignment of a parallel text containing 300
deleted sentences, the computational costs of his
pruned DP procedure is 40 times higher than for a
corpus containing no deletions.
In order to overcome this problem, we devel-
oped a pruning strategy that allows us to avoid the
loss of efficiency occurring when aligning asym-
metrical documents. Instead of exploring a nar-
row band around the main diagonal of the text to
align, we use sentence-length statistics in order to
compute an approximate path through the consid-
ered bitext. Our search procedure then explores
the groups of sentences that are around this path.
If the approximated alignment comes close to the
boundaries of the band, the search is re-iterated.
The path initially provided using a sentence-
length model10 and then iteratively refined is
closer to the correct alignment than the main di-
agonal of the bitext to align. Hence, the approxi-
mated alignment does not come close to the band
10The used model is the sentence-length based component
of (Moore, 2002), which is able to find 1-to-0/0-to-1 corre-
spondences.
85
as often as when searching around the main di-
agonal. This results in relatively high computa-
tional gains, especially for asymmetrical parallel
texts (see section 4).
3.5 Moore?s Alignment Model
The model we use is basically the same as in
(Moore, 2002) but minor modifications have been
made in order to integrate this model in our two-
step clustering approach. The three component
distributions of the model are given by11:
P (ak, sqi , trj) = P (ak)P (s
q
i |ak)P (trj |ak, s
q
i )
(13)
The first component, P (ak), specifies the gen-
eration of a minimal alignment ak. The second
component, P (sqi |ak), specifies the generation of
a sequence sqi of source language sentences in
a minimal alignment ak. The last component,
i.e. P (trj |ak, sqi ), specifies the generation of a se-
quence of target language sentences depending on
a sequence of generated source sentences.
Our first modification to Moore?s model con-
cerns the component distribution P (ak). In the
second pass of our two-step approach, which is
the computation of the model-optimal clustered
alignment A?R, we estimate P (ak) by computing
the relative frequency of sequences of alignments
a?k in the initial alignment A?1 that are candidates
for composing a cluster mr of specific size.12 A
second minimal modification to Moore?s model
concerns the lexical constituent of P (trj |ak, sqi ),
which we denote here by P (fb|en, ak). In contrast
with Moore, we use the best alignment (Viterbi
alignment) of each target word fb with all source
words en, according to IBM Model-1:
P (fb|en, ak) =
argmaxlen=1 Pt(fb|en)
le + 1
(14)
where le denotes the number of words in the
source sentence(s) of ak. Our experimental results
have shown that this variant performed slightly
better than Moore?s summing over all alignments.
11In order to simplify the presentation of the model, we
use the short notation ak for denoting ak(xk, yk)
12For the computation ofA?1, the distribution P (ak) is de-
fined as in Moore?s work.
3.6 Alignment Procedure
In order to align a parallel text (sl1, tm1 ) we use
a multi-pass procedure similar to (Moore, 2002)
but where the last pass is replaced by our two-
step clustering approach. In the first pass, an ap-
proximate alignment is computed using sentence-
length based statistics and the one-to-one corre-
spondences with likelihood higher than a given
threshold are selected for the training of the IBM
Model-1 translation tables13. Furthermore, each
found alignment is cached in order to be used as
the initial diagonal determining the search space
for the next pass. In the second pass, the corpus is
re-aligned according to our two-step approach: (i)
a model-optimal14 alignment containing at most
one sentence on each side of the minimal align-
ments ak(xk, yk) is found; (ii) those alignments
are model-optimally merged in order to obtain an
alignment containing up to R sentences on each
side of the clusters mr(xr, yr). In our experi-
ments, a maximum number of 4 sentences is al-
lowed on each side of a cluster.
4 Experiments
We evaluate our approach (CA) using three base-
lines against which we compare alignment qual-
ity and computational costs.15 The first (Mo) is
the method by (Moore, 2002). As a second base-
line (Std), we have implemented an aligner that
finds the same type of correspondences as our ap-
proach but performs a standard DP search instead
of our two-pass clustering procedure and imple-
ments Moore?s pruning strategy. Our third base-
line (Std P.) is similar to (Std) but integrates our
pruning technique.16 We also evaluate the impact
13Words with frequency < 3 in the corpus have been
dropped.
14This is optimal according to the alignment model which
will be presented in section 3.5.
15We do not evaluate sentence-length based methods in
our experiments because these methods obtain an F1 which
is generally about 10% lower than for our approach on
symmetrical documents. For asymmetrical documents the
performance is even worse. For example, when using
Gale&Church F1 sinks to 13.8 on documents which are not
aligned at paragraph level and contain small deletions.
16We do not include (Deng et al, 2006) in our exper-
iments because our implementation of this aligner is 550
times slower than our proposed method and the inability to
find 1-to-0/0-to-1 correspondences makes it inappropriate for
asymmetrical documents.
86
S 1-1 1-N/N-1 0-1/1-0 Oth. Tot.
1 88.2% 10.9 % 0.005% 0.85% 3,877
2 91.9% 7.5% 0.007% 0.53% 2,646
3 91.6% 2.7% 4.3% 1.4% 23,715
4 44.8% 6.2% 49% 0.01% 2,606
Table 1: Test Set for Evaluation with 2 ? N ? 4
of our aligner on the overall performance of an
MT system.
Evaluation. We evaluate the alignment accu-
racy of our approach using four test sets annotated
at sentence-level. The two first are composed
of hand aligned documents from the Europarl
corpus for the language-pairs German-to-English
and French-to-English. The third is composed
of an asymmetric document from the German-to-
English part of the Europarl corpus. Our fourth
test set is a version of the BAF corpus (Simard,
1998), where we corrected the tokenization. BAF
is an interesting heterogeneous French-to-English
test set composed of 11 texts belonging to four
different genres. The types of correspondences
composing our test sets are given in table 1. The
metrics used are precision, recall and F117. Only
alignments that correspond exactly to reference
alignments count as correct. The computational
costs required for each approach are measured in
seconds. The time required to train IBM Model-1
is not included in our calculations18.
Summary of Results. Regarding alignment ac-
curacy, the results in table 2 show that (CA) ob-
tains, on average, an F1 that is 4.30 better than
for (Mo) on symmetrical documents. The results
in table 3 show that, on asymmetrical texts, (CA)
achieves an F1 which is 8.97 better than (Mo).
The accuracy obtained using (CA), (Std) and (Std
P.) is approximately the same. We have further
compared the accuracy of (CA) with (Std) for
finding 1-to-many/many-to-1 alignments. The ob-
tained results show that (CA) achieves an F1 that
is 5.0 better than (Std).
Regarding computational costs, the time re-
quired by (CA) is on average 4 times larger than
17We measure precision, recall and F1 on the 1-to-N/N-to-
1 alignments,N >= 1, which means that we view insertions
and deletions as ?negative? decisions, like Moore.
18The reason for this decision is that our optimized frame-
work trains the Model-1 translation tables far faster than
Moore?s bilingual sentence aligner.
for (Mo) when aligning symmetrical documents.
On asymmetrical documents, (Mo) is, however,
only 1.5 times faster than (CA). Compared to
(Std), (CA) is approximately 6 times faster on
symmetrical and 80 times faster on asymmetrical
documents. The time of (Std P.) is 3 times higher
than for (CA) on symmetrical documents and 22
times higher on asymmetrical documents. This
shows that, first, our pruning technique is more
efficient than Moore?s and, second, that the main
increase in speed is due to the two step clustering
approach.
Discussion. On the two first test sets, (Mo)
achieves high precision while the obtained recall
is limited by the number of correspondences that
are not 1-to-1 (see table 1). Regarding (Std), (Std
P.) and (CA), all aligners achieve high precision
as well as high recall, leading to an F1 which is
over 98% for both documents. The computational
costs of (CA) for the alignment of symmetrical
documents are, on average, 4 times higher than
(Mo), 6 times lower than (Std) and 3.5 times
lower than (Std P.). On our third test set (Mo)
achieves, with an F1 of 88.70, relatively poor
recall while the other aligners reach precision
and recall values that are over 98%. Regarding
the computational costs, (CA) is only 1.5 times
slower than (Mo) on asymmetrical documents
while it is 80 times faster than (Std) and about 22
times faster than (Std P.). On our fourth test set
all evaluated aligners perform approximately the
same than on Europarl. While (Mo) obtains, with
94.46, an F1 which is the same as for Europarl,
(CA) performs, with an F1 of 97.67, about
1% worse than on Europarl. A slightly larger
decrease of 1.6% is observed for (Std) which
obtains 96.81 F1. Note, however, that (CA), (Std)
and (Std P.) still perform about 3% better than
(Mo). Regarding computational costs, (CA) is
4 times slower than (Mo) and 40 times faster
than (Std). The high difference in speed between
our approach and (Std) is due to the fact that the
BAF corpus contains texts of variable symmetry
while (Std) shows a great speed decrease when
aligning asymmetrical documents. Finally, we
have compared the accuracy of (Std) and (CA) for
the finding of 1-to-many/many-to-1 alignments
containing at least 3 sentences on the ?many?
87
Appr. Lang. Prec. Rec. F1 Speed
Mo D-E 98.75 87.88 92.99 935s
Mo F-E 98.97 91.56 95.12 1,661s
Std D-E 98.42 98.57 98.49 24,152s
Std F-E 98.45 98.83 98.64 35,041s
Std P. D-E 98.37 98.49 98.43 13,387s
Std P. F-E 98.41 98.78 98.60 21,848s
CA D-E 98.25 98.70 98.47 3,461s
CA F-E 98.00 98.60 98.30 6,978s
Table 2: Performance on Europarl
Appr. Prec. Rec. F1 Speed
Mo 97.90 81.08 88.70 552s
Std 97.66 97.74 97.70 71,475s
Std P. 97.74 97.81 97.77 17,502s
CA 97.38 97.97 97.67 800s
Table 3: Performance on asym. documents
Appr. Prec. Rec. F1 Speed
Mo 96.58 92.43 94.46 563s
Std 96.82 96.80 96.81 84,988s
CA 97.05 97.63 97.34 2,137s
Table 4: Performance on BAF
side. This experiment has shown that (Std)
finds a larger amount of those alignments while
making numerous wrong conjectures. On the
other hand, (CA) finds less 1-to-many/many-to-1
correspondences but makes only few incorrect
hypotheses. Hence, F1 is about 5% better for
(CA).
MT evaluation We also measured the impact
of 1-to-N/N-to-1 alignments (which are not ex-
tracted by Moore) on MT. We used standard set-
tings of the Moses toolkit, and the Europarl de-
vtest2006 set as our test set. We ran MERT sep-
arately for each system. System (s1) was trained
just on the 1-to-1 alignments extracted from the
Europarl v3 corpus by our system while system
(s2) was trained with all correspondences found.
(s1) obtains a BLEU score of 0.2670 while (s2)
obtains a BLEU score of 0.2703. Application of
the pairwise bootstrap test (Koehn, 2004) shows
that (s2) is significantly better than (s1).
5 Conclusion
We have addressed the problem of unsupervised
and language-pair independent alignment of sym-
metrical and asymmetrical parallel corpora. We
have developed a novel approach which is fast
and allows us to achieve high accuracy in terms
of F1 for the alignment of bilingual corpora.
Our method achieved high accuracy on symmet-
rical and asymmetrical parallel corpora, and we
have shown that the 1-to-N/N-to-1 alignments ex-
tracted by our approach are useful. The source
code of the aligner and the test sets are available
at http://sourceforge.net/projects/gargantua .
6 Acknowledgements
The first author was partially supported by the
Hasler Stiftung19. Support for both authors was
provided by Deutsche Forschungsgemeinschaft
grants Models of Morphosyntax for Statistical
Machine Translation and SFB 732.
References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In In
Proceedings of 29th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 169?176.
Ceausu, Alexandru, Dan Stefanescu, and Dan Tufis.
2006. Acquis communautaire sentence alignment
using support vector machines. In LREC 2006:
Fifth International Conference on Language Re-
sources and Evaluation.
Chen, Stanley F. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 9?16.
Deng, Yoggang, Shankar Kumar, and William Byrne.
2006. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 12:1?26.
Fattah, Mohamed Abdel, David B. Bracewell, Fuji
Ren, and Shingo Kuroiwa. 2007. Sentence align-
ment using p-nnt and gmm. Computer Speech and
Language, (21):594?608.
Gale, William A. and Kenneth Ward Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Gautam, Mrityunjay and R. M. K. Sinha. 2007. A
program for aligning sentences in bilingual cor-
pora. Proceedings of the International Conference
19http://www.haslerstiftung.ch/.
88
on Computing: Theory and Applications, ICCTA
?07, (1):480?484.
Haruno, M. and T. Yamazaki. 1996. High-
performance bilingual text alignment using statisti-
cal and dictionary information. In Proceedings of
ACL ?96, pages 131?138.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Lin, Dekang and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation.
Melamed, I. Dan. 2000. Models of translational
equivalence among words. Computational Linguis-
tics, 26:221?249.
Moore, Robert. 2002. Fast and accurate sentence
alignment of bilingual corpora. In In Proceedings
of 5th Conference of the Association for Machine
Translation in the Americas, pages 135?244.
Simard, Michel and Pierre Plamondon. 1998. Bilin-
gual sentence alignment: Balancing robustness and
accuracy. Machine Translation, 13(1):59?80.
Simard, Michel. 1998. The baf: A corpus of english-
french bitext. In Proceedings of LREC 98, Granada,
Spain.
Wu, Dekai. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In In Pro-
ceedings of the 32nd Annual Conference of the
Association for Computational Linguistics, 80?87,
Las, pages 80?87.
89
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 808?817,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Composing extended top-down tree transducers?
Aure?lie Lagoutte
E?cole normale supe?rieure de Cachan, De?partement Informatique
alagoutt@dptinfo.ens-cachan.fr
Fabienne Braune and Daniel Quernheim and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing
{braunefe,daniel,maletti}@ims.uni-stuttgart.de
Abstract
A composition procedure for linear and
nondeleting extended top-down tree trans-
ducers is presented. It is demonstrated that
the new procedure is more widely applica-
ble than the existing methods. In general,
the result of the composition is an extended
top-down tree transducer that is no longer
linear or nondeleting, but in a number of
cases these properties can easily be recov-
ered by a post-processing step.
1 Introduction
Tree-based translation models such as syn-
chronous tree substitution grammars (Eisner,
2003; Shieber, 2004) or multi bottom-up tree
transducers (Lilin, 1978; Engelfriet et al 2009;
Maletti, 2010; Maletti, 2011) are used for sev-
eral aspects of syntax-based machine transla-
tion (Knight and Graehl, 2005). Here we consider
the extended top-down tree transducer (XTOP),
which was studied in (Arnold and Dauchet,
1982; Knight, 2007; Graehl et al 2008; Graehl
et al 2009) and implemented in the toolkit
TIBURON (May and Knight, 2006; May, 2010).
Specifically, we investigate compositions of linear
and nondeleting XTOPs (ln-XTOP). Arnold and
Dauchet (1982) showed that ln-XTOPs compute
a class of transformations that is not closed under
composition, so we cannot compose two arbitrary
ln-XTOPs into a single ln-XTOP. However, we
will show that ln-XTOPs can be composed into a
(not necessarily linear or nondeleting) XTOP. To
illustrate the use of ln-XTOPs in machine transla-
tion, we consider the following English sentence
together with a German reference translation:
? All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).
RC
PREL
that
C
NP VP
7?
C
NP VP
C
NP VP
VAUX VPART NP
7?
C
NP VP
VAUX NP VPART
Figure 1: Word drop [top] and reordering [bottom].
The newswire reported yesterday that the Serbs have
completed the negotiations.
Gestern [Yesterday] berichtete [reported] die [the]
Nachrichtenagentur [newswire] die [the] Serben
[Serbs] ha?tten [would have] die [the] Verhandlungen
[negotiations] beendet [completed].
The relation between them can be described
(Yamada and Knight, 2001) by three operations:
drop of the relative pronoun, movement of the
participle to end of the clause, and word-to-word
translation. Figure 1 shows the first two oper-
ations, and Figure 2 shows ln-XTOP rules per-
forming them. Let us now informally describe
the execution of an ln-XTOP on the top rule ?
of Figure 2. In general, ln-XTOPs process an in-
put tree from the root towards the leaves using
a set of rules and states. The state p in the left-
hand side of ? controls the particular operation of
Figure 1 [top]. Once the operation has been per-
formed, control is passed to states pNP and pVP,
which use their own rules to process the remain-
ing input subtree governed by the variable below
them (see Figure 2). In the same fashion, an ln-
XTOP containing the bottom rule of Figure 2 re-
orders the English verbal complex.
In this way we model the word drop by an ln-
XTOP M and reordering by an ln-XTOP N . The
syntactic properties of linearity and nondeletion
yield nice algorithmic properties, and the mod-
808
pRC
PREL
that
C
y1 y2
?
C
pNP
y1
pVP
y2
q
C
z1 VP
z2 z3 z4
?
C
qNP
z1
VP
qVA
z2
qVP
z4
qNP
z3
Figure 2: XTOP rules for the operations of Figure 1.
ular approach is desirable for better design and
parametrization of the translation model (May et
al., 2010). Composition allows us to recombine
those parts into one device modeling the whole
translation. In particular, it gives all parts the
chance to vote at the same time. This is especially
important if pruning is used because it might oth-
erwise exclude candidates that score low in one
part but well in others (May et al 2010).
Because ln-XTOP is not closed under compo-
sition, the composition ofM andN might be out-
side ln-XTOP. These cases have been identified
by Arnold and Dauchet (1982) as infinitely ?over-
lapping cuts?, which occur when the right-hand
sides of M and the left-hand sides of N are un-
boundedly overlapping. This can be purely syn-
tactic (for a given ln-XTOP) or semantic (inher-
ent in all ln-XTOPs for a given transformation).
Despite the general impossibility, several strate-
gies have been developed: (i) Extension of the
model (Maletti, 2010; Maletti, 2011), (ii) online
composition (May et al 2010), and (iii) restric-
tion of the model, which we follow. Composi-
tions of subclasses in which the XTOP N has at
most one input symbol in its left-hand sides have
already been studied in (Engelfriet, 1975; Baker,
1979; Maletti and Vogler, 2010). Such compo-
sitions are implemented in the toolkit TIBURON.
However, there are translation tasks in which the
used XTOPs do not fulfill this requirement. Sup-
pose that we simply want to compose the rules of
Figure 2, The bottom rule does not satisfy the re-
quirement that there is at most one input symbol
in the left-hand side.
We will demonstrate how to compose two lin-
ear and nondeleting XTOPs into a single XTOP,
which might however no longer be linear or non-
deleting. However, when the syntactic form of
?(?)
q(1)
x(11)1
?(2)
?(21) q(22)
x(221)2
?(3)
?(31)
p(311)
x(3111)3
?
q
x1
? ?
?
p
x3
Figure 3: Linear normalized tree t ? T?(Q(X)) [left]
and t[?]2 [right] with var(t) = {x1, x2, x3}. The posi-
tions are indicated in t as superscripts. The subtree t|2
is ?(?, q(x2)).
the composed XTOP has only bounded overlap-
ping cuts, post-processing will get rid of them
and restore an ln-XTOP. In the remaining cases,
in which unbounded overlapping is necessary or
occurs in the syntactic form but would not be nec-
essary, we will compute an XTOP. This is still
an improvement on the existing methods that just
fail. Since general XTOPs are implemented in
TIBURON and the new composition covers (essen-
tially) all cases currently possible, our new com-
position procedure could replace the existing one
in TIBURON. Our approach to composition is the
same as in (Engelfriet, 1975; Baker, 1979; Maletti
and Vogler, 2010): We simply parse the right-
hand sides of the XTOP M with the left-hand
sides of the XTOP N . However, to facilitate this
approach we have to adjust the XTOPs M and N
in two pre-processing steps. In a first step we cut
left-hand sides of rules of N into smaller pieces,
which might introduce non-linearity and deletion
into N . In certain cases, this can also intro-
duce finite look-ahead (Engelfriet, 1977; Graehl
et al 2009). To compensate, we expand the rules
of M slightly. Section 4 explains those prepa-
rations. Next, we compose the prepared XTOPs
as usual and obtain a single XTOP computing the
composition of the transformations computed by
M and N (see Section 5). Finally, we apply a
post-processing step to expand rules to reobtain
linearity and nondeletion. Clearly, this cannot be
successful in all cases, but often removes the non-
linearity introduced in the pre-processing step.
2 Preliminaries
Our trees have labels taken from an alphabet ?
of symbols, and in addition, leaves might be
labeled by elements of the countably infinite
809
?x1 ?
?
? ? x2
?
7?
?
? ?
?
? ? x2
?
?[
?
? x3
Figure 4: Substitution where ?(x1) = ?, ?(x2) = x2,
and ?(x3) = ?(?(?, ?, x2)).
set X = {x1, x2, . . . } of formal variables. For-
mally, for every V ? X the set T?(V ) of
?-trees with V -leaves is the smallest set such that
V ? T?(V ) and ?(t1, . . . , tk) ? T?(V ) for all
k ? N, ? ? ?, and t1, . . . , tk ? T?(V ). To avoid
excessive universal quantifications, we drop them
if they are obvious from the context.
For each tree t ? T?(X) we identify nodes by
positions. The root of t has position ? and the po-
sition iw with i ? N and w ? N? addresses the
position w in the i-th direct subtree at the root.
The set of all positions in t is pos(t). We write
t(w) for the label (taken from ? ?X) of t at po-
sition w ? pos(t). Similarly, we use
? t|w to address the subtree of t that is rooted
in position w, and
? t[u]w to represent the tree that is ob-
tained from replacing the subtree t|w at w
by u ? T?(X).
For a given set L ? ? ?X of labels, we let
posL(t) = {w ? pos(t) | t(w) ? L}
be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).
The tree t ? T?(V ) is linear if |posx(t)| ? 1 for
every x ? X . Moreover,
var(t) = {x ? X | posx(t) 6= ?}
collects all variables that occur in t. If the vari-
ables occur in the order x1, x2, . . . in a pre-order
traversal of the tree t, then t is normalized. Given
a finite set Q, we write Q(T ) with T ? T?(X)
for the set {q(t) | q ? Q, t ? T}. We will treat
elements of Q(T ) as special trees of T??Q(X).
The previous notions are illustrated in Figure 3.
A substitution ? is a mapping ? : X ? T?(X).
When applied to a tree t ? T?(X), it will return
the tree t?, which is obtained from t by replacing
all occurrences of x ? X (in parallel) by ?(x).
This can be defined recursively by x? = ?(x) for
all x ? X and ?(t1, . . . , tk)? = ?(t1?, . . . , tk?)
qS
S
x1 VP
x2 x3
?
S?
qV
x2
qNP
x1
qNP
x1
t
qS
S
t1
VP
t2 t3
?
t
S?
qV
t2
qNP
t1
qNP
t1
Figure 5: Rule and its use in a derivation step.
for all ? ? ? and t1, . . . , tk ? T?(X). The effect
of a substitution is displayed in Figure 4. Two
substitutions ?, ?? : X ? T?(X) can be com-
posed to form a substitution ??? : X ? T?(X)
such that ???(x) = ?(x)?? for every x ? X .
Next, we define two notions of compatibility
for trees. Let t, t? ? T?(X) be two trees. If there
exists a substitution ? such that t? = t?, then t? is
an instance of t. Note that this relation is not sym-
metric. A unifier ? for t and t? is a substitution ?
such that t? = t??. The unifier ? is a most gen-
eral unifier (short: mgu) for t and t? if for every
unifier ??? for t and t? there exists a substitution ??
such that ??? = ???. The set mgu(t, t?) is the set of
all mgus for t and t?. Most general unifiers can be
computed efficiently (Robinson, 1965; Martelli
and Montanari, 1982) and all mgus for t and t?
are equal up to a variable renaming.
Example 1. Let t = ?(x1, ?(?(?, ?, x2))) and
t? = ?(?, x3). Then mgu(t, t?) contains ? such
that ?(x1) = ? and ?(x3) = ?(?(?, ?, x2)). Fig-
ure 4 illustrates the unification.
3 The model
The discussed model in this contribution is an
extension of the classical top-down tree trans-
ducer, which was introduced by Rounds (1970)
and Thatcher (1970). The extended top-down
tree transducer with finite look-ahead or just
XTOPF and its variations were studied in (Arnold
and Dauchet, 1982; Knight and Graehl, 2005;
810
qS
S
x1 VP
x2 x3
S?
qV
x2
qNP
x1
qNP
x3
?
qS
S?
x2 x1 x3
S
qNP
x1
VP
qV
x2
qNP
x3
?
Figure 6: Rule [left] and reversed rule [right].
Knight, 2007; Graehl et al 2008; Graehl et
al., 2009). Formally, an extended top-down tree
transducer with finite look-ahead (XTOPF) is a
system M = (Q,?,?, I, R, c) where
? Q is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? I ? Q is a set of initial states,
? R is a finite set of (rewrite) rules of the form
` ? r where ` ? Q(T?(X)) is linear and
r ? T?(Q(var(`))), and
? c : R ? X ? T?(X) assigns a look-ahead
restriction to each rule and variable such that
c(?, x) is linear for each ? ? R and x ? X .
The XTOPF M is linear (respectively, nondelet-
ing) if r is linear (respectively, var(r) = var(`))
for every rule ` ? r ? R. It has no look-ahead
(or it is an XTOP) if c(?, x) ? X for all rules
? ? R and x ? X . In this case, we drop the look-
ahead component c from the description. A rule
` ? r ? R is consuming (respectively, produc-
ing) if pos?(`) 6= ? (respectively, pos?(r) 6= ?).
We let Lhs(M) = {l | ?q, r : q(l)? r ? R}.
Let M = (Q,?,?, I, R, c) be an XTOPF. In
order to facilitate composition, we define senten-
tial forms more generally than immediately nec-
essary. Let ?? and ?? be such that ? ? ??
and ? ? ??. To keep the presentation sim-
ple, we assume that Q ? (?? ? ??) = ?. A
sentential form of M (using ?? and ??) is a
tree of SF(M) = T??(Q(T??)). For every
?, ? ? SF(M), we write ? ?M ? if there exist a
positionw ? posQ(?), a rule ? = `? r ? R, and
a substitution ? : X ? T?? such that ?(x) is an in-
stance of c(?, x) for every x ? X and ? = ?[`?]w
and ? = ?[r?]w. If the applicable rules are re-
stricted to a certain subset R? ? R, then we also
write ? ?R? ?. Figure 5 illustrates a derivation
step. The tree transformation computed by M is
?M = {(t, u) ? T? ? T? | ?q ? I : q(t)?
?
M u}
where ??M is the reflexive, transitive closure
of?M . It can easily be verified that the definition
p
C
y1 y2
?
RC
PREL
that
C
pNP
y1
pVP
y2
Figure 7: Top rule of Figure 2 reversed.
of ?M is independent of the choice of ?? and ??.
Moreover, it is known (Graehl et al 2009) that
each XTOPF can be transformed into an equiva-
lent XTOP preserving both linearity and nondele-
tion. However, the notion of XTOPF will be con-
venient in our composition construction. A de-
tailed exposition to XTOPs is presented by Arnold
and Dauchet (1982) and Graehl et al(2009).
A linear and nondeleting XTOP M with
rules R can easily be reversed to obtain
a linear and nondeleting XTOP M?1 with
rules R?1, which computes the inverse transfor-
mation ?M?1 = ?
?1
M , by reversing all its rules.
A (suitable) rule is reversed by exchanging the
locations of the states. More precisely, given
a rule q(l) ? r ? R, we obtain the rule
q(r?) ? l? of R?1, where l? = l? and r? is the
unique tree such that there exists a substitution
? : X ? Q(X) with ?(x) ? Q({x}) for every
x ? X and r = r??. Figure 6 displays a rule
and its corresponding reversed rule. The reversed
form of the XTOP rule modeling the insertion op-
eration in Figure 2 is displayed in Figure 7.
Finally, let us formally define composition.
The XTOP M computes the tree transformation
?M ? T? ? T?. Given another XTOP N that
computes a tree transformation ?N ? T? ? T?,
we might be interested in the tree transforma-
tion computed by the composition of M and N
(i.e., running M first and then N ). Formally, the
composition ?M ; ?N of the tree transformations
?M and ?N is defined by
?M ; ?N = {(s, u) | ?t : (s, t) ? ?M , (t, u) ? ?N}
and we often also use the notion ?composition? for
XTOP with the expectation that the composition
of M and N computes exactly ?M ; ?N .
4 Pre-processing
We want to compose two linear and nondelet-
ing XTOPs M = (P,?,?, IM , RM ) and
811
LHS(M?1) LHS(N)
C
y1 y2
C
z1 VP
z2 z3 z4
Figure 8: Incompatible left-hand sides of Example 3.
N = (Q,?,?, IN , RN ). Before we actually per-
form the composition, we will prepare M and N
in two pre-processing steps. After these two steps,
the composition is very simple. To avoid com-
plications, we assume that (i) all rules of M are
producing and (ii) all rules of N are consuming.
For convenience, we also assume that the XTOPs
M and N only use variables of the disjoint sets
Y ? X and Z ? X , respectively.
4.1 Compatibility
In the existing composition results for subclasses
of XTOPs (Engelfriet, 1975; Baker, 1979; Maletti
and Vogler, 2010) the XTOP N has at most one
input symbol in its left-hand sides. This restric-
tion allows us to match rule applications of N to
positions in the right-hand sides of M . Namely,
for each output symbol in a right-hand side of M ,
we can select a rule of N that can consume that
output symbol. To achieve a similar decompo-
sition strategy in our more general setup, we in-
troduce a compatibility requirement on right-hand
sides of M and left-hand sides of N . Roughly
speaking, we require that the left-hand sides of N
are small enough to completely process right-
hand sides of M . However, a comparison of
left- and right-hand sides is complicated by the
fact that their shape is different (left-hand sides
have a state at the root, whereas right-hand sides
have states in front of the variables). We avoid
these complications by considering reversed rules
of M . Thus, an original right-hand side of M is
now a left-hand side in the reversed rules and thus
has the right format for a comparison. Recall that
Lhs(N) contains all left-hand sides of the rules
of N , in which the state at the root was removed.
Definition 2. The XTOP N is compatible to M
if ?(Y ) ? X for all unifiers ? ? mgu(l1|w, l2)
between a subtree at a ?-labeled position
w ? pos?(l1) in a left-hand side l1 ? Lhs(M
?1)
and a left-hand side l2 ? Lhs(N).
Rule of M?1 Rule of N
?
p1
y1
p2
y2
? ?
p
?
y1 y2
q
?
? ?
z1 z2
?
?
q1
z1
q2
z2
Figure 9: Rules used in Example 5.
Intuitively, for every ?-labeled position w in a
right-hand side r1 of M and any left-hand side l2
of N , we require (ignoring the states) that either
(i) r1|w and l2 are not unifiable or (ii) r1|w is an
instance of l2.
Example 3. The XTOPs for the English-to-
German translation task in the Introduction are
not compatible. This can be observed on the
left-hand side l1 ? Lhs(M?1) of Figure 7
and the left-hand side l2 ? Lhs(N) of Fig-
ure 2[bottom]. These two left-hand sides are il-
lustrated in Figure 8. Between them there is an
mgu such that ?(Y ) 6? X (e.g., ?(y1) = z1 and
?(y2) = VP(z2, z3, z4) is such an mgu).
Theorem 4. There exists an XTOPF N ? that is
equivalent to N and compatible with M .
Proof. We achieve compatibility by cutting of-
fending rules of the XTOP N into smaller pieces.
Unfortunately, both linearity and nondeletion
of N might be lost in the process. We first let
N ? = (Q,?,?, IN , RN , cN ) be the XTOPF such
that cN (?, x) = x for every ? ? RN and x ? X .
If N ? is compatible with M , then we are done.
Otherwise, let l1 ? Lhs(M?1) be a left-hand side,
q(l2) ? r2 ? RN be a rule, and w ? pos?(l1)
be a position such that ?(y) /? X for some
? ? mgu(l1|w, l2) and y ? Y . Let v ? posy(l1|w)
be the unique position of y in l1|w.
Now we have to distinguish two cases: (i) Ei-
ther var(l2|v) = ? and there is no leaf in r2 la-
beled by a symbol from ?. In this case, we have
to introduce deletion and look-ahead into N ?. We
replace the old rule ? = q(l2) ? r2 by the new
rule ?? = q(l2[z]v) ? r2, where z ? X \ var(l2)
is a variable that does not appear in l2. In addition,
we let cN (??, z) = l2|v and cN (??, x) = cN (?, x)
for all x ? X \ {z}.
(ii) Otherwise, let V ? var(l2|v) be a maximal
set such that there exists a minimal (with respect
to the prefix order) position w? ? pos(r2) with
812
Another rule of N
q
?
z1 ?
z2 z3
?
?
q1
z1
q2
z2
q3
z3
Figure 10: Additional rule used in Example 5.
var(r2|w?) ? var(l2|v) and var(r2[?]w?)?V = ?,
where ? ? ? is arbitrary. Let z ? X \ var(l2) be
a fresh variable, q? be a new state of N , and
V ? = var(l2|v) \ V . We replace the rule
? = q(l2)? r2 of RN by
?1 = q(l2[z]v)? trans(r2)[q
?(z)]w?
?2 = q
?(l2|v)? r2|w? .
The look-ahead for z is trivial and other-
wise we simply copy the old look-ahead, so
cN (?1, z) = z and cN (?1, x) = cN (?, x) for all
x ? X \ {z}. Moreover, cN (?2, x) = cN (?, x)
for all x ? X . The mapping ?trans? is given for
t = ?(t1, . . . , tk) and q??(z??) ? Q(Z) by
trans(t) = ?(trans(t1), . . . , trans(tk))
trans(q??(z??)) =
{
?l2|v, q??, v??(z) if z?? ? V ?
q??(z??) otherwise,
where v? = posz??(l2|v).
Finally, we collect all newly generated states
of the form ?l, q, v? in Ql and for every such
state with l = ?(l1, . . . , lk) and v = iw, let
l? = ?(z1, . . . , zk) and
?l, q, v?(l?)?
{
q(zi) if w = ?
?li, q, w?(zi) otherwise
be a new rule of N without look-ahead.
Overall, we run the procedure until N ? is com-
patible with M . The procedure eventually ter-
minates since the left-hand sides of the newly
added rules are always smaller than the replaced
rules. Moreover, each step preserves the seman-
tics of N ?, which completes the proof.
We note that the look-ahead ofN ? after the con-
struction used in the proof of Theorem 4 is either
trivial (i.e., a variable) or a ground tree (i.e., a tree
without variables). Let us illustrate the construc-
tion used in the proof of Theorem 4.
?1 :
q
C
z1 z
?
C
qNP
z1
q?
z
?2 :
q?
VP
z2 z3 z4
?
VP
qVA
z2
qVP
z4
qNP
z3
Figure 11: Rules replacing the rule in Figure 7.
Example 5. Let us consider the rules illustrated
in Figure 9. We might first note that y1 has to
be unified with ?. Since ? does not contain any
variables and the right-hand side of the rule of N
does not contain any non-variable leaves, we are
in case (i) in the proof of Theorem 4. Conse-
quently, the displayed rule of N is replaced by a
variant, in which ? is replaced by a new variable z
with look-ahead ?.
Secondly, with this new rule there is an mgu,
in which y2 is mapped to ?(z1, z2). Clearly, we
are now in case (ii). Furthermore, we can select
the set V = {z1, z2} and position w? = . Cor-
respondingly, the following two new rules for N
replace the old rule:
q(?(z, z?))? q?(z?)
q?(?(z1, z2))? ?(q1(z1), q2(z2)) ,
where the look-ahead for z remains ?.
Figure 10 displays another rule of N . There is
an mgu, in which y2 is mapped to ?(z2, z3). Thus,
we end up in case (ii) again and we can select the
set V = {z2} and position w? = 2. Thus, we
replace the rule of Figure 10 by the new rules
q(?(z1, z))? ?(q1(z1), q
?(z), q3(z)) (?)
q?(?(z2, z3))? q2(z2)
q3(?(z1, z2))? q3(z2) ,
where q3 = ??(z2, z3), q3, 2?.
Let us use the construction in the proof of The-
orem 4 to resolve the incompatibility (see Exam-
ple 3) between the XTOPs presented in the Intro-
duction. Fortunately, the incompatibility can be
resolved easily by cutting the rule of N (see Fig-
ure 7) into the rules of Figure 11. In this example,
linearity and nondeletion are preserved.
813
4.2 Local determinism
After the first pre-processing step, we have the
original linear and nondeleting XTOP M and
an XTOPF N ? = (Q?,?,?, IN , R?N , cN ) that is
equivalent to N and compatible with M . How-
ever, in the first pre-processing step we might
have introduced some non-linear (copying) rules
in N ? (see rule (?) in Example 5), and it is known
that ?nondeterminism [in M ] followed by copy-
ing [inN ?]? is a feature that prevents composition
to work (Engelfriet, 1975; Baker, 1979). How-
ever, our copying is very local and the copies
are only used to project to different subtrees.
Nevertheless, during those projection steps, we
need to make sure that the processing in M pro-
ceeds deterministically. We immediately note that
all but one copy are processed by states of the
form ?l, q, v? ? Ql. These states basically pro-
cess (part of) the tree l and project (with state q)
to the subtree at position v. It is guaranteed that
each such subtree (indicated by v) is reached only
once. Thus, the copying is ?resolved? once the
states of the form ?l, q, v? are left. To keep the
presentation simple, we just add expanded rules
to M such that any rule that can produce a part of
a tree l immediately produces the whole tree. A
similar strategy is used to handle the look-ahead
of N ?. Any right-hand side of a rule of M that
produces part of a left-hand side of a rule of N ?
with look-ahead is expanded to produce the re-
quired look-ahead immediately.
Let L ? T?(Z) be the set of trees l such that
? ?l, q, v? appears as a state of Ql, or
? l = l2? for some ?2 = q(l2) ? r2 ? R?N
of N ? with non-trivial look-ahead (i.e.,
cN (?2, z) /? X for some z ? X), where
?(x) = cN (?2, x) for every x ? X .
To keep the presentation uniform, we assume
that for every l ? L, there exists a state of the
form ?l, q, v? ? Q?. If this is not already the
case, then we can simply add useless states with-
out rules for them. In other words, we assume that
the first case applies to each l ? L.
Next, we add two sets of rules to RM , which
will not change the semantics but prove to be use-
ful in the composition construction. First, for
every tree t ? L, let Rt contain all the rules
p(l) ? r, where p = p(l) ? r is a new state
with p ? P , minimal normalized tree l ? T?(X),
and an instance r ? T?(P (X)) of t such that
q
p
?
y1 y2
?
i
ps
y1
q
?
y2
q?
?
y2
?
i
ps
s?
y1
?
s
i
ps
y1
i
ps

? 
q
?s
?
y1 y2
i
ps
y1
?
q?
?s
?
y1 y2
q
p
y2
?
q
?s,s?/??s,s?
?
y1 y2 y3
i
ps
y1
?
q?
??s,s?
?
y1 y2 y3
?
i
ps?
y2
i
p?
y3
?
q?
?s,s?
?
y1 y2 y3
?
i
ps?
y2
q
?
y3
q?
?
y3
?
Figure 12: Useful rules for the composition M ? ;N ? of
Example 8, where s, s? ? {?, ?} and ? ? P?(z2,z3).
p(l) ??M ? ? ?M ? r for some ? that is not an
instance of t. In other words, we construct each
rule of Rt by applying existing rules of RM in
sequence to generate a (minimal) right-hand side
that is an instance of t. We thus potentially make
the right-hand sides of M bigger by joining sev-
eral existing rules into a single rule. Note that
this affects neither compatibility nor the seman-
tics. In the second step, we add pure ?-rules
that allow us to change the state to one that we
constructed in the previous step. For every new
state p? = p(l) ? r, let base(p?) = p. Then
R?M = RM ? RL ? RE and P
? = P ?
?
t?L Pt
where
RL =
?
t?L
Rt and Pt = {`(?) | `? r ? Rt}
RE = {base(p?)(x1)? p?(x1) | p? ?
?
t?L
Pt} .
Clearly, this does not change the semantics be-
cause each rule of R?M can be simulated by a
chain of rules of RM . Let us now do a full ex-
ample for the pre-processing step. We consider a
nondeterministic variant of the classical example
by Arnold and Dauchet (1982).
Example 6. Let M = (P,?,?, {p}, RM )
be the linear and nondeleting XTOP such that
P = {p, p?, p?}, ? = {?, ?, ?, ?, }, and
RM contains the following rules
p(?(y1, y2))? ?(ps(y1), p(y2)) (?)
814
p(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p(y3)))
p(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p?(y3)))
ps(s
?(y1))? s(ps(y1))
ps()? 
for every s, s? ? {?, ?}. Similarly, we let
N = (Q,?,?, {q}, RN ) be the linear and non-
deleting XTOP such thatQ = {q, i} andRN con-
tains the following rules
q(?(z1, z2))? ?(i(z1), i(z2))
q(?(z1, ?(z2, z3)))? ?(i(z1), i(z2), q(z3)) (?)
i(s(z1))? s(i(z1))
i()? 
for all s ? {?, ?}. It can easily be verified that
M and N meet our requirements. However, N is
not yet compatible with M because an mgu be-
tween rules (?) of M and (?) of N might map y2
to ?(z2, z3). Thus, we decompose (?) into
q(?(z1, z))? ?(i(z1), q(z), q
?(z))
q?(?(z2, z3))? q(z3)
q(?(z1, z2))? i(z1)
where q = ??(z2, z3), i, 1?. This newly obtained
XTOP N ? is compatible with M . In addition, we
only have one special tree ?(z2, z3) that occurs in
states of the form ?l, q, v?. Thus, we need to com-
pute all minimal derivations whose output trees
are instances of ?(z2, z3). This is again simple
since the first three rule schemes ?s, ?s,s? , and
??s,s? of M create such instances, so we simply
create copies of them:
?s(?(y1, y2))? ?(ps(y1), p(y2))
?s,s?(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p(y3)))
??s,s?(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p?(y3)))
for all s, s? ? {?, ?}. These are all the rules
of R?(z2,z3). In addition, we create the following
rules of RE :
p(x1)? ?s(x1) p(x1)? ?s,s?(x1)
p(x1)? ?
?
s,s?(x1)
for all s, s? ? {?, ?}.
Especially after reading the example it might
seem useless to create the rule copies inRl [in Ex-
ample 6 for l = ?(z2, z3)]. However, each such
rule has a distinct state at the root of the left-hand
side, which can be used to trigger only this rule.
In this way, the state selects the next rule to apply,
which yields the desired local determinism.
?q, p?
RC
PREL
that
C
x1 x2
?
C
?qNP, pNP?
x1
?q?, pVP?
x2
Figure 13: Composed rule created from the rule of Fig-
ure 7 and the rules of N ? displayed in Figure 11.
5 Composition
Now we are ready for the actual composition. For
space efficiency reasons we reuse the notations
used in Section 4. Moreover, we identify trees of
T?(Q?(P ?(X))) with trees of T?((Q? ? P ?)(X)).
In other words, when meeting a subtree q(p(x))
with q ? Q?, p ? P ?, and x ? X , then we also
view this equivalently as the tree ?q, p?(x), which
could be part of a rule of our composed XTOP.
However, not all combinations of states will be
allowed in our composed XTOP, so some combi-
nations will never yield valid rules.
Generally, we construct a rule ofM ? ;N ? by ap-
plying a single rule of M ? followed by any num-
ber of pure ?-rules of RE , which can turn states
base(p) into p. Then we apply any number of
rules of N ? and try to obtain a sentential form that
has the required shape of a rule of M ? ;N ?.
Definition 7. Let M ? = (P ?,?,?, IM , R?M ) and
N ? = (Q?,?,?, IN , R?N ) be the XTOPs con-
structed in Section 4, where
?
l?L Pl ? P
? and
?
l?LQl ? Q
?. Let Q?? = Q? \
?
l?LQl. We con-
struct the XTOPM ? ;N ? = (S,?,?, IN?IM , R)
where
S =
?
l?L
(Ql ? Pl) ? (Q
?? ? P ?)
and R contains all normalized rules `? r (of the
required shape) such that
`?M ? ? ?
?
RE ? ?
?
N ? r
for some ?, ? ? T?(Q?(T?(P ?(X)))).
The required rule shape is given by the defi-
nition of an XTOP. Most importantly, we must
have that ` ? S(T?(X)), which we identify
with a certain subset of Q?(P ?(T?(X))), and
r ? T?(S(X)), which similarly corresponds to
a subset of T?(Q?(P ?(X))). The states are sim-
ply combinations of the states of M ? and N ?, of
815
qp
?
y1 ?
y2 y3
?
?
i
ps
y1
i
ps
y2
q
p
y3
Figure 14: Successfully expanded rule from Exam-
ple 9.
which however the combinations of a state q ? Ql
with a state p /? Pl are forbidden. This reflects the
intuition of the previous section. If we entered a
special state of the form ?l, q, v?, then we should
use a corresponding state p ? Pl of M , which
only has rules producing instances of l. We note
that look-ahead of N ? is checked normally in the
derivation process.
Example 8. Now let us illustrate the composition
on Example 6. Let us start with rule (?) of M .
q(p(?(x1, x2)))
?M ? q(?(ps(x1), p(x2)))
?RE q(?(ps(x1), ?s?,s??(x2)))
?N ? ?(i(ps(x1)), q(?s?,s??(x2)), q
?(?s?,s??(x2)))
is a rule of M ? ; N ? for every s, s?, s?? ? {?, ?}.
Note if we had not applied the RE-step, then we
would not have obtained a rule of M ; N (be-
cause we would have obtained the state combina-
tion ?q, p? instead of ?q, ?s?,s???, and ?q, p? is not a
state of M ? ; N ?). Let us also construct a rule for
the state combination ?q, ?s?,s???.
q(?s?,s??(?(x1, x2, x3)))
?M ? q(?(ps?(x1), ?(ps??(x2), p(x3))))
?N ? q
?(ps?(x1))
Finally, let us construct a rule for the state combi-
nation ?q??, ?s?,s???.
q??(?s?,s??(?(x1, x2, x3)))
?M ? q(?(ps?(x1), ?(ps??(x2), p(x3))))
?RE q(?(ps?(x1), ?(ps??(x2), ?s(x3))))
?N ? q(?(ps??(x2), ?s(x3)))
?N ? ?(q
?(ps??(x1)), q(?s(x2)), q
??(?s(x2)))
for every s ? {?, ?}.
After having pre-processed the XTOPs in our
introductory example, the devices M and N ? can
be composed into M ; N ?. One rule of the com-
posed XTOP is illustrated in Figure 13.
q
p
?
y1 ?
y2 y3 y4
?
?
i
ps
y1
i
ps?
y2
?
i
ps??
y3
q
??
y4
q?
??
y4
Figure 15: Expanded rule that remains copying (see
Example 9).
6 Post-processing
Finally, we will compose rules again in an ef-
fort to restore linearity (and nondeletion). Since
the composition of two linear and nondeleting
XTOPs cannot always be computed by a single
XTOP (Arnold and Dauchet, 1982), this method
can fail to return such an XTOP. The presented
method is not a characterization, which means it
might even fail to return a linear and nondelet-
ing XTOP although an equivalent linear and non-
deleting XTOP exists. However, in a significant
number of examples, the recombination succeeds
to rebuild a linear (and nondeleting) XTOP.
Let M ? ;N ? = (S,?,?, I, R) be the composed
XTOP constructed in Section 5. We simply in-
spect each non-linear rule (i.e., each rule with a
non-linear right-hand side) and expand it by all
rule options at the copied variables. Since the
method is pretty standard and variants have al-
ready been used in the pre-processing steps, we
only illustrate it on the rules of Figure 12.
Example 9. The first (top row, left-most) rule of
Figure 12 is non-linear in the variable y2. Thus,
we expand the calls ?q, ??(y2) and ?q?, ??(y2). If
? = ?s for some s ? {?, ?}, then the next rules
are uniquely determined and we obtain the rule
displayed in Figure 14. Here the expansion was
successful and we could delete the original rule
for ? = ?s and replace it by the displayed ex-
panded rule. However, if ? = ??s?,s?? , then we can
also expand the rule to obtain the rule displayed in
Figure 15. It is still copying and we could repeat
the process of expansion here, but we cannot get
rid of all copying rules using this approach (as ex-
pected since there is no linear XTOP computing
the same tree transformation).
816
References
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoretical Computer
Science, 20(1):33?93.
Brenda S. Baker. 1979. Composition of top-down
and bottom-up tree transductions. Information and
Control, 41(2):186?213.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. ACL,
pages 205?208. Association for Computational Lin-
guistics.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Composition and decomposition of extended
multi bottom-up tree transducers. Acta Informatica,
46(8):561?590.
Joost Engelfriet. 1975. Bottom-up and top-down
tree transformations?A comparison. Mathemati-
cal Systems Theory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Mathematical Systems
Theory, 10(1):289?303.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410?430.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. CICLing, volume
3406 of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Eric Lilin. 1978. Une ge?ne?ralisation des transduc-
teurs d?e?tats finis d?arbres: les S-transducteurs.
The`se 3e`me cycle, Universite? de Lille.
Andreas Maletti and Heiko Vogler. 2010. Composi-
tions of top-down tree transducers with ?-rules. In
Proc. FSMNLP, volume 6062 of LNAI, pages 69?
80. Springer.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876?884. Association for Computational Linguis-
tics.
Andreas Maletti. 2011. An alternative to synchronous
tree substitution grammars. Natural Language En-
gineering, 17(2):221?242.
Alberto Martelli and Ugo Montanari. 1982. An effi-
cient unification algorithm. ACM Transactions on
Programming Languages and Systems, 4(2):258?
282.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proc. CIAA, vol-
ume 4094 of LNCS, pages 102?113. Springer.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted
tree transducers. In Proc. ACL, pages 1058?1066.
Association for Computational Linguistics.
Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Pro-
cessing. Ph.D. thesis, University of Southern Cali-
fornia, Los Angeles.
John Alan Robinson. 1965. A machine-oriented logic
based on the resolution principle. Journal of the
ACM, 12(1):23?41.
William C. Rounds. 1970. Mappings and grammars
on trees. Mathematical Systems Theory, 4(3):257?
287.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. Journal of Computer and System
Sciences, 4(4):339?367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530. Association for Computational Lin-
guistics.
817
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811?821,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Shallow Local Multi Bottom-up Tree Transducers
in Statistical Machine Translation
Fabienne Braune and Nina Seemann and Daniel Quernheim and Andreas Maletti
Institute for Natural Language Processing, University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de
Abstract
We present a new translation model in-
tegrating the shallow local multi bottom-
up tree transducer. We perform a large-
scale empirical evaluation of our obtained
system, which demonstrates that we sig-
nificantly beat a realistic tree-to-tree base-
line on the WMT 2009 English?German
translation task. As an additional contribu-
tion we make the developed software and
complete tool-chain publicly available for
further experimentation.
1 Introduction
Besides phrase-based machine translation sys-
tems (Koehn et al, 2003), syntax-based systems
have become widely used because of their abil-
ity to handle non-local reordering. Those systems
use synchronous context-free grammars (Chi-
ang, 2007), synchronous tree substitution gram-
mars (Eisner, 2003) or even more powerful for-
malisms like synchronous tree-sequence substitu-
tion grammars (Sun et al, 2009). However, those
systems use linguistic syntactic annotation at dif-
ferent levels. For example, the systems proposed
by Wu (1997) and Chiang (2007) use no linguis-
tic information and are syntactic in a structural
sense only. Huang et al (2006) and Liu et al
(2006) use syntactic annotations on the source lan-
guage side and show significant improvements in
translation quality. Using syntax exclusively on
the target language side has also been success-
fully tried by Galley et al (2004) and Galley et
al. (2006). Nowadays, open-source toolkits such
as Moses (Koehn et al, 2007) offer syntax-based
components (Hoang et al, 2009), which allow
experiments without expert knowledge. The im-
provements observed for systems using syntactic
annotation on either the source or the target lan-
guage side naturally led to experiments with mod-
els that use syntactic annotations on both sides.
However, as noted by Lavie et al (2008), Liu et
al. (2009), and Chiang (2010), the integration of
syntactic information on both sides tends to de-
crease translation quality because the systems be-
come too restrictive. Several strategies such as
(i) using parse forests instead of single parses (Liu
et al, 2009) or (ii) soft syntactic constraints (Chi-
ang, 2010) have been developed to alleviate this
problem. Another successful approach has been
to switch to more powerful formalisms, which al-
low the extraction of more general rules. A par-
ticularly powerful model is the non-contiguous
version of synchronous tree-sequence substitu-
tion grammars (STSSG) of Zhang et al (2008a),
Zhang et al (2008b), and Sun et al (2009),
which allows sequences of trees on both sides of
the rules [see also (Raoult, 1997)]. The multi
bottom-up tree transducer (MBOT) of Arnold and
Dauchet (1982) and Lilin (1978) offers a middle
ground between traditional syntax-based models
and STSSG. Roughly speaking, an MBOT is an
STSSG, in which all the discontinuities must oc-
cur on the target language side (Maletti, 2011).
This restriction yields many algorithmic advan-
tages over both the traditional models as well as
STSSG as demonstrated by Maletti (2010). For-
mally, they are expressive enough to express all
sensible translations (Maletti, 2012)1. Figure 2
displays sample rules of the MBOT variant, called
`MBOT, that we use (in a graphical representation
of the trees and the alignment).
In this contribution, we report on our novel sta-
tistical machine translation system that uses an
`MBOT-based translation model. The theoreti-
cal foundations of `MBOT and their integration
into our translation model are presented in Sec-
tions 2 and 3. In order to empirically evaluate the
`MBOT model, we implemented a machine trans-
1A translation is sensible if it is of linear size increase
and can be computed by some (potentially copying) top-down
tree transducer.
811
S?
NP1
JJ11
Official111
NNS12
forecasts121
VP2
VBD21
predicted211
NP22
QP221
RB2211
just22111
CD2212
322121
NN222
%2221
Figure 1: Example tree t with indicated positions.
We have t(21) = VBD and t|221 is the subtree
marked in red.
lation system that we are going to make available
to the public. We implemented `MBOT inside
the syntax-based component of the Moses open
source toolkit. Section 4 presents the most im-
portant algorithms of our `MBOT decoder. We
evaluate our new system on the WMT 2009 shared
translation task English ? German. The trans-
lation quality is automatically measured using
BLEU scores, and we confirm the findings by pro-
viding linguistic evidence (see Section 5). Note
that in contrast to several previous approaches, we
perform large scale experiments by training sys-
tems with approx. 1.5 million parallel sentences.
2 Theoretical Model
In this section, we present the theoretical genera-
tive model used in our approach to syntax-based
machine translation. Essentially, it is the local
multi bottom-up tree transducer of Maletti (2011)
with the restriction that all rules must be shallow,
which means that the left-hand side of each rule
has height at most 2 (see Figure 2 for shallow
rules and Figure 4 for rules including non-shallow
rules). The rules extracted from the training exam-
ple of Figure 3 are displayed in Figure 4. Those
extracted rules are forcibly made shallow by re-
moving internal nodes. The application of those
rules is illustrated in Figures 5 and 6.
For those that want to understand the inner
workings, we recall the principal model in full de-
tail in the rest of this section. Since we utilize syn-
tactic parse trees, let us introduce trees first. Given
an alphabet ? of labels, the set T? of all ?-trees is
the smallest set T such that ?(t1, . . . , tk) ? T for
all ? ? ?, integer k ? 0, and t1, . . . , tk ? T . In-
tuitively, a tree t consists of a labeled root node ?
followed by a sequence t1, . . . , tk of its children.
A tree t ? T? is shallow if t = ?(t1, . . . , tk) with
? ? ? and t1, . . . , tk ? ?.
NP
QP NN ?
( PP
von AP NN
)
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
Figure 2: Sample `MBOT rules.
To address a node inside a tree, we use its po-
sition, which is a word consisting of positive in-
tegers. Roughly speaking, the root of a tree is
addressed with the position ? (the empty word).
The position iw with i ? N addresses the po-
sition w in the ith direct child of the root. In
this way, each node in the tree is assigned a
unique position. We illustrate this notion in Fig-
ure 1. Formally, the positions pos(t) ? N? of
a tree t = ?(t1, . . . , tk) are inductively defined
by pos(t) = {?} ? pos(k)(t1, . . . , tk), where
pos(k)(t1, . . . , tk) =
?
1?i?k
{iw | w ? pos(ti)} .
Let t ? T? and w ? pos(t). The label of t at
position w is t(w), and the subtree rooted at posi-
tion w is t|w. These notions are also illustrated in
Figure 1. A position w ? pos(t) is a leaf (in t) if
w1 /? pos(t). In other words, leaves do not have
any children. Given a subset N ? ?, we let
leafN (t) = {w ? pos(t) | t(w) ? N, w leaf in t}
be the set of all leaves labeled by elements of N .
When N is the set of nonterminals, we call them
leaf nonterminals. We extend this notion to se-
quences t1, . . . , tk ? T? by
leaf(k)N (t1, . . . , tk) =
?
1?i?k
{iw | w ? leafN (ti)}.
Let w1, . . . , wn ? pos(t) be (pairwise prefix-
incomparable) positions and t1, . . . , tn ? T?.
Then t[wi ? ti]1?i?n denotes the tree that is ob-
tained from t by replacing (in parallel) the subtrees
at wi by ti for every 1 ? i ? n.
Now we are ready to introduce our model,
which is a minor variation of the local multi
bottom-up tree transducer of Maletti (2011). Let
? and ? be the input and output symbols, respec-
tively, and let N ? ? ?? be the set of nontermi-
nal symbols. Essentially, the model works on pairs
?t, (u1, . . . , uk)? consisting of an input tree t ? T?
812
and a sequence u1, . . . , uk ? T? of output trees.
Such pairs are pre-translations of rank k. The pre-
translation ?t, (u1, . . . , uk)? is shallow if all trees
t, u1, . . . , uk in it are shallow.
Together with a pre-translation we typically
have to store an alignment. Given a pre-translation
?t, (u1, . . . , uk)? of rank k and 1 ? i ? k,
we call ui the ith translation of t. An align-
ment for this pre-translation is an injective map-
ping ? : leaf(k)N (u1, . . . , uk)? leafN (t)?N such
that if (w, j) ? ran(?), then also (w, i) ? ran(?)
for all 1 ? j ? i.2 In other words, if an alignment
requests the ith translation, then it should also re-
quest all previous translations.
Definition 1 A shallow local multi bottom-up tree
transducer (`MBOT) is a finite set R of rules to-
gether with a mapping c : R ? R such that every
rule, written t ?? (u1, . . . , uk), contains a shal-
low pre-translation ?t, (u1, . . . , uk)? and an align-
ment ? for it.
The components t, (u1, . . . , uk), ?, and c(?)
are called the left-hand side, the right-hand
side, the alignment, and the weight of the
rule ? = t ?? (u1, . . . , uk). Figure 2 shows two
example `MBOT rules (without weights). Overall,
the rules of an `MBOT are similar to the rules of
an SCFG (synchronous context-free grammar), but
our right-hand sides contain a sequence of trees
instead of just a single tree. In addition, the align-
ments in an SCFG rule are bijective between leaf
nonterminals, whereas our model permits multi-
ple alignments to a single leaf nonterminal in the
left-hand side (see Figure 2).
Our `MBOT rules are obtained automatically
from data like that in Figure 3. Thus, we (word)
align the bilingual text and parse it in both the
source and the target language. In this manner
we obtain sentence pairs like the one shown in
Figure 3. To these sentence pairs we apply the
rule extraction method of Maletti (2011). The
rules extracted from the sentence pair of Figure 3
are shown in Figure 4. Note that these rules
are not necessarily shallow (the last two rules are
not). Thus, we post-process the extracted rules
and make them shallow. The shallow rules corre-
sponding to the non-shallow rules of Figure 4 are
shown in Figure 2.
Next, we define how to combine rules to form
derivations. In contrast to most other models, we
2ran(f) for a mapping f : A? B denotes the range of f ,
which is {f(a) | a ? A}.
S
NP
JJ
Official
NNS
forecasts
VP
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
VP
PP
APPR
von
AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
Figure 3: Aligned parsed sentences.
only introduce a derivation semantics that does
not collapse multiple derivations for the same
input-output pair.3 We need one final notion.
Let ? = t ?? (u1, . . . , uk) be a rule and
w ? leafN (t) be a leaf nonterminal (occurrence)
in the left-hand side. The w-rank rk(?, w) of the
rule ? is
rk(?, w) = max {i ? N | (w, i) ? ran(?)} .
For example, for the lower rule ? in Figure 2 we
have rk(?, 1) = 1, rk(?, 2) = 2, and rk(?, 3) = 1.
Definition 2 The set ?(R, c) of weighted pre-
translations of an `MBOT (R, c) is the smallest
set T subject to the following restriction: If there
exist
? a rule ? = t?? (u1, . . . , uk) ? R,
? a weighted pre-translation
?tw, cw, (uw1 , . . . , uwkw)? ? T
for every w ? leafN (t) with
? rk(?, w) = kw,4
? t(w) = tw(?),5 and
? for every iw? ? leaf(k)N (u1, . . . , uk),6
ui(w?) = uvj (?) with ?(iw?) = (v, j),
then ?t?, c?, (u?1, . . . , u?k)? ? T is a weighted pre-
translation, where
? t? = t[w ? tw | w ? leafN (t)],
3A standard semantics is presented, for example,
in (Maletti, 2011).
4If w has n alignments, then the pre-translation selected
for it has to have suitably many output trees.
5The labels have to coincide for the input tree.
6Also the labels for the output trees have to coincide.
813
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
) RB
just ?
( ADV
nur
) CD
3 ?
( CARD
3
) NN
% ?
( NN
%
)
NP
JJ NNS ?
( NP
ADJA NN
) QP
RB CD ?
( AP
ADV CARD
) NP
QP NN ?
( PPAPPR
von
AP NN )
S
NP VP
VBD NP
? (
S
NP VAFIN VP
PP VVPP
)
Figure 4: Extracted (even non-shallow) rules. We obtain our rules by making those rules shallow.
? c? = c(?) ??w?leafN (t) cw, and
? u?i = ui[iw? ? uvj | ?(iw?) = (v, j)] for
every 1 ? i ? k.
Rules that do not contain any nonterminal
leaves are automatically weighted pre-translations
with their associated rule weight. Otherwise, each
nonterminal leaf w in the left-hand side of a rule ?
must be replaced by the input tree tw of a pre-
translation ?tw, cw, (uw1 , . . . , uwkw)?, whose root islabeled by the same nonterminal. In addition, the
rank rk(?, w) of the replaced nonterminal should
match the number kw of components in the se-
lected weighted pre-translation. Finally, the non-
terminals in the right-hand side that are aligned
to w should be replaced by the translation that the
alignment requests, provided that the nontermi-
nal matches with the root symbol of the requested
translation. The weight of the new pre-translation
is obtained simply by multiplying the rule weight
and the weights of the selected weighted pre-
translations. The overall process is illustrated in
Figures 5 and 6.
3 Translation Model
Given a source language sentence e, our transla-
tion model aims to find the best corresponding tar-
get language translation g?;7 i.e.,
g? = arg maxg p(g|e) .
We estimate the probability p(g|e) through a log-
linear combination of component models with pa-
rameters ?m scored on the pre-translations ?t, (u)?
such that the leaves of t concatenated read e.8
p(g|e) ?
7?
m=1
hm
(
?t, (u)?
)?m
Our model uses the following features
hm(?t, (u1, . . . , uk)?) for a general pre-translation
? = ?t, (u1, . . . , uk)?:
7Our main translation direction is English to German.
8Actually, t must embed in the parse tree of e; see Sec-
tion 4.
(1) The forward translation weight using the rule
weights as described in Section 2
(2) The indirect translation weight using the rule
weights as described in Section 2
(3) Lexical translation weight source? target
(4) Lexical translation weight target? source
(5) Target side language model
(6) Number of words in the target sentences
(7) Number of rules used in the pre-translation
(8) Number of target side sequences; here k times
the number of sequences used in the pre-
translations that constructed ? (gap penalty)
The rule weights required for (1) are relative
frequencies normalized over all rules with the
same left-hand side. In the same fashion the rule
weights required for (2) are relative frequencies
normalized over all rules with the same right-
hand side. Additionally, rules that were extracted
at most 10 times are discounted by multiplying
the rule weight by 10?2. The lexical weights
for (2) and (3) are obtained by multiplying the
word translationsw(gi|ej) [respectively,w(ej |gi)]
of lexically aligned words (gi, ej) accross (possi-
bly discontiguous) target side sequences.9 When-
ever a source word ej is aligned to multiple target
words, we average over the word translations.10
h3(?t, (u1, . . . , uk)?)
=
?
lexical item
e occurs in t
average {w(g|e) | g aligned to e}
The computation of the language model esti-
mates for (6) is adapted to score partial transla-
tions consisting of discontiguous units. We ex-
plain the details in Section 4. Finally, the count c
of target sequences obtained in (7) is actually used
as a score 1001?c. This discourages rules with
many target sequences.
9The lexical alignments are different from the alignments
used with a pre-translation.
10If the word ej has no alignment to a target word, then
it is assumed to be aligned to a special NULL word and this
alignment is scored.
814
Combining a rule with pre-translations:
NP
JJ NNS ?
( NP
ADJA NN
)
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
)
Obtained new pre-translation:
NP
JJ
Official
NNS
forecasts
?
( NPADJA
Offizielle
NN
Prognosen
)
Figure 5: Simple rule application.
Combining a rule with pre-translations:
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
NP
JJ
Official
NNS
forecasts
? (
NP
ADJA
Offizielle
NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
)
NP
QP
RB
just
CD
3
NN
% ?
(
PP
von AP
ADV
nur
CARD
3
NN
%
)
Obtained new pre-translation:
S
NP
JJ
Official
NNS
forecasts
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
?
(
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
PP
von AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
)
Figure 6: Complex rule application.
S
NP VAFIN PP VVPP
Offizielle Prognosen ( sind , ausgegangen ) von nur 3 %
Figure 7: Illustration of LM scoring.
815
4 Decoding
We implemented our model in the syntax-based
component of the Moses open-source toolkit
by Koehn et al (2007) and Hoang et al (2009).
The standard Moses syntax-based decoder only
handles SCFG rules; i.e, rules with contiguous
components on the source and the target lan-
guage side. Roughly speaking, SCFG rules are
`MBOT rules with exactly one output tree. We
thus had to extend the system to support our
`MBOT rules, in which arbitrarily many output
trees are allowed.
The standard Moses syntax-based decoder uses
a CYK+ chart parsing algorithm, in which each
source sentence is parsed and contiguous spans are
processed in a bottom-up fashion. A rule is appli-
cable11 if the left-hand side of it matches the non-
terminal assigned to the full span by the parser and
the (non-)terminal assigned to each subspan.12 In
order to speed up the decoding, cube pruning (Chi-
ang, 2007) is applied to each chart cell in order
to select the most likely hypotheses for subspans.
The language model (LM) scoring is directly in-
tegrated into the cube pruning algorithm. Thus,
LM estimates are available for all considered hy-
potheses. To accommodate `MBOT rules, we had
to modify the Moses syntax-based decoder in sev-
eral ways. First, the rule representation itself is ad-
justed to allow sequences of shallow output trees
on the target side. Naturally, we also had to ad-
just hypothesis expansion and, most importantly,
language model scoring inside the cube pruning
algorithm. An overview of the modified pruning
procedure is given in Algorithm 1.
The most important modifications are hidden
in lines 5 and 8. The expansion in Line 5 in-
volves matching all nonterminal leaves in the rule
as defined in Definition 2, which includes match-
ing all leaf nonterminals in all (discontiguous) out-
put trees. Because the output trees can remain
discontiguous after hypothesis creation, LM scor-
ing has to be done individually over all output
trees. Algorithm 2 describes our LM scoring in
detail. In it we use k strings w1, . . . , wk to col-
lect the lexical information from the k output com-
11Note that our notion of applicable rules differs from the
default in Moses.
12Theoretically, this allows that the decoder ignores unary
parser nonterminals, which could also disappear when we
make our rules shallow; e.g., the parse tree left in the pre-
translation of Figure 5 can be matched by a rule with left-
hand side NP(Official, forecasts).
Algorithm 1 Cube pruning with `MBOT rules
Data structures:
- r[i, j]: list of rules matching span e[i . . . j]
- h[i, j]: hypotheses covering span e[i . . . j]
- c[i, j]: cube of hypotheses covering span e[i . . . j]
1: for all `MBOT rules ? covering span e[i . . . j] do
2: Insert ? into r[i, j]
3: Sort r[i, j]
4: for all (l?? r) ? r[i, j] do
5: Create h[i, j] by expanding all nonterminals in l with
best scoring hypotheses for subspans
6: Add h[i, j] to c[i, j]
7: for all hypotheses h ? c[i, j] do
8: Estimate LM score for h // see Algorithm 2
9: Estimate remaining feature scores
10: Sort c[i, j]
11: Retrieve first ? elements from c[i, j] // we use ? = 103
ponents (u1, . . . , uk) of a rule. These strings can
later be rearranged in any order, so we LM-score
all of them separately. Roughly speaking, we ob-
tain wi by traversing ui depth-first left-to-right.
If we meet a lexical element (terminal), then we
add it to the end of wi. On the other hand, if we
meet a nonterminal, then we have to consult the
best pre-translation ? ? = ?t?, (u?1, . . . , u?k?)?, which
will contribute the subtree at this position. Sup-
pose that u?j will be substituted into the nontermi-
nal in question. Then we first LM-score the pre-
translation ? ? to obtain the string w?j correspond-
ing to u?j . This string w?j is then appended to wi.
Once all the strings are built, we score them using
our 4-gram LM. The overall LM score for the pre-
translation is obtained by multiplying the scores
for w1, . . . , wk. Clearly, this treats w1, . . . , wk as
k separate strings, although they eventually will
be combined into a single string. Whenever such
a concatenation happens, our LM scoring will au-
tomatically compute n-gram LM scores based on
the concatenation, which in particular means that
the LM scores get more accurate for larger spans.
Finally, in the final rule only one component is al-
lowed, which yields that the LM indeed scores the
complete output sentence.
Figure 7 illustrates our LM scoring for a pre-
translation involving a rule with two (discontigu-
ous) target sequences (the construction of the pre-
translation is illustrated in Figure 6). When pro-
cessing the rule rooted at S, an LM estimate is
computed by expanding all nonterminal leaves. In
our case, these are NP, VAFIN, PP, and VVPP.
However, the nodes VAFIN and VVPP are assem-
bled from a (discontiguous) tree sequence. This
means that those units have been considered as in-
816
Algorithm 2 LM scoring
Data structures:
- (u1, . . . , uk): right-hand side of a rule
- (w1, . . . , wk): k strings all initially empty
1: score = 1
2: for all 1 ? i ? k do
3: for all leaves ` in ui (in lexicographic order) do
4: if ` is a terminal then
5: Append ` to wi
6: else
7: LM score the best hypothesis for the subspan
8: Expand wi by the corresponding w?j
9: score = score ? LM(wi)
dependent until now. So far, the LM scorer could
only score their associated unigrams. However,
we also have their associated strings w?1 and w?2,
which can now be used. Since VAFIN and VVPP
now become parts of a single tree, we can perform
LM scoring normally. Assembling the string we
obtain
Offizielle Prognosen sind von nur 3 %
ausgegangen
which is scored by the LM. Thus, we first score
the 4-grams ?Offizielle Prognosen sind von?, then
?Prognosen sind von nur?, etc.
5 Experiments
5.1 Setup
The baseline system for our experiments is the
syntax-based component of the Moses open-
source toolkit of Koehn et al (2007) and Hoang
et al (2009). We use linguistic syntactic anno-
tation on both the source and the target language
side (tree-to-tree). Our contrastive system is the
`MBOT-based translation system presented here.
We provide the system with a set of SCFG as well
as `MBOT rules. We do not impose any maximal
span restriction on either system.
The compared systems are evaluated on the
English-to-German13 news translation task of
WMT 2009 (Callison-Burch et al, 2009). For
both systems, the used training data is from the
4th version of the Europarl Corpus (Koehn, 2005)
and the News Commentary corpus. Both trans-
lation models were trained with approximately
1.5 million bilingual sentences after length-ratio
filtering. The word alignments were generated
by GIZA++ (Och and Ney, 2003) with the grow-
diag-final-and heuristic (Koehn et al, 2005). The
13Note that our `MBOT-based system can be applied to any
language pair as it involves no language-specific engineering.
System BLEU
Baseline 12.60
`MBOT ?13.06
Moses t-to-s 12.72
Table 1: Evaluation results. The starred results
are statistically significant improvements over the
Baseline (at confidence p < 0.05).
English side of the bilingual data was parsed us-
ing the Charniak parser of Charniak and John-
son (2005), and the German side was parsed us-
ing BitPar (Schmid, 2004) without the function
and morphological annotations. Our German 4-
gram language model was trained on the Ger-
man sentences in the training data augmented
by the Stuttgart SdeWaC corpus (Web-as-Corpus
Consortium, 2008), whose generation is detailed
in (Baroni et al, 2009). The weights ?m in the
log-linear model were trained using minimum er-
ror rate training (Och, 2003) with the News 2009
development set. Both systems use glue-rules,
which allow them to concatenate partial transla-
tions without performing any reordering.
5.2 Results
We measured the overall translation quality with
the help of 4-gram BLEU (Papineni et al, 2002),
which was computed on tokenized and lower-
cased data for both systems. The results of our
evaluation are reported in Table 1. For com-
parison, we also report the results obtained by
a system that utilizes parses only on the source
side (Moses tree-to-string) with its standard fea-
tures.
We can observe from Table 1 that our `MBOT-
based system outperforms the baseline. We ob-
tain a BLEU score of 13.06, which is a gain of
0.46 BLEU points over the baseline. This im-
provement is statistically significant at confidence
p < 0.05, which we computed using the pairwise
bootstrap resampling technique of Koehn (2004).
Our system is also better than the Moses tree-to-
string system. However this improvement (0.34)
is not statistically significant. In the next section,
we confirm the result of the automatic evaluation
through a manual examination of some transla-
tions generated by our system and the baseline.
In Table 2, we report the number of `MBOT
rules used by our system when decoding the test
set. By lex we denote rules containing only lexical
817
lex non-term total
contiguous 23,175 18,355 41,530
discontiguous 315 2,516 2,831
Table 2: Number of rules used in decoding test
(lex: only lexical items; non-term: at least one
nonterminal).
2-dis 3-dis 4-dis
2,480 323 28
Table 3: Number of k-discontiguous rules.
items. The label non-term stands for rules contain-
ing at least one leaf nonterminal. The results show
that approx. 6% of all rules used by our `MBOT-
system have discontiguous target sides. Further-
more, the reported numbers show that the system
also uses rules in which lexical items are com-
bined with nonterminals. Finally, Table 3 presents
the number of rules with k target side components
used during decoding.
5.3 Linguistic Analysis
In this section we present linguistic evidence sup-
porting the fact that the `MBOT-based system sig-
nificantly outperforms the baseline. All exam-
ples are taken from the translation of the test set
used for automatic evaluation. We show that when
our system generates better translations, this is di-
rectly related to the use of `MBOT rules.
Figures 8 and 9 show the ability of our system to
correctly reorder multiple segments in the source
sentence where the baseline translates those seg-
ments sequentially. An analysis of the generated
derivations shows that our system produces the
correct translation by taking advantage of rules
with discontiguous units on target language side.
The rules used in the presented derivations are dis-
played in Figures 10 and 11. In the first example
(Figure 8), we begin by translating ?((smuggle)VB
(eight projectiles)NP (into the kingdom)PP)VP? into
the discontiguous sequence composed of (i) ?(acht
geschosse)NP? ; (ii) ?(in das ko?nigreich)PP? and
(iii) ?(schmuggeln)VP?. In a second step we as-
semble all sequences in a rule with contiguous tar-
get language side and, at the same time, insert the
word ?(zu)PTKZU? between ?(in das ko?nigreich)PP?
and ?(schmuggeln)VP?.
The second example (Figure 9) illustrates a
more complex reordering. First, we trans-
VP
VB NP PP
?
( NP
NP
, PP
PP
, VVINF
VVINF
)
S
TO VP
?
( VP
NP PP PTKZU VVINF
)
Figure 10: Used `MBOT rules for verbal reorder-
ing
VP
ADV commented on NP
?
( NP
NP
, ADV
ADV
, VPP
kommentiert
)
VP
VBZ VP
?
( NP
NP
, VAFIN
VAFIN
, ADV
ADV
, VPP
VPP
)
TOP
NP VP
?
( TOP
NP VAFIN NP ADV VVPP
)
Figure 11: Used `MBOT rules for verbal reorder-
ing
late ?((again)ADV commented on (the problem
of global warming)NP)VP? into the discontigu-
ous sequence composed of (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(wieder)ADV?
and (iii) ?(kommentiert)VPP?. In a second step,
we translate the auxiliary ?(has)VBZ? by in-
serting ?(hat)VAFIN? into the sequence. We
thus obtain, for the input segment ?((has)VBZ
(again)ADV commented on (the problem of global
warming)NP)VP?, the sequence (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(hat)VAFIN?;
(iii) ?(wieder)ADV?; (iv) ?(kommentiert)VVPP?. In
a last step, the constituent ?(president va?clav
klaus)NP? is inserted between the discontiguous
units ?(hat)VAFIN? and ?(wieder)ADV? to form the
contiguous sequence ?((das problem der glob-
alen erwa?rmung)NP (hat)VAFIN (pra?sident va?clav
klaus)NP (wieder)ADV (kommentiert)VVPP)TOP?.
Figures 12 and 13 show examples where our
system generates complex words in the target
language out of a simple source language word.
Again, an analysis of the generated derivation
shows that `MBOT takes advantage of rules hav-
ing several target side components. Examples of
such rules are given in Figure 14. Through its
ability to use these discontiguous rules, our sys-
tem correctly translates into reflexive or particle
verbs such as ?konzentriert sich? (for the English
?focuses?) or ?besteht darauf ? (for the English
?insist?). Another phenomenon well handled by
our system are relative pronouns. Pronouns such
as ?that? or ?whose? are systematically translated
818
. . . geplant hatten 8 geschosse in das ko?nigreich zu schmuggeln
. . . had planned to smuggle 8 projectiles into the kingdom
. . . vorhatten zu schmuggeln 8 geschosse in das ko?nigreich
Figure 8: Verbal Reordering (top: our system, bottom: baseline)
das problem der globalen erwa?rmung hat pra?sident va?clav klaus wieder kommentiert
president va?clav klaus has again commented on the problem of global warming
pra?sident va?clav klaus hat wieder kommentiert das problem der globalen erwa?rmung
Figure 9: Verbal Reordering (top: our system, bottom: baseline)
. . . die serbische delegation bestand darauf , dass jede entscheidung . . .
. . . the serbian delegation insisted that every decision . . .
. . . die serbische delegation bestand , jede entscheidung . . .
Figure 12: Relative Clause (top: our system, bot-
tom: baseline)
. . . die roadmap von bali , konzentriert sich auf die bemu?hungen . . .
. . . the bali roadmap that focuses on efforts . . .
. . . die bali roadmap , konzentriert auf bemu?hungen . . .
Figure 13: Reflexive Pronoun (top: our system,
bottom: baseline)
into both both, ?,? and ?dass? or ?,? and ?deren?
(Figure 12).
6 Conclusion and Future Work
We demonstrated that our `MBOT-based machine
translation system beats a standard tree-to-tree
system (Moses tree-to-tree) on the WMT 2009
translation task English ? German. To achieve
this we implemented the formal model as de-
scribed in Section 2 inside the Moses machine
translation toolkit. Several modifications were
necessary to obtain a working system. We publicly
release all our developed software and our com-
plete tool-chain to allow independent experiments
and evaluation. This includes our `MBOT decoder
IN
that
?
( $,
,
, KOUS
dass
) VBZ
focuses
?
( VVFIN
konzentriert
, PRF
sich
)
Figure 14: `MBOT rules generating a relative
clause/reflexive pronoun
presented in Section 4 and a separate C++ module
that we use for rule extraction (see Section 3).
Besides the automatic evaluation, we also per-
formed a small manual analysis of obtained trans-
lations and show-cased some examples (see Sec-
tion 5.3). We argue that our `MBOT approach can
adequately handle discontiguous phrases, which
occur frequently in German. Other languages that
exhibit such phenomena include Czech, Dutch,
Russian, and Polish. Thus, we hope that our sys-
tem can also successfully be applied for other lan-
guage pairs, which we plan to pursue as well.
In other future work, we want to investigate
full backwards application of `MBOT rules, which
would be more suitable for the converse transla-
tion direction German? English. The current in-
dependent LM scoring of components has some
negative side-effects that we plan to circumvent
with the use of lazy LM scoring.
Acknowledgement
The authors thank Alexander Fraser for his ongo-
ing support and advice. All authors were finan-
cially supported by the German Research Founda-
tion (DFG) grant MA 4959 / 1-1.
819
References
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Workshop on Statistical Machine Trans-
lation, pages 1?28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd ACL, pages 173?180.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computat. Linguist., 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443?
1452.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961?968.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th Int. Workshop Spoken Language Transla-
tion, pages 152?159.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. 7th Conf. Association
for Machine Translation of the Americas, pages 66?
73.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd Int. Workshop Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. 10th Ma-
chine Translation Summit, pages 79?86.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd ACL Workshop on Syntax and
Structure in Statistical Translation, pages 87?95.
Eric Lilin. 1978. Une ge?ne?ralisation des transducteurs
d?e?tats finis d?arbres: les S-transducteurs. The`se
3e`me cycle, Universite? de Lille.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. 44th ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558?566.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876?884.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825?834.
Andreas Maletti. 2012. Every sensible extended top-
down tree transducer is a multi bottom-up tree trans-
ducer. In Proc. HLT-NAACL, pages 263?273.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computat. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. 41st ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. 40th
ACL, pages 311?318.
Jean-Claude Raoult. 1997. Rational tree relations.
Bull. Belg. Math. Soc. Simon Stevin, 4(1):149?176.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. 20th COLING, pages 162?168.
820
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914?922.
Web-as-Corpus Consortium. 2008. SDeWaC ? a 0.88
billion word corpus for german. Website: http:
//wacky.sslmit.unibo.it/doku.php.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computat. Linguist., 23(3):377?403.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. 46th ACL, pages 559?567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study
for translational equivalence modeling and statis-
tical machine translation. In Proc. 22nd Inter-
national Conference on Computational Linguistics,
pages 1097?1104.
821
Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 1?10,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Preservation of Recognizability for
Weighted Linear Extended Top-Down Tree Transducers?
Nina Seemann and Daniel Quernheim and Fabienne Braune and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing
{seemanna,daniel,braunefe,maletti}@ims.uni-stuttgart.de
Abstract
An open question in [FU?LO?P, MALETTI,
VOGLER: Weighted extended tree trans-
ducers. Fundamenta Informaticae 111(2),
2011] asks whether weighted linear ex-
tended tree transducers preserve recogniz-
ability in countably complete commuta-
tive semirings. In this contribution, the
question is answered positively, which is
achieved with a construction that utilizes
inside weights. Due to the completeness
of the semiring, the inside weights always
exist, but the construction is only effective
if they can be effectively determined. It is
demonstrated how to achieve this in a num-
ber of important cases.
1 Introduction
Syntax-based statistical machine translation
(Knight, 2007) created renewed interest in tree
automata and tree transducer theory (Fu?lo?p
and Vogler, 2009). In particular, it sparked
research on extended top-down tree transduc-
ers (Graehl et al, 2009), which are top-down
tree transducers (Rounds, 1970; Thatcher, 1970)
in which the left-hand sides can contain several
(or no) input symbols. A recent contribution
by Fu?lo?p et al (2011) investigates the theoretical
properties of weighted extended tree transduc-
ers over countably complete and commutative
semirings (Hebisch and Weinert, 1998; Golan,
1999). Such semirings permit sums of countably
many summands, which still obey the usual
associativity, commutativity, and distributivity
laws. We will use the same class of semirings.
? All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).
Input? Parser ? TM ? LM ? Output
Figure 1: Syntax-based machine translation pipeline.
Extended top-down tree transducers are used as
translation models (TM) in syntax-based machine
translation. In the standard pipeline (see Figure 1;
LM is short for language model) the translation
model is applied to the parses of the input sen-
tence, which can be represented as a recogniz-
able weighted forest (Fu?lo?p and Vogler, 2009).
In practice, only the best or the n-best parses are
used, but in principle, we can use the recogniz-
able weighted forest of all parses. In either case,
the translation model transforms the input trees
into a weighted forest of translated output trees.
A class of transducers preserves recognizability
if for every transducer of the class and each rec-
ognizable weighted forest, this weighted forest
of translated output trees is again recognizable.
Fu?lo?p et al (2011) investigates which extended
top-down tree transducers preserve recognizabil-
ity under forward (i.e., the setting previously de-
scribed) and backward application (i.e., the set-
ting, in which we start with the output trees and
apply the inverse of the translation model), but the
question remained open for forward application
of weighted linear extended top-down tree trans-
ducers [see Table 1 for an overview of the exist-
ing results for forward application due to Engel-
friet (1975) in the unweighted case and Fu?lo?p et
al. (2010) and Fu?lo?p et al (2011) for the weighted
case]. In conclusion, Fu?lo?p et al (2011) ask: ?Are
there a commutative semiring S that is count-
ably complete wrt.
?
, a linear wxttM [weighted
extended top-down tree transducer with regular
look-ahead; see Section 4], and a recognizable
1
model preserves regularity
unweighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
weighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
Table 1: Overview of the known results due to Engel-
friet (1975) and Fu?lo?p et al (2011) and our results in
boxes.
weighted tree language ? such that M(?) [for-
ward application] is not recognizable? Or even
harder, are there S and M with the same prop-
erties such that M(1?) [1? is the weighted forest
in which each tree has weight 1] is not recogniz-
able??
In this contribution, we thus investigate preser-
vation of recognizability (under forward applica-
tion) for linear extended top-down tree transduc-
ers with regular look-ahead (Engelfriet, 1977),
which are equivalent to linear weighted extended
tree transducers by Fu?lo?p et al (2011). We show
that they always preserve recognizability, thus
confirming the implicit hypothesis of Fu?lo?p et al
(2011). The essential tool for our construction is
the inside weight (Lari and Young, 1990; Graehl
et al, 2008) of the states of the weighted tree
grammar (Alexandrakis and Bozapalidis, 1987)
representing the parses. The inside weight of a
state q is the sum of all weights of trees accepted
in this state. In our main construction (see Sec-
tion 5) we first compose the input weighted tree
grammar with the transducer (input restriction).
This is particularly simple since we just abuse
the look-ahead of the initial rules. In a second
step, we normalize the obtained transducer, which
yields the standard product construction typically
used for input restriction. Finally, we project to
the output by basically eliminating the left-hand
sides. In this step, the inside weights of states
belonging to deleted subtrees are multiplied to
the production weight. Due to the completeness
of the semiring, the inside weights always ex-
ist, but the infinite sums have to be computed ef-
fectively for the final step of the construction to
be effective. This problem is addressed in Sec-
tion 6, where we show several methods to effec-
tively compute or approximate the inside weights
for all states of a weighted tree grammar.
2 Notation
Our weights will be taken from a commuta-
tive semiring (A,+, ?, 0, 1), which is an algebraic
structure of two commutative monoids (A,+, 0)
and (A, ?, 1) such that ? distributes over + and
0 ? a = 0 for all a ? A. An infinitary sum opera-
tion
?
is a family (
?
I)I where I is a countable
index set and
?
I : A
I ? A. Given f : I ? A,
we write
?
i?I f(i) instead of
?
I f . The semi-
ring together with the infinitary sum operation
?
is countably complete (Eilenberg, 1974; Hebisch
and Weinert, 1998; Golan, 1999; Karner, 2004) if
for all countable sets I and ai ? A with i ? I
?
?
i?I ai = am + an if I = {m,n},
?
?
i?I ai =
?
j?J
(?
i?Ij
ai
)
if I =
?
j?J Ij
for countable sets J and Ij with j ? J such
that Ij ? Ij? = ? for all different j, j? ? J ,
and
? a ?
(?
i?I ai
)
=
?
i?I(a ? ai) for all a ? A.
For such a semiring, we let a? =
?
i?N a
i for
every a ? A. In the following, we assume that
(A,+, ?, 0, 1) is a commutative semiring that is
countably complete with respect to
?
.
Our trees have node labels taken from an al-
phabet ? and leaves might also be labeled by el-
ements of a set V . Given a set T , we write ?(T )
for the set
{?(t1, . . . , tk) | k ? N, ? ? ?, t1, . . . , tk ? T} .
The set T?(V ) of ?-trees with V -leaves is defined
as the smallest set T such that V ? ?(T ) ? T .
We write T? for T?(?). For each tree t ? T?(V )
we identify nodes by positions. The root of t has
position ? and the position iw with i ? N and
w ? N? addresses the position w in the i-th di-
rect subtree at the root. The set of all positions
in t is pos(t). We write t(w) for the label (taken
from ? ? V ) of t at position w ? pos(t). Sim-
ilarly, we use t|w to address the subtree of t that
is rooted in position w, and t[u]w to represent the
tree that is obtained from replacing the subtree t|w
at w by u ? T?(V ). For a given set L ? ? ? V
of labels, we let
posL(t) = {w ? pos(t) | t(w) ? L}
2
be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).
We often use the set X = {x1, x2, . . . } of vari-
ables and its finite subsets Xk = {x1, . . . , xk}
for every k ? N to label leaves. Let V
be a set potentially containing some variables
of X . The tree t ? T?(V ) is linear if
|posx(t)| ? 1 for every x ? X . Moreover,
var(t) = {x ? X | posx(t) 6= ?} collects all
variables that occur in t. Given a finite set Q and
T ? T?(V ), we let
Q[T ] = {q(t) | q ? Q, t ? T} .
We will treat elements ofQ[T ] (in which elements
ofQ are always used as unary symbols) as special
trees of T??Q(V ). A substitution ? is a mapping
? : X ? T?(V ). When applied to t ? T?(V ),
it returns the tree t?, which is obtained from t
by replacing all occurrences of x ? X (in par-
allel) by ?(x). This can be defined recursively
by x? = ?(x) for all x ? X , v? = v for all
v ? V \X , and ?(t1, . . . , tk)? = ?(t1?, . . . , tk?)
for all ? ? ? and t1, . . . , tk ? T?(V ).
3 Weighted Tree Grammars
In this section, we will recall weighted tree
grammars (Alexandrakis and Bozapalidis, 1987)
[see (Fu?lo?p and Vogler, 2009) for a modern treat-
ment and a complete historical account]. In gen-
eral, weighted tree grammars (WTGs) offer an ef-
ficient representation of weighted forests, which
are sets of trees such that each individual tree
is equipped with a weight. The representation
is even more efficient than packed forests (Mi et
al., 2008) and moreover can represent an infinite
number of weighted trees. To avoid confusion
between the nonterminals of a parser, which pro-
duces the forests considered here, and our WTGs,
we will refer to the nonterminals of our WTG as
states.
Definition 1. A weighted tree grammar (WTG) is
a system (Q,?, q0, P ) where
? Q is a finite set of states (nonterminals),
? ? is the alphabet of symbols,
? q0 ? Q is the starting state, and
? P is a finite set of productions q
a
? t, where
q ? Q, a ? A, and t ? T?(Q).
Example 2. We illustrate our notation on the
WTG Gex = (Q,?, qs, P ) where
? Q = {qs, qnp, qprp, qn, qadj},
? ? contains ?S?, ?NP?, ?VP?, ?PP?, ?DT?,
?NN?, ?N?, ?VBD?, ?PRP?, ?ADJ?, ?man?,
?hill?, ?telescope?, ?laughs?, ?the?, ?on?,
?with?, ?old?, and ?young?, and
? P contains the productions
qs
1.0
? S(qnp,VP(VBD(laughs))) (?1)
qnp
0.4
? NP(qnp,PP(qprp, qnp))
qnp
0.6
? NP(DT(the), qn) (?2)
qprp
0.5
? PRP(on)
qprp
0.5
? PRP(with)
qn
0.3
? N(qadj , qn)
qn
0.3
? NN(man) (?3)
qn
0.2
? NN(hill)
qn
0.2
? NN(telescope)
qadj
0.5
? ADJ(old)
qadj
0.5
? ADJ(young)
It produces a weighted forest representing sen-
tences about young and old men with telescopes
on hills.
In the following, let G = (Q,?, q0, P ) be a
WTG. For every production ? = q
a
? t in P , we
let wtG(?) = a. The semantics of G is defined
with the help of derivations. Let ? ? T?(Q) be
a sentential form, and let w ? posQ(?) be such
that w is the lexicographically smallest Q-labeled
position in ?. Then ? ??G ?[t]w if ?(w) = q. For
a sequence ?1, . . . , ?n ? P of productions, we
let wtG(?1 ? ? ? ?n) =
?n
i=1 wtG(?i). For every
q ? Q and t ? T?(Q), we let
wtG(q, t) =
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) .
The WTG G computes the weighted forest
LG : T? ? A such that LG(t) = wtG(q0, t) for
every t ? T?. Two WTGs are equivalent if they
compute the same weighted forest. Since produc-
tions of weight 0 are useless, we often omit them.
Example 3. For the WTG Gex of Example 2 we
display a derivation with weight 0.18 for the sen-
tence ?the man laughs? in Figure 2.
The notion of inside weights (Lari and Young,
1990) is well-established, and Maletti and Satta
3
qs ??1G
S
qnp VP
VBD
laughs
??2G
S
NP
DT
the
qn
VP
VBD
laughs
??3G
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 2: Derivation with weight 1.0 ? 0.6 ? 0.3.
(2009) consider them for WTGs. Let us recall the
definition.
Definition 4. The inside weight of state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
In Section 6 we demonstrate how to compute
inside weights. Finally, let us introduce WTGs in
normal form. The WTG G is in normal form if
t ? ?(Q) for all its productions q
a
? t in P . The
following theorem was proven by Alexandrakis
and Bozapalidis (1987) as Proposition 1.2.
Theorem 5. For every WTG there exists an
equivalent WTG in normal form.
Example 6. The WTG Gex of Example 2 is not
normalized. To illustrate the normalization step,
we show the normalization of the production ?2,
which is replaced by the following three produc-
tions:
qnp
0.6
? NP(qdt, qn) qdt
1.0
? DT(qt)
qt
1.0
? the .
4 Weighted linear extended tree
transducers
The model discussed in this contribution is an ex-
tension of the classical top-down tree transducer,
which was introduced by Rounds (1970) and
Thatcher (1970). Here we consider a weighted
and extended variant that additionally has regular
look-ahead. The weighted top-down tree trans-
ducer is discussed in (Fu?lo?p and Vogler, 2009),
and extended top-down tree transducers were
studied in (Arnold and Dauchet, 1982; Knight and
Graehl, 2005; Knight, 2007; Graehl et al, 2008;
Graehl et al, 2009). The combination (weighted
extended top-down tree transducer) was recently
investigated by Fu?lo?p et al (2011), who also con-
sidered (weighted) regular look-ahead, which was
first introduced by Engelfriet (1977) in the un-
weighted setting.
Definition 7. A linear extended top-down
tree transducer with full regular look-ahead
(l-XTOPRf ) is a system (S,?,?, s0, G,R) where
? S is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? s0 ? S is an initial state,
? G = (Q,?, q0, P ) is a WTG, and
? R is a finite set of weighted rules of the form
`
a
?? r where
? a ? A is the rule weight,
? ` ? S[T?(X)] is the linear left-hand
side,
? ? : var(`)? Q is the look-ahead, and
? r ? T?(S[var(`)]) is the linear right-
hand side.
In the following, let M = (S,?,?, s0, G,R)
be an l-XTOPRf . We assume that the WTG G
contains a state > such that wtG(>, t) = 1 for
every t ? T?. In essence, this state represents
the trivial look-ahead. If ?(x) = > for every
rule `
a
?? r ? R and x ? var(r) (respectively,
x ? var(`)), then M is an l-XTOPR (respectively,
l-XTOP). l-XTOPR and l-XTOP coincide exactly
with the models of Fu?lo?p et al (2011), and in the
latter model we drop the look-ahead component ?
and the WTG G completely.
Example 8. The rules of our running example
l-XTOP Mex (over the input and output alpha-
bet ?, which is also used by the WTG Gex of Ex-
ample 2) are displayed in Figure 3.
Next, we present the semantics. Without loss
of generality, we assume that we can distin-
guish states from input and output symbols (i.e.,
S ? (? ? ?) = ?). A sentential form of M is a
tree of SF(M) = T?(Q[T?]). Let ? = `
a
?? r be
a rule of R. Moreover, let ?, ? ? SF(M) be sen-
tential forms and w ? N? be the lexicographically
smallest position in posQ(?). We write ?
b
?M,? ?
if there exists a substitution ? : X ? T? such that
? ? = ?[`?]w,
? ? = ?[r?]w, and
? b = a ?
?
x?var(`) wtG(?(x), ?(x)).
4
s0
S
NP
x1 x2
VP
x3
? 0.6
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
? 0.4
S
s1
x1
VP
s3
x3
s2
N
ADJ
x1
x2
? 0.7
N
ADJ
s5
x1
s2
x2
?
?
? 0.3
s2
x2
s1
NP
x1 x2
? 0.5
NP
s1
x1
s2
x2
?
?
? 0.5
s1
x1
s1
DT
the
? 1.0
DT
the
s3
VBD
laughs
? 1.0
VBD
laughs
s2
PP
x1 x2
? 1.0
PP
s4
x1
s1
x2
s2
NN
man /
hill /
telescope
? 1.0
NN
man /
hill /
telescope
s4
PRP
on /
with
? 1.0
PRP
on /
with
Figure 3: Example rules of an l-XTOP. We collapsed rules with the same left-hand side as well as several lexical
items to save space.
s0
S
NP
NP
DT
the
NN
man
PP
PRP
on
NP
DT
the
NN
hill
VP
VBD
laughs
0.4?M
S
s1
NP
DT
the
NN
man
VP
s3
VBD
laughs
0.5?M
S
NP
s1
DT
the
s2
NN
man
VP
s3
VBD
laughs
??M
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 4: Derivation with weight 0.4 ? 0.5 ? 1.0 (rules omitted).
The tree transformation ?M computed byM is de-
fined by
?M (t, u) =
?
?1,...,?n?R
s0(t)
a1?M,?1 ???
an?M,?nu
a1 ? . . . ? an
for every t ? T? and u ? T?.
Example 9. A sequence of derivation steps of the
l-XTOP Mex is illustrated in Figure 4. The trans-
formation it computes is capable of deleting the
PP child of every NP-node with probability 0.4 as
well as deleting the ADJ child of every N-node
with probability 0.3.
A detailed exposition to unweighted l-XTOPR
is presented by Arnold and Dauchet (1982) and
Graehl et al (2009).
5 The construction
In this section, we present the main construction
of this contribution, in which we will construct a
WTG for the forward application of another WTG
via an l-XTOPR. Let us first introduce the main
notions. Let L : T? ? A be a weighted forest
and ? : T??T? ? A be a weighted tree transfor-
mation. Then the forward application of L via ?
yields the weighted forest ?(L) : T? ? A such
that (?(L))(u) =
?
t?T?
L(t) ? ?(t, u) for ev-
ery u ? T?. In other words, to compute the
weight of u in ?(L), we consider all input trees t
and multiply their weight in L with their trans-
lation weight to u. The sum of all those prod-
ucts yields the weight for u in ?(L). In the par-
ticular setting considered in this contribution, the
weighted forest L is computed by a WTG and the
weighted tree transformation ? is computed by an
l-XTOPR. The question is whether the resulting
weighted forest ?(L) can be computed by a WTG.
Our approach to answer this question con-
sists of three steps: (i) composition, (ii) nor-
malization, and (iii) range projection, which
we address in separate sections. Our input is
5
qs ?
S
qnp qvp
?
S
NP
qnp qpp
qvp ?2
S
NP
qnp qpp
VP
VBD
qv
qs ?
S
qnp qvp
?
S
NP
qdt qn
qvp ?2
S
NP
qdt qn
VP
VBD
qv
Figure 5: Two derivations (without production and
grammar decoration) with weight 0.4 [top] and
0.6 [bottom] of the normalized version of the
WTG Gex (see Example 10).
the WTG G? = (Q?,?, q?0, P
?), which com-
putes the weighted forest L = LG? , and
the l-XTOPR M = (S,?,?, s0, G,R) with
G = (Q,?, q0, P ), which computes the weighted
tree transformation ? = ?M . Without loss of gen-
erality, we suppose thatG andG? contain a special
state > such that wtG(>, t) = wtG?(>, t) = 1
for all t ? T?. Moreover, we assume that the
WTG G? is in normal form. Finally, we assume
that s0 is separated, which means that the initial
state of M does not occur in any right-hand side.
Our example l-XTOP Mex has this property. All
these restrictions can be assumed without loss of
generality. Finally, for every state s ? S, we let
Rs = {`
a
?? r ? R | `(?) = s} .
5.1 Composition
We combine the WTG G? and the l-XTOPR M
into a single l-XTOPRf M
? that computes
?M ?(t, u) = LG?(t) ? ?M (t, u) = L(t) ? ?(t, u)
for every t ? T? and u ? T?. To this end, we
construct
M ? = (S,?,?, s0, G?G
?, (R \Rs0) ?R
?)
such that G ? G? is the classical product WTG
[see Proposition 5.1 of (Berstel and Reutenauer,
1982)] and for every rule `
a
?? r in Rs0 and
? : var(`)? Q?, the rule
`
a?wtG? (q
?
0,`?)??????????? r
is in R?, where ??(x) = ??(x), ?(x)? for every
x ? var(`).
Example 10. Let us illustrate the construction on
the WTG Gex of Example 2 and the l-XTOP Mex
of Example 8. According to our assumptions,
Gex should first be normalized (see Theorem 5).
We have two rules in Rs0 and they have the same
left-hand side `. It can be determined easily that
wtG?ex(qs, `?) 6= 0 only if
? ?(x1)?(x2)?(x3) = qnpqppqv or
? ?(x1)?(x2)?(x3) = qdtqnqv.
Figure 5 shows the two corresponding derivations
and their weights. Thus, the s0-rules are replaced
by the 4 rules displayed in Figure 6.
Theorem 11. For every t ? T? and u ? T?, we
have ?M ?(t, u) = L(t) ? ?(t, u).
Proof. We prove an intermediate property for
each derivation of M . Let
s0(t)
b1?M,?1 ? ? ?
bn?M,?n u
be a derivation of M . Let ?1 = `
a1?? r be the
first rule, which trivially must be in Rs0 . Then for
every ? : var(`)? Q?, there exists a derivation
s0(t)
c1?M ?,??1 ?2
b2?M ?,?2 ? ? ?
bn?M ?,?n u
in M ? such that
c1 = b1?wtG?(q
?
0, `?)?
?
x?var(`)
wtG?(?(x), ?
?(x)) ,
where ?? : var(`) ? T? is such that t = `??.
Since we sum over all such derivations and
?
? : var(`)?Q?
wtG?(q
?
0, `?) ?
?
x?var(`)
wtG?(?(x), ?
?(x))
= wtG?(q
?
0, t) = LG?(t)
by a straightforward extension of Lemma 4.1.8
of (Borchardt, 2005), we obtain that the deriva-
tions in M ? sum to LG?(t) ? b1 ? . . . ? bn as desired.
The main property follows trivially from the in-
termediate result.
5.2 Normalization
Currently, the weights of the input WTG are
only on the initial rules and in its look-ahead.
Next, we use essentially the same method as
in the previous section to remove the look-
ahead from all variables that are not deleted.
Let M ? = (S,?,?, s0, G ? G?, R) be the
l-XTOPRf constructed in the previous section and
6
s0
S
NP
x1 x2
VP
x3
??
0.6 ? c
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
?
0.4 ? c
S
s1
x1
VP
s3
x3
Figure 6: 4 new l-XTOPRf rules, where ? and c are
either (i) ?(x1)?(x2)?(x3) = qnpqppqv and c = 0.4
or (ii) ?(x1)?(x2)?(x3) = qdtqnqv and c = 0.6 (see
Example 10).
s0
S
NP
x1 x2
VP
x3
??
0.4 ? 0.4
S
?s1, qnp?
x1
VP
?s3, qv?
x3
?
?
?
0.4 ? 0.6
S
?s1, qdt?
x1
VP
?s3, qv?
x3
Figure 7: New l-XTOPR rules, where ?(x2) = qpp
[left] and ?(x2) = qn [right] (see Figure 6).
? = `
a
?? r ? R be a rule with ?(x) = ?>, q??
for some q? ? Q? \ {>} and x ? var(r). Note
that ?(x) = ?>, q?? for some q? ? Q? for all
x ? var(r) since M is an l-XTOPR. Then we
construct the l-XTOPRf M
??
(S ? S ?Q?,?,?, s0, G?G
?, (R \ {?}) ?R?)
such that R? contains the rule `
a
??? r?, where
??(x?) =
{
?>,>? if x = x?
?(x?) otherwise
for all x? ? var(`) and r? is obtained from r by re-
placing the subtree s(x) with s ? S by ?s, q??(x).
Additionally, for every rule `??
a??
???? r?? in Rs and
? : var(`??)? Q?, the rule
`??
a???wtG? (q
?,`???)
?????????????? r
??
is in R?, where ????(x) = ????(x), ?(x)? for ev-
ery x ? var(`). This procedure is iterated until
we obtain an l-XTOPR M ??. Clearly, the iteration
must terminate since we do not change the rule
shape, which yields that the size of the potential
rule set is bounded.
Theorem 12. The l-XTOPR M ?? and the
l-XTOPRf M
? are equivalent.
?s2, qn?
N
ADJ
x1
x2
??
0.32 ? 0.5
?s2, qn?
x2
?s1, qnp?
NP
x1 x2
???|???
0.5 ? 0.4
?s1, qnp?
x1
?
?
?
0.5 ? 0.6
?s1, qdt?
x1
Figure 8: New l-XTOPR rules, where ?(x1) is either
qold or qyoung , ??(x2) = qpp, and ???(x2) = qn.
Proof. It can be proved that the l-XTOPRf con-
structed after each iteration is equivalent to its
input l-XTOPRf in the same fashion as in Theo-
rem 11 with the only difference that the rule re-
placement now occurs anywhere in the derivation
(not necessarily at the beginning) and potentially
several times. Consequently, the finally obtained
l-XTOPR M ?? is equivalent to M ?.
Example 13. Let us reconsider the l-XTOPRf con-
structed in the previous section and apply the nor-
malization step. The interesting rules (i.e., those
rules l
a
?? r where var(r) 6= var(l)) are dis-
played in Figures 7 and 8.
5.3 Range projection
We now have an l-XTOPR M ?? with rules R??
computing ?M ??(t, u) = L(t) ? ?(t, u). In the fi-
nal step, we simply disregard the input and project
to the output. Formally, we want to construct a
WTG G?? such that
LG??(u) =
?
t?T?
?M ??(t, u) =
?
t?T?
L(t) ? ?(t, u)
for every u ? T?. Let us suppose that G is the
WTG inside M ??. Recall that the inside weight of
state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
We construct the WTG
G?? = (S ? S ?Q?,?, s0, P
??)
such that `(?)
c
? r? is in P ?? for every rule
`
a
?? r ? R??, where
c = a ?
?
x?var(`)\var(r)
inG(?(x))
and r? is obtained from r by removing the vari-
ables of X . If the same production is constructed
from several rules, then we add the weights. Note
that the WTG G?? can be effectively computed if
inG(q) is computable for every state q.
7
qs qprp
qnp qn qadj
Figure 9: Dependency graph of the WTG Gex.
Theorem 14. For every u ? T?, we have
LG??(u) =
?
t?T?
L(t) ? ?(t, u) = (?(L))(u) .
Example 15. The WTG productions for the rules
of Figures 7 and 8 are
s0
0.4?0.4
? S(?s1, qnp?,VP(?s3, qv?))
s0
0.4?0.6
? S(?s1, qdt?,VP(?s3, qv?))
?s2, qn?
0.3?0.3
? ?s2, qn?
?s1, qnp?
0.5?0.4
? ?s1, qnp?
?s1, qnp?
0.5?0.6
? ?s1, qdt? .
Note that all inside weights are 1 in our exam-
ple. The first production uses the inside weight
of qpp, whereas the second production uses the in-
side weight of qn. Note that the third production
can be constructed twice.
6 Computation of inside weights
In this section, we address how to effectively com-
pute the inside weight for every state. If the WTG
G = (Q,?, q0, P ) permits only finitely many
derivations, then for every q ? Q, the inside
weight inG(q) can be computed according to Def-
inition 4 because wtG(q, t) = 0 for almost all
t ? T?. If P contains (useful) recursive rules,
then this approach does not work anymore. Our
WTG Gex of Example 2 has the following two re-
cursive rules:
qnp
0.4
? NP(qnp,PP(qprp, qnp)) (?4)
qn
0.3
? N(qadj , qn) . (?5)
The dependency graph of Gex, which is shown in
Figure 9, has cycles, which yields that Gex per-
mits infinitely many derivations. Due to the com-
pleteness of the semiring, even the infinite sum of
Definition 4 is well-defined, but we still have to
compute it. We will present two simple methods
to achieve this: (a) an analytic method and (b) an
approximation in the next sections.
6.1 Analytic computation
In simple cases we can compute the inside weight
using the stars a?, which we defined in Section 2.
Let us first list some interesting countably com-
plete semirings for NLP applications and their
corresponding stars.
? Probabilities: (R??0,+, ?, 0, 1) where R
?
?0
contains all nonnegative real numbers
and ?, which is bigger than every real
number. For every a ? R??0 we have
a? =
{
1
1?a if 0 ? a < 1
? otherwise
? VITERBI: ([0, 1],max, ?, 0, 1) where [0, 1] is
the (inclusive) interval of real numbers be-
tween 0 and 1. For every 0 ? a ? 1 we have
a? = 1.
? Tropical: (R??0,min,+,?, 0) where
a? = 0 for every a ? R??0.
? Tree unification: (2T?(X1),?,unionsq, ?, {x1})
where 2T?(X1) = {L | L ? T?(X1)} and
unionsq is unification (where different occurrences
of x1 can be replaced differently) extended
to sets as usual. For every L ? T?(Xk) we
have L? = {x1} ? (L unionsq L).
We can always try to develop a regular expres-
sion (Fu?lo?p and Vogler, 2009) for the weighted
forest recognized by a certain state, in which we
then can drop the actual trees and only compute
with the weights. This is particularly easy if our
WTG has only left- or right-recursive productions
because in this case we obtain classical regular
expressions (for strings). Let us consider produc-
tion ?5. It is right-recursive. On the string level,
we obtain the following unweighted regular ex-
pression for the string language generated by qn:
L(qadj)
?(man | hill | telescope)
where L(qadj) = {old, young} is the set of strings
generated by qadj . Correspondingly, we can de-
rive the inside weight by replacing the generated
string with the weights used to derive them. For
example, the production ?5, which generates the
state qadj , has weight 0.3. We obtain the expres-
sion
inG(qn) = (0.3 ? inG(qadj))
? ? (0.3 + 0.2 + 0.2) .
8
Example 16. If we calculate in the probability
semiring and inG(qadj) = 1, then
inG(qn) =
1
1? 0.3
? (0.3 + 0.2 + 0.2) = 1 ,
as expected (since our productions induce a prob-
ability distribution on all trees generated from
each state).
Example 17. If we calculate in the tropical semi-
ring, then we obtain
inG(qn) = min(0.3, 0.2, 0.2) = 0.2 .
It should be stressed that this method only
allows us to compute inG(q) in very simple
cases (e.g., WTG containing only left- or right-
recursive productions). The production ?4 has
a more complicated recursion, so this simple
method cannot be used for our full example WTG.
However, for extremal semirings the inside
weight always coincides with a particular deriva-
tion. Let us also recall this result. The semiring is
extremal if a+ a? ? {a, a?} for all a, a? ? A. The
VITERBI and the tropical semiring are extremal.
Recall that
inG(q) =
?
t?T?
wtG(q, t)
=
?
t?T?
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) ,
which yields that inG(q) coincides with the
derivation weight wtG(?1 ? ? ? ?n) of some deriva-
tion q ??1G ? ? ? ?
?n
G t for some t ? T?. In
the VITERBI semiring this is the highest scor-
ing derivation and in the tropical semiring it is
the lowest scoring derivation (mind that in the
VITERBI semiring the production weights are
multiplied in a derivation, whereas they are added
in the tropical semiring). There are efficient algo-
rithms (Viterbi, 1967) that compute those deriva-
tions and their weights.
6.2 Numerical Approximation
Next, we show how to obtain a numerical ap-
proximation of the inside weights (up to any
desired precision) in the probability semiring,
which is the most important of all semirings
discussed here. A similar approach was used
by Stolcke (1995) for context-free grammars. To
keep the presentation simple, let us suppose that
G = (Q,?, q0, P ) is in normal form (see The-
orem 5). The method works just as well in the
general case.
We first observe an important property of the
inside weights. For every state q ? Q
inG(q) =
?
q
a
??(q1,...,qn)?P
a ? inG(q1) ? . . . ? inG(qn) ,
which can trivially be understood as a system of
equations (where each inG(q) with q ? Q is a
variable). Since there is one such equation for
each variable inG(q) with q ? Q, we have a
system of |Q| non-linear polynomial equations in
|Q| variables.
Several methods to solve non-linear systems of
equations are known in the numerical calculus lit-
erature. For example, the NEWTON-RAPHSON
method allows us to iteratively compute the roots
of any differentiable real-valued function, which
can be used to solve our system of equations be-
cause we can compute the JACOBI matrix for our
system of equations easily. Given a good starting
point, the NEWTON-RAPHSON method assures
quadratic convergence to a root. A good start-
ing point can be obtained, for example, by bisec-
tion (Corliss, 1977). Another popular root-finding
approximation is described by Brent (1973).
Example 18. For the WTG of Example 2 we ob-
tain the following system of equations:
inG(qs) = 1.0 ? inG(qnp)
inG(qnp) = 0.4 ? inG(qnp) ? inG(qprp) ? inG(qnp)
+ 0.6 ? inG(qn)
inG(qn) = 0.3 ? inG(qadj) ? inG(qn)
+ 0.3 + 0.2 + 0.2
inG(qadj) = 0.5 + 0.5
inG(qprp) = 0.5 + 0.5 .
Together with inG(qn) = 1, which we already
calculated in Example 16, the only interesting
value is
inG(qs) = inG(qnp) = 0.4 ? inG(qnp)
2 + 0.6 ,
which yields the roots inG(qnp) = 1 and
inG(qnp) = 1.5. The former is the desired solu-
tion. As before, this is the expected solution.
9
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Jean Berstel and Christophe Reutenauer. 1982. Rec-
ognizable formal power series on trees. Theoret.
Comput. Sci., 18(2):115?148.
Bjo?rn Borchardt. 2005. The Theory of Recognizable
Tree Series. Ph.D. thesis, Technische Universita?t
Dresden.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Series in Automatic Computa-
tion. Prentice Hall, Englewood Cliffs, NJ, USA.
George Corliss. 1977. Which root does the bisection
algorithm find? SIAM Review, 19(2):325?327.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines ? Volume A, volume 59 of Pure and Ap-
plied Math. Academic Press.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations ? a comparison. Math. Systems
Theory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289?303.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs
on Theoret. Comput. Sci., chapter 9, pages 313?
403. Springer.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2010. Preservation of recognizability for syn-
chronous tree substitution grammars. In Proc. 1st
Workshop Applications of Tree Automata in Natu-
ral Language Processing, pages 1?9. Association
for Computational Linguistics.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2011. Weighted extended tree transducers. Fun-
dam. Inform., 111(2):163?202.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended
top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Georg Karner. 2004. Continuous monoids and semi-
rings. Theoret. Comput. Sci., 318(3):355?372.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. 6th Int. Conf. Com-
putational Linguistics and Intelligent Text Process-
ing, volume 3406 of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4(1):35?56.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Workshop Parsing Technologies, pages 1?12. Asso-
ciation for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting of
the ACL, pages 192?199. Association for Computa-
tional Linguistics.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Comput. Linguist., 21(2):165?201.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymptotically optimum de-
coding algorithm. IEEE Trans. Inform. Theory,
13(2):260?269.
10
