IBM MASTOR SYSTEM: Multilingual Automatic Speech-to-speech Translator * 
 
Yuqing Gao, Liang Gu, Bowen Zhou, Ruhi Sarikaya, Mohamed Afify, Hong-Kwang Kuo, 
Wei-zhong Zhu, Yonggang Deng, Charles Prosser, Wei Zhang and Laurent Besacier 
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 
 
ABSTRACT 
In this paper, we describe the IBM MASTOR, a speech-to-speech 
translation system that can translate spontaneous free-form 
speech in real-time on both laptop and hand-held PDAs. Chal-
lenges include speech recognition and machine translation in 
adverse environments, lack of training data and linguistic re-
sources for under-studied languages, and the need to rapidly de-
velop capabilities for new languages. Another challenge is de-
signing algorithms and building models in a scalable manner to 
perform well even on memory and CPU deficient hand-held com-
puters. We describe our approaches, experience, and success in 
building working free-form S2S systems that can handle two 
language pairs (including a low-resource language). 
 
1. INTRODUCTION 
Automatic speech-to-speech (S2S) translation breaks down com-
munication barriers between people who do not share a common 
language and hence enable instant oral cross-lingual communica-
tion for many critical applications such as emergency medical 
care. The development of an accurate, efficient and robust S2S 
translation system poses a lot of challenges. This is especially 
true for colloquial speech and resource deficient languages. 
The IBM MASTOR speech-to-speech translation system has been 
developed for the DARPA CAST and Transtac programs whose 
mission is to develop technologies that enable rapid deployment 
of real-time S2S translation of low-resource languages on port-
able devices. It originated from the IBM MARS S2S system 
handling the air travel reservation domain described in [1], which 
was later significantly improved in all components, including 
ASR, MT and TTS, and later evolved into the MASTOR multi-
lingual S2S system that covers much broader domains such as 
medical treatment and force protection [2,3]. More recently, we 
have further broadened our experience and efforts to very rapidly 
develop systems for under-studied languages, such as regional 
dialects of Arabic. The intent of this program is to provide lan-
guage support to military, medical and humanitarian personnel 
during operations in foreign territories, by deciphering possibly 
critical language communications with a two-way real-time 
speech-to-speech translation system designed for specific tasks 
such as medical triage and force protection.  
The initial data collection effort for the project has shown that the 
domain of force protection and medical triage is, though limited, 
rather broad. In fact, the definition of domain coverage is tough 
when the speech from responding foreign language speakers are 
concerned, as their responses are less constrained and may in-
clude out-of-domain words and concepts. Moreover, flexible 
casual or colloquial speaking style inevitably appears in the hu-
man-to-human conversational communications. Therefore, the 
project is a great challenge that calls for major research efforts. 
Among all the challenges for speech recognition and translation 
for under-studied languages, there are two main issues: 1) Lack of 
appropriate amount of speech data that represent the domain of 
interest and the oral language spoken by the target speakers, re-
sulting in difficulties in accurate estimation of statistical models 
for speech recognition and translation. 2) Lack of linguistic 
knowledge realization in spelling standards, transcriptions, lexi-
cons and dictionaries, or annotated corpora. Therefore, various 
different approaches have to be explored.  
Another critical challenge is to embed complicated algorithms 
and programs into small devices for mobile users. A hand-held 
computing device may have a CPU of 256MHz and 64MB mem-
ory; to fit the programs, as well as the models and data files into 
this memory and operate the system in real-time are tremendous 
challenges [4]. 
In this paper, we will describe the overall framework of the 
MASTOR system and our approaches for each major component, 
i.e., speech recognition and translation. Various statistical ap-
proaches [5,6,7,8] are explored and used to solve different techni-
cal challenges. We will show how we addressed the challenges 
that arise when building automatic speech recognition (ASR) and 
machine translation (MT) for colloquial Arabic on both the laptop 
and handheld PDA platforms. 
 
2. SYSTEM OVERVIEW 
The general framework of our speech translation system is illus-
trated in Figure 1. The general framework of our MASTOR sys-
tem has components of ASR, MT and TTS. The cascaded ap-
proach allows us to deploy the power of the existing advanced 
speech and language processing techniques, while concentrating 
on the unique problems in speech-to-speech translation. Figure 2 
illustrates the MASTOR GUI (Graphic User Interface) on laptop 
and PDA, respectively. 
Acoustic models for English and Mandarin baseline are devel-
oped for large-vocabulary continuous speech and trained on over 
200 hours of speech collected from about 2000 speakers for each 
language. However, the Arabic dialect speech recognizer was 
only trained using about 50 hours of dialectal speech.  The train-
ing data for Arabic consists of about 200K short utterances. Large 
efforts were invested in initial cleaning and normalization of the 
training data because of large number of irregular dialectal words 
and variations in spellings. We experimented with three ap-
proaches for pronunciation and acoustic modeling: i.e. grapheme, 
phonetic, and context-sensitive grapheme as will be described in 
ASR TTS 
Statistical NLU/NLG 
based MT 
Figure 1 IBM MASTOR Speech-to-Speech Translation System 
Statistical MT using 
WFST/SIPL  
* Thanks to DARPA for funding 
section 3.A. We found that using context-sensitive pronunciation 
rules reduces the WER of the grapheme based acoustic model by 
about 3% (from 36.7% to 35.8%). Based on these results, we 
decided to use context-sensitive grapheme models in our system.  
The Arabic language model (LM) is an interpolated model con-
sisting of a trigram LM, a class-based LM and a morphologically 
processed LM, all trained from a corpus of a few hundred thou-
sand words. We also built a compact language model for the 
hand-held system, where singletons are eliminated and bigram 
and trigram counts are pruned with increased thresholds. The LM 
footprint size is 10MB. 
There are two approaches for translation. The concept based ap-
proach uses natural language understanding (NLU) and natural 
language generation models trained from an annotated corpus. 
Another approach is the phrase-based finite state transducer 
which is trained using an un-annotated parallel corpus. 
A trainable, phrase-splicing and variable substitution TTS system 
is adopted to synthesize speech from translated sentences, which 
has a special ability to generate speech of mixed languages seam-
lessly [9]. In addition, a small footprint TTS is developed for the 
handheld devices using embedded concatenative TTS technolo-
gies.[10] 
Next, we will describe our approaches in automatic speech recog-
nition and machine translation in greater detail. 
 
3. AUTOMATIC SPEECH RECOGNITION 
A. Acoustic Models 
Acoustic models and the pronunciation dictionary greatly influ-
ence the ASR performance. In particular, creating an accurate 
pronunciation dictionary poses a major challenge when changing 
the language. Deriving pronunciations for resource rich languages 
like English or Mandarin is relatively straight forward using ex-
isting dictionaries or letter to sound models. In certain languages 
such as Arabic and Hebrew, the written form does not typically 
contain short vowels which a native speaker can infer from con-
text. Deriving automatic phonetic transcription for speech corpora 
is thus difficult. This problem is even more apparent when con-
sidering colloquial Arabic, mainly due to the large number of 
irregular dialectal words. 
One approach to overcome the absence of short vowels is to use 
grapheme based acoustic models. This leads to straightforward 
construction of pronunciation lexicons and hence facilitates 
model training and decoding. However, the same grapheme may 
lead to different phonetic sounds depending on its context. This 
results in less accurate acoustic models. For this reason we ex-
perimented with two other different approaches. The first is a full 
phonetic approach which uses short vowels, and the second uses 
context-sensitive graphemes for the letter "A" (Alif) where two 
different phonemes are used for "A" depending on its position in 
the word. 
Using phoneme based pronunciations would require vowelization 
of every word. To perform vowelization, we used a mix of dic-
tionary search and a statistical approach. The word is first 
searched in an existing vowelized dictionary, and if not found it is 
passed to the statistical vowelizer [11].  Due to the difficulties in 
accurately vowelizing dialectal words, our experiments have not 
shown any improvements using phoneme based ASR compared 
to grapheme based.  
Speech recognition for both the laptop and hand-held systems is 
based on the IBM ViaVoice engine. This highly robust and effi-
cient framework uses rank based acoustic scores [12] which are 
derived from tree-clustered context dependent Gaussian models. 
These acoustic scores together with n-gram LM probabilities are 
incorporated into a stack based search algorithm to yield the most 
probable word sequence given the input speech. 
The English acoustic models use an alphabet of 52 phones. Each 
phone is modeled with a 3-state left-to-right hidden Markov 
model (HMM). The system has approximately 3,500 context-
dependent states modeled using 42K Gaussian distributions and 
trained using 40 dimensional features. The context-dependent 
states are generated using a decision-tree classifier. The collo-
quial Arabic acoustic models use about 30 phones that essentially 
correspond to graphemes in the Arabic alphabet. The colloquial 
Arabic HMM structure is the same as that of the English model. 
The Arabic acoustic models are also built using 40 dimensional 
features. The compact model for the PDA has about 2K leaves 
and 28K Gaussian distributions.  The laptop version has over 3K 
leaves and 60K Gaussians. All acoustic models are trained using 
discriminative training [13]. 
B. Language Modeling   
Language modeling (LM) of the probability of various word se-
quences is crucial for high-performance ASR of free-style open-
 
   
 
 
Figure 2  IBM MASTOR system in Windows XP and Win-
dows CE 
ended coversational systems. Our approaches to build statistical 
tri-gram LMs fall into three categories: 1) obtaining additional 
training material automatically; 2) interpolating domain-specific 
LMs with other LMs; 3) improving distribution estimation ro-
bustness and accuracy with limited in-domain resources. Auto-
matic data collection and expansion is the most straight-forward 
way to achieve efficient LM, especially when little in-domain 
data is available. For resource-rich languages such as English and 
Chinese, we retrieve additional data from the World Wide Web 
(WWW) to enhance our limited domain specific data, which 
shows significant improvement [6]. 
In Arabic, words can take prefixes and suffixes to generate new 
words which are semantically related to the root form of the word 
(stem). As a result, the vocabulary size in Arabic can become 
very large even for specific domains. To alleviate this problem, 
we built a language model on morphologically tokenized data by 
applying morphological analysis and hence splitting some of the 
words into prefix+stem+suffix, prefix+stem or stem+suffix forms. 
We refer the reader to [14] to learn more about the morphological 
tokenization algorithm. Morphological analysis reduced the vo-
cabulary size by about 30% without sacrificing the coverage. 
More specifically, in our MASTOR system, the English language 
model has two components that are linearly interpolated. The first 
one is built using in-domain data. The second component acts as a 
background model and is built using a very large generic text 
inventory that is domain independent. The language model counts 
are also pruned to control the size of this background model. The 
colloquial Arabic language model for our laptop system is com-
posed of three components that are linearly interpolated. The first 
one is the basic word tri-gram model. The second one is a class 
based language model with 13 classes that covers names for Eng-
lish and Arabic, numbers, months, days, etc. The third one is the 
morphological language model described above. 
4. SPEECH TRANSLATION 
A. NLU/NLG-based Speech Translation 
One of the translation algorithms we proposed and applied in 
MASTOR is the statistical translation method based on natural 
language understanding (NLU) and natural language generation 
(NLG). Statistical machine translation methods translate a sen-
tence W in the source language into a sentence A in the target 
language by using a statistical model that estimates the probabil-
ity of A given W, i.e. ( )WAp . Conventionally, ( )WAp  is opti-
mized on a set of pairs of sentences that are translations of one 
another. To alleviate this data sparseness problem and, hence, 
enhance both the accuracy and robustness of estimating ( )WAp , 
we proposed a statistical concept-based machine translation para-
digm that predicts A with not only W but also the underlying con-
cepts embedded in W and/or A. As a result, the optimal sentence 
A is picked by first understanding the meaning of the source sen-
tence W.  
Let C denote the concepts in the source language and S denote the 
concepts in the target language, our proposed statistical concept-
based algorithm should select a word sequence A? as 
( ) ( ) ( ) ( )
??
?
??
?
== ? WCpWCSpWCSApWApA
CSAA
,,,maxargmaxarg?
,
 , 
where the conditional probabilities ( )WCp , ( )WCSp ,  and 
( )WCSAp ,,  are estimated by the Natural Language Understand-
ing (NLU), Natural Concept Generation (NCG) and Natural 
Word Generation (NWG) procedures, respectively. The probabil-
ity distributions are estimated and optimized upon a pre-annotated 
bilingual corpus. In our MASTOR system, ( )WCp  is estimated 
by a decision-tree based statistical semantic parser, and 
( )WCSp ,  and ( )WCSAp ,,  are estimated by maximizing the 
conditional entropy as depicted in [2] and [7], respectively. 
We are currently developing a new translation method that unifies 
statistical phrase-based translation models and the above 
NLU/NLG based approach. We will discuss this work in future 
publications. 
 
B. Fast and Memory Efficient Machine Translation Using SIPL 
Another translation method we proposed in MASTOR is based on 
the Weighted Finite-State Transducer (WFST). In particular, we 
developed a novel phrase-based translation framework using 
WFSTs that achieves both memory efficiency and fast speed, 
which is suitable for real time speech-to-speech translation on 
scalable computational platforms. In the proposed framework [15] 
which we refer to as Statistical Integrated Phrase Lattices (SIPLs), 
we statically construct a single optimized WFST encoding the 
entire translation model. In addition, we introduce a Viterbi de-
coder that can combine the translation model and language model 
FSTs with the input lattice efficiently, resulting in translation 
speeds of up to thousands of words per second on a PC and hun-
dred words per second on a PDA device. This WFST-based ap-
proach is well-suited to devices with limited computation and 
memory. We achieve this efficiency by using methods that allow 
us to perform more composition and graph optimization offline 
(such as, the determinization of the phrase segmentation trans-
ducer P) than in previous work, and by utilizing a specialized 
decoder involving multilayer search.  
During the offline training, we separate the entire translation lat-
tice H into two pieces: the language model L and the translation 
model M: 
( )( )( )WTPDetMinMinM =  
where   is the composition operator, Min  denotes the 
minimization operation, and Det  denotes the determinization 
operation; T is the phrase translation transducer, and W is the 
phrase-to-word transducer. Due to the determinizability of P, M 
can be computed offline using a moderate amount of memory. 
The translation problem can be framed as finding the best path in 
the full search lattice given an input sentence/automaton I. To 
address the problem of efficiently computing LMI  , we have 
developed a multilayer search algorithm. 
Specifically, we have one layer for each of the input FSM's: I, L, 
and M. At each layer, the search process is performed via a state 
traversal procedure starting from the start state 0s

, and consum-
ing an input word in each step in a left-to-right manner.  
We represent each state s in the search space using the following 
7-tuple: Is , Ms , Ls , Mc , Lc , h
 
, prevs , where Is , Ms , and 
Ls record the current state in each input FSM; Mc and Lc  record 
the accumulated cost in L and M in the best path up to this point; 
h
 
 records the target word sequence labeling the best path up to 
this point; and prevs  records the best previous state. 
To reduce the search space, two active search states are merged 
whenever they have identical Is , Ms , and Ls values; the re-
maining state components are inherited from the state with lower 
cost.  In addition, two pruning methods, histogram pruning and 
threshold or beam pruning, are used to achieve the desired bal-
ance between translation accuracy and speed. 
To provide the decoder for the PDA devices as well that lacks a 
floating-point processor, the search algorithm is implemented 
using fixed-point arithmetic. 
 
 
5. CONCLUSION 
We described the framework of the IBM MASTOR system, the 
various technologies used in building major components for lan-
guages with different levels of data resources. The technologies 
have shown successes in building real-time S2S systems on both 
laptop and small computation resource platforms for two lan-
guage pairs, English-Mandarin Chinese, and English-Arabic dia-
lect. In the latter case, we also developed approaches which lead 
to very rapid (in the matter of 3-4 months) development of sys-
tems using very limited language and domain resources. We are 
working on improving spontaneous speech recognition accuracy 
and more naturally integrating two translation approaches.  
 
6. ACKNOWLEDGEMENT 
The authors sincerely thank Drs. Yoshinori Tahara, Fu-hua Liu, 
Yongxing Li, Etienne Marcheret, Raimo Bakis, Ellen Eide, Burn 
Lewis, Tony Lee, Ossama Emam, and Lubos Ures for their help 
and contributions to the MASTOR S2S system. 
 
7. REFERENCES 
[1] Y. Gao et al ?MARS: A Statistical Semantic Parsing and Generation 
Based Multilingual Automatic tRanslation System,? Machine Trans-
lation, vol. 17, pp.185-212, 2004. 
[2] L. Gu et al ?Improving Statistical Natural Concept Generation in 
Interlingua-based Speech-to-Speech Translation,? in Proc. Eu-
rospeech?2003, pp.2769-2772. 
[3] F.-H. Liu, ?Robustness in Speech-to-Speech Translation,? in Proc. 
Eurospeech?2003, pp.2797-2800. 
[4] B. Zhou et al ?Two-way speech-to-speech translation on handheld 
devices,? in Proc. ICSLP'04, South Korea, Oct, 2004. 
[5] H. Erdogan et al ?Using Semantic Analysis to Improve Speech 
Recognition Performance,? Computer Speech and Language, vol.19, 
pp.321-343, 2005. 
[6] R. Sarikaya, et al  ?Rapid Language Model Development Using 
External Resources for New Spoken Dialog Domains,? in Proc. 
ICASSP'05, Philadelphia, PA, Mar, 2005. 
[7] L. Gu et al ?Concept-based Speech-to-Speech Translation using 
Maximum Entropy Models for Statistical Natural Concept Genera-
tion,? IEEE Trans. Speech and Audio Processing, vol.14, no.2, 
pp.377-392, March, 2006. 
[8] B. Zhou et al ?Constrained phrase-based translation using weighted 
finite-state transducers,? in Proc. ICASSP'05, Philadelphia, Mar, 
2005. 
[9] E. Eide et al ?Recent Improvements to the IBM Trainable Speech 
Synthesis System,? in Proc.  ICASSP, Hong Kong, China, 2003. 
[10] Dan Chazan et al ?Reducing the Footprint of the IBM Trainable 
Speech Synthesis System,? in ICSLP-2002, pp.2381-2384  
[11] R. Sarikaya et al ?Maximum Entropy Based Vowelization of Ara-
bic,? Interspeech2006 (submitted for publication). 
[12] L.R. Bahl, et al ?Robust methods for using context-dependent fea-
tures and models in a continuous speech recognizer,? in Proc. 
ICASSP, 1994 
[13] D. Povey & P.C. Woodland, ?Minimum Phone Error and I-
Smoothing for Improved Discriminative Training,? In Proc. ICASSP, 
Orlando, 2002. 
[14] M. Afify et.al, ?On the Use of Morphological Analysis for Dialectal 
Arabic Speech Recognition,? Interspeech 2006 (submitted for publi-
cation). 
[15] B. Zhou, S. Chen, and Y. Gao, ?Fast Machine Translation Using 
Statistical Integrated Phrase Lattices,? submitted to COL-
ING/ACL'2006. 
 
 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 459?467,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tied?Mixture Language Modeling in Continuous Space
Ruhi Sarikaya
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
sarikaya@us.ibm.com
Mohamed Afify
Orange Labs.
Cairo, Egypt
mohamed afify2001@yahoo.com
Brian Kingsbury
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
bedk@us.ibm.com
Abstract
This paper presents a new perspective to the
language modeling problem by moving the
word representations and modeling into the
continuous space. In a previous work we in-
troduced Gaussian-Mixture Language Model
(GMLM) and presented some initial experi-
ments. Here, we propose Tied-Mixture Lan-
guage Model (TMLM), which does not have
the model parameter estimation problems that
GMLM has. TMLM provides a great deal of
parameter tying across words, hence achieves
robust parameter estimation. As such, TMLM
can estimate the probability of any word that
has as few as two occurrences in the train-
ing data. The speech recognition experiments
with the TMLM show improvement over the
word trigram model.
1 Introduction
Despite numerous studies demonstrating the serious
short-comings of the n?gram language models, it
has been surprisingly difficult to outperform n?gram
language models consistently across different do-
mains, tasks and languages. It is well-known that n?
gram language models are not effective in modeling
long range lexical, syntactic and semantic dependen-
cies. Nevertheless, n?gram models have been very
appealing due to their simplicity; they require only
a plain corpus of data to train the model. The im-
provements obtained by some more elaborate lan-
guage models (Chelba & Jelinek, 2000; Erdogan et
al., 2005) come from the explicit use of syntactic and
semantic knowledge put into the annotated corpus.
In addition to the mentioned problems above, tra-
ditional n?gram language models do not lend them-
selves easily to rapid and effective adaptation and
discriminative training. A typical n?gram model
contains millions of parameters and has no structure
capturing dependencies and relationships between
the words beyond a limited local context. These pa-
rameters are estimated from the empirical distribu-
tions, and suffer from data sparseness. n?gram lan-
guage model adaptation (to new domain, speaker,
genre and language) is difficult, simply because of
the large number of parameters, for which large
amount of adaptation data is required. Instead of up-
dating model parameters with an adaptation method,
the typical practice is to collect some data in the tar-
get domain and build a domain specific language
model. The domain specific language model is in-
terpolated with a generic language model trained
on a larger domain independent data to achieve ro-
bustness. On the other hand, rapid adaptation for
acoustic modeling, using such methods as Maxi-
mum Likelihood Linear Regression (MLLR) (Leg-
etter & Woodland, 1995), is possible using very
small amount of acoustic data, thanks to the inher-
ent structure of acoustic models that allow large de-
grees of parameter tying across different words (sev-
eral thousand context dependent states are shared
by all the words in the dictionary). Likewise,
even though discriminatively trained acoustic mod-
els have been widely used, discriminatively trained
languages models (Roark et al, 2007) have not
widely accepted as a standard practice yet.
In this study, we present a new perspective to the
language modeling. In this perspective, words are
not treated as discrete entities but rather vectors of
real numbers. As a result, long?term semantic re-
lationships between the words could be quantified
and can be integrated into a model. The proposed
formulation casts the language modeling problem as
459
an acoustic modeling problem in speech recognition.
This approach opens up new possibilities from rapid
and effective adaptation of language models to using
discriminative acoustic modeling tools and meth-
ods, such as Minimum Phone Error (MPE) (Povey
& Woodland, 2002) training to train discriminative
language models.
We introduced the idea of language modeling in
continuous space from the acoustic modeling per-
spective and proposed Gaussian Mixture Language
Model (GMLM) (Afify et al, 2007). However,
GMLM has model parameter estimation problems.
In GMLM each word is represented by a specific set
of Gaussian mixtures. Robust parameter estimation
of the Gaussian mixtures requires hundreds or even
thousands of examples. As a result, we were able
to estimate the GMLM probabilities only for words
that have at least 50 or more examples. Essentially,
this was meant to estimate the GMLM probabilities
for only about top 10% of the words in the vocab-
ulary. Not surprisingly, we have not observed im-
provements in speech recognition accuracy (Afify et
al., 2007). Tied-Mixture Language Model (TMLM)
does not have these requirements in model estima-
tion. In fact, language model probabilities can be es-
timated for words having as few as two occurrences
in the training data.
The concept of language modeling in continuous
space was previously proposed (Bengio et al, 2003;
Schwenk & Gauvain, 2003) using Neural Networks.
However, our method offers several potential advan-
tages over (Schwenk & Gauvain, 2003) including
adaptation, and modeling of semantic dependencies
because of the way we represent the words in the
continuous space. Moreover, our method also al-
lows efficient model training using large amounts of
training data, thanks to the acoustic modeling tools
and methods which are optimized to handle large
amounts of data efficiently.
It is important to note that we have to realize the
full potential of the proposed model, before investi-
gating the potential benefits such as adaptation and
discriminative training. To this end, we propose
TMLM, which does not have the problems GMLM
has and, unlike GMLM we report improvements in
speech recognition over the corresponding n?gram
models.
The rest of the paper is organized as follows. Sec-
tion 2 presents the concept of language modeling
in continuous space. Section 3 describes the tied?
mixture modeling. Speech recognition architecture
is summarized in Section 4, followed by the experi-
mental results in Section 5. Section 6 discusses var-
ious issues with the proposed method and finally,
Section 7 summarizes our findings.
2 Language Modeling In Continuous
Space
The language model training in continuous space
has three main steps; namely, creation of a co?
occurrence matrix, mapping discrete words into a
continuous parameter space in the form of vectors
of real numbers and training a statistical parametric
model. Now, we will describe each step in detail.
2.1 Creation of a co?occurrence Matrix
There are many ways that discrete words can
be mapped into a continuous space. The ap-
proach we take is based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), and begins
with the creation of a co?occurrence matrix. The
co?occurrence matrix can be constructed in sev-
eral ways, depending on the morphological com-
plexity of the language. For a morphologically
impoverished language, such as English the co?
occurrence matrix can be constructed using word bi-
gram co?occurrences. For morphologically rich lan-
guages, there are several options to construct a co?
occurrence matrix. For example, the co?occurrence
matrix can be constructed using either words (word?
word co?occurrences) or morphemes (morpheme?
morpheme co?occurrences), which are obtained af-
ter morphologically tokenizing the entire corpus.
In addition to word?word or morpheme?morpheme
co?occurrence matrices, a word?morpheme co?
occurrence matrix can also be constructed. A word
w can be decomposed into a set of prefixes, stem
and suffixes: w = [pfx1 + pfx2 + pfxn + stem+
sfx1+sfx2+sfxn]. The columns of such a matrix
contain words and the rows contain the correspond-
ing morphological decomposition (i.e. morphemes)
making up the word. The decomposition of this ma-
trix (as will be described in the next sub-section) can
allow joint modeling of words and morphemes in
one model.
460
In this study, we use morpheme level bigram co?
occurrences to construct the matrix. All the mor-
pheme1 bigrams are accumulated for the entire cor-
pus to fill in the entries of a co?occurrence matrix,
C, where C(wi, wj) denotes the counts for which
word wi follows word wj in the corpus. This is a
large, but very sparse matrix, since typically a small
number of words follow a given word. Because of
its large size and sparsity, Singular Value Decom-
position (SVD) is a natural choice for producing a
reduced-rank approximation of this matrix.
The co?occurrence matrices typically contain a
small number of high frequency events and a large
number of less frequent events. Since SVD derives
a compact approximation of the co?occurrence ma-
trix that is optimal in the least?square sense, it best
models these high-frequency events, which may not
be the most informative. Therefore, the entries of
a word-pair co?occurrence matrix are smoothed ac-
cording to the following expression:
C?(wi, wj) = log(C(wi, wj) + 1) (1)
Following the notation presented in (Bellegarda,
2000) we proceed to perform the SVD as follows:
C? ? USV T (2)
where U is a left singular matrix with row vectors
ui (1 ? i ? M) and dimension M ? R. S is a
diagonal matrix of singular values with dimension
R?R. V is a right singular matrix with row vectors
vj (1 ? j ? N) and dimension N ? R. R is the
order of the decomposition and R ? min(M,N).
M and N are the vocabulary sizes on the rows
and columns of the co?occurrence matrix, respec-
tively. For word?word or morpheme?morpheme
co?occurrence matrices M = N , but for word?
morpheme co?occurrence matrix, M is the number
of unique words in the training corpus and N is the
number of unique morphemes in morphologically
tokenized training corpus.
2.2 Mapping Words into Continuous Space
The continuous space for the words listed on the
rows of the co?occurrence matrix is defined as the
space spanned by the column vectors of AM?R =
1For the generality of the notation, from now on we use
?word? instead of ?morpheme?.
US. Similarly, the continuous space for the words
on the columns are defined as the space spanned
by the row vectors of BR?N = SV T . Here, for
a word?word co?occurrence matrix, each of the
scaled vectors (by S) in the columns of A and rows
of B are called latent word history vectors for the
forward and backward bigrams, respectively. Now,
a bigram wij = (wi, wj) (1 ? i, j ? M ) is repre-
sented as a vector of dimension M ? 1, where the
ith entry of wij is 1 and the remaining ones are zero.
This vector is mapped to a lower dimensional vector
w?ij by:
w?ij = ATwij (3)
where w?ij has dimension of R ? 1. Similarly, the
backward bigram wji (1 ? j, i ? N ) is mapped to a
lower dimensional vector w?ji by:
w?ji = Bwji (4)
where w?ji has dimension of R ? 1. Note that for a
word?morpheme co?occurrence matrix the rows of
B would contain latent morpheme vectors.
Since a trigram history consists of two bigram his-
tories, a trigram history vector is obtained by con-
catenating two bigram vectors. Having generated
the features, now we explain the structure of the
parametric model and how to train it for language
modeling in continuous space.
2.3 Parametric Model Training in Continuous
Space
Recalling the necessary inputs to train an acoustic
model for speech recognition would be helpful to
explain the new language modeling method. The
acoustic model training in speech recognition needs
three inputs: 1) features (extracted from the speech
waveform), 2) transcriptions of the speech wave-
forms and 3) baseforms, which show the pronuncia-
tion of each word in the vocabulary. We propose to
model the language model using HMMs. The HMM
parameters are estimated in such way that the given
set of observations is represented by the model in
the ?best? way. The ?best? can be defined in vari-
ous ways. One obvious choice is to use Maximum
Likelihood (ML) criterion. In ML, we maximize the
probability of a given sequence of observations O,
belonging to a given class, given the HMM ? of the
class, with respect to the parameters of the model ?.
461
This probability is the total likelihood of the obser-
vations and can be expressed mathematically as:
Ltot = p(O|?) (5)
However, there is no known way to analytically
solve for the model ? = {A,B, pi} , which max-
imize the quantity Ltot, where A is the transi-
tion probabilities, B is the observation probabili-
ties, and pi is the initial state distribution. But we
can choose model parameters such that it is locally
maximized, using an iterative procedure, like Baum-
Welch method (Baum et al, 1970).
The objective function given in Eq. 5 is the same
objective function used to estimate the parameters
of an HMM based acoustic model. By drawing an
analogy between the acoustic model training and
language modeling in continuous space, the history
vectors are considered as the acoustic observations
(feature vectors) and the next word to be predicted is
considered as the label the acoustic features belong
to, and words with their morphological decomposi-
tions can be considered as the lexicon or dictionary.
Fig. 1 presents the topology of the model for model-
ing a word sequence of 3 words. Each word is mod-
eled with a single state left?to?right HMM topology.
Using a morphologically rich language (or a char-
acter based language like Chinese) to explain how
HMMs can be used for language modeling will be
helpful. In the figure, let the states be the words and
the observations that they emit are the morphemes
(or characters in the case of Chinese). The same
topology (3 states) can also be used to model a sin-
gle word, where the first state models the prefixes,
the middle state models the stem and the final state
models the suffixes. In this case, words are repre-
sented by network of morphemes. Each path in a
word network represents a segmentation (or ?pro-
nunciation?) of the word.
The basic idea of the proposed modeling is to cre-
ate a separate model for each word of the language
and use the language model corpus to estimate the
parameters of the model. However, one could argue
that the basic model could be improved by taking
the contexts of the morphemes into account. Instead
of building a single HMM for each word, several
models could be trained according to the context of
the morphemes. These models are called context?
<s> </s>s1 s2 s3
a11
a12
22a
a23
a33
Observation
Sequence
O1 O2 O3 O4 O5
Figure 1: HMM topology for language modeling in con-
tinuous space.
dependent morphemes. The most obvious choice is
to use both left and right neighbor of a morpheme as
context, and creating, what we call tri?morphemes.
In principal even if context-dependent morphemes
could improve the modeling accuracy, the number
of models increase substantially. For a vocabulary
size of V , the number of tri?morpheme could be as
high as V 3. However, most of the tri?morphemes
are either rare or will not be observed in the training
data altogether.
Decision tree is one approach that can solve this
problem. The main idea is to find similar tri?
morphemes and share the parameters between them.
The decision tree uses a top-down approach to split
the samples, which are in a single cluster at the root
of the tree, into smaller clusters by asking questions
about the current morpheme and its context. In our
case, the questions could be syntactic and/or seman-
tic in nature.
What we hope for is that in the new continuous
space there is some form of distance or similarity
between histories such that histories not observed in
the data for some words are smoothed by similar ob-
served histories.
2.4 Summary of the Continuous Language
Model Training and Using it for Decoding
In the upper part of Fig. 2 the language model train-
ing steps are shown. The training process starts with
the language model training corpus. From the sen-
tences a bigram word co?occurrence matrix is con-
structed. This is a square matrix where the num-
ber of rows (columns) equal to the vocabulary size
of the training data. The bigram co?occurrence ma-
462
trix is decomposed using SVD. The columns of the
left?singular matrix obtained from SVD is used to
map the bigram word histories into a lower dimen-
sional continuous parameter space. The projected
word history vectors are stacked together depending
on the size of the n?gram. For example, for trigram
modeling two history vectors are stacked together.
Even though, we have not done so, at this stage one
could cluster the word histories for robust parame-
ter estimation. Now, the feature vectors, their corre-
sponding transcriptions and the lexicon (baseforms)
are ready to perform the ?acoustic model training?.
One could use maximum likelihood criterion or any
other objective function such as Minimum Phone Er-
ror (MPE) training to estimate the language model
parameters in the continuous space.
The decoding phase could employ an adaptation
step, if one wants to adapt the language model to
a different domain, speaker or genre. Then, given
a hypothesized sequence of words the decoder ex-
tracts the corresponding feature vectors. The fea-
ture vectors are used to estimate the likelihood of
the word sequence using the HMM parameters. This
likelihood is used to compute the probability of the
word sequence. Next, we introduce Tied?Mixture
Modeling, which is a special HMM structure to ro-
bustly estimate model parameters.
3 Tied?Mixture Modeling
Hidden Markov Models (HMMs) have been exten-
sively used virtually in all aspects of speech and
language processing. In speech recognition area
continuous-density HMMs have been the standard
for modeling speech signals, where several thousand
context?dependent states have their own Gaussian
density functions to model different speech sounds.
Typically, speech data have hundreds of millions of
frames, which are sufficient to robustly estimate the
model parameters. The amount of data for language
modeling is orders of magnitude less than that of
the acoustic data in continuous space. Tied?Mixture
Hidden Markov Models (TM?HMMs) (Bellegarda
& Nahamoo, 1989; Huang & Jack, 1988) have a bet-
ter decoupling between the number of Gaussians and
the number of states compared to continuous den-
sity HMMs. The TM?HMM is useful for language
modeling because it allows us to choose the num-
HMM Training(e.g. ML, MMIE,MPE)
Model Adaptation
Decoding/Search
Model Training
Word History Clustering
Adaptation Data
Probability Computation
Nw1
to Create Feature Vector
,...,2ww DimensionalityReduction (e.g.,SVD
Decomposition)
Create Bigram
Co-occurance Matrix
Map Word Historiesinto Continuous Space
 i-2i-1
 i-1i
f(w     |   w    )
f(w    |   w     )Stack the History Vectors
Figure 2: Language Model Training and Adaptation in
Continuous Space.
ber of Gaussian densities and the number of mixture
weights independently. Much more data is required
to reliably estimate Gaussian densities than to esti-
mate mixture weights.
The evaluation of the observation density func-
tions for TM?HMMs can be time consuming due to
the large mixture weight vector and due to the fact
that for each frame all Gaussians have to be evalu-
ated. However, there are a number of solutions pro-
posed in the past that significantly reduces the com-
putation (Duchateau et al, 1998).
The function p(w | h), defined in a continu-
ous space, represents the conditional probability of
the word w given the history h. In general, h
contains previous words and additional information
(e.g. part-of-speech (POS) tags for the previous
words) that may help to the prediction of the next
word. Unlike TM?HMMs, using a separate HMM
for each word as in the case of Gaussian Mixture
Models (GMMs), to represent the probability distri-
bution functions results in the estimation problems
for the model parameters since each n?gram does
not have hundreds of examples. TM?HMMs use
Gaussian mixture probability density functions per
463
state in which a single set of Gaussians is shared
among all states:
p(o|w) =
J?
j
cw,jNj(o, ?w,j ,?w,j) (6)
where w is the state, Nj is the jth Gaussian, and o
is the observation (i.e. history) vectors. and J is the
number of component mixtures in the TM-HMM.
In order to avoid zero variance in word mapping
into continuous space, all the latent word vectors are
added a small amount of white noise.
The TM?HMM topology is given in Fig. 3. Each
state models a word and they all share the same set of
Gaussian densities. However, each state has a spe-
cific set of mixture weights associated with them.
This topology can model a word?sequence that con-
sist of three words in them. The TM?HMM esti-
mates the probability of observing the history vec-
tors (h) for a given word w. However, what we need
is the posterior probability p(w | h) of observing w
as the next word given the history, h. This can be
obtained using the Bayes rule:
p(w|h) = p(h|w)p(w)p(h) (7)
= p(h|w)p(w)?V
v=1 p(h|v)p(v)
(8)
where p(w) is the unigram probability of the word
w. The unigram probabilities can also be substituted
for more accurate higher order n?gram probabilities.
If this n?gram has an order that is equal to or greater
than the one used in defining the continuous contexts
h, then the TMLM can be viewed as performing a
kind of smoothing of the original n?gram model:
Ps(w | h) = P (w | h)p(h | w)?V
v=1 P (v | h)p(h | v)
(9)
where Ps(w | h) and P (w | h) are the smoothed
and original n?grams.
The TM?HMM parameters are estimated through
an iterative procedure called the Baum-Welch, or
forward-backward, algorithm (Baum et al, 1970).
The algorithm locally maximizes the likelihood
function via an iterative procedure. This type of
33
<s> </s>s1 s2 s3
a11
a12
22a
a23
a
Figure 3: Tied-Mixture HMM topology for language
modeling in continuous space. The mixtures are tied
across states. Each state represents a word. The TM-
HMM is completely defined with the mixture weights,
mixture densities and transition probabilities.
training is identical to training continuous density
HMMs except the Gaussians are tied across all arcs.
For the model estimation equations the readers are
referred to (Bellegarda & Nahamoo, 1989; Huang &
Jack, 1988).
Next, we introduce the speech recognition system
used for the experiments.
4 Speech Recognition Architecture
The speech recognition experiments are carried out
on the Iraqi Arabic side of an English to Iraqi Ara-
bic speech-to-speech translation task. This task cov-
ers the military and medical domains. The acoustic
data has about 200 hours of conversational speech
collected in the context of a DARPA supported
speech-to-speech (S2S) translation project (Gao et
al., 2006).
The feature vectors for training acoustic models
are generated as follows. The speech data is sampled
at 16kHz and the feature vectors are computed every
10ms. First, 24-dimensional MFCC features are ex-
tracted and appended with the frame energy. The
feature vector is then mean and energy normalized.
Nine vectors, including the current vector and four
vectors from its right and left contexts, are stacked
leading to a 216-dimensional parameter space. The
feature space is finally reduced from 216 to 40 di-
mensions using a combination of linear discriminant
analysis (LDA), feature space MLLR (fMLLR) and
feature space MPE (fMPE) training (Povey et al,
464
2005). The baseline speech recognition system used
in our experiments is the state?of?the?art and pro-
duces a competitive performance.
The phone set consists of 33 graphemes represent-
ing speech and silence for acoustic modeling. These
graphemes correspond to letters in Arabic plus si-
lence and short pause models. Short vowels are im-
plicitly modeled in the neighboring graphemes. Fea-
ture vectors are first aligned, using initial models,
to model states. A decision tree is then built for
each state using the aligned feature vectors by ask-
ing questions about the phonetic context; quinphone
questions are used in this case. The resulting tree has
about 3K leaves. Each leaf is then modeled using
a Gaussian mixture model. These models are first
bootstrapped and then refined using three iterations
of forward?backward training. The current system
has about 75K Gaussians.
The language model training data has 2.8M words
with 98K unique words and it includes acoustic
model training data as a subset. The morpholog-
ically analyzed training data has 58K unique vo-
cabulary items. The pronunciation lexicon consists
of the grapheme mappings of these unique words.
The mapping to graphemes is one-to-one and there
are very few pronunciation variants that are sup-
plied manually mainly for numbers. A statistical tri-
gram language model using Modified Kneser-Ney
smoothing (Chen& Goodman, 1996) has been built
using the training data, which is referred to as Word-
3gr.
For decoding a static decoding graph is com-
piled by composing the language model, the pro-
nunciation lexicon, the decision tree, and the HMM
graphs. This static decoding scheme, which com-
piles the recognition network off?line before decod-
ing, is widely adopted in speech recognition (Ri-
ley et al, 2002). The resulting graph is further op-
timized using determinization and minimization to
achieve a relatively compact structure. Decoding is
performed on this graph using a Viterbi beam search.
5 Experimental Results
We used the following TMLM parameters to build
the model. The SVD projection size is set to 200
(i.e. R = 200) for each bigram history. This re-
sults into a trigram history vector of size 400. This
?8 ?7 ?6 ?5 ?4 ?3 ?2 ?1 0?9
?8
?7
?6
?5
?4
?3
?2
?1
0
N?gram Probability
TML
M P
roba
bility
Figure 4: Scatter plot of the n?gram and TMLM proba-
bilities.
vector is further projected down to a 50 dimensional
feature space using LDA transform. The total num-
ber of Gaussian densities used for the TM?HMM is
set to 1024. In order to find the overall relationship
between trigram and TMLM probabilities we show
the scatter plot of the trigram and TMMT probabili-
ties in Fig. 4. While calculating the TMLM score the
TMLM likelihood generated by the model is divided
by 40 to balance its dynamic range with that of the
n?gram model. Most of the probabilities lie along
the diagonal line. However, some trigram proba-
bilities are modulated making TMLM probabilities
quite different than the corresponding trigram prob-
abilities. Analysis of TMLM probabilities with re-
spect to the trigram probabilities would be an inter-
esting future research.
We conducted the speech recognition language
modeling experiments on 3 testsets: TestA, TestB
and TestC. All three test sets are from July?07
official evaluations of the IBM?s speech-to-speech
translation system by DARPA. TestA consists of
sentences spoken out in the field to the IBM?s S2S
system during live evaluation. TestB contains sen-
tences spoken in an office environment to the live
S2S system. Using on-the-spot speakers for TestA
and TestB meant to have shorter and clean sentences.
Finally TestC contains pre-recorded sentences with
much more hesitations and more casual conversa-
tions compared to the other two testsets. TestA,
TestB and TestC have 309, 320 and 561 sentences,
respectively.
465
LM TestA TestB TestC All
Word-3gr 18.7 18.6 38.9 32.9
TMLM 18.8 18.9 38.2 32.5
TMLM + Word-3gr 17.6 18.0 37.4 31.9
Table 1: Speech Recognition Language Model Rescoring
Results.
In order to evaluate the performance of the
TMLM, a lattice with a low oracle error rate was
generated by a Viterbi decoder using the word tri-
gram model (Word-3gr) model. From the lattice at
most 30 (N=30) sentences are extracted for each ut-
terance to form an N-best list. The N?best error rate
for the combined test set (All) is 22.7%. The N?
best size is limited (it is not in the hundreds), simply
because of faster experiment turn-around. These ut-
terances are rescored using TMLM. The results are
presented in Table 1. The first two rows in the ta-
ble show the baseline numbers for the word trigram
(Word?3gr) model. TestA has a WER of 18.7% sim-
ilar to that of TestB (18.6%). The WER for TestC
is relatively high (38.9%), because, as explained
above, TestC contains causal conversation with hes-
itations and repairs, and speakers do not necessar-
ily stick to the domain. Moreover, when users are
speaking to a device, as in the case of TestA and
TestB, they use clear and shorter sentences, which
are easier to recognize. The TMLM does not pro-
vide improvements for TestA and TestB but it im-
proves the WER by 0.7% for TestC. The combined
overall result is a 0.4% improvement over base-
line. This improvement is not statistically signifi-
cant. However, interpolating TMLM with Word-3gr
improves the WER to 31.9%, which is 1.0% better
than that of the Word-3gr. Standard p-test (Matched
Pairs Sentence-Segment Word Error test available
in standard SCLITEs statistical system comparison
program from NIST) shows that this improvement
is significant at p < 0.05 level. The interpolation
weights are set equally to 0.5 for each LM.
6 Discussions
Despite limited but encouraging experimental re-
sults, we believe that the proposed perspective is a
radical departure from the traditional n?gram based
language modeling methods. The new perspective
opens up a number of avenues which are impossible
to explore in one paper.
We realize that there are a number of outstand-
ing issues with the proposed perspective that re-
quire a closer look. We make a number of deci-
sions to build a language model within this perspec-
tive. The decisions are sometimes ad hoc. The de-
cisions are made in order to build a working sys-
tem and are by no means the best decisions. In
fact, it is quite likely that a different set of de-
cisions may result into a better system. Using a
word?morpheme co?occurrence matrix instead of a
morpheme?morpheme co?occurrence matrix is one
such decision. Another one is the clustering/tying
of the rarely observed events to achieve robust para-
meter estimation both for the SVD and TMLM pa-
rameter estimation. We also use a trivial decision
tree to build the models where there were no con-
text questions. Clustering morphemes with respect
to their syntactic and semantic context is another
area which should be explored. In fact, we are in
the process of building these models. Once we have
realized the full potential of the baseline maximum
likelihood TMLM, then we will investigate the dis-
criminative training methods such as MPE (Povey
& Woodland, 2002) to further improve the language
model performance and adaptation to new domains
using MLLR (Legetter & Woodland, 1995).
We also realize that different problems such as
segmentation (e.g. Chinese) of words or morpholog-
ical decomposition of words into morphemes can be
addressed within the proposed perspective.
7 Conclusions
We presented our progress in improving continuous-
space language modeling. We proposed the Tied-
Mixture Language Model (TMLM), which allows
for robust parameter estimation through the use
of tying and improves on the previously presented
GMLM. The new formulation lets us train a para-
metric language model using off?the?shelf acoustic
model training tools. Our initial experimental results
validated the proposed approach with encouraging
results.
466
References
M.Afify, O. Siohan and R. Sarikaya. 2007. Gaussian
Mixture Language Models for Speech Recognition,
ICASSP, Honolulu, Hawaii.
C.J. Legetter and P.C. Woodland. 1995. Maximum like-
lihood linear regression for speaker adaptation of con-
tinuous density hidden Markov models, Computer
Speech and Language, vol.9, pp. 171-185.
J. Bellegarda. 2000. Large Vocabulary Speech Recogni-
tion with Multispan Language Models, IEEE Trans-
actions on Speech and Audio Processing, vol. 8, no. 1,
pp. 76-84.
H. Schwenk, and J.L. Gauvain. 2003. Using Continuous
Space Language Models for Conversational Telephony
Speech Recognition, IEEE Workshop on Spontaneous
Speech Processing and Recognition, Tokyo, Japan.
J. Duchateau, K. Demuynck, D.V. Compernolle and P.
Wambacq. 1998. Improved Parameter Tying for Ef-
ficient Acoustic Model Evaluation in Large Vocabu-
lary Continuous Speech Recognition. Proc. of ICSLP,
Sydney, Australia.
Y. Gao, L. Gu, B. Zhou, R. Sarikaya, H.-K. Kuo. A.-
V.I. Rosti, M. Afify, W. Zhu. 2006. IBM MASTOR:
Multilingual Automatic Speech-to-Speech Translator.
Proc. of ICASSP, Toulouse, France.
S. Chen, J. Goodman. 1996. An Empirical Study of
Smoothing Techniques for Language Modeling, ACL,
Santa Cruz, CA.
J. Bellagarda and D. Nahamoo. 1989. Tied mixture con-
tinuous parameter models for large vocabulary isolated
speech recognition, Proc. of ICASSP, pp. 13-16.
X.D. Huang and M.A. Jack. 1988. Hidden Markov Mod-
elling of Speech Based on a Semicontinuous Model,
Electronic Letters, 24(1), pp. 6-7, 1988.
D. Povey and P.C. Woodland. 2002. Minimum phone er-
ror and I-smoothing for improved discriminative train-
ing, Proc. of ICASSP, pp. 105?108, Orlando, Florida.
D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,
G. Zweig. 2005. fMPE: Discriminatively Trained
Features for Speech Recognition, Proc. of ICASSP,
pp. 961?964, Philadelphia, PA.
C. Chelba and F. Jelinek. 2000. Structured language
modeling, Computer Speech and Language, 14(4),
283?332, 2000.
H. Erdogan, R. Sarikaya, S.F. Chen, Y. Gao and M.
Picheny. 2005. Using Semantic Analysis to Improve
Speech Recognition Performance, Computer Speech
& Language Journal, vol. 19(3), pp: 321?343.
B. Roark, M. Saraclar, M. Collins. 2007. Using Seman-
tic Analysis to Improve Speech Recognition Perfor-
mance, Computer Speech & Language, vol. 21(2), pp:
373?392.
M. Riley, E. Bocchieri, A. Ljolje and M. Saraclar. 2007.
The AT&T 1x real-time Switchboard speech-to-text
system, NIST RT02 Workshop, Vienna, Virginia.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research, vol. 3, 11371155.
S. Deerwester, Susan Dumais, G. W. Furnas, T. K. Lan-
dauer, R. Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society for
Information Science, 41 (6): 391?407.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A
Maximization Techniques Occurring in the Statistical
Analysis of Probabilistic Functions of Markov Chains,
The Annals of Mathematical Statistics, 41(1):164?171.
467
