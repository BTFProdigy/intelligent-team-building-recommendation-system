Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 151?160, Prague, June 2007. c?2007 Association for Computational Linguistics
Using Foreign Inclusion Detection to Improve Parsing Performance
Beatrice Alex, Amit Dubey and Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
{balex,adubey,keller}@inf.ed.ac.uk
Abstract
Inclusions from other languages can be a
significant source of errors for monolin-
gual parsers. We show this for English in-
clusions, which are sufficiently frequent to
present a problem when parsing German.
We describe an annotation-free approach for
accurately detecting such inclusions, and de-
velop two methods for interfacing this ap-
proach with a state-of-the-art parser for Ger-
man. An evaluation on the TIGER cor-
pus shows that our inclusion entity model
achieves a performance gain of 4.3 points in
F-score over a baseline of no inclusion de-
tection, and even outperforms a parser with
access to gold standard part-of-speech tags.
1 Introduction
The status of English as a global language means
that English words and phrases are frequently bor-
rowed by other languages, especially in domains
such as science and technology, commerce, adver-
tising, and current affairs. This is an instance of lan-
guage mixing, whereby inclusions from other lan-
guages appear in an otherwise monolingual text.
While the processing of foreign inclusions has re-
ceived some attention in the text-to-speech (TTS) lit-
erature (see Section 2), the natural language process-
ing (NLP) community has paid little attention both
to the problem of inclusion detection, and to poten-
tial applications thereof. Also the extent to which
inclusions pose a problem to existing NLP methods
has not been investigated.
In this paper, we address this challenge. We focus
on English inclusions in German text. Anglicisms
and other borrowings from English form by far the
most frequent foreign inclusions in German. In spe-
cific domains, up to 6.4% of the tokens of a Ger-
man text can be English inclusions. Even in regular
newspaper text as used for many NLP applications,
English inclusions can be found in up to 7.4% of all
sentences (see Section 3 for both figures).
Virtually all existing NLP algorithms assume that
the input is monolingual, and does not contain for-
eign inclusions. It is possible that this is a safe
assumption, and inclusions can be dealt with ac-
curately by existing methods, without resorting to
specialized mechanisms. The alternative hypothe-
sis, however, seems more plausible: foreign inclu-
sions pose a problem for existing approaches, and
sentences containing them are processed less ac-
curately. A parser, for example, is likely to have
problems with inclusions ? most of the time, they
are unknown words, and as they originate from
another language, standard methods for unknown
words guessing (suffix stripping, etc.) are unlikely to
be successful. Furthermore, the fact that inclusions
are often multiword expressions (e.g., named enti-
ties) means that simply part-of-speech (POS) tag-
ging them accurately is not sufficient: if the parser
posits a phrase boundary within an inclusion this is
likely to severely decrease parsing accuracy.
In this paper, we focus on the impact of En-
glish inclusions on the parsing of German text. We
describe an annotation-free method that accurately
recognizes English inclusions, and demonstrate that
inclusion detection improves the performance of a
state-of-the-art parser for German. We show that the
way of interfacing the inclusion detection and the
parser is crucial, and propose a method for modify-
ing the underlying probabilistic grammar in order to
151
enable the parser to process inclusions accurately.
This paper is organized as follows. We review re-
lated work in Section 2, and present the English in-
clusion classifier in Section 3. Section 4 describes
our results on interfacing inclusion detection with
parsing, and Section 5 presents an error analysis.
Discussion and conclusion follow in Section 6.
2 Related Work
Previous work on inclusion detection exists in the
TTS literature. Here, the aim is to design a sys-
tem that recognizes foreign inclusions on the word
and sentence level and functions at the front-end to
a polyglot TTS synthesizer. Pfister and Romsdor-
fer (2003) propose morpho-syntactic analysis com-
bined with lexicon lookup to identify foreign words
in mixed-lingual text. While they state that their sys-
tem is precise at detecting the language of tokens
and determining the sentence structure, it is not eval-
uated on real mixed-lingual text. A further approach
to inclusion detection is that of Marcadet et. al
(2005). They present experiments with a dictionary-
driven transformation-based learning method and a
corpus-based n-gram approach and show that a com-
bination of both methods yields the best results.
Evaluated on three mixed-lingual test sets in differ-
ent languages, the combined approach yields word-
based language identification error rates (i.e. the per-
centage of tokens for which the language is identi-
fied incorrectly) of 0.78% on the French data, 1.33%
on the German data and 0.84% on the Spanish data.
Consisting of 50 sentences or less for each language,
their test sets are very small and appear to be se-
lected specifically for evaluation purposes. It would
therefore be interesting to determine the system?s
performance on random and unseen data and exam-
ine how it scales up to larger data sets.
Andersen (2005), noting the importance of rec-
ognizing anglicisms to lexicographers, tests algo-
rithms based on lexicon lookup, character n-grams
and regular expressions and a combination thereof to
automatically extract anglicisms in Norwegian text.
On a 10,000 word subset of the neologism archive
(Wangensteen, 2002), the best method of combin-
ing character n-grams and regular expression match-
ing yields an accuracy of 96.32% and an F-score of
59.4 (P = 75.8%, R = 48.8%). This result is unsur-
prisingly low as no differentiation is made between
full-word anglicisms and tokens with mixed-lingual
morphemes in the gold standard.
In the context of parsing, Forst and Kaplan (2006)
have observed that the failure to properly deal with
foreign inclusions is detrimental to a parser?s accu-
racy. However, they do not substantiate this claim
using numeric results.
3 English Inclusion Detection
Previous work reported by Alex (2006; 2005) has
focused on devising a classifier that detects angli-
cisms and other English inclusions in text written in
other languages, namely German and French. This
inclusion classifier is based on a lexicon and search
engine lookup as well as a post-processing step.
The lexicon lookup is performed for tokens
tagged as noun (NN ), named entity (NE ), foreign
material (FM ) or adjective (ADJA/ADJD ) using the
German and English CELEX lexicons. Tokens only
found in the English lexicon are classified as En-
glish. Tokens found in neither lexicon are passed
to the search engine module. Tokens found in
both databases are classified by the post-processing
module. The search engine module performs lan-
guage classification based on the maximum nor-
malised score of the number of hits returned for two
searches per token, one for each language (Alex,
2005). This score is determined by weighting the
number of hits, i.e. the ?absolute frequency? by the
estimated size of the accessible Web corpus for that
language (Alex, 2006). Finally, the rule-based post-
processing module classifies single-character tokens
and resolves language classification ambiguities for
interlingual homographs, English function words,
names of currencies and units of measurement. A
further post-processing step relates language infor-
mation between abbreviations or acronyms and their
definitions in combination with an abbreviation ex-
traction algorithm (Schwartz and Hearst, 2003). Fi-
nally, a set of rules disambiguates English inclusions
from person names (Alex, 2006).
For German, the classifier has been evaluated
on test sets in three different domains: newspaper
articles, selected from the Frankfurter Allgemeine
Zeitung, on internet and telecoms, space travel and
European Union related topics. Table 1 presents an
152
Domain EI tokens EI types EI TTR Accuracy Precision Recall F
Internet 6.4% 5.9% 0.25 98.13% 91.58% 78.92% 84.78
Space 2.8% 3.5% 0.33 98.97% 84.02% 85.31% 84.66
EU 1.1% 2.1% 0.50 99.65% 82.16% 87.36% 84.68
Table 1: English inclusion (EI) token and type statistics, EI type-token-ratios (TTR) as well as accuracy,
precision, recall and F-scores for the unseen German test sets.
overview of the percentages of English inclusion to-
kens and types within the gold standard annotation
of each test set, and illustrates how well the English
inclusion classifier is able to detect them in terms
of F-score. The figures show that the frequency of
English inclusions varies considerably depending on
the domain but that the classifier is able to detect
them equally well with an F-score approaching 85
for each domain.
The recognition of English inclusions bears sim-
ilarity to classification tasks such as named en-
tity recognition, for which various machine learning
(ML) techniques have proved successful. In order to
compare the performance of the English inclusion
classifier against a trained ML classifier, we pooled
the annotated English inclusion evaluation data for
all three domains. As the English inclusion classifier
does not rely on annotated data, it can be tested and
evaluated once for the entire corpus. The ML classi-
fier used for this experiment is a conditional Markov
model tagger which is designed for, and proved suc-
cessful in, named entity recognition in newspaper
and biomedical text (Klein et al, 2003; Finkel et al,
2005). It can be trained to perform similar informa-
tion extraction tasks such as English inclusion detec-
tion. To determine the tagger?s performance over the
entire set and to investigate the effect of the amount
of annotated training data available, a 10-fold cross-
validation test was conducted whereby increasing
sub-parts of the training data are provided when test-
ing on each fold. The resulting learning curves in
Figure 1 show that the English inclusion classifier
has an advantage over the supervised ML approach,
despite the fact the latter requires expensive hand-
annotated data. A large training set of 80,000 tokens
is required to yield a performance that approximates
that of our annotation-free inclusion classifier. This
system has been shown to perform similarly well on
unseen texts in different domains, plus it is easily
 20
 30
 40
 50
 60
 70
 80
 90
 10000  20000  30000  40000  50000  60000  70000  80000
F-
sc
or
e
Amount of training data (in tokens)
Statistical Tagger
English Inclusion Classifier
Figure 1: Learning curve of a ML classifier versus
the English inclusion classifier?s performance.
extendable to a new language (Alex, 2006).
4 Experiments
The primary focus of this paper is to apply the En-
glish inclusion classifier to the German TIGER tree-
bank (Brants et al, 2002) and to evaluate the clas-
sifier on a standard NLP task, namely parsing. The
aim is to investigate the occurrence of English in-
clusions in more general newspaper text, and to ex-
amine if the detection of English inclusions can im-
prove parsing performance.
The TIGER treebank is a bracketed corpus con-
sisting of 40,020 sentences of newspaper text. The
English inclusion classifier was run once over the
entire TIGER corpus. In total, the system detected
English inclusions in 2,948 of 40,020 sentences
(7.4%), 596 of which contained at least one multi-
word inclusion. This subset of 596 sentences is the
focus of the work reported in the remainder of this
paper, and will be referred to as the inclusion set.
A gold standard parse tree for a sentence contain-
ing a typical multi-word English inclusion is illus-
trated in Figure 2. The tree is relatively flat, which
153
is a trait trait of TIGER treebank annotation (Brants
et al, 2002). The non-terminal nodes of the tree rep-
resent the phrase categories, and the edge labels the
grammatical functions. In the example sentence, the
English inclusion is contained in a proper noun (PN )
phrase with a grammatical function of type noun
kernel element (NK ). Each terminal node is POS-
tagged as a named entity (NE ) with the grammatical
function ot type proper noun component (PNC ).
4.1 Data
Two different data sets are used in the experiments:
(1) the inclusion set, i.e., the sentences containing
multi-word English inclusions recognized by the in-
clusion classifier, and (2) a stratified sample of sen-
tences randomly extracted from the TIGER corpus,
with strata for different sentence lengths. The strata
were chosen so that the sentence length distribution
of the random set matches that of the inclusion set.
The average sentence length of this random set and
the inclusion set is therefore the same at 28.4 tokens.
This type of sampling is necessary as the inclusion
set has a higher average sentence length than a ran-
dom sample of sentences from TIGER, and because
parsing accuracy is correlated with sentence length.
Both the inclusion set and the random set consist of
596 sentences and do not overlap.
4.2 Parser
The parsing experiments were performed with a
state-of-the-art parser trained on the TIGER corpus
which returns both phrase categories and grammati-
cal functions (Dubey, 2005b). Following Klein and
Manning (2003), the parser uses an unlexicalized
probabilistic context-free grammar (PCFG) and re-
lies on treebank transformations to increase parsing
accuracy. Crucially, these transformations make use
of TIGER?s grammatical functions to relay pertinent
lexical information from lexical elements up into the
tree.
The parser also makes use of suffix analysis.
However, beam search or smoothing are not em-
ployed. Based upon an evaluation on the NEGRA
treebank (Skut et al, 1998), using a 90%-5%-5%
training-development-test split, the parser performs
with an accuracy of 73.1 F-score on labelled brack-
ets with a coverage of 99.1% (Dubey, 2005b). These
figures were derived on a test set limited to sentences
containing 40 tokens or less. In the data set used
in this paper, however, sentence length is not lim-
ited. Moreover, the average sentence length of our
test sets is considerably higher than that of the NE-
GRA test set. Consequently, a slightly lower perfor-
mance and/or coverage is anticipated, albeit the type
and domain as well as the annotation of both the NE-
GRA and the TIGER treebanks are very similar. The
minor annotation differences that do exist between
NEGRA and TIGER are explained in Brants et. al
(2002).
4.3 Parser Modifications
We test several variations of the parser. The baseline
parser does not treat foreign inclusions in any spe-
cial way: the parser attempts to guess the POS tag
and grammatical function labels of the word using
the same suffix analysis as for rare or unseen Ger-
man words. The additional versions of the parser
are inspired by the hypothesis that inclusions make
parsing difficult, and this difficulty arises primarily
because the parser cannot detect inclusions prop-
erly. Therefore, a suitable upper bound is to give
the parser perfect tagging information. Two further
versions interface with our inclusion classifier and
treat words marked as inclusions differently from
native words. The first version does so on a word-
by-word basis. In contrast, the inclusion entity ap-
proach attempts to group inclusions, even if a group-
ing is not posited by phrase structure rules. We now
describe each version in more detail.
In the TIGER annotation, preterminals include
both POS tags and grammatical function labels.
For example, rather than a preterminal node hav-
ing the category PRELS (personal pronoun), it is
given the category PRELS-OA (accusative personal
pronoun). Due to these grammatical function tags,
the perfect tagging parser may disambiguate more
syntactic information than provided with POS tags
alone. Therefore, to make this model more realistic,
the parser is required to guess grammatical functions
(allowing it to, for example, mistakenly tag an ac-
cusative pronoun as nominative, dative or genitive).
This gives the parser information about the POS tags
of English inclusions (along with other words), but
does not give any additional hints about the syntax
of the sentence.
The two remaining models both take advantage
154
SNP-SB
ART-NK
Das
ADJA-NK
scho?nste
PN-NK
NE-PNC
Road
NE-PNC
Movie
VVFIN-HD
kam
PP-MO
APPR-AC
aus
ART-NK
der
NE-NK
Schweiz
Figure 2: Example parse tree of a German TIGER sentence containing an English inclusion. Translation:
The nicest road movie came from Switzerland.
NE FM NN KON CARD ADJD APPR
1185 512 44 8 8 1 1
Table 2: POS tags of foreign inclusions.
PN
FOM
. . .
FOM
. . .
(a) Whenever a FOM is encoun-
tered...
PN
FP
FOM
. . .
FOM
. . .
(b) ...a new FP category is cre-
ated
Figure 3: Tree transformation employed in the in-
clusion entity parser.
of information from the inclusion detector. To inter-
face the detector with the parser, we simply mark
any inclusion with a special FOM (foreign mate-
rial) tag. The word-by-word parser attempts to guess
POS tags itself, much like the baseline. However,
whenever it encounters a FOM tag, it restricts itself
to the set of POS tags observed in inclusions during
training (the tags listed in Table 2). When a FOM is
detected, these and only these POS tags are guessed;
all other aspects of the parser remain the same.
The word-by-word parser fails to take advantage
of one important trend in the data: that foreign in-
clusion tokens tend to be adjacent, and these adja-
cent words usually refer to the same entity. There
is nothing stopping the word-by-word parser from
positing a constituent boundary between two adja-
cent foreign inclusions. The inclusion entity model
was developed to restrict such spurious bracketing.
It does so by way of another tree transformation.
The new category FP (foreign phrase) is added be-
low any node dominating at least one token marked
FOM during training. For example, when encoun-
tering a FOM sequence dominated by PN as in Fig-
ure 3(a), the tree is modified so that it is the FP rule
which generates the FOM tokens. Figure 3(b) shows
the modified tree. In all cases, a unary rule PN?FP
is introduced. As this extra rule decreases the proba-
bility of the entire tree, the parser has a bias to intro-
duce as few of these rules as possible ? thus limiting
the number of categories which expand to FOMs.
Once a candidate parse is created during testing, the
inverse operation is applied, removing the FP node.
4.4 Method
For all experiments reported in this paper, the parser
is trained on the TIGER treebank. As the inclusion
and random sets are drawn from the whole TIGER
treebank, it is necessary to ensure that the data used
to train the parser does not overlap with these test
sentences. The experiments are therefore designed
as multifold cross-validation tests. Using 5 folds,
each model is trained on 80% of the data while the
remaining 20% are held out. The held out set is then
155
Data P R F Dep. Cov. AvgCB 0CB ?2CB
Baseline model
Inclusion set 56.1 62.6 59.2 74.9 99.2 2.1 34.0 69.0
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Perfect tagging model
Inclusion set 61.3 63.0 62.2 75.1 92.7 1.7 41.5 72.6
Random set 65.8 68.9 67.3 82.4 97.7 1.4 45.9 77.1
Word-by-word model
Inclusion set 55.6 62.8 59.0 73.1 99.2 2.1 34.2 70.2
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Inclusion entity model
Inclusion set 61.3 65.9 63.5 78.3 99.0 1.7 42.4 77.1
Random set 63.4 67.5 65.4 80.8 99.2 1.6 40.1 75.7
Table 3: Baseline and perfect tagging for inclusion and random sets and results for the word-by-word and
the inclusion entity models.
intersected with the inclusion set (or, respectively,
the random set). The evaluation metrics are calcu-
lated on this subset of the inclusion set (or random
set), using the parser trained on the corresponding
training data. This process ensures that the test sen-
tences are not contained in the training data.
The overall performance metrics of the parser are
calculated on the aggregated totals of the five held
out test sets. For each experiment, we report pars-
ing performance in terms of the standard PARSE-
VAL scores (Abney et al, 1991), including cov-
erage (Cov), labeled precision (P) and recall (R),
F-score, the average number of crossing brackets
(AvgCB), and the percentage of sentences parsed
with zero and with two or fewer crossing brack-
ets (0CB and ?2CB). In addition, we also report
dependency accuracy (Dep), calculated using the
approach described in Lin (1995), using the head-
picking method used by Dubey (2005a). The la-
beled bracketing figures (P, R and F), and the de-
pendency score are calculated on all sentences, with
those which are out-of-coverage getting zero nodes.
The crossing bracket scores are calculated only on
those sentences which are successfully parsed.
4.5 Baseline and Perfect Tagging
The baseline, for which the unmodified parser is
used, achieves a high coverage at over 99% for both
the inclusion and the random sets (see Table 3).
However, scores differ for the bracketing measures.
Using stratified shuffling1, we performed a t-test on
precision and recall, and found both to be signif-
icantly worse in the inclusion condition. Overall,
the harmonic mean (F) of precision and recall was
65.2 on the random set, 6 points better than 59.2
F observed on the inclusion set. Similarly, depen-
dency and cross-bracketing scores are higher on the
random test set. This result strongly indicates that
sentences containing English inclusions present dif-
ficulty for the parser, compared to length-matched
sentences without inclusions.
When providing the parser with perfect tagging
information, scores improve both for the inclusion
and the random TIGER samples, resulting in F-
scores of 62.2 and 67.3, respectively. However, the
coverage for the inclusion set decreases to 92.7%
whereas the coverage for the random set is 97.7%.
In both cases, the lower coverage is caused by the
parser being forced to use infrequent tag sequences,
with the much lower coverage of the inclusion set
likely due to infrequent tags (notable FM ), solely
associated with inclusions. While perfect tagging
increases overall accuracy, a difference of 5.1 in F-
score remains between the random and inclusion test
sets. Although smaller than that of the baseline runs,
this difference shows that even with perfect tagging,
1This approach to statistical testing is described in: http:
//www.cis.upenn.edu/?dbikel/software.html
156
parsing English inclusions is harder than parsing
monolingual data.
So far, we have shown that the English inclusion
classifier is able to detect sentences that are difficult
to parse. We have also shown that perfect tagging
helps to improve parsing performance but is insuffi-
cient when it comes to parsing sentences containing
English inclusions. In the next section, we will ex-
amine how the knowledge provided by the English
inclusion classifier can be exploited to improve pars-
ing performance for such sentences.
4.6 Word-by-word Model
The word-by-word model achieves the same cover-
age on the inclusion set as the baseline but with a
slightly lower F of 59.0. All other scores, includ-
ing dependency accuracy and cross bracketing re-
sults are similar to those of the baseline (see Ta-
ble 3). This shows that limiting the parser?s choice
of POS tags to those encountered for English inclu-
sions is not sufficient to deal with such constructions
correctly. In the error analysis presented in Sec-
tion 5, we report that the difficulty in parsing multi-
word English inclusions is recognizing them as con-
stituents, rather than recognizing their POS tags. We
attempt to overcome this problem with the inclusion
entity model.
4.7 Inclusion Entity Model
The inclusion entity parser attains a coverage of
99.0% on the inclusion set, similiar to the cover-
age of 99.2% obtained by the baseline model on
the same data. On all other measures, the inclu-
sion entity model exceeds the performance of the
baseline, with a precision of 61.3% (5.2% higher
than the baseline), a recall of 65.9% (3.3% higher),
an F of 63.5 (4.3 higher) and a dependency accu-
racy of 78.3% (3.4% higher). The average number
of crossing brackets is 1.7 (0.4 lower), with 42.4%
of the parsed sentences having no crossing brack-
ets (8.2% higher), and 77.1% having two or fewer
crossing brackets (8.1% higher). When testing the
inclusion entity model on the random set, the per-
formance is very similar to the baseline model on
this data. While coverage is the same, F and cross-
brackting scores are marginally improved, and the
dependency score is marginally deteriorated. This
shows that the inclusion entity model does not harm
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 10  20  30  40  50  60  70  80
A
ve
ra
ge
 T
ok
en
 F
re
qu
en
cy
Sentence Length in Tokens
Inclusion sample
Stratified random sample
Figure 4: Average relative token frequencies for sen-
tences of equal length.
the parsing accuracy of sentences that do not actu-
ally contain foreign inclusions.
Not only did the inclusion entity parser perform
above the baseline on every metric for the inclusion
set, its performance also exceeds that of the perfect
tagging model on all measures except precision and
average crossing brackets, where both models are
tied. These results clearly indicate that the inclusion
entity model is able to leverage the additional infor-
mation about English inclusions provided by our in-
clusion classifier. However, it is also important to
note that the performance of this model on the in-
clusion set is still consistently lower than that of all
models on the random set. This demonstrates that
sentences with inclusions are more difficult to parse
than monolingual sentences, even in the presence of
information about the inclusions that the parser can
exploit.
Comparing the inclusion set to the length-
matched random set is arguably not entirely fair as
the latter may not contain as many infrequent tokens
as the inclusion set. Figure 4 shows the average rel-
ative token frequencies for sentences of equal length
for both sets. The frequency profiles of the two data
sets are broadly similar (the difference in means of
both groups is only 0.000676), albeit significantly
different according to a paired t-test (p? 0.05). This
is one reason why the inclusion entity model?s per-
formance on the inclusion set does not reach the up-
per limit set by the random sample.
157
Phrase cat. Frequency Example
PN 91 The Independent
CH 10 Made in Germany
NP 4 Peace Enforcement
CNP 2 Botts and Company
? 2 Chief Executives
Table 4: Gold phrase categories of inclusions.
5 Error Analysis
The error analysis is limited to 100 sentences se-
lected from the inclusion set parsed with both the
baseline and the inclusion entity model. This sam-
ple contains 109 English inclusions, five of which
are false positives, i.e., the output of the English in-
clusion classifier is incorrect. The precision of the
classifier in recognizing multi-word English inclu-
sions is therefore 95.4% for this TIGER sample.
Table 4 illustrates that the majority of multi-word
English inclusions are contained in a proper noun
(PN ) phrase, including names of companies, politi-
cal parties, organizations, films, newspapers, etc. A
less frequent phrasal category is chunk (CH ) which
tends to be used for slogans, quotes or expressions
like Made in Germany. Even in this small sam-
ple, annotations of inclusions as either PN or CH ,
and not the other, can be misleading. For example,
the organization Friends of the Earth is annotated
as a PN , whereas another organization International
Union for the Conservation of Nature is marked as
a CH in the gold standard. This suggests that the
annotation guidelines on foreign inclusions could be
improved when differentiating between phrase cate-
gories containing foreign material.
For the majority of sentences (62%), the baseline
model predicts more brackets than are present in the
gold standard parse tree (see Table 5). This number
decreases by 11% to 51% when parsing with the in-
clusion entity model. This suggests that the baseline
parser does not recognize English inclusions as con-
stituents, and instead parses their individual tokens
as separate phrases. Provided with additional infor-
mation of multi-word English inclusions in the train-
ing data, the parser is able to overcome this problem.
We now turn our attention to how accurately the
various parsers are at predicting both phrase brack-
eting and phrase categories (see Table 6). For 46
Phrase bracket (PB) frequency BL IE
PBPRED > PBGOLD 62% 51%
PBPRED < PBGOLD 11% 13%
PBPRED = PBGOLD 27% 36%
Table 5: Bracket frequency of the predicted baseline
(BL) and inclusion entity (IE) model output com-
pared to the gold standard.
(42.2%) of inclusions, the baseline model makes an
error with a negative effect on performance. In 39
cases (35.8%), the phrase bracketing and phrase cat-
egory are incorrect, and constituent boundaries oc-
cur within the inclusion, as illustrated in Figure 5(a).
Such errors also have a detrimental effect on the
parsing of the remainder of the sentence. Overall,
the baseline model predicts the correct phrase brack-
eting and phrase category for 63 inclusions (57.8%).
Conversely, the inclusion entity model, which is
given information on tag consistency within inclu-
sions via the FOM tags, is able to determine the
correct phrase bracketing and phrase category for
67.9% inclusions (10.1% more), e.g. see Figure 5(b).
Both the phrase bracketing and phrase category are
predicted incorrectly in only 6 cases (5.5%). The
inclusion entity model?s improved phrase boundary
prediction for 31 inclusions (28.4% more correct) is
likely to have an overall positive effect on the pars-
ing decisions made for the context which they ap-
pear in. Nevertheless, the inclusion entity parser still
has difficulty determining the correct phrase cate-
gory in 25 cases (22.9%). The main confusion lies
between assigning the categories PN , CH and NP ,
the most frequent phrase categories of multi-word
English inclusions. This is also partially due to the
ambiguity between these phrases in the gold stan-
dard. Finally, few parsing errors (4) are caused by
the inclusion entity parser due to the markup of false
positive inclusions (mainly boundary errors).
6 Discussion and Conclusion
This paper has argued that English inclusions in
German text is an increasingly pervasive instance
of language mixing. Starting with the hypothesis
that such inclusions can be a significant source of
errors for monolingual parsers, we found evidence
that an unmodified state-of-the-art parser for Ger-
158
...
PN-NK
NP-PNC
NE-NK
Made
PP-MNR
APPR-AD
In
NE-NK
Heaven
(a) Partial parsing output of the baseline model with a con-
stiuent boundary in the English inclusion.
...
PN-NK
FOM
Made
FOM
In
FOM
Heaven
(b) Partial parsing output of the inclusion en-
tity model with the English inclusion parsed cor-
rectly.
Figure 5: Comparing baseline model output to inclusion entity model output.
Errors No. of inclusions (in %)
Parser: baseline model, data: inclusion set
Incorrect PB and PC 39 (35.8%)
Incorrect PC 5 (4.6%)
Incorrect PB 2 (1.8%)
Correct PB and PC 63 (57.8%)
Parser: inclusion entity model, data: inclusion set
Incorrect PB and PC 6 (5.5%)
Incorrect PC 25 (22.9%)
Incorrect PB 4 (3.7%)
Correct PB and PC 74 (67.9%)
Table 6: Baseline and inclusion entity model errors
for inclusions with respect to their phrase bracketing
(PB) and phrase category (PC).
man performs substantially worse on a set of sen-
tences with English inclusions compared to a set of
length-matched sentences randomly sampled from
the same corpus. The lower performance on the
inclusion set persisted even when the parser when
given gold standard POS tags in the input.
To overcome the poor accuracy of parsing inclu-
sions, we developed two methods for interfacing the
parser with an existing annotation-free inclusion de-
tection system. The first method restricts the POS
tags for inclusions that the parser can assign to those
found in the data. The second method applies tree
transformations to ensure that inclusions are treated
as phrases. An evaluation on the TIGER corpus
shows that the second method yields a performance
gain of 4.3 in F-score over a baseline of no inclusion
detection, and even outperforms a model involving
perfect POS tagging of inclusions.
To summarize, we have shown that foreign inclu-
sions present a problem for a monolingual parser.
We also demonstrated that it is insufficient to know
where inclusions are or even what their parts of
speech are. Parsing performance only improves if
the parser also has knowledge about the structure of
the inclusions. It is particularly important to know
when adjacent foreign words are likely to be part of
the same phrase. As our error analysis showed, this
prevents cascading errors further up in the parse tree.
Finally, our results indicate that future work could
improve parsing performance for inclusions further:
we found that parsing the inclusion set is still harder
than parsing a randomly sampled test set, even for
our best-performing model. This provides an up-
per bound on the performance we can expect from
a parser that uses inclusion detection. Future work
will also involve determining the English inclusion
classifier?s merit when applied to rule-based parsing.
Acknowledgements
This research is supported by grants from the Scot-
tish Enterprise Edinburgh-Stanford Link (R36759),
ESRC, and the University of Edinburgh. We would
also like to thank Claire Grover for her comments
and feedback.
159
References
Steven Abney, Dan Flickenger, Claudia Gdaniec, Ralph
Grishman, Philip Harrison, Donald Hindle, Robert In-
gria, Frederick Jelinek, Judith Klavans, Mark Liber-
man, Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Ezra Black, editor, HLT?91: Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 306?311, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Beatrice Alex. 2005. An unsupervised system for identi-
fying English inclusions in German text. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005), Student Re-
search Workshop, pages 133?138, Ann Arbor, Michi-
gan, USA.
Beatrice Alex. 2006. Integrating language knowledge
resources to extend the English inclusion classifier to
a new language. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006), Genoa, Italy.
Gisle Andersen. 2005. Assessing algorithms for auto-
matic extraction of Anglicisms in Norwegian texts. In
Corpus Linguistics 2005, Birmingham, UK.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories (TLT02), pages 24?41, So-
zopol, Bulgaria.
Amit Dubey. 2005a. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Amit Dubey. 2005b. What to do when lexicalization
fails: parsing German with suffix analysis and smooth-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pages 314?321, Ann Arbor, Michigan, USA.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Martin Forst and Ronald M. Kaplan. 2006. The impor-
tance of precise tokenizing for deep grammars. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006), pages
369?372, Genoa, Italy.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), pages 423?430,
Saporo, Japan.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
(CoNLL-03), pages 180?183, Edmonton, Canada.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI-95), pages 1420?1425, Montreal,
Canada.
Jean-Christophe Marcadet, Volker Fischer, and Claire
Waast-Richard. 2005. A transformation-based learn-
ing approach to language identification for mixed-
lingual text-to-speech synthesis. In Proceedings of
Interspeech 2005 - ICSLP, pages 2249?2252, Lisbon,
Portugal.
Beat Pfister and Harald Romsdorfer. 2003. Mixed-
lingual analysis for polyglot TTS synthesis. In
Proceedings of Eurospeech 2003, pages 2037?2040,
Geneva, Switzerland.
Ariel Schwartz and Marti Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB 2003), pages 451?462,
Kauai, Hawaii.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper text. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC 1998), pages 705?712, Granada, Spain.
Boye Wangensteen. 2002. Nettbasert nyordsinnsamling.
Spra?knytt, 2:17?19.
160
Modelling Semantic Role Plausibility in Human Sentence Processing
Ulrike Pad? and Matthew Crocker
Computational Linguistics
Saarland University
66041 Saarbr?cken
Germany
{ulrike,crocker}@coli.uni-sb.de
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
We present the psycholinguistically moti-
vated task of predicting human plausibility
judgements for verb-role-argument triples
and introduce a probabilistic model that
solves it. We also evaluate our model on
the related role-labelling task, and com-
pare it with a standard role labeller. For
both tasks, our model benefits from class-
based smoothing, which allows it to make
correct argument-specific predictions de-
spite a severe sparse data problem. The
standard labeller suffers from sparse data
and a strong reliance on syntactic cues, es-
pecially in the prediction task.
1 Introduction
Computational psycholinguistics is concerned
with modelling human language processing.
Much work has gone into the exploration of sen-
tence comprehension. Syntactic preferences that
unfold during the course of the sentence have been
successfully modelled using incremental proba-
bilistic context-free parsing models (e.g., Jurafsky,
1996; Crocker and Brants, 2000). These models
assume that humans prefer the most likely struc-
tural alternative at each point in the sentence. If
the preferred structure changes during processing,
such models correctly predict processing difficulty
for a range of experimentally investigated con-
structions. They do not, however, incorporate an
explicit notion of semantic processing, while there
are many phenomena in human sentence process-
ing that demonstrate a non-trivial interaction of
syntactic preferences and semantic plausibility.
Consider, for example, the well-studied case of
reduced relative clause constructions. When incre-
mentally processing the sentence The deer shot by
the hunter was used as a trophy, there is a local
ambiguity at shot between continuation as a main
clause (as in The deer shot the hunter) or as a re-
duced relative clause modifying deer (equivalent
to The deer which was shot . . . ). The main clause
continuation is syntactically more likely.
However, there is a second, semantic clue pro-
vided by the high plausibility of deer being shot
and the low plausibility of them shooting. This
influences readers to choose the syntactically dis-
preferred reduced relative reading which interprets
the deer as an object of shot (McRae et al, 1998).
Plausibility has overridden the syntactic default.
On the other hand, for a sentence like The hunter
shot by the teenager was only 30 years old, se-
mantic plausibility initially reinforces the syntac-
tic main clause preference and readers show diffi-
culty accommodating the subsequent disambigua-
tion towards the reduced relative.
In order to model effects like these, we need
to extend existing models of sentence process-
ing by introducing a semantic dimension. Pos-
sible ways of integrating different sources of in-
formation have been presented e.g. by McRae
et al (1998) and Narayanan and Jurafsky (2002).
Our aim is to formulate a model that reliably pre-
dicts human plausibility judgements from corpus
resources, in parallel to the standard practice of
basing the syntax component of psycholinguistic
models on corpus probabilities or even probabilis-
tic treebank grammars. We can then use both the
syntactic likelihood and the semantic plausibility
score to predict the preferred syntactic alterna-
tive, thus accounting for the effects shown e.g. by
McRae et al (1998).
Independent of a syntactic model, we want any
semantic model we define to satisfy two criteria:
First, it needs to be able to make predictions in-
345
crementally, in parallel with the syntactic model.
This entails dealing with incomplete or unspeci-
fied (syntactic) information. Second, we want to
extend to semantics the assumption made in syn-
tactic models that the most probable alternative is
the one preferred by humans. The model therefore
must be probabilistic.
We present such a probabilistic model that can
assign roles incrementally as soon as a predicate-
argument pair is seen. It uses the likelihood of the-
matic role assignments to model human interpre-
tation of verb-argument relations. Thematic roles
are a description of the link between verb and ar-
gument at the interface between syntax and se-
mantics. Thus, they provide a shallow level of
sentence semantics which can be learnt from an-
notated corpora.
We evaluate our model by verifying that it in-
deed correctly predicts human judgements, and by
comparing its performance with that of a standard
role labeller in terms of both judgement prediction
and role assignment. Our model has two advan-
tages over the standard labeller: It does not rely
on syntactic features (which can be hard to come
by in an incremental task) and our smoothing ap-
proach allows it to make argument-specific role
predictions in spite of extremely sparse training
data. We conclude that (a) our model solves the
task we set, and (b) our model is better equipped
for our task than a standard role labeller.
The outline of the paper is as follows: After
defining the prediction task more concretely (Sec-
tion 2), we present our simple probabilistic model
that is tailoured to the task (Section 3). We in-
troduce our test and training data in Section 4. It
becomes evident immediately that we face a se-
vere sparse data problem, which we tackle on two
levels: By smoothing the distribution and by ac-
quiring additional counts for sparse cases. The
smoothed model succeeds on the prediction task
(Section 5). Finally, in Section 6, we compare our
model to a standard role labeller.
2 The Judgement Prediction Task
We can measure our intuitions about the plau-
sibility of hunters shooting and deer being shot
in terms of plausibility judgements for verb-role-
argument triples. Two example items from McRae
et al (1998) are presented in Table 1. The judge-
ments were gathered by asking raters to assign a
value on a scale from 1 (not plausible) to 7 (very
Verb Noun Role Rating
shoot hunter agent 6.9
shoot hunter patient 2.8
shoot deer agent 1.0
shoot deer patient 6.4
Table 1: Test items: Verb-noun pairs with ratings
on a 7 point scale from McRae et al (1998).
plausible) to questions like How common is it for
a hunter to shoot something? (subject reading:
hunter must be agent) or How common is it for a
hunter to be shot? (object reading: hunter must be
patient). The number of ratings available in each
of our three sets of ratings is given in Table 2 (see
also Section 4).
The task for our model is to correctly predict the
plausibility of each verb-role-argument triple. We
evaluate this by correlating the model?s predicted
values and the judgements. The judgement data
is not normally distributed, so we correlate using
Spearman?s ? (a non-parametric rank-order test).
The ? value ranges between 0 and 1 and indicates
the strength of association between the two vari-
ables. A significant positive value indicates that
the model?s predictions are accurate.
3 A Model of Human Plausibility
Judgements
We can formulate a model to solve the prediction
task if we equate the plausibility of a role assign-
ment to a verb-argument pair with its probability,
as suggested above. This value is influenced as
well by the verb?s semantic class and the grammat-
ical function of the argument. The plausibility for
a verb-role-argument triple can thus be estimated
as the joint probability of the argument head a, the
role r, the verb v, the verb?s semantic class c and
the grammatical function g f of a:
Plausibilityv,r,a = P(r,a,v,c,g f )
This joint probability cannot be easily estimated
from co-occurrence counts due to lack of data.
But we can decompose this term into a number
of subterms that approximate intuitively impor-
tant information such as syntactic subcategorisa-
tion (P(g f |v,c)), the syntactic realisation of a se-
mantic role (P(r|v,c,g f )) and selectional prefer-
ences (P(a|v,c,g f ,r)):
Plausibilityv,r,a = P(r,a,v,c,g f ) =
P(v) ?P(c|v) ?P(g f |v,c) ?
P(r|v,c,g f ) ?P(a|v,c,g f ,r)
346
shoot.02: [The hunter Arg0] shot [the deer Arg1].
Killing: [The hunter Killer] shot [the deer Victim].
Figure 1: Example annotation: PropBank (above)
and FrameNet (below).
Each of these subterms can be estimated more eas-
ily from the semantically annotated training data
simply using the maximum likelihood estimate.
However, we still need to smooth our estimates,
especially as the P(a|v,c,g f ,r) term remains very
sparse. We describe our use of two complemen-
tary smoothing methods in Section 5.
Our model fulfils the requirements we have
specified: It is probabilistic, able to work incre-
mentally as soon as a single verb-argument pair
is available, and can make predictions even if the
input information is incomplete. The model gen-
erates the missing values if, e.g., the grammatical
function or the verb?s semantic class are not spec-
ified. This means that we can immediately evalu-
ate on the judgement data without needing further
verb sense or syntactic information.
4 Test and Training data
Training Data To date, there are two main
annotation efforts that have produced semanti-
cally annotated corpora: PropBank (PB) and
FrameNet (FN). Their approaches to annotation
differ enough to warrant a comparison of the cor-
pora as training resources. Figure 1 gives an exam-
ple sentence annotated in PropBank and FrameNet
style. The PropBank corpus (c. 120,000 propo-
sitions, c. 3,000 verbs) adds semantic annotation
to the Wall Street Journal part of the Penn Tree-
bank. Arguments and adjuncts are annotated for
every verbal proposition in the corpus. A common
set of argument labels Arg0 to Arg5 and ArgM
(adjuncts) is interpreted in a verb-specific way.
Some consistency in mapping has been achieved,
so that Arg0 generally denotes agents and Arg1
patients/themes.
The FrameNet corpus (c. 58,000 verbal propo-
sitions, c. 1,500 verbs in release 1.1) groups verbs
with similar meanings together into frames (i.e.
descriptions of situations) with a set of frame-
specific roles for participants and items involved
(e.g. a killer, instrument and victim in the Killing
frame). Both the definition of frames as semantic
verb classes and the semantic characterisation of
frame-specific roles introduces a level of informa-
tion that is not present in PropBank. Since corpus
annotation is frame-driven, only some senses of a
verb may be present and word frequencies may not
be representative of English.
Test Data Our main data set consists of 160 data
points from McRae et al (1998) that were split
randomly into a 60 data point development set and
a 100 data point test set. The data is made up of
two arguments per verb and two ratings for each
verb-argument pair, one for the subject and one
for the object reading of the argument (see Section
2). Each argument is highly plausible in one of
the readings, but implausible in the other (recall
Table 1). Human ratings are on a 7-point scale.
In order to further test the coverage of our
model, we also include 76 items from Trueswell
et al (1994) with one highly plausible object per
verb and a rating each for the subject and object
reading of the argument. The data were gath-
ered in the same rating study as the McRae et
al. data, so we can assume consistency of the rat-
ings. However, in comparison to the McRae data
set, the data is impoverished as it lacks ratings for
plausible agents (in terms of the example in Ta-
ble 1, this means there are no ratings for hunter).
Lastly, we use 180 items from Keller and Lapata
(2003). In contrast with the previous two studies,
the verbs and nouns for these data were not hand-
selected for the plausibility of their combination.
Rather, they were extracted from the BNC corpus
by frequency criteria: Half the verb-noun combi-
nations are seen in the BNC with high, medium
and low frequency, half are unseen combinations
of the verb set with nouns from the BNC. The
data consists of ratings for 30 verbs and 6 argu-
ments each, interpreted as objects. The human
ratings were gathered using the Magnitude Esti-
mation technique (Bard et al, 1996). This data
set alows us to test on items that were not hand-
selected for a psycholinguistic study, even though
the data lacks agenthood ratings and the items are
poorly covered by the FrameNet corpus.
All test pairs were hand-annotated with
FrameNet and PropBank roles following the
specifications in the FrameNet on-line database
and the PropBank frames files.1
The judgement prediction task is very hard to
solve if the verb is unseen during training. Back-
ing off to syntactic information or a frequency
1Although a single annotator assigned the roles, the anno-
tation should be reliable as roles were mostly unambiguous
and the annotated corpora were used for reference.
347
Total Revised
Source FN PB
McRae et al (1998) 100 64 (64%) 92 (92%)
Trueswell et al (1994) 76 52 (68.4%) 72 (94.7%)
Keller and Lapata (2003) 180 ? 162 (90%)
Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen
verbs (% of total ratings). ?: Coverage too low (26.7%).
baseline only works if the role set is small and syn-
tactically motivated, which is the case for Prop-
Bank, but not FrameNet. We present results both
for the complete test sets and and for revised sets
containing only items with seen verbs. Exclud-
ing unseen verbs seems justified for FrameNet and
has little effect for the PropBank corpus, since its
coverage is generally much better. Table 2 shows
the total number of ratings for each test set and
the sizes of the revised test sets containing only
items with seen verbs. FrameNet alays has sub-
stantially lower coverage. Since only 27% of the
verbs in the Keller & Lapata items are covered in
FrameNet, we do not test this combination.
5 Experiment 1: Smoothing Methods
We now turn to evaluating our model. It is im-
mediately clear that we have a severe sparse data
problem. Even if all the verbs are seen, the com-
binations of verbs and arguments are still mostly
unseen in training for all data sets.
We describe two complementary approaches
to smoothing sparse training data. One, Good-
Turing smoothing, approaches the problem of un-
seen data points by assigning them a small proba-
bility. The other, class-based smoothing, attempts
to arrive at semantic generalisations for words.
These serve to identify equivalent verb-argument
pairs that furnish additional counts for the estima-
tion of P(a|v,c,g f ,r).
5.1 Good-Turing Smoothing and Linear
Interpolation
We first tackle the sparse data problem by smooth-
ing the distribution of co-occurrence counts. We
use the Good-Turing re-estimate on zero and one
counts to assign a small probability to unseen
events. This method relies on re-estimating the
probability of seen and unseen events based on
knowledge about more frequent events.
Adding Linear Interpolation We also exper-
imented with the linear interpolation method,
which is typically used for smoothing n-gram
models. It re-estimates the probability of the n-
gram in question as a weighted combination of the
n-gram, the n-1-gram and the n-2-gram. For ex-
ample, P(a|v,c,g f ,r) is interpolated as
P(a|v,c,g f ,r) = ?1P(a|v,c,g f ,r)+
?2P(a|v,c,r)+?3P(a|v,c)
The ? values were estimated on the training
data, separately for each of the model?s four con-
ditional probability terms, by maximising five-fold
cross-validation likelihood to avoid overfitting.
We smoothed all model terms using the Good-
Turing method and then interpolated the smoothed
terms. Table 3 lists the test results for both train-
ing corpora and all test sets when Good-Turing
smoothing (GT) is used alone and with linear in-
terpolation (GT/LI). We also give the unsmoothed
coverage and correlation. The need for smoothing
is obvious: Coverage is so low that we can only
compute correlations in two cases, and even for
those, less than 20% of the data are covered.
GT smoothing alone always outperforms the
combination of GT and LI smoothing, especially
for the FrameNet training set. Maximising the
data likelihood during ? estimation does not ap-
proximate our final task well enough: The log
likelihood of the test data is duly improved from
?797.1 to ?772.2 for the PropBank data and from
?501.9 to ?446.3 for the FrameNet data. How-
ever, especially for the FrameNet training data,
performance on the correlation task diminishes as
data probability rises. A better solution might be
to use the correlation task directly as a ? estima-
tion criterion, but this is much more complex, re-
quiring us to estimate all ? terms simultaneously.
Also, the main problem seems to be that the ? in-
terpolation smoothes by de-emphasising the most
specific (and sparsest) term, so that, on our final
task, the all-important argument-specific informa-
tion is not used efficiently when it is available. We
therefore restrict ourselves to GT smoothing.
348
Smoothed Unsmoothed
Train Smoothing Test Coverage ? Coverage ?
PB
GT
McRae 93.5% (86%) 0.112, ns 2% (2%) ?
Trueswell 100% (94.7%) 0.454, ** 17% (16%) ns
Keller&Lapata 100% (90%) 0.285, ** 5% (4%) 0.727, *
GT/LI
McRae 93.5% (86%) 0.110, ns 2% (2%) ?
Trueswell 100% (94.7%) 0.404, ** 17% (16%) ns
Keller&Lapata 100% (90%) 0.284, ** 5% (4%) 0.727, *
FN
GT McRae 87.5% (56%) 0.164, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.046, ns 6% (4%) ?
GT/LI McRae 87.5% (56%) 0.042, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.009, ns 6% (4%) ?
Table 3: Experiment 1, GT and Interpolation smoothing. Coverage on seen verbs (and on all items) and
correlation strength (Spearman?s ? for PB and FN data on all test sets. ?: too few data points, ns: not
significant, *: p < 0.05, **: p < 0.01.
Model Performance Both versions of the
smoothed model make predictions for all seen
verbs; the remaining uncovered data points are
those where the correct role is not accounted for
in the training data (the verb may be very sparse
or only seen in a different FrameNet frame). For
the FrameNet training data, there are no significant
correlations, but for the PropBank data, we see
correlations for the Trueswell and Keller&Lapata
sets. One reason for the good performance of
the PB-Trueswell and PB-Keller&Lapata combi-
nations is that in the PropBank training data, the
object role generally seems to be the most likely
one. If the most specific probability term is sparse
and expresses no role preference (which is the case
for most items: see Unsmoothed Coverage), our
model is biased towards the most likely role given
the verb, semantic class and grammatical function.
Recall that the Trueswell and Keller&Lapata data
contain ratings for (plausible) objects only, so that
preferring the patient role is a good strategy. This
also explains why the model performs worse for
the McRae et al data, which also has ratings for
good agents (and bad patients). On FrameNet, this
preference for ?patient? roles is not as marked, so
the FN-Trueswell case does not behave like the
PB-Trueswell case.
5.2 Class-Based Smoothing
In addition to smoothing the training distribution,
we also attempt to acquire more counts to es-
timate each P(a|v,c,g f ,r) by generalising from
tokens to word classes. The term we estimate
becomes P(classa|classv,g f ,r). This allows us
to make argument-specific predictions as we do
not rely on a uniform smoothed term for unseen
P(a|v,c,g f ,r) terms. We use lexicographic noun
classes from WordNet and verb classes induced
by soft unsupervised clustering, which outperform
lexicographic verb classes.
Noun Classes We tested both the coarsest and
the finest noun classification available in Word-
Net, namely the top-level ontology and the noun
synsets which contain only synonyms of the target
word.2 The top-level ontology proved to overgen-
erate alternative nouns, which raises coverage but
does not produce meaningful role predictions. We
therefore use the noun synsets below.
Verb Classes Verbs are clustered according to
linguistic context information, namely argument
head lemmas, the syntactic configuration of verb
and argument, the verb?s semantic class, the gold
role information and a combined feature of gold
role and syntactic configuration. The evaluation of
the clustering task itself is task-based: We choose
the clustering configuration that produces optimal
results in the the prediction task on the McRae de-
velopment set. The base corpus for clustering was
always used for frequency estimation.
We used an implementation of two soft clus-
tering algorithms derived from information the-
ory (Marx, 2004): the Information Distortion (ID)
(Gedeon et al, 2003) and Information Bottleneck
(IB) (Tishby et al, 1999) methods. Soft cluster-
ing allows us to take verb polysemy into account
that is often characterised by different patterns of
syntactic behaviour for each verb meaning.
2For ambiguous nouns, we chose the sense that led to the
highest probability for the current role assignment.
349
A number of parameters were set on the devel-
opment set, namely the clustering algorithm, the
smoothing method within the algorithms and the
number of clusters within each run. For our task,
the IB algorithm generally yielded better results.
We decided which clustering parametrisations
should be tried on the test sets based on the notion
of stability: Both algorithms increase the number
of clusters by one at each iteration. Thus, each
parametrisation yields a series of cluster configu-
rations as the number of iterations increases. We
chose those parametrisations where a series of at
least three consecutive cluster configurations re-
turned significant correlations on the development
set. This should be an indication of a generalisable
success, rather than a fluke caused by peculiarities
of the data. On the test sets, results are reported
for the configuration (characterised by the itera-
tion number) that returned the first significant re-
sult in such a series on the development set, as this
is the most general grouping.
5.3 Combining the Smoothing Methods
We now present results for combining the GT
and class-based smoothing methods. We use in-
duced verb classes and WordNet noun synsets for
class-based smoothing of P(a|v,c,g f ,r), and rely
on GT smoothing if the counts for this term are
still sparse. All other model terms are always
smoothed using the GT method. Table 4 contains
results for three clustering configurations each for
the PropBank and FrameNet data that have proven
stable on the development set. We characterise
them by the clustering algorithm (IB or ID) and
number of clusters. Note that the upper bound for
our ? values, human agreement or inter-rater cor-
relation, is below 1 (as indicated by a correlation
of Pearson?s r = .640 for the seen pairs from the
Keller and Lapata (2003) data).
For the FrameNet data, there is a marked in-
crease in performance for both test sets. The hu-
man judgements are now reliably predicted with
good coverage in five out of six cases. Clearly,
equivalent verb-argument counts have furnished
accurate item-specific estimates. On the PropBank
data set, class-based smoothing is less helpful: ?
values generally drop slightly. Apparently, the
FrameNet style of annotation allows us to induce
informative verb classes, whereas the PropBank
classes introduce noise at most.
6 Experiment 2: Role Labelling
We have shown that our model performs well on
its intended task of predicting plausibility judge-
ments, once we have proper smoothing methods
in place. But since this task has some similarity
to role labelling, we can also compare the model
to a standard role labeller on both the prediction
and role labelling tasks. The questions are: How
well do we do labelling, and does a standard role
labeller also predict human judgements?
Beginning with work by Gildea and Jurafsky
(2002), there has been a large interest in se-
mantic role labelling, as evidenced by its adop-
tion as a shared task in the Senseval-III compe-
tition (FrameNet data, Litkowski, 2004) and at
the CoNLL-2004 and 2005 conference (PropBank
data, Carreras and M?rquez, 2005). As our model
currently focuses on noun phrase arguments only,
we do not adopt these test sets but continue to use
ours, defining the correct role label to be the one
with the higher probability judgement. We evalu-
ate the model on the McRae test set (recall that the
other sets only contain good patients/themes and
are therefore susceptible to labeller biases).
We formulate frequency baselines for our train-
ing data. For PropBank, always assigning Arg1
results in F = 45.7 (43.8 on the full test set). For
FrameNet, we assign the most frequent role given
the verb, so the baseline is F = 34.4 (26.8).
We base our standard role labelling system on
the SVM labeller described in Giuglea and Mos-
chitti (2004), although without integrating infor-
mation from PropBank and VerbNet for FrameNet
classification as presented in their paper. Thus, we
are left with a set of fairly standard features, such
as phrase type, voice, governing category or path
through parse tree from predicate. These are used
to train two classifiers, one which decides which
phrases should be considered arguments and one
which assigns role labels to these arguments. The
SVM labeller?s F score on an unseen test set is
F = 80.5 for FrameNet data when using gold ar-
gument boundaries. We also trained the labeller
on the PropBank data, resulting in an F score of
F = 98.6 on Section 23, again on gold boundaries.
We also evaluate the SVM labeller on the cor-
relation task by normalising the scores that the la-
beller assigns to each role and then correlating the
normalised scores to the human ratings.
In order to extract features for the SVM labeller,
we had to present the verb-noun pairs in full sen-
350
Train Test Verb Clusters Coverage ?
PB
McRae
ID 4 93.5% (86%) 0.097, ns
IB 10 93.5% (86%) 0.104, ns
IB 5 93.5% (86%) 0.107, ns
Trueswell
ID 4 100% (94.7%) 0.419, **
IB 10 100% (94.7%) 0.366, **
IB 5 100% (94.7%) 0.439, **
Keller&Lapata
ID 4 100% (90%) 0.300, **
IB 10 100% (90%) 0.255, **
IB 5 100% (90%) 0.297, **
FN
McRae
ID 4 87.5% (56%) 0.304, *
IB 9 87.5% (56%) 0.275, *
IB 10 87.5% (56%) 0.267, *
Trueswell
ID 4 76.9% (52.6%) 0.256, ns
IB 9 76.9% (52.6%) 0.342, *
IB 10 76.9% (52.6%) 0.365, *
Table 4: Experiment 1: Combining the smoothing methods. Coverage on seen verbs (and on all items)
and correlation strength (Spearman?s ?) for PB and FN data. WN synsets as noun classes. Verb classes:
IB/ID: smoothing algorithm, followed by number of clusters. ns: not significant, *: p<0.05, **: p<0.01
tences, as the labeller relies on a number of fea-
tures from parse trees. We used the experimental
items from the McRae et al study, which are all
disambiguated towards a reduced relative reading
(object interpretation: The hunter shot by the ...)
of the argument. In doing this, we are potentially
biasing the SVM labeller towards one label, de-
pending on the influence of syntactic features on
role assignment. We therefore also created a main
clause reading of the verb-argument pairs (sub-
ject interpretation: The hunter shot the ...) and
present the results for comparison. For our model,
we have previously not specified the grammatical
function of the argument, but in order to put both
models on a level playing field, we now supply the
grammatical function of Ext (external argument),
which applies for both formulations of the items.
Table 5 shows that for the labelling task, our
model outperforms the labelling baseline and the
SVM labeller on the FrameNet data by at least
16 points F score while the correlation with hu-
man data remains significant. For the PropBank
data, labelling performance is on baseline level,
below the better of the two SVM labeller condi-
tions. This result underscores the usefulness of
argument-specific plausibility estimates furnished
by class-based smoothing for the FrameNet data.
For the PropBank data, our model essentially as-
signs the most frequent role for the verb.
The performance of the SVM labeller suggests
a strong influence of syntactic features: On the
PropBank data set, it always assigns the Arg0 la-
bel if the argument was presented as a subject
(this is correct in 50% of cases) and mostly the
appropriate ArgN label if the argument was pre-
sented as an object. On FrameNet, performance
again is above baseline only for the subject condi-
tion, where there is also a clear trend for assign-
ing agent-style roles. (The object condition is less
clear-cut.) This strong reliance on syntactic cues,
which may be misleading for our data, makes the
labeller perform much worse than on the standard
test sets. For both training corpora, it does not
take word-specific plausibility into account due to
data sparseness and usually assigns the same role
to both arguments of a verb. This precludes a sig-
nificant correlation with the human ratings.
Comparing the training corpora, we find that
both models perform better on the FrameNet data
even though there are many more role labels in
FrameNet, and the SVM labeller does not profit
from the greater smoothing power of FrameNet
verb clusters. Overall, FrameNet has proven more
useful to us, despite its smaller size.
In sum, our model does about as well (PB data)
or better (FN data) on the labelling task as the
SVM labeller, while the labeller does not solve
the prediction task. The success of our model, es-
pecially on the prediction task, stems partly from
the absence of global syntactic features that bias
the standard labeller strongly. This also makes our
model suited for an incremental task. Instead of
351
Train Model Coverage ? Labelling F Labelling Cov.
PB
Baseline ? ? 45.7 (43.8%) 100%
SVM Labeller (subj) 100% (92%) ns 50 (47.9%) 100%
SVM Labeller (obj) 100% (92%) ns 45.7 (43.8%) 100%
IB 5 (subj/obj) 93.5% (86%) ns 45.7 (43.8%) 100%
FN
Baseline ? ? 34.4 (26.8%) 100%
SVM Labeller (subj) 87.5% (56%) ns 40.6 (31.7%) 100%
SVM Labeller (obj) 87.5% (56%) ns 34.4 (26.8%) 100%
ID 4 (subj/obj) 87.5% (56%) 0.271, * 56.3 (43.9%) 100%
Table 5: Experiment 2: Standard SVM labeller vs our model. Coverage on seen verbs (and on all items),
correlation strength (Spearman?s ?), labelling F score and labelling coverage on seen verbs (and on all
items, if different) for PB and FN data on the McRae test set. ns: not significant, *: p<0.05.
syntactic cues, we successfully rely on argument-
specific plausibility estimates furnished by class-
based smoothing. Our joint probability model has
the further advantage of being conceptually much
simpler than the SVM labeller, which relies on a
sophisticated machine learning paradigm. Also,
we need to compute only about one-fifth of the
number of SVM features.
7 Conclusions
We have defined the psycholinguistically moti-
vated task of predicting human plausibility ratings
for verb-role-argument triples. To solve it, we
have presented an incremental probabilistic model
of human plausibility judgements. When we em-
ploy two complementary smoothing methods, the
model achieves both good coverage and reliable
correlations with human data. Our model per-
forms as well as or better than a standard role la-
beller on the task of assigning the preferred role to
each item in our test set. Further, the standard la-
beller does not succeed on the prediction task, as it
cannot overcome the extreme sparse data problem.
Acknowledgements Ulrike Pad? acknowledges
a DFG studentship in the International Post-
Graduate College ?Language Technology and
Cognitive Systems?. We thank Ana-Maria Giu-
glea, Alessandro Moschitti and Zvika Marx for
making their software available and are grateful to
Amit Dubey, Katrin Erk, Mirella Lapata and Se-
bastian Pad? for comments and discussions.
References
Bard, E. G., Robertson, D., and Sorace, A. (1996). Magnitude
estimation of linguistic acceptability. Language, 72(1),
32?68.
Carreras, X. and M?rquez, L. (2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2005.
Crocker, M. and Brants, T. (2000). Wide-coverage proba-
bilistic sentence processing. Journal of Psycholinguistic
Research, 29(6), 647?669.
Gedeon, T., Parker, A., and Dimitrov, A. (2003). Information
distortion and neural coding. Canadian Applied Mathe-
matics Quarterly, 10(1), 33?70.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3), 245?
288.
Giuglea, A.-M. and Moschitti, A. (2004). Knowledge discov-
ery using FrameNet, VerbNet and PropBank. In Proceed-
ings of the Workshop on Ontology and Knowledge Discov-
ering at ECML 2004.
Jurafsky, D. (1996). A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20,
137?194.
Keller, F. and Lapata, M. (2003). Using the web to obtain fre-
quencies for unseen bigrams. Computational Linguistics,
29(3), 459?484.
Litkowski, K. (2004). Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Marx, Z. (2004). Structure-Based computational aspects of
similarity and analogy in natural language. Ph.D. thesis,
Hebrew University, Jerusalem.
McRae, K., Spivey-Knowlton, M., and Tanenhaus, M.
(1998). Modeling the influence of thematic fit (and other
constraints) in on-line sentence comprehension. Journal
of Memory and Language, 38, 283?312.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian model
predicts human parse preference and reading time in sen-
tence processing. In S. B. T. G. Dietterich and Z. Ghahra-
mani, editors, Advances in Neural Information Processing
Systems 14, pages 59?65. MIT Press.
Tishby, N., Pereira, F. C., and Bialek, W. (1999). The in-
formation bottleneck method. In Proc. of the 37-th An-
nual Allerton Conference on Communication, Control and
Computing, pages 368?377.
Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994). Seman-
tic influences on parsing: Use of thematic role information
in syntactic ambiguity resolution. Journal of Memory and
Language, 33, 285?318.
352
c? 2003 Association for Computational Linguistics
Using the Web to Obtain Frequencies for
Unseen Bigrams
Frank Keller? Mirella Lapata?
University of Edinburgh University of Sheffield
This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen
in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun,
and verb-object bigrams from the Web by querying a search engine. We evaluate this method
by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a
reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correla-
tion between Web frequencies and frequencies recreated using class-based smoothing; (d) a good
performance of Web frequencies in a pseudodisambiguation task.
1. Introduction
In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP
algorithms are typically optimized, tested, and compared on fairly small data sets (cor-
pora with millions of words), even though data sets several orders of magnitude larger
are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment
with context-sensitive spelling correction, a task for which large amounts of data can
be obtained straightforwardly, as no manual annotation is required. They demonstrate
that the learning algorithms typically used for spelling correction benefit significantly
from larger training sets, and that their performance shows no sign of reaching an
asymptote as the size of the training set increases.
Arguably, the largest data set that is available for NLP is the Web,1 which currently
consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide
enormous potential for training NLP algorithms, if Banko and Brill?s (2001a, 2001b)
findings for spelling corrections generalize; potential applications include tasks that
involve word n-grams and simple surface syntax. There is a small body of existing re-
search that tries to harness the potential of the Web for NLP. Grefenstette and Nioche
(2000) and Jones and Ghani (2000) use the Web to generate corpora for languages
for which electronic resources are scarce, and Resnik (1999) describes a method for
mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and
Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001)
proposes a method for resolving PP attachment ambiguities based on Web data, Mark-
ert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora,
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk
? Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK.
E-mail: mlap@dcs.shef.ac.uk
1 A reviewer points out that information providers such as Lexis Nexis ?http://www.lexisnexis.com/?
might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news
sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case
law, codes, regulations, legal news, and law reviews).
2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine
Showdown ?http://www.searchengineshowdown.com/?.
460
Computational Linguistics Volume 29, Number 3
and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language
modeling.
A particularly interesting application is proposed by Grefenstette (1998), who uses
the Web for example-based machine translation. His task is to translate compounds
from French into English, with corpus evidence serving as a filter for candidate transla-
tions. An example is the French compound groupe de travail. There are five translations
of groupe and three translations for travail (in the dictionary that Grefenstette [1998]
is using), resulting in 15 possible candidate translations. Only one of them, namely,
work group, has a high corpus frequency, which makes it likely that this is the correct
translation into English. Grefenstette (1998) observes that this approach suffers from
an acute data sparseness problem if the counts are obtained from a conventional cor-
pus. However, as Grefenstette (1998) demonstrates, this problem can be overcome by
obtaining counts through Web searches, instead of relying on a corpus. Grefenstette
(1998) therefore effectively uses the Web as a way of obtaining counts for compounds
that are sparse in a given corpus.
Although this is an important initial result, it raises the question of the generality
of the proposed approach to overcoming data sparseness. It remains to be shown that
Web counts are generally useful for approximating data that are sparse or unseen
in a given corpus. It seems possible, for instance, that Grefenstette?s (1998) results
are limited to his particular task (filtering potential translations) or to his particular
linguistic phenomenon (noun-noun compounds). Another potential problem is the fact
that Web counts are far more noisy than counts obtained from a well-edited, carefully
balanced corpus. The effect of this noise on the usefulness of the Web counts is largely
unexplored.
Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling.
They obtain a standard language model from a 103-million-word corpus and employ
Web-based counts to interpolate unreliable trigram estimates. They compare their in-
terpolated model against a baseline trigram language model (without interpolation)
and show that the interpolated model yields an absolute reduction in word error rate
of .93% over the baseline. Zhu and Rosenfeld?s (2001) results demonstrate that the
Web can be a source of data for language modeling. It is not clear, however, whether
their result carries over to tasks that employ linguistically meaningful word sequences
(e.g., head-modifier pairs or predicate-argument tuples) rather than simply adjacent
words. Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that eval-
uate Web frequencies directly (i.e., without a task such as language modeling). This
could be done, for instance, by comparing Web frequencies to corpus frequencies, or
to frequencies re-created by smoothing techniques.
The aim of the present article is to generalize Grefenstette?s (1998) and Zhu and
Rosenfeld?s (2001) findings by testing the hypothesis that the Web can be employed
to obtain frequencies for bigrams that are unseen in a given corpus. Instead of hav-
ing a particular task in mind (which would introduce a sampling bias), we rely on
sets of bigrams that are randomly selected from a corpus. We use a Web-based ap-
proach for bigrams that encode meaningful syntactic relations and obtain Web fre-
quencies not only for noun-noun bigrams, but also for adjective-noun and verb-object
bigrams. We thus explore whether this approach generalizes to different predicate-
argument combinations. We evaluate our Web counts in four ways: (a) comparison
with actual corpus frequencies from two different corpora, (b) comparison with human
plausibility judgments, (c) comparison with frequencies re-created using class-based
smoothing, and (d) performance in a pseudodisambiguation task on data sets from the
literature.
461
Keller and Lapata Web Frequencies for Unseen Bigrams
2. Obtaining Frequencies from the Web
2.1 Sampling Bigrams from the BNC
The data sets used in the present experiment were obtained from the British National
Corpus (BNC) (see Burnard [1995]). The BNC is a large, synchronic corpus, consisting
of 90 million words of text and 10 million words of speech. The BNC is a balanced
corpus (i.e., it was compiled so as to represent a wide range of present day British
English). The written part includes samples from newspapers, magazines, books (both
academic and fiction), letters, and school and university essays, among other kinds of
text. The spoken part consists of spontaneous conversations, recorded from volunteers
balanced by age, region, and social class. Other samples of spoken language are also
included, ranging from business or government meetings to radio shows and phone-
ins. The corpus represents many different styles and varieties and is not limited to
any particular subject field, genre, or register.
For the present study, the BNC was used to extract data for three types of predicate-
argument relations. The first type is adjective-noun bigrams, in which we assume
that the noun is the predicate that takes the adjective as its argument.3 The second
predicate-argument type we investigated is noun-noun compounds. For these, we
assume that the rightmost noun is the predicate that selects the leftmost noun as
its argument (as compound nouns are generally right-headed in English). Third, we
included verb-object bigrams, in which the verb is the predicate that selects the object
as its argument. We considered only direct NP objects; the bigram consists of the verb
and the head noun of the object. For each of the three predicate-argument relations,
we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur in
the BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC).
For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, and
Keller (1999), who compiled a set of 90 bigrams as follows. First, 30 adjectives were
randomly chosen from a part-of-speech-tagged and lemmatized version of the BNC
so that each adjective had exactly two senses according to WordNet (Miller et al 1990)
and was unambiguously tagged as ?adjective? 98.6% of the time. Lapata, McDonald,
and Keller used the part-of-speech-tagged version that is made available with the BNC
and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic part-
of-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of
the corpus was obtained using Karp et al?s (1992) morphological analyzer.
The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words;
that is, they covered the whole range from fairly infrequent to highly frequent items.
Gsearch (Corley et al 2001), a chart parser that detects syntactic patterns in a tagged
corpus by exploiting a user-specified context-free grammar and a syntactic query, was
used to extract all nouns occurring in a head-modifier relationship with one of the
30 adjectives. Examples of the syntactic patterns the parser identified are given in Ta-
ble 1. In the case of adjectives modifying compound nouns, only sequences of two
nouns were included, and the rightmost-occurring noun was considered the head.
Bigrams involving proper nouns or low-frequency nouns (less than 10 per million
words) were discarded. This was necessary because the bigrams were used in exper-
iments involving native speakers (see Section 3.2), and we wanted to reduce the risk
of including words unfamiliar to the experimental subjects. For each adjective, the set
of bigrams was divided into three frequency bands based on an equal division of the
3 This assumption is disputed in the theoretical linguistics literature. For instance, Pollard and Sag (1994)
present an analysis in which there is mutual selection between the noun and the adjective.
462
Computational Linguistics Volume 29, Number 3
Table 1
Example of patterns used for the extraction of adjective-noun bigrams.
Pattern Example
A N educational material
A Adv N usual weekly classes
A N N environmental health officers
range of log-transformed co-occurrence frequencies. Then one bigram was chosen at
random from each band. This procedure ensures that the whole range of frequencies
is represented in our sample.
Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-noun
bigrams using the same 30 adjectives. For each adjective, Gsearch was used to com-
pile a list of all nouns that did not co-occur in a head-modifier relationship with the
adjective. Again, proper nouns and low-frequency nouns were discarded from this
list. Then each adjective was paired with three randomly chosen nouns from its list
of non-co-occurring nouns. Examples of seen and unseen adjective-noun bigrams are
shown in Table 2.
For the present study, we applied the procedure used by Lapata, McDonald, and
Keller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and to
verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type
of predicate-argument relationship. More specifically, 30 nouns and 30 verbs were
chosen according to the same criteria proposed for the adjective study (i.e., minimal
sense ambiguity and unambiguous part of speech). All nouns modifying one of the
30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks
for consecutive pairs of nouns that are neither preceded nor succeeded by another
Table 2
Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams
(with log-transformed BNC counts).
Adjective-Noun Bigrams
Adjective High Medium Low Unseen
hungry animal 1.79 pleasure 1.38 application 0 tradition, innovation, prey
guilty verdict 3.91 secret 2.56 cat 0 system, wisdom, wartime
naughty girl 2.94 dog 1.6 lunch .69 regime, rival, protocol
Noun-Noun Bigrams
High Medium Low Unseen Head Noun
process 1.14 user .95 gala 0 collection, clause, coat directory
television 1.53 satellite .95 edition 0 chain, care, vote broadcast
plasma 1.78 nylon 1.20 unit .60 fund, theology, minute membrane
Verb-Object Bigrams
Verb High Medium Low Unseen
fulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, grade
intensify problem 1.79 effect 1.10 alarm 0 score, quota, chest
choose name 3.74 law 1.61 series 1.10 lift, bride, listener
463
Keller and Lapata Web Frequencies for Unseen Bigrams
noun. Lauer?s heuristic (see (1)) effectively avoids identifying as two-word compounds
noun sequences that are part of a larger compound.
(1) C = {(w2, w3) | w1w2w3w4; w1, w4 ? N; w2, w3 ? N}
Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is the
set of words tagged as nouns in the corpus. C is the set of compounds identified by
Lauer?s (1995) heuristic.
Verb-object bigrams for the 30 preselected verbs were obtained from the BNC
using Cass (Abney 1996), a robust chunk parser designed for the shallow analysis
of noisy text. The parser recognizes chunks and simplex clauses (i.e., sequences of
nonrecursive clauses) using a regular expression grammar and a part-of-speech-tagged
corpus, without attempting to resolve attachment ambiguities. It comes with a large-
scale grammar for English and a built-in tool that extracts predicate-argument tuples
out of the parse trees that Cass produces.
The parser?s output was postprocessed to remove bracketing errors and errors in
identifying chunk categories that could potentially result in bigrams whose members
do not stand in a verb-argument relationship. Tuples containing verbs or nouns at-
tested in a verb-argument relationship only once were eliminated. Particle verbs were
retained only if the particle was adjacent to the verb (e.g., come off heroin). Verbs fol-
lowed by the preposition by and a head noun were considered instances of verb-subject
relations. It was assumed that PPs adjacent to the verb headed by any of the preposi-
tions in, to, for, with, on, at, from, of, into, through, and upon were prepositional objects (see
Lapata [2001] for details on the filtering process). Only nominal heads were retained
from the objects returned by the parser. As in the adjective study, noun-noun bigrams
and verb-object bigrams with proper nouns or low-frequency nouns (less than 10 per
million words) were discarded. The sets of noun-noun and verb-object bigrams were
divided into three frequency bands, and one bigram was chosen at random from each
band.
The procedure described by Lapata, Keller, and McDonald (2001) was followed for
creating sets of unseen noun-noun and verb-object bigrams: for each noun or verb, we
compiled a list of all nouns with which it did not co-occur within a noun-noun or verb-
object bigram in the BNC. Again, Lauer?s (1995) heuristic and Abney?s (1996) partial
parser were used to identify bigrams, and proper nouns and low-frequency nouns
were excluded. For each noun and verb, three bigrams were formed by pairing it with
a noun randomly selected from the set of the non-co-occurring nouns for that noun
or verb. Table 2 lists examples for the seen and unseen noun-noun and verb-object
bigrams generated by this procedure.
The extracted bigrams are in several respects an imperfect source of information
about adjective-noun or noun-noun modification and verb-object relations. First notice
that both Gsearch and Cass detect syntactic patterns on part-of-speech-tagged corpora.
This means that parsing errors are likely to result because of tagging mistakes. Second,
even if one assumes perfect tagging, the heuristic nature of our extraction procedures
may introduce additional noise or miss bigrams for which detailed structural infor-
mation would be needed.
For instance, our method for extracting adjective-noun pairs ignores cases in which
the adjective modifies noun sequences of length greater than two. The heuristic in (1)
considers only two-word noun sequences. Abney?s (1996) chunker recognizes basic
syntactic units without resolving attachment ambiguities or recovering missing infor-
mation (such as traces resulting from the movement of constituents). Although pars-
ing is robust and fast (since unlike in traditional parsers, no global optimization takes
464
Computational Linguistics Volume 29, Number 3
place), the identified verb-argument relations are undoubtedly somewhat noisy, given
the errors inherent in the part-of-speech tagging and chunk recognition procedure.
When evaluated against manually annotated data, Abney?s (1996) parser identified
chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word
accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk
length identified correctly).
Despite their imperfect output, heuristic methods for the extraction of syntactic
relations are relatively common in statistical NLP. Several statistical models employ
frequencies obtained from the output of partial parsers and other heuristic methods;
these include models for disambiguating the attachment site of prepositional phrases
(Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns
(Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the
induction of selectional preferences (Abney and Light 1999), methods for automati-
cally clustering words according to their distribution in particular syntactic contexts
(Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994;
Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee
1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways
for obtaining bigram frequencies that are potentially useful for such models despite
the fact that some of these bigrams are identified in a heuristic manner and may be
noisy.
2.2 Sampling Bigrams from the NANTC
We also obtained corpus counts from a second corpus, the North American News
Text Corpus (NANTC). This corpus differs in several important respects from the
BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not
a balanced corpus, as it contains material from only one genre, namely, news text.
However, the text originates from a variety of sources (Los Angeles Times, Washington
Post, New York Times News Syndicate, Reuters News Service, and Wall Street Journal).
Whereas the BNC covers British English, the NANTC covers American English. All
these differences mean that the NANTC provides a second, independent standard
against which to compare Web counts. At the same time the correlation found between
the counts obtained from the two corpora can serve as an upper limit for the correlation
that we can expect between corpus counts and Web counts.
The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverage
parser for English. MINIPAR employs a manually constructed grammar and a lexicon
derived from WordNet with the addition of proper names (130,000 entries in total).
Lexicon entries contain part-of-speech and subcategorization information. The gram-
mar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic [dependency] relationships). MINIPAR employs a distributed-
chart parsing algorithm. Instead of a single chart, each node in the grammar network
maintains a chart containing partially built structures belonging to the grammatical
category represented by the node. Grammar rules are implemented as constraints as-
sociated with the nodes and edges.
The output of MINIPAR is a dependency tree that represents the dependency
relations between words in a sentence. Table 3 shows a subset of the dependencies
MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch
and Cass, MINIPAR produces all possible parses for a given sentence. The parses are
ranked according to the product of the probabilities of their edges, and the most likely
parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson
1995), a domain-independent corpus of British English, and achieved a recall of 79%
and precision of 89% on the dependency relations.
465
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 3
Examples of dependencies generated by MINIPAR for The fat cat ate the door mat.
Head Relation Modifier Description
cat N:det:Det the determiner of noun
cat N:mod:A fat adjective modifier of noun
eat V:subj:N cat subject of verb
eat V:obj:N mat object of verb
mat N:det:Det the determiner of noun
mat N:nn:N door prenominal modifier of noun
For our experiments, we concentrated solely on adjective-noun, noun-noun, and
verb object relations (denoted as N:mod:A, N:nn:N, and V:obj:N in Table 3). From the
syntactic analysis provided by the parser, we extracted all occurrences of bigrams that
were attested both in the BNC and the NANTC corpus. In this way, we obtained
NANTC frequency counts for the bigrams that we had randomly selected from the
BNC. Table 4 shows the NANTC counts for the set of seen bigrams from Table 2.
Because of the differences in the extraction methodology (chunking versus full
parsing) and the text genre (balanced corpus versus news text), we expected that
some BNC bigrams would not be attested in the NANTC corpus. More precisely, zero
frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun
bigrams. The fact that more zero frequencies were observed for noun-noun bigrams
than for the other two types is perhaps not surprising considering the ease with which
novel compounds are created (Levi 1978). We adjusted the zero counts by setting
them to .5. This was necessary because all further analyses were carried out on log-
transformed frequencies (see Table 4).
Table 4
Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams.
Adjective-Noun Bigrams
Adjective High Medium Low
hungry animal .90 pleasure -.30 application .60
guilty verdict 2.82 secret .95 cat -.30
naughty girl .69 dog -.30 lunch -.30
Noun-Noun Bigrams
High Medium Low Head Noun
process - .30 user -.30 gala -.30 directory
television 2.70 satellite -.30 edition -.30 broadcast
plasma - .30 nylon 0 unit 0 membrane
Verb-Object Bigrams
Verb High Medium Low
fulfill obligation 2.38 goal 2.04 scripture -.30
intensify problem 1.20 effect .60 alarm -.30
choose name 2.25 law .90 series .48
466
Computational Linguistics Volume 29, Number 3
2.3 Obtaining Web Counts
Web counts for bigrams were obtained using a simple heuristic based on queries to the
search engines AltaVista and Google. All search terms took into account the inflectional
morphology of nouns and verbs.
The search terms for verb-object bigrams matched not only cases in which the
object was directly adjacent to the verb (e.g., fulfill obligation), but also cases in which
there was an intervening determiner (e.g., fulfill the/an obligation). The following search
terms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively:
(2) "A N", where A is the adjective and N is the singular or plural form of the
noun.
(3) "N1 N2", where N1 is the singular form of the first noun and N2 is the
singular or plural form of the second noun.
(4) "V Det N", where V is the infinitive, singular present, plural present,
past, perfect, or gerund form of the verb, Det is the determiner the, the
determiner a, or the empty string, and N is the singular or plural form of
the noun.
Note that all searches were for exact matches, which means that the words in the search
terms had to be directly adjacent to score a match. This is encoded by enclosing the
search term in quotation marks. All our search terms were in lower case. We searched
the whole Web (as indexed by AltaVista and Google); that is, the queries were not
restricted to pages in English.
Based on the Web searches, we obtained bigram frequencies by adding up the
number of pages that matched the morphologically expanded forms of the search
terms (see the patterns in (2)?(4)). This process can be automated straightforwardly
using a script that generates all the search terms for a given bigram, issues an AltaVista
or Google query for each of the search terms, and then adds up the resulting number
of matches for each bigram. We applied this process to all the bigrams in our data set,
covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., a
set of 540 bigrams in total). The queries were carried out in January 2003 (and thus
the counts are higher than those reported in Keller, Lapata, and Ourioupina [2002],
which were generated about a year earlier).
For some bigrams that were unseen in the BNC, our Web-based procedure returned
zero counts; that is, there were no matches for those bigrams in the Web searches. It
is interesting to compare the Web and NANTC with respect to zero counts: Both
data sources are larger than the BNC and hence should be able to mitigate the data
sparseness problem to a certain extent. Table 5 provides the number of zero counts for
both Web search engines and compares them to the number of bigrams that yielded no
matches in the NANTC. We observe that the Web counts are substantially less sparse
than the NANTC counts: In the worst case, there are nine bigrams for which our Web
queries returned no matches (10% of the data), whereas up to 82 bigrams were unseen
in the NANTC (91% of the data). Recall that the NANTC is 3.5 times larger than the
BNC, which does not seem to be enough to substantially mitigate data sparseness. All
further analyses were carried out on log-transformed frequencies; hence we adjusted
zero counts by setting them to .5.
Table 6 shows descriptive statistics for the bigram counts we obtained using
AltaVista and Google. For comparison, this table also provides descriptive statis-
tics for the BNC and NANTC counts (for seen bigrams only) and for the counts
467
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 5
Number of zero counts returned by queries to search engines and in the NANTC (for bigrams
unseen in BNC).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 2 9 1
Google 2 5 0
NANTC 76 82 78
Table 6
Descriptive statistics for Web counts, corpus counts, and counts re-created using class-based
smoothing (log-transformed).
Adjective-Noun Noun-Noun Verb-Object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Seen Bigrams
AltaVista 1.15 5.84 3.72 1.02 .60 6.16 3.52 1.22 .48 5.86 3.42 1.13
Google 1.54 6.11 4.01 1.01 .90 6.30 3.80 1.23 .60 5.96 3.70 1.11
BNC 0 2.19 .89 .69 0 2.14 .74 .64 0 2.55 .68 .58
NANTC - .30 2.84 .84 .96 - .30 3.02 .56 .94 -.30 3.73 1.90 .98
Smoothing - .06 2.32 1.28 .51 - .70 1.71 .30 .61 -.51 2.07 .53 .57
Unseen Bigrams
AltaVista - .30 5.00 1.50 .99 - .30 3.97 1.20 1.14 - .30 3.88 1.55 1.06
Google - .30 4.11 1.79 .95 - .30 4.15 1.60 1.12 0 4.19 1.90 1.04
Smoothing - .03 2.10 1.25 .46 -1.01 1.93 .28 .66 -.70 1.95 .53 .58
Table 7
Average factor by which Web counts are larger than BNC counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 665 691 550
Google 1,306 1,151 1,064
re-created using class-based smoothing (see Section 3.3 for details on the re-created
frequencies).
From these data, we computed the average factor by which the Web counts are
larger than the BNC counts. The results are given in Table 7 and indicate that the
AltaVista counts are between 550 and 691 times larger than the BNC counts, and that
the Google counts are between 1,064 and 1,306 times larger than the BNC counts.
As we know the size of the BNC (100 million words), we can use these figures to
estimate the number of words available on the Web: between 55.0 and 69.1 billion
words for AltaVista, and between 106.4 and 139.6 billion words for Google. These
estimates are in the same order of magnitude as Grefenstette and Nioche?s (2000)
estimate that 48.1 billion words of English are available on the Web (based on AltaVista
counts compiled in February 2000). They also agree with Zhu and Rosenfeld?s (2001)
estimate that the effective size of the Web is between 79 and 108 billion words (based
on AltaVista, Lycos, and FAST counts; no date given).
468
Computational Linguistics Volume 29, Number 3
2.4 Potential Sources of Noise in Web Counts
The method we used to retrieve Web counts is based on very simple heuristics; it is
thus inevitable that the counts generated will contain a certain amount of noise. In
this section we discuss a number of potential sources of such noise.
An obvious limitation of our method is that it relies on the page counts returned by
the search engines; we do not download the pages themselves for further processing.
Note that many of the bigrams in our sample are very frequent (up to 106 matches;
see the ?Max? columns in Table 6), hence the effort involved in downloading all pages
would be immense (though methods for downloading a representative sample could
probably be devised).
Our approach estimates Web frequencies based not on bigram counts directly, but
on page counts. In other words, it ignores the fact that a bigram can occur more than
once on a given Web page. This approximation is justified, as Zhu and Rosenfeld (2001)
demonstrated for unigrams, bigrams, and trigrams: Page counts and n-gram counts
are highly correlated on a log-log scale. This result is based on Zhu and Rosenfeld?s
queries to AltaVista, a search engine that at the time of their research returned both
the number of pages and the overall number of matches for a given query.4
Another important limitation of our approach arises from the fact that both Google
and AltaVista disregard punctuation and capitalization, even if the search term is
placed within quotation marks. This can lead to false positives, for instance, if the
match crosses a phrase boundary, such as in (5), which matches hungry prey. Other
false positives can be generated by page titles and links, such as the examples (6)
and (7) which match edition broadcast.5
(5) The lion will kill only when it?s hungry. Prey can usually sense when
lions are hunting.
(6) 10th Edition Broadcast Products Catalog (as a page title)
(7) Issue/Edition/Broadcast (as a link)
The fact that our method does not download Web pages means that no tagging, chunk-
ing, or parsing can be carried out to ensure that the matches are correct. Instead we
rely on the simple adjacency of the search terms, which is enforced by using queries
enclosed within quotation marks (see Section 2.3 for details). This means that we miss
any nonadjacent matches, even though a chunker or parser (such as the one used for
extracting BNC or NANTC bigrams) would find them. An example is an adjective-
noun bigram in which an adverbial intervenes between the adjective and the noun
(see Table 1).
Furthermore, the absence of tagging, chunking, and parsing can also generate false
positives, in particular for queries containing words with part-of-speech ambiguity.
(Recall that our bigram selection procedure ensures that the predicate word, but not
the argument word, is unambiguous in terms of its POS tagging in the BNC.) As an
example, consider process directory, which in our data set is a noun-noun bigram (see
Table 2). One of the matches returned by Google is (8), in which process is a verb.
Another example is fund membrane, which is a noun-noun bigram in our data set but
matches (9) in Google.
4 Note that this feature of AltaVista has since been discontinued; hence in the present article we had no
option but to use page counts. However, Keller, Lapata, and Ourioupina (2002) used AltaVista match
counts (instead of page counts) on the same data sets; their results agree with the ones reported in the
present article very closely.
5 Some of the examples in (5)?(9) were kindly provided by a reviewer.
469
Keller and Lapata Web Frequencies for Unseen Bigrams
(8) The global catalog server?s function is to process directory searches for
the entire forest.
(9) Green grants fund membrane technology.
Another source of noise is the fact that Google (but not AltaVista) will sometimes
return pages that do not include the search term at all. This can happen if the search
term is contained in a link to the page (but not on the page itself).
As we did not limit our Web searches to English (even though many search engines
now allow the target language for a search to be set), there is also a risk that false posi-
tives are generated by cross-linguistic homonyms, that is, by words of other languages
that are spelled in the same way as the English words in our data sets. However, this
problem is mitigated by the fact that English is by far the most common language on
the Web, as shown by Grefenstette and Nioche (2000). Also, the chance of two such
homonyms forming a valid bigram in another language is probably fairly small.
To summarize, Web counts are certainly less sparse than the counts in a corpus of a
fixed size (see Section 2.3). However, Web counts are also likely to be significantly more
noisy than counts obtained from a carefully tagged and chunked or parsed corpus, as
the examples in this section show. It is therefore essential to carry out a comprehensive
evaluation of the Web counts generated by our method. This is the topic of the next
section.
3. Evaluation
3.1 Evaluation against Corpus Frequencies
Since Web counts can be relatively noisy, as discussed in the previous section, it is
crucial to determine whether there is a reliable relationship between Web counts and
corpus counts. Once this is assured, we can explore the usefulness of Web counts
for overcoming data sparseness. We carried out a correlation analysis to determine
whether there is a linear relationship between BNC and NANTC counts and AltaVista
and Google counts. All correlation coefficients reported in this article refer to Pear-
son?s r.6 All results were obtained on log-transformed counts.7
Table 8 shows the results of correlating Web counts with corpus counts from the
BNC, the corpus from which our bigrams were sampled (see Section 2.1). A high
correlation coefficient was obtained across the board, ranging from .720 to .847 for
AltaVista counts and from .720 to .850 for Google counts. This indicates that Web
counts approximate BNC counts for the three types of bigrams under investigation.
Note that there is almost no difference between the correlations achieved using Google
and AltaVista counts.
It is important to check that these results are also valid for counts obtained from
other corpora. We therefore correlated our Web counts with the counts obtained from
NANTC, a corpus that is larger than the BNC but is drawn from a single genre,
namely, news text (see Section 2.2). The results are shown in Table 9. We find that
6 Correlation analysis is a way of measuring the degree of linear association between two variables.
Effectively, we are fitting a linear equation y = ax + b to the data; this means that the two variables x
and y (which in our case represent frequencies or judgments) can still differ by a multiplicative
constant a and an additive constant b, even if they are highly correlated.
7 It is well-known that corpus frequencies have a Zipfian distribution. Log-transforming them is a way
of normalizing the counts before applying statistical tests. We apply correlation analysis on the
log-transformed data, which is equivalent to computing a log-linear regression coefficient on the
untransformed data.
470
Computational Linguistics Volume 29, Number 3
Table 8
Correlation of BNC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .847** .720** .762**
Google .850** .720** .766**
Smoothing .248* .277** .342**
*p < .05 (one-tailed). **p < .01 (one-tailed).
Table 9
Correlation of NANTC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .712** .667** .788**
Google .712** .662** .787**
BNC .710** .672** .814**
Smoothing .338** .317** .263*
*p < .05 (one-tailed). **p < .01 (one-tailed).
Google and AltaVista counts also correlate significantly with NANTC counts. The
correlation coefficients range from .667 to .788 for AltaVista and from .662 to .787 for
Google. Again, there is virtually no difference between the correlations for the two
search engines. We also observe that the correlation between Web counts and BNC is
generally slightly higher than the correlation between Web counts and NANTC counts.
We carried out one-tailed t-tests to determine whether the differences in the correlation
coefficients were significant. We found that both AltaVista counts (t(87) = 3.11, p < .01)
and Google counts (t(87) = 3.21, p < .01) were significantly better correlated with
BNC counts than with NANTC counts for adjective-noun bigrams. The difference in
correlation coefficients was not significant for noun-noun and verb-object bigrams, for
either search engine.
Table 9 also shows the correlations between BNC counts and NANTC counts.
The intercorpus correlation can be regarded as an upper limit for the correlations we
can expect between counts from two corpora that differ in size and genre and that
have been obtained using different extraction methods. The correlation between Al-
taVista and Google counts and NANTC counts reached the upper limit for all three
bigram types (one-tailed t-tests found no significant differences between the correla-
tion coefficients). The correlation between BNC counts and Web counts reached the
upper limit for noun-noun and verb-object bigrams (no significant differences for either
search engine) and significantly exceeded it for adjective-noun bigrams for AltaVista
(t(87) = 3.16, p < .01) and Google (t(87) = 3.26, p < .01).
We conclude that simple heuristics (see Section 2.3) are sufficient to obtain useful
frequencies from the Web; it seems that the large amount of data available for Web
counts outweighs the associated problems (noisy, unbalanced, etc.). We found that
Web counts were highly correlated with frequencies from two different corpora. Fur-
thermore, Web counts and corpus counts are as highly correlated as counts from two
different corpora (which can be regarded as an upper bound).
Note that Tables 8 and 9 also provide the correlation coefficients obtained when
corpus frequencies are compared with frequencies that were re-created through class-
471
Keller and Lapata Web Frequencies for Unseen Bigrams
based smoothing, using the BNC as a training corpus (after removing the seen bi-
grams). This will be discussed in more detail in Section 3.3.
3.2 Evaluation against Plausibility Judgments
Previous work has demonstrated that corpus counts correlate with human plausibility
judgments for adjective-noun bigrams. This result holds both for seen bigrams (La-
pata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been
re-created using smoothing techniques (Lapata, Keller, and McDonald 2001). Based on
these findings, we decided to evaluate our Web counts on the task of predicting plausi-
bility ratings. If the Web counts for bigrams correlate with plausibility judgments, then
this indicates that the counts are valid, in the sense of being useful for predicting the
intuitive plausibility of predicate-argument pairs. The degree of correlation between
Web counts and plausibility judgments is an indicator of the quality of the Web counts
(compared to corpus counts or counts re-created using smoothing techniques).
3.2.1 Method. For seen and unseen adjective-noun bigrams, we used the two sets of
plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata,
Keller, and McDonald (2001), respectively. We conducted four additional experiments
to collect judgments for noun-noun and verb-object bigrams, both seen and unseen.
The experimental method was the same for all six experiments.
Materials. The experimental stimuli were based on the six sets of seen or unseen
bigrams extracted from the BNC as described in Section 2.1 (adjective-noun, noun-
noun, and verb-object bigrams). In the adjective-noun and noun-noun cases, the stimuli
consisted simply of the bigrams. In the verb-object case, the bigrams were embedded
in a short sentence to make them more natural: A proper-noun subject was added.
Procedure. The experimental paradigm was magnitude estimation (ME), a tech-
nique standardly used in psychophysics to measure judgments of sensory stimuli
(Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have ap-
plied to the elicitation of linguistic judgments. The ME procedure requires subjects to
estimate the magnitude of physical stimuli by assigning numerical values proportional
to the stimulus magnitude they perceive. In contrast to the five- or seven-point scale
conventionally used to measure human intuitions, ME employs an interval scale and
therefore produces data for which parametric inferential statistics are valid.
ME requires subjects to assign numbers to a series of linguistic stimuli in a propor-
tional fashion. Subjects are first exposed to a modulus item, to which they assign an
arbitrary number. All other stimuli are rated proportional to the modulus. In this way,
each subject can establish his or her own rating scale, thus yielding maximally fine-
graded data and avoiding the known problems with the conventional ordinal scales
for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Schu?tze 1996).
The experiments reported in this article were carried out using the WebExp soft-
ware package (Keller et al 1998). A series of previous studies has shown that data
obtained using WebExp closely replicate results obtained in a controlled laboratory set-
ting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou
2001), coreference judgments (Keller and Asudeh 2001), and sentence completions
(Corley and Scheepers 2002).
In the present experiments, subjects were presented with bigram pairs and were
asked to rate the degree of plausibility proportional to a modulus item. They first saw a
set of instructions that explained the ME technique and the judgment task. The concept
of plausibility was not defined, but examples of plausible and implausible bigrams
were given (different examples for each stimulus set). Then subjects were asked to
fill in a questionnaire with basic demographic information. The experiment proper
472
Computational Linguistics Volume 29, Number 3
Table 10
Descriptive statistics for plausibility judgments (log-transformed). N is the number of subjects
used in each experiment.
Adjective-Noun Bigrams Noun-Noun Bigrams Verb-Object Bigrams
N Min Max Mean SD N Min Max Mean SD N Min Max Mean SD
Seen 30 -.85 .11 -.13 .22 25 -.15 .69 .40 .21 27 -.52 .45 .12 .24
Unseen 41 -.56 .37 -.07 .20 25 -.49 .52 -.01 .23 21 -.51 .28 -.16 .22
consisted of three phases: (1) a calibration phase, designed to familiarize subjects with
the task, in which they had to estimate the length of five horizontal lines; (2) a practice
phase, in which subjects judged the plausibility of eight bigrams (similar to the ones
in the stimulus set); (3) the main experiment, in which each subject judged one of the
six stimulus sets (90 bigrams). The stimuli were presented in random order, with a
new randomization being generated for each subject.
Subjects. A separate experiment was conducted for each set of stimuli. The number
of subjects per experiment is shown in Table 10 (in the column labeled N). All sub-
jects were self-reported native speakers of English; they were recruited by postings to
newsgroups and mailing lists. Participation was voluntary and unpaid.
WebExp collects by-item response time data; subjects whose response times were
very short or very long were excluded from the sample, as they are unlikely to have
completed the experiment adequately. We also excluded the data of subjects who had
participated more than once in the same experiment, based on their demographic data
and on their Internet connection data, which is logged by WebExp.
3.2.2 Results and Discussion. The experimental data were normalized by dividing
each numerical judgment by the modulus value that the subject had assigned to the
reference sentence. This operation creates a common scale for all subjects. Then the
data were transformed by taking the decadic logarithm. This transformation ensures
that the judgments are normally distributed and is standard practice for magnitude
estimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975). All
further analyses were conducted on the normalized, log-transformed judgments.
Table 10 shows the descriptive statistics for all six judgment experiments: the
original experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and
McDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun and
verb-object bigrams.
We used correlation analysis to compare corpus counts and Web counts with plau-
sibility judgments. Table 11 (top half) lists the correlation coefficients that were ob-
tained when correlating log-transformed Web counts (AltaVista and Google) and cor-
pus counts (BNC and NANTC) with mean plausibility judgments for seen adjective-
noun, noun-noun, and verb-object bigrams. The results show that both AltaVista and
Google counts correlate well with plausibility judgments for seen bigrams. The corre-
lation coefficient for AltaVista ranges from .641 to .700; for Google, it ranges from .624
to .692. The correlations for the two search engines are very similar, which is also what
we found in Section 3.1 for the correlations between Web counts and corpus counts.
Note that the Web counts consistently achieve a higher correlation with the judg-
ments than the BNC counts, which range from .488 to .569. We carried out a series
of one-tailed t-tests to determine whether the differences between the correlation co-
efficients for the Web counts and the correlation coefficients for the BNC counts were
significant. For the adjective-noun bigrams, the AltaVista coefficient was significantly
473
Keller and Lapata Web Frequencies for Unseen Bigrams
higher than the BNC coefficient (t(87) = 1.76, p < .05), whereas the difference between
the Google coefficient and the BNC coefficient failed to reach significance. For the
noun-noun bigrams, both the AltaVista and the Google coefficients were significantly
higher than the BNC coefficient (t(87) = 3.11, p < .01 and t(87) = 2.95, p < .01).
Also, for the verb-object bigrams, both the AltaVista coefficient and the Google coef-
ficient were significantly higher than the BNC coefficient (t(87) = 2.64, p < .01 and
t(87) = 2.32, p < .05).
A similar picture was observed for the NANTC counts. Again, the Web counts
outperformed the corpus counts in predicting plausibility. For the adjective-noun bi-
grams, both the AltaVista and the Google coefficient were significantly higher than the
NANTC coefficient (t(87) = 1.97, p < .05; t(87) = 1.81, p < .05). For the noun-noun bi-
grams, the AltaVista coefficient was higher than the NANTC coefficient (t(87) = 1.64,
p < .05), but the Google coefficient was not significantly different from the NANTC co-
efficient. For verb-object bigrams, the difference was significant for both search engines
(t(87) = 2.74, p < .01; t(87) = 2.38, p < .01).
In sum, for all three types of bigrams, the correlation coefficients achieved with
AltaVista were significantly higher than the ones achieved by either the BNC or the
NANTC. Google counts outperformed corpus counts for all bigrams with the exception
of adjective-noun counts from the BNC and noun-noun counts from the NANTC.
The bottom panel of Table 11 shows the correlation coefficients obtained by com-
paring log-transformed judgments with log-transformed Web counts for unseen adjec-
tive-noun, noun-noun, and verb-object bigrams. We observe that the Web counts con-
sistently show a significant correlation with the judgments, with the coefficient rang-
ing from .480 to .578 for AltaVista counts and from .473 to .595 for the Google counts.
Table 11 also provides the correlations between plausibility judgments and counts
re-created using class-based smoothing, which we will discuss in Section 3.3.
An important question is how well humans agree when judging the plausibility of
adjective-noun, noun-noun, and verb-noun bigrams. Intersubject agreement gives an
upper bound for the task and allows us to interpret how well our Web-based method
performs in relation to humans. To calculate intersubject agreement we used leave-
Table 11
Correlation of plausibility judgments with Web counts, corpus counts, and counts re-created
using class-based smoothing. ?Agreement? refers to the intersubject agreement on the
judgment task.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .650** .700** .641**
Google .641** .692** .624**
BNC .569** .517** .488**
NANTC .526** .597** .491**
Smoothing .329** .318** .223*
Agreement .630** .641** .604**
Unseen Bigrams
AltaVista .480** .578** .551**
Google .473** .595** .520**
Smoothing .342** .372** .298**
Agreement .550** .570** .640**
*p < .05 (one-tailed). **p < .01 (one-tailed).
474
Computational Linguistics Volume 29, Number 3
one-out resampling. This technique is a special case of n-fold cross-validation (Weiss
and Kulikowski 1991) and has been previously used for measuring how well humans
agree in judging semantic similarity (Resnik 1999, 2000).
For each subject group, we divided the set of the subjects? responses with size n
into a set of size n? 1 (i.e., the response data of all but one subject) and a set of size 1
(i.e., the response data of a single subject). We then correlated the mean ratings of the
former set with the ratings of the latter. This was repeated n times (see the number
of participants in Table 6); the mean of the correlation coefficients for the seen and
unseen bigrams is shown in Table 11 in the rows labeled ?Agreement.?
For both seen and unseen bigrams, we found no significant difference between the
upper bound (intersubject agreement) and the correlation coefficients obtained using
either AltaVista or Google counts. This finding holds for all three types of bigrams. The
same picture emerged for the BNC and NANTC counts: These correlation coefficients
were not significantly different from the upper limit, for all three types of bigrams,
both for seen and for unseen bigrams.
To conclude, our evaluation demonstrated that Web counts reliably predict human
plausibility judgments, both for seen and for unseen predicate-argument bigrams.
AltaVista counts for seen bigrams are a better predictor of human judgments than
BNC and NANTC counts. These results show that our heuristic method yields valid
frequencies; the simplifications we made in obtaining the Web counts (see Section 2.3),
as well as the fact that Web data are noisy (see Section 2.4), seem to be outweighed
by the fact that the Web is up to a thousand times larger than the BNC.
3.3 Evaluation against Class-Based Smoothing
The evaluation in the last two sections established that Web counts are useful for
approximating corpus counts and for predicting plausibility judgments. As a further
step in our evaluation, we correlated Web counts with counts re-created by applying
a class-based smoothing method to the BNC.
We re-created co-occurrence frequencies for predicate-argument bigrams using a
simplified version of Resnik?s (1993) selectional association measure proposed by La-
pata, Keller, and McDonald (2001). In a nutshell, this measure replaces Resnik?s (1993)
information-theoretic approach with a simpler measure that makes no assumptions
with respect to the contribution of a semantic class to the total quantity of information
provided by the predicate about the semantic classes of its argument. It simply substi-
tutes the argument occurring in the predicate-argument bigram with the concept by
which it is represented in the WordNet taxonomy. Predicate-argument co-occurrence
frequency is estimated by counting the number of times the concept corresponding
to the argument is observed to co-occur with the predicate in the corpus. Because
a given word is not always represented by a single class in the taxonomy (i.e., the
argument co-occurring with a predicate can generally be the realization of one of
several conceptual classes), Lapata, Keller, and McDonald (2001) constructed the fre-
quency counts for a predicate-argument bigram for each conceptual class by dividing
the contribution from the argument by the number of classes to which it belongs.
They demonstrate that the counts re-created using this smoothing technique correlate
significantly with plausibility judgments for adjective-noun bigrams. They also show
that this class-based approach outperforms distance-weighted averaging (Dagan, Lee,
and Pereira 1999), a smoothing method that re-creates unseen word co-occurrences on
the basis of distributional similarity (without relying on a predefined taxonomy), in
predicting plausibility.
In the current study, we used the smoothing technique of Lapata, Keller, and
McDonald (2001) to re-create not only adjective-noun bigrams, but also noun-noun
475
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 12
Correlation of counts re-created using class-based smoothing with Web counts.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .344** .362** .361**
Google .330** .343** .349**
Unseen Bigrams
AltaVista .439** .386** .412**
Google .444** .421** .397**
*p < .05 (one-tailed). **p < .01 (one-tailed).
and verb-object bigrams. As already mentioned in Section 2.1, it was assumed that the
noun is the predicate in adjective-noun bigrams; for noun-noun bigrams, we treated
the right noun as the predicate, and for verb-object bigrams, we treated the verb as the
predicate. We applied Lapata, Keller, and McDonald?s (2001) technique to the unseen
bigrams for all three bigram types. We also used it on the seen bigrams, which we
were able to treat as unseen by removing all instances of the bigrams from the training
corpus.
To test the claim that Web frequencies can be used to overcome data sparseness,
we correlated the frequencies re-created using class-based smoothing on the BNC with
the frequencies obtained from the Web. The correlation coefficients for both seen and
unseen bigrams are shown in Table 12. In all cases, a significant correlation between
Web counts and re-created counts is obtained. For seen bigrams, the correlation coef-
ficient ranged from .344 to .362 for AltaVista counts and from .330 to .349 for Google
counts. For unseen bigrams, the correlations were somewhat higher, ranging from .386
to .439 for AltaVista counts and from .397 to .444 for Google counts. For both seen
and unseen bigrams, there was only a very small difference between the correlation
coefficients obtained with the two search engines.
It is also interesting to compare the performance of class-based smoothing and Web
counts on the task of predicting plausibility judgments. The correlation coefficients are
listed in Table 11. The re-created frequencies are correlated significantly with all three
types of bigrams, both for seen and unseen bigrams. For the seen bigrams, we found
that the correlation coefficients obtained using smoothed counts were significantly
lower than the upper bound for all three types of bigrams (t(87) = 3.01, p < .01;
t(87) = 3.23, p < .01; t(87) = 3.43, p < .01). This result also held for the unseen
bigrams: The correlations obtained using smoothing were significantly lower than the
upper bound for all three types of bigrams (t(87) = 1.86, p < .05; t(87) = 1.97, p < .05;
t(87) = 3.36, p < .01).
Recall that the correlation coefficients obtained using the Web counts were not
found to be significantly different from the upper bound, which indicates that Web
counts are better predictors of plausibility than smoothed counts. This fact was con-
firmed by further significance testing: For seen bigrams, we found that the AltaVista
correlation coefficients were significantly higher than correlation coefficients obtained
using smoothing, for all three types of bigrams (t(87) = 3.31, p < .01; t(87) = 4.11,
p < .01; t(87) = 4.32, p < .01). This also held for Google counts (t(87) = 3.16, p < .01;
t(87) = 4.02, p < .01; t(87) = 4.03, p < .01). For unseen bigrams, the AltaVista coef-
ficients and the coefficients obtained using smoothing were not significantly different
476
Computational Linguistics Volume 29, Number 3
for adjective-noun bigrams, but the difference reached significance for noun-noun and
verb-object bigrams (t(87) = 2.08, p < .05; t(87) = 2.53, p < .01). For Google counts,
the difference was again not significant for adjective-noun bigrams, but it reached sig-
nificance for noun-noun and verb-object bigrams (t(87) = 2.34, p < .05; t(87) = 2.15,
p < .05).
Finally, we conducted a small study to investigate the validity of the counts that
were re-created using class-based smoothing. We correlated the re-created counts for
the seen bigrams with their actual BNC and NANTC frequencies. The correlation
coefficients are reported in Tables 8 and 9. We found that the correlation between re-
created counts and corpus counts was significant for all three types of bigrams, for
both corpora. This demonstrates that the smoothing technique we employed generates
realistic corpus counts, in the sense that the re-created counts are correlated with
the actual counts. However, the correlation coefficients obtained using Web counts
were always substantially higher than those obtained using smoothed counts. These
differences were significant for the BNC counts for AltaVista (t(87) = 8.38, p < .01;
t(87) = 5.00, p < .01; t(87) = 5.03, p < .01) and Google (t(87) = 8.35, p < .01;
t(87) = 5.00, p < .01; t(87) = 5.03, p < .01). They were also significant for the NANTC
counts for AltaVista (t(87) = 4.12, p < .01; t(87) = 3.72, p < .01; t(87) = 6.58, p < .01)
and Google (t(87) = 4.08, p < .01; t(87) = 3.06, p < .01; t(87) = 6.47, p < .01).
To summarize, the results presented in this section indicate that Web counts are
indeed a valid way of obtaining counts for bigrams that are unseen in a given corpus:
They correlate reliably with counts re-created using class-based smoothing. For seen
bigrams, we found that Web counts correlate with counts that were re-created using
smoothing techniques (after removing the seen bigrams from the training corpus). For
the task of predicting plausibility judgments, we were able to show that Web counts
outperform re-created counts, both for seen and for unseen bigrams. Finally, we found
that Web counts for seen bigrams correlate better than re-created counts with the real
corpus counts.
It is beyond the scope of the present study to undertake a full comparison between
Web counts and frequencies re-created using all available smoothing techniques (and
all available taxonomies that might be used for class-based smoothing). The smooth-
ing method discussed above is simply one type of class-based smoothing. Other, more
sophisticated class-based methods do away with the simplifying assumption that the
argument co-occurring with a given predicate (adjective, noun, verb) is distributed
evenly across its conceptual classes and attempt to find the right level of generalization
in a concept hierarchy, by discounting, for example, the contribution of very general
classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing ap-
proaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman
and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word com-
binations by exploiting only corpus-internal evidence, without relying on taxonomic
information. Our goal was to demonstrate that frequencies retrieved from the Web
are a viable alternative to conventional smoothing methods when data are sparse; we
do not claim that our Web-based method is necessarily superior to smoothing or that
it should be generally preferred over smoothing methods. However, the next section
will present a small-scale study that compares the performance of several smoothing
techniques with the performance of Web counts on a standard task from the literature.
3.4 Pseudodisambiguation
In the smoothing literature, re-created frequencies are typically evaluated using pseu-
dodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999;
Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al 1999).
477
Keller and Lapata Web Frequencies for Unseen Bigrams
The aim of the pseudodisambiguation task is to decide whether a given algorithm
re-creates frequencies that make it possible to distinguish between seen and unseen
bigrams in a given corpus. A set of pseudobigrams is constructed according to a set
of criteria (detailed below) that ensure that they are unattested in the training corpus.
Then the seen bigrams are removed from the training data, and the smoothing method
is used to re-create the frequencies of both the seen bigrams and the pseudobigrams.
The smoothing method is then evaluated by comparing the frequencies it re-creates
for both types of bigrams.
We evaluated our Web counts by applying the pseudodisambiguation procedure
that Rooth et al (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001)
employed for evaluating re-created verb-object bigram counts. In this procedure, the
noun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with a
randomly chosen verb v? that does not take n as its object within the corpus. This results
in an unseen verb-object bigram (v?, n). The seen bigram is now treated as unseen (i.e.,
all of its occurrences are removed from the training corpus), and the frequencies of
both the seen and the unseen bigram are re-created (using smoothing, or Web counts,
in our case). The task is then to decide which of the two verbs v and v? takes the
noun n as its object. For this, the re-created bigram frequency is used: The bigram
with the higher re-created frequency (or probability) is taken to be the seen bigram.
If this bigram is really the seen one, then the disambiguation is correct. The overall
percentage of correct disambiguations is a measure of the quality of the re-created
frequencies (or probabilities). In the following, we will first describe in some detail
the experiments that Rooth et al (1999) and Clark and Weir (2001) conducted. We
will then discuss how we replicated their experiments using the Web as an alternative
smoothing method.
Rooth et al (1999) used pseudodisambiguation to evaluate a class-based model
that is derived from unlabeled data using the expectation maximization (EM) algo-
rithm. From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carroll
and Rooth?s [1998] parser), they randomly selected 3,000 pairs, with each pair con-
taining a fairly frequent verb and noun (only verbs and nouns that occurred between
30 and 3,000 times in the data were considered). For each pair (v, n) a fairly frequent
verb v? was randomly chosen such that the pair (v?, n) did not occur in the data set.
Given the set of (v, n, v?) triples (a total of 1,337), the task was to decide whether (v, n)
or (v?, n) was the correct (i.e., seen) pair by comparing the probabilities P(n|v) and
P(n|v?). The probabilities were re-created using Rooth et al?s (1999) EM-based clus-
tering model on a training set from which all seen pairs (v, n) had been removed. An
accuracy of 80% on the pseudodisambiguation task was achieved (see Table 13).
Prescher, Riezler, and Rooth (2000) evaluated Rooth et al?s (1999) EM-based clus-
tering model again using pseudodisambiguation, but on a separate data set using a
Table 13
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Rooth et al 1999).
Data Set N AltaVista AltaVista Rooth et al
Conditional Probability Joint Probability
Subject 717 71.2 68.5 ?
Objects 620 85.2 77.5 ?
Subjects and objects 1,337 77.7 72.7 80.0
478
Computational Linguistics Volume 29, Number 3
Table 14
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Prescher, Riezler, and Rooth 2000).
Data Set N AltaVista AltaVista VA Model VO Model
Conditional Probability Joint Probability
Subjects 159 66.7 59.1 ? ?
Objects 139 70.5 66.2 ? ?
Subjects and objects 298 68.5 62.4 79.0 88.3
slightly different method for constructing the pseudobigrams. They used a set of 298
(v, n, n?) BNC triples in which (v, n) was chosen as in Rooth et al (1999) but paired with
a randomly chosen noun n?. Given the set of (v, n, n?) triples, the task was to decide
whether (v, n) or (v, n?) was the correct pair in each triple. Prescher, Riezler, and Rooth
(2000) reported pseudodisambiguation results with two clustering models: (1) Rooth
et al?s (1999) clustering approach, which models the semantic fit between a verb and
its argument (VA model), and (2) a refined version of this approach that models only
the fit between a verb and its object (VO model), disregarding other arguments of the
verb. The results of the two models on the pseudodisambiguation task are shown in
Table 14.
At this point, it is important to note that neither Rooth et al (1999) nor Prescher,
Riezler, and Rooth (2000) used pseudodisambiguation for the final evaluation of their
models. Rather, the performance on the pseudodisambiguation task was used to op-
timize the model parameters. The results in Tables 13 and 14 show the pseudodisam-
biguation performance achieved for the best parameter settings. In other words, these
results were obtained on the development set (i.e., on the same data set that was used
to optimize the parameters), not on a completely unseen test set. This procedure is
well-justified in the context of Rooth et al?s (1999) and Prescher, Riezler, and Rooth?s
(2000) work, which aimed at building models of lexical semantics, not of pseudodis-
ambiguation. Therefore, they carried out their final evaluations on unseen test sets for
the tasks of lexicon induction (Rooth et al 1999) and target language disambiguation
(Prescher, Riezler, and Rooth 2000), once the model parameters had been fixed using
the pseudodisambiguation development set.8
Clark and Weir (2002) use a setting similar to that of Rooth et al (1999) and
Prescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evalu-
ate the performance of a class-based probability estimation method. In order to address
the problem of estimating conditional probabilities in the face of sparse data, Clark
and Weir (2002) define probabilities in terms of classes in a semantic hierarchy and
propose hypothesis testing as a means of determining a suitable level of generalization
in the hierarchy. Clark and Weir (2002) report pseudodisambiguation results on two
data sets, with an experimental setup similar to that of Rooth et al (1999). For the first
data set, 3,000 pairs were randomly chosen from 1.3 million (v, n) tuples extracted
from the BNC (using the parser of Briscoe and Carroll [1997]). The selected pairs con-
8 Stefan Riezler (personal communication, 2003) points out that the main variance in Rooth et al?s (1999)
pseudodisambiguation results comes from the class cardinality parameter (start values account for only
2% of the performance, and iterations do not seem to make a difference at all). Figure 3 of Rooth et al
(1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes.
This indicates that a ?proper? pseudodisambiguation setting with separate development and test data
would have resulted in a similar choice of class cardinality and thus achieved the same 80%
performance that is cited in Table 13.
479
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 15
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using class-based smoothing (Clark and Weir 2002).
Data Set N AltaVista AltaVista Clark and Li and Resnik
Conditional Joint Probability Weir Abe
Probability
Objects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6
Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9
tained relatively frequent verbs (occurring between 500 and 5,000 times in the data).
The data sets were constructed as proposed by Rooth et al (1999). The procedure for
creating the second data set was identical, but this time only verbs that occurred be-
tween 100 and 1,000 times were considered. Clark and Weir (2002) further compared
their approach with Resnik?s (1993) selectional association model and Li and Abe?s
(1998) tree cut model on the same data sets. These methods are directly comparable,
as they can be used for class-based probability estimation and address the question
of how to find a suitable level of generalization in a hierarchy (i.e., WordNet). The
results of the three methods used on the two data sets are shown in Table 15.
We employed the same pseudodisambiguation method to test whether Web-based
frequencies can be used for distinguishing between seen and artificially constructed
unseen bigrams. We obtained the data sets of Rooth et al (1999), Prescher, Riezler,
and Rooth (2000), and Clark and Weir (2002) described above. Given a set of (v, n, v?)
triples, the task was to decide whether (v, n) or (v?, n) was the correct pair. We obtained
AltaVista counts for f (v, n), f (v?, n), f (v), and f (v?) as described in Section 2.3.9 Then
we used two models for pseudodisambiguation: the joint probability model compared
the joint probability estimates f (v, n) and f (v?, n) and predicted that the bigram with
the highest estimate is the seen one. The conditional probability model compared the
conditional probability estimates f (v, n)/f (v) and f (v?, n)/f (v?) and again selected as
the seen bigram the one with the highest estimate (in both cases, ties were resolved
by choosing at random).10 The same two models were used to perform pseudodisam-
biguation for the (v, n, n?) triples, where we have to choose between (v, n) and (v, n?).
Here, the probability estimates f (v, n) and f (v, n?) were used for the joint probability
model, and f (v, n)/f (n) and f (v, n?)/f (n?) for the conditional probability model.
The results for Rooth et al?s (1999) data set are given in Table 13. The conditional
probability model achieves a performance of 71.2% correct for subjects and 85.2% cor-
rect for objects. The performance on the whole data set is 77.7%, which is below the
performance of 80.0% reported by Rooth et al (1999). However, the difference is not
found to be significant using a chi-square test comparing the number of correct and
incorrect classifications (?2(1) = 2.02, p = .16). The joint probability model performs
consistently worse than the conditional probability model: It achieves an overall accu-
racy of 72.7%, which is significantly lower than the accuracy of the Rooth et al (1999)
model (?2(1) = 19.50, p < .01).
9 We used only AltaVista counts, as there was virtually no difference between AltaVista and Google
counts in our previous evaluations (see Sections 3.1?3.3). Google allows only 1,000 queries per day (for
registered users), which makes it time-consuming to obtain large numbers of Google counts. AltaVista
has no such restriction.
10 The probability estimates are P(a, b) = f (a, b)/N and P(a|b) = f (a, b)/f (a) for the joint probability and
the conditional probability, respectively. However, the corpus size N can be ignored, as it is constant.
480
Computational Linguistics Volume 29, Number 3
A similar picture emerges with regard to Prescher, Riezler, and Rooth?s (2000) data
set (see Table 14). The conditional probability model achieves an accuracy of 66.7%
for subjects and 70.5% for objects. The combined performance of 68.5% is significantly
lower than the performance of both the VA model (?2(1) = 7.78, p < .01) and the
VO model (?2(1) = 33.28, p < .01) reported by Prescher, Riezler, and Rooth (2000).
Again, the joint probability model performs worse than the conditional probability
model, achieving an overall accuracy of 62.4%.
We also applied our Web-based method to the pseudodisambiguation data set of
Clark and Weir (2002). Here, the conditional probability model reached a performance
of 83.9% correct on the low-frequency data set. This is significantly higher than the
highest performance of 72.4% reported by Clark and Weir (2002) on the same data
set (?2(1) = 115.50, p < .01). The joint probability model performs worse than the
conditional model, at 81.1%. However, this is still significantly better than the best
result of Clark and Weir (2002) (?2(1) = 63.14, p < .01). The same pattern is observed
for the high-frequency data set, on which the conditional probability model achieves
87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained
73.9% (?2(1) = 283.73, p < .01). The joint probability model achieved 85.3% on this data
set, also significantly outperforming Clark and Weir (2002) (?2(1) = 119.35, p < .01).
To summarize, we demonstrated that the simple Web-based approach proposed
in this article yields results for pseudodisambiguation that outperform class-based
smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998),
and Clark and Weir (2002). We were also able to show that a Web-based approach is
able to achieve the same performance as an EM-based smoothing model proposed by
Rooth et al (1999). However, the Web-based approach was not able to outperform the
more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another
result we obtained is that Web-based models that use conditional probabilities (where
unigram frequencies are used to normalize the bigram frequencies) generally outper-
form a more simple-minded approach that relies directly on bigram frequencies for
pseudodisambiguation.
There are a number of reasons why our results regarding pseudodisambiguation
have to be treated with some caution. First of all, the two smoothing methods (i.e.,
EM-based clustering and class-based probability estimation using WordNet) were not
evaluated on the same data set, and therefore the two results are not directly compa-
rable. For instance, Clark and Weir?s (2002) data set is substantially less noisy than
Rooth et al?s (1999) and Prescher, Riezler, and Rooth?s (2000), as it contains only words
and nouns that occur in WordNet. Furthermore, Stephen Clark (personal communica-
tion, 2003) points out that WordNet-based approaches are at a disadvantage when it
comes to pseudodisambiguation. Pseudodisambiguation assumes that the correct pair
is unseen in the training data; this makes the task deliberately hard, because some of
the pairs might be frequent enough that reliable corpus counts can be obtained with-
out having to use WordNet (using WordNet is likely to be more noisy than using the
actual counts). Another problem with WordNet-based approaches is that they offer
no systematic treatment of word sense ambiguity, which puts them at a disadvan-
tage with respect to approaches that do not rely on a predefined inventory of word
senses.
Finally, recall that the results for the EM-based approaches in Tables 13 and 14 were
obtained on the development set (as pseudodisambiguation was used as a means of
parameter tuning by Rooth et al [1999] and Prescher, Riezler, and Rooth [2000]). It is
possible that this fact inflates the performance values for the EM-based approaches
(but see note 8).
481
Keller and Lapata Web Frequencies for Unseen Bigrams
4. Conclusions
This article explored a novel approach to overcoming data sparseness. If a bigram
is unseen in a given corpus, conventional approaches re-create its frequency using
techniques such as back-off, linear interpolation, class-based smoothing or distance-
weighted averaging (see Dagan, Lee, and Pereira [1999] and Lee [1999] for overviews).
The approach proposed here does not re-create the missing counts but instead retrieves
them from a corpus that is much larger (but also much more noisy) than any existing
corpus: it launches queries to a search engine in order to determine how often the
bigram occurs on the Web.
We systematically investigated the validity of this approach by using it to obtain
frequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verb-
object bigrams). We first applied the approach to seen bigrams randomly sampled from
the BNC. We found that the counts obtained from the Web are highly correlated with
the counts obtained from the BNC. We then obtained bigram counts from NANTC,
a corpus that is substantially larger than the BNC. Again, we found that Web counts
are highly correlated with corpus counts. This indicates that Web queries can generate
frequencies that are comparable to the ones obtained from a balanced, carefully edited
corpus such as the BNC, but also from a large news text corpus such as NANTC.
Secondly, we performed an evaluation that used the Web frequencies to predict
human plausibility judgments for predicate-argument bigrams. The results show that
Web counts correlate reliably with judgments, for all three types of predicate-argument
bigrams tested, both seen and unseen. For the seen bigrams, we showed that the Web
frequencies correlate better with judged plausibility than corpus frequencies.
To substantiate the claim that the Web counts can be used to overcome data sparse-
ness, we compared bigram counts obtained from the Web with bigram counts re-
created using a class-based smoothing technique (a variant of the one proposed by
Resnik [1993]). We found that Web frequencies and re-created frequencies are reliably
correlated, and that Web frequencies are better at predicting plausibility judgments
than smoothed frequencies. This holds both for unseen bigrams and for seen bigrams
that are treated as unseen by omitting them from the training corpus.
Finally, we tested the performance of our frequencies in a standard pseudodis-
ambiguation task. We applied our method to three data sets from the literature. The
results show that Web counts outperform counts re-created using a number of class-
based smoothing techniques. However, counts re-created using an EM-based smooth-
ing approach yielded better pseudodisambiguation performance than Web counts.
To summarize, we have proposed a simple heuristic method for obtaining bigram
counts from the Web. Using four different types of evaluation, we demonstrated that
this simple heuristic method is sufficient to obtain valid frequency estimates. It seems
that the large amount of data available outweighs the problems associated with using
the Web as a corpus (such as the fact that it is noisy and unbalanced).
A number of questions arise for future research: (1) Are Web frequencies suitable
for probabilistic modeling, in particular since Web counts are not perfectly normalized,
as Zhu and Rosenfeld (2001) have shown? (2) How can existing smoothing methods
benefit from Web counts? (3) How do the results reported in this article carry over
to languages other than English (for which a much smaller amount of Web data are
available)? (4) What is the effect of the noise introduced by our heuristic approach?
The last question could be assessed by reproducing our results using a snapshot of
the Web, from which argument relations can be extracted more accurately using POS
tagging and chunking techniques.
482
Computational Linguistics Volume 29, Number 3
Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic
NLP tasks. Preliminary results are reported by Lapata and Keller (2003), who use Web
counts successfully for a range of NLP tasks, including candidate selection for ma-
chine translation, context-sensitive spelling correction, bracketing and interpretation
of compounds, adjective ordering, and PP attachment.
Acknowledgments
This work was conducted while both
authors were at the Department of
Computational Linguistics, Saarland
University, Saarbru?cken. The work was
inspired by a talk that Gregory Grefenstette
gave in Saarbru?cken in 2001 about his
research on using the Web as a corpus. The
present article is an extended and revised
version of Keller, Lapata, and Ourioupina
(2002). Stephen Clark and Stefan Riezler
provided valuable comments on this
research. We are also grateful to four
anonymous reviewers for Computational
Linguistics; their feedback helped to
substantially improve the present article.
Special thanks are due to Stephen Clark and
Detlef Prescher for making their
pseudodisambiguation data sets available.
References
Abney, Steve. 1996. Partial parsing via
finite-state cascades. In John Carroll,
editor, Workshop on Robust Parsing Eighth
European Summer School in Logic, Language
and Information, pages 8?15, Prague, Czech
Republic.
Abney, Steve and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Andrew Kehler and Andreas
Stolcke, editors, Proceedings of the ACL
Workshop on Unsupervised Learning in
Natural Language Processing, pages 1?8,
College Park, MD.
Agirre, Eneko and David Martinez. 2000.
Exploring automatic word sense
disambiguation with decision lists and the
Web. In Proceedings of the 18th International
Conference on Computational Linguistics,
pages 11?19, Saarbru?cken, Germany.
Banko, Michele and Eric Brill. 2001a.
Mitigating the paucity-of-data problem:
Exploring the effect of training corpus
size on classifier performance for natural
language processing. In James Allan,
editor, Proceedings of the First International
Conference on Human Language Technology
Research. Morgan Kaufmann, San
Francisco.
Banko, Michele and Eric Brill. 2001b. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics and the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 26?33, Toulouse, France.
Bard, Ellen Gurman, Dan Robertson, and
Antonella Sorace. 1996. Magnitude
estimation of linguistic acceptability.
Language, 72(1):32?68.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Burnard, Lou, editor, 1995. Users Reference
Guide, British National Corpus. British
National Corpus Consortium, Oxford
University Computing Services, Oxford,
England.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference
on Empirical Methods in Natural Language
Processing, pages 36?45, Granada, Spain.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh, PA.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using a
semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Corley, Martin and Christoph Scheepers.
2002. Syntactic priming in English
sentence production: Categorical and
latency evidence from an Internet-based
study. Psychonomic Bulletin and Review,
9(1):126?131.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81?94.
Cowart, Wayne. 1997. Experimental Syntax:
Applying Objective Methods to Sentence
Judgments. Sage, Thousand Oaks, CA.
Curran, James. 2002. Scaling context space.
In Proceedings of the 40th Annual Meeting of
483
Keller and Lapata Web Frequencies for Unseen Bigrams
the Association for Computational Linguistics,
pages 231?238, Philadelphia.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
word cooccurrence probabilities. Machine
Learning, 34(1):43?69.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic, Boston.
Grefenstette, Gregory. 1998. The World
Wide Web as a resource for
example-based machine translation tasks.
In Proceedings of the ASLIB Conference on
Translating and the Computer, London.
Grefenstette, Gregory and Jean Nioche.
2000. Estimation of English and
non-English language use on the WWW.
In Proceedings of the RIAO Conference on
Content-Based Multimedia Information
Access, pages 237?246, Paris.
Grishman, Ralph and John Sterling. 1994.
Generalizing automatically generated
selectional patterns. In Proceedings of the
15th International Conference on
Computational Linguistics, pages 742?747,
Kyoto, Japan.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Jones, Rosie and Rayid Ghani. 2000.
Automatically building a corpus for a
minority language from the Web. In
Proceedings of the Student Research Workshop
at the 38th Annual Meeting of the Association
for Computational Linguistics, pages 29?36,
Hong Kong.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1992. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 14th
International Conference on Computational
Linguistics, pages 950?954, Nantes, France.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 33(3):400?401.
Keller, Frank and Theodora Alexopoulou.
2001. Phonology competes with syntax:
Experimental evidence for the interaction
of word order and accent placement in
the realization of information structure.
Cognition, 79(3):301?372.
Keller, Frank and Ash Asudeh. 2001.
Constraints on linguistic coreference:
Structural vs. pragmatic factors. In
Johanna D. Moore and Keith Stenning,
editors, Proceedings of the 23rd Annual
Conference of the Cognitive Science Society,
pages 483?488. Erlbaum, Mahwah, NJ.
Keller, Frank, Martin Corley, Steffan Corley,
Lars Konieczny, and Amalia Todirascu.
1998. WebExp: A Java toolbox for
Web-based psychological experiments.
Technical Report HCRC/TR-99, Human
Communication Research Centre,
University of Edinburgh.
Keller, Frank, Maria Lapata, and Olga
Ourioupina. 2002. Using the Web to
overcome data sparseness. In Jan Hajic?
and Yuji Matsumoto, editors, Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages
230?237, Philadelphia.
Lapata, Maria. 2001. A corpus-based
account of regular polysemy: The case of
context-sensitive adjectives. In Proceedings
of the Second Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 63?70,
Pittsburgh, PA.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Lapata, Maria and Frank Keller. 2003.
Evaluating the performance of
unsupervised Web-based models for a
range of NLP tasks. Unpublished
manuscript, University of Sheffield,
Sheffield, England, and University of
Edinburgh, Edinburgh, Scotland.
Lapata, Maria, Frank Keller, and Scott
McDonald. 2001. Evaluating smoothing
algorithms against plausibility judgments.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
and the 10th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 346?353, Toulouse,
France.
Lapata, Maria, Scott McDonald, and Frank
Keller. 1999. Determinants of
adjective-noun plausibility. In Proceedings
of the Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 30?36, Bergen, Norway.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. thesis, Macquarie
University, Sydney, Australia.
Lee, Lilian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park,
MD.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. The tagging of the British
national corpus. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 622?628, Kyoto, Japan.
Levi, Judith N. 1978. The Syntax and
Semantics of Complex Nominals. Academic
484
Computational Linguistics Volume 29, Number 3
Press, New York.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1994. PRINCIPAR?An
efficient broad-coverage, principle-based
parser. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 482?488, Kyoto, Japan.
Lin, Dekang. 1998. Dependency-based
evaluation of MINIPAR. In Proceedings of
the LREC Workshop on the Evaluation of
Parsing Systems, pages 48?56, Granada,
Spain.
Lin, Dekang. 2001. LaTaT: Language and
text analysis tools. In Proceedings of the
First International Conference on Human
Language Technology Research. Morgan
Kaufmann, San Francisco.
Markert, Katja, Malvina Nissim, and
Natalia N. Modjeska. 2003. Using the Web
for nominal anaphora resolution. In
Proceedings of the EACL Workshop on the
Computational Treatment of Anaphora,
Budapest, pages 39?46.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching
alternations. In Proceedings of the First
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 256?263, Seattle, WA.
Mihalcea, Rada and Dan Moldovan. 1999. A
method for word sense disambiguation of
unrestricted text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
College Park, MD.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?244.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Prescher, Detlef, Stefan Riezler, and Mats
Rooth. 2000. Using a probabilistic
class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th
International Conference on Computational
Linguistics, pages 649?655, Saarbru?cken,
Germany.
Ratnaparkhi, Adwait. 1998. Unsupervised
statistical models for prepositional phrase
attachment. In Proceedings of the
17th International Conference on
Computational Linguistics and 36th Annual
Meeting of the Association for Computational
Linguistics, pages 1079?1085, Montre?al.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527?534,
College Park, MD.
Resnik, Philip. 2000. Measuring verb
similarity. In Lila R. Gleitman and
Aravid K. Joshi, editors, Proceedings of the
22nd Annual Conference of the Cognitive
Science Society, pages 399?404. Erlbaum,
Mahwah, NJ.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
College Park, MD.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Oxford University Press,
Oxford.
Schu?tze, Carson T. 1996. The Empirical Base of
Linguistics: Grammaticality Judgments and
Linguistic Methodology. University of
Chicago Press, Chicago.
Stevens, S. S. 1975. Psychophysics:
Introduction to its Perceptual, Neural, and
Social Prospects. John Wiley, New York.
Volk, Martin. 2001. Exploiting the WWW as
a corpus to resolve PP attachment
ambiguities. In Paul Rayson, Andrew
Wilson, Tony McEnery, Andrew Hardie,
and Shereen Khoja, editors, Proceedings of
the Corpus Linguistics Conference, pages
601?606, Lancaster, England.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems That
Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
Zhu, Xiaojin and Ronald Rosenfeld. 2001.
Improving trigram language modeling
with the World Wide Web. In Proceedings
of the International Conference on Acoustics
Speech and Signal Processing, Salt Lake City,
UT.
The Web as a Baseline: Evaluating the Performance of
Unsupervised Web-based Models for a Range of NLP Tasks
Mirella Lapata
Department of Computer Science
University of Sheffield
211 Portobello St., Sheffield S1 4DP
mlap@dcs.shef.ac.uk
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Pl., Edinburgh EH8 9LW
keller@inf.ed.ac.uk
Abstract
Previous work demonstrated that web counts
can be used to approximate bigram frequen-
cies, and thus should be useful for a wide va-
riety of NLP tasks. So far, only two gener-
ation tasks (candidate selection for machine
translation and confusion-set disambiguation)
have been tested using web-scale data sets. The
present paper investigates if these results gener-
alize to tasks covering both syntax and seman-
tics, both generation and analysis, and a larger
range of n-grams. For the majority of tasks, we
find that simple, unsupervised models perform
better when n-gram frequencies are obtained
from the web rather than from a large corpus.
However, in most cases, web-based models fail
to outperform more sophisticated state-of-the-
art models trained on small corpora. We ar-
gue that web-based models should therefore be
used as a baseline for, rather than an alternative
to, standard models.
1 Introduction
Keller and Lapata (2003) investigated the validity of web
counts for a range of predicate-argument bigrams (verb-
object, adjective-noun, and noun-noun bigrams). They
presented a simple method for retrieving bigram counts
from the web by querying a search engine and demon-
strated that web counts (a) correlate with frequencies ob-
tained from a carefully edited, balanced corpus such as
the 100M words British National Corpus (BNC), (b) cor-
relate with frequencies recreated using smoothing meth-
ods in the case of unseen bigrams, (c) reliably predict hu-
man plausibility judgments, and (d) yield state-of-the-art
performance on pseudo-disambiguation tasks.
Keller and Lapata?s (2003) results suggest that web-
based frequencies can be a viable alternative to bigram
frequencies obtained from smaller corpora or recreated
using smoothing. However, they do not demonstrate that
realistic NLP tasks can benefit from web counts. In or-
der to show this, web counts would have to be applied to
a diverse range of NLP tasks, both syntactic and seman-
Task n POS Ling Type
MT candidate select. 1,2 V, N Sem Generation
Spelling correction 1,2,3 Any Syn/Sem Generation
Adjective ordering 1,2 Adj Sem Generation
Compound bracketing 1,2 N Syn Analysis
Compound interpret. 1,2,3 N, P Sem Analysis
Countability detection 1,2 N, Det Sem Analysis
Table 1: Overview of the tasks investigated in this paper
(n: size of n-gram; POS: parts of speech; Ling: linguistic
knowledge; Type: type of task)
tic, involving analysis (e.g., disambiguation) and gener-
ation (e.g., selection among competing outputs). Also, it
remains to be shown that the web-based approach scales
up to larger n-grams (e.g., trigrams), and to combinations
of different parts of speech (Keller and Lapata 2003 only
tested bigrams involving nouns, verbs, and adjectives).
Another important question is whether web-based meth-
ods, which are by definition unsupervised, can be com-
petitive alternatives to supervised approaches used for
most tasks in the literature.
This paper aims to address these questions. We start by
using web counts for two generation tasks for which the
use of large data sets has shown promising results: (a) tar-
get language candidate selection for machine translation
(Grefenstette, 1998) and (b) context sensitive spelling
correction (Banko and Brill, 2001a,b). Then we investi-
gate the generality of the web-based approach by apply-
ing it to a range of analysis and generations tasks, involv-
ing both syntactic and semantic knowledge: (c) ordering
of prenominal adjectives, (d) compound noun bracketing,
(e) compound noun interpretation, and (f) noun count-
ability detection. Table 1 gives an overview of these tasks
and their properties.
In all cases, we propose a simple, unsupervised n-gram
based model whose parameters are estimated using web
counts. We compare this model both against a baseline
(same model, but parameters estimated on the BNC) and
against state-of-the-art models from the literature, which
are either supervised (i.e., use annotated training data) or
unsupervised but rely on taxonomies to recreate missing
counts.
2 Method
Following Keller and Lapata (2003), web counts for n-
grams were obtained using a simple heuristic based on
queries to the search engine Altavista.1 In this approach,
the web count for a given n-gram is simply the number of
hits (pages) returned by the search engine for the queries
generated for this n-gram. Three different types of queries
were used for the NLP tasks in the present paper:
Literal queries use the quoted n-gram directly as a
search term for Altavista (e.g., the bigram history changes
expands to the query "history changes").
Near queries use Altavista?s NEAR operator to ex-
pand the n-gram; a NEAR b means that a has to oc-
cur in the same ten word window as b; the window is
treated as a bag of words (e.g., history changes expands
to "history" NEAR "changes").
Inflected queries are performed by expanding an
n-gram into all its morphological forms. These forms
are then submitted as literal queries, and the result-
ing hits are summed up (e.g., history changes ex-
pands to "history change", "histories change",
"history changed", etc.). John Carroll?s suite of mor-
phological tools (morpha, morphg, and ana) was used
to generate inflected forms of verbs and nouns.2 In cer-
tain cases (detailed below), determiners were inserted be-
fore nouns in order to make it possible to recognize sim-
ple NPs. This insertion was limited to a/an, the, and the
empty determiner (for bare plurals).
All queries (other than the ones using the NEAR oper-
ator) were performed as exact matches (using quotation
marks in Altavista). All search terms were submitted to
the search engine in lower case. If a query consists of a
single, highly frequent word (such as the), Altavista will
return an error message. In these cases, we set the web
count to a large constant (108). This problem is limited
to unigrams, which were used in some of the models de-
tailed below. Sometimes the search engine fails to return
a hit for a given n-gram (for any of its morphological vari-
ants). We smooth zero counts by setting them to .5.
For all tasks, the web-based models are compared
against identical models whose parameters were esti-
mated from the BNC (Burnard, 1995). The BNC is a
static 100M word corpus of British English, which is
about 1000 times smaller than the web (Keller and La-
pata, 2003). Comparing the performance of the same
model on the web and on the BNC allows us to assess
how much improvement can be expected simply by using
a larger data set. The BNC counts were retrieved using
the Gsearch corpus query tool (Corley et al, 2001); the
morphological query expansion was the same as for web
queries; the NEAR operator was simulated by assuming
a window of five words to the left and five to the right.
1We did not use Google counts, as Google limits the number
of queries to 1000 per day, which makes the process of retriev-
ing a large number of web counts very time consuming.
2The tools can be downloaded from http://www.cogs.
susx.ac.uk/lab/nlp/carroll/morph.html.
# best model on development set
? 6 ? (not) sign. different from best BNC model on test set
? 6 ? (not) sign. different from baseline
? 6 ? (not) sign. different from best model in the literature
Table 2: Meaning of diacritics indicating statistical sig-
nificance (?2 tests)
Gsearch was used to search solely for adjacent words; no
POS information was incorporated in the queries, and no
parsing was performed.
For all of our tasks, we have to select either the best of
several possible models or the best parameter setting for
a single model. We therefore require a separate develop-
ment set. This was achieved by using the gold standard
data set from the literature for a given task and randomly
dividing it into a development set and a test set (of equal
size). We report the test set performance for all models
for a given task, and indicate which model shows optimal
performance on the development set (marked by a ?#? in
all subsequent tables). We then compare the test set per-
formance of this optimal model to the performance of the
models reported in the literature. It is important to note
that the figures taken from the literature were typically
obtained on the whole gold standard data set, and hence
may differ from the performance on our test set. We work
on the assumption that such differences are negligible.
We use ?2 tests to determine whether the performance
of the best web model on the test set is significantly differ-
ent from that of the best BNC model. We also determine
whether both models differ significantly from the base-
line and from the best model in the literature. A set of
diacritics is used to indicate significance throughout this
paper, see Table 2.
3 Candidate Selection for Machine
Translation
Target word selection is a generation task that occurs in
machine translation (MT). A word in a source language
can often be translated into different words in the target
language and the choice of the appropriate translation de-
pends on a variety of semantic and pragmatic factors. The
task is illustrated in (1) where there are five translation al-
ternatives for the German noun Geschichte listed in curly
brackets, the first being the correct one.
(1) a. Die Geschichte a?ndert sich, nicht jedoch die
Geographie.
b. {History, story, tale, saga, strip} changes but
geography does not.
Statistical approaches to target word selection rely on
bilingual lexica to provide all possible translations of
words in the source language. Once the set of translation
candidates is generated, statistical information gathered
from target language corpora is used to select the most
appropriate alternative (Dagan and Itai, 1994). The task is
somewhat simplified by Grefenstette (1998) and Prescher
et al (2000) who do not produce a translation of the en-
tire sentence. Instead, they focus on specific syntactic re-
lations. Grefenstette translates compounds from German
and Spanish into English, and uses BNC frequencies as
a filter for candidate translations. He observes that this
approach suffers from an acute data sparseness problem
and goes on to obtain counts for candidate compounds
through web searches, thus achieving a translation accu-
racy of 86?87%.
Prescher et al (2000) concentrate on verbs and their
objects. Assuming that the target language translation of
the verb is known, they select from the candidate transla-
tions the noun that is semantically most compatible with
the verb. The semantic fit between a verb and its argument
is modeled using a class-based lexicon that is derived
from unlabeled data using the expectation maximization
algorithm (verb-argument model). Prescher et al also
propose a refined version of this approach that only mod-
els the fit between a verb and its object (verb-object
model), disregarding other arguments of the verb. The
two models are trained on the BNC and evaluated against
two corpora of 1,340 and 814 bilingual sentence pairs,
with an average of 8.63 and 2.83 translations for the ob-
ject noun, respectively. Table 4 lists Prescher et al?s re-
sults for the two corpora and for both models together
with a random baseline (select a target noun at random)
and a frequency baseline (select the most frequent target
noun).
Grefenstette?s (1998) evaluation was restricted to com-
pounds that are listed in a dictionary. These com-
pounds are presumably well-established and fairly fre-
quent, which makes it easy to obtain reliable web fre-
quencies. We wanted to test if the web-based approach
extends from lexicalized compounds to productive syn-
tactic units for which dictionary entries do not exist. We
therefore performed our evaluation using Prescher et al?s
(2000) test set of verb-object pairs. Web counts were re-
trieved for all possible verb-object translations; the most
likely one was selected using either co-occurrence fre-
quency ( f (v,n)) or conditional probability ( f (v,n)/ f (n)).
The web counts were gathered using inflected queries in-
volving the verb, a determiner, and the object (see Sec-
tion 2). Table 3 compares the web-based models against
the BNC models. For both the high ambiguity and the
low ambiguity data set, we find that the performance
of the best Altavista model is not significantly different
from that of the best BNC model. Table 4 compares our
simple, unsupervised methods with the two sophisticated
class-based models discussed above. The results show
that there is no significant difference in performance be-
tween the best model reported in the literature and the
best Altavista or the best BNC model. However, both
models significantly outperform the baseline. This holds
for both the high and low ambiguity data sets.
Altavista BNC
high low high low
Model ambig ambig ambig ambig
f (v,n) 45.74 68.73#6 ? 45.89# 70.06#
f (v,n)/ f (n) 45.16#6 ? 64.96 46.18 66.07
Table 3: Performance of Altavista counts and BNC counts
for candidate selection for MT (data from Prescher et al
2000)
high low
Model ambig ambig
Random baseline 14.20 45.90
Frequency baseline 31.90 45.50
Prescher et al (2000): verb-argument 43.30 61.50
Best Altavista 45.16?6 ? 68.73?6 ?
Best BNC 45.89?6 ? 70.06?6 ?
Prescher et al (2000): verb-object 49.40 68.20
Table 4: Performance comparison with the literature for
candidate selection for MT
4 Context-sensitive Spelling Correction
Context-sensitive spelling correction is the task of cor-
recting spelling errors that result in valid words. Such
a spelling error is illustrated in (4) where principal was
typed when principle was intended.
(2) Introduction of the dialogue principal proved strik-
ingly effective.
The task can be viewed as generation task, as it consists
of choosing between alternative surface realizations of a
word. This choice is typically modeled by confusion sets
such as {principal, principle} or {then, than} under the
assumption that each word in the set could be mistakenly
typed when another word in the set was intended. The
task is to infer which word in a confusion set is the cor-
rect one in a given context. This choice can be either syn-
tactic (as for {then, than}) or semantic (as for {principal,
principle}).
A number of machine learning methods have been pro-
posed for context-sensitive spelling correction. These in-
clude a variety of Bayesian classifiers (Golding, 1995;
Golding and Schabes, 1996), decision lists (Golding,
1995) transformation-based learning (Mangu and Brill,
1997), Latent Semantic Analysis (LSA) (Jones and
Martin, 1997), multiplicative weight update algorithms
(Golding and Roth, 1999), and augmented mixture mod-
els (Cucerzan and Yarowsky, 2002). Despite their differ-
ences, most approaches use two types of features: context
words and collocations. Context word features record the
presence of a word within a fixed window around the tar-
get word (bag of words); collocational features capture
the syntactic environment of the target word and are usu-
ally represented by a small number of words and/or part-
of-speech tags to the left or right of the target word.
The results obtained by a variety of classification meth-
ods are given in Table 6. All methods use either the full
set or a subset of 18 confusion sets originally gathered by
Golding (1995). Most methods are trained and tested on
Model Alta BNC Model Alta BNC
f (t) 72.98 70.00 f (w1, t,w2)/ f (t) 87.77 76.33
f (w1, t) 84.40 83.02 f (w1,w2, t)/ f (t) 86.27 74.47
f (t,w1) 84.89 82.74 f (t,w2,w2)/ f (t) 84.94 74.23
f (w1, t,w2) 89.24#*77.13 f (w1, t,w2)/ f (w1, t) 80.70 73.69
f (w1,w2, t) 87.13 74.89 f (w1, t,w2)/ f (t,w2) 82.24 75.10
f (t,w1,w2) 84.68 75.08 f (w1,w2, t)/ f (w2, t) 72.11 69.28
f (w1, t)/ f (t) 82.81 77.84 f (t,w1,w2)/ f (t,w1) 75.65 72.57
f (t,w1)/ f (t) 77.49 80.71#
Table 5: Performance of Altavista counts and BNC
counts for context sensitive spelling correction (data from
Cucerzan and Yarowsky 2002)
Model Accuracy
Baseline BNC 70.00
Baseline Altavista 72.98
Best BNC 80.71??
Golding (1995) 81.40
Jones and Martin (1997) 84.26
Best Altavista 89.24??
Golding and Schabes (1996) 89.82
Mangu and Brill (1997) 92.79
Cucerzan and Yarowsky (2002) 92.20
Golding and Roth (1999) 94.23
Table 6: Performance comparison with the literature for
context sensitive spelling correction
the Brown corpus, using 80% for training and 20% for
testing.3
We devised a simple, unsupervised method for
performing spelling correction using web counts.
The method takes into account collocational features,
i.e., words that are adjacent to the target word. For each
word in the confusion set, we used the web to estimate
how frequently it co-occurs with a word or a pair of words
immediately to its left or right. Disambiguation is then
performed by selecting the word in the confusion set with
the highest co-occurrence frequency or probability. The
web counts were retrieved using literal queries (see Sec-
tion 2). Ties are resolved by comparing the unigram fre-
quencies of the words in the confusion set and defaulting
to the word with the highest one. Table 5 shows the types
of collocations we considered and their corresponding ac-
curacy. The baseline ( f (t)) in Table 5 was obtained by
always choosing the most frequent unigram in the confu-
sion set. We used the same test set (2056 tokens from the
Brown corpus) and confusion sets as Golding and Sch-
abes (1996), Mangu and Brill (1997), and Cucerzan and
Yarowsky (2002).
Table 5 shows that the best result (89.24%) for the web-
based approach is obtained with a context of one word
to the left and one word to the right of the target word
( f (w1, t,w2)). The BNC-based models perform consis-
tently worse than the web-based models with the excep-
tion of f (t,w1)/t; the best Altavista model performs sig-
nificantly better than the best BNC model. Table 6 shows
3An exception is Golding (1995), who uses the entire Brown
corpus for training (1M words) and 3/4 of the Wall Street Jour-
nal corpus (Marcus et al, 1993) for testing.
that both the best Altavista model and the best BNC
model outperform their respective baselines. A compari-
son with the literature shows that the best Altavista model
outperforms Golding (1995), Jones and Martin (1997)
and performs similar to Golding and Schabes (1996). The
highest accuracy on the task is achieved by the class of
multiplicative weight-update algorithms such as Winnow
(Golding and Roth, 1999). Both the best BNC model and
the best Altavista model perform significantly worse than
this model. Note that Golding and Roth (1999) use al-
gorithms that can handle large numbers of features and
are robust to noise. Our method uses a very small feature
set, it relies only on co-occurrence frequencies and does
not have access to POS information (the latter has been
shown to have an improvement on confusion sets whose
words belong to different parts of speech). An advantage
of our method is that it can be used for a large number
of confusion sets without relying on the availability of
training data.
5 Ordering of Prenominal Adjectives
The ordering of prenominal modifiers is important for
natural language generation systems where the text must
be both fluent and grammatical. For example, the se-
quence big fat Greek wedding is perfectly acceptable,
whereas fat Greek big wedding sounds odd. The ordering
of prenominal adjectives has sparked a great deal of the-
oretical debate (see Shaw and Hatzivassiloglou 1999 for
an overview) and efforts have concentrated on defining
rules based on semantic criteria that account for different
orders (e.g., age ? color, value ? dimension).
Data intensive approaches to the ordering problem rely
on corpora for gathering evidence for the likelihood of
different orders. They rest on the hypothesis that the rel-
ative order of premodifiers is fixed, and independent of
context and the noun being modified. The simplest strat-
egy is what Shaw and Hatzivassiloglou (1999) call di-
rect evidence. Given an adjective pair {a,b}, they count
how many times ?a,b? and ?b,a? appear in the corpus and
choose the pair with the highest frequency.
Unfortunately the direct evidence method performs
poorly when a given order is unseen in the training
data. To compensate for this, Shaw and Hatzivassiloglou
(1999) propose to compute the transitive closure of the
ordering relation: if a ? c and c ? b, then a ? b. Mal-
ouf (2000) further proposes a back-off bigram model
of adjective pairs for choosing among alternative orders
(P(?a,b?|{a,b}) vs. P(?b,a?|{a,b})). He also proposes
positional probabilities as a means of estimating how
likely it is for a given adjective a to appear first in a se-
quence by looking at each pair in the training data that
contains the adjective a and recording its position. Fi-
nally, he uses memory-based learning as a means to en-
code morphological and semantic similarities among dif-
ferent adjective orders. Each adjective pair ab is encoded
as a vector of 16 features (the last eight characters of a
and the last eight characters of b) and a class (?a,b? or
Model Altavista BNC
f (a1,a2) : f (a2,a1) 89.6#?6 ? 80.4#?
f (a1,a2)/ f (a2) : f (a2,a1)/ f (a1) 83.2 77.0
f (a1,a2)/ f (a1) : f (a2,a1)/ f (a2) 80.2 80.6
Malouf (2000): memory-based ? 91.0
Table 7: Performance of Altavista counts and BNC counts
for adjective ordering (data from Malouf 2000)
?b,a?).
Malouf (2000) extracted 263,838 individual pairs of
adjectives from the BNC which he randomly partitioned
into test (10%) and training data (90%) and evaluated
all the above methods for ordering prenominal adjec-
tives. His results showed that a memory-based classi-
fier that uses morphological information as well as po-
sitional probabilities as features outperforms all other
methods (see Table 7). For the ordering task we restricted
ourselves to the direct evidence strategy which simply
chooses the adjective order with the highest frequency
or probability (see Table 7). Web counts were obtained
by submitting literal queries to Altavista (see Section 2).
We used the same 263,838 adjective pairs that Malouf ex-
tracted from the BNC. These were randomly partitioned
into a training (90%) and test corpus (10%). The test
corpus contained 26,271 adjective pairs. Given that sub-
mitting 26,271 queries to Altavista would be fairly time-
consuming, a random sample of 1000 sequences was ob-
tained from the test corpus and the web frequencies of
these pairs were retrieved. The best Altavista model sig-
nificantly outperformed the best BNC model, as indicated
in Table 7. We also found that there was no significant
difference between the best Altavista model and the best
model reported by Malouf, a supervised method using
positional probability estimates from the BNC and mor-
phological variants.
6 Bracketing of Compound Nouns
The first analysis task we consider is the syntactic disam-
biguation of compound nouns, which has received a fair
amount of attention in the NLP literature (Pustejovsky
et al, 1993; Resnik, 1993; Lauer, 1995). The task can
be summarized as follows: given a three word compound
n1 n3 n3, determine the correct binary bracketing of the
word sequence (see (3) for an example).
(3) a. [[backup compiler] disk]
b. [backup [compiler disk]]
Previous approaches typically compare different brack-
etings and choose the most likely one. The adjacency
model compares [n1 n2] against [n2 n3] and adopts a right
branching analysis if [n2 n3] is more likely than [n1 n2].
The dependency model compares [n1 n2] against [n1 n3]
and adopts a right branching analysis if [n1 n3] is more
likely than [n1 n2].
The simplest model of compound noun disambiguation
compares the frequencies of the two competing analyses
and opts for the most frequent one (Pustejovsky et al,
Model Alta BNC
Baseline 63.93 63.93
f (n1,n2) : f (n2,n3) 77.86 66.39
f (n1,n2) : f (n1,n3) 78.68#? 65.57
f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) 68.85 65.57
f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) 70.49 63.11
f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) 80.32 66.39
f (n1,n2) : f (n2,n3) (NEAR) 68.03 63.11
f (n1,n2) : f (n1,n3) (NEAR) 71.31 67.21
f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) (NEAR) 61.47 62.29
f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) (NEAR) 65.57 57.37
f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) (NEAR) 75.40 68.03#
Table 8: Performance of Altavista counts and BNC counts
for compound bracketing (data from Lauer 1995)
Model Accuracy
Baseline 63.93
Best BNC 68.036 ??
Lauer (1995): adjacency 68.90
Lauer (1995): dependency 77.50
Best Altavista 78.68?6 ?
Lauer (1995): tuned 80.70
Upper bound 81.50
Table 9: Performance comparison with the literature for
compound bracketing
1993). Lauer (1995) proposes an unsupervised method
for estimating the frequencies of the competing brack-
etings based on a taxonomy or a thesaurus. He uses a
probability ratio to compare the probability of the left-
branching analysis to that of the right-branching (see (4)
for the dependency model and (5) for the adjacency
model).
Rdep =
?
ti?cats(wi)
P(t1 ? t2)P(t2 ? t3)
?
ti?cats(wi)
P(t1 ? t3)P(t2 ? t3)
(4)
Radj =
?
ti?cats(wi)
P(t1 ? t2)
?
ti?cats(wi)
P(t2 ? t3)
(5)
Here t1, t2 and t3 are conceptual categories in the taxon-
omy or thesaurus, and the nouns w1 . . .wi are members of
these categories. The estimation of probabilities over con-
cepts (rather than words) reduces the number of model
parameters and effectively decreases the amount of train-
ing data required. The probability P(t1 ? t2) denotes the
modification of a category t2 by a category t1.
Lauer (1995) tested both the adjacency and de-
pendency models on 244 compounds extracted from
Grolier?s encyclopedia, a corpus of 8 million words. Fre-
quencies for the two models were obtained from the same
corpus and from Roget?s thesaurus (version 1911) by
counting pairs of nouns that are either strictly adjacent or
co-occur within a window of a fixed size (e.g., two, three,
fifty, or hundred words). The majority of the bracketings
in our test set were left-branching, yielding a baseline
of 63.93% (see Table 9). Lauer?s best results (77.50%)
were obtained with the dependency model and a training
scheme which takes strictly adjacent nouns into account.
Performance increased further by 3.2% when POS tags
were taken into account. The results for this tuned model
are also given in Table 9. Finally, Lauer conducted an ex-
periment with human judges to assess the upper bound
for the bracketing task. An average accuracy of 81.50%
was obtained.
We replicated Lauer?s (1995) results for compound
noun bracketing using the same test set. We compared
the performance of the adjacency and dependency mod-
els (see (4) and (5)), but instead of relying on a corpus and
a thesaurus, we estimated the relevant probabilities us-
ing web counts. The latter were obtained using inflected
queries (see Section 2) and Altavista?s NEAR operator.
Ties were resolved by defaulting to the most frequent
analysis (i.e., left-branching). To gauge the performance
of the web-based models we compared them against their
BNC-based alternatives; the performance of the best Al-
tavista model was significantly higher than that of the
best BNC model (see Table 8). A comparison with the
literature (see Table 9) shows that the best BNC model
fails to significantly outperform the baseline, and it per-
forms significantly worse than the best model in the liter-
ature (Lauer?s tuned model). The best Altavista model, on
the other hand, is not significantly different from Lauer?s
tuned model and significantly outperforms the baseline.
Hence we achieve the same performance as Lauer with-
out recourse to a predefined taxonomy or a thesaurus.
7 Interpretation of Compound Nouns
The second analysis task we consider is the semantic
interpretation of compound nouns. Most previous ap-
proaches to this problem have focused on the interpre-
tation of two word compounds whose nouns are related
via a basic set of semantic relations (e.g., CAUSE relates
onion tears, FOR relates pet spray). The majority of pro-
posals are symbolic and therefore limited to a specific
domain due to the large effort involved in hand-coding
semantic information (see Lauer 1995 for an extensive
overview).
Lauer (1995) is the first to propose and evaluate an un-
supervised probabilistic model of compound noun inter-
pretation for domain independent text. By recasting the
interpretation problem in terms of paraphrasing, Lauer
assumes that the semantic relations of compound heads
and modifiers can be expressed via prepositions that (in
contrast to abstract semantic relations) can be found in a
corpus. For example, in order to interpret war story, one
needs to find in a corpus related paraphrases: story about
the war, story of the war, story in the war, etc. Lauer uses
eight prepositions for the paraphrasing task (of, for, in,
at, on, from, with, about). A simple model of compound
noun paraphrasing is shown in (6):
p? = argmax
p
P(p|n1,n2)(6)
Lauer (1995) points out that the above model contains
one parameter for every triple ?p,n1,n2?, and as a result
Model Altavista BNC
f (n1, p) f (p,n2) 50.71 27.85#
f (n1, p,n2) 55.71#* 11.42
f (n1, p) f (p,n2)/ f (p) 47.14 26.42
f (n1, p,n2)/ f (p) 55.00 10.71
Table 10: Performance of Altavista counts and BNC
counts for compound interpretation (data from Lauer
1995)
Model Accuracy
Best BNC 27.856 ??
Lauer (1995): concept-based 28.00
Baseline 33.00
Lauer (1995): word-based 40.00
Best Altavista 55.71??
Table 11: Performance comparison with the literature for
compound interpretation
hundreds of millions of training instances would be nec-
essary. As an alternative to (6), he proposes the model
in (7) which combines the probability of the modifier
given a certain preposition with the probability of the
head given the same preposition, and assumes that these
two probabilities are independent.
p? = argmax
p ?
t1 ? cats(n1)
t2 ? cats(n2)
P(t1|p)P(t2|p)(7)
Here, t1 and t2 represent concepts in Roget?s thesaurus.
Lauer (1995) also experimented with a lexicalized ver-
sion of (7) where probabilities are calculated on the basis
of word (rather than concept) frequencies which Lauer
obtained from Grolier?s encyclopedia heuristically via
pattern matching.
Lauer (1995) tested the model in (7) on 282 com-
pounds that he selected randomly from Grolier?s encyclo-
pedia and annotated with their paraphrasing prepositions.
The preposition of accounted for 33% of the paraphrases
in this data set (see Baseline in Table 11). The concept-
based model (see (7)) achieved an accuracy of 28% on
this test set, whereas its lexicalized version reached an
accuracy of 40% (see Table 11).
We attempted the interpretation task with the lexi-
calized version of the bigram model (see (7)), but also
tried the more data intensive trigram model (see (6)),
again in its lexicalized form. Furthermore, we experi-
mented with several conditional and unconditional vari-
ants of (7) and (6). Co-occurrence frequencies were es-
timated from the web using inflected queries (see Sec-
tion 2). Determiners were inserted before nouns result-
ing in queries of the type story/stories about and
about the/a/0 war/wars for the compound war story.
As shown in Table 10, the best performance was ob-
tained using the web-based trigram model ( f (n1, p,n2));
it significantly outperformed the best BNC model. The
comparison with the literature in Table 11 showed that
the best Altavista model significantly outperformed both
the baseline and the best model in the literature (Lauer?s
word-based model). The BNC model, on the other hand,
Altavista BNC
Model Count Uncount Count Uncount
f (n) 87.01 90.13 87.32# 90.39#
f (det,n) 88.38#6 ? 91.22#6 ? 51.01 50.23
f (det,n)/ f (n) 83.19 85.38 50.95 50.23
Backoff 87.01 89.80 ? ?
Table 12: Performance of Altavista counts and BNC
counts for noun countability detection (data from Bald-
win and Bond 2003)
achieved a performance that is not significantly different
from the baseline, and significantly worse than Lauer?s
best model.
8 Noun Countability Detection
The next analysis task that we consider is the problem
of determining the countability of nouns. Countability is
the semantic property that determines whether a noun can
occur in singular and plural forms, and affects the range
of permissible modifiers. In English, nouns are typically
either countable (e.g., one dog, two dogs) or uncountable
(e.g., some peace, *one peace, *two peaces).
Baldwin and Bond (2003) propose a method for auto-
matically learning the countability of English nouns from
the BNC. They obtain information about noun countabil-
ity by merging lexical entries from COMLEX (Grishman
et al, 1994) and the ALTJ/E Japanese-to-English seman-
tic transfer dictionary (Ikehara et al, 1991). Words are
classified into four classes: countable, uncountable, bi-
partite (e.g., trousers), and plural only (e.g., goods). A
memory-based classifier is used to learn the four-way dis-
tinction on the basis of several linguistically motivated
features such as: number of the head noun, number of the
modifier, subject-verb agreement, plural determiners.
We devised unsupervised models for the countability
learning task and evaluated their performance on Bald-
win and Bond?s (2003) test data. We concentrated solely
on countable and uncountable nouns, as they account
for the vast majority of the data. Four models were
tested: (a) compare the frequency of the singular and
plural forms of the noun; (b) compare the frequency of
determiner-noun pairs that are characteristic of countable
or uncountable nouns; the determiners used were many
for countable and much for uncountable ones; (c) same
as model (b), but the det-noun frequencies are normalized
by the frequency of the noun; (d) backoff: try to make
a decision using det-noun frequencies; if these are too
sparse, back off to singular/plural frequencies.
Unigram and bigram frequencies were estimated from
the web using literal queries; for models (a)?(c) a thresh-
old parameter was optimized on the development set (this
parameter determines the ratio of singular/plural frequen-
cies or det-noun frequencies above which a noun was
considered as countable). For model (b), an additional
backoff parameter was used, specifying the minimum fre-
quency that triggers backoff.
The models and their performance on the test set are
Model Count Uncount
Baseline 74.60 78.30
Best BNC 87.32?? 90.39??
Best Altavista 88.38?? 91.22??
Baldwin and Bond (2003) 93.90 95.20
Table 13: Performance comparison with the literature for
noun countability detection
listed in Table 12. The best Altavista model is the condi-
tional det-noun model ( f (det,n)/ f (n)), which achieves
88.38% on countable and 91.22% on uncountable nouns.
On the BNC, the simple unigram model performs best. Its
performance is not statistically different from that of the
best Altavista model. Note that for the BNC models, data
sparseness means the det-noun models perform poorly,
which is why the backoff model was not attempted here.
Table 13 shows that both the Altavista model and BNC
model significantly outperform the baseline (relative fre-
quency of the majority class on the gold-standard data).
The comparison with the literature shows that both the
Altavista and the BNC model perform significantly worse
than the best model proposed by Baldwin and Bond
(2003); this is a supervised model that uses many more
features than just singular/plural frequency and det-noun
frequency.
9 Conclusions
We showed that simple, unsupervised models using web
counts can be devised for a variety of NLP tasks. The
tasks were selected so that they cover both syntax and se-
mantics, both generation and analysis, and a wider range
of n-grams than have been previously used.
For all but two tasks (candidate selection for MT and
noun countability detection) we found that simple, un-
supervised models perform significantly better when n-
gram frequencies are obtained from the web rather than
from a standard large corpus. This result is consistent
with Keller and Lapata?s (2003) findings that the web
yields better counts than the BNC. The reason for this
seems to be that the web is much larger than the BNC
(about 1000 times); the size seems to compensate for
the fact that simple heuristics were used to obtain web
counts, and for the noise inherent in web data.
Our results were less encouraging when it comes to
comparisons with state-of-the-art models. We found that
in all but one case, web-based models fail to significantly
outperform the state of the art. The exception was com-
pound noun interpretation, for which the Altavista model
was significantly better than the Lauer?s (1995) model.
For three tasks (candidate selection for MT, adjective or-
dering, and compound noun bracketing), we found that
the performance of the web-based models was not signif-
icantly different from the performance of the best models
reported in the literature.
Note that for all the tasks we investigated, the best
performance in the literature was obtained by supervised
models that have access not only to simple bigram or tri-
gram frequencies, but also to linguistic information such
as part-of-speech tags, semantic restrictions, or context
(or a thesaurus, in the case of Lauer?s models). When un-
supervised web-based models are compared against su-
pervised methods that employ a wide variety of features,
we observe that having access to linguistic information
makes up for the lack of vast amounts of data.
Our results therefore indicate that large data sets such
as those obtained from the web are not the panacea that
they are claimed to be (at least implicitly) by authors
such as Grefenstette (1998) and Keller and Lapata (2003).
Rather, in our opinion, web-based models should be used
as a new baseline for NLP tasks. The web baseline indi-
cates how much can be achieved with a simple, unsuper-
vised model based on n-grams with access to a huge data
set. This baseline is more realistic than baselines obtained
from standard corpora; it is generally harder to beat, as
our comparisons with the BNC baseline throughout this
paper have shown.
Note that for certain tasks, the performance of a web
baseline model might actually be sufficient, so that the ef-
fort of constructing a sophisticated supervised model and
annotating the necessary training data can be avoided.
Another possibility that needs further investigation is the
combination of web-based models with supervised meth-
ods. This can be done with ensemble learning methods
or simply by using web-based frequencies (or probabil-
ities) as features (in addition to linguistically motivated
features) to train supervised classifiers.
Acknowledgments
We are grateful to Tim Baldwin, Silviu Cucerzan, Mark
Lauer, Rob Malouf, Detelef Prescher, and Adwait Ratna-
parkhi for making their data sets available.
References
Baldwin, Timothy and Francis Bond. 2003. Learning the count-
ability of English nouns from corpus data. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, Japan, pages 463?470.
Banko, Michele and Eric Brill. 2001a. Mitigating the paucity-
of-data problem: Exploring the effect of training corpus size
on classifier performance for natural language processing.
In James Allan, editor, Proceedings of the 1st International
Conference on Human Language Technology Research. Mor-
gan Kaufmann, San Francisco.
Banko, Michele and Eric Brill. 2001b. Scaling to very very
large corpora for natural language disambiguation. In Pro-
ceedings of the 39th Annual Meeting of the Association for
Computational Linguistics. Toulouse, France.
Burnard, Lou. 1995. The Users Reference Guide for the British
National Corpus. British National Corpus Consortium, Ox-
ford University Computing Service.
Corley, Steffan, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntactic struc-
ture in unparsed corpora: The Gsearch corpus query system.
Computers and the Humanities 35(2):81?94.
Cucerzan, Silviu and David Yarowsky. 2002. Augmented mix-
ture models for lexical disambiguation. In Jan Hajic? and Yuji
Matsumoto, editors, Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. Philadel-
phia, PA, pages 33?40.
Dagan, Ido and Alon Itai. 1994. Machine translation diver-
gences: A formal description and proposed solution. Com-
putational Linguistics 20(4):563?597.
Golding, Andrew R. 1995. A Bayesian hybrid method for
context-sensitive spelling correction. In David Yarowsky and
Kenneth W. Church, editors, Proceedings of the 3rd Work-
shop on Very Large Corpora. Cambridge, MA, pages 39?53.
Golding, Andrew R. and Dan Roth. 1999. A winnow-based
approach to context sensitive spelling correction. Machine
Learning 34(1?3):1?25.
Golding, Andrew R. and Yves Schabes. 1996. Combin-
ing trigram-based and feature-based methods for context-
sensitive spelling correction. In Proceedings of the 34th An-
nual Meeting of the Association for Computational Linguis-
tics. Santa Cruz, CA, pages 71?78.
Grefenstette, Gregory. 1998. The World Wide Web as a resource
for example-based machine translation tasks. In Proceedings
of the ASLIB Conference on Translating and the Computer.
London.
Grishman, Ralph, Catherine Macleod, and Adam Meyers. 1994.
COMLEX syntax: Building a computational lexicon. In Pro-
ceedings of the 15th International Conference on Computa-
tional Linguistics. Kyoto, Japan, pages 268?272.
Ikehara, Satoru, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
effects of new methods in ALT-J/E. In Proceedings of the
Third Machine Translation Summit. Washington, DC, pages
101?106.
Jones, Michael P. and James H. Martin. 1997. Contextual
spelling correction using latent semantic analysis. In Pro-
ceedings of the 5th Conference on Applied Natural Language
Processing. Washington, DC, pages 166?173.
Keller, Frank and Mirella Lapata. 2003. Using the web to obtain
frequencies for unseen bigrams. Computational Linguistics
29(3):459?484.
Lauer, Mark. 1995. Corpus statistics meet the noun compound:
Some empirical results. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics.
Cambridge, MA, pages 47?54.
Malouf, Robert. 2000. The order of prenominal adjectives in
natural language generation. In Proceedings of the 38th An-
nual Meeting of the Association for Computational Linguis-
tics. Hong Kong, pages 85?92.
Mangu, Lidia and Eric Brill. 1997. Automatic rule acquisi-
tion of spelling correction. In Proceedings of the 14th Inter-
national Conference on Machine Learning. Nashville, Ten-
nessee, pages 187?194.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguistics
19(2):313?330.
Prescher, Detlef, Stefan Riezler, and Mats Rooth. 2000. Using
a probabilistic class-based lexicon for lexical ambiguity res-
olution. In Proceedings of the 18th International Conference
on Computational Linguistics. Saarbru?cken, Germany, pages
649?655.
Pustejovsky, James, Sabine Bergler, and Peter Anick. 1993.
Lexical semantic techniques for corpus analysis. Computa-
tional Linguistics 19(3):331?358.
Resnik, Philip Stuart. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
Shaw, James and Vassilis Hatzivassiloglou. 1999. Ordering
among premodifiers. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics.
College Park, MD, pages 135?143.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 121?124,
New York, June 2006. c?2006 Association for Computational Linguistics
Computational Modelling of Structural Priming in Dialogue
David Reitter, Frank Keller, Johanna D. Moore
dreitter | keller | jmoore @ inf.ed.ac.uk
School of Informatics
University of Edinburgh
United Kingdom
Abstract
Syntactic priming effects, modelled as in-
crease in repetition probability shortly af-
ter a use of a syntactic rule, have the
potential to improve language processing
components. We model priming of syn-
tactic rules in annotated corpora of spo-
ken dialogue, extending previous work
that was confined to selected construc-
tions. We find that speakers are more re-
ceptive to priming from their interlocutor
in task-oriented dialogue than in spona-
neous conversation. Low-frequency rules
are more likely to show priming.
1 Introduction
Current dialogue systems overlook an interesting
fact of language-based communication. Speakers
tend to repeat their linguistic decisions rather than
making them from scratch, creating entrainment
over time. Repetition is evident not just on the ob-
vious lexical level: syntactic choices depend on pre-
ceding ones in a way that can be modelled and, ul-
timately, be leveraged in parsing and language gen-
eration. The statistical analysis in this paper aims to
make headway towards such a model.
Recently, priming phenomena1 have been ex-
ploited to aid automated processing, for instance in
automatic speech recognition using cache models,
but only recently have attempts been made at using
1The term priming refers to a process that influences lin-
guistic decision-making. An instance of priming occurs when a
syntactic structure or lexical item giving evidence of a linguistic
choice (prime) influences the recipient to make the same deci-
sion, i.e. re-use the structure, at a later choice-point (target).
them in parsing (Charniak and Johnson, 2005). In
natural language generation, repetition can be used
to increase the alignment of human and computers.
A surface-level approach is possible by biasing the
n-gram language model used to select the output
string from a variety of possible utterances (Brock-
mann et al, 2005).
Priming effects are common and well known. For
instance, speakers access lexical items more quickly
after a semantically or phonologically similar prime.
Recent work demonstrates large effects for partic-
ular synonymous alternations (e.g., active vs. pas-
sive voice) using traditional laboratory experiments
with human subjects (Bock, 1986; Branigan et al,
2000). In this study, we look at the effect from a
computational perspective, that is, we assume some
form of parsing and syntax-driven generation com-
ponents. While previous studies singled out syntac-
tic phenomena, we assume a phrase-structure gram-
mar where all syntactic rules may receive priming.
We use large-scale corpora, which reflect the reali-
ties of natural interaction, where limited control ex-
ists over syntax and the semantics of the utterances.
Thus, we quantify priming for the general case in
the realistic setting provided by corpus based exper-
iments. As a first hypothesis, we predict that after a a
syntactic rule occurs, it is more likely to be repeated
shortly than a long time afterwards.
From a theoretical perspective, priming opens a
peephole into the architecture of the human lan-
guage faculty. By identifying units in which prim-
ing occurs, we can pinpoint the structures used in
processing. Also, priming may help explain the ease
with which humans engange in conversations.
This study is interested in the differences relevant
to systems implementing language-based human-
121
computer interaction. Often, HCI is a means for
user and system to jointly plan or carry out a task.
Thus, we look at repetition effects in task-oriented
dialogue. A recent psychological perspective mod-
els Interactive Alignment between speakers (Picker-
ing and Garrod, 2004), where mutual understand-
ing about task and situation depends on lower-level
priming effects. Under the model, we expect prim-
ing effects to be stronger when a task requires high-
level alignment of situation models.
2 Method
2.1 Dialogue types
We examined two corpora. Switchboard con-
tains 80,000 utterances of spontaneous spoken con-
versations over the telephone among randomly
paired, North American speakers, syntactically an-
notated with phrase-structure grammar (Marcus
et al, 1994). The HCRC Map Task corpus comprises
more than 110 dialogues with a total of 20, 400 ut-
terances (Anderson et al, 1991). Like Switchboard,
HCRC Map Task is a corpus of spoken, two-person
dialogue in English. However, Map Task contains
task-oriented dialogue: interlocutors work together
to achieve a task as quickly and efficiently as pos-
sible. Subjects were asked to give each other direc-
tions with the help of a map. The interlocutors are in
the same room, but have separate, slightly different
maps and are unable to see each other?s maps.
2.2 Syntactic repetitions
Both corpora are annotated with phrase structure
trees. Each tree was converted into the set of phrase
structure productions that license it. This allows us
to identify the repeated use of rules. Structural prim-
ing would predict that a rule (target) occurs more
often shortly after a potential prime of the same rule
than long afterwards ? any repetition at great dis-
tance is seen as coincidental. Therefore, we can cor-
relate the probability of repetition with the elapsed
time (DIST) between prime and target.
We considered very pair of two equal syntactic
rules up to a predefined maximal distance to be a
potential case of priming-enhanced production. If
we consider priming at distances 1 . . . n, each rule
instance produces up to n data points. Our binary
response variable indicates whether there is a prime
for the target between n ? 0.5 and n + 0.5 seconds
before the target. As a prime, we see the invocation
of the same rule. Syntactic repetitions resulting from
lexical repetition and repetitions of unary rules are
excluded. We looked for repetitions within windows
(DIST) of n = 15 seconds (Section 3.1).
Without priming, one would expect that there is a
constant probability of syntactic repetition, no mat-
ter the distance between prime and target. The anal-
ysis tries to reject this null hypothesis and show a
correlation of the effect size with the type of corpus
used. We expect to see the syntactic priming effect
found experimentally should translate to more cases
for shorter repetition distances, since priming effects
usually decay rapidly (Branigan et al, 1999).
The target utterance is included as a random fac-
tor in our model, grouping all 15 measurements of
all rules of an utterance as repeated measurements,
since they depend on the same target rule occurrence
or at least on other other rules in the utterance, and
are, thus, partially inter-dependent.
We distinguish production-production priming
within (PP) and comprehension-production priming
between speakers (CP), encoded in the factor ROLE.
Models were estimated on joint data sets derived
from both corpora, with a factor SOURCE included
to discriminate the two dialogue types.
Additionally, we build a model estimating the ef-
fect of the raw frequency of a particular syntactic
rule on the priming effect (FREQ). This is of par-
ticular interest for priming in applications, where a
statistical model will, all other things equal, prefer
the more frequent linguistic choice; recall for com-
peting low-frequency rules will be low.
2.3 Generalized Linear Mixed Effect
Regression
In this study, we built generalized linear mixed ef-
fects regression models (GLMM). In all cases, a rule
instance target is counted as a repetition at distance
d iff there is an utterance prime which contains the
same rule, and prime and target are d units apart.
GLMMs with a logit-link function are a form of lo-
gistic regression.2
2We trained our models using Penalized Quasi-Likelihood
(Venables and Ripley, 2002). We will not generally give classi-
calR2 figures, as this metric is not appropriate to such GLMMs.
The below experiments were conducted on a sample of 250,000
122
SWBD PP MT PP MT CP
?0
.1
0
?0
.0
5
0.
00
0.
05
0.
10
0.
15
0.
20
Switchboard Map Task
PP PP CPCP
*
*
*
-
-
-
-
0 5 10 15
0.0
10
0.0
12
0.0
14
0.0
16
0.0
18
0.0
20
distance: Temporal Distance between prime and target (seconds)
p(p
rim
e=t
arg
et|t
arg
et,d
ista
nce
)
Map Task: 
production-production
Switchboard: 
production-production
Map Task: 
comprehension-production
Switchboard: 
comprehension-production
Figure 1: Left: Estimated priming strength (repetition probability decay rate) for Switchboard and Map
Task, for within-speaker (PP) and between-speaker (CP) priming. Right: Fitted model for the development
of repetition probability (y axis) over time (x axis, in seconds). Here, decay (slope) is the relevant factor for
priming strength, as shown on the left. These are derived from models without FREQ.
Regression allows us not only to show that prim-
ing exists, but it allows us to predict the decline of
repetition probability with increasing distance be-
tween prime and target and depending on other vari-
ables. If we see priming as a form of pre-activation
of syntactic nodes, it indicates the decay rate of pre-
activation. Our method quantifies priming and cor-
relates the effect with secondary factors.
3 Results
3.1 Task-oriented and spontaneous dialogue
Structural repetition between speakers occured in
both corpora and its probability decreases logarith-
mically with the distance between prime and target.
Figure 1 provides the model for the influence
of the four factorial combinations of ROLE and
SOURCE on priming (left) and the development of
repetition probability at increasing distance (right).
SOURCE=Map Task has an interaction effect on the
priming decay ln(DIST), both for PP priming (? =
?0.024, t = ?2.0, p < 0.05) and for CP priming
(? = ?0.059, t = ?4.0, p < 0.0005). (Lower coef-
ficients indicate more decay, hence more priming.)
data points per corpus.
In both corpora, we find positive priming effects.
However, PP priming is stronger, and CP priming is
much stronger in Map Task.
The choice of corpus exhibits a marked interac-
tion with priming effect. Spontaneous conversation
shows significantly less priming than task-oriented
dialogue. We believe this is not a side-effect of vary-
ing grammar size or a different syntactic entropy in
the two types of dialogue, since we examine the de-
cay of repetition probability with increasing distance
(interactions with DIST), and not the overall proba-
bility of chance repetition (intercepts / main effects
except DIST).
3.2 Frequency effects
An additional model was built which included
ln(FREQ) as a predictor that may interact with the
effect coefficient for ln(DIST).
ln(FREQ) is inversely correlated with
the priming effect (Paraphrase: ?lnDist =
?1.05, ?lnDist:lnFreq = 0.54, Map Task:
?lnDist = ?2.18, ?lnDist:lnFreq = 0.35, all
p < 0.001). Priming weakens with higher
(logarithmic) frequency of a syntactic rule.
123
4 Discussion
Evidence from Wizard-of-Oz experiments (with sys-
tems simulated by human operators) have shown
that users of dialogue systems strongly align their
syntax with that of a (simulated) computer (Brani-
gan et al, 2003). Such an effect can be leveraged
in an application, provided there is a priming model
interfacing syntactic processing.
We found evidence of priming in general, that is,
when we assume priming of each phrase structure
rule. The priming effects decay quickly and non-
linearly, which means that a dialogue system would
best only take a relatively short preceding context
into account, e.g., the previous few utterances.
An important consideration in the context of di-
alogue systems is whether user and system collab-
orate on solving a task, such as booking tickets or
retrieving information. Here, syntactic priming be-
tween human speakers is strong, so a system should
implement it. In other situations, systems do not
have to use a unified syntactic architecture for pars-
ing and generation, but bias their output on previous
system utterances, and possibly improve parsing by
looking at previously recognized inputs.
The fact that priming is more pronounced within
(PP) a speaker suggests that optimizing parsing and
generation separately is the most promising avenue
in either type of dialogue system.
One explanation for this lies in a reduced cog-
nitive load of spontaneous, everyday conversation.
Consequently, the more accessible, highly-frequent
rules prime less.
In task-oriented dialogue, speakers need to pro-
duce a common situation model. Interactive Align-
ment Model argues that this process is aided by syn-
tactic priming. In support of this model, we find
more priming in task-oriented dialogue.3
5 Conclusions
Syntactic priming effects are reliably present in di-
alogue even in computational models where the full
range of syntactic rules is considered instead of se-
lected constructions with known strong priming.
This is good news for dialogue systems, which
tend to be task-oriented. Linguistically motivated
3For a more detailed analysis from the perspective of inter-
active alignment, see Reitter et al (2006).
systems can possibly exploit the user?s tendency to
repeat syntactic structures by anticipating repetition.
Future systems may also align their output with their
recognition capabilities and actively align with the
user to signal understanding. Parsers and realizers in
natural language generation modules may make the
most of priming if they respect important factors that
influence priming effects, such as task-orientation of
the dialogue and frequency of the syntactic rule.
Acknowledgements
The authors would like to thank Amit Dubey, Roger Levy and
Martin Pickering. The first author?s work is supported by a grant
from the Edinburgh Stanford Link.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty,
S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller,
C. Sotillo, H. Thompson, and R. Weinert. 1991. The HCRC
Map Task corpus. Language and Speech, 34(4):351?366.
J. Kathryn Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18:355?387.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 1999. Syntactic priming in language production: Ev-
idence for rapid decay. Psychonomic Bulletin and Review,
6(4):635?640.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 2000. Syntactic co-ordination in dialogue. Cognition,
75:B13?25.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson, Janet F.
McLean, and Clifford Nass. 2003. Syntactic alignment be-
tween computers and people: the role of belief about mental
states. In Proceedings of the Twenty-fifth Annual Conference
of the Cognitive Science Society.
Carsten Brockmann, Amy Isard, Jon Oberlander, and Michael
White. 2005. Modelling alignment for affective dialogue. In
Workshop on Adapting the Interaction Style to Affective Fac-
tors at the 10th International Conference on User Modeling
(UM-05). Edinburgh, UK.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Proc.
43th ACL.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In Proc.
ARPA Human Language Technology Workshop.
Martin J. Pickering and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and Brain Sci-
ences, 27:169?225.
David Reitter, Johanna D. Moore, and Frank Keller. 2006. Prim-
ing of syntactic rules in task-oriented dialogue and sponta-
neous conversation. In Proceedings of the 28th Annual Con-
ference of the Cognitive Science Society.
William N. Venables and Brian D. Ripley. 2002. Modern Ap-
plied Statistics with S. Fourth Edition. Springer.
124
Proceedings of NAACL HLT 2007, pages 348?355,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Information Retrieval Approach to Sense Ranking
Mirella Lapata and Frank Keller
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{mlap,keller}@inf.ed.ac.uk
Abstract
In word sense disambiguation, choosing
the most frequent sense for an ambigu-
ous word is a powerful heuristic. However,
its usefulness is restricted by the availabil-
ity of sense-annotated data. In this paper,
we propose an information retrieval-based
method for sense ranking that does not re-
quire annotated data. The method queries
an information retrieval engine to estimate
the degree of association between a word
and its sense descriptions. Experiments on
the Senseval test materials yield state-of-
the-art performance.We also show that the
estimated sense frequencies correlate reli-
ably with native speakers? intuitions.
1 Introduction
Word sense disambiguation (WSD), the ability to
identify the intended meanings (senses) of words
in context, is crucial for accomplishing many NLP
tasks that require semantic processing. Examples in-
clude paraphrase acquisition, discourse parsing, or
metonymy resolution. Applications such as machine
translation (Vickrey et al, 2005) and information re-
trieval (Stokoe, 2005) have also been shown to ben-
efit from WSD.
Given the importance of WSD for basic NLP
tasks and multilingual applications, much work has
focused on the computational treatment of sense
ambiguity, primarily using data-driven methods.
Most accurate WSD systems to date are super-
vised and rely on the availability of training data
(see Yarowsky and Florian 2002; Mihalcea and Ed-
monds 2004 and the references therein). Although
supervised methods typically achieve better perfor-
mance than unsupervised alternatives, their appli-
cability is limited to those words for which sense
labeled data exists, and their accuracy is strongly
correlated with the amount of labeled data avail-
able. Furthermore, current supervised approaches
rarely outperform the simple heuristic of choosing
the most common or dominant sense in the train-
ing data (henceforth ?the first sense heuristic?), de-
spite taking local context into account. One reason
for this is the highly skewed distribution of word
senses (McCarthy et al, 2004a). A large number of
frequent content words is often associated with only
one dominant sense.
Obtaining the first sense via annotation is ob-
viously costly and time consuming. Sense anno-
tated corpora are not readily available for different
languages or indeed sense inventories. Moreover,
a word?s dominant sense will vary across domains
and text genres (the word court in legal documents
will most likely mean tribunal rather than yard).
It is therefore not surprising that recent work (Mc-
Carthy et al, 2004a; Mohammad and Hirst, 2006;
Brody et al, 2006) attempts to alleviate the anno-
tation bottleneck by inferring the first sense auto-
matically from raw text. Automatically acquired first
senses will undoubtedly be noisy when compared to
human annotations. Nevertheless, they can be use-
fully employed in two important tasks: (a) to create
preliminary annotations, thus supporting the ?anno-
tate automatically, correct manually? methodology
used to provide high volume annotation in the Penn
Treebank project; and (b) in combination with super-
vised WSD methods that take context into account;
for instance, such methods could default to the dom-
inant sense for unseen words or words with uninfor-
mative contexts.
This paper focuses on a knowledge-lean sense
ranking method that exploits a sense inventory like
WordNet and corpus data to automatically induce
dominant senses. The proposed method infers the
associations between words and sense descriptions
automatically by querying an IR engine whose in-
dex terms have been compiled from the corpus
of interest. The approach is inexpensive, language-
independent, requires minimal supervision, and uses
no additional knowledge other than the word senses
proper and morphological query expansions. We
348
evaluate our method on two tasks. First, we use
the acquired dominant senses to disambiguate the
meanings of words in the Senseval-2 (Palmer et al,
2001) and Senseval-3 (Snyder and Palmer, 2004)
data sets. Second, we simulate native speakers? intu-
itions about the salience of word meanings and ex-
amine whether the estimated sense frequencies cor-
relate with sense production data. In all cases our ap-
proach outperforms a naive baseline and yields per-
formances comparable to state of the art.
In the following section, we provide an overview
of existing work on sense ranking. In Section 3, we
introduce our IR-based method, and describe several
sense ranking models. In Section 4, we present our
results. Discussion of our results and future work
conclude the paper (Section 5).
2 Related Work
McCarthy et al (2004a) were the first to pro-
pose a computational model for acquiring dominant
senses from text corpora. Key in their approach is
the observation that distributionally similar neigh-
bors often provide cues about a word?s senses. The
model quantifies the degree of similarity between
a word?s sense descriptions and its closest neigh-
bors, thus delivering a ranking over senses where the
most similar sense is intuitively the dominant sense.
Their method exploits two notions of similarity,
distributional and semantic. Distributionally similar
words are acquired from the British National Cor-
pus using an information-theoretic similarity mea-
sure (Lin, 1998) operating over dependency re-
lations (e.g., verb-subject, verb-object). The latter
are obtained from the output of Briscoe and Car-
roll?s (2002) parser. The semantic similarity between
neighbors and senses is measured using a manually
crafted taxonomy such as WordNet (see Budanitsky
and Hirst 2001 for an overview of WordNet-based
similarity measures).
Mohammad and Hirst (2006) propose an algo-
rithm for inferring dominant senses without rely-
ing on distributionally similar neighbors. Their ap-
proach capitalizes on the collocational nature of
semantically related words. Assuming a coarse-
grained sense inventory (e.g., the Macquarie The-
saurus), it first creates a matrix whose columns rep-
resent all categories (senses) c1 . . .cn in the inven-
tory and rows the ambiguous target words w1 . . .wm;
the matrix cells record the number of times a tar-
get word ti co-occurs with category c j within a win-
dow of size s. Using an appropriate statistical test,
they estimate the relative strength of association be-
tween an ambiguous word and each of its senses.
The sense with the highest association is the pre-
dominant sense.
Our work shares with McCarthy et al (2004a) and
Mohammad and Hirst (2006) the objective of infer-
ring dominant senses automatically. We propose a
knowledge-lean method that relies on word associa-
tion and requires no syntactic annotation. The latter
may be unavailable when working with languages
other than English for which state-of-the-art parsers
or taggers have not been developed. Mohammad and
Hirst (2006) estimate the co-occurrence frequency
of a word and its sense descriptors by considering
small window sizes of up to five words. These esti-
mates will be less reliable for moderately frequent
words or for sense inventories with many senses.
Our approach is more robust to sparse data ? we
work with document-based frequencies ? and thus
suitable for both coarse and fine grained sense in-
ventories. Furthermore, it is computationally inex-
pensive; in contrast to McCarthy et al (2004a) we
do not rely on the structure of the sense inventory
for measuring the similarity between synonyms and
their senses. Moreover, unlike Mohammad and Hirst
(2006), our algorithm only requires co-occurrence
frequencies for the target word and its senses, with-
out considering all senses in the inventory and all
words in the corpus simultaneously.
3 Method
3.1 Motivation
Central in our approach is the assumption that con-
text provides important cues regarding a word?s
meaning. The idea dates back at least to Firth (1957)
(?You shall know a word by the company it keeps?)
and underlies most WSD work to date. Another ob-
servation that has found wide application in WSD is
that words tend to exhibit only one sense in a given
discourse or document (Gale et al, 1992). Further-
more, documents are typically written with certain
topics in mind which are often indicated by word
distributional patterns (Harris, 1982).
For example, documents talking about congres-
sional tenure are likely to contain words such as term
of office or incumbency, whereas documents talking
about legal tenure (i.e., the right to hold property)
349
are likely to include the words right or land. Now,
we could estimate which sense of tenure is most
prevalent simply by comparing whether tenure co-
occurs more often with term of office than with land
provided we knew that both of these terms are se-
mantically related to tenure. Fortunately, senses in
WordNet (and related taxonomies) are represented
by synonym terms. So, all we need to do for esti-
mating a word?s sense frequencies is to count how
often it co-occurs with its synonyms. We adopt here
a fairly broad definition of co-occurrence, two words
co-occur if they are attested in the same document.
We could obtain such counts from any document
collection; however, to facilitate comparisons with
prior work (e.g., McCarthy et al 2004a), all our ex-
periments use the British National Corpus (BNC). In
what follows we describe in detail how we retrieve
co-occurrence counts from the BNC and how we ac-
quire dominant senses.
3.2 Dominant Sense Acquisition
Throughout the paper we use the term frequency as a
shorthand for document frequency, i.e., the number
of documents that contain a word or a set of words
which may or may not be adjacent. The method
we propose here exploits document frequencies of
words and their sense definitions. We base our dis-
cussion below on the WordNet sense inventory and
its representation of senses in terms of synonym
sets (synsets). However, our approach is not lim-
ited to this particular lexicon; any dictionary with
synonym-based sense definitions could serve our
purposes.
As an example consider the noun tenure, which
has the following senses in WordNet:
(1) Sense 1
tenure, term of office, incumbency
=> term
Sense 2
tenure, land tenure
=> legal right
The senses are represented by the two synsets
{tenure, term of office, incumbency} and
{tenure, land tenure}. (The hypernyms for each
sense are also listed; indicated by the arrows.) We
can now approximate the frequency with which a
word w1 occurs with the sense s by computing its
synonym frequencies: for each word w2 ? syns(s),
the set of synonyms of s, we field a query of the form
w1 AND w2. These synonym frequencies can then be
used to determine the most frequent sense of w1 in a
variety of ways (to be detailed below).
The synsets for the two senses in (1) give rise to
the queries in (2) and (3). Note that two queries are
generated for the first synset, as it contains two syn-
onyms of the target word tenure.
(2) a. "tenure" AND "term of office"
b. "tenure" AND "incumbency"
(3) "tenure" AND "land tenure"
For example, query (2-a) will return the number of
documents in which tenure and term of office co-
occur. Presumably, tenure is mainly used in its dom-
inant sense in these documents. In the same way,
query (3) will return documents in which tenure is
used in the sense of land tenure. Note that this way
of approximating synonym frequencies as document
frequencies crucially relies on the ?one sense per
discourse? hypothesis (Gale et al, 1992), under the
assumption that a document counts as a discourse
for word sense disambiguation purposes.
Apart from synonym frequencies, we also gener-
ate hypernym frequencies by submitting queries of
the form w1 AND w2, for each w2 ? hype(s), the set of
immediate hypernyms of the sense s. The hypernym
queries for the two senses of tenure are:
(4) "tenure" AND "term"
(5) "tenure" AND "legal right"
Hypernym queries are particularly useful for synsets
of size one, i.e., where a word in a given sense has
no synonyms, and is only differentiated from other
senses by its hypernyms.
Before submitting queries such as the ones in
(2) and (3) to an IR engine, we perform query
expansion to make sure that all relevant in-
flected forms are included. For example the query
term "tenure" is expanded to ("tenure" OR
"tenures"), i.e., both singular and plural noun
forms are generated. Similarly, all inflected verb
forms are generated, e.g., "keep up" gives rise to
the query term ("keep up" OR "keeps up" OR
"keeping up" OR "kept up"). John Carroll?s
suite of morphological tools (morpha and morphg)
is used to generate inflected forms for verbs and
350
nouns.1
The queries generated this way are then submitted
to an IR engine to obtain document counts. Specifi-
cally, we indexed the BNC using GLIMPSE (Global
Implicit Search) a fast and flexible indexing and
query system2 (Manber and Wu, 1994). GLIMPSE
supports approximate and exact matching, Boolean
queries, wild cards, regular expressions, and many
other options. The text is divided into equal size
blocks and an inverted index is created containing
the words and the block numbers in which they oc-
cur. Given a query, GLIMPSE will retrieve the rele-
vant documents using a two-level search method. It
will first locate the query in the inverted index and
then use sequential search to find an exact answer.
Once synonym frequencies and hypernym fre-
quencies are in place, we can compute a word?s pre-
dominant sense in a number of ways. First, we can
vary the way the frequency of a given sense is esti-
mated based on synonym frequencies:
? Sum: The frequency of a given synset is com-
puted as the sum of the synonym frequen-
cies. For example, the frequency of the dom-
inant sense of tenure would be computed by
adding up the document frequencies returned
by queries (2-a) and (2-b).
? Average (Avg): The frequency of a synset is
computed by taking the average of synonym
frequencies.
? Highest (High): The frequency of a synset is
determined by the synonym with the highest
frequency.
Secondly, we can vary whether or not hypernyms are
taken into account:
? No hypernyms (?Hyp): Only the synonym
frequencies are included when computing the
frequency of a synset. For example, only the
queries of (2-a) and (2-b) are relevant for esti-
mating the dominant sense of tenure.
? Hypernyms (+Hyp): Both synonym and hy-
pernym frequencies are taken into account
1The tools can be downloaded from http://www.
informatics.susx.ac.uk/research/nlp/carroll/
morph.html.
2The software can be downloaded from http:
//webglimpse.net/download.php
when computing sense frequency. For example,
the frequency for the senses of tenure would
be computed based on the document frequen-
cies returned by queries (2-a), (2-b), and (4)
(by summing, averaging, or taking the highest
value, as before).
The third option relates to whether the sense fre-
quencies are used in raw or in normalized form:
? Non-normalized (?Norm): The raw synonym
frequencies are used as estimates of sense fre-
quencies.
? Normalized (+Norm): Sense frequencies are
computed by dividing the word-synonym fre-
quency by the frequency of the synonym in
isolation. For example, the normalized fre-
quency for (2-a) is computed by dividing
the document frequency for "tenure" AND
"term of office" by the document fre-
quency for "term of office". Normalizing
takes into account the fact that the members of
the synset of a sense may differ in frequency.
The combination of the above parameters yields 12
sense ranking models. We explore the parameter
space exhaustively on the Senseval-2 benchmark
data set. The best performing model on this data set
is then used in all our subsequent experiments. We
use Senseval-2 as a development set, but we also
demonstrate that a far smaller manually annotated
sample is sufficient for selecting the best model.
4 Experiments
Our experiments were driven by three questions:
(1) Is WSD feasible at all with a model that does
not employ any syntactic or semantic knowledge?
Recall that McCarthy et al (2004a) propose a model
that crucially relies on a robust parser for estimat-
ing dominant senses. (2) What is the best parameter
setting for our model? (3) Do the acquired dominant
senses correlate with human judgments? If our sense
frequencies exhibit no such correlation, it is unlikely
that they will be useful in practical applications.
To address the first two questions we use the in-
duced first senses to perform WSD on the Senseval-
2 and Senseval-3 data sets. For our third question we
compare native speakers? semantic intuitions against
the BNC sense frequencies.
351
?Norm +Norm
+Hyp ?Hyp +Hyp ?Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.9 44.3 48.6 46.8
High 51.6 49.8 51.1 49.3 57.2 55.1 59.7 57.6
Avg 44.1 42.6 48.5 46.8 49.6 47.8 51.5 49.6
Table 1: Results for Senseval-2 data by model in-
stantiation
4.1 Model Selection
The goal of our first experiment is to establish which
model configuration (see Section 3.2) is best suited
for the WSD task. We thus varied how the overall
frequency is computed (Sum, High, Avg), whether
hyponyms are included (?Hyp), and whether the
frequencies are normalized (?Norm). To explore the
parameter space, we used the Senseval-2 all-words
test data as our development set. This data set con-
sists of three documents from the Wall Street Jour-
nal containing approximately 2,400 content words.
Following McCarthy et al (2004a), we first use our
method to find the dominant sense for all word types
in the corpus and then use that sense to disambiguate
tokens without taking contextual information into
account. We used WordNet 1.7.1 (Fellbaum, 1998)
senses.3
We compared our results to a baseline that se-
lects for each word type a random sense, assumes
it is the dominant one, and uses it to disambiguate
all instances of the target word (McCarthy et al,
2004a). We also report the WSD performance of a
more competitive baseline that always chooses the
sense with the largest synset as the dominant sense.
Consider again the word tenure from Section 3.2.
According to this baseline, the dominant sense for
tenure is the first one since it is represented by the
largest synset (three members).
Our results on Senseval-2 are summarized in Ta-
ble 1. We observe that models that do not include
hypernyms yield consistently better precision and
recall than models that include them. On the one
hand, hypernyms render the estimated sense distri-
butions less sparse. On the other hand, they intro-
duce considerable noise; the resulting sense frequen-
cies are often similar ? the same hypernyms can be
3Senseval-2 is annotated with WordNet 1.7 senses which
we converted to 1.7.1 using a publicly available mapping (see
http://www.cs.unt.edu/?rada/downloads.html).
BaseR BaseS Model
P R P R P R N
Noun 26.8 25.4 45.8 43.4 53.1?# 50.2?# 1,063
Verb 11.2 11.1 19.9 19.5 48.2?# 47.3?# 569
Adj 22.1 21.4 56.5 56.0 56.7? 56.2? 451
Adv 48.0 45.9 66.4 62.9 86.4?# 81.8?# 301
All 26.3 25.4 42.2 40.7 59.7?# 57.6?# 2,384
Table 2: Results of best model (High, +Norm,
?Hyp) for Senseval-2 data by part of speech
(?: sig. diff. from BaseR, #: sig. diff. from BaseS;
p < 0.01 using ?2 test)
shared among several senses ? and selecting one pre-
dominant sense over the other can be due to very
small frequency differences. We also find that mod-
els with normalized document counts outperform
models without normalization. This is not surpris-
ing, there is ample evidence in the literature (Mo-
hammad and Hirst, 2006; Turney, 2001) that associ-
ation measures (e.g., conditional probability, mutual
information) are better indicators of lexical similar-
ity than raw frequency. Finally, selecting the syn-
onym with the highest frequency (and defaulting to
its sense) achieves better results in comparison to av-
eraging or summing over all synsets.
In sum, the best performing model is High,
+Norm, ?Hyp, achieving a precision of 59.7% and
a recall of 57.9%. The results for this model are bro-
ken down by part of speech in Table 2. Here, we
also include a comparison with the random base-
line (BaseR) and a baseline that selects the dominant
sense by synset size (BaseS). We observe that the
optimal model significantly outperforms both base-
lines on the complete data set (see row All in Ta-
ble 2) and on most individual parts of speech (perfor-
mances are comparable for our model and BaseS on
adjectives). BaseS is far better than BaseR and gen-
erally harder to beat. Defaulting to synset size in the
absence of any other information is a good heuristic;
large synsets often describe frequent senses. Vari-
ants of our model that select a dominant sense by
summing over synset members are closest to this
baseline. Note that our best performing model does
not rely on synset size; it simply selects the synonym
with the highest frequency, despite the fact that it
might belong to a large or small synset. We con-
jecture that its superior performance is due to the
collocational nature of semantic similarity (Turney,
352
?Norm +Norm
+Hyp ?Hyp +Hyp ?Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.2 44.7 44.6 44.0
High 51.6 49.8 51.1 49.3 55.0 54.3 61.3 60.5
Avg 44.1 42.6 48.5 46.8 51.5 50.8 50.4 49.8
Table 3: Results for 10% of Senseval-2 data by
model instantiation
2001).
In order to establish that High, +Norm, ?Hyp is
the optimal model, we utilized the whole Senseval-
2 data set. Using such a large dataset is more likely
to yield a stable parameter setting, but it also raises
the question whether parameter optimization could
take place on a smaller dataset which is less costly
to produce. Table 3 explores the parameter space on
a sample randomly drawn from Senseval-2 that con-
tains only 240 tokens (i.e., one tenth of the original
data set). The behavior of our models on this smaller
sample is comparable to that on the entire Senseval-
2 data. Importantly, both sets yield the same best
model, i.e., High, +Norm, ?Hyp. In the remainder
of this paper we will use this model for further ex-
periments without additional parameter tuning.
4.2 Application to Senseval-3 Data
We next evaluate our best model the on the
Senseval-3 English all-words data set. Senseval-3
consists of two Wall Street Journal articles and
one excerpt from the Brown corpus (approximately
5,000 content words in total). Similarly to the ex-
periments reported in the previous section, we used
WordNet 1.7.1. We calculate recall and precision
with the Senseval-3 scorer.
Our results are given in Table 4. Besides the
two baselines (BaseR and BaseS), we also com-
pare our model to McCarthy et al (2004b)4 and
the best unsupervised (IRST-DDD) and supervised
(GAMBLE) systems that participated in Senseval-3.
IRST-DDD was developed by Strapparava et al
(2004) and performs domain driven disambiguation.
Specifically, the approach compares the domain of
the context surrounding the target word with the do-
mains of its senses and uses a version of WordNet
4Comparison against Mohammad and Hirst (2006) was not
possible since they use a sense inventory other than WordNet
(i.e., Roget?s thesaurus) and evaluate their model on artificially
generated sense-tagged data.
P R
BaseR 23.1#?$? 22.7#?$?
BaseS 36.6??$? 35.9??$?
McCarthy 49.0?#$? 43.0?#$?
IR-Model 58.0?#?? 57.0?#??
IRST-DDD 58.3?#?? 58.2?#??
Semcor 62.4?#?$ 62.4?#?$
GAMBLE 65.1?#?$? 65.2?#?$?
Table 4: Comparison of results on Senseval-3 data
(?: sig. diff. from BaseR, #: sig. diff. from BaseS,
?: sig. diff. from McCarthy, $: sig. diff. from IR-
Model, ?: sig. diff. from SemCor; p < 0.01 using
?2 test)
BaseR BaseS Model
P R P R P R N
Noun 27.8 12.2 41.1 41.0 58.1?# 58.0?# 900
Verb 12.8 4.6 20.0 19.9 61.0?# 60.8?# 732
Adj 29.2 5.2 56.5 56.5 50.3? 50.3? 363
Adv 100.0 0.6 100.0 81.2 100.0 81.2 16
All 23.1 22.7 36.6 35.9 58.0?# 57.0?# 2,011
Table 5: Results of best model (High, +Norm,
?Hyp) for Senseval-3 data by part of speech
(?: sig. diff. from BaseR, #: sig. diff. from BaseS;
p < 0.01 using ?2 test)
augmented with domain labels (e.g., economy, ge-
ography). GAMBL (Decadt et al, 2004) is a super-
vised system: a classifier is trained for each ambigu-
ous word using memory-based learning. We also re-
port the performance achieved by defaulting to the
first WordNet entry for a given word and part of
speech. Entries in WordNet are ranked according
to the sense frequency estimates obtained from the
manually annotated SemCor corpus. First senses ob-
tained from SemCor will be naturally less noisy than
those computed by our method which does not make
use of manual annotation in any way. We therefore
consider the WSD performance achieved with Sem-
Cor first senses as an upper bound for automatically
acquired first senses.
Our model significantly outperforms the two
baselines and McCarthy et al (2004b). Its precision
and recall according to individual parts of speech is
shown in Table 5. The model performs comparably
to IRST-DDD and significantly worse than GAM-
BLE. This is not entirely surprising given that GAM-
353
BLE is a supervised system trained on a variety
of manually annotated resources including SemCor,
data from previous Senseval workshops and the ex-
ample sentences in WordNet 1.7.1. GAMBLE is the
only system that significantly outperforms the Sem-
Cor upper bound. Finally, note that our model is
conceptually simpler than McCarthy et al (2004b)
and IRST-DDD. It neither requires a parser (for ob-
taining distributionally similar neighbors) nor any
knowledge other than WordNet (e.g., domain la-
bels). This makes our method portable to languages
for which syntactic analysis tools and elaborate se-
mantic resources are not available.
4.3 Modeling Human Data
Research in psycholinguistics has shown that the
meanings of ambiguous words are not perceived as
equally salient in the absence of a biasing context
(Durkin and Manning, 1989; Twilley et al, 1994).
Rather, language users often ascribe dominant and
subordinate meanings to polysemous words. Previ-
ous studies have elicited intuitions with regard to
word senses using a free association task. For ex-
ample, Durkin and Manning (1989) collected asso-
ciation norms from native speakers for 175 ambigu-
ous words. They asked subjects to read each word
and write down the first meaning that came to mind.
The words were presented out of context. From the
subjects? responses, they computed sense frequen-
cies, which revealed that most words were attributed
a particular meaning with a markedly higher fre-
quency than other meanings.
In this experiment, we examine whether our
model agrees with human intuitions regarding the
prevalence of word senses. We inferred the dominant
meanings for the polysemous words used in Durkin
and Manning (1989). These exhibit a relatively high
degree of ambiguity (the average number of senses
per word is three) and cover a wide variety of parts
of speech (for the full set of words and elicited
sense frequencies see their Appendix A, pp. 501?
609). One stumbling block to using this data are
the meanings associated with the ambiguous words.
These were provided by native English speakers and
may not necessarily correspond to senses described
by trained lexicographers. Fortunately, we were able
to map most of them (except for six which we dis-
carded) on WordNet synsets (version 1.6); two an-
notators performed the mapping by comparing the
sense descriptions provided by Durkin and Manning
act Freq answer Freq
pretense/performance 37 response 81
to perform 30 solution 18
to take action 16
division 12
a deed 3
Table 6: Meaning frequencies for act and answer;
normative data from Durkin and Manning (1989)
to WordNet synsets. The annotators agreed in their
assignments 81% of the time. Disagreements were
resolved through mediation.
Examples of Durkin and Manning?s (1989)
normative data are given in Table 6. The sense
response for answer was mapped to the WordNet
synset {answer, reply, response} (Sense 1),
the sense solution was mapped to the synset
{solution, answer, result, resolution,
solvent} (Sense 2), etc. Durkin and Manning did
not take part of speech ambiguity into account, as
Table 6 shows, subjects came up with meanings
relating to the verb and noun part of speech of act.
We explored the relationship between the sense
frequencies provided by human subjects and those
estimated by our model by computing the Spearman
rank correlation coefficient ?. We obtained sense
frequencies from the BNC using the best model
from Section 4.1 (High, +Norm, ?Hyp). We found
that the resulting sense frequencies were signifi-
cantly correlated with the human sense frequencies
(? = 0.384, p < 0.01). We performed the same ex-
periment using McCarthy et al?s (2004a) model,
which also achieved a significant correlation (? =
0.316, p < 0.01). This result provides an additional
validation of our model as it demonstrates that the
sense frequencies it generates can capture the sense
preferences of naive human subjects (rather than
trained lexicographers).
5 Discussion
In this paper we proposed an IR-based approach
for inducing dominant senses automatically. Our
method estimates the degree of association between
words and their sense descriptions (represented by
synsets in WordNet) simply by querying an IR en-
gine. Evaluation on the Senseval data sets showed
that our model significantly outperformed a naive
random sense baseline and a more competitive one
354
based on synset size. Our method was significantly
better than McCarthy et al (2004b) on Senseval-2
and Senseval-3. On the latter data set, its perfor-
mance was comparable to that of the best unsuper-
vised system (Strapparava et al, 2004).
An important future direction lies in evaluating
the disambiguation potential of our models across
domains and languages. Furthermore, our experi-
ments have relied on WordNet for providing the
appropriate sense descriptions. Future work must
assess whether the models presented in this pa-
per can be extended to alternative sense invento-
ries (e.g., dictionary definitions) that may differ in
granularity and structure. We will also experiment
with a wider range of lexical association measures
for quantifying the similarity of a word and its
synonyms. Examples include odds ratio (Moham-
mad and Hirst, 2006) and Turney?s (2001) IR-based
pointwise mutual information (PMI-IR).
Our experiments revealed that the IR-based model
is particularly good at disambiguating certain parts
of speech (e.g., verbs, see Tables 2 and 5). A promis-
ing direction is the combination of different ranking
models (Brody et al, 2006) and the integration of
dominant sense models with supervised WSD.
Acknowledgments We are grateful to Diana Mc-
Carthy for her help with this work. The au-
thors acknowledge the support of EPSRC (grant
EP/C538447/1).
References
Briscoe, Ted and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evaluation.
Las Palmas, Gran Canaria, pages 1499?1504.
Brody, Samuel, Roberto Navigli, and Mirella Lapata. 2006. En-
semble methods for unsupervised WSD. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics. Sydney, Australia, pages 97?
104.
Budanitsky, Alexander and Graeme Hirst. 2001. Semantic
distance in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the NAACL
Workshop on WordNet and Other Lexical Resources. Pitts-
burgh, PA.
Decadt, Bart, Ve?ronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In Mihalcea and Edmonds
(2004), pages 108?112.
Durkin, Kevin and Jocelyn Manning. 1989. Polysemy and
the subjective lexicon: Semantic relatedness and the salience
of intraword senses. Journal of Psycholinguistic Research
18(6):577?612.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gale, William A., Kenneth W. Church, and David Yarowsky.
1992. A method for disambiguating word senses in a large
corpus. Computers and the Humanities 26(5?6):415?439.
Harris, Zellig. 1982. Discourse and sublanguage. In R. Kit-
tredge and J. Lehrberger, editors, Language in Restricted
Semantic Domains, Walter de Gruyter, Berlin; New York,
pages 231?236.
Lin, Dekang. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of the 15th International Conference
on Machine Learning. Madison, WI, pages 296?304.
Manber, Udi and Sun Wu. 1994. GLIMPSE: a tool to search
through entire file systems. In Proceedings of USENIX Win-
ter 1994 Technical Conference. San Francisco, CA, pages
23?32.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004a. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics. Barcelona, pages 279?286.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004b. Using automatically acquired predominant senses
for word sense disambiguation. In Mihalcea and Edmonds
(2004), pages 151?154.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of Senseval-3: The 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of Text.
Barcelona.
Mohammad, Saif and Graeme Hirst. 2006. Determining word
sense dominance using a thesaurus. In Proceedings of the
11th Conference of the European Chapter of the Association
for Computational Linguistics. Trento, Italy, pages 121?128.
Palmer, Martha, Christiane Fellbaum, Scott Cotton, Lauren
Delfs, and Hoa Trang Dang. 2001. English tasks: All words
and verb lexical sample. In Proceedings of Senseval-2: The
3rd International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text. Toulouse.
Snyder, Benjamin and Martha Palmer. 2004. The English all-
words task. In Mihalcea and Edmonds (2004).
Stokoe, Christopher. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of the Human
Language Technology Conference and the Conference on
Empirical Methods in Natural Language Processing. Van-
couver, pages 403?410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano. 2004.
Word-sense disambiguation for machine translation. In Mi-
halcea and Edmonds (2004), pages 229?234.
Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR
versus LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning. Freiburg, Germany, pages
491?502.
Twilley, L. C., P. Dixon, D. Taylor, and K. Clark. 1994. Univer-
sity of Alberta norms of relative meaning frequency for 566
homographs. Memory and Cognition 22(1):111?126.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine trans-
lation. In Proceedings of the Human Language Technology
Conference and the Conference on Empirical Methods in
Natural Language Processing. Vancouver, pages 771?778.
Yarowsky, David and Radu Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter spaces. Natural Lan-
guage Engineering 9(4):293?310.
355
Probabilistic Parsing for German using Sister-Head Dependencies
Amit Dubey
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
adubey@coli.uni-sb.de
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
We present a probabilistic parsing model
for German trained on the Negra tree-
bank. We observe that existing lexicalized
parsing models using head-head depen-
dencies, while successful for English, fail
to outperform an unlexicalized baseline
model for German. Learning curves show
that this effect is not due to lack of training
data. We propose an alternative model that
uses sister-head dependencies instead of
head-head dependencies. This model out-
performs the baseline, achieving a labeled
precision and recall of up to 74%. This in-
dicates that sister-head dependencies are
more appropriate for treebanks with very
flat structures such as Negra.
1 Introduction
Treebank-based probabilistic parsing has been the
subject of intensive research over the past few years,
resulting in parsing models that achieve both broad
coverage and high parsing accuracy (e.g., Collins
1997; Charniak 2000). However, most of the ex-
isting models have been developed for English and
trained on the Penn Treebank (Marcus et al, 1993),
which raises the question whether these models
generalize to other languages, and to annotation
schemes that differ from the Penn Treebank markup.
The present paper addresses this question by
proposing a probabilistic parsing model trained on
Negra (Skut et al, 1997), a syntactically annotated
corpus for German. German has a number of syn-
tactic properties that set it apart from English, and
the Negra annotation scheme differs in important re-
spects from the Penn Treebank markup. While Ne-
gra has been used to build probabilistic chunkers
(Becker and Frank, 2002; Skut and Brants, 1998),
the research reported in this paper is the first attempt
to develop a probabilistic full parsing model for Ger-
man trained on a treebank (to our knowledge).
Lexicalization can increase parsing performance
dramatically for English (Carroll and Rooth, 1998;
Charniak, 1997, 2000; Collins, 1997), and the lexi-
calized model proposed by Collins (1997) has been
successfully applied to Czech (Collins et al, 1999)
and Chinese (Bikel and Chiang, 2000). However, the
resulting performance is significantly lower than the
performance of the same model for English (see Ta-
ble 1). Neither Collins et al (1999) nor Bikel and
Chiang (2000) compare the lexicalized model to an
unlexicalized baseline model, leaving open the pos-
sibility that lexicalization is useful for English, but
not for other languages.
This paper is structured as follows. Section 2 re-
views the syntactic properties of German, focusing
on its semi-flexible wordorder. Section 3 describes
two standard lexicalized models (Carroll and Rooth,
1998; Collins, 1997), as well as an unlexicalized
baseline model. Section 4 presents a series of experi-
ments that compare the parsing performance of these
three models (and several variants) on Negra. The
results show that both lexicalized models fail to out-
perform the unlexicalized baseline. This is at odds
with what has been reported for English. Learning
curves show that the poor performance of the lexi-
calized models is not due to lack of training data.
Section 5 presents an error analysis for Collins?s
(1997) lexicalized model, which shows that the
head-head dependencies used in this model fail to
cope well with the flat structures in Negra. We pro-
pose an alternative model that uses sister-head de-
pendencies instead. This model outperforms the two
original lexicalized models, as well as the unlexical-
ized baseline. Based on this result and on the review
of the previous literature (Section 6), we argue (Sec-
tion 7) that sister-head models are more appropriate
for treebanks with very flat structures (such as Ne-
gra), typically used to annotate languages with semi-
free wordorder (such as German).
2 Parsing German
2.1 Syntactic Properties
German exhibits a number of syntactic properties
that distinguish it from English, the language that
has been the focus of most research in parsing.
Prominent among these properties is the semi-free
Language Size LR LP Source
English 40,000 87.4% 88.1% (Collins, 1997)
Chinese 3,484 69.0% 74.8% (Bikel and Chiang, 2000)
Czech 19,000 ?- 80.0% ?- (Collins et al, 1999)
Table 1: Results for the Collins (1997) model for
various languages (dependency precision for Czech)
wordorder, i.e., German wordorder is fixed in some
respects, but variable in others. Verb order is largely
fixed: in subordinate clauses such as (1a), both the
finite verb hat ?has? and the non-finite verb kom-
poniert ?composed? are in sentence final position.
(1) a. Weil
because
er
er
gestern
yesterday
Musik
music
komponiert
composed
hat.
has
?Because he has composed music yesterday.?
b. Hat er gestern Musik komponiert?
c. Er hat gestern Musik komponiert.
In yes/no questions such as (1b), the finite verb is
sentence initial, while the non-finite verb is sen-
tence final. In declarative main clauses (see (1c)), on
the other hand, the finite verb is in second position
(i.e., preceded by exactly one constituent), while the
non-finite verb is final.
While verb order is fixed in German, the order
of complements and adjuncts is variable, and influ-
enced by a variety of syntactic and non-syntactic
factors, including pronominalization, information
structure, definiteness, and animacy (e.g., Uszkor-
eit 1987). The first position in a declarative sen-
tence, for example, can be occupied by various con-
stituents, including the subject (er ?he? in (1c)), the
object (Musik ?music? in (2a)), an adjunct (gestern
?yesterday? in (2b)), or the non-finite verb (kom-
poniert ?composed? in (2c)).
(2) a. Musik hat er gestern komponiert.
b. Gestern hat er Musik komponiert .
c. Komponiert hat er gestern Musik.
The semi-free wordorder in German means that a
context-free grammar model has to contain more
rules than for a fixed wordorder language. For tran-
sitive verbs, for instance, we need the rules S !
V NP NP, S ! NP V NP, and S ! NP NP V to
account for verb initial, verb second, and verb final
order (assuming a flat S, see Section 2.2).
2.2 Negra Annotation Scheme
The Negra corpus consists of around 350,000 words
of German newspaper text (20,602 sentences). The
annotation scheme (Skut et al, 1997) is modeled to a
certain extent on that of the Penn Treebank (Marcus
et al, 1993), with crucial differences. Most impor-
tantly, Negra follows the dependency grammar tra-
dition in assuming flat syntactic representations:
(a) There is no S ! NP VP rule. Rather, the sub-
ject, the verb, and its objects are all sisters of each
other, dominated by an S node. This is a way of
accounting for the semi-free wordorder of German
(see Section 2.1): the first NP within an S need not
be the subject.
(b) There is no SBAR ! Comp S rule. Main
clauses, subordinate clauses, and relative clauses all
share the category S in Negra; complementizers and
relative pronouns are simply sisters of the verb.
(c) There is no PP ! P NP rule, i.e., the prepo-
sition and the noun it selects (and determiners and
adjectives, if present) are sisters, dominated by a
PP node. An argument for this representation is that
prepositions behave like case markers in German; a
preposition and a determiner can merge into a single
word (e.g., in dem ?in the? becomes im).
Another idiosyncrasy of Negra is that it assumes
special coordinate categories. A coordinated sen-
tence has the category CS, a coordinate NP has the
category CNP, etc. While this does not make the
annotation more flat, it substantially increases the
number of non-terminal labels. Negra also contains
grammatical function labels that augment phrasal
and lexical categories. Example are MO (modifier),
HD (head), SB (subject), and OC (clausal object).
3 Probabilistic Parsing Models
3.1 Probabilistic Context-Free Grammars
Lexicalization has been shown to improve pars-
ing performance for the Penn Treebank (e.g., Car-
roll and Rooth 1998; Charniak 1997, 2000; Collins
1997). The aim of the present paper is to test if this
finding carries over to German and to the Negra cor-
pus. We therefore use an unlexicalized model as our
baseline against which to test the lexicalized models.
More specifically, we used a standard proba-
bilistic context-free grammar (PCFG; see Charniak
1993). Each context-free rule RHS ! LHS is anno-
tated with an expansion probability P(RHSjLHS).
The probabilities for all rules with the same lefthand
side have to sum to one, and the probability of a
parse tree T is defined as the product of the prob-
abilities of all rules applied in generating T .
3.2 Carroll and Rooth?s Head-Lexicalized
Model
The head-lexicalized PCFG model of Carroll and
Rooth (1998) is a minimal departure from the stan-
dard unlexicalized PCFG model, which makes it
ideal for a direct comparison.1
A grammar rule LHS ! RHS can be written as
P ! C1 . . .Cn, where P is the mother category, and
C1 . . .Cn are daughters. Let l(C) be the lexical head
1Charniak (1997) proposes essentially the same model; we
will nevertheless use the label ?Carroll and Rooth model? as we
are using their implementation (see Section 4.1).
of the constituent C. The rule probability is then de-
fined as (see also Beil et al 2002):
P(RHSjLHS) = Prule(C1 . . .CnjP, l(P))(3)

n
?
i=1
Pchoice(l(Ci)jCi,P, l(P))
Here Prule(C1 . . .CnjP, l(P)) is the probability that
category P with lexical head l(P) is expanded by the
rule P ! C1 . . .Cn, and Pchoice(l(C)jC,P, l(P)) is the
probability that the (non-head) category C has the
lexical head l(C) given that its mother is P with lex-
ical head l(P).
3.3 Collins?s Head-Lexicalized Model
In contrast to Carroll and Rooth?s (1998) approach,
the model proposed by Collins (1997) does not com-
pute rule probabilities directly. Rather, they are gen-
erated using a Markov process that makes certain in-
dependence assumptions. A grammar rule LHS !
RHS can be written as P ! Lm . . .L1 H R1 . . .Rn
where P is the mother and H is the head daughter.
Let l(C) be the head word of C and t(C) the tag of
the head word of C. Then the probability of a rule is
defined as:
P(RHSjLHS) = P(Lm . . .L1 H R1 . . .RnjP)(4)
= Ph(HjP)Pl(Lm . . .L1jP,H)Pr(R1 . . .RnjP,H)
= Ph(HjP)
m
?
i=0
Pl(LijP,H,d(i))
n
?
i=0
Pr(RijP,H,d(i))
Here, Ph is the probability of generating the head,
and Pl and Pr are the probabilities of generating the
nonterminals to the left and right of the head, re-
spectively; d(i) is a distance measure. (L0 and R0 are
stop categories.) At this point, the model is still un-
lexicalized. To add lexical sensitivity, the Ph, Pr and
Pl probability functions also take into account head
words and their POS tags:
P(RHSjLHS) = Ph(HjP,t(P), l(P))(5)

m
?
i=0
Pl(Li,t(Li), l(Li)jP,H,t(H), l(H),d(i))

n
?
i=0
Pr(Ri,t(Ri), l(Ri)jP,H,t(H), l(H),d(i))
4 Experiment 1
This experiment was designed to compare the per-
formance of the three models introduced in the
last section. Our main hypothesis was that the lex-
icalized models will outperform the unlexicalized
baseline model. Another prediction was that adding
Negra-specific information to the models will in-
crease parsing performance. We therefore tested a
model variant that included grammatical function la-
bels, i.e., the set of categories was augmented by the
function tags specified in Negra (see Section 2.2).
Adding grammatical functions is a way of deal-
ing with the wordorder facts of German (see Sec-
tion 2.1) in the face of Negra?s very flat annota-
tion scheme. For instance, subject and object NPs
have different wordorder preferences (subjects tend
to be preverbal, while objects tend to be postver-
bal), a fact that is captured if subjects have the la-
bel NP-SB, while objects are labeled NP-OA (ac-
cusative object), NP-DA (dative object), etc. Also
the fact that verb order differs between subordinate
and main clauses is captured by the function labels:
the former are labeled S, while the latter are labeled
S-OC (object clause), S-RC (relative clause), etc.
Another idiosyncrasy of the Negra annotation is
that conjoined categories have separate labels (S and
CS, NP and CNP, etc.), and that PPs do not contain
an NP node. We tested a variant of the Carroll and
Rooth (1998) model that takes this into account.
4.1 Method
Data Sets All experiments reported in this paper
used the treebank format of Negra. This format,
which is included in the Negra distribution, was de-
rived from the native format by replacing crossing
branches with traces. We split the corpus into three
subsets. The first 18,602 sentences constituted the
training set. Of the remaining 2,000 sentences, the
first 1,000 served as the test set, and the last 1000 as
the development set. To increase parsing efficiency,
we removed all sentences with more than 40 words.
This resulted in a test set of 968 sentences and a
development set of 975 sentences. Early versions
of the models were tested on the development set,
and the test set remained unseen until all parameters
were fixed. The final results reported this paper were
obtained on the test set, unless stated otherwise.
Grammar Induction For the unlexicalized PCFG
model (henceforth baseline model), we used the
probabilistic left-corner parser Lopar (Schmid,
2000). When run in unlexicalized mode, Lopar im-
plements the model described in Section 3.1. A
grammar and a lexicon for Lopar were read off the
Negra training set, after removing all grammatical
function labels. As Lopar cannot handle traces, these
were also removed from the training data.
The head-lexicalized model of Carroll and Rooth
(1998) (henceforth C&R model) was again realized
using Lopar, which in lexicalized mode implements
the model in Section 3.2. Lexicalization requires that
each rule in a grammar has one of the categories on
its righthand side annotated as the head. For the cate-
gories S, VP, AP, and AVP, the head is marked in Ne-
gra. For the other categories, we used rules to heuris-
tically determine the head, as is standard practice for
the Penn Treebank.
The lexicalized model proposed by Collins (1997)
(henceforth Collins model) was re-implemented by
one of the authors. For training, empty categories
were removed from the training data, as the model
cannot handle them. The same head finding strategy
was applied as for the C&R model.
In this experiment, only head-head statistics were
used (see (5)). The original Collins model uses
sister-head statistics for non-recursive NPs. This will
be discussed in detail in Section 5.
Training and Testing For all three models, the
model parameters were estimated using maximum
likelihood estimation. Both Lopar and the Collins
model use various backoff distributions to smooth
the estimates. The reader is referred to Schmid
(2000) and Collins (1997) for details. For the C&R
model, we used a cutoff of one for rule frequencies
Prule and lexical choice frequencies Pchoice (the cutoff
value was optimized on the development set).
We also tested variants of the baseline model and
the C&R model that include grammatical function
information, as we hypothesized that this informa-
tion might help the model to handle wordorder vari-
ation more adequately, as explained above.
Finally, we tested variant of the C&R model that
uses Lopar?s parameter pooling feature. This fea-
ture makes it possible to collapse the lexical choice
distribution Pchoice for either the daughter or the
mother categories of a rule (see Section 3.2). We
pooled the estimates for pairs of conjoined and non-
conjoined daughter categories (S and CS, NP and
CNP, etc.): these categories should be treated as the
same daughters; e.g., there should be no difference
between S !NP V and S!CNP V. We also pooled
the estimates for the mother categories NPs and PPs.
This is a way of dealing with the fact that there is no
separate NP node within PPs in Negra.
Lopar and the Collins model differ in their han-
dling of unknown words. In Lopar, a POS tag distri-
bution for unknown words has to be specified, which
is then used to tag unknown words in the test data.
The Collins model treats any word seen fewer than
five times in the training data as unseen and uses an
external POS tagger to tag unknown words. In order
to make the models comparable, we used a uniform
approach to unknown words. All models were run
on POS-tagged input; this input was created by tag-
ging the test set with a separate POS tagger, for both
known and unknown words. We used TnT (Brants,
2000), trained on the Negra training set. The tagging
accuracy was 97.12% on the development set.
In order to obtain an upper bound for the perfor-
mance of the parsing models, we also ran the parsers
on the test set with the correct tags (as specified in
Negra), again for both known and unknown words.
We will refer to this mode as ?perfect tagging?.
All models were evaluated using standard PAR-
SEVAL measures. We report labeled recall (LR)
labeled precision (LP), average crossing brackets
(CBs), zero crossing brackets (0CB), and two or less
crossing brackets (2CB). We also give the cover-
age (Cov), i.e., the percentage of sentences that the
parser was able to parse.
4.2 Results
The results for all three models and their variants
are given in Table 2, for both TnT tags and per-
fect tags. The baseline model achieves 70.56% LR
and 66.69% LP with TnT tags. Adding grammatical
functions reduces both figures slightly, and cover-
age drops by about 15%. The C&R model performs
worse than the baseline, at 68.04% LR and 60.07%
LP (for TnT tags). Adding grammatical function
again reduces performance slightly. Parameter pool-
ing increases both LR and LP by about 1%. The
Collins models also performs worse than the base-
line, at 67.91% LR and 66.07% LP.
Performance using perfect tags (an upper bound
of model performance) is 2?3% higher for the base-
line and for the C&R model. The Collins model
gains only about 1%. Perfect tagging results in a per-
formance increase of over 10% for the models with
grammatical functions. This is not surprising, as the
perfect tags (but not the TnT tags) include grammat-
ical function labels. However, we also observe a dra-
matic reduction in coverage (to about 65%).
4.3 Discussion
We added grammatical functions to both the base-
line model and the C&R model, as we predicted
that this would allow the model to better capture the
wordorder facts of German. However, this predic-
tion was not borne out: performance with grammat-
ical functions (on TnT tags) was slightly worse than
without, and coverage dropped substantially. A pos-
sible reason for this is sparse data: a grammar aug-
mented with grammatical functions contains many
additional categories, which means that many more
parameters have to be estimated using the same
training set. On the other hand, a performance in-
crease occurs if the tagger also provides grammati-
cal function labels (simulated in the perfect tags con-
dition). However, this comes at the price of an unac-
ceptable reduction in coverage.
When training the C&R model, we included a
variant that makes use of Lopar?s parameter pool-
ing feature. We pooled the estimates for conjoined
daughter categories, and for NP and PP mother cat-
egories. This is a way of taking the idiosyncrasies of
the Negra annotation into account, and resulted in a
small improvement in performance.
The most surprising finding is that the best per-
formance was achieved by the unlexicalized PCFG
TnT tagging Perfect tagging
LR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB Cov
Baseline 70.56 66.69 1.03 58.21 84.46 94.42 72.99 70.00 0.88 60.30 87.42 95.25
Baseline + GF 70.45 65.49 1.07 58.02 85.01 79.24 81.14 78.37 0.46 74.25 95.26 65.39
C&R 68.04 60.07 1.31 52.08 79.54 94.42 70.79 63.38 1.17 54.99 82.21 95.25
C&R + pool 69.07 61.41 1.28 53.06 80.09 94.42 71.74 64.73 1.11 56.40 83.08 95.25
C&R + GF 67.66 60.33 1.31 55.67 80.18 79.24 81.17 76.83 0.48 73.46 94.15 65.39
Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23
Table 2: Results for Experiment 1: comparison of lexicalized and unlexicalized models (GF: grammatical
functions; pool: parameter pooling for NPs/PPs and conjoined categories)
0 20 40 60 80 100
percent of training corpus
45
50
55
60
65
70
75
f-s
co
re
unlexicalized PCFG
lexicalized PCFG (Collins)
lexicalized PCFG (C&R)
Figure 1: Learning curves for all three models
baseline model. Both lexicalized models (C&R and
Collins) performed worse than the baseline. This re-
sults is at odds with what has been found for En-
glish, where lexicalization is standardly reported to
increase performance by about 10%. The poor per-
formance of the lexicalized models could be due to
a lack of sufficient training data: our Negra training
set contains approximately 18,000 sentences, and is
therefore significantly smaller than the Penn Tree-
bank training set (about 40,000 sentences). Negra
sentences are also shorter: they contain, on average,
15 words compared to 22 in the Penn Treebank.
We computed learning curves for the unmodified
variants (without grammatical functions or parame-
ter pooling) of all three models (on the development
set). The result (see Figure 1) shows that there is no
evidence for an effect of sparse data. For both the
baseline and the C&R model, a fairly high f-score
is achieved with only 10% of the training data. A
slow increase occurs as more training data is added.
The performance of the Collins model is even less
affected by training set size. This is probably due to
the fact that it does not use rule probabilities directly,
but generates rules using a Markov chain.
5 Experiment 2
As we saw in the last section, lack of training data is
not a plausible explanation for the sub-baseline per-
formance of the lexicalized models. In this experi-
ment, we therefore investigate an alternative hypoth-
esis, viz., that the lexicalized models do not cope
Penn Negra
NP 2.20 3.08
PP 2.03 2.66
Penn Negra
VP 2.32 2.59
S 2.22 4.22
Table 3: Average number of daughters for the gram-
matical categories in the Penn Treebank and Negra
well with the fact that Negra rules are so flat (see
Section 2.2). We will focus on the Collins model, as
it outperformed the C&R model in Experiment 1.
An error analysis revealed that many of the errors
of the Collins model in Experiment 1 are chunking
errors. For example, the PP neben den Mitteln des
Theaters should be analyzed as (6a). But instead the
parser produces two constituents as in (6b)):
(6) a. [PP neben
apart
den
the
Mitteln
means
[NP des
the
Theaters]]
theater?s
?apart from the means of the theater?.
b. [PP neben den Mitteln] [NP des Theaters]
The reason for this problem is that neben is the head
of the constituent in (6), and the Collins model uses
a crude distance measure together with head-head
dependencies to decide if additional constituents
should be added to the PP. The distance measure is
inadequate for finding PPs with high precision.
The chunking problem is more widespread than
PPs. The error analysis shows that other con-
stituents, including Ss and VPs, also have the wrong
boundary. This problem is compounded by the fact
that the rules in Negra are substantially flatter than
the rules in the Penn Treebank, for which the Collins
model was developed. Table 3 compares the average
number of daughters in both corpora.
The flatness of PPs is easy to reduce. As detailed
in Section 2.2, PPs lack an intermediate NP projec-
tion, which can be inserted straightforwardly using
the following rule:
(7) [PP P . . . ] ! [PP P [NP . . . ]]
In the present experiment, we investigated if parsing
performance improves if we test and train on a ver-
sion of Negra on which the transformation in (7) has
been applied.
In a second series of experiments, we investigated
a more general way of dealing with the flatness of
C&R Collins Charniak Current
Head sister category X X X
Head sister head word X X X
Head sister head tag X X
Prev. sister category X X X
Prev. sister head word X
Prev. sister head tag X
Table 4: Linguistic features in the current model
compared to the models of Carroll and Rooth
(1998), Collins (1997), and Charniak (2000)
Negra, based on Collins?s (1997) model for non-
recursive NPs in the Penn Treebank (which are also
flat). For non-recursive NPs, Collins (1997) does not
use the probability function in (5), but instead sub-
stitutes Pr (and, by analogy, Pl) by:
Pr(Ri,t(Ri), l(Ri)jP,Ri?1,t(Ri?1), l(Ri?1),d(i))(8)
Here the head H is substituted by the sister Ri?1
(and Li?1). In the literature, the version of Pr in (5)
is said to capture head-head relationships. We will
refer to the alternative model in (8) as capturing
sister-head relationships.
Using sister-head relationships is a way of coun-
teracting the flatness of the grammar productions;
it implicitly adds binary branching to the grammar.
Our proposal is to extend the use of sister-head re-
lationship from non-recursive NPs (as proposed by
Collins) to all categories.
Table 4 shows the linguistic features of the result-
ing model compared to the models of Carroll and
Rooth (1998), Collins (1997), and Charniak (2000).
The C&R model effectively includes category infor-
mation about all previous sisters, as it uses context-
free rules. The Collins (1997) model does not use
context-free rules, but generates the next category
using zeroth order Markov chains (see Section 3.3),
hence no information about the previous sisters is
included. Charniak?s (2000) model extends this to
higher order Markov chains (first to third order), and
therefore includes category information about previ-
ous sisters.The current model differs from all these
proposals: it does not use any information about the
head sister, but instead includes the category, head
word, and head tag of the previous sister, effectively
treating it as the head.
5.1 Method
We first trained the original Collins model on a mod-
ified versions of the training test from Experiment 1
in which the PPs were split by applying rule (7).
In a second series of experiments, we tested a
range of models that use sister-head dependencies
instead of head-head dependencies for different cat-
egories. We first added sister-head dependencies for
NPs (following Collins?s (1997) original proposal)
and then for PPs, which are flat in Negra, and thus
similar in structure to NPs (see Section 2.2). Then
we tested a model in which sister-head relationships
are applied to all categories.
In a third series of experiments, we trained mod-
els that use sister-head relationships everywhere ex-
cept for one category. This makes it possible to de-
termine which sister-head dependencies are crucial
for improving performance of the model.
5.2 Results
The results of the PP experiment are listed in Ta-
ble 5. Again, we give results obtained using TnT tags
and using perfect tags. The row ?Split PP? contains
the performance figures obtained by including split
PPs in both the training and in the testing set. This
leads to a substantial increase in LR (6?7%) and LP
(around 8%) for both tagging schemes. Note, how-
ever, that these figures are not directly comparable to
the performance of the unmodified Collins model: it
is possible that the additional brackets artificially in-
flate LR and LP. Presumably, the brackets for split
PPs are easy to detect, as they are always adjacent to
a preposition. An honest evaluation should therefore
train on the modified training set (with split PPs),
but collapse the split categories for testing, i.e., test
on the unmodified test set. The results for this evalu-
ation are listed in rows ?Collapsed PP?. Now there is
no increase in performance compared to the unmod-
ified Collins model; rather, a slight drop in LR and
LP is observed.
Table 5 also displays the results of our exper-
iments with the sister-head model. For TnT tags,
we observe that using sister-head dependencies for
NPs leads to a small decrease in performance com-
pared to the unmodified Collins model, resulting in
67.84% LR and 65.96% LP. Sister-head dependen-
cies for PPs, however, increase performance sub-
stantially to 70.27% LR and 68.45% LP. The high-
est improvement is observed if head-sister depen-
dencies are used for all categories; this results in
71.32% LR and 70.93% LP, which corresponds to an
improvement of 3% in LP and 5% in LR compared
to the unmodified Collins model. Performance with
perfect tags is around 2?4% higher than with TnT
tags. For perfect tags, sister-head dependencies lead
to an improvement for NPs, PPs, and all categories.
The third series of experiments was designed to
determine which categories are crucial for achiev-
ing this performance gain. This was done by train-
ing models that use sister-head dependencies for all
categories but one. Table 6 shows the change in LR
and LP that was found for each individual category
(again for TnT tags and perfect tags). The highest
drop in performance (around 3%) is observed when
the PP category is reverted to head-head dependen-
cies. For S and for the coordinated categories (CS,
TnT tagging Perfect tagging
LR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB Cov
Unmod. Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23
Split PP 73.84 73.77 0.82 62.89 88.98 95.11 75.93 75.27 0.77 65.36 89.03 93.79
Collapsed PP 66.45 66.07 0.89 66.60 87.04 95.11 68.22 67.32 0.94 66.67 85.88 93.79
Sister-head NP 67.84 65.96 0.75 65.85 88.97 95.11 71.54 70.31 0.60 68.03 93.33 94.60
Sister-head PP 70.27 68.45 0.69 66.27 90.33 94.81 73.20 72.44 0.60 68.53 93.21 94.50
Sister-head all 71.32 70.93 0.61 69.53 91.72 95.92 73.93 74.24 0.54 72.30 93.47 95.21
Table 5: Results for Experiment 2: performance for models using split phrases and sister-head dependencies
CNP, etc.), a drop in performance of around 1% each
is observed. A slight drop is observed also for VP
(around 0.5%). Only minimal fluctuations in perfor-
mance are observed when the other categories are
removed (AP, AVP, and NP): there is a small effect
(around 0.5%) if TnT tags are used, and almost no
effect for perfect tags.
5.3 Discussion
We showed that splitting PPs to make Negra less
flat does not improve parsing performance if test-
ing is carried out on the collapsed categories. How-
ever, we observed that LR and LP are artificially in-
flated if split PPs are used for testing. This finding
goes some way towards explaining why the parsing
performance reported for the Penn Treebank is sub-
stantially higher than the results for Negra: the Penn
Treebank contains split PPs, which means that there
are lot of brackets that are easy to get right. The re-
sulting performance figures are not directly compa-
rable to figures obtained on Negra, or other corpora
with flat PPs.2
We also obtained a positive result: we demon-
strated that a sister-head model outperforms the un-
lexicalized baseline model (unlike the C&R model
and the Collins model in Experiment 1). LR was
about 1% higher and LP about 4% higher than the
baseline if lexical sister-head dependencies are used
for all categories. This holds both for TnT tags and
for perfect tags (compare Tables 2 and 5). We also
found that using lexical sister-head dependencies for
all categories leads to a larger improvement than us-
ing them only for NPs or PPs (see Table 5). This
result was confirmed by a second series of experi-
ments, where we reverted individual categories back
to head-head dependencies, which triggered a de-
crease in performance for all categories, with the ex-
ception of NP, AP, and AVP (see Table 6).
On the whole, the results of Experiment 2 are at
odds with what is known about parsing for English.
The progression in the probabilistic parsing litera-
ture has been to start with lexical head-head depen-
dencies (Collins, 1997) and then add non-lexical sis-
2This result generalizes to Ss, which are also flat in Negra
(see Section 2.2). We conducted an experiment in which we
added an SBAR above the S. No increase in performance was
obtained if the evaluation was carried using collapsed Ss.
TnT tagging Perfect tagging
?LR ?LP ?LR ?LP
PP ?3.45 ?1.60 ?4.21 ?3.35
S ?1.28 0.11 ?2.23 ?1.22
Coord ?1.87 ?0.39 ?1.54 ?0.80
VP ?0.72 0.18 ?0.58 ?0.30
AP ?0.57 0.10 0.08 ?0.07
AVP ?0.32 0.44 0.10 0.11
NP 0.06 0.78 ?0.15 0.02
Table 6: Change in performance when reverting to
head-head statistics for individual categories
ter information (Charniak, 2000), as illustrated in
Table 4. Lexical sister-head dependencies have only
been found useful in a limited way: in the original
Collins model, they are used for non-recursive NPs.
Our results show, however, that for parsing Ger-
man, lexical sister-head information is more im-
portant than lexical head-head information. Only a
model that replaced lexical head-head with lexical
sister-head dependencies was able to outperform a
baseline model that uses no lexicalization.3 Based
on the error analysis for Experiment 1, we claim that
the reason for the success of the sister-head model is
the fact that the rules in Negra are so flat; using a
sister-head model is a way of binarizing the rules.
6 Comparison with Previous Work
There are currently no probabilistic, treebank-
trained parsers available for German (to our knowl-
edge). A number of chunking models have been pro-
posed, however. Skut and Brants (1998) used Ne-
gra to train a maximum entropy-based chunker, and
report LR and LP of 84.4% for NP and PP chunk-
ing. Using cascaded Markov models, Brants (2000)
reports an improved performance on the same task
(LR 84.4%, LP 88.3%). Becker and Frank (2002)
train an unlexicalized PCFG on Negra to perform
a different chunking task, viz., the identification of
topological fields (sentence-based chunks). They re-
port an LR and LP of 93%.
The head-lexicalized model of Carroll and Rooth
(1998) has been applied to German by Beil et al
3It is unclear what effect bi-lexical statistics have on the
sister-head model; while Gildea (2001) shows bi-lexical statis-
tics are sparse for some grammars, Hockenmaier and Steedman
(2002) found they play a greater role in binarized grammars.
(1999, 2002). However, this approach differs in the
number of ways from the results reported here: (a) a
hand-written grammar (instead of a treebank gram-
mar) is used; (b) training is carried out on unan-
notated data; (c) the grammar and the training set
cover only subordinate and relative clauses, not un-
restricted text. Beil et al (2002) report an evaluation
using an NP chunking task, achieving 92% LR and
LP. They also report the results of a task-based eval-
uation (extraction of sucategorization frames).
There is some research on treebank-based pars-
ing of languages other than English. The work by
Collins et al (1999) and Bikel and Chiang (2000)
has demonstrated the applicability of the Collins
(1997) model for Czech and Chinese. The perfor-
mance reported by these authors is substantially
lower than the one reported for English, which might
be due to the fact that less training data is avail-
able for Czech and Chinese (see Table 1). This hy-
pothesis cannot be tested, as the authors do not
present learning curves for their models. However,
the learning curve for Negra (see Figure 1) indicates
that the performance of the Collins (1997) model
is stable, even for small training sets. Collins et al
(1999) and Bikel and Chiang (2000) do not compare
their models with an unlexicalized baseline; hence
it is unclear if lexicalization really improves parsing
performance for these languages. As Experiment 1
showed, this cannot be taken for granted.
7 Conclusions
We presented the first probabilistic full parsing
model for German trained on Negra, a syntactically
annotated corpus. This model uses lexical sister-
head dependencies, which makes it particularly suit-
able for parsing Negra?s flat structures. The flatness
of the Negra annotation reflects the syntactic proper-
ties of German, in particular its semi-free wordorder.
In Experiment 1, we applied three standard pars-
ing models from the literature to Negra: an un-
lexicalized PCFG model (the baseline), Carroll
and Rooth?s (1998) head-lexicalized model, and
Collins?s (1997) model based on head-head depen-
dencies. The results show that the baseline model
achieves a performance of up to 73% recall and 70%
precision. Both lexicalized models perform substan-
tially worse. This finding is at odds with what has
been reported for parsing models trained on the Penn
Treebank. As a possible explanation we considered
lack of training data: Negra is about half the size of
the Penn Treebank. However, the learning curves for
the three models failed to produce any evidence that
they suffer from sparse data.
In Experiment 2, we therefore investigated an al-
ternative hypothesis: the poor performance of the
lexicalized models is due to the fact that the rules in
Negra are flatter than in the Penn Treebank, which
makes lexical head-head dependencies less useful
for correctly determining constituent boundaries.
Based on this assumption, we proposed an alterna-
tive model hat replaces lexical head-head dependen-
cies with lexical sister-head dependencies. This can
the thought of as a way of binarizing the flat rules in
Negra. The results show that sister-head dependen-
cies improve parsing performance not only for NPs
(which is well-known for English), but also for PPs,
VPs, Ss, and coordinate categories. The best perfor-
mance was obtained for a model that uses sister-head
dependencies for all categories. This model achieves
up to 74% recall and precision, thus outperforming
the unlexicalized baseline model.
It can be hypothesized that this finding carries
over to other treebanks that are annotated with flat
structures. Such annotation schemes are often used
for languages that (unlike English) have a free or
semi-free wordorder. Testing our sister-head model
on these languages is a topic for future research.
References
Becker, Markus and Anette Frank. 2002. A stochastic topological parser of Ger-
man. In Proceedings of the 19th International Conference on Computational
Linguistics. Taipei.
Beil, Franz, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999.
Inside-outside estimation of a lexicalized PCFG for German. In Proceedings
of the 37th Annual Meeting of the Association for Computational Linguistics.
College Park, MA.
Beil, Franz, Detlef Prescher, Helmut Schmid, and Sabine Schulte im Walde. 2002.
Evaluation of the Gramotron parser for German. In Proceedings of the LREC
Workshop Beyond Parseval: Towards Improved Evaluation Measures for Pars-
ing Systems. Las Palmas, Gran Canaria.
Bikel, Daniel M. and David Chiang. 2000. Two statistical parsing models applied
to the Chinese treebank. In Proceedings of the 2nd ACL Workshop on Chinese
Language Processing. Hong Kong.
Brants, Thorsten. 2000. TnT: A statistical part-of-speech tagger. In Proceedings
of the 6th Conference on Applied Natural Language Processing. Seattle.
Carroll, Glenn and Mats Rooth. 1998. Valence induction with a head-lexicalized
PCFG. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing. Granada.
Charniak, Eugene. 1993. Statistical Language Learning. MIT Press, Cambridge,
MA.
Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word
statistics. In Proceedings of the 14th National Conference on Artificial Intel-
ligence. AAAI Press, Cambridge, MA.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings
of the 1st Conference of the North American Chapter of the Association for
Computational Linguistics. Seattle.
Collins, Michael. 1997. Three generative, lexicalised models for statistical pars-
ing. In Proceedings of the 35th Annual Meeting of the Association for Com-
putational Linguistics and the 8th Conference of the European Chapter of the
Association for Computational Linguistics. Madrid.
Collins, Michael, Jan Hajic?, Lance Ramshaw, and Christoph Tillmann. 1999. A
statistical parser for Czech. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics. College Park, MA.
Gildea, Daniel. 2001. Corpus variation and parser performance. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing.
Pittsburgh.
Hockenmaier, Julia and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory categorial grammar. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics. Philadelphia.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn Treebank. Compu-
tational Linguistics 19(2).
Schmid, Helmut. 2000. LoPar: Design and implementation. Ms., Institute for
Computational Linguistics, University of Stuttgart.
Skut, Wojciech and Thorsten Brants. 1998. A maximum-entropy partial parser for
unrestricted text. In Proceedings of the 6th Workshop on Very Large Corpora.
Montre?al.
Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order languages. In Proceedings of the 5th
Conference on Applied Natural Language Processing. Washington, DC.
Uszkoreit, Hans. 1987. Word Order and Constituent Structure in German. CSLI
Publications, Stanford, CA.
Proceedings of the 43rd Annual Meeting of the ACL, pages 306?313,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Lexicalization in Crosslinguistic Probabilistic Parsing:
The Case of French
Abhishek Arun and Frank Keller
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
a.arun@sms.ed.ac.uk, keller@inf.ed.ac.uk
Abstract
This paper presents the first probabilistic
parsing results for French, using the re-
cently released French Treebank. We start
with an unlexicalized PCFG as a base-
line model, which is enriched to the level
of Collins? Model 2 by adding lexical-
ization and subcategorization. The lexi-
calized sister-head model and a bigram
model are also tested, to deal with the flat-
ness of the French Treebank. The bigram
model achieves the best performance:
81% constituency F-score and 84% de-
pendency accuracy. All lexicalized mod-
els outperform the unlexicalized baseline,
consistent with probabilistic parsing re-
sults for English, but contrary to results
for German, where lexicalization has only
a limited effect on parsing performance.
1 Introduction
This paper brings together two strands of research
that have recently emerged in the field of probabilis-
tic parsing: crosslinguistic parsing and lexicalized
parsing. Interest in parsing models for languages
other than English has been growing, starting with
work on Czech (Collins et al, 1999) and Chinese
(Bikel and Chiang, 2000; Levy and Manning, 2003).
Probabilistic parsing for German has also been ex-
plored by a range of authors (Dubey and Keller,
2003; Schiehlen, 2004). In general, these authors
have found that existing lexicalized parsing models
for English (e.g., Collins 1997) do not straightfor-
wardly generalize to new languages; this typically
manifests itself in a severe reduction in parsing per-
formance compared to the results for English.
A second recent strand in parsing research has
dealt with the role of lexicalization. The conven-
tional wisdom since Magerman (1995) has been that
lexicalization substantially improves performance
compared to an unlexicalized baseline model (e.g., a
probabilistic context-free grammar, PCFG). How-
ever, this has been challenged by Klein and Man-
ning (2003), who demonstrate that an unlexicalized
model can achieve a performance close to the state
of the art for lexicalized models. Furthermore, Bikel
(2004) provides evidence that lexical information
(in the form of bi-lexical dependencies) only makes
a small contribution to the performance of parsing
models such as Collins?s (1997).
The only previous authors that have directly ad-
dressed the role of lexicalization in crosslinguistic
parsing are Dubey and Keller (2003). They show
that standard lexicalized models fail to outperform
an unlexicalized baseline (a vanilla PCFG) on Ne-
gra, a German treebank (Skut et al, 1997). They
attribute this result to two facts: (a) The Negra an-
notation assumes very flat trees, which means that
Collins-style head-lexicalization fails to pick up the
relevant information from non-head nodes. (b) Ger-
man allows flexible word order, which means that
standard parsing models based on context free gram-
mars perform poorly, as they fail to generalize over
different positions of the same constituent.
As it stands, Dubey and Keller?s (2003) work does
not tell us whether treebank flatness or word order
flexibility is responsible for their results: for English,
the annotation scheme is non-flat, and the word or-
der is non-flexible; lexicalization improves perfor-
mance. For German, the annotation scheme is flat
and the word order is flexible; lexicalization fails to
improve performance. The present paper provides
the missing piece of evidence by applying proba-
bilistic parsing models to French, a language with
non-flexible word order (like English), but with a
treebank with a flat annotation scheme (like Ger-
man). Our results show that French patterns with En-
glish: a large increase of parsing performance can be
obtained by using a lexicalized model. We conclude
that the failure to find a sizable effect of lexicaliza-
tion in German can be attributed to the word order
flexibility of that language, rather than to the flatness
of the annotation in the German treebank.
The paper is organized as follows: In Section 2,
we give an overview of the French Treebank we use
for our experiments. Section 3 discusses its anno-
tation scheme and introduces a set of tree transfor-
mations that we apply. Section 4 describes the pars-
306
<NP>
<w lemma="eux" ei="PROmp"
ee="PRO-3mp" cat="PRO"
subcat="3mp">eux</w>
</NP>
Figure 1: Word-level annotation in the French Tree-
bank: eux ?they? (cat: POS tag, subcat: subcate-
gory, ei, ee: inflection)
ing models, followed by the results for the unlexi-
calized baseline model in Section 6 and for a range
of lexicalized models in Section 5. Finally, Section 7
provides a crosslinguistic comparison involving data
sets of the same size extracted from the French, En-
glish, and German treebanks.
2 The French Treebank
2.1 Annotation Scheme
The French Treebank (FTB; Abeille? et al 2000) con-
sists of 20,648 sentences extracted from the daily
newspaper Le Monde, covering a variety of authors
and domains (economy, literature, politics, etc.).1
The corpus is formatted in XML and has a rich mor-
phosyntactic tagset that includes part-of-speech tag,
?subcategorization? (e.g., possessive or cardinal), in-
flection (e.g., masculine singular), and lemma in-
formation. Compared to the Penn Treebank (PTB;
Marcus et al 1993), the POS tagset of the French
Treebank is smaller (13 tags vs. 36 tags): all punc-
tuation marks are represented as the single PONCT
tag, there are no separate tags for modal verbs, wh-
words, and possessives. Also verbs, adverbs and
prepositions are more coarsely defined. On the other
hand, a separate clitic tag (CL) for weak pronouns is
introduced. An example for the word-level annota-
tion in the FTB is given in Figure 1
The phrasal annotation of the FTB differs from
that for the Penn Treebank in several aspects. There
is no verb phrase: only the verbal nucleus (VN) is
annotated. A VN comprises the verb and any clitics,
auxiliaries, adverbs, and negation associated with it.
This results in a flat syntactic structure, as in (1).
(1) (VN (V sont) (ADV syste?matiquement) (V
arre?te?s)) ?are systematically arrested?
The noun phrases (NPs) in the FTB are also flat; a
noun is grouped together with any associated deter-
miners and prenominal adjectives, as in example (2).
Note that postnominal adjectives, however, are ad-
joined to the NP in an adjectival phrase (AP).
1The French Treebank was developed at Universite? Paris 7.
A license can be obtained by emailing Anne Abeille? (abeille@
linguist.jussieu.fr).
<w compound="yes" lemma="d?entre"
ei="P" ee="P" cat="P">
<w catint="P">d?</w>
<w catint="P">entre</w>
</w>
Figure 2: Annotation of compounds in the French
Treebank: d?entre ?between? (catint: compound-
internal POS tag)
(2) (NP (D des) (A petits) (N mots) (AP (ADV tre`s)
(A gentils))) ?small, very gentle words?
Unlike the PTB, the FTB annotates coordinated
phrases with the syntactic tag COORD (see the left
panel of Figure 3 for an example).
The treatment of compounds is also different in
the FTB. Compounds in French can comprise words
which do not exist otherwise (e.g., insu in the com-
pound preposition a` l?insu de ?unbeknownst to?) or
can exhibit sequences of tags otherwise ungrammat-
ical (e.g., a` la va vite ?in a hurry?: Prep + Det +
finite verb + adverb). To account for these proper-
ties, compounds receive a two-level annotation in
the FTB: a subordinate level is added for the con-
stituent parts of the compound (both levels use the
same POS tagset). An example is given in Figure 2.
Finally, the FTB differs from the PTB in that it
does not use any empty categories.
2.2 Data Sets
The version of the FTB made available to us (ver-
sion 1.4, May 2004) contains numerous errors. Two
main classes of inaccuracies were found in the data:
(a) The word is present but morphosyntactic tags
are missing; 101 such cases exist. (b) The tag in-
formation for a word (or a part of a compound) is
present but the word (or compound part) itself is
missing. There were 16,490 instances of this error
in the dataset.
Initially we attempted to correct the errors, but
this proved too time consuming, and we often found
that the errors cannot be corrected without access to
the raw corpus, which we did not have. We therefore
decided to remove all sentences with errors, which
lead to a reduced dataset of 10,552 sentences.
The remaining data set (222,569 words at an av-
erage sentence length of 21.1 words) was split into
a training set, a development set (used to test the
parsing models and to tune their parameters), and a
test set, unseen during development. The training set
consisted of the first 8,552 sentences in the corpus,
with the following 1000 sentences serving as the de-
velopment set and the final 1000 sentences forming
the test set. All results reported in this paper were
obtained on the test set, unless stated otherwise.
307
3 Tree Transformations
We created a number of different datasets from the
FTB, applying various tree transformation to deal
with the peculiarities of the FTB annotation scheme.
As a first step, the XML formatted FTB data was
converted to PTB-style bracketed expressions. Only
the POS tag was kept and the rest of the morphologi-
cal information for each terminal was discarded. For
example, the NP in Figure 1 was transformed to:
(3) (NP (PRO eux))
In order to make our results comparable to re-
sults from the literature, we also transformed the
annotation of punctuation. In the FTB, all punc-
tuations is tagged uniformly as PONCT. We re-
assigned the POS for punctuation using the PTB
tagset, which differentiates between commas, peri-
ods, brackets, etc.
Compounds have internal structure in the FTB
(see Section 2.1). We created two separate data sets
by applying two alternative tree transformation to
make FTB compounds more similar to compounds
in other annotation schemes. The first was collaps-
ing the compound by concatenating the compound
parts using an underscore and picking up the cat
information supplied at the compound level. For ex-
ample, the compound in Figure 2 results in:
(4) (P d? entre)
This approach is similar to the treatment of com-
pounds in the German Negra treebank (used by
Dubey and Keller 2003), where compounds are not
given any internal structure (compounds are mostly
spelled without spaces or apostrophes in German).
The second approach is expanding the compound.
Here, the compound parts are treated as individual
words with their own POS (from the catint tag),
and the suffix Cmp is appended the POS of the com-
pound, effectively expanding the tagset.2 Now Fig-
ure 2 yields:
(5) (PCmp (P d?) (P entre)).
This approach is similar to the treatment of com-
pounds in the PTB (except hat the PTB does not use
a separate tag for the mother category). We found
that in the FTB the POS tag of the compound part
is sometimes missing (i.e., the value of catint is
blank). In cases like this, the missing catint was
substituted with the cat tag of the compound. This
heuristic produces the correct POS for the subparts
of the compound most of the time.
2An alternative would be to retain the cat tag of the com-
pound. The effect of this decision needs to be investigated in
future work.
XP
X COORD
C XP
X
XP
X C XP
X
XP
X C X
Figure 3: Coordination in the FTB: before (left) and
after transformation (middle); coordination in the
PTB (right)
As mentioned previously, coordinate structures
have their own constituent label COORD in the
FTB annotation. Existing parsing models (e.g., the
Collins models) have coordination-specific rules,
presupposing that coordination is marked up in PTB
format. We therefore created additional datasets
where a transformation is applied that raises coor-
dination. This is illustrated in Figure 3. Note that
in the FTB annotation scheme, a coordinating con-
junction is always followed by a syntactic category.
Hence the resulting tree, though flatter, is still not
fully compatible with the PTB treatment of coordi-
nation.
4 Probabilistic Parsing Models
4.1 Probabilistic Context-Free Grammars
The aim of this paper is to further explore the
crosslinguistic role of lexicalization by applying lex-
icalized parsing models to the French Treebank pars-
ing accuracy. Following Dubey and Keller (2003),
we use a standard unlexicalized PCFG as our base-
line. In such a model, each context-free rule RHS ?
LHS is annotated with an expansion probability
P(RHS|LHS). The probabilities for all the rules with
the same left-hand side have to sum up to one and
the probability of a parse tree T is defined as the
product of the probabilities of each rule applied in
the generation of T .
4.2 Collins? Head-Lexicalized Models
A number of lexicalized models can then be applied
to the FTB, comparing their performance to the un-
lexicalized baseline. We start with Collins? Model 1,
which lexicalizes a PCFG by associating a word w
and a POS tag t with each non-terminal X in the
tree. Thus, a non-terminal is written as X(x) where
x = ?w, t? and X is constituent label. Each rule now
has the form:
P(h) ? Ln(ln) . . .L1(l1)H(h)R1(r1) . . .Rm(rm)(1)
Here, H is the head-daughter of the phrase, which
inherits the head-word h from its parent P. L1 . . .Ln
and R1 . . .Rn are the left and right sisters of H. Either
n or m may be zero, and n = m for unary rules.
308
The addition of lexical heads leads to an enor-
mous number of potential rules, making direct esti-
mation of P(RHS|LHS) infeasible because of sparse
data. Therefore, the generation of the RHS of a rule
given the LHS is decomposed into three steps: first
the head is generated, then the left and right sisters
are generated by independent 0th-order Markov pro-
cesses. The probability of a rule is thus defined as:
P(RHS|LHS) =
P(Ln(ln) . . .L1(l1)H(h),R1(r1) . . .Rm(rm)|P(h))
= Ph(H|P,h)??m+1i=1 Pr(Ri(ri)|P,h,H,d(i))
??n+1i=1 Pl(Li(li)|P,h,H,d(i))
(2)
Here, Ph is the probability of generating the head, Pl
and Pr are the probabilities of generating the left and
right sister respectively. Lm+1(lm+1) and Rm+1(rm+1)
are defined as stop categories which indicate when to
stop generating sisters. d(i) is a distance measure, a
function of the length of the surface string between
the head and the previously generated sister.
Collins? Model 2 further refines the initial model
by incorporating the complement/adjunct distinction
and subcategorization frames. The generative pro-
cess is enhanced to include a probabilistic choice of
left and right subcategorization frames. The proba-
bility of a rule is now:
Ph(H|P,h)?Plc(LC|P,H,h)?Prc(RC|P,H,h)
??m+1i=1 Pr(Ri(ri)|P,h,H,d(i),RC)
??n+1i=1 Pl(Li(li)|P,h,H,d(i),LC)
(3)
Here, LC and RC are left and right subcat frames,
multisets specifying the complements that the head
requires in its left or right sister. The subcat re-
quirements are added to the conditioning context. As
complements are generated, they are removed from
the appropriate subcat multiset.
5 Experiment 1: Unlexicalized Model
5.1 Method
This experiment was designed to compare the per-
formance of the unlexicalized baseline model on
four different datasets, created by the tree trans-
formations described in Section 3: compounds
expanded (Exp), compounds contracted (Cont),
compounds expanded with coordination raised
(Exp+CR), and compounds contracted with coordi-
nation raised (Cont+CR).
We used BitPar (Schmid, 2004) for our unlexi-
calized experiments. BitPar is a parser based on a
bit-vector implementation of the CKY algorithm. A
grammar and lexicon were read off our training set,
along with rule frequencies and frequencies for lex-
ical items, based on which BitPar computes the rule
Model LR LP CBs 0CB ?2CB Tag Cov
Exp 59.97 58.64 1.74 39.05 73.23 91.00 99.20
Exp+CR 60.75 60.57 1.57 40.77 75.03 91.08 99.09
Cont 64.19 64.61 1.50 46.74 76.80 93.30 98.48
Cont+CR 66.11 65.55 1.39 46.99 78.95 93.22 97.94
Table 1: Results for unlexicalized models (sentences
?40 words); each model performed its own POS
tagging.
probabilities using maximum likelihood estimation.
A frequency distribution for POS tags was also read
off the training set; this distribution is used by BitPar
to tag unknown words in the test data.
All models were evaluated using standard Par-
seval measures of labeled recall (LR), labeled pre-
cision (LP), average crossing brackets (CBs), zero
crossling brackets (0CB), and two or less crossing
brackets (?2CB). We also report tagging accuracy
(Tag), and coverage (Cov).
5.2 Results
The results for the unlexicalized model are shown in
Table 1 for sentences of length ?40 words. We find
that contracting compounds increases parsing per-
formance substantially compared to expanding com-
pounds, raising labeled recall from around 60% to
around 64% and labeled precision from around 59%
to around 65%. The results show that raising co-
ordination is also beneficial; it increases precision
and recall by 1?2%, both for expanded and for non-
expanded compounds.
Note that these results were obtained by uni-
formly applying coordination raising during evalu-
ation, so as to make all models comparable. For the
Exp and Cont models, the parsed output and the gold
standard files were first converted by raising coordi-
nation and then the evaluation was performed.
5.3 Discussion
The disappointing performance obtained for the ex-
panded compound models can be partly attributed
to the increase in the number of grammar rules
(11,704 expanded vs. 10,299 contracted) and POS
tags (24 expanded vs. 11 contracted) associated with
that transformation.
However, a more important point observation is
that the two compound models do not yield compa-
rable results, since an expanded compound has more
brackets than a contracted one. We attempted to ad-
dress this problem by collapsing the compounds for
evaluation purposes (as described in Section 3). For
example, (5) would be contracted to (4). However,
this approach only works if we are certain that the
model is tagging the right words as compounds. Un-
309
fortunately, this is rarely the case. For example, the
model outputs:
(6) (NCmp (N jours) (N commerc?ants))
But in the gold standard file, jours and commerc?ants
are two distinct NPs. Collapsing the compounds
therefore leads to length mismatches in the test data.
This problem occurs frequently in the test set, so that
such an evaluation becomes pointless.
6 Experiment 2: Lexicalized Models
6.1 Method
Parsing We now compare a series of lexicalized
parsing models against the unlexicalized baseline es-
tablished in the previous experiment. Our is was to
test if French behaves like English in that lexicaliza-
tion improves parsing performance, or like German,
in that lexicalization has only a small effect on pars-
ing performance.
The lexicalized parsing experiments were run us-
ing Dan Bikel?s probabilistic parsing engine (Bikel,
2002) which in addition to replicating the models
described by Collins (1997) also provides a con-
venient interface to develop corresponding parsing
models for other languages.
Lexicalization requires that each rule in a gram-
mar has one of the categories on its right hand side
annotated as the head. These head rules were con-
structed based on the FTB annotation guidelines
(provided along with the dataset), as well as by us-
ing heuristics, and were optimized on the develop-
ment set. Collins? Model 2 incorporates a comple-
ment/adjunct distinction and probabilities over sub-
categorization frames. Complements were marked
in the training phase based on argument identifica-
tion rules, tuned on the development set.
Part of speech tags are generated along with
the words in the models; parsing and tagging are
fully integrated. To achieve this, Bikel?s parser
requires a mapping of lexical items to ortho-
graphic/morphological word feature vectors. The
features implemented (capitalization, hyphenation,
inflection, derivation, and compound) were again
optimized on the development set.
Like BitPar, Bikel?s parser implements a prob-
abilistic version of the CKY algorithm. As with
normal CKY, even though the model is defined in
a top-down, generative manner, decoding proceeds
bottom-up. To speed up decoding, the algorithm im-
plements beam search. Collins uses a beam width of
104, while we found that a width of 105 gave us the
best coverage vs. parsing speed trade-off.
Label FTB PTB Negra Label FTB PTB Negra
SENT 5.84 2.22 4.55 VPpart 2.51 ? ?
Ssub 4.41 ? ? VN 1.76 ? ?
Sint 3.44 ? ? PP 2.10 2.03 3.08
Srel 3.92 ? ? NP 2.45 2.20 3.08
VP ? 2.32 2.59 AdvP 2.24 ? 2.08
VPinf 3.07 ? ? AP 1.34 ? 2.22
Table 2: Average number of daughter nodes per con-
stituents in three treebanks
Flatness As already pointed out in Section 2.1,
the FTB uses a flat annotation scheme. This can
be quantified by computing the average number of
daughters for each syntactic category in the FTB,
and comparing them with the figures available for
PTB and Negra (Dubey and Keller, 2003). This is
done in Table 2. The absence of sentence-internal
VPs explains the very high level of flatness for the
sentential category SENT (5.84 daughters), com-
pared to the PTB (2.44), and even to Negra, which is
also very flat (4.55 daughters). The other sentential
categories Ssub (subordinate clauses), Srel (relative
clause), and Sint (interrogative clause) are also very
flat. Note that the FTB uses VP nodes only for non-
finite subordinate clauses: VPinf (infinitival clause)
and VPpart (participle clause); these categories are
roughly comparable in flatness to the VP category
in the PTB and Negra. For NP, PPs, APs, and AdvPs
the FTB is roughly as flat as the PTB, and somewhat
less flat than Negra.
Sister-Head Model To cope with the flatness of
the FTB, we implemented three additional parsing
models. First, we implemented Dubey and Keller?s
(2003) sister-head model, which extends Collins?
base NP model to all syntactic categories. This
means that the probability function Pr in equation (2)
is no longer conditioned on the head but instead on
its previous sister, yielding the following definition
for Pr (and by analogy Pl):
Pr(Ri(ri)|P,Ri?1(ri?1),d(i))(4)
Dubey and Keller (2003) argue that this implicitly
adds binary branching to the grammar, and therefore
provides a way of dealing with flat annotation (in
Negra and in the FTB, see Table 2).
Bigram Model This model, inspired by the ap-
proach of Collins et al (1999) for parsing the Prague
Dependency Treebank, builds on Collins? Model 2
by implementing a 1st order Markov assumption for
the generation of sister non-terminals. The latter are
now conditioned, not only on their head, but also on
the previous sister. The probability function for Pr
(and by analogy Pl) is now:
Pr(Ri(ri)|P,h,H,d(i),Ri?1,RC)(5)
310
Model LR LP CBs 0CB ?2CB Tag Cov
Model 1 80.35 79.99 0.78 65.22 89.46 96.86 99.68
Model 2 80.49 79.98 0.77 64.85 90.10 96.83 99.68
SisterHead 80.47 80.56 0.78 64.96 89.34 96.85 99.57
Bigram 81.15 80.84 0.74 65.21 90.51 96.82 99.46
BigramFlat 80.30 80.05 0.77 64.78 89.13 96.71 99.57
Table 3: Results for lexicalized models (sentences
?40 words); each model performed its own POS
tagging; all lexicalized models used the Cont+CR
data set
The intuition behind this approach is that the model
will learn that the stop symbol is more likely to fol-
low phrases with many sisters. Finally, we also ex-
perimented with a third model (BigramFlat) that ap-
plies the bigram model only for categories with high
degrees of flatness (SENT, Srel, Ssub, Sint, VPinf,
and VPpart).
6.2 Results
Constituency Evaluation The lexicalized models
were tested on the Cont+CR data set, i.e., com-
pounds were contracted and coordination was raised
(this is the configuration that gave the best perfor-
mance in Experiment 1).
Table 3 shows that all lexicalized models achieve
a performance of around 80% recall and precision,
i.e., they outperform the best unlexicalized model by
at least 14% (see Table 1). This is consistent with
what has been reported for English on the PTB.
Collins? Model 2, which adds the comple-
ment/adjunct distinction and subcategorization
frames achieved only a very small improvement
over Collins? Model 1, which was not statistically
significant using a ?2 test. It might well be that
the annotation scheme of the FTB does not lend
itself particularly well to the demands of Model 2.
Moreover, as Collins (1997) mentions, some of
the benefits of Model 2 are already captured by
inclusion of the distance measure.
A further small improvement was achieved us-
ing Dubey and Keller?s (2003) sister-head model;
however, again the difference did not reach sta-
tistical significance. The bigram model, however,
yielded a statistically significant improvement over
Collins? Model 1 (recall ?2 = 3.91, df = 1, p? .048;
precision ?2 = 3.97, df = 1, p ? .046). This is con-
sistent with the findings of Collins et al (1999)
for Czech, where the bigram model upped depen-
dency accuracy by about 0.9%, as well as for En-
glish where Charniak (2000) reports an increase
in F-score of approximately 0.3%. The BigramFlat
model, which applies the bigram model to only those
labels which have a high degree of flatness, performs
Model LR LP CBs 0CB ?2CB Tag Cov
Exp+CR 65.50 64.76 1.49 42.36 77.48 100.0 97.83
Cont+CR 69.35 67.93 1.34 47.43 80.25 100.0 96.97
Model1 81.51 81.43 0.78 64.60 89.25 98.54 99.78
Model2 81.69 81.59 0.78 63.84 89.69 98.55 99.78
SisterHead 81.08 81.56 0.79 64.35 89.57 98.51 99.57
Bigram 81.78 81.91 0.78 64.96 89.12 98.81 99.67
BigramFlat 81.14 81.19 0.81 63.37 88.80 98.80 99.67
Table 4: Results for lexicalized and unlexical-
ized models (sentences ?40 words) with correct
POS tags supplied; all lexicalized models used the
Cont+CR data set
at roughly the same level as Model 1.
The models in Tables 1 and 3 implemented their
own POS tagging. Tagging accuracy was 91?93%
for BitPar (unlexicalized models) and around 96%
for the word-feature enhanced tagging model of the
Bikel parser (lexicalized models). POS tags are an
important cue for parsing. To gain an upper bound
on the performance of the parsing models, we reran
the experiments by providing the correct POS tag
for the words in the test set. While BitPar always
uses the tags provided, the Bikel parser only uses
them for words whose frequency is less than the un-
known word threshold. As Table 4 shows, perfect
tagging increased parsing performance in the lexi-
calized models by around 3%. This shows that the
poor POS tagging performed by BitPar is one of the
reasons of the poor performance of the lexicalized
models. The impact of perfect tagging is less dras-
tic on the lexicalized models (around 1% increase).
However, our main finding, viz., that lexicalized
models outperform unlexicalized models consider-
able on the FTB, remains valid, even with perfect
tagging.3
Dependency Evaluation We also evaluated our
models using dependency measures, which have
been argued to be more annotation-neutral than
Parseval. Lin (1995) notes that labeled bracketing
scores are more susceptible to cascading errors,
where one incorrect attachment decision causes the
scoring algorithm to count more than one error.
The gold standard and parsed trees were con-
verted into dependency trees using the algorithm de-
scribed by Lin (1995). Dependency accuracy is de-
fined as the ratio of correct dependencies over the to-
tal number of dependencies in a sentence. (Note that
this is an unlabeled dependency measure.) Depen-
dency accuracy and constituency F-score are shown
3It is important to note that the Collins model has a range
of other features that set it apart from a standard unlexicalized
PCFG (notably Markovization), as discussed in Section 4.2. It
is therefore likely that the gain in performance is not attributable
to lexicalization alone.
311
Model Dependency F-score
Cont+CR 73.09 65.83
Model 2 83.96 80.23
SisterHead 84.00 80.51
Bigram 84.20 80.99
Table 5: Dependency vs. constituency scores for lex-
icalized and unlexicalized models
in Table 5 for the most relevant FTB models. (F-
score is computed as the geometric mean of labeled
recall and precision.)
Numerically, dependency accuracies are higher
than constituency F-scores across the board. How-
ever, the effect of lexicalization is the same on both
measures: for the FTB, a gain of 11% in dependency
accuracy is observed for the lexicalized model.
7 Experiment 3: Crosslinguistic
Comparison
The results reported in Experiments 1 and 2 shed
some light on the role of lexicalization for parsing
French, but they are not strictly comparable to the
results that have been reported for other languages.
This is because the treebanks available for different
languages typically vary considerably in size: our
FTB training set was about 8,500 sentences large,
while the standard training set for the PTB is about
40,000 sentences in size, and the Negra training set
used by Dubey and Keller (2003) comprises about
18,600 sentences. This means that the differences in
the effect of lexicalization that we observe could be
simply due to the size of the training set: lexicalized
models are more susceptible to data sparseness than
unlexicalized ones.
We therefore conducted another experiment in
which we applied Collins? Model 2 to subsets of
the PTB that were comparable in size to our FTB
data sets. We combined sections 02?05 and 08 of
the PTB (8,345 sentences in total) to form the train-
ing set, and the first 1,000 sentences of section 23
to form our test set. As a baseline model, we also
run an unlexicalized PCFG on the same data sets.
For comparison with Negra, we also include the re-
sults of Dubey and Keller (2003): they report the
performance of Collins? Model 1 on a data set of
9,301 sentences and a test set of 1,000 sentences,
which are comparable in size to our FTB data sets.4
The results of the crosslinguistic comparison are
shown in Table 6.5 We conclude that the effect of
4Dubey and Keller (2003) report only F-scores for the re-
duced data set (see their Figure 1); the other scores were pro-
vided by Amit Dubey. No results for Model 2 are available.
5For this experiments, the same POS tagging model was ap-
plied to the PTB and the FTB data, which is why the FTB fig-
Corpus Model LR LP CBs 0CB ?2CB
FTB Cont+CR 66.11 65.55 1.39 46.99 78.95
Model 2 79.20 78.58 0.83 63.33 89.23
PTB Unlex 72.79 75.23 2.54 31.56 58.98
Model 2 86.43 86.79 1.17 57.80 82.44
Negra Unlex 69.64 67.27 1.12 54.21 82.84
Model 1 68.33 67.32 0.83 60.43 88.78
Table 6: The effect of lexicalization on different cor-
pora for training sets of comparable size (sentences
?40 words)
lexicalization is stable even if the size of the train-
ing set is held constant across languages: For the
FTB we find that lexicalization increases F-score by
around 13%. Also for the PTB, we find an effect of
lexicalization of about 14%. For the German Negra
treebank, however, the performance of the lexical-
ized and the unlexicalized model are almost indis-
tinguishable. (This is true for Collins? Model 1; note
that Dubey and Keller (2003) do report a small im-
provement for the lexicalized sister-head model.)
8 Related Work
We are not aware of any previous attempts to build
a probabilistic, treebank-trained parser for French.
However, there is work on chunking for French. The
group who built the French Treebank (Abeille? et al,
2000) used a rule-based chunker to automatically
annotate the corpus with syntactic structures, which
were then manually corrected. They report an un-
labeled recall/precision of 94.3/94.2% for opening
brackets and 92.2/91.4% for closing brackets, and a
label accuracy of 95.6%. This result is not compara-
ble to our results for full parsing.
Giguet and Vergne (1997) present use a memory-
based learner to predict chunks and dependencies
between chunks. The system is evaluated on texts
from Le Monde (different from the FTB texts). Re-
sults are only reported for verb-object dependencies,
for which recall/precision is 94.04/96.39%. Again,
these results are not comparable to ours, which were
obtained using a different corpus, a different depen-
dency scheme, and for a full set of dependencies.
9 Conclusions
In this paper, we provided the first probabilis-
tic, treebank-trained parser for French. In Exper-
iment 1, we established an unlexicalized baseline
model, which yielded a labeled precision and re-
call of about 66%. We experimented with a num-
ber of tree transformation that take account of the
peculiarities of the annotation of the French Tree-
ures are slightly lower than in Table 3.
312
bank; the best performance was obtained by rais-
ing coordination and contracting compounds (which
have internal structure in the FTB). In Experiment 2,
we explored a range of lexicalized parsing models,
and found that lexicalization improved parsing per-
formance by up to 15%: Collins? Models 1 and 2
performed at around 80% LR and LP. No signifi-
cant improvement could be achieved by switching to
Dubey and Keller?s (2003) sister-head model, which
has been claimed to be particularly suitable for tree-
banks with flat annotation, such as the FTB. A small
but significant improvement (to 81% LR and LP)
was obtained by a bigram model that combines fea-
tures of the sister-head model and Collins? model.
These results have important implications for
crosslinguistic parsing research, as they allow us
to tease apart language-specific and annotation-
specific effects. Previous work for English (e.g.,
Magerman, 1995; Collins, 1997) has shown that lex-
icalization leads to a sizable improvement in pars-
ing performance. English is a language with non-
flexible word order and with a treebank with a non-
flat annotation scheme (see Table 2). Research on
German (Dubey and Keller, 2003) showed that lex-
icalization leads to no sizable improvement in pars-
ing performance for this language. German has a
flexible word order and a flat treebank annotation,
both of which could be responsible for this counter-
intuitive effect. The results for French presented in
this paper provide the missing piece of evidence:
they show that French behaves like English in that
it shows a large effect of lexicalization. Like En-
glish, French is a language with non-flexible word
order, but like the German Treebank, the French
Treebank has a flat annotation. We conclude that
Dubey and Keller?s (2003) results for German can be
attributed to a language-specific factor (viz., flexible
word order) rather than to an annotation-specific fac-
tor (viz., flat annotation). We confirmed this claim in
Experiment 3 by showing that the effects of lexical-
ization observed for English, French, and German
are preserved if the size of the training set is kept
constant across languages.
An interesting prediction follows from the claim
that word order flexibility, rather than flatness of
annotation, is crucial for lexicalization. A language
which has a flexible word order (like German), but
a non-flat treebank (like English) should show no
effect of lexicalization, i.e., lexicalized models are
predicted not to outperform unlexicalized ones. In
future work, we plan to test this prediction for Ko-
rean, a flexible word order language whose treebank
(Penn Korean Treebank) has a non-flat annotation.
References
Abeille?, Anne, Lionel Clement, and Alexandra Kinyon. 2000.
Building a treebank for French. In Proceedings of the 2nd In-
ternational Conference on Language Resources and Evalu-
ation. Athens.
Bikel, Daniel M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings of the
2nd International Conference on Human Language Technol-
ogy Research. Morgan Kaufmann, San Francisco.
Bikel, Daniel M. 2004. A distributional analysis of a lexicalized
statistical parsing model. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Barcelona, pages 182?189.
Bikel, Daniel M. and David Chiang. 2000. Two statistical pars-
ing models applied to the Chinese treebank. In Proceedings
of the 2nd ACL Workshop on Chinese Language Processing.
Hong Kong.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics.
Seattle, WA, pages 132?139.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguistics
and the 8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics. Madrid, pages 16?23.
Collins, Michael, Jan Hajic?, Lance Ramshaw, and Christoph
Tillmann. 1999. A statistical parser for Czech. In Pro-
ceedings of the 37th Annual Meeting of the Association for
Computational Linguistics. University of Maryland, College
Park.
Dubey, Amit and Frank Keller. 2003. Probabilistic parsing for
German using sister-head dependencies. In Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, pages 96?103.
Giguet, Emmanuel and Jacques Vergne. 1997. From part-of-
speech tagging to memory-based deep syntactic analysis. In
Proceedings of the International Workshop on Parsing Tech-
nologies. Boston, pages 77?88.
Klein, Dan and Christopher Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics. Sapporo.
Levy, Roger and Christopher Manning. 2003. Is it harder to
parse Chinese, or the Chinese treebank? In Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo.
Lin, Dekang. 1995. A dependency-based method for evaluating
broad-coverage parsers. In Proceedings of the International
Joint Conference on Artificial Intelligence. Montreal, pages
1420?1425.
Magerman, David. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics. Cambridge, MA,
pages 276?283.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics
19(2):313?330.
Schiehlen, Michael. 2004. Annotation strategies for probabilis-
tic parsing in German. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics. Geneva.
Schmid, Helmut. 2004. Efficient parsing of highly ambiguous
context-free grammars with bit vectors. In Proceedings of
the 20th International Conference on Computational Lin-
guistics. Geneva.
Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word order
languages. In Proceedings of the 5th Conference on Applied
Natural Language Processing. Washington, DC.
313
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 417?424,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrating Syntactic Priming into an Incremental Probabilistic Parser,
with an Application to Psycholinguistic Modeling
Amit Dubey and Frank Keller and Patrick Sturt
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{amit.dubey,patrick.sturt,frank.keller}@ed.ac.uk
Abstract
The psycholinguistic literature provides
evidence for syntactic priming, i.e., the
tendency to repeat structures. This pa-
per describes a method for incorporating
priming into an incremental probabilis-
tic parser. Three models are compared,
which involve priming of rules between
sentences, within sentences, and within
coordinate structures. These models sim-
ulate the reading time advantage for par-
allel structures found in human data, and
also yield a small increase in overall pars-
ing accuracy.
1 Introduction
Over the last two decades, the psycholinguistic
literature has provided a wealth of experimental
evidence for syntactic priming, i.e., the tendency
to repeat syntactic structures (e.g., Bock, 1986).
Most work on syntactic priming has been con-
cerned with sentence production; however, recent
studies also demonstrate a preference for struc-
tural repetition in human parsing. This includes
the so-called parallelism effect demonstrated by
Frazier et al (2000): speakers processes coordi-
nated structures more quickly when the second
conjunct repeats the syntactic structure of the first
conjunct.
Two alternative accounts of the parallelism ef-
fect have been proposed. Dubey et al (2005) ar-
gue that the effect is simply an instance of a perva-
sive syntactic priming mechanism in human pars-
ing. They provide evidence from a series of cor-
pus studies which show that parallelism is not lim-
ited to co-ordination, but occurs in a wide range
of syntactic structures, both within and between
sentences, as predicted if a general priming mech-
anism is assumed. (They also show this effect is
stronger in coordinate structures, which could ex-
plain Frazier et al?s (2000) results.)
Frazier and Clifton (2001) propose an alterna-
tive account of the parallelism effect in terms of a
copying mechanism. Unlike priming, this mecha-
nism is highly specialized and only applies to co-
ordinate structures: if the second conjunct is en-
countered, then instead of building new structure,
the language processor simply copies the structure
of the first conjunct; this explains why a speed-
up is observed if the two conjuncts are parallel. If
the copying account is correct, then we would ex-
pect parallelism effects to be restricted to coordi-
nate structures and not to apply in other contexts.
This paper presents a parsing model which im-
plements both the priming mechanism and the
copying mechanism, making it possible to com-
pare their predictions on human reading time data.
Our model also simulates other important aspects
of human parsing: (i) it is broad-coverage, i.e.,
it yields accurate parses for unrestricted input,
and (ii) it processes sentences incrementally, i.e.,
on a word-by-word basis. This general modeling
framework builds on probabilistic accounts of hu-
man parsing as proposed by Jurafsky (1996) and
Crocker and Brants (2000).
A priming-based parser is also interesting from
an engineering point of view. To avoid sparse
data problems, probabilistic parsing models make
strong independence assumptions; in particular,
they generally assume that sentences are indepen-
dent of each other, in spite of corpus evidence for
structural repetition between sentences. We there-
fore expect a parsing model that includes struc-
tural repetition to provide a better fit with real cor-
pus data, resulting in better parsing performance.
A simple and principled approach to handling
structure re-use would be to use adaptation prob-
abilities for probabilistic grammar rules (Church,
2000), analogous to cache probabilities used in
caching language models (Kuhn and de Mori,
1990). This is the approach we will pursue in this
paper.
Dubey et al (2005) present a corpus study that
demonstrates the existence of parallelism in cor-
pus data. This is an important precondition for un-
derstanding the parallelism effect; however, they
417
do not develop a parsing model that accounts for
the effect, which means they are unable to evaluate
their claims against experimental data. The present
paper overcomes this limitation. In Section 2, we
present a formalization of the priming and copy-
ing models of parallelism and integrate them into
an incremental probabilistic parser. In Section 3,
we evaluate this parser against reading time data
taken from Frazier et al?s (2000) parallelism ex-
periments. In Section 4, we test the engineering
aspects of our model by demonstrating that a small
increase in parsing accuracy can be obtained with
a parallelism-based model. Section 5 provides an
analysis of the performance of our model, focus-
ing on the role of the distance between prime and
target.
2 Priming Models
We propose three models designed to capture the
different theories of structural repetition discussed
above. To keep our model as simple as possi-
ble, each formulation is based on an unlexicalized
probabilistic context free grammar (PCFG). In this
section, we introduce the models and discuss the
novel techniques used to model structural similar-
ity. We also discuss the design of the probabilistic
parser used to evaluate the models.
2.1 Baseline Model
The unmodified PCFG model serves as the Base-
line. A PCFG assigns trees probabilities by treat-
ing each rule expansion as conditionally indepen-
dent given the parent node. The probability of a
rule LHS ? RHS is estimated as:
P(RHS|LHS) = c(LHS ? RHS)
c(LHS)
2.2 Copy Model
The first model we introduce is a probabilistic
variant of Frazier and Clifton?s (2001) copying
mechanism: it models parallelism in coordination
and nothing else. This is achieved by assuming
that the default operation upon observing a coordi-
nator (assumed to be anything with a CC tag, e.g.,
?and?) is to copy the full subtree of the preced-
ing coordinate sister. Copying impacts on how the
parser works (see Section 2.5), and in a probabilis-
tic setting, it also changes the probability of trees
with parallel coordinated structures. If coordina-
tion is present, the structure of the second item is
either identical to the first, or it is not.1 Let us call
1The model only considers two-item coordination or thelast two sisters of multiple-item coordination.
the probability of having a copied tree as pident.This value may be estimated directly from a cor-
pus using the formula
p?ident =
cident
ctotal
Here, cident is the number of coordinate structuresin which the two conjuncts have the same internal
structure and ctotal is the total number of coordi-nate structures. Note we assume there is only one
parameter pident applicable everywhere (i.e., it hasthe same value for all rules).
How is this used in a PCFG parser? Let t1 and t2represent, respectively, the first and second coor-
dinate sisters and let PPCFG(t) be the PCFG prob-ability of an arbitrary subtree t.
Because of the independence assumptions of
the PCFG, we know that pident ? PPCFG(t). Oneway to proceed would be to assign a probability
of pident when structures match, and (1? pident) ?
PPCFG(t2) when structures do not match. However,some probability mass is lost this way: there is
a nonzero PCFG probability (namely, PPCFG(t1))that the structures match.
In other words, we may have identical subtrees
in two different ways: either due to a copy oper-
ation, or due to a PCFG derivation. If pcopy is theprobability of a copy operation, we can write this
fact more formally as: pident = PPCFG(t1)+ pcopy.Thus, if the structures do match, we assign the
second sister a probability of:
pcopy +PPCFG(t1)
If they do not match, we assign the second con-
junct the following probability:
1?PPCFG(t1)? pcopy
1?PPCFG(t1) ?PPCFG(t2)
This accounts for both a copy mismatch and a
PCFG derivation mismatch, and assures the prob-
abilities still sum to one. These probabilities for
parallel and non-parallel coordinate sisters, there-
fore, gives us the basis of the Copy model.
This leaves us with the problem of finding an
estimate for pcopy. This value is approximated as:
p?copy = p?ident ?
1
|T2| ?t?T2 PPCFG(t)
In this equation, T2 is the set of all second con-juncts.
2.3 Between Model
While the Copy model limits itself to parallelism
in coordination, the next two models simulate
structural priming in general. Both are similar in
design, and are based on a simple insight: we may
418
condition a PCFG rule expansion on whether the
rule occurred in some previous context. If Prime is
a binary-valued random variable denoting if a rule
occurred in the context, then we define:
P(RHS|LHS,Prime) = c(LHS ? RHS,Prime)
c(LHS,Prime)
This is essentially an instantiation of Church?s
(2000) adaptation probability, albeit with PCFG
rules instead of words. For our first model, this
context is the previous sentence. Thus, the model
can be said to capture the degree to which rule use
is primed between sentences. We henceforth refer
to this as the Between model. Following the con-
vention in the psycholinguistic literature, we refer
to a rule use in the previous sentence as a ?prime?,
and a rule use in the current sentence as the ?tar-
get?. Each rule acts once as a target (i.e., the event
of interest) and once as a prime. We may classify
such adapted probabilities into ?positive adapta-
tion?, i.e., the probability of a rule given the rule
occurred in the preceding sentence, and ?negative
adaptation?, i.e., the probability of a rule given that
the rule did not occur in the preceding sentence.
2.4 Within Model
Just as the Between model conditions on rules
from the previous sentence, the Within sentence
model conditions on rules from earlier in the cur-
rent sentence. Each rule acts once as a target, and
possibly several times as a prime (for each subse-
quent rule in the sentence). A rule is considered
?used? once the parser passes the word on the left-
most corner of the rule. Because the Within model
is finer grained than the Between model, it can be
used to capture the parallelism effect in coordina-
tion. In other words, this model could explain par-
allelism in coordination as an instance of a more
general priming effect.
2.5 Parser
As our main purpose is to build a psycholinguistic
model of structure repetition, the most important
feature of the parsing model is to build structures
incrementally.2
Reading time experiments, including the paral-
lelism studies of Frazier et al (2000), make word-
by-word measurements of the time taken to read
2In addition to incremental parsing, a characteristic someof psycholinguistic models of sentence comprehension is toparse deterministically. While we can compute the best in-cremental analysis at any point, ours models do not parse de-terministically. However, following the principles of rationalanalysis (Anderson, 1991), our goal is not to mimic the hu-man parsing mechanism, but rather to create a model of hu-man parsing behavior.
a novel and a bookwrote
0 3
Terry
4 5 61 2 7
NP NP
NP
a novel and a bookTerry wrote
0 31 4 5 62 7
NP
NP NP
Figure 1: Upon encountering a coordinator, the
copy model copies the most likely first conjunct.
sentences. Slower reading times are known to be
correlated with processing difficulty, and faster
reading times (as is the case with parallel struc-
tures) are correlated with processing ease. A prob-
abilistic parser may be considered to be a sen-
tence processing model via a ?linking hypothesis?,
which links the parser?s word-by-word behavior to
human reading behavior. We discuss this topic in
more detail in Section 3. At this point, it suffices
to say that we require a parser which has the pre-
fix property, i.e., which parses incrementally, from
left to right.
Therefore, we use an Earley-style probabilis-
tic parser, which outputs Viterbi parses (Stolcke,
1995). We have two versions of the parser: one
which parses exhaustively, and a second which
uses a variable width beam, pruning any edges
whose merit is 12000 of the best edge. The meritof an edge is its inside probability times a prior
P(LHS) times a lookahead probability (Roark and
Johnson, 1999). To speed up parsing time, we right
binarize the grammar,3 remove empty nodes, coin-
dexation and grammatical functions. As our goal
is to create the simplest possible model which can
nonetheless model experimental data, we do not
make any tree modification designed to improve
accuracy (as, e.g., Klein and Manning 2003).
The approach used to implement the Copy
model is to have the parser copy the subtree of the
first conjunct whenever it comes across a CC tag.
Before copying, though, the parser looks ahead to
check if the part-of-speech tags after the CC are
equivalent to those inside the first conjunct. The
copying model is visualized in Figure 1: the top
panel depicts a partially completed edge upon see-
ing a CC tag, and the second panel shows the com-
pleted copying operation. It should be clear that
3We found that using an unbinarized grammar did not al-ter the results, at least in the exhaustive parsing case.
419
the copy operation gives the most probable sub-
tree in a given span. To illustrate this, consider Fig-
ure 1. If the most likely NP between spans 2 and 7
does not involve copying (i.e. only standard PCFG
rule derivations), the parser will find it using nor-
mal rule derivations. If it does involve copying, for
this particular rule, it must involve the most likely
NP subtree from spans 2 to 3. As we parse in-
crementally, we are guaranteed to have found this
edge, and can use it to construct the copied con-
junct over spans 5 to 7 and therefore the whole
co-ordinated NP from spans 2 to 7.
To simplify the implementation of the copying
operation, we turn off right binarization so that the
constituent before and after a coordinator are part
of the same rule, and therefore accessible from the
same edge. This makes it simple to calculate the
new probability: construct the copied subtree, and
decide where to place the resulting edge on the
chart.
The Between and Within models require a cache
of recently used rules. This raises two dilem-
mas. First, in the Within model, keeping track of
full contextual history is incompatible with chart
parsing. Second, whenever a parsing error occurs,
the accuracy of the contextual history is compro-
mised. As we are using a simple unlexicalized
parser, such parsing errors are probably quite fre-
quent.
We handle the first problem by using one sin-
gle parse as an approximation of the history. The
more realistic choice for this single parse is the
best parse so far according to the parser. Indeed,
this is the approach we use for our main results in
Section 3. However, because of the second prob-
lem noted above, in Section 4, we simulated the
context by filling the cache with rules from the
correct tree. In the Between model, these are the
rules of the correct parse of the previous tree; in
the Within model, these are the rules used in the
correct parse at points up to (but not including) the
current word.
3 Human Reading Time Experiment
In this section, we test our models by applying
them to experimental reading time data. Frazier
et al (2000) reported a series of experiments that
examined the parallelism preference in reading. In
one of their experiments, they monitored subjects?
eye-movements while they read sentences like (1):
(1) a. Hilda noticed a strange man and a tall
woman when she entered the house.
b. Hilda noticed a man and a tall woman
when she entered the house.
They found that total reading times were faster on
the phrase tall woman in (1a), where the coordi-
nated noun phrases are parallel in structure, com-
pared with in (1b), where they are not.
There are various approaches to modeling pro-
cessing difficulty using a probabilistic approach.
One possibility is to use an incremental parser
with a beam search or an n-best approach. Pro-
cessing difficulty is predicted at points in the input
string where the current best parse is replaced by
an alternative derivation (Jurafsky, 1996; Crocker
and Brants, 2000). An alternative is to keep track
of all derivations, and predict difficulty at points
where there is a large change in the shape of
the probability distribution across adjacent pars-
ing states (Hale, 2001). A third approach is to
calculate the forward probability (Stolcke, 1995)
of the sentence using a PCFG. Low probabilities
are then predicted to correspond to high process-
ing difficulty. A variant of this third approach is
to assume that processing difficulty is correlated
with the (log) probability of the best parse (Keller,
2003). This final formulation is the one used for
the experiments presented in this paper.
3.1 Method
The item set was adapted from that of Frazier et al
(2000). The original two relevant conditions of
their experiment (1a,b) differ in terms of length.
This results in a confound in the PCFG frame-
work, because longer sentences tend to result in
lower probabilities (as the parses tend to involve
more rules). To control for such length differences,
we adapted the materials by adding two extra con-
ditions in which the relation between syntactic
parallelism and length was reversed. This resulted
in the following four conditions:
(2) a. DT JJ NN and DT JJ NN (parallel)
Hilda noticed a tall man and a strange
woman when she entered the house.
b. DT NN and DT JJ NN (non-parallel)
Hilda noticed a man and a strange
woman when she entered the house.
c. DT JJ NN and DT NN (non-parallel)
Hilda noticed a tall man and a woman
when she entered the house.
d. DT NN and DT NN (parallel)
Hilda noticed a man and a woman when
she entered the house.
420
In order to account for Frazier et al?s paral-
lelism effect a probabilistic model should pre-
dict a greater difference in probability be-
tween (2a) and (2b) than between (2c) and (2d)
(i.e., (2a)?(2b) > (2c)?(2d)). This effect will not
be confounded with length, because the relation
between length and parallelism is reversed be-
tween (2a,b) and (2c,d). We added 8 items to the
original Frazier et al materials, resulting in a new
set of 24 items similar to (2).
We tested three of our PCFG-based models on
all 24 sets of 4 conditions. The models were the
Baseline, the Within and the Copy models, trained
exactly as described above. The Between model
was not tested as the experimental stimuli were
presented without context. Each experimental sen-
tence was input as a sequence of correct POS tags,
and the log probability estimate of the best parse
was recorded.
3.2 Results and Discussion
Table 1 shows the mean log probabilities estimated
by the models for the four conditions, along with
the relevant differences between parallel and non-
parallel conditions.
Both the Within and the Copy models show a
parallelism advantage, with this effect being much
more pronounced for the Copy model than the
Within model. To evaluate statistical significance,
the two differences for each item were compared
using a Wilcoxon signed ranks test. Significant
results were obtained both for the Within model
(N = 24, Z = 1.67, p < .05, one-tailed) and for
the Copy model (N = 24, Z = 4.27, p < .001, one-
tailed). However, the effect was much larger for
the Copy model, a conclusion which is confirmed
by comparing the differences of differences be-
tween the two models (N = 24, Z = 4.27, p < .001,
one-tailed). The Baseline model was not evalu-
ated statistically, because by definition it predicts a
constant value for (2a)?(2b) and (2c)?(2d) across
all items. This is simply a consequence of the
PCFG independence assumption, coupled with the
fact that the four conditions of each experimen-
tal item differ only in the occurrences of two NP
rules.
The results show that the approach taken here
can be successfully applied to the modeling of
experimental data. In particular, both the Within
and the Copy models show statistically reliable
parallelism effects. It is not surprising that the
copy model shows a large parallelism effect for
the Frazier et al (2000) items, as it was explicitly
designed to prefer structurally parallel conjuncts.
The more interesting result is the parallelism ef-
fect found for the Within model, which shows that
such an effect can arise from a more general prob-
abilistic priming mechanism.
4 Parsing Experiment
In the previous section, we were able to show that
the Copy and Within models are able to account
for human reading-time performance for parallel
coordinate structures. While this result alone is
sufficient to claim success as a psycholinguistic
model, it has been argued that more realistic psy-
cholinguistic models ought to also exhibit high ac-
curacy and broad-coverage, both crucial properties
of the human parsing mechanism (e.g., Crocker
and Brants, 2000).
This should not be difficult: our starting point
was a PCFG, which already has broad coverage
behavior (albeit with only moderate accuracy).
However, in this section we explore what effects
our modifications have to overall coverage, and,
perhaps more interestingly, to parsing accuracy.
4.1 Method
The models used here were the ones introduced
in Section 2 (which also contains a detailed de-
scription of the parser that we used to apply the
models). The corpus used for both training and
evaluation is the Wall Street Journal part of the
Penn Treebank. We use sections 1?22 for train-
ing, section 0 for development and section 23 for
testing. Because the Copy model posits coordi-
nated structures whenever POS tags match, pars-
ing efficiency decreases if POS tags are not pre-
determined. Therefore, we assume POS tags as in-
put, using the gold-standard tags from the treebank
(following, e.g., Roark and Johnson 1999).
4.2 Results and Discussion
Table 2 lists the results in terms of F-score on
the test set.4 Using exhaustive search, the base-
line model achieves an F-score of 73.3, which is
comparable to results reported for unlexicalized
incremental parsers in the literature (e.g. the RB1
model of Roark and Johnson, 1999). All models
exhibit a small decline in performance when beam
search is used. For the Within model we observe a
slight improvement in performance over the base-
line, both for the exhaustive search and the beam
4Based on a ?2 test on precision and recall, all results arestatistically different from each other. The Copy model actu-ally performs slightly better than the Baseline in the exhaus-tive case.
421
Model para: (2a) non-para: (2b) non-para: (2c) para: (2d) (2a)?(2b) (2c)?(2d)
Baseline ?33.47 ?32.37 ?32.37 ?31.27 ?1.10 ?1.10
Within ?33.28 ?31.67 ?31.70 ?29.92 ?1.61 ?1.78
Copy ?16.18 ?27.22 ?26.91 ?15.87 11.04 ?11.04
Table 1: Mean log probability estimates for Frazier et al(2000) items
Exhaustive Search Beam Search Beam + Coord Fixed Coverage
Model F-score Coverage F-score Coverage F-score Coverage F-score Coverage
Baseline 73.3 100 73.0 98.0 73.1 98.1 73.0 97.5
Within 73.6 100 73.4 98.4 73.0 98.5 73.4 97.5
Between 71.6 100 71.7 98.7 71.5 99.0 71.8 97.5
Copy 73.3 100 ? ? 73.0 98.1 73.1 97.5
Table 2: Parsing results for the Within, Between, and Copy model compared to a PCFG baseline.
search conditions. The Between model, however,
resulted in a decrease in performance.
We also find that the Copy model performs at
the baseline level. Recall that in order to simplify
the implementation of the copying, we had to dis-
able binarization for coordinate constituents. This
means that quaternary rules were used for coordi-
nation (X ? X1 CC X2 X ?), while normal binaryrules (X ? Y X ?) were used everywhere else. It
is conceivable that this difference in binarization
explains the difference in performance between
the Between and Within models and the Copy
model when beam search was used. We there-
fore also state the performance for Between and
Within models with binarization limited to non-
coordinate structures in the column labeled ?Beam
+ Coord? in Table 2. The pattern of results, how-
ever, remains the same.
The fact that coverage differs between models
poses a problem in that it makes it difficult to
compare the F-scores directly. We therefore com-
pute separate F-scores for just those sentences that
were covered by all four models. The results are
reported in the ?Fixed Coverage? column of Ta-
ble 2. Again, we observe that the copy model per-
forms at baseline level, while the Within model
slightly outperforms the baseline, and the Between
model performs worse than the baseline. In Sec-
tion 5 below we will present an error analysis that
tries to investigate why the adaptation models do
not perform as well as expected.
Overall, we find that the modifications we intro-
duced to model the parallelism effect in humans
have a positive, but small, effect on parsing ac-
curacy. Nonetheless, the results also indicate the
success of both the Copy and Within approaches
to parallelism as psycholinguistic models: a mod-
ification primarily useful for modeling human be-
havior has no negative effects on computational
measures of coverage or accuracy.
5 Distance Between Rule Uses
Although both the Within and Copy models suc-
ceed at the main task of modeling the paral-
lelism effect, the parsing experiments in Section 4
showed mixed results with respect to F-scores:
a slight increase in F-score was observed for the
Within model, but the Between model performed
below the baseline. We therefore turn to an error
analysis, focusing on these two models.
Recall that the Within and Between models es-
timate two probabilities for a rule, which we have
been calling the positive adaptation (the probabil-
ity of a rule when the rule is also in the history),
and the negative adaptation (the probability of a
rule when the rule is not in the history). While
the effect is not always strong, we expect positive
adaptation to be higher than negative adaptation
(Dubey et al, 2005). However, this is not always
the case.
In the Within model, for example, the rule
NP ? DT JJ NN has a higher negative than posi-
tive adaptation (we will refer to such rules as ?neg-
atively adapted?). The more common rule NP ?
DT NN has a higher positive adaptation (?pos-
itively adapted?). Since the latter is three times
more common, this raises a concern: what if adap-
tation is an artifact of frequency? This ?frequency?
hypothesis posits that a rule recurring in a sentence
is simply an artifact of the its higher frequency.
The frequency hypothesis could explain an inter-
esting fact: while the majority of rules tokens have
positive adaptation, the majority of rule types have
negative adaptation. An important corollary of the
frequency hypothesis is that we would not expect
to find a bias towards local rule re-uses.
422
Iterate through the treebank
Remember how many words each constituent spans
Iterate through the treebank
Iterate through each tree
Upon finding a constituent spanning 1-4 words
Swap it with a randomly chosen constituent
of 1-4 words
Update the remembered size of the swapped
constituents and their subtrees
Iterate through the treebank 4 more times
Swap constituents of size 5-9, 10-19, 20-35
and 35+ words, respectively
Figure 2: The treebank randomization algorithm
Nevertheless, the NP ? DT JJ NN rule is
an exception: most negatively adapted rules have
very low frequencies. This raises the possibility
that sparse data is the cause of the negatively
adapted rules. This makes intuitive sense: we need
many rule occurrences to accurately estimate pos-
itive or negative adaptation.
We measure the distribution of rule use to ex-
plore if negatively adapted rules owe more to fre-
quency effects or to sparse data. This distributional
analysis also serves to measure ?decay? effects in
structural repetition. The decay effect in priming
has been observed elsewhere (Szmrecsanyi, 2005),
and suggests that positive adaptation is higher the
closer together two rules are.
5.1 Method
We investigate the dispersion of rules by plot-
ting histograms of the distance between subse-
quent rule uses. The basic premise is to look for
evidence of an early peak or skew, which sug-
gests rule re-use. To ensure that the histogram it-
self is not sensitive to sparse data problems, we
group all rules into two categories: those which are
positively adapted, and those which are negatively
adapted.
If adaptation is not due to frequency alone, we
would expect the histograms for both positively
and negatively adapted rules to be skewed towards
local rule repetition. Detecting a skew requires a
baseline without repetition. We propose the con-
cept of ?randomizing? the treebank to create such
a baseline. The randomization algorithm is de-
scribed in Figure 2. The algorithm entails swap-
ping subtrees, taking care that small subtrees are
swapped first (otherwise large chunks would be
swapped at once, preserving a great deal of con-
text). This removes local effects, giving a distribu-
tion due frequency alone.
After applying the randomization algorithm to
the treebank, we may construct the distance his-
0 5 10Logarithm of Word Distance
0
0.005
0.01
0.015
0.02
No
rm
aliz
ed 
Fre
que
ncy
 of 
Ru
le O
ccu
ran
ce
+ Adapt, Untouched Corpus
+ Adapt, Randomized Corpus
- Adapt, Untouched Corpus
- Adapt, Randomized Corpus
Figure 3: Log of number of words between rule
invocations
togram for both the non-randomized and random-
ized treebanks. The distance between two occur-
rences of a rule is calculated as the number of
words between the first word on the left corner of
each rule. A special case occurs if a rule expansion
invokes another use of the same rule. When this
happens, we do not count the distance between the
first and second expansion. However, the second
expansion is still remembered as the most recent.
We group rules into those that have a higher
positive adaptation and those that have a higher
negative adaptation. We then plot a histogram of
rule re-occurrence distance for both groups, in
both the non-randomized and randomized corpora.
5.2 Results and Discussion
The resulting plot for the Within model is shown
in Figure 3. For both the positive and negatively
adapted rules, we find that randomization results
in a lower, less skewed peak, and a longer tail.
We conclude that rules tend to be repeated close
to one another more than we expect by chance,
even for negatively adapted rules. This is evidence
against the frequency hypothesis, and in favor of
the sparse data hypothesis. This means that the
small size of the increase in F-score we found in
Section 4 is not due to the fact that the adaption
is just an artifact of rule frequency. Rather, it can
probably be attributed to data sparseness.
Note also that the shape of the histogram pro-
vides a decay curve. Speculatively, we suggest that
this shape could be used to parameterize the decay
effect and therefore provide an estimate for adap-
tation which is more robust to sparse data. How-
ever, we leave the development of such a smooth-
ing function to future research.
423
6 Conclusions and Future Work
The main contribution of this paper has been to
show that an incremental parser can simulate syn-
tactic priming effects in human parsing by incor-
porating probability models that take account of
previous rule use. Frazier et al (2000) argued that
the best account of their observed parallelism ad-
vantage was a model in which structure is copied
from one coordinate sister to another. Here, we ex-
plored a probabilistic variant of the copy mecha-
nism, along with two more general models based
on within- and between-sentence priming. Al-
though the copy mechanism provided the strongest
parallelism effect in simulating the human reading
time data, the effect was also successfully simu-
lated by a general within-sentence priming model.
On the basis of simplicity, we therefore argue that
it is preferable to assume a simpler and more gen-
eral mechanism, and that the copy mechanism is
not needed. This conclusion is strengthened when
we turn to consider the performance of the parser
on the standard Penn Treebank test set: the Within
model showed a small increase in F-score over the
PCFG baseline, while the copy model showed no
such advantage.5
All the models we proposed offer a broad-
coverage account of human parsing, not just a lim-
ited model on a hand-selected set of examples,
such as the models proposed by Jurafsky (1996)
and Hale (2001) (but see Crocker and Brants
2000).
A further contribution of the present paper has
been to develop a methodology for analyzing the
(re-)use of syntactic rules over time in a corpus. In
particular, we have defined an algorithm for ran-
domizing the constituents of a treebank, yielding
a baseline estimate of chance repetition.
In the research reported in this paper, we have
adopted a very simple model based on an unlex-
icalized PCFG. In the future, we intend to ex-
plore the consequences of introducing lexicaliza-
tion into the parser. This is particularly interest-
ing from the point of view of psycholinguistic
modeling, because there are well known inter-
actions between lexical repetition and syntactic
priming, which require lexicalization for a proper
treatment. Future work will also involve the use
of smoothing to increase the benefit of priming
for parsing accuracy. The investigations reported
5The broad-coverage parsing experiment speaks againsta ?facilitation? hypothesis, i.e., that the copying and prim-ing mechanisms work together. However, a full test of this(e.g., by combining the two models) is left to future research.
in Section 5 provide a basis for estimating the
smoothing parameters.
References
Anderson, John. 1991. Cognitive architectures in a ratio-nal analysis. In K. VanLehn, editor, Architectures for In-
telligence, Lawrence Erlbaum Associates, Hillsdale, N.J.,pages 1?24.
Bock, J. Kathryn. 1986. Syntactic persistence in languageproduction. Cognitive Psychology 18:355?387.
Church, Kenneth W. 2000. Empirical estimates of adapta-
tion: the chance of two Noriegas is closer to p/2 than p2.In Proceedings of the 17th Conference on Computational
Linguistics. Saarbru?cken, Germany, pages 180?186.
Crocker, Matthew W. and Thorsten Brants. 2000. Wide-coverage probabilistic sentence processing. Journal of
Psycholinguistic Research 29(6):647?669.
Dubey, Amit, Patrick Sturt, and Frank Keller. 2005. Paral-lelism in coordination as an instance of syntactic priming:Evidence from corpus-based modeling. In Proceedings
of the Human Language Technology Conference and the
Conference on Empirical Methods in Natural Language
Processing. Vancouver, pages 827?834.
Frazier, Lyn, Alan Munn, and Chuck Clifton. 2000. Process-ing coordinate structures. Journal of Psycholinguistic Re-
search 29(4):343?370.
Frazier, Lynn and Charles Clifton. 2001. Parsing coordinatesand ellipsis: Copy ?. Syntax 4(1):1?22.
Hale, John. 2001. A probabilistic Earley parser as a psy-cholinguistic model. In Proceedings of the 2nd Confer-
ence of the North American Chapter of the Association
for Computational Linguistics. Pittsburgh, PA.
Jurafsky, Daniel. 1996. A probabilistic model of lexical andsyntactic access and disambiguation. Cognitive Science20(2):137?194.
Keller, Frank. 2003. A probabilistic parser as a model ofglobal processing difficulty. In R. Alterman and D. Kirsh,editors, Proceedings of the 25th Annual Conference of the
Cognitive Science Society. Boston, pages 646?651.
Klein, Dan and Christopher D. Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.Sapporo, Japan, pages 423?430.
Kuhn, Roland and Renate de Mori. 1990. A cache-based nat-ural language model for speech recognition. IEEE Tran-
sanctions on Pattern Analysis and Machine Intelligence12(6):570?583.
Roark, Brian and Mark Johnson. 1999. Efficient probabilistictop-down and left-corner parsing. In Proceedings of the
37th Annual Meeting of the Association for Computational
Linguistics. pages 421?428.
Stolcke, Andreas. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.
Computational Linguistics 21(2):165?201.
Szmrecsanyi, Benedikt. 2005. Creatures of habit: A corpus-linguistic analysis of persistence in spoken English. Cor-
pus Linguistics and Linguistic Theory 1(1):113?149.
424
Using the Web to Overcome Data Sparseness
Frank Keller and Maria Lapata
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
fkeller, mlapg@cogsci.ed.ac.uk
Olga Ourioupina
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
ourioupi@coli.uni-sb.de
Abstract
This paper shows that the web can be em-
ployed to obtain frequencies for bigrams
that are unseen in a given corpus. We
describe a method for retrieving counts
for adjective-noun, noun-noun, and verb-
object bigrams from the web by querying
a search engine. We evaluate this method
by demonstrating that web frequencies
and correlate with frequencies obtained
from a carefully edited, balanced corpus.
We also perform a task-based evaluation,
showing that web frequencies can reliably
predict human plausibility judgments.
1 Introduction
In two recent papers, Banko and Brill (2001a;
2001b) criticize the fact that current NLP algo-
rithms are typically optimized, tested, and compared
on fairly small data sets (corpora with millions of
words), even though data sets several orders of mag-
nitude larger are available, at least for some tasks.
Banko and Brill go on to demonstrate that learning
algorithms typically used for NLP tasks benefit sig-
nificantly from larger training sets, and their perfor-
mance shows no sign of reaching an asymptote as
the size of the training set increases.
Arguably, the largest data set that is available
for NLP is the web, which currently consists of
at least 968 million pages.1 Data retrieved from
the web therefore provides enormous potential
1This is the number of pages indexed by Google in
March 2002, as estimated by Search Engine Showdown (see
http://www.searchengineshowdown.com/).
for training NLP algorithms, if Banko and Brill?s
findings generalize. There is a small body of
existing research that tries to harness the potential
of the web for NLP. Grefenstette and Nioche (2000)
and Jones and Ghani (2000) use the web to
generate corpora for languages where elec-
tronic resources are scarce, while Resnik (1999)
describes a method for mining the web for bilin-
gual texts. Mihalcea and Moldovan (1999) and
Agirre and Martinez (2000) use the web for word
sense disambiguation, and Volk (2001) proposes a
method for resolving PP attachment ambiguities
based on web data.
A particularly interesting application is pro-
posed by Grefenstette (1998), who uses the web
for example-based machine translation. His task is
to translate compounds from French into English,
with corpus evidence serving as a filter for candi-
date translations. As an example consider the French
compound groupe de travail. There are five transla-
tion of groupe and three translations for travail (in
the dictionary that Grefenstette (1998) is using), re-
sulting in 15 possible candidate translations. Only
one of them, viz., work group has a high corpus
frequency, which makes it likely that this is the
correct translation into English. Grefenstette (1998)
observes that this approach suffers from an acute
data sparseness problem if the corpus counts are
obtained from a conventional corpus such as the
British National Corpus (BNC) (Burnard, 1995).
However, as Grefenstette (1998) demonstrates, this
problem can be overcome by obtaining counts
through web searches, instead of relying on the
BNC. Grefenstette (1998) therefore effectively uses
the web as a way of obtaining counts for compounds
that are sparse in the BNC.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 230-237.
                         Proceedings of the Conference on Empirical Methods in Natural
While this is an important initial result, it raises
the question of the generality of the proposed ap-
proach to overcoming data sparseness. It remains
to be shown that web counts are generally useful
for approximating data that is sparse or unseen in
a given corpus. It seems possible, for instance, that
Grefenstette?s (1998) results are limited to his par-
ticular task (filtering potential translations) or to his
particular linguistic phenomenon (noun-noun com-
pounds). Another potential problem is the fact that
web counts are far more noisy than counts obtained
from a well-edited, carefully balanced corpus such
as the BNC. The effect of this noise on the useful-
ness of the web counts is largely unexplored.
The aim of the present paper is to generalize
Grefenstette?s (1998) findings by testing the hypoth-
esis that the web can be employed to obtain frequen-
cies for bigrams that are unseen in a given corpus.
Instead of having a particular task in mind (which
would introduce a sampling bias), we rely on sets of
bigrams that are randomly selected from the corpus.
We use a web-based approach not only for noun-
noun bigrams, but also for adjective-noun and verb-
object bigrams, so as to explore whether this ap-
proach generalizes to different predicate-argument
combinations. We evaluate our web counts in two
different ways: (a) comparison with actual corpus
frequencies, and (b) task-based evaluation (predict-
ing human plausibility judgments).
2 Obtaining Frequencies from the Web
2.1 Sampling Bigrams
Two types of adjective-noun bigrams were used in
the present study: seen bigrams, i.e., bigrams that
occur in a given corpus, and unseen bigrams, i.e.,
bigrams that fail to occur in the corpus. For the
seen adjective-noun bigrams, we used the data of
Lapata et al (1999), who compiled a set of 90 bi-
grams as follows. First, 30 adjectives were randomly
chosen from a lemmatized version of the BNC so
that each adjective had exactly two senses accord-
ing to WordNet (Miller et al, 1990) and was unam-
biguously tagged as ?adjective? 98.6% of the time.
The 30 adjectives ranged in BNC frequency from 1.9
to 49.1 per million. Gsearch (Corley et al, 2001),
a chart parser which detects syntactic patterns in a
tagged corpus by exploiting a user-specified con-
text free grammar and a syntactic query, was used
to extract all nouns occurring in a head-modifier re-
lationship with one of the 30 adjectives. Bigrams in-
volving proper nouns or low-frequency nouns (less
than 10 per million) were discarded. For each ad-
jective, the set of bigrams was divided into three fre-
quency bands based on an equal division of the range
of log-transformed co-occurrence frequencies. Then
one bigram was chosen at random from each band.
Lapata et al (2001) compiled a set of 90 unseen
adjective-noun bigrams using the same 30 adjec-
tives. For each adjective, the Gsearch chunker was
used to compile a list of all nouns that failed to co-
occur in a head-modifier relationship with the adjec-
tive. Proper nouns and low-frequency nouns were
discarded from this list. Then each adjective was
paired with three randomly chosen nouns from its
list of non-co-occurring nouns.
For the present study, we applied the procedure
used by Lapata et al (1999) and Lapata et al (2001)
to noun-noun bigrams and to verb-object bigrams,
creating a set of 90 seen and 90 unseen bigrams for
each type of predicate-argument relationship. More
specifically, 30 nouns and 30 verbs were chosen ac-
cording to the same criteria proposed for the adjec-
tive study (i.e., minimal sense ambiguity and unam-
biguous part of speech). All nouns modifying one of
the 30 nouns were extracted from the BNC using a
heuristic which looks for consecutive pairs of nouns
that are neither preceded nor succeeded by another
noun (Lauer, 1995). Verb-object bigrams for the
30 preselected verbs were obtained from the BNC
using Cass (Abney, 1996), a robust chunk parser de-
signed for the shallow analysis of noisy text. The
parser?s output was post-processed to remove brack-
eting errors and errors in identifying chunk cate-
gories that could potentially result in bigrams whose
members do not stand in a verb-argument relation-
ship (see Lapata (2001) for details on the filtering
process). Only nominal heads were retained from
the objects returned by the parser. As in the adjec-
tive study, noun-noun bigrams and verb-object bi-
grams with proper nouns or low-frequency nouns
(less than 10 per million) were discarded. The sets
of noun-noun and verb-object bigrams were divided
into three frequency bands and one bigram was cho-
sen at random from each band.
The procedure described by Lapata et al (2001)
was followed for creating sets of unseen noun-noun
and verb-object bigrams: for each of noun or verb,
we compiled a list of all nouns with which it failed
to co-occur with in a noun-noun or verb-object bi-
gram in the BNC. Again, Lauer?s (1995) heuristic
and Abney?s (1996) partial parser were used to iden-
tify bigrams, and proper nouns and low-frequency
nouns were excluded. For each noun and verb, three
bigrams were randomly selected from the set of their
non-co-occurring nouns.
Table 1 lists examples for the seen and unseen
noun-noun and verb-object bigrams generated by
this procedure.
2.2 Obtaining Web Counts
Web counts for bigrams were obtained using a sim-
ple heuristic based on queries to the search engines
Altavista and Google. All search terms took into
account the inflectional morphology of nouns and
verbs.
The search terms for verb-object bigrams matched
not only cases in which the object was directly ad-
jacent to the verb (e.g., fulfill obligation), but also
cases where there was an intervening determiner
(e.g., fulfill the/an obligation). The following search
terms were used for adjective-noun, noun-noun, and
verb-object bigrams, respectively:
(1) "A N", where A is the adjective and N is the sin-
gular or plural form of the noun.
(2) "N1 N2" where N1 is the singular form of the
first noun and N2 is the singular or plural form
of the second noun.
(3) "V Det N" where V is the infinitive, singular
present, plural present, past, perfect, or gerund
for of the verb, Det is the determiner the, a or
the empty string, and N is the singular or plural
form of the noun.
Note that all searches were for exact matches, which
means that the search terms were required to be di-
rectly adjacent on the matching page. This is en-
coded using quotation marks to enclose the search
term. All our search terms were in lower case.
For Google, the resulting bigram frequencies
were obtained by adding up the number of pages
that matched the expanded forms of the search terms
in (1), (2), and (3). Altavista returns not only the
number of matches, but also the number of words
adj-noun noun-noun verb-object
Altavista 14 10 16
Google 5 3 5
Table 2: Number of zero counts returned by the
queries to search engines (unseen bigrams)
that match the search term. We used this count, as it
takes multiple matches per page into account, and is
thus likely to produce more accurate frequencies.
The process of obtaining bigram frequencies from
the web can be automated straightforwardly using a
script that generates all the search terms for a given
bigram (from (1)?(3)), issues an Altavista or Google
query for each of the search terms, and then adds
up the resulting number of matches for each bigram.
We applied this process to all the bigrams in our data
set, covering seen and unseen adjective-noun, noun-
noun, and verb-object bigrams, i.e., 540 bigrams in
total.
A small number of bigrams resulted in zero
counts, i.e., they failed to yield any matches in the
web search. Table 2 lists the number of zero bigrams
for both search engines. Note that Google returned
fewer zeros than Altavista, which presumably indi-
cates that it indexes a larger proportion of the web.
We adjusted the zero counts by setting them to one.
This was necessary as all further analyses were car-
ried out on log-transformed frequencies.
Table 3 lists the descriptive statistics for the
bigram counts we obtained using Altavista and
Google.
From these data, we computed the average fac-
tor by which the web counts are larger than the
BNC counts. The results are given in Table 4 and
indicate that the Altavista counts are between 331
and 467 times larger than the BNC counts, while
the Google counts are between 759 and 977 times
larger than the BNC counts. As we know the size
of the BNC (100 million words), we can use these
figures to estimate the number of words on the web:
between 33.1 and 46.7 billion words for Altavista,
and between 75.9 and 97.7 billion words for Google.
These estimates are in the same order of magnitude
as Grefenstette and Nioche?s (2000) estimate that
48.1 billion words of English are available on the
web (based on Altavista counts in February 2000).
noun-noun bigrams
high medium low unseen predicate
process 1.14 user .95 gala 0 collection, clause, coat directory
television 1.53 satellite .95 edition 0 chain, care, vote broadcast
plasma 1.78 nylon 1.20 unit .60 fund, theology, minute membrane
verb-object bigrams
predicate high medium low unseen
fulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, grade
intensify problem 1.79 effect 1.10 alarm 0 score, quota, chest
choose name 3.74 law 1.61 series 1.10 lift, bride, listener
Table 1: Example stimuli for seen and unseen noun-noun and verb-object bigrams (with log-transformed
BNC counts)
seen bigrams
adj-noun noun-noun verb-object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Altavista 0 5.67 3.55 1.06 .67 6.28 3.41 1.21 0 5.46 3.20 1.14
Google 1.26 5.98 3.89 1.00 .90 6.11 3.66 1.20 0 5.85 3.56 1.16
BNC 0 2.19 .90 .69 0 2.14 .74 .64 0 2.55 .68 .58
unseen bigrams
adj-noun noun-noun verb-object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Altavista 0 4.04 1.29 .94 0 3.80 1.08 1.12 0 3.72 1.38 1.06
Google 0 3.99 1.68 .96 0 4.00 1.42 1.09 0 4.07 1.76 1.04
Table 3: Descriptive statistics for web counts and BNC counts (log-transformed)
adj-noun noun-noun verb-object
Altavista 447 467 331
Google 977 831 759
Table 4: Average factor by which the web counts are
larger than the BNC counts (seen bigrams)
3 Evaluation
3.1 Evaluation Against Corpus Frequencies
While the procedure for obtaining web counts de-
scribed in Section 2.2 is very straightforward, it also
has obvious limitations. Most importantly, it is based
on bigrams formed by adjacent words, and fails to
take syntactic variants into account (other than in-
tervening determiners for verb-object bigrams). In
the case of Google, there is also the problem that the
counts are based on the number of matching pages,
not the number of matching words. Finally, there is
the problem that web data is very noisy and unbal-
anced compared to a carefully edited corpus like the
BNC.
Given these limitations, it is necessary to explore
if there is a reliable relationship between web counts
and BNC counts. Once this is assured, we can ex-
plore the usefulness of web counts for overcoming
data sparseness. We carried out a correlation analy-
sis to determine if there is a linear relationship be-
tween the BNC counts and Altavista and Google
counts. The results of this analysis are listed in Ta-
ble 5. All correlation coefficients reported in this pa-
per refer to Pearson?s r and were computed on log-
transformed counts.
A high correlation coefficient was obtained across
the board, ranging from .675 to .822 for Altavista
counts and from .737 to .849 for Google counts.
This indicates that web counts approximate BNC
counts for the three types of bigrams under inves-
tigation, with Google counts slightly outperform-
ing Altavista counts. We conclude that our simple
adj-noun noun-noun verb-object
Altavista .821** .744** .675**
Google .849** .737** .751**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 5: Correlation of BNC counts with web counts
(seen bigrams)
heuristics (see (1)?(3)) are sufficient to obtain use-
ful frequencies from the web. It seems that the large
amount of data available for web counts outweighs
the associated problems (noisy, unbalanced, etc.).
Note that the highest coefficients were obtained
for adjective-noun bigrams, which probably indi-
cates that this type of predicate-argument relation-
ship is least subject to syntactic variation and thus
least affected by the simplifications of our search
heuristics.
3.2 Task-based Evaluation
Previous work has demonstrated that corpus counts
correlate with human plausibility judgments for
adjective-noun bigrams. This results holds for both
seen bigrams (Lapata et al, 1999) and for unseen
bigrams whose counts were recreated using smooth-
ing techniques (Lapata et al, 2001). Based on these
findings, we decided to evaluate our web counts on
the task of predicting plausibility ratings. If the web
counts for bigrams correlate with plausibility judg-
ments, then this indicates that the counts are valid,
in the sense of being useful for predicting intuitive
plausibility.
Lapata et al (1999) and Lapata et al (2001) col-
lected plausibility ratings for 90 seen and 90 unseen
adjective-noun bigrams (see Section 2.1) using mag-
nitude estimation. Magnitude estimation is an exper-
imental technique standardly used in psychophysics
to measure judgments of sensory stimuli (Stevens,
1975), which Bard et al (1996) and Cowart (1997)
have applied to the elicitation of linguistic judg-
ments. Magnitude estimation requires subjects to
assign numbers to a series of linguistic stimuli in
a proportional fashion. Subjects are first exposed
to a modulus item, which they assign an arbitrary
number. All other stimuli are rated proportional
to the modulus. In the experiments conducted by
Lapata et al (1999) and Lapata et al (2001), native
speakers of English were presented with adjective-
noun bigrams and were asked to rate the degree
of adjective-noun fit proportional to the modulus
item. The resulting judgments were normalized by
dividing them by the modulus value and by log-
transforming them. Lapata et al (1999) report a cor-
relation of .570 between mean plausibility judg-
ments and BNC counts for the seen adjective-
noun bigrams. For unseen adjective-noun bigrams,
Lapata et al (2001) found a correlation of .356 be-
tween mean judgments and frequencies recreated
using class-based smoothing (Resnik, 1993).
In the present study, we used the plausibil-
ity judgments collected by Lapata et al (1999) and
Lapata et al (2001) for adjective-noun bigrams and
conducted additional experiments to obtain noun-
noun and verb-object judgments for the materi-
als described in Section 2.1. We used the same
experimental procedure as the original study (see
Lapata et al (1999) and Lapata et al (2001) for de-
tails). Four experiments were carried out, one each
for seen and unseen noun-noun bigrams, and for
seen and unseen verb-object bigrams. Unlike the
adjective-noun and the noun-noun bigrams, the
verb-object bigrams were not presented to subjects
in isolation, but embedded in a minimal sentence
context involving a proper name as the subject
(e.g., Paul fulfilled the obligation).
The experiments were conducted over the web
using the WebExp software package (Keller et al,
1998). A series of previous studies has shown that
data obtained using WebExp closely replicates re-
sults obtained in a controlled laboratory setting;
this was demonstrated for acceptability judgments
(Keller and Alexopoulou, 2001), co-reference judg-
ments (Keller and Asudeh, 2001), and sentence
completions (Corley and Scheepers, 2002). These
references also provide a detailed discussion of the
WebExp experimental setup.
Table 6 lists the descriptive statistics for all
six judgment experiments: the original experiments
by Lapata et al (1999) and Lapata et al (2001) for
adjective-noun bigrams, and our new ones for noun-
noun and verb-object bigrams.
We used correlation analysis to compare web
counts with plausibility judgments for seen
adjective-noun, noun-noun, and verb-object bi-
grams. Table 7 (top half) lists the correlation
coefficients that were obtained when correlat-
adj-noun bigrams noun-noun bigrams verb-object bigrams
N Min Max Mean SD N Min Max Mean SD N Min Max Mean SD
Seen 30 ?.85 .11 ?.13 .22 25 ?.15 .69 .40 .21 27 ?.52 .45 .12 .24
Unseen 41 ?.56 .37 ?.07 .20 25 ?.49 .52 ?.01 .23 21 ?.51 .28 ?.16 .22
Table 6: Descriptive statistics for plausibility judgments (log-transformed); N is the number of subjects used
in each experiment
ing log-transformed web and BNC counts with
log-transformed plausibility judgments.
The results show that both Altavista and Google
counts correlate with plausibility judgments for seen
bigrams. Google slightly outperforms Altavista: the
correlation coefficient for Google ranges from .624
to .693, while for Altavista, it ranges from .638 to
.685. A surprising result is that the web counts con-
sistently achieve a higher correlation with the judg-
ments than the BNC counts, which range from .488
to .569. We carried out a series of one-tailed t-tests
to determine if the differences between the correla-
tion coefficients for the web counts and the corre-
lation coefficients for the BNC counts were signifi-
cant. For the adjective-noun bigrams, the difference
between the BNC coefficient and the Altavista coef-
ficient failed to reach significance (t(87) = 1.46, p >
.05), while the Google coefficient was significantly
higher than the BNC coefficient (t(87) = 1.78, p <
.05). For the noun-noun bigrams, both the Altavista
and the Google coefficients were significantly higher
than the BNC coefficient (t(87) = 2.94, p < .01 and
t(87) = 3.06, p < .01). Also for the verb-object bi-
grams, both the Altavista coefficient and the Google
coefficient were significantly higher than the BNC
coefficient (t(87) = 2.21, p < .05 and t(87) = 2.25,
p < .05). In sum, for all three types of bigrams, the
correlation coefficients achieved with Google were
significantly higher than the ones achieved with the
BNC. For Altavista, the noun-noun and the verb-
object coefficients were higher than the coefficients
obtained from the BNC.
Table 7 (bottom half) lists the correlations co-
efficients obtained by comparing log-transformed
judgments with log-transformed web counts for un-
seen adjective-noun, noun-noun, and verb-object bi-
grams. We observe that the web counts consistently
show a significant correlation with the judgments,
the coefficient ranging from .466 to .588 for Al-
seen bigrams
adj-noun noun-noun verb-object
Altavista .642** .685** .638**
Google .650** .693** .624**
BNC .569** .517** .488**
unseen bigrams
Altavista .466** .588** .568**
Google .446** .611** .542**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 7: Correlation of plausibility judgments with
web counts and BNC counts
tavista counts, and from .446 to .611 for the Google
counts. Note that a small number of bigrams pro-
duced zero counts even in our web queries; these fre-
quencies were set to one for the correlation analysis
(see Section 2.2).
To conclude, this evaluation demonstrated that
web counts reliably predict human plausibility judg-
ments, both for seen and for unseen predicate-
argument bigrams. In the case of Google counts
for seen bigrams, we were also able to show that
web counts are a better predictor of human judg-
ments than BNC counts. These results show that our
heuristic method yields useful frequencies; the sim-
plifications we made in obtaining the counts, as well
as the fact that web data are noisy, seem to be out-
weighed by the fact that the web is up to three orders
of magnitude larger than the BNC (see our estimate
in Section 2.2).
4 Conclusions
This paper explored a novel approach to overcoming
data sparseness. If a bigram is unseen in a given cor-
pus, conventional approaches recreate its frequency
using techniques such as back-off, linear interpo-
lation, class-based smoothing or distance-weighted
averaging (see Dagan et al (1999) and Lee (1999)
for overviews). The approach proposed here does
not recreate the missing counts, but instead re-
trieves them from a corpus that is much larger (but
also much more noisy) than any existing corpus: it
launches queries to a search engine in order to deter-
mine how often a bigram occurs on the web.
We systematically investigated the validity of
this approach by using it to obtain frequencies for
predicate-argument bigrams (adjective-noun, noun-
noun, and verb-object bigrams). We first applied
the approach to seen bigrams randomly sampled
from the BNC. We found that the counts obtained
from the web are highly correlated with the counts
obtained from the BNC, which indicates that web
queries can generate frequencies that are compara-
ble to the ones obtained from a balanced, carefully
edited corpus such as the BNC.
Secondly, we performed a tasked-based evalua-
tion that used the web frequencies to predict hu-
man plausibility judgments for predicate-argument
bigrams. The results show that web counts corre-
late reliably with judgments, for all three types of
predicate-argument bigrams tested, both seen and
unseen. For the seen bigrams, we showed that the
web frequencies correlate better with judged plausi-
bility than the BNC frequencies.
To summarize, we have proposed a simple heuris-
tic for obtaining bigram counts from the web. Using
two different types of evaluation, we demonstrated
that this simple heuristic is sufficient to obtain useful
frequency estimates. It seems that the large amount
of data available outweighs the problems associated
with using the web as a corpus (such as the fact that
it is noisy and unbalanced).
In future work, we plan to compare web counts
for unseen bigrams with counts recreated using
standard smoothing algorithms, such as similarity-
based smoothing (Dagan et al, 1999) or class-based
smoothing (Resnik, 1993). If web counts correlate
reliable with smoothed counts, then this provides
further evidence for our claim that the web can be
used to overcome data sparseness.
References
Steve Abney. 1996. Partial parsing via finite-state cas-
cades. In John Carroll, editor, Workshop on Robust
Parsing, pages 8?15, 8th European Summer School in
Logic, Language and Information, Prague.
Eneko Agirre and David Martinez. 2000. Exploring
automatic word sense disambiguation with decision
lists and the web. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics,
Saarbru?cken/Luxembourg/Nancy.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In James Allan, editor, Proceed-
ings of the 1st International Conference on Human
Language Technology Research, San Francisco. Mor-
gan Kaufmann.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics and the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, Toulouse.
Ellen Gurman Bard, Dan Robertson, and Antonella So-
race. 1996. Magnitude estimation of linguistic ac-
ceptability. Language, 72(1):32?68.
Lou Burnard, 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service.
Martin Corley and Christoph Scheepers. 2002. Syntac-
tic priming in English sentence production: Categori-
cal and latency evidence from an internet-based study.
Psychonomic Bulletin and Review, 9(1).
Steffan Corley, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntac-
tic structure in unparsed corpora: The Gsearch cor-
pus query system. Computers and the Humanities,
35(2):81?94.
Wayne Cowart. 1997. Experimental Syntax: Applying
Objective Methods to Sentence Judgments. Sage Pub-
lications, Thousand Oaks, CA.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence prob-
abilities. Machine Learning, 34(1):43?69.
Gregory Grefenstette and Jean Nioche. 2000. Estima-
tion of English and non-English language use on the
WWW. In Proceedings of the RIAO Conference on
Content-Based Multimedia Information Access, pages
237?246, Paris.
Gregory Grefenstette. 1998. The World Wide Web as a
resource for example-based machine translation tasks.
In Proceedings of the ASLIB Conference on Translat-
ing and the Computer, London.
Rosie Jones and Rayid Ghani. 2000. Automatically
building a corpus for a minority language from the
web. In Proceedings of the Student Research Work-
shop at the 38th Annual Meeting of the Association for
Computational Linguistics, pages 29?36, Hong Kong.
Frank Keller and Theodora Alexopoulou. 2001. Phonol-
ogy competes with syntax: Experimental evidence for
the interaction of word order and accent placement in
the realization of information structure. Cognition,
79(3):301?372.
Frank Keller and Ash Asudeh. 2001. Constraints on lin-
guistic coreference: Structural vs. pragmatic factors.
In Johanna D. Moore and Keith Stenning, editors, Pro-
ceedings of the 23rd Annual Conference of the Cog-
nitive Science Society, pages 483?488, Mahwah, NJ.
Lawrence Erlbaum Associates.
Frank Keller, Martin Corley, Steffan Corley, Lars
Konieczny, and Amalia Todirascu. 1998. WebExp:
A Java toolbox for web-based psychological experi-
ments. Technical Report HCRC/TR-99, Human Com-
munication Research Centre, University of Edinburgh.
Maria Lapata, Scott McDonald, and Frank Keller. 1999.
Determinants of adjective-noun plausibility. In Pro-
ceedings of the 9th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 30?36, Bergen.
Maria Lapata, Frank Keller, and Scott McDonald. 2001.
Evaluating smoothing algorithms against plausibility
judgments. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
and the 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
346?353, Toulouse.
Maria Lapata. 2001. A corpus-based account of regular
polysemy: The case of context-sensitive adjectives. In
Proceedings of the 2nd Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Pittsburgh, PA.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University, Sydney.
Lilian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 25?32,
University of Maryland, College Park.
Rada Mihalcea and Dan Moldovan. 1999. A method
for word sense disambiguation of unrestricted text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 152?158,
University of Maryland, College Park.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
Philip Stuart Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Philadelphia,
PA.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, University of
Maryland, College Park.
S. S. Stevens. 1975. Psychophysics: Introduction to its
Perceptual, Neural, and Social Prospects. John Wiley,
New York.
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Paul Rayson,
Andrew Wilson, Tony McEnery, Andrew Hardie, and
Shereen Khoja, editors, Proceedings of the Corpus
Linguistics Conference, pages 601?606, Lancaster.
Robust Models of Human Parsing
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
1 Robustness and Human Parsing
A striking property of the human parser is its effi-
ciency and robustness. For the vast majority of sen-
tences, the parser will effortlessly and rapidly de-
liver the correct analysis. In doing so, it is robust to
noise, i.e., it can provide an analysis even if the input
is distorted, e.g., by ungrammaticalities. Further-
more, the human parser achieves broad coverage:
it deals with a wide variety of syntactic construc-
tions, and is not restricted by the domain, genre, or
modality of the input.
Current research on human parsing rarely investi-
gates the issues of efficiency, robustness, and broad
coverage, as pointed out by Crocker and Brants
(2000). Instead, most researchers have focussed on
the difficulties that the human parser has with cer-
tain types of sentences. Based on the study of gar-
den path sentences (which involve a local ambiguity
that makes the sentence hard to process), theories
have been developed that successfully explain how
the human parser deals with ambiguities in the in-
put. However, garden path sentences are arguably
a pathological case for the parser; garden paths are
not representative of naturally occurring text. This
means that the corresponding processing theories
face a scaling problem: it is not clear how they can
explain the normal behavior of the human parser,
where sentence processing is highly efficient and
very robust (see Crocker and Brants 2000 for details
on this scalability argument).
This criticism applies to most existing theories
of human parsing, including the classical garden
path model advanced by Frazier and Rayner (1982)
and Frazier (1989), and more recent lexicalist pars-
ing frameworks, of which MacDonald et al (1994)
and MacDonald (1994) are representative examples.
Both the garden path model and the lexicalist model
are designed to deal with idealized input, i.e., with
input that is (locally) ambiguous, but fully well-
formed. A real life parser, however, has to cope
with a large amount of noise, which often renders
the input ungrammatical or fragmentary, due to er-
rors such as typographical mistakes in the case of
text, or slips of the tongue, disfluencies, or repairs
in the case of speech. A quick search in the Penn
Treebank (Marcus et al, 1993) shows that about
17% of all sentences contain parentheticals or other
sentence fragments, interjections, or unbracketable
constituents. Note that this figure holds for carefully
edited newspaper text; the figure is likely to be much
higher for speech. The human parser is robust to
such noise, i.e., it is able to assign an (approximate)
analysis to a sentence even if it is ungrammatical or
fragmentary.
2 Probabilistic Parsing Models
In computational linguistics, probabilistic ap-
proaches to language processing play a central
role. Significant advances toward robust, broad-
coverage parsing models have been made based on
probabilistic techniques such as maximum likeli-
hood estimation or expectation maximization (for
an overview, see Manning and Schu?tze, 1999).
An example of a simple probabilistic pars-
ing model are probabilistic context-free grammars
(PCFGs), which extend the formalism of context-
free grammars (CFGs) by annotating each rule with
a probability. PCFGs constitute an efficient, well-
understood technique for assigning probabilities to
the analyses produced by a context-free grammar.
They are commonly used for broad-coverage gram-
mars, as CFGs large enough to parse unrestricted
text are typically highly ambiguous, i.e., a single
sentence will receive a large number of parses. The
probabilistic component of the grammar can then be
used to rank the analyses a sentence might receive,
and improbable ones can be eliminated.
In the computational linguistics literature, a num-
ber of highly successful extensions to the basic
PCFG model have been proposed. Of particular in-
terest are lexicalized parsing models such as the
ones developed by Collins (1996, 1997) and Carroll
and Rooth (1998).
In the human parsing literature, a PCFG-based
model has been proposed by Jurafsky (1996) and
Narayanan and Jurafsky (1998). This model shows
how different sources of probabilistic information
(such as subcategorization information and rule fre-
quencies) can be combined using Bayesian infer-
ence. The model accounts for a range of disam-
biguation phenomena in linguistic processing. How-
ever, the model is only small scale, and it is not clear
if it can be extended to provide robustness and cov-
erage of unrestricted text.
This problem is addressed by Brants and Crocker
(2000) and Crocker and Brants (2000), who pro-
pose a broad-coverage model of human parsing
based on PCFGs. This model is incremental, i.e.,
it makes word-by-word predictions, thus mimick-
ing the behavior of the human parser. Also, Brants
and Crocker?s (2000) model imposes memory re-
strictions on the parser that are inspired by findings
from the human sentence processing literature.
3 Robust Models of Human Parsing
The main weakness of both the Narayanan/Jurafsky
and the Crocker/Brants model (discussed in the pre-
vious section) is that they have not been evaluated
systematically. The authors only describe the per-
formance of their models on a small set of hand-
picked example sentences. No attempts are made
to test the models against a full set of experimental
materials and the corresponding reading times, even
though a large amount of suitable data are available
in the literature. This makes it very hard to obtain a
realistic estimate of how well these models achieve
the aim of providing robust, broad coverage mod-
els of human parsing. This can only be assessed by
testing the models against realistic samples of unre-
stricted text or speech obtained from corpora.
In this talk, we will present work that aims
to perform such an evaluation. We train a se-
ries of increasingly sophisticated probabilistic pars-
ing models on an identical training set (the Penn
Treebank). These models include a standard un-
lexicalized PCFG parser, a head-lexicalized parser
(Collins, 1997), and a maximum-entropy inspired
parser (Charniak, 2000). We test all three models
on the Embra corpus, a corpus of newspaper texts
annotated with eye-tracking data from 23 subjects
(McDonald and Shillcock, 2003). A series of re-
gression analyses are conducted to determine if per-
sentence reading time measures correlate with sen-
tence probabilities predicted by the parsing models.
Three baseline models are also included in the eval-
uation: word frequency, bigram and trigram prob-
ability (as predicted by a language model), and
part of speech (POS) probability (as predicted by
a POS tagger). Models based on n-grams have al-
ready been used successfully to model eye-tracking
data, both on a word-by-word basis (McDonald and
Shillcock, 2003) and for whole sentences (Keller,
2004).
Our results show that for all three parsing models,
sentence probability is significantly correlated with
reading times measures. However, the models differ
as to whether they predict early or late measures:
the PCFG and the Collins model significantly pre-
dict late reading time measures (total time and gaze
duration), but not early measures (first fixation time
and skipping rate). The Charniak model is able to
significantly predict both early and late measures.
An analysis of the baseline models shows that
word frequency and POS probability only predict
early measures, while bigram and trigram probabil-
ity only predict late measures. This indicates that
the Charniak model is able to predict both early and
late measures because it successfully combines lex-
ical information (word frequencies and POS proba-
bilities) with phrasal information (as modeled by a
PCFG). This finding is in line with Charniak?s own
analysis, which shows that the high performance of
his model is due to the fact that it combines a third-
order Markov grammar with sophisticated phrasal
and lexical features (Charniak, 2000).
4 Implications
The results reported in the previous section have in-
teresting theoretical implications. Firstly, there is a
methodological lesson here: simple baseline mod-
els based on n-gram or POS probabilities perform
surprisingly well as robust, broad coverage models
of human language processing. This is an important
point that has not been recognized in the literature,
as previous models have not been tested on realis-
tic corpus samples, and have not been compared to
plausible baselines.
A second point concerns the role of lexical in-
formation in human parsing. We found that the
best performing model was Charniak?s maximum
entropy-inspired parser, which combines lexical and
phrasal information, and manages to predict both
early and late eye-tracking measures. A number of
existing theories of human parsing incorporate lexi-
cal information (MacDonald et al, 1994; MacDon-
ald, 1994), but have so far failed to demonstrate
how the use of such information can be scaled up
to yield robust, broad coverage parsing models that
can be tested on realistic data such as the Embra
eye-tracking corpus.
Finally, a major challenge that remains is the
crosslinguistic aspect of human parsing. Virtually
all existing computational models have only been
implemented and tested for English data. However,
a wide range of interesting problems arise for other
languages. An examples are head-final languages, in
which the probabilistic information associated with
the head becomes available only at the end of the
phrase, which poses a potential problem for incre-
mental parsing models. Some initial results on a
limited dataset have been obtained by Baldewein
and Keller (2004) for head-final constructions in
German.
References
Baldewein, Ulrike and Frank Keller. 2004. Mod-
eling attachment decisions with a probabilistic
parser: The case of head final structures. In Pro-
ceedings of the 26th Annual Conference of the
Cognitive Science Society. Chicago.
Brants, Thorsten and Matthew W. Crocker. 2000.
Probabilistic parsing and psychological plausi-
bility. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics.
Saarbru?cken/Luxembourg/Nancy.
Carroll, Glenn and Mats Rooth. 1998. Valence in-
duction with a head-lexicalized PCFG. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Granada,
pages 36?45.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics. Seattle,
WA, pages 132?139.
Collins, Michael. 1996. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the 34th Annual Meeting of the As-
sociation for Computational Linguistics. Santa
Cruz, CA, pages 184?191.
Collins, Michael. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
Association for Computational Linguistics and
the 8th Conference of the European Chapter of
the Association for Computational Linguistics.
Madrid, pages 16?23.
Crocker, Matthew W. and Thorsten Brants. 2000.
Wide-coverage probabilistic sentence processing.
Journal of Psycholinguistic Research 29(6):647?
669.
Frazier, Lynn. 1989. Against lexical generation of
syntax. In William D. Marslen-Wilson, editor,
Lexical Representation and Process, MIT Press,
Cambridge, Mass., pages 505?528.
Frazier, Lynn and Keith Rayner. 1982. Making
and correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psychol-
ogy 14:178?210.
Jurafsky, Daniel. 1996. A probabilistic model of
lexical and syntactic access and disambiguation.
Cognitive Science 20(2):137?194.
Keller, Frank. 2004. The entropy rate principle as
a predictor of processing effort: An evaluation
against eye-tracking data. In Dekang Lin and
Dekai Wu, editors, Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. Barcelona.
MacDonald, Maryellen C. 1994. Probabilistic con-
straints and syntactic ambiguity resolution. Lan-
guage and Cognitive Processes 9:157?201.
MacDonald, Maryellen C., Neal J. Pearlmutter, and
Mark S. Seidenberg. 1994. Lexical nature of syn-
tactic ambiguity resolution. Psychological Re-
view 101:676?703.
Manning, Christopher D. and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press, Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn Treebank.
Computational Linguistics 19(2):313?330.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in reading:
The influence of transitional probabilities on eye
movements. Vision Research 43:1735?1751.
Narayanan, Srini and Daniel Jurafsky. 1998.
Bayesian models of human sentence processing.
In Morton A. Gernsbacher and Sharon J. Derry,
editors, Proceedings of the 20th Annual Confer-
ence of the Cognitive Science Society. Lawrence
Erlbaum Associates, Mahwah, NJ.
The Entropy Rate Principle as a Predictor of Processing Effort: An
Evaluation against Eye-tracking Data
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
This paper provides evidence for Genzel and Char-
niak?s (2002) entropy rate principle, which predicts
that the entropy of a sentence increases with its po-
sition in the text. We show that this principle holds
for individual sentences (not just for averages), but
we also find that the entropy rate effect is partly
an artifact of sentence length, which also correlates
with sentence position. Secondly, we evaluate a set
of predictions that the entropy rate principle makes
for human language processing; using a corpus of
eye-tracking data, we show that entropy and pro-
cessing effort are correlated, and that processing ef-
fort is constant throughout a text.
1 Introduction
Genzel and Charniak (2002, 2003) introduce the en-
tropy rate principle, which states that speakers pro-
duce language whose entropy rate is on average
constant. The motivation for this comes from in-
formation theory: the most efficient way of trans-
mitting information through a noisy channel is at a
constant rate. If human communication has evolved
to be optimal in this sense, then we would expect
humans to produce text and speech with approx-
imately constant entropy. There is some evidence
that this is true for speech (Aylett, 1999).
For text, the entropy rate principle predicts that
the entropy of an individual sentence increases with
its position in the text, if entropy is measured out of
context. Genzel and Charniak (2002) show that this
prediction is true for the Wall Street Journal corpus,
for both function words and for content words. They
estimate entropy either using a language model or
using a probabilistic parser; the effect can be ob-
served in both cases. Genzel and Charniak (2003)
extend this results in several ways: they show that
the effect holds for different genres (but the effect
size varies across genres), and also applies within
paragraphs, not only within whole texts. Further-
more, they show that the effect can also be ob-
tained for language other than English (Russian and
Spanish). The entropy rate principle also predicts
that a language model that takes context into ac-
count should yield lower entropy estimates com-
pared to an out of context language model. Genzel
and Charniak (2002) show that this prediction holds
for caching language models such as the ones pro-
posed by Kuhn and de Mori (1990).
The aim of the present paper is to shed further
light on the entropy rate effect discovered by Gen-
zel and Charniak (2002, 2003) (henceforth G&C)
by providing new evidence in two areas.
In Experiment 1, we replicate G&C?s entropy
rate effect and investigate the source of the effect.
The results show that the correlation coefficients
that G&C report are inflated by averaging over sen-
tences with the same position, and by restricting
the range of the sentence position considered. Once
these restrictions are removed the effect is smaller,
but still significant. We also show that the effect is
to a large extend due to a confound with sentence
length: longer sentences tend to occur later in the
text. However, we are able to demonstrate that the
entropy rate effect still holds once this confound has
been removed.
In Experiment 2, we test the psycholinguistic pre-
dictions of the entropy rate principle. This experi-
ment uses a subset of the British National Corpus
as training data and tests on the Embra corpus, a set
of newspaper articles annotated with eye-movement
data. We find that there is a correlation between
the entropy of a sentence and the processing ef-
fort it causes, as measured by reading times in eye-
tracking data. We also show that there is no corre-
lation between processing effort and sentence posi-
tion, which indicates that processing effort in con-
text is constant through a text, which is one of the
assumptions underlying the entropy rate principle.
2 Predictions for Human Language
Processing
Let us examine the psycholinguistic predictions of
G&C?s entropy rate principle in more detail. We
need to distinguish two types of predictions: in-
context predictions and out-of-context predictions.
The principle states that the entropy rate in a text
is constant, i.e., that speakers produce sentences so
that on average, all sentences in a text have the same
entropy. In other words, communication is optimal
in the sense that all sentences in the text are equally
easy to understand, as they all have the same en-
tropy.
This constancy principle is claimed to hold for
connected text: all sentences in a text should be
equally easy to process if they are presented in con-
text. If we take reading time as a measure of pro-
cessing effort, then the principle predicts that there
should be no significant correlation between sen-
tence position and reading time in context. We will
test this prediction in Experiment 2 using an eye-
tracking corpus consisting of connected text.
The entropy rate principle also makes the follow-
ing prediction: if the entropy of a sentence is mea-
sured out of context (i.e., without taking the preced-
ing sentences into account), then entropy will in-
crease with sentence position. This prediction was
tested extensively by G&C, whose results will be
replicated in Experiment 1. With respect to process-
ing difficulty, the entropy rate principle also predicts
that processing difficulty out of context (i.e., if iso-
lated sentences are presented to experimental sub-
jects) should increase with sentence position. We
could not test this prediction, as we only had in-
context reading time data available for the present
study.
However, there is another important prediction
that can be derived from the entropy rate principle:
sentences with a higher entropy should have higher
reading times. This is an important precondition for
the entropy rate principle, whose claims about the
relationship between entropy and sentence position
are only meaningful if entropy and processing effort
are correlated. If there was no such correlation, then
there would be no reason to assume that the out-
of-context entropy of a sentence increases with sen-
tence position. G&C explicitly refer to this relation-
ship i.e., they assume that a sentence that is more
informative is harder to process (Genzel and Char-
niak, 2003, p. 65). Experiment 1 will try to demon-
strate the validity of this important prerequisite of
the entropy rate principle.
3 Experiment 1: Entropy Rate and
Sentence Length
The main aim of this experiment was to replicate
G&C?s entropy rate effect. A second aim was to
test the generality of their result by determining if
the relationship between sentence position and en-
tropy also holds for individual sentences (rather than
for averages over sentences of a given position, as
tested by G&C). We also investigated the effect of
two parameters that G&C did not explore: the cut-
off for article position (G&C only deal with sen-
tences up to position 25), and the size of the n-gram
used for estimating sentence probability. Finally, we
include sentence length as a baseline that entropy-
based models should be evaluated against.
3.1 Method
3.1.1 Materials
This experiment used the same corpus as Genzel
and Charniak (2002), viz., the Wall Street Journal
part of the Penn Treebank, divided into a training
set (section 0?20) and a test set (sections 21?24).
Each article was treated as a separate text, and sen-
tence positions were computed by counting the sen-
tences from the beginning of the text. The training
set contained 42,075 sentences, the test set 7,133
sentences. The sentence positions in the test set var-
ied between one and 149.
3.1.2 Procedure
The per-word entropy was computed using an n-
gram language model, as proposed by G&C:1
?H(X) = ? 1|X | ?xi?X logP(xi|xi?(n?1) . . .xi?1)(1)
Here, ?H(X) is the estimate of the per-word en-
tropy of the sentence X , consisting of the words xi,
and n is the size of the n-gram. The n-gram proba-
bilities were computed using the CMU-Cambridge
language modeling toolkit (Clarkson and Rosen-
feld, 1997), with the following parameters: vocab-
ulary size 50,000; smoothing by absolute discount-
ing; sentence beginning and sentence end as context
cues (default values were used for all other parame-
ters).
G&C use n = 3, i.e., a trigram model. We
experimented with this parameter and used n =
1, . . . ,5. For n = 1, equation (1) reduces to ?H(X) =
? 1|X | ?xi?X logP(xi), i.e., a model based on word
frequency.
The experiment also includes a simple model that
does not take any probabilistic information into ac-
count, but simply uses the sentence length |X | to
predict sentence position. This model will serve as
the baseline.
1Note that the original definition given by Genzel and Char-
niak (2002, 2003) does not include the minus sign. However,
all their graphs display entropy as a positive quantity, hence we
conclude that this is the definition they are using.
0 20 40 60 80
Sentence position
7
7.5
8
8.5
9
9.5
En
tro
py
 [b
its
]
Figure 1: Experiment 1: correlation of sentence en-
tropy and sentence position (bins, 3-grams, cut-
off 76)
We also vary another parameter: c, the cut-off
for the position. Genzel and Charniak (2002) use
c = 25, i.e., only sentences with a position of 25
or lower are considered. In Genzel and Charniak
(2003), an even smaller cut-off of c = 10 is used.
This severely restricts the generality of the results
obtained. We will therefore report results not only
for c = 25, but also for c = 76. This cut-off has been
set so that there are at least 10 items in the test set
for each position. Furthermore, we also repeated the
experiment without a cut-off for sentence length.
3.2 Results
Table 1 shows the results for the replication of Gen-
zel and Charniak?s (2002) entropy rate effect. The
results at the top of the table were obtained using
binning, i.e., we computed the mean entropy of all
sentences of a given position, and then correlated
these mean entropies with the sentence positions.
The parameters n (n-gram size) and c (cut-off value)
were varied as indicated in the previous section.
The bottom of Table 1 gives the correlation co-
efficients computed on the raw data, i.e., without
binning: here, we correlated the entropy of a given
sentence directly with its position. The graphs in
Figure 1 and Figure 2 illustrate the relationship be-
tween position and entropy and between position
and length, respectively.
3.3 Discussion
3.3.1 Entropy Rate and Sentence Length
The results displayed in Table 1 confirm G&C?s
main finding, i.e., that entropy increases with sen-
tence length. For a cut-off of c = 25 (as used by
G&C), a maximum correlation of 0.6480 is ob-
tained (for the 4-gram model). The correlations for
the other n-gram models are lower. All correlations
0 20 40 60 80
Sentence position
15
20
25
Se
nt
en
ce
 le
ng
th
Figure 2: Experiment 1: correlation of sentence
length and sentence position (bins, cut-off 76)
are significant (with the exception of the unigram
model). However, we also find that a substantial cor-
relation of ?0.4607 is obtained even for the base-
line model: there is a negative correlation between
sentence length and sentence position, i.e., longer
sentences tend to occur earlier in the text. This find-
ing potentially undermines the entropy rate effect,
as it raises the possibility that this effect is simply
an effect of sentence length, rather than of sentence
entropy. Note that the correlation coefficient for the
none of the n-gram models is significantly higher
than the baseline (significance was computed on the
absolute values of the correlation coefficients).
The second finding concerns the question
whether the entropy rate effect generalizes to sen-
tences with a position of greater than 25. The re-
sults in Table 1 show that the effect generalizes to
a cut-off of c = 76 (recall that this value was cho-
sen so that each position is represented at least ten
times in the test data). Again, we find a significant
correlation between entropy and sentence position
for all values of n. This is illustrated in Figure 1.
However, none of the n-gram models is able to beat
the baseline of simple sentence position; in fact,
now all models (with the exception of the unigram
model) perform significantly worse than the base-
line. The correlation obtained by the baseline model
is graphed in Figure 2.
Finally, we tried to generalize the entropy rate ef-
fect to sentences with arbitrary position (no cut-off).
Here, we find that there is no significant positive
correlation between entropy and position for any of
the n-gram models. Only sentence length yields a
reliable correlation, though it is smaller than if a
cut-off is applied. This result is perhaps not surpris-
ing, as a lot of the data is very sparse: for positions
between 77 and 149, less than ten data points are
c = 25 c = 76 c = ?
Binned data r p r p r p
Entropy 1-gram 0.0593? 0.7784 0.3583 0.0015 ?0.3486? 0.0000
Entropy 2-gram 0.4916 0.0126 0.2849? 0.0126 0.0723 0.3808
Entropy 3-gram 0.6387 0.0006 0.2427? 0.0346 0.1350 0.1006
Entropy 4-gram 0.6480 0.0005 0.2378? 0.0386 0.1354 0.0996
Entropy 5-gram 0.6326 0.0007 0.2281? 0.0475 0.1311 0.1111
Sentence length ?0.4607 0.0205 ?0.4943 0.0000 ?0.1676 0.0410
c = 25 c = 76 c = ?
Raw data r p r p r p
Entropy 1-gram ?0.0023? 0.8781 0.0598? 0.0000 0.0301? 0.0110
Entropy 2-gram 0.0414 0.0056 0.0755? 0.0000 0.0615? 0.0000
Entropy 3-gram 0.0598 0.0001 0.0814? 0.0000 0.0706? 0.0000
Entropy 4-gram 0.0625 0.0000 0.0830? 0.0000 0.0712? 0.0000
Entropy 5-gram 0.0600 0.0001 0.0812? 0.0000 0.0695? 0.0000
Sentence length ?0.0635 0.0000 ?0.1099 0.0000 ?0.1038 0.0000
Table 1: Results of Experiment 1: correlation of sentence entropy and sentence position, on binned data; c:
cut-off, r: correlation coefficient, p: significance level, ?: correlation significantly different from baseline
(sentence length)
available per position. Based on data this sparse, no
reliable correlation coefficients can be expected.
Let us now turn to Table 1, which displays the
results that were obtained by computing correlation
coefficients on the raw data, i.e., without computing
the mean entropy for all sentences with the same po-
sition. We find that for all parameter settings a sig-
nificant correlation between sentence entropy and
sentence position is obtained (with the exception of
n = 1, c = 25). The correlation coefficients are sig-
nificantly lower than the ones obtained using bin-
ning, the highest coefficient is 0.0830. This means
that a small but reliable entropy effect can be ob-
served even on the raw data, i.e., for individual sen-
tences rather than for bins of sentences with the
same position.
However, the results in Table 1 also confirm our
findings regarding the baseline model (simple sen-
tence length): in all cases the correlation coefficient
achieved for the baseline is higher than the one
achieved by the entropy models, in some cases even
significantly so.
3.3.2 Disconfounding Entropy and Sentence
Length
Taken together, the results in Table 1 seem to indi-
cate that the entropy rate effect reported by G&C is
not really an effect of entropy, but just an effect of
sentence length. The effect seems to be due to the
fact that G&C compute entropy rate by dividing the
entropy of a sentence by its length: sentence length
is correlated with sentence position, hence entropy
rate will be correlated with position as well.
It is therefore necessary to conduct additional
analyses that remove the confound of sentence
length. This can be achieved by computing partial
correlations; the partial correlation coefficient be-
tween a factor 1 and a factor 2 expresses the degree
of association between the factors that is left once
the influence of a third factor has been removed
from both factors. For example, we can compute the
correlation of position and entropy, with sentence
length partialled out. This will tell us use the amount
of association between position and entropy that is
left once the influence of length has been removed
from both position and entropy.
Table 2 shows the results of partial correlation
analyses for length and entropy. Note that these
results were obtained using total entropy, not per-
word entropy, i.e., the normalizing term 1|X | was
dropped from (1). The partial correlations are only
reported for the trigram model.
The results indicate that entropy is a signifi-
cant predictor sentence position, even once sentence
length has been partialled out. This result holds for
both the binned data and the raw data, and for all
cut-offs (with the exception of c = 76 for the binned
data). Note however, that entropy is always a worse
predictor than sentence length; the absolute value of
the correlation coefficient is always lower. This in-
dicates that the entropy rate effect is a much weaker
effect than the results presented by G&C suggest.
4 Entropy Rate Effect and Processing
Effort
The previous experiment confirmed the validity of
the entropy rate effect: it demonstrated a signifi-
c = 25 c = 76 c = ?
Binned data r p r p r p
Entropy 3-gram 0.6708 0.0000 0.1473 0.2067 0.1703 0.0383
Sentence length ?0.7435 0.0000 ?0.3020 0.0084 ?0.2131 0.0093
Raw data r p r p r p
Entropy 3-gram 0.0784 0.0000 0.0929 0.0000 0.0810 0.0000
Sentence length ?0.0983 0.0000 ?0.1311 0.0000 ?0.1176 0.0000
Table 2: Results of Experiment 1: correlation of entropy and sentence length with sentence position, with
the other factor partialled out
cant correlation between sentence entropy and sen-
tence position, even when sentence length, which
was shown to be a confounding factor, was con-
trolled for. The effect, however, was smaller than
claimed by G&C, in particular when applied to in-
dividual sentences, as opposed to means obtained
for sentences at the same position.
In the present experiment, we will test a crucial
aspect of the entropy rate principle, viz., that en-
tropy should correlate with processing effort. We
will test this using a corpus of newspaper text that
is annotated with eye-tracking data. Eye-tracking
measures of reading time are generally thought to
reflect the amount of cognitive effort that is required
for the processing of a given word or sentence.
A second prediction of the entropy rate princi-
ple is that sentences with higher position should be
harder to process than sentences with lower posi-
tion. This relationship should hold out of context,
but not in context (see Section 2).
4.1 Method
4.1.1 Materials
As a test corpus, we used the Embra corpus (Mc-
Donald and Shillcock, 2003). This corpus consists
of 10 articles from Scottish and UK national broad-
sheet newspapers. The excerpts cover a wide range
of topics; they are slightly edited to make them com-
patible with eye-tracking.2 The length of the articles
varies between 97 and 405 words, the total size of
the corpus is 2,262 words (125 sentences). Twenty-
three native speakers of English read all 10 arti-
cles while their eye-movements were recorded us-
ing a Dual-Purkinke Image eye-tracker. To make
sure that subjects read the texts carefully, compre-
hension questions were also administered. For de-
tails on method used to create the Embra corpus,
see McDonald and Shillcock (2003).
The training and development sets for this exper-
iment were compiled so as to match the test corpus
in terms of genre. This was achieved by selecting
2This includes, e.g., the removal of quotation marks and
brackets, which can disrupt the eye-movement record.
all files from the British National Corpus (Burnard,
1995) that originate from UK national or regional
broadsheet newspapers. This subset of the BNC was
divided into a 90% training set and a 10% develop-
ment set. This resulted in a training set consisting of
6,729,104 words (30,284 sentences), and a develop-
ment set consisting of 746,717 words (34,269 sen-
tences). The development set will be used to test if
the entropy rate effect holds on this new corpus.
The sentence positions in the test set varied be-
tween one and 24, in the development, they varied
between one and 206.
4.1.2 Procedure
To compute per-word entropy, we trained n-
gram models on the training set using the CMU-
Cambridge language modeling toolkit, with the
same parameters as in Experiment 1. Again, n was
varied from 1 to 5. We determined the correlation
between per-word entropy and sentence position for
both the development set (derived from the BNC)
and for the test set (the Embra corpus).
Then, we investigated the predictions of G&C?s
entropy rate principle by correlating the position
and entropy of a sentence with its reading time in
the Embra corpus.
The reading measure used was total reading time,
i.e., the total time it takes a subject to read a sen-
tence; this includes second fixations and re-fixations
of words. We also experimented with other reading
measures such as gaze duration, first fixation time,
second fixation time, regression duration, and skip-
ping probability. However, the results obtained with
these measures were similar to the ones obtained
with total reading time, and will not be reported
here.
Total reading time is trivially correlated with sen-
tence length (longer sentences taker longer to read).
Hence we normalized total reading time by sen-
tence length, i.e., by multiplying with the factor 1|X | ,
also used in the computation of per-word entropy.
It is also well-known that reading time is corre-
lated with two other factors: word length and word
frequency; shorter and more frequent words take
c = 25 c = 76 c = ?
Binned data r p r p r p
Entropy 1-gram ?0.5495 0.0044 ?0.2510 0.0287 0.0232 0.7419
Entropy 2-gram 0.0602 0.7751 0.4249 0.0001 0.0392 0.5773
Entropy 3-gram 0.4523 0.0232 0.5395 0.0000 0.1238 0.0776
Entropy 4-gram 0.4828 0.0145 0.5676 0.0000 0.1229 0.0800
Entropy 5-gram 0.4834 0.0144 0.5723 0.0000 0.1223 0.0813
Sentence length ?0.8584 0.0000 ?0.2947 0.0098 ?0.2161 0.0019
Raw data r p r p r p
Entropy 1-gram ?0.0636 0.0000 ?0.0543 0.0000 0.0364 0.0000
Entropy 2-gram ?0.0069 0.2783 0.0435 0.0000 0.0477 0.0000
Entropy 3-gram 0.0162 0.0103 0.0659 0.0000 0.0687 0.0000
Entropy 4-gram 0.0193 0.0022 0.0691 0.0000 0.0711 0.0000
Entropy 5-gram 0.0192 0.0024 0.0685 0.0000 0.0707 0.0000
Sentence length ?0.0747 0.0000 ?0.1027 0.0000 ?0.0913 0.0000
Table 3: Results of Experiment 2: correlation of sentence entropy and sentence position on the BNC
less time to read (Just and Carpenter, 1980). We
removed these confounding factors by conducting
multiple regression analyses involving word length,
word frequency, and the predictor variable (entropy
or sentence position). The aim was to establish if
there is a significant effect of entropy or sentence
length, even when the other factors are controlled
for. Word frequency was estimated using the uni-
gram model trained on the training corpus.
In the eye-tracking literature, it is generally rec-
ommended to run regression analyses on the reading
times collected from individual subjects. In other
words, it is not good practice to compute regressions
on average reading times, as this fails take between-
subject variation in reading behavior into account,
and leads to inflated correlation coefficients. We
therefore followed the recommendations of Lorch
and Myers (1990) for computing regressions with-
out averaging over subjects (see also McDonald and
Shillcock (2003) for details on this procedure).
4.2 Results
Table 3 shows the results of the correlation analyses
on the development set. These results were obtained
after excluding all sentences at positions 1 and 2.
In the newspaper texts in the BNC, these positions
have a special function: position 1 contains the title,
and position 2 contains the name of the author. The
first sentence of the text is therefore on position 3
(unlike in the Penn Treebank, in which no title or
author information is included and texts start at po-
sition 1).
We then conducted the same correlation analyses
on the test set, i.e., on the Embra eye-tracking cor-
pus. The results are tabulated in Table 4. Note we set
no threshold for sentence position in the test set, as
Binned data Raw data
r p r p
1-gram ?0.6505 0.0006 ?0.2087 0.0195
2-gram ?0.3471 0.0965 ?0.1498 0.0954
3-gram ?0.5512 0.0052 ?0.1674 0.0620
4-gram ?0.5824 0.0028 ?0.1932 0.0308
5-gram ?0.5750 0.0033 ?0.1919 0.0320
Length 0.3902 0.0594 0.0885 0.3264
Table 4: Results of Experiment 2: correlation of sen-
tence entropy and sentence position on the Embra
corpus
r p
Entropy 2-gram 0.1551 0.007
Entropy 3-gram 0.1646 0.000
Entropy 4-gram 0.1650 0.000
Entropy 5-gram 0.1648 0.000
Sentence position ?0.0266 0.564
Table 5: Results of Experiment 2: correlation of
reading times with sentence entropy and sentence
position
the maximum article length in this corpus was only
24 sentences.
Finally, we investigated if the total reading times
in the Embra corpus are correlated with sentence po-
sition and entropy. We computed regression analysis
that partialled out word length, word frequency, and
subject effects as recommended by Lorch and My-
ers (1990). All variables other than position were
normalized by sentence length. Table 5 lists the re-
sulting correlation coefficients. Note that no binning
was carried out here. Figure 3 plots one of the cor-
relations for illustration.
0 200 400 600 800 1000
Reading time [ms]
4
6
8
10
12
14
En
tro
py
 [b
its
]
Figure 3: Experiment 2: correlation of reading times
and sentence entropy (3-grams)
4.3 Discussion
The results in Table 3 confirm that the results ob-
tained on the Penn Treebank also hold for the news-
paper part of the BNC. The top half of the table lists
the correlation coefficients for the binned data. We
find a significant correlation between sentence po-
sition and entropy for the cut-off values 25 and 76.
In both cases, there is also a significant correlation
with sentence length; this correlation is particularly
high (?0.8584) for c = 25. The entropy rate effect
does not seem to hold if there is no cut-off; here,
we fail to find a significant correlation (though the
correlation with length is again significant). This is
probably explained by the fact that the BNC test set
contains sentences with a maximum position of 206,
and data for these high sentence positions is very
sparse.
The lower half of Table 3 confirms another result
from Experiment 1: there is generally a low, but sig-
nificant correlation between entropy and position,
even if the correlation is computed for individual
sentences rather than for bins of sentences with the
same position. Furthermore, we find that sentence
length is again a significant predictor of sentence
position, even on the raw data. This is in line with
the results of Experiment 1.
Table 4 lists the results obtained on the test set
(i.e., the Embra corpus). Note that no cut-off was
applied here, as the maximum sentence position in
this set is only 24. Both on the binned data and
on the raw data, we find significant correlations be-
tween sentence position and both entropy and sen-
tence length. However, compared to the results on
the BNC, the signs of the correlations are inverted:
there is a significant negative correlation between
position and entropy, and a significant positive cor-
relation between position and length. It seems that
the Embra corpus is peculiar in that longer sen-
tences appear later in the text, rather than earlier.
This is at odds with what we found on the Penn
Treebank and on the BNC. Note that the positive
correlation of position and length explains the neg-
ative correlation of position and entropy: length en-
ters into the entropy calculation as 1|X | , hence a high
|X | will lead to low entropy, and vice versa.
We have no immediate explanation for the inver-
sion of the relationship between position and length
in the Embra corpus; it might be an idiosyncrasy
of this corpus (note that the texts were specifically
picked for eye-tracking, and are unlikely to be a ran-
dom sample; they are also shorter than usual news-
paper texts). Note in particular that the Embra cor-
pus is not a subset of the BNC (although it was sam-
pled from UK broadsheet newspapers, and hence
should be similar to our development and training
corpora).
Let us now turn to Table 5, which lists the re-
sults of the analyses correlating the total reading
time for a sentence with its position and its entropy
(derived from n-grams with n = 2, . . . ,5). Note that
these correlation analyses were conducted by par-
tialling out word length and word frequency, which
are well-know to correlate with reading times. We
find that even once these factors have been con-
trolled, there is still a significant positive correlation
between entropy and reading time: sentences with
higher entropy are harder to process and hence have
higher reading times. This is illustrated in Figure 3
for one of the correlations. As we argued in Sec-
tion 2, this relationship between entropy and pro-
cessing effort is a crucial prerequisite of the entropy
rate principle. The increase of entropy with sen-
tence position observed by G&C (and in our Exper-
iment 1) only makes sense if increased entropy cor-
responds to increased processing difficulty (e.g., to
increased reading time). Note that this result is com-
patible with previous research by McDonald and
Shillcock (2003), who demonstrate a correlation be-
tween reading time measures and bigram probabil-
ity (though their analysis is on the word level, not
on the sentence level).
The second main finding in Table 5 is that there is
no significant correlation between sentence position
and reading time. As we argued in Section 2, this
is predicted by the entropy rate principle: the opti-
mal way to send information is at a constant rate.
In other words, speakers should produce sentences
with constant informativeness, which means that if
context is taken into account, all sentences should
be equally difficult to process, no matter which po-
sition they are at. This manifests itself in the absence
of a correlation between position and reading time
in the eye-tracking corpus.
5 Conclusions
This paper made a contribution to the understanding
of the entropy rate principle, first proposed by Gen-
zel and Charniak (2002). This principle predicts that
the position of a sentence in a text should correlate
with its entropy, defined as the sentence probabil-
ity normalized by sentence length. In Experiment 1,
we replicated the entropy rate effect reported by
Genzel and Charniak (2002, 2003) and showed that
it generalizes to a larger range of sentence posi-
tions and also holds for individual sentences, not
just averaged over all sentences with the same posi-
tion. However, we also found that a simple baseline
model based on sentence length achieves a corre-
lation with sentence position. In many cases, there
was no significant difference between the entropy
rate model and the baseline. This raises the possibil-
ity that the entropy rate effect is simply an artifact
of the way entropy rate is computed, which involves
sentence length as a normalizing factor. However,
using partial correlation analysis, we were able to
show that entropy is a significant predictor of sen-
tence position, even when sentence length is con-
trolled.
In Experiment 2, we tested a number of important
predictions of the entropy rate principle for human
sentence processing. First, we replicated the entropy
rate effect on a different corpus, a subset of the BNC
restricted to newspaper text. We found essentially
the same pattern as in Experiment 1. Using a cor-
pus of eye-tracking data, we showed that entropy
is correlated with processing difficulty, as measured
by reading times in the eye-movement record. This
confirms an important assumption that underlies the
entropy rate principle. As the eye-tracking corpus
we used was a corpus of connected sentences, it en-
abled us to also test another prediction of the en-
tropy rate principle: in context, all sentences should
be equally difficult to process, as speakers gener-
ate sentences with constant informativeness. This
means that no correlation between sentence position
and reading times was expected, which is what we
found.
Another important prediction of the entropy rate
principle remains to be evaluated in future work: for
out-of-context sentences, there should be a correla-
tion between sentence position and processing ef-
fort. This prediction can be tested by obtaining read-
ing times for sentences sampled from a corpus and
read by experimental subjects in isolation.
References
Aylett, Matthew. 1999. Stochastic suprasegmen-
tals: Relationships between redundancy, prosodic
structure and syllabic duration. In Proceedings of
the 14th International Congress of Phonetic Sci-
ences. San Francisco.
Burnard, Lou. 1995. Users Guide for the British
National Corpus. British National Corpus Con-
sortium, Oxford University Computing Service.
Clarkson, Philip R. and Ronald Rosenfeld. 1997.
Statistical language modeling using the CMU-
Cambridge toolkit. In Proceedings of Eu-
rospeech. ESCA, Rhodes, Greece, pages 2707?
2710.
Genzel, Dmitriy and Eugene Charniak. 2002. En-
tropy rate constancy in text. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics. Philadelphia, pages 199?
206.
Genzel, Dmitriy and Eugene Charniak. 2003. Vari-
ation of entropy and parse trees of sentences as
a function of the sentence number. In Michael
Collins and Mark Steedman, editors, Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing. Sapporo, pages 65?72.
Just, Marcel A. and Patricia A. Carpenter. 1980. A
theory of reading: From eye fixations to compre-
hension. Psychological Review 87:329?354.
Kuhn, Roland and Renato de Mori. 1990. A cache-
based natural language model for speech repro-
duction. IEEE Transactions on Pattern Analysis
and Machine Intelligence 12(6):570?583.
Lorch, Robert F. and Jerome L. Myers. 1990. Re-
gression analyses of repeated measures data in
cognitive research. Journal of Experimental
Psychology: Learning, Memory, and Cognition
16(1):149?157.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in reading:
The influence of transitional probabilities on eye
movements. Vision Research 43:1735?1751.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 308?316,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Priming Effects in Combinatory Categorial Grammar
David Reitter
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
dreitter@inf.ed.ac.uk
Julia Hockenmaier
Inst. for Res. in Cognitive Science
University of Pennsylvania
3401 Walnut Street
Philadelphia PA 19104, USA
juliahr@cis.upenn.edu
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
This paper presents a corpus-based ac-
count of structural priming in human sen-
tence processing, focusing on the role that
syntactic representations play in such an
account. We estimate the strength of struc-
tural priming effects from a corpus of
spontaneous spoken dialogue, annotated
syntactically with Combinatory Catego-
rial Grammar (CCG) derivations. This
methodology allows us to test a range of
predictions that CCG makes about prim-
ing. In particular, we present evidence
for priming between lexical and syntactic
categories encoding partially satisfied sub-
categorization frames, and we show that
priming effects exist both for incremental
and normal-form CCG derivations.
1 Introduction
In psycholinguistics, priming refers to the fact that
speakers prefer to reuse recently encountered lin-
guistic material. Priming effects typically man-
ifest themselves in shorter processing times or
higher usage frequencies for reused material com-
pared to non-reused material. These effects are at-
tested both in language comprehension and in lan-
guage production. Structural priming occurs when
a speaker repeats a syntactic decision, and has
been demonstrated in numerous experiments over
the past two decades (e.g., Bock, 1986; Branigan
et al, 2000). These experimental findings show
that subjects are more likely to choose, e.g., a
passive voice construction if they have previously
comprehended or produced such a construction.
Recent studies have used syntactically anno-
tated corpora to investigate structural priming.
The results have demonstrated the existence of
priming effects in corpus data: they occur for spe-
cific syntactic constructions (Gries, 2005; Szm-
recsanyi, 2005), consistent with the experimen-
tal literature, but also generalize to syntactic rules
across the board, which repeated more often than
expected by chance (Reitter et al, 2006b; Dubey
et al, 2006). In the present paper, we build on
this corpus-based approach to priming, but focus
on the role of the underlying syntactic represen-
tations. In particular, we use priming to evaluate
claims resulting from a particular syntactic theory,
which is a way of testing the representational as-
sumptions it makes.
Using priming effects to inform syntactic the-
ory is a novel idea; previous corpus-based priming
studies have simply worked with uncontroversial
classes of constructions (e.g., passive/active). The
contribution of this paper is to overcome this limi-
tation by defining a computational model of prim-
ing with a clear interface to a particular syntac-
tic framework. The general assumption we make
is that priming is a phenomenon relating to gram-
matical constituents ? these constituents determine
the syntactic choices whose repetition can lead to
priming. Crucially, grammatical frameworks dif-
fer in the grammatical constituents they assume,
and therefore predict different sets of priming ef-
fects.
We require the following ingredients to pursue
this approach: a syntactic theory that identifies
a set of constituents, a corpus of linguistic data
annotated according to that syntactic theory, and
a statistical model that estimates the strength of
priming based on a set of external factors. We can
then derive predictions for the influence of these
factors from the syntactic theory, and test them
using the statistical model. In this paper, we use
regression models to quantify structural priming
effects and to verify predictions made by Com-
binatory Categorial Grammar (CCG, Steedman
(2000)), a syntactic framework that has the theo-
retical potential to elegantly explain some of the
phenomena discovered in priming experiments.
308
CCG is distinguished from most other gram-
matical theories by the fact that its rules are
type-dependent, rather than structure-dependent
like classical transformations. Such rules adhere
strictly to the constituent condition on rules, i.e.,
they apply to and yield constituents. Moreover,
the syntactic types that determine the applicability
of rules in derivations are transparent to (i.e., are
determined, though not necessarily uniquely, by)
the semantic types that they are associated with.
As a consequence, syntactic types are more ex-
pressive and more numerous than standard parts of
speech: there are around 500 highly frequent CCG
types, against the standard 50 or so Penn Treebank
POS tags. As we will see below, these properties
allow CCG to discard a number of traditional as-
sumptions concerning surface constituency. They
also allow us to make a number of testable pre-
dictions concerning priming effects, most impor-
tantly (a) that priming effects are type-driven and
independent of derivation, and, as a corollary;
(b) that lexical and derived constituents of the
same type can prime each other. These effects are
not expected under more traditional views of prim-
ing as structure-dependent.
This paper is organized as follows: Section 2
explains the relationship between structural prim-
ing and CCG, which leads to a set of specific pre-
dictions, detailed in Section 3. Sections 4 and 5
present the methodology employed to test these
predictions, describing the corpus data and the sta-
tistical analysis used. Section 6 then presents the
results of three experiments that deal with priming
of lexical vs. phrasal categories, priming in incre-
mental vs. normal form derivations, and frequency
effects in priming. Section 7 provides a discussion
of the implications of these findings.
2 Background
2.1 Structural Priming
Previous studies of structural priming (Bock,
1986; Branigan et al, 2000) have made few the-
oretical assumptions about syntax, regardless of
whether the studies were based on planned exper-
iments or corpora. They leverage the fact that al-
ternations such as He gave Raquel the car keys vs.
He gave the car keys to Raquel are nearly equiva-
lent in semantics, but differ in their syntactic struc-
ture (double object vs. prepositional object). In
such experiments, subjects are first exposed to a
prime, i.e., they have to comprehend or produce
either the double object or the prepositional ob-
ject structure. In the subsequent trial, the target,
they are the free to produce or comprehend either
of the two structures, but they tend to prefer the
one that has been primed. In corpus studies, the
frequencies of the alternative constructions can be
compared in a similar fashion (Gries, 2005; Szm-
recsanyi, 2005).
Reitter et al (2006b) present a different method
to examine priming effects in the general case.
Rather than selecting specific syntactic alterna-
tions, general syntactic units are identified. This
method detects syntactic repetition in corpora and
correlates its probability with the distance between
prime and target, where at great distance, any rep-
etition can be attributed to chance. The size of
the priming effect is then estimated as the differ-
ence between the repetition probability close to
the prime and far away from the prime. This is
a way of factoring out chance repetition (which
is required if we do not deal with syntactic alter-
nations). By relying on syntactic units, the prim-
ing model includes implicit assumptions about the
particular syntactic framework used to annotate
the corpus under investigation.
2.2 Priming and Lexicalized Grammar
Previous work has demonstrated that priming ef-
fects on different linguistic levels are not indepen-
dent (Pickering and Branigan, 1998). Lexical rep-
etition makes repetition on the syntactic level more
likely. For instance, suppose we have two verbal
phrases (prime, target) produced only a few sec-
onds apart. Priming means that the target is more
likely to assume the same syntactic form (e.g., a
passive) as the prime. Furthermore, if the head
verbs in prime and target are identical, experi-
ments have demonstrated a stronger priming ef-
fect. This effect seems to indicate that lexical and
syntactic representations in the grammar share the
same information (e.g., subcategorization infor-
mation), and therefore these representations can
prime each other.
Consequently, we treat subcategorization as
coterminous with syntactic type, rather than as a
feature exclusively associated with lexemes. Such
types determine the context of a lexeme or phrase,
and are determined by derivation. Such an anal-
ysis is exactly what categorial grammars suggest.
The rich set of syntactic types that categories af-
ford may be just sufficient to describe all and only
309
the units that can show priming effects during
syntactic processing. That is to say that syntac-
tic priming is categorial type-priming, rather than
structural priming.
Consistent with this view, Pickering and Brani-
gan (1998) assume that morphosyntactic features
such as tense, aspect or number are represented in-
dependently from combinatorial properties which
specify the contextual requirements of a lexical
item. Property groups are represented centrally
and shared between lexicon entries, so that they
may ? separately ? prime each other. For ex-
ample, the pre-nominal adjective red in the red
book primes other pre-nominal adjectives, but not
a post-nominal relative clause (the book that?s red)
(Cleland and Pickering, 2003; Scheepers, 2003).
However, if a lexical item can prime a phrasal
constituent of the same type, and vice versa, then
a type-driven grammar formalism like CCG can
provide a simple account of the effect, because
lexical and derived syntactic types have the same
combinatory potential, which is completely spec-
ified by the type, whereas in structure-driven the-
ories, this information is only implicitly given in
the derivational process.
2.3 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a mildly context-
sensitive, lexicalized grammar formalism with a
transparent syntax-semantics interface and a flex-
ible constituent structure that is of particular in-
terest to psycholinguistics, since it allows the con-
struction of incremental derivations. CCG has also
enjoyed the interest of the NLP community, with
high-accuracy wide-coverage parsers(Clark and
Curran, 2004; Hockenmaier and Steedman, 2002)
and generators1 available (White and Baldridge,
2003).
Words are associated with lexical categories
which specify their subcategorization behaviour,
eg. ((S[dcl]\NP)/NP)/NP is the lexical category
for (tensed) ditransitive verbs in English such as
gives or send, which expect two NP objects to
their right, and one NP subject to their left. Com-
plex categories X/Y or X\Y are functors which
yield a constituent with category X, if they are ap-
plied to a constituent with category Y to their right
(/Y) or to their left (\Y).
Constituents are combined via a small set of
combinatory rule schemata:
Forward Application: X/Y Y ?> X
1http://opennlp.sourceforge.net/
Backward Application: Y X\Y ?> X
Forward Composition: X/Y Y/Z ?B X/Z
Backward Composition: Y\Z X\Y ?B X\Z
Backw. Crossed Composition: Y/Z X\Y ?B X/Z
Forward Type-raising: X ?T T/(T\X)
Coordination: X conj X ?? X
Function application is the most basic operation
(and used by all variants of categorial grammar):
I saw the man
NP (S\NP)/NP NP
>
S\NP
<
S
Composition (B) and type-raising (T) are neces-
sary for the analysis of long-range dependencies
and for incremental derivations. CCG uses the
same lexical categories for long-range dependen-
cies that arise eg. in wh-movement or coordina-
tion as for local dependencies, and does not re-
quire traces:
the man that I saw
NP (NP\NP)/(S/NP) NP (S\NP)/NP
>T
S/(S\NP)
>B
S/NP
>
NP\NP
I saw and you heard the man
NP (S\NP)/NP conj NP (S\NP)/NP
>T >T
S/(S\NP) S/(S\NP)
>B >B
S/NP S/NP
<?>
S/NP
>
S
The combinatory rules of CCG allow multiple,
semantically equivalent, syntactic derivations of
the same sentence. This spurious ambiguity is
the result of CCG?s flexible constituent structure,
which can account for long-range dependencies
and coordination (as in the above example), and
also for interaction with information structure.
CCG parsers often limit the use of the combi-
natory rules (in particular: type-raising) to obtain
a single right-branching normal form derivation
(Eisner, 1996) for each possible semantic inter-
pretation. Such normal form derivations only use
composition and type-raising where syntactically
necessary (eg. in relative clauses).
3 Predictions
3.1 Priming Effects
We expect priming effects to apply to CCG cat-
egories, which describe the type of a constituent
including the arguments it expects. Under our as-
sumption that priming manifests itself as a ten-
dency for repetition, repetition probability should
be higher for short distances from a prime (see
Section 5.2 for details).
310
3.2 Terminal and Non-terminal Categories
In categorial grammar, lexical categories specify
the subcategorization behavior of their heads, cap-
turing local and non-local arguments, and a small
set of rule schemata defines how constituents can
be combined.
Phrasal constituents may have the same cate-
gories as lexical items. For example, the verb saw
might have the (lexical) category (S\NP)/NP,
which allows it to combine with an NP to the right.
The resulting constituent for saw Johanna would
be of category S\NP ? a constituent which expects
an NP (the subject) to its left, and also the lexi-
cal category of an intransitive verb. Similarly, the
constituent consisting of a ditransitive verb and its
object, gives the money, has the same category as
saw. Under the assumption that priming occurs for
these categories, we proceed to test a hypothesis
that follows from the fact that categories merely
encode unsatisfied subcategorized arguments.
Given that a transitive verb has the same cat-
egory as the constituent formed by a ditransitive
verb and its direct object, we would expect that
both categories can prime each other, if they are
cognitive units. More generally, we would expect
that lexical (terminal) and phrasal (non-terminal)
categories of the same syntactic type may prime
each other. The interaction of such conditions with
the priming effect can be quantified in the statisti-
cal model.
3.3 Incrementality of Analyses
Type-raising and composition allow derivations
that are mostly left-branching, or incremental.
Adopting a left-to-right processing order for a sen-
tence is important, if the syntactic theory is to
make psycholinguistically viable predictions (Niv,
1994; Steedman, 2000).
Pickering et al (2002) present priming experi-
ments that suggest that, in production, structural
dominance and linearization do not take place in
different stages. Their argument involves verbal
phrases with a shifted prepositional object such
as showed to the mechanic a torn overall. At a
dominance-only level, such phrases are equivalent
to non-shifted prepositional constructions (showed
a torn overall to the mechanic), but the two vari-
ants may be differentiated at a linearization stage.
Shifted primes do not prime prepositional objects
in their canonical position, thus priming must oc-
cur at a linearized level, and a separate dominance
level seems unlikely (unless priming is selective).
CCG is compatible with one-stage formulations of
syntax, as no transformation is assumed and cate-
gories encode linearization together with subcate-
gorization.
CCG assumes that the processor may produce
syntactically different, but semantically equivalent
derivations.2 So, while neither the incremental
analysis we generate, nor the normal-form, rep-
resent one single correct derivation, they are two
extremes of a ?spectrum? of derivations. We hy-
pothesize that priming effects predicted on the ba-
sis of incremental CCG analyses will be as strong
than those predicted on the basis of their normal-
form equivalents.
4 Corpus Data
4.1 The Switchboard Corpus
The Switchboard (Marcus et al, 1994) corpus con-
tains transcriptions of spoken, spontaneous con-
versation annotated with phrase-structure trees.
Dialogues were recorded over the telephone
among randomly paired North American speak-
ers, who were just given a general topic to talk
about. 80,000 utterances of the corpus have been
annotated with syntactic structure. This portion,
included in the Penn Treebank, has been time-
aligned (per word) in the Paraphrase project (Car-
letta et al, 2004).
Using the same regression technique as em-
ployed here, Reitter et al (2006b) found a marked
structural priming effect for Penn-Treebank style
phrase structure rules in Switchboard.
4.2 Disfluencies
Speech is often disfluent, and speech repairs are
known to repeat large portions of the preceding
context (Johnson and Charniak, 2004). The orig-
inal Switchboard transcripts contains these disflu-
encies (marked up as EDITED):
( (S >>>(EDITED
(RM (-DFL- \bs [) )
(EDITED
(RM (-DFL- \bs [) )
(CC And)
(, ,)
(IP (-DFL- \bs +) ))
(CC and)
(, ,)
(RS (-DFL- \bs ]) )
(IP (-DFL- \bs +) ))<<<
2Selectional criteria such as information structure and in-
tonation allow to distinguish between semantically different
analyses.
311
(CC and)
>>>(RS (-DFL- \bs ]) )<<<
(NP-SBJ (PRP I) )
(VP (VBP guess)
(SBAR (-NONE- 0)
(S (NP-SBJ (DT that) )
(VP (BES ?s)
(SBAR-NOM-PRD
(WHNP-1 (WP what) )
(S (NP-SBJ (PRP I) )
(ADVP (RB really) )
(VP (VBP like)
(NP (-NONE- *T*-1) ))))))))
(. .) (-DFL- E_S) ))
It is unclear to what extent these repetitions
are due to priming rather than simple correc-
tion. In disfluent utterances, we therefore elimi-
nate reparanda and only keep repairs (the portions
marked with >...< are removed). Hesitations (uh,
etc.), and utterances with unfinished constituents
are also ignored.
4.3 Translating Switchboard to CCG
Since the Switchboard annotation is almost iden-
tical to the one of the Penn Treebank, we use a
similar translation algorithm to Hockenmaier and
Steedman (2005). We identify heads, arguments
and adjuncts, binarize the trees, and assign cat-
egories in a recursive top-down fashion. Nonlo-
cal dependencies that arise through wh-movement
and right node raising (*T* and *RNR* traces) are
captured in the resulting derivation. Figure 1 (left)
shows the rightmost normal form CCG derivation
we obtain for the above tree. We then transform
this normal form derivation into the most incre-
mental (i.e., left-branching) derivation possible, as
shown in Figure 1 (right).
This transformation is done by a top-down re-
cursive procedure, which changes each tree of
depth two into an equivalent left-branching anal-
ysis if the combinatory rules allow it. This pro-
cedure is run until no further transformation can
be executed. The lexical categories of both deriva-
tions are identical.
5 Statistical Analysis
5.1 Priming of Categories
CCG assumes a minimal set of combinatory rule
schemata. Much more than in those rules, syntac-
tic decisions are evident from the categories that
occur in the derivation.
Given the categories for each utterance, we can
identify their repeated use. A certain amount
of repetition will obviously be coincidental. But
structural priming predicts that a target category
will occur more frequently closer to a potential
prime of the same category. Therefore, we can
correlate the probability of repetition with the dis-
tance between prime and target. Generalized Lin-
ear Mixed Effects Models (GLMMs, see next sec-
tion) allow us to evaluate and quantify this corre-
lation.
Every syntactic category is counted as a poten-
tial prime and (almost always) as a target for prim-
ing. Because interlocutors tend to stick to a topic
during a conversation for some time, we exclude
cases of syntactic repetition that are a results of
the repetition of a whole phrase.
Previous work points out that priming is sensi-
tive to frequency (Scheepers (2003) for high/low
relative clause attachments, (Reitter et al, 2006a)
for phrase structure rules). Highly frequent items
do not receive (as much) priming. We include
the logarithm of the raw frequency of the syntac-
tic category in Switchboard (LNFREQ) to approx-
imate the effect that frequency has on accessibility
of the category.
5.2 Generalized Linear Mixed Effects
Regression
We use generalized linear mixed effects regression
models (GLMM, Venables and Ripley (2002)) to
predict a response for a number of given categorial
(?factor?) or continuous (?predictor?) explanatory
variables (features). Our data is made up of in-
stances of repetition examples and non-repetition
examples from the corpus. For each target in-
stance of a syntactic category c occurring in a
derivation and spanning a constituent that begins
at time t, we look back for possible instances of
constituents with the same category (the prime)
in a time frame of [t ? d ? 0.5; t ? d + 0.5] sec-
onds. If such instances can be found, we have a
positive example of repetition. Otherwise, c is in-
cluded as a data point with a negative outcome.
We do so for a range of different distances d, com-
monly 1 ? d ? 15 seconds.3 For each data point,
we include the logarithm of the distance d between
priming period and target as an explanatory vari-
able LNDIST. (See Reitter et al (2006b) for a
worked example.)
In order to eliminate cases of lexical repeti-
tion of a phrase, e.g., names or lexicalized noun
3This approach uses a number of data points per target,
looking backwards for primes. The opposite way ? looking
forwards for targets ? would make similar predictions.
312
Normal form derivation Incremental derivation
S[dcl]
S/S
and
S[dcl]
S/(S\NP)
NP
I
S[dcl]\NP
(S[dcl]\NP)/S[dcl]
guess
S[dcl]
S/(S\NP)
NP
that
S[dcl]\NP
(S[dcl]\NP)/NP
?s
NP
NP/(S[dcl]/NP)
what
S[dcl]/NP
S/(S\NP)
NP
I
(S[dcl]\NP)/NP
(S\NP)/(S\NP)
really
(S[dcl]\NP)/NP
like
S[dcl]
S[dcl]/(S[dcl]/NP)
S[dcl]/NP
S[dcl]/(S\NP)
S[dcl]/S[dcl]
S/(S\NP)
S/S
and
S/(S\NP)
NP
I
(S[dcl]\NP)/S[dcl]
guess
S/(S\NP)
NP
that
(S[dcl]\NP)/NP
?s
NP/(S[dcl]/NP)
what
S[dcl]/NP
S/(S\NP)
S/(S\NP)
NP
I
(S\NP)/(S\NP)
really
(S[dcl]\NP)/NP
like
Figure 1: Two derivations (normal form: left), incremental: right) for the sentence fragment and I guess
that?s what I really like from Switchboard.
phrases, which we consider topic-dependent or in-
stances of lexical priming, we only collect syntac-
tic repetitions with at least one differing word.
Without syntactic priming, we would assume
that there is no correlation between the probabil-
ity that a data point is positive (repetition occurs)
and distance d. With priming, we would expect
that the probability is inversely proportional to d.
Our model uses lnd as predictor LNDIST, since
memory effects usually decay exponentially.
The regression model fitted is then simply a
choice of coefficients ?i, among them one for each
explanatory variable i. ?i expresses the contribu-
tion of i to the probability of the outcome event,
that is, in our case, successful priming. The coeffi-
cient of interest is the one for the time correlation,
i.e. ?lnDist . It specifies the strength of decay of
repetition probability over time. If no other vari-
ables are present, a model estimates the repetition
probability for a data point i as
p?i = ?0 +?lnDist ln DISTi
Priming is present if the estimated parameter is
negative, i.e. the repetition probability decreases
with increasing distance between prime and target.
Other explanatory variables, such as ROLE,
which indicates whether priming occurs within a
speaker (production-production priming, PP) or
in between speakers (comprehension-production
priming, CP), receive an interaction coefficient
that adds linearly to ?lnDist . Additional interac-
tion variables are included depending on the ex-
perimental question.4
4Lastly, we identify the target utterance in a random fac-
tor in our model, grouping the several measurements (15 for
the different distances from each target) as repeated measure-
ments, since they depend on the same target category occur-
rence and are partially inter-dependent.
From the data produced, we include all cases
of reptition and a an equal number of randomly
sampled non-repetition cases.5
6 Experiments
6.1 Experiment 1: Priming in Incremental
and Normal-form Derivations
Hypothesis CCG assumes a multiplicity of se-
mantically equivalent derivations with different
syntactic constituent structures. Here, we in-
vestigate whether two of these, the normal-form
and the most incremental derivation, differ in the
strength with which syntactic priming occurs.
Method A joint model was built containing rep-
etition data from both types of derivations. Since
we are only interested in cases where the two
derivations differ, we excluded all constituents
where a string of words was analyzed as a con-
stituent in both derivations. This produced a data
set where the two derivations could be contrasted.
A factor DERIVATION in the model indicates
whether the repetition occurred in a normal-form
(NF) or an incremental derivation (INC).
Results Significant and substantial priming is
present in both types of derivations, for both PP
and CP priming. There is no significant difference
in priming strength between normal-form and
incremental derivations (?lnDist:NF = 0.008, p =
0.95). The logarithm of the raw category fre-
quency is negatively correlated with the priming
strength (?lnDist:lnFreq = 0.151, p < 0.0001. Note
that a negative coefficient for LNDIST indicates
5We trained our models using Penalized Quasi-
Likelihood (Venables and Ripley, 2002). This technique
works best if data is balanced, i.e. we avoid having very rare
positive examples in the data. Experiment 2 was conducted
on a subset of the data.
313
CP:NormalForm
PP:NormalForm
CP:Incremental
PP:Incremental
1.0 1.2 1.4 1.6
- - - -
Figure 2: Decay effect sizes in Experiment 1
for combinations of comprehension-production or
production-production priming and in incremental
or normal-form derivations. Error bars show (non-
simultaneous) 95% confidence intervals.
decay. The lower this coefficient, the more decay,
hence priming).
If there was no priming of categories for incre-
mentally formed constituents, we would expect to
see a large effect of DERIVATION. In the contrary,
we see no effect at a high p, where the that the
regression method used is demonstrably powerful
enough to detect even small changes in the prim-
ing effect. We conclude that there is no detectable
difference in priming between the two derivation
types. In Fig. 2, we give the estimated priming
effect sizes for the four conditions.6
The result is compatible with CCG?s separation
of derivation structure and the type of the result
of derivation. It is not the derivation structure that
primes, but rather the type of the result. It is also
compatible with the possibility of a non-traditional
constituent structure (such as the incremental anal-
ysis), even though it is clear that neither incremen-
tal nor normal-form derivations necessarily repre-
sent the ideal analysis.
The category sets occurring in both derivation
variants was largely disjunct, making testing for
actual overlap between different derivations im-
possible.
6.2 Experiment 2: Priming between Lexical
and Phrasal Categories
Hypothesis Since CCG categories simply en-
code unsatisfied subcategorization constraints,
constituents which are very different from a tradi-
tional linguistic perspective can receive the same
category. This is, perhaps, most evident in phrasal
6Note that Figures 2 and 3 stem from nested models that
estimate the effect of LNDIST within the four/eight condi-
tions. Confidence intervals will be larger, as fewer data-
points are available than when the overall effect of a single
factor is compared.
CP:lex?lex
PP:lex?lex
CP:lex?phr
PP:lex?phr
CP:phr?lex
PP:phr?lex
CP:phr?phr
PP:phr?phr
?1.0 ?1.2 ?1.4 ?1.6 ?1.8 ?2.0
Figure 3: Decay effect sizes in Experiment 2,
for combinations of comprehension-production
or production-production priming and lexical or
phrasal primes and targets, e.g. the third bar
denotes the decay in repetition probability of a
phrasal category as prime and a lexical one as
target, where prime and target occurred in utter-
ances by the same speaker. Error bars show (non-
simultaneous) 95% confidence intervals.
and lexical categories (where, e.g., an intransitive
verb is indistinguishable from a verb phrase).
Bock and Loebell (1990)?s experiments suggest
that priming effects are independent of the subcat-
egorization frame. There, an active voice sentence
primed a passive voice one with the same phrase
structure, but a different subcategorization. If we
find priming from lexical to phrasal categories,
then our model demonstrates priming of subcat-
egorization frames.
From a processing point of view, phrasal cat-
egories are distinct from lexical ones. Lexical
categories are bound to the lemma and thereby
linked to the lexicon, while phrasal categories
are the result of a structural composition or de-
composition process. The latter ones represent
temporary states, encoding the syntactic process.
Here, we test whether lexical and phrasal cate-
gories can prime each other, and if so, contrast the
strength of these priming effects.
Method We built a model which allowed lex-
ical and phrasal categories to prime each other.
A factor, STRUCTURAL LEVEL was introduced
314
to distinguish the four cases: priming in between
phrasal categories and in between lexical ones,
from lexical ones to phrasal ones and from phrasal
ones to lexical ones.
Recall that each data point encodes a possibility
to repeat a CCG category, referring to a particular
instance of a target category at time t and a time
span of duration of one second [t?d?0.5, t?d +
0.5] in which a priming instance of the same cate-
gory could occur. If it occurred at least once, the
data point was counted as a possible example of
priming (response variable: true), otherwise it was
included as a counter-example (response variable:
false). For the target category, its type (lexical or
phrasal) was clear. For the category of the prime,
we included two data points, one for each type,
with a response indicating whether a prime of the
category of such a type occurred in the time win-
dow. We built separate models for incremental and
normal form derivations. Models were fitted to
a balanced subset, including all repetitions and a
randomly sampled subset of non-repetitions.
Results Both the normal-form and the incre-
mental model show qualitatively the same re-
sults. STRUCTURALLEVEL has a significant
influence on priming strength (LN DIST) for
the cases where a lexical item serves as prime
(e.g., normal-form PP: ?lnDist:lex?lex = 0.261,
p < 0.0001; ?lnDist:lex?phr = 0.166, p < 0.0001;
?lnDist:phr?lex = 0.056, p < 0.05; as compared to
the baseline phr? phr. N.B. higher values denote
less decay & priming). Phrasal categories prime
other phrasal and lexical categories, but there is a
lower priming effect to be seen from lexical cate-
gories. Figure 3 presents the resulting effect sizes.
Albeit significant, we assume the effect of prime
type is attributable to processing differences rather
than the strong difference that would indicate that
there is no priming of, e.g., lexical subcategoriza-
tion frames. As the analysis of effect sizes shows,
we can see priming from and in between both lex-
ical and phrasal categories.
Additionally, there is no evidence suggesting
that, once frequency is taken into account, syntac-
tic processes happening high up in derivation trees
show more priming (see Scheepers 2003).
7 Discussion
We can confirm the syntactic priming effect for
CCG categories. Priming occurs in incremental
as well as in normal-form CCG derivations, and at
different syntactic levels in those derivations: we
demonstrated that priming effects persists across
syntactic stages, from the lowest one (lexical cate-
gories) up to higher ones (phrasal categories). This
is what CCG predicts if priming of categories is
assumed.
Linguistic data is inherently noisy. Annotations
contain errors, and conversions such as the one to
CCG may add further error. However, since noise
is distributed across the corpus, it is unlikely to af-
fect priming effect strength or its interaction with
the factors we used: priming, in this study, is de-
fined as decay of repetition probability. We see
the lack of control in the collection of a corpus like
Switchboard not only as a challenge, but also as an
advantage: it means that realistic data is present in
the corpus, allowing us to conduct a controlled ex-
periments to validate a claim about a specific the-
ory of competence grammar.
The fact that CCG categories prime could be
explained in a model that includes a basic form
of subcategorization. All categories, if lexical or
phrasal, contain a subcategorization frame, with
only those categories present that have yet to be
satisfied. Our CCG based models make predic-
tions for experimental studies, e.g., that specific
heads with open subcategorization slots (such as
transitive verbs) will be primed by phrases that re-
quire the same kinds of arguments (such as verbal
phrases with a ditransitive verb and an argument).
The models presented take the frequency of the
syntactic category into account, reducing noise,
especially in the conditions with lower numbers
of (positive) reptition examples (e.g., CP and in-
cremental derivations in Experiment 1). Whether
there are significant qualitative and quantitative
differences of PP and CP priming with respect to
choice of derivation type ? which would point out
processing differences in comprehension vs. pro-
duction priming ? will be a matter of future work.
At this point, we do not explicitly discriminate
different syntactic frameworks. Comparing prim-
ing effects in a corpus annotated in parallel accord-
ing to different theories will be a matter of future
work.
8 Conclusions
We have discussed an empirical, corpus-based ap-
proach to use priming effects in the validation of
general syntactic models. The analysis we pre-
sented is compatible with the reality of a lexical-
315
ized, categorial grammar such as CCG as a com-
ponent of the human sentence processor. CCG is
unusual in allowing us to compare different types
of derivational analyses within the same grammar
framework. Focusing on CCG allowed us to con-
trast priming under different conditions, while still
making a statistical and general statement about
the priming effects for all syntactic phenomena
covered by the grammar.
Acknowledgements
We would like to thank Mark Steedman, Roger Levy, Jo-
hanna Moore and three anonymous reviewers for their com-
ments. The authors are grateful for being supported by the
following grants: DR by The Edinburgh Stanford Link, JH
by NSF ITR grant 0205456, FK by The Leverhulme Trust
(grant F/00 159/AL ? Syntactic Parallelism).
References
J. Kathryn Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18:355?387.
J. Kathryn Bock and Helga Loebell. 1990. Framing sen-
tences. Cognition, 35:1?39.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 2000. Syntactic co-ordination in dialogue. Cogni-
tion, 75:B13?25.
Jean Carletta, S. Dingare, Malvina Nissim, and T. Nikitina.
2004. Using the NITE XML toolkit on the Switchboard
corpus to study syntactic choice: a case study. In Proc. 4th
Language Resources and Evaluation Conference. Lisbon,
Portugal.
Stephen Clark and James R. Curran. 2004. Parsing the WSJ
using CCG and log-linear models. In Proc. of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics. Barcelona, Spain.
A. A. Cleland and M. J. Pickering. 2003. The use of lexi-
cal and syntactic information in language production: Ev-
idence from the priming of noun-phrase structure. Journal
of Memory and Language, 49:214?230.
Amit Dubey, Frank Keller, and Patrick Sturt. 2006. Inte-
grating syntactic priming into an incremental probabilistic
parser, with an application to psycholinguistic modeling.
In Proc. of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Mtg of the Association
for Computational Linguistics. Sydney, Australia.
Jason Eisner. 1996. Efficient normal-form parsing for com-
binatory categorial grammar. In Proceedings of the 34th
Annual Meeting of the Association for Computational Lin-
guistics, pages 79?86. Santa Cruz,CA.
Stefan Th. Gries. 2005. Syntactic priming: A corpus-
based approach. Journal of Psycholinguistic Research,
34(4):365?399.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Catego-
rial Grammar. In Proc. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics. Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users? manual. Technical Report MS-CIS-05-09, Com-
puter and Information Science, University of Pennsylva-
nia.
Mark Johnson and Eugene Charniak. 2004. A tag-based noisy
channel model of speech repairs. In Proc. 42nd Annual
Meeting of the Association for Computational Linguistics,
pages 33?39. Barcelona, Spain.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994.
The Penn treebank: Annotating predicate argument struc-
ture. In Proc. ARPA Human Language Technology Work-
shop. Plainsboro, NJ.
Michael Niv. 1994. A psycholinguistically motivated parser
for CCG. In Mtg. of the Association for Computational
Linguistics, pages 125?132.
Martin J. Pickering and Holly P. Branigan. 1998. The rep-
resentation of verbs: Evidence from syntactic priming in
language production. Journal of Memory and Language,
39:633?651.
Martin J. Pickering, Holly P. Branigan, and Janet F. McLean.
2002. Constituent structure is formulated in one stage.
Journal of Memory and Language, 46:586?605.
David Reitter, Frank Keller, and Johanna D. Moore. 2006a.
Computational modelling of structural priming in dia-
logue. In Proc. Human Language Technology conference
- North American chapter of the Association for Compu-
tational Linguistics annual mtg. New York City.
David Reitter, Johanna D. Moore, and Frank Keller. 2006b.
Priming of syntactic rules in task-oriented dialogue and
spontaneous conversation. In Proc. 28th Annual Confer-
ence of the Cognitive Science Society.
Christoph Scheepers. 2003. Syntactic priming of relative
clause attachments: Persistence of structural configuration
in sentence production. Cognition, 89:179?205.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
Benedikt Szmrecsanyi. 2005. Creatures of habit: A corpus-
linguistic analysis of persistence in spoken english. Cor-
pus Linguistics and Linguistic Theory, 1(1):113?149.
William N. Venables and Brian D. Ripley. 2002. Modern
Applied Statistics with S. Fourth Edition. Springer.
Mike White and Jason Baldridge. 2003. Adapting chart re-
alization to CCG. In Proc. 9th European Workshop on
Natural Language Generation. Budapest, Hungary.
316
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 43?46,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
The Interaction of Syntactic Theory and Computational Psycholinguistics
Frank Keller
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
keller@inf.ed.ac.uk
1 Introduction
Typically, current research in psycholinguistics
does not rely heavily on results from theoretical
linguistics. In particular, most experimental work
studying human sentence processing makes very
straightforward assumptions about sentence struc-
ture; essentially only a simple context-free gram-
mar is assumed. The main text book in psycholin-
guistics, for instance, mentions Minimalism in its
chapter on linguistic description (Harley, 2001,
ch. 2), but does not provide any details, and all the
examples in this chapter, as well as in the chapters
on sentence processing and language production
(Harley, 2001, chs. 9, 12), only use context-free
syntactic structures with uncontroversial phrase
markers (S, VP, NP, etc.). The one exception is
traces, which the textbook discusses in the context
of syntactic ambiguity resolution.
Harley?s (2001) textbook is typical of experi-
mental psycholinguistics, a field in which most
of the work is representationally agnostic, i.e., as-
sumptions about syntactic structure left implicit,
or limited to uncontroversial ones. However, the
situation is different in computational psycholin-
guistics, where researchers built computationally
implemented models of human language process-
ing. This typically involves making one?s theoret-
ical assumptions explicit, a prerequisite for being
able to implement them. For example, Crocker?s
(1996) model explicitly implements assumptions
from the Principles and Parameters framework,
while Hale (2006) uses probabilistic Minimalist
grammars, or Mazzei et al (2007) tree-adjoining
grammars.
Here, we will investigate how evidence regard-
ing human sentence processing can inform our as-
sumptions about syntactic structure, at least in so
far as this structure is used in computational mod-
els of human parsing.
2 Prediction and Incrementality
Evidence from psycholinguistic research suggests
that language comprehension is largely incremen-
tal, i.e., that comprehenders build an interpretation
of a sentence on a word-by-word basis. Evidence
for incrementality comes from speech shadow-
ing, self-paced reading, and eye-tracking studies
(Marslen-Wilson, 1973; Konieczny, 2000; Tanen-
haus et al, 1995): as soon as a reader or listener
perceives a word in a sentence, they integrate it as
fully as possible into a representation of the sen-
tence thus far. They experience differential pro-
cessing difficulty during this integration process,
depending on the properties of the word and its re-
lationship to the preceding context.
There is also evidence for full connectivity in
human language processing (Sturt and Lombardo,
2005). Full connectivity means that all words
are connected by a single syntactic structure; the
parser builds no unconnected tree fragments, even
for the incomplete sentences (sentence prefixes)
that arise during incremental processing.
Furthermore, there is evidence that readers or
listeners make predictions about upcoming mate-
rial on the basis of sentence prefixes. Listeners
can predict an upcoming post-verbal element, de-
pending on the semantics of the preceding verb
(Kamide et al, 2003). Prediction effects can also
be observed in reading. Staub and Clifton (2006)
showed that following the word either readers pre-
dict the conjunction or and the complement that
follows it; processing was facilitated compared to
structures that include or without either. In an
ERP study, van Berkum et al (1999) found that
listeners use contextual information to predict spe-
cific lexical items and experience processing diffi-
culty if the input is incompatible with the predic-
tion.
The concepts of incrementality, connectedness,
and prediction are closely related: in order to guar-
43
antee that the syntactic structure of a sentence pre-
fix is fully connected, it may be necessary to build
phrases whose lexical anchors (the words that they
relate to) have not been encountered yet. Full con-
nectedness is required to ensure that a fully inter-
pretable structure is available at any point during
incremental sentence processing.
Here, we explore how these key psycholinguis-
tic concepts (incrementality, connectedness, and
prediction) can be realized within a new version of
tree-adjoining grammar, which we call Psycholin-
guistically Motivated TAG (PLTAG). We propose
a formalization of PLTAG and a linking theory that
derives predictions of processing difficulty from it.
We then present an implementation of this model
and evaluate it against key experimental data re-
lating to incrementality and prediction. This ap-
proach is described in more detail in Demberg and
Keller (2008) and Demberg and Keller (2009).
3 Modeling Explicit Prediction
We propose a theory of sentence processing
guided by the principles of incrementality, con-
nectedness, and prediction. The core assumption
of our proposal is that a sentence processor that
maintains explicit predictions about the upcoming
structure has to validate these predictions against
the input it encounters. Using this assumption, we
can naturally combine the forward-looking aspect
of Hale?s (2001) surprisal (sentence structures are
computed incrementally and unexpected continu-
ations cause difficulty) with the backward-looking
integration view of Gibson?s (1998) dependency
locality theory (previously predicted structures are
verified against new evidence, leading to process-
ing difficulty as predictions decay with time).
In order to build a model that implements this
theory, we require an incremental parser that is ca-
pable of building fully connected structures and
generating explicit predictions from which we can
then derive a measure of processing difficulty. Ex-
isting parsers and grammar formalism do not meet
this specification. While there is substantial previ-
ous work on incremental parsing, none of the ex-
isting model observes full connectivity. One likely
reason for this is that full connectivity cannot be
achieved using canonical linguistic structures as
assumed in standard grammar formalisms such
as CFG, CCG, TAG, LFG, or HPSG. Instead, a
stack has to be used to store partial structures
and retrieve them later when it has become clear
(through additional input) how to combine them.
We therefore propose a new variant of the tree-
adjoining grammar (TAG) formalism which real-
izes full connectedness. The key idea is that in
cases where new input cannot be combined im-
mediately with the existing structure, we need to
predict additional syntactic material, which needs
to be verified against future input later on.
4 Incremental Processing with PLTAG
PLTAG extends normal LTAG in that it specifies
not only the canonical lexicon containing lexical-
ized initial and auxiliary trees, but also a predictive
lexicon which contains potentially unlexicalized
trees, which we will call prediction trees. Each
node in a prediction tree is annotated with indices
of the form
s j
s j , where inner nodes have two iden-
tical indices, root nodes only have a lower index
and foot and substitution nodes only have an up-
per index. The reason for only having half of the
indices is that these nodes (root, foot, and substitu-
tion nodes) still need to combine with another tree
in order to build a full node. If an initial tree substi-
tutes into a substitution node, the node where they
are integrated becomes a full node, with the upper
half contributed by the substitution node and the
lower half contributed by the root node.
Prediction trees have the same shape as trees
from the normal lexicon, with the difference that
they do not contain substitution nodes to the right
of their spine (the spine is the path from the root
node to the anchor), and that their spine does not
have to end with a lexical item. The reason for
the missing right side of the spine and the miss-
ing lexical item are considerations regarding the
granularity of prediction. This way, for example,
we avoid predicting verbs with specific subcate-
gorization frames (or even a specific verb). In gen-
eral, we only predict upcoming structure as far
as we need it, i.e., as required by connectivity or
subcategorization. (However, this is a preliminary
assumption, the optimal prediction grain size re-
mains an open research question.)
PLTAG allows the same basic operations (sub-
stitution and adjunction) as normal LTAG, the only
difference is that these operations can also be ap-
plied to prediction trees. In addition, we assume
a verification operation, which is needed to val-
idate previously integrated prediction trees. The
tree against which verification happens has to al-
ways match the predicted tree in shape (i.e., the
44
verification tree must contain all the nodes with
a unique, identical index that were present in the
prediction tree, and in the same order; any addi-
tional nodes present in the verification tree must
be below the prediction tree anchor or to the right
of its spine). This means that the verification op-
eration does not introduce any tree configurations
that would not be allowed by normal LTAG. Note
that substitution or adjunction with a predictive
tree and the verification of that tree always occur
pairwise, since each predicted node has to be veri-
fied. A valid parse for a sentence must not contain
any nodes that are still annotated as being predic-
tive ? all of them have to be validated through ver-
ification by the end of the sentence.
5 Modeling Processing Difficulty
Our variant of TAG is designed to implement a
specific set of assumptions about human language
processing (strong incrementality with full con-
nectedness, prediction, ranked parallel process-
ing). The formalism forms the basis for the pro-
cessing theory, which uses the parser states to de-
rive estimates of processing difficulty. In addition,
we need a linking theory that specifies the mathe-
matical relationship between parser states and pro-
cessing difficulty in our model.
During processing, the elementary tree of each
new word is integrated with any previous struc-
ture, and a set of syntactic expectations is gener-
ated (these expectations can be easily read off the
generated tree in the form of predicted trees). Each
of these predicted trees has a time-stamp that en-
codes when it was first predicted, or last activated
(i.e., accessed). Based on the timestamp, a tree?s
decay at verification time is calculated, under the
assumption that recently-accessed structures are
easier to integrate.
In our model, processing difficulty is thus in-
curred during the construction of the syntactic
analyses, as calculated from the probabilities of
the elementary trees (this directly corresponds to
Haleian surprisal calculated over PLTAG struc-
tures instead of over CFG structures). In addition
to this, processing difficulty has a second com-
ponent, the cost of verifying earlier predictions,
which is subject to decay.
The verification cost component bears similari-
ties to DLT integration costs, but we do not calcu-
late distance in terms of number of discourse refer-
ents intervening between a dependent and its head.
Rather verification cost is determined by the num-
ber of words intervening between a prediction and
its verification, subject to decay. This captures the
intuition that a prediction becomes less and less
useful the longer ago it was made, as it decays
from memory with increasing distance.
6 Evaluation
In Demberg and Keller (2009), we present an im-
plementation of the PLTAG model, including a
lexicon induction procedure, a parsing algorithm,
and a probability model. We show that the result-
ing framework can capture experimental results
from the literature, and can explain both locality
and prediction effects, which standard models of
sentence processing like DLT and surprisal are un-
able to account for simultaneously.
Our model therefore constitutes a step towards
a unified theory of human parsing that potentially
captures a broad range of experimental findings.
This work also demonstrates that (computational)
psycholinguistics cannot afford to be representa-
tionally agnostic ? a comprehensive, computation-
ally realistic theory of human sentence process-
ing needs to make explicit assumptions about syn-
tactic structure. Here, we showed how the fact
that human parsing is incremental and predic-
tive necessitates certain assumptions about syntac-
tic structure (such as full connectedness), which
can be implemented by augmenting an existing
grammar formalism, viz., tree-adjoining grammar.
Note, however, that it is difficult to show that this
approach is the only one that is able to realize the
required representational assumptions; other solu-
tions using different grammar formalisms are pre-
sumably possible.
7 Acknowledgments
This abstract reports joint work with Vera Dem-
berg, described in more detail in Demberg and
Keller (2008) and Demberg and Keller (2009).
The research was supported by EPSRC grant
EP/C546830/1 Prediction in Human Parsing.
References
Crocker, Matthew W. 1996. Computational Psy-
cholinguistics: An Interdisciplinary Approach
to the Study of Language. Kluwer, Dordrecht.
Demberg, Vera and Frank Keller. 2008. A psy-
cholinguistically motivated version of TAG. In
45
Proceedings of the 9th International Workshop
on Tree Adjoining Grammars and Related For-
malisms. Tu?bingen.
Demberg, Vera and Frank Keller. 2009. A com-
putational model of prediction in human pars-
ing: Unifying locality and surprisal effects.
Manuscript under review.
Gibson, Edward. 1998. Linguistic complexity:
locality of syntactic dependencies. Cognition
68:1?76.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Pittsburgh, PA, volume 2, pages 159?166.
Hale, John. 2006. Uncertainty about the rest of the
sentence. Cognitive Science 30(4):609?642.
Harley, Trevor. 2001. The Psychology of Lan-
guage: From Data to Theory. Psychology
Press,, Hove, 2 edition.
Kamide, Yuki, Christoph Scheepers, and Gerry
T. M. Altmann. 2003. Integration of syntactic
and semantic information in predictive process-
ing: Cross-linguistic evidence from german and
english. Journal of Psycholinguistic Research
32:37?55.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627?645.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522?523.
Mazzei, Alessandro, Vicenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5(3):309?332.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either . . . or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425?436.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291?305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632?1634.
van Berkum, J. J. A., C. M. Brown, and Peter Ha-
goort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147?182.
46
Evaluating Smoothing Algorithms against Plausibility Judgements
Maria Lapata and Frank Keller
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
fmlap, kellerg@coli.uni-sb.de
Scott McDonald
Language Technology Group
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
scottm@cogsci.ed.ac.uk
Abstract
Previous research has shown that the
plausibility of an adjective-noun com-
bination is correlated with its corpus
co-occurrence frequency. In this paper,
we estimate the co-occurrence frequen-
cies of adjective-noun pairs that fail to
occur in a 100 million word corpus
using smoothing techniques and com-
pare them to human plausibility rat-
ings. Both class-based smoothing and
distance-weighted averaging yield fre-
quency estimates that are significant
predictors of rated plausibility, which
provides independent evidence for the
validity of these smoothing techniques.
1 Introduction
Certain combinations of adjectives and nouns are
perceived as more plausible than others. A classi-
cal example is strong tea, which is highly plausi-
ble, as opposed to powerful tea, which is not. On
the other hand, powerful car is highly plausible,
whereas strong car is less plausible. It has been
argued in the theoretical literature that the plausi-
bility of an adjective-noun pair is largely a collo-
cational (i.e., idiosyncratic) property, in contrast
to verb-object or noun-noun plausibility, which is
more predictable (Cruse, 1986; Smadja, 1991).
The collocational hypothesis has recently
been investigated in a corpus study by
Lapata et al (1999). This study investigated
potential statistical predictors of adjective-noun
plausibility by using correlation analysis to com-
pare judgements elicited from human subjects
with five corpus-derived measures: co-occurrence
frequency of the adjective-noun pair, noun
frequency, conditional probability of the noun
given the adjective, the log-likelihood ratio, and
Resnik?s (1993) selectional association measure.
All predictors but one were positively correlated
with plausibility; the highest correlation was
obtained with co-occurrence frequency. Resnik?s
selectional association measure surprisingly
yielded a significant negative correlation with
judged plausibility. These results suggest that
the best predictor of whether an adjective-noun
combination is plausible or not is simply how
often the adjective and the noun collocate in a
record of language experience.
As a predictor of plausibility, co-occurrence
frequency has the obvious limitation that it can-
not be applied to adjective-noun pairs that never
occur in the corpus. A zero co-occurrence count
might be due to insufficient evidence or might
reflect the fact that the adjective-noun pair is in-
herently implausible. In the present paper, we ad-
dress this problem by using smoothing techniques
(distance-weighted averaging and class-based
smoothing) to recreate missing co-occurrence
counts, which we then compare to plausibility
judgements elicited from human subjects. By
demonstrating a correlation between recreated
frequencies and plausibility judgements, we show
that these smoothing methods produce realistic
frequency estimates for missing co-occurrence
data. This approach allows us to establish the va-
lidity of smoothing methods independent from a
specific natural language processing task.
2 Smoothing Methods
Smoothing techniques have been used in a variety
of statistical natural language processing applica-
tions as a means to address data sparseness, an in-
herent problem for statistical methods which rely
on the relative frequencies of word combinations.
The problem arises when the probability of word
combinations that do not occur in the training
data needs to be estimated. The smoothing meth-
ods proposed in the literature (overviews are pro-
vided by Dagan et al (1999) and Lee (1999)) can
be generally divided into three types: discount-
ing (Katz, 1987), class-based smoothing (Resnik,
1993; Brown et al, 1992; Pereira et al, 1993),
and distance-weighted averaging (Grishman and
Sterling, 1994; Dagan et al, 1999).
Discounting methods decrease the probability
of previously seen events so that the total prob-
ability of observed word co-occurrences is less
than one, leaving some probability mass to be re-
distributed among unseen co-occurrences.
Class-based smoothing and distance-weighted
averaging both rely on an intuitively simple idea:
inter-word dependencies are modelled by relying
on the corpus evidence available for words that
are similar to the words of interest. The two ap-
proaches differ in the way they measure word
similarity. Distance-weighted averaging estimates
word similarity from lexical co-occurrence infor-
mation, viz., it finds similar words by taking into
account the linguistic contexts in which they oc-
cur: two words are similar if they occur in sim-
ilar contexts. In class-based smoothing, classes
are used as the basis according to which the co-
occurrence probability of unseen word combina-
tions is estimated. Classes can be induced directly
from the corpus (Pereira et al, 1993; Brown et al,
1992) or taken from a manually crafted taxonomy
(Resnik, 1993). In the latter case the taxonomy is
used to provide a mapping from words to concep-
tual classes.
In language modelling, smoothing techniques
are typically evaluated by showing that a lan-
guage model which uses smoothed estimates in-
curs a reduction in perplexity on test data over a
model that does not employ smoothed estimates
(Katz, 1987). Dagan et al (1999) use perplexity
to compare back-off smoothing against distance-
weighted averaging methods and show that the
latter outperform the former. They also com-
pare different distance-weighted averaging meth-
ods on a pseudo-word disambiguation task where
the language model decides which of two verbs
v1 and v2 is more likely to take a noun n as its
object. The method being tested must reconstruct
which of the unseen (v1,n) and (v2,n) is a valid
verb-object combination.
In our experiments we recreated co-occurrence
frequencies for unseen adjective-noun pairs using
two different approaches: taxonomic class-based
smoothing and distance-weighted averaging.1 We
evaluated the recreated frequencies by comparing
them with plausibility judgements elicited from
human subjects. In contrast to previous work, this
type of evaluation does not presuppose that the
recreated frequencies are needed for a specific
natural language processing task. Rather, our aim
is to establish an independent criterion for the
validity of smoothing techniques by comparing
them to plausibility judgements, which are known
to correlate with co-occurrence frequency (Lapata
et al, 1999).
In the remainder of this paper we present class-
1Discounting methods were not included as
Dagan et al (1999) demonstrated that distance-weighted
averaging achieves better language modelling performance
than back-off.
based smoothing and distance-weighted averag-
ing as applied to unseen adjective-noun combina-
tions (see Sections 2.1 and 2.2). Section 3 details
our judgement elicitation experiment and reports
our results.
2.1 Class-based Smoothing
We recreated co-occurrence frequencies for un-
seen adjective-noun pairs using a simplified ver-
sion of Resnik?s (1993) selectional association
measure. Selectional association is defined as the
amount of information a given predicate carries
about its argument, where the argument is rep-
resented by its corresponding classes in a taxon-
omy such as WordNet (Miller et al, 1990). This
means that predicates which impose few restric-
tions on their arguments have low selectional as-
sociation values, whereas predicates selecting for
a restricted number of arguments have high se-
lectional association values. Consider the verbs
see and polymerise: intuitively there is a great
variety of things which can be seen, whereas
there is a very specific set of things which can
be polymerised (e.g., ethylene). Resnik demon-
strated that his measure of selectional associa-
tion successfully captures this intuition: selec-
tional association values are correlated with verb-
argument plausibility as judged by native speak-
ers.
However, Lapata et al (1999) found that the
success of selectional association as a predictor
of plausibility does not seem to carry over to
adjective-noun plausibility. There are two poten-
tial reasons for this: (1) the semantic restrictions
that adjectives impose on the nouns with which
they combine appear to be less strict than the
ones imposed by verbs (consider the adjective su-
perb which can combine with nearly any noun);
and (2) given their lexicalist nature, adjective-
noun combinations may defy selectional restric-
tions yet be intuitively plausible (consider the pair
sad day, where sadness is not an attribute of day).
To address these problems, we replaced
Resnik?s information-theoretic measure with a
simpler measure which makes no assumptions
with respect to the contribution of a semantic
class to the total quantity of information provided
by the predicate about the semantic classes of
its argument. We simply substitute the noun oc-
curring in the adjective-noun combination with
the concept by which it is represented in the
taxonomy and estimate the adjective-noun co-
occurrence frequency by counting the number of
times the concept corresponding to the noun is ob-
served to co-occur with the adjective in the cor-
pus. Because a given word is not always repre-
sented by a single class in the taxonomy (i.e., the
Adjective Class f (a,n)
proud hentityi 13.70
proud hlife fromi 9.80
proud hcausal agenti 9.50
proud hpersoni 9.00
proud hleaderi .75
proud hsuperiori .08
proud hsupervisori .00
Table 1: Frequency estimation for proud chief us-
ing WordNet
noun co-occurring with an adjective can gener-
ally be the realisation of one of several conceptual
classes), we constructed the frequency counts for
an adjective-noun pair for each conceptual class
by dividing the contribution from the adjective by
the number of classes to which it belongs (Lauer,
1995; Resnik, 1993):
f (a,c)  ?
n02c
f (a,n0)
jclasses(n0)j(1)
where f (a,n0) is the number of times the ad-
jective a was observed in the corpus with con-
cept c 2 classes(n0) and jclasses(n0)j is the num-
ber of conceptual classes noun n0 belongs to. Note
that the estimation of the frequency f (a,c) relies
on the simplifying assumption that the noun co-
occurring with the adjective is distributed evenly
across its conceptual classes. This simplification
is necessary unless we have a corpus of adjective-
noun pairs labelled explicitly with taxonomic in-
formation.2
Consider the pair proud chief which is
not attested in the British National Corpus
(BNC) (Burnard, 1995). The word chief has
two senses in WordNet and belongs to seven
conceptual classes (hcausal agenti, hentityi,
hleaderi, hlife formi, hpersoni, hsuperiori,
and hsupervisori) This means that the co-
occurrence frequency of the adjective-noun pair
will be constructed for each of the seven classes,
as shown in Table 1. Suppose for example that
we see the pair proud leader in the corpus. The
word leader has two senses in WordNet and
belongs to eight conceptual classes (hpersoni,
hlife fromi, hentityi, hcausal agenti,
hfeaturei, hmerchandisei, hcommodityi, and
hobjecti). The words chief and leader have four
conceptual classes in common, i.e., hpersoni and
hlife formi, hentityi, and hcausal agenti.
This means that we will increment the observed
co-occurrence count of proud and hpersoni,
proud and hlife formi, proud and hentityi,
and proud and hcausal agenti by 18 . Since we
2There are several ways of addressing this problem, e.g.,
by discounting the contribution of very general classes by
finding a suitable class to represent a given concept (Clark
and Weir, 2001).
do not know the actual class of the noun chief in
the corpus, we weight the contribution of each
class by taking the average of the constructed
frequencies for all seven classes:
f (a,n) =
?
c2classes(n)
?
n02c
f (a,n0)
jclasses(n0)j
jclasses(n)j(2)
Based on (2) the recreated frequency for the pair
proud chief in the BNC is 6.12 (see Table 1).
2.2 Distance-Weighted Averaging
Distance-weighted averaging induces classes of
similar words from word co-occurrences with-
out making reference to a taxonomy. A key fea-
ture of this type of smoothing is the function
which measures distributional similarity from co-
occurrence frequencies. Several measures of dis-
tributional similarity have been proposed in the
literature (Dagan et al, 1999; Lee, 1999). We
used two measures, the Jensen-Shannon diver-
gence and the confusion probability. Those two
measures have been previously shown to give
promising performance for the task of estimat-
ing the frequencies of unseen verb-argument pairs
(Dagan et al, 1999; Grishman and Sterling, 1994;
Lapata, 2000; Lee, 1999). In the following we
describe these two similarity measures and show
how they can be used to recreate the frequencies
for unseen adjective-noun pairs.
Jensen-Shannon Divergence. The Jensen-
Shannon divergence is an information-theoretic
measure that recasts the concept of distributional
similarity into a measure of the ?distance?
(i.e., dissimilarity) between two probability
distributions.
Let w1 and w01 be an unseen sequence of
two words whose distributional similarity is to
be determined. Let P(w2jw1) denote the condi-
tional probability of word w2 given word w1 and
P(w2jw01) denote the conditional probability of
w2 given w01. For notational simplicity we write
p(w2) for P(w2jw1) and q(w2) for P(w2jw01). The
Jensen-Shannon divergence is defined as the av-
erage Kullback-Leibler divergence of each of two
distributions to their average distribution:
J(p,q) =
1
2

D

p
?
?
?
?
p+q
2

+D

q
?
?
?
?
p+q
2

(3)
where (p+q)/2 denotes the average distribution:
1
2
(
P(w2jw1)+P(w2jw01)
(4)
The Kullback-Leibler divergence is an
information-theoretic measure of the dissim-
ilarity of two probability distributions p and q,
defined as follows:
D(pjjq) = ?
i
pi log
pi
qi
(5)
In our case the distributions p and q are the
conditional probability distributions P(w2jw1)
and P(w2jw01), respectively. Computation of the
Jensen-Shannon divergence depends only on the
linguistic contexts w2 which the two words w1
and w01 have in common. The Jensen-Shannon di-
vergence, a dissimilarity measure, is transformed
to a similarity measure as follows:
WJ(p,q) = 10??J(p,q)(6)
The parameter ? controls the relative influence of
the words most similar to w1: if ? is high, only
words extremely similar to w1 contribute to the
estimate, whereas if ? is low, less similar words
also contribute to the estimate.
Confusion Probability. The confusion proba-
bility is an estimate of the probability that word
w01 can be substituted by word w1, in the sense of
being found in the same linguistic contexts.
Pc(w1jw01) = ?
w2
P(w1jw2)P(w2jw01)(7)
where Pc(w01jw1) is the probability that word w01
occurs in the same contexts w2 as word w1, aver-
aged over these contexts.
Let w2w1 be two unseen co-occurring words.
We can estimate the conditional probability
P(w2jw1) of the unseen word pair w2w1 by com-
bining estimates for co-occurrences involving
similar words:
PSIM(w2jw1) = ?
w012S(w1)
W (w1,w01)
N(w1)
P(w2jw01)(8)
where S(w1) is the set of words most similar to
w1, W (w1,w01) is the similarity function between
w1 and w01, and N(w1) is a normalising factor
N(w1) = ?w01 W (w1,w01). The conditional proba-
bility PSIM(w2jw1) can be trivially converted to
co-occurrence frequency as follows:
f (w1,w2) = PSIM(w2jw1) f (w1)(9)
Parameter Settings. We experimented with
two approaches to computing P(w2jw01): (1) us-
ing the probability distribution P(nja), which dis-
covers similar adjectives and treats the noun as
the context; and (2) using P(ajn), which discovers
similar nouns and treats the adjective as the con-
text. These conditional probabilities can be easily
estimated from their relative frequency in the cor-
pus as follows:
P(nja) =
f (a,n)
f (a) P(ajn) =
f (a,n)
f (n)(10)
The performance of distance-weighted averaging
depends on two parameters: (1) the number of
items over which the similarity function is com-
puted (i.e., the size of the set S(w1) denoting the
set of words most similar to w1), and (2) the
Jensen-Shannon Confusion Probability
proud chief proud chief
young chairman lone venture
old venture adverse chairman
dying government grateful importance
wealthy leader sole force
lone official wealthy representative
dead scientist elderly president
rich manager registered official
poor initiative dear manager
elderly president deliberate director
Table 2: The ten most similar adjectives to proud
and the ten most similar nouns to chief
value of the parameter ? (which is only relevant
for the Jensen-Shannon divergence). In this study
we recreated adjective-noun frequencies using
the 1,000 and 2,000 most frequent items (nouns
and adjectives), for both the confusion probabil-
ity and the Jensen-Shannon divergence.3 Further-
more, we set ? to .5, which experiments showed
to be the best value for this parameter.
Once we know which words are most simi-
lar to the either the adjective or the noun (irre-
spective of the function used to measure similar-
ity) we can exploit this information in order to
recreate the co-occurrence frequency for unseen
adjective-noun pairs. We use the weighted aver-
age of the evidence provided by the similar words,
where the weight given to a word w01 depends
on its similarity to w1 (see (8) and (9)). Table 2
shows the ten most similar adjectives to the word
proud and then the ten most similar nouns to the
word chief using the Jensen-Shannon divergence
and the confusion probability. Here the similarity
function was calculated over the 1,000 most fre-
quent adjectives in the BNC.
3 Collecting Plausibility Ratings
In order to evaluate the smoothing methods intro-
duced above, we first needed to establish an inde-
pendent measure of plausibility. The standard ap-
proach used in experimental psycholinguistics is
to elicit judgements from human subjects; in this
section we describe our method for assembling
the set of experimental materials and collecting
plausibility ratings for these stimuli.
3.1 Method
Materials. We used a part-of-speech annotated,
lemmatised version of the BNC. The BNC is a
large, balanced corpus of British English, consist-
ing of 90 million words of text and 10 million
words of speech. Frequency information obtained
3These were shown to be the best parameter settings by
Lapata (2000). Note that considerable latitude is available
when setting these parameters; there are 151,478 distinct ad-
jective types and 367,891 noun types in the BNC.
Adjective Nouns
hungry tradition innovation prey
guilty system wisdom wartime
temporary conception surgery statue
naughty regime rival protocol
Table 3: Example stimuli for the plausibility
judgement experiment
from the BNC can be expected to be a reason-
able approximation of the language experience of
a British English speaker.
The experiment used the same set of 30 adjec-
tives discussed in Lapata et al (1999). These ad-
jectives were chosen to be minimally ambiguous:
each adjective had exactly two senses according
to WordNet and was unambiguously tagged as
?adjective? 98.6% of the time, measured as the
number of different part-of-speech tags assigned
to the word in the BNC. For each adjective we
obtained all the nouns (excluding proper nouns)
with which it failed to co-occur in the BNC.
We identified adjective-noun pairs by using
Gsearch (Corley et al, 2001), a chart parser which
detects syntactic patterns in a tagged corpus by
exploiting a user-specified context free grammar
and a syntactic query. From the syntactic anal-
ysis provided by the parser we extracted a ta-
ble containing the adjective and the head of the
noun phrase following it. In the case of compound
nouns, we only included sequences of two nouns,
and considered the rightmost occurring noun as
the head. From the adjective-noun pairs obtained
this way, we removed all pairs where the noun
had a BNC frequency of less than 10 per million,
in order to reduce the risk of plausibility ratings
being influenced by the presence of a noun un-
familiar to the subjects. Each adjective was then
paired with three randomly-chosen nouns from its
list of non-co-occurring nouns. Example stimuli
are shown in Table 3.
Procedure. The experimental paradigm was
magnitude estimation (ME), a technique stan-
dardly used in psychophysics to measure judge-
ments of sensory stimuli (Stevens, 1975), which
Bard et al (1996) and Cowart (1997) have ap-
plied to the elicitation of linguistic judgements.
The ME procedure requires subjects to estimate
the magnitude of physical stimuli by assigning
numerical values proportional to the stimulus
magnitude they perceive. In contrast to the 5- or
7-point scale conventionally used to measure hu-
man intuitions, ME employs an interval scale, and
therefore produces data for which parametric in-
ferential statistics are valid.
ME requires subjects to assign numbers to
a series of linguistic stimuli in a proportional
Plaus Jena Confa Jenn Confn
Jena .058
Confa .214* .941**
Jenn .124 .781** .808**
Confn .232* .782** .864** .956**
WN .356** .222* .348** .451** .444**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 4: Correlation matrix for plausibility and
the five smoothed frequency estimates
fashion. Subjects are first exposed to a modulus
item, which they assign an arbitrary number. All
other stimuli are rated proportional to the modu-
lus. In this way, each subject can establish their
own rating scale, thus yielding maximally fine-
graded data and avoiding the known problems
with the conventional ordinal scales for linguis-
tic data (Bard et al, 1996; Cowart, 1997; Schu?tze,
1996).
In the present experiment, subjects were pre-
sented with adjective-noun pairs and were asked
to rate the degree of adjective-noun fit propor-
tional to a modulus item. The experiment was car-
ried out using WebExp, a set of Java-Classes for
administering psycholinguistic studies over the
World-Wide Web (Keller et al, 1998). Subjects
first saw a set of instructions that explained the
ME technique and included some examples, and
had to fill in a short questionnaire including basic
demographic information. Each subject saw the
entire set of 90 experimental items.
Subjects. Forty-one native speakers of English
volunteered to participate. Subjects were re-
cruited over the Internet by postings to relevant
newsgroups and mailing lists.
3.2 Results
Correlation analysis was used to assess the degree
of linear relationship between plausibility ratings
(Plaus) and the three smoothed co-occurrence
frequency estimates: distance-weighted averaging
using Jensen-Shannon divergence (Jen), distance-
weighted averaging using confusion probability
(Conf), and class-based smoothing using Word-
Net (WN). For the two similarity-based measures,
we smoothed either over the similarity of the ad-
jective (subscript a) or over the similarity of the
noun (subscript n). All frequency estimates were
natural log-transformed.
Table 4 displays the results of the corre-
lation analysis. Mean plausibility ratings were
significantly correlated with co-occurrence fre-
quency recreated using our class-based smooth-
ing method based on WordNet (r = .356, p <
.01).
As detailed in Section 2.2, the Jensen-Shannon
divergence and the confusion probability are pa-
rameterised measures. There are two ways to
smooth the frequency of an adjective-noun com-
bination: over the distribution of adjectives or
over the distribution of nouns. We tried both ap-
proaches and found a moderate correlation be-
tween plausibility and both the frequency recre-
ated using distance-weighted averaging and con-
fusion probability. The correlation was significant
both for frequencies recreated by smoothing over
adjectives (r = .214, p < .05) and over nouns
(r = .232, p < .05). However, co-occurrence fre-
quency recreated using the Jensen-Shannon di-
vergence was not reliably correlated with plausi-
bility. Furthermore, there was a reliable correla-
tion between the two Jensen-Shannon measures
Jena and Jenn (r = .781, p < .01), and similarly
between the two confusion measures Confa and
Confn (r = .864, p < .01). We also found a high
correlation between Jena and Confa (r = .941,
p < .01) and Jenn and Confn (r = .956, p < .01).
This indicates that the two similarity measures
yield comparable results for the given task.
We also examined the effect of varying one
further parameter (see Section 2.2). The recre-
ated frequencies were initially estimated using
the n = 1,000 most similar items. We examined
the effects of applying the two smoothing meth-
ods using a set of similar items of twice the size
(n = 2,000). No improvement in terms of the cor-
relations with rated plausibility was found when
using this larger set, whether smoothing over the
adjective or the noun: a moderate correlation with
plausibility was found for Confa (r = .239, p <
.05) and Confn (r = .239, p < .05), while the cor-
relation with Jena and Jenn was not significant.
An important question is how well people agree
in their plausibility judgements. Inter-subject
agreement gives an upper bound for the task and
allows us to interpret how well the smoothing
techniques are doing in relation to the human
judges. We computed the inter-subject correlation
on the elicited judgements using leave-one-out re-
sampling (Weiss and Kulikowski, 1991). Aver-
age inter-subject agreement was .55 (Min = .01,
Max = .76, SD = .16). This means that our ap-
proach performs satisfactorily given that there is
a fair amount of variability in human judgements
of adjective-noun plausibility.
One remaining issue concerns the validity
of our smoothing procedures. We have shown
that co-occurrence frequencies recreated using
smoothing techniques are significantly correlated
with rated plausibility. But this finding consti-
tutes only indirect evidence for the ability of this
method to recreate corpus evidence; it depends on
the assumption that plausibility and frequency are
adequate indicators of each other?s values. Does
WN Jena Confa Jenn Confn
Actual freq. .218* .324** .646** .308** .728**
Plausibility .349** .268* .395** .247* .416**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 5: Correlation of recreated frequencies with
actual frequencies and plausibility (using Lapata
et al?s (1999) stimuli)
smoothing accurately recreate the co-occurrence
frequency of combinations that actually do occur
in the corpus? To address this question, we ap-
plied the class-based smoothing procedure to a
set of adjective-noun pairs that occur in the cor-
pus with varying frequencies, using the materials
from Lapata et al (1999).
First, we removed all relevant adjective-noun
combinations from the corpus. Effectively we
assumed a linguistic environment with no evi-
dence for the occurrence of the pair, and thus
no evidence for any linguistic relationship be-
tween the adjective and the noun. Then we recre-
ated the co-occurrence frequencies using class-
based smoothing and distance-weighted averag-
ing, and log-transformed the resulting frequen-
cies. Both methods yielded reliable correlation
between recreated frequency and actual BNC fre-
quency (see Table 5 for details). This result pro-
vides additional evidence for the claim that these
smoothing techniques produce reliable frequency
estimates for unseen adjective-noun pairs. Note
that the best correlations were achieved for Confa
and Confn (r = .646, p < .01 and r = .728, p <
.01, respectively).
Finally, we carried out a further test of the
quality of the recreated frequencies by correlat-
ing them with the plausibility judgements re-
ported by Lapata et al (1999). Again, a signifi-
cant correlation was found for all methods (see
Table 5). However, all correlations were lower
than the correlation of the actual frequencies
with plausibility (r = .570, p < .01) reported
by Lapata et al (1999). Note also that the con-
fusion probability outperformed Jensen-Shannon
divergence, in line with our results on unfamiliar
adjective-noun pairs.
3.3 Discussion
Lapata et al (1999) demonstrated that the co-
occurrence frequency of an adjective-noun com-
bination is the best predictor of its rated plausibil-
ity. The present experiment extended this result to
adjective-noun pairs that do not co-occur in the
corpus.
We applied two smoothing techniques in order
to recreate co-occurrence frequency and found
that the class-based smoothing method was the
best predictor of plausibility. This result is inter-
guilty dangerous stop giant
guilty dangerous stop giant
interested certain moon company
innocent different employment manufacturer
injured particular length artist
labour difficult detail industry
socialist other page firm
strange strange time star
democratic similar potential master
ruling various list army
honest bad turn rival
Table 6: The ten most similar words to the adjec-
tives guilty and dangerous and the nouns stop and
giant discovered by the Jensen-Shannon measure
esting because the class-based method does not
use detailed knowledge about word-to-word rela-
tionships in real language; instead, it relies on the
notion of equivalence classes derived from Word-
Net, a semantic taxonomy. It appears that making
predictions about plausibility is most effectively
done by collapsing together the speaker?s experi-
ence with other words in the semantic class occu-
pied by the target word.
The distance-weighted averaging smoothing
methods yielded a lower correlation with plausi-
bility (in the case of the confusion probability),
or no correlation at all (in the case of the Jensen-
Shannon divergence). The worse performance of
distance-weighted averaging is probably due to
the fact that this method conflates two kinds of
distributional similarity: on the one hand, it gen-
erates words that are semantically similar to the
target word. On the other hand, it also generates
words whose syntactic behaviour is similar to that
of the target word. Rated plausibility, however,
seems to be more sensitive to semantic than to
syntactic similarity.
As an example refer to Table 6, which displays
the ten most distributionally similar words to the
adjectives guilty and dangerous and to the nouns
stop and giant discovered by the Jensen-Shannon
measure. The set of similar words is far from se-
mantically coherent. As far as the adjective guilty
is concerned the measure discovered antonyms
such as innocent and honest. Semantically unre-
lated adjectives such as injured, democratic, or in-
terested are included; it seems that their syntactic
behaviour is similar to that of guilty, e.g., they all
co-occur with party. The same pattern can be ob-
served for the adjective dangerous, to which none
of the discovered adjectives are intuitively seman-
tically related, perhaps with the exception of bad.
The set of words most similar to the noun stop
also does not appear to be semantically coherent.
This problem with distance-weighted averag-
ing is aggravated by the fact that the adjective
or noun that we smooth over can be polysemous.
Take the set of similar words for giant, for in-
stance. The words company, manufacturer, indus-
try and firm are similar to the ?enterprise? sense
of giant, whereas artist, star, master are similar
to the ?important/influential person? sense of gi-
ant. However, no similar word was found for ei-
ther the ?beast? or ?heavyweight person? sense of
giant. This illustrates that the distance-weighted
averaging approach fails to take proper account
of the polysemy of a word. The class-based ap-
proach, on the other hand, relies on WordNet, a
lexical taxonomy that can be expected to cover
most senses of a given lexical item.
Recall that distance-weighted averaging dis-
covers distributionally similar words by look-
ing at simple lexical co-occurrence information.
In the case of adjective-noun pairs we concen-
trated on combinations found in the corpus in
a head-modifier relationship. This limited form
of surface-syntactic information does not seem
to be sufficient to reproduce the detailed knowl-
edge that people have about the semantic relation-
ships between words. Our class-based smoothing
method, on the other hand, relies on the semantic
taxonomy of WordNet, where fine-grained con-
ceptual knowledge about words and their rela-
tions is encoded. This knowledge can be used to
create semantically coherent equivalence classes.
Such classes will not contain antonyms or items
whose behaviour is syntactically related, but not
semantically similar, to the words of interest.
To summarise, it appears that distance-
weighted averaging smoothing is only partially
successful in reproducing the linguistic depen-
dencies that characterise and constrain the forma-
tion of adjective-noun combinations. The class-
based smoothing method, however, relies on a
pre-defined taxonomy that allows these depen-
dencies to be inferred, and thus reliably estimates
the plausibility of adjective-noun combinations
that fail to co-occur in the corpus.
4 Conclusions
This paper investigated the validity of smoothing
techniques by using them to recreate the frequen-
cies of adjective-noun pairs that fail to occur in
a 100 million word corpus. We showed that the
recreated frequencies are significantly correlated
with plausibility judgements. These results were
then extended by applying the same smoothing
techniques to adjective-noun pairs that occur in
the corpus. These recreated frequencies were sig-
nificantly correlated with the actual frequencies,
as well as with plausibility judgements.
Our results provide independent evidence for
the validity of the smoothing techniques we em-
ployed. In contrast to previous work, our evalu-
ation does not presuppose that the recreated fre-
quencies are used in a specific natural language
processing task. Rather, we established an in-
dependent criterion for the validity of smooth-
ing techniques by comparing them to plausibil-
ity judgements, which are known to correlate
with co-occurrence frequency. We also carried
out a comparison of different smoothing meth-
ods, and found that class-based smoothing outper-
forms distance-weighted averaging.4
From a practical point of view, our findings
provide a very simple account of adjective-
noun plausibility. Extending the results of
Lapata et al (1999), we confirmed that co-
occurrence frequency can be used to estimate the
plausibility of an adjective-noun pair. If no co-
occurrence counts are available from the corpus,
then counts can be recreated using the corpus and
a structured source of taxonomic knowledge (for
the class-based approach). Distance-weighted
averaging can be seen as a ?cheap? way to obtain
this sort of taxonomic knowledge. However, this
method does not draw upon semantic informa-
tion only, but is also sensitive to the syntactic
distribution of the target word. This explains the
fact that distance-weighted averaging yielded
a lower correlation with perceived plausibility
than class-based smoothing. A taxonomy like
WordNet provides a cleaner source of conceptual
information, which captures essential aspects of
the type of knowledge needed for assessing the
plausibility of an adjective-noun combination.
References
Ellen Gurman Bard, Dan Robertson, and Antonella Sorace.
1996. Magnitude estimation of linguistic acceptability.
Language, 72(1):32?68.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
and Robert L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Linguistics,
18(4):467?479.
Lou Burnard, 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service.
Stephen Clark and David Weir. 2001. Class-based probabil-
ity estimation using a semantic hierarchy. In Proceedings
of the 2nd Conference of the North American Chapter
of the Association for Computational Linguistics, Pitts-
burgh, PA.
Steffan Corley, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntactic
4Two anonymous reviewers point out that this conclusion
only holds for an approach that computes similarity based on
adjective-noun co-occurrences. Such co-occurrences might
not reflect semantic relatedness very well, due to the idiosyn-
cratic nature of adjective-noun combinations. It is possible
that distance-weighted averaging would yield better results if
applied to other co-occurrence data (e.g., subject-verb, verb-
object), which could be expected to produce more reliable
information about semantic similarity.
structure in unparsed corpora: The Gsearch corpus query
system. Computers and the Humanities, 35(2):81?94.
Wayne Cowart. 1997. Experimental Syntax: Applying Ob-
jective Methods to Sentence Judgments. Sage Publica-
tions, Thousand Oaks, CA.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Text-
books in Linguistics. Cambridge University Press, Cam-
bridge.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence probabil-
ities. Machine Learning, 34(1):43?69.
Ralph Grishman and John Sterling. 1994. Generalizing au-
tomatically generated selectional patterns. In Proceed-
ings of the 15th International Conference on Computa-
tional Linguistics, pages 742?747, Kyoto.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 33(3):400?401.
Frank Keller, Martin Corley, Steffan Corley, Lars Konieczny,
and Amalia Todirascu. 1998. WebExp: A Java tool-
box for web-based psychological experiments. Technical
Report HCRC/TR-99, Human Communication Research
Centre, University of Edinburgh.
Maria Lapata, Scott McDonald, and Frank Keller. 1999.
Determinants of adjective-noun plausibility. In Proceed-
ings of the 9th Conference of the European Chapter of the
Association for Computational Linguistics, pages 30?36,
Bergen.
Maria Lapata. 2000. The Acquisition and Modeling of Lexi-
cal Knowledge: A Corpus-based Investigation of System-
atic Polysemy. Ph.D. thesis, University of Edinburgh.
Mark Lauer. 1995. Designing Statistical Language Learn-
ers: Experiments on Compound Nouns. Ph.D. thesis,
Macquarie University, Sydney.
Lilian Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 25?32, Univer-
sity of Maryland, College Park.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: An on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proceed-
ings of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 183?190, Columbus,
OH.
Philip Stuart Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Carson T. Schu?tze. 1996. The Empirical Base of Linguis-
tics: Grammaticality Judgments and Linguistic Method-
ology. University of Chicago Press, Chicago.
Frank Smadja. 1991. Macrocoding the lexicon with co-
occurrence knowledge. In Uri Zernik, editor, Lexical Ac-
quisition: Using Online Resources to Build a Lexicon,
pages 165?189. Lawrence Erlbaum Associates, Hillsdale,
NJ.
S. S. Stevens. 1975. Psychophysics: Introduction to its Per-
ceptual, Neural, and Social Prospects. John Wiley, New
York.
Sholom M. Weiss and Casimir A. Kulikowski. 1991. Com-
puter Systems that Learn: Classification and Prediction
Methods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo, CA.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 827?834, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Parallelism in Coordination as an Instance of Syntactic Priming:
Evidence from Corpus-based Modeling
Amit Dubey and Patrick Sturt and Frank Keller
Human Communication Research Centre, Universities of Edinburgh and Glasgow
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{adubey,sturt,keller}@inf.ed.ac.uk
Abstract
Experimental research in psycholinguis-
tics has demonstrated a parallelism effect
in coordination: speakers are faster at pro-
cessing the second conjunct of a coordi-
nate structure if it has the same internal
structure as the first conjunct. We show
that this phenomenon can be explained by
the prevalence of parallel structures in cor-
pus data. We demonstrate that parallelism
is not limited to coordination, but also ap-
plies to arbitrary syntactic configurations,
and even to documents. This indicates that
the parallelism effect is an instance of a
general syntactic priming mechanism in
human language processing.
1 Introduction
Experimental work in psycholinguistics has pro-
vided evidence for the so-called parallelism prefer-
ence effect: speakers processes coordinated struc-
tures more quickly when the two conjuncts have
the same internal syntactic structure. The processing
advantage for parallel structures has been demon-
strated for a range coordinate constructions, includ-
ing NP coordination (Frazier et al, 2000), sentence
coordination (Frazier et al, 1984), and gapping and
ellipsis (Carlson, 2002; Mauner et al, 1995).
The parallelism preference in NP coordination
can be illustrated using Frazier et al?s (2000) Exper-
iment 3, which recorded subjects? eye-movements
while they read sentences like (1):
(1) a. Terry wrote a long novel and a short poem
during her sabbatical.
b. Terry wrote a novel and a short poem dur-
ing her sabbatical
Total reading times for the underlined region were
faster in (1-a), where short poem is coordinated with
a syntactically parallel noun phrase (a long novel),
compared to (1-b), where it is coordinated with a
syntactically non-parallel phrase.
These results raise an important question that the
present paper tries to answer through corpus-based
modeling studies: what is the mechanism underlying
the parallelism preference? One hypothesis is that
the effect is caused by low-level processes such as
syntactic priming, i.e., the tendency to repeat syntac-
tic structures (e.g., Bock, 1986). Priming is a very
general mechanism that can affect a wide range of
linguistic units, including words, constituents, and
semantic concepts. If the parallelism effect is an in-
stance of syntactic priming, then we expect it to ap-
ply to a wide range of syntactic construction, and
both within and between sentences. Previous work
has demonstrated priming effects in corpora (Gries,
2005; Szmrecsanyi, 2005); however, these results
are limited to instances of priming that involve a
choice between two structural alternatives (e.g., da-
tive alternation). In order to study the parallelism ef-
fect, we need to model priming as general syntac-
tic repetition (independent of the structural choices
available). This is what the present paper attempts.
Frazier and Clifton (2001) propose an alternative
account of the parallelism effect in terms of a copy-
ing mechanism. Unlike priming, this mechanism is
highly specialized and only applies to coordinate
structures: if the second conjunct is encountered,
then instead of building new structure, the language
processor simply copies the structure of the first con-
junct; this explains why a speed-up is observed if
the second conjunct is parallel to the first one. If
the copying account is correct, then we would ex-
pect parallelism effects to be restricted to coordinate
structures and would not apply in other contexts.
In the present paper, we present corpus evidence
that allows us to distinguish between these two com-
peting explanations. Our investigation will proceed
as follows: we first establish that there is evidence
827
for a parallelism effect in corpus data (Section 3).
This is a crucial prerequisite for our wider inves-
tigation: previous work has only dealt with paral-
lelism in comprehension, hence we need to establish
that parallelism is also present in production data,
such as corpus data. We then investigate whether
the parallelism effect is restricted to coordination, or
whether it also applies also arbitrary syntactic con-
figurations. We also test if parallelism can be found
for larger segments of text, including, in the limit,
the whole document (Section 4). Then we investi-
gate parallelism in dialog, testing the psycholinguis-
tic prediction that parallelism in dialog occurs be-
tween speakers (Section 5). In the next section, we
discuss a number of methodological issues and ex-
plain the way we measure parallelism in corpus data.
2 Adaptation
Psycholinguistic studies have shown that priming
affects both speech production (Bock, 1986) and
comprehension (Branigan et al, 2005). The impor-
tance of comprehension priming has also been noted
by the speech recognition community (Kuhn and
de Mori, 1990), who use so-called caching language
models to improve the performance of speech com-
prehension software. The concept of caching lan-
guage models is quite simple: a cache of recently
seen words is maintained, and the probability of
words in the cache is higher than those outside the
cache.
While the performance of caching language mod-
els is judged by their success in improving speech
recognition accuracy, it is also possible to use an
abstract measure to diagnose their efficacy more
closely. Church (2000) introduces such a diagnostic
for lexical priming: adaptation probabilities. Adap-
tation probabilities provide a method to separate the
general problem of priming from a particular imple-
mentation (i.e., caching models). They measure the
amount of priming that occurs for a given construc-
tion, and therefore provide an upper limit for the per-
formance of models such as caching models.
Adaptation is based upon three concepts. First is
the prior, which serves as a baseline. The prior mea-
sures the probability of a word appearing, ignoring
the presence or absence of a prime. Second is the
positive adaptation, which is the probability of a
word appearing given that it has been primed. Third
is the negative adaptation, the probability of a word
appearing given it has not been primed.
In Church?s case, the prior and adaptation prob-
abilities are estimated as follows. If a corpus is di-
vided into individual documents, then each docu-
ment is then split in half. We refer to the halves as the
prime set (or prime half) and the target set (or target
half).1 We measure how frequently a document half
contains a particular word. For each word w, there
are four combinations of the prime and target halves
containing the word. This gives us four frequencies
to measure, which are summarized in the following
table:
fwp,t fwp?,t
fwp,?t fwp?,?t
These frequencies represent:
fwp,t = # of times w occurs in prime set
and target set
fwp?,t = # of times w occurs in target set
but not prime set
fwp,?t = # of times w occurs in prime set
but not target set
fwp?,?t = # of times w does not occur in either
target set or prime set
In addition, let N represent the sum of these four
frequencies. From the frequencies, we may formally
define the prior, positive adaptation and negative
adaptation:
Prior Pprior(w) =
fwp,t + fw p?,t
N
(1)
Positive Adaptation P+(w) =
fwp,t
fwp,t + fwp,?t
(2)
Negative Adaptation P?(w) =
fw p?,t
fw p?,t+ fw p?,?t
(3)
In the case of lexical priming, Church observes that
P+  Pprior > P?. In fact, even in cases when Pprior
quite small, P+ may be higher than 0.8. Intuitively,
a positive adaptation which is higher than the prior
entails that a word is likely to reappear in the target
set given that it has already appeared in the prime
set. We intend to show that adaptation probabilities
provide evidence that syntactic constructions behave
1Our terminology differs from that of Church, who uses ?his-
tory? to describe the first half, and ?test? to describe the second.
Our terms avoid the ambiguity of the phrase ?test set? and coin-
cide with the common usage in the psycholinguistic literature.
828
similarity to lexical priming, showing positive adap-
tation P+ greater than the prior. As P? must become
smaller than Pprior whenever P+ is larger than Pprior,
we only report the positive adaptation P+ and the
prior Pprior.
While Church?s technique was developed with
speech recognition in mind, we will show that
it is useful for investigating psycholinguistic phe-
nomenon. However, the connection between cogni-
tive phenomenon and engineering approaches go in
both directions: it is possible that syntactic parsers
could be improved using a model of syntactic prim-
ing, just as speech recognition has been improved
using models of lexical priming.
3 Experiment 1: Parallelism in
Coordination
In this section, we investigate the use of Church?s
adaptation metrics to measure the effect of syntac-
tic parallelism in coordinated constructions. For the
sake of comparison, we restrict our study to several
constructions used in Frazier et al (2000). All of
these constructions occur in NPs with two coordi-
nate sisters, i.e., constructions such as NP1 CC NP2,
where CC represents a coordinator such as and.
3.1 Method
The application of the adaptation metric is straight-
forward: we pick NP1 as the prime set and NP2 as
the target set. Instead of measuring the frequency of
lexical elements, we measure the frequency of the
following syntactic constructions:
SBAR An NP with a relative clause, i.e.,
NP ? NP SBAR.
PP An NP with a PP modifier, i.e., NP ? NP PP.
NN An NP with a single noun, i.e., NP ? NN.
DT NN An NP with a determiner and a noun, i.e.,
NP ? DT NN.
DT JJ NN An NP with a determiner, an adjective
and a noun, i.e., NP ? DT JJ NN.
Parameter estimation is accomplished by iterating
through the corpus for applications of the rule NP
? NP CC NP. From each rule application, we create
a list of prime-target pairs. We then estimate adap-
tation probabilities for each construction, by count-
ing the number of prime-target pairs in which the
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 1: Adaptation within coordinate structures in
the Brown corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 2: Adaptation within coordinate structures in
the WSJ corpus
construction does or does not occur. This is done
similarly to the document half case described above.
There are four frequencies of interest, but now they
refer to the frequency that a particular construction
(rather than a word) either occurs or does not occur
in the prime and target set.
To ensure results were general across genres, we
used all three parts of the English Penn Treebank:
the Wall Street Journal (WSJ), the balanced Brown
corpus of written text (Brown) and the Switchboard
corpus of spontaneous dialog. In each case, we use
the entire corpus.
Therefore, in total, we report 30 probabilities: the
prior and positive adaptation for each of the five con-
structions in each of the three corpora. The primary
objective is to observe the difference between the
prior and positive adaptation for a given construction
in a particular corpus. Therefore, we also perform a
?2 test to determine if the difference between these
two probabilities are statistically significant.
829
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 3: Adaptation within coordinate structures in
the Switchboard corpus
3.2 Results
The results are shown in Figure 1 for the Brown cor-
pus, Figure 2 for the WSJ and Figure 3 for Switch-
board. Each figure shows the prior and positive
adaptation for all five constructions: relative clauses
(SBAR) a PP modifier (PP), a single common noun
(N), a determiner and noun (DT N), and a determiner
adjective and noun (DT ADJ N). Only in the case of
a single common noun in the WSJ and Switchboard
corpora is the prior probability higher than the posi-
tive adaptation. In all other cases, the probability of
the given construction is more likely to occur in NP2
given that it has occurred in NP1. According to the
?2 tests, all differences between priors and positive
adaptations were significant at the 0.01 level. The
size of the data sets means that even small differ-
ences in probability are statistically significant. All
differences reported in the remainder of this paper
are statistically significant; we omit the details of in-
dividual ?2 tests.
3.3 Discussion
The main conclusion we draw is that the parallelism
effect in corpora mirrors the ones found experimen-
tally by Frazier et al (2000), if we assume higher
probabilities are correlated with easier human pro-
cessing. This conclusion is important, as the experi-
ments of Frazier et al (2000) only provided evidence
for parallelism in comprehension data. Corpus data,
however, are production data, which means that the
our findings are first ones to demonstrate parallelism
effects in production.
The question of the relationship between compre-
hension and production data is an interesting one.
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 4: Adaptation within sentences in the Brown
corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 5: Adaptation within sentences in the WSJ
corpus
We can expect that production data, such as corpus
data, are generated by speakers through a process
that involves self-monitoring. Written texts (such as
the WSJ and Brown) involve proofreading and edit-
ing, i.e., explicit comprehension processes. Even the
data in a spontaneous speech corpus such as Swtich-
board can be expected to involve a certain amount
of self-monitoring (speakers listen to themselves and
correct themselves if necessary). It follows that it is
not entirely unexpected that similar effects can be
found in both comprehension and production data.
4 Experiment 2: Parallelism in Documents
The results in the previous section showed that
the parallelism effect, which so far had only been
demonstrated in comprehension studies, is also at-
tested in corpora, i.e., in production data. In the
present experiment, we will investigate the mech-
anisms underlying the parallelism effect. As dis-
cussed in Section 1, there are two possible explana-
830
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 6: Adaptation between sentences in the
Brown corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 7: Adaptation between sentences in the WSJ
corpus
tion for the effect: one in terms of a construction-
specific copying mechanism, and one in terms of
a generalized syntactic priming mechanism. In the
first case, we predict that the parallelism effect is re-
stricted to coordinate structures, while in the second
case, we expect that parallelism (a) is independent of
coordination, and (b) occurs in the wider discourse,
i.e., not only within sentences but also between sen-
tences.
4.1 Method
The method used was the same as in Experiment 1
(see Section 3.1), with the exception that the prime
set and the target set are no longer restricted to
being the first and second conjunct in a coordi-
nate structure. We investigated three levels of gran-
ularity: within sentences, between sentences, and
within documents. Within-sentence parallelism oc-
curs when the prime NP and the target NP oc-
cur within the same sentence, but stand in an ar-
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 8: Adaptation within documents in the Brown
corpus (all items exhibit weak yet statistically signif-
icant positive adaptation)
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 9: Adaptation within documents in the WSJ
corpus
bitrary structural relationship. Coordinate NPs were
excluded from this analysis, so as to make sure that
any within-sentence parallelism is not confounded
coordination parallelism as established in Experi-
ment 1. Between-sentence parallelism was measured
by regarding as the target the sentence immediately
following the prime sentence. In order to investi-
gate within-document parallelism, we split the doc-
uments into equal-sized halves; then the adaptation
probability was computed by regarding the first half
as the prime and the second half as the target (this
method is the same as Church?s method for measur-
ing lexical adaptation).
The analyses were conducted using the Wall
Street Journal and the Brown portion of the Penn
Treebank. The document boundary was taken to be
the file boundary in these corpora. The Switchboard
corpus is a dialog corpus, and therefore needs to
be treated differently: turns between speakers rather
831
than sentences should be level of analysis. We will
investigate this separately in Experiment 3 below.
4.2 Results
The results for the within-sentence analysis are
graphed in Figures 4 and 5 for the Brown and WSJ
corpus, respectively. We find that there is a paral-
lelism effect in both corpora, for all the NP types
investigated. Figures 6?9 show that the same is true
also for the between-sentence and within-document
analysis: parallelism effects are obtained for all NP
types and for both corpora, even it the parallel struc-
tures occur in different sentences or in different doc-
ument halves. (The within-document probabilities
for the Brown corpus (in Figure 8) are close to one
in most cases; the differences between the prior and
adaptation are nevertheless significant.)
In general, note that the parallelism effects un-
covered in this experiment are smaller than the
effect demonstrated in Experiment 1: The differ-
ences between the prior probabilities and the adap-
tation probabilities (while significant) are markedly
smaller than those uncovered for parallelism in co-
ordinate structure.2
4.3 Discussion
This experiment demonstrated that the parallelism
effect is not restricted to coordinate structures.
Rather, we found that it holds across the board: for
NPs that occur in the same sentence (and are not part
of a coordinate structure), for NPs that occur in ad-
jacent sentences, and for NPs that occur in differ-
ent document halves. The between-sentence effect
has been demonstrated in a more restricted from by
Gries (2005) and Szmrecsanyi (2005), who investi-
gate priming in corpora for cases of structural choice
(e.g., between a dative object and a PP object for
verbs like give). The present results extend this find-
ing to arbitrary NPs, both within and between sen-
tences.
The fact that parallelism is a pervasive phe-
nomenon, rather than being limited to coordinate
structures, strongly suggests that it is an instance of
a general syntactic priming mechanism, which has
been an established feature of accounts of the human
sentence production system for a while (e.g., Bock,
2The differences between the priors and adaptation proba-
bilities are also much smaller than noted by Church (2000). The
probabilities of the rules we investigate have a higher marginal
probability than the lexical items of interest to Church.
1986). This runs counter to the claims made by Fra-
zier et al (2000) and Frazier and Clifton (2001), who
have argued that parallelism only occurs in coordi-
nate structures, and should be accounted for using a
specialized copying mechanism. (It is important to
bear in mind, however, that Frazier et al only make
explicit claims about comprehension, not about pro-
duction.)
However, we also found that parallelism effects
are clearly strongest in coordinate structures (com-
pare the differences between prior and adaptation
in Figures 1?3 with those in Figures 4?9). This
could explain why Frazier et al?s (2000) experi-
ments failed to find a significant parallelism effect
in non-coordinated structures: the effect is simply
too week to detect (especially using the self-paced
reading paradigm they employed).
5 Experiment 3: Parallelism in
Spontaneous Dialog
Experiment 1 showed that parallelism effects can be
found not only in written corpora, but also in the
Switchboard corpus of spontaneous dialog. We did
not include Switchboard in our analysis in Experi-
ment 2, as this corpus has a different structure from
the two text corpora we investigated: it is organized
in terms of turns between two speakers. Here, we
exploit this property and conduct a further experi-
ment in which we compare parallelism effects be-
tween speakers and within speakers.
The phenomenon of structural repetition between
speakers has been discussed in the experimental
psycholinguistic literature (see Pickering and Gar-
rod 2004 for a review). According to Pickering
and Garrod (2004), the act of engaging in a dia-
log facilitates the use of similar representations at
all linguistic levels, and these representations are
shared between speech production and comprehen-
sion processes. Thus structural adaptation should be
observed in a dialog setting, both within and be-
tween speakers. An alternative view is that produc-
tion and comprehension processes are distinct. Bock
and Loebell (1990) suggest that syntactic priming
in speech production is due to facilitation of the
retrieval and assembly procedures that occur dur-
ing the formulation of utterances. Bock and Loebell
point out that this production-based procedural view
predicts a lack of priming between comprehension
and production or vice versa, on the assumption that
832
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 10: Adaptation between speakers in the
Switchboard corpus
production and parsing use distinct mechanisms. In
our terms, it predicts that between-speaker positive
adaptation should not be found, because it can only
result from priming from comprehension to produc-
tion, or vice versa. Conversely, the prodedural view
outlined by Bock and Loebell predicts that positive
adaptation should be found within a given speaker?s
dialog turns, because such adaptation can indeed be
the result of the facilitation of production routines
within a given speaker.
5.1 Method
We created two sets of prime and target data to
test within-speaker and between-speaker adaptation.
The prime and target sets were defined in terms of
pairs of utterances. To test between-speaker adapta-
tion, we took each adjacent pair of utterances spo-
ken by speaker A and speaker B, in each dialog, and
these were treated as prime and target sets respec-
tively. In the within-speaker analysis, the prime and
target sets were taken from the dialog turns of only
one speaker?we took each adjacent pair of dialog
turns uttered by a given speaker, excluding the in-
tervening utterance of the other speaker. The earlier
utterance of the pair was treated as the prime, and
the later utterance as the target. The remainder of
the method was the same as in Experiments 1 and 2
(see Section 3.1).
5.2 Results
The results for the between-speaker and within-
speaker adaptation are shown in Figure 10 and Fig-
ure 11 for same five phrase types as in the previous
experiments.
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 11: Adaptation within speakers in the Switch-
board corpus
A positive adaptation effect can be seen in the
between-speaker data. For each phrase type, the
adaptation probability is greater than the prior. In the
within-speaker data, by comparison, the magnitude
of the adaptation advantage is greatly decreased, in
comparison with Figure 10. Indeed, for most phrase
types, the adaptation probability is lower than the
prior, i.e., we have a case of negative adaptation.
5.3 Discussion
The results of the two analyses confirm that adap-
tation can indeed be found between speakers in di-
alog, supporting the results of experimental work
reviewed by Pickering and Garrod (2004). The re-
sults do not support the notion that priming is due
to the facilitation of production processes within a
given speaker, an account which would have pre-
dicted adaptation within speakers, but not between
speakers.
The lack of clear positive adaptation effects in
the within-speaker data is harder to explain?all
current theories of priming would predict some ef-
fect here. One possibility is that such effects may
have been obscured by decay processes: doing a
within-speaker analysis entails skipping an interven-
ing turn, in which priming effects were lost. We in-
tend to address these concerns using more elaborate
experimental designs in future work.
6 Conclusions
In this paper, we have demonstrated a robust, perva-
sive effect of parallelism for noun phrases. We found
the tendency for structural repetition in two different
corpora of written English, and also in a dialog cor-
833
pus. The effect occurs in a wide range of contexts:
within coordinate structures (Experiment 1), within
sentences for NPs in an arbitrary structural config-
uration, between sentences, and within documents
(Experiment 2). This strongly indicates that the par-
allelism effect is an instance of a general processing
mechanism, such as syntactic priming (Bock, 1986),
rather than specific to coordination, as suggested
by (Frazier and Clifton, 2001). However, we also
found that the parallelism effect is strongest in co-
ordinate structures, which could explain why com-
prehension experiments so far failed to demonstrate
the effect for other structural configurations (Frazier
et al, 2000). We leave it to future work to explain
why adaptation is much stronger in co-ordination:
is co-ordination special because of extra constrains
(i.e., some kind of expected contrast/comparison be-
tween co-ordinate sisters) or because of fewer con-
straints (i.e., both co-ordinate sisters have a similar
grammatical role in the sentence)?
Another result (Experiment 3) is that the paral-
lelism effect occurs between speakers in dialog. This
finding is compatible with Pickering and Garrod?s
(2004) interactive alignment model, and strengthens
the argument for parallelism as an instance of a gen-
eral priming mechanism.
Previous experimental work has found parallelism
effects, but only in comprehension data. The present
work demonstrates that parallelism effects also oc-
cur in production data, which raises an interesting
question of the relationship between the two data
types. It has been hypothesized that the human lan-
guage processing system is tuned to mirror the prob-
ability distributions in its environment, including the
probabilities of syntactic structures (Mitchell et al,
1996). If this tuning hypothesis is correct, then the
parallelism effect in comprehension data can be ex-
plained as an adaptation of the human parser to the
prevalence of parallel structures in its environment
(as approximated by corpus data) that we demon-
strated in this paper.
Note that the results in this paper not only have an
impact on theoretical issues regarding human sen-
tence processing, but also on engineering problems
in natural language processing, e.g., in probabilistic
parsing. To avoid sparse data problems, probabilistic
parsing models make strong independence assump-
tions; in particular, they generally assume that sen-
tences are independent of each other. This is partly
due to the fact it is difficult to parameterize the many
possible dependencies which may occur between
adjacent sentences. However, in this paper, we show
that structure re-use is one possible way in which
the independence assumption is broken. A simple
and principled approach to handling structure re-use
would be to use adaptation probabilities for prob-
abilistic grammar rules, analogous to cache proba-
bilities used in caching language models (Kuhn and
de Mori, 1990). We are currently conducting further
experiments to investigate of the effect of syntactic
priming on probabilistic parsing.
References
Bock, J. Kathryn. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology 18:355?387.
Bock, Kathryn and Helga Loebell. 1990. Framing sentences.
Cognition 35(1):1?39.
Branigan, Holly P., Marin J. Pickering, and Janet F. McLean.
2005. Priming prepositional-phrase attachment during com-
prehension. Journal of Experimental Psychology: Learning,
Memory and Cognition 31(3):468?481.
Carlson, Katy. 2002. The effects of parallelism and prosody on
the processing of gapping structures. Language and Speech
44(1):1?26.
Church, Kenneth W. 2000. Empirical estimates of adaptation:
the chance of two Noriegas is closer to p/2 than p2. In Pro-
ceedings of the 17th Conference on Computational Linguis-
tics. Saarbru?cken, Germany, pages 180?186.
Frazier, Lyn, Alan Munn, and Chuck Clifton. 2000. Processing
coordinate structures. Journal of Psycholinguistic Research
29(4):343?370.
Frazier, Lyn, Lori Taft, Tom Roeper, Charles Clifton, and Kate
Ehrlich. 1984. Parallel structure: A source of facilitation in
sentence comprehension. Memory and Cognition 12(5):421?
430.
Frazier, Lynn and Charles Clifton. 2001. Parsing coordinates
and ellipsis: Copy ?. Syntax 4(1):1?22.
Gries, Stefan T. 2005. Syntactic priming: A corpus-based ap-
proach. Journal of Psycholinguistic Research 35.
Kuhn, Roland and Renate de Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Transanctions
on Pattern Analysis and Machine Intelligence 12(6):570?
583.
Mauner, Gail, Michael K. Tanenhaus, and Greg Carlson. 1995.
A note on parallelism effects in processing deep and surface
verb-phrase anaphors. Language and Cognitive Processes
10:1?12.
Mitchell, Don C., Fernando Cuetos, Martin M. B. Corley, and
Marc Brysbaert. 1996. Exposure-based models of human
parsing: Evidence for the use of coarse-grained (non-lexical)
statistical records. Journal of Psycholinguistic Research
24(6):469?488.
Pickering, Martin J. and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and Brain Sci-
ences 27(2):169?225.
Szmrecsanyi, Benedikt. 2005. Creatures of habit: A corpus-
linguistic analysis of persistence in spoken English. Corpus
Linguistics and Linguistic Theory 1(1):113?149.
834
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 109?120, Dublin, Ireland, August 23-29 2014.
Query-by-Example Image Retrieval
using Visual Dependency Representations
Desmond Elliott, Victor Lavrenko and Frank Keller
Institute of Language, Communication, and Computation
School of Informatics
University of Edinburgh
d.elliott@ed.ac.uk {vlavrenk,keller}@inf.ed.ac.uk
Abstract
Image retrieval models typically represent images as bags-of-terms, a representation that is well-
suited to matching images based on the presence or absence of terms. For some information
needs, such as searching for images of people performing actions, it may be useful to retain data
about how parts of an image relate to each other. If the underlying representation of an image
can distinguish between images where objects only co-occur from images where people are in-
teracting with objects, then it should be possible to improve retrieval performance. In this paper
we model the spatial relationships between image regions using Visual Dependency Represen-
tations, a structured image representation that makes it possible to distinguish between object
co-occurrence and interaction. In a query-by-example image retrieval experiment on data set
of people performing actions, we find an 8.8% relative increase in MAP and an 8.6% relative
increase in Precision@10 when images are represented using the Visual Dependency Represen-
tation compared to a bag-of-terms baseline.
1 Introduction
Every day millions of people search for images on the web, both professionally and for personal amuse-
ment. The majority of image searches are aimed at finding a particular named entity, such as Justin
Bieber or supernova, and a typical image retrieval system is well-suited to this type of information need
because it represents an image as a bag-of-terms drawn from data surrounding the image, such as text,
manual tags, and anchor text (Datta et al., 2008). It is not always possible to find useful terms in the sur-
rounding data; the last decade has seen advances in automatic methods for assigning terms to images that
have neither user-assigned tags, nor a textual description (Duygulu et al., 2002; Lavrenko et al., 2003;
Guillaumin and Mensink, 2009). These automatic methods learn to associate the presence and absence
of labels with the visual characteristics of an image, such as colour and texture distributions, shape, and
points of interest, and can automatically generate a bag of terms for an unlabelled image.
It is important to remember that not all information needs are entity-based: people also search for im-
ages reflecting a mood, such as people having fun at a party, or an action, such as using a computer. The
bag-of-terms representation is limited to matching images based on the presence or absence of terms,
and not the relation of the terms to each other. Figures 1(a) and (b) highlight the problem with using
unstructured representations for image retrieval: there is a person and a computer in both images but only
(a) depicts a person actually using the computer. To address this problem with unstructured represen-
tations we propose to represent the structure of an image using the Visual Dependency Representation
(Elliott and Keller, 2013). The Visual Dependency Representation is a directed labelled graph over the
regions of an image that captures the spatial relationships between regions. The representation is inspired
by evidence from the psychology literature that people are better at recognising and searching for objects
when the spatial relationships between the objects in the image are consistent with our expectations of
the world.(Biederman, 1972; Bar and Ullman, 1996). In an automatic image description task, Elliott
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/
109
usingcomputer
ROOT Lamp Picture Girl Laptop Bed
beside
above
(a)
playinginstrument
ROOT Table Laptop Man Trumpet Boy
on beside
beside
(b)
using computer
ROOT Sofa Man Laptop Chair
beside
on
(c)
Figure 1: Three examples of images depicting a person and a computer, alongside a respective Visual
Dependency Representation for each image. The bag-of-terms representation can be observed in the
annotated regions of the Visual Dependency Representations. In (a) and (c) there is a person using a
laptop, whereas in (b) the man is actually using the trumpet. The gold-standard action annotation is
shown in the yellow bounding box.
and Keller (2013) showed that encoding the spatial relationships between objects in the Visual Depen-
dency Representation helped to generate significantly better descriptions than approaches based on the
spatial proximity of objects (Farhadi et al., 2010) or corpus-based models (Yang et al., 2011). In this
paper we study whether the Visual Dependency Representation of images can improve the performance
of query-by-example image retrieval models. The main finding is that encoding images using the Visual
Dependency Representation leads to significantly better retrieval accuracy compared to a bag-of-terms
baseline, and that the improvements are most pronounced for transitive verbs.
2 Related Work
2.1 Representing Images
A central problem in image retrieval is how to abstractly represent images (Datta et al., 2008). A bag-
of-terms representation of an image is created by grouping visual features, such as color, shape (Shi
and Malik, 2000), texture, and interest points (Lowe, 1999), in a vector or as a probability distribution
over the features. Image retrieval can then be performed by trying to find the best matchings of terms
across an image collection. Spatial Pyramid Matching is an approach to constructing low-level image
representations that capture the relationships between features at differently sized partitions of the im-
age (Lazebnik et al., 2006). This approach has proven successful for scene categorisation tasks. An
alternative approach to representing images is to learn a mapping (Duygulu et al., 2002; Lavrenko et al.,
110
2003; Guillaumin and Mensink, 2009) between the bags-of-terms and object tags. An image can then be
represented as a bag-of-terms and image retrieval is similar to text retrieval (Wu et al., 2012).
In this work, we represent an image as a directed acyclic graph over a set of labeled object region
annotations. This representation captures the important spatial relationships between the image regions
and makes it possible to distinguish between co-occurring regions and interacting regions.
2.2 Still-Image Action Recognition
One approach to recognizing actions is to learn appearance models for visual phrases and use these
models to predict actions (Sadeghi and Farhadi, 2011). A visual phrase is defined as the people and the
objects they interact with in an action. In this approach, a fixed number of visual phrase models are
trained using the deformable parts object detector (Felzenszwalb et al., 2010) and used to perform action
recognition.
An alternative approach is to model the relationships between objects in an image, and hence the
visible actions, as a Conditional Random Field (CRF), where each node in the field is an object and the
factors between nodes correspond to features that capture the relationships between the objects (Zitnick
et al., 2013). The factors between object nodes in the CRF include object occurrence, absolute position,
person attributes, and the relative location of pairs of objects. This model has been used to generate novel
images of people performing actions and to retrieve images of people performing actions.
Most recently, actions have been predicted in images by selecting the most likely verb and object pair
given a set of candidate objects detected in an image (Le et al., 2013a). The verb and object is selected
amongst those that maximize the distributional similarity of the pair in a large and diverse collection of
documents. This approach is most similar to ours but it relies on an external corpus and, depending on
the text collections used to train the distributional model, will compound the problem of co-occurrence
of objects instead of the relationships between the objects.
The work presented in this paper uses ground-truth annotation for region labels, an assumption similar
to (Zitnick et al., 2013), but requires no external data to make predictions of the relationships between
objects, unlike the approach of (Le et al., 2013a). The directed acyclic graph representation we propose
for images can be seen as a latent representation of the depicted action in the image, where the spatial
relationships between the regions capture the different types of actions.
3 Task and Baseline
In this paper we study the task of query-by-example image retrieval within the restricted domain of
images depicting actions. More specifically, given an image that depicts a given action, such as using a
computer, the aim of the retrieval model is to find all other images in the image collection that depict the
same action. We define an action as an event involving one or more entities in an image, e.g., a woman
running or boy using a computer, and assume all images have been manually annotated for objects. This
assumption means we can explore the utility of the Visual Dependency Representation without the noise
introduced by automatic computer vision methods. The data available to the retrieval models can be seen
in Figure 1, and Section 5 provides further details about the different sources of data The action label -
which is only used for evaluation - is shown in the labelled bounding box, and the Visual Dependency
Representation - not used by the baseline model - is shown as a tree at the bottom of the figure.
The main hypothesis explored in this paper is that the accuracy of an image retrieval model will
increase if the representation encodes information about the relationships between the objects in images.
This hypothesis is tested by encoding images as either an unstructured bag-of-terms representation or
as the structured Visual Dependency Representation. The Bag-of-Terms baseline represents the query
image and the image collection as an unstructured bags-of-terms vector. All of the models used to test
the main hypothesis use the cosine similarity function is to determine the similarity of the query image
to other images in the collection, and thus to generate a ranked list from the similarity values.
111
4 Visual Dependency Representation
The Visual Dependency Representation (VDR) is a structured representation of an image that captures the
spatial relationships between pairs of image regions in a directed labelled graph. The Visual Dependency
Grammar defines eight possible spatial relationships between pairs of regions, as shown in Table 1.
The relationships in the grammar were designed to provide sufficient coverage of the types of spatial
relationships required to describe the data, and are mathematically defined in terms of pixel overlap,
distance between regions, and the angle between regions. The frame of reference for annotating spatial
relationships is the image itself and not the object in the image, and angles and distance measurements
are taken or estimated from the centroids of the regions. The VDR of an image is created by a trained
human annotator in a two-stage process:
1. The annotator draws and labels boundaries around the parts of the image they think contribute to
defining the action depicted in the image, and the context within which the action occurs;
2. The annotator draws labelled directed edges between the annotated regions that captures how the
relationships between the image convey the action. In Section 4.1, we will explain how to automate
the second stage of the process from a collection of labelled region annotations.
In addition to the annotated image regions, a VDR also contains a ROOT node, which acts as a place-
holder for the image. In the remainder of this section we describe how a gold-standard VDR is created
by a human annotator. The starting point for the VDR in Figure 1(a) is the following set of regions and
the ROOT node:
ROOT Lamp Picture Girl Laptop Bed
First, the regions are attached to each other based on how the relationship between the objects con-
tributes to the depicted action. In Figure 1(a), the Girl is using the Laptop, therefore a labelled directed
edge is created from the Girl region to the Laptop region. The spatial relationship is labelled as BESIDE.
ROOT Lamp Picture Girl Laptop Bed
beside
The Girl is also attached to the Bed because the bed supports her body. The spatial relation label is
ABOVE because it expresses the spatial relationship between the regions, not the semantic relationship
ON. ROOT is attached to the Girl without an edge label to symbolize that she is an actor in the image.
ROOT Lamp Picture Girl Laptop Bed
beside
above
Now the regions that are not concerned with the depicted action are first attached to each other if there
is a clear spatial relationship between them (for an example, see Figure 1(b), where the laptop is attached
to the table because it is sitting on the table), and then to the ROOT node to signify that they do not play
a part in the depicted action. In this example, neither the Lamp nor the Picture are related to the action
of using the computer, so they are attached to the ROOT node.
112
X??
on Y
More than 50% of the pix-
els of region X overlap
with region Y.
X
????
beside Y
The angle between the cen-
troid of X and the centroid
of Y lies between 315
?
and
45
?
or 135
?
and 225
?
.
X
????
above Y
The angle between X and
Y lies between 225
?
and
315
?
.
X
?????
infront Y
The Z-plane relationship
between the regions is
dominant.
X
????????
surrounds Y
The entirety of region X
overlaps with region Y.
X
??????
opposite Y
Similar to beside, but
used when there X and
Y are at opposite sides
of the image.
X
????
below Y
The angle between X
and Y lies between 45
?
and 135
?
.
X
?????
behind Y
Identical to infront ex-
cept X is behind Y in the
Z-plane.
Table 1: Visual Dependency Grammar defines eight relations between pairs of annotated regions. To
simplify explanation, all regions are circles, where X is the grey region and Y is the white region. All
relations are considered with respect to the centroid of a region and the angle between those centroids.
ROOT Lamp Picture Girl Laptop Bed
beside
above
This now forms a completed VDR for the image in Figure 1(a). This structured representation of
an image captures the prominent relationship between the girl, the laptop, and the bed. There is no
prominent relationship defined between the girl and either the lamp of the picture, in effect these regions
have been relegated to background objects. The central hypothesis underpinning the Visual Dependency
Representation is that images that contain similar VDR substructures are more likely to depict the same
action than images that only contain the same set of objects. For example, the VDR for Figure 1(a)
correctly captures the relationship between the people and the laptops, whereas this relationship is not
present in Figure 1(b), where the person is playing a trumpet.
4.1 Predicting Visual Dependency Representations
We follow the approach of Elliott and Keller (2013) and predict the VDR y of an image over a collection
of labelled region annotations x. This task is framed as a supervised learning problem, where the aim is
to construct a Maximum Spanning Tree from a fully-connected directed weighted graph over the labelled
regions (McDonald et al., 2005). Reducing the fully-connected graph to the Maximum Spanning Tree
removes the region?region edges that are not important in defining the prominent relationships between
the regions in an image. The score of the VDR y over the image regions is calculated as the sum of the
scores of the directed labelled edges:
score(x, y) =
?
(a,b)?y
w ? f(a, b) (1)
where the score of an edge between image regions a and b is calculated using a vector of weighted feature
functions f . The feature functions characterize the image regions and the edge between pairs of regions,
and include: the labels of the regions and the spatial relation annotated on the edge; the (normalized)
distance between the centroids of the regions; the angle formed between the annotated regions, which is
113
mapped onto the set of spatial relations; the relative size of the region compared to the image; and the
distance of the region centroid from the center of the image.
The model is trained over i instances of region-annotated images x
i
associated with human-created
VDR structures y
i
, I
train
= {x
i
, y
i
}. The score of each edge a, b is calculated by applying the feature
functions to the data associated with that edge, and this is performed over each edge in a VDR to obtain
a score for a complete gold-standard structure. The parameters of the weight vector w are iteratively
adjusted to maximise the score of the gold-standard structures in the training data using the Margin
Infused Relaxation Algorithm (Crammer and Singer, 2002).
The test data contains i instances of region-annotated images with image regions x
i
, I
test
= {x
i
}.
The parsing model computes the highest scoring structure y?
i
for each instance in the test data by scoring
each possible directed edge between pairs of regions in x
i
. This process forms a fully-connected graph
over the image regions, from which the Maximum Spanning Tree is taken and returned as the predicted
VDR.
We evaluate the performance of this VDR prediction model by comparing how well it can recover
the manually created trees in the data set. This evaluation is performed on the development data in a
10-fold cross validation setting where each fold of the data is split 80%/10%/10%. Unlabelled directed
accuracy means the model correctly proposes an edge between a pair of regions in the correct direction;
Labelled directed accuracy means it additionally proposes the correct edge label. The baseline approach
is to assume no latent image structure and attach all image regions to the ROOT node of the VDR; this
achieves 51.6% labelled and unlabelled directed attachment accuracy. The accuracy of our automatic
approach to VDR prediction is 61.3% labelled and 68.8% unlabelled attachment accuracy.
4.2 Comparing Visual Dependency Representations
It remains to define how to compare the Visual Dependency Representation of a pair of images. The most
obvious approach is to use the labelled directed accuracy measurement used for the VDR prediction
evaluation in the previous section, but we did not find significant improvements in retrieval accuracy
using this method. We hypothesise that the lack of weight given to the edges between nodes in the Visual
Dependency Representation results in this comparison function not distinguishing between object?object
relationships that matter, such as PERSON
?????
beside BIKE, compared to ROOT ?? TREES. The former
is a potential person?object relationship that explains the depicted event, whereas the latter is only a
background object.
The approach we adopted in this paper is to compare Visual Dependency Representations of images
by decomposing the structure into a set of labelled and a unlabelled parent?child subtrees in a depth-first
traversal of the VDR. The decomposition process allows use to use the same similarity function as the
Bag-of-Terms baseline model, removing the confound of choosing different similarity functions. The
subtrees can be transformed into tokens and these tokens can be used as weighted terms in a vector
representation. An example of a labelled transformation is shown below:
Girl Bed ? Girl above Bed
above
We now demonstrate the outcome of comparing images represented using either a vector that con-
catenates the decomposed transformed VDR and bag-of-terms, or a vector that contains only the bag-of-
terms. In this demonstration, each term has a tf-idf weight of 1. The first illustration (Similar) compares
images that depict the same underlying action: Figure 1 (a) and (c). The second illustration (Dissimilar)
compares images that depict different actions: Figure 1 (a) and (b).
Similar : cos(VDR
a
,VDR
c
) = 0.56 > cos(Bag
a
,Bag
c
) = 0.52
Dissimilar : cos(VDR
b
,VDR
a
) = 0.201 cos(Bag
b
,Bag
a
) = 0.4
It can be seen that when the images represent the same action, the decomposed VDR increases the
similarity of the pair of images compared to the bag-of-terms representation; and when images do not
114
represent the same action, the decomposed VDR yields a lower similarity than the bag-of-terms repre-
sentation. These illustrations confirm that Visual Dependency Representations can be used to distinguish
the difference between presence or absence of objects, and the prominent relationships between objects.
5 Data
We use an existing dataset of VDR-annotated images to study whether modelling the structure of an
image can improve image retrieval in the domain of action depictions. The data set of Elliott and Keller
(2013) contains 341 images annotated with region annotations, three visual dependency representations
per image (making a total of 1,023 instances), and a ground-truth action label for each image. An
example of the annotations can be seen in Figure 1. The image collection is drawn from the PASCAL
Visual Object Classification Challenge 2011 action recognition taster and covers a set of 10 actions
(Everingham et al., 2011): riding a bike, riding a horse, reading, running, jumping, walking, playing an
instrument, using a computer, taking a photo, and talking on the phone.
Image Descriptions
Each image is associated with three human-written descriptions collected from untrained annotators
on Amazon Mechanical Turk. The descriptions do not form any part of the models presented in the
current paper; they were used in the automatic image description task of Elliott and Keller (2013). Each
description contains two sentences: the first sentence describes the action depicted in the image, and
the second sentence describes other objects not involved in the action. A two sentence description of
an image helps distinguish objects that are central to depicting the action from objects that may be
distractors.
Region Annotations
The images contain human-drawn labelled region annotations. The annotations were drawn using the
LabelMe toolkit, which allows for arbitrary labelled polygons to be created over an image (Russell
et al., 2008). The annotated regions were restricted to those present in at least one of three human-
written descriptions. To reduce the effects of label sparsity, frequently occurring equivalent labels were
conflated, i.e., man, child, and boy? person; bike, bicycle, motorbike? bike; this reduced the object
label vocabulary from 496 labels to 362 labels. The data set contains a total of 5,034 region annotations,
with a mean of 4.19 ? 1.94 annotations per image.
Visual Dependency Representations
Recall that each image is associated with three descriptions, and that people were free to decide how to
describe the action and background of the image. The differences between how people describe images
leads to the creation of one Visual Dependency Representation per image?description pair in the data
set, resulting in a total of 1,023 instances. The process for creating a visual dependency representation
of an image is described in Section 4. The annotated dataset comprises a total of 5,748 spatial relations,
corresponding to a mean of 4.79 ? 3.51 relations per image. Elliott and Keller (2013) report inter-
annotator agreement on a subset of the data at 84% agreement for labelled directed attachments and
95.1% for unlabelled directed attachments.
Action Labels
The original PASCAL action recognition dataset contains ground truth action class annotations for each
image. These annotations are in the form of labelled bounding boxes around the person performing the
action in the image. The action labels are only used as the gold-standard relevance judgements for the
query-by-example image retrieval experiments.
6 Experiments
In this section we present the results of a query-by-example image retrieval experiment to determine
the utility of the Visual Dependency Representation compared to a bag-of-terms representation. In this
115
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.0
0.2
0.4
0.6
0.8
1.0
Prec
ision
Manual VDRAutomatic VDRBag-of-Terms
Figure 2: Average 11-point precision/recall curves show that the VDR-based retrieval models are con-
sistently better than the Bag-of-Terms model.
experiment, a single image (the query image) is used to rank the images in the test collection, where the
goal is to construct a ranking where the top images depict the same action as the query image.
6.1 Protocol
The image retrieval experiment is performed using 10-fold cross-validation in the following manner.
The 341 images in the dataset are randomly partitioned into 80%/10%/10% splits, resulting in 1011 test
queries
1
. For each query we compute average precision and Precision@10 of the ranked list, and use the
resulting values to test the statistical significance of the results.
The training set is used to train the VDR prediction model and to estimate inverse document frequency
statistics. During the training phase, the VDR-based models have access to region boundaries, region
labels and three manually-created VDRs for each training image. In the test set, all models have access to
the region boundaries and labels for each image. Each image in the test set forms a query and the models
produce a ranked list of the remaining images in the test collection. Images are marked for relevance
as follows: a image at rank r is considered relevant if it has the same action label as the query image;
otherwise it is non-relevant. The dev set was used to experiment with different matching functions and
to optimise the feature functions used in the VDR prediction model.
6.2 Models
We compare the retrieval accuracy of three approaches: Bag-of-Terms uses an unstructured representa-
tion for each image. A tf-idf weight is assigned to each region label in an image, and the cosine measure
is used to calculate the similarity of images. This model allows us to compare the usefulness of a struc-
tured vs. unstructured image representation. Automatic VDR is a model using the VDR prediction
method from Section 4.1, and Manual VDR uses the gold-standard data described in Section 5. Both
1
Recall there are three Visual Dependency Representations for each image. The partitions are the same as those used in the
VDR prediction experiment in Section 4.1
116
MAP P@10
Manual VDR 0.514
??
0.454
?
Automatic VDR 0.508
?
0.451
?
Bag-of-Terms 0.467 0.415
Table 2: Overall Mean Average Precision and Precision@10 images. The VDR-based models are sig-
nificantly better than the Bag-of-Terms model, supporting the hypothesis that modelling the structure
of an image using the Visual Dependency Representation is useful for image retrieval. ?: significantly
different than Bag-of-Terms at p < 0.01; ?: significantly different than Automatic VDR at p < 0.01.
of the VDR-based models have a tf-idf weight assigned to the transformed decomposed terms and the
cosine similarity measure is used to calculate the similarity of images.
6.3 Results
Figure 2(a) shows the interpolated precision/recall curve and Table 2 shows the Mean Average Precision
(MAP) and Precision at 10 retrieved images (P@10). The MAP of the Automatic VDR model increases
by 8.8% relative to the Bag-of-Terms model, and a relative improvement up to 10.1% would possible if
we had a better structure prediction model, as evidenced by Manual VDR. Furthermore, if we assume a
user will only view the top results returned by the retrieval model, then P@10 increases by 8.6% when we
model the structure of an image, relative to using an unstructured representation; a relative improvement
of up to 9.4% would be possible if we had a better image parser.
To determine whether the differences are statistically significant, we perform the Wilcoxon Signed
Ranks Test on the average precision and P@10 values over the 1011 queries in our cross-validation
data set. The results support the main hypothesis of this paper: structured image representations allow
us to find images depicting actions more accurately than the standard bag-of-terms representation. We
find significant differences in average precision and P@10 between the Bag-of-Terms baseline and both
Automatic VDR (p < 0.01) and Manual VDR (p < 0.01). This suggests that structure is very useful in
the query-by-example scenario. We find a significant difference in average precision between Automatic
VDR and Manual VDR (p < 0.01), but no difference in P@10 between Automatic VDR and Manual
VDR (p = 0.442).
6.4 Retrieval Performance by Type of Action and Verb
We now analyse whether image structure is useful when the action does not require a direct object. The
analysis presented here compares the Bag-of-Terms model against the Automatic VDR model because
there was no significant difference in P@10 between the Automatic and Manual VDR models. Table 3
shows the MAP and Precision@10 per type of action. Figure 3 shows the precision/recall curves for (a)
transitive verbs, (b) intransitive verbs, and (c) light verbs.
In Figure 3(a), it can be seen that the actions that can be classified as transitive verbs benefit from
exploiting the structure encoded in the Visual Dependency Representation. The only exception is for the
action to read, which frequently behaves as an intransitive verb: the man reads on a train. The consistent
improvement in both the entirety of the ranked list and at the top of the ranked list can be seen in the
MAP and P@10 results in Table 3.
Figure 3(b) shows that there is a small increase in retrieval performance for intransitive verbs compared
to the transitive verbs. We conjecture this is because there are fewer objects to annotate in an image when
the verb does not require a direct object. The summary results for the intransitive verbs in Table 3 confirm
the small but insignificant increase in MAP and P@10.
Finally, the light verbs, shown in Figure 3(c), exhibit variable behaviour in retrieval performance. One
reason for this could be that if the light verb encodes information about the object, as in using a computer,
then the computer can be annotated in the image, and thus it acts as a transitive verb. Conversely, when
117
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0
ride horseride bikeread
(a)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0 jumpwalkrun
(b)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0 use computertake photo
(c)
Figure 3: Precision/recall curves grouped by the type of verb. The solid lines represent the Automatic
VDR model; the dashed lines represent the Bag-of-Terms model; y-axis is Precision, and the x-axis is
Recall. (a) Images depicting transitive verbs benefit the most from the Visual Dependency Representation
and are easiest to retrieve. (b) Intransitive verbs are difficult to retrieve and there is is a negligible
improvement in performance when using Visual Dependency Representation. (c) Light verbs benefit
from the Visual Dependency Representation depending on the type of the object involved in the action.
MAP P@10
VDR Bag VDR Bag
Ride bike 0.721
?
0.601 0.596
?
0.513
Ride horse 0.833
?
0.768 0.787
?
0.726
Talk on phone 0.762
?
0.679 0.666
?
0.582
Play instrument 0.774
?
0.705 0.634
?
0.586
Read 0.483 0.454 0.498 0.475
Walk 0.198 0.186 0.184 0.174
Run 0.193 0.165 0.151 0.132
Jump 0.211 0.189 0.142 0.136
Use computer 0.814
?
0.761 0.694
?
0.648
Take photo 0.241 0.223 0.212 0.198
Table 3: Mean Average Precision and Precision@10 for each action in the data set, grouped into transitive
(top), intransitive (middle), and light (bottom) verbs. VDR is the Automatic VDR model and Bag is the
Bag-of-Terms model. It can be seen that the Automatic VDR retrieval model is consistently better than
the Bag-of-Terms model on both MAP and Precision@10. ?: the Automatic VDR model is significantly
different than Bag-of-Terms at p < 0.01.
118
the light verb conveys information about the outcome of the event, as in the action take a photograph,
the outcome is rarely possible to annotate in an image, and so no improvements can be gained from
structured image representations.
6.5 Discussion
In our experiments we observed that all models can achieve high precision at very low levels of recall. We
found that this happens for testing images that are almost identical to the query image. For such images,
objects that are unrelated to the target action form an effective context, which allows this image to be
placed at the top of the ranking. However, near-identical images are relatively rare, and performance
degrades for higher levels of recall.
It is surprising that image retrieval using automatically predicted VDR model is statistically indistin-
guishable from the manually crafted VDR model, given the relatively low accuracy of our VDR predic-
tion model: 61.3% by the labelled dependency attachment accuracy measure. One possible explanation
could be that not all parts of the VDR structure are useful for retrieval purposes, and our VDR prediction
model does well on the useful ones. This observation also suggests that we are unlikely to achieve better
retrieval performance by continuing to improve the accuracy of VDR prediction. We believe a more
promising direction is refining the current formulation of the VDR, and exploring more sophisticated
ways to measure the similarity of two structured representations.
7 Conclusion
In this paper we argued that a limiting factor of retrieving images depicting actions is the unstructured
bag-of-terms representation typically used for images. In a bag-of-terms representation, images that
share similar sets of regions are deemed to be related even when the depicted actions are different. We
proposed that representing an image using the Visual Dependency Representation (VDR) can prevent
this type of misclassification in image retrieval. The VDR of an image captures the region?region re-
lationships that explain what is happening in an image, and it can be automatically predicted from a
region-annotated image.
In a query-by-example image retrieval task, we found that representing images as automatically pre-
dicted VDRs resulted in statistically significant 8.8% relative improvement in MAP and 8.6% relative
improvement in Precision@10 compared to a Bag-of-Terms model. There was a significant difference
in MAP when using manually or automatically predicted image structures, but no difference in the Pre-
cision@10, suggesting that the proposed automatic prediction model is accurate enough for retrieval
purposes. Future work will focus on using automatically generated visual input, such as the output of
the image tagger (Guillaumin and Mensink, 2009), or an automatic object detector (Felzenszwalb et al.,
2010), which will make it possible to tackle image ranking tasks (Hodosh et al., 2013). It would also be
interesting to explore alternative structure prediction methods, such as predicting the relationships using
a conditional random field (Zitnick et al., 2013), or by leveraging distributional lexical semantics (Le et
al., 2013b).
Acknowledgments
The anonymous reviewers provided valuable feedback on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.
References
Moshe Bar and Shimon Ullman. 1996. Spatial Context in Recognition. Perception, 25(3):343?52, January.
I Biederman. 1972. Perceiving real-world scenes. Science, 177(4043):77?80.
Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265?292.
119
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang. 2008. Image retrieval: Ideas, influences, and trends of
the new age. ACM Computing Surveys, 40(2):1?60.
P Duygulu, Kobus Barnard, J F G de Freitas, and David A Forsyth. 2002. Object Recognition as Machine
Translation: Learning a Lexicon for a Fixed Image Vocabulary. In Proceedings of the 7th European Conference
on Computer Vision, pages 97?112, Copenhagen, Denmark.
Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Pro-
ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,
Seattle, Washington, U.S.A.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2011. The
PASCAL Visual Object Classes Challenge 2011.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and
David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 15th
European Conference on Computer Vision, pages 15?29, Heraklion, Crete, Greece.
P F Felzenszwalb, R B Girshick, D McAllester, and D Ramanan. 2010. Object Detection with Discriminatively
Trained Part-Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627?
1645.
Matthieu Guillaumin and Thomas Mensink. 2009. Tagprop: Discriminative metric learning in nearest neighbor
models for image auto-annotation. In IEEE 12th International Conference on Computer Vision, pages 309?316,
Kyoto, Japan.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data,
Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853?899.
Victor Lavrenko, R Manmatha, and Jiwoon Jeon. 2003. A Model for Learning the Semantics of Pictures. In
Advances in Neural Information Processing Systems 16, Vancouver and Whistler, British Columbia, Canada.
S. Lazebnik, C. Schmid, and J. Ponce. 2006. Beyond Bags of Features: Spatial Pyramid Matching for Recog-
nizing Natural Scene Categories. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pages 2169?2178, New York, NY, USA.
DT Le, R Bernardi, and Jasper Uijlings. 2013a. Exploiting language models to recognize unseen actions. In
Proceedings of the International Conference on Multimedia Retrieval, pages 231?238, Dallas, Texas, U.S.A.
DT Le, Jasper Uijlings, and Raffaella Bernardi. 2013b. Exploiting language models for visual recognition. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,
Seattle, Washington, U.S.A.
D G Lowe. 1999. Object recognition from local scale-invariant features. In Proceedings of the International
Conference on Computer Vision, pages 1150?1157, Washington, D.C., USA.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 523?530, Vancouver, British Columbia, Canada.
Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and William T. Freeman. 2008. LabelMe: A Database and
Web-Based Tool for Image Annotation. International Journal of Computer Vision, 77(1-3):157?173.
Mohammad A Sadeghi and Ali Farhadi. 2011. Recognition Using Visual Phrases. In 2011 IEEE Conference on
Computer Vision and Pattern Recognition, pages 1745?1752, Colorado Springs, Colorado, U.S.A.
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888?905, August.
Lei Wu, Rong Jin, and Anil K Jain. 2012. Tag Completion for Image Retrieval. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(3):716?727.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation
of Natural Images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 444?454, Edinburgh, Scotland, UK.
CL Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the Visual Interpretation of Sentences. In IEEE
International Conference on Computer Vision, pages 1681?1688, Sydney, Australia.
120
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 304?312,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Model of Discourse Predictions in Human Sentence Processing
Amit Dubey and Frank Keller and Patrick Sturt
Human Communication Research Centre, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{amit.dubey,frank.keller,patrick.sturt}@ed.ac.uk
Abstract
This paper introduces a psycholinguistic
model of sentence processing which combines
a Hidden Markov Model noun phrase chun-
ker with a co-reference classifier. Both mod-
els are fully incremental and generative, giv-
ing probabilities of lexical elements condi-
tional upon linguistic structure. This allows
us to compute the information theoretic mea-
sure of surprisal, which is known to correlate
with human processing effort. We evaluate
our surprisal predictions on the Dundee corpus
of eye-movement data show that our model
achieve a better fit with human reading times
than a syntax-only model which does not have
access to co-reference information.
1 Introduction
Recent research in psycholinguistics has seen a
growing interest in the role of prediction in sentence
processing. Prediction refers to the fact that the hu-
man sentence processor is able to anticipate upcom-
ing material, and that processing is facilitated when
predictions turn out to be correct (evidenced, e.g.,
by shorter reading times on the predicted word or
phrase). Prediction is presumably one of the factors
that contribute to the efficiency of human language
understanding. Sentence processing is incremental
(i.e., it proceeds on a word-by-word basis); there-
fore, it is beneficial if unseen input can be antici-
pated and relevant syntactic and semantic structure
constructed in advance. This allows the processor to
save time and makes it easier to cope with the con-
stant stream of new input.
Evidence for prediction has been found in a range
of psycholinguistic processing domains. Semantic
prediction has been demonstrated by studies that
show anticipation based on selectional restrictions:
listeners are able to launch eye-movements to the
predicted argument of a verb before having encoun-
tered it, e.g., they will fixate an edible object as soon
as they hear the word eat (Altmann and Kamide,
1999). Semantic prediction has also been shown in
the context of semantic priming: a word that is pre-
ceded by a semantically related prime or by a seman-
tically congruous sentence fragment is processed
faster (Stanovich and West, 1981; Clifton et al,
2007). An example for syntactic prediction can be
found in coordinate structures: readers predict that
the second conjunct in a coordination will have the
same syntactic structure as the first conjunct (Fra-
zier et al, 2000). In a similar vein, having encoun-
tered the word either, readers predict that or and a
conjunct will follow it (Staub and Clifton, 2006).
Again, priming studies corroborate this: Compre-
henders are faster at naming words that are syntacti-
cally compatible with prior context, even when they
bear no semantic relationship to it (Wright and Gar-
rett, 1984).
Predictive processing is not confined to the sen-
tence level. Recent experimental results also provide
evidence for discourse prediction. An example is the
study by van Berkum et al (2005), who used a con-
text that made a target noun highly predictable, and
found a mismatch effect in the ERP (event-related
brain potential) when an adjective appeared that was
inconsistent with the target noun. An example is (we
give translations of their Dutch materials):
(1) The burglar had no trouble locating the secret
family safe.
a. Of course, it was situated behind a
304
bigneu but unobtrusive paintingneu.
b. Of course, it was situated behind a
bigcom but unobtrusive bookcasecom.
Here, the adjective big, which can have neutral or
common gender in Dutch, is consistent with the pre-
dicted noun painting in (1-a), but inconsistent with it
in (1-b), leading to a mismatch ERP on big in (1-b)
but not in (1-a).
Previous results on discourse effects in sentence
processing can also be interpreted in terms of pre-
diction. In a classical paper, Altmann and Steed-
man (1988) demonstrated that PP-attachment pref-
erences can change through discourse context: if the
context contains two potential referents for the tar-
get NP, then NP-attachment of a subsequent PP is
preferred (to disambiguate between the two refer-
ents), while if the context only contains one target
NP, VP-attachment is preferred (as there is no need
to disambiguate). This result (and a large body of
related findings) is compatible with an interpretation
in which the processor predicts upcoming syntactic
attachment based on the presence of referents in the
preceding discourse.
Most attempts to model prediction in human lan-
guage processing have focused on syntactic pre-
diction. Examples include Hale?s (2001) surprisal
model, which relates processing effort to the con-
ditional probability of the current word given the
previous words in the sentence. This approach has
been elaborated by Demberg and Keller (2009) in a
model that explicitly constructs predicted structure,
and includes a verification process that incurs ad-
ditional processing cost if predictions are not met.
Recent work has attempted to integrate semantic
and discourse prediction with models of syntactic
processing. This includes Mitchell et al?s (2010)
approach, which combines an incremental parser
with a vector-space model of semantics. However,
this approach only provides a loose integration of
the two components (through simple addition of
their probabilities), and the notion of semantics used
is restricted to lexical meaning approximated by
word co-occurrences. At the discourse level, Dubey
(2010) has proposed a model that combines an incre-
mental parser with a probabilistic logic-based model
of co-reference resolution. However, this model
does not explicitly model discourse effects in terms
of prediction, and again only proposes a loose in-
tegration of co-reference and syntax. Furthermore,
Dubey?s (2010) model has only been tested on two
experimental data sets (pertaining to the interaction
of ambiguity resolution with context), no broad cov-
erage evaluation is available.
The aim of the present paper is to overcome these
limitations. We propose a computational model that
captures discourse effects on syntax in terms of pre-
diction. The model comprises a co-reference com-
ponent which explicitly stores discourse mentions
of NPs, and a syntactic component which adjust
the probabilities of NPs in the syntactic structure
based on the mentions tracked by the discourse com-
ponent. Our model is HMM-based, which makes
it possible to efficiently process large amounts of
data, allowing an evaluation on eye-tracking cor-
pora, which has recently become the gold-standard
in computational psycholinguistics (e.g., Demberg
and Keller 2008; Frank 2009; Boston et al 2008;
Mitchell et al 2010).
The paper is structured as follows: In Section 2,
we describe the co-reference and the syntactic mod-
els and evaluate their performance on standard data
sets. Section 3 presents an evaluation of the overall
model on the Dundee eye-tracking corpus. The pa-
per closes with a comparison with related work and
a general discussion in Sections 4 and 5.
2 Model
This model utilises an NP chunker based upon a hid-
den Markov model (HMM) as an approximation to
syntax. Using a simple model such as an HMM fa-
cilitates the integration of a co-reference component,
and the fact that the model is generative is a prereq-
uisite to using surprisal as our metric of interest (as
surprisal require the computation of prefix probabil-
ities). The key insight in our model is that human
sentence processing is, on average, facilitated when
a previously-mentioned discourse entity is repeated.
This facilitation depends upon keeping track of a list
of previously-mentioned entities, which requires (at
the least) shallow syntactic information, yet the fa-
cilitation itself is modeled primarily as a lexical phe-
nomenon. This allows a straightforward separation
of concerns: shallow syntax is captured using the
HMM?s hidden states, whereas the co-reference fa-
305
cilitation is modeled using the HMM?s emissions.
The vocabulary of hidden states is described in Sec-
tion 2.1 and the emission distribution in Section 2.2
2.1 Syntactic Model
A key feature of the co-reference component of our
model (described below) is that syntactic analysis
and co-reference resolution happen simultaneously.
This could potentially slow down the syntactic anal-
ysis, which tends to already be quite slow for ex-
haustive surprisal-based incremental parsers. There-
fore, rather than using full parsing, we use an HMM-
based NP chunker which allows for a fast analysis.
NP chunking is sufficient to extract NP discourse
mentions and, as we show below, surprisal values
computed using HMM chunks provide a useful fit
on the Dundee eye-movement data.
To allow the HMM to handle possessive construc-
tions as well as NP with simple modifiers and com-
plements, the HMM decodes NP subtrees with depth
of 2, by encoding the start, middle and end of a
syntactic category X as ?(X?, ?X? and ?X)?, respec-
tively. To reduce an explosion in the number of
states, the category begin state ?(X? only appears at
the rightmost lexical token of the constituent?s left-
most daughter. Likewise, ?X)? only appears at the
leftmost lexical token of the constituent?s rightmost
daughter. An example use of this state vocabulary
can be seen in Figure 1. Here, a small degree of re-
cursion allows for the NP ((new york city?s) general
obligation fund) to be encoded, with the outer NP?s
left bracket being ?announced? at the token ?s, which
is the rightmost lexical token of the inner NP. Hid-
den states also include part-of-speech (POS) tags,
allowing simultaneous POS tagging. In the exam-
ple given in Figure 1, the full state can be read by
listing the labels written above a word, from top to
bottom. For example, the full state associated with
?s is (NP-NP)-POS. As ?s can also be a contraction
of is, another possible state for ?s is VBZ (without
recursive categories as we are only interested in NP
chunks).
The model uses unsmoothed bi-gram transition
probabilities, along with a maximum entropy dis-
tribution to guess unknown word features. The re-
sulting distribution has the form P(tag|word) and is
therefore unsuitable for computing surprisal values.
However, using Bayes? theorem we can compute:
P(word|tag) = P(tag|word)P(word)P(tag) (1)
which is what we need for surprisal. The pri-
mary information from this probability comes from
P(tag|word), however, reasonable estimates of
P(tag) and P(word) are required to ensure the prob-
ability distribution is proper. P(tag) may be esti-
mated on a parsed treebank. P(word), the probabil-
ity of a particular unseen word, is difficult to esti-
mate directly. Given that our training data contains
approximately 106 words, we assume that this prob-
ability must be bounded above by 10?6. As an ap-
proximation, we use this upper bound as the proba-
bility of P(word).
Training The chunker is trained on sections 2?
22 of the Wall Street Journal section of the Penn
Treebank. CoNLL 2000 included chunking as a
shared task, and the results are summarized by Tjong
Kim Sang and Buchholz (2000). Our chunker is not
comparable to the systems in the shared task for sev-
eral reasons: we use more training data, we tag si-
multaneously (the CoNLL systems used gold stan-
dard tags) and our notion of a chunk is somewhat
more complex than that used in CoNLL. The best
performing chunker from CoNLL 2000 achieved an
F-score of 93.5%, and the worst performing system
an F-score of 85.8%. Our chunker achieves a com-
parable F-score of 85.5%, despite the fact that it si-
multaneously tags and chunks, and only uses a bi-
gram model.
2.2 Co-Reference Model
In a standard HMM, the emission probabilities are
computed as P(wi|si) where wi is the ith word and si
is the ith state. In our model, we replace this with a
choice between two alternatives:
P(wi|si) =
{ ?Pseen before(wi|si)
(1??)Pdiscourse new(wi|si) (2)
The ?discourse new? probability distribution is the
standard HMM emission distribution. The ?seen be-
fore? distribution is more complicated. It is in part
based upon caching language models. However, the
contents of the cache are not individual words but
306
(NP NP NP NP)
(NP NP) (NP NP NP NP) NP (NP NP NP)
JJ NN IN NNP NNP NNP POS JJ NN NNS VBN RP DT NN NN
strong demand for new york city ?s general obligation bonds propped up the municipal market
Figure 1: The chunk notation of a tree from the training data.
Variable Type
l, l? List of trie nodes
w,wi Words
t Tag
n,n? Trie nodes
l? List(root of mention trie)
for w? w0 to wn do
l?? l
l? /0
Clear tag freq array f t
Clear word freq array f wt
for t ? tag set do
for n ? l? do
f t(t)? f t(t)+FreqO f (n, t)
n?? Getchild(w, t)
if n? 6= /0 then
f wt(t)? f wt(t)+FreqO f (n?,w, t)
l? n? :: l
end if
end for
end for
Pseen before(w|t) = f t(t)/ f wt(t)
end for
Figure 2: Looking up entries from the NP Cache
rather a collection of all NPs mentioned so far in the
document.
Using a collection of NPs rather than individual
words complicates the decoding process. If m is the
size of a document, and n is the size of the current
sentence, decoding occurs in O(mn) time as opposed
to O(n), as the collection of NPs needs to be ac-
cessed at each word. However, we do not store the
NPs in a list, but rather a trie. This allows decoding
to occur in O(n logm) time, which we have found
to be quite fast in practise. The algorithm used to
keep track of currently active NPs is presented in
Figure 2. This shows how the distribution Pseen before
is updated on a word-by-word basis. At the end of
each sentence, the NPs of the Viterbi parse are added
to the mention trie after having their leading arti-
cles stripped. A weakness of the algorithm is that
mentions are only added on a sentence-by-sentence
basis (disallowing within-sentence references). Al-
though the algorithm is intended to find whole-string
matches, in practise, it will count any NP whose pre-
fix matches as being co-referent.
A consequence of Equation 2 is that co-reference
resolution is handled at the same time as HMM de-
coding. Whenever the ?seen before? distribution is
applied, an NP is co-referent with one occurring ear-
lier. Likewise, whenever the ?discourse new? dis-
tribution is applied, the NP is not co-referent with
any NP appearing previously. As one choice or the
other is made during decoding, the decoder there-
fore also selects a chain of co-referent entities. Gen-
erally, for words which have been used in this dis-
course, the magnitude of probabilities in the ?seen
before? distribution are much higher than in the ?dis-
course new? distribution. Thus, there is a strong
bias to classify NPs which match word-for-word as
being co-referent. There remains a possibility that
the model primarily captures lexical priming, rather
than co-reference. However, we note that string
match is a strong indicator of two NPs being corefer-
307
ent (cf. Soon et al 2001), and, moreover, the match-
ing is done on an NP-by-NP basis, which is more
suitable for finding entity coreference, rather than a
word-by-word basis, which would be more suitable
for lexical priming.
An appealing side-effect of using a simple co-
reference decision rule which is applied incremen-
tally is that it is relatively simple to incremen-
tally compute the transitive closure of co-reference
chains, resulting in the entity sets which are then
used in evaluation.
The co-reference model only has one free param-
eter, ?, which is estimated from the ACE-2 corpus.
The estimate is computed by counting how often a
repeated NP actually is discourse new. In the current
implementation of the model, ? is constant through-
out the test runs. However, ? could possibly be
a function of the previous discourse, allowing for
more complicated classification probabilities.
3 Evaluation
3.1 Data
Our evaluation experiments were conducted upon
the Dundee corpus (Kennedy et al, 2003), which
contains the eye-movement record of 10 participants
each reading 2,368 sentences of newspaper text.
This data set has previously been used by Demberg
and Keller (2008) and Frank (2009) among others.
3.2 Evaluation
Eye tracking data is noisy for a number of rea-
sons, including the fact that experimental partici-
pants can look at any word which is currently dis-
played. While English is normally read in a left-
to-right manner, readers often skip words or make
regressions (i.e., look at a word to the left of the
one they are currently fixating). Deviations from
a strict left-to-right progression of fixations moti-
vate the need for several different measures of eye
movement. The model presented here predicts the
Total Time that participants spent looking at a re-
gion, which includes any re-fixations after looking
away. In addition to total time, other possible mea-
sures include (a) First Pass, which measures the ini-
tial fixation and any re-fixations before looking at
any other word (this occurs, for instance, if the eye
initially lands at the start of a long word ? the eye
will tend to re-fixate on a more central viewing lo-
cation), (b) Right Bounded reading time, which in-
cludes all fixations on a word before moving to the
right of the word (i.e., re-fixations after moving left
are included), and (c) Second Pass, which includes
any re-fixation on a word after looking at any other
word (be it to the left or the right of the word of inter-
est). We found that the model performed similarly
across all these reading time metrics, we therefore
only report results for Total Time.
As mentioned above, reading measures are hy-
pothesised to correlate with Surprisal, which is de-
fined as:
S(wt) =? log(P(wt |w1...wt1) (3)
We compute the surprisal scores for the syntax-only
HMM, which does not have access to co-reference
information (henceforth referred to as ?HMM?)
and the full model, which combines the syntax-
only HMM with the co-reference model (henceforth
?HMM+Ref?). To determine if our Dundee corpus
simulations provide a reasonable model of human
sentence processing, we perform a regression anal-
ysis with the Dundee corpus reading time measure
as the dependent variable and the surprisal scores as
the independent variable.
To account for noise in the corpus, we also use
a number of additional explanatory variables which
are known to strongly influence reading times.
These include the logarithm of the frequency of a
word (measured in occurrences per million) and the
length of a word in letters. Two additional explana-
tory variables were available in the Dundee corpus,
which we also included in the regression model.
These were the position of a word on a line, and
which line in a document a word appeared in. As
participants could only view one line at a time (i.e.,
one line per screen), these covariates are known as
line position and screen position, respectively.
All the covariates, including the surprisal es-
timates, were centered before including them in
the regression model. Because the HMM and
HMM+Ref surprisal values are highly collinear, the
HMM+Ref surprisal values were added as residuals
of the HMM surprisal values.
In a normal regression analysis, one must either
assume that participants or the particular choice of
308
items add some randomness to the experiment, and
either each participant?s responses for all items must
be averaged (treating participants as a random fac-
tor), or all participant?s responses for each item is
averaged (treating items as a random factor). How-
ever, in the present analysis we utilise a mixed ef-
fects model, which allows both items and partici-
pants to be treated as random factors.1
The are a number of criteria which can be used
to test the efficacy of one regression model over an-
other. These include the Aikake Information Cri-
terion (AIC), the Bayesian Information Criterion
(BIC), which trade off model fit and number of
model parameters (lower scores are better). It is also
common to compare the log-likelihood of the mod-
els (higher log-likelihood is better), in which case a
?2 can be used to evaluate if a model offers a sig-
nificantly better fit, given the number of parameters
is uses. We test three models: (i) a baseline, with
only low-level factors as independent variables; (ii)
the HMM model, with the baseline factors plus sur-
prisal computed by the syntax-only HMM; and (iii)
the HMM+Ref model which includes the raw sur-
prisal values of the syntax-only HMM and the sur-
prisal of the HMM+Ref models as computed as a
residual of the HMM surprisal score. We compare
the HMM and HMM+Ref to the baseline, and the
HMM+Ref model against the HMM model.
Some of the data needed to be trimmed. If, due to
data sparsity, the surprisal of a word goes to infinity
for one of the models, we entirely remove that word
from the analysis. This occurred seven times form
the HMM+Ref model, but did not occur at all with
the HMM model. Some of the eye-movement data
was trimmed, as well. Fixations on the first and last
words of a line were excluded, as were tracklosses.
However, we did not trim any items due to abnor-
1We assume that each participant and item bias the reading
time of the experiment. Such an analysis is known as having
random intercepts of participant and item. It is also possible
to assume a more involved analysis, known as random slopes,
where the participants and items bias the slope of the predictor.
The model did not converge when using random intercept and
slopes on both participant and item. If random slopes on items
were left out, the HMM regression model did converge, but not
the HMM+Ref model. As the HMM+Ref is the model of inter-
est random slopes were left out entirely to allow a like-with-like
comparison between the HMM and HMM+Ref regression mod-
els.
mally short or abnormally long fixation durations.
3.3 Results
The result of the model comparison on Total Time
reading data is summarised in Table 1. To allow this
work to be compared with other models, the lower
part of the table gives the abosolute AIC, BIC and
log likelihood of the baseline model, while the upper
part gives delta AIC, BIC and log likelihood scores
of pairs of models.
We found that both the HMM and HMM+Ref
provide a significantly better fit with the reading
time data than the Baseline model; all three crite-
ria agree: AIC and BIC lower than for the base-
line, and log-likelihood is higher. Moreover, the
HMM+Ref model provides a significantly better fit
than the HMM model, which demonstrates the bene-
fit of co-reference information for modeling reading
times. Again, all three measures provide the same
result.
Table 2 corroborates this result. It list the
mixed-model coefficients for the HMM+Ref model
and shows that all factors are significant predic-
tors, including both HMM surprisal and residualized
HMM+Ref surprisal.
4 Related Work
There have been few computational models of hu-
man sentence processing that have incorporated
a referential or discourse-level component. Niv
(1994) proposed a parsing model based on Com-
binatory Categorial Grammar (Steedman, 2001), in
which referential information was used to resolve
syntactic ambiguities. The model was able to cap-
ture effects of referential information on syntactic
garden paths (Altmann and Steedman, 1988). This
model differs from that proposed in the present pa-
per, as it is intended to capture psycholinguistic pref-
erences in a qualitative manner, whereas the aim
of the present model is to provide a quantitative
fit to measures of processing difficulty. Moreover,
the model was not based on a large-scale grammar,
and was not tested on unrestricted text. Spivey and
Tanenhaus (1998) proposed a sentence processing
model that examined the effects of referential infor-
mation, as well as other constraints, on the resolu-
tion of ambiguous sentences. Unlike Niv (1994),
309
From To ? AIC ? BIC ? logLik ?2 Significance
Baseline HMM -80 -69 41 82.112 p < .001
Baseline HMM+Ref -99 -89 51 101.54 p < .001
HMM HMM+Ref -19 -8 11 21.424 p < .001
Model AIC BIC logLik
Baseline 10567789 10567880 -5283886
Table 1: Model comparison (upper part) and absolute scores for the Baseline model (lower part)
Coefficient Estimate Std Error t-value
(Intercept) 991.4346 23.7968 41.66
log(Word Frequency) -55.3045 1.4830 -37.29
Word Length 128.6216 1.4677 87.63
Screen Position -1.7769 0.1326 -13.40
Line Position 10.1592 0.7387 13.75
HMM 12.1287 1.3366 9.07
HMM+Ref 19.2772 4.1627 4.63
Table 2: Coefficients of the HMM+Ref model on Total Reading Times. Note that t > 2 indicates that the factor in
question is a significant predictor.
Spivey and Tanenhaus?s (1998) model was specifi-
cally designed to provide a quantitative fit to reading
times. However, the model lacked generality, being
designed to deal with only one type of sentence. In
contrast to both of these earlier models, the model
proposed here aims to be general enough to provide
estimated reading times for unrestricted text. In fact,
as far as we are aware, the present paper represents
the first wide-coverage model of human parsing that
has incorporated discourse-level information.
5 Discussion
The primary finding of this work is that incorporat-
ing discourse information such as co-reference into
an incremental probabilistic model of sentence pro-
cessing has a beneficial effect on the ability of the
model to predict broad-coverage human parsing be-
haviour.
Although not thoroughly explored in this paper,
our finding is related to an ongoing debate about the
structure of the human sentence processor. In par-
ticular, the model of Dubey (2010), which also sim-
ulates the effect of discourse on syntax, is aimed at
examining interactivity in the human sentence pro-
cessor. Interactivity describes the degree to which
human parsing is influenced by non-syntactic fac-
tors. Under the weakly interactive hypothesis, dis-
course factors may prune or re-weight parses, but
only when assuming the strongly interactive hypoth-
esis would we argue that the sentence processor pre-
dicts upcoming material due to discourse factors.
Dubey found that a weakly interactive model sim-
ulated a pattern of results in an experiment (Grodner
et al, 2005) which was previously believed to pro-
vide evidence for the strongly interactive hypothesis.
However, as Dubey does not provide broad-coverage
parsing results, this leaves open the possibility that
the model cannot generalise beyond the experiments
expressly modeled in Dubey (2010).
The model presented here, on the other hand,
is not only broad-coverage but could also be de-
scribed as a strongly interactive model. The strong
interactivity arises because co-reference resolution
is strongly tied to lexical generation probabilities,
which are part of the syntactic portion of our model.
This cannot be achieve in a weakly interactive
model, which is limited to pruning or re-weighting
of parses based on discourse information. As our
analysis on the Dundee corpus showed, the lexical
probabilities (in the form of HMM+Ref surprisal)
are key to improving the fit on eye-tracking data.
We therefore argue that our results provide evidence
310
against a weakly interactive approach, which may be
sufficient to model individual phenomena (as shown
by Dubey 2010), but is unlikely to be able to match
the broad-coverage result we have presented here.
We also note that psycholinguistic evidence for dis-
course prediction (such as the context based lexi-
cal prediction shown by van Berkum et al 2005,
see Section 1) is also evidence for strong interac-
tivity; prediction goes beyond mere pruning or re-
weighting and requires strong interactivity.
References
Gerry Altmann and Mark Steedman. Interaction
with context during human sentence processing.
Cognition, 30:191?238, 1988.
Gerry T. M. Altmann and Yuki Kamide. Incremen-
tal interpretation at verbs: Restricting the domain
of subsequent reference. Cognition, 73:247?264,
1999.
Marisa Ferrara Boston, John T. Hale, Reinhold
Kliegl, and Shravan Vasisht. Surprising parser
actions and reading difficulty. In Proceedings of
ACL-08:HLT, Short Papers, pages 5?8, 2008.
Charles Clifton, Adrian Staub, and Keith Rayner.
Eye movement in reading words and sentences.
In R V Gompel, M Fisher, W Murray, and R L
Hill, editors, Eye Movements: A Window in Mind
and Brain, pages 341?372. Elsevier, 2007.
Vera Demberg and Frank Keller. Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109:
192?210, 2008.
Vera Demberg and Frank Keller. A computational
model of prediction in human parsing: Unifying
locality and surprisal effects. In Proceedings of
the 29th meeting of the Cognitive Science Society
(CogSci-09), 2009.
Amit Dubey. The influence of discourse on syntax:
A psycholinguistic model of sentence processing.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL
2010), Uppsala, Sweden, 2010.
Stefan Frank. Surprisal-based comparison between
a symbolic and a connectionist model of sentence
processing. In 31st Annual Conference of the
Cognitive Science Society (COGSCI 2009), Ams-
terdam, The Netherlands, 2009.
Lyn Frazier, Alan Munn, and Charles Clifton. Pro-
cessing coordinate structure. Journal of Psy-
cholinguistic Research, 29:343?368, 2000.
Daniel J. Grodner, Edward A. F. Gibson, and Du-
ane Watson. The influence of contextual constrast
on syntactic processing: Evidence for strong-
interaction in sentence comprehension. Cogni-
tion, 95(3):275?296, 2005.
John T. Hale. A probabilistic earley parser as a psy-
cholinguistic model. In In Proceedings of the Sec-
ond Meeting of the North American Chapter of
the Asssociation for Computational Linguistics,
2001.
A. Kennedy, R. Hill, and J. Pynte. The dundee cor-
pus. In Proceedings of the 12th European confer-
ence on eye movement, 2003.
Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. Syntactic and semantic factors in
processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, Uppsala,
Sweden, 2010.
M. Niv. A psycholinguistically motivated parser for
CCG. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-94), pages 125?132, Las Cruces, NM,
1994.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A ma-
chine learning approach to coreference resolution
of noun phrases. Computational Linguistics, 27
(4):521?544, 2001.
M. J. Spivey and M. K. Tanenhaus. Syntactic am-
biguity resolution in discourse: Modeling the ef-
fects of referential context and lexical frequency.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 24(6):1521?1543, 1998.
Kieth E. Stanovich and Richard F. West. The effect
of sentence context on ongoing word recognition:
Tests of a two-pricess theory. Journal of Exper-
imental Psychology: Human Perception and Per-
formance, 7:658?672, 1981.
Adrian Staub and Charles Clifton. Syntactic predic-
tion in language comprehension: Evidence from
311
either . . .or. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 32:425?436,
2006.
Mark Steedman. The Syntactic Process. Bradford
Books, 2001.
Erik F. Tjong Kim Sang and Sabine Buchholz. In-
troduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-
2000, pages 127?132. Lisbon, Portugal, 2000.
Jos J. A. van Berkum, Colin M. Brown, Pienie Zwit-
serlood, Valesca Kooijman, and Peter Hagoort.
Anticipating upcoming words in discourse: Evi-
dence from erps and reading times. Journal of Ex-
perimental Psychology: Learning, Memory and
Cognition, 31(3):443?467, 2005.
Barton Wright and Merrill F. Garrett. Lexical deci-
sion in sentences: Effects of syntactic structure.
Memory and Cognition, 12:31?45, 1984.
312
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30?41,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring the utility of joint morphological and syntactic learning
from child-directed speech
Stella Frank
sfrank@inf.ed.ac.uk
Frank Keller
keller@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
Abstract
Children learn various levels of linguistic
structure concurrently, yet most existing mod-
els of language acquisition deal with only
a single level of structure, implicitly assum-
ing a sequential learning process. Developing
models that learn multiple levels simultane-
ously can provide important insights into how
these levels might interact synergistically dur-
ing learning. Here, we present a model that
jointly induces syntactic categories and mor-
phological segmentations by combining two
well-known models for the individual tasks.
We test on child-directed utterances in English
and Spanish and compare to single-task base-
lines. In the morphologically poorer language
(English), the model improves morphological
segmentation, while in the morphologically
richer language (Spanish), it leads to better
syntactic categorization. These results provide
further evidence that joint learning is useful,
but also suggest that the benefits may be dif-
ferent for typologically different languages.
1 Introduction
Models of language acquisition seek to infer lin-
guistic structure from data with minimal amounts of
prior knowledge, in order to discover which char-
acteristics of the input data are useful for learn-
ing, and thus potentially utilised by human learners.
Most previous work has focused on learning individ-
ual aspects of linguistic structure. However, children
clearly learn multiple aspects in parallel, rather than
sequentially, implying that models of language ac-
quisition should also incorporate joint learning. Joint
models investigate the interaction between different
levels of linguistic structure during learning. These
interactions are often (but not necessarily) synergis-
tic, enabling better, more robust, learning by making
use of cues from multiple sources. Recent models
using joint learning to model language acquisition
have spanned various domains including phonology,
word segmentation, syntax and semantics (Feldman
et al, 2009; Elsner et al, 2012; Doyle and Levy,
2013; Johnson, 2008; Kwiatkowski et al, 2012).
In this paper we examine the joint learning of
syntactic categories and morphology, which are ac-
quired by children at roughly the same age (Clark,
2003b), implying possible interactions in the learn-
ing process. Both morphology and word order de-
pend on categorising words based on their morpho-
syntactic function. However, previous models of
syntactic category learning have relied principally
on surrounding context, i.e., word order constraints,
whereas models of morphology use word-internal
cues. Our joint model integrates both sources of
information, allowing the model to flexibly weigh
them according to their utility.
Languages differ in the richness of their mor-
phology and strictness of word order. These char-
acteristics appear to be (anti)correlated, with rich
morphology co-occurring with free word order and
vice versa (Blake, 2001; McFadden, 2003). The
timecourse of acquisition is also influenced by lan-
guage typology: learners of morphologically rich
languages become productive in morphology ear-
lier (Xanthos et al, 2011), suggesting that richer
morphology may be more salient for learners than
impoverished morphology. Sentence comprehension
in children also shows cross-linguistic differences
in the cues used to make sense of non-canonical
sentence structure: learners of a morphologically
rich language (Turkish) disregard word order in
30
favour of morphology, whereas learners of En-
glish favour word order (Slobin, 1982; MacWhin-
ney et al, 1984). These interactions between mor-
phology and word order suggest that a joint model
will be better able to support the differences in cue
strength (rich morphology versus strict word order),
and thus be more language-general, than single-task
models.
Both syntactic category and morphology induc-
tion have been the focus of much recent work. (See
Hammarstro?m and Borin (2011) for an overview
of unsupervised morphology learning, likewise
Christodoulopoulos et al (2010) for a comparison
of part of speech/syntactic category induction sys-
tems.) However, given the tightly coupled nature of
these two tasks, there has been surprisingly little
work in joint learning of morphology and syntac-
tic categories. Systems for inducing syntactic cat-
egories often make use of morpheme-like features,
such as word-final characters (Smith and Eisner,
2005; Haghighi and Klein, 2006; Berg-Kirkpatrick
et al, 2010; Lee et al, 2010), or model words
at the character-level (Clark, 2003a; Blunsom and
Cohn, 2011), but do not include morphemes ex-
plicitly. Other systems (Dasgupta and Ng, 2007;
Christodoulopoulos et al, 2011) use morphologi-
cal segmentations learned by a separate morphology
model as features in a pipeline approach.
Models of morphology induction generally oper-
ate over a lexicon, i.e. a list of word types, rather
than token corpora (Goldsmith, 2006; Creutz and
Lagus, 2007; Kurimo et al, 2010). These models
find morphological categories on the basis of word-
internal features, without taking syntactic context
into account (which is of course not available in a
lexicon).
Lee et al (2011) and Sirts and Aluma?e (2012)
present models that infer morphological segmenta-
tions and syntactic categories jointly, although Lee
et al (2011) do not evaluate the inferred syntactic
categories. Both make use of a word-type constraint
which limits each word form to a single analysis
(i.e., all instances of ducks are assigned to a single
category and will have the same morpheme analy-
sis, ignoring the gold standard distinction between a
plural noun and third person singular verb). This can
make inference more tractable, and often increases
performance, but does not respect the ambiguity in-
herent in natural language, both over syntactic cat-
egories and morphological analyses. The degree of
ambiguity is language dependent, so that even if a
type-constraint is perhaps relatively unproblematic
in English, it will pose problems in morphologically
richer languages. Furthermore, these two models
make use of an array of heuristics that may not allow
them to be easily generalisable across languages and
datasets (e.g., likelihood scaling (Sirts and Aluma?e,
2012), sequential suffix matching (Lee et al, 2011)).
In this paper, we present a joint model composed
of two well-known individual models. This allows
us to cleanly investigate the effects of joint learning
and its potential benefits over the single task models.
The simplicity of our models also allows us to avoid
modelling and inference heuristics.
Previous models have used adult-directed written
texts, which differs significantly from the type of
language available to child learners. We test our joint
model on child-directed utterances in English (a
morphologically poor language) and Spanish (with
richer morphology)1. Our results indicate that our
joint model is able to flexibly accommodate lan-
guages with differing levels of morphological rich-
ness. The joint model matches the performance of
single task models on both tasks, demonstrating that
the additional complexity is not a problem (i.e., it
does not add noise). Moreover, the joint model im-
proves performance significantly on the task corre-
sponding to the language?s weaker cue, indicating a
transfer of information from the stronger cue. The
fact that the nature of this improvement varies by
language provides evidence that joint learning can
effectively accommodate typological diversity.
2 Model
The task is to assign word tokens to part of speech
categories and simultaneously segment the tokens
into morphemes. We assume a relatively simple yet
commonly used concatenative morphology which
models a word as a stem plus (possibly null) suffix2.
1There are languages with much richer morphology than
Spanish, but none with a child-directed corpus suitably anno-
tated for evaluation.
2Fullwood and O?Donnell (2013) recently presented a
model of non-concatenative morphology that could be inte-
grated into this model; however, it does not perform well on En-
glish (and presumably other mostly concatenative languages).
31
Since this is an unsupervised model, the inferred cat-
egories and morphemes lack meaningful labels, but
ideally will correspond to gold standard categories
and morphemes.
2.1 Word Order
We model a sequence of words as a Hidden Markov
Model (HMM) with a non-parametric emission dis-
tribution. As usual, the latent states of the HMM rep-
resent syntactic categories. The tag sequence is gen-
erated by a trigram Dirichlet-multinomial distribu-
tion, where transition parameters ? are drawn from
a symmetric Dirichlet distribution with the hyperpa-
rameter ?t . Each tag ti in the sequence is then drawn
from the transition distribution conditioned on the
previous two tags:
?(t,t ?) ? Dir(?t)
ti = t|ti?1 = t
?, ti?2 = t
??,?? Mult(?(t ?,t ??))
This model is token-based, permitting different
tokens of the same word type to have different
syntactic categories. Most recent models have in-
cluded a constraint forcing all tokens of a given
type into the same category, which improves per-
formance but often complicates inference. The
Bayesian HMM?s performance is therefore not state-
of-the-art, but is comparable to other token-based
models (Christodoulopoulos et al, 2010) and the
model is easy to extend within the Bayesian frame-
work, allowing us to compare multiple versions.
This part of the model is parametric, operat-
ing over a fixed number of tags T , and is iden-
tical to the formulation of tag transitions in the
Bayesian HMM (Goldwater and Griffiths, 2007).
However, we replace the BHMM?s emission dis-
tribution with the morphologically-informed distri-
butions described below. As in the BHMM, the
emission distributions are conditioned on the tag,
i.e., each tag has its own morphology.
2.2 Morphology
The morphology model introduced by Goldwater
et al (2006) generates morphological analyses for a
set of tokens. These analyses consist of a tag plus a
stem and suffix pair, which are concatenated to form
the observed words. Both stem s and suffix f are
generated from Dirichlet-multinomials conditioned
on the tag t:
?? Dir(??)
t|?? Mult(?)
?? Dir(?s)
s|t,?? Mult(?t)
?? Dir(? f )
f |t,?? Mult(?t)
The ?s are hyperparameters governing the Dirich-
let distributions from which the multinomials ?,?,?
are drawn. In turn, t,s, and f are drawn from these
multinomials.
The probability of a word under this model is the
sum of the probabilities of all possible analyses l =
(t,s, f ):
P0(w) =?
l
P0(l) = ?
t,s, f s.t.
s? f=w
P(s|t)P( f |t)P(t) (1)
where s? f = w denotes that the concatenation of
stem and suffix results in the word w.
On its own, this distribution over morphologi-
cal analyses makes independence assumptions that
are too strong: most word tokens of a word type
have the same analysis, but P0 will re-generate
that analysis for every token. To resolve this prob-
lem, a Pitman-Yor process (PYP) is placed over the
generating distribution above. The Pitman-Yor pro-
cess has been found to be useful for representing
the power-law distributions common in natural lan-
guage (Teh, 2006; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011).
The distribution of draws from a Pitman-Yor pro-
cess (which, in our case, determines the distribu-
tion of word tokens with each morphological anal-
ysis) is commonly described using the metaphor of
a Chinese restaurant. A series of customers (tokens
z = z1 . . .zN) enter a restaurant with an infinite num-
ber of initially empty tables. Upon entering, each
customer is seated at a table k with probability
p(zi = k|z1 . . .zi?1,a,b) = (2)
{
nk?a
i?1+b if 1? k ? K
Ka+b
i?1+b if k = K +1
32
tk sk
fk lk
K
zi
wi
N
Figure 1: Plate diagram depicting the morphology model
(adapted from Goldwater et al (2006)). Hyperparameters
have been omitted for clarity. The left-hand plate depicts
the base distribution P0; note that the morphological anal-
yses lk are generated deterministically as (tk,sk, fk). The
observed words wi are also deterministic given zi = k and
lk, since wi = sk? fk.
where nk is the number of customers already sitting
at table k, K is the total number of tables occupied by
the i?1 previous customers, and 0? a < 1 and b? 0
are hyperparameters of the process. The probability
of being seated at a table increases with the number
of customers already seated at that table, creating a
?rich-get-richer? power-law distribution of tokens to
tables; a and b control the amount of reuse of exist-
ing tables, with smaller values leading to more reuse.
Crucially, each table serves a dish generated by
the base distribution P0?i.e., the dish is a morpho-
logical analysis lk = (t,s, f )?and all the customers
seated at the same table share the same dish, which
is generated only once (at the point when that table
is first occupied). The model can thus reuse the anal-
ysis for a particular word and avoid regenerating the
same analysis multiple times. Note that multiple ta-
bles may have identical analyses, lk = lk? . Figure 1
illustrates how the full PYP morphology model gen-
erates the observed sequence of word tokens.
2.3 Combined Model
The full model (Figure 2) combines the latent tag se-
quence with the morphology model. Tag tokens are
generated conditioned on local context, not the base
distribution, as in the morphology model. Instead of
a single PYP generating morphological analyses for
all tokens, as in the Goldwater et al (2006) model,
we have a separate PYP for each tag type, i.e., each
tag has its own restaurant with its own customers
(the tokens labeled with that tag) and its own mor-
phological analyses. The distribution of customers
ti?2 ti?1 ti zi
wi
sk fk
lk
N
Kt
T
Figure 2: Plate diagram depicting the joint model. Hyper-
parameters have been omitted for clarity. The L-shaped
plate contains the tokens, while the square plates contain
the morphological analyses. The t are latent tags, zi is an
assignment to a morphological analysis lk = (sk, fk), and
wi is the observed word. T is the number of distinct tags,
and Kt the number of tables used by tag type t.
in each of the tag-specific restaurants is still deter-
mined by Equation 2, except that all of the counts
and indices are with respect to only the tokens and
tables assigned to that tag.
Each tag-specific PYP (restaurant) also has a sep-
arate base distribution, P(t)0 , resulting in distinct dis-
tributions over stems and suffixes for each tag. The
analyses generated by the base distributions consist
of (stem, suffix) pairs; the tag is given by the identity
of the generating PYP.
P(t)0 (w) =?
l
P(t)0 (l = (s, f )) = ?
s, f s.t.
s? f=w
P(s|t)P( f |t)
(3)
The full joint posterior distribution of a sequence
of words, tags, and morpheme analyses is shown in
Figure 3. Note that all tag-specific morphology mod-
els share the same Pitman-Yor parameters a and b.
3 Inference
We use Gibbs sampling for inference over the three
sets of discrete variables: tags t, their assignments to
morphological analyses (tables) z, and the analyses
themselves l.
Each iteration of the sampler has two stages: First
the morphological analyses l are sampled, and then
each token samples a new tag and a new assignment
to an analysis/table. Because the table assignments
33
P(t, l,z|?t ,a,b,?s,? f ) =P(t|?t)P(l|t,?s,? f )P(z|a,b) (4)
P(t|?t) =
N
?
i=2
P(ti|ti?1, ti?2,t1...i?1,?t) =
T
?
t,t ?=1
?(T?t)
?(ntt ? +T?t)
T
?
t ??=1
?(ntt ?t ?? +?t)
?(?t)
(5)
P(l|t,?s,? f ) =
T
?
t=1
Kt
?
k=1
Pt(lk = (s, f )|l1...k?1,?s,? f ) (6)
=
T
?
t=1
?(S?s)
?(mt +S?s)
?(F? f )
?(mt +F? f )
S
?
s=1
?(mts +?s)
?(?s)
F
?
f=1
?(mt f +? f )
?(? f )
(7)
P(z|a,b) =
T
?
t=1
Nt
?
i=1
P(zi|t,z1...i?1,a,b) (8)
=
T
?
t=1
?(1+b)
?(nt +b)
Kt
?
k=1
(ka+b)
?(nk?a)
?(1?a)
(9)
Figure 3: The posterior distribution of our joint model. Because the sequence of words w is deterministic given
analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w,t, l,z|?t ,a,b,?s,? f ) is
equal to P(t, l,z|?t ,a,b,?s,? f ) when lzi = wi for all i, and 0 otherwise. We give equations for the non-zero case. ns
refer to token counts, ms to table counts. We add two dummy tokens at the start, end, and between sentences to pad
the context history.
are conditioned on tags (i.e., a token must be as-
signed to a table in the correct PYP restaurant) re-
sampling the tag requires immediate resampling of
the table assignment as well.
3.1 Initialization
The tags are initialized uniformly at random. For
each token, a segmentation point is chosen uni-
formly at random (we disallow segmentations with
a null stem). If this segmentation is new within the
PYP associated with that token?s tag, a new table is
created for the token in that PYP. If it matches an ex-
isting analysis, zi is sampled from the existing tables
k plus a possible new table k?.
3.2 Morphological Analyses
Each lk represents the morphological analysis for the
set of tokens assigned to table k. Resampling the
segmentation point (stem and suffix identity) of the
analysis changes the segmentation of all of the word
tokens assigned to that analysis. Note that the tag is
not included in lk in the combined model, because
the tag identity is dependent on the local contexts of
all the tokens seated at the table.
Analyses are sampled from a product of Dirichlet-
multinomial posteriors as follows:
p(lk = (s, f )|t, l
\k) =
m\ks +?s
m\k +S?s
m\kf +? f
m\k +F? f
(10)
where ms and m f are the number of analyses for
this tag that share a stem or suffix with lk, and m
is the total number of analyses for this tag. S and
F are the total number of stems and suffixes in the
model. l\k indicates that the current analysis lk has
been removed from the distribution and the appro-
priate counts, to create the correct conditioning dis-
tribution for the Gibbs sampler.
3.3 Tags
Tags are sampled from the product of posteri-
ors of the transition and emission distributions.
The transition distribution is a standard Dirichlet-
multinomial posterior. Calculating the emission dis-
tribution probability, i.e. the marginal probability of
the word given the tag, involves summing over the
probability of all the existing tables in the given PYP
that emit the correct word, plus the probability of
a new table being created, which also includes the
probability of a new analysis from P(t)0 .
34
More precisely, tags are sampled from the follow-
ing distribution:
p(ti = t|wi = w,t\i,z\i, l,?t ,a,b) (11)
? p(ti = t|ti?1, ti?2,t\i,?t)? p(w|t,z\i, l)
= p(ti = t|ti?1, ti?2,t\i,?t)
? ( ?
k s.t. lk=w
p(zi = k|t,w,z\i)+ p(zi = knew|t,w,z\i))
=
nti?2ti?1t +?t
nti?2ti?1 +T?t
? ( ?
k s.t. lk=w
nk?a
nt +b
+
Kta+b
nt +b
P(t)0 (w))
where lk = w matches tables compatible with w,
i.e., the concatenation of stem and suffix form the
word, slk ? flk = w. nk is the number of words as-
signed to the table k and Kt is the total number of
tables in the PYP for tag t. Note that all counts are
obtained after the removal of the current ti and zi,
i.e., from t\i and z\i.
3.4 Table Assignments
Once a new tag has been sampled for a token, the ta-
ble assignment must be resampled conditioned on
the new tag. The assignment zi is drawn over all
compatible tables in the tag?s PYP (that is, where
lk = w), plus a possible new table:
p(zi = k|ti = t,w,z\i,a,b) ? (12)
{
nk?a
nt+b
if 1? k ? Kt
Kt a+b
nt+b
P(t)0 (w) if k = Kt +1
P(t)0 is calculated by summing over the probability
of all possible segmentations for a new analysis for
word wi, using Equation 3. If a new table is drawn
(k > Kt) then we also sample a new analysis for that
table from P(t)0 .
4 Preliminary Experiments
An important argument for joint learning is that it
affords increased flexibility and robustness across a
wider range of input data. A model that relies on
word order cannot learn syntactic categories from a
morphologically complex language with free word
order; likewise a model attempting to categorise
words using morphology alone will fail on a lan-
guage without morphology. An effective joint model
Language A
abdc fefh pomo rtut usst
cdcc bcba gghh npop npoo
cdca aaaa fefh hfeg pnon
Language B
noom.no usrs.st bbdb.ac cbab.cc cdaa.cc
rttt.uu cbab.aa mnom.oo ccda.bc onmm.om
rruu.ts npop.mm gehg.fh trrt.uu tssu.uu
Table 1: Example sentences in the synthetic languages.
Words in Category 1 are made of characters a-d, Cate-
gory 2 e-h, Category 3 m-p, Category 4 r-u. Suffixes in
Language B are separated with periods (.) for illustrative
purposes only.
will be able to make use of the different cues in both
language types in a flexible way.
In order to test the proposed model, we run two
experiments on synthetic languages, which simulate
languages in which either word order or morphology
is the sole cue. Most natural languages fall between
these extremes, but these experiments show that our
model can capture the full spectrum.
Language A is a strict word order language lack-
ing morphology. It has a vocabulary of 200 word
types, split into four different categories. The 50
word types in each category are created by com-
bining four letters, with replacement, into four-letter
words, with a different set of letters used in each cat-
egory3. Words within a category may thus share be-
ginning or ending characters, which could be posited
as stems or suffixes by the model, but since only
50 of 256 possible strings are used, there will be
no strong evidence for consistent stem and suffixes
(i.e. stems appearing with multiple suffixes and vice
versa). Each sentence in Language A consists of five
words in one of twenty possible category sequences.
In these sequences, each category is either followed
by itself or the next category (i.e. [2,2,2,3,4] is valid
but [2,4,3,1,4] is not). Word order is thus strongly
constrained by category membership.
Language B has free word order, with category
membership signalled by suffixes. Words are cre-
3We achieved the same results with a language using the
same four characters in all categories, but using different char-
acters makes the categories human-readable. The model does
not have a orthographic/phonological component and so will
not recognise the within-category similarity, other than possi-
bly positing spurious stems or suffixes.
35
-70000
-60000
-50000
-40000
-30000
-20000
-10000
 0
 0  200  400  600  800  1000
L
o
g
 
P
r
o
b
a
b
i
l
i
t
y
Iteration
LangA TotalLangA TransitionsLangA MorphologyLangB TotalLangB TransitionsLangB Morphology
Figure 4: Log probability of the sampler state over 1000
iterations on Languages A and B.
ated by the concatenation of a stem and a suffix,
where the stems are the same as the words in lan-
guage A (50 stems in each of four categories). One
of six category-specific suffixes is appended to each
stem, resulting in 300 word types per category. Each
suffix is two letters long, created by combining three
possible letters (the same letters used to create the
stems), thus making mis-segmentation possible (for
instance, up to three of the suffixes could have the
same final letter). Sentences are again five words
long, but the sequence of categories is drawn at ran-
dom, resulting in uniformly random word order. See
Table 1 for example sentences in both languages.
We create a 5000 word corpus for each language,
and run our model on these corpora. Hyperparame-
ters are set to the same values in both languages4.
We run the sampler on each dataset for 1000 it-
erations with simulated annealing. In both cases,
the correct solution is found by iteration 500. Fig-
ure 4 shows that the morphology component con-
tinues to increase the log probability by increasing
the number of tokens seated at a table. Note that
the correct solution in Language A involves learn-
ing a very peaked transition distribution as well as an
even more extreme distribution over suffixes (where
only the null suffix has high probability), whereas
the same distributions in Language B are much flat-
ter. The fact that the same hyperparameter setting is
4The PYP parameters are set to a = 0.1,b = 1.0 and the
HMM transition parameter ?t = 1.0; the parameters in the base
distribution are ?s,? f = 0.001,?k = 0.5.
able to correctly identify the two language extremes
indicates that the model is robust to hyperparameter
values.
These experiments demonstrate that our joint
model is able to learn correctly even when only ei-
ther morphology or word order is informative in a
language. We now turn to acquisition data from nat-
ural languages in which both morphology and word
order are useful cues but to varying degrees.
5 CDS Experiments
5.1 Data
We use two corpora, Eve (Brown, 1973) and Or-
nat (Ornat, 1994), from the CHILDES database
(MacWhinney, 2000). These corpora consist of the
child-directed utterances heard by two children,
the former learning English and the latter Spanish.
These have been annotated for part of speech cate-
gories and morphemes.
The CHILDES corpora are tagged with a very rich
set of part of speech tags (74 tags), which we col-
lapse to a smaller set of tags5. The Eve corpus has
61224 tokens and is thus larger than the Spanish cor-
pus, which has 40497 tokens. However, the English
corpus has only 17 gold suffix types, while Spanish
has 83. The increased richness of Spanish morphol-
ogy also has an effect on the number of word types in
the corpus: the Spanish dataset has 3046 word types,
whereas the larger English dataset has only 1957.
Morphology is annotated using a stem-affix en-
coding which does not directly correspond to our
segmentation-based model. The word running is an-
notated as run-ING, jumping as jump-ING; the anno-
tation is thus agnostic about ortho-morphemic seg-
mentation (i.e., whether to segment as run.ning or
runn.ing), whereas the model is forced to choose
a segmentation point. Syncretic suffixes (sharing
an identical surface form) are disambiguated: sings
is annotated as sing-3S, plums as plum-PL. Con-
versely, the annotation scheme merges allomorphs
into a single suffix: infinitive verbs in Spanish,
for instance, are encoded as ending with -INF,
corresponding to -ar, -er, and -ir surface forms.
5These are 13 for English (ADJ, ADV, AUX, CONJ, DET,
INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10
for Spanish, since the gold standard does not distinguish AUX,
PART or INF.
36
We ignore irregular/non-affixing forms annotated
with & (e.g. was, annotated as be&PAST) and
use only hyphen-separated suffixes to evaluate.
Where multiple suffixes are concatenated together
(e.g., dog-DIM-PL) we treat this as a single suffix
(-DIM-PL) for evaluation purposes.
In Spanish, many words are annotated as having
a suffix of effectively zero length, e.g. the imper-
ative gusta is annotated as gusta-2S&IMP. We re-
place these suffixes (where the stem is equal to the
word) with a null suffix, excluding them from eval-
uation, as they are impossible for a segmentation-
based model to find.
5.2 Evaluation
Tags are evaluated using VM (Rosenberg and
Hirschberg, 2007), as has become standard for this
task (Christodoulopoulos et al, 2010). VM is a mea-
sure of the normalised cross-entropy between gold
and proposed clusters; it ranges between 0 and 100,
with higher scores being better.
We also use VM to evaluate the morphological
segmentation: all tokens with a common suffix are
clustered together, and these clusters are compared
against the gold suffix clusters6. Using a clustering
metric avoids the need to evaluate against a gold seg-
mentation point (which the annotation lacks). Tag
membership is added to the non-null model suffixes,
so that a final -s suffix found in tag 2 is distinguished
from the same suffix found in tag 8 (creating suffixes
-s-T8 and -s-T2), analogous to the gold annotation
distinction between syncretic morphemes -PL and
-3S.
Note that ceiling performance of our model on
Suffix VM will be below 100, since our model can-
not cluster allomorphs, which are represented by a
single abstract morpheme in the gold standard.
5.3 Baselines
We test the full model, MORTAG, against a number
of variations to investigate the advantages of jointly
modelling the two tasks.
Two variants remove the transition distributions,
and thus local syntactic context, from the model.
6We also evaluated stem morpheme clusters and found near-
ceiling performance due to the high number of null-suffix words
in both corpora.
MORTAGNOTRANS is the full model without tran-
sitions between tag tokens; morphology PYP draws
remain conditioned on token tags. We add a Dirich-
let prior over tags (?t = 0.1) to encourage tag spar-
sity (analogous to the transition distribution in the
full model). MORCLUSTERS is the original model
of Goldwater et al (2006), in which tags (called
clusters in the original) are drawn by P0.
MORTAGNOSEG is a variant in which the only
available suffix is the null suffix; thus segmentations
are trivial and only tags are inferred. This model
is approximately equivalent to a simple Bayesian
HMM but with the addition of PYPs within the
emission distribution. We also evaluate against tags
found by the BHMM, with a Dirichlet-multinomial
emission distribution and no morphology.
MORTAGTRUETAGS is the full model but with all
tags fixed to their gold values. This model gives us
oracle-type results for morphology. (Due to the an-
notation scheme used in CHILDES, oracle morpho-
logical segmentations are unavailable, so we were
unable to test a model with gold morphology and in-
ferred tags.)
5.4 Experimental Procedure
Hyperparameter values for the Pitman-Yor process
were found using grid search on a development set
(Section 10 of Eve and Section 8 of Ornat; these sec-
tions are removed from the dataset we report results
on). We use the values which give the best Suffix
VM performance on the development data; however
we stress that the development results did not vary
greatly over a wide range of hyperparameter values,
and only deteriorated significantly at extreme values
of a.
There are a number of other hyperparameters in
the model which we set to fixed values. The transi-
tion hyperparameter ?t is set to 0.1 in all models.
We set the hyperparameters for the stem and suf-
fix distributions in the morphology base distribution
P0 to 0.001 for both ?s and ? f ; ?k over tags in the
MORCLUSTERS model is set to 0.5. The number of
possible stems and suffixes is given by the dataset: in
the Eve dataset there are 5339 candidate stems and
6617 candidate suffixes; in the Ornat dataset these
numbers are 8649 and 6598, respectively. The num-
ber of tags available to the model is set to the number
of gold tags in the data.
37
Tag VM Suffix VM
MORTAG 59.1(1.9) 41.9(10.0)
MORCLUSTERS 22.4(1.0)? 28.0(11.9)?
MORTAGNOTRANS 19.3(1.2)? 24.4(5.2)?
MORTAGNOSEG 59.4(1.7) ?
BHMM 56.2(2.3)? ?
MORTAGTRUETAGS ? 42.5(5.2)
Table 2: English Eve corpus results. Standard deviations
are in parentheses; ? denotes a significant difference from
the MORTAG model.
Sampling is run for 5000 iterations with anneal-
ing. Inspection of the posterior log-likelihood indi-
cates that the models converge after about 1000 it-
erations. We run inference over all models ten times
and report the average performance. Significance is
reported using the non-parametric Wilcoxon rank-
sum test with a significance level of ?< 0.05.
5.5 Results: English
Results on the English Eve corpus are shown in Ta-
ble 2. We use PYP parameters a = 0.3 and b = 10,
though we found similar performance over a wide
range of values of a and b. Our results show a clear
improvement in the morphological segmentations
found by the joint model and stable tagging perfor-
mance across all models with context information.
The syntactic clusters found by models using
only morphological patterns, MORTAGNOTRANS
and MORCLUSTERS, are clearly inferior and lead to
low Tag VM results. The models with local syntac-
tic context all perform approximately equally well
in terms of finding tags. We find no improvement on
tagging performance in English when adding mor-
phology, compared to the MORTAGNOSEG base-
line in which words are not segmented. However, we
do see a small but significant improvement over the
BHMM for both of these models, due to the replace-
ment of the multinomial emission distribution in the
BHMM with the PYP.
Morphological segmentations, as measured by
Suffix VM, clearly improve with the addition of lo-
cal contexts (and the ensuing better tags): the full
model outperforms the baselines without syntactic
contexts. On this dataset, the joint MORTAG model
even matches the performance of the model us-
Tag VM Suffix VM
MORTAG 43.4(2.6) 41.4(2.5)
MORCLUSTERS 20.3(2.5)? 46.5(3.2)
MORTAGNOTRANS 14.4(1.7)? 36.4(2.0)?
MORTAGNOSEG 39.6(3.7)? ?
BHMM 36.4(0.7)? ?
MORTAGTRUETAGS ? 59.8(0.4)?
Table 3: Spanish Ornat corpus results. Standard devia-
tions are in parentheses; ? denotes a significant difference
from the MORTAG model.
ing oracle tags. The standard deviation over Suf-
fix VM scores is quite large for MORTAG and
MORCLUSTERS; this is due to frequent words hav-
ing two high probability segmentations (most no-
tably is, which in some runs was segmented as i.s).
5.6 Results: Spanish
For the Spanish Ornat corpus, we found slightly dif-
ferent optimal PYP hyperparameters and set a = 0.1
and b = 0.1. Results are shown in Table 3.
The Spanish results pattern in the opposite way
as English. Here we see a statistically significant
improvement in tagging performance of the full
joint model over both models without morphology
(MORTAGNOSEG and BHMM). Models without
context information again find much worse tags,
mainly because (as in English) function words are
not identifiable by suffixes.
However, the full model does not find better mor-
phological segmentations than the MORCLUSTERS
model, despite better tags (the two models? Suffix
VM scores are not statistically significantly differ-
ent). We also see that the difference between the seg-
mentations found by the model using gold tags and
estimated tags is quite large. This is due to the ora-
cle model finding the rarer suffixes which were not
distinguished by the models with noisier tags. This
demonstrates the importance of syntactic categorisa-
tion for the morpheme induction task, and suggests
that a more sophisticated tagging model (with better
performance) may yet improve morpheme segmen-
tation performance in Spanish.
38
6 Conclusion
We have presented a model of joint syntactic cate-
gory and morphology induction. Operating within a
generative Bayesian framework means that combin-
ing single-task components is straightforward and
well-founded. Our model is token-based, allowing
for syntactic and morphemic ambiguity.
To our knowledge, this is the first joint model to
be tested on child-directed speech data, which is less
complex than the newswire corpora used by previ-
ous joint models. Child-directed speech may be sim-
ple enough for joint learning not to be necessary: our
results indicate the contrary, namely that joint learn-
ing is indeed helpful when learning from realistic
acquisition data.
We tested this model on two languages with dif-
ferent morphological characteristics. On English, a
language with relatively little morphology, espe-
cially in child directed speech, we found that bet-
ter categorisation of words yielded much better mor-
phology in terms of suffixes learned. Conversely, in
Spanish we saw less difference on the morphology
task between models with categories inferred solely
from morphemic patterns and models that also used
local syntactic context for categorisation. However,
in Spanish we saw an improvement in the tagging
task when morphology information was included.
This suggests that English and Spanish make dif-
ferent word-order and morphology trade-offs. In En-
glish, local context provides at least as much in-
formation as morphology in terms of determining
the correct syntactic category, but knowing a good
estimate of the correct syntactic category is use-
ful for determining a word?s morphology. In Span-
ish, a word?s morphology can more easily be deter-
mined simply by looking at frequent suffixes within
a purely morphological system. On the other hand,
word order is freer, making local syntactic context
unreliable, so taking morphological information into
account can improve tagging. These differences be-
tween languages demonstrate the benefits of joint
learning, which enables the learner to more flexibly
utilise the information available in the input data.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. Painless unsuper-
vised learning with features. In Proceedings of the
North American Association for Computational
Linguistics (NAACL), 2010.
Barry J. Blake. Case. Cambridge University Press,
2001.
Phil Blunsom and Trevor Cohn. A hierarchical
Pitman-Yor process HMM for unsupervised part
of speech induction. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2011.
Roger Brown. A first language: The early stages.
Harvard University Press, Cambridge, MA, 1973.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. A Bayesian mixture model
for part-of-speech induction using multiple fea-
tures. In Proceedings of the 16th Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), 2011.
Alexander Clark. Combining distributional and
morphological information for part of speech in-
duction. In Proceedings of the 10th annual Meet-
ing of the European Association for Computa-
tional Linguistics (EACL), 2003a.
Eve V. Clark. First Language Acquisition. Cam-
bridge University Press, 2003b.
Mathias Creutz and Krista Lagus. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing, 4(1):1?34, 2007.
Sajib Dasgupta and Vincent Ng. Unsupervised part-
of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the 12th Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), 2007.
Gabriel Doyle and Roger Levy. Combining multi-
ple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL-HLT 2013, pages
117?126, 2013.
39
Micha Elsner, Sharon Goldwater, and Jacob Eisen-
stein. Bootstrapping a unified model of lexical
and phonetic acquisition. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2012.
Naomi Feldman, Thomas Griffiths, and James Mor-
gan. Learning phonetic categories by learning a
lexicon. In Proceedings of the 31st Annual Con-
ference of the Cognitive Science Society (CogSci),
2009.
Michelle A. Fullwood and Timothy J. O?Donnell.
Learning non-concatenative morphology. In Pro-
ceedings of the Workshop on Cognitive Modeling
and Computational Linguistics, 2013.
John Goldsmith. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(4):353?371, December 2006.
Sharon Goldwater and Thomas L. Griffiths. A
fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In Ad-
vances in Neural Information Processing Systems
18, 2006.
Aria Haghighi and Dan Klein. Prototype-driven
grammar induction. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2006.
Harald Hammarstro?m and Lars Borin. Unsupervised
learning of morphology. Computational Linguis-
tics, 37(2):309?350, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2008.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.
Proceedings of the MorphoChallenge 2010 work-
shop. Technical Report TKK-ICS-R37, Aalto
University School of Science and Technology, Es-
poo, Finland, 2010.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. A probabilistic
model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL), 2012.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Simple type-level unsupervised POS tagging.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning, 2011.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ, 2000.
Brian MacWhinney, Elizabeth Bates, and Reinhold
Kliegl. Cue validity and sentence interpretation
in English, German, and Italian. Journal of Ver-
bal Learning and Verbal Behavior, 23:127?150,
1984.
Thomas McFadden. On morphological case and
word-order freedom. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society,
volume 29, pages 295?306, 2003.
S. Lopez Ornat. La adquisicion de la lengua espag-
nola. Siglo XXI, Madrid, 1994.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
12th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Aluma?e. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL), 2012.
Dan Slobin. Universal and particular in the acqui-
sition of language. In Eric Wanner and Lila R.
Gleitman, editors, Language acquisition: the state
40
of the art, pages 128?170. Cambridge University
Press, 1982.
Noah A. Smith and Jason Eisner. Contrastive esti-
mation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL), 2005.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
2006.
Aris Xanthos, Sabine Laaha, Steven Gillis, Ursula
Stephany, Ayhan Aksu-Koc?, Anastasia Christofi-
dou, Natalia Gagarina, Gordana Hrzica, F. Ni-
han Ketrez, Marianne Kilani-Schoch, Katharina
Korecky-Kro?ll, Melita Kovac?evic?, Klaus Laalo,
Marijan Palmovic?, Barbara Pfeiler, Maria D.
Voeikova, and Wolfgang U. Dressler. On the role
of morphological richness in the early develop-
ment of noun and verb inflection. First Language,
31(4):461?479, 2011.
41
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Image Description using Visual Dependency Representations
Desmond Elliott
School of Informatics
University of Edinburgh
d.elliott@ed.ac.uk
Frank Keller
School of Informatics
University of Edinburgh
keller@inf.ed.ac.uk
Abstract
Describing the main event of an image in-
volves identifying the objects depicted and
predicting the relationships between them.
Previous approaches have represented images
as unstructured bags of regions, which makes
it difficult to accurately predict meaningful
relationships between regions. In this pa-
per, we introduce visual dependency represen-
tations to capture the relationships between
the objects in an image, and hypothesize that
this representation can improve image de-
scription. We test this hypothesis using a
new data set of region-annotated images, as-
sociated with visual dependency representa-
tions and gold-standard descriptions. We de-
scribe two template-based description gener-
ation models that operate over visual depen-
dency representations. In an image descrip-
tion task, we find that these models outper-
form approaches that rely on object proxim-
ity or corpus information to generate descrip-
tions on both automatic measures and on hu-
man judgements.
1 Introduction
Humans are readily able to produce a description of
an image that correctly identifies the objects and ac-
tions depicted. Automating this process is useful for
applications such as image retrieval, where users can
go beyond keyword-search to describe their infor-
mation needs, caption generation for improving the
accessibility of existing image collections, story il-
lustration, and in assistive technology for blind and
partially sighted people. Automatic image descrip-
tion presents challenges on a number of levels: rec-
ognizing the objects in an image and their attributes
are difficult computer vision problems; while deter-
mining how the objects interact, which relationships
hold between them, and which events are depicted
requires considerable background knowledge.
Previous approaches to automatic description
generation have typically tackled the problem us-
ing an object recognition system in conjunction with
a natural language generation component based on
language models or templates (Kulkarni et al, 2011;
Li et al, 2011). Some approaches have utilised the
visual attributes of objects (Farhadi et al, 2010),
generated descriptions by retrieving the descriptions
of similar images (Ordonez et al, 2011; Kuznetsova
et al, 2012), relied on an external corpus to pre-
dict the relationships between objects (Yang et al,
2011), or combined sentence fragments using a tree-
substitution grammar (Mitchell et al, 2012).
A common aspect of existing work is that an im-
age is represented as a bag of image regions. Bags
of regions encode which objects co-occur in an im-
age, but they are unable to express how the regions
relate to each other, which makes it hard to describe
what is happening. As an example, consider Fig-
ure 1a, which depicts a man riding a bike. If the
man was instead repairing the bike, then the bag-
of-regions representation would be the same, even
though the image would depict a different action and
would have to be described differently. This type
of co-occurrence of regions indicates the need for a
more structured image representation; an image de-
scription system that has access to structured repre-
1292
(a)
A man is riding a bike down the road.
A car and trees are in the background.
(b)
ROOT bike car man road trees
- -
-
on
above
A man is riding a bike down the road.
det
nsubj
aux
root
det
dobj
advmod
det
pobj
(c)
Figure 1: (a) Image with regions marked up: BIKE, CAR,
MAN, ROAD, TREES; (b) human-generated image de-
scription; (c) visual dependency representation express-
ing the relationships between MAN, BIKE, and ROAD
aligned to the syntactic dependency parse of the first sen-
tence in the human-generated description (b).
sentations would be able to correctly infer the action
that is taking place, such as the distinction between
repairing or riding a bike, which would greatly im-
prove the descriptions it is able to generate.
In this paper, we introduce visual dependency rep-
resentations (VDRs) to represent the structure of im-
ages. This representation encodes the geometric re-
lations between the regions of an image. An ex-
ample can be found in Figure 1c, which depicts the
VDR for Figure 1a. It encodes that the MAN is above
the BIKE, and that the BIKE is on the ROAD. These
relationships make it possible to infer that the man
is riding a bike down the road, which corresponds
to the first sentence of the human-generated image
description in Figure 1b.
In order to test the hypothesis that structured im-
age representations are useful for description gener-
ation, we present a series of template-based image
description models. Two of these models are based
on approaches in the literature that represent images
as bags of regions. The other two models use vi-
sual dependency representations, either on their own
or in conjunction with gold-standard image descrip-
tions at training time.
We find that descriptions generated using the
VDR-based models are significantly better than
those generated using bag-of-region models in au-
tomatic evaluations using smoothed BLEU scores
and in human judgements. The BLEU score im-
provements are found at bi-, tri-, and four-gram lev-
els, and humans rate VDR-based image descriptions
1.2 points above the next-best model on a 1?5 scale.
Finally, we also show that the benefit of the vi-
sual dependency representation is maintained when
image descriptions are generated from automatically
parsed VDRs. We use a modified version of the
edge-factored parser of McDonald et al (2005) to
predict VDRs over a set of annotated object regions.
This result reaffirms the potential utility of this rep-
resentation as a means to describe events in images.
Note that throughout the paper, we work with gold-
standard region annotations; this makes it possible
to explore the effect of structured image representa-
tions independently of automatic object detection.
2 Visual Dependency Representation
In analogy to dependency grammar for natural lan-
guage syntax, we define Visual Dependency Gram-
mar to describe the spatial relations between pairs
of image regions. A directed arc between two re-
gions is labelled with the spatial relationship be-
tween those regions, defined in terms of three ge-
ometric properties: pixel overlap, the angle between
regions, and the distance between regions. Table 1
presents a detailed explanation of the spatial rela-
tionships defined in the grammar.
A visual dependency representation of an image
is constructed by creating a directed acyclic graph
1293
X ??on Y
More than 50% of the pixels of re-
gion X overlap with region Y.1
X
???????
surrounds Y
The entirety of region X overlaps
with region Y.
X
????
beside Y
The angle between the centroid of
X and the centroid of Y lies be-
tween 315? and 45? or 135? and
225?.
X
??????
opposite Y
Similar to beside, but used when
there X and Y are at opposite sides
of the image.
X
????
above Y
The angle between X and Y lies be-
tween 225? and 315?.
X
????
below Y
The angle between X and Y lies be-
tween 45? and 135?.
X
?????
infront Y
The Z-plane relationship between
the regions is dominant.
X
?????
behind Y
Identical to infront except X is be-
hind Y in the Z-plane.
Table 1: Visual Dependency Grammar defines eight re-
lations between pairs of annotated regions. To simplify
explanation, all regions are circles, where X is the grey
region and Y is the white region. All relations are consid-
ered with respect to the centroid of a region and the angle
between those centroids. We follow the definition of the
unit circle, in which 0? lies to the right and a turn around
the circle is counter-clockwise.
over the set of regions in an image using the spa-
tial relationships in the Visual Dependency Gram-
mar. It is created from a region-annotated image and
a corresponding image description by first identify-
ing the central actor of the image. The central actor
is the person or object carrying out the depicted ac-
tion; this typically corresponds to the subject of the
sentence describing the image. The region corre-
sponding to the central actor is attached to the ROOT
node of the graph. The remaining regions are then
attached based on their relationship with either the
actor or the other regions in the image as they are
1As per the PASCAL VOC definition of overlap in the object
detection task (Everingham et al, 2011).
mentioned in the description. Each arc introduced
is labelled with one of the spatial relations defined
in the grammar, or with no label if the region is not
described in relation to anything else in the image.
As an example of the output of this annotation
process, consider Figure 1a, its description in 1b,
and its VDR in 1c. Here, the MAN is the central
actor in the image, as he is carrying out the depicted
action (riding a bike). The region corresponding to
MAN is therefore attached to ROOT without a spa-
tial relation. The BIKE region is then attached to the
MAN region using the
????
above relation and BIKE is at-
tached to the ROAD with the ??on relation. In the sec-
ond sentence of the description, CAR and TREES are
mentioned without a relationship to anything else in
the image, so they are attached to the ROOT node. If
these regions were attached to other regions, such as
CAR
????
above ROAD then this would imply structure in
the image that is not conveyed in the description.
2.1 Data
Our data set uses the images from the PASCAL
Visual Object Classification Challenge 2011 action
recognition taster competition (Everingham et al,
2011). This is a closed-domain data set containing
images of people performing ten types of actions,
such as making a phone call, riding a bike, and tak-
ing a photo. We annotated the data set in a three-step
process: (1) collect a description for each image;
(2) annotate the regions in the image; and (3) create a
visual dependency representation of the image. Note
that Steps (2) and (3) are dependent on the image de-
scription, as both the region labels and the relations
between them are derived from the description.
2.2 Image Descriptions
We collected three descriptions of each image in our
data set from Amazon Mechanical Turk. Workers
were asked to describe an image in two sentences.
The first sentence describes the action in the image,
the person performing the action and the region in-
volved in the action; the second sentence describes
any other regions in the image not directly involved
in the action. An example description is given in
Figure 1b.
A total of 2,424 images were described by three
workers each, resulting in a total of 7,272 image de-
1294
ma
n
wo
ma
n
per
son
peo
ple tree
s
hor
se girl wal
l
boy
co
mp
ute
r
chi
ld
boo
k
pho
ne
cha
ir
win
dow gra
ss
ca
me
ra
bic
ycle bik
e
lap
top
Fre
que
ncy
0
100
200
300
400
500
Figure 2: Top 20 annotated regions.
scriptions. The workers, drawn from those regis-
tered in the US with a minimum HIT acceptance rate
of 95%, described an average of 145 ? 93 images;
they were encouraged to describe fewer than 300 im-
ages each to ensure a linguistically diverse data set.
They were paid $0.04 per image and it took on av-
erage 67 ? 123 seconds to describe a single image.
The average length of a description was 19.9 ? 6.5
words in a range of 8?50 words. Dependency parses
of the descriptions were produced using the MST-
Parser (McDonald et al, 2005) trained on sections
2-21 of the WSJ portion of the Penn Treebank.
2.3 Region Annotations
We trained two annotators to draw polygons around
the outlines of the regions in an image using the La-
belMe annotation tool (Russell et al, 2008). The
regions annotated for a given image were limited to
those mentioned in the description paired with the
image. Region annotation was performed on a sub-
set of 341 images and resulted in a total of 5,034
annotated regions with a mean of 4.19 ? 1.94 an-
notations per image. A total of 496 distinct labels
were used to label regions. Figure 2 shows the
distribution of the top 20 region annotations in the
data; people-type regions are the most commonly
annotated regions. Given the prevalence of labels
referring to the same types of regions, we defined
26 sets of equivalent labels to reduce label sparsity
(e.g., BIKE was considered equivalent to BICYCLE).
This normalization process reduced the size of the
region label vocabulary from 496 labels to 362 la-
no
ne
infr
ont
bes
ide
abo
ve on
su
rro
un
ds
beh
ind
bel
ow
opp
osi
te
Fre
que
ncy
0
500
1000
1500
2000
2500
Figure 3: Distribution of the spatial relations.
bels. Inter-annotator agreement was 74.3% for re-
gion annotations, this was measured by computing
polygon overlap over the annotated regions.
2.4 Visual Dependency Representations
The same two annotators were trained to construct
gold-standard visual dependency representations for
annotated image?description pairs. The process for
creating a visual dependency representation of an
image is described earlier in this section of the pa-
per. The 341 region-annotated images resulted in a
set of 1,023 visual dependency representations. The
annotated data set comprised a total of 5,748 spatial
relations, corresponding to a mean of 4.79 ? 3.51
relations per image. Figure 3 shows the distribution
of spatial relation labels in the data set. It can be
seen that the majority of regions are attached to the
ROOT node, i.e., they have the relation label none.
Inter-annotator agreement on a subset of the data
was measured at 84% agreement for labelled de-
pendency accuracy and 95.1% for unlabelled depen-
dency accuracy. This suggests the task of generating
visual dependency representations can be performed
reliably by human annotators. We induced an align-
ment between the annotated region labels and words
in the image description using simple lexical match-
ing augmented with WordNet hyponym lookup. See
Figure 1c for an example of the alignments.
3 Image Description Models
We present four template-based models for gener-
ating image descriptions in this section. Table 2
1295
Regions VDR
External
Corpus
Parallel
text
PROXIMITY X
CORPUS X X
STRUCTURE X X
PARALLEL X X X
Table 2: The data available to each model at training time.
presents an overview of the amount of information
available to each model at training time, ranging
from only the annotated regions of an image to us-
ing visual dependency representation of an image
aligned with the syntactic dependency representa-
tion of its description. At test time, all models have
access to image regions and their labels, and use
these to generate image descriptions. Two of the
models also have access to VDRs at test time, al-
lowing us to test the hypothesis that image structure
is useful for generating good image descriptions.
The aim of each model is to determine what is
happening in the image, which regions are impor-
tant for describing it, and how these regions relate to
each other. Recall that all our images depict actions,
and that the gold-standard annotation was performed
with this in mind. A good description therefore is
one that relates the main actors depicted in the im-
age to each other, typically through a verb; a mere
enumeration of the regions in the image is not suffi-
cient. All models attempt to generate a two-sentence
description, as per the gold standard descriptions.
In the remainder of this section, we will use Fig-
ure 1 as a running example to demonstrate the type
of language each model is capable of generating. All
models share the set of templates in Table 3.
3.1 PROXIMITY
PROXIMITY is based on the assumption that people
describe the relationships between regions that are
near each other. It has access to only the annotated
image regions and their labels.
Region?region relationships that are potentially
relevant for the description are extracted by calculat-
ing the proximity of the annotated regions. Here, oi
is the subject region, o j is the object region, and si j
is the spatial relationship between the regions. Let
T1 DT Oi AUX REL DT O j. T5?
T2 There AUX also {DT Oi}
|unrelated|
i=1 in the image.
T3 DT Oi AUX REL DT O j REL DT Ok. T5?
T4 REL DT O j.
T5 PRP AUX {REL DT Oi}
|dependents|
i=1 .
Table 3: The language generation templates.
R = {(oi, si j, o j), . . .} be the set of possible region?
region relationships found by calculating the near-
est neighbour of each region in Euclidean space be-
tween the centroids of the polygons that mark the re-
gion boundaries. The tuple with the subject closest
to the centre of the image is used to describe what is
happening in the image, and the remaining regions
are used to describe the background.
The first sentence of the description is realised
with template T1 from Table 3. oi is the label of
the subject region and o j is the label of the object
region. DT is a simple determiner chosen from {the,
a}, depending on whether the region label is a plural
noun; AUX is either {is, are}, depending on the num-
ber of the region label; and REL is a word to describe
the relationship between the regions. For this model,
REL is the spatial relationship between the centroids
chosen from {above, below, beside}, depending on
the angle formed between the region centroids, us-
ing the definitions in Table 1. The second sentence
of the description is realised with template T2 over
the subjects oi in R that were not used in the first
sentence. An example of the language generated is:
(1) The man is beside the bike. There is also a
road, a car, and trees in the image.
With the exception of visual attributes to describe
size, colour, or texture, this model is based on the
approach described by Kulkarni et al (2011).
3.2 CORPUS
The biggest limitation of PROXIMITY is that regions
that are near each other are not always in a rele-
vant relationship for a description. For example, in
Figure 1, the BIKE and the CAR regions are near-
est neighbours but they are unlikely to be described
as being in an relationship by a human annotator.
The model CORPUS addresses this issue by using an
1296
external text corpus to determine which pairs of re-
gions are likely to be in a describable relationship.
Furthermore, CORPUS can generate verbs instead of
spatial relations between regions, leading to more
human-like descriptions. CORPUS is based on Yang
et al (2011), except we do not use scene type (in-
door, outdoor, etc.) as part of the model. At training
time, the model has access to the annotated image
regions and labels, and to the dependency-parsed
version of the English Gigaword Corpus (Napoles
et al, 2012). The corpus is used to extract subject?
verb?object subtrees, which are then used to predict
the best pairs of regions, as well as the verb that re-
lates the regions.
The set of region?region relationships
R = {(oi, vi j, o j), . . .} is determined by search-
ing for the most likely o?j ,v
? given an oi over a set
of verbs V extracted from the corpus and the other
regions in the image. This is shown in Equation 1.
o?j ,v
?|oi = argmax
o j ,v
p(oi) ? p(v|oi) ? p(o j|v,oi) (1)
We can easily estimate p(oi), p(v|oi), and p(o j|v,oi)
directly from the corpus. If we cannot find an o?j ,v
?
for a region, we back-off to the spatial relationship
calculation as defined in PROXIMITY. When we
have found the best pairs of regions, we select the
most probable pair and generate the first sentence of
the description using that pair an template T1. The
second sentence is realised with template T2 over the
subjects in R not used in generating the first sen-
tence. An example of the language generated is:
(2) The man is riding the bike. There is also a
car, a road, and trees in the image.
In comparison to PROXIMITY, this model will only
describe pairs of regions that have observed rela-
tions in the external corpus. The corpus also pro-
vides a verb that relates the regions, which pro-
duces descriptions that are more in line with human-
generated text. However, since noun co-occurrence
in the corpus controls which regions can be men-
tioned in the description, this model will be prone
to relating regions simply because their labels occur
together frequently in the corpus.
3.3 STRUCTURE
The model STRUCTURE exploits the visual depen-
dency representation of an image to generate lan-
guage for only the relationships that hold between
pairs of regions. It has access to the image regions,
the region labels, and the visual dependency repre-
sentation of an image.
Region?region relationships are generated during
a depth-first traversal of the VDR using templates
T1, T3, T4, and T5. The VDR of an image is traversed
and language fragments are generated and then com-
bined depending on the number of children of a node
in the tree. If a node has only one child then we
use T1 to generate text for the head-child relation-
ship. If a node has more than one child, we need to
decide how to order the language generated by the
model. We generate sentence fragments using T4 for
each child independently and combine them later. In
STRUCTURE, the sentence fragments are sorted by
the Euclidean distance of the children from the par-
ent. In order to avoid problematic descriptions such
as ?The woman is above the horse is above the field
is beside the house?, we include a special case for
when a node has more than one child. In these cases,
the nearest region is realized in direct relation to the
head using either T3 (two children) or T1 (more than
two children), and the remaining regions form a sep-
arate sentence using T5. This sorting and combing
process would result in ?The woman is above the
horse. She is above field and beside the house? for
the case mentioned above.
An example of the type of description that can be
generated during a traversal is:
(3) The man is above the bike above the road.
There is also a car and trees in the image.
In comparison to PROXIMITY, this model can exploit
a representation of an image that encodes the rela-
tionships between regions in an image (the VDR).
However, it is limited to generating spatial relations,
because it cannot predict verbs to relate regions.
3.4 PARALLEL
The model PARALLEL is an extension of STRUC-
TURE that uses the image descriptions available to
1297
predict verbs that relate regions in parent-child re-
lationships in a VDR. At training time it has ac-
cess to the annotated regions and labels, the visual
dependency representations, and the gold-standard
image descriptions. Recall from Section 2.1 that
the descriptions were dependency-parsed using the
parser of McDonald et al (2005) and alignments
were calculated between the nodes in the VDRs and
the words in the parsed image descriptions.
We estimate two distributions from
the image descriptions using the align-
ments: p(verb|ohead ,ochild ,relhead?child) and
p(verb|ohead ,ochild). The second distribution is used
as a backoff when we do not observe the arc label
between the regions in the training data. The gener-
ation process is similar to that used in STRUCTURE,
with two exceptions: (1) it can generate verbs
during the generation steps, and (2) when a node
has multiple dependents, the sentence fragments
are sorted by the probability of the verb associated
with them. This sorting step governs which child is
in a relationship with its parent. When the model
generates text, it only generates a verb for the
most probable sentence fragment. The remaining
fragments revert back to spatial relationships to
avoid generating language that places the subject
region in multiple relationships with other regions.
An example of the language generated is:
(4) The man is riding the bike on the road. There
is also a car and trees in the image.
In comparison to CORPUS, this model generates de-
scriptions in which the relations between the regions
determined by the image itself and not by an external
corpus. In comparison to PROXIMITY and STRUC-
TURE, this model generates descriptions that express
meaningful relations between the regions and not
simple spatial relationships.
4 Image Parsing
The STRUCTURE and PARALLEL models rely on vi-
sual dependency representations, but it is unreal-
istic to assume gold-standard representations will
always be available because they are expensive to
construct. In this section we describe an image
parser that can induce VDRs automatically from
region-annotated images, providing the input for
the STRUCTURE-PARSED and PARALLEL-PARSED
models at test time.
The parser is based on the arc-factored depen-
dency parsing model of McDonald et al (2005).
This model generates a dependency representation
by maximizing the score s computed over all edges
of the representation. In our notation, xvis is the set
of annotated regions and yvis is a visual dependency
representation of the image; (i, j) is a directed arc
from node i to node j in xvis, f(i, j) is a feature rep-
resentation of the arc (i, j), and w is a vector of fea-
ture weights to be learned by the model. The overall
score of a visual dependency representation is:
s(xvis,yvis) = ?
(i, j)?yvis
w ? f(i, j) (2)
The features in the model are defined over re-
gion labels in the visual dependency representation
as well as the relationship labels. As our depen-
dency representations are unordered, none of the
features encode the linear order of region labels,
unlike the feature set of the original model. Uni-
gram features describe how likely individual region
labels are to appear as either heads or arguments and
bigram feature captures which region labels are in
head-argument relationships. All features are con-
joined with the relationship label.
We evaluate our parser on the 1,023 visual depen-
dency representations from the data set. The evalu-
ation is run over 10 random splits into 80% train-
ing, 10% development, and 10% test data.2 Per-
formance is measured with labelled and unlabelled
directed dependency accuracy. The parser achieves
58.2%? 3.1 labelled accuracy and 65.5%? 3.3 un-
labelled accuracy, significantly better than the base-
line of 51.6% ? 2.5 for both labelled and unlabelled
accuracy (the baseline was calculated by attaching
all image regions to the root node; this is the most
frequent form of attachment in our data).
5 Language Generation Experiments
We evaluate the image description models in an au-
tomatic setting and with human judgements. In
2Different visual dependency representations of the same
image are never split between the training and test data.
1298
Automatic Evaluation Human Judgements
BLEU-1 BLEU-2 BLEU-3 BLEU-4 Grammar Action Scene
PARALLEL-PARSED 45.4 ? 2.0 16.1 ? 0.9 6.4 ? 0.7 2.7 ? 0.5 4.2 ? 1.3 3.3 ? 1.7 3.5 ? 1.3
PROXIMITY 45.1 ? 0.8 10.2 ? 1.0? 2.1 ? 0.6? 0.4 ? 0.2? 3.7 ? 1.5? 2.1 ? 0.3? 3.0 ? 1.4?
CORPUS 46.1 ? 1.1 12.4 ? 1.3? 3.1 ? 0.8? 0.7 ? 0.3? 4.4 ? 1.1 2.2 ? 1.3? 3.4 ? 1.3
STRUCTURE 40.2 ? 3.0? 11.5 ? 1.2? 3.5 ? 0.5? 0.3 ? 0.1? 4.1 ? 1.4 2.1 ? 1.4? 3.0 ? 1.4?
STRUCTURE-PARSED 41.1 ? 2.1? 12.2 ? 0.9? 3.6 ? 0.4? 0.4 ? 0.2? 4.0 ? 1.4 1.6 ? 1.3? 3.2 ? 1.3
PARALLEL 44.6 ? 3.1 16.0 ? 1.5 6.8 ? 1.0 2.9 ? 0.7 4.5 ? 1.0? 3.4 ? 1.6 3.7 ? 1.3
GOLD - - - - 4.8 ? 0.4? 4.8 ? 0.6? 4.6 ? 0.7?
Table 4: Automatic evaluation results averaged over 10 random test splits of the data, and human judgements on the
median scoring BLEU-4 test split for PARALLEL. We find significant differences (?p < 0.05) in the descriptions gener-
ated by PARALLEL-PARSED compared to models that operate over an unstructured bag of image regions representation.
Bold means PARALLEL-PARSED is significantly better than PROXIMITY, CORPUS, and STRUCTURE.
the automatic setting, we follow previous work and
measure how close the model-generated descrip-
tions are to the gold-standard descriptions using the
BLEU metric. Human judgements were collected
from Amazon Mechanical Turk.
5.1 Methodology
The task is to produce a description of an image.
The PROXIMITY and CORPUS models have access
to gold-standard region labels and region bound-
aries at test time. The STRUCTURE and PARALLEL
models have additional access to the visual depen-
dency representation of the image. These represen-
tations are either the gold-standard, or in the case of
STRUCTURE-PARSED and PARALLEL-PARSED, pro-
duced by the image parser described in Section 4.
Table 2 provides a reminder of the information the
different models have access to at training time.
Our data set of 1,023 image?description?VDR
tuples was randomly split into 10 folds of 80%
training data, 10% development data, and 10% test
data. The results we report are means computed
over the 10 splits. The image parser used for mod-
els STRUCTURE-PARSED and PARALLEL-PARSED
is trained on the gold-standard VDRs of the train-
ing splits, and then predicts VDRs on the develop-
ment and test splits. Significant differences were
measured using a one-way ANOVA with PARALLEL-
PARSED as the reference3, with differences between
pairs of mean checked with a Tukey HSD test.
5.2 Automatic Evaluation
The model-generated descriptions are compared
against the human-written gold-standard descrip-
tions using the smoothed BLEU measure (Lin and
Och, 2004). BLEU is commonly used in ma-
chine translation experiments to measure the effec-
tive overlap between a reference sentence and a pro-
posed translation sentence. Table 4 shows the re-
sults on the test data and Figure 4 shows sample out-
puts for two images. PARALLEL, the model with
access to both image structure and aligned image
descriptions at training time outperforms all other
models on higher-order BLEU measures. One rea-
son for this improvement is that PARALLEL can for-
mulate sentence fragments that relate the subject, a
verb, and an object without trying to predict the best
object, unlike CORPUS. The probability associated
with each fragment generated for nodes with mul-
tiple children also tends to lead to a more accurate
order of mentioning image regions. It can also be
seen that PARALLEL-PARSED remains significantly
better than the other models when the VDRs of im-
ages are predicted by an image parser, rather than
being gold-standard.
3Recall that PARALLEL uses gold-standard VDRs and
PARALLEL-PARSED uses the output of the image parser de-
scribed in Section 4.
1299
The weakest results are obtained from a model
that relies on the proximity of regions to generate de-
scriptions. PROXIMITY achieves competitive BLEU-
1 scores but this is mostly due to it correctly gener-
ating region names and determiners. CORPUS is bet-
ter than PROXIMITY at correctly producing higher-
order n-grams than because it has a better model of
the region?region relationships in an image. How-
ever, it has difficulties guessing the correct verb for
a description, as it relies on corpus co-occurrences
for this (see the second example in Table 4). STRUC-
TURE uses the VDR of an image to generate the de-
scription, which this leads to an improvement over
PROXIMITY on some of the BLEU metrics; however,
it is not sufficient to outperform CORPUS.
5.3 Human Judgements
We conducted a human judgement study on Me-
chanical Turk to complement the automatic evalu-
ation. Workers were paid $0.05 to rate the quality of
an image?description pair generated by one of the
models using three criteria on a scale from 1 to 5:
1. Grammaticality: give high scores if the de-
scription is correct English and doesn?t contain
any grammatical mistakes.
2. Action: give high scores if the description cor-
rectly describes what people are doing in the
image.
3. Scene: give high scores if the description cor-
rectly describes the rest of the image (back-
ground, other objects, etc).
A total of 101 images were used for this evalua-
tion and we obtained five judgments for each image-
description pair, resulting in a total of 3,535 judg-
ments. To ensure a fair evaluation, we chose the
images from the split of the data that gave median
BLEU-4 accuracy for PARALLEL, the best perform-
ing model in the automatic evaluations.
The right side of Table 4 shows the mean judge-
ments for each model for across the three evalua-
tion criteria. The gold-standard descriptions elicited
judgements around five, and were significantly bet-
ter than the model outputs on all aspects. Further-
more, all models produce highly grammatical out-
put, with mean ratings of between 3.7 and 4.5. This
can be explained by the fact that the models all relied
on templates to ensure grammatical output.
The ratings of the action descriptions reveal the
usefulness of structural information. PROXIMITY,
CORPUS, and STRUCTURE all perform badly with
mean judgements around two, PARALLEL, which
uses both image structure and aligned descriptions,
significantly outperforms all other models with the
exception of PARALLEL-PARSED, which has very
similar performance. The fact that PARALLEL and
PARALLEL-PARSED perform similarly on all three
human measures confirms that automatically parsed
VDRs are as useful for image description as gold-
standard VDRs.
When we compare the quality of the scene de-
scriptions, we notice that all models perform simi-
larly, around the middle of the scale. This is proba-
bly due to the fact that they all have access to gold-
standard region labels, which enables them to cor-
rectly refer to regions in the scene most of the time.
The additional information about the relationships
between regions that STRUCTURE and PARALLEL
have access to does not improve the quality of the
background scene description.
6 Related Work
Previous work on image description can be grouped
into three approaches: description-by-retrieval, de-
scription using language models, and template-
based description. Ordonez et al (2011), Farhadi
et al (2010), and Kuznetsova et al (2012) gener-
ate descriptions by retrieving the most similar image
from a large data set of images paired with descrip-
tions. These approaches are restricted to generating
descriptions that are only present in the training set;
also, they typically require large amounts of training
data and assume images that share similar properties
(scene type, objects present) should be described in
a similar manner.
Kulkarni et al (2011) and Li et al (2011) generate
descriptions using n-gram language models trained
on a subset of Wikipedia. Both approaches first
determine the attributes and relationships between
regions in an image as region?preposition?region
triples. The disadvantage of relying on region?
preposition?region triples is that they cannot distin-
guish between the main event of the image and the
1300
PROXIMITY A man is beside a phone. There is also a wall and a sign in the image.
CORPUS A man is holding a sign. There is also a wall and a phone in the image.
STRUCTURE A wall is above a wall. A man is beside a sign.
PARALLEL A man is holding a phone. A wall is beside a sign.
GOLD A foreign man with sunglasses talking on a cell phone.
A large building and a mountain in the background.
PROXIMITY A beach is above a beach.
There are also horses, a woman, and a man in the image.
CORPUS A woman is outnumbering a man.
There are also horses and beaches in the image.
STRUCTURE A man is beside a woman above a horse.
A horse is beside a woman beside a beach.
PARALLEL A man is riding a horse above a beach.
A horse is beside a beach beside a woman.
GOLD There is a man and women both on horses.
They are on a beach during the day.
Figure 4: Some example descriptions produced by PROXIMITY, CORPUS, STRUCTURE and PARALLEL.
background regions. Kulkarni et al (2011) is closely
related to our PROXIMITY baseline.
Yang et al (2011) fill in a sentence template
by selecting the likely objects, verbs, prepositions,
and scene types based on a Hidden Markov Model.
Verbs are generated by finding the most likely pair-
ing of object labels in an external corpus. This
model is closely related to our CORPUS baseline.
Mitchell et al (2012) over-generates syntactically
well-formed sentence fragments and then recom-
bines these using a tree-substitution grammar.
Previous research has relied extensively on auto-
matically detecting object regions in an image using
state-of-the art object detectors (Felzenszwalb et al,
2010). We use gold-standard region annotations to
remove this noisy component from the description
generation pipeline, allowing us to focus on the util-
ity of image structure for description generation.
7 Conclusion
In this paper we introduced a novel representation
of an image as a set of dependencies over its an-
notated regions. This visual dependency represen-
tation encodes which regions are related to each
other in an image, and can be used to infer the ac-
tion or event that is depicted. We found that im-
age description models based on visual dependency
representations significantly outperform competing
models in both automatic and human evaluations.
We showed that visual dependency representations
can be induced automatically using a standard de-
pendency parser and that the descriptions generated
from the induced representations are as good as the
ones generated from gold-standard representations.
Future work will focus on improvements to the im-
age parser, on exploring this representation in open-
domain data sets, and on using the output of an ob-
ject detector to obtain a fully automated model.
Acknowledgments
The authors would like to thank M. Lapata and S.
Frank for feedback on an earlier draft of the pa-
per and the anonymous reviewers for their feed-
back. A. M. Enoch, N. Ghahremani-Azghandi, L. S.
McAlpine, and K. Tsagkaridis helped annotate the
data. The research presented here was supported by
the European Research Council under award 203427
Synchronous Linguistic and Visual Processing.
1301
References
Mark Everingham, Luc Van Gool, Christopher K. I.
Williams, John Winn, and Andrew Zisserman. 2011.
The PASCAL Visual Object Classes Challenge 2011
(VOC2011) Results.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In ECCV ?10,
pages 15?29, Heraklion, Crete, Greece.
P F Felzenszwalb, R B Girshick, D McAllester, and
D Ramanan. 2010. Object Detection with Discrimi-
natively Trained Part-Based Models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627?1645.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR ?11, pages
1601?1608, Colorado Springs, Colorado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2012. Collective
Generation of Natural Image Descriptions. In ACL
?12, pages 359?368, Jeju Island, South Korea.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CoNLL ?11, pages 220?228, Portland, Oregon, U.S.A.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
ACL ?04, pages 605?612, Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL ?05, pages 91?98, University of
Michigan, U.S.A.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,
Tamara Berg, and Hal Daum. 2012. Midge : Generat-
ing Image Descriptions From Computer Vision Detec-
tions. In EACL ?12, pages 747?756, Avignon, France.
Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated Gigaword. In AKBC-
WEKEX Workshop at NAACL-HLT ?12, Montreal,
Canada.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS 24, Granada, Spain.
Bryan C. Russell, Antonio Torralba, Kevin P. Murphy,
and William T. Freeman. 2008. LabelMe: A Database
and Web-Based Tool for Image Annotation. IJCV,
77(1-3):157?173.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yiannis
Aloimonos. 2011. Corpus-Guided Sentence Genera-
tion of Natural Images. In EMNLP ?11, pages 444?
454, Edinburgh, Scotland, UK.
1302
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301?312,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incremental Semantic Role Labeling with Tree Adjoining Grammar
Ioannis Konstas
?
, Frank Keller
?
, Vera Demberg
?
and Mirella Lapata
?
?: Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{ikonstas,keller,mlap}@inf.ed.ac.uk
?: Cluster of Excellence Multimodal Computing and Interaction,
Saarland University
vera@coli.uni-saarland.de
Abstract
We introduce the task of incremental se-
mantic role labeling (iSRL), in which se-
mantic roles are assigned to incomplete
input (sentence prefixes). iSRL is the
semantic equivalent of incremental pars-
ing, and is useful for language model-
ing, sentence completion, machine trans-
lation, and psycholinguistic modeling. We
propose an iSRL system that combines
an incremental TAG parser with a seman-
tically enriched lexicon, a role propaga-
tion algorithm, and a cascade of classi-
fiers. Our approach achieves an SRL F-
score of 78.38% on the standard CoNLL
2009 dataset. It substantially outper-
forms a strong baseline that combines
gold-standard syntactic dependencies with
heuristic role assignment, as well as a
baseline based on Nivre?s incremental de-
pendency parser.
1 Introduction
Humans are able to assign semantic roles such as
agent, patient, and theme to an incoming sentence
before it is complete, i.e., they incrementally build
up a partial semantic representation of a sentence
prefix. As an example, consider:
(1) The athlete realized [her
goals]
PATIENT/THEME
were out of reach.
When reaching the noun phrase her goals, the hu-
man language processor is faced with a semantic
role ambiguity: her goals can either be the PA-
TIENT of the verb realize, or it can be the THEME
of a subsequent verb that has not been encoun-
tered yet. Experimental evidence shows that the
human language processor initially prefers the PA-
TIENT role, but switches its preference to the
theme role when it reaches the subordinate verb
were. Such semantic garden paths occur because
human language processing occurs word-by-word,
and are well attested in the psycholinguistic litera-
ture (e.g., Pickering et al., 2000).
Computational systems for performing seman-
tic role labeling (SRL), on the other hand, proceed
non-incrementally. They require the whole sen-
tence (typically together with its complete syntac-
tic structure) as input and assign all semantic roles
at once. The reason for this is that most features
used by current SRL systems are defined globally,
and cannot be computed on sentence prefixes.
In this paper, we propose incremental SRL
(iSRL) as a new computational task that mimics
human semantic role assignment. The aim of an
iSRL system is to determine semantic roles while
the input unfolds: given a sentence prefix and its
partial syntactic structure (typically generated by
an incremental parser), we need to (a) identify
which words in the input participate in the seman-
tic roles as arguments and predicates (the task of
role identification), and (b) assign correct seman-
tic labels to these predicate/argument pairs (the
task of role labeling). Performing these two tasks
incrementally is substantially harder than doing it
non-incrementally, as the processor needs to com-
mit to a role assignment on the basis of incom-
plete syntactic and semantic information. As an
example, take (1): on reaching athlete, the proces-
sor should assign this word the AGENT role, even
though it has not seen the corresponding predicate
yet. Similarly, upon reaching realized, the pro-
cessor can complete the AGENT role, but it should
also predict that this verb also has a PATIENT role,
even though it has not yet encountered the argu-
ment that fills this role. A system that performs
SRL in a fully incremental fashion therefore needs
to be able to assign incomplete semantic roles,
unlike existing full-sentence SRL models.
The uses of incremental SRL mirror the applica-
tions of incremental parsing: iSRL models can be
used in language modeling to assign better string
probabilities, in sentence completion systems to
301
provide semantically informed completions, in
any real time application systems, such as dia-
log processing, and to incrementalize applications
such as machine translation (e.g., in speech-to-
speech MT). Crucially, any comprehensive model
of human language understanding needs to com-
bine an incremental parser with an incremental se-
mantic processor (Pad?o et al., 2009; Keller, 2010).
The present work takes inspiration from the
psycholinguistic modeling literature by proposing
an iSRL system that is built on top of a cogni-
tively motivated incremental parser, viz., the Psy-
cholinguistically Motivated Tree Adjoining Gram-
mar parser of Demberg et al. (2013). This parser
includes a predictive component, i.e., it predicts
syntactic structure for upcoming input during in-
cremental processing. This makes PLTAG par-
ticularly suitable for iSRL, allowing it to predict
incomplete semantic roles as the input string un-
folds. Competing approaches, such as iSRL based
on an incremental dependency parser, do not share
this advantage, as we will discuss in Section 4.3.
2 Related Work
Most SRL systems to date conceptualize seman-
tic role labeling as a supervised learning prob-
lem and rely on role-annotated data for model
training. Existing models often implement a
two-stage architecture in which role identification
and role labeling are performed in sequence. Su-
pervised methods deliver reasonably good perfor-
mance with F-scores in the low eighties on stan-
dard test collections for English (M`arquez et al.,
2008; Bj?orkelund et al., 2009).
Current approaches rely primarily on syntactic
features (such as path features) in order to iden-
tify and label roles. This has been a mixed bless-
ing as the path from an argument to the predi-
cate can be very informative but is often quite
complicated, and depends on the syntactic formal-
ism used. Many paths through the parse tree are
likely to occur infrequently (or not at all), result-
ing in very sparse information for the classifier to
learn from. Moreover, as we will discuss in Sec-
tion 4.4, such path information is not always avail-
able when the input is processed incrementally.
There is previous SRL work employing Tree Ad-
joining Grammar, albeit in a non-incremental set-
ting, as a means to reduce the sparsity of syntax-
based features. Liu and Sarkar (2007) extract a
rich feature set from TAG derivations and demon-
strate that this improves SRL performance.
In contrast to incremental parsing, incremental
semantic role labeling is a novel task. Our model
builds on an incremental Tree Adjoining Gram-
mar parser (Demberg et al., 2013) which predicts
the syntactic structure of upcoming input. This al-
lows us to perform incremental parsing and incre-
mental SRL in tandem, exploiting the predictive
component of the parser to assign (potentially in-
complete) semantic roles on a word-by-word ba-
sis. Similar to work on incremental parsing that
evaluates incomplete trees (Sangati and Keller,
2013), we evaluate the incomplete semantic struc-
tures produced by our model.
3 Psycholinguistically Motivated TAG
Demberg et al. (2013) introduce Psycholin-
guistically Motivated Tree Adjoining Grammar
(PLTAG), a grammar formalism that extends stan-
dard TAG (Joshi and Schabes, 1992) in order to
enable incremental parsing. Standard TAG as-
sumes a lexicon of elementary trees, each of
which contains at least one lexical item as an an-
chor and at most one leaf node as a foot node,
marked with A?. All other leaves are marked with
A? and are called substitution nodes. Elementary
trees that contain a foot node are called auxiliary
trees; those that do not are called initial trees. Ex-
amples for TAG elementary trees are given in Fig-
ure 1a?c.
To derive a TAG parse for a sentence, we start
with the elementary tree of the head of the sen-
tence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These can be used to predict
syntactic structure anchored by words that appear
later in an incremental derivation. The use of pre-
diction trees ensures that fully connected prefix
trees can be built for every prefix of the input sen-
tence.
Each node in a prediction tree carries mark-
ers to indicate that this node was predicted, rather
than being anchored by the current sentence pre-
fix. An example is Figure 1d, which contains a
prediction tree with marker ?1?. In PLTAG, mark-
ers are eliminated through a new operation called
verification, which matches them with the nodes
302
(a) NP
NNS
Banks
(b) S
VP
VB
open
NP?
(c) VP
VP*AP
RB
rarely
(d) S
1
VP
1
1
NP
1
?
Figure 1: PLTAG lexicon entries: (a) and (b) ini-
tial trees, (c) auxiliary tree, (d) prediction tree.
a
S
 B? 
 C? 
a
S
B
 C? 
b
a
S
 B? 
C
c
(a) valid (b) invalid
Figure 3: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
of non-predictive elementary trees. An example
of a PLTAG derivation is given in Figure 2. In
step 1, a prediction tree is introduced through sub-
stitution, which then allows the adjunction of an
adverb in step 2. Step 3 involves the verification
of the marker introduced by the prediction tree
against the elementary tree for open.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 3 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 3b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
The parsing algorithm proposed by Demberg
et al. (2013) exploits fringes to tabulate interme-
diate results. It manipulates a chart in which each
cell (i, f ) contains all the prefix trees whose first
i leaves are the first i words and whose current
fringe is f . To extend the prefix trees for i to
the prefix trees for i+ 1, the algorithm retrieves
all current fringes f such that the chart has entries
in the cell (i, f ). For each such fringe, it needs
to determine the elementary trees in the lexicon
that can be combined with f using substitution or
adjunction. In spite of the large size of a typi-
cal TAG lexicon, this can be done efficiently, as
it only requires matching the current fringes. For
each match, the parser then computes the new pre-
Banks refused to open today
A0
A1
A1
AM-TMP
nsbj aux
xcomp
tmod
?A0,Banks,refused?
?A1,to,refused?
?A1,Banks,open?
?AM-TMP,today,open?
Figure 4: Syntactic dependency graph with se-
mantic role annotation and the accompanying se-
mantic triples, for Banks refused to open today.
fix trees and its new current fringe f
?
and enters it
into cell (i+1, f
?
).
Demberg et al. (2013) convert the Penn Tree-
bank (Marcus et al., 1993) into TAG for-
mat by enriching it with head information and
argument/modifier information from Propbank
(Palmer et al., 2005). This makes it possible
to decompose the Treebank trees into elementary
trees as proposed by Xia et al. (2000). Predic-
tion trees can be learned from the converted Tree-
bank by calculating the connection path (Mazzei
et al., 2007) at each word in a tree. Intuitively,
a prediction tree for word w
n
contains the struc-
ture that is necessary to connect w
n
to the prefix
tree w
1
. . .w
n?1
, but is not part of any of the ele-
mentary trees of w
1
. . .w
n?1
. Using this lexicon, a
probabilistic model over PLTAG operations can be
estimated following Chiang (2000).
4 Model
4.1 Problem Formulation
In a typical semantic role labeling scenario, the
goal is to first identify words that are predicates
in the sentence and then identify and label all the
arguments for each predicate. This translates into
spotting specific words in a sentence that repre-
sent the predicate?s arguments, and assigning pre-
defined semantic role labels to them. Note that in
this work we focus on verb predicates only. The
output of a semantic role labeler is a set of seman-
tic dependency triples ?l,a, p?, with l ? R , and
a, p ? w, where R is a set of semantic role labels
denoting a specific relationship between a predi-
cate and an argument (e.g., ARG0, ARG1, ARGM
in Propbank), w is the list of words in the sentence,
l denotes a specific role label, a the argument, and
p the predicate. An example is shown in Figure 4.
As discussed in the introduction, standard se-
mantic role labelers make their decisions based on
evidence from the whole sentence. In contrast, our
aim is to assign semantic roles incrementally, i.e.,
303
NP
NNS
Banks
S
1
VP
1
1
NP
NNS
Banks
S
1
VP
1
VP
1
AP
RB
rarely
NP
NNS
Banks
S
VP
VP
VB
open
AP
RB
rarely
NP
NNS
Banks
1. subst
2. adj
3. verif
Figure 2: Incremental parse for Banks rarely open using the operations substitution (with a prediction
tree), adjunction, and verification.
we want to produce a set of (potentially incom-
plete) semantic dependency triples for each prefix
of the input sentence. Note that not every word
is an argument to a predicate, therefore the set of
triples will not necessarily change at every input
word. Furthermore, the triples themselves may
be incomplete, as either the predicate or the argu-
ment may not have been observed yet (predicate-
incomplete or argument-incomplete triples).
Our iSRL system relies on PLTAG, using a se-
mantically augmented lexicon. We parse an in-
put sentence incrementally, applying a novel in-
cremental role propagation algorithm (IRPA) that
creates or updates existing semantic triple candi-
dates whenever an elementary (or prediction) tree
containing role information is attached to the ex-
isting prefix tree. As soon as a triple is completed
we apply a two-stage classification process, that
first identifies whether the predicate/argument pair
is a good candidate, and then disambiguates role
labels in case there is more than one candidate.
4.2 Semantic Role Lexicon
Recall that Propbank is used to construct the
PLTAG treebank, in order to distinguish between
arguments and modifiers, which result in elemen-
tary trees with substitution nodes, and auxiliary
trees, i.e., trees with a foot node, respectively (see
Figure 1). Conveniently, we can use the same in-
formation to also enrich the extracted lexicon with
the semantic role annotations, following the pro-
cess described by Sayeed and Demberg (2013).
1
For arguments, annotations are retained on the
substitution node in the parental tree, while for
modifiers, the role annotation is displayed on the
foot node of the auxiliary tree. Note that we dis-
play role annotation on traces that are leaf nodes,
1
Contrary to Sayeed and Demberg (2013) we put role la-
bel annotations for PPs on the preposition rather than their
NP child, following of the CoNLL 2005 shared task (Carreras
and M`arquez, 2005).
which enables us to recover long-range dependen-
cies (third and fifth tree in Figure 5a). Likewise,
we annotate prediction trees with semantic roles,
which enables our system to predict upcoming in-
complete triples.
Our annotation procedure unavoidably intro-
duces some role ambiguity, especially for fre-
quently occurring trees. This can give rise to two
problems when we generate semantic triples incre-
mentally: IRPA tends to create many spurious can-
didate semantic triples for elementary trees that
correspond to high frequency words (e.g., preposi-
tions or modals). Secondly, a semantic triple may
be identified correctly but is assigned several role
labels. (See the elementary tree for refuse in Fig-
ure 5a.) We address these issues by applying clas-
sifiers for role label disambiguation at every pars-
ing operation (substitution, adjunction, or verifica-
tion), as detailed in Section 4.4.
4.3 Incremental Role Propagation Algorithm
The main idea behind IRPA is to create or up-
date existing semantic triples as soon as there is
available role information during parsing. Our al-
gorithm (lines 1?6 in Algorithm 1) is applied af-
ter every PLTAG parsing operation, i.e., when an
elementary or prediction tree T is adjoined to a
particular integration point node pi
ip
of the prefix
tree of the sentence, via substitution or adjunction
(lines 3?4).
2
In case an elementary tree T
v
verifies
a prediction tree T
pr
(lines 5?6), the same method-
ology applies, the only difference being that we
have to tackle multiple integration point nodes
T
pr,ip
, one for each prediction marker of T
pr
that
matches the corresponding nodes in T
v
.
For simplicity of presentation, we will use a
concrete example, see Figure 5. Figure 5a shows
the lexicon entries for the words of the sentence
2
Prediction tree T
pr
in our algorithm is only used during
verification, so it set to nil for substitution and adjunction op-
erations.
304
Banks refused to open. Naturally, some nodes in
the lexicon trees might have multiple candidate
role labels. For example, the substitution NP node
of the second tree takes two labels, namely A0
and A1. These stem from different role signatures
when the same elementary tree occurs in differ-
ent contexts during training (A1 only on the NP;
A0 on the NP and A1 on S). For simplicity?s sake,
we collapse different signatures, and let a classi-
fier labeller to disambiguate such cases (see Sec-
tion 4.4).
Algorithm 1 Incremental Role Propagation Alg.
1: procedure IRPA(pi
ip
, T , T
pr
)
2: ??? . ? is a dictionary of (pi
ip
, ?l,a, p?) pairs
3: if parser operation is substitution or adjunction then
4: CREATE-TRIPLES(pi
ip
, T )
5: else if parser operation is verification then
6: CREATE-TRIPLES-VERIF(pi
ip
, T , T
pr
)
return set of triples ?l,a, p? for prefix tree pi
7: procedure CREATE-TRIPLES(pi
ip
, T )
8: if HAS-ROLES(pi
ip
) then
9: UPDATE-TRIPLE(pi
ip
, T )
10: else if HAS-ROLES(T ) then
11: T
ip
? substitution or foot node of T
12: ADD-TRIPLE(pi
ip
, T
ip
, T )
13: for all remaining nodes n ? T with roles do
14: ADD-TRIPLE(pi
ip
, n, T ) . incomplete triples
15: procedure CREATE-TRIPLES-VERIF(pi
ip
, T
v
, T
pr
)
16: if HAS-ROLES(T
v
) then
17: anchor? lexeme of T
v
18: for all T
ip
? node in T
v
with role do
19: T
pr,ip
? matching node of T
ip
in T
pr
20: CREATE-TRIPLES(T
pr,ip
, T
v
)
. Process the rest of covered nodes in T
pr
with roles
21: for all remaining T
pr,ip
? node in T
pr
with role do
22: UPDATE-TRIPLE(T
pr,ip
, T
pr
)
23: function UPDATE-TRIPLE(pi
ip
, T )
24: dep? FIND-INCOMPLETE(?, T
ip
)
25: anchor? lexeme of T
26: if anchor of T is predicate then
27: SET-PREDICATE(dep, anchor)
28: else if anchor of T is argument then
29: SET-ARGUMENT(dep, anchor)
return dep
30: procedure ADD-TRIPLE(pi
ip
, T
ip
, T )
31: dep? ?[roles of T
ip
], nil, nil?
32: anchor? lexeme of T
33: if anchor of T is predicate then
34: SET-PREDICATE(dep, anchor)
35: SET-ARGUMENT(dep, head of pi
ip
)
36: else if anchor of T is argument then
37: if T is auxiliary then . adjunction
38: SET-ARGUMENT(dep, anchor)
39: else . substitution: arg is head of prefix tree
40: SET-ARGUMENT(dep, head of T
ip
)
41: pred? find dep ? ? with matching pi
ip
42: SET-PREDICATE(dep, pred)
43: ?? (pi
ip
, dep)
Once we process Banks, the prefix tree becomes
the lexical entry for this word, see the first col-
umn of Figure 5b. Next, we process refused:
the parser substitutes the prefix tree into the ele-
mentary tree T of refused;
3
the integration point
pi
ip
on the prefix tree is the topmost NP. Since
the operation is a substitution (line 3), we create
triples between T and pi
ip
via CREATE-TRIPLES
(lines 7?12). pi
ip
does not have any role infor-
mation (line 8), so we proceed to add a new se-
mantic triple between the role-labeled integration
point T
ip
, i.e., substitution NP node of T , and pi
ip
,
via ADD-TRIPLE (lines 30?43). First, we create
an incomplete semantic triple with all roles from
T
ip
(line 31). Then we set the predicate to the an-
chor of T to be the word refused, and the argu-
ment to be the head word of the prefix tree, Banks
(lines 34?35). Note that predicate identification is
a trivial task based on part-of-speech information
in the elementary tree.
4
Then, we add the pair (NP? ?{A0,A1},Banks,
refused?) to a dictionary (line 43). Storing the in-
tegration point along with the semantic triple is
essential, to be able to recover incomplete triples
in later stages of the algorithm. Finally, we re-
peat this process for all remaining nodes on T that
have roles, in our example the substitution node S
(lines 13?14). This outputs an incomplete triple,
?{A1},nil,refused?.
Next, the parser decides to substitute a predic-
tion tree (third tree in Figure 5a) into the substitu-
tion node S of the prefix tree. Since the integration
point is on the prefix tree and has role information
(line 8), the corresponding triple should already be
present in our dictionary. Upon retrieving it, we
set the nil argument to the anchor of the incoming
tree. Since it is a prediction tree, we set it to the
root of the tree, namely S
2
(phrase labels in triples
are denoted by italics), but mark the triple as yet
incomplete. This distinction allows us to fill in the
correct lexical information once it becomes avail-
able, i.e, when the tree gets verified. We also add
an incomplete triple for the trace t in the subject
position of the prediction tree, as described above.
Note that this triple contains multiple roles; this is
expected given that prediction trees are unlexical-
ized and occur in a wide variety of contexts.
When the next verb arrives, the parser success-
fully verifies it against the embedded prediction
3
PLTAG parsing operations can occur in two ways: An
elementary tree can be substituted into the substitution node
of the prefix tree, or the prefix tree can be substituted into a
node of an elementary tree. The same holds for adjunction.
4
Most predicates can be identified as anchors of non-
modifier auxiliary trees. However, there are exceptions to
this rule, i.e., modifier auxiliary trees and non-modifier non-
auxiliary trees being also verbs in our lexicon, hence the use
of the more reliable POS tags.
305
IRPA MaltParser
Banks ? ?
refused ?{A0,A1},Banks,refused?,
?A1,S
2
,refused?,
?{A0,A1,A2},t,nil?
?A0,Banks,refused?
to ? ?
open ?A1,to,refused?,
?A1,Banks,open?
?A1,to,refused?,
?A0,Banks,open?
today ?AM-TMP,today,open? ?AM-TMP,today,open?
Table 1: Complete and incomplete semantic triple
generation, comparing IRPA and a system that
maps gold-standard role labels onto MaltParser in-
cremental dependencies for Figure 4.
tree within the prefix tree (last step of Figure 5b).
Our algorithm first cycles through all nodes that
match between the verification tree T
v
and the pre-
diction tree T
pr
and will complete or create new
triples via CREATE-TRIPLES (lines 18?20). In
our example, the second semantic triple gets com-
pleted by replacing S
2
with the head of the sub-
tree rooted in S. Normally, this would be the verb
open, but in this case the verb is followed by the
infinitive marker to, hence we heuristically set it
to be the argument of the triple instead, following
Carreras and M`arquez (2005). For the last triple,
we set the predicate to the anchor of T
v
open, and
now are able to remove the excess role labels A0
and A2. This illustrated how the lexicalized veri-
fication tree disambiguates the semantic informa-
tion stored in the prediction tree. Finally, trace t is
set to the closest NP head that is below the same
phrase subtree, in this case Banks. Note that Banks
is part of two triples as shown in the last tree of
Figure 5b: it is either an A0 or an A1 for refused
and an A1 for open.
We are able to create incomplete semantic
triples after the prediction of the upcoming verb at
step 2, as shown in Figure 5b. This is not possible
using an incremental dependency parser such as
MaltParser (Nivre et al., 2007) that lacks a predic-
tive component. Table 1 illustrates this by compar-
ing the output of IRPA for Figure 5b with the out-
put of a baseline system that maps role labels onto
the syntactic dependencies in Figure 4, generated
incrementally by MaltParser (see Section 5.3 for
a description of the MaltParser baseline). Malt-
Parser has to wait for the verb open before out-
putting the relevant semantic triples. In contrast,
IRPA outputs incomplete triples as soon as the in-
formation is available, and later on updates its de-
cision. (MaltParser also incorrectly assigns A0 for
the Banks?open pair.)
4.4 Argument Identification and Role Label
Disambiguation
IRPA produces semantic triples for every role an-
notation present in the lexicon entries, which will
often overgenerate role information. Furthermore,
some triples have more than one role label at-
tached to them. During verification, we are able to
filter out the majority of labels in the correspond-
ing prediction trees; However, most triples are cre-
ated via substitution and adjunction.
In order to address these problems we adhere to
the following classification and ranking strategy:
after each semantic triple gets completed, we per-
form a binary classification that evaluates its suit-
ability as a whole, given bilexical and syntactic in-
formation. If the triple is identified as a good can-
didate, then we perform multi-class classification
over role labels: we feed the same bilexical and
syntactic information to a logistic classifier, and
get a ranked list of labels. We then use this list to
re-rank the existing ambiguous role labels in the
semantic triple, and output the top scoring ones.
The identifier is a binary L2-loss support vec-
tor classifier, and the role disambiguator an L2-
regularized logistic regression classifier, both im-
plemented using the efficient LIBLINEAR frame-
work of Fan et al. (2008). The features used are
based on Bj?orkelund et al. (2009) and Liu and
Sarkar (2007), and are listed in Table 2.
The bilexical features are: predicate POS tag,
predicate lemma, argument word form, argument
POS tag, and position. The latter indicates the po-
sition of the argument relative to the predicate, i.e.,
before, on, or after. The syntactic features are:
the predicate and argument elementary trees with-
out the anchors (to avoid sparsity), the category of
the integration point node on the prefix tree where
the elementary tree of the argument attaches to,
an alphabetically ordered set of the categories of
the fringe nodes of the prefix tree after attaching
the argument tree, and the path of PLTAG opera-
tions applied between the argument and the pred-
icate. Note that most of the original features used
by Bj?orkelund et al. (2009) and others are not ap-
plicable in our context, as they exploit information
that is not accessible incrementally. For example,
sibling information to the right of the word is not
available. Furthermore, our PLTAG parser does
not compute syntactic dependencies, hence these
cannot serve as features (and in any case not all
dependencies are available incrementally, see Fig-
ure 4). To counterbalance this, we use local syn-
tactic information stored in the fringe of the pre-
306
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP?
{A0,A1}
S
2
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VP?TO
to
S
VP
VB
open
NP
t
{A1}
(a) Lexicon entries
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
VP
2
VB
2
2
TO
to
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
VP
VP
VB
open
TO
to
{A1}
NP
t
VP
VBD
refused
NP
NNS
Banks
{A0,A1}/{A1}
1. subst 2. subst
3. adj
4. verif
1. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,nil,refused?
2. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,S
2
,refused?
NP ? ?{A0,A1,A2},t,nil?
3. ?
4. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,to,refused?
NP ? ?A1,Banks,open?
(b) Incremental parsing using PLTAG and incremental propagation of roles
Figure 5: Incremental Role Propagation Algorithm application for the sentence Banks refused to open.
Bilexical Syntactic
PredPOS PredElemTree
PredLemma ArgElemTree
ArgWord IntegrationPoint
ArgPOS PrefixFringe
Position OperationPath
Table 2: Features for argument identification and
role label disambiguation.
fix tree. We also store the series of operations ap-
plied by our parser between argument and predi-
cate, in an effort to emulate the effect of recover-
ing longer-range patterns.
5 Experimental Design
5.1 PLTAG and Classifier Training
We extracted the semantically-enriched lexicon
and trained the PLTAG parser by converting the
Wall Street Journal part of Penn Treebank to
PLTAG format. We used Propbank to retrieve
semantic role annotation, as described in Sec-
tion 4.2. We trained the PLTAG parser according
to Demberg et al. (2013) and evaluated the parser
on section 23, on sentences with 40 words or less,
given gold POS tags for each word, and achieved
a labeled bracket F
1
score of 79.41.
In order to train the argument identification and
role label disambiguation classifiers, we used the
English portion of the CoNLL 2009 Shared Task
(Haji?c et al., 2009; Surdeanu et al., 2008). It
consists of the Penn Treebank, automatically con-
verted to dependencies following Johansson and
307
Nugues (2007), accompanied by semantic role la-
bel annotation for every argument pair. The latter
is converted from Propbank based on Carreras and
M`arquez (2005). We extracted the bilexical fea-
tures for the classifiers directly from the gold stan-
dard annotation of the training set. The syntactic
features were obtained as follows: for every sen-
tence in the training set we applied IRPA using the
trained PLTAG parser, with gold standard lexicon
entries for each word of the input sentence. This
ensures near perfect parsing accuracy. Then for
each semantic triple predicted incrementally, we
extracted the relevant syntactic information in or-
der to construct training vectors. If the identified
predicate-argument pair was in the gold standard
then we assigned a positive label for the identifi-
cation classifier, otherwise we flagged it as nega-
tive. For those pairs that are not identified by IRPA
but exist in the gold standard (false negatives), we
extracted syntactic information from already iden-
tified similar triples, as follows: We first look for
correctly identified arguments, wrongly attached
to a different predicate and re-create the triple with
correct predicate/argument information. If no ar-
gument is found, we then pick the argument in the
list of identified arguments for a correct predicate
with the same POS-tag as the gold-standard argu-
ment. In the case of the role label disambigua-
tion classifier we just assign the gold label for ev-
ery correctly identified pair, and ignore the (possi-
bly ambiguous) predicted one. After tuning on the
development set, the argument identifier achieved
an accuracy of 92.18%, and the role label disam-
biguation classifier, 82.37%.
5.2 Evaluation
The focus of this paper is to build a system that is
able to output semantic role labels for predicate-
argument pairs incrementally, as soon as they be-
come available. In order to properly evaluate such
a system, we need to measure its performance in-
crementally. We propose two different cumulative
scores for assessing the (possibly incomplete) se-
mantic triples that have been created so far, as the
input is processed from left to right, per word. The
first metric is called Unlabeled Prediction Score
(UPS) and gets updated for every identified argu-
ment or predicate, even if the corresponding se-
mantic triple is incomplete. Note that UPS does
not take into account the role label, it only mea-
sures predicate and argument identification. In this
respect it is analogous to unlabeled dependency
accuracy reported in the parsing literature. We ex-
pect a model that is able to predict semantic roles
to achieve an improved UPS result compared to a
system that does not do prediction, as illustrated in
Table 1. Our second score, Combined Incremental
SRL Score (CISS), measures the identification of
complete semantic role triples (i.e., correct predi-
cate, predicate sense, argument, and role label) per
word; by the end of the sentence, CISS coincides
with standard combined SRL accuracy, as reported
in CoNLL 2009 SRL-only task. This score is anal-
ogous to labeled dependency accuracy in parsing.
Note that conventional SRL systems such as
Bj?orkelund et al. (2009) typically assume gold
standard syntactic information. In order to emu-
late this, we give our parser gold standard lexicon
entries for each word in the test set; these contain
all possible roles observed in the training set for
a given elementary tree (and all possible senses
for each predicate). This way the parser achieves
a syntactic parsing F
1
score of 94.24, thus ensur-
ing the errors of our system can be attributed to
IRPA and the classifiers. Also note that we evalu-
ate on verb predicates only, therefore trivially re-
ducing the task of predicate identification to the
simple heuristic of looking for words in the sen-
tence with a verb-related POS tag and excluding
auxiliaries and modals. Likewise, predicate sense
disambiguation on verbs presumably is trivial, as
we observed almost no ambiguity of senses among
lexicon entries of the same verb (we adhered to a
simple majority baseline, by picking the most fre-
quent sense, given the lexeme of the verb, in the
few ambiguous cases). It seems that the syntactic
information held in the elementary trees discrimi-
nates well among different senses.
5.3 System Comparison
We evaluated three configurations of our system.
The first configuration (iSRL) uses all seman-
tic roles for each PLTAG lexicon entry, applies
the PLTAG parser, IRPA, and both classifiers to
perform identification and disambiguation, as de-
scribed in Section 4. The second one (Majority-
Baseline), solves the problem of argument identifi-
cation and role disambiguation without the classi-
fiers. For the former we employ a set of heuristics
according to Lang and Lapata (2014), that rely on
gold syntactic dependency information, sourced
from CoNLL input. For the latter, we choose the
most frequent role given the gold standard depen-
dency relation label for the particular argument.
Note that dependencies have been produced in
view of the whole sentence and not incrementally.
308
System Prec Rec F1
iSRL-Oracle 91.00 80.26 85.29
iSRL 81.48 75.51 78.38
Majority-Baseline 71.05 58.10 63.92
Malt-Baseline 60.90 46.14 52.50
Table 3: Full-sentence combined SRL score
This gives the baseline a considerable advantage
especially in case of longer range dependencies.
The third configuration (iSRL-Oracle), is identical
to iSRL, but uses the gold standard roles for each
PLTAG lexicon entry, and thus provides an upper-
bound for our methodology. Finally, we evalu-
ated against Malt-Baseline, a variant of Majority-
Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependencies
MaltParser is a state-of-the-art shift-reduce depen-
dency parser which uses an incremental algorithm.
Following Beuck et al. (2011), we modified the
parser to provide intermediate output at each word
by emitting the current state of the dependency
graph before each shift step. We trained Malt-
Parser using the arc-eager algorithm (which out-
performed the other parsing algorithms available
with MaltParser) on the CoNLL dataset, achiev-
ing a labeled dependency accuracy of 89.66% on
section 23.
6 Results
Figures 6 and 7 show the results on the incremen-
tal SRL task. We plot the F
1
for Unlabeled Predic-
tion Score (UPS) and Combined Incremental SRL
Score (CISS) per word, separately for sentences
of lengths 10, 20, 30, and 40 words. The task gets
harder with increasing sentence length, hence we
can only meaningfully compare the average scores
for sentence of the same length. (This approach
was proposed by Sangati and Keller 2013 for eval-
uating the performance of incremental parsers.)
The UPS results in Figure 6 clearly show that
our system (iSRL) outperforms both baselines
on unlabeled argument and predicate prediction,
across all four sentence lengths. Furthermore,
we note that the iSRL system achieves a near-
constant performance for all sentence prefixes.
Our PLTAG-based prediction/verification archi-
tecture allows us to correctly predict incomplete
semantic role triples, even at the beginning of the
sentence. Both baselines perform worse than the
iSRL system in general. Moreover, the Malt-
Baseline performs badly on the initial sentence
prefixes (up to word 10), presumably as it does
not benefit from syntactic prediction, and thus can-
not generate incomplete triples early in the sen-
tence, as illustrated in Table 1. The Majority-
Baseline also does not do prediction, but it has ac-
cess to gold-standard syntactic dependencies, and
thus outperforms the Malt-Baseline on initial sen-
tence prefixes. Note that due to prediction, our
system tends to over-generate incomplete triples
in the beginning of sentences, compared to non-
incremental output, which may inflate UPS for
the first words. However, this cancels out later
in the sentence if triples are correctly completed;
failure to do so would decrease UPS. The near-
constant performance of our output illustrates this
phenomenon. Finally, the iSRL-Oracle outper-
forms all other systems, as it benefits from correct
role labels and correct PLTAG syntax, thus provid-
ing an upper limit on performance.
The CISS results in Figure 7 present a simi-
lar picture. Again, the iSRL system outperforms
both baselines at all sentence lengths. In addition,
it shows particularly strong performance (almost
at the level of the iSRL-Oracle) at the beginning
of the sentence. This presumably is due to the
fact that our system uses prediction and is able to
identify correct semantic role triples earlier in the
sentence. The baselines also show higher perfor-
mance early in the sentence, but to a lesser degree.
Table 3 reports traditional combined SRL scores
for full sentences over all sentence lengths, as
defined for the CoNLL task. Our iSRL system
outperforms the Majority-Baseline by almost 15
points, and the Malt-Baseline by 25 points. It re-
mains seven points below the iSRL-Oracle upper
limit.
Finally, in order to test the effect of syntactic
parsing on our system, we also experimented with
a variant of our iSRL system that utilizes all lex-
icon entries for each word in the test set. This is
similar to performing the CoNLL 2009 joint task,
which is designed for systems that carry out both
syntactic parsing and semantic role labeling. This
variant achieved a full sentence F-score of 68.0%,
i.e., around 10 points lower than our iSRL system.
This drop in score correlates with the difference
in syntactic parsing F-score between the two ver-
sions of PLTAG parser (94.24 versus 79.41), and
is expected given the high ambiguity of the lex-
icon entries for each word. Note, however, that
the full-parsing version of our system still outper-
forms Malt-Baseline by 15 points.
309
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 6: Unlabeled Prediction Score (UPS)
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 7: Combined iSRL Score (CISS)
7 Conclusions
In this paper, we introduced the new task of incre-
mental semantic role labeling and proposed a sys-
tem that solves this task by combining an incre-
mental TAG parser with a semantically enriched
lexicon, a role propagation algorithm, and a cas-
cade of classifiers. This system achieved a full-
sentence SRL F-score of 78.38% on the standard
CoNLL dataset. Not only is the full-sentence
score considerably higher than the Majority-
Baseline (which is a strong baseline, as it uses
gold-standard syntactic dependencies), but we
also observe that our iSRL system performs well
incrementally, i.e., it predicts both complete and
incomplete semantic role triples correctly early on
in the sentence. We attributed this to the fact that
our TAG-based architecture makes it possible to
predict upcoming syntactic structure together with
the corresponding semantic roles.
Acknowledgments
EPSRC support through grant EP/I032916/1 ?An
integrated model of syntactic and semantic predic-
tion in human language processing? to FK and ML
is gratefully acknowledged.
References
Beuck, Niels, Arne Khn, and Wolfgang Menzel.
2011. Incremental parsing and the evaluation
of partial dependency analyses. In Proceedings
of the 1st International Conference on Depen-
dency Linguistics. Depling 2011.
Bj?orkelund, Anders, Love Hafdell, and Pierre
Nugues. 2009. Multilingual semantic role la-
beling. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning: Shared Task. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
CoNLL ?09, pages 43?48.
Carreras, Xavier and Llu??s M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Con-
ference on Computational Natural Language
Learning. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, CONLL ?05,
pages 152?164.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics.
pages 456?463.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025?1066.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
310
blinear: A library for large linear classification.
Journal of Machine Learning Research 9:1871?
1874.
Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Mart??, Llu??s M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel
Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for english. In Joakim Nivre, Heiki-Jaan
Kalep, Kadri Muischnek, and Mare Koit, edi-
tors, NODALIDA 2007 Proceedings. University
of Tartu, pages 105?112.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409?432.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, Companion Vol-
ume: Short Papers. Uppsala, pages 60?67.
Lang, Joel and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph par-
titioning. Computational Linguistics Accepted
pages 1?62. To appear.
Liu, Yudong and Anoop Sarkar. 2007. Experimen-
tal evaluation of LTAG-based features for se-
mantic role labeling. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Prague, Czech Republic, pages 590?599.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313?330.
M`arquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic Role Labeling: An Introduction to
the Special Issue. Computational Linguistics
34(2):145?159.
Mazzei, Alessandro, Vincenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5:309?332.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13:95?135.
Pad?o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. The proposition bank: An anno-
tated corpus of semantic roles. Computational
Linguistics 31(1):71?106.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447?475.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111?124.
Sayeed, Asad and Vera Demberg. 2013. The se-
mantic augmentation of a psycholinguistically-
motivated syntactic formalism. In Proceed-
ings of the Fourth Annual Workshop on Cog-
nitive Modeling and Computational Linguistics
(CMCL). Association for Computational Lin-
guistics, Sofia, Bulgaria, pages 57?65.
Surdeanu, Mihai, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. 2008.
The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008).
Witten, Ian H. and Timothy C. Bell. 1991. The
zero-frequency problem: estimating the proba-
bilities of novel events in adaptive text compres-
sion. Information Theory, IEEE Transactions
on 37(4):1085?1094.
Xia, Fei, Martha Palmer, and Aravind Joshi. 2000.
A uniform method of grammar extraction and
its applications. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
311
Natural Language Processing and Very Large
Corpora. pages 53?62.
312
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196?206,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic and Semantic Factors in Processing Difficulty:
An Integrated Measure
Jeff Mitchell, Mirella Lapata, Vera Demberg and Frank Keller
University of Edinburgh
Edinburgh, United Kingdom
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,
v.demberg@ed.ac.uk, keller@inf.ed.ac.uk
Abstract
The analysis of reading times can pro-
vide insights into the processes that under-
lie language comprehension, with longer
reading times indicating greater cognitive
load. There is evidence that the language
processor is highly predictive, such that
prior context allows upcoming linguistic
material to be anticipated. Previous work
has investigated the contributions of se-
mantic and syntactic contexts in isolation,
essentially treating them as independent
factors. In this paper we analyze reading
times in terms of a single predictive mea-
sure which integrates a model of seman-
tic composition with an incremental parser
and a language model.
1 Introduction
Psycholinguists have long realized that language
comprehension is highly incremental, with readers
and listeners continuously extracting the meaning
of utterances on a word-by-word basis. As soon
as they encounter a word in a sentence, they inte-
grate it as fully as possible into a representation
of the sentence thus far (Marslen-Wilson 1973;
Konieczny 2000; Tanenhaus et al 1995; Sturt and
Lombardo 2005). Recent research suggests that
language comprehension can also be highly pre-
dictive, i.e., comprehenders are able to anticipate
upcoming linguistic material. This is beneficial as
it gives them more time to keep up with the in-
put, and predictions can be used to compensate for
problems with noise or ambiguity.
Two types of prediction have been observed in
the literature. The first type is semantic predic-
tion, as evidenced in semantic priming: a word
that is preceded by a semantically related prime
or a semantically congruous sentence fragment is
processed faster (Stanovich and West 1981; van
Berkum et al 1999; Clifton et al 2007). Another
example is argument prediction: listeners are able
to launch eye-movements to the predicted argu-
ment of a verb before having encountered it, e.g.,
they will fixate an edible object as soon as they
hear the word eat (Altmann and Kamide 1999).
The second type of prediction is syntactic predic-
tion. Comprehenders are faster at naming words
that are syntactically compatible with prior con-
text, even when they bear no semantic relationship
to the context (Wright and Garrett 1984). Another
instance of syntactic prediction has been reported
by Staub and Clifton (2006): following the word
either, readers predict or and the complement that
follows it, and process it faster compared to a con-
trol condition without either.
Thus, human language processing takes advan-
tage of the constraints imposed by the preceding
semantic and syntactic context to derive expecta-
tions about the upcoming input. Much recent work
has focused on developing computational mea-
sures of these constraints and expectations. Again,
the literature is split into syntactic and semantic
models. Probably the best known measure of syn-
tactic expectation is surprisal (Hale 2001) which
can be coarsely defined as the negative log proba-
bility of word wt given the preceding words, typ-
ically computed using a probabilistic context-free
grammar.
Modeling work on semantic constraint focuses
on the degree to which a word is related to its
preceding context. Pynte et al (2008) use La-
tent Semantic Analysis (LSA, Landauer and Du-
mais 1997) to assess the degree of contextual con-
straint exerted on a word by its context. In this
framework, word meanings are represented as vec-
tors in a high dimensional space and distance in
this space is interpreted as an index of process-
ing difficulty. Other work (McDonald and Brew
2004) models contextual constraint in information
theoretic terms. The assumption is that words
carry prior semantic expectations which are up-
dated upon seeing the next word. Expectations are
represented by a vector of probabilities which re-
flects the likely location in semantic space of the
upcoming word.
The measures discussed above are typically
computed automatically on real-language corpora
using data-driven methods and their predictions
are verified through analysis of eye-movements
that people make while reading. Ample evidence
196
(Rayner 1998) demonstrates that eye-movements
are related to the moment-to-moment cognitive ac-
tivities of readers. They also provide an accurate
temporal record of the on-line processing of nat-
ural language, and through the analysis of eye-
movement measurements (e.g., the amount of time
spent looking at a word) can give insight into the
processing difficulty involved in reading.
In this paper, we investigate a model of predic-
tion that is incremental and takes into account syn-
tactic as well as semantic constraint. The model
essentially integrates the predictions of an incre-
mental parser (Roark 2001) together with those
of a semantic space model (Mitchell and Lap-
ata 2009). The latter creates meaning representa-
tions compositionally, and therefore builds seman-
tic expectations for word sequences (e.g., phrases,
sentences, even documents) rather than isolated
words. Some existing models of sentence process-
ing integrate semantic information into a prob-
abilistic parser (Narayanan and Jurafsky 2002;
Pado? et al 2009); however, the semantic compo-
nent of these models is limited to semantic role in-
formation, rather than attempting to build a full se-
mantic representation for a sentence. Furthermore,
the models of Narayanan and Jurafsky (2002) and
Pado? et al (2009) do not explicitly model pre-
diction, but rather focus on accounting for garden
path effects. The proposed model simultaneously
captures semantic and syntactic effects in a sin-
gle measure which we empirically show is predic-
tive of processing difficulty as manifested in eye-
movements.
2 Models of Processing Difficulty
As described in Section 1, reading times provide
an insight into the various cognitive activities that
contribute to the overall processing difficulty in-
volved in comprehending a written text. To quan-
tify and understand the overall cognitive load asso-
ciated with processing a word in context, we will
break that load down into a sum of terms repre-
senting distinct computational costs (semantic and
syntactic). For example, surprisal can be thought
of as measuring the cost of dealing with unex-
pected input. When a word conforms to the lan-
guage processor?s expectations, surprisal is low,
and the cognitive load associated with processing
that input will also be low. In contrast, unexpected
words will have a high surprisal and a high cogni-
tive cost.
However, high-level syntactic and semantic fac-
tors are only one source of cognitive costs. A siz-
able proportion of the variance in reading times is
accounted for by costs associated with low-level
features of the stimuli, e.g.. relating to orthography
and eye-movement control (Rayner 1998). In ad-
dition, there may also be costs associated with the
integration of new input into an incremental rep-
resentation. Dependency Locality Theory (DLT,
Gibson 2000) is essentially a distance-based mea-
sure of the amount of processing effort required
when the head of a phrase is integrated with its
syntactic dependents. We do not consider integra-
tion costs here (as they have not been shown to
correlate reliably with reading times; see Demberg
and Keller 2008 for details) and instead focus on
the costs associated with semantic and syntactic
constraint and low-level features, which appear to
make the most substantial contributions.
In the following subsections we describe the
various features which contribute to the process-
ing costs of a word in context. We begin by look-
ing at the low-level costs and move on to con-
sider the costs associated with syntactic and se-
mantic constraint. For readers unfamiliar with the
methodology involved in modeling eye-tracking
data, we note that regression analysis (or the more
general mixed effects models) is typically used to
study the relationship between dependent and in-
dependent variables. The independent variables
are the various costs of processing effort and
the dependent variables are measurements of eye-
movements, three of which are routinely used in
the literature: first fixation duration (the duration
of the first fixation on a word regardless of whether
it is the first fixation on a word or the first of mul-
tiple fixations on the same word), first pass dura-
tion, also known as gaze duration, (the sum of all
fixations made on a word prior to looking at an-
other word), and total reading time (the sum of
all fixations on a word including refixations after
moving on to other words).
2.1 Low-level Costs
Low-level features include word frequency (more
frequent words are read faster), word length
(shorter words are read faster), and the position
of the word in the sentence (later words are read
faster). Oculomotor variables have also been
found to influence reading times. These include
previous fixation (indicating whether or not the
previous word has been fixated), launch distance
(how many characters intervene between the cur-
rent fixation and the previous fixation), and land-
ing position (which letter in the word the fixation
landed on).
Information about the sequential context of a
word can also influence reading times. Mc-
197
Donald and Shillcock (2003) show that forward
and backward transitional probabilities are pre-
dictive of first fixation and first pass durations:
the higher the transitional probability, the shorter
the fixation time. Backward transitional prob-
ability is essentially the conditional probabil-
ity of a word given its immediately preceding
word, P(wk|wk?1). Analogously, forward proba-
bility is the conditional probability of the current
word given the next word, P(wk|wk+1).
2.2 Syntactic Constraint
As mentioned earlier, surprisal (Hale 2001; Levy
2008) is one of the best known models of process-
ing difficulty associated with syntactic constraint,
and has been previously applied to the modeling of
reading times (Demberg and Keller 2008; Ferrara
Boston et al 2008; Roark et al 2009; Frank 2009).
The basic idea is that the processing costs relating
to the expectations of the language processor can
be expressed in terms of the probabilities assigned
by some form of language model to the input.
These processing costs are assumed to arise from
the change in the expectations of the language pro-
cessor as new input arrives. If we express these ex-
pectations in terms of a distribution over all possi-
ble continuations of the input seen so far, then we
can measure the magnitude of this change in terms
of the Kullback-Leibler divergence of the old dis-
tribution to the updated distribution. This measure
of processing cost for an input word, wk+1, given
the previous context, w1 . . .wk, can be expressed
straightforwardly in terms of its conditional prob-
ability as:
S =? logP(wk+1|w1 . . .wk) (1)
That is, the processing cost for a word decreases as
its probability increases, with zero processing cost
incurred for words which must appear in a given
context, as these do not result in any change in the
expectations of the language processor.
The original formulation of surprisal (Hale
2001) used a probabilistic parser to calculate these
probabilities, as the emphasis was on the process-
ing costs incurred when parsing structurally am-
biguous garden path sentences.1 Several variants
of calculating surprisal have been developed in the
literature since using different parsing strategies
1While hearing a sentence like The horse raced past the
barn fell (Bever 1970), English speakers are inclined to in-
terpreted horse as the subject of raced expecting the sentence
to end at the word barn. So upon hearing the word fell they
are forced to revise their analysis of the sentence thus far and
adopt a reduced relative reading.
(e.g., left-to-right vs. top-down, PCFGs vs de-
pendency parsing) and different degrees of lexi-
calization (see Roark et al 2009 for an overview) .
For instance, unlexicalized surprisal can be easily
derived by substituting the words in Equation (1)
with parts of speech (Demberg and Keller 2008).
Surprisal could be also defined using a vanilla
language model that does not take any structural
or grammatical information into account (Frank
2009).
2.3 Semantic Constraint
Distributional models of meaning have been com-
monly used to quantify the semantic relation be-
tween a word and its context in computational
studies of lexical processing. These models are
based on the idea that words with similar mean-
ings will be found in similar contexts. In putting
this idea into practice, the meaning of a word is
then represented as a vector in a high dimensional
space, with the vector components relating to the
strength on occurrence of that word in various
types of context. Semantic similarities are then
modeled in terms of geometric similarities within
the space.
To give a concrete example, Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) cre-
ates a meaning representation for words by con-
structing a word-document co-occurrence matrix
from a large collection of documents. Each row in
the matrix represents a word, each column a doc-
ument, and each entry the frequency with which
the word appeared within that document. Because
this matrix tends to be quite large it is often trans-
formed via a singular value decomposition (Berry
et al 1995) into three component matrices: a ma-
trix of word vectors, a matrix of document vectors,
and a diagonal matrix containing singular values.
Re-multiplying these matrices together using only
the initial portions of each (corresponding to the
use of a lower dimensional spatial representation)
produces a tractable approximation to the original
matrix. In this framework, the similarity between
two words can be easily quantified, e.g., by mea-
suring the cosine of the angle of the vectors repre-
senting them.
As LSA is one the best known semantic space
models it comes as no surprise that it has been
used to analyze semantic constraint. Pynte et al
(2008) measure the similarity between the next
word and its preceding context under the assump-
tion that high similarity indicates high semantic
constraint (i.e., the word was expected) and analo-
gously low similarity indicates low semantic con-
straint (i.e., the word was unexpected). They oper-
198
ationalize preceding contexts in two ways, either
as the word immediately preceding the next word
as the sentence fragment preceding it. Sentence
fragments are represented as the average of the
words they contain independently of their order.
The model takes into account only content words,
function words are of little interest here as they can
be found in any context.
Pynte et al (2008) analyze reading times on the
French part of the Dundee corpus (Kennedy and
Pynte 2005) and find that word-level LSA similar-
ities are predictive of first fixation and first pass
durations, whereas sentence-level LSA is only
predictive of first pass duration (i.e., for a mea-
sure that includes refixation). This latter finding
is somewhat counterintuitive, one would expect
longer contexts to have an immediate effect as
they are presumably more constraining. One rea-
son why sentence-level influences are only visible
on first pass duration may be due to LSA itself,
which is syntax-blind. Another reason relates to
the way sentential context was modeled as vec-
tor addition (or averaging). The idea of averag-
ing is not very attractive from a linguistic perspec-
tive as it blends the meanings of individual words
together. Ideally, the combination of simple el-
ements onto more complex ones must allow the
construction of novel meanings which go beyond
those of the individual elements (Pinker 1994).
The only other model of semantic constraint we
are aware of is Incremental Contextual Distinc-
tiveness (ICD, McDonald 2000; McDonald and
Brew 2004). ICD assumes that words carry prior
semantic expectations which are updated upon
seeing the next word. Context is represented by
a vector of probabilities which reflects the likely
location in semantic space of the upcoming word.
When the latter is observed, the prior expecta-
tion is updated using a Bayesian inference mecha-
nism to reflect the newly arrived information. Like
LSA, ICD is based on word co-occurrence vectors,
however it does not employ singular value decom-
position, and constructs a word-word rather than a
word-document co-occurrence matrix. Although
this model has been shown to successfully simu-
late single- and multiple-word priming (McDon-
ald and Brew 2004), it failed to predict processing
costs in the Embra eye-tracking corpus (McDon-
ald and Shillcock 2003).
In this work we model semantic constraint us-
ing the representational framework put forward in
Mitchell and Lapata (2008). Their aim is not so
much to model processing difficulty, but to con-
struct vector-based meaning representations that
go beyond individual words. They introduce a
general framework for studying vector composi-
tion, which they formulate as a function f of two
vectors u and v:
h = f (u,v) (2)
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Assuming that h is a linear func-
tion of the Cartesian product of u and v allows to
specify additive models which are by far the most
common method of vector combination in the lit-
erature:
hi = ui + vi (3)
Alternatively, we can assume that h is a linear
function of the tensor product of u and v, and thus
derive models based on multiplication:
hi = ui ? vi (4)
Mitchell and Lapata (2008) show that several ad-
ditive and multiplicative models can be formu-
lated under this framework, including the well-
known tensor products (Smolensky 1990) and cir-
cular convolution (Plate 1995). Importantly, com-
position models are not defined with a specific se-
mantic space in mind, they could easily be adapted
to LSA, or simple co-occurrence vectors, or more
sophisticated semantic representations (e.g., Grif-
fiths et al 2007), although admittedly some com-
position functions may be better suited for partic-
ular semantic spaces.
Composition models can be straightforwardly
used as predictors of processing difficulty, again
via measuring the cosine of the angle between a
vector w representing the upcoming word and a
vector h representing the words preceding it:
sim(w,h) =
w ?h
|w||h|
(5)
where h is created compositionally, via some (ad-
ditive or multiplicative) function f .
In this paper we evaluate additive and compo-
sitional models in their ability to capture seman-
tic prediction. We also examine the influence of
the underlying meaning representations by com-
paring a simple semantic space similar to Mc-
Donald (2000) against Latent Dirichlet Allocation
(Blei et al 2003; Griffiths et al 2007). Specif-
ically, the simpler space is based on word co-
occurrence counts; it constructs the vector repre-
senting a given target word, t, by identifying all the
tokens of t in a corpus and recording the counts of
context words, ci (within a specific window). The
context words, ci, are limited to a set of the n most
199
common content words and each vector compo-
nent is given by the ratio of the probability of a ci
given t to the overall probability of ci.
vi =
p(ci|t)
p(ci)
(6)
Despite its simplicity, the above semantic space
(and variants thereof) has been used to success-
fully simulate lexical priming (e.g., McDonald
2000), human judgments of semantic similarity
(Bullinaria and Levy 2007), and synonymy tests
(Pado? and Lapata 2007) such as those included in
the Test of English as Foreign Language (TOEFL).
LDA is a probabilistic topic model offering an
alternative to spatial semantic representations. It
is similar in spirit to LSA, it also operates on a
word-document co-occurrence matrix and derives
a reduced dimensionality description of words and
documents. Whereas in LSA words are repre-
sented as points in a multi-dimensional space,
LDA represents words using topics. Specifically,
each document in a corpus is modeled as a distri-
bution over K topics, which are themselves char-
acterized as distribution over words. The individ-
ual words in a document are generated by repeat-
edly sampling a topic according to the topic distri-
bution and then sampling a single word from the
chosen topic. Under this framework, word mean-
ing is represented as a probability distribution over
a set of latent topics, essentially a vector whose
dimensions correspond to topics and values to the
probability of the word given these topics. Topic
models have been recently gaining ground as a
more structured representation of word meaning
(Griffiths et al 2007; Steyvers and Griffiths 2007).
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topics have an intuitive correspon-
dence to coarse-grained sense distinctions.
3 Integrating Semantic Constraint into
Surprisal
The treatment of semantic and syntactic constraint
in models of processing difficulty has been some-
what inconsistent. While surprisal is a theo-
retically well-motivated measure, formalizing the
idea of linguistic processing being highly predic-
tive in terms of probabilistic language models, the
measurement of semantic constraint in terms of
vector similarities lacks a clear motivation. More-
over, the two approaches, surprisal and similarity,
produce mathematically different types of mea-
sures. Formally, it would be preferable to have
a single approach to capturing constraint and the
obvious solution is to derive some form of seman-
tic surprisal rather than sticking with similarity.
This can be achieved by turning a vector model
of semantic similarity into a probabilistic language
model.
There are in fact a number of approaches to de-
riving language models from distributional mod-
els of semantics (e.g., Bellegarda 2000; Coccaro
and Jurafsky 1998; Gildea and Hofmann 1999).
We focus here on the model of Mitchell and La-
pata (2009) which tackles the issue of the compo-
sition of semantic vectors and also integrates the
output of an incremental parser. The core of their
model is based on the product of a trigram model
p(wn|w
n?1
n?2) and a semantic component ?(wn,h)
which determines the factor by which this proba-
bility should be scaled up or down given the prior
semantic context h:
p(wn) = p(wn|w
n?1
n?2) ??(wn,h) (7)
The factor ?(wn,h) is essentially based on a com-
parison between the vector representing the cur-
rent word wn and the vector representing the prior
history h. Varying the method for constructing
word vectors (e.g., using LDA or a simpler seman-
tic space model) and for combining them into a
representation of the prior context h (e.g., using
additive or multiplicative functions) produces dis-
tinct models of semantic composition.
The calculation of ? is then based on a weighted
dot product of the vector representing the upcom-
ing word w, with the vector representing the prior
context h:
?(w,h) =?
i
wihi p(ci) (8)
As shown in Equation (7) this semantic factor then
modulates the trigram probabilities, to take ac-
count of the effect of the semantic content outside
the n-gram window.
Mitchell and Lapata (2009) show that a com-
bined semantic-trigram language model derived
from this approach and trained on the Wall Street
Journal outperforms a baseline trigram model in
terms of perplexity on a held out set. They also
linearly interpolate this semantic language model
with the output of an incremental parser, which
computes the following probability:
p(w|h) = ?p1(w|h)+(1??)p2(w|h) (9)
where p1(w|h) is computed as in Equation (7)
and p2(w|h) is computed by the parser. Their im-
plementation uses Roark?s (2001) top-down incre-
mental parser which estimates the probability of
200
the next word based upon the previous words of
the sentence. These prefix probabilities are calcu-
lated from a grammar, by considering the likeli-
hood of seeing the next word given the possible
grammatical relations representing the prior con-
text.
Equation (9) essentially defines a language
model which combines semantic, syntactic and
n-gram structure, and Mitchell and Lapata (2009)
demonstrate that it improves further upon a se-
mantic language model in terms of perplexity. We
argue that the probabilities from this model give
us a means to model the incrementally and predic-
tivity of the language processor in a manner that
integrates both syntactic and semantic constraints.
Converting these probabilities to surprisal should
result in a single measure of the processing cost as-
sociated with semantic and syntactic expectations.
4 Method
Data The models discussed in the previous sec-
tion were evaluated against an eye-tracking cor-
pus. Specifically, we used the English portion
of the Dundee Corpus (Kennedy and Pynte 2005)
which contains 20 texts taken from The Indepen-
dent newspaper. The corpus consists of 51,502
tokens and 9,776 types in total. It is annotated
with the eye-movement records of 10 English na-
tive speakers, who each read the whole corpus.
The eye-tracking data was preprocessed following
the methodology described in Demberg and Keller
(2008). From this data, we computed total reading
time for each word in the corpus. Our statistical
analyses were based on actual reading times, and
so we only included words that were not skipped.
We also excluded words for which the previous
word had been skipped, and words on which the
normal left-to-right movement of gaze had been
interrupted, i.e., by blinks, regressions, etc. Fi-
nally, because our focus is the influence of seman-
tic context, we selected only content words whose
prior sentential context contained at least two fur-
ther content words. The resulting data set con-
sisted of 53,704 data points, which is about 10%
of the theoretically possible total.2
2The total of all words read by all subjects is 515,020.
The pre-processing recommended by Demberg and Keller?s
(2008) results in a data sets containing 436,000 data points.
Removing non-content words leaves 205,922 data points. It
only makes sense to consider words that were actually fixated
(the eye-tracking measures used are not defined on skipped
words), which leaves 162,129 data points. Following Pynte
et al (2008), we require that the previous word was fixated,
with 70,051 data points remaining. We exclude words on
which the normal left to right movement of gaze had been
interrupted, e.g., by blinks and regressions, which results in
the final total to 53,704 data points.
Model Implementation All elements of our
model were trained on the BLLIP corpus, a col-
lection of texts from the Wall Street Journal
(years 1987?89). The training corpus consisted of
38,521,346 words. We used a development cor-
pus of 50,006 words and a test corpus of similar
size. All words were converted to lowercase and
numbers were replaced with the symbol ?num?. A
vocabulary of 20,000 words was chosen and the
remaining tokens were replaced with ?unk?.
Following Mitchell and Lapata (2009), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in Equation (6).
We also trained the LDA model on BLLIP, using
the Gibb?s sampling procedure discussed in Grif-
fiths et al (2007). We experimented with different
numbers of topics on the development set (from 10
to 1,000) and report results on the test set with 100
topics. In our experiments, the hyperparameter ?
was initialized to .5, and the ? word probabilities
were initialized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke 2002) with backoff and Kneser-Ney
smoothing. As our incremental parser we used
Roark?s (2001) parser trained on sections 2?21 of
the Penn Treebank containing 936,017 words. The
parser produces prefix probabilities for each word
of a sentence which we converted to conditional
probabilities by dividing each current probability
by the previous one.
Statistical Analysis The statistical analyses in
this paper were carried out using linear mixed
effects models (LME, Pinheiro and Bates 2000).
The latter can be thought of as generalization of
linear regression that allows the inclusion of ran-
dom factors (such as participants or items) as well
as fixed factors (e.g., word frequency). In our
analyses, we treat participant as a random factor,
which means that our models contain an intercept
term for each participant, representing the individ-
ual differences in the rates at which they read.3
We evaluated the effect of adding a factor to a
model by comparing the likelihoods of the mod-
els with and without that factor. If a ?2 test on the
3Other random factors that are appropriate for our anal-
yses are word and sentence; however, due to the large num-
ber of instances for these factors (given that the Dundee cor-
pus contains 51,502 tokens), we were not able to include
them: the model fitting algorithm we used (implemented in
the R package lme4) does not converge for such large models.
201
Factor Coefficient
Intercept ?.011
Word Length .264
Launch Distance .109
Landing Position .612
Word Frequency ?.010
Reading Time of Last Word .151
Table 1: Coefficients of the baseline LME model
for total reading time
likelihood ratio is significant, then this indicates
that the new factor significantly improves model
fit. We also experimented with adding random
slopes for participant to the model (in addition to
the random intercept); however, this either led to
non-convergence of the model fitting procedure, or
failed to result in an increase in model fit accord-
ing to the likelihood ratio test. Therefore, all mod-
els reported in the rest of this paper contain ran-
dom intercept of participants as the sole random
factor.
Rather than model raw reading times, we model
times on the log scale. This is desirable for a
number of reasons. Firstly, the raw reading times
tend to have a skew distribution and taking logs
produces something closer to normal, which is
preferable for modeling. Secondly, the regres-
sion equation makes more sense on the log scale
as the contribution of each term to raw reading
time is multiplicative rather than additive. That is,
log(t) = ?i?ixi implies t = ?i e
?ixi . In particular,
the intercept term for each participant now repre-
sents a multiplicative factor by which that partici-
pant is slower or faster.
5 Results
We computed separate mixed effects models for
three dependent variables, namely first fixation du-
ration, first pass duration, and total reading time.
We report results for total times throughout, as
the results of the other two dependent variables
are broadly similar. Our strategy was to first con-
struct a baseline model of low-level factors influ-
encing reading time, and then to take the resid-
uals from that model as the dependent variable
in subsequent analyses. In this way we removed
the effects of low-level factors before investigating
the factors associated with syntactic and semantic
constraint. This avoids problems with collinear-
ity between low-level factors and the factors we
are interested in (e.g., trigram probability is highly
correlated with word frequency). The baseline
model contained the factors word length, word fre-
Model Composition Coefficient
SSS
Additive ?.03820???
Multiplicative ?.00895???
LDA
Additive ?.02500???
Multiplicative ?.00262???
Table 2: Coefficients of LME models including
simple semantic space (SSS) or Latent Dirichlet
Allocation (LDA) as factors; ???p < .001
quency, launch distance, landing position, and the
reading time for the last fixated word, and its pa-
rameter estimates are given in Table 1. To further
reduce collinearity, we also centered all fixed fac-
tors, both in the baseline model, and in the models
fitted on the residuals that we report in the follow-
ing. Note that some intercorrelations remain be-
tween the factors, which we will discuss at the end
of Section 5.
Before investigating whether an integrated
model of semantic and syntactic constraint im-
proves the goodness of fit over the baseline, we ex-
amined the influence of semantic constraint alone.
This was necessary as compositional models have
not been previously used to model processing
difficulty. Besides, replicating Pynte et al?s
(2008) finding, we were also interested in assess-
ing whether the underlying semantic representa-
tion (simple semantic space or LDA) and com-
position function (additive versus multiplicative)
modulate reading times differentially.
We built an LME model that predicted the resid-
ual reading times of the baseline model using the
similarity scores from our composition models as
factors. We then carried out a ?2 test on the like-
lihood ratio of a model only containing the ran-
dom factor and the intercept, and a model also
containing the semantic factor (cosine similarity).
The addition of the semantic factor significantly
improves model fit for both the simple semantic
space and LDA. This result is observed for both
additive and multiplicative composition functions.
Our results are summarized in Table 2 which re-
ports the coefficients of the four LME models fit-
ted against the residuals of the baseline model, to-
gether with the p-values of the ?2 test.
Before evaluating our integrated surprisal mea-
sure, we evaluated its components individually in
order to tease their contributions apart. For ex-
ample, it may be the case that syntactic surprisal
is an overwhelmingly better predictor of reading
time than semantic surprisal, however we would
not be able to detect this by simply adding a factor
based on Equation (9) to the baseline model. The
202
Factor SSS Coef LDA Coef
? log(p) .00760??? .00760???
A
dd
? log(?) .03810??? .00622???
log(?+(1??) p2p1 ) .00953
??? .00943???
M
ul
t ? log(?) .01110??? ?.00033
log(?+(1??) p2p1 ) .00882
??? .00133
Table 3: Coefficients of nested LME models with
the components of SSS or LDA surprisal as fac-
tors; only the coefficient of the additional factor at
each step are shown
integrated surprisal measure can be written as:
S =? log(?p1 +(1??)p2) (10)
Where p2 is the incremental parser probability and
p1 is the product of the semantic component, ?,
and the trigram probability, p. This can be broken
down into the sum of two terms:
S =? log(p1)? log(?+(1??)
p2
p1
) (11)
Since the first term, ? log(p1) is itself a product it
can also be broken down further:
S =? log(p)? log(?)? log(?+(1??)
p2
p1
) (12)
Thus, to evaluate the contribution of the three
components to the integrated surprisal measure we
fitted nested LME models, i.e., we entered these
terms one at a time into a mixed effects model
and tested the significance of the improvement in
model fit for each additional term.
We again start with an LME model that only
contains the random factor and the intercept, with
the residuals of the baseline models as the depen-
dent variable. Considering the trigram model first,
we find that adding this factor to the model gives a
significant improvement in fit. Also adding the se-
mantic component (? log(?)) improves fit further,
both for additive and multiplicative composition
functions using a simple semantic space. Finally,
the addition of the parser probabilities (log(?+
(1??) p2p1 )) again improves model fit significantly.
As far as LDA is concerned, the additive model
significantly improves model fit, whereas the mul-
tiplicative one does not. These results mirror
the findings of Mitchell and Lapata (2009), who
report that a multiplicative composition function
produced the lowest perplexity for the simple se-
mantic space model, whereas an additive function
gave the best perplexity for the LDA space. Ta-
ble 3 lists the coefficients for the nested models for
Model Composition Coefficient
SSS
Additive .00804???
Multiplicative .00819???
LDA
Additive .00817???
Multiplicative .00640???
Table 4: Coefficients of LME models with inte-
grated surprisal measure (based on SSS or LDA)
as factor
all four variants of our semantic constraint mea-
sure.
Finally, we built a separate LME model where
we added the integrated surprisal measure (see
Equation (9)) to the model only containing the ran-
dom factor and the intercept (see Table 4). We
did this separately for all four versions of the in-
tegrated surprisal measure (SSS, LDA; additive,
multiplicative). We find that model fit improved
significantly all versions of integrated surprisal.
One technical issue that remains to be discussed
is collinearity, i.e., intercorrelations between the
factors in a model. The presence of collinearity
is problematic, as it can render the model fitting
procedure unstable; it can also affect the signifi-
cance of individual factors. As mentioned in Sec-
tion 4 we used two techniques to reduce collinear-
ity: residualizing and centering. Table 5 gives
an overview of the correlation coefficients for all
pairs of factors. It becomes clear that collinear-
ity has mostly been removed; there is a remaining
relationship between word length and word fre-
quency, which is expected as shorter words tend to
be more frequent. This correlation is not a prob-
lem for our analysis, as it is confined to the base-
line model. Furthermore, word frequency and tri-
gram probability are highly correlated. Again this
is expected, given that the frequencies of unigrams
and higher-level n-grams tend to be related. This
correlation is taken care of by residualizing, which
isolates the two factors: word frequency is part
of the baseline model, while trigram probability is
part of the separate models that we fit on the resid-
uals. All other correlations are small (with coeffi-
cients of .27 or less), with one exception: there is
a high correlation between the ? log(?) term and
the log(?+ (1? ?) p2p1 ) term in the multiplicative
LDA model. This collinearity issue may explain
the absence of a significant improvement in model
fit when these two terms are added to the baseline
(see Table 3).
203
Factor Len Freq ?l(p)?l(?)
Frequency ?.310
? log(p) .230?.700
S
S
S
A
dd
? log(?) .016?.120 .025
log(?+(1??) p2p1 ) .024 .036?.270 .065
S
S
S
M
ul
t ? log(?) ?.015?.110 .035
log(?+(1??) p2p1 ) .020 .028?.260 .160
L
D
A
A
dd
? log(?) ?.024?.130 .046
log(?+(1??) p2p1 ) .005 .014?.250 .030
L
D
A
M
ul
t ? log(?) ?.120 .006?.046
log(?+(1??) p2p1 )?.089?.005?.180 .740
Table 5: Intercorrelations between model factors
6 Discussion
In this paper we investigated the contributions of
syntactic and semantic constraint in modeling pro-
cessing difficulty. Our work departs from previ-
ous approaches in that we propose a single mea-
sure which integrates syntactic and semantic fac-
tors. Evaluation on an eye-tracking corpus shows
that our measure predicts reading time better than
a baseline model that captures low-level factors
in reading (word length, landing position, etc.).
Crucially, we were able to show that the semantic
component of our measure improves reading time
predictions over and above a model that includes
syntactic measures (based on a trigram model and
incremental parser). This means that semantic
costs are a significant predictor of reading time in
addition to the well-known syntactic surprisal.
An open issue is whether a single, integrated
measure (as evaluated in Table 4) fits the eye-
movement data significantly better than separate
measures for trigram, syntactic, and semantic sur-
prisal (as evaluated in Table 3. However, we are
not able to investigate this hypothesis: our ap-
proach to testing the significance of factors re-
quires nested models; the log-likelihood test (see
Section 4) is only able to establish whether adding
a factor to a model improves its fit; it cannot com-
pare models with disjunct sets of factors (such as
a model containing the integrated surprisal mea-
sure and one containing the three separate ones).
However, we would argue that a single, integrated
measure that captures human predictive process-
ing is preferable over a collection of separate mea-
sures. It is conceptually simpler (as it is more par-
simonious), and is also easier to use in applica-
tions (such as readability prediction). Finally, an
integrated measure requires less parameters; our
definition of surprisal in 12 is simply the sum of
the trigram, syntactic, and semantic components.
An LME model containing separate factors, on the
other hand, requires a coefficient for each of them,
and thus has more parameters.
In evaluating our model, we adopted a broad
coverage approach using the reading time data
from a naturalistic corpus rather than artificially
constructed experimental materials. In doing so,
we were able to compare different syntactic and
semantic costs on the same footing. Previous
analyses of semantic constraint have been con-
ducted on different eye-tracking corpora (Dundee
and Embra Corpus) and on different languages
(English, French). Moreover, comparisons of the
individual contributions of syntactic and semantic
factors were generally absent from the literature.
Our analysis showed that both of these factors can
be captured by our integrated surprisal measure
which is uniformly probabilistic and thus prefer-
able to modeling semantic and syntactic costs dis-
jointly using a mixture of probabilistic and non-
probabilistic measures.
An interesting question is which aspects of se-
mantics our model is able to capture, i.e., why
does the combination of LSA or LDA representa-
tions with an incremental parser yield a better fit of
the behavioral data. In the psycholinguistic liter-
ature, various types of semantic information have
been investigated: lexical semantics (word senses,
selectional restrictions, thematic roles), senten-
tial semantics (scope, binding), and discourse se-
mantics (coreference and coherence); see Keller
(2010) of a detailed discussion. We conjecture that
our model is mainly capturing lexical semantics
(through the vector space representation of words)
and sentential semantics (through the multiplica-
tion or addition of words). However, discourse
coreference effects (such as the ones reported by
Altmann and Steedman (1988) and much subse-
quent work) are probably not amenable to a treat-
ment in terms of vector space semantics; an ex-
plicit representation of discourse entities and co-
reference relations is required (see Dubey 2010
for a model of human sentence processing that can
handle coreference).
A key objective for future work will be to in-
vestigate models that integrate semantic constraint
with syntactic predictions more tightly. For ex-
ample, we could envisage a parser that uses se-
mantic representations to guide its search, e.g., by
pruning syntactic analyses that have a low seman-
tic probability. At the same time, the semantic
model should have access to syntactic informa-
tion, i.e., the composition of word representations
should take their syntactic relationships into ac-
count, rather than just linear order.
204
References
ACL. 2010. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Lin-
guistics. Uppsala.
Altmann, Gerry T. M. and Yuki Kamide. 1999.
Incremental interpretation at verbs: Restricting
the domain of subsequent reference. Cognition
73:247?264.
Altmann, Gerry T. M. and Mark J. Steedman.
1988. Interaction with context during human
sentence processing. Cognition 30(3):191?238.
Bellegarda, Jerome R. 2000. Exploiting latent se-
mantic information in statistical language mod-
eling. Proceedings of the IEEE 88(8):1279?
1296.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1995. Using linear algebra
for intelligent information retrieval. SIAM re-
view 37(4):573?595.
Bever, Thomas G. 1970. The cognitive basis for
linguistic strutures. In J. R. Hayes, editor, Cog-
nition and the Development of Language, Wi-
ley, New York, pages 279?362.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet alocation. Journal
of Machine Learning Research 3:993?1022.
Bullinaria, John A. and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods 39:510?526.
Clifton, Charles, Adrian Staub, and Keith Rayner.
2007. Eye movement in reading words and sen-
tences. In R V Gompel, M Fisher, W Murray,
and R L Hill, editors, Eye Movements: A Win-
dow in Mind and Brain, Elsevier, pages 341?
372.
Coccaro, Noah and Daniel Jurafsky. 1998. To-
wards better integration of semantic predictors
in satistical language modeling. In Proceedings
of the 5th International Conference on Spoken
Language Processing. Sydney, Australia, pages
2403?2406.
Demberg, Vera and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition
101(2):193?210.
Dubey, Amit. 2010. The influence of discourse on
syntax: A psycholinguistic model of sentence
processing. In ACL.
Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth.
2008. Parsing costs as predictors of reading dif-
ficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Re-
search 2(1):1?12.
Frank, Stefan L. 2009. Surprisal-based compar-
ison between a symbolic and a connectionist
model of sentence processing. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society. Austin, TX, pages 139?1144.
Gibson, Edward. 2000. Dependency locality the-
ory: A distance-dased theory of linguistic com-
plexity. In Alec Marantz, Yasushi Miyashita,
and Wayne O?Neil, editors, Image, Language,
Brain: Papers from the First Mind Articulation
Project Symposium, MIT Press, Cambridge,
MA, pages 95?126.
Gildea, Daniel and Thomas Hofmann. 1999.
Topic-based language models using EM. In
Proceedings of the 6th European Conference
on Speech Communiation and Technology. Bu-
dapest, Hungary, pages 2167?2170.
Griffiths, Thomas L., Mark Steyvers, and
Joshua B. Tenenbaum. 2007. Topics in se-
mantic representation. Psychological Review
114(2):211?244.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chap-
ter of the Association. Association for Compu-
tational Linguistics, Pittsburgh, PA, volume 2,
pages 159?166.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In ACL.
Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
on-foveal effects in normal reading. Vision Re-
search 45:153?168.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627?645.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato?s problem: the latent seman-
tic analysis theory of acquisition, induction and
representation of knowledge. Psychological Re-
view 104(2):211?240.
Levy, Roger. 2008. Expectation-based syntactic
comprehension. Cognition 106(3):1126?1177.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522?523.
McDonald, Scott. 2000. Environmental Determi-
nants of Lexical Processing Effort. Ph.D. thesis,
University of Edinburgh.
205
McDonald, Scott and Chris Brew. 2004. A dis-
tributional model of semantic context effects in
lexical processing. In Proceedings of the 42th
Annual Meeting of the Association for Com-
putational Linguistics. Barcelona, Spain, pages
17?24.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in read-
ing: The influence of transitional probabilities
on eye movements. Vision Research 43:1735?
1751.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Pro-
ceedings of ACL-08: HLT . Columbus, OH,
pages 236?244.
Mitchell, Jeff and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing. Sin-
gapore, pages 430?439.
Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-
ence and reading time in sentence processing. In
Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14. MIT Press,
Cambridge, MA, pages 59?65.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics
33(2):161?199.
Pado?, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Pinheiro, Jose C. and Douglas M. Bates.
2000. Mixed-effects Models in S and S-PLUS.
Springer, New York.
Pinker, Steven. 1994. The Language Instinct: How
the Mind Creates Language. HarperCollins,
New York.
Plate, Tony A. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Net-
works 6(3):623?641.
Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading
normal text: A multiple-regression analysis. Vi-
sion Research 48:2172?2183.
Rayner, Keith. 1998. Eye movements in read-
ing and information processing: 20 years of re-
search. Psychological Bulletin 124(3):372?422.
Roark, Brian. 2001. Probabilistic top-down pars-
ing and language modeling. Computational
Linguistics 27(2):249?276.
Roark, Brian, Asaf Bachrach, Carlos Cardenas,
and Christophe Pallier. 2009. Deriving lex-
ical and syntactic expectation-based measures
for psycholinguistic modeling via incremental
top-down parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing. Association for Compu-
tational Linguistics, Singapore, pages 324?333.
Smolensky, Paul. 1990. Tensor product vari-
able binding and the representation of symbolic
structures in connectionist systems. Artificial
Intelligence 46:159?216.
Stanovich, Kieth E. and Richard F. West. 1981.
The effect of sentence context on ongoing word
recognition: Tests of a two-pricess theory. Jour-
nal of Experimental Psychology: Human Per-
ception and Performance 7:658?672.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either . . .or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425?436.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the
Internatinal Conference on Spoken Language
Processing. Denver, Colorado.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291?305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632?1634.
van Berkum, Jos J. A., Colin M. Brown, and Peter
Hagoort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147?182.
Wright, Barton and Merrill F. Garrett. 1984. Lex-
ical decision in sentences: Effects of syntactic
structure. Memory and Cognition 12:31?45.
206
Proceedings of the ACL 2010 Conference Short Papers, pages 60?67,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cognitively Plausible Models of Human Language Processing
Frank Keller
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
keller@inf.ed.ac.uk
Abstract
We pose the development of cognitively
plausible models of human language pro-
cessing as a challenge for computational
linguistics. Existing models can only deal
with isolated phenomena (e.g., garden
paths) on small, specifically selected data
sets. The challenge is to build models that
integrate multiple aspects of human lan-
guage processing at the syntactic, seman-
tic, and discourse level. Like human lan-
guage processing, these models should be
incremental, predictive, broad coverage,
and robust to noise. This challenge can
only be met if standardized data sets and
evaluation measures are developed.
1 Introduction
In many respects, human language processing is
the ultimate goldstandard for computational lin-
guistics. Humans understand and generate lan-
guage with amazing speed and accuracy, they are
able to deal with ambiguity and noise effortlessly
and can adapt to new speakers, domains, and reg-
isters. Most surprisingly, they achieve this compe-
tency on the basis of limited training data (Hart
and Risley, 1995), using learning algorithms that
are largely unsupervised.
Given the impressive performance of humans
as language processors, it seems natural to turn
to psycholinguistics, the discipline that studies hu-
man language processing, as a source of informa-
tion about the design of efficient language pro-
cessing systems. Indeed, psycholinguists have un-
covered an impressive array of relevant facts (re-
viewed in Section 2), but computational linguists
are often not aware of this literature, and results
about human language processing rarely inform
the design, implementation, or evaluation of artifi-
cial language processing systems.
At the same time, research in psycholinguis-
tics is often oblivious of work in computational
linguistics (CL). To test their theories, psycholin-
guists construct computational models of hu-
man language processing, but these models of-
ten fall short of the engineering standards that
are generally accepted in the CL community
(e.g., broad coverage, robustness, efficiency): typ-
ical psycholinguistic models only deal with iso-
lated phenomena and fail to scale to realistic data
sets. A particular issue is evaluation, which is typ-
ically anecdotal, performed on a small set of hand-
crafted examples (see Sections 3).
In this paper, we propose a challenge that re-
quires the combination of research efforts in com-
putational linguistics and psycholinguistics: the
development of cognitively plausible models of
human language processing. This task can be de-
composed into a modeling challenge (building
models that instantiate known properties of hu-
man language processing) and a data and evalu-
ation challenge (accounting for experimental find-
ings and evaluating against standardized data sets),
which we will discuss in turn.
2 Modeling Challenge
2.1 Key Properties
The first part of the challenge is to develop a model
that instantiates key properties of human language
processing, as established by psycholinguistic ex-
perimentation (see Table 1 for an overview and
representative references).1 A striking property of
the human language processor is its efficiency and
robustness. For the vast majority of sentences, it
will effortlessly and rapidly deliver the correct
analysis, even in the face of noise and ungrammat-
icalities. There is considerable experimental evi-
1Here an in the following, we will focus on sentence
processing, which is often regarded as a central aspect of
human language processing. A more comprehensive answer
to our modeling challenge should also include phonological
and morphological processing, semantic inference, discourse
processing, and other non-syntactic aspects of language pro-
cessing. Furthermore, established results regarding the inter-
face between language processing and non-linguistic cogni-
tion (e.g., the sensorimotor system) should ultimately be ac-
counted for in a fully comprehensive model.
60
Model
Property Evidence
Rank Surp Pred Stack
Efficiency and robustness Ferreira et al (2001); Sanford and Sturt (2002) ? ? ? +
Broad coverage Crocker and Brants (2000) + + ? +
Incrementality and connectedness Tanenhaus et al (1995); Sturt and Lombardo (2005) + + + +
Prediction Kamide et al (2003); Staub and Clifton (2006) ? ? + ?
Memory cost Gibson (1998); Vasishth and Lewis (2006) ? ? + +
Table 1: Key properties of human language processing and their instantiation in various models of sentence processing (see
Section 2 for details)
dence that shallow processing strategies are used
to achieve this. The processor also achieves broad
coverage: it can deal with a wide variety of syntac-
tic constructions, and is not restricted by the do-
main, register, or modality of the input.
Human language processing is also word-by-
word incremental. There is strong evidence that
a new word is integrated as soon as it is avail-
able into the representation of the sentence thus
far. Readers and listeners experience differential
processing difficulty during this integration pro-
cess, depending on the properties of the new word
and its relationship to the preceding context. There
is evidence that the processor instantiates a strict
form of incrementality by building only fully con-
nected trees. Furthermore, the processor is able
to make predictions about upcoming material on
the basis of sentence prefixes. For instance, listen-
ers can predict an upcoming post-verbal element
based on the semantics of the preceding verb. Or
they can make syntactic predictions, e.g., if they
encounter the word either, they predict an upcom-
ing or and the type of complement that follows it.
Another key property of human language pro-
cessing is the fact that it operates with limited
memory, and that structures in memory are subject
to decay and interference. In particular, the pro-
cessor is known to incur a distance-based memory
cost: combining the head of a phrase with its syn-
tactic dependents is more difficult the more depen-
dents have to be integrated and the further away
they are. This integration process is also subject
to interference from similar items that have to be
held in memory at the same time.
2.2 Current Models
The challenge is to develop a computational model
that captures the key properties of human language
processing outlined in the previous section. A
number of relevant models have been developed,
mostly based on probabilistic parsing techniques,
but none of them instantiates all the key proper-
ties discussed above (Table 1 gives an overview of
model properties).2
The earliest approaches were ranking-based
models (Rank), which make psycholinguistic pre-
dictions based on the ranking of the syntactic
analyses produced by a probabilistic parser. Ju-
rafsky (1996) assumes that processing difficulty
is triggered if the correct analysis falls below a
certain probability threshold (i.e., is pruned by
the parser). Similarly, Crocker and Brants (2000)
assume that processing difficulty ensures if the
highest-ranked analysis changes from one word to
the next. Both approaches have been shown to suc-
cessfully model garden path effects. Being based
on probabilistic parsing techniques, ranking-based
models generally achieve a broad coverage, but
their efficiency and robustness has not been evalu-
ated. Also, they are not designed to capture syntac-
tic prediction or memory effects (other than search
with a narrow beam in Brants and Crocker 2000).
The ranking-based approach has been gener-
alized by surprisal models (Surp), which pre-
dict processing difficulty based on the change in
the probability distribution over possible analy-
ses from one word to the next (Hale, 2001; Levy,
2008; Demberg and Keller, 2008a; Ferrara Boston
et al, 2008; Roark et al, 2009). These models
have been successful in accounting for a range of
experimental data, and they achieve broad cover-
age. They also instantiate a limited form of predic-
tion, viz., they build up expectations about the next
word in the input. On the other hand, the efficiency
and robustness of these models has largely not
been evaluated, and memory costs are not mod-
eled (again except for restrictions in beam size).
The prediction model (Pred) explicitly predicts
syntactic structure for upcoming words (Demberg
and Keller, 2008b, 2009), thus accounting for ex-
perimental results on predictive language process-
ing. It also implements a strict form of incre-
2We will not distinguish between model and linking the-
ory, i.e., the set of assumptions that links model quantities
to behavioral data (e.g., more probably structures are easier
to process). It is conceivable, for instance, that a stack-based
model is combined with a linking theory based on surprisal.
61
Factor Evidence
Word senses Roland and Jurafsky (2002)
Selectional re-
strictions
Garnsey et al (1997); Pickering and
Traxler (1998)
Thematic roles McRae et al (1998); Pickering et al
(2000)
Discourse ref-
erence
Altmann and Steedman (1988); Grod-
ner and Gibson (2005)
Discourse
coherence
Stewart et al (2000); Kehler et al
(2008)
Table 2: Semantic factors in human language processing
mentality by building fully connected trees. Mem-
ory costs are modeled directly as a distance-based
penalty that is incurred when a prediction has to be
verified later in the sentence. However, the current
implementation of the prediction model is neither
robust and efficient nor offers broad coverage.
Recently, a stack-based model (Stack) has been
proposed that imposes explicit, cognitively mo-
tivated memory constraints on the parser, in ef-
fect limiting the stack size available to the parser
(Schuler et al, 2010). This delivers robustness, ef-
ficiency, and broad coverage, but does not model
syntactic prediction. Unlike the other models dis-
cussed here, no psycholinguistic evaluation has
been conducted on the stack-based model, so its
cognitive plausibility is preliminary.
2.3 Beyond Parsing
There is strong evidence that human language pro-
cessing is driven by an interaction of syntactic, se-
mantic, and discourse processes (see Table 2 for
an overview and references). Considerable exper-
imental work has focused on the semantic prop-
erties of the verb of the sentence, and verb sense,
selectional restrictions, and thematic roles have all
been shown to interact with syntactic ambiguity
resolution. Another large body of research has elu-
cidated the interaction of discourse processing and
syntactic processing. The most-well known effect
is probably that of referential context: syntactic
ambiguities can be resolved if a discourse con-
text is provided that makes one of the syntactic
alternatives more plausible. For instance, in a con-
text that provides two possible antecedents for a
noun phrase, the processor will prefer attaching a
PP or a relative clause such that it disambiguates
between the two antecedents; garden paths are re-
duced or disappear. Other results point to the im-
portance of discourse coherence for sentence pro-
cessing, an example being implicit causality.
The challenge facing researchers in compu-
tational and psycholinguistics therefore includes
the development of language processing models
that combine syntactic processing with semantic
and discourse processing. So far, this challenge is
largely unmet: there are some examples of models
that integrate semantic processes such as thematic
role assignment into a parsing model (Narayanan
and Jurafsky, 2002; Pado? et al, 2009). However,
other semantic factors are not accounted for by
these models, and incorporating non-lexical as-
pects of semantics into models of sentence pro-
cessing is a challenge for ongoing research. Re-
cently, Dubey (2010) has proposed an approach
that combines a probabilistic parser with a model
of co-reference and discourse inference based on
probabilistic logic. An alternative approach has
been taken by Pynte et al (2008) and Mitchell
et al (2010), who combine a vector-space model
of semantics (Landauer and Dumais, 1997) with a
syntactic parser and show that this results in pre-
dictions of processing difficulty that can be vali-
dated against an eye-tracking corpus.
2.4 Acquisition and Crosslinguistics
All models of human language processing dis-
cussed so far rely on supervised training data. This
raises another aspect of the modeling challenge:
the human language processor is the product of
an acquisition process that is largely unsupervised
and has access to only limited training data: chil-
dren aged 12?36 months are exposed to between
10 and 35 million words of input (Hart and Ris-
ley, 1995). The challenge therefore is to develop
a model of language acquisition that works with
such small training sets, while also giving rise to
a language processor that meets the key criteria
in Table 1. The CL community is in a good posi-
tion to rise to this challenge, given the significant
progress in unsupervised parsing in recent years
(starting from Klein and Manning 2002). How-
ever, none of the existing unsupervised models has
been evaluated against psycholinguistic data sets,
and they are not designed to meet even basic psy-
cholinguistic criteria such as incrementality.
A related modeling challenge is the develop-
ment of processing models for languages other
than English. There is a growing body of ex-
perimental research investigating human language
processing in other languages, but virtually all ex-
isting psycholinguistic models only work for En-
glish (the only exceptions we are aware of are
Dubey et al?s (2008) and Ferrara Boston et al?s
62
(2008) parsing models for German). Again, the
CL community has made significant progress in
crosslinguistic parsing, especially using depen-
dency grammar (Hajic?, 2009), and psycholinguis-
tic modeling could benefit from this in order to
meet the challenge of developing crosslinguisti-
cally valid models of human language processing.
3 Data and Evaluation Challenge
3.1 Test Sets
The second key challenge that needs to be ad-
dressed in order to develop cognitively plausible
models of human language processing concerns
test data and model evaluation. Here, the state of
the art in psycholinguistic modeling lags signif-
icantly behind standards in the CL community.
Most of the models discussed in Section 2 have not
been evaluated rigorously. The authors typically
describe their performance on a small set of hand-
picked examples; no attempts are made to test on
a range of items from the experimental literature
and determine model fit directly against behavioral
measures (e.g., reading times). This makes it very
hard to obtain a realistic estimate of how well the
models achieve their aim of capturing human lan-
guage processing behavior.
We therefore suggest the development of stan-
dard test sets for psycholinguistic modeling, simi-
lar to what is commonplace for tasks in computa-
tional linguistics: parsers are evaluated against the
Penn Treebank, word sense disambiguation sys-
tems against the SemEval data sets, co-reference
systems against the Tipster or ACE corpora, etc.
Two types of test data are required for psycholin-
guistic modeling. The first type of test data con-
sists of a collection of representative experimental
results. This collection should contain the actual
experimental materials (sentences or discourse
fragments) used in the experiments, together with
the behavioral measurements obtained (reading
times, eye-movement records, rating judgments,
etc.). The experiments included in this test set
would be chosen to cover a wide range of ex-
perimental phenomena, e.g., garden paths, syntac-
tic complexity, memory effects, semantic and dis-
course factors. Such a test set will enable the stan-
dardized evaluation of psycholinguistic models by
comparing the model predictions (rankings, sur-
prisal values, memory costs, etc.) against behav-
ioral measures on a large set of items. This way
both the coverage of a model (how many phenom-
ena can it account for) and its accuracy (how well
does it fit the behavioral data) can be assessed.
Experimental test sets should be complemented
by test sets based on corpus data. In order to as-
sess the efficiency, robustness, and broad cover-
age of a model, a corpus of unrestricted, naturally
occurring text is required. The use of contextual-
ized language data makes it possible to assess not
only syntactic models, but also models that capture
discourse effects. These corpora need to be anno-
tated with behavioral measures, e.g., eye-tracking
or reading time data. Some relevant corpora have
already been constructed, see the overview in Ta-
ble 3, and various authors have used them for
model evaluation (Demberg and Keller, 2008a;
Pynte et al, 2008; Frank, 2009; Ferrara Boston
et al, 2008; Patil et al, 2009; Roark et al, 2009;
Mitchell et al, 2010).
However, the usefulness of the psycholinguis-
tic corpora in Table 3 is restricted by the absence
of gold-standard linguistic annotation (though the
French part of the Dundee corpus, which is syn-
tactically annotated). This makes it difficult to test
the accuracy of the linguistic structures computed
by a model, and restricts evaluation to behavioral
predictions. The challenge is therefore to collect
a standardized test set of naturally occurring text
or speech enriched not only with behavioral vari-
ables, but also with syntactic and semantic anno-
tation. Such a data set could for example be con-
structed by eye-tracking section 23 of the Penn
Treebank (which is also part of Propbank, and thus
has both syntactic and thematic role annotation).
In computational linguistics, the development
of new data sets is often stimulated by competi-
tions in which systems are compared on a stan-
dardized task, using a data set specifically de-
signed for the competition. Examples include the
CoNLL shared task, SemEval, or TREC in com-
putational syntax, semantics, and discourse, re-
spectively. A similar competition could be devel-
oped for computational psycholinguistics ? maybe
along the lines of the model comparison chal-
lenges that held at the International Conference
on Cognitive Modeling. These challenges provide
standardized task descriptions and data sets; par-
ticipants can enter their cognitive models, which
were then compared using a pre-defined evalua-
tion metric.3
3The ICCM 2009 challenge was the Dynamic Stock and
Flows Task, for more information see http://www.hss.
cmu.edu/departments/sds/ddmlab/modeldsf/.
63
Corpus Language Words Participants Method Reference
Dundee Corpus English, French 50,000 10 Eye-tracking Kennedy and Pynte (2005)
Potsdam Corpus German 1,138 222 Eye-tracking Kliegl et al (2006)
MIT Corpus English 3,534 23 Self-paced reading Bachrach (2008)
Table 3: Test corpora that have been used for psycholinguistic modeling of sentence processing; note that the Potsdam Corpus
consists of isolated sentences, rather than of continuous text
3.2 Behavioral and Neural Data
As outlined in the previous section, a number of
authors have evaluated psycholinguistic models
against eye-tracking or reading time corpora. Part
of the data and evaluation challenge is to extend
this evaluation to neural data as provided by event-
related potential (ERP) or brain imaging studies
(e.g., using functional magnetic resonance imag-
ing, fMRI). Neural data sets are considerably more
complex than behavioral ones, and modeling them
is an important new task that the community is
only beginning to address. Some recent work has
evaluated models of word semantics against ERP
(Murphy et al, 2009) or fMRI data (Mitchell et al,
2008).4 This is a very promising direction, and the
challenge is to extend this approach to the sentence
and discourse level (see Bachrach 2008). Again,
it will again be necessary to develop standardized
test sets of both experimental data and corpus data.
3.3 Evaluation Measures
We also anticipate that the availability of new test
data sets will facilitate the development of new
evaluation measures that specifically test the va-
lidity of psycholinguistic models. Established CL
evaluation measures such as Parseval are of lim-
ited use, as they can only test the linguistic, but not
the behavioral or neural predictions of a model.
So far, many authors have relied on qualita-
tive evaluation: if a model predicts a difference
in (for instance) reading time between two types
of sentences where such a difference was also
found experimentally, then that counts as a suc-
cessful test. In most cases, no quantitative evalu-
ation is performed, as this would require model-
ing the reading times for individual item and in-
dividual participants. Suitable procedures for per-
forming such tests do not currently exist; linear
mixed effects models (Baayen et al, 2008) pro-
vide a way of dealing with item and participant
variation, but crucially do not enable direct com-
parisons between models in terms of goodness of
fit.
4These data sets were released as part of the NAACL-
2010 Workshop on Computational Neurolinguistics.
Further issues arise from the fact that we of-
ten want to compare model fit for multiple experi-
ments (ideally without reparametrizing the mod-
els), and that various mutually dependent mea-
sures are used for evaluation, e.g., processing ef-
fort at the sentence, word, and character level. An
important open challenge is there to develop eval-
uation measures and associated statistical proce-
dures that can deal with these problems.
4 Conclusions
In this paper, we discussed the modeling and
data/evaluation challenges involved in developing
cognitively plausible models of human language
processing. Developing computational models is
of scientific importance in so far as models are im-
plemented theories: models of language process-
ing allow us to test scientific hypothesis about the
cognitive processes that underpin language pro-
cessing. This type of precise, formalized hypoth-
esis testing is only possible if standardized data
sets and uniform evaluation procedures are avail-
able, as outlined in the present paper. Ultimately,
this approach enables qualitative and quantitative
comparisons between theories, and thus enhances
our understanding of a key aspect of human cog-
nition, language processing.
There is also an applied side to the proposed
challenge. Once computational models of human
language processing are available, they can be
used to predict the difficulty that humans experi-
ence when processing text or speech. This is use-
ful for a number applications: for instance, nat-
ural language generation would benefit from be-
ing able to assess whether machine-generated text
or speech is easy to process. For text simplifica-
tion (e.g., for children or impaired readers), such a
model is even more essential. It could also be used
to assess the readability of text, which is of interest
in educational applications (e.g., essay scoring). In
machine translation, evaluating the fluency of sys-
tem output is crucial, and a model that predicts
processing difficulty could be used for this, or to
guide the choice between alternative translations,
and maybe even to inform human post-editing.
64
References
Altmann, Gerry T. M. and Mark J. Steedman.
1988. Interaction with context during human
sentence processing. Cognition 30(3):191?238.
Baayen, R. H., D. J. Davidson, and D. M. Bates.
2008. Mixed-effects modeling with crossed ran-
dom effects for subjects and items. Journal of
Memory and Language to appear.
Bachrach, Asaf. 2008. Imaging Neural Correlates
of Syntactic Complexity in a Naturalistic Con-
text. Ph.D. thesis, Massachusetts Institute of
Technology, Cambridge, MA.
Brants, Thorsten and Matthew W. Crocker. 2000.
Probabilistic parsing and psychological plau-
sibility. In Proceedings of the 18th Interna-
tional Conference on Computational Linguis-
tics. Saarbru?cken/Luxembourg/Nancy, pages
111?117.
Crocker, Matthew W. and Thorsten Brants. 2000.
Wide-coverage probabilistic sentence process-
ing. Journal of Psycholinguistic Research
29(6):647?669.
Demberg, Vera and Frank Keller. 2008a. Data
from eye-tracking corpora as evidence for theo-
ries of syntactic processing complexity. Cogni-
tion 101(2):193?210.
Demberg, Vera and Frank Keller. 2008b. A psy-
cholinguistically motivated version of TAG. In
Proceedings of the 9th International Workshop
on Tree Adjoining Grammars and Related For-
malisms. Tu?bingen, pages 25?32.
Demberg, Vera and Frank Keller. 2009. A com-
putational model of prediction in human pars-
ing: Unifying locality and surprisal effects. In
Niels Taatgen and Hedderik van Rijn, editors,
Proceedings of the 31st Annual Conference of
the Cognitive Science Society. Cognitive Sci-
ence Society, Amsterdam, pages 1888?1893.
Dubey, Amit. 2010. The influence of discourse on
syntax: A psycholinguistic model of sentence
processing. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics. Uppsala.
Dubey, Amit, Frank Keller, and Patrick Sturt.
2008. A probabilistic corpus-based model of
syntactic parallelism. Cognition 109(3):326?
344.
Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth.
2008. Parsing costs as predictors of reading dif-
ficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Re-
search 2(1):1?12.
Ferreira, Fernanda, Kiel Christianson, and An-
drew Hollingworth. 2001. Misinterpretations of
garden-path sentences: Implications for models
of sentence processing and reanalysis. Journal
of Psycholinguistic Research 30(1):3?20.
Frank, Stefan L. 2009. Surprisal-based compar-
ison between a symbolic and a connectionist
model of sentence processing. In Niels Taat-
gen and Hedderik van Rijn, editors, Proceed-
ings of the 31st Annual Conference of the Cog-
nitive Science Society. Cognitive Science Soci-
ety, Amsterdam, pages 1139?1144.
Garnsey, Susan M., Neal J. Pearlmutter, Elisa-
beth M. Myers, and Melanie A. Lotocky. 1997.
The contributions of verb bias and plausibility
to the comprehension of temporarily ambiguous
sentences. Journal of Memory and Language
37(1):58?93.
Gibson, Edward. 1998. Linguistic complexity:
locality of syntactic dependencies. Cognition
68:1?76.
Grodner, Dan and Edward Gibson. 2005. Conse-
quences of the serial nature of linguistic input.
Cognitive Science 29:261?291.
Hajic?, Jan, editor. 2009. Proceedings of the 13th
Conference on Computational Natural Lan-
guage Learning: Shared Task. Association for
Computational Linguistics, Boulder, CO.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Pittsburgh, PA, volume 2, pages 159?166.
Hart, Betty and Todd R. Risley. 1995. Meaning-
ful Differences in the Everyday Experience of
Young American Children. Paul H. Brookes,
Baltimore, MD.
Jurafsky, Daniel. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science 20(2):137?194.
Kamide, Yuki, Gerry T. M. Altmann, and Sarah L.
Haywood. 2003. The time-course of prediction
in incremental sentence processing: Evidence
65
from anticipatory eye movements. Journal of
Memory and Language 49:133?156.
Kehler, Andrew, Laura Kertz, Hannah Rohde, and
Jeffrey L. Elman. 2008. Coherence and coref-
erence revisited. Journal of Semantics 25(1):1?
44.
Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
on-foveal effects in normal reading. Vision Re-
search 45:153?168.
Klein, Dan and Christopher Manning. 2002. A
generative constituent-context model for im-
proved grammar induction. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics. Philadelphia, pages
128?135.
Kliegl, Reinhold, Antje Nuthmann, and Ralf Eng-
bert. 2006. Tracking the mind during reading:
The influence of past, present, and future words
on fixation durations. Journal of Experimental
Psychology: General 135(1):12?35.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent se-
mantic analysis theory of acquisition, induction
and representation of knowledge. Psychologi-
cal Review 104(2):211?240.
Levy, Roger. 2008. Expectation-based syntactic
comprehension. Cognition 106(3):1126?1177.
McRae, Ken, Michael J. Spivey-Knowlton, and
Michael K. Tanenhaus. 1998. Modeling the in-
fluence of thematic fit (and other constraints)
in on-line sentence comprehension. Journal of
Memory and Language 38(3):283?312.
Mitchell, Jeff, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and semantic fac-
tors in processing difficulty: An integrated mea-
sure. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Lin-
guistics. Uppsala.
Mitchell, Tom M., Svetlana V. Shinkareva, An-
drew Carlson, Kai-Min Chang, Vicente L.
Malave, Robert A. Mason, and Marcel Adam
Just3. 2008. Predicting human brain activity as-
sociated with the meanings of nouns. Science
320(5880):1191?1195.
Murphy, Brian, Marco Baroni, and Massimo Poe-
sio. 2009. EEG responds to conceptual stimuli
and corpus semantics. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing. Singapore, pages 619?
627.
Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-
ence and reading time in sentence processing. In
Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14. MIT Press,
Cambridge, MA, pages 59?65.
Pado?, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Patil, Umesh, Shravan Vasishth, and Reinhold
Kliegl. 2009. Compound effect of probabilis-
tic disambiguation and memory retrievals on
sentence processing: Evidence from an eye-
tracking corpus. In A. Howes, D. Peebles,
and R. Cooper, editors, Proceedings of 9th In-
ternational Conference on Cognitive Modeling.
Manchester.
Pickering, Martin J. and Martin J. Traxler. 1998.
Plausibility and recovery from garden paths: An
eye-tracking study. Journal of Experimental
Psychology: Learning Memory and Cognition
24(4):940?961.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447?475.
Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading
normal text: A multiple-regression analysis. Vi-
sion Research 48(21):2172?2183.
Roark, Brian, Asaf Bachrach, Carlos Cardenas,
and Christophe Pallier. 2009. Deriving lex-
ical and syntactic expectation-based measures
for psycholinguistic modeling via incremental
top-down parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Singapore, pages 324?333.
Roland, Douglas and Daniel Jurafsky. 2002. Verb
sense and verb subcategorization probabilities.
In Paola Merlo and Suzanne Stevenson, editors,
The Lexical Basis of Sentence Processing: For-
mal, Computational, and Experimental Issues,
John Bejamins, Amsterdam, pages 325?346.
Sanford, Anthony J. and Patrick Sturt. 2002.
66
Depth of processing in language comprehen-
sion: Not noticing the evidence. Trends in Cog-
nitive Sciences 6:382?386.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-
coverage parsing using human-like mem-
ory constraints. Computational Linguistics
26(1):1?30.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either . . . or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425?436.
Stewart, Andrew J., Martin J. Pickering, and An-
thony J. Sanford. 2000. The time course of the
influence of implicit causality information: Fo-
cusing versus integration accounts. Journal of
Memory and Language 42(3):423?443.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291?305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632?1634.
Vasishth, Shravan and Richard L. Lewis. 2006.
Argument-head distance and processing com-
plexity: Explaining both locality and antilocal-
ity effects. Language 82(4):767?794.
67
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 452?457,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Comparing Automatic Evaluation Measures for Image Description
Desmond Elliott and Frank Keller
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
d.elliott@ed.ac.uk, keller@inf.ed.ac.uk
Abstract
Image description is a new natural lan-
guage generation task, where the aim is to
generate a human-like description of an im-
age. The evaluation of computer-generated
text is a notoriously difficult problem, how-
ever, the quality of image descriptions has
typically been measured using unigram
BLEU and human judgements. The focus
of this paper is to determine the correlation
of automatic measures with human judge-
ments for this task. We estimate the correla-
tion of unigram and Smoothed BLEU, TER,
ROUGE-SU4, and Meteor against human
judgements on two data sets. The main
finding is that unigram BLEU has a weak
correlation, and Meteor has the strongest
correlation with human judgements.
1 Introduction
Recent advances in computer vision and natural
language processing have led to an upsurge of re-
search on tasks involving both vision and language.
State of the art visual detectors have made it possi-
ble to hypothesise what is in an image (Guillaumin
et al, 2009; Felzenszwalb et al, 2010), paving
the way for automatic image description systems.
The aim of such systems is to extract and reason
about visual aspects of images to generate a human-
like description. An example of the type of image
and gold-standard descriptions available can be
seen in Figure 1. Recent approaches to this task
have been based on slot-filling (Yang et al, 2011;
Elliott and Keller, 2013), combining web-scale n-
grams (Li et al, 2011), syntactic tree substitution
(Mitchell et al, 2012), and description-by-retrieval
(Farhadi et al, 2010; Ordonez et al, 2011; Hodosh
et al, 2013). Image description has been compared
to translating an image into text (Li et al, 2011;
Kulkarni et al, 2011) or summarising an image
1. An older woman with a small dog in the snow.
2. A woman and a cat are outside in the snow.
3. A woman in a brown vest is walking on the
snow with an animal.
4. A woman with a red scarf covering her head
walks with her cat on snow-covered ground.
5. Heavy set woman in snow with a cat.
Figure 1: An image from the Flickr8K data set and
five human-written descriptions. These descrip-
tions vary in the adjectives or prepositional phrases
that describe the woman (1, 3, 4, 5), incorrect or un-
certain identification of the cat (1, 3), and include
a sentence without a verb (5).
(Yang et al, 2011), resulting in the adoption of the
evaluation measures from those communities.
In this paper we estimate the correlation of hu-
man judgements with five automatic evaluation
measures on two image description data sets. Our
work extends previous studies of evaluation mea-
sures for image description (Hodosh et al, 2013),
which focused on unigram-based measures and re-
ported agreement scores such as Cohen?s ? rather
than correlations. The main finding of our analysis
is that TER and unigram BLEU are weakly corre-
452
lated against human judgements, ROUGE-SU4 and
Smoothed BLEU are moderately correlated, and the
strongest correlation is found with Meteor.
2 Methodology
We estimate Spearman?s ? for five different auto-
matic evaluation measures against human judge-
ments for the automatic image description task.
Spearman?s ? is a non-parametric correlation co-
efficient that restricts the ability of outlier data
points to skew the co-efficient value. The automatic
measures are calculated on the sentence level and
correlated against human judgements of semantic
correctness.
2.1 Data
We perform the correlation analysis on the Flickr8K
data set of Hodosh et al (2013), and the data set of
Elliott and Keller (2013).
The test data of the Flickr8K data set contains
1,000 images paired with five reference descrip-
tions. The images were retrieved from Flickr, the
reference descriptions were collected from Me-
chanical Turk, and the human judgements were
collected from expert annotators as follows: each
image in the test data was paired with the highest
scoring sentence(s) retrieved from all possible test
sentences by the TRI5SEM model in Hodosh et al
(2013). Each image?description pairing in the test
data was judged for semantic correctness by three
expert human judges on a scale of 1?4. We calcu-
late automatic measures for each image?retrieved
sentence pair against the five reference descriptions
for the original image.
The test data of Elliott and Keller (2013) con-
tains 101 images paired with three reference de-
scriptions. The images were taken from the PAS-
CAL VOC Action Recognition Task, the reference
descriptions were collected from Mechanical Turk,
and the judgements were also collected from Me-
chanical Turk. Elliott and Keller (2013) gener-
ated two-sentence descriptions for each of the test
images using four variants of a slot-filling model,
and collected five human judgements of the se-
mantic correctness and grammatical correctness of
the description on a scale of 1?5 for each image?
description pair, resulting in a total of 2,042 human
judgement?description pairings. In this analysis,
we use only the first sentence of the description,
which describes the event depicted in the image.
2.2 Automatic Evaluation Measures
BLEU measures the effective overlap between a
reference sentence X and a candidate sentence Y .
It is defined as the geometric mean of the effective
n-gram precision scores, multiplied by the brevity
penalty factor BP to penalise short translations. p
n
measures the effective overlap by calculating the
proportion of the maximum number of n-grams
co-occurring between a candidate and a reference
and the total number of n-grams in the candidate
text. More formally,
BLEU = BP ? exp
(
N
?
n=1
w
n
log p
n
)
p
n
=
?
c?cand
?
ngram?c
count
clip
(ngram)
?
c?cand
?
ngram?c
count(ngram)
BP =
{
1 if c > r
e
(1?r/c)
if c? r
Unigram BLEU without a brevity penalty has been
reported by Kulkarni et al (2011), Li et al (2011),
Ordonez et al (2011), and Kuznetsova et al (2012);
to the best of our knowledge, the only image de-
scription work to use higher-order n-grams with
BLEU is Elliott and Keller (2013). In this paper we
use the smoothed BLEU implementation of Clark et
al. (2011) to perform a sentence-level analysis, set-
ting n = 1 and no brevity penalty to get the unigram
BLEU measure, or n = 4 with the brevity penalty
to get the Smoothed BLEU measure. We note that a
higher BLEU score is better.
ROUGE measures the longest common subse-
quence of tokens between a candidate Y and refer-
ence X . There is also a variant that measures the co-
occurrence of pairs of tokens in both the candidate
and reference (a skip-bigram): ROUGE-SU*. The
skip-bigram calculation is parameterised with d
skip
,
the maximum number of tokens between the words
in the skip-bigram. Setting d
skip
to 0 is equivalent to
bigram overlap and setting d
skip
to ? means tokens
can be any distance apart. If ? = |SKIP2(X ,Y )|
is the number of matching skip-bigrams between
the reference and the candidate, then skip-bigram
ROUGE is formally defined as:
R
SKIP2
= ? /
(
?
2
)
453
ROUGE has been used by only Yang et al (2011)
to measure the quality of generated descriptions,
using a variant they describe as ROUGE-1. We set
d
skip
= 4 and award partial credit for unigram only
matches, otherwise known as ROUGE-SU4. We use
ROUGE v.1.5.5 for the analysis, and configure the
evaluation script to return the result for the average
score for matching between the candidate and the
references. A higher ROUGE score is better.
TER measures the number of modifications a hu-
man would need to make to transform a candidate
Y into a reference X . The modifications available
are insertion, deletion, substitute a single word, and
shift a word an arbitrary distance. TER is expressed
as the percentage of the sentence that needs to be
changed, and can be greater than 100 if the candi-
date is longer than the reference. More formally,
TER =
|edits|
|reference tokens|
TER has not yet been used to evaluate image de-
scription models. We use v.0.8.0 of the TER evalu-
ation tool, and a lower TER is better.
Meteor is the harmonic mean of unigram preci-
sion and recall that allows for exact, synonym, and
paraphrase matchings between candidates and ref-
erences. It is calculated by generating an alignment
between the tokens in the candidate and reference
sentences, with the aim of a 1:1 alignment between
tokens and minimising the number of chunks ch
of contiguous and identically ordered tokens in the
sentence pair. The alignment is based on exact to-
ken matching, followed by Wordnet synonyms, and
then stemmed tokens. We can calculate precision,
recall, and F-measure, where m is the number of
aligned unigrams between candidate and reference.
Meteor is defined as:
M = (1?Pen) ?F
mean
Pen = ?
(
ch
m
)
?
F
mean
=
PR
?P+(1??)R
P =
|m|
|unigrams in candidate|
R =
|m|
|unigrams in reference|
We calculated the Meteor scores using release 1.4.0
with the package-provided free parameter settings
of 0.85, 0.2, 0.6, and 0.75 for the matching compo-
nents. Meteor has not yet been reported to evaluate
Flickr 8K
co-efficient
n = 17,466
E&K (2013)
co-efficient
n = 2,040
METEOR 0.524 0.233
ROUGE SU-4 0.435 0.188
Smoothed BLEU 0.429 0.177
Unigram BLEU 0.345 0.097
TER -0.279 -0.044
Table 1: Spearman?s correlation co-efficient of au-
tomatic evaluation measures against human judge-
ments. All correlations are significant at p < 0.001.
the performance of different models on the image
description task; a higher Meteor score is better.
2.3 Protocol
We performed the correlation analysis as follows.
The sentence-level evaluation measures were cal-
culated for each image?description?reference tu-
ple. We collected the BLEU, TER, and Meteor
scores using MultEval (Clark et al, 2011), and the
ROUGE-SU4 scores using the RELEASE-1.5.5.pl
script. The evaluation measure scores were then
compared with the human judgements using Spear-
man?s correlation estimated at the sentence-level.
3 Results
Table 1 shows the correlation co-efficients between
automatic measures and human judgements and
Figures 2(a) and (b) show the distribution of scores
for each measure against human judgements. To
classify the strength of the correlations, we fol-
lowed the guidance of Dancey and Reidy (2011),
who posit that a co-efficient of 0.0?0.1 is uncor-
related, 0.11?0.4 is weak, 0.41?0.7 is moderate,
0.71?0.90 is strong, and 0.91?1.0 is perfect.
On the Flickr8k data set, all evaluation measures
can be classified as either weakly correlated or mod-
erately correlated with human judgements and all
results are significant. TER is only weakly cor-
related with human judgements but could prove
useful in comparing the types of differences be-
tween models. An analysis of the distribution of
TER scores in Figure 2(a) shows that differences in
candidate and reference length are prevalent in the
image description task. Unigram BLEU is also only
weakly correlated against human judgements, even
though it has been reported extensively for this task.
454
0 20 40 60 80 100
METEOR ?= 0.524
12
34
0.0 0.2 0.4 0.6
ROUGE-SU4 ?= 0.435
12
34
0 20 40 60 80 100
Smoothed BLEU ?= 0.429
12
34
0 20 40 60 80 100
Unigram BLEU ?= 0.345
12
34
0 100 200 300 400
TER ?= -0.279
12
34
Sentence-level automated measure score
Huma
n Jud
geme
nt
(a) Flick8K data set, n=17,466.
0 20 40 60 80 100
METEOR ?= 0.233
1
3
5
0.0 0.2 0.4 0.6 0.8
ROUGE-SU4 ?= 0.188
1
3
5
0 20 40 60 80 100
Smoothed BLEU ?= 0.177
1
3
5
40 50 60 70 80 90 100
Unigram BLEU ?= 0.0965
1
3
5
0 50 100 150
TER ?= -0.0443
1
3
5
Sentence-level automated measure score
Huma
n Jud
geme
nt
(b) E&K (2013) data set, n=2,042.
Figure 2: Distribution of automatic evaluation measures against human judgements. ? is the correlation
between human judgements and the automatic measure. The intensity of each point indicates the number
of occurrences that fall into that range.
Figure 2(a) shows an almost uniform distribution
of unigram BLEU scores, regardless of the human
judgement. Smoothed BLEU and ROUGE-SU4 are
moderately correlated with human judgements, and
the correlation is stronger than with unigram BLEU.
Finally, Meteor is most strongly correlated mea-
sure against human judgements. A similar pattern
is observed in the Elliott and Keller (2013) data set,
though the correlations are lower across all mea-
sures. This could be caused by the smaller sample
size or because the descriptions were generated
by a computer, and not retrieved from a collection
of human-written descriptions containing the gold-
standard text, as in the Flickr8K data set.
Qualitative Analysis
Figure 3 shows two images from the test collec-
tion of the Flickr8K data set with a low Meteor
score and a maximum human judgement of seman-
tic correctness. The main difference between the
candidates and references are in deciding what to
describe (content selection), and how to describe it
(realisation). We can hypothesise that in both trans-
lation and summarisation, the source text acts as a
lexical and semantic framework within which the
translation or summarisation process takes place.
In Figure 3(a), the authors of the descriptions made
different decisions on what to describe. A decision
has been made to describe the role of the officials in
the candidate text, and not in the reference text. The
underlying cause of this is an active area of research
in the human vision literature and can be attributed
to bottom-up effects, such as saliency (Itti et al,
1998), top-down contextual effects (Torralba et al,
2006), or rapidly-obtained scene properties (Oliva
and Torralba, 2001). In (b), we can see the problem
of deciding how to describe the selected content.
The reference uses a more specific noun to describe
the person on the bicycle than the candidate.
4 Discussion
There are several differences between our analysis
and that of Hodosh et al (2013). First, we report
Spearman?s ? correlation coefficient of automatic
measures against human judgements, whereas they
report agreement between judgements and auto-
matic measures in terms of Cohen?s ?. The use of
? requires the transformation of real-valued scores
into categorical values, and thus loses informa-
tion; we use the judgement and evaluation measure
scores in their original forms. Second, our use of
Spearman?s ? means we can readily use all of the
available data for the correlation analysis, whereas
Hodosh et al (2013) report agreement on thresh-
olded subsets of the data. Third, we report the corre-
lation coefficients against five evaluation measures,
455
Candidate: Football players gathering to con-
test something to collaborating officials.
Reference: A football player in red and white
is holding both hands up.
(a)
Candidate: A man is attempting a stunt with a
bicycle.
Reference: Bmx biker Jumps off of ramp.
(b)
Figure 3: Examples in the test data with low Meteor scores and the maximum expert human judgement.
(a) the candidate and reference are from the same image, and show differences in what to describe, in
(b) the descriptions are retrieved from different images and show differences in how to describe an image.
some of which go beyond unigram matchings be-
tween references and candidates, whereas they only
report unigram BLEU and unigram ROUGE. It is
therefore difficult to directly compare the results
of our correlation analysis against Hodosh et al?s
agreement analysis, but they also reach the conclu-
sion that unigram BLEU is not an appropriate mea-
sure of image description performance. However,
we do find stronger correlations with Smoothed
BLEU, skip-bigram ROUGE, and Meteor.
In contrast to the results presented here, Reiter
and Belz (2009) found no significant correlations
of automatic evaluation measures against human
judgements of the accuracy of machine-generated
weather forecasts. They did, however, find signif-
icant correlations of automatic measures against
fluency judgements. There are no fluency judge-
ments available for Flickr8K, but Elliott and Keller
(2013) report grammaticality judgements for their
data, which are comparable to fluency ratings. We
failed to find significant correlations between gram-
matlicality judgements and any of the automatic
measures on the Elliott and Keller (2013) data. This
discrepancy could be explained in terms of the dif-
ferences between the weather forecast generation
and image description tasks, or because the image
description data sets contain thousands of texts and
a few human judgements per text, whereas the data
sets of Reiter and Belz (2009) included hundreds
of texts with 30 human judges.
5 Conclusions
In this paper we performed a sentence-level corre-
lation analysis of automatic evaluation measures
against expert human judgements for the automatic
image description task. We found that sentence-
level unigram BLEU is only weakly correlated with
human judgements, even though it has extensively
reported in the literature for this task. Meteor was
found to have the highest correlation with human
judgements, but it requires Wordnet and paraphrase
resources that are not available for all languages.
Our findings held when judgements were made on
human-written or computer-generated descriptions.
The variability in what and how people describe
images will cause problems for all of the measures
compared in this paper. Nevertheless, we propose
that unigram BLEU should no longer be used as
an objective function for automatic image descrip-
tion because it has a weak correlation with human
accuracy judgements. We recommend adopting
either Meteor, Smoothed BLEU, or ROUGE-SU4 be-
cause they show stronger correlations with human
judgements. We believe these suggestions are also
applicable to the ranking tasks proposed in Hodosh
et al (2013), where automatic evaluation scores
could act as features to a ranking function.
Acknowledgments
Alexandra Birch and R. Calen Walshe, and the
anonymous reviewers provided valuable feedback
on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.
456
References
Jonathon H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA.
Christine Dancey and John Reidy, 2011. Statistics
Without Maths for Psychology, page 175. Prentice
Hall, 5th edition.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1292?1302, Seattle, Washington, U.S.A.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences from im-
ages. In Proceedings of the 11th European Confer-
ence on Computer Vision, pages 15?29, Heraklion,
Crete, Greece.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
Detection with Discriminatively Trained Part-Based
Models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627?1645.
Matthieu Guillaumin, Thomas Mensink, Jakob J. Ver-
beek, and Cornelia Schmid. 2009. Tagprop: Dis-
criminative metric learning in nearest neighbor mod-
els for image auto-annotation. In IEEE 12th Interna-
tional Conference on Computer Vision, pages 309?
316, Kyoto, Japan.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing Image Description as a Ranking
Task : Data , Models and Evaluation Metrics. Jour-
nal of Artificial Intelligence Research, 47:853?899.
Laurent Itti, Christof Koch, and Ernst Niebur. 1998.
A model of saliency-based visual attention for rapid
scene analysis. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 20(11):1254?1259.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In The 24th IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1601?1608, Colorado Springs, Col-
orado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective Generation of Natural Image Descriptions.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 359?
368, Jeju Island, South Korea.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Fifteenth Conference on Computational Natural
Language Learning, pages 220?228, Portland, Ore-
gon, U.S.A.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,
Tamara Berg, and Hal Daum?e III. 2012. Midge :
Generating Image Descriptions From Computer Vi-
sion Detections. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 747?756, Avi-
gnon, France.
Aude Oliva and Antonio Torralba. 2001. Modeling the
Shape of the Scene: A Holistic Representation of
the Spatial Envelope. International Journal of Com-
puter Vision, 42(3):145?175.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In Advances in Neural In-
formation Processing Systems 24, Granada, Spain.
Ehud Reiter and A Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Compu-
tational Linguistics, 35(4):529?558.
Antonio Torralba, Aude Oliva, Monica S. Castelhano,
and John M. Henderson. 2006. Contextual guid-
ance of eye movements and attention in real-world
scenes: the role of global features in object search.
Psychologial Review, 113(4):766?786.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-Guided Sentence
Generation of Natural Images. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 444?454, Edinburgh,
Scotland, UK.
457
Transactions of the Association for Computational Linguistics, 1 (2013) 111?124. Action Editor: David Chiang.
Submitted 10/2012; Revised 2/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Incremental Tree Substitution Grammar
for Parsing and Sentence Prediction
Federico Sangati and Frank Keller
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
federico.sangati@gmail.com keller@inf.ed.ac.uk
Abstract
In this paper, we present the first incremental
parser for Tree Substitution Grammar (TSG).
A TSG allows arbitrarily large syntactic frag-
ments to be combined into complete trees;
we show how constraints (including lexical-
ization) can be imposed on the shape of the
TSG fragments to enable incremental process-
ing. We propose an efficient Earley-based al-
gorithm for incremental TSG parsing and re-
port an F-score competitive with other incre-
mental parsers. In addition to whole-sentence
F-score, we also evaluate the partial trees that
the parser constructs for sentence prefixes;
partial trees play an important role in incre-
mental interpretation, language modeling, and
psycholinguistics. Unlike existing parsers, our
incremental TSG parser can generate partial
trees that include predictions about the up-
coming words in a sentence. We show that
it outperforms an n-gram model in predicting
more than one upcoming word.
1 Introduction
When humans listen to speech, the input becomes
available gradually as the speech signal unfolds.
Reading happens in a similarly gradual manner
when the eyes scan a text. There is good evidence
that the human language processor is adapted to this
and works incrementally, i.e., computes an interpre-
tation for an incoming sentence on a word-by-word
basis (Tanenhaus et al, 1995; Altmann and Kamide,
1999). Also language processing systems often deal
with speech as it is spoken, or text as it is being
typed. A dialogue system should start interpreting
a sentence while it is being spoken, and a question
answering system should start retrieving answers be-
fore the user has finished typing the question.
Incremental processing is therefore essential both
for realistic models of human language processing
and for NLP applications that react to user input
in real time. In response to this, a number of in-
cremental parsers have been developed, which use
context-free grammar (Roark, 2001; Schuler et al,
2010), dependency grammar (Chelba and Jelinek,
2000; Nivre, 2007; Huang and Sagae, 2010), or tree-
adjoining grammar (Demberg et al, 2014). Typical
applications of incremental parsers include speech
recognition (Chelba and Jelinek, 2000; Roark, 2001;
Xu et al, 2002), machine translation (Schwartz
et al, 2011; Tan et al, 2011), reading time modeling
(Demberg and Keller, 2008), or dialogue systems
(Stoness et al, 2004). Another potential use of incre-
mental parsers is sentence prediction, i.e., the task
of predicting upcoming words in a sentence given a
prefix. However, so far only n-gram models and clas-
sifiers have been used for this task (Fazly and Hirst,
2003; Eng and Eisner, 2004; Grabski and Scheffer,
2004; Bickel et al, 2005; Li and Hirst, 2005).
In this paper, we present an incremental parser for
Tree Substitution Grammar (TSG). A TSG contains
a set of arbitrarily large tree fragments, which can be
combined into new syntax trees by means of a sub-
stitution operation. An extensive tradition of parsing
with TSG (also referred to as data-oriented parsing)
exists (Bod, 1995; Bod et al, 2003), but none of the
existing TSG parsers are incremental. We show how
constraints can be imposed on the shape of the TSG
fragments to enable incremental processing. We pro-
pose an efficient Earley-based algorithm for incre-
mental TSG parsing and report an F-score competi-
tive with other incremental parsers.
111
TSG fragments can be arbitrarily large and can
contain multiple lexical items. This property enables
our incremental TSG parser to generate partial parse
trees that include predictions about the upcoming
words in a sentence. It can therefore be applied di-
rectly to the task of sentence prediction, simply by
reading off the predicted items in a partial tree. We
show that our parser outperforms an n-gram model
in predicting more than one upcoming word.
The rest of the paper is structured as follows. In
Section 2, we introduce the ITSG framework and re-
late it to the original TSG formalism. Section 3 de-
scribes the chart-parser algorithm, while Section 4
details the experimental setup and results. Sections 5
and 6 present related work and conclusions.
2 Incremental Tree Substitution Grammar
The current work is based on Tree Substitu-
tion Grammar (TSG, Schabes 1990; for a recent
overview see Bod et al 2003). A TSG is composed
of (i) a set of arbitrarily large fragments, usually ex-
tracted from an annotated phrase-structure treebank,
and (ii) the substitution operation by means of which
fragments can be combined into complete syntactic
analyses (derivations) of novel sentences.
Every fragment?s node is either a lexical node
(word), a substitution site (a non-lexical node in the
yield of the structure),1 or an internal node. An inter-
nal node must always keep the same daughter nodes
as in the original tree. For an example of a binarized2
tree and a fragment extracted from it see Figure 1.
A TSG derivation is constructed in a top-down
generative process starting from a fragment in the
grammar rooted in S (the unique non-lexical node
all syntactic analysis are rooted in). A partial deriva-
tion is extended by subsequently introducing more
fragments: if X is the left-most substitution site in
the yield of the current partial derivation, a fragment
1For example nodes NP, VP, S@ are the substitution sites of
the right fragment in Figure 1.
2The tree is right-binarized via artificial nodes with @ sym-
bols, as explained in Section 4.1. The original tree is
S
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
S
S@
S@
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
S
S@
S@VP
VPVBD
?were?
NP
Figure 1: An example of a binarized2 parse tree and a
lexicalized fragment extracted from it.
rooted in X is chosen from the grammar and sub-
stituted into it. When there are no more substitution
sites (all nodes in the yield are lexical items) the gen-
erative process terminates.
2.1 Incrementality
In this work we are interested in defining an incre-
mental TSG (in short ITSG). The new generative
process, while retaining the original mechanism for
combining fragments (by means of the substitution
operation), must ensure a way for deriving syntactic
analyses of novel sentences in an incremental man-
ner, i.e., one word at the time from left to right. More
precisely, at each stage of the generative process, the
partially derived structure must be connected (as in
standard TSG) and have a prefix of the sentence at
the beginning of its yield. A partial derivation is con-
nected if it has tree shape, i.e., all the nodes are dom-
inated by a common root node (which does not nec-
essarily have to be the root node of the sentence).
For instance, the right fragment in Figure 1 shows a
possible way of starting a standard TSG derivation
which does not satisfy the incrementality constraint:
the partial derivation has a substitution site as the
first element in its yield.
In order to achieve incrementality while maintain-
ing connectedness, we impose one further constraint
on the type of fragments which are allowed in an
ITSG: each fragment should be lexicalized, i.e., con-
tain at least one word (lexical anchor) at the first or
the second position in its yield. Allowing more than
one substitution site at the beginning of a fragment?s
yield would lead to a violation of the incrementality
requirement (as will become clear in Section 2.2).
112
The generative process starts with a fragment an-
chored in the first word of the sentence being gener-
ated. At each subsequent step, a lexicalized fragment
is introduced (by means of the substitution opera-
tion) to extend the current partial derivation in such
a way that the prefix of the yield of the partial struc-
ture is lengthened by one word (the lexical anchor of
the fragment being introduced). The lexicalization
constraint allows a fragment to have multiple lexical
items, not necessarily adjacent to one another. This
is useful to capture the general ability of TSG to pro-
duce in one single step an arbitrarily big syntactic
construction ranging from phrasal verbs (e.g., ask
someone out), to parallel constructions (e.g., either
X or Y), and idiomatic expressions (e.g., took me
to the cleaners). For an example of a fragment with
multiple lexical anchors see the fragment in the mid-
dle of Figure 2.
2.2 Symbolic Grammar
An ITSG is a tuple ?N ,L ,F ,4,5,?, where N
and L are the set of non-lexical and lexical nodes
respectively, F is a collection of lexicalized frag-
ments, 4 and 5 are two variants of the substitution
operation (backward and forward) used to combine
fragments into derivations, and  is the stop opera-
tion which terminates the generative process.
Fragments A fragment f ?F belongs to one of
the three sets Finit ,F Xlex,FYsub:
? An initial fragment ( finit) has the lexical anchor
in the first position of the yield, being the initial
word of a sentence (the left-most lexical node
of the parse tree from which it was extracted).
? A lex-first fragment ( f Xlex) has the lexical anchor
(non sentence-initial) in the first position of the
yield, and is rooted in X .3
? A sub-first fragment ( f Ysub) has the lexical an-
chor in the second position of its yield, and a
substitution site Y in the first.
Fringes We will use fringes (Demberg et al,
2014) as a compressed representation of fragments,
3A fragment can be both an initial and a lex-first fragment
(e.g., if the lexical anchor is a proper noun). We will make use
of two separate instances of the same fragment in the two sets.
NP
NNS
?Terms?
5 S
S@
S@
.
?.?
VP
VPVBD
?were?
NP
4 VP
VBN
?disclosed?
S
Figure 2: An example of an ITSG derivation yielding the
tree on the left side of Figure 1. The second and third frag-
ment are introduced by means of forward and backward
substitution, respectively.
in which the internal structure is replaced by a trian-
gle (a or ) and only the root and the yield are vis-
ible. It is possible in a grammar that multiple frag-
ments map to the same fringe; we will refer to those
as ambiguous fringes. We use both vertical (a, e.g.,
in Figure 3 and 4) and horizontal () fringe nota-
tion. The latter is used for describing the states in our
chart-parsing algorithm in Section 3. For instance,
the horizontal fringe representation of the right frag-
ment in Figure 1 is S  NP ?were? VP S@.
Incremental Derivation An incremental deriva-
tion is a sequence of lexicalized fragments
? f1, f2, . . . , fn? which, combined together in the
specified order, give rise to a complete parse tree
(see Figure 2 for an example). The first fragment f1
being introduced in the derivation must be an initial
fragment, and its lexical anchor constitutes the one-
word prefix of the sentence being generated. Sub-
sequent fragments are introduced by means of the
substitution operation, which has two variants: back-
ward substitution (4), which is used to substitute
lex-first fragments into the partial derivation gener-
ated so far, and forward substitution (5), which is
used to substitute sub-first fragments into the partial
derivation. After a number of fragments are intro-
duced, a stop operation () may terminate the gen-
erative process.
Operations The three ITSG operations take place
under specific conditions within an incremental
derivation, as illustrated in Figure 3 and explained
hereafter. At a given stage of the generative process
(after an initial fragment has been inserted), the con-
nected partial structure may or may not have sub-
113
Partial Structure Operation Accepted Fragment Resulting Structure Terminated
Y
`1 lex. . . `i X ?. . .
4
(backward) X
`i+1 ?. . .
Y
`1 lex. . . `i+1 ?. . . ?. . .
NO
Y
`1 lex. . . `i
5
(forward) X
Y `i+1 ?. . .
X
`1 lex. . . `i `i+1 ?. . .
NO
Y
`1 lex. . . `n

(stop) ?
Y #
?
`1 lex. . . `n #
YES
Figure 3: Schemata of the three ITSG operations. All tree structures (partial structure and fragments) are represented
in a compact notation, which displays only the root nodes and the yields. The i-th words in the structure?s yield is
represented as `i, while ? and ? stands for any (possibly empty) sequence of words and substitution sites.
stitution sites present in its yield. In the first case,
a backward substitution (4) must take place in the
following generative step: if X is the left-most sub-
stitution site, a new fragment of type f Xlex is chosen
from the grammar and substituted into X . If the par-
tially derived structure has no substitution site (all
the nodes in its yield are lexical nodes) and it is
rooted in Y , two possible choices exist: either the
generative process terminates by means of the stop
operation (Y ), or the generative process contin-
ues. In the latter case a forward substitution (5) is
performed: a new f Ysub fragment is chosen from the
grammar, and the partial structure is substituted into
the left-most substitution site Y of the fragment.4
Multiple Derivations As in TSG, an ITSG may
be able to generate the same parse tree in multiple
ways: multiple incremental derivations yielding the
same tree. Figure 4 shows one such example.
Generative Capacity It is useful to clarify the dif-
ference between ITSG and the more general TSG
formalism in terms of generative capacity. Although
both types of grammar make use of the substitu-
tion operation to combine fragments, an ITSG im-
poses more constraints on (i) the type of fragments
which are allowed in the grammar (initial, lex-first,
4A stop operation can be viewed as a forward substitution
when using an artificial sub-first fragment ?Y # (stop frag-
ment), where # is an artificial lexical node indicating the termi-
nation of the sentence. For simplicity, stop fragments are omit-
ted in Figure 2 and 4 and Y is attached to the stop symbol (Y ).
S
S@
S@
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
NP
?Terms?
5 S
NP ?were? VP ?.?
4 VP
?disclosed?
 S
S
?Terms? S@
4 S@
?were? VP ?.?
4 VP
?disclosed?
 S
Figure 4: Above: an example of a set of fragments ex-
tracted from the tree in Figure 1. Below: two incremental
derivations that generate it. Colors (and lines strokes) in-
dicate which derivation fragments belong to.
and sub-first fragments), and (ii) the generative pro-
cess with which fragments are combined (incremen-
tally left to right instead of top-down). If we com-
pare a TSG and an ITSG on the same set of (ITSG-
compatible) fragments, then there are cases in which
the TSG can generate more tree structures than the
ITSG.
In the following, we provide a more formal char-
acterization of the strong and weak generative power
114
S
X?a?
X
?c?X
X
?b?
S
X
?c?X
?c?X
?c?X
?b?
?a?
Figure 5: Left: an example of a CFG with left recursion.
Right: one of the structures the CFG can generate.
of ITSG with respects to context-free grammar
(CFG) and TSG. (However, a full investigation of
this issue is beyond the scope of this paper.) We can
limit our analysis to CFG, as TSG is strongly equiv-
alent to CFG. The weak equivalence between ITSG
and CFG is straightforward: for any CFG there is
a way to produce a weakly equivalent grammar in
Greibach Normal Form in which any production has
a right side beginning with a lexical item (Aho and
Ullman, 1972). The grammar that results from this
transformation is an ITSG which uses only back-
ward substitutions.
Left-recursion seems to be the main obstacle for
strong equivalence between ITSG and CFG. As an
example, the left side of Figure 5 shows a CFG that
contains a left-recursive rule. The types of structures
this grammar can generate (such as the one given on
the right side of the same figure) are characterized by
an arbitrarily long chain of rules that can intervene
before the second word of the string, ?b?, is gener-
ated. Given the incrementality constraints, there is
no ITSG that can generate the same set of struc-
tures that this CFG can generate. However, it may
be possible to circumvent this problem by applying
the left-corner transform (Rosenkrantz and Lewis,
1970; Aho and Ullman, 1972) to generate an equiv-
alent CFG without left-recursive rules.
2.3 Probabilistic Grammar
In the generative process presented above there are
a number of choices which are left open, i.e., which
fragment is being introduced at a specific stage of
a derivation, and when the generative process ter-
minates. A symbolic ITSG can be equipped with
a probabilistic component which deals with these
choices. A proper probability model for ITSG needs
to define three probability distributions over the
three types of fragments in the grammar, such that:
?
finit?Finit
P( finit) = 1 (1)
?
f Xlex?FXlex
P( f Xlex) = 1 (?X ?N ) (2)
P(Y )+ ?
fYsub?FYsub
P( f Ysub) = 1 (?Y ?N ) (3)
The probability that an ITSG generates a specific
derivation d is obtained by multiplying the probabil-
ities of the fragments taking part in the derivation:
P(d) =?
f?d
P( f ) (4)
Since the grammar may generate a tree t via multiple
derivations D(t) = d1,d2, . . . ,dm, the probability of
the parse tree is the sum of the probabilities of the
ITSG derivations in D(t):
P(t) = ?
d?D(t)
P(d) = ?
d?D(t)
?
f?d
P( f ) (5)
3 Probabilistic ITSG Parser
We introduce a probabilistic chart-parsing algorithm
to efficiently compute all possible incremental de-
rivations that an ITSG can generate given an input
sentence (presented one word at the time). The pars-
ing algorithm is an adaptation of the Earley algo-
rithm (Earley, 1970) and its probabilistic instantia-
tion (Stolcke, 1995).
3.1 Parsing Chart
A TSG incremental derivation is represented in the
chart as a sequence of chart states, i.e., a path.
For a given fringe in an incremental derivation,
there will be one or more states in the chart, depend-
ing on the length of the fringe?s yield. This is be-
cause we need to keep track of the extent to which
the yield of each fringe has been consumed within
a derivation as the sentence is processed incremen-
tally.5 At the given stage of the derivation, the states
offer a compact representation over the partial struc-
tures generated so far.
5A fringe (state) may occur in multiple derivations (paths):
for instance in Figure 4 the two derivations will correspond to
two separate paths that will converge to the same fringe (state).
115
Start(`0) X  `0?
0 : 0X ?`0? [?,?,?]
?= ? = P(X  `0?) ? = ?(1 : 0X  `0 ??)
Backward Substitution(`i)i : kX ??Y ? [?,?,?] Y  `i?
i : iY ?`i? [??,??,??]
?? += ? ?P(Y  `i?) ?? = P(Y  `i?)
Forward Substitution(`i)i : 0Y ?? [?,?,?] X Y `i?
i : 0X Y ? `i? [??,??,??]
?? += ? ?P(X Y `i?)?? += ? ?P(X Y `i?) ?
+= ?? ?P(X Y `i?)
Completioni : jY  ` j?? [?,?,?] j : kX ??Y ? [??,??,??]
i : kX ?Y ?? [???,???,???]
??? += ?? ? ?
??? += ?? ? ?
? += ??? ? ??
?? += ??? ? ???
Scan(`i) i : kX ?? `i? [?,?,?]
i+1 : kX ?`i ?? [??,??,??]
?? = ??? = ? ? = ??
Stop(#) n : 0Y ?? [?= ?,?] ?Y #
n : 0?Y ?# [??,??,??]
?? = ?? = ? ?P(Y ) ?? = 1? = P(Y )
Figure 6: Chart operations with forward (?), inner (?),
and outer (?) probabilities.
Each state is composed of a fringe and some ad-
ditional information which keeps track of where the
fringe is located within a path. A chart state can be
generally represented as
i : kX ??? (6)
where X ?? is the state?s fringe, Greek letters are
(possibly empty) sequences of words and substitu-
tion sites, and ? is a placeholder indicating to which
extent the fragment?s yield has been consumed: all
the elements in the yield preceding the dot have
been already accepted. Finally, i and k are indices
of words in the input sentence:
? i signifies that the current state is introduced
after the first i words in the sentence have
been scanned. All states in the chart will be
grouped according to this index, and will con-
stitute state-set i.
? k indicates that the fringe associated with the
current state was first introduced in the chart
after the first k words in the input sentence had
been scanned. The index k is therefore called
the start index.
For instance when generating the first incremental
derivation in Figure 4, the parser will pass through
state 1 : 1S  NP ? ?were? VP ?.? indicating that
the second fringe is introduced right after the parser
has scanned the first word in the sentence and before
having scanned the second word.
3.2 Parsing Algorithm
We will first introduce the symbolic part of the
parsing algorithm, and then discuss its probabilistic
component in Section 3.3. In line with the generative
process illustrated in Section 2.2, the parser operates
on the chart states in order to keep track of all pos-
sible ITSG derivations as new words are fed in. It
starts by reading the first word `0 and introducing
new states to state-set 0 in the chart, those mapping
to initial fragments in the grammar with `0 as lexi-
cal anchor. At a given stage, after i words have been
scanned, the parser reads the next word (`i) and in-
troduces new states in state-sets i and i+1 by apply-
ing specific operations on states present in the chart,
and fringes in the grammar.
Parser Operations The parser begins with the
start operation just described, and continues with a
cycle of four operations for every word in the input
sentence `i (for i ? 0). The order of the four opera-
tions is the following: completion, backward substi-
tution (4), forward substitution (5), and scan. When
there are no more words in input, the parser termi-
nates with a stop operation. We will now describe
the parser operations (see Figure 6 for their formal
definition), ignoring the probabilities for now.
Start(`0): For every initial fringe in the grammar
anchored in `0, the parser inserts a (scan) state for
that fringe in the state-set 0.
116
Backward Substitution(`i) applies to acceptor
states, i.e., those with a substitution site following
the dot, say X . For each acceptor state in state-set i,
and any lex-first fringe in the grammar rooted in X
and anchored in `i, the parser inserts a (scan) state
for that fringe in state-set i.
Forward Substitution(`i) applies to donor
states, i.e., those that have no elements following
the dot and with start index 0. For each donor state
in state-set i, rooted in Y , and any sub-first fringe in
the grammar with Y as the left-most element in its
yield, the parser inserts a (scan) state for that fringe
in state-set i, with the dot placed after Y .
Completion applies to complete states, i.e., those
with no elements following the dot and with start
index j > 0. For every complete state in state-set i,
rooted in Y , with starting index j, and every acceptor
state in set j with Y following the dot, the parser
inserts a copy of the acceptor state in state-set i, and
advances the dot.
Scan(`i) applies to scan states, i.e., those with a
word after the dot. For every scan state in state-set
i having `i after the dot, the parser inserts a copy of
that state in state-set (i+1), and advances the dot.
Stop(#) is a special type of forward substitution
and applies to donor states, but only when the input
word is the terminal symbol #. For every donor state
in state-set n (the length of the sentence), if the root
of the fringe?s state is Y , the parser introduces a stop
state whose fringe is a stop fringe with Y as the left
most substitution site.
Comparison with the Earley Algorithm It is
useful to clarify the differences between the pro-
posed ITSG parsing algorithm and the original Ear-
ley algorithm. Primarily, the ITSG parser is based
on a left-right processing order, whereas the Ear-
ley algorithm uses a top-down generative process.
Moreover, our parser presupposes a restricted in-
ventory of fragments in the grammar (the ones al-
lowed by an ITSG) as opposed to the general CFG
rules allowed by the Earley algorithm. In particular,
the Backward Substitution operation is more limited
than the corresponding Prediction step of the Earley
algorithm: only lex-first fragments can be introduced
using Backward Substitution, and therefore left re-
cursion (allowed by the Earley algorithm) is not pos-
sible here.6 This restriction is compensated for by
the existence of the Forward Substitution operation,
which has no analog in the Earley algorithm.7 The
worst case complexity of Earley algorithm is domi-
nated by the Completion operation which is identical
to that in our parser, and therefore the original total
time complexity applies, i.e., O(l3) for an input sen-
tence of length l, and O(n3) in terms of the number
of non-lexical nodes n in the grammar.
Derivations Incremental (partial) derivations are
represented in the chart as (partial) paths along
states. Each state can lead to one or more succes-
sors, and come from one or more antecedents. Scan
is the only operation which introduces, for every
scan state, a new single successor state (which can
be of any of the four types) in the following state-
set. Complete states may lead to several states within
the current state-set, which may belong to any of the
four types. An acceptor state may lead to a number
of scan states via backward substitution (depending
on the number of lex-first fringes that can combine
with it). Similarly, a donor state may lead to a num-
ber of scan states via forward substitution.
After i words have been scanned, we can retrieve
(partial) paths from the chart. This is done in a back-
ward direction starting from scan states in state-set i
all the way back to the initial states. This is possible
since all the operations are reversible, i.e., given a
state it is possible to retrieve its antecedent state(s).
As an example, consider the ITSG grammar con-
sisting of the fragments in Figure 7 and the two de-
rivations of the same parse tree in the same figure;
Figure 7 represents the parsing chart of the same
grammar, containing the two corresponding paths.
3.3 Probabilistic Parser
In the probabilistic version of the parser, each fringe
in the grammar has a given probability, such that
Equations (1)?(3) are satisfied.8 In the probabilistic
chart, every state i : kX??? is decorated with three
6This further simplifies the probabilistic version of our
parser, as there is no need to resort to the probabilistic reflex-
ive, transitive left-corner relation described by Stolcke (1995).
7This operation would violate Earley?s top-down constraint;
donor states are in fact the terminal states in Earley algorithm.
8The probability of an ambiguous fringe is the marginal
probability of the fragments mapping to it.
117
0 ? ?Terms?
S | 0NP? ?Terms? [1/2, 1/2, 1]|| 0S? ?Terms? S@ [1/2, 1/2, 1]
1 ? ?were?
S | 0SNP ? ?were? V P ?.? [1/2, 1/2, 1]|| 1S@? ?were? V P ?.? [1/2, 1, 1/2]
4 || 0S ?Terms? ? S@ [1/2, 1/2, 1] ? || S@ ?were? V P ?.? [1]
5 | 0NP ?Terms? ? [1/2, 1/2, 1] | SNP ?were? V P ?.? [1]
2 ? ?disclosed?
S 2V P? ?disclosed? [1, 1, 1]
4 | 0SNP ?were? ? V P ?.? [1/2, 1/2, 1] ??|| 1S@ ?were? ? V P ?.? [1/2, 1, 1/2] ??? V P ?disclosed? [1]
3 ? ?.?
S | 0SNP ?were? V P ? ?.? [1/2, 1/2, 1]|| 1S@ ?were? V P ? ?.? [1/2, 1, 1/2]
C 2V P ?disclosed? ? [1, 1, 1] | **|| ***
4 ? #
S 0?S ? # [1, 1, 1]
 || 0S ?Terms? S@ ? [1/2, 1/2, 1]| 0SNP ?were? V P ?.? ? [1/2, 1/2, 1] ?S # [1]C || 1S@ ?were? V P ?.? ? [1/2, 1, 1/2] || *
Figure 7: The parsing chart of the two derivations in Figure 4. Blue states or fringes (also marked with |) are the ones in
the first derivation, red (||) in the second, and yellow (no marks) are the ones in common. Each state-set is represented
as a separate block in the chart, headed by the state-set index and the next word. Each row maps to a chart operation
(specified in the first column, with S and C standing for ?scan? and ?complete? respectively) and follows the same
notation of figure 6. Symbols ? are used as state placeholders.
probabilities [?,?,?] as shown in the chart example
in Figure 7.
? The forward probability ? is the marginal prob-
ability of all the paths starting with an initial
state, scanning all initial words in the sentence
until `i?1 included, and passing through the
current state.
? The inner probability ? is the marginal proba-
bility of all the paths passing through the state
k : kX  ???, scanning words `k, . . . , `i?1 and
passing through the current state.
? The outer probability ? is the marginal prob-
ability of all the paths starting with an initial
state, scanning all initial words in the sentence
until `k?1 included, passing through the current
state, and reaching a stop state.
Forward (?) and inner (?) probabilities are propa-
gated while filling the chart incrementally, whereas
outer probabilities (?) are back-propagated from the
stop states, for which ? = 1 (see Figure 6). These
probabilities are used for computing prefix and sen-
tence probabilities, and for obtaining the most prob-
able partial derivation (MPD) of a prefix, the MPD
of a sentence, its minimum risk parse (MRP), and to
approximate its most probable parse (MPP).
Prefix probabilities are obtained by summing over
the forward probabilities of all scan states in state-set
i having `i after the dot:9
P(`0, . . . , `i) = ?
state si:kX??`i?
?(s) (7)
3.4 Most Probable Derivation (MPD)
The Most Probable (partial) Derivation (MPD) can
be obtained from the chart by backtracking the
Viterbi path. Viterbi forward and inner probabilities
9Sentence probability is obtained by marginalizing the for-
ward probabilities of the stop states in the last state-set n.
118
(??,??) are propagated as standard forward and in-
ner probabilities except that summation is replaced
by maximization, and the probability of an ambigu-
ous fringe is the maximum probability among all the
fragments mapping into it (instead of the marginal
one). The Viterbi partial path for the prefix `0, . . . , `i
can then be retrieved by backtracking from the scan
state in state-set i with max ??: for each state, the
most probable preceding state is retrieved, i.e., the
state among its antecedents with maximum ??. The
Viterbi complete path of a sentence can be obtained
by backtracking the Viterbi path from the stop state
with max ??. Given a Viterbi path, it is possible to
obtain the corresponding MPD. This is done by re-
trieving the associated sequence of fragments10 and
connecting them.
3.5 Most Probable Parse (MPP)
According to Equation (5), if we want to compute
the MPP we need to retrieve all possible derivations
of the current sentence, sum up the probabilities of
those generating the same tree, and returning the
tree with max marginal probability. Unfortunately
the number of possible derivations grows exponen-
tially with the length of the sentence, and computing
the exact MPP is NP-hard (Sima?an, 1996). In our
implementation, we approximate the MPP by per-
forming this marginalization over the Viterbi-best
derivations obtained from all stop states in the chart.
3.6 Minimum Risk Parse (MRP)
MPD and MPP aim at obtaining the structure of a
sentence which is more likely as a whole under the
current probabilistic model. Alternatively, we may
want to focus on the single components of a tree
structures, e.g., CFG rules covering a certain span of
the sentence, and search for the structure which has
the highest number of correct constituents, as pro-
posed by Goodman (1996). Such structure is more
likely to obtain higher results according to standard
parsing evaluations, as the objective being maxi-
mized is closely related to the metric used for eval-
uation (recall/precision on the number of correct la-
beled constituents).
10For each scan state in the path, we obtain the fragment in
the grammar that maps into the state?s fringe. For ambiguous
fringes the most probable fragment that maps into it is selected.
In order to obtain the minimum risk parse (MRP)
we utilize both inner (?) and outer (?) probabilities.
The product of these two probabilities equals the
marginal probability of all paths generating the en-
tire current sentence and passing through the current
state. We can therefore compute the probability of a
fringe f = X ??? covering a specific span [s, t] of
the sentence:
P( f , [s, t]) = ?(t : s f?) ??(t : s f?) (8)
We can then compute the probability of each frag-
ment spanning [s, t],11 and the probability P(r, [s, t])
of a CFG-rule r spanning [s, t].12 Finally the MRP is
computed as
MRP = argmax
T ?r?T P(r, [s, t]) (9)
4 Experiments
For training and evaluating the ITSG parser, we em-
ploy the Penn WSJ Treebank (Marcus et al, 1993).
We use sections 2?21 for training, section 22 and 24
for development and section 23 for testing.
4.1 Grammar Extraction
Following standard practice, we start with some pre-
processing of the treebank. After removing traces
and functional tags, we apply right binarization on
the training treebank (Klein and Manning, 2003),
with no horizontal and vertical conditioning. This
means that when a node X has more than two chil-
dren, new artificial constituents labeled X@ are cre-
ated in a right recursive fashion (see Figure 1).13 We
then replace words appearing less than five times in
the training data by one of 50 unknown word cate-
gories based on the presence of lexical features as
described in Petrov (2009).
Fragment Extraction In order to equip the gram-
mar with a representative set of lexicalized frag-
ments, we use the extraction algorithm of Sangati
11For an ambiguous fringe, the spanning probability of each
fragment mapping into it is the fraction of the fringe?s spanning
probability with respect to the marginal fringe probability.
12Marginalizing the probabilities of all fragments having r
spanning [s, t].
13This shallow binarization (H0V1) was used based on gold
coverage of the unsmoothed grammar (extracted from the train-
ing set) on trees in section 22: H0V1 binarization results on a
coverage of 88.0% of the trees, compared to 79.2% for H1V1.
119
et al (2010) which finds maximal fragments recur-
ring twice or more in the training treebank. To en-
sure better coverage, we additionally extract one-
word fragments from each training parse tree: for
each lexical node ` in the parse tree we percolate
up till the root node, and for every encountered in-
ternal node X0,X1, . . . ,Xi we extract the lexicalized
fragment whose spine is Xi ? Xi?1 ? . . .? X0 ? `,
and where all the remaining children of the inter-
nal nodes are substitution sites (see for instance the
right fragment in Figure 1). Finally, we remove all
fragments which do not comply with the restrictions
presented in Section 2.1.14
For each extracted fragment we keep track of its
frequency, i.e., the number of times it occurs in the
training corpus. Each fragment?s probability is then
derived according to its relative frequency in the
corresponding set of fragments ( finit , f Xlex, f Ysub), so
that equations(1)?(3) are satisfied. The final gram-
mar consists of 2.2M fragments mapping to 2.0M
fringes.
Smoothing Two types of smoothing are per-
formed over the grammar?s fragments: Open class
smoothing adds simple CFG rewriting rules to the
grammar for open-class15 ?PoS,word? pairs not en-
countered in the training corpus, with frequency
10?6. Initial fragments smoothing adds each lex-first
fragment f to the initial fragment set with frequency
10?2 ? freq( f ).16
All ITSG experiments we report used exhaustive
search (no beam was used to prune the search space).
4.2 Evaluation
In addition to standard full-sentence parsing results,
we propose a novel way of evaluating our ITSG on
partial trees, i.e., those that the parser constructs for
sentence prefixes. More precisely, for each prefix of
the input sentence (length two words or longer) we
compute the parsing accuracy on the minimal struc-
ture spanning that prefix. The minimal structure is
obtained from the subtree rooted in the minimum
14The fragment with no lexical items, and those with more
than one substitution site at the beginning of the yield.
15A PoS belongs to the open class if it rewrites to at least 50
different words in the training corpus. A word belongs to the
open class if it has been seen only with open-class PoS tags.
16The parameters were tuned on section 24 of the WSJ.
common ancestor of the prefix nodes, after pruning
those nodes not yielding any word in the prefix.
As observed in the example derivations of Fig-
ure 4, our ITSG generates partial trees for a given
prefix which may include predictions about unseen
parts of the sentence. We propose three new mea-
sures for evaluating sentence prediction:17
Word prediction PRD(m): For every prefix of
each test sentence, if the model predicts m? ? m
words, the prediction is correct if the first m pre-
dicted words are identical to the m words following
the prefix in the original sentence.
Word presence PRS(m): For every prefix of each
test sentence, if the model predicts m? ? m words,
the prediction is correct if the first m predicted words
are present, in the same order, in the words following
the prefix in the original sentence (i.e., the predicted
word sequence is a subsequence of the sequence of
words following the prefix).18
Longest common subsequence LCS: For every
prefix of each test sentence, it computes the longest
common subsequence between the sequence of pre-
dicted words (possibly none) and the words follow-
ing the prefix in the original sentence.
Recall and precision can be computed in the usual
way for these three measures. Recall is the total
number (over all prefixes) of correctly predicted
words (as defined by PRD(m), PRS(m), or LCS)
over the total number of words expected to be pre-
dicted (according to m), while precision is the num-
ber of correctly predicted words over the number of
words predicted by the model.
We compare the ITSG parser with the incremental
parsers of Schuler et al (2010) and Demberg et al
(2014) for full-sentence parsing, with the Roark
(2001) parser19 for full-sentence and partial pars-
17We also evaluated our ITSG model using perplexity; the
results obtained were substantially worse than those obtained
using Roark?s parsers.
18Note that neither PRD(m) nor PRS(m) correspond to word
error rate (WER). PRD requires the predicted word sequence to
be identical to the original sequence, while PRS only requires
the predicted words to be present in the original. In contrast,
WER measures the minimum number of substitutions, inser-
tions, and deletions needed to transform the predicted sequence
into the original sequence.
19Apart from reporting the results in Roark (2001), we also
run the latest version of Roark?s parser, used in Roark et al
(2009), which has higher results compared to the original work.
120
R P F1
Demberg et al (2014) 79.4 79.4 79.4
Schuler et al (2010) 83.4 83.7 83.5
Roark (2001) 86.6 86.5 86.5
Roark et al (2009) 87.7 87.5 87.6
ITSG (MPD) 81.5 83.5 82.5
ITSG (MPP) 81.6 83.6 82.6
ITSG (MRP) 82.6 85.8 84.1
ITSG Smoothing (MPD) 83.0 83.5 83.2
ITSG Smoothing (MPP) 83.2 83.6 83.4
ITSG Smoothing (MRP) 83.9 85.6 84.8
Table 1: Full-sentence parsing results for sentences in the
test set of length up to 40 words.
ing, and with a language model built using SRILM
(Stolcke, 2002) for sentence prediction. We used a
standard 3-gram model trained on the sentences of
the training set using the default setting and smooth-
ing (Kneser-Ney) provided by the SRILM pack-
age. (Higher n-gram model do not seem appropriate,
given the small size of the training corpus.) For ev-
ery prefix in the test set we compute the most prob-
able continuation predicted by the n-gram model.20
4.3 Results
Table 1 reports full-sentence parsing results for our
parser and three comparable incremental parsers
from the literature. While Roark (2001) obtains the
best results, the ITSG parser without smoothing per-
forms on a par with Schuler et al (2010), and out-
performs Demberg et al (2014).21 Adding smooth-
ing results in a gain of 1.2 points F-score over the
Schuler parser. When we compare the different pars-
ing objectives of the ITSG parser, MRP is the best
one, followed by MPP and MPD.
Incremental Parsing The graphs in Figure 8 com-
pare the ITSG and Roark?s parser on the incremental
parsing evaluation, when parsing sentences of length
10, 20, 30 and 40. The performance of both models
declines as the length of the prefix increases, with
Roark?s parser outperforming the ITSG parser on
average, although the ITSG parser seems more com-
20We used a modified version of a script by Nathaniel Smith
available at https://github.com/njsmith/pysrilm.
21Note that the scores reported by Demberg et al (2014) are
for TAG structures, not for the original Penn Treebank trees.
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length
86
88
90
92
94
96
F-sc
ore
Roark (last)ITSG Smooth. (MPD)oark et al (2009)
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length
86
88
90
92
94
96
F-sc
ore
Roark (last)ITSG Smooth. (MPD)SG Smooth. (MPD)
F-s
co
re 2 3 4 5 6 7 8 9 109192
9394
9596
9798
99
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 208586
878889
909192
939495
9697
2 4 6 8 10 12 14 16 18 20 22 24 26 28 307880
828486
889092
949698
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 407880
828486
889092
949698
Prefix Length
Figure 8: Partial parsing results for sentences of length
10, 20, 30, and 40 (from upper left to lower right).
petitive when parsing prefixes for longer (and there-
fore more difficult) sentences.
Sentence Prediction Table 2 compares the sen-
tence prediction results of the ITSG and the lan-
guage model (SRILM). The latter is outperforming
the former when predicting the next word of a pre-
fix, i.e. PRD(1), whereas ITSG is better than the lan-
guage model at predicting a single future word, i.e.
PRS(1). When more than one (consecutive) word
is considered, the SRILM model exhibits a slightly
better recall while ITSG achieves a large gain in pre-
cision. This illustrates the complementary nature of
the two models: while the language model is better
at predicting the next word, the ITSG predicts future
words (rarely adjacent to the prefix) with high con-
fidence (89.4% LCS precision). However, it makes
predictions for only a small number of words (5.9%
LCS recall). Examples of sentence predictions can
be found in Table 3.
5 Related Work
To the best of our knowledge, there are no other in-
cremental TSG parsers in the literature. The parser
of Demberg et al (2014) is closely related, but uses
tree-adjoining grammar, which includes both sub-
stitution and adjunction. That parser makes predic-
tions, but only for upcoming structure, not for up-
coming words, and thus cannot be used directly
for sentence prediction. The incremental parser of
Roark (2001) uses a top-down algorithm and works
121
ITSG SRILM
Correct R P Correct R P
PRD(1) 4,637 8.7 12.5 11,430 21.5 21.6
PRD(2) 864 1.7 13.9 2,686 5.3 5.7
PRD(3) 414 0.9 20.9 911 1.9 2.1
PRD(4) 236 0.5 23.4 387 0.8 1.0
PRS(1) 34,831 65.4 93.9 21,954 41.2 41.5
PRS(2) 4,062 8.0 65.3 5,726 11.3 12.2
PRS(3) 1,066 2.2 53.7 1,636 3.4 3.8
PRS(4) 541 1.2 53.7 654 1.4 1.7
LCS 44,454 5.9 89.4 92,587 12.2 18.4
Table 2: Sentence prediction results.
Prefix Shares of UAL , the parent PRD(3) PRS(3)
ITSG company of United Airlines , ? ?
SRILM company , which is the ? ?
Goldstd of United Airlines , were extremely active all day
Friday .
Prefix PSE said it expects to report earnings of $ 1.3
million to $ 1.7 million , or 14
ITSG cents a share , ? +
SRILM % to $ UNK ? ?
Goldstd cents to 18 cents a share .
Table 3: Examples comparing sentence predictions for
ITSG and SRILM (UNK: unknown word).
on the basis of context-free rules. These are aug-
mented with a large number of non-local fea-
tures (e.g., grandparent categories). Our approach
avoids the need for such additional features, as
TSG fragments naturally contain non-local informa-
tion. Roark?s parser outperforms ours in both full-
sentence and incremental F-score (see Section 4),
but cannot be used for sentence prediction straight-
forwardly: to obtain a prediction for the next word,
we would need to compute an argmax over the
whole vocabulary, then iterate this for each word af-
ter that (the same is true for the parsers of Schuler
et al, 2010 and Demberg et al, 2014). Most in-
cremental dependency parsers use a discriminative
model over parse actions (Nivre, 2007), and there-
fore cannot predict upcoming words either (but see
Huang and Sagae 2010).
Turning to the literature on sentence prediction,
we note that ours is the first attempt to use a parser
for this task. Existing approaches either use n-gram
models (Eng and Eisner, 2004; Bickel et al, 2005) or
a retrieval approach in which the best matching sen-
tence is identified from a sentence collection given a
set of features (Grabski and Scheffer, 2004). There
is also work combining n-gram models with lexical
semantics (Li and Hirst, 2005) or part-of-speech in-
formation (Fazly and Hirst, 2003).
In the language modeling literature, more sophis-
ticated models than simple n-gram models have
been developed in the past few years, and these
could potentially improve sentence prediction. Ex-
amples include syntactic language models which
have applied successfully for speech recognition
(Chelba and Jelinek, 2000; Xu et al, 2002) and ma-
chine translation (Schwartz et al, 2011; Tan et al,
2011), as well as discriminative language models
(Mikolov et al, 2010; Roark et al, 2007). Future
work should evaluate these approaches against the
ITSG model proposed here.
6 Conclusions
We have presented the first incremental parser for
tree substitution grammar. Incrementality is moti-
vated by psycholinguistic findings, and by the need
for real-time interpretation in NLP. We have shown
that our parser performs competitively on both full
sentence and sentence prefix F-score. We also intro-
duced sentence prediction as a new way of evaluat-
ing incremental parsers, and demonstrated that our
parser outperforms an n-gram model in predicting
more than one upcoming word.
The performance of our approach is likely to im-
prove by implementing better binarization and more
advanced smoothing. Also, our model currently con-
tains no conditioning on lexical information, which
is also likely to yield a performance gain. Finally,
future work could involve replacing the relative fre-
quency estimator that we use with more sophisti-
cated estimation schemes.
Acknowledgments
This work was funded by EPSRC grant
EP/I032916/1 ?An integrated model of syntac-
tic and semantic prediction in human language
processing?. We are grateful to Brian Roark for
clarifying correspondence and for guidance in using
his incremental parser. We would also like to thank
Katja Abramova, Vera Demberg, Mirella Lapata,
Andreas van Cranenburgh, and three anonymous
reviewers for useful comments.
122
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
theory of parsing, translation, and compiling.
Prentice-Hall, Inc., Upper Saddle River, NJ.
Gerry T. M. Altmann and Yuki Kamide. 1999. Incre-
mental interpretation at verbs: Restricting the do-
main of subsequent reference. Cognition, 73:247?
264.
Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Predicting sentences using n-gram lan-
guage models. In Proceedings of the Conference
on Human Language Technology and Empirical
Methods in Natural Language Processing, pages
193?200. Vancouver.
Rens Bod. 1995. The problem of computing the
most probable tree in data-oriented parsing and
stochastic tree grammars. In Proceedings of the
7th Conference of the European Chapter of the
Association for Computational Linguistics, pages
104?111. Association for Computer Linguistics,
Dublin.
Rens Bod, Khalil Sima?an, and Remko Scha. 2003.
Data-Oriented Parsing. University of Chicago
Press, Chicago, IL.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14:283?332.
Vera Demberg and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
101(2):193?210.
Vera Demberg, Frank Keller, and Alexander Koller.
2014. Parsing with psycholinguistically moti-
vated tree-adjoining grammar. Computational
Linguistics, 40(1). In press.
Jay Earley. 1970. An efficient context-free pars-
ing algorithm. Communications of the ACM,
13(2):94?102.
John Eng and Jason M. Eisner. 2004. Radiology
report entry with automatic phrase completion
driven by language modeling. Radiographics,
24(5):1493?1501.
Afsaneh Fazly and Graeme Hirst. 2003. Testing
the efficacy of part-of-speech information in word
completion. In Proceedings of the EACL Work-
shop on Language Modeling for Text Entry Meth-
ods, pages 9?16. Budapest.
Joshua Goodman. 1996. Parsing algorithms and
metrics. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
pages 177?183. Association for Computational
Linguistics, Santa Cruz.
Korinna Grabski and Tobias Scheffer. 2004. Sen-
tence completion. In Proceedings of the 27th An-
nual International ACM SIR Conference on Re-
search and Development in Information Retrieval,
pages 433?439. Sheffield.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1077?1086. Association for Computational Lin-
guistics, Uppsala.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423?430. Associa-
tion for Computational Linguistics, Sapporo.
Jianhua Li and Graeme Hirst. 2005. Semantic
knowledge in a word completion task. In Pro-
ceedings of the 7th International ACM SIGAC-
CESS Conference on Computers and Accessibil-
ity, pages 121?128. Baltimore.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313?330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and
Sanjeev. 2010. Recurrent neural network based
language model. In Proceedings of the 11th
Annual Conference of the International Speech
Communication Association, pages 2877?2880.
Florence.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Proceedings of Human
Language Technologies: The Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 396?
403. Association for Computational Linguistics,
Rochester.
123
Slav Petrov. 2009. Coarse-to-Fine Natural Lan-
guage Processing. Ph.D. thesis, University of
California at Bekeley, Berkeley, CA.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tistics, 27:249?276.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psy-
cholinguistic modeling via incremental top-down
parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 324?333. Association for Computational
Linguistics, Singapore.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2):373?392.
D. J. Rosenkrantz and P. M. Lewis. 1970. Deter-
ministic left corner parsing. In Proceedings of
the 11th Annual Symposium on Switching and Au-
tomata Theory, pages 139?152. IEEE Computer
Society, Washington, DC.
Federico Sangati, Willem Zuidema, and Rens Bod.
2010. Efficiently extract recurring tree fragments
from large treebanks. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the 7th
InternationalConference on Language Resources
and Evaluation. European Language Resources
Association, Valletta, Malta.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects of Lexicalized Grammars. Ph.D.
thesis, University of Pennsylvania, Philadelphia,
PA.
William Schuler, Samir AbdelRahman, Tim Miller,
and Lane Schwartz. 2010. Broad-coverage pars-
ing using human-like memory constraints. Com-
putational Linguististics, 36(1):1?30.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental syn-
tactic language models for phrase-based transla-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1, pages
620?631. Association for Computational Linguis-
tics, Portland, OR.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of the 16th Confer-
ence on Computational Linguistics, pages 1175?
1180. Association for Computational Linguistics,
Copenhagen.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Process-
ing, pages 257?286. Denver, CO.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceed-
ings of the ACL Workshop Incremental Parsing:
Bringing Engineering and Cognition Together,
pages 18?25. Association for Computational Lin-
guistics, Barcelona.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun
Wang. 2011. A large scale distributed syntac-
tic, semantic and lexical language model for ma-
chine translation. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1, pages 201?210. Association for
Computational Linguistics, Portland, OR.
Michael K. Tanenhaus, Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguistic
information in spoken language comprehension.
Science, 268:1632?1634.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, pages 191?198. As-
sociation for Computational Linguistics, Philadel-
phia.
124
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 1?8,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Using Sentence Type Information for Syntactic Category Acquisition
Stella Frank (s.c.frank@sms.ed.ac.uk)
Sharon Goldwater (sgwater@inf.ed.ac.uk)
Frank Keller (keller@inf.ed.ac.uk)
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Abstract
In this paper we investigate a new source of
information for syntactic category acquisition:
sentence type (question, declarative, impera-
tive). Sentence type correlates strongly with
intonation patterns in most languages; we hy-
pothesize that these intonation patterns are a
valuable signal to a language learner, indicat-
ing different syntactic patterns. To test this hy-
pothesis, we train a Bayesian Hidden Markov
Model (and variants) on child-directed speech.
We first show that simply training a separate
model for each sentence type decreases perfor-
mance due to sparse data. As an alternative, we
propose two new models based on the BHMM
in which sentence type is an observed variable
which influences either emission or transition
probabilities. Both models outperform a stan-
dard BHMM on data from English, Cantonese,
and Dutch. This suggests that sentence type
information available from intonational cues
may be helpful for syntactic acquisition cross-
linguistically.
1 Introduction
Children acquiring the syntax of their native language
have access to a large amount of contextual informa-
tion. Acquisition happens on the basis of speech, and
the acoustic signal carries rich prosodic and intona-
tional information that children can exploit. A key task
is to separate the acoustic properties of a word from the
underlying sentence intonation. Infants become attuned
to the pragmatic and discourse functions of utterances
as signalled by intonation extremely early; in this they
are helped by the fact that intonation contours of child
and infant directed speech are especially well differen-
tiated between sentence types (Stern et al, 1982; Fer-
nald, 1989). Children learn to use appropriate intona-
tional melodies to communicate their own intentions at
the one word stage, before overt syntax develops (Snow
and Balog, 2002).
It follows that sentence type information (whether a
sentence is declarative, imperative, or a question), as
signaled by intonation, is readily available to children
by the time they start to acquire syntactic categories.
Sentence type also has an effect on sentence structure
in many languages (most notably on word order), so
we hypothesise that sentence type is a useful cue for
syntactic category learning. We test this hypothesis by
incorporating sentence type information into an unsu-
pervised model of part of speech tagging.
We are unaware of previous work investigating the
usefulness of this kind of information for syntactic
category acquisition. In other domains, intonation has
been used to identify sentence types as a means of im-
proving speech recognition language models. Specifi-
cally, (Taylor et al, 1998) found that using intonation
to recognize dialogue acts (which to a significant extent
correspond to sentence types) and then using a special-
ized language model for each type of dialogue act led
to a significant decrease in word error rate.
In the remainder of this paper, we first present the
Bayesian Hidden Markov Model (BHMM; Goldwater
and Griffiths (2007)) that is used as the baseline model
of category acquisition, as well as our extensions to
the model, which incorporate sentence type informa-
tion. We then discuss the distinctions in sentence type
that we used and our evaluation measures, and finally
our experimental results. We perform experiments on
corpora in four different languages: English, Spanish,
Cantonese, and Dutch. Our results on Spanish show no
difference between the baseline and the models incor-
porating sentence type, possibly due to the small size of
the Spanish corpus. Results on all other corpora show
a small improvement in performance when sentence
type is included as a cue to the learner. These cross-
linguistic results suggest that sentence type may be a
useful source of information to children acquiring syn-
tactic categories.
2 BHMM Models
2.1 Standard BHMM
We use a Bayesian HMM (Goldwater and Griffiths,
2007) as our baseline model. Like a standard trigram
HMM, the BHMM assumes that the probability of tag
ti depends only on the previous two tags, and the proba-
bility of word wi depends only on ti. This can be written
as
ti|ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(t,t
?)) (1)
wi|ti = t,?
(t) ? Mult(?(t)) (2)
where ?(t,t
?) are the parameters of the multinomial dis-
tribution over following tags given previous tags (t, t ?)
1
and ?(t) are the parameters of the distribution over out-
puts given tag t. The BHMM assumes that these param-
eters are in turn drawn from symmetric Dirichlet priors
with parameters ? and ?, respectively:
?(t,t
?)|? ? Dirichlet(?) (3)
?(t)|? ? Dirichlet(?) (4)
Using these Dirichlet priors allows the multinomial dis-
tributions to be integrated out, leading to the following
predictive distributions:
P(ti|t?i,?) =
C(ti?2, ti?1, ti)+?
C(ti?2, ti?1)+T?
(5)
P(wi|ti, t?i,w?i,?) =
C(ti,wi)+?
C(ti)+Wti?
(6)
where t?i = t1 . . . ti?1, w?i = w1 . . .wi?1, C(ti?2, ti?1, ti)
and C(ti,wi) are the counts of the trigram (ti?2, ti?1, ti)
and the tag-word pair (ti,wi) in t?i and w?i, T is the
size of the tagset, and Wti is the number of word types
emitted by ti.
Based on these predictive distributions, (Goldwa-
ter and Griffiths, 2007) develop a Gibbs sampler for
the model, which samples from the posterior distri-
bution over tag sequences t given word sequences w,
i.e., P(t|w,?,?)? P(w|t,?)P(t|?). This is done by us-
ing Equations 5 and 6 to iteratively resample each tag
ti given the current values of all other tags.1 The re-
sults show that the BHMM with Gibbs sampling per-
forms better than the standard HMM using expectation-
maximization. In particular, the Dirichlet priors in the
BHMM constrain the model towards sparse solutions,
i.e., solutions in which each tag emits a relatively small
number of words, and in which a tag transitions to few
following tags. This type of model constraint allows
the model to find solutions which correspond to true
syntactic parts of speech (which follow such a sparse,
Zipfian distribution), unlike the uniformly-sized clus-
ters found by standard maximum likelihood estimation
using EM.
In the experiments reported below, we use the Gibbs
sampler described by (Goldwater and Griffiths, 2007)
for the BHMM, and modify it as necessary for our own
extended models. We also follow (Goldwater and Grif-
fiths, 2007) in using Metropolis-Hastings sampling for
the hyperparameters, which are inferred automatically
in all experiments. A separate ? parameter is inferred
for each tag.
2.2 BHMM with Sentence Types
We wish to add a sentence type feature to each time-
step in the HMM, signalling the current sentence type.
We treat sentence type (s) as an observed variable, on
the assumption that it is observed via intonation or
1Slight corrections need to be made to Equation 5 to ac-
count for sampling tags from the middle of the sequence
rather than from the end; these are given in (Goldwater and
Griffiths, 2007) and are followed in our own samplers.
ti?2 ti?1 ti
wisi
?
?
N
Figure 1: Graphical model representation of the
BHMM-T, which includes sentence type as an ob-
served variable on tag transitions (but not emissions).
punctuation features (not part of our model), and these
features are informative enough to reliably distinguish
sentence types (as speech recognition tasks have found
to be the case, see Section 1).
In the BHMM, there are two obvious ways that sen-
tence type could be incorporated into the generative
model: either by affecting the transition probabilities
or by affecting the emission probabilities. The first case
can be modeled by adding si as a conditioning variable
when choosing ti, replacing line 1 from the BHMM
definition with the following:
ti|si = s, ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(s,t,t
?)) (7)
We will refer to this model, illustrated graphically in
Figure 1, as the BHMM-T. It assumes that the distribu-
tion over ti depends not only on the previous two tags,
but also on the sentence type, i.e., that different sen-
tence types tend to have different sequences of tags.
In contrast, we can add si as a conditioning variable
for wi by replacing line 2 from the BHMM with
wi|si = s, ti = t,?
(t) ? Mult(?(s,t)) (8)
This model, the BHMM-E, assumes that different sen-
tence types tend to have different words emitted from
the same tag.
The predictive distributions for these models are
given in Equations 9 (BHMM-T) and 10 (BHMM-E):
P(ti|t?i,si,?) =
C(ti?2, ti?1, ti,si)+?
C(ti?2, ti?1,si)+T?
(9)
P(wi|ti,si,?) =
C(ti,wi,si)+?
C(ti,si)+Wti?
(10)
Of course, we can also create a third new model,
the BHMM-B, in which sentence type is used to con-
dition both transition and emission probabilities. This
model is equivalent to training a separate BHMM on
each type of sentence (with shared hyperparameters).
Note that introducing the extra conditioning variable
in these models has the consequence of splitting the
counts for transitions, emissions, or both. The split dis-
tributions will therefore be estimated using less data,
which could actually degrade performance if sentence
type is not a useful variable.
2
Our prediction is that sentence type is more likely
to be useful as a conditioning variable for transition
probabilities (BHMM-T) than for emission probabili-
ties (BHMM-E). For example, the auxiliary inversion
in questions is likely to increase the probability of
the AUX? PRO transition, compared to declaratives.
Knowing that the sentence is a question may also af-
fect emission probabilities, e.g., it might increase the
probability the word you given a PRO and decrease the
probability of I; one would certainly expect wh-words
to have much higher probability in wh-questions than
in declaratives. However, many other variables also af-
fect the particular words used in a sentence (princi-
pally, the current semantic and pragmatic context). We
expect that sentence type plays a relatively small role
compared to these other factors. The ordering of tags
within an utterance, on the other hand, is principally
constrained by sentences type (especially in the short
and grammatically simple utterances found in child-
directed speech).
3 Sentence Types
We experiment with a number of sentence-type cate-
gories, leading to increasingly fine grained distinctions.
The primary distinction is between questions (Q) and
declaratives (D). Questions are marked by punctuation
(in writing) or by intonation (in speech), as well as by
word order or other morpho-syntactic markers in many
languages.
Questions may be separated into categories, most
notably wh-questions and yes/no-questions. Many lan-
guages (including several English dialects) have dis-
tinct intonation patterns for wh- and yes/no-questions
(Hirst and Cristo, 1998).
Imperatives are a separate type from declaratives,
with distinct word order and intonation patterns.
Declaratives may be further subdivided into frag-
ments and full sentences. We define fragments as ut-
terances without a verb (including auxiliary verbs).
As an alternate sentence-level feature to sentence
type, we use length. Utterances are classified accord-
ing to their length, as either shorter or longer than av-
erage. Shorter utterances are more likely to be frag-
ments and may have distinct syntactic patterns. How-
ever these patterns are likely to be less strong than in
the above type-based types. In effect this condition is
a pseudo-baseline, testing the effects of less- or non-
informative sentence features on our proposed models.
4 Evaluation Measures
Evaluation of fully unsupervised part of speech tagging
is known to be problematic, due to the fact that the
part of speech clusters found by the model are unla-
beled, and do not automatically correspond to any of
the gold standard part of speech categories. We report
three evaluation measures in our experiments, in order
to avoid the weaknesses inherent in any single measure
and in an effort to be comparable to previous work.
Matched accuracy (MA), also called many-to-one
accuracy, is a commonly used measure for evaluating
unlabeled clusterings in part of speech tagging. Each
unlabeled cluster is given the label of the gold category
with which it shares the most members. Given these la-
bels, accuracy can be measured as usual, as the percent-
age of tokens correctly labeled. Note that multiple clus-
ters may have the same label if several clusters match
the same gold standard category. This can lead to a de-
generate solution if the model is allowed an unbounded
number of categories, in which each word is in a sepa-
rate cluster. In less extreme cases, it makes comparing
MA across clustering results with different numbers of
clusters difficult. Another serious issue with MA is the
?problem of matching? (Meila, 2007): matched accu-
racy only evaluates whether or not the items in the clus-
ter match the majority class label. The non-matching
items within a cluster might all be from a second gold
class, or they might be from many different classes. In-
tuitively, the former clustering should be evaluated as
better, but matched accuracy is the same for both clus-
terings.
Variation of Information (VI) (Meila, 2007) is a
clustering evaluation measure that avoids the match-
ing problem. It measures the amount of information
lost and gained when moving between two clusterings.
More precisely:
V I(C,K) = H(C)+H(K)?2I(C,K)
= H(C|K)+H(K|C)
A lower score implies closer clusterings, since each
clustering has less information not shared with the
other: two identical clusterings have a VI of zero. How-
ever, VI?s upper bound is dependent on the maximum
number of clusters in C or K, making it difficult to com-
pare clustering results with different numbers of clus-
ters.
As a third, and, in our view, most informative
measure, we use V-measure (VM; Rosenberg and
Hirschberg (2007)). Like VI, VM uses the conditional
entropy of clusters and categories to evaluate cluster-
ings. However, it also has the useful characteristic of
being analogous to the precision and recall measures
commonly used in NLP. Homogeneity, the precision
analogue, is defined as
V H = 1?
H(C|K)
H(C)
.
VH is highest when the distribution of categories
within each cluster is highly skewed towards a small
number of categories, such that the conditional entropy
is low. Completeness (recall) is defined symmetrically
to VH as:
VC = 1?
H(K|C)
H(K)
.
VC measures the conditional entropy of the clusters
within each gold standard category, and is highest if
each category maps to a single cluster so that each
3
Eve Manchester
Sentence type Counts |w| Counts |w|
Total 13494 4.39 13216 4.23
D
Total 8994 4.48 8315 3.52
I 623 4.87 757 4.22
F 2996 1.73 4146 1.51
Q
Total 4500 4.22 4901 5.44
wh 2105 4.02 1578 4.64
Short utts 5684 1.89 6486 1.74
Long utts 7810 6.21 6730 6.64
Table 1: Counts of sentence types in the Eve and
Manchester training set. (Test and dev sets are approx-
imately 10% of the size of training.) |w| is the average
length in words of utterances of this type. D: declar-
atives, I: imperatives, F: fragments, Q: questions, wh:
wh-questions.
model cluster completely contains a category. The V-
measure VM is simply the harmonic mean of VH and
VC, analogous to traditional F-score. Unlike MA and
VI, VM is invariant with regards to both the number of
items in the dataset and to the number of clusters used,
and consequently it is best suited for comparing results
across different corpora.
5 English experiments
5.1 Corpora
We use the Eve corpus (Brown, 1973) and the
Manchester corpus (Theakston et al, 2001) from the
CHILDES collection (MacWhinney, 2000). The Eve
corpus is a longitudinal study of a single US Ameri-
can child from the age of 1.5 to 2.25 years, whereas
the Manchester corpus follows a cohort of 12 British
children from the ages of 2 to 3. Using both corpora
ensures that any effect is not due to a particular child,
and is not specific to a type of English.
From both corpora we remove all utterances spoken
by a child; the remaining utterances are nearly exclu-
sively child-directed speech (CDS). We use the full Eve
corpus and a similarly sized subset of the Manchester
corpus, consisting of the first 70 CDS utterances from
each file. Files from the chronological middle of each
corpus are set aside for development and testing (Eve:
file 10 for testing, 11 for dev; Manchester: file 17 from
each child for testing, file 16 for dev).
Both corpora have been tagged using the relatively
rich CHILDES tagset, which we collapse to a smaller
set of thirteen tags: adjectives, adverbs, auxiliaries,
conjunctions, determiners, infinitival-to, nouns, nega-
tion, participles, prepositions, pronouns, verbs and
other (communicators, interjections, fillers and the
like). wh-words are tagged as adverbs (why,where,
when and how, or pronouns (who and the rest).
Table 1 show the sizes of the training sets, and
the breakdown of sentence types within them. Each
sentence type can be identified using a distinguish-
ing characteristic. Sentence-final punctuation is used to
differentiate between questions and declaratives; wh-
questions are then further differentiated by the pres-
ence of a wh-word. Imperatives are separated from the
declaratives by a heuristic (since CHILDES does not
have an imperative verb tag): if an utterance includes
a base verb within the first two words, without a pro-
noun proceeding it (with the exception of you, as in
you sit down right now), the utterance is coded as an
imperative. Fragments are also identified using the tag
annotations, namely by the lack of a verb or auxiliary
tag in an utterance.
The CHILDES annotation guide specifies that the
question mark is to be used with any utterance with ?fi-
nal rising contour?, even if syntactically the utterance
might appear to be a declarative or exclamation. The
question category consequently includes echo ques-
tions (Finger stuck?) and non-inverted questions (You
want me to have it?).
5.2 Inference and Evaluation Procedure
Unsupervised models do not suffer from overfitting,
so generally it is thought unnecessary to use separate
training and testing data, with results being reported
on the entire set of input data. However, there is still
a danger, in the course of developing a model, of over-
fitting in the sense of becoming too finely attuned to a
particular set of input data. To avoid this, we use sep-
arate test and development sets. The BHMM is trained
on (train+dev) or (train+test), but evaluation scores are
computed based on the dev or test portions of the data
only. 2
We run the Gibbs sampler for 2000 iterations, with
hyperparameter resampling and simulated annealing.
Each iteration produces an assignment of tags to the
tokens in the corpus; the final iteration is used for eval-
uation purposes. Since Gibbs sampling is a stochas-
tic algorithm, we run all models multiple times (three,
except where stated otherwise) and report average val-
ues for all evaluation measures, as well as confidence
intervals. We run our experiments using a variety of
sentence type features, ranging from the coarse ques-
tion/declarative (Q/D) distinction to the full five types.
For reasons of space we do not report all results here,
instead confining ourselves to representative samples.
5.3 BHMM-B: Type-specific Sub-Models
When separate sub-models are used for each sen-
tence type, as in the BHMM-B, where both transition
and emission probabilities are conditioned on sentence
type, the hidden states (tags) in each sub-model do
not correspond to each other, e.g., a hidden state 9 in
one sub-model is not the same state 9 in another sub-
model. Consequently, when evaluating the tagged out-
put, each sentence type must be evaluated separately
(otherwise the evaluation would equate declaratives-
tag-9 with questions-tag-9).
2The results presented in this paper are all evaluated on
the dev set; preliminary test set results on the Eve corpus
show the same patterns.
4
Model VM VC VH VI MA
wh-questions
BHMM: 63.0 (1.0) 59.8 (0.4) 66.6 (1.8) 1.63 (0.03) 70.7 (2.7)
BHMM-B: 58.7 (2.0) 58.2 (2.1) 59.2 (2.0) 1.74 (0.09) 59.7 (2.0)
Other Questions
BHMM: 65.6 (1.4) 62.7 (1.3) 68.7 (1.5) 1.62 (0.06) 74.5 (0.5)
BHMM-B: 64.4 (3.6) 62.6 (4.4) 66.2 (2.8) 1.65 (0.19) 70.8 (2.5)
Declaratives
BHMM: 60.9 (1.3) 58.7 (1.1) 63.3 (1.6) 1.84 (0.06) 73.5 (0.8)
BHMM-B: 58.0 (1.2) 55.5 (1.1) 60.7 (1.5) 1.99 (0.06) 69.0 (1.5)
Table 2: Results for BHMM-B on W/Q/D sentence types (dev set evaluation) in the Manchester corpus, compared
to the standard BHMM. The confidence interval is indicated in parentheses. Note that lower VI is better.
Model VM VC VH VI MA
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 61.2 (1.2) 58.6 (1.2) 64.0 (1.4) 1.88 (0.06) 72.1 (1.5)
W/Q/D: 61.0 (1.7) 59.0 (1.5) 63.0 (2.0) 1.86 (0.08) 69.6 (2.2)
F/I/D/Q/W: 61.7 (1.7) 58.9 (1.8) 64.8 (1.6) 1.80 (0.09) 70.5 (1.3)
Table 3: Results for BHMM-E on the Eve corpus (dev set evaluation), compared to the standard BHMM. The
confidence interval is indicated in parentheses.
Table 2 shows representative results for the W/Q/D
condition on the Manchester corpus, separated into wh-
questions, other questions, and declaratives. For each
sentence type, the BHMM-B performs significantly
worse than the BHMM. The wh-questions sub-model,
which is trained on the smallest subset of the input cor-
pus, performs the worst across all measures except VI.
This suggests that lack of data is why these sub-models
perform worse than the standard model.
5.4 BHMM-E: Type-specific Emissions
Having demonstrated that using entirely separate sub-
models does not improve tagging performance, we turn
to the BHMM-E, in which emission probability distri-
butions are sentence-type specific, but transition prob-
abilities are shared between all sentence types.
The results in Table 3 show that BHMM-E does re-
sult in slightly better tagging performance as evaluated
by VI (lower VI is better) and VM and its components.
Matched accuracy does not capture this same trend. In-
specting the clusters found by the model, we find that
clusters for the most part do match gold categories. The
tokens that do not fall into the highest matching gold
categories are not distributed randomly, however; for
instance, nouns and pronouns often end up in the same
cluster. VI and VM capture these secondary matches
while MA does not. Some small gold categories (e.g.
the single word infinitival-to and negation-not cate-
gories) are often merged by the model into a single
cluster, with the result that MA considers nearly half
the cluster as misclassified.
The largest increase in performance with regards to
the standard BHMM is obtained by adding the distinc-
tion between declaratives and questions. Thereafter,
adding the wh-question, fragment and imperative sen-
tence types does not worsen performance, but also does
not significantly improve performance on any measure.
5.5 BHMM-T: Type-specific Transitions
Lastly, the BHMM-T shares emission probabilities
among sentence types and uses sentence type specific
transition probabilities.
Results comparing the standard BHMM with the
BHMM-T with sentence-type-specific transition prob-
abilities are presented in Table 4. Once again, VM
and VI show a clear trend: the models using sen-
tence type information outperform both the standard
BHMM and models splitting according to utterance
length (shorter/longer than average). MA shows no sig-
nificant difference in performance between the differ-
ent models (aside from clearly showing that utterance
length is an unhelpful feature). The fact that the MA
measure shows no clear change in performance is likely
a fault of the measure itself; as explained above, VI and
VM take into account the distribution of words within
a category, which MA does not.
As with the BHMM-E, the improvements to VM and
VI are obtained by distinguishing between questions
and declaratives, and then between wh- and other ques-
tions. Both of these distinctions are marked by intona-
tion in English. In contrast, distinguishing fragments
and imperatives, which are less easily detected by in-
tonation, provides no obvious benefit in any case. Us-
ing sentence length as a feature degrades performance
considerably, confirming that improvements in perfor-
mance are due to sentence types capturing useful infor-
mation about the tagging task, and not simply due to
splitting the input in some arbitrary way.
One reason for the improvement when adding the
wh-question type is that the models are learning to
identify and cluster the wh-words in particular. If we
evaluate the wh-words separately, VM rises from 32.3
5
Model VM VC VH VI MA
Eve
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 60.9 (0.5) 58.3 (0.4) 63.7 (0.6) 1.89 (0.02) 72.7 (0.3)
W/Q/D: 62.5 (1.2) 60.0 (1.3) 65.2 (1.0) 1.81 (0.06) 72.9 (0.8)
F/I/D/Q/W: 62.2 (1.5) 59.5 (1.6) 65.2 (1.3) 1.77 (0.08) 71.5 (1.4)
Length: 57.9 (1.2) 55.3 (1.1) 60.7 (1.3) 2.04 (0.06) 69.7 (2.0)
Manchester
BHMM: 60.2 (0.9) 57.6 (0.9) 63.1 (1.0) 1.92 (0.05) 72.1 (0.7)
Q/D: 61.5 (0.7) 59.2 (0.6) 63.9 (0.9) 1.84 (0.03) 71.6 (1.5)
W/Q/D: 62.7 (0.2) 60.6 (0.2) 65.0 (0.3) 1.78 (0.01) 71.2 (0.6)
F/I/D/Q/W: 62.5 (0.4) 60.3 (0.5) 64.9 (0.4) 1.79 (0.02) 71.3 (0.9)
Length: 58.1 (0.7) 55.6 (0.8) 60.8 (0.6) 2.02 (0.04) 71.0 (0.6)
Table 4: Results on the Eve and Manchester corpora for the various sentence types in the BHMM and BHMM-T
models. The confidence interval is indicated in parentheses.
in the baseline BHMM to 41.5 in the W/Q/D condition
with the BHMM-T model and 46.8 with the BHMM-
E model. Performance for the non-wh-words also im-
proves in the W/Q/D condition, albeit less dramati-
cally: from 61.1 in the baseline BHMM to 63.6 with
BHMM-T and 62.0 with BHMM-E. The wh-question
type enables the models to pick up on the defining char-
acteristics of these sentences, namely wh-words.
We predicted the sentence-type specific transition
probabilities in the BHMM-T to be more useful than
the sentence-type specific emission probabilities in the
BHMM-E. The BHMM-T does perform slightly better
than the BHMM-E, however, the effect is small. Word
or tag order may be the most overt difference between
questions and declaratives in English, but word choice,
especially the use of wh-words varies sufficiently be-
tween sentence types for sentence-type specific emis-
sion probabilities to be equally useful.
6 Crosslinguistic Experiments
In the previous section we found that sentence type
information improved syntactic categorisation in En-
glish. In this section, we evaluate the BHMM?s perfor-
mance on a range of languages other than English, and
investigate whether sentence type information is use-
ful across languages. To our knowledge this is the first
application of the BHMM to non-English data.
Nearly all human languages distinguish between
yes/no-questions and declaratives in intonation; ques-
tions are most commonly marked by rising intonation
(Hirst and Cristo, 1998). wh-questions do not always
have a distinct intonation type, but they are signalled
by the presence of members of the small class of wh-
words.
The CHILDES collection includes tagged corpora
for Spanish and Cantonese: the Ornat corpus (Ornat,
1994) and the Lee Wong Leung (LWL) corpus (Lee
et al, 1994) respectively. To cover a greater variety of
word order patterns, a Dutch corpus of adult dialogue
(not CDS) is also tested. We describe each corpus in
turn below; Table 5 describes their relative sizes.
Total Ds all Qs wh-Qs
Spanish 8759 4825 3934 1507
|w| 4.29 4.41 4.14 3.72
Cantonese 12544 6689 5855 2287
|w| 4.16 3.85 4.52 4.80
Dutch 8967 7812 1155 363
|w| 6.16 6.19 6.00 7.08
Table 5: Counts of sentence types in the training sets
for Spanish. Cantonese and Dutch. (Test and dev sets
are approximately 10% of the size of training.) |w| is
the average length in words of utterances of this type.
D: declaratives, Qs: questions, wh-Qs: wh-questions.
6.1 Spanish
The Ornat corpus is a longitudinal study of a single
child between the ages of one and a half and nearly
four years, consisting of 17 files. Files 08/09 are used
testing/development.
We collapse the Spanish tagset used in the Ornat cor-
pus in a similar fashion to the English corpora. There
are 11 tags in the final set: adjectives, adverbs, con-
juncts, determiners, nouns, prepositions, pronouns, rel-
ative pronouns, auxiliaries, verbs, and other.
Spanish wh-questions are formed by fronting the
wh-word (but without the auxiliary verbs added in
English); yes/no-questions involve raising the main
verb (again without the auxiliary inversion in English).
Spanish word order in declaratives is generally freer
than English word order. Verb- and object-fronting is
more common, and pronouns may be dropped (since
verbs are marked for gender and number).
6.2 Cantonese
The LWL corpus consists of transcripts from a set of
children followed over the course of a year, totalling
128 files. The ages of the children are not matched, but
they range between one and three years old. Our train-
ing set consists of the first 500 utterances of all train-
ing files, in order to create a data set of similar size as
the other corpora used. Files from children aged two
6
years and five months are used as the test set; files from
two years and six months are the development set files
(again, the first 500 utterances from each of these make
up the test/dev corpus).
The tagset used in the LWL is larger than the En-
glish corpus. It consists of 20 tags: adjective, ad-
verb, aspectual marker, auxiliary or modal verb, clas-
sifier, communicator, connective, determiners, genitive
marker, preposition or locative, noun, negation, pro-
nouns, quantifiers, sentence final particle, verbs, wh-
words, foreign word, and other. We remove all sen-
tences that are encoded as being entirely in English but
leave single foreign, mainly English, words (generally
nouns) in a Cantonese context.
Cantonese follows the same basic SVO word order
as English, but with a much higher frequency of topic-
raising. Questions are not marked by different word or-
der. Instead, particles are inserted to signal questioning.
These particles can signal either a yes/no-question or a
wh-question; in the case of wh-questions they replace
the item being questioned (e.g., playing-you what?),
without wh-raising as in English or Spanish. Despite
the use of tones in Cantonese, questions are marked
with rising final intonation.
6.3 Dutch
The Corpus of Spoken Dutch (CGN) contains Dutch
spoken in a variety of settings. We use the ?spontaneous
conversation? component, consisting of 925 files, since
it is the most similar to CDS. However, the utterances
are longer, and there are far fewer questions, especially
wh-questions (see Table 5).
The corpus does not have any meaningful timeline,
so we designated all files with numbers ending in 0 as
test files and files ending in 9 as dev files. The first
60 utterances from each file were used, to create train-
ing/test/dev sets similar in size to the other corpora.
The coarse CGN tagset consists of 11 tags, which
we used directly: adjective, adverb, conjunction, deter-
miner, interjection, noun, number, pronoun/determiner,
preposition, and verb.
Dutch follows verb-second word order in main
clauses and SOV word order in embedded clauses.
Yes/no-questions are created by verb-fronting, as in
Spanish. wh-questions involve a wh-word at the begin-
ning of the utterance followed by the verb in second
position.
6.4 Results
We trained standard BHMM, BHMM-T and BHMM-E
models in the same manner as with the English corpora.
Given the poor performance of the BHMM-B, we did
not test it here.
Due to inconsistent annotation and lack of familiar-
ity with the languages we tested only two sentence type
distinctions, Q/D and W/Q/D. Punctuation was used
to distinguish between questions and declaratives. wh-
questions were identified by using a list of wh-words
for Spanish and Dutch; for Cantonese we relied on the
wh-word tag annotation.
Results are shown in Table 6. Since the corpora
are different sizes and use tagsets of varying sizes, VI
and MA results are not comparable between corpora.
VM (and VC and VH) are more robust, but even so
cross-corpora comparisons should be made carefully.
The English corpora VM scores are significantly higher
(around 10 points higher) than the non-English corpora
scores.
In Cantonese and Dutch, the W/Q/D BHMM-T
model performs best; in both cases significantly bet-
ter than the BHMM. In Cantonese, the separation of
wh-questions improves tagging significantly in both the
BHMM-T and BHMM-E models, whereas simply sep-
arating questions and declaratives helps far less. In
the Dutch corpus, wh-questions improved only in the
BHMM-T, not in the BHMM-E.
The Spanish models have higher variance, due to the
small size of the corpus. Due to the high variance, there
are no significant differences between any of the con-
ditions; it is also difficult to spot a trend.
7 Future Work
We have shown sentence type information to be use-
ful for syntactic tagging. However, the BHMM-E and
BHMM-T models are successful in part however be-
cause they also share information as well as split it;
the completely split BHMM-B does not perform well.
Many aspects of tagging do not change significantly
between sentence types. Within a noun phrase, the or-
dering of determiners and nouns is the same whether
it is in a question or an imperative, and to a large ex-
tent the determiners and nouns used will be the same
as well. Learning these patterns over as much input as
possible is essential. Therefore, the next step in this line
of work will be to add a general (corpus-level) model
alongside type-specific models. Ideally, the model will
learn when to use the type-specific model (when tag-
ging the beginning of questions, for instance) and when
to use the general model (when tagging NPs). Such a
model would make use of sentence-type information in
a better way, hopefully leading to further improvements
in performance. A further, more sophisticated model
could learn the useful sentence types distinctions auto-
matically, perhaps forgoing the poorly performing im-
perative or fragment types we tested here in favor of a
more useful type we did not identify.
8 Conclusions
We set out to investigate a hitherto unused source of
information for models of syntactic category learning,
namely intonation and its correlate, sentence type. We
showed that this information is in fact useful, and in-
cluding it in a Bayesian Hidden Markov Model im-
proved unsupervised tagging performance.
We found tagging performance increases if sentence
type information is used to generate either transition
probabilities or emission probabilities in the BHMM.
However, we found that performance decreases if sen-
tence type information is used to generate both transi-
7
Model VM VC VH VI MA
Spanish
BHMM: 49.4 (1.8) 47.2 (1.9) 51.8 (1.8) 2.27 (0.09) 61.5 (2.1)
BHMM-E Q/D: 49.4 (1.5) 47.0 (1.4) 52.1 (1.7) 2.28 (0.06) 60.9 (2.6)
BHMM-E W/Q/D: 48.7 (2.5) 46.4 (2.4) 51.2 (2.6) 2.31 (0.11) 60.2 (3.0)
BHMM-T Q/D: 49.0 (1.7) 46.7 (1.6) 51.6 (1.7) 2.30 (0.07) 60.9 (2.5)
BHMM-T W/Q/D: 49.5 (2.5) 47.2 (2.3) 52.1 (2.8) 2.27 (0.11) 61.0 (3.0)
Cantonese
BHMM: 49.4 (0.8) 44.5 (0.7) 55.4 (1.0) 2.60 (0.04) 67.2 (1.0)
BHMM-E Q/D: 50.7 (1.6) 45.4 (1.5) 57.5 (1.7) 2.55 (0.09) 69.0 (1.0)
BHMM-E W/Q/D: 52.3 (0.3) 46.9 (0.3) 59.3 (0.4) 2.46 (0.02) 69.4 (0.9)
BHMM-T Q/D: 50.3 (0.9) 45.0 (0.9) 57.0 (1.0) 2.57 (0.05) 68.4 (0.8)
BHMM-T W/Q/D: 52.2 (0.8) 46.8 (0.9) 59.1 (0.7) 2.47 (0.05) 69.9 (1.9)
Dutch
BHMM: 48.4 (0.7) 47.1 (0.8) 49.7 (0.7) 2.38 (0.04) 62.3 (0.3)
BHMM-E Q/D: 48.4 (0.4) 47.3 (0.4) 49.7 (0.5) 2.37 (0.02) 62.2 (0.3)
BHMM-E W/Q/D 47.6 (0.3) 46.3 (0.4) 48.8 (0.2) 2.41 (0.02) 61.2 (1.1)
BHMM-T Q/D: 47.9 (0.5) 46.7 (0.4) 49.1 (0.5) 2.40 (0.02) 61.5 (0.4)
BHMM-T W/Q/D: 49.6 (0.2) 48.5 (0.2) 50.8 (0.2) 2.31 (0.10) 64.1 (0.2)
Table 6: Results for BHMM, BHMM-E, and BHMM-T on non-English corpora.
tion and emission probabilities (which is equivalent to
training a separate BHMM for each sentence type).
To test the generality of our findings, we carried out a
series of cross-linguistic experiments, integrating sen-
tence type information in unsupervised tagging mod-
els for Spanish, Cantonese, and Dutch. The results for
Cantonese and Dutch mirrored those for English, show-
ing a small increase in tagging performance for models
that included sentence type information. For Spanish,
no improvement was observed.
References
Roger Brown. 1973. A first language: The early
stages. Harvard University Press.
Anne Fernald. 1989. Intonation and communicative
intent in mothers? speech to infants: Is the melody
the message? Child Development, 60(6):1497?
1510.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics.
Daniel Hirst and Albert Di Cristo, editors. 1998. Into-
nation systems: a survey of twenty languages. Cam-
bridge University Press.
Thomas H.T. Lee, Colleen H Wong, Samuel Leung,
Patricia Man, Alice Cheung, Kitty Szeto, and Cathy
S P Wong. 1994. The development of grammatical
competence in cantonese-speaking children. Techni-
cal report, Report of RGC earmarked grant 1991-94.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
Marina Meila. 2007. Comparing clusterings ? an in-
formation based distance. Journal of Multivariate
Analysis, 98:873?895.
Susana Lopez Ornat. 1994. La adquisicion de la
lengua espagnola. Siglo XXI, Madrid.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In EMNLP.
David Snow and Heather Balog. 2002. Do chil-
dren produce the melody before the words? A re-
view of developmental intonation research. Lingua,
112:1025?1058.
Daniel N. Stern, Susan Spieker, and Kristine MacK-
ain. 1982. Intonation contours as signals in mater-
nal speech to prelinguistic infants. Developmental
Psychology, 18(5):727?735.
Paul A. Taylor, S. King, S. D. Isard, and H. Wright.
1998. Intonation and dialogue context as constraints
for speech recognition. Language and Speech,
41(3):493?512.
Anna Theakston, Elena Lieven, Julian M. Pine, and
Caroline F. Rowland. 2001. The role of per-
formance limitations in the acquisition of verb-
argument structure: an alternative account. Journal
of Child Language, 28:127?152.
8
