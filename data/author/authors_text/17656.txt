Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task
Raphael Rubino??, Jennifer Foster?, Joachim Wagner?,
Johann Roturier?, Rasul Samad Zadeh Kaljahi??, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland
?firstname.lastname@computing.dcu.ie
?firstname lastname@symantec.com
Abstract
This paper describes the features and the ma-
chine learning methods used by Dublin City
University (DCU) and SYMANTEC for the
WMT 2012 quality estimation task. Two sets
of features are proposed: one constrained, i.e.
respecting the data limitation suggested by the
workshop organisers, and one unconstrained,
i.e. using data or tools trained on data that was
not provided by the workshop organisers. In
total, more than 300 features were extracted
and used to train classifiers in order to predict
the translation quality of unseen data. In this
paper, we focus on a subset of our feature set
that we consider to be relatively novel: fea-
tures based on a topic model built using the
Latent Dirichlet Allocation approach, and fea-
tures based on source and target language syn-
tax extracted using part-of-speech (POS) tag-
gers and parsers. We evaluate nine feature
combinations using four classification-based
and four regression-based machine learning
techniques.
1 Introduction
For the first time, the WMT organisers this year pro-
pose a Quality Estimation (QE) shared task, which
is divided into two sub-tasks: scoring and ranking
automatic translations. The aim of this workshop is
to define useful sets of features and machine learn-
ing techniques in order to predict the quality of a
machine translation (MT) output T (Spanish) given
a source segment S (English). Quality is measured
using a 5-point likert scale which is based on post-
editing effort, following the scoring scheme:
1. The MT output is incomprehensible
2. About 50-70% of the MT output needs to be
edited
3. About 25-50% of the MT output needs to be
edited
4. About 10-25% of the MT output needs to be
edited
5. The MT output is perfectly clear and intelligi-
ble
The final score is a combination of the scores as-
signed by three evaluators. The use of a 5-point scale
makes the scoring task more difficult than a binary
classification task where a translation is considered
to be either good or bad. However, if the task is
successfully carried out, the score produced is more
useful.
Dublin City University and Symantec jointly ad-
dress the scoring task. For each pair (S, T ) of source
segment S and machine translation T , we train three
classifiers and one classifier combination using the
training data provided by the organisers to predict
5-point Likert scores. In this paper, we present the
classification results on the test set alng with addi-
tional results obtained using regression techniques.
We evaluate the usefulness of two new sets of fea-
tures:
1. topic-based features using Latent Dirichlet Al-
location (LDA (Blei et al, 2003)),
2. syntax-based features using POS taggers and
parsers (Wagner et al, 2009)
The remainder of this paper is organised as fol-
lows. In Section 2, we give an overview of all the
138
features employed in our QE system. Then, in Sec-
tion 3, we describe the topic and syntax-based fea-
tures in more detail. Section 4 presents the vari-
ous classification and regression techniques we ex-
plored. Our results are presented and discussed in
Section 5. Finally, we summarise and outline our
plans in Section 6.
2 Features Overview
In this section, we describe the features used in our
QE system. In the first subsection, the features in-
cluded in our constrained system are presented. In
the second subsection, we detail the features in-
cluded in our unconstrained system. Both of these
systems include the 17 baseline features provided
for the shared task.
2.1 Constrained System
The constrained system is based only on the data
provided by the organisers. We extracted 70 fea-
tures in total (including the baseline features) and
we present them here according to the type of infor-
mation they capture.
Word and Phrase-Level Features
? Ratio of source and target segment length:
the number of source words divided by the
number of target words
? Ratio of source and target number of punc-
tuation marks: the number of source punctua-
tion marks divided by the number of target ones
? Number of phrases comprising the MT out-
put: given a phrase-table, we assume that a
sentence composed of several phrases indicates
uncertainty on the part of the MT system.
? Average length of source and target phrases:
concatenating short phrases may result in lower
fluency compared to the use of longer ones.
? Ratio of source and target averaged phrase
length
? Number of source prepositions and conjunc-
tions word: our assumption here is that seg-
ments containing a relatively high number of
prepositions and conjunctions may be more
complex and difficult to translate.
? Number of source out-of-vocabulary words
Language Model Features
All the language models (LMs) used in our work
are n-gram LMs with Kneser-Ney smoothing built
with the SRI Toolkit (Stolcke, 2002).
? Backward 2-gram and 3-gram source and
target log probabilities: as proposed by
Duchateau et al (2002)
? Log probability of target segments on
5-gram MT-output-based LM: using
MOSES (Koehn et al, 2007) trained on the
provided parallel corpus, we translated the En-
glish side of this corpus into Spanish, assuming
that the MT output contains mistakes. This
MT output is used to build a LM that models
the behavior of the MT system. We assume
that for a given MT output, a high n-gram
probability (or a low perplexity) of the LM
indicates that the MT output contains mistakes.
MT-system Features
? 15 scores provided by Moses: phrase-table,
language model, reordering model and word
penalty (weighted and unweighted)
? Number of n-bests for each source segment
? MT output back-translation: from Spanish to
English using MOSES trained on the provided
parallel corpus, scored with TER (Snover et
al., 2006), BLEU (Papineni et al, 2002) and
the Levenshtein distance (Levenshtein, 1966),
based on the source segments as a translation
reference
Topic Model Features
? Probability distribution over topics: Source
and target segment probability distribution over
topics for a 10-dimension topic model
? Cosine distance between source and target
topic vectors
More details about these two features are provided
in Section 3.1.
2.2 Unconstrained System
In addition to the features used for the constrained
system, a further 238 unconstrained features were
included in our unconstrained system.
139
MT System Features
As for our constrained system, we use MT output
back-translation from Spanish to English, but this
time using Bing Translator1 in addition to Moses.
Each back-translated segment is scored with TER,
BLEU and the Levenshtein distance, based on the
source segments as a translation reference.
Source Syntax Features
Wagner et al (2007; 2009) propose a series of
features to measure sentence grammaticality. These
features rely on a part-of-speech tagger, a probabilis-
tic parser and a precision grammar/parser. We have
at our disposal these tools for English and so we ap-
ply them to the source data. The features themselves
are described in more detail in Section 3.2.
Target Syntax Features
We use a part-of-speech tagger trained on Spanish
to extract from the target data the subset of grammat-
icality features proposed by Wagner et al (2007;
2009) that are based on POS n-grams. In addition
we extract features which reflect the prevalence of
particular POS tags in each target segment. These
are explained in more detail in Section 3.2 below.
Grammar Checker Features
LANGUAGETOOL (based on (Naber, 2003)) is an
open-source grammar and style proofreading tool
that finds errors based on pre-defined, language-
specific rules. The latest version of the tool can
be run in server mode, so individual sentences can
be checked and assigned a total number of errors
(which may or may not be true positives).2 This
number is used as a feature for each source segment
and its corresponding MT output.
3 Topic and Syntax-based Features
In this section, we focus on the set of features
that aim to capture adequacy using topic modelling
and grammaticality using POS tagging and syntactic
parsing.
1http://www.microsofttranslator.com/
2The list of English and Spanish rules is available at:
http://languagetool.org/languages.
3.1 Topic-based Features
We extract source and target features based on a
topic model built using LDA. The main idea in topic
modelling is to produce a set of thematic word clus-
ters from a collection of documents. Using the par-
allel corpus provided for the task, a bilingual corpus
is built where each line is composed of a source seg-
ment and its translation separated by a space. Each
pair of segments is considered as a bilingual docu-
ment. This corpus is used to train a bilingual topic
model after stopwords removal. The resulting model
is one set of bilingual topics z containing words w
with a probability p(wn|zn, ?) (with n equal to the
vocabulary size in the whole parallel corpus). This
model can be used to infer the probability distri-
bution of unseen source and target segments over
bilingual topics. During the test step, each source
segment and its translation are considered individu-
ally, as two monolingual documents. This method
allows us to compare the source and target topic dis-
tributions. We assume that a source segment and its
translation share topic similarities.
We propose two ways of using topic-based fea-
tures for quality estimation: keeping source and tar-
get topic vectors as two sets of k features, or com-
puting a vector distance between these two vectors
and using one feature only. To measure the prox-
imity of two vectors, we decided to used the Co-
sine distance, as it leads to the best results in terms
of classification accuracy. However, we plan to
study different metrics in further experiments, like
the Manhattan or the Euclidean distances. Some
parameters related to LDA have to be studied more
carefully too, such as the number of topics (dimen-
sions in the topic space), the number of words per
topic, the Dirichlet hyperparameter ?, etc. In our
experiments, we built a topic model composed of 10
dimensions using Gibbs sampling with 1000 itera-
tions. We assume that a higher dimensionality can
lead to a better repartitioning of the vocabulary over
the topics.
Multilingual LDA has been used before in nat-
ural language processing, e.g. polylingual topic
models (Mimno et al, 2009) or multilingual topic
models for unaligned text (Boyd-Graber and Blei,
2009). In the field of machine translation, Tam et
al. (2007) propose to adapt a translation and a lan-
140
guage model to a specific topic using Latent Se-
mantic Analysis (LSA, or Latent Semantic Index-
ing, LSI (Deerwester et al, 1990)). More recently,
some studies were conducted on the use of LDA to
adapt SMT systems to specific domains (Gong et al,
2010; Gong et al, 2011) or to extract bilingual lexi-
con from comparable corpora (Rubino and Linare`s,
2011). Extracting features from a topic model is, to
the best of our knowledge, the first attempt in ma-
chine translation quality estimation.
3.2 Syntax-based Features
Syntactic features have previously been used in MT
for confidence estimation and for building automatic
evaluation measures. Corston-Oliver et al (2001)
build a classifier using 46 parse tree features to pre-
dict whether a sentence is a human translation or MT
output. Quirk (2004) uses a single parse tree feature
in the quality estimation task with a 4-point scale,
namely whether a spanning parse can be found, in
addition to LM perplexity and sentence length. Liu
and Gildea (2005) measure the syntactic similarity
between MT output and reference translation. Al-
brecht and Hwa (2007) measure the syntactic simi-
larity between MT output and reference translation
and between MT output and a large monolingual
corpus. Gimenez and Marquez (2007) explore lexi-
cal, syntactic and shallow semantic features and fo-
cus on measuring the similarity of MT output to ref-
erence translation. Owczarzak et al (2007) use la-
belled dependencies together with WordNet to avoid
penalising valid syntactic and lexical variations in
MT evaluation. In what follows, we describe how
we make use of syntactic information in the QE task,
i.e. evaluating MT output without a reference trans-
lation.
Wagner et al (2007; 2009) use three sources
of linguistic information in order to extract features
which they use to judge the grammaticality of En-
glish sentences:
1. For each POS n-gram (with n ranging from 2 to
7), a feature is extracted which represents the
frequency of the least frequent n-gram in the
sentence according to some reference corpus.
TreeTagger (Schmidt, 1994) is used to produce
POS tags.
2. Features provided by a hand-crafted, broad-
coverage precision grammar of English (Butt
et al, 2002) and a Lexical Functional Grammar
parser (Maxwell and Kaplan, 1996). These in-
clude whether or not a sentence could be parsed
without resorting to robustness measures, the
number of analyses found and the parsing time.
3. Features extracted from the output of three
probabilistic parsers of English (Charniak and
Johnson, 2005), one trained on Wall Street
Journal trees (Marcus et al, 1993), one trained
on a distorted version of the treebank obtained
by automatically creating grammatical error
and adjusting the parse trees, and the third
trained on the union of the original and dis-
torted versions.
These features were originally designed to distin-
guish grammatical sentences from ungrammatical
ones and were tested on sentences from learner cor-
pora by Wagner et al (2009) and Wagner (2012).
In this work we extract all three sets of features
from the source side of our data and the POS-based
subset from the target side.3 We use the publicly
available pre-trained TreeTagger models for English
and Spanish4. The reference corpus used to obtain
POS n-gram frequences is the MT translation model
training data.5
In addition to the POS-based features described in
Wagner et al (2007; 2009), we also extract the fol-
lowing features from the Spanish POS-tagged data:
for each POS tag P and target segment T , we ex-
tract a feature which is the proportion of words in
T that are tagged as P . Two additional features are
extracted to represent the proportion of words in T
that are assigned more than one tag by the tagger,
3Unfortunately, due to time constraints, we were unable to
source a suitable probabilistic phrase-structure parser and a pre-
cision grammar for Spanish and were thus unable to extract
parser-based features for Spanish. We expect that these features
would be more useful on the target side than the source side.
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5To aid machine learning methods that linearly combine fea-
ture values, we add binarised features derived from the raw XLE
and POS n-gram features described above, for example we add
a feature indicating whether the frequency of the least frequent
POS 5-gram is below 10. We base the choice of binary fea-
tures on (a) decision rules observed in decision trees trained for
a binary scoring task and (b) decision rules of simple classifiers
(decision trees with just one decision node and 2 leaf nodes)
that form a convex hull of optimal classifiers in ROC space.
141
and the proportion of words in T that are unknown
to the tagger.
4 Machine Learning
In this section, we describe the machine learning
methods that we experimented with. Our final sys-
tems submitted for the shared task are based on clas-
sification methods. However, we also performed
some experiments with regression methods.
We evaluate the systems on the test set using the
official evaluation script and the reference scores.
We report the evaluation results as Mean Aver-
age Error (MAE) and Root Mean Squared Error
(RMSE).
4.1 Classification
In order to apply classification algorithms to the
set of features associated with each source and tar-
get segment, we rounded the training data scores
to the closest integer. We tested several classifiers
and empirically chose three algorithms: Support
Vector Machine using sequential minimal optimiza-
tion and RBF kernel (parameters optimized by grid-
search) (Platt, 1999), Naive Bayes (John and Lang-
ley, 1995) and Random Forest (Breiman, 2001) (the
latter two techniques were applied with default pa-
rameters). We use the Weka toolkit (Hall et al,
2009) to train the classifiers and predict the scores
on the test set. Each method is evaluated individu-
ally and then combined by averaging the predicted
scores.
4.2 Regression
We applied three different regression techniques:
SVM epsilon-SVR with RBF kernel, Linear Regres-
sion and M5P (Quinlan, 1992; Wang and Witten,
1997). The two latter algorithms were used with
default parameters, whereas SVM parameters (?, c
and ) were optimized by grid-search. We also per-
formed a combination of the three algorithms by av-
eraging the predicted scores. We apply a linear func-
tion on the predicted scores S in order to keep them
in the correct range (from 1 to 5) as detailed in (1),
where S? is the rescaled sentence score, Smin is the
lowest predicted score and Smax is the highest pre-
dicted score.
S? = 1 + 4?
S ? Smin
Smax ? Smin
(1)
5 Evaluation
Table 1 shows the results obtained by our classifi-
cation approach on various feature subsets. Note
that the two submitted systems used the combined
classifier approach with the constrained and uncon-
strained feature sets. Table 2 shows the results for
the same feature combinations, this time using re-
gression rather than classification.
The results of quality estimation using classifica-
tion methods show that the baseline and the syntax-
based features with the classifier combination leads
to the best results with an MAE of 0.71 and an
RMSE of 0.87. However, these scores are substan-
tially lower than the ones obtained using regression,
where the unconstrained set of features with SVM
leads to an MAE of 0.62 and an RMSE of 0.78.
It seems that the classification methods are not
suitable for this task according to the different sets
of features studied. Furthermore, the topic-distance
feature is not correlated with the quality scores, ac-
cording to the regression results. On the other hand,
the syntax-based features appear to be the most in-
formative and lead to an MAE of 0.70.
6 Conclusion
We presented in this paper our submission for the
WMT12 Quality Estimation shared task. We also
presented further experiments using different ma-
chine learning techniques and we evaluated the im-
pact of two sets of features - one set which is based
on linguistic features extracted using POS tagging
and parsing, and a second set which is based on topic
modelling. The best results are obtained by our un-
constrained system containing all features and us-
ing an -SVR regression method with a Radial Basis
Function kernel. This setup leads to a Mean Aver-
age Error of 0.62 and a Root Mean Squared Error
of 0.78. Unfortunately, we did not submit our best
configuration for the shared task.
We plan to continue working on the task of ma-
chine translation quality estimation. Our immediate
next steps are to continue to investigate the contribu-
tion of individual features, to explore feature selec-
tion in a more detailed fashion and to apply our best
system to other types of data including sentences
taken from an online discussion forum.
142
SMO NAIVE BAYES RANDOM FOREST Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88
topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98
topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04
syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90
baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95
baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87
baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93
all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ? 1.12 ?
all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ? 0.97 ?
Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with ?
and ? correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted
for the shared task.
SVM LINEAR REG. M5P Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88
topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95
topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24
syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92
baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10
baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22
baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21
all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88
all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91
Table 2: MAE and RMSE results for different sets of features using three regression methods.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of
the ACL, pages 880?887.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet Allocation. The Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the 25th Conference on Uncertainty in Artificial In-
telligence, pages 75?82.
L. Breiman. 2001. Random forests. Machine learning,
45(1):5?32.
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
Proceedings of the Coling Workshop on Grammar En-
gineering and Evaluation.
E. Charniak and M. Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 173?180, Ann Arbor.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic evalu-
ation of machine translation. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 148?155, Toulouse, France, July.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41(6):391?407.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings IEEE international confer-
ence on acoustics, speech, and signal processing,
ICASSP?2002, volume 1, pages 221?224.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 256?264, Prague, Czech
Republic, June.
143
Z. Gong, Y. Zhang, and G. Zhou. 2010. Statistical ma-
chine translation based on lda. In Universal Commu-
nication Symposium (IUCS), 2010 4th International,
pages 286?290.
Z. Gong, G. Zhou, and L. Li. 2011. Improve smt with
source-side ?topic-document? distributions. In MT
Summit, pages 496?501.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
G.H. John and P. Langley. 1995. Estimating continuous
distributions in bayesian classifiers. In Eleventh con-
ference on uncertainty in artificial intelligence, pages
338?345. Morgan Kaufmann Publishers Inc.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, volume 10-8, pages 707?710.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 25?32, Ann Arbor, Michigan.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
D. Mimno, H.M. Wallach, J. Naradowsky, D.A. Smith,
and A. McCallum. 2009. Polylingual topic models.
In Proceedings of EMNLP: Volume 2-Volume 2, pages
880?889. Association for Computational Linguistics.
D. Naber. 2003. A rule-based style and grammar
checker. Technical report, Bielefeld University Biele-
feld, Germany.
K. Owczarzak, J. van Genabith, and A. Way. 2007. La-
belled dependencies in machine translation evaluation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 104?111, Prague, Czech
Republic, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
kernel methods, pages 185?208. MIT Press.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of LREC,
Lisbon, June.
R. Rubino and G. Linare`s. 2011. A multi-view approach
for term translation spotting. Computational Linguis-
tics and Intelligent Text Processing, 6609:29?40.
H. Schmidt. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Natural Lan-
guage Processing.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In InterSpeech, volume 2, pages 901?
904.
Y.C. Tam, I. Lane, and T. Schultz. 2007. Bilingual
lsa-based adaptation for statistical machine translation.
Machine Translation, 21(4):187?207.
J. Wagner, J. Foster, and J. van Genabith. 2007. A com-
parative evaluation of deep and shallow approaches to
the automatic detection of common grammatical er-
rors. In Proceedings of EMNLP-CoNLL, pages 112?
121, Prague, Czech Republic, June.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
J. Wagner. 2012. Detecting grammatical errors with
treebank-induced probabilistic parsers. Ph.D. thesis,
Dublin City University.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
144
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
