Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 209?216
Manchester, August 2008
Syntactic Reordering Integrated with Phrase-based SMT
Jakob Elming
Computational Linguistics
Copenhagen Business School
jel.isv@cbs.dk
Abstract
We present a novel approach to word
reordering which successfully integrates
syntactic structural knowledge with
phrase-based SMT. This is done by con-
structing a lattice of alternatives based
on automatically learned probabilistic
syntactic rules. In decoding, the alter-
natives are scored based on the output
word order, not the order of the input.
Unlike previous approaches, this makes it
possible to successfully integrate syntactic
reordering with phrase-based SMT. On
an English-Danish task, we achieve an
absolute improvement in translation qual-
ity of 1.1 % BLEU. Manual evaluation
supports the claim that the present ap-
proach is significantly superior to previous
approaches.
1 Introduction
The emergence of phrase-based statistical machine
translation (PSMT) (Koehn et al, 2003) has been
one of the major developments in statistical ap-
proaches to translation. Allowing translation of
word sequences (phrases) instead of single words
provides SMT with a robustness in word selection
and local word reordering.
PSMT has two means of reordering the words.
Either a phrase pair has been learned where the tar-
get word order differs from the source (phrase in-
ternal reordering), or distance penalized orderings
of target phrases are attempted in decoding (phrase
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
external reordering). The first solution is strong,
the second is weak.
The second solution is necessary for reorderings
within a previously unseen sequence or over dis-
tances greater than the maximal phrase length. In
this case, the system in essence relies on the tar-
get side language model to get the correct word
order. The choice is made without knowing what
the source is. Basically, it is a bias against phrase
external reordering.
It seems clear that reordering often depends on
higher level linguistic information, which is ab-
sent from PSMT. In recent work, there has been
some progress towards integrating syntactic infor-
mation with the statistical approach to reorder-
ing. In works such as (Xia and McCord, 2004;
Collins et al, 2005; Wang et al, 2007; Habash,
2007), reordering decisions are done ?determinis-
tically?, thus placing these decisions outside the
actual PSMT system by learning to translate from
a reordered source language. (Crego and Mari?no,
2007; Zhang et al, 2007; Li et al, 2007) are more
in the spirit of PSMT, in that multiple reorderings
are presented to the PSMT system as (possibly
weighted) options.
Still, there remains a basic conflict between the
syntactic reordering rules and the PSMT system:
one that is most likely due to the discrepancy be-
tween the translation units (phrases) and units of
the linguistic rules, as (Zhang et al, 2007) point
out.
In this paper, we proceed in the spirit of the non-
deterministic approaches by providing the decoder
with multiple source reorderings. But instead of
scoring the input word order, we score the order of
the output. By doing this, we avoid the integration
problems of previous approaches.
It should be noted that even though the experi-
209
ments are conducted within a source reordering ap-
proach, this scoring is also compatible with other
approach. We will, however, not look further into
this possiblity in the present paper.
In addition, we automatically learn reordering
rules based on several levels of linguistic informa-
tion fromword form to subordination and syntactic
structure to produce reordering rules that are not
restricted to operations on syntactic tree structure
nodes.
In the next section, we discuss and contrast re-
lated work. Section 3 describes aspects of English
and Danish structure that are relevant to reorder-
ing. Section 4 describes the automatic induction
of reordering rules and its integration in PSMT. In
section 5, we describe the SMT system used in the
experiments. Section 6 evaluates and discusses the
present approach.
2 Related Work
While several recent authors have achieved posi-
tive results, it has been difficult to integrate syn-
tactic information while retaining the strengths of
the statistical approach.
Several approaches do deterministic reordering.
These do not integrate the reordering in the PSMT
system; instead they place it outside the system
by first reordering the source language, and then
having a PSMT system translate from reordered
source language to target language. (Collins et al,
2005; Wang et al, 2007) do this using manually
created rules, and (Xia and McCord, 2004) and
(Habash, 2007) use automatically extracted rules.
All use rules extracted from syntactic parses.
As mentioned by (Al-Onaizan and Papineni,
2006), it can be problematic that these determin-
istic choices are beyond the scope of optimization
and cannot be undone by the decoder. That is,
there is no way to make up for bad information
in later translation steps.
Another approach is non-deterministic. This
provides the decoder with both the original and
the reordered source sentence. (Crego and Mari?no,
2007) operate within Ngram-based SMT. They
make use of syntactic structure to reorder the in-
put into a word lattice. Since the paths are not
weighted, the lattice merely narrows down the size
of the search space. The decoder is not given rea-
son to trust one path (reordering) over another.
(Zhang et al, 2007) assign weights to the paths
of their input word lattice. Instead of hierarchical
linguistic structure, they use reordering rules based
on POS and syntactic chunks, and train the system
with both original and reordered source word order
on a restricted data set (<500K words). Their sys-
tem does not out-perform a standard PSMT sys-
tem. As they themselves point out, a reason for
this might be that their reordering approach is not
fully integrated with PSMT. This is one of the main
problems addressed in the present work.
(Li et al, 2007) use weighted n-best lists as in-
put for the decoder. They use rules based on a
syntactic parse, allowing children of a tree node
to swap place. This is excessively restrictive. For
example, a common reordering in English-Danish
translation has the subject change place with the
finite verb. Since the verb is often embedded in
a VP containing additional words that should not
be moved, such rules cannot be captured by local
reordering on tree nodes.
In many cases, the exact same word order that is
obtained through a source sentence reordering, is
also accessible through a phrase internal reorder-
ing. A negative consequence of source order (SO)
scoring as done by (Zhang et al, 2007) and (Li
et al, 2007) is that they bias against the valuable
phrase internal reorderings by only promoting the
source sentence reordering. As described in sec-
tion 4.3, we solve this problem by reordering the
input string, but scoring the output string, thus
allowing the strengths of PSMT to co-exist with
rule-based reordering.
3 Language comparison
The two languages examined in this investigation,
English and Danish, are very similar from a struc-
tural point of view. A word alignment will most of-
ten display an almost one-to-one correlation. In the
hand-aligned data, only 39% of the sentences con-
tain reorderings (following the notion of reorder-
ing as defined in 4.1). On average, a sentence con-
tains 0.66 reorderings.
One of the main differences between English
and Danish word order is that Danish is a verb-
second language: the finite verb of a declarative
main clause must always be the second constituent.
Since this is not the case for English, a reordering
rule should move the subject of an English sen-
tence to the right of the finite verb, if the first po-
sition is filled by something other than the subject.
This is exemplified by (1) (examples are annotated
with English gloss and translation), where ?they?
210
t7
? ? ? ? ? ? 
t
6
? ?  ? ? ? ?
t
5
?  ? ? ? ? ?
t
4
? ? ? ?  ? ?
t
3
? ? ? ? ?  ?
t
2
? ? ?  ? ? ?
t
1
 ? ? ? ? ? ?
s
1
s
2
s
3
s
4
s
5
s
6
s
7
Table 1: Reordering example
should move to the right of ?come? to get the Dan-
ish word order as seen in the gloss.
(1)
[
nu
now
kommer
come
de
they ]
?here they come?
Another difference is that Danish sentence adver-
bials in a subordinate clause move to the left of
the finite verb. This is illustrated in example (2).
This example also shows the difficulty for a PSMT
system. Since the trigram ?han kan ikke? is fre-
quent in Danish main clauses, and ?han ikke kan?
is frequent in subordinate clauses, we need infor-
mation on subordination to get the correct word
order. This information can be obtained from the
conjunction ?that?. A trigram PSMT system would
not be able to handle the reordering in (2), since
?that? is beyond the scope of ?not?.
(2)
[
han
he
siger
says
at
that
han
he
ikke
not
kan
can
se
see ]
?he says that he can not see?
In the main clause, on the other hand, Danish
prefers the sentence adverbial to appear to the right
of the finite verb. Therefore, if the English adver-
bial appears to the left of the finite verb in a main
clause, it should move right as in example (3).
(3)
[
hun
she
s?a
saw
aldrig
never
skibet
the ship ]
?she never saw the ship?
4 Reordering rules
4.1 Definition of reordering
In this experiment, reordering is defined as two
word sequences exchanging positions. These
two sequences are restricted by the following
conditions:
? Parallel consecutive: They have to make up
consecutive sequences of words, and each has
to align to a consecutive sequence of words.
? Maximal: They have to be the longest possi-
ble consecutive sequences changing place.
? Adjacent: They have to appear next to each
other on both source and target side.
The sequences are not restricted in length, mak-
ing both short and long distance reordering possi-
ble. Furthermore, they need not be phrases in the
sense that they appear as an entry in the phrase ta-
ble.
Table 1 illustrates reordering in a word align-
ment matrix. The table contains reorderings be-
tween the light grey sequences (s
3
2
and s
6
4
)
1
and
the dark grey sequences (s
5
5
and s
6
6
). On the other
hand, the sequences s
3
3
and s
5
4
are e.g. not consid-
ered reordered, since neither are maximal, and s
5
4
is not consecutive on the target side.
4.2 Rule induction
In section 3, we pointed out that subordination is
very important for word order differences between
English and Danish. In addition, the sentence po-
sition of constituents plays a role. All this infor-
mation is present in a syntactic sentence parse. A
subordinate clause is defined as inside an SBAR
constituent; otherwise it is a main clause. The con-
stituent position can be extracted from the sentence
start tag and the following syntactic phrases. POS
and word form are also included to allow for more
specific/lexicalized rules.
Besides including this information for the candi-
date reordering sequences (left sequence (LS) and
right sequence (RS)), we also include it for the set
of possible left (LC) and right (RC) contexts of
these. The span of the contexts varies from a single
word to all the way to the sentence border. Table
2 contains an example of the information available
to the learning algorithm. In the example, LS and
RS should change place, since the first position is
occupied by something other than the subject in a
main clause.
In order to minimize the training data, word
and POS sequences are limited to 4 words, and
phrase structure (PS) sequences are limited to 3
constituents. In addition, an entry is only used if
at least one of these three levels is not too long for
1
Notation: s
y
x
means the consecutive source sequence cov-
ering words x to y.
211
Level LC LS RS RC
WORD <s> today , || today , || , he was driving home || home . || home . < /s>
POS <S> NN , || NN , || , PRP AUX VBG NN || NN . || NN . < /S>
PS <S> NP , || NP , || , NP AUX VBG ADVP || ADVP . || ADVP . < /S>
SUBORD main main main main
Table 2: Example of experience for learning. Possible contexts separated by ||.
both LS and RS, and too long contexts are not in-
cluded in the set. This does not constrain the pos-
sible length of a reordering, since a PS sequence of
length 1 can cover an entire sentence.
In order to extract rules from the annotated
data, we use a rule-based classifier, Ripper (Cohen,
1996). The motivation for using Ripper is that it al-
lows features to be sets of strings, which fits well
with our representation of the context, and it pro-
duces easily readable rules that allow better under-
standing of the decisions being made. In section
6.2, extracted rules are exemplified and analyzed.
The probabilities of the rules are estimated using
Maximum Likelihood Estimation based on the in-
formation supplied by Ripper on the performance
of the individual rules on the training data. These
logarithmic probabilities are easily integratable in
the log-linear PSMT model as an additional pa-
rameter by simple addition.
The rules are extracted from the hand-aligned,
Copenhagen Danish-English Dependency Tree-
bank (Buch-Kromann et al, 2007). 5478 sentences
from the news paper domain containing 111,805
English words and 100,185 Danish words. The
English side is parsed using a state-of-the-art sta-
tistical English parser (Charniak, 2000).
4.3 Integrating rule-based reordering in
PSMT
The integration of the rule-based reordering in our
PSMT system is carried out in two separate stages:
1. Reorder the source sentence to assimilate the
word order of the target language.
2. Score the target word order according to the
relevant rules.
Stage 1) is done in a non-deterministic fashion by
generating a word lattice as input in the spirit of
e.g. (Zens et al, 2002; Crego and Mari?no, 2007;
Zhang et al, 2007). This way, the system has both
the original word order, and the reorderings pre-
dicted by the rule set. The different paths of the
word lattice are merely given as equal suggestions
to the decoder. They are in no way individually
weighted.
Separating stage 2) from stage 1) is motivated
by the fact that reordering can have two distinct
origins. They can occur because of stage 1), i.e.
the lattice reordering of the original English word
order (phrase external reordering), and they can
occur inside a single phrase (phrase internal re-
ordering). We are, however, interested in doing
phrase-independent, word reordering. We want to
promote rule-predicted reorderings, regardless of
whether they owe their existence to a syntactic rule
or a phrase table entry.
This is accomplished by letting the actual scor-
ing of the reordering focus on the target string. The
decoder is informed of where a rule has predicted
a reordering, how much it costs to do the reorder-
ing, and how much it costs to avoid it. This is
then checked for each hypothezised target string
by keeping track of what source position target or-
der (SPTO) it corresponds to.
The SPTO is a representation of which source
position the word in each target position originates
from. Putting it differently, the hypotheses contain
two parallel strings; a target word string and its
SPTO string. In order to access this information,
each phrase table entry is annotated with its inter-
nal word alignment, which is available as an in-
termediate product from phrase table creation. If a
phrase pair has multiple word alignments, the most
frequent is chosen.
Table 3 exemplifies the SPTO scoring. The
source sentence is ?today he was late?, and a rule
has predicted that word 3 and 4 should change
place. When the decoder has covered the first four
input words, two of the hypotheses might be H1
and H2. At this point, it becomes apparent that H2
contains the desired SPTO (namely ?4 3?), and it
get assigned the reordering cost. H1 does not con-
tain the rule-suggested SPTO (in stead, the words
are in the order ?3 4?), and it gets the violation cost.
Both these scorings are performed in a phrase-
212
Source sentence: today
1
,
2
he
3
was
4
late
5
Rule: 3 4 ? 4 3
Hypothesis Target string SPTO
H1 idag han var 1 3 4
H2 idag var han 1 4 3
Table 3: Example of SPTO scoring during decod-
ing at source word 4.
independent manner. The decoder assigns the re-
ordering cost to H2 without knowing whether the
reordering is internal (due to a phrase table entry)
or external (due to a syntactic rule).
Phrase internal reorderings at other points of the
sentence, i.e. points that are not covered by a rule,
are not judged by the reordering model. Our rule
extraction does not learn every possible reordering
between the two languages, but only the most gen-
eral ones. If no rule has an opinion at a certain
point in a sentence, the decoder is free to chose the
translation it prefers without reordering cost.
Separating the scoring from the source language
reordering also has the advantage that the SPTO
scoring in essence is compatible with other ap-
proaches such as a traditional PSMT system. We
will, however, not examine this possibility further
in the present paper.
5 The PSMT system
The baseline is the PSMT system used for the
2006 NAACL SMT workshop (Koehn and Monz,
2006) with phrase length 3 and a trigram language
model (Stolcke, 2002). The system was trained
on the English and Danish part of the Europarl
corpus version 3 (Koehn, 2005). Fourth quarter
of 2000 was removed in order to use the com-
mon test set of 11369 sentences (330,082 English
words and 309,942 Danish words with one ref-
erence) for testing. In addition, fourth quarter
of 2001 was removed for development purposes.
Of these, 10194 were used for various analysis
purposes, thereby keeping the test data perfectly
unseen. 500 sentences were taken from the de-
velopment set for tuning the decoder parameters.
This was done using the Downhill Simplex algo-
rithm. In total, 1,137,088 sentences containing
31,376,034 English words and 29,571,518 Danish
words were left for training the phrase table and
language model.
The decoder used for the baseline system
is Pharaoh (Koehn, 2004) with its distance-
Figure 1: Example word lattice.
System Dev Test Swap Subset
Baseline 0.262 0.252 0.234
no scoring 0.267 0.256 0.241
SO scoring 0.268 0.258 0.244
SPTO scoring 0.268 0.258 0.245
Table 4: BLEU scores for different scoring meth-
ods.
penalizing reordering model. For the experiments,
we use our own decoder which ? except for the
reordering model ? uses the same knowledge
sources as Pharaoh, i.e. bidirectional phrase trans-
lation model and lexical weighting model, phrase
and word penalty, and target language model. Its
behavior is comparable to Pharaoh when doing
monotone decoding.
The search algorithm of our decoder is similar
to the RG graph decoder of (Zens et al, 2002). It
expects a word lattice as input. Figure 1 shows the
word lattice for the example in table 3.
Since the input format defines all possible word
orders, a simple monotone search is sufficient. Us-
ing a language model of order n, for each hy-
pothezised target string ending in the same n-1-
gram, we only have to extend the highest scoring
hypothesis. None of the others can possibly out-
perform this one later on. This is because the max-
imal context evaluating a phrase extending this hy-
pothesis, is the history (n-1-gram) of the first word
of that phrase. The decoder is not able to look any
further back at the preceeding string.
6 Evaluation
6.1 Results and discussion
The SPTO reordering approach is evaluated on the
11369 sentences of the common test set. Results
are listed in table 4 along with results on the de-
velopment set. We also report on the swap subset.
These are the 3853 sentences where the approach
actually motivated reorderings in the test set, in-
ternal or external. The remaining 7516 sentences
were not influenced by the SPTO reordering ap-
proach.
213
System BLEU Avr. Human rating
Baseline 0.234 3.00 (2.56)
no scoring 0.240 3.00 (2.74)
SO scoring 0.239 3.00 (2.62)
SPTO scoring 0.244 2.00 (2.08)
Table 5: Evaluation on the set where SO and SPTO
produce different translations. Average human rat-
ings are medians with means in parenthesis, lower
scores are better, 1 is the best score.
We report on 1) the baseline PSMT system, 2) a
system provided with a rule reordered word lattice
but no scoring, 3) the same system but with an SO
scoring in the spirit of (Zhang et al, 2007; Li et al,
2007), and finally 4) the same system but with the
SPTO scoring.
The SPTO approach gets an increase over the
baseline PSMT system of 0.6 % BLEU. The swap
subset, however, shows that the extracted rules are
somewhat restricted, only resulting in swap in
1
3
of the sentences. The relevant set, i.e. the set
where the present approach actually differs from
the baseline, is therefore the swap subset. This
way, we concentrate on the actual focus of the pa-
per, namely the syntactically motivated SPTO re-
ordering. Here we achieve an increase in perfor-
mance of 1.1 % BLEU.
Comparing to the other scoring approaches does
not show much improvement. A possible explana-
tion is that the rules do not apply very often, in
combination with the fact that the SO and SPTO
scoring mechanisms most often behave alike. The
difference in SO and SPTO scoring only leads to
a difference in translation in 10% of the sentences
where reordering is done. This set is interesting,
since it provides a focus on the difference between
the SO and the SPTO approaches. In table 5, we
evaluate on this set.
The BLEU scores on the entire set indicate that
SPTO is a superior scoring method. To back
this observation, the 100 first sentences are man-
ually evaluated by two native speakers of Danish.
(Callison-Burch et al, 2007) show that ranking
sentences gives higher inter-annotator agreement
than scoring adequacy and fluency. We therefore
employ this evaluation method, asking the evalua-
tors to rank sentences from the four systems given
the input sentence. Ties are allowed. The an-
notators had reasonable inter-annotator agreement
(? = 0.523, P (A) = 0.69, P (E) = 0.35). Table 5
Decoder choice SO SPTO
Phrase internal reordering 401 1538
Phrase external reordering 3846 2849
Reject reordering 1468 1328
Table 6: The choices made based on the SO and
SPTO scoring for the 5715 reorderings proposed
by the rules for the test data.
shows the average ratings of the systems. This
clearly shows the SPTO scoring to be significantly
superior to the other methods (p < 0.05).
Most of the cases (55) where SPTO outperforms
SO are cases where SPTO knows that a phrase pair
contains the desired reordering, but SO does not.
Therefore, SO has to use an external reordering
which brings poorer translation than the internal
reordering, because the words are translated indi-
vidually rather than by a single phrase (37 cases),
or it has to reject the desired reordering (18 cases),
which also hurts translation, since it does not get
the correct word order.
Table 6 shows the effect of SO and SPTO scor-
ing in decoding. Most noticeable is that the SO
scoring is strongly biased against phrase inter-
nal reorderings; SPTO uses nearly four times as
many phrase internal reorderings as SO. In addi-
tion, SPTO is a little less likely to reject a rule pro-
posed reordering.
6.2 Rule analysis
The rule induction resulted in a rule set containing
27 rules. Of these, 22 concerned different ways
of identifying contexts where a reordering should
occur due to the verb second nature of Danish. 4
rules had to do with adverbials in main and in sub-
ordinate clauses, and the remaining rule expressed
that currency is written after the amount in Dan-
ish, while it is the other way around in English.
Since the training data however only includes Dan-
ish Crowns, the rule was lexicalized to ?DKK?.
Table 7 shows a few of the most frequently used
rules. The first three rules deal with the verb
second phenomenon. The only difference among
these is the left context. Either it is a prepositional
phrase, a subordinate clause or an adverbial. These
are three ways that the algorithm has learned to
identify the verb second phenomenon conditions.
Rule 3 is interesting in that it is lexicalized. In the
learning data, the Danish correspondent to ?how-
ever? is most often not topicalized, and the subject
214
No LC LS RS RC
1 PS: <S> PP , PS: NP POS: FV
2 PS: SBAR , PS: NP POS: FV
3 PS: ADVP , PS: NP POS: FV
! WORD:
however ,
4 PS: FV POS: RB PS: VP
SUB: sub
5 PS: <S> NP PS: ADVP POS: FV
SUB: main
Table 7: Example rules and their application statis-
tics.
is therefore not forced from the initial position. As
a consequence, the rule states that it should only
apply, if ?however? is not included in the left con-
text of the reordering.
Rule 4 handles the placement of adverbials in a
subordinate clause. Since the right context is sub-
ordinate and a verb phrase, the current sequences
must also be subordinate. In contrast, the fifth rule
deals with adverbials in a main clause, since the
left context noun phrase is in a main clause.
A problem with the hand-aligned data used for
rule-induction is that it is out of domain compared
to the Europarl data used to train the SMT system.
The hand-aligned data is news paper texts, and Eu-
roparl is transcribed spoken language from the Eu-
ropean Parliament. Due to its spoken nature, Eu-
roparl contains frequent sentence-initial forms of
address. That is, left adjacent elements that are not
integrated parts of the sentence as in example (4).
This is not straightforward, because on the sur-
face these look a lot like topicalized constructions,
as in example (5). In topicalized constructions, it
is an integrated part of the sentence that is moved
to the front in order to affect the flow of discourse
information. This difference is crucial for the re-
ordering rules, since ?i? and ?have? should reorder
in (5), but not in (4), in order to get Danish word
order.
(4) mr president , i have three points .
(5) as president , i have three points .
When translating the development set, it became
clear that many constructions like (4) were re-
ordered. Since these constructions were not
present in the hand-aligned data, the learning algo-
rithm did not have the data to learn this difference.
We therefore included a manual, lexicalized rule
stating that if the left context contained one of a set
of titles (mr, mrs, ms, madam, gentlemen), the re-
ordering should not take place. Since the learning
includes word form information, this is a rule that
the learning algorithm is able to learn. To a great
extent, the rule eliminates the problem.
The above examples also illustrate that local re-
ordering (in this case as local as two neighbor-
ing words) can be a problem for PSMT, since
even though the reordering is local, the informa-
tion about whether to reorder or not is not neces-
sarily local.
6.3 Reordering analysis
In this section, we will show and discuss a few
examples of the reorderings made by the SPTO
approach. Table 8 contain two translations taken
from the test set.
In translation 1), the subject (bold) is correctly
moved to the right of the finite verb (italics), which
the baseline system fails to do. Moving the finite
verb away from the infinite verb ?feature?, how-
ever, leads to incorrect agreement between these.
While the baseline correctly retains the infinite
form (?st?a?), the language model forces another fi-
nite form (the past tense ?stod?) in the SPTO re-
ordering approach.
Translation 2) illustrates the handling of adver-
bials. The first reordering is in a main clause,
therefore, the adverbial is moved to the right of the
finite verb. The second reordering occurs in a sub-
ordinate clause, and the adverbial is moved to the
left of the finite verb. Neither of these are handled
successfully by the baseline system.
In this case, the reordering leads to better word
selection. The English ?aims to? corresponds to
the Danish ?sigter mod?, which the SPTO approach
gets correct. However, the baseline system trans-
lates ?to? to its much more common translation
?at?, because ?to? is separated from ?aims? by the
adverbial ?principally?.
7 Conclusion and Future Plans
We have described a novel approach to word re-
ordering in SMT, which successfully integrates
syntactically motivated reordering in phrase-based
SMT. This is achieved by reordering the input
string, but scoring on the output string. As op-
posed to previous approaches, this neither biases
against phrase internal nor external reorderings.
We achieve an absolute improvement in translation
quality of 1.1 % BLEU. A result that is supported
by manual evaluation, which shows that the SPTO
215
1 S based on this viewpoint , every small port and every ferry port which handles
a great deal of tourist traffic should feature on the european list .
B baseret p?a dette synspunkt , ethvert lille havn og alle f?rgehavnen som
h
?
andterer en stor turist trafik skal st?a p?a den europ?iske liste .
P baseret p?a dette synspunkt , skal alle de sm
?
a havne , og alle f?rgehavnen
som behandler mange af turister trafik stod p?a den europ?iske liste .
2 S the rapporteur generally welcomes the proposals in the commission white paper on this
subject but is apprehensive of the possible implications of the reform , which aims
principally to decentralise the implementation of competition rules .
B ordf?reren generelt bifalder forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform , som sigter hovedsagelig at
decentralisere gennemf?relsen af konkurrencereglerne .
P ordf?reren bifalder generelt forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform , som is?r sigter mod at
decentralisere gennemf?relsen af konkurrencereglerne .
Table 8: Examples of reorderings. S is source, B is baseline, and P is the SPTO approach. The elements
that have been reordered in the P sentence are marked alike in all sentences. The text in bold has changed
place with the text in italics.
approach is significantly superior to previous ap-
proaches.
In the future, we plan to apply this approach
to English-Arabic translation. We expect greater
gains, due to the higher need for reordering be-
tween these less-related languages. We also want
to examine the relation between word alignment
method and the extracted rules and the relationship
between reordering and word selection. Finally, a
limitation of the current experiments is that they
only allow rule-based external reorderings. Since
the SPTO scoring is not tied to a source reordering
approach, we want to examine the effect of simply
adding it as an additional parameter to the base-
line PSMT system. This way, all external reorder-
ings are made possible, but only the rule-supported
ones get promoted.
References
Al-Onaizan, Y. and K. Papineni. 2006. Distortion models
for statistical machine translation. In Proceedings of 44th
ACL.
Buch-Kromann, M., J. Wedekind, and J. Elming. 2007. The
Copenhagen Danish-English Dependency Treebank v. 2.0.
http://www.isv.cbs.dk/?mbk/cdt2.0.
Callison-Burch, C., C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine transla-
tion. In Proceedings of ACL-2007 Workshop on Statistical
Machine Translation.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st NAACL.
Cohen, W. 1996. Learning trees and rules with set-valued
features. In Proceedings of the 14th AAAI.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause restruc-
turing for statistical machine translation. In Proceedings of
the 43rd ACL.
Crego, J. M. and J. B. Mari?no. 2007. Syntax-enhanced n-
gram-based smt. In Proceedings of the 11th MT Summit.
Habash, N. 2007. Syntactic preprocessing for statistical ma-
chine translation. In Proceedings of the 11th MT Summit.
Koehn, P. and C. Monz. 2006. Manual and automatic evalu-
ation of machine translation between european languages.
In Proceedings on the WSMT.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of NAACL.
Koehn, P. 2004. Pharaoh: a beam search decoder for phrase-
based statistical machine translation models. In Proceed-
ings of AMTA.
Koehn, P. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of MT Summit.
Li, C., M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan. 2007.
A probabilistic approach to syntax-based reordering for
statistical machine translation. In Proceedings of the 45th
ACL.
Stolcke, A. 2002. Srilm ? an extensible language modeling
toolkit. In Proceedings of the International Conference on
Spoken Language Processing.
Wang, C., M. Collins, and P. Koehn. 2007. Chinese syntactic
reordering for statistical machine translation. In Proceed-
ings of EMNLP-CoNLL.
Xia, F. and M. McCord. 2004. Improving a statistical mt sys-
tem with automatically learned rewrite patterns. In Pro-
ceedings of Coling.
Zens, R., F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In Jarke, M., J. Koehler, and G. Lake-
meyer, editors, KI - 2002: Advances in Artificial Intel-
ligence. 25. Annual German Conference on AI. Springer
Verlag.
Zhang, Y., R. Zens, and H. Ney. 2007. Improved chunk-level
reordering for statistical machine translation. In Proceed-
ings of the IWSLT.
216
Proceedings of NAACL HLT 2007, Companion Volume, pages 25?28,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combination of Statistical Word Alignments
Based on Multiple Preprocessing Schemes
Jakob Elming
Center for Comp. Modeling of Language
Copenhagen Business School
je.id@cbs.dk
Nizar Habash
Center for Comp. Learning Systems
Columbia University
habash@cs.columbia.edu
Abstract
We present an approach to using multiple
preprocessing schemes to improve statis-
tical word alignments. We show a relative
reduction of alignment error rate of about
38%.
1 Introduction
Word alignments over parallel corpora have be-
come an essential supporting technology to a va-
riety of natural language processing (NLP) appli-
cations most prominent among which is statisti-
cal machine translation (SMT).1 Although phrase-
based approaches to SMT tend to be robust to word-
alignment errors (Lopez and Resnik, 2006), improv-
ing word-alignment is still useful for other NLP re-
search that is more sensitive to alignment quality,
e.g., projection of information across parallel cor-
pora (Yarowsky et al, 2001).
In this paper, we present a novel approach to
using and combining multiple preprocessing (tok-
enization) schemes to improve word alignment. The
intuition here is similar to the combination of dif-
ferent preprocessing schemes for a morphologically
rich language as part of SMT (Sadat and Habash,
2006) except that the focus is on improving the
alignment quality. The language pair we work with
is Arabic-English.
In the following two sections, we present related
work and Arabic preprocessing schemes. Section 4
and 5 present our approach to alignment preprocess-
ing and combination, respectively. Results are pre-
sented in Section 6.
1The second author was supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under Contract No.
HR0011-06-C-0023. Any opinions, findings and conclusions or
recommendations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of DARPA. We
thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah,
Martin Jansche and Owen Rambow for helpful discussions.
2 Related Work
Recently, several successful attempts have been
made at using supervised machine learning for word
alignment (Liu et al, 2005; Taskar et al, 2005; Itty-
cheriah and Roukos, 2005; Fraser and Marcu, 2006).
In contrast to generative models, this framework is
easier to extend with new features. With the ex-
ception of Fraser and Marcu (2006), these previous
publications do not entirely discard the generative
models in that they integrate IBM model predictions
as features. We extend on this approach by includ-
ing alignment information based on multiple prepro-
cessing schemes in the alignment process.
In other related work, Tillmann et al (1997) use
several preprocessing strategies on both source and
target language to make them more alike with re-
gards to sentence length and word order. Lee (2004)
only changes the word segmentation of the morpho-
logically complex language (Arabic) to induce mor-
phological and syntactic symmetry between the par-
allel sentences. We differ from these two in that we
do not decide on a certain scheme to make source
and target sentences more symmetrical. Instead, it
is left to the alignment algorithm to decide under
which circumstances alignment information based
on a specific scheme is more likely to be correct than
information based on other schemes.
3 Arabic Preprocessing Schemes
Arabic is a morphologically complex language
with a large set of morphological features. As
such, the set of possible preprocessing schemes
is rather large (Habash and Sadat, 2006). We
focus here on a subset of schemes pertaining to
Arabic attachable clitics. There are three de-
grees of cliticization that apply to a word BASE:
([CONJ+ [PART+ [Al+ BASE +PRON]]]).
At the deepest level, the BASE can have a def-
inite article +
  (Al+ the)2 or a member of the
2Arabic is transliterated in Buckwalter?s transliteration
scheme.
25
Table 1: Arabic preprocessing scheme variants for
  
 
	
 ?and he will write it?
Preprocessing Scheme Example
AR simple      Proceedings of the ACL Student Research Workshop, pages 109?114,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
A corpus-based approach to topic in Danish dialog?
Philip Diderichsen
Lund University Cognitive Science
Lund University
Sweden
philip.diderichsen lucs.lu.se
Jakob Elming
CMOL / Dept. of Computational Linguistics
Copenhagen Business School
Denmark
je.id cbs.dk
Abstract
We report on an investigation of the prag-
matic category of topic in Danish dia-
log and its correlation to surface features
of NPs. Using a corpus of 444 utter-
ances, we trained a decision tree system
on 16 features. The system achieved near-
human performance with success rates of
84?89% and F1-scores of 0.63?0.72 in 10-
fold cross validation tests (human perfor-
mance: 89% and 0.78). The most im-
portant features turned out to be prever-
bal position, definiteness, pronominalisa-
tion, and non-subordination. We discov-
ered that NPs in epistemic matrix clauses
(e.g. ?I think . . . ?) were seldom topics and
we suspect that this holds for other inter-
personal matrix clauses as well.
1 Introduction
The pragmatic category of topic is notoriously dif-
ficult to pin down, and it has been defined in many
ways (Bu?ring, 1999; Davison, 1984; Engdahl and
Vallduv??, 1996; Gundel, 1988; Lambrecht, 1994;
Reinhart, 1982; Vallduv??, 1992). The common de-
nominator is the notion of topic as what an utter-
ance is about. We take this as our point of depar-
ture in this corpus-based investigation of the corre-
lations between linguistic surface features and prag-
matic topicality in Danish dialog.
?We thank Daniel Hardt and two anonymous reviewers for
many helpful comments on drafts of this paper.
Danish is a verb-second language. Its word order
is fixed, but only to a certain degree, in that it al-
lows any main clause constituent to occur in the pre-
verbal position. The first position thus has a privi-
leged status in Danish, often associated with topical-
ity (Harder and Poulsen, 2000; Togeby, 2003). We
were thus interested in investigating how well the
topic correlates with the preverbal position, along
with other features, if any.
Our findings could prove useful for the further in-
vestigation of local dialog coherence in Danish. In
particular, it may be worthwile in future work to
study the relation of our notion of topic to the Cb
of Grosz et als (1995) Centering Theory.
2 The corpus
The basis of our investigation was two dialogs from
a corpus of doctor-patient conversations (Hermann,
1997). Each of the selected dialogs was between a
woman in her thirties and her doctor. The doctor was
the same in the two conversations, and the overall
topic of both was the weight problems of the patient.
One of the dialogs consisted of 125 utterances (165
NPs), the other 319 (449 NPs).
3 Method
The investigation proceeded in three stages: first,
the topic expressions (see below) of all utterances
were identified1; second, all NPs were annotated for
linguistic surface features; and third, decision trees
1 Utterances with dicourse regulating purpose (e.g. yes/no-
answers), incomplete utterances, and utterances without an NP
were excluded.
109
were generated in order to reveal correlations be-
tween the topic expressions and the surface features.
3.1 Identification of topic expressions
Topics are distinguished from topic expressions fol-
lowing Lambrecht (1994). Topics are entities prag-
matically construed as being what an utterance is
about. A topic expression, on the other hand, is an
NP that formally expresses the topic in the utterance.
Topic expressions were identified through a two-step
procedure; 1) identifying topics and 2) determining
the topic expressions on the basis of the topics.
First, the topic was identified strictly based on
pragmatic aboutness using a modified version of the
?about test? (Lambrecht, 1994; Reinhart, 1982).
The about test consists of embedding the utter-
ance in question in an ?about-sentence? as in Lam-
brecht?s example shown below as (1):
(1) He said about the children that they went to school.
This is a paraphrase of the sentence the children
went to school which indicates that the referent of
the children is the topic because it is appropriate (in
the imagined discourse context) to embed this refer-
ent as an NP in the about matrix clause. (Again, the
referent of the children is the topic, while the NP the
children is the topic expression.)
We adapted the about test for dialog by adding a
request to ?say something about . . . ? or ?ask about
. . . ? before the utterance in question. Each utter-
ance was judged in context, and the best topic was
identified as illustrated below. In example (2), the
last utterance, (2-D3), was assigned the topic TIME
OF LAST WEIGHING. This happened after consider-
ing which about construction gave the most coherent
and natural sounding result combined with the utter-
ance. Example (3) shows a few about constructions
that the coder might come up with, and in this con-
text (3-iv) was chosen as the best alternative.
(2) D1 sid
sit
ned
down
og
and
lad
let
mig
me
h?re,
hear,
Annette (made-up name)
Annette
P1 jeg
I
skal
shall
bare
just
vejes
be.weighed
P2 og
and
sa?
then
skal
shall
jeg
I
have
have
svar
answer
fra
from
sidste
last
gang
time
D2 sa?
then
skal
let
vi
us
se
see
en
one
gang
time
D3 det...
it...
er...
is...
fjorten
fourteen
dage
days
siden
since
du
you
blev
were
vejet...
weighed...
(3) i. Say something about THE PATIENT (=you).
ii. Say something about THE WEIGHING OF THE PA-
TIENT.
iii. Say something about THE LAST WEIGHING OF THE
PATIENT.
iv. Say something about THE TIME OF LAST WEIGHING
OF THE PATIENT.
Creating the about constructions involved a great
deal of creativity and made them difficult to com-
pare. Sometimes the coders chose the exact same
topic, at other times they were obviously differ-
ent, but frequently it was difficult to decide. For
instance, for one utterance Coder 1 chose OTHER
CAUSES OF EDEMA SYMPTOM, while Coder 2
chose THE EDEMA?S CONNECTION TO OTHER
THINGS. Slightly different wordings like these made
it impossible to test the intersubjectivity of the topic
coding.
The second step consisted in actually identifying
the topic expression. This was done by selecting the
NP in the utterance that was the best formal repre-
sentation of the topic, using 3 criteria:
1. The topic expression is the NP in the utterance that refers
to the topic.
2. If no such NP exists, then the topic expression is the NP
whose referent the topic is a property or aspect of.
3. If no NP fulfills one of these criteria, then the utterance
has no topic expression.
In the example from before, (2-D3), it was judged
that det ?it? (emphasized) was the topic expression
of the utterance, because it shared reference with the
chosen topic from (3-iv).
If two NPs in an utterance had the same reference,
the best topic representative was chosen. In reflexive
constructions like (4), the non-reflexive NP, in this
case jeg ?I?, is considered the best representative.
(4) men
but
jeg
I
har
have
ikke
not
tabt
lost
mig
me (i.e. lost weight)
In syntactially complex utterances, the best repre-
sentative of the topic was considered the one occur-
ring in the clause most closely related to the topic. In
the following example, since the topic was THE PA-
TIENT?S HANDLING OF EATING, the topic expres-
sion had to be one of the two instances of jeg ?I?.
Since the topic arguably concerns ?handling? more
than ?eating?, the NP in the matrix clause (empha-
sized) is the topic expression.
110
(5) jeg
I
har
have
slet
really
ikke
not
t?nkt
thought
pa?
about
hvad
what
jeg
I
har
have
spist
eaten
A final example of several NPs referring to the
same topic has to do with left-dislocation. In ex-
ample (6), the preverbal object ham ?him? is imme-
diately preceded by its antecedent min far ?my fa-
ther?. Both NPs express the topic of the utterance. In
Danish, resumptive pronouns in left-dislocation con-
structions always occur in preverbal position, and in
cases where they express the topic there will thus
always be two NPs directly adjacent to each other
which both refer to the topic. In such cases, we con-
sider the resumptive pronoun the topic expression,
partly because it may be considered a more inte-
grated part of the sentence (cf. Lambrecht (1994)).
(6) min
my
far
father
ham
him
sa?
saw
jeg
I
sj?ldent
seldom
The intersubjectivity of the topic expression an-
notation was tested in two ways. First, all the topic
expression annotations of the two coders were com-
pared. This showed that topic expressions can be an-
notated reasonably reliably (? = 0.70 (see table 1)).
Second, to make sure that this intersubjectivity was
not just a product of mutual influence between the
two authors, a third, independent coder annotated a
small, random sample of the data for topic expres-
sions (50 NPs). Comparing this to the annotation of
the two main coders confirmed reasonable reliability
(? = 0.70).
3.2 Surface features
After annotating the topics and topic expressions, 16
grammatical, morphological, and prosodic features
were annotated. First the smaller corpus was anno-
tated by the two main coders in collaboration in or-
der to establish annotating policies in unclear cases.
Then the features were annotated individually by the
two coders in the larger corpus.
Grammatical roles. Each NP was categorized as
grammatical subject (sbj), object (obj), or oblique
(obl).These features can be annotated reliably (sbj: C1
(number of sbj?s identified by Coder 1) = 208, C2 (sbj?s identified by Coder 2) =
207, C1+2 (Coder 1 and 2 overlap) = 207, ?sbj = 1.00; obj: C1 = 110, C2 = 109,
C1+2 = 106, ?obj = 0.97; obl: C1 = 30, C2 = 50, C1+2 = 29, ?obl = 0.83).
Morphological and phonological features. NPs
were annotated for pronominalisation (pro), defi-
niteness (def), and main stress (str). (Note that the
main stress distinction only applies to pronouns in
Danish.) These can also be annotated reliably (pro:
C1 = 289, C2 = 289, C1+2 = 289, ?pro = 1.00; def : C1 = 319, C2 = 318, C1+2 =
318, ?def = 0.99; str: C1 = 226, C2 = 226, C1+2 = 203, ?str = 0.80).
Unmarked surface position. NPs were anno-
tated for occurrence in pre-verbal (pre) or post-
verbal (post) position relative to their subcategoriz-
ing verb. Thus, in the following example, det ?it? is
+pre, but ?post, because det is not subcategorized
by tror ?think?.
(7) ?
(I)
tror
think
[+pre,?post
[+pre,?post
det]
it]
hj?lper
helps
lidt
a little
In addition to this, NPs occurring in pre-verbal
position were annotated for whether they were rep-
etitions of a left-dislocated element (ldis). Example
(8) further exemplifies the three position-related fea-
tures.
(8) min
my
far
father
[+ldis,+pre ham]
[+ldis,+pre him]
sa?
saw
[+post jeg]
[+post I]
sj?ldent
seldom
All three features can be annotated highly reliably
(pre: C1 = 142, C2 = 142, C1+2 = 142, ?pre = 1.00; post: C1 = 88, C2 = 88,
C1+2 = 88, ?post = 1.00; ldis: C1 = 2, C2 = 2, C1+2 = 2, ?ldis = 1.00).
Marked NP-fronting. This group contains NPs
fronted in marked constructions such as the pas-
sive (pas), clefts (cle), Danish ?sentence intertwin-
ing? (dsi), and XVS-constructions (xvs).
NPs fronted as subjects of passive utterances were
annotated as +pas.
(9) [+pas jeg]
[+pas I]
skal
shall
bare
just
vejes
be.weighed
A cleft construction is defined as a complex con-
struction consisting of a copula matrix clause with
a relative clause headed by the object of the matrix
clause. The object of the matrix clause is also an
argument or adjunct of the relative clause predicate.
The clefted element det ?that?, which we annotate as
+cle, leaves an ?empty slot?, e, in the relative clause,
as shown in example (10):
(10) det
it
er
is
jo
after all
ikke
not
[+cle deti]
[+cle thati]
du
you
skal
shall
tabe dig
lose weight
af
from
ei
ei
som
as
sa?dan
such
Danish sentence intertwining can be defined as
a special case of extraction where a non-WH con-
stituent of a subordinate clause occurs in the first
111
position of the matrix clause. As in cleft construc-
tions, an ?empty slot? is left behind in the subordi-
nate clause. NPs in the fronted position were anno-
tated as +dsi:
(11) [+dsi deti]
[+dsi thati]
tror
think
jeg
I
ikke
not
det
it
g?r
does
ei
ei
The XVS construction is defined as a simple
declarative sentence with anything but the subject in
the preverbal position. Since only one constituent is
allowed preverbally2, the subject occurs after the fi-
nite verb. In example (12), the finite verb is an auxil-
iary, and the canonical position of the object after the
main verb is indicated with the ?empty slot? marker
e. The preverbal element in XVS-constructions is
annotated as +xvs.
(12) [+xvs deti]
[+xvs thati]
har
have
jeg
I
altsa?
truly
haft
had
ei
ei
f?r
before
All four features can be annotated highly reliably
(pas: C1 = 1, C2 = 1, C1+2 = 1, ?pas = 1.00; cle: C1 = 4, C2 = 4, C1+2 = 4,
?cle = 1.00; dsi C1 = 3, C2 = 3, C1+2 = 3, ?dsi = 1.00; xvs: C1 = 18, C2 = 18,
C1+2 = 18, ?xvs = 1.00).
Sentence type and subordination. Each NP was
annotated with respect to whether or not it appeared
in an interrogative sentence (int) or a subordinate
clause (sub), and finally, all NPs were coded as to
whether they occurred in an epistemic matrix clause
or in a clause subordinated to an epistemic matrix
clause (epi). An epistemic matrix clause is defined
as a matrix clause whose function it is to evaluate
the truth of its subordinate clause (such as ?I think
. . . ?). The following example illustrates how we an-
notated both NPs in the epistemic matrix clause and
NPs in its immediate subordinate clause as +epi, but
not NPs in further subordinated clauses. The +epi
feature requires a +/?sub feature in order to deter-
mine whether the NP in question is in the epistemic
matrix clause or subordinated under it. Subordina-
tion is shown here using parentheses.
(13) [+epi
[+epi
jeg]
I]
tror
think
mere
rather
(
(
[+epi,+sub
[+epi,+sub
det]
it]
er
is
fordi
because
(at
(that
[+sub
[+sub
man]
you]
spiser
eat
pa?
at
[+sub
[+sub
dumme
stupid
tidspunkter]
times]
ik?))
right))
All features in this group can be annotated reli-
2 Only one constituent is allowed in the intrasentential pre-
verbal position. Left-dislocated elements are not considered
part of the sentence proper, and thus do not count as preverbal
elements, cf. Lambrecht (1994).
ably (int: C1 = 55, C2 = 55, C1+2 = 55, ?int = 1.00; sub: C1 = 117, C2 =
111, C1+2 = 107, ?sub = 0.93; epi: C1 = 38, C2 = 45, C1+2 = 37, ?epi = 0.92).
3.3 Decision trees
In the third stage of our investigation, a decision tree
(DT) generator was used to extract correlations be-
tween topic expressions and surface features. Three
different data sets were used to train and test the
DTs, all based on the larger dialog.
Two of these data sets were derived from the com-
plete set of NPs annotated by each main coder in-
dividually. These two data sets will be referred to
below as the ?Coder 1? and ?Coder 2? data sets.
The third data set was obtained by including only
NPs annotated identically by both main coders in
relevant features3. This data set represents a higher
degree of intersubjectivity, especially in the topic ex-
pression category, but at the cost of a smaller number
of NPs. 63 out of a total of 449 NPs had to be ex-
cluded because of inter-coder disagreement, 50 due
to disagreement on the topic expression category.
This data set will be referred to below as the ?In-
tersection? data set.
A DT was generated for each of these three data
sets, and each DT was tested using 10-fold cross val-
idation, yielding the success rates reported below.
4 Results
Our results were on the one hand a subset of the
features examined that correlated with topic expres-
sions, and on the other the discovery of the impor-
tance of different types of subordination. These re-
sults are presented in turn.
4.1 Topic-indicating features
The optimal classification of topic expressions in-
cluded a subset of important features which ap-
peared in every DT, i.e. +pro, +def, +pre, and ?sub.
Several other features occurred in some of the DTs,
i.e. dsi, int, and epi. The performance of all the DTs
is summarized in table 2 below.
3
?Relevant features? were determined in the following way:
A DT was generated using a data set consisting only of NPs
annotated identically by the two coders in all the features, i.e.
the 16 surface features as well as the topic expression feature.
The features constituting this DT, i.e. pro, def, sub, and pre, as
well as the topic expression category, were relevant features for
the third data set, which consisted only of NPs coded identically
by the two coders in these 5 features.
112
The DT for the Coder 1 data set contains the fea-
tures def, pro, dsi, sub, and pre. According to this
classification, a definite pronoun in the fronted po-
sition of a Danish sentence intertwining construc-
tion is a topic expression, and other than that, def-
inite pronouns in the preverbal position of non-
subordinate clauses are topic expressions. The 10-
fold cross validation test yields an 84% success rate.
F1-score: 0.63.
The Coder 2 DT contains the features pro, def,
sub, pre, int, and epi. Here, if a definite pronoun
occurs in a subordinate clause it is not a topic ex-
pression, and otherwise it is a topic expression if it
occurs in the preverbal position. If it does not oc-
cur in preverbal position, but in a question, it is also
a topic expression unless it occurs in an epistemic
matrix clause. Success rate: 85%. F1-score: 0.67.
Finally, the Intersection DT contains the features
pro, def, sub, and pre. According to this DT,
only definite pronouns in preverbal position in non-
subordinate clauses are topic expressions. The DT
has a high success rate of 89% in the cross vali-
dation test ? which is not surprising, given that a
large number of possibly difficult cases have been
removed (mainly the 50 NPs where the two coders
disagreed on the annotation of topic expressions).
F1-score: 0.72.
Since there is no gold standard for annotating
topic expressions, the best evaluation of the human
performance is in terms of the amount of agreement
between the two coders. Success rate and F1 analogs
for human performance were therefore computed as
follows, using the figures displayed in table 1.
Coder 2 Total
Topic Non-topic
Coder 1 Topic 88 27 115
Non-topic 23 311 334
Total 111 338 449
Table 1: The topic annotation of Coder 1 and Coder 2.
Success rate analog: The agreement percentage
between the human coders when annotating topic
expressions (449 NPS?(23+27) NPS449 NPS ?100 = 89%).
F1 analog: The performance of Coder 1 eval-
uated against the performance of Coder 2 (?Preci-
sion?: 8888+27 = 0.77; ?Recall?:
88
88+23 = 0.79; ?F1?:
2? 0.77?0.790.77+0.79 = 0.78).
Data set Coder 1 Coder 2 Intersect. Human
Total NPs 449 449 386 449
Success rate 84% 85% 89% 89%
Precision 0.77 0.74 0.79 0.79
Recall 0.53 0.61 0.67 0.77
F1-score 0.63 0.67 0.72 0.78
Table 2: Success rates, Precision, Recall, and F1-scores for
the three different data sets. For comparison, we added success
rate and F1 analogs for human performance.
4.2 Interpersonal subordination
We found that syntactic subordination does not have
an invariant function as far as information structure
is concerned. The emphasized NPs in the following
examples are definite pronouns in preverbal position
in syntactically non-subordinate clauses. But none
of them are perceived as topic expressions.
(14) sa?
so
det
it
kan
may
godt
well
v?re
be
at
that
hvis
if
man
you
har...
have...
tabt
lost
noget
some
mere
more
i l?bet af
during
ugen
the.week
ik?
right
(15) jeg
I
tror
think
mere
rather
det
it
er
is
fordi
because
at
that
man
you
spiser
eat
pa?
at
dumme
stupid
tidspunkter
times
ik?
right
The reason seems to be that these NPs occur in
epistemic matrix clauses (+epi).
The following utterances have not been annotated
for the +epi feature, since the matrix clauses do not
seem to state the speaker?s attitude towards the truth
of the subordinate clause. However, the emphasized
NPs seem to stand in a very similar relation to the
message being conveyed, and none of them were
perceived as topic expressions.
(16) men
but
altsa?
you know
jeg
I
har
have
bare
just
bem?rket
noticed
at
that
at
that
det
it
er
has
blevet
become
v?rre
worse
ik?
right
(17) og
and
det
that
kan
can
man
you
da
though
sige
say
pa?
in
tre
three
uger
weeks
det
that
er
is
da
surely
ikke
not
vildt
wildly
meget
much
This suggests that a more general type of matrix
clause than the epistemic matrix clause, namely the
interpersonal matrix clause (Jensen, 2003) would be
relevant in this context. This category would cover
all of the above cases. It is defined as a matrix
clause that expresses some attitude towards the mes-
113
sage conveyed in its subordinate clause. This more
general category presumably signals non-topicality
rather than topicality just like the special case of
epistemic subordination.
5 Summary and future work
We have shown that it is possible to generate al-
gorithms for Danish dialog that are able to predict
the topic expressions of utterances with near-human
performance (success rates of 84?89%, F1 scores of
0.63?0.72).
Furthermore, our investigation has shown that
the most characteristic features of topic expres-
sions are preverbal position (+pre), definiteness
(+def), pronominal realisation (+pro), and non-
subordination (?sub). This supports the traditional
view of topic as the constituent in preverbal position.
Most interesting is subordination in connection
with certain matrix clauses. We discovered that NPs
in epistemic matrix clauses were seldom topics. In
complex constructions like these the topic expres-
sion occurs in the subordinate clause, not the ma-
trix clause as would be expected. We suspect that
this can be extended to the more general category of
inter-personal matrix clauses.
Future work on dialog coherence in Danish, par-
ticularly pronoun resolution, may benefit from our
results. The centering model, originally formulated
by Grosz et al (1995), models discourse coherence
in terms of a ?local center of attention?, viz. the
backward-looking center, Cb. Insofar as the Cb cor-
responds to a notion like topic, the corpus-based in-
vestigation reported here might serve as the empiri-
cal basis for an adaptation for Danish dialog of the
centering model. Attempts have already been made
to adapt centering to dialog (Byron and Stent, 1998),
and, importantly, work has also been done on adapt-
ing the centering model to other, freer word order
languages such as German (Strube and Hahn, 1999).
References
Daniel Bu?ring. 1999. Topic. In Peter Bosch and Rob
van der Sandt, editors, Focus ? Linguistic, Cogni-
tive, and Computational Perspectives, pages 142?165.
Cambridge University Press.
Donna K. Byron and Amanda J. Stent. 1998. A prelim-
inary model of centering in dialog. Technical report,
The University of Rochester.
Alice Davison. 1984. Syntactic markedness and the def-
inition of sentence topic. Language, 60(4).
Elisabeth Engdahl and Enric Vallduv??. 1996. Informa-
tion packaging in HPSG. Edinburgh working papers
in cognitive science: Studies in HPSG, 12:1?31.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: a framework for modeling the lo-
cal coherence of discourse. Computational linguistics,
21(2):203?225.
Jeanette K. Gundel. 1988. Universals of topic-comment
structure. In Michael Hammond, Edith Moravcsik,
and Jessica Wirth, editors, Studies in syntactic typol-
ogy, volume 17 of Studies in syntactic typology, pages
209?239. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Peter Harder and Signe Poulsen. 2000. Editing for
speaking: first position, foregrounding and object
fronting in Danish and English. In Elisabeth Engberg-
Pedersen and Peter Harder, editors, Ikonicitet og struk-
tur, pages 1?22. Netv?rk for funktionel lingvistik,
Copenhagen.
Jesper Hermann. 1997. Dialogiske forsta?elser og deres
grundlag. In Peter Widell and Mette Kun?e, editors,
6. m?de om udforskningen af dansk sprog, pages 117?
129. MUDS, A?rhus.
K. Anne Jensen. 2003. Clause Linkage in Spoken Dan-
ish. Ph.D. thesis from the University of Copenhagen,
Copenhagen.
Knud Lambrecht. 1994. Information structure and sen-
tence form: topic, focus and the mental representa-
tions of discourse referents. Cambridge University
Press, Cambridge.
Tanya Reinhart. 1982. Pragmatics and linguistics. an
analysis of sentence topics. Distributed by the Indiana
University Linguistics Club., pages 1?38.
Michael Strube and Udo Hahn. 1999. Functional center-
ing ? grounding referential coherence in information
structure. Computational linguistics, 25(3):309?344.
Ole Togeby. 2003. Fungerer denne s?tning? ? Funk-
tionel dansk sprogl?re. Gads forlag, Copenhagen.
Enric Vallduv??. 1992. The informational component.
Ph.D. thesis from the University of Pennsylvania,
Philadelphia.
114
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 46?54,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Syntactic Reordering Integrated with Phrase-based SMT
Jakob Elming
Computational Linguistics
Copenhagen Business School
jel.isv@cbs.dk
Abstract
We present a novel approach to word re-
ordering which successfully integrates syn-
tactic structural knowledge with phrase-based
SMT. This is done by constructing a lattice
of alternatives based on automatically learned
probabilistic syntactic rules. In decoding, the
alternatives are scored based on the output
word order, not the order of the input. Un-
like previous approaches, this makes it possi-
ble to successfully integrate syntactic reorder-
ing with phrase-based SMT. On an English-
Danish task, we achieve an absolute improve-
ment in translation quality of 1.1 % BLEU.
Manual evaluation supports the claim that the
present approach is significantly superior to
previous approaches.
1 Introduction
The emergence of phrase-based statistical machine
translation (PSMT) (Koehn et al, 2003) has been
one of the major developments in statistical ap-
proaches to translation. Allowing translation of
word sequences (phrases) instead of single words
provides SMT with a robustness in word selection
and local word reordering.
PSMT has two means of reordering the words. Ei-
ther a phrase pair has been learned where the target
word order differs from the source (phrase internal
reordering), or distance penalized orderings of target
phrases are attempted in decoding (phrase external
reordering). The first solution is strong, the second
is weak.
The second solution is necessary for reorderings
within a previously unseen sequence or over dis-
tances greater than the maximal phrase length. In
this case, the system in essence relies on the tar-
get side language model to get the correct word or-
der. The choice is made without knowing what the
source is. Basically, it is a bias against phrase exter-
nal reordering.
It seems clear that reordering often depends on
higher level linguistic information, which is absent
from PSMT. In recent work, there has been some
progress towards integrating syntactic information
with the statistical approach to reordering. In works
such as (Xia and McCord, 2004; Collins et al, 2005;
Wang et al, 2007; Habash, 2007), reordering de-
cisions are done ?deterministically?, thus placing
these decisions outside the actual PSMT system by
learning to translate from a reordered source lan-
guage. (Crego andMarin?o, 2007; Zhang et al, 2007;
Li et al, 2007) are more in the spirit of PSMT, in
that multiple reorderings are presented to the PSMT
system as (possibly weighted) options.
Still, there remains a basic conflict between the
syntactic reordering rules and the PSMT system:
one that is most likely due to the discrepancy be-
tween the translation units (phrases) and units of the
linguistic rules, as (Zhang et al, 2007) point out.
In this paper, we proceed in the spirit of the non-
deterministic approaches by providing the decoder
with multiple source reorderings. But instead of
scoring the input word order, we score the order of
the output. By doing this, we avoid the integration
problems of previous approaches.
It should be noted that even though the experi-
ments are conducted within a source reordering ap-
proach, this scoring is also compatible with other ap-
46
proach. We will, however, not look further into this
possiblity in the present paper.
In addition, we automatically learn reordering
rules based on several levels of linguistic informa-
tion from word form to subordination and syntac-
tic structure to produce reordering rules that are not
restricted to operations on syntactic tree structure
nodes.
In the next section, we discuss and contrast re-
lated work. Section 3 describes aspects of English
and Danish structure that are relevant to reordering.
Section 4 describes the automatic induction of re-
ordering rules and its integration in PSMT. In sec-
tion 5, we describe the SMT system used in the
experiments. Section 6 evaluates and discusses the
present approach.
2 Related Work
While several recent authors have achieved positive
results, it has been difficult to integrate syntactic in-
formation while retaining the strengths of the statis-
tical approach.
Several approaches do deterministic reordering.
These do not integrate the reordering in the PSMT
system; instead they place it outside the system by
first reordering the source language, and then having
a PSMT system translate from reordered source lan-
guage to target language. (Collins et al, 2005; Wang
et al, 2007) do this using manually created rules,
and (Xia and McCord, 2004) and (Habash, 2007)
use automatically extracted rules. All use rules ex-
tracted from syntactic parses.
As mentioned by (Al-Onaizan and Papineni,
2006), it can be problematic that these determinis-
tic choices are beyond the scope of optimization and
cannot be undone by the decoder. That is, there is no
way to make up for bad information in later transla-
tion steps.
Another approach is non-deterministic. This pro-
vides the decoder with both the original and the re-
ordered source sentence. (Crego and Marin?o, 2007)
operate within Ngram-based SMT. They make use
of syntactic structure to reorder the input into a word
lattice. Since the paths are not weighted, the lattice
merely narrows down the size of the search space.
The decoder is not given reason to trust one path (re-
ordering) over another.
(Zhang et al, 2007) assign weights to the paths
of their input word lattice. Instead of hierarchical
linguistic structure, they use reordering rules based
on POS and syntactic chunks, and train the system
with both original and reordered source word order
on a restricted data set (<500K words). Their sys-
tem does not out-perform a standard PSMT system.
As they themselves point out, a reason for this might
be that their reordering approach is not fully inte-
grated with PSMT. This is one of the main problems
addressed in the present work.
(Li et al, 2007) use weighted n-best lists as input
for the decoder. They use rules based on a syntac-
tic parse, allowing children of a tree node to swap
place. This is excessively restrictive. For example,
a common reordering in English-Danish translation
has the subject change place with the finite verb.
Since the verb is often embedded in a VP contain-
ing additional words that should not be moved, such
rules cannot be captured by local reordering on tree
nodes.
In many cases, the exact same word order that
is obtained through a source sentence reordering, is
also accessible through a phrase internal reordering.
A negative consequence of source order (SO) scor-
ing as done by (Zhang et al, 2007) and (Li et al,
2007) is that they bias against the valuable phrase
internal reorderings by only promoting the source
sentence reordering. As described in section 4.3, we
solve this problem by reordering the input string, but
scoring the output string, thus allowing the strengths
of PSMT to co-exist with rule-based reordering.
3 Language comparison
The two languages examined in this investigation,
English and Danish, are very similar from a struc-
tural point of view. A word alignment will most of-
ten display an almost one-to-one correlation. In the
hand-aligned data, only 39% of the sentences con-
tain reorderings (following the notion of reordering
as defined in 4.1). On average, a sentence contains
0.66 reorderings.
One of the main differences between English and
Danish word order is that Danish is a verb-second
language: the finite verb of a declarative main clause
must always be the second constituent. Since this
is not the case for English, a reordering rule should
47
move the subject of an English sentence to the right
of the finite verb, if the first position is filled by
something other than the subject. This is exempli-
fied by (1) (examples are annotated with English
gloss and translation), where ?they? should move to
the right of ?come? to get the Danish word order as
seen in the gloss.
(1)
[
nu
now
kommer
come
de
they ]
?here they come?
Another difference is that Danish sentence adver-
bials in a subordinate clause move to the left of the
finite verb. This is illustrated in example (2). This
example also shows the difficulty for a PSMT sys-
tem. Since the trigram ?han kan ikke? is frequent in
Danish main clauses, and ?han ikke kan? is frequent
in subordinate clauses, we need information on sub-
ordination to get the correct word order. This infor-
mation can be obtained from the conjunction ?that?.
A trigram PSMT system would not be able to handle
the reordering in (2), since ?that? is beyond the scope
of ?not?.
(2)
[
han
he
siger
says
at
that
han
he
ikke
not
kan
can
se
see ]
?he says that he can not see?
In the main clause, on the other hand, Danish prefers
the sentence adverbial to appear to the right of the
finite verb. Therefore, if the English adverbial ap-
pears to the left of the finite verb in a main clause, it
should move right as exemplified by example (3).
(3)
[
hun
she
sa?
saw
aldrig
never
skibet
the ship ]
?she never saw the ship?
Other differences are of a more conventionalized na-
ture. E.g. address numbers are written after the
street in Danish (example (4)).
(4)
[
han
he
bor
lives
nygade
nygade
14
14 ]
?he lives at 14 nygade?
t7 ? ? ? ? ? ? 
t6 ? ?  ? ? ? ?
t5 ?  ? ? ? ? ?
t4 ? ? ? ?  ? ?
t3 ? ? ? ? ?  ?
t2 ? ? ?  ? ? ?
t1  ? ? ? ? ? ?
s1 s2 s3 s4 s5 s6 s7
Table 1: Reordering example
4 Reordering rules
4.1 Definition of reordering
In this experiment, reordering is defined as two
word sequences exchanging positions. These two
sequences are restricted by the following conditions:
? Parallel consecutive: They have to make up
consecutive sequences of words, and each has
to align to a consecutive sequence of words.
? Maximal: They have to be the longest possible
consecutive sequences changing place.
? Adjacent: They have to appear next to each
other on both source and target side.
The sequences are not restricted in length, mak-
ing both short and long distance reordering possible.
Furthermore, they need not be phrases in the sense
that they appear as an entry in the phrase table.
Table 1 illustrates reordering in a word alignment
matrix. The table contains reorderings between the
light grey sequences (s32 and s
6
4)
1 and the dark grey
sequences (s55 and s
6
6). On the other hand, the se-
quences s33 and s
5
4 are e.g. not considered reordered,
since neither are maximal, and s54 is not consecutive
on the target side.
4.2 Rule induction
In section 3, we pointed out that subordination is
very important for word order differences between
English and Danish. In addition, the sentence posi-
tion of constituents plays a role. All this informa-
tion is present in a syntactic sentence parse. A sub-
ordinate clause is defined as inside an SBAR con-
1Notation: syx means the consecutive source sequence cov-
ering words x to y.
48
Level LC LS RS RC
WORD <s> today , || today , || , he was driving home || home . || home . < /s>
POS <S> NN , || NN , || , PRP AUX VBG NN || NN . || NN . < /S>
PS <S> NP , || NP , || , NP AUX VBG ADVP || ADVP . || ADVP . < /S>
SUBORD main main main main
Table 2: Example of experience for learning. Possible contexts separated by ||.
stituent; otherwise it is a main clause. The con-
stituent position can be extracted from the sentence
start tag and the following syntactic phrases. POS
and word form are also included to allow for more
specific/lexicalized rules.
Besides including this information for the candi-
date reordering sequences (left sequence (LS) and
right sequence (RS)), we also include it for the set of
possible left (LC) and right (RC) contexts of these.
The span of the contexts varies from a single word to
all the way to the sentence border. Table 2 contains
an example of the information available to the learn-
ing algorithm. In the example, LS and RS should
change place, since the first position is occupied by
something other than the subject in a main clause.
In order to minimize the training data, word
and POS sequences are limited to 4 words, and
phrase structure (PS) sequences are limited to 3 con-
stituents. In addition, an entry is only used if at least
one of these three levels is not too long for both LS
and RS, and too long contexts are not included in
the set. This does not constrain the possible length
of a reordering, since a PS sequence of length 1 can
cover an entire sentence.
In order to extract rules from the annotated data,
we use a rule-based classifier, Ripper (Cohen, 1996).
The motivation for using Ripper is that it allows fea-
tures to be sets of strings, which fits well with our
representation of the context, and it produces easily
readable rules that allow better understanding of the
decisions being made. In section 6.2, extracted rules
are exemplified and analyzed.
The probabilities of the rules are estimated using
Maximum Likelihood Estimation based on the in-
formation supplied by Ripper on the performance of
the individual rules on the training data. These log-
arithmic probabilities are easily integratable in the
log-linear PSMT model as an additional parameter
by simple addition.
The rules are extracted from the hand-aligned,
Copenhagen Danish-English Dependency Treebank
(Buch-Kromann et al, 2007). 5478 sentences from
the news paper domain containing 111,805 English
words and 100,185 Danish words. The English side
is parsed using a state-of-the-art statistical English
parser (Charniak, 2000).
4.3 Integrating rule-based reordering in PSMT
The integration of the rule-based reordering in our
PSMT system is carried out in two separate stages:
1. Reorder the source sentence to assimilate the
word order of the target language.
2. Score the target word order according to the rel-
evant rules.
Stage 1) is done in a non-deterministic fashion by
generating a word lattice as input in the spirit of e.g.
(Zens et al, 2002; Crego and Marin?o, 2007; Zhang
et al, 2007). This way, the system has both the orig-
inal word order, and the reorderings predicted by the
rule set. The different paths of the word lattice are
merely given as equal suggestions to the decoder.
They are in no way individually weighted.
Separating stage 2) from stage 1) is motivated by
the fact that reordering can have two distinct ori-
gins. They can occur because of stage 1), i.e. the
lattice reordering of the original English word or-
der (phrase external reordering), and they can oc-
cur inside a single phrase (phrase internal reorder-
ing). We are, however, interested in doing phrase-
independent, word reordering. We want to promote
rule-predicted reorderings, regardless of whether
they owe their existence to a syntactic rule or a
phrase table entry.
This is accomplished by letting the actual scoring
of the reordering focus on the target string. The de-
49
Source sentence: today1 ,2 he3 was4 late5
Rule: 3 4 ? 4 3
Hypothesis Target string SPTO
H1 idag han var 1 3 4
H2 idag var han 1 4 3
Table 3: Example of SPTO scoring during decoding at
source word 4.
coder is informed of where a rule has predicted a re-
ordering, howmuch it costs to do the reordering, and
how much it costs to avoid it. This is then checked
for each hypothezised target string by keeping track
of what source position target order (SPTO) it cor-
responds to.
The SPTO is a representation of which source
position the word in each target position originates
from. Putting it differently, the hypotheses con-
tain two parallel strings; a target word string and its
SPTO string. In order to access this information,
each phrase table entry is annotated with its internal
word alignment, which is available as an interme-
diate product from phrase table creation. If a phrase
pair has multiple word alignments, the most frequent
is chosen.
Table 3 exemplifies the SPTO scoring. The source
sentence is ?today he was late?, and a rule has pre-
dicted that word 3 and 4 should change place. When
the decoder has covered the first four input words,
two of the hypothesis target strings might be H1
and H2. At this point, it becomes apparent that H2
contains the desired SPTO (namely ?4 3?), and it
get assigned the reordering cost. H1 does not con-
tain the rule-suggested SPTO (in stead, the words
are in the order ?3 4?), and it gets the violation
cost. Both these scorings are performed in a phrase-
independent manner. The decoder assigns the re-
ordering cost to H2 without knowing whether the
reordering is internal (due to a phrase table entry)
or external (due to a syntactic rule).
Phrase internal reorderings at other points of the
sentence, i.e. points that are not covered by a rule,
are not judged by the reordering model. Our rule
extraction does not learn every possible reordering
between the two languages, but only the most gen-
eral ones. If no rule has an opinion at a certain point
in a sentence, the decoder is free to chose the phrase
Figure 1: Example word lattice.
translation it prefers without reordering cost.
Separating the scoring from the source language
reordering also has the advantage that the SPTO
scoring in essence is compatible with other ap-
proaches such as a traditional PSMT system. We
will, however, not examine this possibility further in
the present paper.
5 The PSMT system
The baseline is the PSMT system used for the 2006
NAACL SMT workshop (Koehn and Monz, 2006)
with phrase length 3 and a trigram language model
(Stolcke, 2002). The system was trained on the En-
glish and Danish part of the Europarl corpus version
3 (Koehn, 2005). Fourth quarter of 2000 was re-
moved in order to use the common test set of 11369
sentences (330,082 English words and 309,942 Dan-
ish words with one reference) for testing. In addi-
tion, fourth quarter of 2001 was removed for devel-
opment purposes. Of these, 10194 were used for
various analysis purposes, thereby keeping the test
data perfectly unseen. 500 sentences were taken
from the development set for tuning the decoder pa-
rameters. This was done using the Downhill Sim-
plex algorithm. In total, 1,137,088 sentences con-
taining 31,376,034 English words and 29,571,518
Danish words were left for training the phrase table
and language model.
The decoder used for the baseline system is
Pharaoh (Koehn, 2004) with its distance-penalizing
reordering model. For the experiments, we use
our own decoder which ? except for the reorder-
ing model ? uses the same knowledge sources
as Pharaoh, i.e. bidirectional phrase translation
model and lexical weighting model, phrase and word
penalty, and target language model. Its behavior is
comparable to Pharaoh when doing monotone de-
coding.
The search algorithm of our decoder is similar to
the RG graph decoder of (Zens et al, 2002). It ex-
50
System Dev Test Swap Subset
Baseline 0.262 0.252 0.234
no scoring 0.267 0.256 0.241
SO scoring 0.268 0.258 0.244
SPTO scoring 0.268 0.258 0.245
Table 4: BLEU scores for different scoring methods.
pects a word lattice as input. Figure 1 shows the
word lattice for the example in table 3.
Since the input format defines all possible word
orders, a simple monotone search is sufficient. Us-
ing a language model of order n, for each hy-
pothezised target string ending in the same n-1-
gram, we only have to extend the highest scoring
hypothesis. None of the others can possibly outper-
form this one later on. This is because the maximal
context evaluating a phrase extending this hypothe-
sis, is the history (n-1-gram) of the first word of that
phrase. The decoder is not able to look any further
back at the preceeding string.
6 Evaluation
6.1 Results and discussion
The SPTO reordering approach is evaluated on the
11369 sentences of the common test set. Results are
listed in table 4 along with results on the develop-
ment set. We also report on the swap subset. These
are the 3853 sentences where the approach actually
motivated reorderings in the test set, internal or ex-
ternal. The remaining 7516 sentences were not in-
fluenced by the SPTO reordering approach.
We report on 1) the baseline PSMT system, 2) a
system provided with a rule reordered word lattice
but no scoring, 3) the same system but with an SO
scoring in the spirit of (Zhang et al, 2007; Li et al,
2007), and finally 4) the same system but with the
SPTO scoring.
The SPTO approach gets an increase over the
baseline PSMT system of 0.6 % BLEU. The swap
subset, however, shows that the extracted rules are
somewhat restricted, only resulting in swap in 13 of
the sentences. The relevant set, i.e. the set where the
present approach actually differs from the baseline,
is therefore the swap subset. This way, we concen-
trate on the actual focus of the paper, namely the
syntactically motivated SPTO reordering. Here we
System BLEU Avr. Human rating
Baseline 0.234 3.00 (2.56)
no scoring 0.240 3.00 (2.74)
SO scoring 0.239 3.00 (2.62)
SPTO scoring 0.244 2.00 (2.08)
Table 5: Evaluation on the set where SO and SPTO pro-
duce different translations. Average human ratings are
medians with means in parenthesis, lower scores are bet-
ter, 1 is the best score.
achieve an increase in performance of 1.1 % BLEU.
Comparing to the other scoring approaches does
not show much improvement. A possible explana-
tion is that the rules do not apply very often, in com-
bination with the fact that the SO and SPTO scoring
mechanisms most often behave alike. The difference
in SO and SPTO scoring only leads to a difference in
translation in 10% of the sentences where reordering
is done. This set is interesting, since it provides a fo-
cus on the difference between the SO and the SPTO
approaches. In table 5, we evaluate on this set.
The BLEU scores on the entire set indicate that
SPTO is a superior scoring method. To back this ob-
servation, the 100 first sentences are manually eval-
uated by two native speakers of Danish. (Callison-
Burch et al, 2007) show that ranking sentences
gives higher inter-annotator agreement than scor-
ing adequacy and fluency. We therefore employ
this evaluation method, asking the evaluators to rank
sentences from the four systems given the input sen-
tence. Ties are allowed. The annotators had reason-
able inter-annotator agreement (? = 0.523, P (A) =
0.69, P (E) = 0.35). Table 5 shows the aver-
age ratings of the systems. This clearly shows the
SPTO scoring to be significantly superior to the
other methods (p < 0.05).
Most of the cases (55) where SPTO outperforms
SO are cases where SPTO knows that a phrase pair
contains the desired reordering, but SO does not.
Therefore, SO has to use an external reordering
which brings poorer translation than the internal re-
ordering, because the words are translated individ-
ually rather than by a single phrase (37 cases), or it
has to reject the desired reordering (18 cases), which
also hurts translation, since it does not get the correct
word order.
51
Decoder choice SO SPTO
Phrase internal reordering 401 1538
Phrase external reordering 3846 2849
Reject reordering 1468 1328
Table 6: The choices made based on the SO and SPTO
scoring for the 5715 reorderings proposed by the rules
for the test data.
Table 6 shows the effect of SO and SPTO scoring
in decoding. Most noticeable is that the SO scoring
is strongly biased against phrase internal reorder-
ings; SPTO uses nearly four times as many phrase
internal reorderings as SO. In addition, SPTO is a
little less likely to reject a rule proposed reordering.
6.2 Rule analysis
The rule induction resulted in a rule set containing
27 rules. Of these, 22 concerned different ways of
identifying contexts where a reordering should oc-
cur due to the verb second nature of Danish. 4 rules
had to do with adverbials in main and in subordinate
clauses, and the remaining rule expressed that cur-
rency is written after the amount in Danish, while it
is the other way around in English. Since the train-
ing data however only includes Danish Crowns, the
rule was lexicalized to ?DKK?.
Table 7 shows a few of the most frequently used
rules. The first three rules deal with the verb second
phenomenon. The only difference among these is
the left context. Either it is a prepositional phrase, a
subordinate clause or an adverbial. These are three
ways that the algorithm has learned to identify the
verb second phenomenon conditions. Rule 3 is inter-
esting in that it is lexicalized. In the learning data,
the Danish correspondent to ?however? is most of-
ten not topicalized, and the subject is therefore not
forced from the initial position. As a consequence,
the rule states that it should only apply, if ?however?
is not included in the left context of the reordering.
Rule 4 handles the placement of adverbials in a
subordinate clause. Since the right context is subor-
dinate and a verb phrase, the current sequences must
also be subordinate. In contrast, the fifth rule deals
with adverbials in a main clause, since the left con-
text noun phrase is in a main clause.
A problem with the hand-aligned data used for
rule-induction is that it is out of domain compared
No LC LS RS RC
1 PS: <S> PP , PS: NP POS: FV
2 PS: SBAR , PS: NP POS: FV
3 PS: ADVP , PS: NP POS: FV
! WORD:
however ,
4 PS: FV POS: RB PS: VP
SUB: sub
5 PS: <S> NP PS: ADVP POS: FV
SUB: main
Table 7: Example rules and their application statistics.
to the Europarl data used to train the SMT system.
The hand-aligned data is news paper texts, and Eu-
roparl is transcribed spoken language from the Euro-
pean Parliament. Due to its spoken nature, Europarl
contains frequent sentence-initial forms of address.
That is, left adjacent elements that are not integrated
parts of the sentence as illustrated by example (5).
This is not straightforward, because on the surface
these look a lot like topicalized constructions, as in
example (6). In topicalized constructions, it is an
integrated part of the sentence that is moved to the
front in order to affect the flow of discourse infor-
mation. This difference is crucial for the reordering
rules, since ?i? and ?have? should reorder in (6), but
not in (5), in order to get Danish word order.
(5) mr president , i have three points .
(6) as president , i have three points .
When translating the development set, it became
clear that many constructions like (5) were reordered
by a rule. Since these constructions were not present
in the hand-aligned data, the learning algorithm did
not have the data to learn this difference.
We therefore included a manual, lexicalized rule
stating that if the left context contained one of a set
of titles (mr, mrs, ms, madam, gentlemen), the re-
ordering should not take place. Since the learning
includes word form information, this is a rule that
the learning algorithm is able to learn. To a great
extent, the rule eliminates the problem.
The above examples also illustrate that local re-
ordering (in this case as local as two neighboring
words) can be a problem for PSMT, since even
though the reordering is local, the information about
whether to reorder or not is not necessarily local.
52
1 S based on this viewpoint , every small port and every ferry port which handles
a great deal of tourist traffic should feature on the european list .
B baseret pa? dette synspunkt , ethvert lille havn og alle f?rgehavnen som
ha?ndterer en stor turist trafik skal sta? pa? den europ?iske liste .
P baseret pa? dette synspunkt , skal alle de sma? havne , og alle f?rgehavnen
som behandler mange af turister trafik stod pa? den europ?iske liste .
2 S the rapporteur generally welcomes the proposals in the commission white paper on this
subject but is apprehensive of the possible implications of the reform , which aims
principally to decentralise the implementation of competition rules .
B ordf?reren generelt bifalder forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform , som sigter hovedsagelig at
decentralisere gennemf?relsen af konkurrencereglerne .
P ordf?reren bifalder generelt forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform , som is?r sigter mod at
decentralisere gennemf?relsen af konkurrencereglerne .
Table 8: Examples of reorderings. S is source, B is baseline, and P is the SPTO approach. The elements that have
been reordered in the P sentence are marked alike in all sentences. The text in bold has changed place with the text in
italics.
6.3 Reordering analysis
In this section, we will show and discuss a few ex-
amples of the reorderings made by the SPTO ap-
proach. Table 8 contain two translations taken from
the test set.
In translation 1), the subject (bold) is correctly
moved to the right of the finite verb (italics), which
the baseline system fails to do. Moving the finite
verb away from the infinite verb ?feature?, however,
leads to incorrect agreement between these. While
the baseline correctly retains the infinite form (?sta??),
the language model forces another finite form (the
past tense ?stod?) in the SPTO reordering approach.
Translation 2) illustrates the handling of adver-
bials. The first reordering is in a main clause, there-
fore, the adverbial is moved to the right of the finite
verb. The second reordering occurs in a subordinate
clause, and the adverbial is moved to the left of the
finite verb. Neither of these are handled successfully
by the baseline system.
In this case, the reordering leads to better word
selection. The English ?aims to? corresponds to the
Danish ?sigter mod?, which the SPTO approach gets
correct. However, the baseline system translates ?to?
to its much more common translation ?at?, because
?to? is separated from ?aims? by the adverbial ?prin-
cipally?.
7 Conclusion and Future Plans
We have described a novel approach to word re-
ordering in SMT, which successfully integrates
syntactically motivated reordering in phrase-based
SMT. This is achieved by reordering the input string,
but scoring on the output string. As opposed to pre-
vious approaches, this neither biases against phrase
internal nor external reorderings. We achieve an ab-
solute improvement in translation quality of 1.1 %
BLEU. A result that is supported by manual evalua-
tion, which shows that the SPTO approach is signif-
icantly superior to previous approaches.
In the future, we plan to apply this approach to
English-Arabic translation. We expect greater gains,
due to the higher need for reordering between these
less-related languages. We also want to examine the
relation between word alignment method and the ex-
tracted rules and the relationship between reordering
and word selection. Finally, a limitation of the cur-
rent experiments is that they only allow rule-based
external reorderings. Since the SPTO scoring is not
tied to a source reordering approach, we want to ex-
amine the effect of simply adding it as an additional
parameter to the baseline PSMT system. This way,
all external reorderings are made possible, but only
the rule-supported ones get promoted.
53
References
Y. Al-Onaizan and K. Papineni. 2006. Distortion models
for statistical machine translation. In Proceedings of
44th ACL.
M. Buch-Kromann, J. Wedekind, and J. Elming. 2007.
The Copenhagen Danish-English Dependency Tree-
bank v. 2.0. http://www.isv.cbs.dk/?mbk/cdt2.0.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proceedings of ACL-2007 Workshop on
Statistical Machine Translation.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st NAACL.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In Proceedings of the 14th AAAI.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of the 43rd ACL.
J. M. Crego and J. B. Marin?o. 2007. Syntax-enhanced n-
gram-based smt. In Proceedings of the 11th MT Sum-
mit.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In Proceedings of the 11th MT
Summit.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. In Proceedings on the WSMT.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proceedings of AMTA.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of MT Sum-
mit.
C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007. A probabilistic approach to syntax-based re-
ordering for statistical machine translation. In Pro-
ceedings of the 45th ACL.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proceedings of EMNLP-CoNLL.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of Coling.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In M. Jarke, J. Koehler,
and G. Lakemeyer, editors, KI - 2002: Advances in
Artificial Intelligence. 25. Annual German Conference
on AI. Springer Verlag.
Y. Zhang, R. Zens, and H. Ney. 2007. Improved chunk-
level reordering for statistical machine translation. In
Proceedings of the IWSLT.
54
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69?77,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Syntactic Reordering for English-Arabic
Phrase-Based Machine Translation
Jakob Elming
Languagelens
Copenhagen, Denmark
je@languagelens.com
Nizar Habash
Center for Computational Learning Systems
Columbia University, New York, USA
habash@ccls.columbia.edu
Abstract
We investigate syntactic reordering within
an English to Arabic translation task. We
extend a pre-translation syntactic reorder-
ing approach developed on a close lan-
guage pair (English-Danish) to the dis-
tant language pair, English-Arabic. We
achieve significant improvements in trans-
lation quality over related approaches,
measured by manual as well as automatic
evaluations. These results prove the viabil-
ity of this approach for distant languages.
1 Introduction
The emergence of phrase-based statistical ma-
chine translation (PSMT) (Koehn et al, 2003a)
has been one of the major developments in statis-
tical approaches to translation. Allowing transla-
tion of word sequences (phrases) instead of single
words provides PSMT with a high degree of ro-
bustness in word selection and in local-word re-
ordering. Recent developments have shown that
improvements in PSMT quality are possible us-
ing syntax. One such development is the pre-
translation reordering approach, which adjusts the
source sentence to resemble target-language word
order prior to translation. This is typically done
using rules that are either manually created or
automatically learned from word-aligned parallel
corpora.
One particular variety of this approach, pro-
posed by Elming (2008), uses a large set of
linguistic features to automatically learn re-
ordering rules. The rules are applied non-
deterministically; however, phrase-internal word-
alignments are used to ensure that the intended re-
ordering does not come undone because of phrase
internal reordering (Elming, 2008). This approach
was shown to produce improved MT output on
English-Danish MT, a relatively closely-related
and similarly-structured language pair. In this
paper, we study whether this approach can be
extended to distant language pairs, specifically
English-to-Arabic. We achieve significant im-
provement in translation quality over related ap-
proaches, measured by manual as well as auto-
matic evaluations on this task. This proves the
viability of this approach on distant languages.
We also examined the effect of the alignment
method on learning reordering rules. Interestingly,
our experiments produced better translation using
rules learned from automatic alignments than us-
ing rules learned from manual alignments.
In the next section, we discuss and contrast re-
lated work. Section 3 describes aspects of English
and Arabic structure that are relevant to reorder-
ing. Section 4 describes the automatic induction
of reordering rules and its integration in PSMT. In
section 5, we describe the SMT system used in the
experiments. In section 6, we evaluate and discuss
the results of our English-Arabic MT system.
2 Related Work
Much work has been done in syntactic reorder-
ing for SMT, focusing on both source and target-
language syntax. In this paper, we adapt an ap-
proach that utilizes source-syntax information as
opposed to target-side syntax systems (Yamada
and Knight, 2001; Galley et al, 2004). This is
because we are translating from English to Arabic
and we are discouraged by recent results indicat-
ing Arabic parsing is not at a stage that makes it
usable in MT (Habash et al, 2006). While sev-
eral recent authors using a pre-translation (source-
side) reordering approach have achieved positive
results, it has been difficult to integrate syntactic
69
information while retaining the strengths of the
statistical approach. In some studies, reordering
decisions are done ?deterministically? by supply-
ing the decoder with a canonical word order (Xia
and McCord, 2004; Collins et al, 2005; Wang
et al, 2007; Habash, 2007). These reordering
rules are either manually specified or automati-
cally learned from alignments; and they are al-
ways placed outside the actual PSMT system. By
contrast, other studies (Crego and Mari?o, 2007;
Zhang et al, 2007; Li et al, 2007; Elming, 2008)
are more in the spirit of PSMT, in that multi-
ple reorderings are presented to the PSMT sys-
tem as (possibly weighted) options that are al-
lowed to contribute alongside other parameters.
Specifically, we follow the pre-translation reorder-
ing approach of Elming (2008). This approach
has been proven to remedy shortcomings of other
pre-translation reordering approaches by reorder-
ing the input word sequence, but scoring the out-
put word sequence.
Elming (2008) only examined the approach
within English ? Danish, a language pair that dis-
plays little reordering. By contrast, in this pa-
per, we target the more demanding reordering task
of translating between two distant languages, En-
glish and Arabic. While much work has been
done on Arabic to English MT (Habash and Sa-
dat, 2006; Lee, 2004) mostly focusing on ad-
dressing the problems caused by the rich mor-
phology of Arabic, we handle the less described
translation direction: English to Arabic. Recently,
there are some new publications on English to
Arabic MT. Sarikaya and Deng (2007) use joint
morphological-lexical language models to re-rank
the output of English dialectal-Arabic MT, and
Badr et al (2008) report results on the value of
the morphological decomposition of Arabic dur-
ing training and describe different techniques for
re-composition of Arabic in the output. We differ
from the previous efforts targeting Arabic in that
(1) we do not address morphology issues through
segmentation (more on this in section 3) and (2)
we focus on utilizing syntactic knowledge to ad-
dress the reordering challenges of this translation
direction.
3 Arabic Syntactic Issues
Arabic is a morphologically and syntactically
complex language with many differences from En-
glish. Arabic morphology has been well studied
in the context of MT. Previous results all sug-
gest that some degree of tokenization is helpful
when translating from Arabic (Habash and Sa-
dat, 2006; Lee, 2004). However, when trans-
lating into a morphologically rich language, tar-
get tokenization means that the translation process
is broken into multiple steps (Badr et al, 2008).
For our experiments, Arabic was not segmented
apart from simple punctuation tokenization. This
low level of segmentation was maintained in or-
der to agree with the segmentation provided in
the manually aligned corpus we used to learn our
rules (section 6.1). We found no simple means for
transferring the manual alignments to more seg-
mented language. We expect that better perfor-
mance would be achieved by introducing more
Arabic segmentation as reported by Badr et al
(2008).1 As such, and unlike previous work in
PSMT translating into Arabic, we focus here on
syntax. We plan to investigate different tokeniza-
tion schemes for syntactic preprocessing in future
work. Next, we describe three prominent English-
Arabic syntactic phenomena that have motivated
some of our decisions in this paper.
First is verb-subject order. Arabic verb subjects
may be: (a.) pro-dropped (verb conjugated), (b.)
pre-verbal (SVO), or (c.) post-verbal (VSO). Al-
though the English SVO order is possible in Ara-
bic, it is not always preferred, especially when the
subject is particularly long. Unfortunately, this is
the harder case for PSMT to handle. For small
subject noun phrases (NP), PSMT might be able
to handle the reordering in the phrase table if the
verb and subject were seen in training. But this be-
comes much less likely with very long NPs that ex-
ceed the size of the phrases in a phrase table. The
example in Figure 1 illustrates this point. Bolding
and italics are used to mark the verb and subor-
dinating conjunction that surround the subject NP
(19 tokens) in English and what they map to in
Arabic, respectively.2
Secondly, Arabic adjectival modifiers typically
follow their nouns with the exception of some su-
perlative adjectives. However, English adjectival
modifiers can follow or precede their nouns de-
pending on the size of the adjectival phrase: single
word adjectives precede but multi-word adjectives
phrases follow (or precede while hyphenated). For
example, a tall man translates as ?K
?? ?g. P rjl
1Our results are not comparable to their results, since they
report on non-standard data sets.
2All Arabic transliterations in this paper are provided in
the Habash-Soudi-Buckwalter scheme (Habash et al, 2007).
70
[NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Coopera-
tion Council , Hamid Khaja ,] [V announced] [SUB that ...]
[ ?k. A
	
g Y?Ag ?


j
.
J
?
	
m?'@
	
??A?

J? @ ??m.
? ??X
	?
K. YK
Ym
?'@

????@ ??Q???? ?A?? @

??
	
?? @ NP-SBJ] [ 	???@ V]
[. . . 	?@ SUB]
[V A?ln] [NP-SBJ Almnsq Al?Am lm?rw? Alsk~ AlHdyd byn dwl mjls Alt?Awn Alxlyjy HAmd
xAjh] [SUB An ...]
Figure 1: An example of long distance reordering of English SVO order to Arabic VSO order
t7 ? ? ? ? ? ? 
t6 ? ?  ? ? ? ?
t5 ?  ? ? ? ? ?
t4 ? ? ? ?  ? ?
t3 ? ? ? ? ?  ?
t2 ? ? ?  ? ? ?
t1  ? ? ? ? ? ?
s1 s2 s3 s4 s5 s6 s7
Figure 2: Abstract alignment matrix example of
reordering.
Twyl ?man tall?; however, the English phrase a
man tall of stature translates with no reordering as

??A

?? @ ?K
?? ?g. P rjl Twyl AlqAm~ ?man tall the-
stature?. So does the superlative the tallest man
translating into ?g. P ???@ ATwl rjl ?tallest man.?
Finally, Arabic has one syntactic construction,
called Idafa, for indicating possession and com-
pounding, while English has three. The Idafa con-
struction typically consists of one or more indef-
inite nouns followed by a definite noun. For ex-
ample, the English phrases the car keys, the car?s
keys and the keys of the car all translate into the
Arabic

?PAJ
??@ iJ


KA
	
?? mfAtyH AlsyAr~ ?keys the-
car.? Only one of the three English constructions
does not require content word reordering.
4 Reordering rules
4.1 Definition of reordering
Following Elming (2008), we define reordering as
two word sequences, left sequence (LS) and right
sequence (RS), exchanging positions. These two
sequences are restricted by being parallel consecu-
tive, maximal and adjacent. The sequences are not
restricted in length, making both short and long
distance reordering possible. Furthermore, they
need not be phrases in the sense that they appear
as an entry in the phrase table.
Figure 2 illustrates reordering in a word align-
ment matrix. The matrix contains reorderings be-
tween the light grey sequences (s32 and s
6
4)
3 and
3Notation: syx means the consecutive source sequence
the dark grey sequences (s55 and s
6
6). On the other
hand, the sequences s33 and s
5
4 are not considered
for reordering, since neither one is maximal, and
s54 is not consecutive on the target side.
4.2 Learning rules
Table 1 contains an example of the features avail-
able to the algorithm learning reordering rules.
We include features for the candidate reorder-
ing sequences (LS and RS) and for their possi-
ble left (LC) and right (RC) contexts. In addi-
tion to words and parts-of-speech (POS), we pro-
vide phrase structure (PS) sequences and subordi-
nation information (SUBORD). The PS sequence
is made up of the highest level nodes in the syntax
tree that cover the words of the current sequence
and only these. Subordinate information can also
be extracted from the syntax tree. A subordinate
clause is defined as inside an SBAR constituent;
otherwise it is a main clause. Our intuition is that
all these features will allow us to learn the best
rules possible to address the phenomena discussed
in section 3 at the right level of generality.
In order to minimize the amount of training
data, word and POS sequences are annotated as
too long (T/L) if they are longer than 4 words,
and the same for phrase structure (PS) sequences
if they are longer than 3 units. A feature vector
is only used if at least one of these three levels is
not T/L for both LS and RS, and T/L contexts are
not included in the set. This does not constrain
the possible length of a reordering, since a PS se-
quence of length 1 can cover an entire sentence.
In the example in Table 1, LS and RS are single
words, but they are not restricted in length. The
span of the contexts varies from a single neighbor-
ing word to all the way to the sentence border. In
the example, LS and RS should be reordered, since
adjectives appear as post-modifiers in Arabic.
In order to learn rules from the annotated data,
we use a rule-based classifier, Ripper (Cohen,
covering word positions x to y.
71
Level LC LS RS RC
WORD <s> he bought || he bought || bought new books today || today . || today . < /s>
POS <S> NN VBD || NN VBD || VBD JJ NNS NN || NN . || NN . < /S>
PS <S> NP VBD || NP VBD || VBD JJ NNS NP || NP . || NP . < /S>
SUBORD MAIN MAIN MAIN MAIN
Table 1: Example of features for rule-learning. Possible contexts separated by ||.
Figure 3: Example word lattice.
1996). The motivation for using Ripper is that it
allows features to be sets of strings, which fits well
with our representation of the context, and it pro-
duces easily readable rules that allow better under-
standing of the decisions being made. In section
6.3, extracted rules are exemplified and analyzed.
The probabilities of the rules are estimated us-
ing Maximum Likelihood Estimation based on
the information supplied by Ripper on the perfor-
mance of the individual rules on the training data.
These logarithmic probabilities are easily integrat-
able in the log-linear PSMT model as an additional
parameter by simple addition.
5 The PSMT system
Our baseline is the PSMT system used for the
2006 NAACL SMT workshop (Koehn and Monz,
2006) with phrase length 3 and a trigram language
model (Stolcke, 2002). The decoder used for the
baseline system is Pharaoh (Koehn, 2004) with
its distance-penalizing reordering model. Since
Pharaoh does not support word lattice input, we
use our own decoder for the experiments. Ex-
cept for the reordering model, it uses the same
knowledge sources as Pharaoh, i.e. a bidirectional
phrase translation model, a lexical weight model,
phrase and word penalties, and a target language
model. Its behavior is comparable to Pharaoh
when doing monotone decoding.
The search algorithm of our decoder is similar
to the RG graph decoder of (Zens et al, 2002). It
expects a word lattice as input. Figure 3 shows
the word lattice for the example in table 2. In the
example used here, we choose to focus on the re-
ordering of adjective and noun. For readability,
we do not describe the possibility of reordering the
subject and verb. This will also be the case in later
use of the example.
Since the input format defines all possible word
orders allowed by the rule set, a simple monotone
search is sufficient. Using a language model of or-
der n, for each hypothesized target string ending
in the same n-1-gram, we only have to extend the
highest scoring hypothesis. None of the others can
possibly outperform this one later on. This is be-
cause the maximal context evaluating a phrase ex-
tending this hypothesis, is the history (n-1-gram)
of the first word of that phrase. The decoder is
not able to look any further back at the preceding
string.
5.1 The reordering approach
Similar to Elming (2008), the integration of the
rule-based reordering in our PSMT system is car-
ried out in two separate stages:
1. Reordering the source sentence to assimilate
the word order of the target language.
2. Weighting of the target word order according
to the rules.
Stage (1) is done in a non-deterministic fashion
by generating a word lattice as input. This way, the
system has both the original word order, and the
reorderings predicted by the rule set. The different
paths of the word lattice are merely given as equal
suggestions to the decoder. They are in no way
individually weighted.
Separating stage (2) from stage (1) is motivated
by the fact that reordering can have two distinct
origins. They can occur because of stage (1), i.e.
the lattice reordering of the original English word
order (phrase external reordering), and they can
occur inside a single phrase (phrase internal re-
ordering). The focus of this approach lies in do-
ing phrase-independent word reordering. Rule-
predicted reorderings should be promoted regard-
less of whether they owe their existence to a syn-
tactic rule or a phrase table entry.
This is accomplished by letting the actual scor-
ing of the reordering focus on the target string.
72
Source: he1 bought2 new3 books4 today5
Rule: 3 4? 4 3
Hypothesis Target string Alignment
H1 A?tr? jdyd~ ktbA 1+2 3 4
H2 A?tr? ktbA jdyd~ 1+2 4 3
Table 2: Example of the scoring approach during
decoding at source word 4.
The decoder is informed of where a rule has pre-
dicted a reordering, how much it costs to do the
reordering, and how much it costs to avoid it. This
is then checked for each hypothesized target string
via a word alignment.
The word alignment keeps track of which
source position the word in each target position
originates from. In order to access this informa-
tion, each phrase table entry is annotated with its
internal word alignment, which is available as an
intermediate product from phrase table creation.
If a phrase pair has multiple word alignments, the
most frequent one is chosen.
Table 2 exemplifies the scoring approach, again
with focus on the adjective-noun reordering. The
source sentence is ?he bought new books today?,
and a rule has predicted that source word 3 and
4 should change place. Due to the pro-drop na-
ture of Arabic, the first Arabic word is linked to
the two first English words (1+2). When the de-
coder has covered the first four input words, two
of the hypothesis target strings might be H1 and
H2. At this point, it becomes apparent that H2
contains the desired reordering (namely what cor-
responds to source word order ?4 3?), and it get
assigned the reordering cost. H1 does not contain
the rule-suggested reordering (instead, the words
are in the original order ?3 4?), and it gets the vi-
olation cost. Both these scorings are performed
in a phrase-independent manner. The decoder as-
signs the reordering cost to H2 without knowing
whether the reordering is internal (due to a phrase
table entry) or external (due to a syntactic rule).
Phrase internal reorderings at other points of the
sentence, i.e. points that are not covered by a rule,
are not judged by the reordering model. Our rule
extraction does not learn every possible reorder-
ing between the two languages, but only the most
general ones. If no rule has an opinion at a certain
point in a sentence, the decoder is free to choose
the phrase translation it prefers without reordering
cost.
Separating the scoring from the source lan-
guage reordering also has the advantage that the
approach in essence is compatible with other
approaches such as a traditional PSMT system
(Koehn et al, 2003b) or a hierarchical phrase sys-
tem (Chiang, 2005). We will, however, not exam-
ine this possibility further in the present paper.
6 Evaluation
6.1 Data
We learn the reordering rules from the IBM
Arabic-English aligned corpus (IBMAC) (Itty-
cheriah and Roukos, 2005). Of its total 13.9K sen-
tence pairs, we only use 8.8K sentences because
the rest of the corpus uses different normalizations
for numerals that make the two sets incompatible.
6.6K of the sentences (179K English and 146K
Arabic words) are used to learn rule, while the rest
are used for development purposes. In addition to
the manual alignment supplied with these data, we
create an automatic word alignment for them using
GIZA++ (Och and Ney, 2003) and the grow-diag-
final (GDF) symmetrization algorithm (Koehn et
al., 2005). This was done together with the data
used to train the MT system. The English side
is parsed using a state-of-the-art statistical English
parser (Charniak, 2000). Two rule sets are learned
based on the manual alignments (MAN) and the
automatic alignments (GDF).
The MT system is trained on a corpus con-
sisting of 126K sentences with 4.2M English
and 3.3M Arabic words in simple tokeniza-
tion scheme. The domain is newswire (LDC-
NEWS) taken from Arabic News (LDC2004T17),
eTIRR (LDC2004E72), English translation of
Arabic Treebank (LDC2005E46), and Ummah
(LDC2004T18). Although there are additional
corpora available, we restricted ourselves to this
set to allow for a fast development cycle. We plan
to extend the data size in the future. The Ara-
bic language model is trained on the 5.4M sen-
tences (133M words) of newswire text in the 1994
to 1996 part of the Arabic Gigaword corpus. We
restricted ourselves to this part, since we are not
able to run Pharaoh with a larger language model.4
For test data, we used NIST MTEval test sets
from 2004 (MT04) and 2005 (MT05)5. Since
these data sets are created for Arabic-English eval-
uation with four English reference sentences for
4All of the training data we use is available from the Lin-
guistic Data Consortium (LDC): http://www.ldc.upenn.edu/.
5 http://www.nist.gov/speech/tests/mt/
73
System Dev MT04 MT05
Pharaoh Free 28.37 23.53 24.79
Pharaoh DL4 29.52 24.72 25.88
Pharaoh Monotone 27.93 23.55 24.72
MAN NO weight 29.53 24.72 25.82
SO weight 29.43 24.74 25.82
TO weight 29.40 24.78 25.93
GDF NO weight 29.87 25.11 26.04
SO weight 29.84 25.06 26.01
TO weight 29.95 25.17 26.09
Table 3: Automatic evaluation scores for different
systems using rules extracted from manual align-
ments (MAN) and automatic alignments (GDF).
The TO system using GDF rules is significantly
better than the light grey cells at a 95% confidence
level (Zhang et al, 2004).
each Arabic sentence, we invert the sets by con-
catenating all English sentences to one file. This
means that the Arabic reference file contains four
duplicates of each sentence. Each duplicate is the
reference of a different English source sentence.
Following this merger, MT04 consists of 5.4K
sentences with 193K English and 144K Arabic
words, and MT05 consists of 4.2K sentences with
143K English and 114K Arabic words. MT04 is
a mix of domains containing speeches, editorials
and newswire texts. On the other hand, MT05 is
only newswire.
The NIST MTEval test set from 2002 (MT02)
is split into a tuning set for optimizing decoder pa-
rameter weights and a development set for ongo-
ing experimentation. The same merging procedure
as for MT04 and MT05 is employed. This results
in a tune set of 1.0K sentences with 34K English
and 26K Arabic words, and a development set of
3.1K sentences with 102K English and 79K Ara-
bic words.
6.2 Results and discussion
The reordering approach is evaluated on the MT04
and MT05 test sets. Results are listed in table 3
along with results on the development set. We re-
port on (a) Pharaoh with no restriction on reorder-
ing (Pharaoh Free), (b) Pharaoh with distortion
limit 4 (Pharaoh DL4), (c) Pharaoh with monotone
decoding (Pharaoh Monotone), and (d) a system
provided with a rule reordered word lattice but no
(NO) weighting in the spirit of (Crego and Mari?o,
2007), (e) the same system but with a source order
System MT04 MT05 Avr. human
Pharaoh Free 24.07 25.15 3.0 (2.80)
Pharaoh DL4 25.42 26.51 ?
NO scoring 25.68 26.29 2.5 (2.43)
SO scoring 25.42 26.02 2.5 (2.64)
TO scoring 25.98 26.49 2.0 (2.08)
Table 4: Evaluation on the diff set. Average hu-
man ratings are medians with means in parenthe-
sis, lower scores are better, 1 is the best score.
(SO) weighting in the spirit of (Zhang et al, 2007;
Li et al, 2007), and finally (f) the same system but
with the target order (TO) weighting.
In addition to evaluating the reordering ap-
proaches, we also report on supplying them with
different reordering rule sets: a set that was
learned on manually aligned data (MAN), and a
set learned on the same data but with automatic
alignments (GDF).
6.2.1 Overall Results
Pharaoh Monotone performs similarly to Pharaoh
Free. This shows that the question of improved
reordering is not about quantity, but rather qual-
ity: what constraints are optimal to generate the
best word order. The TO approach gets an increase
over Pharaoh Free of 1.3 and 1.6 %BLEU on the
test sets, and 0.2 and 0.5 %BLEU over Pharaoh
DL4.
Improvement is less noticeable over the other
pre-translation reordering approaches (NO and
SO). A possible explanation is that the rules do not
apply very often, in combination with the fact that
the approaches often behave alike. The difference
in SO and TO scoring only leads to a difference
in translation in ?14% of the sentences. This set,
the diff set, is interesting, since it provides a focus
on the difference between these approaches. In ta-
ble 4, we evaluate on this set.
6.2.2 Diff Set
Overall the TO approach seems to be a superior
reordering method. To back this observation, 50
sentences of MT04 are manually evaluated by a
native speaker of Arabic. Callison-Burch et al
(2007) show that ranking sentences gives higher
inter-annotator agreement than scoring adequacy
and fluency. We therefore employ this evaluation
method, asking the evaluator to rank sentences
from four of the systems given the input sentence.
Ties are allowed. Table 4 shows the average rat-
74
Decoder choice NO SO TO
MT04 Phrase internal 20.7 0.6 21.2
Phrase external 30.1 43.0 33.1
Reject 49.2 56.5 45.7
MT05 Phrase internal 21.3 0.7 21.6
Phrase external 29.5 42.9 31.8
Reject 49.2 56.4 46.5
Table 5: The reordering choices made based on
the three pre-translation reordering approaches for
the 20852 and 17195 reorderings proposed by the
rules for the MT04 and MT05 test sets. Measured
in %.
ings of the systems. This shows the TO scoring
to be significantly superior to the other methods
(p < 0.01 using Wilcoxon signed-rank testing).
6.2.3 MAN vs GDF
Another interesting observation is that reordering
rules learned from automatic alignments lead to
significantly better translation than rules learned
from manual alignment. Due to the much higher
quality of the manual alignment, the opposite
might be expected. However, this may be just
a variant on the observation that alignment im-
provements (measured against human references)
seldom lead to MT improvements (Lopez and
Resnik, 2006). The MAN alignments may in fact
be better than GDF, but they are most certainly
more different in nature from real alignment than
the GDF alignments are. As such, the MAN align-
ments are not as powerful as we would have liked
them to be. In our data sets, the GDF rules, seem
less specific, and they therefore apply more fre-
quently than the MAN rules. On average, this re-
sults in more than 7 times as many possible re-
ordering paths per sentence. This means that the
GDF rules supply the decoder with a larger search
space, which in turn means more proposed trans-
lation hypotheses. This may play a big part in the
effect of the rule sets.
6.2.4 Reordering Choices
Table 5 shows the reordering choices made by the
approaches in decoding. Most noticeable is that
the SO approach is strongly biased against phrase
internal reorderings; TO uses more than 30 times
as many phrase internal reorderings as SO. In ad-
dition, TO is less likely to reject a rule proposed
reordering.
The 50 sentences from the manual evaluation
are also manually analyzed with regards to re-
ordering. For each reordering in these sentences,
the four systems are ranked according to how well
the area affected by the reordering is translated.
This indicates that the SO approach?s bias against
phrase internal reorderings may hurt performance.
25% of the time, when SO chooses an external re-
ordering, while the TO approach chooses an in-
ternal reordering, the TO approach gets a better
translation. Only in 7% of the cases is it the other
way around.
Another discovery from the analysis is when TO
chooses an internal reordering and NO rejects the
reordering. Here, TO leads to a better translation
45% of the time, while NO never outperforms TO.
In these cases, either approach has used a phrase
to cover the area, but via rule-based motivation,
TO has forced a less likely phrase with the correct
word order through. This clearly shows that lo-
cal reordering is not handled sufficiently by phrase
internal reordering alone. These need to be con-
trolled too.
6.3 Rule analysis
The rule learning resulted in 61 rules based on
manual alignments and 39 based on automatic
alignments. Of these, the majority handled the
placement of adjectives, while only a few handled
the placement of the verb.
A few of the rules that were learned from the
manual alignment are shown in table 6. The first
two rules handle the placement of the finite verb
in Arabic. Rule 16 states that if a finite verb
appears in front of a subordinate clause, then it
should be moved to sentence initial position with
a probability of 68%. Due to the restrictions of
sequence lengths, it can only swap across maxi-
mally 4 words or a sequence of words that is de-
scribable by maximally 3 syntactic phrases. The
SBAR condition may help restrict the reordering
to finite verbs of the main clause. This rule and its
probability goes well with the description given in
sections 3, since VSO order is not obligatory. The
subject may be unexpressed, or it may appear in
front of the verb. This is even more obvious in
rule 27, which has a probability of only 43%.
Rules 11 and 1 deal with the inverse ordering of
adjectives and nouns. The first is general but un-
certain, the second is lexicalized and certain. The
reason for the low probability of rule 11 is primar-
ily that many proper names have been mis-tagged
by the parser as either JJ or NN, and to a lesser
75
No LC LS RS RC Prob.
16 WORD: <s> POS: FVF PS: SBAR 68%
27 WORD: <s> PS: NP POS: FVF 43%
11 POS: IN POS: JJ POS: NN 46%
1 ! POS: JJ POS: JJ WORD: president 90%
37 ! POS: NN POS: NN POS: NNS POS: IN 71%
! POS: JJ
Table 6: Example rules. ! specifies negative conditions.
extent that the rule should often not apply if the
right context is also an NN. Adding the latter re-
striction narrows the scope of the rule but would
have increased the probability to 54%.
Rule 1, on the other hand, has a high proba-
bility of 90%. It is only restricted by the con-
dition that the left context should not be an ad-
jective. In these cases, the adjectives should of-
ten be moved together, as is the case with ?the
south african president?? ?



?K
Q
	
? @ H. ?
	
Jm.
?'@ ?


KQ? @
Alry?ys Aljnwb Afryqy where ?south african? is
moved to the right of ?president?.
Finally, rule 37 handles compound nouns. Here
a singular noun is moved to the right of a plural
noun, if the right context is a preposition, and the
left context is neither an adjective nor a singular
noun. This rule handles compound nouns, where
the modifying function of the first noun often is
hard to distinguish from that of an adjective. The
left context restrictions server the same purpose as
the left context in rule 1; these should often be
moved together with the singular noun. The func-
tion of the right context is harder to explain, but
without this restriction, the rule would have been
much less successful; dropping from a probability
of 71% to 51%.
An overall comparison of the rules produced
based on the manual and automatic alignments
shows no major difference in quality. This is espe-
cially interesting in light of the better translation
using the GDF rules. It is also very interesting
that it seems possible to get as good rules from the
GDF as from the MAN alignments. This is a new
result compared to Elming (2008), where results
on manual alignments only are reported.
7 Conclusion and Future Plans
We have explored the syntactic reordering ap-
proach previously presented in (Elming, 2008)
within a more distant language pair, English-
Arabic. A translation direction that is highly
under-represented in MT research, compared to
the opposite direction. We achieve significant im-
provement in translation quality over related ap-
proaches, measured by manual as well as auto-
matic evaluations on this task. Thus proving the
viability of the approach on distant languages.
We also examined the effect of the alignment
method on learning reordering rules. Interestingly,
our experiments produced better translation using
rules learned from automatic alignments than us-
ing rules learned from manual alignments. This is
an aspect we want to explore further in the future.
In future work, we would also like to address
the morphological complexity of Arabic together
with syntax. We plan to consider different seg-
mentations for Arabic and study their interaction
with translation and syntactic reordering.
An important aspect of the TO approach is that
it uses phrase internal alignments during transla-
tion. In the future, we wish to examine the effect
their quality has on translation. We are also inter-
ested in examining the approach within a standard
phrase-based decoder such as Moses (Koehn et al,
2003b) or a hierarchical phrase system (Chiang,
2005).
The idea of training on reordered source lan-
guage is often connected with pre-translation re-
ordering. The present approach does not em-
ploy this strategy, since this is no trivial matter
in a non-deterministic, weighted approach. Zhang
et al (2007) proposed an approach that builds
on unfolding alignments. This is not an opti-
mal solution, since this may not reflect their rules.
Training on both original and reordered data may
strengthen the approach, but it would not remedy
the problems of the SO approach, since it would
still be ignorant of the internal reorderings of a
phrase. Nevertheless, it may strengthen the TO
approach even further. We also wish to examine
this in future work.
76
References
I. Badr, R. Zbib, and J. Glass. 2008. Segmentation for
English-to-Arabic statistical machine translation. In
Proceedings of ACL?08: HLT, Short Papers, Colum-
bus, OH, USA.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proceedings of ACL?07 Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00, Seattle, WA,
USA.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL?05, Ann Arbor, MI, USA.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In Proceedings of AAAI, Portland,
OR, USA.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL?05, Ann Arbor, MI, USA.
J. M. Crego and J. B. Mari?o. 2007. Syntax-enhanced
n-gram-based smt. In Proceedings of the MT Sum-
mit, Copenhagen, Denmark.
J. Elming. 2008. Syntactic reordering integrated with
phrase-based smt. In Proceedings of the ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion (SSST-2), Columbus, OH, USA.
M. Galley, M. Hopkins, K. Knight, and D. Marcu.
2004. What?s in a translation rule? In Proceedings
of HLT/NAACL?04, Boston, MA, USA.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of HLT-NAACL?06, New York, NY, USA.
N. Habash, B. Dorr, and C. Monz. 2006. Challenges
in Building an Arabic-English GHMT System with
SMT Components. In Proceedings of AMTA?06,
Cambridge, MA, USA.
N. Habash, A. Soudi, and T. Buckwalter. 2007.
On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
N. Habash. 2007. Syntactic preprocessing for statisti-
cal machine translation. In Proceedings of the MT
Summit, Copenhagen, Denmark.
A. Ittycheriah and S. Roukos. 2005. A maximum
entropy word aligner for arabic-english machine
translation. In Proceedings of EMNLP, Vancouver,
Canada.
P. Koehn and C. Monz. 2006. Manual and auto-
matic evaluation of machine translation between Eu-
ropean languages. In Proceedings of the Workshop
on Statistical Machine Translation at NAACL?06,
New York, NY, USA.
P. Koehn, F. J. Och, and D. Marcu. 2003a. Statis-
tical phrase-based translation. In Proceedings of
NAACL?03, Edmonton, Canada.
P. Koehn, F. J. Och, and D. Marcu. 2003b. Statis-
tical Phrase-based Translation. In Proceedings of
NAACL?03, Edmonton, Canada.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for the 2005 IWSLT
speech translation evaluation. In International
Workshop on Spoken Language Translation 2005
(IWSLT?05), Pittsburgh, PA, USA.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proceedings of AMTA?04, Washington, DC, USA.
Y. Lee. 2004. Morphological Analysis for Statisti-
cal Machine Translation. In Proceedings of HLT-
NAACL?04, Boston, MA, USA.
C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007. A probabilistic approach to syntax-based re-
ordering for statistical machine translation. In Pro-
ceedings of ACL?07, Prague, Czech Republic.
A. Lopez and P. Resnik. 2006. Word-based alignment,
phrase-based translation: what?s the link? In Pro-
ceedings of AMTA?06, Cambridge, MA, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
R. Sarikaya and Y. Deng. 2007. Joint morphological-
lexical language modeling for machine translation.
In Proceedings of HLT-NAACL?07, Short Papers,
Rochester, NY, USA.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Den-
ver, CO, USA.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of EMNLP-CoNLL, Prague,
Czech Republic.
F. Xia and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proceedings of COLING?04, Geneva,
Switzerland.
K. Yamada and K. Knight. 2001. A Syntax-Based
Statistical Translation Model. In Proceedings of
ACL?01, Toulouse, France.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In M. Jarke, J. Koehler,
and G. Lakemeyer, editors, KI - 2002: Advances in
Artificial Intelligence. 25. Annual German Confer-
ence on AI. Springer Verlag.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpret-
ing bleu/nist scores: How much improvement do we
need to have a better system? In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Portu-
gal.
Y. Zhang, R. Zens, and H. Ney. 2007. Improved
chunk-level reordering for statistical machine trans-
lation. In Proceedings of the IWSLT, Trento, Italy.
77
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476?1480,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using crowdsourcing to get representations based on regular expressions
Anders S?gaard and Hector Martinez and Jakob Elming and Anders Johannsen
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dk
Abstract
Often the bottleneck in document classifica-
tion is finding good representations that zoom
in on the most important aspects of the doc-
uments. Most research uses n-gram repre-
sentations, but relevant features often occur
discontinuously, e.g., not. . . good in sentiment
analysis. In this paper we present experi-
ments getting experts to provide regular ex-
pressions, as well as crowdsourced annota-
tion tasks from which regular expressions can
be derived. Somewhat surprisingly, it turns
out that these crowdsourced feature combina-
tions outperform automatic feature combina-
tion methods, as well as expert features, by a
very large margin and reduce error by 24-41%
over n-gram representations.
1 Introduction
Finding good representations of classification prob-
lems is often glossed over in the literature. Sev-
eral authors have emphasized the need to pay more
attention to finding such representations (Wagstaff,
2012; Domingos, 2012), but in document classifica-
tion most research still uses n-gram representations.
This paper considers two document classification
problems where such representations seem inade-
quate. The problems are answer scoring (Burstein
et al, 1998), on data from stackoverflow.com, and
multi-attribute sentiment analysis (McAuley et al,
2012). We argue that in order to adequately repre-
sent such problems we need discontinuous features,
i.e., regular expressions.
The problem with using regular expressions as
features is of course that even with a finite vocab-
ulary we can generate infinitely many regular ex-
pressions that match our documents. We suggest to
use expert knowledge or crowdsourcing in the loop.
In particular we present experiments where standard
representations are augmented with features from a
few hours of manual work, by machine learning ex-
perts or by turkers.
Somewhat surprisingly, we find that features de-
rived from crowdsourced annotation tasks lead to the
best results across the three datasets. While crowd-
sourcing of annotation tasks has become increasing
popular in NLP, this is, to the best of our knowledge,
the first attempt to crowdsource the problem of find-
ing good representations.
1.1 Related work
Musat et al (2012) design a collaborative two-player
game for sentiment annotation and collecting a sen-
timent lexicon. One player guesses the sentiment of
a text and picks a word from it that is representative
of its sentiment. The other player also provides a
guess observing only this word. If the two guesses
agree, both players get a point. The idea of gam-
ifying the problem of finding good representations
goes beyond crowdsourcing, but is not considered
here. Boyd-Graber et al (2012) crowdsource the
feature weighting problem, but using standard rep-
resentations. The work most similar to ours is prob-
ably Tamuz et al (2011), who learn a ?crowd kernel?
by asking annotators to rate examples by similarity,
providing an embedding that promotes feature com-
binations deemed relative when measuring similar-
ity.
1476
BoW Exp AMT
n P (1) m ?x m ?x m ?x
STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331
TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285
APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289
Table 1: Characteristics of the n?m data sets
2 Experiments
Data The three datasets used in our experi-
ments come from two sources, namely stackover-
flow.com and ratebeer.com. The two beer review
datasets (TASTE and APPEARANCE) are described
in McAuley et al (2012) and available for down-
load.1 Each input example is an unstructured review
text, and the associated label is the score assigned to
taste or appearance by the reviewer. We randomly
sample about 152k data points, as well as 500 exam-
ples for experiments with experts and turks.
We extracted the STACKOVERFLOW dataset from
a publicly available data dump,2, and we briefly de-
scribe our sampling process here. We select pairs of
answers, where one is ranked higher than the other
by stackoverflow.com users. Obviously the answers
submitted first have a better chance of being ranked
highly, so we also require that the highest ranked
answer was submitted last. From this set of answer
pairs, we randomly sample 97,519 pairs, as well as
500 examples for our experiments with experts and
turks.
Our experiments are classification experiments
using the same learning algorithm in all experi-
ments, namely L1-regularized logistic regression.
We don?t set any parameters The only differences
between our systems are in the feature sets. Results
are from 5-fold cross-validation. The four feature
sets are described below: BoW, HI, Exp and AMT.
For motivating using regular expressions, con-
sider the following sentence from a review of John
Harvard?s Grand Cru:
(1) Could have been more flavorful.
The only word carrying direct sentiment in this
sentence is flavorful, which is positive, but the sen-
tence is a negative evaluation of the Grand Cru?s
1http://snap.stanford.edu/data/web-RateBeer.html
2http://www.clearbits.net/torrents/2076-aug-2012
taste. The trigram been more flavorful seems neg-
ative at first, but in the context of negation or in a
comparative, it can become positive again. How-
ever, note that this trigram may occur discontinu-
ously, e.g., in been less watery and more flavorful.
In order to match such occurrences, we need simple
regular expressions, e.g.,:
been.*more.*flavorful
This is exactly the kind of regular expressions we
asked experts to submit, and that we derived from
the crowdsourced annotation tasks. Note that the
sentence says nothing about the beer?s appearance,
so this feature is only relevant in TASTE, not in
APPEARANCE.
BoW and BoW+HI Our most simple baseline ap-
proach is a bag-of-words model of unigram features
(BoW). We lower-case our data, but leave in stop
words. We also introduce a semantically enriched
unigram model (BoW)+HI, where in addition to
representing what words occur in a text, we also
represent what Harvard Inquirer (HI)3 word classes
occur in it. The HI classes are used to generate
features from the crowdsourced annotation tasks,
so the semantically enriched unigram model is an
important baseline in our experiments below.
BoW+Exp In order to collect regular expressions
from experts, we set up a web interface for query-
ing held-out portions of the datasets with regular ex-
pressions that reports how occurrences of the sub-
mitted regular expressions correlate with class. We
used the Python re syntax for regular expressions
after augmenting word forms with POS and seman-
tic classes from the HI. Few of the experts made use
of the POS tags, but many regular expressions in-
cluded references to HI classes.
3http://www.wjh.harvard.edu/ inquirer/homecat.htm
1477
Regular expressions submitted by participants
were visible to other participants during the exper-
iment, and participants were allowed to work to-
gether. Participants had 15 minutes to familiarize
themselves with the syntax used in the experiments.
Each query was executed in 2-30 seconds.
Seven researchers and graduate students spent
five effective hours querying the datasets with
regular expressions. In particular, they spent three
hours on the Stack Exchange dataset, and one hour
on each of the two RateBeer datasets. One had to
leave an hour early. So, in total, we spent 20 person
hours on Stack Exchange, and seven person hours
on each of the RateBeer datasets. In the five hours,
we collected 1,156 regular expressions for the
STACKOVERFLOW dataset, and about 650 regular
expressions for each of the two RateBeer datasets.
Exp refers to these sets of regular expressions. In
our experiments below we concatenate these with
the BoW features to form BoW+Exp.
BoW+AMT For each dataset, we also had 500 held-
out examples annotated by three turkers each, using
Amazon Mechanical Turk,4 obtaining 1,500 HITs
for each dataset. The annotators were presented with
each text, a review or an answer, twice: once as run-
ning text, once word-by-word with bullets to tick off
words. The annotators were instructed to tick off
words or phrases that they found predictive of the
text?s sentiment or answer quality. They were not in-
formed about the class of the text. We chose this an-
notation task, because it is relatively easy for annota-
tors to mark spans of text with a particular attribute.
This set-up has been used in other applications, in-
cluding NER (Finin et al, 2010) and error detection
(Dahlmeier et al, 2013). The annotators were con-
strained to tick off at least three words, including
one closed class item (closed class items were col-
ored differently). Finally, we only used annotators
with a track record of providing high-quality anno-
tations in previous tasks. It was clear from the aver-
age time spent by annotators that annotating STACK-
OVERFLOW was harder than annotating the Rate-
beer datasets. The average time spent on a Rate-
beer HIT was 44s, while for STACKOVERFLOW it
was 3m:8s. The mean number of words ticked off
4www.mturk.com
BoW HI Exp AMT
STACKOVERF 0.655 0.654 0.683 0.739
TASTE 0.798 0.797 0.798 0.867
APPEARANCE 0.758 0.760 0.761 0.859
Table 2: Results using all features
was between 5.6 and 7, with more words ticked off
in STACKOVERFLOW. The maximum number of
words ticked off by an annotator was 41. We spent
$292.5 on the annotations, including a trial round.
This was supposed to match, roughly, the cost of the
experts consulted for BoW+Exp.
The features generated from the annotations were
constructed as follows: We use a sliding window of
size 3 to extract trigrams over the possibly discon-
tinuous words ticked off by the annotators. These
trigrams were converted into regular expressions by
placing Kleene stars between the words. This gives
us a manually selected subset of skip trigrams. For
each skip trigram, we add copies with one or more
words replaced by one of their HI classes.
Feature combinations This subsection introduces
some harder baselines for our experiments, consid-
ered in Experiment #2. The simplest possible way
of combining unigram features is by considering n-
gram models. An n-gram extracts features from a
sliding window (of size n) over the text. We call this
model BoW(N = n). Our BoW(N = 1) model
takes word forms as features, and there are obvi-
ously more advanced ways of automatically combin-
ing such features.
Kernel representations We experimented with ap-
plying an approximate feature map for the addi-
tive ?2-kernel. We used two sample steps, result-
ing in 4N + 1 features. See Vedaldi and Zimmer-
man (2011) for details.
Deep features We also ran denoising autoen-
coders (Pascal et al, 2008), previously applied
to a wide range of NLP tasks (Ranganath et al,
2009; Socher et al, 2011; Chen et al, 2012), with
2N nodes in the middle layer to obtain a deep
representation of our datasets from ?2-BoW input.
The network was trained for 15 epochs. We set the
drop-out rate to 0.0 and 0.3.
Summary of feature sets The feature sets ? BoW,
1478
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
StackOverflow
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Taste
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Appearance
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
Figure 1: Results selecting N features using ?2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE. The
x-axis is logarithmic scale.
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
72
74
76
78
80
82
84
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
Figure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, and
APPEARANCE. The x-axis is logarithmic scale.
Exp and AMT ? are very different. Their character-
istics are presented in Table 1. P (1) is the class dis-
tribution, e.g., the prior probability of positive class.
n is the number of data points, m the number of
features. Finally, ?x is the average density of data
points. One observation is of course that the expert
feature set Exp is much smaller than BoW and AMT,
but note also that the expert features fire about 150
times more often on average than the BoW features.
HI is only a small set of additional features.
3 Results
Experiment #1: BoW vs. Exp and AMT We present
results using all features, as well as results obtained
after selecting k features as ranked by a simple ?2
test. The results using all collected features are pre-
sented in Table 2. The error reduction on STACK-
OVERFLOW when adding crowdsourced features to
our baseline model (BoW+AMT), is 24.3%. On
TASTE, it is 34.2%. On APPEARANCE, it is 41.0%.
The BoW+AMT feature set is bigger than those of
the other models. We therefore report results using
the top-k features as ranked by a simple ?2 test.
The result curves are presented in the three plots in
Fig. 1. With +500 features, BoW+AMT outperforms
the other models by a large margin.
Experiment #2: AMT vs. more baselines The
BoW baseline uses a standard representation that,
while widely used, is usually thought of as a weak
baseline. BoW+HIT did not provide a stronger base-
line. We also show that bigram features, kernel-
based decomposition and deep features do not pro-
vide much stronger baselines either. The result
curves are presented in the three plots in Fig. 2.
BoW+AMT is still significantly better than all other
models with +500 features. Since autoencoders
are consistently worse than denoising autoencoders
(drop-out 0.3), we only plot denoising autoencoders.
4 Conclusion
We presented a new method for deriving feature
representations from crowdsourced annotation tasks
and showed how it leads to 24%-41% error reduc-
tions on answer scoring and multi-aspect sentiment
analysis problems. We saw no significant improve-
ments using features contributed by experts, kernel
representations or learned deep representations.
1479
References
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal
Daume. 2012. Besting the quiz master: Crowdsourc-
ing incremental classification games. In NAACL.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In ACL.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei
Sha. 2012. Marginalized denoising autoencoders for
domain adaptation. In ICML.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English. In Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL.
Pedro Domingos. 2012. A few useful things to know
about machine learning. In CACM.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect re-
views. In ICDM.
Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-
ings. 2012. Sentiment analysis using a novel human
computation game. In Workshop on the People?s Web
Meets NLP, ACL.
Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders. In
ICML.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: detecting flirting and its
misperception in speed-dates. In NAACL.
Richard Socher, Eric Huan, Jeffrey Pennington, Andrew
Ng, and Christopher Manning. 2011. Dynamic pool-
ing and unfolding recursive autoencoders for para-
phrase detection. In NIPS.
Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and
Adam Tauman Kalai. 2011. Adaptively learning the
crowd kernel. In ICML.
Andrea Vedaldi and Andrew Zisserman. 2011. Efficient
additive kernels via explicit feature maps. In CVPR.
Kiri Wagstaff. 2012. Machine learning that matters. In
ICML.
1480
Proceedings of NAACL-HLT 2013, pages 617?626,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Down-stream effects of tree-to-dependency conversions
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,
Hector Martinez, Anders S?gaard
Center for Language Technology, University of Copenhagen
?Institute for Informatics, University of Oslo
Abstract
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
1 Introduction
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School?s Functional Generative Description,
Meaning-Text Theory, or Hudson?s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
?conll07? flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
617
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
Approach in this work
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
?oldLTH? flag set.
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2?21 of the Wall Street Journal section of the
English Treebank (Marcus et al, 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al, 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
Previous work
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/?treebank/tokenizer.sed
618
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
performance, showing that lth leads to superior per-
formance.
Miyao et al (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al, 2011; Tsarfaty et al, 2012).
Hall et al (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
2 Applications
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
619
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SUBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
2.1 Negation resolution
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al, 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
(1) Since we have been so
unfortunate as to miss him [. . . ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
Syntactic
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
Cue-dependent
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
Figure 4: Features used to train the conditional random
field models
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
2.2 Semantic role labeling
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
620
2.3 Statistical machine translation
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: ?3M par-
allel words of news, ?46M parallel words of Eu-
roparl, and ?309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al, 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
11 http://www.statmt.org/wmt11/translation-task.html
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al (2011).
2.4 Sentence compression
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ?2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
2.5 Perspective classification
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ?contribute to mutual understanding
through the open exchange of ideas.? In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
621
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36? 87.52
PTB-23 (UAS) - 90.21 90.12 84.22? 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
Table 1: Results. ?: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = {0.1, 1, 5, 10}.
3 Results and discussion
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al (2012) used Maltparser (Nivre
et al, 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
14http://www.maltparser.org/mco/english parser/engmalt.html
dition to Mate. The pre-trained model was trained
on Sections 2?21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al, 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2?21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. F1 score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p < 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
15http://www.computing.dcu.ie/?jjudge/qtreebank/
622
REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
Figure 5: Examples of SMT output.
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
Figure 6: Examples of sentence compression output.
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
623
ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
L
a
b
e
l
s
srl
neg
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
4 Conclusions
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al (2012).
Acknowledgements
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders S?gaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
624
References
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMNLP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLING.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLING-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMNLP-CoNLL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
2005, pages 523?530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLING.
Yusuke Miyao, Rune S? tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223?260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
625
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369?410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
626
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?7,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Robust Cross-Domain Sentiment Analysis for Low-Resource Languages
Jakob Elming Dirk Hovy Barbara Plank
Centre for Language Technology
University of Copenhagen
zmk867@hum.ku.dk,{dirk,bplank}@cst.dk
Abstract
While various approaches to domain adap-
tation exist, the majority of them requires
knowledge of the target domain, and ad-
ditional data, preferably labeled. For a
language like English, it is often feasible
to match most of those conditions, but in
low-resource languages, it presents a prob-
lem. We explore the situation when nei-
ther data nor other information about the
target domain is available. We use two
samples of Danish, a low-resource lan-
guage, from the consumer review domain
(film vs. company reviews) in a sentiment
analysis task. We observe dramatic perfor-
mance drops when moving from one do-
main to the other. We then introduce a
simple offline method that makes models
more robust towards unseen domains, and
observe relative improvements of more
than 50%.
1 Introduction
Sentiment analysis, the task of determining the
polarity of a text, is a valuable tool for gather-
ing information from the vast amount of opin-
ionated text produced today. It is actively used
in reputation management and consumer assess-
ment (Amig?o et al., 2012; Amig?o et al., 2013).
While supervised approaches achieve reasonable
performance (Mohammad et al., 2013), they are
typically highly domain-dependent. In fact, mov-
ing from one (source) domain to a different (tar-
get) domain will often lead to severe performance
drops (Blitzer et al., 2007; Daum?e et al., 2010).
This is mainly due to the models overfitting the
source (training) data, both in terms of its la-
bel and word distribution. The task of overcom-
ing this tendency is known as domain adaptation
(DA) (Blitzer et al., 2007; Daum?e et al., 2010).
There are three different approaches to DA: in
Supervised DA, labeled training data for the target
domain exists, in Unsupervised DA, data for the
target domain exists, but it is unlabeled. A third,
less investigated scenario is Blind DA: the target
domain is not known at all in advance. Super-
vised DA effectively counteracts domain-bias by
including labeled data from the target domain dur-
ing training, thus preventing overfitting to both the
label and the word distribution of the source. Un-
supervised methods usually rely either on external
data, in the form of gazetteers, dictionaries, or on
unlabeled data from the target domain. While they
do not prevent overfitting to the source domain?s
label distribution, the additional data acts as a reg-
ularizer by introducing a larger vocabulary.
However, both cases presuppose that we already
know the target domain and have data from it. In
many real-world settings, these conditions are not
met, especially when dealing with low-resource
languages. We thus need to regularize our models
independent of the possible target domains. Ef-
fectively, this means that we need to prevent our
models from memorizing the observed label distri-
bution, and from putting too much weight on fea-
tures that are predictive in the source domain, but
might not even be present in the target domain.
In this paper, we investigate sentiment analysis
for Danish, a low-resource language, and therefore
approach it as a Blind DA problem. We perform
experiments on two types of domains, namely re-
views for movies and companies. The challenge
lies in the fact that the label distribution (posi-
tive, negative, neutral) changes dramatically when
moving from one domain to the other, and many
highly predictive words in the company domain
(e.g., ?reliable?) are unlikely to carry over to the
movie domain, and vice versa. To the best of our
knowledge, this is the first study to perform senti-
ment analysis for Danish, a low-resource language
where relevant resources like polarity dictionaries
2
are hard to come by.
We present a simple offline-learning version in-
spired by previous work on corruptions (S?gaard,
2013), which also addresses the sparsity of avail-
able training data. Our method introduces a rela-
tive improvement on out-of-domain performance
by up to 54%.
2 Robust Learning
The main idea behind robust learning is to steer the
model away from overfitting the source domain.
Overfitting can occur either by
1. putting too much weight on certain features
(which might not be present in the target do-
main), or
2. over-using certain labels (since the label dis-
tribution on the target domain might differ).
One approach that has been proven to re-
duce overfitting is data corruption, also known as
dropout training (S?gaard and Johannsen, 2012;
S?gaard, 2013), which is a way of regularizing
the model by randomly leaving out features. In-
tuitively, this approach can be viewed as coercing
the learning algorithm to rely on more general, but
less consistent features. Rather than learning to
mainly trust the features that are highly predictive
for the given training data, the algorithm is encour-
aged to use the less predictive features, since the
highly predictive features might be deleted by the
corruption. Most prior work on dropout regular-
ization (S?gaard and Johannsen, 2012; Wang and
Manning, 2012; S?gaard, 2013) has used online
corruptions, i.e., the specific dropout function is
integrated into the learning objective and thus tied
to the specific learner. Here, we propose a simple
approximation, i.e., a wrapper function that cor-
rupts instances in an off-line fashion based on the
weights learned from a base model. The advan-
tage is that it can be used for any learning func-
tion, thereby abstracting away from the underlying
learner.
2.1 Our approach
Our off-line feature corruption algorithm works as
follows:
1. train an uncorrupted (base) model,
2. create k copies of the training data instances,
3. corrupt copies based on the feature weights of
the base model and an exponential function
(described below), and
4. train a new model on the corrupted training
data.
The advantages of this algorithm compared to
online corruption are
1. it is a wrapper method, so it becomes very
easy to move to a different learning algo-
rithm, and
2. corruption is done based on knowledge from
a full, uncorrupted model, which provides a
better picture of the overfitting.
This comes, however, at the cost of longer training
times, but in a low-resource language training time
is less of an issue.
Specifically, multiple copies of the training data
are used in the corrupted training stage. This re-
sults in each data point appearing in different, cor-
rupted versions, as visualized in Figure 1. The
copying process retains more of the information in
the training data, since it is unlikely that the same
feature is deleted in each copy. In our experiments,
we used k=5. Larger values of k resulted in longer
training times without improving performance.
1 11 1Original
1 1 11 1 111
1
1
1
Corrupted Copies?!?!?111
Figure 1: Example of an original feature vector
and its multiple corrupted copies.
We experiment with a random and a biased
corruption approach. The first approach (S?gaard
and Johannsen, 2012) does not utilize the feature
weight information from the base model, but ran-
domly deletes 10% of the features. We use this
approach to test whether an effect is merely the
result of deleting features.
The biased approach, on the other hand, tar-
gets the most predictive features in the base model
for deletion. We use a function that increases
the probability of deleting a feature exponentially
3
0	 ?
25	 ?
50	 ?
75	 ?
100	 ?
-??0.33	 ? -??0.23	 ? -??0.13	 ? -??0.03	 ? 0.07	 ? 0.17	 ? 0.27	 ? 0.37	 ?
%	 ?
Feature	 ?weight	 ?
Figure 2: The corruption function conditioning the
probability of deleting a feature in a positive in-
stance on its weight in the Scope baseline model.
with its model weight. That is, a highly predic-
tive feature (with a high weight in the model) will
be more likely to be deleted. A feature with a
low weight, on the other hand, has a much lower
chance of being deleted. Figure 2 visualizes the
exponential corruption function used. The func-
tion assigns the lowest weighted feature of the
model zero likelihood of deletion, and the highest
weighted feature a 0.9 likelihood of deletion. In
order to mainly corrupt the highly predictive fea-
tures, the exponential function is shifted to an area
with a steeper gradient. That is, instead of scal-
ing to the exponential function between 0 and 1, it
is scaled to the area between -3 and 2 (parameters
set experimentally on the development set). The
corruption probability p
cor
of deleting a feature f
given a category c is defined as
p
cor
(f |c) =
exp(
w(f |c)?w
min
(c)
w
max
(c)?w
min
(c)
?5?3)?exp(?3)
exp(2)?exp(?3)
? 0.9
(1)
with w(f |c) being the weight of f given the in-
stance category c in the model, and w
min
(c) and
w
max
(c) being the lowest and highest weights of
the model respectively for category c.
3 Experiments
Our experiments use Danish reviews from two do-
mains: movies and companies. The specifications
of the data sets are listed in Table 1 and Figure 3.
The two data sets differ considerably in data size
and label distribution.
DOMAIN SPLIT REVIEWS WORDS
Scope Train 8,718 749,952
Dev 1,198 107,351
Test 2,454 210,367
Total 12,370 1,067,670
Trustpilot Train 170,137 7,180,160
Dev 23,958 1,000,443
Test 48,252 2,040,956
Total 242,347 10,221,559
Table 1: Overview of data set and split sizes in
number of reviews and number of words.
3.1 Data preparation
The movie reviews are downloaded from a Dan-
ish movie website, www.scope.dk. They con-
tain reviews of 829 movies, each rated on a scale
from 1 to 6 stars. The company reviews are
downloaded from a Danish consumer review web-
site, www.trustpilot.dk. They consist of re-
views of 19k companies, each rated between 1 and
5 stars.
Similar to prior work on sentiment analy-
sis (Blitzer et al., 2007), the star ratings are binned
into the three standard categories; positive, neu-
tral, and negative. For the Scope data, a 6 star rat-
ing is considered positive, a 3 or 4 rating neutral,
and a 1 star rating negative. 2 and 5 star ratings are
excluded to retain more distinct categories. For the
Trustpilot data, 5 star reviews are categorized as
positive, 3 stars as neutral, and 1 star as negative.
Similar to Scope data, 2 and 4 stars are excluded.
0%
25%
50%
75%
100%
scope trustpilot
84.85%
27.36%
5.40%
60.85%
9.75%11.79%
negative neutral positive
Figure 3: Label distribution in the two data sets.
Apart from the difference in size, the two data
sets also differ in the distribution of categories (see
Figure 3). This means that a majority label base-
line estimated from one would perform horribly on
4
- N-gram presence for token lengths 1 to 4
- Skip-grams (n-gram with one middle word replaced by *) presence for token lengths 3 to 4
- Character n-gram presence for entire document string for token lengths 1 to 5
- Brown clusters (Brown et al., 1992; Liang, 2005) estimated on the source training data
- Number of words with only upper case characters
- Number of contiguous sequences of question marks, exclamation marks, or both
- Presence of question mark or exclamation mark in last word
- Number of words with characters repeated more than two times e.g. ?sooooo?
- Number of negated contexts using algorithm described in the text
- Most positive, most negative, or same amount of polar words according to a sentiment lexicon
Table 2: Feature set description.
the other domain. For instance, the majority base-
line on Scope (assigning neutral to all instances)
achieves a 5% accuracy on Trustpilot data. Sim-
ilarly, the Trustpilot majority baseline obtains an
accuracy of 27% on Scope data by always assign-
ing positive.
We choose not to balance the data sets, in keep-
ing with the blind DA setup. Knowing the target
label distribution can help greatly, but we can as-
sume no prior knowledge about that. In fact, the
difference in label distribution is one of the ma-
jor challenges when predicting on out-of-domain
data.
3.2 Features
The features we use (described in Table 2) are
inspired by the top performing system from the
SemEval-2013 task on Twitter sentiment analy-
sis (Mohammad et al., 2013).
One main difference is that Mohammad et al.
(2013) had several high-quality sentiment lexicons
at their disposal, shown to be effective. Working
with a low-resource language, we only have ac-
cess to a single lexicon created by an MA student
(containing 2248 positive and 4736 negative word
forms). Our lexicon features are therefore simpler,
i.e., based on whether words are considered pos-
itive or negative in the lexicon, as opposed to the
score-based features in Mohammad et al. (2013).
We adopted the simple negation scope reso-
lution algorithm directly from Mohammad et al.
(2013). Anything appearing in-between a negation
token
1
and the first following punctuation mark is
considered a negated context. This works well for
English, but Danish has different sentence adver-
bial placement, so the negation may also appear
1
We use the following negation markers: ikke, ingen, in-
tet, ingenting, aldrig, hverken. n?ppe.
after the negated constituent. This simple algo-
rithm is therefore less likely to be beneficial in a
Danish system. We plan to extend the system for
better negation handling in future work.
3.3 Corruption
The corruption happens at the feature-instance
level. When we refer to the deletion of a feature
in the following, it does not mean the deletion of
this feature throughout the training data, but the
deletion of a single instance in a feature vector (cf.
Figure 1).
Corrupting the Scope data deleted 9.24% of all
feature instances in the training data. Most fea-
tures are deleted from positive instances (16.7%
of all features) and least from the majority neutral
instances (6.5% of all features). Only 9.4% of the
minority class negative are deleted.
For Trustpilot, the corruption deleted 11.73%
of the feature instances. The pattern is the same
here, though more extreme. The majority positive
class has the fewest features removed (2.2%), the
minority class neutral has 22.8% of its features
deleted, and the negative class has an overwhelm-
ing 35.6% of its features deleted.
The fact that the corruption function does not
take the weight distribution of the individual la-
bels into account, and therefore corrupts the data
of some labels much more than others, does prove
to be a problem. We will get back to this in the
results section.
4 Results
Table 3 shows the results of the experiments. We
report both accuracy and the average f-score for
positive and negative instances (AF).
AF is the official SemEval-2013 metric (Nakov
et al., 2013). It offers a more detailed insight into
5
In-domain Out-of-domain
System Dev set Test set Dev set Test set
Acc. AF Acc. AF Acc. AF Acc. AF
Scope baseline 84.2 75.6 82.4 72.1 35.5 43.3 36.0 44.3
Scope random corrupt 83.1 72.9 82.7 72.8 35.7 43.9 36.2 44.5
Scope biased corrupt 82.7 72.6 81.5 70.6 55.5 48.6 55.5 44.9
Trustpilot baseline 94.8 91.8 94.3 91.2 39.9 45.0 39.9 46.2
Trustpilot random corrupt 94.8 91.7 94.4 91.4 39.8 45.6 40.0 46.0
Trustpilot biased corrupt 93.7 89.0 93.4 89.5 43.6 45.7 43.4 44.7
Table 3: Evaluation on development and test sets measured in accuracy (Acc.) and the average f-score
for positive and negative instances (AF).
the model?s performance on the two ?extreme?
classes, but it is highly skewed, since it ignores the
neutral label. As we have seen in our data, this
can make up the majority of the instances. Ac-
curacy has the advantage that it provides a clear
picture of how often the system makes a correct
prediction, but can be harder to interpret when the
data sets are highly skewed in favor of one class.
The results show that randomly corrupting the
data (cf. S?gaard and Johannsen (2012), Sec. 5)
does not have much influence on the model. Per-
formance on in- and out-of-domain data is similar
to the baseline system. This indicates that we can
not just delete any features to help domain adapta-
tion.
The biased corruption model, on the other hand,
makes informed choices about deleting features.
As expected, this leads to a drop on in-domain
data, since we are underfitting the model. Con-
sidering that the algorithm is targeting the most
important features for this particular domain, the
drop is relatively small, though. The percentage
of features deleted is roughly the same as the 10%
for the random system (see section 3.3).
With the exception of AF on Trustpilot test,
our biased corruption approach always increases
out-of-domain performance. The increase is es-
pecially notable when the model is trained on the
small domain, Scope. On both test and develop-
ment, the corruption approach increases accuracy
more than 50%. On the AF measure, the increase
is smaller, which indicates that most of the in-
crease stems from the neutral category. On the
test set, the f-score for positive labels increases
from 49.1% to 71.2%, neutral increases from
13.5% to 18.4%, but negative decreases from
39.4% to 27.5%. The fact that f-score decreases on
negative indicates that the corruption algorithm
is too aggressive for this category. We previously
saw that this was the category where 35% of the
features are deleted.
The lower degree of overfitting in the corrupted
model is also reflected in the overall label distri-
bution. For the Scope system, the training data
has a negative/neutral/positive distribution (in per-
centages) of 27/61/12. The baseline predictions
on the Trustpilot data has a very similar distribu-
tion of 30/63/7, while the corrupted system results
in a very different distribution of 52/35/13, which
is more similar to the Trustpilot gold distribution
of 85/5/10. The KL divergence between the base-
line system and the Trustpilot data is 1.26, while
for the corrupted system it is 0.46.
5 Related Work
There is a large body of prior work on sen-
timent analysis (Pang and Lee, 2008), ranging
from work on well-edited newswire data using
the MPQA corpus (Wilson et al., 2005), to Ama-
zon reviews (Blitzer et al., 2007), blogs (Kessler
et al., 2010) and user-generated content such as
tweets (Mohammad et al., 2013). All of these
studies worked with English, while this study ? to
the best of our knowledge ? is the first to present
results for Danish.
As far as we are aware of, the only related work
on Danish is Hardt and Wulff (2012). In their ex-
ploratory paper, they investigate whether user pop-
ulations differ systematically in the way they ex-
press sentiment, finding that positive ratings are
far more common in U.S. reviews than in Danish
ones. However, their paper focuses on a quantita-
tive analysis and a single domain (movie reviews),
while we build an actual sentiment classification
system that performs well across domains.
Data corruption has been used for other NLP
6
tasks (S?gaard and Johannsen, 2012; S?gaard,
2013). Our random removal setup is basi-
cally an offline version of the approach presented
in (S?gaard and Johannsen, 2012). Their online
algorithm removes a random subset of the features
in each iteration and was successfully applied to
cross-domain experiments on part-of-speech tag-
ging and document classification. S?gaard (2013)
presents a follow-up online approach that takes
the weights of the current model into considera-
tion, regularizing the most predictive features. Our
biased approach is inspired by this, but has the ad-
vantage that it abstracts away from the underlying
learner.
6 Discussion and Future Work
We investigate cross-domain sentiment analysis
for a low-resource language, Danish. We observe
that performance drops precipitously when train-
ing on one domain and evaluating on the other. We
presented a robust offline-learning approach that
deletes features proportionate to their predictive-
ness. Applied to blind domain adaptation, this cor-
ruption method prevents overfitting to the source
domain, and results in relative improvements of
more than 50%.
In the future, we plan to experiment with in-
tegrating the weight distribution of a label into
the corruption function in order to prevent over-
corrupting of certain labels.
Acknowledgments
We would like to thank Daniel Hardt for host-
ing the Copenhagen Sentiment Analysis Work-
shop and making the data sets available. The last
two authors are supported by the ERC Starting
Grant LOWLANDS No. 313695.
References
Enrique Amig?o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview of
RepLab 2012: Evaluating Online Reputation Man-
agement Systems. In CLEF.
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of RepLab 2013: Eval-
uating Online Reputation Monitoring Systems. In
CLEF.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J. DellaPi-
etra, and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
Hal Daum?e, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In ACL Workshop on Domain Adapta-
tion for NLP.
Daniel Hardt and Julie Wulff. 2012. What is the mean-
ing of 5 *?s? An investigation of the expression and
rating of sentiment. In Proceedings of KONVENS
2012.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In
SemEval-2013.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Anders S?gaard. 2013. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Sida Wang and Christopher D Manning. 2012. Fast
dropout training for logistic regression. In NIPS
workshop on log-linear models.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
7
