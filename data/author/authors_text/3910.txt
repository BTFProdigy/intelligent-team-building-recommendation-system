Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152?159,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation through Global Lexical Selection and
Sentence Reconstruction
Srinivas Bangalore, Patrick Haffner, Stephan Kanthak
AT&T Labs - Research
180 Park Ave, Florham Park, NJ 07932
{srini,haffner,skanthak}@research.att.com
Abstract
Machine translation of a source language
sentence involves selecting appropriate tar-
get language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this pa-
per, we present a novel approach to lexical
selection where the target words are associ-
ated with the entire source sentence (global)
without the need to compute local associa-
tions. Further, we present a technique for
reconstructing the target language sentence
from the selected words. We compare the re-
sults of this approach against those obtained
from a finite-state based statistical machine
translation system which relies on local lex-
ical associations.
1 Introduction
Machine translation can be viewed as consisting of
two subproblems: (a) lexical selection, where appro-
priate target language lexical items are chosen for
each source language lexical item and (b) lexical re-
ordering, where the chosen target language lexical
items are rearranged to produce a meaningful target
language string. Most of the previous work on statis-
tical machine translation, as exemplified in (Brown
et al, 1993), employs word-alignment algorithm
(such as GIZA++ (Och and Ney, 2003)) that pro-
vides local associations between source and target
words. The source-to-target word alignments are
sometimes augmented with target-to-source word
alignments in order to improve precision. Further,
the word-level alignments are extended to phrase-
level alignments in order to increase the extent of
local associations. The phrasal associations compile
some amount of (local) lexical reordering of the tar-
get words ? those permitted by the size of the phrase.
Most of the state-of-the-art machine translation sys-
tems use phrase-level associations in conjunction
with a target language model to produce sentences.
There is relatively little emphasis on (global) lexical
reordering other than the local reorderings permit-
ted within the phrasal alignments. A few exceptions
are the hierarchical (possibly syntax-based) trans-
duction models (Wu, 1997; Alshawi et al, 1998;
Yamada and Knight, 2001; Chiang, 2005) and the
string transduction models (Kanthak et al, 2005).
In this paper, we present an alternate approach to
lexical selection and lexical reordering. For lexical
selection, in contrast to the local approaches of as-
sociating target to source words, we associate tar-
get words to the entire source sentence. The intu-
ition is that there may be lexico-syntactic features of
the source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might be
difficult to exactly associate a target word to a source
word in many situations ? (a) when the translations
are not exact but paraphrases (b) when the target lan-
guage does not have one lexical item to express the
same concept that is expressed by a source word.
Extending word to phrase alignments attempts to ad-
dress some of these situations while alleviating the
noise in word-level alignments.
As a consequence of this global lexical selection
approach, we no longer have a tight association be-
tween source and target language words. The re-
sult of lexical selection is simply a bag of words in
the target language and the sentence has to be recon-
structed using this bag of words. The words in the
bag, however, might be enhanced with rich syntactic
information that could aid in reconstructing the tar-
get sentence. This approach to lexical selection and
152
Translation modelWFSA
BilanguagePhrase Segmented
FSA to FST
Bilanguage
WFSTTransformation
Bilanguage
Reordering
Local Phrase Joint Language
Modeling
Joint Language
Alignment
WordAlignmentSentence AlignedCorpus
Figure 1: Training phases for our system
ConstructionPermutation
Permutation Lattice
Lexical Choice 
FST Composition
Decoding
SourceSentence/
WeightedLattice
Target
Decoding Lexical Reodering
 CompositionFSA Sentence
Model
Translation ModelLanguage
Target
Figure 2: Decoding phases for our system
sentence reconstruction has the potential to circum-
vent limitations of word-alignment based methods
for translation between languages with significantly
different word order (e.g. English-Japanese).
In this paper, we present the details of training
a global lexical selection model using classifica-
tion techniques and sentence reconstruction mod-
els using permutation automata. We also present a
stochastic finite-state transducer (SFST) as an exam-
ple of an approach that relies on local associations
and use it to compare and contrast our approach.
2 SFST Training and Decoding
In this section, we describe each of the components
of our SFST system shown in Figure 1. The SFST
approach described here is similar to the one de-
scribed in (Bangalore and Riccardi, 2000) which has
subsequently been adopted by (Banchs et al, 2005).
2.1 Word Alignment
The first stage in the process of training a lexical se-
lection model is obtaining an alignment function (f )
that given a pair of source (s1s2 . . . sn) and target
(t1t2 . . . tm) language sentences, maps source lan-
guage word subsequences into target language word
subsequences, as shown below.
?i?j(f(si) = tj ? f(si) = ?) (1)
For the work reported in this paper, we have used
the GIZA++ tool (Och and Ney, 2003) which im-
plements a string-alignment algorithm. GIZA++
alignment however is asymmetric in that the word
mappings are different depending on the direction
of alignment ? source-to-target or target-to-source.
Hence in addition to the functions f as shown in
Equation 1 we train another alignment function g :
?j?i(g(tj) = si ? g(tj) = ?) (2)
English: I need to make a collect call
Japanese: ?H ???? ?ff?k $*d ?^%ffcW2
Alignment: 1 5 0 3 0 2 4
Figure 3: Example bilingual texts with alignment in-
formation
I:?H need:?^%ffcW2 to:? make:?ff?k
a:? collect ???? call $*d
Figure 4: Bilanguage strings resulting from align-
ments shown in Figure 3.
2.2 Bilanguage Representation
From the alignment information (see Figure 3), we
construct a bilanguage representation of each sen-
tence in the bilingual corpus. The bilanguage string
consists of source-target symbol pair sequences as
shown in Equation 3. Note that the tokens of a bilan-
guage could be either ordered according to the word
order of the source language or ordered according to
the word order of the target language.
Bf = bf1 bf2 . . . bfm (3)
bfi = (si?1; si, f(si)) if f(si?1) = ?
= (si, f(si?1); f(si)) if si?1 = ?
= (si, f(si)) otherwise
Figure 4 shows an example alignment and the
source-word-ordered bilanguage strings correspond-
ing to the alignment shown in Figure 3.
We also construct a bilanguage using the align-
ment function g similar to the bilanguage using the
alignment function f as shown in Equation 3.
Thus, the bilanguage corpus obtained by combin-
ing the two alignment functions is B = Bf ?Bg.
2.3 Bilingual Phrases and Local Reordering
While word-to-word translation only approximates
the lexical selection process, phrase-to-phrase map-
ping can greatly improve the translation of colloca-
tions, recurrent strings, etc. Using phrases also al-
lows words within the phrase to be reordered into the
correct target language order, thus partially solving
the reordering problem. Additionally, SFSTs can
take advantage of phrasal correlations to improve the
computation of the probability P (WS ,WT ).
The bilanguage representation could result in
some source language phrases to be mapped to ?
153
(empty target phrase). In addition to these phrases,
we compute subsequences of a given length k on the
bilanguage string and for each subsequence we re-
order the target words of the subsequence to be in
the same order as they are in the target language sen-
tence corresponding to that bilanguage string. This
results in a retokenization of the bilanguage into to-
kens of source-target phrase pairs.
2.4 SFST Model
From the bilanguage corpus B, we train an n-gram
language model using standard tools (Goffin et al,
2005). The resulting language model is represented
as a weighted finite-state automaton (S ? T ?
[0, 1]). The symbols on the arcs of this automaton
(si ti) are interpreted as having the source and target
symbols (si:ti), making it into a weighted finite-state
transducer (S ? T?[0, 1]) that provides a weighted
string-to-string transduction from S into T :
T ? = argmax
T
P (si, ti|si?1, ti?1 . . . si?n?1, ti?n?1)
2.5 Decoding
Since we represent the translation model as a
weighted finite-state transducer (TransFST ), the
decoding process of translating a new source in-
put (sentence or weighted lattice (Is)) amounts to
a transducer composition (?) and selection of the
best probability path (BestPath) resulting from the
composition and projecting the target sequence (pi1).
T ? = pi1(BestPath(Is ? TransFST )) (4)
However, we have noticed that on the develop-
ment corpus, the decoded target sentence is typically
shorter than the intended target sentence. This mis-
match may be due to the incorrect estimation of the
back-off events and their probabilities in the train-
ing phase of the transducer. In order to alleviate
this mismatch, we introduce a negative word inser-
tion penalty model as a mechanism to produce more
words in the target sentence.
2.6 Word Insertion Model
The word insertion model is also encoded as a
weighted finite-state automaton and is included in
the decoding sequence as shown in Equation 5. The
word insertion FST has one state and |?T | number
of arcs each weighted with a ? weight representing
the word insertion cost. On composition as shown
in Equation 5, the word insertion model penalizes or
rewards paths which have more words depending on
whether ? is positive or negative value.
T ? = pi1(BestPath(Is?TransFST?WIP )) (5)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2Figure 5: Locally constraint permutation automatonfor a sentence with 4 words and window size of 2.
2.7 Global Reordering
Local reordering as described in Section 2.3 is re-
stricted by the window size k and accounts only for
different word order within phrases. As permuting
non-linear automata is too complex, we apply global
reordering by permuting the words of the best trans-
lation and weighting the result by an n-gram lan-
guage model (see also Figure 2):
T ? = BestPath(perm(T ?) ? LMt) (6)
Even the size of the minimal permutation automa-
ton of a linear automaton grows exponentially with
the length of the input sequence. While decoding by
composition simply resembles the principle of mem-
oization (i.e. here: all state hypotheses of a whole
sentence are kept in memory), it is necessary to ei-
ther use heuristic forward pruning or constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)). We have cho-
sen to constrain permutations here. Figure 5 shows
the resulting minimal permutation automaton for an
input sequence of 4 words and a window size of 2.
Decoding ASR output in combination with global
reordering uses n-best lists or extracts them from lat-
tices first. Each entry of the n-best list is decoded
separately and the best target sentence is picked
from the union of the n intermediate results.
3 Discriminant Models for Lexical
Selection
The approach from the previous section is a genera-
tive model for statistical machine translation relying
on local associations between source and target sen-
tences. Now, we present our approach for a global
lexical selection model based on discriminatively
trained classification techniques. Discriminant mod-
eling techniques have become the dominant method
for resolving ambiguity in speech and other NLP
tasks, outperforming generative models. Discrimi-
native training has been used mainly for translation
model combination (Och and Ney, 2002) and with
the exception of (Wellington et al, 2006; Tillmann
and Zhang, 2006), has not been used to directly train
parameters of a translation model. We expect dis-
criminatively trained global lexical selection models
154
to outperform generatively trained local lexical se-
lection models as well as provide a framework for
incorporating rich morpho-syntactic information.
Statistical machine translation can be formulated
as a search for the best target sequence that maxi-
mizes P (T |S), where S is the source sentence and
T is the target sentence. Ideally, P (T |S) should
be estimated directly to maximize the conditional
likelihood on the training data (discriminant model).
However, T corresponds to a sequence with a ex-
ponentially large combination of possible labels,
and traditional classification approaches cannot be
used directly. Although Conditional Random Fields
(CRF) (Lafferty et al, 2001) train an exponential
model at the sequence level, in translation tasks such
as ours the computational requirements of training
such models are prohibitively expensive.
We investigate two approaches to approximating
the string level global classification problem, using
different independence assumptions. A comparison
of the two approaches is summarized in Table 1.
3.1 Sequential Lexical Choice Model
In the first approach, we formulate a sequential lo-
cal classification problem as shown in Equations 7.
This approach is similar to the SFST approach in
that it relies on local associations between the source
and target words(phrases). We can use a conditional
model (instead of a joint model as before) and the
parameters are determined using discriminant train-
ing which allows for richer conditioning context.
P (T |S) =
?N
i=1 P (ti|?(S, i)) (7)
where ?(S, i) is a set of features extracted from the
source string S (shortened as ? in the rest of the
section).
3.2 Bag-of-Words Lexical Choice Model
The sequential lexical choice model described in
the previous section treats the selection of a lexical
choice for a source word in the local lexical context
as a classification task. The data for training such
models is derived from word alignments obtained
by e.g. GIZA++. The decoded target lexical items
have to be further reordered, but for closely related
languages the reordering could be incorporated into
correctly ordered target phrases as discussed previ-
ously.
For pairs of languages with radically different
word order (e.g. English-Japanese), there needs to
be a global reordering of words similar to the case
in the SFST-based translation system. Also, for such
differing language pairs, the alignment algorithms
such as GIZA++ perform poorly.
These observations prompted us to formulate the
lexical choice problem without the need for word
alignment information. We require a sentence
aligned corpus as before, but we treat the target sen-
tence as a bag-of-words or BOW assigned to the
source sentence. The goal is, given a source sen-
tence, to estimate the probability that we find a given
word in the target sentence. This is why, instead of
producing a target sentence, what we initially obtain
is a target bag of words. Each word in the target vo-
cabulary is detected independently, so we have here
a very simple use of binary static classifiers. Train-
ing sentence pairs are considered as positive exam-
ples when the word appears in the target, and neg-
ative otherwise. Thus, the number of training ex-
amples equals the number of sentence pairs, in con-
trast to the sequential lexical choice model which
has one training example for each token in the bilin-
gual training corpus. The classifier is trained with n-
gram features (BOgrams(S)) from the source sen-
tence. During decoding the words with conditional
probability greater than a threshold ? are considered
as the result of lexical choice decoding.
BOW ?T = {t|P (t|BOgrams(S)) > ?} (8)
For reconstructing the proper order of words in
the target sentence we consider all permutations of
words in BOW ?T and weight them by a target lan-
guage model. This step is similar to the one de-
scribed in Section 2.7. The BOW approach can also
be modified to allow for length adjustments of tar-
get sentences, if we add optional deletions in the fi-
nal step of permutation decoding. The parameter ?
and an additional word deletion penalty can then be
used to adjust the length of translated outputs. In
Section 6, we discuss several issues regarding this
model.
4 Choosing the classifier
This section addresses the choice of the classifi-
cation technique, and argues that one technique
that yields excellent performance while scaling well
is binary maximum entropy (Maxent) with L1-
regularization.
4.1 Multiclass vs. Binary Classification
The Sequential and BOW models represent two dif-
ferent classification problems. In the sequential
model, we have a multiclass problem where each
class ti is exclusive, therefore, all the classifier out-
puts P (ti|?) must be jointly optimized such that
155
Table 1: A comparison of the sequential and bag-of-words lexical choice models
Sequential Lexical Model Bag-of-Words Lexical Model
Output target Target word for each source position i Target word given a source sentence
Input features BOgram(S, i? d, i+ d) : bag of n-grams BOgram(S, 0, |S|): bag of n-grams
in source sentence in the interval [i? d, i+ d] in source sentence
Probabilities P (ti|BOgram(S, i? d, i+ d)) P (BOW (T )|BOgram(S, 0, |S|))
Independence assumption between the labels
Number of classes One per target word or phrase
Training samples One per source token One per sentence
Preprocessing Source/Target word alignment Source/Target sentence alignment
?
i P (ti|?) = 1. This can be problematic: with
one classifier per word in the vocabulary, even allo-
cating the memory during training may exceed the
memory capacity of current computers.
In the BOW model, each class can be detected
independently, and two different classes can be de-
tected at the same time. This is known as the 1-vs-
other scheme. The key advantage over the multiclass
scheme is that not all classifiers have to reside in
memory at the same time during training which al-
lows for parallelization. Fortunately for the sequen-
tial model, we can decompose a multiclass classifi-
cation problem into separate 1-vs-other problems. In
theory, one has to make an additional independence
assumption and the problem statement becomes dif-
ferent. Each output label t is projected into a bit
string with components bj(t) where probability of
each component is estimated independently:
P (bj(t)|?) = 1? P (b?j(t)|?) = 11 + e?(?j??j?)??
In practice, despite the approximation, the 1-vs-
other scheme has been shown to perform as well as
the multiclass scheme (Rifkin and Klautau, 2004).
As a consequence, we use the same type of binary
classifier for the sequential and the BOW models.
The excellent results recently obtained with the
SEARN algorithm (Daume et al, 2007) also sug-
gest that binary classifiers, when properly trained
and combined, seem to be capable of matching more
complex structured output approaches.
4.2 Geometric vs. Probabilistic Interpretation
We separate the most popular classification tech-
niques into two broad categories:
? Geometric approaches maximize the width of
a separation margin between the classes. The
most popular method is the Support Vector Ma-
chine (SVM) (Vapnik, 1998).
? Probabilistic approaches maximize the con-
ditional likelihood of the output class given
the input features. This logistic regression is
also called Maxent as it finds the distribution
with maximum entropy that properly estimates
the average of each feature over the training
data (Berger et al, 1996).
In previous studies, we found that the best accuracy
is achieved with non-linear (or kernel) SVMs, at the
expense of a high test time complexity, which is un-
acceptable for machine translation. Linear SVMs
and regularized Maxent yield similar performance.
In theory, Maxent training, which scales linearly
with the number of examples, is faster than SVM
training, which scales quadratically with the num-
ber of examples. In our first experiments with lexi-
cal choice models, we observed that Maxent slightly
outperformed SVMs. Using a single threshold with
SVMs, some classes of words were over-detected.
This suggests that, as theory predicts, SVMs do not
properly approximate the posterior probability. We
therefore chose to use Maxent as the best probability
approximator.
4.3 L1 vs. L2 regularization
Traditionally, Maxent is regularized by imposing a
Gaussian prior on each weight: this L2 regulariza-
tion finds the solution with the smallest possible
weights. However, on tasks like machine translation
with a very large number of input features, a Lapla-
cian L1 regularization that also attempts to maxi-
mize the number of zero weights is highly desirable.
A new L1-regularized Maxent algorithms was
proposed for density estimation (Dudik et al, 2004)
and we adapted it to classification. We found this al-
gorithm to converge faster than the current state-of-
the-art in Maxent training, which is L2-regularized
L-BFGS (Malouf, 2002)1. Moreover, the number of
trained parameters is considerably smaller.
5 Data and Experiments
We have performed experiments on the IWSLT06
Chinese-English training and development sets from
1We used the implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
156
Table 2: Statistics of training and development data from 2005/2006 (? = first of multiple translations only).
Training (2005) Dev 2005 Dev 2006
Chinese English Chinese English Chinese English
Sentences 46,311 506 489
Running Words 351,060 376,615 3,826 3,897 5,214 6,362?
Vocabulary 11,178 11,232 931 898 1,136 1,134?
Singletons 4,348 4,866 600 538 619 574?
OOVs [%] - - 0.6 0.3 0.9 1.0
ASR WER [%] - - - - 25.2 -
Perplexity - - 33 - 86 -
# References - - 16 7
2005 and 2006. The data are traveler task ex-
pressions such as seeking directions, expressions in
restaurants and travel reservations. Table 2 presents
some statistics on the data sets. It must be noted
that while the 2005 development set matches the
training data closely, the 2006 development set has
been collected separately and shows slightly differ-
ent statistics for average sentence length, vocabulary
size and out-of-vocabulary words. Also the 2006
development set contains no punctuation marks in
Chinese, but the corresponding English translations
have punctuation marks. We also evaluated our
models on the Chinese speech recognition output
and we report results using 1-best with a word er-
ror rate of 25.2%.
For the experiments, we tokenized the Chinese
sentences into character strings and trained the mod-
els discussed in the previous sections. Also, we
trained a punctuation prediction model using Max-
ent framework on the Chinese character strings in
order to insert punctuation marks into the 2006 de-
velopment data set. The resulting character string
with punctuation marks is used as input to the trans-
lation decoder. For the 2005 development set, punc-
tuation insertion was not needed since the Chinese
sentences already had the true punctuation marks.
In Table 3 we present the results of the three dif-
ferent translation models ? FST, Sequential Maxent
and BOW Maxent. There are a few interesting ob-
servations that can be made based on these results.
First, on the 2005 development set, the sequential
Maxent model outperforms the FST model, even
though the two models were trained starting from
the same GIZA++ alignment. The difference, how-
ever, is due to the fact that Maxent models can cope
with increased lexical context2 and the parameters
of the model are discriminatively trained. The more
surprising result is that the BOW Maxent model sig-
nificantly outperforms the sequential Maxent model.
2We use 6 words to the left and right of a source word for
sequential Maxent, but only 2 preceding source and target words
for FST approach.
The reason is that the sequential Maxent model re-
lies on the word alignment, which, if erroneous, re-
sults in incorrect predictions by the sequential Max-
ent model. The BOW model does not rely on the
word-level alignment and can be interpreted as a dis-
criminatively trained model of dictionary lookup for
a target word in the context of a source sentence.
Table 3: Results (mBLEU) scores for the three dif-
ferent models on the transcriptions for development
set 2005 and 2006 and ASR 1-best for development
set 2006.
Dev 2005 Dev 2006
Text Text ASR 1-best
FST 51.8 19.5 16.5
Seq. Maxent 53.5 19.4 16.3
BOW Maxent 59.9 19.3 16.6
As indicated in the data release document, the
2006 development set was collected differently com-
pared to the one from 2005. Due to this mis-
match, the performance of the Maxent models are
not very different from the FST model, indicating
the lack of good generalization across different gen-
res. However, we believe that the Maxent frame-
work allows for incorporation of linguistic features
that could potentially help in generalization across
genres. For translation of ASR 1-best, we see a sys-
tematic degradation of about 3% in mBLEU score
compared to translating the transcription.
In order to compensate for the mismatch between
the 2005 and 2006 data sets, we computed a 10-fold
average mBLEU score by including 90% of the 2006
development set into the training set and using 10%
of the 2006 development set for testing, each time.
The average mBLEU score across these 10 runs in-
creased to 22.8.
In Figure 6 we show the improvement of mBLEU
scores with the increase in permutation window size.
We had to limit to a permutation window size of 10
due to memory limitations, even though the curve
has not plateaued. We anticipate using pruning tech-
niques we can increase the window size further.
157
 0.46
 0.48
 0.5
 0.52
 0.54
 0.56
 0.58
 0.6
 6  6.5  7  7.5  8  8.5  9  9.5  10Permutation Window SizeFigure 6: Improvement in mBLEU score with the
increase in size of the permutation window
5.1 United Nations and Hansard Corpora
In order to test the scalability of the global lexical
selection approach, we also performed lexical se-
lection experiments on the United Nations (Arabic-
English) corpus and the Hansard (French-English)
corpus using the SFST model and the BOW Maxent
model. We used 1,000,000 training sentence pairs
and tested on 994 test sentences for the UN corpus.
For the Hansard corpus we used the same training
and test split as in (Zens and Ney, 2004): 1.4 million
training sentence pairs and 5432 test sentences. The
vocabulary sizes for the two corpora are mentioned
in Table 4. Also in Table 4, are the results in terms of
F-measure between the words in the reference sen-
tence and the decoded sentences. We can see that the
BOW model outperforms the SFST model on both
corpora significantly. This is due to a systematic
10% relative improvement for open class words, as
they benefit from a much wider context. BOW per-
formance on close class words is higher for the UN
corpus but lower for the Hansard corpus.
Table 4: Lexical Selection results (F-measure) on
the Arabic-English UN Corpus and the French-
English Hansard Corpus. In parenthesis are F-
measures for open and closed class lexical items.
Corpus Vocabulary SFST BOW
Source Target
UN 252,571 53,005 64.6 69.5
(60.5/69.1) (66.2/72.6)
Hansard 100,270 78,333 57.4 60.8
(50.6/67.7) (56.5/63.4)
6 Discussion
The BOW approach is promising as it performs rea-
sonably well despite considerable losses in the trans-
fer of information between source and target lan-
guage. The first and most obvious loss is about word
position. The only information we currently use to
restore the target word position is the target language
model. Information about the grammatical role of a
word in the source sentence is completely lost. The
language model might fortuitously recover this in-
formation if the sentence with the correct grammat-
ical role for the word happens to be the maximum
likelihood sentence in the permutation automaton.
We are currently working toward incorporating
syntactic information on the target words so as to be
able to recover some of the grammatical role infor-
mation lost in the classification process. In prelimi-
nary experiments, we have associated the target lex-
ical items with supertag information (Bangalore and
Joshi, 1999). Supertags are labels that provide linear
ordering constraints as well as grammatical relation
information. Although associating supertags to tar-
get words increases the class set for the classifier, we
have noticed that the degradation in the F-score is
on the order of 3% across different corpora. The su-
pertag information can then be exploited in the sen-
tence construction process. The use of supertags in
phrase-based SMT system has been shown to im-
prove results (Hassan et al, 2006).
A less obvious loss is the number of times a word
or concept appears in the target sentence. Func-
tion words like ?the? and ?of? can appear many
times in an English sentence. In the model dis-
cussed in this paper, we index each occurrence of the
function word with a counter. In order to improve
this method, we are currently exploring a technique
where the function words serve as attributes (e.g.
definiteness, tense, case) on the contentful lexical
items, thus enriching the lexical item with morpho-
syntactic information.
A third issue concerning the BOW model is the
problem of synonyms ? target words which translate
the same source word. Suppose that in the training
data, target words t1 and t2 are, with equal probabil-
ity, translations of the same source word. Then, in
the presence of this source word, the probability to
detect the corresponding target word, which we as-
sume is 0.8, will be, because of discriminant learn-
ing, split equally between t1 and t2, that is 0.4 and
0.4. Because of this synonym problem, the BOW
threshold ? has to be set lower than 0.5, which is
observed experimentally. However, if we set the
threshold to 0.3, both t1 and t2 will be detected in
the target sentence, and we found this to be a major
source of undesirable insertions.
The BOW approach is different from the pars-
ing based approaches (Melamed, 2004; Zhang and
Gildea, 2005; Cowan et al, 2006) where the transla-
tion model tightly couples the syntactic and lexical
items of the two languages. The decoupling of the
158
two steps in our model has the potential for gener-
ating paraphrased sentences not necessarily isomor-
phic to the structure of the source sentence.
7 Conclusions
We view machine translation as consisting of lexi-
cal selection and lexical reordering steps. These two
steps need not necessarily be sequential and could be
tightly integrated. We have presented the weighted
finite-state transducer model of machine translation
where lexical choice and a limited amount of lexical
reordering are tightly integrated into a single trans-
duction. We have also presented a novel approach
to translation where these two steps are loosely cou-
pled and the parameters of the lexical choice model
are discriminatively trained using a maximum en-
tropy model. The lexical reordering model in this
approach is achieved using a permutation automa-
ton. We have evaluated these two approaches on the
2005 and 2006 IWSLT development sets and shown
that the techniques scale well to Hansard and UN
corpora.
References
H. Alshawi, S. Bangalore, and S. Douglas. 1998. Automatic
acquisition of hierarchical transduction models for machine
translation. In ACL, Montreal, Canada.
R.E. Banchs, J.M. Crego, A. Gispert, P. Lambert, and J.B.
Marino. 2005. Statistical machine translation of euparl data
by using bilingual n-grams. In Workshop on Building and
Using Parallel Texts. ACL.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An ap-
proach to almost parsing. Computational Linguistics, 25(2).
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state
models for spoken language machine translation. In Pro-
ceedings of the Workshop on Embedded Machine Transla-
tion Systems, pages 52?59.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent.
1996. A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39?71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The
Mathematics of Machine Translation: Parameter Estimation.
Computational Linguistics, 16(2):263?312.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proceedings of the ACL Con-
ference, Ann Arbor, MI.
B. Cowan, I. Kucerova, and M. Collins. 2006. A discrimi-
native model for tree-to-tree translation. In Proceedings of
EMNLP.
H. Daume, J. Langford, and D. Marcu. 2007. Search-based
structure prediction. submitted to Machine Learning Jour-
nal.
M. Dudik, S. Phillips, and R.E. Schapire. 2004. Perfor-
mance Guarantees for Regularized Maximum Entropy Den-
sity Estimation. In Proceedings of COLT?04, Banff, Canada.
Springer Verlag.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje,
S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&T WATSON Speech Recognizer. In Pro-
ceedings of ICASSP, Philadelphia, PA.
H. Hassan, M. Hearne, K. Sima?an, and A. Way. 2006. Syntac-
tic phrase-based statistical machine translation. In Proceed-
ings of IEEE/ACL first International Workshop on Spoken
Language Technology (SLT), Aruba, December.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 167?174, Ann Ar-
bor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, San Fran-
cisco, CA.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL-
2002, pages 49?55. Taipei, Taiwan.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings of ACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings of ACL.
F.J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-
vs-all classification. Journal of Machine Learning Research,
pages 101?141.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for statistical mt. In COLING-ACL.
V.N. Vapnik. 1998. Statistical Learning Theory. John Wiley &
Sons.
B. Wellington, J. Turian, C. Pike, and D. Melamed. 2006. Scal-
able purely-discriminative training for word and tree trans-
ducers. In AMTA.
D. Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proceedings of 39th ACL.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proceedings of HLT-NAACL,
pages 257?264, Boston, MA.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inver-
sion transduction grammar for alignment. In Proceedings of
ACL.
159
FSA: An Efficient and Flexible C++ Toolkit for Finite State Automata
Using On-Demand Computation
Stephan Kanthak and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen ? University of Technology
52056 Aachen, Germany
{kanthak,ney}@informatik.rwth-aachen.de
Abstract
In this paper we present the RWTH FSA toolkit ? an
efficient implementation of algorithms for creating
and manipulating weighted finite-state automata.
The toolkit has been designed using the principle
of on-demand computation and offers a large range
of widely used algorithms. To prove the superior
efficiency of the toolkit, we compare the implemen-
tation to that of other publically available toolkits.
We also show that on-demand computations help to
reduce memory requirements significantly without
any loss in speed. To increase its flexibility, the
RWTH FSA toolkit supports high-level interfaces
to the programming language Python as well as a
command-line tool for interactive manipulation of
FSAs. Furthermore, we show how to utilize the
toolkit to rapidly build a fast and accurate statisti-
cal machine translation system. Future extensibility
of the toolkit is ensured as it will be publically avail-
able as open source software.
1 Introduction
Finite-state automata (FSA) methods proved to el-
egantly solve many difficult problems in the field
of natural language processing. Among the most
recent ones are full and lazy compilation of the
search network for speech recognition (Mohri et
al., 2000a), integrated speech translation (Vidal,
1997; Bangalore and Riccardi, 2000), speech sum-
marization (Hori et al, 2003), language modelling
(Allauzen et al, 2003) and parameter estimation
through EM (Eisner, 2001) to mention only a few.
From this list of different applications it is clear that
there is a high demand for generic tools to create
and manipulate FSAs.
In the past, a number of toolkits have been pub-
lished, all with different design principles. Here, we
give a short overview of toolkits that offer an almost
complete set of algorithms:
? The FSM LibraryTM from AT&T (Mohri et
al., 2000b) is judged the most efficient im-
plementation, offers various semirings, on-
demand computation and many algorithms, but
is available only in binary form with a propri-
etary, non commercial license.
? FSA6.1 from (van Noord, 2000) is imple-
mented in Prolog. It is licensed under the terms
of the (GPL, 1991).
? The WFST toolkit from (Adant, 2000) is built
on top of the Automaton Standard Template
Library (LeMaout, 1998) and uses C++ tem-
plate mechanisms for efficiency and flexibil-
ity, but lacks on-demand computation. Also
licensed under the terms of the (GPL, 1991).
This paper describes a highly efficient new im-
plementation of a finite-state automata toolkit that
uses on-demand computation. Currently, it is
being used at the Lehrstuhl fu?r Informatik VI,
RWTH Aachen in different speech recognition
and translation research applications. The toolkit
will be available under an open source license
(GPL, 1991) and can be obtained from our website
http://www-i6.informatik.rwth-aachen.de.
The remaining part of the paper is organized
as follows: Section 2 will give a short introduc-
tion to the theory of finite-state automata to re-
call part of the terminology and notation. We will
also give a short explanation of composition which
we use as an exemplary object of study in the fol-
lowing sections. In Section 2.3 we will discuss
the locality of algorithms defined on finite-state au-
tomata. This forms the basis for implementations
using on-demand computations. Then the RWTH
FSA toolkit implementation is detailed in Section
3. In Section 4.1 we will compare the efficiency of
different toolkits. As a showcase for the flexibility
we show how to use the toolkit to build a statistical
machine translation system in Section 4.2. We con-
clude the paper with a short summary in Section 5
and discuss some possible future extensions in Sec-
tion 6.
2 Finite-State Automata
2.1 Weighted Finite-State Transducer
The basic theory of weighted finite-state automata
has been reviewed in numerous papers (Mohri,
1997; Allauzen et al, 2003). We will introduce the
notation briefly.
A semiring (K,?,?, 0, 1) is a structure with a
set K and two binary operations ? and ? such
that (K,?, 0) is a commutative monoid, (K,?, 1)
is a monoid and ? distributes over ? and 0 ?
x = x ? 0 = 0 for any x ? K. We will
also associate the term weights with the elements
of a semiring. Semirings that are frequently used
in speech recognition are the positive real semir-
ing (IR?{??,+?},?log,+,+?, 0) with a?log
b = ?log(e?a + e?b) and the tropical semiring
(IR?{??,+?},min,+,+?, 0) representing the
well-known sum and maximum weighted path cri-
teria.
A weighted finite-state transducer (Q,? ?
{?},? ? {?},K,E, i, F, ?, ?) is a structure with a
set Q of states1, an alphabet ? of input symbols,
an alphabet ? of output symbols, a weight semir-
ing K (we assume it k-closed here for some algo-
rithms as described in (Mohri and Riley, 2001)), a
set E ? Q ? (? ? {?}) ? (? ? {?}) ?K ? Q of
arcs, a single initial state i with weight ? and a set of
final states F weighted by the function ? : F ? K.
To simplify the notation we will also denote with
QT and ET the set of states and arcs of a trans-
ducer T. A weighted finite-state acceptor is simply
a weighted finite-state transducer without the output
alphabet.
2.2 Composition
As we will refer to this example throughout the pa-
per we shortly review the composition algorithm
here. Let T1 : ????? ? K and T2 : ????? ? K
be two transducers defined over the same semiring
K. Their composition T1 ? T2 realizes the function
T : ????? ? K and the theory has been described
in detail in (Pereira and Riley, 1996).
For simplification purposes, let us assume that the
input automata are ?-free and S = (Q1?Q2,?,?
, empty) is a stack of state tuples of T1 and T2 with
push, pop and empty test operations. A non lazy
version of composition is shown in Figure 1.
Composition of automata containing ? labels is
more complex and can be solved by using an in-
termediate filter transducer that also has been de-
scribed in (Pereira and Riley, 1996).
1we do not restrict this to be a finite set as most algorithms
of the lazy implementation presented in this paper also support
a virtually infinite set
T = T1 ? T2 :
i = (i1, i2)
S ? (i1, i2)
while not S empty
(s1, s2) ? S
QT = QT ? (s1, s2)
foreach (s1, i1, o1, w1, t1) ? ET1
foreach (s2, i2, o2, w2, t2) ? ET2 with o1 = i2
ET = ET ? ((s1, s2), i1, o2, w1 ? w2, (t1, t2))
if (t1, t2) 6? QT then S ? (t1, t2)
Figure 1: Simplified version of composition (as-
sumes ?-free input transducers).
What we can see from the pseudo-code above is
that composition uses tuples of states of the two in-
put transducers to describe states of the target trans-
ducer. Other operations defined on weighted finite-
state automata use different abstract states. For
example transducer determinization (Mohri, 1997)
uses a set of pairs of states and weights. However,
it is more convenient to use integers as state indices
for an implementation. Therefore algorithms usu-
ally maintain a mapping from abstract states to in-
teger state indices. This mapping has linear mem-
ory requirements of O(|QT |) which is quite attrac-
tive, but that depends on the structure of the abstract
states. Especially in case of determinization where
the size of an abstract state may vary, the complex-
ity is no longer linear in general.
2.3 Local Algorithms
Mohri and colleagues pointed out (Mohri et al,
2000b) that a special class of transducer algorithms
can be computed on demand. We will give a more
detailed analysis here. We focus on algorithms that
produce a single transducer and refer to them as al-
gorithmic transducers.
Definition: Let ? be the input configuration of
an algorithm A(?) that outputs a single finite-state
transducer T. Additionally, let M : S ? QT be
a one-to-one mapping from the set of abstract state
descriptions S that A generates onto the set of states
of T . We call A local iff for all states s ? QT A
can generate a state s of T and all outgoing arcs
(s, i, o, w, s?) ? ET , depending only on its abstract
state M?1(s) and the input configuration ?.
With the preceding definition it is quite easy to
prove the following lemma:
Lemma: An algorithm A that has the local prop-
erty can be built on demand starting with the ini-
tial state iTA of its associated algorithmic transducer
TA.
Proof: For the proof it is sufficient to show that
we can generate and therefore reach all states of TA.
Let S be a stack of states of TA that we still have
to process. Due to the one-to-one mapping M we
can map each state of TA back to an abstract state
of A. By definition the abstract state is sufficient to
generate the complete state and its outgoing arcs.
We then push those target states of all outgoing arcs
onto the stack S that have not yet been processed.
As TA is finite the traversal ends after all states of
TA as been processed exactly once. 2
Algorithmic transducers that can be computed
on-demand are also called lazy or virtual transduc-
ers. Note, that due to the local property the set of
states does not necessarily be finite anymore.
3 The Toolkit
The current implementation is the second version of
this toolkit. For the first version ? which was called
FSM ? we opted for using C++ templates to gain ef-
ficiency, but algorithms were not lazy. It turned out
that the implementation was fast, but many opera-
tions wasted a lot of memory as their resulting trans-
ducer had been fully expanded in memory. How-
ever, we plan to also make this initial version publi-
cally available.
The design principles of the second version of the
toolkit, which we will call FSA, are:
? decoupling of data structures and algorithms,
? on-demand computation for increased memory
efficiency,
? low computational costs,
? an abstract interface to alphabets to support
lazy mappings from strings to indices for arc
labels,
? an abstract interface to semirings (should be k-
closed for at least some algorithms),
? implementation in C++, as it is fast, ubiquitous
and well-known by many other researchers,
? easy to use interfaces.
3.1 The C++ Library Implementation
We use the lemma from Section 2.3 to specify an
interface for lazy algorithmic transducers directly.
The code written in pseudo-C++ is given in Figure
2. Note that all lazy algorithmic transducers are de-
rived from the class Automaton.
The lazy interface also has disadvantages. The
virtual access to the data structure might slow com-
putations down, and obtaining global information
about the automaton becomes more complicated.
For example the size of an automaton can only be
class Automaton {
public:
struct Arc {
StateId target();
Weight weight();
LabelId input();
LabelId output();
};
struct State {
StateId id();
Weight weight();
ConstArcIterator arcsBegin();
ConstArcIterator arcsEnd();
};
virtual R<Alphabet> inputAlphabet();
virtual R<Alphabet> outputAlphabet();
virtual StateId initialState();
virtual R<State> getState(StateId);
};
Figure 2: Pseudo-C++ code fragment for the ab-
stract datatype of transducers. Note that R<T>
refers to a smart pointer of T.
computed by traversing it. Therefore central al-
gorithms of the RWTH FSA toolkit are the depth-
first search (DFS) and the computation of strongly
connected components (SCC). Efficient versions of
these algorithms are described in (Mehlhorn, 1984)
and (Cormen et al, 1990).
It is very costly to store arbitrary types as arc la-
bels within the arcs itself. Therefore the RWTH
FSA toolkit offers alphabets that define mappings
between strings and label indices. Alphabets are
implemented using the abstract interface shown in
Figure 4. With alphabets arcs only need to store
the abstract label indices. The interface for alpha-
bets is defined using a single constant: for each la-
bel index an alphabet reports it must ensure to al-
ways deliver the same symbol on request through
getSymbol().
class Alphabet {
public:
virtual LabelId begin();
virtual LabelId end();
virtual LabelId next(LabelId);
virtual string getSymbol(LabelId);
};
Figure 4: Pseudo-C++ code fragment for the ab-
stract datatype of alphabets.
3.2 Algorithms
The current implementation of the toolkit offers a
wide range of well-known algorithms defined on
weighted finite-state transducers:
? basic operations
sort (by input labels, output labels or by to-
compose(T1, T2) = simple-compose( cache(sort-output(map-output(T1, AT2,I))),
cache(sort-input(T2)))
Figure 3: Optimized composition where AT2,I denotes the input alphabet of T2. Six algorithmic transducers
are used to gain maximum efficiency. Mapping of arc labels is necessary as symbol indices may differ
between alphabets.
tal arc), map-input and -output labels sym-
bolically (as the user expects that two alpha-
bets match symbolically, but their mapping
to label indices may differ), cache (helps to
reduce computations with lazy implementa-
tions), topologically-sort states
? rational operations
project-input, project-output, transpose (also
known as reversal: calculates an equivalent au-
tomaton with the adjacency matrix being trans-
posed), union, concat, invert
? classical graph operations
depth-first search (DFS), single-source short-
est path (SSSP), connect (only keep accessi-
ble and coaccessible state), strongly connected
components (SCCs)
? operations on relations of sets
compose (filtered), intersect, complement
? equivalence transformations
determinize, minimize, remove-epsilons
? search algorithms
best, n-best
? weight/probability-based algorithms
prune (based on forward/backward state po-
tentials), posterior, push (push weights toward
initial/final states), failure (given an accep-
tor/transducer defined over the tropical semir-
ing converts ?-transitions to failure transitions)
? diagnostic operations
count (counts states, final states, different arc
types, SCCs, alphabet sizes, . . .)
? input/output operations
supported input and/or output formats are:
AT&T (currently, ASCII only), binary (fast,
uses fixed byte-order), XML (slower, any en-
coding, fully portable), memory-mapped
(also on-demand), dot (AT&T graphviz)
We will discuss some details and refer to the pub-
lication of the algorithms briefly. Most of the basic
operations have a straigthforward implementation.
As arc labels are integers in the implementation
and their meaning is bound to an appropriate sym-
bolic alphabet, there is the need for symbolic map-
ping between different alphabets. Therefore the
toolkit provides the lazy map-input and map-output
transducers, which map the input and output arc in-
dices of an automaton to be compatible with the in-
dices of another given alphabet.
The implementations of all classical graph algo-
rithms are based on the descriptions of (Mehlhorn,
1984) and (Cormen et al, 1990) and (Mohri and Ri-
ley, 2001) for SSSP. The general graph algorithms
DFS and SCC are helpful in the realisation of many
other operations, examples are: transpose, connect
and count. However, counting the number of states
of an automaton or the number of symbols of an al-
phabet is not well-defined in case of an infinite set
of states or symbols.
SSSP and transpose are the only two algorithms
without a lazy implementation. The result of SSSP
is a list of state potentials (see also (Mohri and Ri-
ley, 2001)). And a lazy implementation for trans-
pose would be possible if the data structures provide
lists of both successor and predecessor arcs at each
state. This needs either more memory or more com-
putations and increases the size of the abstract inter-
face for the lazy algorithms, so as a compromise we
omitted this.
The implementations of compose (Pereira and
Riley, 1996), determinize (Mohri, 1997), minimize
(Mohri, 1997) and remove-epsilons (Mohri, 2001)
use more refined methods to gain efficiency. All
use at least the lazy cache transducer as they re-
fer to states of the input transducer(s) more than
once. With respect to the number of lazy trans-
ducers involved in computing the result, compose
has the most complicated implementation. Given
the implementations for the algorithmic transduc-
ers cache, map-output, sort-input, sort-output and
simple-compose that assumes arc labels to be com-
patible and sorted in order to perform matching as
fast as possible, the final implementation of com-
pose in the RWTH FSA toolkit is given in figure 3.
So, the current implementation of compose uses 6
algorithmic transducers in addition to the two input
automata. Determinize additionally uses lazy cache
and sort-input transducers.
The search algorithms best and n-best are based
on (Mohri and Riley, 2002), push is based on (Mohri
and Riley, 2001) and failure mainly uses ideas from
(Allauzen et al, 2003). The algorithms posterior
and prune compute arc posterior probabilities and
prune arcs with respect to them. We believe they
are standard algorithms defined on probabilistic net-
works and they were simply ported to the frame-
work of weighted finite-state automata.
Finally, the RWTH FSA toolkit can be loosely
interfaced to the AT&T FSM LibraryTM through
its ASCII-based input/output format. In addition,
a new XML-based file format primarly designed as
being human readable and a fast binary file format
are also supported. All file formats support optional
on-the-fly compression using gzip.
3.3 High-Level Interfaces
In addition to the C++ library level interface the
toolkit also offers two high-level interfaces: a
Python interface, and an interactive command-line
interface.
The Python interface has been built using the
SWIG interface generator (Beazley et al, 1996)
and enables rapid development of larger applica-
tions without lengthy compilation of C++ code. The
command-line interface comes handy for quickly
applying various combinations of algorithms to
transducers without writing any line of code at all.
As the Python interface is mainly identical to the
C++ interface we will only give a short impression
of how to use the command-line interface.
The command-line interface is a single exe-
cutable and uses a stack-based execution model
(postfix notation) for the application of operations.
This is different from the pipe model that AT&T
command-line tools use. The disadvantage of us-
ing pipes is that automata must be serialized and
get fully expanded by the next executable in chain.
However, an advantage of multiple executables is
that memory does not get fragmented through the
interaction of different algorithms.
With the command-line interface, operations are
applied to the topmost transducers of the stack and
the results are pushed back onto the stack again. For
example,
> fsa A B compose determinize draw -
reads A and B from files, calculates the determinized
composition and writes the resulting automaton to
the terminal in dot format (which may be piped to
dot directly). As you can see from the examples
some operations like write or draw take additional
arguments that must follow the name of the opera-
tion. Although this does not follow the strict postfix
design, we found it more convenient as these param-
eters are not automata.
4 Experimental Results
4.1 Comparison of Toolkits
A crucial aspect of an FSA toolkit is its computa-
tional and memory efficiency. In this section we will
compare the efficiency of four different implemen-
tations of weighted-finite state toolkits, namely:
? RWTH FSA,
? RWTH FSM (predecessor of RWTH FSA),
? AT&T FSM LibraryTM 4.0 (Mohri et al,
2000b),
? WFST (Adant, 2000).
We opted to not evaluate the FSA6.1 from (van
Noord, 2000) as we found that it is not easy to in-
stall and it seemed to be significantly slower than
any of the other implementations. RWTH FSA and
the AT&T FSM LibraryTM use on-demand com-
putations whereas FSM and WFST do not. As the
algorithmic code between RWTH FSA and its pre-
decessor RWTH FSM has not changed much ex-
cept for the interface of lazy transducers, we can
also compare lazy versus non lazy implementation.
Nevertheless, this direct comparison is also possible
with RWTH FSA as it provides a static storage class
transducer and a traversing deep copy operation.
Table 1 summarizes the tasks used for the eval-
uation of efficiency together with the sizes of the
resulting transducers. The exact meaning of the dif-
ferent transducers is out of scope of this compari-
son. We simply focus on measuring the efficiency of
the algorithms. Experiment 1 is the full expansion
of the static part of a speech recognition search net-
work. Experiment 2 deals with a translation prob-
lem and splits words of a ?bilanguage? into single
words. The meaning of the transducers used for
Experiment 2 will be described in detail in Section
4.2. Experiment 3 is similar to Experiment 1 except
for that the grammar transducer is exchanged with
a translation transducer and the result represents the
static network for a speech-to-text translation sys-
tem.
Table 1: Tasks used for measuring the efficiency of
the toolkits. Sizes are given for the resulting trans-
ducers (VM = Verbmobil).
Experiment states arcs
1 VM, HCL ?G 12,203,420 37,174,684
2 VM, C1 ?A ? C2 341,614 832,225
3 Eutrans, HCL ? T 1,201,718 3,572,601
All experiments were performed on a PC with a
1.2GHz AMD Athlon processor and 2 GB of mem-
ory using Linux as operating system. Table 2 sum-
marizes the peak memory usage of the different
toolkit implementations for the given tasks and Ta-
ble 3 shows the CPU usage accordingly.
As can be seen from Tables 2 and 3 for all given
tasks the RWTH FSA toolkit uses less memory and
computational power than any of the other toolk-
its. However, it is unclear to the authors why the
AT&T LibraryTM is a factor of 1800 slower for ex-
periment 2. The numbers also do not change much
after additionally connecting the composition result
(as in RWTH FSA compose does not connect the
result by default): memory usage rises to 62 MB
and execution time increases to 9.7 seconds. How-
ever, a detailed analysis for the RWTH FSA toolkit
has shown that the composition task of experiment
2 makes intense use of the lazy cache transducer
due to the loop character of the two transducers C1
and C2.
It can also be seen from the two tables that
the lazy implementation RWTH FSA uses signif-
icantly less memory than the non lazy implemen-
tation RWTH FSM and less than half of the CPU
time. One explanation for this is the poor mem-
ory management of RWTH FSM as all interme-
diate results need to be fully expanded in mem-
ory. In contrast, due to its lazy transducer inter-
face, RWTH FSA may allocate memory for a state
only once and reuse it for all subsequent calls to the
getState() method.
Table 2: Comparison of peak memory usage in MB
(? aborted due to exceeded memory limits).
Exp. FSA FSM AT&T WFST
1 360 1700 1500 > 1850?
2 59 310 69 > 1850?
3 48 230 176 550
Table 3: Comparison of CPU time in seconds in-
cluding I/O using a 1.2GHz AMD Athlon proces-
sor (? exceeded memory limits: given time indicates
point of abortion).
Exp. FSA FSM AT&T WFST
1 105 203 515 > 40?
2 6.5 182 11760 > 64?
3 6.6 21 28 3840
4.2 Statistical Machine Translation
Statistical machine translation may be viewed as
a weighted language transduction problem (Vidal,
1997). Therefore it is fairly easy to build a machine
translation system with the use of weighted finite-
state transducers.
Let fJ1 and eIi be two sentences from a source
and target language respectively. Also assume that
we have word level alignments A of all sentences
from a bilingual training corpus. We denote with
epJp1 the segmentation of a target sentence eI1 into
phrases such that fJ1 and epJp1 can be aligned mono-
toneously. This segmentation can be directly calcu-
lated from the alignments A. Then we can formu-
late the problem of finding the best translation e?I1 of
a source sentence as follows:
e?I1 = argmax
eI1
Pr(fJ1 , eI1)
? argmax
A,epJp1
Pr(fJ1 , epJp1 )
= argmax
A,epJp1
?
fj :j=1..J
Pr(fj , epj |f j?11 , epj?1p1 )
? argmax
A,epJp1
?
fj :j=1..J
Pr(fj , epj |f j?1j?n, e
pj?1pj?n)
The last line suggests to solve the translation
problem by estimating a language model on a bi-
language (see also (Bangalore and Riccardi, 2000;
Casacuberta et al, 2001)). An example of sentences
from this bilanguage is given in Figure 5 for the
translation task Vermobil (German ? English). For
technical reasons, ?-labels are represented by a $
symbol. Note, that due to the fixed segmentation
given by the alignments, phrases in the target lan-
guage are moved to the last source word of an align-
ment block.
So, given an appropriate alignment which can
be obtained by means of the pubically available
GIZA++ toolkit (Och and Ney, 2000), the approach
is very easy in practice:
1. Transform the training corpus with a given
alignment into the corresponding bilingual cor-
pus
2. Train a language model on the bilingual corpus
3. Build an acceptor A from the language model
The symbols of the resulting acceptor are still a mix-
ture of words from the source language and phrases
from the target language. So, we additionally use
two simple transducers to split these bilingual words
(C1 maps source words fj to bilingual words that
start with fj and C2 maps bilingual words with the
target sequence epj to the sequences of target words
the phrase was made of):
4. Split the bilingual phrases of A into single
words:
T = C1 ?A ? C2
Then the translation problem from above can be
rewritten using finite-state terminology:
dann|$ melde|$ ich|I_am_calling mich|$ noch|$ einmal|once_more .|.
11U|eleven Uhr|o?clock ist|is hervorragend|excellent .|.
ich|I bin|have da|$ relativ|quite_a_lot_of frei|free_days_then .|.
Figure 5: Example corpus for the bilanguage (Verbmobil, German ? English).
Table 4: Translation results for different tasks compared to similar systems using the alignment template
(AT) approach (Tests were performed on a 1.2GHz AMD Athlon).
Task System Translation WER PER 100-BLEU Memory Time/Sentence
[%] [%] [MB] [ms]
Eutrans FSA Spanish ? English 8.12 7.64 10.7 6-8 20
AT 8.25 - - - -
FUB FSA Italian ? English 27.0 21.5 37.7 3-5 22
AT 23.7 18.1 36.0 - -
Verbmobil FSA German ? English 48.3 41.6 69.8 65-90 460
AT 40.5 30.1 62.2 - -
PF-Star FSA Italian ? English 39.8 34.1 58.4 12-15 35
AT 36.8 29.1 54.3 - -
e? = project-output(best(f ? T ))
Translation results using this approach are summa-
rized in Table 4 and are being compared with results
obtained using the alignment template approach
(Och and Ney, 2000). Results for both approaches
were obtaining using the same training corpus align-
ments. Detailed task descriptions for Eutrans/FUB
and Verbmobil can be found in (Casacuberta et al,
2001) and (Zens et al, 2002) respectively. We use
the usual definitions for word error rate (WER), po-
sition independent word error rate (PER) and BLEU
statistics here.
For the simpler tasks Eutrans, FUB and PF-Star,
the WER, PER and the inverted BLEU statistics
are close for both approaches. On the German-to-
English Verbmobil task the FSA approach suffers
from long distance reorderings (captured through
the fixed training corpus segmentation), which is not
very surprising.
Although we do not have comparable numbers of
the memory usage and the translation times for the
alignment template approach, resource usage of the
finite-state approach is quite remarkable as we only
use generic methods from the RWTH FSA toolkit
and full search (i.e. we do not prune the search
space). However, informal tests have shown that
the finite-state approach uses much less memory
and computations than the current implementation
of the alignment template approach.
Two additional advantages of finite-state methods
for translation in general are: the input to the search
algorithm may also be a word lattice and it is easy
to combine speech recognition with translation in
order to do speech-to-speech translation.
5 Summary
In this paper we have given a characterization of al-
gorithms that produce a single finite-state automa-
ton and bear an on-demand implementation. For
this purpose we formally introduced the local prop-
erty of such an algorithm.
We have described the efficient implementation
of a finite-state toolkit that uses the principle of
lazy algorithmic transducers for almost all algo-
rithms. Among several publically available toolkits,
the RWTH FSA toolkit presented here turned out to
be the most efficient one, as several tests showed.
Additionally, with lazy algorithmic transducers we
have reduced the memory requirements and even in-
creased the speed significantly compared to a non
lazy implementation.
We have also shown that a finite-state automata
toolkit supports rapid solutions to problems from
the field of natural language processing such as sta-
tistical machine translation. Despite the genericity
of the methods, statistical machine translation can
be done very efficiently.
6 Shortcomings and Future Extensions
There is still room to improve the RWTH FSA
toolkit. For example, the current implementation
of determinization is not as general as described in
(Allauzen and Mohri, 2003). In case of ambiguous
input the algorithm still produces an infinite trans-
ducer. At the moment this can be solved in many
cases by adding disambiguation symbols to the in-
put transducer manually.
As the implementation model is based on virtual
C++ methods for all types of objects in use (semir-
ings, alphabets, transducers and algorithmic trans-
ducers) it should also be fairly easy to add support
for dynamically loadable objects to the toolkit.
Other semirings like the expectation semiring de-
scribed in (Eisner, 2001) are supported but not yet
implemented.
7 Acknowledgment
The authors would like to thank Andre Altmann for
his help with the translation experiments.
References
Alfred V. Aho and Jeffrey D. Ullman, 1972, The The-
ory of Parsing, Translation and Compiling, volume 1,
Prentice-Hall, Englewood Cliffs, NJ, 1972.
Arnaud Adant, 2000, WFST: A Finite-State Template Li-
brary in C++, http://membres.lycos.fr/adant/tfe/.
Cyril Allauzen, Mehryar Mohri, and Brian Roark, 2003,
Generalized Algorithms for Constructing Statistical
Language Models, In Proc. of the 41st Meeting of the
Association for Computational Linguistics, Sapporo,
Japan, July 2003.
Cyril Allauzen and Mehryar Mohri, 2003, General-
ized Optimization Algorithm for Speech Recognition
Transducers, In Proc. of the IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing, pp. , Hong
Kong, China, April 2003.
Srinivas Bangalore and Giuseppe Riccardi, 2000,
Stochastic Finite-State models for Spoken Language
Machine Translation, In Proc. of the Workshop on
Embedded Machine Translation Systems, pp. 52?59,
2000.
David Beazley, William Fulton, Matthias Ko?ppe, Lyle
Johnson, Richard Palmer, 1996, SWIG - Simplified
Wrapper and Interface Generator, Electronic Docu-
ment, http://www.swig.org, February 1996.
F. Casacuberta, D. Llorens, C. Martinez, S. Molau, F.
Nevado, H. Ney, M. Pasto, D. Pico, A. Sanchis, E. Vi-
dal and J.M. Vilar, 2001, Speech-to-Speech Transla-
tion based on Finite-State Transducer, In Proc. IEEE
Int. Conf. on Acoustics, Speech and Signal Process-
ing, pp. 613-616, Salt Lake City, Utah, May 2001.
Thomas H. Cormen, Charles E. Leiserson and Ronald L.
Rivest, 1990, Introductions to Algorithms, The MIT
Press, Cambridge, MA, 1990.
Jason Eisner, 2001, Expectation Semirings: Flexible
EM for Finite-State Transducers, In Proc. of the
ESSLLI Workshop on Finite-State Methods in NLP
(FSMNLP), Helsinki, August 2001.
Free Software Foundation, 1991, GNU General
Public License, Version 2, Electronic Document,
http://www.gnu.org/copyleft/gpl.html, June 1991.
Takaaki Hori, Chiori Hori and Yasuhiro Minami, 2003,
Speech Summarization using Weighted Finite-State
Transducers, In Proc. of the European Conf. on
Speech Communication and Technology, Geneva,
Switzerland, September 2003.
Vincent Le Maout, 1998, ASTL: Automaton Stan-
dard Template Library, http://www-igm.univ-
mlv.fr/?lemaout/.
Kurt Mehlhorn, 1984, Data Structures and Efficient Al-
gorithms, Chapter 4, Springer Verlag, EATCS Mono-
graphs, 1984, also available from http://www.mpi-
sb.mpg.de/m?ehlhorn/DatAlgbooks.html.
Mehryar Mohri, 1997, Finite-State Transducers in Lan-
guage and Speech Processing, Computational Lin-
guistics, 23:2, 1997.
Mehryar Mohri, Fernando C.N. Pereira, and Michael
Riley, 2000, Weighted Finite-State Transducers in
Speech Recognition, In Proc. of the ISCA Tutorial and
Research Workshop, Automatic Speech Recognition:
Challenges for the new Millenium (ASR2000), Paris,
France, September 2000.
Mehryar Mohri, Fernando C.N. Pereira, and Michael Ri-
ley, 2000, The Design Principles of a Weighted Finite-
State Transducer Library, Theoretical Computer Sci-
ence, 231:17-32, January 2000.
Mehryar Mohri and Michael Riley, 2000, A Weight
Pushing Algorithm for Large Vocabulary Speech
Recognition, In Proc. of the European Conf. on
Speech Communication and Technology, pp. 1603?
1606, A?alborg, Denmark, September 2001.
Mehryar Mohri, 2001, Generic Epsilon-Removal Algo-
rithm for Weighted Automata, In Sheng Yu and An-
drei Paun, editor, 5th Int. Conf., CIAA 2000, London
Ontario, Canada. volume 2088 of Lecture Notes in
Computer Science, pages 230-242. Springer-Verlag,
Berlin-NY, 2001.
Mehryar Mohri and Michael Riley, 2002, An Efficient
Algorithm for the N-Best-Strings Problem, In Proc.
of the Int. Conf. on Spoken Language Processing, pp.
1313?1316, Denver, Colorado, September 2002.
Franz J. Och and Hermann Ney, 2000, Improved Sta-
tistical Alignment Models, In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hongkong, China, October
2000.
Fernando C.N. Pereira and Michael Riley, 1996, Speech
Recognition by Composition of Weighted Finite
Automata, Available from http://xxx.lanl.gov/cmp-
lg/9603001, Computation and Language, 1996.
Gertjan van Noord, 2000, FSA6 Reference Manual,
http://odur.let.rug.nl/v?annoord/Fsa/.
Enrique Vidal, 1997, Finite-State Speech-to-Speech
Translation, In Proc. of the IEEE Int. Conf. on Acous-
tics, Speech and Signal Processing, pp. 111?114, Mu-
nich, Germany, 1997.
Richard Zens, Franz J. Och and H. Ney, 2002, Phrase-
Based Statistical Machine Translation, In: M. Jarke,
J. Koehler, G. Lakemeyer (Eds.) : KI - 2002: Ad-
vances in artificial intelligence. 25. Annual German
Conference on AI, KI 2002, Vol. LNAI 2479, pp. 18-
32, Springer Verlag, September 2002.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Novel Reordering Approaches in Phrase-Based Statistical Machine
Translation
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney
The authors are with the Lehrstuhl fu?r Informatik VI,
Computer Science Department, RWTH Aachen University,
D-52056 Aachen, Germany.
E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.
Abstract
This paper presents novel approaches to
reordering in phrase-based statistical ma-
chine translation. We perform consistent
reordering of source sentences in train-
ing and estimate a statistical translation
model. Using this model, we follow a
phrase-based monotonic machine transla-
tion approach, for which we develop an ef-
ficient and flexible reordering framework
that allows to easily introduce different re-
ordering constraints. In translation, we
apply source sentence reordering on word
level and use a reordering automaton as in-
put. We show how to compute reordering
automata on-demand using IBM or ITG
constraints, and also introduce two new
types of reordering constraints. We further
add weights to the reordering automata.
We present detailed experimental results
and show that reordering significantly im-
proves translation quality.
1 Introduction
Reordering is of crucial importance for machine
translation. Already (Knight et al, 1998) use full un-
weighted permutations on the level of source words
in their early weighted finite-state transducer ap-
proach which implemented single-word based trans-
lation using conditional probabilities. In a refine-
ment with additional phrase-based models, (Kumar
et al, 2003) define a probability distribution over
all possible permutations of source sentence phrases
and prune the resulting automaton to reduce com-
plexity.
A second category of finite-state translation ap-
proaches uses joint instead of conditional probabili-
ties. Many joint probability approaches originate in
speech-to-speech translation as they are the natural
choice in combination with speech recognition mod-
els. The automated transducer inference techniques
OMEGA (Vilar, 2000) and GIATI (Casacuberta et
al., 2004) work on phrase level, but ignore the re-
ordering problem from the view of the model. With-
out reordering both in training and during search,
sentences can only be translated properly into a lan-
guage with similar word order. In (Bangalore et al,
2000) weighted reordering has been applied to tar-
get sentences since defining a permutation model on
the source side is impractical in combination with
speech recognition. In order to reduce the computa-
tional complexity, this approach considers only a set
of plausible reorderings seen on training data.
Most other phrase-based statistical approaches
like the Alignment Template system of Bender
et al (2004) rely on (local) reorderings which are
implicitly memorized with each pair of source and
target phrases in training. Additional reorderings on
phrase level are fully integrated into the decoding
process, which increases the complexity of the sys-
tem and makes it hard to modify. Zens et al (2003)
reviewed two types of reordering constraints for this
type of translation systems.
In our work we follow a phrase-based transla-
tion approach, applying source sentence reordering
on word level. We compute a reordering graph on-
demand and take it as input for monotonic trans-
lation. This approach is modular and allows easy
introduction of different reordering constraints and
probabilistic dependencies. We will show that it per-
forms at least as well as the best statistical machine
translation system at the IWSLT Evaluation.
167
In the next section we briefly review the basic
theory of our translation system based on weighted
finite-state transducers (WFST). In Sec. 3 we in-
troduce new methods for reordering and alignment
monotonization in training. To compare differ-
ent reordering constraints used in the translation
search process we develop an on-demand com-
putable framework for permutation models in Sec. 4.
In the same section we also define and analyze un-
restricted and restricted permutations with some of
them being first published in this paper. We con-
clude the paper by presenting and discussing a rich
set of experimental results.
2 Machine Translation using WFSTs
Let fJ1 and eIi be two sentences from a source and
target language. Assume that we have word level
alignments A of all sentence pairs from a bilingual
training corpus. We denote with e?J1 the segmenta-
tion of a target sentence eI1 into J phrases such that
fJ1 and e?J1 can be aligned to form bilingual tuples
(fj , e?j). If alignments are only functions of target
words A? : {1, . . . , I} ? {1, . . . , J}, the bilingual
tuples (fj , e?j) can be inferred with e. g. the GIATI
method of (Casacuberta et al, 2004), or with our
novel monotonization technique (see Sec. 3). Each
source word will be mapped to a target phrase of one
or more words or an ?empty? phrase ?. In particular,
the source words which will remain non-aligned due
to the alignment functionality restriction are paired
with the empty phrase.
We can then formulate the problem of finding the
best translation e?I1 of a source sentence fJ1 :
e?I1 = argmax
eI1
Pr(fJ1 , e
I
1)
= argmax
e?J1
?
A?A
Pr(fJ1 , e?
J
1 , A)
?= argmax
e?J1
max
A?A
Pr(A) ? Pr(fJ1 , e?
J
1 |A)
?= argmax
e?J1
max
A?A
?
fj :j=1...J
Pr(fj , e?j |f
j?1
1 , e?
j?1
1 , A)
= argmax
e?J1
max
A?A
?
fj :j=1...J
p(fj , e?j |f
j?1
j?m, e?
j?1
j?m, A)
In other words: if we assume a uniform distri-
bution for Pr(A), the translation problem can be
mapped to the problem of estimating an m-gram lan-
guage model over a learned set of bilingual tuples
(fj , e?j). Mapping the bilingual language model to a
WFST T is canonical and it has been shown in (Kan-
thak et al, 2004) that the search problem can then be
rewritten using finite-state terminology:
e?I1 = project-output(best(fJ1 ? T )) .
This implementation of the problem as WFSTs may
be used to efficiently solve the search problem in
machine translation.
3 Reordering in Training
When the alignment function A? is not monotonic,
target language phrases e? can become very long.
For example in a completely non-monotonic align-
ment all target words are paired with the last aligned
source word, whereas all other source words form
tuples with the empty phrase. Therefore, for lan-
guage pairs with big differences in word order, prob-
ability estimates may be poor.
This problem can be solved by reordering either
source or target training sentences such that align-
ments become monotonic for all sentences. We
suggest the following consistent source sentence re-
ordering and alignment monotonization approach in
which we compute optimal, minimum-cost align-
ments.
First, we estimate a cost matrix C for each sen-
tence pair (fJ1 , eI1). The elements of this matrix cij
are the local costs of aligning a source word fj to a
target word ei. Following (Matusov et al, 2004), we
compute these local costs by interpolating state oc-
cupation probabilities from the source-to-target and
target-to-source training of the HMM and IBM-4
models as trained by the GIZA++ toolkit (Och et al,
2003). For a given alignment A ? I ? J , we define
the costs of this alignment c(A) as the sum of the
local costs of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (1)
The goal is to find an alignment with the minimum
costs which fulfills certain constraints.
3.1 Source Sentence Reordering
To reorder a source sentence, we require the
alignment to be a function of source words A1:
{1, . . . , J} ? {1, . . . , I}, easily computed from the
cost matrix C as:
A1(j) = argmini cij (2)
168
We do not allow for non-aligned source words. A1
naturally defines a new order of the source words fJ1
which we denote by f?J1 . By computing this permu-
tation for each pair of sentences in training and ap-
plying it to each source sentence, we create a corpus
of reordered sentences.
3.2 Alignment Monotonization
In order to create a ?sentence? of bilingual tuples
(f?J1 , e?
J
1 ) we required alignments between reordered
source and target words to be a function of target
words A2 : {1, . . . , I} ? {1, . . . , J}. This align-
ment can be computed in analogy to Eq. 2 as:
A2(i) = argminj c?ij (3)
where c?ij are the elements of the new cost matrix
C? which corresponds to the reordered source sen-
tence. We can optionally re-estimate this matrix by
repeating EM training of state occupation probabili-
ties with GIZA++ using the reordered source corpus
and the original target corpus. Alternatively, we can
get the cost matrix C? by reordering the columns of
the cost matrix C according to the permutation given
by alignment A1.
In alignment A2 some target words that were pre-
viously unaligned in A1 (like ?the? in Fig. 1) may
now still violate the alignment monotonicity. The
monotonicity of this alignment can not be guaran-
teed for all words if re-estimation of the cost matri-
ces had been performed using GIZA++.
The general GIATI technique (Casacuberta et al,
2004) is applicable and can be used to monotonize
the alignment A2. However, in our experiments
the following method performs better. We make
use of the cost matrix representation and compute
a monotonic minimum-cost alignment with a dy-
namic programming algorithm similar to the Lev-
enshtein string edit distance algorithm. As costs of
each ?edit? operation we consider the local align-
ment costs. The resulting alignment A3 represents
a minimum-cost monotonic ?path? through the cost
matrix. To make A3 a function of target words we
do not consider the source words non-aligned in A2
and also forbid ?deletions? (?many-to-one? source
word alignments) in the DP search.
An example of such consistent reordering and
monotonization is given in Fig. 1. Here, we re-
order the German source sentence based on the ini-
tial alignment A1, then compute the function of tar-
get words A2, and monotonize this alignment to A3
the very beginning of May would suit me .
the very beginning of May would suit me .
sehr gut Anfang Mai w?rde passen mir .
sehr gut Anfang Mai w?rde passen mir .
the very beginning of May would suit me .
mir sehrw?rde gut Anfang Mai passen .
.Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginning
A
A
A1
2
3
Figure 1: Example of alignment, source sentence re-
ordering, monotonization, and construction of bilin-
gual tuples.
with the dynamic programming algorithm. Fig. 1
also shows the resulting bilingual tuples (f?j , e?j).
4 Reordering in Search
When searching the best translation e?J1 for a given
source sentence fJ1 , we permute the source sentence
as described in (Knight et al, 1998):
e?I1 = project-output(best(permute(fJ1 ) ? T ))
Permuting an input sequence of J symbols re-
sults in J ! possible permutations and representing
the permutations as a finite-state automaton requires
at least 2J states. Therefore, we opt for computing
the permutation automaton on-demand while apply-
ing beam pruning in the search.
4.1 Lazy Permutation Automata
For on-demand computation of an automaton in the
flavor described in (Kanthak et al, 2004) it is suffi-
cient to specify a state description and an algorithm
that calculates all outgoing arcs of a state from the
state description. In our case, each state represents
a permutation of a subset of the source words fJ1 ,
which are already translated.
This can be described by a bit vector bJ1 (Zens
et al, 2002). Each bit of the state bit vector corre-
sponds to an arc of the linear input automaton and is
set to one if the arc has been used on any path from
the initial to the current state. The bit vectors of two
states connected by an arc differ only in a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
169
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
Even with on-demand computation, complexity
using full permutations is unmanagable for long sen-
tences. We further reduce complexity by addition-
ally constraining permutations. Refer to Figure 2 for
visualizations of the permutation constraints which
we describe in the following.
4.2 IBM Constraints
The IBM reordering constraints are well-known in
the field of machine translation and were first de-
scribed in (Berger et al, 1996). The idea behind
these constraints is to deviate from monotonic trans-
lation by postponing translations of a limited num-
ber of words. More specifically, at each state we
can translate any of the first l yet uncovered word
positions. The implementation using a bit vector is
straightforward. For consistency, we associate win-
dow size with the parameter l for all constraints pre-
sented here.
4.3 Inverse IBM Constraints
The original IBM constraints are useful for a large
number of language pairs where the ability to skip
some words reflects the differences in word order
between the two languages. For some other pairs,
it is beneficial to translate some words at the end of
the sentence first and to translate the rest of the sen-
tence nearly monotonically. Following this idea we
can define the inverse IBM constraints. Let j be the
first uncovered position. We can choose any posi-
tion for translation, unless l ? 1 words on positions
j? > j have been translated. If this is the case we
must translate the word in position j. The inverse
IBM constraints can also be expressed by
invIBM(x) = transpose(IBM(transpose(x))) .
As the transpose operation can not be computed
on-demand, our specialized implementation uses bit
vectors bJ1 similar to the IBM constraints.
4.4 Local Constraints
For some language pairs, e.g. Italian ? English,
words are moved only a few words to the left or
right. The IBM constraints provide too many alter-
native permutations to chose from as each word can
be moved to the end of the sentence. A solution that
allows only for local permutations and therefore has
a)
0000 10001 11002 11103 11114
b)
0000
10001
01002 1100
2
10103
1
0110
3
11103
110141
01114
1111
4
13
2
10114 2
c)
0000
10001
01002
0010
3
0001
4 10014
1010
3 11002
1
1
1
1101
2
11113
11102 4
43
d)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2
Figure 2: Permutations of a) positions j = 1, 2, 3, 4
of a source sentence f1f2f3f4 using a window size
of 2 for b) IBM constraints, c) inverse IBM con-
straints and d) local constraints.
very low complexity is given by the following per-
mutation rule: the next word for translation comes
from the window of l positions1 counting from the
first yet uncovered position. Note, that the local con-
straints define a true subset of the permutations de-
fined by the IBM constraints.
4.5 ITG Constraints
Another type of reordering can be obtained using In-
version Transduction Grammars (ITG) (Wu, 1997).
These constraints are inspired by bilingual bracket-
ing. They proved to be quite useful for machine
translation, e.g. see (Bender et al, 2004). Here,
we interpret the input sentence as a sequence of seg-
ments. In the beginning, each word is a segment of
its own. Longer segments are constructed by recur-
sively combining two adjacent segments. At each
1both covered and uncovered
170
Chinese English Japanese English Italian English
train sentences 20 000 20 000 66107
words 182 904 160 523 209 012 160 427 410 275 427 402
singletons 3 525 2 948 4 108 2 956 6 386 3 974
vocabulary 7 643 6 982 9 277 6 932 15 983 10 971
dev sentences 506 506 500
words 3 515 3 595 4 374 3 595 3 155 3 253
sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25
test sentences 500 500 506
words 3 794 ? 4 370 ? 2 931 3 595
sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28
Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.
combination step, we either keep the two segments
in monotonic order or invert the order. This pro-
cess continues until only one segment for the whole
sentence remains. The on-demand computation is
implemented in spirit of Earley parsing.
We can modify the original ITG constraints to
further limit the number of reorderings by forbid-
ding segment inversions which violate IBM con-
straints with a certain window size. Thus, the re-
sulting reordering graph contains the intersection of
the reorderings with IBM and the original ITG con-
straints.
4.6 Weighted Permutations
So far, we have discussed how to generate the per-
mutation graphs under different constraints, but per-
mutations were equally probable. Especially for the
case of nearly monotonic translation it is make sense
to restrict the degree of non-monotonicity that we
allow when translating a sentence. We propose a
simple approach which gives a higher probability
to the monotone transitions and penalizes the non-
monotonic ones.
A state description bJ1 , for which the following
condition holds:
Mon(j) : bj? = ?(j
? ? j) ? 1 ? j? ? J
represents the monotonic path up to the word fj . At
each state we assign the probability ? to that out-
going arc where the target state description fullfills
Mon(j+1) and distribute the remaining probability
mass 1? ? uniformly among the remaining arcs. In
case there is no such arc, all outgoing arcs get the
same uniform probability. This weighting scheme
clearly depends on the state description and the out-
going arcs only and can be computed on-demand.
5 Experimental Results
5.1 Corpus Statistics
The translation experiments were carried out on the
Basic Travel Expression Corpus (BTEC), a multilin-
gual speech corpus which contains tourism-related
sentences usually found in travel phrase books.
We tested our system on the so called Chinese-to-
English (CE) and Japanese-to-English (JE) Supplied
Tasks, the corpora which were provided during the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). In ad-
dition, we performed experiments on the Italian-to-
English (IE) task, for which a larger corpus was
kindly provided to us by ITC/IRST. The corpus
statistics for the three BTEC corpora are given in
Tab. 1. The development corpus for the Italian-to-
English translation had only one reference transla-
tion of each Italian sentence. A set of 506 source
sentences and 16 reference translations is used as
a development corpus for Chinese-to-English and
Japanese-to-English and as a test corpus for Italian-
to-English tasks. The 500 sentence Chinese and
Japanese test sets of the IWSLT 2004 evaluation
campaign were translated and automatically scored
against 16 reference translations after the end of the
campaign using the IWSLT evaluation server.
5.2 Evaluation Criteria
For the automatic evaluation, we used the crite-
ria from the IWSLT evaluation campaign (Akiba et
al., 2004), namely word error rate (WER), position-
independent word error rate (PER), and the BLEU
and NIST scores (Papineni et al, 2002; Doddington,
2002). The two scores measure accuracy, i. e. larger
scores are better. The error rates and scores were
computed with respect to multiple reference transla-
171
 40
 42
 44
 46
 48
 50
 52
 54
 56
 58
 60
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
Figure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:
Japanese-to-English (left) and Chinese-to-English (right) translation.
tions, when they were available. To indicate this, we
will label the error rate acronyms with an m. Both
training and evaluation were performed using cor-
pora and references in lowercase and without punc-
tuation marks.
5.3 Experiments
We used reordering and alignment monotonization
in training as described in Sec. 3. To estimate the
matrices of local alignment costs for the sentence
pairs in the training corpus we used the state occupa-
tion probabilities of GIZA++ IBM-4 model training
and interpolated the probabilities of source-to-target
and target-to-source training directions. After that
we estimated a smoothed 4-gram language model on
the level of bilingual tuples fj , e?j and represented it
as a finite-state transducer.
When translating, we applied moderate beam
pruning to the search automaton only when using re-
ordering constraints with window sizes larger than 3.
For very large window sizes we also varied the prun-
ing thresholds depending on the length of the input
sentence. Pruning allowed for fast translations and
reasonable memory consumption without a signifi-
cant negative impact on performance.
In our first experiments, we tested the four re-
ordering constraints with various window sizes. We
aimed at improving the translation results on the de-
velopment corpora and compared the results with
two baselines: reordering only the source training
sentences and translation of the unreordered test sen-
tences; and the GIATI technique for creating bilin-
gual tuples (fj , e?j) without reordering of the source
sentences, neither in training nor during translation.
5.3.1 Highly Non-Monotonic Translation (JE)
Fig. 3 (left) shows word error rate on the
Japanese-to-English task as a function of the win-
dow size for different reordering constraints. For
each of the constraints, good results are achieved
using a window size of 9 and larger. This can be
attributed to the Japanese word order which is very
different from English and often follows a subject-
object-verb structure. For small window sizes, ITG
or IBM constraints are better suited for this task, for
larger window sizes, inverse IBM constraints per-
form best. The local constraints perform worst and
require very large window sizes to capture the main
word order differences between Japanese and En-
glish. However, their computational complexity is
low; for instance, a system with local constraints
and window size of 9 is as fast (25 words per sec-
ond) as the same system with IBM constraints and
window size of 5. Using window sizes larger than
10 is computationally expensive and does not sig-
nificantly improve the translation quality under any
of the constraints.
Tab. 2 presents the overall improvements in trans-
lation quality when using the best setting: inverse
IBM constraints, window size 9. The baseline with-
out reordering in training and testing failed com-
pletely for this task, producing empty translations
for 37 % of the sentences2. Most of the original
alignments in training were non-monotonic which
resulted in mapping of almost all Japanese words to
? when using only the GIATI monotonization tech-
nique. Thus, the proposed reordering methods are of
crucial importance for this task.
2Hence a NIST score of 0 due to the brevity penalty.
172
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
BTEC Japanese-to-English (JE) dev
none 59.7 58.8 13.0 0.00
in training 57.8 39.4 14.7 3.27
+ 9-inv-ibm 40.3 32.1 45.1 8.59
+ rescoring* 39.1 30.9 53.2 9.93
BTEC Chinese-to-English (CE) dev
none 55.2 52.1 24.9 1.34
in training 54.0 42.3 23.0 4.18
+ 7-inv-ibm 47.1 39.4 34.5 6.53
+ rescoring* 48.3 40.7 39.1 8.11
Table 2: Translation results with optimal reorder-
ing constraints and window sizes for the BTEC
Japanese-to-English and Chinese-to-English devel-
opment corpora. *Optimized for the NIST score.
mWER mPER BLEU NIST
[%] [%] [%]
BTEC Japanese-to-English (JE) test
AT 41.9 33.8 45.3 9.49
WFST 42.1 35.6 47.3 9.50
BTEC Chinese-to-English (CE) test
AT 45.6 39.0 40.9 8.55
WFST 46.4 38.8 40.8 8.73
Table 3: Comparison of the IWSLT-2004 automatic
evaluation results for the described system (WFST)
with those of the best submitted system (AT).
Further improvements were obtained with a
rescoring procedure. For rescoring, we produced
a k-best list of translation hypotheses and used the
word penalty and deletion model features, the IBM
Model 1 lexicon score, and target language n-gram
models of the order up to 9. The scaling factors for
all features were optimized on the development cor-
pus for the NIST score, as described in (Bender et
al., 2004).
5.3.2 Moderately Non-Mon. Translation (CE)
Word order in Chinese and English is usually sim-
ilar. However, a few word reorderings over quite
large distances may be necessary. This is especially
true in case of questions, in which question words
like ?where? and ?when? are placed at the end of
a sentence in Chinese. The BTEC corpora contain
many sentences with questions.
The inverse IBM constraints are designed to per-
form this type of reordering (see Sec. 4.3). As shown
in Fig. 3, the system performs well under these con-
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
none 25.6 22.0 62.1 10.46
in training 28.0 22.3 58.1 10.32
+ 4-local 26.3 20.3 62.2 10.81
+ weights 25.3 20.3 62.6 10.79
+ 3-ibm 27.2 20.5 61.4 10.76
+ weights 25.2 20.3 62.9 10.80
+ rescoring* 22.2 19.0 69.2 10.47
Table 4: Translation results with optimal reordering
constraints and window sizes for the test corpus of
the BTEC IE task. *Optimized for WER.
straints already with relatively small window sizes.
Increasing the window size beyond 4 for these con-
straints only marginally improves the translation er-
ror measures for both short (under 8 words) and long
sentences. Thus, a suitable language-pair-specific
choice of reordering constraints can avoid the huge
computational complexity required for permutations
of long sentences.
Tab. 2 includes error measures for the best setup
with inverse IBM constraints with window size of 7,
as well as additional improvements obtained by a k-
best list rescoring.
The best settings for reordering constraints and
model scaling factors on the development corpora
were then used to produce translations of the IWSLT
Japanese and Chinese test corpora. These trans-
lations were evaluated against multiple references
which were unknown to the authors. Our system
(denoted with WFST, see Tab. 3) produced results
competitive with the results of the best system at the
evaluation campaign (denoted with AT (Bender et
al., 2004)) and, according to some of the error mea-
sures, even outperformed this system.
5.3.3 Almost Monotonic Translation (IE)
The word order in the Italian language does not
differ much from the English. Therefore, the abso-
lute translation error rates are quite low and translat-
ing without reordering in training and search already
results in a relatively good performance. This is re-
flected in Tab. 4. However, even for this language
pair it is possible to improve translation quality by
performing reordering both in training and during
translation. The best performance on the develop-
ment corpus is obtained when we constrain the re-
odering with relatively small window sizes of 3 to 4
and use either IBM or local reordering constraints.
173
On the test corpus, as shown in Tab. 4, all error mea-
sures can be improved with these settings.
Especially for languages with similar word order
it is important to use weighted reorderings (Sec. 4.6)
in order to prefer the original word order. Introduc-
tion of reordering weights for this task results in no-
table improvement of most error measures using ei-
ther the IBM or local constraints. The optimal prob-
ability ? for the unreordered path was determined
on the development corpus as 0.5 for both of these
constraints. The results on the test corpus using this
setting are also given in Tab. 4.
6 Conclusion
In this paper, we described a reordering framework
which performs source sentence reordering on word
level. We suggested to use optimal alignment func-
tions for monotonization and improvement of trans-
lation model training. This allowed us to translate
monotonically taking a reordering graph as input.
We then described known and novel reordering con-
straints and their efficient finite-state implementa-
tions in which the reordering graph is computed on-
demand. We also utilized weighted permutations.
We showed that our monotonic phrase-based trans-
lation approach effectively makes use of the reorder-
ing framework to produce quality translations even
from languages with significantly different word or-
der. On the Japanese-to-English and Chinese-to-
English IWSLT tasks, our system performed at least
as well as the best machine translation system.
Acknowledgement
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04 Evalu-
ation Campaign. Proc. Int. Workshop on Spoken Lan-
guage Translation, pp. 1?12, Kyoto, Japan.
S. Bangalore and G. Riccardi. 2000. Stochastic Finite-
State Models for Spoken Language Machine Transla-
tion. Proc. Workshop on Embedded Machine Transla-
tion Systems, pp. 52?59.
O. Bender, R. Zens, E. Matusov, and H. Ney. 2004.
Alignment Templates: the RWTH SMT System. Proc.
Int. Workshop on Spoken Language Translation, pp.
79?84, Kyoto, Japan.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language Translation Apparatus and Method
of Using Context-based Translation Models. United
States Patent 5510981.
F. Casacuberta and E. Vidal. 2004. Machine Transla-
tion with Inferred Stochastic Finite-State Transducers.
Computational Linguistics, vol. 30(2):205-225.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using n-gram Co-Occurrence
Statistics. Proc. Human Language Technology Conf.,
San Diego, CA.
S. Kanthak and H. Ney. 2004. FSA: an Efficient and
Flexible C++ Toolkit for Finite State Automata using
On-demand Computation. Proc. 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pp. 510?517, Barcelona, Spain.
K. Knight and Y. Al-Onaizan. 1998. Translation with
Finite-State Devices. Lecture Notes in Artificial Intel-
ligence, Springer-Verlag, vol. 1529, pp. 421?437.
S. Kumar and W. Byrne. 2003. A Weighted Finite State
Transducer Implementation of the Alignment Template
Model for Statistical Machine Translation. Proc. Hu-
man Language Technology Conf. NAACL, pp. 142?
149, Edmonton, Canada.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word
Alignments for Statistical Machine Translation. Proc.
20th Int. Conf. on Computational Linguistics, pp. 219?
225, Geneva, Switzerland.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, vol. 29, number 1, pp. 19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Machine
Translation. Proc. 40th Annual Meeting of the Associ-
ation for Computational Linguistics, Philadelphia, PA,
pp. 311?318.
J. M. Vilar, 2000. Improve the Learning of Sub-
sequential Transducers by Using Alignments and Dic-
tionaries. Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 1891, pp. 298?312.
D. Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
R. Zens, F. J. Och and H. Ney. 2002. Phrase-Based Sta-
tistical Machine Translation. In: M. Jarke, J. Koehler,
G. Lakemeyer (Eds.): KI - Conference on AI, KI 2002,
Vol. LNAI 2479, pp. 18-32, Springer Verlag.
R. Zens and H. Ney. 2003. A Comparative Study on
Reordering Constraints in Statistical Machine Trans-
lation. Proc. Annual Meeting of the Association
for Computational Linguistics, pp. 144?151, Sapporo,
Japan.
174
