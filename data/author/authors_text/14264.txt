Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 485?494,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Improving Interactive Machine Translation via Mouse Actions
Germa?n Sanchis-Trilles and Daniel Ortiz-Mart??nez and Jorge Civera
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{gsanchis,dortiz,jorcisai}@iti.upv.es
Francisco Casacuberta and Enrique Vidal
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{fcn,evidal}@dsic.upv.es
Hieu Hoang
University of Edinburgh
hhoang@sms.ed.ac.uk
Abstract
Although Machine Translation (MT) is a very
active research field which is receiving an in-
creasing amount of attention from the research
community, the results that current MT sys-
tems are capable of producing are still quite
far away from perfection. Because of this,
and in order to build systems that yield correct
translations, human knowledge must be inte-
grated into the translation process, which will
be carried out in our case in an Interactive-
Predictive (IP) framework. In this paper, we
show that considering Mouse Actions as a sig-
nificant information source for the underly-
ing system improves the productivity of the
human translator involved. In addition, we
also show that the initial translations that the
MT system provides can be quickly improved
by an expert by only performing additional
Mouse Actions. In this work, we will be using
word graphs as an efficient interface between
a phrase-based MT system and the IP engine.
1 Introduction
Information technology advances in modern society
have led to the need of more efficient methods of
translation. It is important to remark that current
MT systems are not able to produce ready-to-use
texts (Kay, 1997; Hutchins, 1999; Arnold, 2003).
Indeed, MT systems are usually limited to specific
semantic domains and the translations provided re-
quire human post-editing in order to achieve a cor-
rect high-quality translation.
A way of taking advantage of MT systems is to
combine them with the knowledge of a human trans-
lator, constituting the so-called Computer-Assisted
Translation (CAT) paradigm. CAT offers different
approaches in order to benefit from the synergy be-
tween humans and MT systems.
An important contribution to interactive CAT
technology was carried out around the TransType
(TT) project (Langlais et al, 2002; Foster et al,
2002; Foster, 2002; Och et al, 2003). This project
entailed an interesting focus shift in which interac-
tion directly aimed at the production of the target
text, rather than at the disambiguation of the source
text, as in former interactive systems. The idea
proposed was to embed data driven MT techniques
within the interactive translation environment.
Following these TT ideas, (Barrachina and oth-
ers, 2008) propose the usage of fully-fledged statis-
tical MT (SMT) systems to produce full target sen-
tence hypotheses, or portions thereof, which can be
partially or completely accepted and amended by a
human translator. Each partial correct text segment
is then used by the SMT system as additional infor-
mation to achieve further, hopefully improved sug-
gestions. In this paper, we also focus on the inter-
active and predictive, statistical MT (IMT) approach
to CAT. The IMT paradigm fits well within the In-
teractive Pattern Recognition framework introduced
in (Vidal and others, 2007).
485
SOURCE (x): Para encender la impresora:
REFERENCE (y): To power on the printer:
ITER-0 (p) ( )(s?h) To switch on:
ITER-1
(p) To
(sl) switch on:
(k) power
(s?h) on the printer:
ITER-2
(p) To power on the printer:
(sl) ( )
(k) (#)
(s?h) ( )
FINAL (p ? y) To power on the printer:
Figure 1: IMT session to translate a Spanish sentence into English. Non-validated hypotheses are displayed in italics,
whereas accepted prefixes are printed in normal font.
Figure 1 illustrates a typical IMT session. Ini-
tially, the user is given an input sentence x to be
translated. The reference y provided is the trans-
lation that the user would like to achieve at the end
of the IMT session. At iteration 0, the user does not
supply any correct text prefix to the system, for this
reason p is shown as empty. Therefore, the IMT sys-
tem has to provide an initial complete translation sh,
as it were a conventional SMT system. At the next
iteration, the user validates a prefix p as correct by
positioning the cursor in a certain position of sh. In
this case, after the words ?To print a?. Implicitly, he
is also marking the rest of the sentence, the suffix sl,
as potentially incorrect. Next, he introduces a new
word k, which is assumed to be different from the
first word sl1 in the suffix sl which was not validated,
k 6= sl1 . This being done, the system suggests a new
suffix hypothesis s?h, subject to s?h1 = k. Again, the
user validates a new prefix, introduces a new word
and so forth. The process continues until the whole
sentence is correct that is validated introducing the
special word ?#?.
As the reader could devise from the IMT session
described above, IMT aims at reducing the effort
and increasing the productivity of translators, while
preserving high-quality translation. For instance, in
Figure 1, only three interactions were necessary in
order to achieve the reference translation.
In this paper, we will show how Mouse Actions
performed by the human expert can be taken advan-
tage of in order to further reduce this effort.
2 Statistical interactive-predictive MT
In this section we will briefly describe the statistical
framework of IMT. IMT can be seen as an evolution
of the SMT framework, which has proved to be an
efficient framework for building state-of-the-art MT
systems with little human effort, whenever adequate
corpora are available (Hutchings and Somers, 1992).
The fundamental equation of the statistical approach
to MT is
y? = argmax
y
Pr(y |x) (1)
= argmax
y
Pr(x |y)Pr(y) (2)
where Pr(x |y) is the translation model modelling
the correlation between source and target sentence
and Pr(y) is the language model representing the
well-formedness of the candidate translation y.
In practise, the direct modelling of the posterior
probability Pr(y|x) has been widely adopted. To
this purpose, different authors (Papineni et al, 1998;
Och and Ney, 2002) propose the use of the so-called
log-linear models, where the decision rule is given
by the expression
y? = argmax
y
M
?
m=1
?mhm(x,y) (3)
where hm(x,y) is a score function representing an
important feature for the translation of x into y, M
is the number of models (or features) and ?m are the
weights of the log-linear combination.
486
One of the most popular instantiations of log-
linear models is that including phrase-based (PB)
models (Zens et al, 2002; Koehn et al, 2003).
Phrase-based models allow to capture contextual in-
formation to learn translations for whole phrases in-
stead of single words. The basic idea of phrase-
based translation is to segment the source sentence
into phrases, then to translate each source phrase
into a target phrase, and finally to reorder the trans-
lated target phrases in order to compose the tar-
get sentence. Phrase-based models were employed
throughout this work.
In log-linear models, the maximisation problem
stated in Eq. 3 is solved by means of the beam search
algorithm1 which was initially introduced in (Low-
erre, 1976) for its application in the field of speech
recognition. The beam search algorithm attempts to
generate partial solutions, called hypotheses, until
a complete sentence is found; these hypotheses are
stored in a stack and ordered by their score. Such a
score is given by the log-linear combination of fea-
ture functions.
However, Eq. 1 needs to be modified according to
the IMT scenario in order to take into account part
of the target sentence that is already translated, that
is p and k
s?h = argmax
sh
Pr(sh|x,p, k) (4)
where the maximisation problem is defined over the
suffix sh. This allows us to rewrite Eq. 4, by decom-
posing the right side appropriately and eliminating
constant terms, achieving the equivalent criterion
s?h = argmax
sh
Pr(p, k, sh|x). (5)
An example of the intuition behind these variables
can be seen in Figure 1.
Note that, since (p k sh) = y, Eq. 5 is very simi-
lar to Eq. 1. The main difference is that the argmax
search is now performed over the set of suffixes sh
that complete (p k) instead of complete sentences
(y in Eq. 1). This implies that we can use the same
models if the search procedures are adequately mod-
ified (Barrachina and others, 2008).
1Also known as stack decoding algorithm.
3 Phrase-based IMT
The phrase-based approach presented above can be
easily adapted for its use in an IMT scenario. The
most important modification is to rely on a word
graph that represents possible translations of the
given source sentence. The use of word graphs
in IMT has been studied in (Barrachina and oth-
ers, 2008) in combination with two different trans-
lation techniques, namely, the Alignment Templates
technique (Och et al, 1999; Och and Ney, 2004),
and the Stochastic Finite State Transducers tech-
nique (Casacuberta and Vidal, 2007).
3.1 Generation of word graphs
A word graph is a weighted directed acyclic graph,
in which each node represents a partial translation
hypothesis and each edge is labelled with a word of
the target sentence and is weighted according to the
scores given by an SMT model (see (Ueffing et al,
2002) for more details). In (Och et al, 2003), the
use of a word graph is proposed as interface between
an alignment-template SMT model and the IMT en-
gine. Analogously, in this work we will be using
a word graph built during the search procedure per-
formed on a PB SMT model.
During the search process performed by the above
mentioned beam search algorithm, it is possible to
create a segment graph. In such a graph, each node
represents a state of the SMT model, and each edge
a weighted transition between states labelled with a
sequence of target words. Whenever a hypothesis is
extended, we add a new edge connecting the state
of that hypothesis with the state of the extended hy-
pothesis. The new edge is labelled with the sequence
of target words that has been incorporated to the ex-
tended hypothesis and is weighted appropriately by
means of the score given by the SMT model.
Once the segment graph is generated, it can be
easily converted into a word graph by the introduc-
tion of artificial states for the words that compose
the target phrases associated to the edges.
3.2 IMT using word graphs
During the process of IMT for a given source sen-
tence, the system makes use of the word graph gen-
erated for that sentence in order to complete the pre-
fixes accepted by the human translator. Specifically,
487
SOURCE (x): Para encender la impresora:
REFERENCE (y): To power on the printer:
ITER-0 (p) ( )(s?h) To switch on:
ITER-1
(p) To
(sl) |switch on:
(s?h) power on the printer:
ITER-2
(p) To power on the printer:
(sl) ( )
(k) (#)
(s?h) ( )
FINAL (p ? y) To power on the printer:
Figure 2: Example of non-explicit positioning MA which solves an error of a missing word. In this case, the system
produces the correct suffix sh immediately after the user validates a prefix p, implicitly indicating that we wants the
suffix to be changed, without need of any further action. In ITER-1, character | indicates the position where a MA
was performed, sl is the suffix which was rejected by that MA, and s?h is the new suffix that the system suggests after
observing that sl is to be considered incorrect. Character # is a special character introduced by the user to indicate that
the hypothesis is to be accepted.
the system finds the best path in the word graph as-
sociated with a given prefix so that it is able to com-
plete the target sentence, being capable of providing
several completion suggestions for each prefix.
A common problem in IMT arises when the user
sets a prefix which cannot be found in the word
graph, since in such a situation the system is un-
able to find a path through the word graph and pro-
vide an appropriate suffix. The common procedure
to face this problem is to perform a tolerant search
in the word graph. This tolerant search uses the well
known concept of Levenshtein distance in order to
obtain the most similar string for the given prefix
(see (Och et al, 2003) for more details).
4 Enriching user?machine interaction
Although the IMT paradigm has proved to offer in-
teresting benefits to potential users, one aspect that
has not been reconsidered as of yet is the user?
machine interface. Hence, in traditional IMT the
system only received feedback whenever the user
typed in a new word. In this work, we show how
to enrich user?machine interaction by introducing
Mouse Actions (MA) as an additional information
source for the system. By doing so, we will consider
two types of MAs, i.e. non-explicit (or positioning)
MAs and interaction-explicit MAs.
4.1 Non-explicit positioning MAs
Before typing in a new word in order to correct a hy-
pothesis, the user needs to position the cursor in the
place where he wants to type such a word. In this
work, we will assume that this is done by perform-
ing a MA, although the same idea presented can also
be applied when this is done by some other means.
It is important to point out that, by doing so, the user
is already providing some very useful information to
the system: he is validating a prefix up to the posi-
tion where he positioned the cursor, and, in addition,
he is signalling that whatever word is located after
the cursor is to be considered incorrect. Hence, the
system can already capture this fact and provide a
new translation hypothesis, in which the prefix re-
mains unchanged and the suffix is replaced by a new
one in which the first word is different to the first
word of the previous suffix. We are aware that this
does not mean that the new suffix will be correct, but
given that we know that the first word in the previ-
ous suffix was incorrect, the worst thing which can
happen is that the the first word of the new suffix is
incorrect as well. However, if the new suffix hap-
pens to be correct, the user will happily find that he
does not need to correct that word any more.
An example of such behaviour can be seen in
Figure 2. In this example, the SMT system first
provides a translation which the user does not
488
like. Hence, he positions the cursor before word
?postscript?, with the purpose of typing in ?lists?.
By doing so, he is validating the prefix ?To print
a?, and signalling that he wants ?postscript? to be
replaced. Before typing in anything, the system re-
alises that he is going to change the word located
after the cursor, and replaces the suffix by another
one, which is the one the user had in mind in the
first place. Finally, the user only has to accept the
final translation.
We are naming this kind of MA non-explicit be-
cause it does not require any additional action from
the user: he has already performed a MA in order to
position the cursor at the place he wants, and we are
taking advantage of this fact to suggest a new suffix
hypothesis.
Since the user needs to position the cursor before
typing in a new word, it is important to point out
that any improvement achieved by introducing non-
explicit MAs does not require any further effort from
the user, and hence is considered to have no cost.
Hence, we are now considering two different situ-
ations: the first one, the traditional IMT framework,
in which the system needs to find a suffix according
to Eq. 5, and a new one, in which the system needs
to find a suffix in which the first word does not need
to be a given k, but needs to be different to a given
sl1. This constraint can be expressed by the follow-
ing equation:
s?h = argmax
sh:sh1 6=sl1
Pr(p, sh|x, sl) (6)
where sl is the suffix generated in the previous iter-
ation, already discarded by the user, and sl1 is the
first word in sl. k is omitted in this formula because
the user did not type any word at all.
4.2 Interaction-explicit MAs
If the system is efficient and provides suggestions
which are good enough, one could easily picture a
situation in which the expert would ask the system
to replace a given suffix, without typing in any word.
We will be modelling this as another kind of MA,
interaction-explicit MA, since the user needs to in-
dicate explicitly that he wants a given suffix to be
replaced, in contrast to the non-explicit positioning
MA. However, if the underlying MT engine provid-
ing the suffixes is powerful enough, the user would
quickly realise that performing a MA is less costly
that introducing a whole new word, and would take
advantage of this fact by systematically clicking be-
fore introducing any new word. In this case, as
well, we assume that the user clicks before an in-
correct word, hence demanding a new suffix whose
first word is different, but by doing so he is adopting
a more participative and interactive attitude, which
was not demanded in the case of non-explicit posi-
tioning MAs. An example of such an explicit MA
correcting an error can be seen in Figure 3
In this case, however, there is a cost associated to
this kind of MAs, since the user does need to per-
form additional actions, which may or may not be
beneficial. It is very possible that, even after asking
for several new hypothesis, the user will even though
need to introduce the word he had in mind, hence
wasting the additional MAs he had performed.
If we allow the user to perform n MAs before in-
troducing a word, this problem can be formalised in
an analogous way as in the case of non-explicit MAs
as follows:
s?h= argmax
sh:sh1 6=sil1?i?{1..n}
Pr(p, sh|x, s1l , s2l , . . . , snl ) (7)
where sil1 is the first word of the i-th suffix dis-
carded and s1l , s2l , . . . , snl is the set of all n suffixes
discarded.
Note that this kind of MA could also be imple-
mented with some other kind of interface, e.g. by
typing some special key such as F1 or Tab. How-
ever, the experimental results would not differ, and
in our user interface we found it more intuitive to
implement it as a MA.
5 Experimental setup
5.1 System evaluation
Automatic evaluation of results is a difficult problem
in MT. In fact, it has evolved to a research field with
own identity. This is due to the fact that, given an
input sentence, a large amount of correct and differ-
ent output sentences may exist. Hence, there is no
sentence which can be considered ground truth, as is
the case in speech or text recognition. By extension,
this problem is also applicable to IMT.
In this paper, we will be reporting our results as
measured by Word Stroke Ratio (WSR) (Barrachina
489
SOURCE (x): Seleccione el tipo de instalacio?n.
REFERENCE (y): Select the type of installation.
ITER-0 (p) ( )(s?h) Select the installation wizard.
ITER-1
(p) Select the
(sl) |installation wizard.
(s?h) install script.
ITER-2
(p) Select the
(k) type
(s?h) installation wizard.
ITER-3
(p) Select the type
(sl) |installation wizard.
(s?h) of installation.
ITER-4
(p) Select the type of installation.
(sl) ( )
(k) (#)
(s?h) ( )
FINAL (p ? y) Select the type of installation.
Figure 3: Example of explicit interactive MA which corrects an erroneous suffix. In this case, a non-explicit MA is
performed in ITER-1 with no success. Hence, the user introduces word ?type? in ITER-2, which leaves the cursor
position located immediately after word ?type?. In this situation the user would not need to perform a MA to re-
position the cursor and continue typing in order to further correct the remaining errors. However, since he has learnt
the potential benefit of MAs, he performs an interaction-explicit MA in order to ask for a new suffix hypothesis, which
happens to correct the error.
and others, 2008), which is computed as the quotient
between the number of word-strokes a user would
need to perform in order to achieve the translation
he has in mind and the total number of words in
the sentence. In this context, a word-stroke is in-
terpreted as a single action, in which the user types
a complete word, and is assumed to have constant
cost. Moreover, each word-stroke also takes into ac-
count the cost incurred by the user when reading the
new suffix provided by the system.
In the present work, we decided to use WSR in-
stead of Key Stroke Ratio (KSR), which is used in
other works on IMT such as (Och et al, 2003). The
reason for this is that KSR is clearly an optimistic
measure, since in such a scenario the user is often
overwhelmed by receiving a great amount of trans-
lation options, as much as one per key stroke, and
it is not taken into account the time the user would
need to read all those hypotheses.
In addition, and because we are also introducing
MAs as a new action, we will also present results in
terms of Mouse Action Ratio (MAR), which is the
quotient between the amount of explicit MAs per-
formed and the number of words of the final trans-
lation. Hence, the purpose is to elicit the number of
times the user needed to request a new translation
(i.e. performed a MA), on a per word basis.
Lastly, we will also present results in terms of
uMAR (useful MAR), which indicates the amount
of MAs which were useful, i.e. the MAs that actu-
ally produced a change in the first word of the suffix
and such word was accepted. Formally, uMAR is
defined as follows:
uMAR = MAC ? n ?WSCMAC (8)
where MAC stands for ?Mouse Action Count?,
WSC for ?Word Stroke Count? and n is the max-
imum amount of MAs allowed before the user types
in a word. Note that MAC?n ?WSC is the amount
of MAs that were useful since WSC is the amount
of word-strokes the user performed even though he
had already performed n MAs.
Since we will only use single-reference WSR and
MAR, the results presented here are clearly pes-
simistic. In fact, it is relatively common to have the
underlying SMT system provide a perfectly correct
490
Table 1: Characteristics of Europarl for each of the sub-
corpora. OoV stands for ?Out of Vocabulary? words,
Dev. for Development, K for thousands of elements and
M for millions of elements.
De En Es En Fr En
Tr
ai
n
in
g Sentences 751K 731K 688K
Run. words 15.3M16.1M 15.7M15.2M 15.6M13.8M
Avg. len. 20.3 21.4 21.5 20.8 22.7 20.1
Voc. 195K 66K 103K 64K 80K 62K
D
ev
.
Sentences 2000 2000 2000
Run. words 55K 59K 61K 59K 67K 59K
Avg. len. 27.6 29.3 30.3 29.3 33.6 29.3
OoV 432 125 208 127 144 138
Te
st
Sentences 2000 2000 2000
Run. words 54K 58K 60K 58K 66K 58K
Avg. len. 27.1 29.0 30.2 29.0 33.1 29.3
OoV 377 127 207 125 139 133
translation, which is ?corrected? by the IMT proce-
dure into another equivalent translation, increasing
WSR and MAR significantly by doing so.
5.2 Corpora
Our experiments were carried out on the Eu-
roparl (Koehn, 2005) corpus, which is a corpus
widely used in SMT and that has been used in sev-
eral MT evaluation campaigns. Moreover, we per-
formed our experiments on the partition established
for the Workshop on Statistical Machine Translation
of the NAACL 2006 (Koehn and Monz, 2006). The
Europarl corpus (Koehn, 2005) is built from the pro-
ceedings of the European Parliament. Here, we will
focus on the German?English, Spanish?English and
French?English tasks, since these were the language
pairs selected for the cited workshop. The corpus is
divided into three separate sets: one for training, one
for development, and one for test. The characteris-
tics of the corpus can be seen in Table 1.
5.3 Experimental results
As a first step, we built a SMT system for each of
the language pairs cited in the previous subsection.
This was done by means of the Moses toolkit (Koehn
and others, 2007), which is a complete system for
building Phrase-Based SMT models. This toolkit in-
volves the estimation from the training set of four
different translation models, which are in turn com-
Table 2: WSR improvement when considering non-
explicit MAs. ?rel.? indicates the relative improvement.
All results are given in %.
pair baseline non-explicit rel.
Es?En 63.0?0.9 59.2?0.9 6.0?1.4
En?Es 63.8?0.9 60.5?1.0 5.2?1.6
De?En 71.6?0.8 69.0?0.9 3.6?1.3
En?De 75.9?0.8 73.5?0.9 3.2?1.2
Fr?En 62.9?0.9 59.2?1.0 5.9?1.6
En?Fr 63.4?0.9 60.0?0.9 5.4?1.4
bined in a log-linear fashion by adjusting a weight
for each of them by means of the MERT (Och, 2003)
procedure, optimising the BLEU (Papineni et al,
2002) score obtained on the development partition.
This being done, word graphs were generated
for the IMT system. For this purpose, we used a
multi-stack phrase-based decoder which will be dis-
tributed in the near future together with the Thot
toolkit (Ortiz-Mart??nez et al, 2005). We discarded
the use of the Moses decoder because preliminary
experiments performed with it revealed that the de-
coder by (Ortiz-Mart??nez et al, 2005) performs
clearly better when used to generate word graphs
for use in IMT. In addition, we performed an ex-
perimental comparison in regular SMT with the Eu-
roparl corpus, and found that the performance dif-
ference was negligible. The decoder was set to
only consider monotonic translation, since in real
IMT scenarios considering non-monotonic transla-
tion leads to excessive waiting time for the user.
Finally, the word graphs obtained were used
within the IMT procedure to produce the reference
translation contained in the test set, measuring WSR
and MAR. The results of such a setup can be seen in
Table 2. As a baseline system, we report the tradi-
tional IMT framework, in which no MA is taken into
account. Then, we introduced non-explicit MAs, ob-
taining an average improvement in WSR of about
3.2% (4.9% relative). The table also shows the
confidence intervals at a confidence level of 95%.
These intervals were computed following the boot-
strap technique described in (Koehn, 2004). Since
the confidence intervals do not overlap, it can be
stated that the improvements obtained are statisti-
cally significant.
491
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 50
 100
 150
 200
 250
 300
W
SR
M
AR
max. MAs per incorrect word
Spanish -> English
WSR
MAR
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 4
 6
 8
 10
 12
W
SR
u
M
AR
max. MAs per incorrect word
Spanish -> English
WSR
uMAR
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 50
 100
 150
 200
 250
 300
W
SR
M
AR
max. MAs per incorrect word
German -> English
WSR
MAR
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 4
 6
 8
 10
 12
W
SR
u
M
AR
max. MAs per incorrect word
German -> English
WSR
uMAR
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 50
 100
 150
 200
 250
 300
W
SR
M
AR
max. MAs per incorrect word
French -> English
WSR
MAR
 40
 45
 50
 55
 60
 65
 70
 0  1  2  3  4  5
 4
 6
 8
 10
 12
W
SR
u
M
AR
max. MAs per incorrect word
French -> English
WSR
uMAR
Figure 4: WSR improvement when considering one to five maximum MAs. All figures are given in %. The left
column lists WSR improvement versus MAR degradation, and the right column lists WSR improvement versus uMAR.
Confidence intervals at 95% confidence level following (Koehn, 2004).
Once the non-explicit MAs were considered and
introduced into the system, we analysed the effect
of performing up to a maximum of 5 explicit MAs.
Here, we modelled the user in such a way that, in
case a given word is considered incorrect, he will
always ask for another translation hypothesis until
he has asked for as many different suffixes as MAs
considered. The results of this setup can be seen in
Figure 4. This yielded a further average improve-
ment in WSR of about 16% (25% relative improve-
ment) when considering a maximum of 5 explicit
MAs. However, relative improvement in WSR and
492
uMAR increase drop significantly when increasing
the maximum allowed amount of explicit MAs from
1 to 5. For this reason, it is difficult to imagine that
a user would perform more than two or three MAs
before actually typing in a new word. Nevertheless,
just by asking twice for a new suffix before typing
in the word he has in mind, the user might be saving
about 15% of word-strokes.
Although the results in Figure 4 are only
for the translation direction ?foreign??English,
the experiments in the opposite direction (i.e.
English??foreign?) were also performed. How-
ever, the results were very similar to the ones dis-
played here. Because of this, and for clarity pur-
poses, we decided to omit them and only display the
direction ?foreign??English.
6 Conclusions and future work
In this paper, we have considered new input sources
for IMT. By considering Mouse Actions, we have
shown that a significant benefit can be obtained, in
terms of word-stroke reduction, both when consid-
ering only non-explicit MAs and when considering
MAs as a way of offering the user several suffix hy-
potheses. In addition, we have applied these ideas
on a state-of-the-art SMT baseline, such as phrase-
based models. To achieve this, we have first ob-
tained a word graph for each sentence which is to be
translated. Experiments were carried out on a refer-
ence corpus in SMT.
Note that there are other systems (Esteban and
others, 2004) that, for a given prefix, provide n-
best lists of suffixes. However, the functionality of
our system is slightly (but fundamentally) different,
since the suggestions are demanded to be different
in their first word, which implies that the n-best list
is scanned deeper, going directly to those hypothe-
ses that may be of interest to the user. In addition,
this can be done ?on demand?, which implies that
the system?s response is faster and that the user is
not confronted with a large list of hypotheses, which
often results overwhelming.
As future work, we are planning on performing a
human evaluation that assesses the appropriateness
of the improvements described.
Acknowledgements
This work has been partially supported by the Span-
ish MEC under scholarship AP2005-4023 and un-
der grants CONSOLIDER Ingenio-2010 CSD2007-
00018, and by the EC (FEDER) and the Spanish
MEC under grant TIN2006-15694-CO2-01.
References
D. J. Arnold, 2003. Computers and Translation: A trans-
lator?s guide, chapter 8, pages 119?142.
S. Barrachina et al 2008. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, page In press.
F. Casacuberta and E. Vidal. 2007. Learning finite-state
models for machine translation. Machine Learning,
66(1):69?91.
J. Esteban et al 2004. Transtype2 - an innovative
computer-assisted translation system. In The Compan-
ion Volume to the Proc. ACL?04, pages 94?97.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Proc. of
EMNLP?02, pages 148?155.
G. Foster. 2002. Text Prediction for Translators. Ph.D.
thesis, Universite? de Montre?al.
J. Hutchings and H. Somers. 1992. An introduction to
machine translation. In Ed. Academic Press.
J. Hutchins. 1999. Retrospect and prospect in computer-
based translation. In Proc. of MT Summit VII, pages
30?44.
M. Kay. 1997. It?s still the proper place. Machine Trans-
lation, 12(1-2):35?38.
P. Koehn and C. Monz, editors. 2006. Proc. of the Work-
shop on SMT.
P. Koehn et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of the ACL?07.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT/NAACL?03,
pages 48?54.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP?04, pages
388?395, Barcelona, Spain.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of the MT Summit X,
pages 79?86.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost
translator?s productivity. Machine Translation,
15(4):77?98.
Bruce T. Lowerre. 1976. The harpy speech recogni-
tion system. Ph.D. thesis, Carnegie Mellon University,
Pittsburgh, PA, USA.
493
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the ACL?02, pages 295?302.
F.J. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Comput. Lin-
guist., 30(4):417?449.
F. Och, C. Tillmann, and H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
Proc. of EMNLP/WVLC?99, pages 20?28.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient search for
interactive statistical machine translation. In Proc. of
EACL?03, pages 387?393.
F.J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. of ACL?03, pages
160?167.
D. Ortiz-Mart??nez, I. Garc??a-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statisti-
cal translation models. In Proc. of the MT Summit X,
pages 141?148.
K. Papineni, S. Roukos, and T. Ward. 1998. Maximum
likelihood and discriminative training of direct transla-
tion models. In Proc. of ICASSP?98, pages 189?192.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proc. of ACL?02.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP?02, pages 156?163.
E. Vidal et al 2007. Interactive pattern recognition. In
Proc. of MLMI?07, pages 60?71.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Proc. of KI?02, pages
18?32.
494
Coling 2010: Poster Volume, pages 1077?1085,
Beijing, August 2010
Log-linear weight optimisation via Bayesian Adaptation in Statistical
Machine Translation
Germa?n Sanchis-Trilles and Francisco Casacuberta
Departamento de Sistemas Informa?ticos y Computacio?n
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{gsanchis,fcn}@dsic.upv.es
Abstract
We present an adaptation technique for
statistical machine translation, which ap-
plies the well-known Bayesian learning
paradigm for adapting the model param-
eters. Since state-of-the-art statistical ma-
chine translation systems model the trans-
lation process as a log-linear combination
of simpler models, we present the formal
derivation of how to apply such paradigm
to the weights of the log-linear combina-
tion. We show empirical results in which
a small amount of adaptation data is able
to improve both the non-adapted system
and a system which optimises the above-
mentioned weights on the adaptation set
only, while gaining both in reliability and
speed.
1 Introduction
The adaptation problem is a very common issue in
statistical machine translation (SMT), where it is
frequent to have very large collections of bilingual
data belonging to e.g. proceedings from interna-
tional entities such as the European Parliament or
the United Nations. However, if we are currently
interested in translating e.g. printer manuals or
news data, we will need to find a way in which we
can take advantage of such data.
The grounds of modern SMT were established
in (Brown et al, 1993), where the machine trans-
lation problem was defined as follows: given a
sentence f from a certain source language, an
equivalent sentence e? in a given target language
that maximises the posterior probability is to be
found. According to the Bayes decision rule, such
statement can be specified as follows:
e? = argmax
e
Pr(e|f) (1)
Recently, a direct modelling of the posterior
probability Pr(e|f) has been widely adopted, and,
to this purpose, different authors (Papineni et al,
1998; Och and Ney, 2002) proposed the use of the
so-called log-linear models, where
p(e|f) = exp
?K
k=1 ?khk(f , e)?
e? exp
?K
k=1 ?khk(f , e?)
(2)
and the decision rule is given by the expression
e? = argmax
e
K?
k=1
?khk(f , e) (3)
where hk(f , e) is a score function representing an
important feature for the translation of f into e, as
for example the language model of the target lan-
guage, a reordering model or several translation
models. K is the number of models (or features)
and ?k are the weights of the log-linear combina-
tion. Typically, the weights ? = [?1, . . . , ?K ]T
are optimised with the use of a development set.
The use of log-linear models implied an impor-
tant break-through in SMT, allowing for a signifi-
cant increase in the quality of the translations pro-
duced. In this work, we present a Bayesian tech-
nique for adapting the weights of such log-linear
models according to a small set of adaptation data.
In this paper, we will be focusing on adapting
the weights vector ?, since appropriate values of
such vector for a given domain do not necessarily
imply a good combination in other domains. One
na??ve way in which some sort of adaptation can
be performed on ? is to re-estimate these weights
1077
from scratch only on the adaptation data. How-
ever, such re-estimation may not be a good idea,
whenever the amount of adaptation data available
is not too big. On the one hand, because small
amounts of adaptation data may easily yield over-
trained values of ?, which may even lead to a
degradation of the translation quality. On the other
hand, because in some scenarios it is not feasible
to re-estimate them because of the time it would
take. Moreover, considering a re-estimation of
? by using both the out-of-domain data and the
adaptation set would not be appropriate either. For
small amounts of adaptation data, such data would
have no impact on the final value of ?, and the
time required would be even higher. One such
situation may be the Interactive Machine Trans-
lation (IMT) paradigm (Barrachina et al, 2009),
in which a human translator may start translating
a new document, belonging to a specific domain,
and the system is required to produce an appro-
priate output as soon as possible without any prior
re-training.
In this paper, a Bayesian adaptation approach
solving both problems is presented. Nevertheless,
adapting ? constitutes just a first step towards
the adaptation of all the parameters of the SMT
model.
The rest of this paper is structured as follows. In
next Section, we perform a brief review of current
approaches to adaptation and Bayesian learning in
SMT. Section 3 describes the typical framework
for phrase-based translation in SMT. In Section 4,
we present the way in which we apply Bayesian
adaptation (BA) to log-linear models in SMT. In
Section 5, we describe the practical approxima-
tions applied before implementing the BA tech-
nique described. In Section 6, experimental de-
sign and results are detailed. Conclusions and fu-
ture work are explained in Section 7.
2 Related work
Adaptation in SMT is a research field that is re-
ceiving an increasing amount of attention. In
(Nepveu et al, 2004), adaptation techniques were
applied to IMT, following the ideas by (Kuhn
and Mori, 1990) and adding cache language mod-
els (LM) and TMs to their system. In (Koehn
and Schroeder, 2007), different ways to combine
available data belonging to two different sources
was explored; in (Bertoldi and Federico, 2009)
similar experiments were performed, but consid-
ering only additional source data. In (Civera and
Juan, 2007), alignment model mixtures were ex-
plored as a way of performing topic-specific adap-
tation. Other authors (Zhao et al, 2004; Sanchis-
Trilles et al, 2009), have proposed the use of clus-
tering in order to extract sub-domains of a large
parallel corpus and build more specific LMs and
TMs, which are re-combined in test time.
With respect to BA in SMT, the authors are not
aware of any work up to the date that follows such
paradigm. Nevertheless, there have been some re-
cent approaches towards dealing with SMT from
the Bayesian learning point of view. In (Zhang
et al, 2008), Bayesian learning was applied for
estimating word-alignments within a synchronous
grammar.
3 Phrase-based SMT
One of the most popular instantiations of log-
linear models in SMT are phrase-based (PB) mod-
els (Zens et al, 2002; Koehn et al, 2003). PB
models allow to capture contextual information to
learn translations for whole phrases instead of sin-
gle words. The basic idea of PB translation is to
segment the source sentence into phrases, then to
translate each source phrase into a target phrase,
and finally reorder the translated target phrases in
order to compose the target sentence. For this
purpose, phrase-tables are produced, in which a
source phrase is listed together with several tar-
get phrases and the probability of translating the
former into the latter. PB models were employed
throughout this work.
Typically, the weights of the log-linear com-
bination in Equation 3 are optimised by means
of Minimum Error Rate Training (MERT) (Och,
2003). Such algorithm consists of two basic steps.
First, n-best hypotheses are extracted for each one
of the sentences of a given development set. Next,
the optimum ? is computed so that the best hy-
potheses in the n-best list, according to a reference
translation and a given metric, are ranked higher
within such n-best list. These two steps are re-
peated until convergence.
This approach has two main problems. On the
1078
one hand, that it heavily relies on having a fair
amount of data available as development set. On
the other hand, that it only relies on the data in
the development set. These two problems have
as consequence that, if the development set made
available to the system is not big enough, MERT
will most likely become unstable and fail in ob-
taining an appropriate weight vector ?.
However, it is quite common to have a great
amount of data available in a given domain, but
only a small amount from the specific domain we
are interested in translating. Precisely this sce-
nario is appropriate for BA: under this paradigm,
the weight vector ? is biased towards the opti-
mal one according to the adaptation set, while
avoiding over-training towards such set by not
forgetting the generality provided by the training
set. Furthermore, recomputing ? from scratch
by means of MERT may imply a computational
overhead which may not be acceptable in certain
environments, such as SMT systems configured
for online translation, IMT or Computer Assisted
Translation, in which the final human user is wait-
ing for the translations to be produced.
4 Bayesian adaptation for SMT
The main idea behind Bayesian learning (Duda et
al., 2001; Bishop, 2006) is that model parameters
are viewed as random variables having some kind
of a priori distribution. Observing these random
variables leads to a posterior density, which typi-
cally peaks at the optimal values of these parame-
ters. Following the notation in Equation 1, previ-
ous statement is specified as
p(e|f ;T ) =
?
p(e, ?|f ;T )d? (4)
where T represents the complete training set and
? are the model parameters.
However, since we are interested in Bayesian
adaptation, we need to consider one training set
T and one adaptation set A, leading to
p(e|f ;T,A) ?
?
p(?|T,A)p(e|f , ?)d? (5)
In Equation 5, the integral over the complete para-
metric space forces the model to take into account
all possible values of the model parameters, al-
though the prior over the parameters implies that
our model will prefer parameter values which are
closer to our prior knowledge. Two assumptions
have been made: first, that the output sentence e
only depends on the model parameters (and not on
the complete training and adaptation data). Sec-
ond, that the model parameters do not depend
on the actual input sentence f . Such simplifica-
tions lead to a decomposition of the integral in
two parts: the first one, p(?|T,A) will assess how
good the current model parameters are, and the
second one, p(e|f , ?), will account for the quality
of the translation e given the current model pa-
rameters.
Then, the decision rule given in Equation 1 is
redefined as
e? = argmax
e
Pr(e|f ;T,A) (6)
Operating with the probability of ?, we obtain:
p(?|T,A) = p(A|?;T ) p(?|T )? p(A|?) p(?|T ) d? (7)
p(A|?;T ) =
?
?a?A
p(fa|?) p(ea|fa, ?) (8)
where the probability of the adaptation data has
been assumed to be independent of the training
data and has been modelled as the probability of
each bilingual sample (fa, ea) ? A being gener-
ated by our translation model.
Assuming that the model parameters depend on
the training data and follow a normal distribution,
we obtain
p(?|T )= 1(2pi)d/2 exp
{
?12(???T)
T(???T)
}
(9)
where ?T is the set of parameters estimated on the
training set and the variance has been assumed to
be bounded for all parameters. d is the dimension-
ality of ?.
Lastly, assuming that our translation model is a
log-linear model as described in Equation 3 and
that the only parameters we want to adapt are the
log-linear weights:
p(e|f , ?) = exp
?
k ?k fk(f , e)?
e? exp
?
k ?k fk(f , e?)
(10)
1079
where the model parameters ? have been instanti-
ated to include only the log-linear weights ?.
Finally, combining Equations 8, 9 and 10, and
considering only as model parameters the log-
linear weights, we obtain:
p(e|f ;T,A)=Z
?
p(A|?;T )p(?|T )p(e|f ,?)d?
= Z
? ?
?a?A
exp?k ?k fk(fa, ea)?
e? exp
?
k ?k fk(fa, e?)
?
exp
{
?12(???T )
T (???T )
}
?
exp?k ?k fk(f , e)?
e? exp
?
k ?k fk(f , e?)
d? (11)
where Z is the denominator present in the previ-
ous equation and may be factored out because it
does not depend on the integration variable. It has
also been assumed that p(fa|?) is uniform and can
also be factored out.
5 Practical approximations
Although the integral described in Equation 11 is
the right thing to do from the theoretical point of
view, there are several issues which need to be
treated first before implementing it.
Since computing the integral over the complete
parametric space is computationally impossible in
the case of SMT, we decided to perform a Monte
Carlo like sampling of these parameters by assum-
ing that the parameters follow a normal distribu-
tion centred in ?T , the weight vector obtained
from the training data. This sampling was done
by choosing alternatively only one of the weights
in ?T , modifying it randomly within a given inter-
val, and re-normalising accordingly. Equation 11
is approximated in practise as
p(e|f ;T,A) =
?
?m?MC(?T )
p(A|?;T )p(?|T )p(e|f ,?)
where MC(?T ) is the set of ?m weights gener-
ated by the above-mentioned procedure.
There is still one issue when trying to imple-
ment Equation 11. The denominator within the
components p(A|?;T ) and p(e|f ,?) contains a
sum over all possible sentences of the target lan-
guage, which is not computable. For this reason,
?
e? is approximated as the sum over all the hy-
pothesis within a given n-best list. Moreover, in-
stead of performing a full search of the best pos-
sible translation of a given input sentence, we will
perform a rerank of the n-best list provided by the
decoder according to Equation 11.
Typical state-of-the-art PB SMT systems do not
guarantee complete coverage of all possible sen-
tence pairs due to the great number of heuris-
tic decisions involved in the estimation of the
translation models. Moreover, out-of-vocabulary
words may imply that the SMT model is unable
to explain a certain bilingual sentence completely.
Hence, p(A|?;T ) is approximated as
p(A|?;T )?
?
?a?A
exp?k ?k fk(fa, e?a)?
e? exp
?
k ?kfk(fa, e?)
(12)
where e? represents the best hypothesis the search
algorithm is able to produce, according to a given
translation quality measure. As in Equation 11,
p(fa|?) has been assumed uniform.
Once the normalisation factor within Equa-
tion 7 has been removed, and the above-
mentioned approximations have been introduced,
p(e|f ;T,A) is no longer a probability. This fact
cannot be underestimated, since it means that the
terms p(A|?;T ) and p(e|f ,?) on the one hand,
and p(?|T ) on the other, may have very different
numeric ranges. For this reason, and in order to
weaken the influence of this fact, we introduce a
leveraging term ?, such that
p(e|f ;T,A) =
?
?m?MC(?T )
(p(A|?;T )p(e|f ,?)) 1? p(?|T ) (13)
Although there are other, more standard, ways of
adding this leveraging term, we chose this one for
numeric reasons.
6 Experiments
6.1 Experimental setup
Translation quality will be assessed by means
of BLEU and TER scores. BLEU measures n-
gram precision with a penalty for sentences that
are too short (Papineni et al, 2001), whereas
TER (Snover et al, 2006) is an error metric that
1080
Spanish English
Training
Sentences 731K
Run. words 15.7M 15.2M
Vocabulary 103K 64K
Development
Sentences 2K
Run. words 61K 59K
OoV words 208 127
Table 1: Main figures of the Europarl corpus. OoV
stands for Out of Vocabulary. K/M stands for
thousands/millions of elements.
Spanish English
Test 2008
Sentences 2051
Run. words 50K 53K
OoV. words 1247 1201
Test 2010
Sentences 2489
Run. words 62K 66K
OoV. words 1698 1607
Table 2: Main figures of the News-Commentary
test sets. OoV stands for Out of Vocabulary words
with respect to the Europarl corpus.
computes the minimum number of edits required
to modify the system hypotheses so that they
match the references. Possible edits include in-
sertion, deletion, substitution of single words and
shifts of word sequences.
For computing e? as described in Equation 12,
TER was used, since BLEU implements a geo-
metrical average which is zero whenever there is
no common 4-gram between reference and hy-
pothesis. Hence, it is not well suited for our pur-
poses since the complete set of n-best candidates
provided by the decoder can score zero.
As a first baseline system, we trained a SMT
system on the Europarl Spanish?English training
data, in the partition established in the Workshop
on SMT of the NAACL 2006 (Koehn and Monz,
2006), using the training and development data
provided that year. The Europarl corpus (Koehn,
2005) is built from the transcription of European
Parliament speeches published on the web. Statis-
tics are provided in Table 1.
We used the open-source MT toolkit
Moses (Koehn et al, 2007)1 in its default
monotonic setup, and estimated the weights of
the log-linear combination using MERT on the
Europarl development set. A 5-gram LM with
interpolation and Kneser-Ney smoothing (Kneser
and Ney, 1995) was also estimated.
Since our purpose is to adapt the initial weight
1Available from http://www.statmt.org/moses/
vector obtained during the training stage (i.e. the
one obtained after running MERT on the Eu-
roparl development set), the tests sets provided for
the 2008 and 2010 evaluation campaigns of the
above-mentioned workshop (Table 2) were also
used. These test sets, unlike the one provided in
2006, were extracted from a news data corpus, and
can be considered out of domain if the system has
been trained on Europarl data.
All the experiments displaying BA results were
carried out by sampling a total of 100 random
weights, according to preliminary investigation,
following the procedure described in Section 5.
For doing this, one single weight was added a ran-
dom amount between 0.5 and ?0.5, and then the
whole ? was re-normalised.
With the purpose of providing robustness to the
results, every point in each plot of this paper con-
stitutes the average of 10 repetitions, in which
the adaptation data was randomly drawn from the
News-Commentary test set 2008.
6.2 Comparison between BA and MERT
The effect of increasing the number of adaptation
samples made available to the system was inves-
tigated. The adaptation data was used either for
estimating ? using MERT, or as adaptation sam-
ple for our BA technique. Results can be seen in
Figure 1. The ? scaling factor described in Equa-
tion 13 was set to 8. As it can be seen, the BA
adaptation technique is able to improve consis-
tently the translation quality obtained by the non-
adapted system, both in terms of BLEU and TER.
These improvements are quite stable even with
as few as 10 adaptation samples. This result is
very interesting, since re-estimating ? by means
of MERT is only able to yield improvements when
provided with at least 100 adaptation samples, dis-
playing a very chaotic behaviour until that point.
In order to get a bit more insight about this
chaotic behaviour, confidence interval sizes are
shown in Figure 2, at a 95% confidence level, re-
sulting of the repetitions described above. MERT
yields very large confidence intervals (as large
as 10 TER/BLEU points for less than 100 sam-
ples), turning a bit more stable from that point
on, where the size of the confidence interval con-
verges slowly to 1 TER/BLEU point. In contrast,
1081
 57
 58
 59
 60
 61
 62
 63
 10  100  1000
TE
R
Number of adaptation samples
BA ? =  8
mertbaseline
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 22
 22.5
 10  100  1000
BL
EU
Number of adaptation samples
BA ? =  8
mertbaseline
Figure 1: Comparison of translation quality, as measured by BLEU and TER, for baseline system,
adapted systems by means of BA and MERT. Increasing number of samples is considered.
 0.001
 0.01
 0.1
 1
 10
 100
 10  100  1000
TE
R C
DS
Number of adaptation samples
BA ? =  1BA ? =  32
mert
 0.001
 0.01
 0.1
 1
 10
 10  100  1000
BL
EU
 CD
S
Number of adaptation samples
BA ? =  1BA ? =  32
mert
Figure 2: Confidence interval sizes (CDS) for MERT and two BA systems, for different number of
adaptation samples. For visibility purposes, both axes are in logarithmic scale.
our BA technique yields very small confidence in-
tervals, about half a TER/BLEU point in the worst
case, with only 10 adaptation samples. This is
worth emphasising, since estimating ? by means
of MERT when very few adaptation data is avail-
able may improve the final translation quality, but
may also degrade it to a much larger extent. In
contrast, our BA technique shows stable and reli-
able improvements from the very beginning. Pre-
cisely under such circumstances is an adaptation
technique useful: when the amount of adaptation
data is small. In other cases, the best thing one
can do is to re-estimate the model parameters from
scratch.
Example translations, extracted from the exper-
iments detailed above, are shown in Figure 5.
6.3 Varying ?
So as to understand the role of scaling factor ?,
results obtained varying it are shown in Figure 3.
Several things should be noted about these plots:
? Increasing ? leads to smoother adaptation
curves. This is coherent with the confidence
interval sizes shown in Figure 1.
? Smaller values of ? lead to a slight degrada-
tion in translation quality when the amount
of adaptation samples becomes larger. The
reason for this can be explained by look-
ing at Equation 13. Since p(A|?;T ) is
implemented as a product of probabilities,
the more adaptation samples the smaller be-
comes p(A|?;T ), and a higher value of ? is
needed to compensate this fact. This sug-
gests the need of a ? which depends on the
size of the adaptation sample.
? Larger values of ? do not suffer the prob-
lem described above, but yield smaller im-
provements in terms of translation quality for
smaller amount of samples.
1082
 57.8
 57.9
 58
 58.1
 58.2
 58.3
 10  100  1000
TE
R
Number of adaptation samples
? =  1? =  2? =  4? =  8? =  16? =  32
 21.9
 22
 22.1
 22.2
 22.3
 22.4
 22.5
 10  100  1000
BL
EU
Number of adaptation samples
? =  1? =  2? =  4? =  8? =  16? =  32
Figure 3: Translation quality comparison for different ? values and number of adaptation samples.
It might seem odd that translation quality as
measured by BLEU drops almost constantly as the
number of adaptation samples increases. How-
ever, it must be noted that the BA technique im-
plemented is set to optimise TER, and not BLEU.
Analysing the BLEU scores obtained, we realised
that the n-gram precision does increase, but the
final BLEU score drops because of a worsening
brevity penalty, which is not taken into account
when optimising the TER score.
6.3.1 Increasing the n-best order
The effect of increasing the order of n-best con-
sidered was also analysed. In order to avoid an
overwhelming amount of results, only those ob-
tained when considering 100 adaptation samples
are displayed in Figure 4. As it can be seen,
TER drops monotonically for all ? values, until
about 800, where it starts to stabilise. Similar
behaviour is observed in the case of BLEU, al-
though depending on ? the curve shows an im-
provement or a degradation. Again, this is due
to the brevity penalty, which TER does not imple-
ment, and which induces this inverse correlation
between TER and BLEU when optimising TER.
7 Conclusions and future work
We have presented a Bayesian theoretical frame-
work for adapting the parameters of a SMT sys-
tem. We have derived the equations needed to im-
plement BA of the log-linear weights of a SMT
system, and present promising results with a state-
of-the-art SMT system using standard corpora in
SMT. Such results prove that the BA framework
can be very effective when adapting the men-
tioned weights. Consistent improvements are ob-
tained over the baseline system with as few as
10 adaptation samples. The BA technique imple-
mented is able to yield results comparable with
a complete re-estimation of the parameters even
when the amount of adaptation data is sufficient
for such re-estimation to be feasible. Experi-
mental results show that our adaptation technique
proves to be much more stable than MERT, which
relies very heavily on the amount of adaptation
data and turns very unstable whenever few adap-
tation samples are available. It should be empha-
sised that an adaptation technique, by nature, is
only useful whenever few adaptation data is avail-
able, and our technique proves to behave well in
such context.
Intuitively, the BA technique presented needs
first to compute a set of random weights, which
are the result of sampling a gaussian distribution
whose mean is the best weight vector obtained in
training. Then, each hypothesis of a certain test
source sentence is rescored according to the fol-
lowing three components:
? The probability of the adaptation corpus un-
der each specific random weight
? The probability of such random weight ac-
cording to a prior over the weight vector
? The probability of the current hypothesis un-
der those weights
Concerning computational time, our adaptation
technique can easily be implemented within the
decoder itself, without any significant increase in
computational complexity. We consider this im-
1083
 57.6
 57.7
 57.8
 57.9
 58
 58.1
 100  200  300  400  500  600  700  800  900  1000
TE
R
Order of n-best considered
? = 1? = 2? = 4? = 8? = 16? = 32
 21.7
 21.8
 21.9
 22
 22.1
 22.2
 22.3
 22.4
 22.5
 22.6
 100  200  300  400  500  600  700  800  900  1000
BL
EU
Order of n-best considered
?=1?=2?=4?=8?=16?=32
Figure 4: Translation quality for different ? values and n-best sizes considered in the BA system.
source en afganista?n , barack obama espera que se repita el milagro .
reference barack obama hopes that , in afghanistan , the miracle will repeat itself .
baseline in afghanistan , barack obama waiting to be repeated the miracle .
BA s10 in afghanistan , barack obama expected to repeat the miracle .
BA s600 in afghanistan , barack obama expected to repeat the miracle .
MERT s10 in afghanistan , barack obama expected to repeat of the miracle .
MERT s600 in afghanistan , barack obama hopes that a repetition of the miracle .
source al final todo fue ma?s rpido de lo que se penso? .
reference it all happened a lot faster than expected .
baseline at the end of all was more quickly than we thought .
BA s10 ultimately everything was more quickly than we thought .
BA s600 ultimately everything was more quickly than we though .
MERT s10 the end all was quicker than i thought .
MERT s600 ultimately everything was quicker than i thought .
Figure 5: Example of translations found in the corpus. s10 means that only 10 adaptation samples
were considered, whereas s600 means that 600 were considered.
portant, since it implies that rerunning MERT for
each adaptation set is not needed, and this is im-
portant whenever the final system is set up in an
on-line environment.
The derivation presented here can be easily ex-
tended in order to adapt the feature functions of
the log-linear model (i.e. not the weights). This is
bound to have a more important impact on transla-
tion quality, since the amount of parameters to be
adapted is much higher. We plan to address this
issue in future work.
In addition, very preliminary experiments show
that, when considering reordering, the advantages
described here are larger.
A preliminary version of the present paper
was accepted at the Joint IAPR International
Workshops on Structural and Syntactic Pattern
Recognition and Statistical Techniques in Pattern
Recognition 2010. The main contributions of
the present paper constitute more extensive ex-
periments, which have been conducted on stan-
dard SMT corpora. Furthermore, in this paper we
present the results of adding the leveraging term
?, of applying a random, Monte-Carlo like weight
sampling (which was not done previously), and an
extensive analysis of the effect of varying the or-
der of n-best considered.
We also plan to implement Markov Chain
Monte Carlo for sampling the parameters, and
analyse the effect of combining the in-domain and
out of domain data for MERT. Such results were
not included here for time constraints.
Acknowledgments
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV ?Consolider Ingenio 2010?
program (CSD2007-00018) and the iTrans2
(TIN2009-14511) project. Also supported by
the Spanish MITyC under the erudito.com (TSI-
020110-2009-439) project and by the Generalitat
Valenciana under grant Prometeo/2009/014.
The authors would like to thank the anonimous
reviewers for their constructive comments.
1084
References
Barrachina, S., O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda H. Ney, J. Toma?s,
and E. Vidal. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
Bertoldi, N. and M. Federico. 2009. Domain adapta-
tion in statistical machine translation with monolin-
gual resources. In Proc. of EACL WMT.
Bishop, C. M. 2006. Pattern Recognition and Ma-
chine Learning. Springer.
Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics,
volume 19, pages 263?311, June.
Civera, J. and A. Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proc. of ACL WMT.
Duda, R., P. Hart, and D. Stork. 2001. Pattern Classi-
fication. Wiley-Interscience.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, II:181?
184, May.
Koehn, P. and C. Monz, editors. 2006. Proc. on the
Workshop on SMT. Association for Computational
Linguistics, June.
Koehn, P. and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proc. of ACL WMT.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT/NAACL?03,
pages 48?54.
Koehn et al, P. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of the ACL
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Koehn, P. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Kuhn, R. and R. De Mori. 1990. A cache-based nat-
ural language model for speech recognition. IEEE
Transactions on PAMI, 12(6):570?583.
Nepveu, L., G. Lapalme, P. Langlais, and G. Foster.
2004. Adaptive language and translation models for
interactive machine translation. In Proc. of EMNLP.
Och, F. and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the ACL?02, pages
295?302.
Och, F.J. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. of Annual Meet-
ing of the ACL, July.
Papineni, K., S. Roukos, and T. Ward. 1998. Max-
imum likelihood and discriminative training of di-
rect translation models. In Proc. of ICASSP, pages
189?192.
Papineni, K., A. Kishore, S. Roukos, T. Ward, and
W. Jing Zhu. 2001. Bleu: A method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022).
Sanchis-Trilles, G., M. Cettolo, N. Bertoldi, and
M. Federico. 2009. Online Language Model Adap-
tation for Spoken Dialog Translation. In Proc. of
IWSLT, Tokyo.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA?06.
Zens, R., F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proc. of KI?02,
pages 18?32.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Zhao, B., M. Eck, and S. Vogel. 2004. Language
model adaptation for statistical machine translation
with structured query models. In Proc. of CoLing.
1085
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 152?161,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Does more data always yield better translations?
Guillem Gasco?, Martha-Alicia Rocha, Germa?n Sanchis-Trilles,
Jesu?s Andre?s-Ferrer and Francisco Casacuberta
Departament de Sistemes Informa`tics i Computacio?
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera s/n, 46022 Vale`ncia, Spain
{ggasco,mrocha,gsanchis,jandres,fcn}@dsic.upv.es
Abstract
Nowadays, there are large amounts of data
available to train statistical machine trans-
lation systems. However, it is not clear
whether all the training data actually help
or not. A system trained on a subset of such
huge bilingual corpora might outperform
the use of all the bilingual data. This paper
studies such issues by analysing two train-
ing data selection techniques: one based
on approximating the probability of an in-
domain corpus; and another based on in-
frequent n-gram occurrence. Experimental
results not only report significant improve-
ments over random sentence selection but
also an improvement over a system trained
with the whole available data. Surprisingly,
the improvements are obtained with just a
small fraction of the data that accounts for
less than 0.5% of the sentences. After-
wards, we show that a much larger room for
improvement exists, although this is done
under non-realistic conditions.
1 Introduction
Globalisation and the popularisation of the Inter-
net have lead to a rapid increase in the amount of
bilingual corpora available. Entities such as the
European Union, the United Nations and other
multinational organisations need to translate all
the documentation they generate. Such transla-
tions happen every day and provide very large
multilingual corpora, which are oftentimes diffi-
cult to process and significantly increase the com-
putational requirements needed to train statistical
machine translation (SMT) systems. For instance,
the corpora made available for recent machine
translation evaluations are in the order of 1 billion
running words (Callison-Burch et al 2010).
However, two main problems arise when at-
tempting to use this huge pool of sentences for
training SMT systems: firstly, a large portion of
this data is obtained from domains that differ from
that in which the SMT system is to be used or as-
sessed; secondly, the use of all this data for train-
ing the system increases the computational train-
ing requirements. Despite the previous remarks,
the de facto standard consists in training SMT sys-
tems with all the available data. This is due to
the widespread misconception that the more data
a system is trained with, the better its performance
should be. Although the previous statement is the-
oretically true if all the data belongs to the same
domain, this is not the case in the problems tack-
led by most of the SMT systems. For instance,
enterprises often need to build on-demand sys-
tems (Yuste et al 2010). In this case, since we
are interested in translating some specific text, it
is not clear whether training a system with all data
yields better performance than training it with a
wisely selected subset of bilingual sentences.
The bilingual sentence selection (BSS) task is
stated as the problem of selecting the best sub-
set of bilingual sentences from an available pool
of sentences, with which to train a SMT system.
This paper is concerned to BSS, and mainly two
ideas are developed.
On the one hand, two BSS strategies that at-
tempt to build better translation systems are anal-
ysed. Such strategies are able to improve state-of-
the-art translation quality without the very high
computational resources that are required when
using the complete pool of sentences. Both tech-
niques span through two orthogonal criteria when
selecting bilingual sentences from the available
pool: avoiding to introduce a bias in the original
data distribution, and increasing the informative-
ness of the corpus.
On the other hand, we prove that among all pos-
sible subsets from the sentence pool, there is at
least a small one that yields large improvements
(up to 10 BLEU points) with respect to a system
trained with all the data. In order to retrieve such
subset, we had to use an oracle that employs infor-
mation extracted from the reference translations
152
only for the purpose of selecting bilingual sen-
tences. However, references are not used at any
stage within the translation system for obtaining
the hypotheses. Note that although we are not
able to achieve such an improvement without an
oracle, this result restates the BSS problem as an
interesting approach not only for reducing com-
putational effort but also for significantly boost-
ing performance. To our knowledge, no previous
work has quantified the room of improvement in
which BSS techniques could incur.
In order to assess the performance of the dif-
ferent BSS techniques, translation results are ob-
tained by using a standard state-of-the-art SMT
system (Koehn et al 2007). The most recent lit-
erature defines the SMT problem (Papineni et al
1998; Och and Ney, 2002) as follows: given an
input sentence f from a certain source language,
the purpose is to find an output sentence e? in a
certain target language such that
e? = argmax
e
K?
k=1
?khk(f , e) (1)
where hk(f , e) is a score function representing an
important feature for the translation of f into e,
as for example the language model of the target
language, a reordering model or several transla-
tion models. ?k are the log-linear combination
weights.
The main contributions of this paper are:
? A BSS technique is analysed, which im-
proves the results obtained with a random
bilingual sentence selection strategy when
the specific domain to be translated signifi-
cantly differs from that of the pool of sen-
tences.
? Another BSS technique is analysed that, us-
ing less than 0.5% of the sentences avail-
able, significantly improves over random se-
lection, beating a system trained with all the
pool of sentences.
? We prove, by means of an oracle, that a wise
BSS technique can yield large improvements
when compared with systems trained with all
data available.
The remaining of the paper is structured as fol-
lows. Section 2 summarises the related work.
Sections 3 and 4 present two BSS techniques,
namely, probabilistic sampling and recovery of
infrequent n-grams. In Section 5 experimental re-
sults are reported. Finally, the main results of the
work and several future work directions are dis-
cussed in Section 6.
2 Related Work
Training data selection has been receiving an in-
creasing amount of attention within the SMT
community. For instance, in (Li et al 2010;
Gasco? et al 2010) several BSS techniques, sim-
ilar to those analysed in this paper, have been
applied for training MT systems when there are
large training corpora available. However, nei-
ther such techniques have been formalised, nor its
performance thoroughly analysed. A similar ap-
proach that gives weights to different subcorpora
was proposed in (Matsoukas et al 2009).
In (Lu et al 2007), information retrieval meth-
ods are used in order to produce different sub-
models which are then weighted according to the
sentence to be translated. In such work, authors
define the baseline as the result obtained train-
ing only with the corpus that share the same do-
main of the test. Afterwards they claim that they
are able to improve baseline translation quality by
adding new sentences retrieved with their method.
However, they neither compare their technique
with random sentence selection, nor with a model
trained with all the corpora.
Although the techniques that are applied for
BSS are often very similar to those applied for ac-
tive learning (AL), both problems are essentially
different. Since the AL strategies assume that
the pool of sentences are not translated, they are
usually interested in finding the best monolingual
subset of sentences to be translated by a human
annotator. In contrast, in BSS, it is assumed that a
fairly large amount of bilingual corpora is readily
available, and the main goal consists in selecting
only those sentences which will maximise system
performance.
Some works have applied sentence selection in
small scale AL frameworks. These works extend
the training corpora at most with 5000 sentences.
In (Ananthakrishnan et al 2010), sentences are
selected by means of discriminative techniques.
In (Haffari et al 2009) a technique is proposed
for increasing the counts of phrases that are con-
sidered infrequent. Both works significantly dif-
fer from the current work not only on the frame-
work, but also on the scale of the experiments, the
153
proposed techniques and the obtained improve-
ments. Similar ideas applied to adaptation prob-
lems have been proposed in (Moore and Lewis,
2010; Axelrod et al 2011).
3 Probabilistic Sampling
As discussed in Section 2, BSS has inherently
attached many meaningful links with AL tech-
niques. Selecting samples for learning our mod-
els, incurs in a well-known difficulty in AL, the
so-called sample bias problem (Dasgupta, 2009).
This problem, which is spread to the BSS case,
is summarised as the distortion introduced by the
active strategy into the probability distribution un-
derlying the training corpus. This bias forces the
training algorithm to learn a distorted probability
model which can significantly differ from the ac-
tual one.
In order to further analyse the sampling bias
problem, consider the maximum likelihood esti-
mation (MLE) of a probability model, p?(e, f)
for a given corpus of N data points,{(en, fn)},
sampled from the actual probability distribution,
Pr(e, f). Recall that e denotes a target sen-
tence whereas f stands for its source counter-
part. MLE techniques aims at minimising the
Kullback-Leibler divergence between the actual
unknown probability distribution and the proba-
bility model (Bishop, 2006), defined as
KL(Pr | p?) =
?
e,f
Pr(e, f) log
(
Pr(e, f)
p?(e, f)
)
(2)
When minimising, Eq. (2) is simplified to
?? = argmax
?
?
e,f
Pr(e, f) log(p?(e, f)) (3)
which is approximated by a sufficiently large
dataset under the commonly hold assumption that
it is independently and identically distributed ac-
cording to Pr(e, f) as
?? = argmax
?
?
n
log(p?(en, fn)) (4)
Therefore, by perturbing the sample {(en, fn)}
with an active strategy, we are, in fact, modifying
the approximation to Eq.(3) and learning a differ-
ent underlying probability distribution.
In this section a statistical framework is pro-
posed to build systems with BSS while avoiding
the sample bias. The proposed approach relies in
conserving the probability distribution of the task
domain by wisely selecting the bilingual pairs to
be used from the whole pool of sentences. Hence,
it is mandatory to exclude sentences from the pool
that distort the actual probability. In order to ap-
proximate the probability distribution, we assume
that a small but representative corpus is avail-
able from the task domain. This corpus, referred
henceforth as the in-domain corpus, provides a
way to build an initial model which approximates
the actual probability of the system. The pool of
sentences will be oppositely denoted as the out-
of-domain corpus.
The actual probability of the task domain, the
so called in-domain probability, is approximated
with the following model
p(e, f , |e|, |f |) = p(e, f | |e|, |f |) ? p(|e|, |f |) (5)
where p(|e|, |f |) denotes the in-domain length
probability, and p(e, f | |e|, |f |) the in-domain
bilingual probability.
The length probability is estimated by MLE
p(|e|, |f |) =
N(|e|+ |f |)
N
(6)
where N(|e|+|f |) is the number of bilingual pairs
in the in-domain corpus such that their lengths
sum up to |e|+|f | and N denotes the total num-
ber of sentences. Note that no distinction is made
between source and target lengths since the model
is intended for sampling.
The complexity of the in-domain bilingual
probability distribution, p(e, f | |e|, |f |), requires
a more sophisticated approximation
p(e, f/|e|, |f |) =
exp(
?
k ?kfk(e, f))
Z
(7)
being Z a normalisation constant; and where
fk(. . .) and ?k are the features of the model and
their respective parametric weights. Specifically,
four logarithmic features were considered for this
sampling technique: a direct and an inverse IBM
model 4 (Brown et al 1994); and both, source
and target, 5-gram language models. All fea-
ture models are estimated in the in-domain cor-
pus with standard techniques (Brown et al 1994;
Stolcke, 2002). As a first approach, the parame-
ters of the log-linear model in Eq. (7), ?k, were
uniformly fixed to 1.
154
Once we have an appropriate model for the
in-domain probability distribution, the proposed
method randomly samples a given number of
bilingual pairs from the out-of-domain corpora
(the pool of sentences). The process of extend-
ing the in-domain corpus with additional bilin-
gual pairs from the out-of-domain corpus is sum-
marised as follows:
? Decide according to the in-domain length
probability in Eq. (6), how many samples
should be drawn for each length, i.e. divide
the number of sentences to add into length
dependent buckets.
? Randomly draw the number of samples
specified in each bucket according to the
in-domain bilingual probability in Eq. (7)
among all the bilingual sentences that share
the current bucket length.
Although the pool of sentences is typically
large, it is not large enough to gather a signifi-
cant amount of probability mass. Consequently,
a small set of sentences accumulate most of the
probability mass and tend to be selected multi-
ple times. To avoid this awkward and undesired
behaviour, the sampling is performed without re-
placement.
4 Infrequent n-gram Recovery
Another criterion when confronting the BSS task
is to increase the informativeness of the training
set. Thus, it seems important to choose sentences
that provide information not seen in the training
corpus. Note that this criterion is sometimes op-
posed to the one presented in Section 3.
The performance of phrase-based machine
translation systems strongly relies in the quality
of the phrases extracted from the training sam-
ples. In most of the cases, the inference of such
phrases or rules is based on word alignments,
which cannot be computed accurately when ap-
pearing rarely in the training corpus. The extreme
case are the out-of-vocabulary words: words that
do not appear in the training set, cannot be trans-
lated. Moreover, this problem can be extended to
sequences of words (n-grams). Consider a 2-gram
fifj appearing few or no times in the training set.
Although fi and fj may appear separately in the
training set, the system might not be able to in-
fer the translation of the 2-gram fifj , which may
be different from the concatenation of the transla-
tions of both words separately.
When selecting sentences from the pool it is
important to choose sentences that contain n-
grams that have never been seen (or have been
seen just a few times) in the training set. Such
n-grams will be henceforth referred to as infre-
quent n-grams . An n-gram is considered infre-
quent when it appears less times than an infre-
quent threshold t. If the source language sen-
tences to be translated are known beforehand, the
set of infrequent n-grams can be reduced to those
present in such sentences. Then, the technique
consists in selecting from the pool those sentences
which contain infrequent n-grams present in the
source sentences to be translated.
Sentences in the pool are sorted by their infre-
quency score in order to select first the most in-
formative. Let X the set of n-grams that appear
in the sentences to be translated and w one of
them; C(w) the counts of w in the source lan-
guage training set; and N(w) the counts of w
in the source sentence f to be scored. The infre-
quency score of f is:
i(f) =
?
w?X
min(1, N(w))max(0, t?C(w)) (8)
In order to avoid giving a high score to noisy
sentences with a lot of occurrences of the same in-
frequent n-gram, only one occurrence of each n-
gram is taken into account to compute the score.
In addition, the score gives more importance to
the n-grams with lowest counts in the training
set. Although it could be possible to select the
highest scored sentences, we updated the scores
each time a sentence is selected. This decision
was taken to avoid the selection of too many sen-
tences with the same infrequent n-gram. First,
sentences in the pool are scored using Equation
(8). Then, in each iteration, the sentence f? with
the highest score is selected, added to the training
set and removed from the pool. In addition, the
counts of the n-grams present in f? are updated
and, hence, the scores of the rest of the sentences
in the pool. Since rescoring the whole pool would
incur in a very high computational cost, a subop-
timal search strategy was followed, in which the
search was constrained to a given set of highest
scoring sentences. Here it was set to one million.
155
t = 1 t = 10 t = 25
tr all tr all tr all
1-gr 11.6 1.3 40.5 3.5 59.9 5.1
2-gr 38 9.8 73.2 21.3 84.9 27.9
3-gr 66.8 33.5 91.1 55.7 96.4 64.9
4-gr 87.1 65.8 98.2 85.5 99.4 90.7
Table 1: Percentage of infrequent n-grams in the TED
test set when considering only the TED training set
(tr), and when adding the out-of-domain pool (all),
for different infrequency thresholds t.
Table 1 shows the percentage of source lan-
guage infrequent n-grams for the test of a rela-
tively small corpus such as the TED corpus (for
details see Section 5) when considering just the
in-domain training set (? 40K sentences) and the
same percentage when adding the larger out of do-
main corpora. The percentages in the table have
been computed separately for different values of
the threshold t and for n-grams of order from 1 to
4. Note that the reduction in the number of infre-
quent n-grams is very high for the 1-grams but de-
creases progressively when considering n-grams
of higher order. This indicates that the infrequent
n-grams recovery technique should be very effec-
tive for lower order n-grams, but might have less
effect for higher order n-grams. Therefore, and
in order to lower the computational cost involved,
the experiments carried out for this paper were
performed considering only infrequent 1-grams,
2-grams and 3-grams.
5 Experiments
In the present Section, we first describe the exper-
imental framework employed to assess the perfor-
mance of the BSS techniques described. Then, re-
sults for the probabilistic sentence selection strat-
egy are shown, followed by results obtained with
the infrequent n-grams technique. Some exam-
ple translations are shown and, finally, we also
report experiments using the infrequent n-grams
technique in Oracle mode, in order to establish
the potential improvement for such technique and
for BSS in general.
5.1 Experimental Setup
All experiments were carried out using the
open-source SMT toolkit Moses (Koehn et al
2007), in its standard non-monotonic configura-
tion. The phrase tables were generated by means
of symmetrised word alignments obtained with
Subset Language |S| |W | |V |
train
English
47.5K
747K 24.6K
French 793K 31.7K
dev
English
571
9.2K 1.9K
French 10.3K 2.2K
test
English
641
12.6K 2.4K
French 12.8K 2.7K
Table 2: TED corpus main figures. K denotes thou-
sands of elements. |S| stands for number of sentences,
|W | for number of running words, and |V | for vocab-
ulary size.
Subset Language |S| |W | |V |
train
English
77.2K
1.71M 29.9K
French 1.99M 48K
dev 08
English
2.1K
49.8K 8.7K
French 55.4K 7.7K
test 09
English
2.5K
65.6K 8.9K
French 72.5K 10.6K
test 10
English
2.5K
62K 8.9K
French 70.5K 10.3K
Table 3: News Commentary corpus main figures.
GIZA++ (Och and Ney, 2003). The language
model used was a 5-gram with modified Kneser-
Ney smoothing (Kneser and Ney, 1995), built
with SRILM toolkit (Stolcke, 2002). The log-
linear combination weights in Eq. (1) were opti-
mised using Minimum Error Rate Training (Och
and Ney, 2002) on the corresponding develop-
ment sets.
Experiments were carried out on two corpora:
TED (Paul et al 2010) and News Commentary
(NC) (Callison-Burch et al 2010). TED is an
English-French corpus composed of subtitles for
a collection of public speeches on a variety of top-
ics. The same partitions as in the IWSLT2010
evaluation task (Paul et al 2010) have been used.
Subtitles have been concatenated into complete
sentences. NC is a slightly larger English-French
corpus in the news domain. Main figures of both
corpora are shown in Tables 2 and 3. As for the
pool of sentences, three large corpora have been
used: Europarl (Euro), United Nations (UN) and
Gigaword (Giga), in the partition established for
the 2010 workshop on SMT of the ACL (Callison-
Burch et al 2010). Sentences of length greater
than 50 have been pruned. Table 4 shows the main
figures of the tokenised and lowercased corpora.
When translating between some language
pairs, there are words that remain invariable, like
for example numbers or punctuation marks in the
case of European languages. In fact, an easy and
156
Corpus Language |S| |W | |V |
Euro
English
1.25M
25.6M 81K
French 28.2M 101K
UN
English
5M
94.4M 302K
French 107M 283K
Giga
English
15.5M
303M 1.6M
French 361M 1.6M
Table 4: Figures of the corpora used as sentence pool.
M stands for millions of elements.
effective technique that is commonly used is to re-
produce out-of-vocabulary words from the source
sentence in the target hypothesis. However, in-
variable n-grams are usually infrequent as well,
which implies that the infrequent n-grams tech-
nique would select sentences containing such n-
grams, even though they do not provide further
information. As a first approach, we exclude n-
grams without any letter.
Baseline experiments have been carried out for
TED and NC corpora using the corresponding
training set. For comparison purposes, we also
included results for a purely random sentence se-
lection without replacement. In the plots, each
point corresponding to random selection represent
the average of 10 repetitions. Experiments using
all data are also reported, although a 64GB ma-
chine was necessary, even with binarized phrase
and distortion tables.
Experiments were conducted by selecting a
fixed amount of sentences according to each one
of the techniques described above. Then, these
sentences were included into the training data and
subsequent SMT systems were built for translat-
ing the test set.
Results are shown in terms of BLEU (Papineni
et al 2001), which is an accuracy metric that
measures n-gram precision, with a penalty for
sentences that are too short. Although it could
be argued that improvements obtained might be
due to a side effect of the brevity penalty, this
was not found to be true: the BSS techniques (in-
cluding random) and considering all data yielded
very similar brevity penalties (?0.005), within
each corpus. In addition, TER scores (Snover et
al., 2006) were also computed, but are omitted
for clarity purposes and since they were found to
be coherent with BLEU. TER is an error metric
that computes the minimum number of edits re-
quired to modify the system hypotheses so that
they match the references translations.
 0
 0.01
 0.02
 0.03
 0  10  20  30  40  50  60  70  80  90  100
Re
lati
ve 
fre
que
ncy
Combined sentence length
EuroparlGigawordUNTEDNC
Figure 2: Combined length relative frequency.
5.2 Results for Probabilistic Sampling
In addition to the probabilistic sampling tech-
nique proposed in Section 3, we also analysed the
effect of sampling only according to the combined
source-reference length, with the purpose of es-
tablishing whether potential improvements were
only due to the length component, or rather to the
complete sampling model. Results for the 2009
test set are shown in Figure 1. Several things
should be noted:
? Performing sentence selection only according
to sentence lengths does not achieve better
performance than random selection.
? Selecting sentences according to probabilis-
tic sampling is able to improve random se-
lection in the case of the TED corpus, but
is not able to do so in the case of the NC
corpus. Significance tests for the 500K case
reported that the differences were significant
in the case of the TED corpus, but not in the
case of the NC corpus.
? In the case of the TED corpus, the perfor-
mance achieved with the system built by
sampling 500K sentences is only 0.5 BLEU
points below the performance achieved by
the system built with all the data available.
The explanation to the fact that probabilistic
sampling is able to improve over random sam-
pling only in the case of the TED corpus, but not
in the case of NC, relies in the nature of the cor-
pora. Although both of them belong to a very
generic domain, their characteristics are very dif-
ferent. In fact, the NC data is very similar to the
sentences in the pool, but, in contrast, the sen-
tences present in the TED corpus have a much
more different structure. This difference is illus-
trated in Figure 2, where the relative frequency of
157
 21
 22
 23
 24
0 100K 200K 300K 400K 500K
BL
EU
Number of sentences added
TED corpus
in domain
all
random
length
sampling
 19
 20
 21
 22
0 100K 200K 300K 400K 500K
BL
EU
Number of sentences added
NC corpus
in domain
all
random
length
sampling
Figure 1: Effect of adding sentences over the BLEU score using the probabilistic sampling, length sampling and
random selection techniques for the two corpora, TED and News Commentary. Horizontal lines represent the
scores when using just the in domain training set and all the data available.
 21
 22
 23
 24
 25
 26
0 50k 100k 200k
BL
EU
Number of sentences added
TED corpus
allt=10 in domaint=25 random
 19
 20
 21
 22
 23
0 50k 100k 200k
BL
EU
Number of sentences added
NC corpus
allin domain
randomt=10t=25
Figure 3: Effect of adding sentences over the BLEU score using the infrequent n-grams (with different thresh-
olds) and random selection techniques for the two corpora, TED and News Commentary. Horizontal lines repre-
sent the scores when using just the in domain training set and all the data available.
each combined sentence length is shown. In this
plot, it stands out clearly that the TED corpus has
a very different length distribution than the other
four corpora considered, whereas the NC corpus
presents a very similar distribution. This implies
that, when considering TED, an intelligent data
selection strategy will have better chances to im-
prove random selection than in the case of NC.
5.3 Results for Infrequent n-grams Recovery
Figure 3 shows the effect of adding sentences us-
ing the infrequent n-grams and the random se-
lection techniques on the 2009 test set. Once
all the infrequent n-grams have been covered
t times, the infrequency score for all the sen-
tences remaining in the pool is 0, and none of
them can be selected. Hence, the number of
sentences that can be selected for each t is lim-
ited. Although for clarity we only show results
for t = {10, 25}, experiments have also been car-
ried out for t = {1, 5, 10, 25}. Such results pre-
sented similar curves, although less sentences can
be selected and hence improvements obtained are
slightly lower. Several conclusions can be drawn:
? The translation quality provided by the in-
frequent n-grams technique is significantly
better than the results achieved with random
selection, comparing similar amount of sen-
tences. Specifically, the improvements ob-
tained are in the range of 3 BLEU points.
? Results for the TED corpus are more irreg-
ular. The best performance is achieved for
t = 25 and 50K sentences added. In NC, the
best result is for t = 10 and 112K.
? Selecting sentences with the infrequent n-
grams technique provides better results than
including all the available data. While using
less than 0.5% of the data, improvements be-
tween 0.5 and 1 BLEU points are achieved.
When looking at Figure 3, one might suspect
that t needs to be set specifically for a given test
158
set, and that results from one set are not to be ex-
trapolated to other test sets. For this reason, we
selected the best configuration in Figure 3 and
used it to build a new system for translating the
unseen NC 2010 test set. Such experiment, with
t = 10 and including all sentences with score
greater than 0 (? 110K), is shown in Table 5 and
evidences that improvements are actually coher-
ent among different test sets.
technique BLEU TER #phrases
in-domain 19.0 65.2 5.1M
all data 22.7 60.8 1236M
infreq. t = 10 23.6 59.2 16.5M
Table 5: Effect of the infrequent n-gram recovery tech-
nique for an unseen test set, when setting t = 10 and
number of phrases (parameters) of the models.
5.4 Oracle Results
In order to analyse the potential of BSS tech-
niques, the infrequent n-grams recovery tech-
nique in Section 4 was implemented in oracle
mode. In this way, sentences from the pool
were selected according to the infrequent n-grams
present in the reference translations of the test set.
Note that test references were not included into
the training data as such, but were rather used
to establish which bilingual sentences within the
pool were best suitable for training the SMT sys-
tem. In this way, we were able to establish the po-
tential for improvement of a BSS technique. In-
terestingly, the SMT system trained in this way
achieved 31 BLEU points on the News Commen-
tary 2009 test set, i.e. an 8 BLEU points improve-
ment over the system trained with all the data
available. This result would have beaten all the
systems that took part in the 2009 Workshop on
Machine translation (Callison-Burch et al 2009).
This result is really important: although we are
aware that the sentences were selected in a non-
realistic manner, it proves that an appropriate BSS
technique would be able to boost SMT perfor-
mance in a very significant manner. Similar re-
sults were obtained with the TED and NC 2010
test sets, with 10 and 7 points improvement, re-
spectively.
5.5 Example Translations
Example translations are shown in Figure 4. In
the first example, the baseline system is not able
Src the budget has also been criticised by klaus .
Bsl le budget a e?galement e?te? criticised par m. klaus .
Rdm le budget a e?galement e?te? critique?es par m. klaus .
PS le budget a e?galement e?te? critique?e par klaus .
All le budget a e?galement e?te? critique? par klaus .
Infr le budget a e?galement e?te? critique? par klaus .
Ref klaus critique e?galement le budget .
Src and one has come from music .
Bsl et un a de la musique .
Rdm et on vient de musique .
PS et on a viennent de musique .
All et de la musique .
Infr et un est venu de la musique .
Ref et un vient du monde de la musique .
Figure 4: Examples of two translations for each of the
SMT systems built: Src (source sentence), Bsl (base-
line), Rdm (random selection), PS (probabilistic sam-
pling), All (all the data available), Infr (Infrequent n-
grams) and Ref (reference).
to translate criticised, which is considered out-of-
vocabulary. Even though random selection is able
to solve this problem (luckily), it does not achieve
to translate it correctly, introducing a concordance
error. A similar thing happens when using prob-
abilistic sampling, where a grammatical error is
also present, and only Infr and All are able
to present a correct translation. This is not only
casual, since, by ensuring that a given n-gram ap-
pears at least a certain number of times t, the odds
of including all possible translations of criticised
are incremented significantly. Note that, even if
the Infr translation is different from the refer-
ence, it is equally correct. In the second example,
the baseline translation is pretty much correct, but
has a different meaning (something like ?and one
has music?). Similarly, when including all data
the translation obtained by the system means ?and
some music?. In this case, both random and prob-
abilistic selection present grammatically incorrect
sentences, and only Infr is able to provide a cor-
rect translation, although pretty literal and differ-
ent from the reference.
6 Discussion
Bilingual sentence selection (BSS) might be un-
derstood to be closely related to adaptation, even
though both paradigms tackle problems which
are, in essence, different. The goal of an adap-
tation technique is to adapt model parameters,
which have been estimated on a large out-of-
domain (or generic) data set, so that they are
159
best suitable for dealing with a domain-specific
test set. This adaptation process is ought to be
achieved by means of a (potentially small) adapta-
tion set, which belongs to the same domain as the
test data. In contrast, BSS tackles with the prob-
lem of how to select samples from a large pool
of training data, regardless of whether such pool
of data is in-domain or out-of-domain. Hence, in
one case we can assume to have a fairly well es-
timated translation model, which is to be adapted,
whereas in BSS we still have full control over the
estimation of such model and need not to aim at a
specific domain, although it might often be so.
BSS is related with instance weighting (Jiang
and Zhai, 2007; Foster et al 2010). Adapta-
tion and BSS can be considered to be orthogo-
nal (yet complementary) problems under the in-
stance weighting paradigm. In such case, instance
weighting can be considered to span a complete
paradigmatic space between both. At one end,
there is sample selection (BSS for SMT), while at
the other end there is adaptation. For instance, it
is quite common to confront the adaptation prob-
lem by extracting different phrase-tables from dif-
ferent corpora, and then interpolate such tables.
This technique could be also applied to promote
the performance of the system built by means of
BSS. However, this is left out as future work.
We thoroughly analysed two BSS approaches
that obtain competitive results, while using a
small fraction of the training data, although there
is still much to be gained. For instance, oracle re-
sults have also been reported in this work, yield-
ing improvements of up to 10 BLEU points. Even
though the use of an oracle typically implies that
the results obtained are not realistic, recall that
the proposed oracle is special, in the sense that it
only uses the reference sentences for the specific
purpose of selecting training samples, but the ref-
erences are not included into the training data as
such. This is useful for assessing the potential be-
hind BSS: ideally, if we were able to design a BSS
strategy that, without using the references, would
select exactly those training samples, we would be
boosting system performance by 10 BLEU points.
This re-states BSS as a compelling technique that
has not yet received the attention it deserves.
BSS is not aimed at optimising computational
requirements, but does so as a byproduct. This
may seem despicable but it would allow to run
more experiments with the same resources, use
larger corpora or even more complex techniques,
such as synchronous grammars or hierarchical
models. For instance, the infrequent n-grams
technique has beaten all the other systems using
just a small fraction of the corpus, only 0.5%, and
is yet able to outperform a system trained with all
the data by 0.9 BLEU points and the random base-
line by 3 points. This baseline has been proved to
be difficult to beat by other works.
Preliminary experiments were performed in or-
der to analyse the perplexity of the references, the
number of out of vocabulary words (OoVs) and
the ratio of target-source phrases. These exper-
iments revealed that the improvements obtained
are largely correlated with a decrease in perplex-
ity and in the number of OoVs. On the one hand,
reducing the amount of OoVs was mirrored by
an important improvement in BLEU when the
amount of additional data was small, and also
entailed a decrease in perplexity. However, a
reduction in perplexity by itself did not always
imply significant improvements. Moreover, no
real conclusion could be drawn from the analy-
sis of target-source phrase ratio. Hence, we un-
derstand that the improvements obtained are pro-
vided mainly by a more specialised estimation of
the model parameters. However, further experi-
ments should still be conducted in order to verify
this conclusion.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under
grant agreement nr. 287755. This work was
also supported by the Spanish MEC/MICINN un-
der the MIPRCV ?Consolider Ingenio 2010? pro-
gram (CSD2007-00018), and iTrans2 (TIN2009-
14511) project. Also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and Instituto Tecnolo?gico de
Leo?n, DGEST-PROMEP y CONACYT, Me?xico.
160
References
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proc. of the EMNLP, pages 626?635,
Cambridge, MA, October.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proc of the EMNLP, pages 355?
362.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc of the WSMT, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint Workshop on Sta-
tistical Machine Translation and Metrics for Ma-
chine Translation. In Proc. of the MATR(ACL),
pages 17?53, Uppsala, Sweden, July.
Sanjoy Dasgupta. 2009. The two faces of active learn-
ing. In Proc. of The twentieth Conference on Algo-
rithmic Learning Theory, page 1, Porto (Portugal),
October.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proc. of
the EMNLP, pages 451?459, Cambridge, MA, Oc-
tober.
Guillem Gasco?, Vicent Alabau, Jesu?s Andre?s-Ferrer,
Jesu?s Gonza?lez-Rubio, Martha-Alicia Rocha,
Germa?n Sanchis-Trilles, Francisco Casacuberta,
Jorge Gonza?lez, and Joan-Andreu Sa?nchez. 2010.
ITI-UPV system description for IWSLT 2010. In
Proc. of the IWSLT 2010, Paris, France, December.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proc. of HLT/NAACL?09,
pages 415?423, Morristown, NJ, USA.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Proc.
of ACL?07, pages 264?271.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Proc.
of ICASSP, II:181?184, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christie
Moran, Richard Zens, Chris Dyer, Ontraj Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, pages 177?180.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proc. of the MATR(ACL), pages 139?
143, Uppsala, Sweden, July.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
training data selection and optimization. In Proc. of
the EMNLP-CoNLL, pages 343?350, Prague, Czech
Republic, June.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proc. of the
EMNLP, pages 708?717, Singapore, August.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
ACL (Short Papers), pages 220?224.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL, pages
295?302.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29, pages
19?51.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proc. of
ICASSP?98, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: A method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022).
Michael Paul, Marcello Federico, and Sebastian Stker.
2010. Overview of the IWSLT 2010 evaluation
campaign. In Proc. of the IWSLT 2010, Paris,
France, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. of AMTA?06.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Elia Yuste, Manuel Herranz, Antonio Lagarda, Li-
onel Tarazo?n, Isa??as Sa?nchez-Cortina, and Fran-
cisco Casacuberta. 2010. Pangeamt - putting
open standards to work... well. In Proc. of the
AMTA2010. Denver, CO, USA, November.
161
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172?176,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UPV-PRHLT English?Spanish system for WMT10
Germa?n Sanchis-Trilles and Jesu?s Andre?s-Ferrer and Guillem Gasco?
Jesu?s Gonza?lez-Rubio and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{gsanchis|jandres|fcn}@dsic.upv.es
{ggasco|jegonzalez|pmartinez}@dsic.upv.es
{mrocha|jandreu}@dsic.upv.es
Abstract
In this paper, the system submitted by
the PRHLT group for the Fifth Work-
shop on Statistical Machine Translation of
ACL2010 is presented. On this evalua-
tion campaign, we have worked on the
English?Spanish language pair, putting
special emphasis on two problems derived
from the large amount of data available.
The first one, how to optimize the use of
the monolingual data within the language
model, and the second one, how to make
good use of all the bilingual data provided
without making use of unnecessary com-
putational resources.
1 Introduction
For this year?s translation shared task, the Pat-
tern Recognition and Human Language Technolo-
gies (PRHLT) research group of the Universidad
Polite?cnica de Valencia submitted runs for the
English?Spanish translation task. In this paper, we
report the configuration of such a system, together
with preliminary experiments performed to estab-
lish the final setup.
As in 2009, the central focus of the Shared Task
is on Domain Adaptation, where a system typi-
cally trained using out-of-domain data is adjusted
to translate news commentaries.
For the preliminary experiments, we used only a
small amount of the largest available bilingual cor-
pus, i.e. the United Nations corpus, by including
into our system only those sentences which were
considered similar.
Language model interpolation using a develop-
ment set was explored in this work, together with
a technique to cope with the problem of ?out of
vocabulary words?.
Finally, a reordering constraint using walls and
zones was used in order to improve the perfor-
mance of the submitted system.
In the final evaluation, our system was ranked
fifth, considering only primary runs.
2 Language Model interpolation
Nowadays, it is quite common to have very large
amounts of monolingual data available from sev-
eral different domains. Despite of this fact, in
most of the cases we are only interested in trans-
lating from one specific domain, as is the case in
this year?s shared task, where the provided mono-
lingual training data belonged to European parlia-
mentary proceedings, news related domains, and
the United Nations corpus, which consists of data
crawled from the web.
Although the most obvious thing to do is to con-
catenate all the data available and train a single
language model on the whole data, we also inves-
tigated a ?smarter? use of such data, by training
one language model for each of the available cor-
pora.
3 Similar sentences selection
Currently, it is common to of huge bilingual cor-
pora for SMT. For some common language pairs,
corpora of millions of parallel sentences are avail-
able. In some of the cases big corpora are used
as out-of-domain corpora. For example, in the
case of the shared task, we try to translate a news
text using a small in-domain bilingual news corpus
(News Commentary) and two big out-of-domain
corpora: Europarl and United Nations.
Europarl is a medium size corpus and can be
completely incorporated to the training set. How-
ever, the use of the UN corpus requires a big com-
putational effort. In order to alleviate this prob-
lem, we have chosen only those bilingual sen-
tences from the United Nations that are similar to
the in-domain corpus sentences. As a similarity
measure, we have chosen the alignment score.
Alignment scores have already been used as a
172
filter for noisy corpora (Khadivi and Ney, 2005).
We trained an IBM model 4 using GIZA++ (Och
and Ney, 2003) with the in-domain corpus and
computed the alignment scores over the United
Nations sentences. We assume that the alignment
score is a good measure of similarity.
An important factor in the alignment score is
the length of the sentences, so we clustered the
bilingual sentences in groups with the same sum of
source and target language sentence sizes. In each
of the groups, the higher the alignment score is,
the more similar the sentence is to the in-domain
corpus sentences. Hence, we computed the aver-
age alignment score for each one of the clusters
obtained for the corpus considered in-domain (i.e.
the News-Commentary corpus). This being done,
we assessed the similarity of a given sentence by
computing the probability of such sentence with
respect to the alignment model of the in-domain
corpus, and established the following similarity
levels:
? Level 1: Sentences with an alignment score
equal or higher than the in-domain average.
? Level 2: Sentences with an alignment score
equal or higher than the in-domain average,
minus one standard deviation.
? Level 3: Sentences with an alignment score
equal or higher than the in-domain average,
minus two standard deviations.
Naturally, such similarity levels establish parti-
tions of the out-of-domain corpus. Then, such par-
titions were included into the training set used for
building the SMT system, and re-built the com-
plete system from scratch.
4 Out of Vocabulary Recovery
As stated in the previous section, in order to avoid
a big computational effort, we do not use the
whole United Nations corpus to train the trans-
lation system. Out of vocabulary words are a
common problem for machine translation systems.
When translating the test set, there are test words
that are not in the reduced training set (out of vo-
cabulary words). Some of those out of vocabulary
words are present in the sentences discarded from
the United Nations Corpus. Thus, recovering the
discarded sentences with out of vocabulary words
is needed.
The out of vocabulary words recovery method
is simple: the out of vocabulary words from the
test, when taking into account the reduced training
set, are obtained and then discarded sentences that
contain at least one of them are retrieved. Then,
those sentences are added to the reduced training
set.
Finally, alignments with the resulting training
set were computed and the usual training proce-
dure for phrase-based systems was performed.
5 Walls and zones
In translation, as in other linguistics areas, punc-
tuation marks are essential as they help to un-
derstand the intention of a message and organise
the ideas to avoid ambiguity. They also indicate
pauses, hierarchies and emphasis.
In our system, punctuation marks have been
taken into account during decoding. Traditionally,
in SMT punctuation marks are treated as words
and this has undesirable effects (Koehn and Had-
dow, 2009). For example, commas have a high
probability of occurrence and many possible trans-
lations are generated. Most of them are not consis-
tent across languages. This introduces too much
noise to the phrase tables.
(Koehn and Haddow, 2009) established a
framework to specify reordering constraints with
walls and zones, where commas and end
of sentence are not mixed with various clauses.
Gains between 0.1 and 0.2 of BLEU are reported.
Specifying zones and walls with XML tags
in input sentences allows us to identify structured
fragments that the Moses decoder uses with the
following restrictions:
1. If a <zone> tag is detected, then a block
is identified and must be translated until a
</zone> tag is found. The text between tags
<zone> and </zone> is identified and trans-
lated as a block.
2. If the decoder detects a <wall/> tag, the text
is divided into a prefix and suffix and Moses
must translate all the words of the prefix be-
fore the suffix.
3. If both zones and walls are specified,
then local walls are considered where
the constraint 2 applies only to the area es-
tablished by zones.
173
corpus Language |S| |W | |V |
Europarl v5
Spanish
1272K
28M 154K
English 27M 106K
NC
Spanish
81K
1.8M 54K
English 1.6M 39K
Table 1: Main figures of the Europarl v5 and
News-Commentary (NC) corpora. K/M stands
for thousands/millions. |S| is the number of sen-
tences, |W | the number of running words, and |V |
the vocabulary size. Statistics are reported on the
tokenised and lowercased corpora.
We used quotation marks, parentheses, brackets
and dashes as zone delimiters. Quotation marks
(when appearing once in the sentence), com-
mas, colons, semicolons, exclamation and ques-
tion marks and periods are used as wall delimiters.
The use of zone delimiters do not alter the per-
formance. When using walls, a gain of 0.1
BLEU is obtained in our best model.
6 Experiments
6.1 Experimental setup
For building our SMT systems, the open-source
SMT toolkit Moses (Koehn et al, 2007) was used
in its standard setup. The decoder includes a log-
linear model comprising a phrase-based transla-
tion model, a language model, a lexicalised dis-
tortion model and word and phrase penalties. The
weights of the log-linear interpolation were opti-
mised by means of MERT (Och, 2003). In addi-
tion, a 5-gram LM with Kneser-Ney (Kneser and
Ney, 1995) smoothing and interpolation was built
by means of the SRILM (Stolcke, 2002) toolkit.
For building our baseline system, the News-
Commentary and Europarl v5 (Koehn, 2005) data
were employed, with maximum sentence length
set to 40 in the case of the data used to build the
translation models, and without restriction in the
case of the LM. Statistics of the bilingual data can
be seen in Table 1.
In all the experiments reported, MERT was run
on the 2008 test set, whereas the test set 2009 was
considered as test set as such. In addition, all the
experiments described below were performed in
lowercase and tokenised conditions. For the fi-
nal run, the detokenisation and recasing was per-
formed according to the technique described in the
Workshop baseline description.
corpus |S| |W | |V |
Europarl 1822K 51M 172K
NC 108K 3M 68K
UN 6.2M 214M 411K
News 3.9M 107M 512K
Table 2: Main figures of the Spanish resources
provided: Europarl v5, News-Commentary (NC),
United Nations (UN) and News-shuffled (News).
6.2 Language Model interpolation
The final system submitted to the shared task
included a linear interpolation of four language
models, one for each of the monolingual resources
available for Spanish (see Table 2). The results
can be seen in Table 3. As a first experiment, only
the in-domain corpus, i.e. the News-Commentary
data (NC data) was used for building the LM.
Then, all the available monolingual Spanish data
was included into a single LM, by concatenat-
ing all the data together (pooled). Next, in
interpolated, one LM for each one of the
provided monolingual resources was trained, and
then they were linearly interpolated so as to min-
imise the perplexity of the 2008 test set, and fed
such interpolation to the SMT system. We found
out that weights were distributed quite unevenly,
since the News-shuffled LM received a weight of
0.67, whereas the other three corpora received a
weight of 0.11 each. It must be noted that even
the in-domain LM received a weight of 0.11 (less
than the News-shuffled LM). The reason for this
might be that, although the in-domain LM should
be more appropriate and should receive a higher
weight, the News-shuffled corpus is also news re-
lated (hence not really out-of-domain), but much
larger. For this reason, the result of using only
such LM (News) was also analysed. As expected,
the translation quality dropped slightly. Never-
theless, since the differences are not statistically
significant, we used the News-shuffled LM for in-
ternal development purposes, and the interpolated
LM only whenever an improvement prooved to be
useful.
6.3 Including UN data
We analysed the impact of the selection technique
detailed in Section 3. In this case, the LM used
was the interpolated LM described in the previous
section. The result can be seen in Table 4. As
it can be seen, translation quality as measured by
174
Table 3: Effect of considering different LMs
LM used BLEU
NC data 21.86
pooled 23.53
interpolated 24.97
news 24.79
BLEU improves constantly as the number of sen-
tences selected increases. However, further sen-
tences were not included for computational rea-
sons.
In the same table, we also report the effect of
adding the UN sentences selected by our out-of-
vocabulary technique described in Section 4. In
this context, it should be noted that MERT was
not rerun once such sentences had been selected,
since such sentences are related with the test set,
and not with the development set on which MERT
is run.
Table 4: Effect of including selected sentences
system BLEU
baseline 24.97
+ oovs 25.08
+ Level 1 24.98
+ Level 2 25.07
+ Level 3 25.13
6.4 Final system
Since the News-shuffled, UN and Europarl cor-
pora are large corpora, a new LM interpolation
was estimated by using a 6-gram LM on each one
of these corpora, obtaining a gain of 0.17 BLEU
points by doing so. Further increments in the n-
gram order did not show further improvements.
In addition, preliminary experimentation re-
vealed that the use of walls, as described in
Section 5, also provided slight improvements, al-
though using zones or combining both did not
prove to improve further. Hence, only walls
were included into the final system.
Lastly, the final system submitted to the Work-
shop was the result of combining all the techniques
described above. Such combination yielded a fi-
nal BLEU score of 25.31 on the 2009 test set, and
28.76 BLEU score on the 2010 test set, both in
tokenised and lowercased conditions.
7 Conclusions and future work
In this paper, the SMT system presented by the
UPV-PRHLT team for WMT 2010 has been de-
scribed. Specifically, preliminary results about
how to make use of larger data collections for
translating more focused test sets have been pre-
sented.
In this context, there are still some things which
need a deeper investigation, since the results pre-
sented here give only a small insight about the po-
tential of the similar sentence selection technique
described.
However, a deeper analysis is needed in order
to assess the potential of such technique and other
strategies should be implemented to explore new
kids of reordering constraints.
Acknowledgments
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV ?Consolider Ingenio 2010?
program (CSD2007-00018),iTrans2 (TIN2009-
14511) project, and the FPU scholarship AP2006-
00691. This work was also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and by the Generalitat Valen-
ciana under grant Prometeo/2009/014 and schol-
arships BFPI/2007/117 and ACIF/2010/226 and
by the Mexican government under the PROMEP-
DGEST program.
References
Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In Natural Language Processing and In-
formation Systems, 10th Int. Conf. on Applications
of Natural Language to Information Systems, vol-
ume 3513 of Lecture Notes in Computer Science,
pages 263?274, Alicante, Spain, June. Springer.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, II:181?184, May.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In The 4th EACL Workshop on Statistical
Machine Translation, ACL, pages 160?164, Athens,
Greece, March. Springer.
P. Koehn et al 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of
175
the ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. of ICSLP?02, pages 901?
904, September.
176
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 207?211,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UCH-UPV English?Spanish system for WMT10
Francisco Zamora-Mart??nez
Dep. de F??sica, Matema?ticas y Computacio?n
Universidad CEU-Cardenal Herrera
Alfara del Patriarca (Valencia), Spain
fzamora@dsic.upv.es
Germa?n Sanchis-Trilles
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Valencia, Spain
gsanchis@dsic.upv.es
Abstract
This paper describes the system developed
in collabaration between UCH and UPV
for the 2010 WMT. For this year?s work-
shop, we present a system for English-
Spanish translation. Output N -best lists
were rescored via a target Neural Network
Language Model, yielding improvements
in the final translation quality as measured
by BLEU and TER.
1 Introduction
In Statistical Machine Translation (SMT), the goal
is to translate a sentence f from a given source lan-
guage into an equivalent sentence e? from a certain
target language. Such statement is typically for-
malised by means of the so-called log-linear mod-
els (Papineni et al, 1998; Och and Ney, 2002) as
follows:
e? = argmax
e
K?
k=1
?khk(f , e) (1)
where hk(f , e) is a score function representing
an important feature for the translation of f into
e, K is the number of models (or features) and
?k are the weights of the log-linear combination.
Typically, the weights ?k are optimised during
the tuning stage with the use of a development
set. Such features typically include the target lan-
guage model p(e), which is one of the core com-
ponents of an SMT system. In fact, most of the
times it is assigned a relatively high weight in the
log-linear combination described above. Tradi-
tionally, language modelling techniques have been
classified into two main groups, the first one in-
cluding traditional grammars such as context-free
grammars, and the second one comprising more
statistical, corpus-based models, such as n-gram
models. In order to assign a probability to a given
word, such models rely on the assumption that
such probability depends on the previous history,
i.e. the n ? 1 preceding words in the utterance.
Nowadays, n-gram models have become a ?de
facto? standard for language modelling in state-of-
the-art SMT systems.
In the present work, we present a system which
follows a coherent and natural evolution of prob-
abilistic Language Models. Specifically, we pro-
pose the use of a continuous space language model
trained in the form of a Neural Network Language
Model (NN LM).
The use of continuous space representation of
language has been successfully applied in recent
NN approaches to language modelling (Bengio et
al., 2003; Schwenk and Gauvain, 2002; Castro-
Bleda and Prat, 2003; Schwenk et al, 2006).
However, the use of Neural Network Language
Models (NN LMs) (Bengio, 2008) in state-of-the-
art SMT systems is not so popular. The only com-
prehensive work refers to (Schwenk, 2010), where
the target LM is presented in the form of a fully-
connected Multilayer Perceptron.
The presented system combines a standard,
state-of-the-art SMT system with a NN LM via
log-linear combination and N -best output re-
scoring. We chose to participate in the English-
Spanish direction.
2 Neural Network Language Models
In SMT the most extended language models are
n-grams (Bahl et al, 1983; Jelinek, 1997; Bahl et
al., 1983). They compute the probability of each
word given the context of the n?1 previous words:
p(s1 . . . s|S|) ?
|S|?
i=1
p(si|si?n+1 . . . si?1) . (2)
where S is the sequence of words for which we
want compute the probability, and si ? S, from a
vocabulary ?.
207
A NN LM is a statistical LM which follows
equation (2) as n-grams do, but where the proba-
bilities that appear in that expression are estimated
with a NN (Bengio et al, 2003; Castro-Bleda and
Prat, 2003; Schwenk, 2007; Bengio, 2008). The
model naturally fits under the probabilistic inter-
pretation of the outputs of the NNs: if a NN, in this
case a MLP, is trained as a classifier, the outputs
associated to each class are estimations of the pos-
terior probabilities of the defined classes (Bishop,
1995).
The training set for a LM is a sequence
s1s2 . . . s|S| of words from a vocabulary ?. In or-
der to train a NN to predict the next word given
a history of length n ? 1, each input word must
be encoded. A natural representation is a local en-
coding following a ?1-of-|?|? scheme. The prob-
lem of this encoding for tasks with large vocab-
ularies (as is typically the case) is the huge size
of the resulting NN. We have solved this prob-
lem following the ideas of (Bengio et al, 2003;
Schwenk, 2007), learning a distributed represen-
tation for each word. Figure 1 illustrates the archi-
tecture of the feed-forward NN used to estimate
the NN LM:
? The input is composed of words
si?n+1, . . . , si?1 of equation (2). Each
word is represented using a local encoding.
? P is the projection layer of the input words,
formed by Pi?n+1, . . . , Pi?1 subsets of pro-
jection units. The subset of projection units
Pj represents the distributed encoding of in-
put word sj . The weights of this projection
layer are linked, that is, the weights from
each local encoding of input word sj to the
corresponding subset of projection units Pj
are the same for all input words. After train-
ing, the codification layer is removed from
the network by pre-computing a table of size
|?| which serves as a distributed encoding.
? H denotes the hidden layer.
? The output layerO has |?| units, one for each
word of the vocabulary.
This n-gram NN LM predicts the posterior
probability of each word of the vocabulary given
the n ? 1 previous words. A single forward pass
of the MLP gives p(?|si?n+1 . . . si?1) for every
word ? ? ?.
Figure 1: Architecture of the continuous space
NN LM during training. The input words are
si?n+1, . . . , si?1 (in this example, the input words
are si?3, si?2, and si?1 for a 4-gram). I , P , H ,
andO are the input, projection, hidden, and output
layer, respectively, of the MLP.
The major advantage of the connectionist ap-
proach is the automatic smoothing performed by
the neural network estimators. This smoothing is
done via a continuous space representation of the
input words. Learning the probability of n-grams,
together with their representation in a continous
space (Bengio et al, 2003), is an appropriate ap-
proximation for large vocabulary tasks. However,
one of the drawbacks of such approach is the high
computational cost entailed whenever the NN LM
is computed directly, with no simplification what-
soever. For this reason, in this paper we will be
restricting vocabulary size.
3 Experiments
3.1 Baseline system
For building the baseline SMT system, we used
the open-source SMT toolkit Moses (Koehn et
al., 2007), in its standard setup. The decoder in-
cludes a log-linear model comprising a phrase-
based translation model, a language model, a lex-
icalised distortion model and word and phrase
penalties. The weights of the log-linear interpo-
lation were optimised by means of MERT (Och,
2003).
For the baseline LM, we computed a regular
n-gram LM with Kneser-Ney smoothing (Kneser
208
and Ney, 1995) and interpolation by means of the
SRILM (Stolcke, 2002) toolkit. Specifically, we
trained a 6-gram LM on the larger Spanish corpora
available (i.e. UN, News-Shuffled and Europarl),
and a 5-gram LM on the News-Commentary cor-
pus. Once these LMs had been built, they were
finally interpolated so as to maximise the perplex-
ity of the News-Commentary test set of the 2008
shared task. This was done so according to pre-
liminary investigation.
3.2 NN LM system architecture
The presented systems follow previous works
of (Schwenk et al, 2006; Khalilov et al, 2008;
Schwenk and Koehn, 2008; Schwenk, 2010)
where the use of a NN LM helps achieving better
performance in the final system.
The NN LM was incorporated to the baseline
system via log-linear combination, adding a new
feature to the output N -best list generated by the
baseline system (in this case N = 1 000). Specif-
ically, the NN LM was used to compute the log-
probability of each sentence within theN -best list.
Then, the scores of such list were extended with
our new, NN LM-based feature. This being done,
we optimised the coefficients of the log-linear in-
terpolation by means of MERT, taking into ac-
count the newly introduced feature. Finally the
list was re-scored and the best hypothesis was
extracted and returned as final output. Figure 2
shows a diagram of the system structure.
3.3 Experimental setup and results
NN LM was trained with the concatenation of the
News-shuffled and News-Commentary10 Span-
ish corpora. Other language resources were dis-
carded due to the large amount of computational
resources that would have been needed for train-
ing a NN LM with such material. Table 1 shows
some statistics of the corpora. In order to reduce
the complexity of the model, the vocabulary was
restricted to the 20K more frequent words in the
concatenation of news corpora. Using this re-
stricted vocabulary implies that 6.4% of the run-
ning words of the news-test2008 set, and 7.3% of
the running words within the official 2010 test set,
will be considered as unknown for our system. In
addition, the vocabulary includes a special token
for unknown words used for compute probabili-
ties when an unknown word appears, as described
in Equation 2.
Table 1: Spanish corpora statistics. NC stands for
News-Commentary and UN for United Nations,
while |?| stands for vocabulary size, and M/K for
millions/thousands of elements.
Set # Lines # Words |?|
NC 108K 2.96M 67K
News-Shuffled 3.86M 107M 512K
Europarl 1.82M 51M 172K
UN 6.22M 214M 411K
Total 3.96M 110M 521K
A 6-gram NN LM was trained for this task,
based in previous works (Khalilov et al, 2008).
The distributed encoding input layer consists of
640 units (128 for each word), the hidden layer
has 500 units, and the output layer has 20K units,
one for each word in the restricted vocabulary.
The total number of weights in the network was
10 342 003. The training procedure was conducted
by means of the stochastic back-propagation al-
gorithm with weight decay, with a replacement of
300K training samples and 200K validation sam-
ples in each training epoch. The training and vali-
dation sets were randomly extracted from the con-
catenation of news corpora. The training set con-
sisted of 102M words (3M sentences) and valida-
tion set 8M words (300K sentences). The network
needed 129 epochs for achieving convergence, re-
sulting in 38.7M and 25.8M training and valida-
tion samples respectively. For training the NN LM
we used the April toolkit (Espan?a-Boquera et al,
2007; Zamora-Mart??nez et al, 2009), which im-
plements a pattern recognition and neural net-
works toolkit. The perplexity achieved by the 6-
gram NN LM in the Spanish news-test08 devel-
opment set was 116, versus 94 obtained with a
standard 6-gram language model with interpola-
tion and Kneser-Ney smoothing (Kneser and Ney,
1995).
The number of sentences in the N -best list was
set to 1 000 unique output sentences. Results can
be seen in Table 2. In order to assess the reliability
of such results, we computed pairwise improve-
ment intervals as described in (Koehn, 2004), by
means of bootstrapping with 1000 bootstrap itera-
tions and at a 95% confidence level. Such confi-
dence test reported the improvements to be statis-
tically significant.
Four more experiments have done in order to
study the influence of the N -best list size in the
209
Figure 2: Architecture of the system.
Table 2: English-Spanish translation quality for
development and official test set. Results are given
in BLEU/TER.
test08 (dev) test10 (test)
Baseline 24.8/60.0 26.7/55.1
NN LM 25.2/59.6 27.8/54.0
Table 3: Test set BLEU/TER performance for each
N -best list size.
N -best list size BLEU TER
200 27.5 54.2
400 27.6 54.2
600 27.7 54.1
800 27.6 54.2
1000 27.8 54.0
performance achieved by the NN LM rescoring.
For each N -best list size (200, 400, 600 and 800)
the weights of the log-linear interpolation were op-
timised by means of MERT over the test08 set. Ta-
ble 3 shows the test results for eachN -best list size
using the correspondent optimised weights. As it
can be seen, the size of the N -best list seems to
have an impact on the final translation quality pro-
duced. Although in this case the results are not
statistically significant for each size step, the final
difference (from 27.5 to 27.8) is already signifi-
cant.
4 Conclusions
In this paper, an improved SMT system by using a
NN LM was presented. Specifically, it has been
shown that the final translation quality, as mea-
sured by BLEU and TER, is improved over the
quality obtained with a state-of-the-art SMT sys-
tem. Such improvements, of 1.1 BLEU points,
were found to be statistically significant. The sys-
tem presented uses a neural network only for com-
puting the language model probabilities. As an
immediate future work, we intend to compute the
language model by means of a linear interpola-
tion of several neural networks. Another interest-
ing idea is to integrate the NN LM within the de-
coder itself, instead of performing a subsequent re-
scoring step. This can be done extending the ideas
presented in a previous work (Zamora-Mart??nez et
al., 2009), in which the evaluation of NN LM is
significantly sped-up.
Acknowledgments
This paper was partially supported by the EC
(FEDER/FSE) and by the Spanish Government
(MICINN and MITyC) under the MIPRCV
?Consolider Ingenio 2010? program (CSD2007-
00018), iTrans2 (TIN2009-14511) project and the
erudito.com (TSI-020110-2009-439) project.
References
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A Max-
imum Likelihood Approach to Continuous Speech
Recognition. IEEE Trans. on Pat. Anal. and Mach.
Intel., 5(2):179?190.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A Neural Probabilistic Language Model.
Journal of Machine Learning Research, 3(2):1137?
1155.
210
Y. Bengio. 2008. Neural net language models. Schol-
arpedia, 3(1):3881.
C. M. Bishop. 1995. Neural networks for pattern
recognition. Oxford University Press.
M.J. Castro-Bleda and F. Prat. 2003. New Directions
in Connectionist Language Modeling. In Computa-
tional Methods in Neural Modeling, volume 2686 of
LNCS, pages 598?605. Springer-Verlag.
S. Espan?a-Boquera, F. Zamora-Mart??nez, M.J. Castro-
Bleda, and J. Gorbe-Moya. 2007. Efficient BP Al-
gorithms for General Feedforward Neural Networks.
In Bio-inspired Modeling of Cognitive Tasks, vol-
ume 4527 of LNCS, pages 327?336. Springer.
F. Jelinek. 1997. Statistical Methods for Speech
Recognition. Language, Speech, and Communica-
tion. The MIT Press.
M. Khalilov, J. A. R. Fonollosa, F. Zamora-Mart??nez,
M. J. Castro-Bleda, and S. Espan?a-Boquera. 2008.
Neural network language models for translation with
limited data. In 20th International Conference on
Tools with Artificial Intelligence, ICTAI?08, pages
445?451, november.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, II:181?
184, May.
P. Koehn et al 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proc. of the ACL
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP,
pages 388?395.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of ACL?02, pages 295?
302.
F.J. Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proc. of ACL, pages
160?167, Sapporo, Japan.
K. Papineni, S. Roukos, and T. Ward. 1998. Maxi-
mum likelihood and discriminative training of direct
translation models. In Proc. of ICASSP, pages 189?
192.
H. Schwenk and J. L. Gauvain. 2002. Connectionist
language modeling for large vocabulary continuous
speech recognition. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP?02), pages 765?768, Orlando,
Florida (USA), May.
H. Schwenk and P. Koehn. 2008. Large and diverse
language models for statistical machine translation.
In International Joint Conference on Natural Lan-
guage Processing, pages 661?668.
H. Schwenk, D. De?chelotte, and J. L. Gauvain. 2006.
Continuous space language models for statistical
machine translation. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 723?730.
H. Schwenk. 2007. Continuous space language mod-
els. Computer Speech and Language, 21(3):492?
518.
H. Schwenk. 2010. Continuous space language mod-
els for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, 93.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. of ICSLP?02, pages 901?
904, September.
F. Zamora-Mart??nez, M.J. Castro-Bleda, and
S. Espan?a-Boquera. 2009. Fast Evaluation of
Connectionist Language Models. In International
Work-Conference on Artificial Neural Networks,
volume 5517 of LNCS, pages 33?40. Springer.
211
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 296?300,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The UPV-PRHLT Combination System for WMT 2010
Jesu?s Gonza?lez-Rubio and Jesu?s Andre?s-Ferrer and Germa?n Sanchis-Trilles
Guillem Gasco? and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{jegonzalez|jandres|gsanchis}@dsic.upv.es
{ggasco|pmartinez|mrocha}@dsic.upv.es
{jandreu|fcn}@dsic.upv.es
Abstract
UPV-PRHLT participated in the System
Combination task of the Fifth Workshop
on Statistical Machine Translation (WMT
2010). On each translation direction, all
the submitted systems were combined into
a consensus translation. These consen-
sus translations always improve transla-
tion quality of the best individual system.
1 Introduction
The UPV-PRHLT approach to MT system combi-
nation is based on a refined version of the algo-
rithm described in (Gonza?lez-Rubio and Casacu-
berta, 2010), with additional information to cope
with hypotheses of different quality.
In contrast to most of the previous approaches
to combine the outputs of multiple MT sys-
tems (Bangalore et al, 2001; Jayaraman and
Lavie, 2005; Matusov et al, 2006; Schroeder et
al., 2009), which are variations over the ROVER
voting scheme (Fiscus, 1997), we consider the
problem of computing a consensus translation as
the problem of modelling a set of string patterns
with an adequate prototype. Under this frame-
work, the translation hypotheses of each of the
MT systems are considered as individual patterns
in a set of string patterns. The (generalised) me-
dian string, which is the optimal prototype of a set
of strings (Fu, 1982), is the chosen prototype to
model the set of strings.
2 System Combination Algorithm
The median string of a set is defined as the string
that minimises the sum of distances to the strings
in the set. Therefore, defining a distance between
strings is the primary problem to deal with.
The most common definition of distance be-
tween two strings is the Levenshtein distance,
also known as edit distance (ED). This metric
computes the optimal sequence of edit operations
(insertions, deletions and substitutions of words)
needed to transform one string into the other. The
main problem with the ED is its dependence on the
length of the compared strings. This fact led to the
definition of a new distance whose value is inde-
pendent from the length of the strings compared.
This normalised edit distance (NED) (Vidal et al,
1995) is computed by averaging the number of edit
operations by the length of the edit path. The ex-
perimentation in this work was carried out using
the NED.
2.1 Median String
Given a set E = e1, . . . , en, . . . , eN of translation
hypotheses from N MT systems, let ? be the vo-
cabulary in the target language and ?? be the free
monoid over that vocabulary (E ? ??). The me-
dian string of the set E (noted as M(E)) can be
formally defined as:
M(E) = argmin
e????
N
?
n=1
[
wn ? D(e?, en)
]
, (1)
where D is the distance used to compare two
strings and the value wn, 1 ? n ? N weights
the contribution of the hypothesis n to the sum of
distances, and therefore, it denotes the significance
of hypothesis n in the computation of the median
string. The value wn can be seen as a measure of
the ?quality? of hypothesis n.
Computing the median string is a NP-Hard
problem (de la Higuera and Casacuberta, 2000),
therefore we can only build approximations to the
median string by using several heuristics. In this
work, we follow two different approximations: the
set median string (Fu, 1982) and the approximate
median string (Mart??nez et al, 2000).
296
2.2 Set Median String
The most straightforward approximation to the
median string corresponds to the search of a set
median string. Under this approximation, the
search is constrained to the strings in the given in-
put set. The set median string can be informally
defined as the most ?centred? string in the set. The
set median string of the set E (noted as Ms(E))
is given by:
Ms(E) = argmin
e??E
N
?
n=1
[
wn ? D(e?, en)
]
. (2)
The set median string can be computed in poly-
nomial time (Fu, 1982; Juan and Vidal, 1998).
Unfortunately, in some cases, the set median may
not be a good approximation to the median string.
For example, in the extreme case of a set of two
strings, either achieves the minimum accumulated
distance to the set. However, the set median string
is a useful initialisation in the computation of the
approximate median string.
2.3 Approximate Median String
A good approximation to efficiently compute the
median string is proposed in (Mart??nez et al,
2000). To compute the approximate median string
of the set E, the algorithm starts with an initial
string e which is improved by successive refine-
ments in an iterative process. This iterative pro-
cess is based on the application of different edit
operations over each position of the string e look-
ing for a reduction of the accumulated distance to
the strings in the set. Algorithm 1 describes this
iterative process.
The initial string can be a random string or
a string computed from the set E. Martinez et
al. (2000) proposed two kinds of initial strings: the
set median string of E and a string computed by a
greedy algorithm, both of them obtained similar
results. In this work, we start with the set median
string in the initialisation of the computation of the
approximate median string of the set E. Over this
initial string we apply the iterative procedure de-
scribed in Algorithm 1 until there is no improve-
ment. The final median string may be different
from the original hypotheses.
The computational time cost of Algorithm 1 is
linear with the number of hypotheses in the com-
bination, and usually only a moderate number of
iterations is needed to converge.
For each position i in the string e:
1. Build alternatives:
Substitution: Make x = e. For each word a ? ?:
? Make x? the result string of substituting the ith
word of x by a.
? If the accumulated distance of x? to E is lower
than the accumulated distance from x to E, then
make x = x?.
Deletion: Make y the result string of deleting the ith
word of e.
Insertion: Make z = e. For each word a ? ?:
? Make z? the result of inserting a at position i of
e.
? If the accumulated distance from z? to E is lower
than the accumulated distance from z to E, then
make z = z?.
2. Choose an alternative:
? From the set {e,x,y, z} take the string e? with
less accumulated distance to E. Make e = e?.
Algorithm 1: Iterative process to refine a string
e in order to reduce its accumulated distance to a
given set E.
3 Experiments
Experiments were conducted on all the 8 transla-
tion directions cz?en, en?cz, de?en, en?de,
es?en, en?es, fr?en and en?fr. Some of the
entrants to the shared translation task submit lists
of n-best translations, but, in our experience, if a
large number of systems is available, using n-best
translations does not allow to obtain better consen-
sus translations than using single best translations,
but raises computation time significantly. Conse-
quently, we compute consensus translations only
using the single best translation of each individ-
ual MT system. Table 1 shows the number of sys-
tems submitted and gives an overview of the test
corpus on each translation direction. The number
of running words is the average number of run-
ning words in the test corpora, from where the
consensus translations were computed; the vocab-
ulary is the merged vocabulary of these test cor-
pora. All the experiments were carried out with
the true-cased, detokenised version of the tuning
and test corpora, following the WMT 2010 sub-
mission guidelines.
3.1 Evaluation Criteria
We will present translation quality results in terms
of translation edit rate (TER) (Snover et al, 2006)
and bilingual evaluation understudy (BLEU) (Pa-
297
cz?en en?cz de?en en?de es?en en?es fr?en en?fr
Submitted systems 6 11 16 12 8 10 14 13
Avg. Running words 45K 37K 47K 41K 47K 47K 47K 49K
Distinct words 24K 51K 38K 40K 23K 30K 27K 37K
Table 1: Number of systems submitted and main figures of test corpora on each translation direction. K
stands for thousands of elements.
pineni et al, 2002). TER is computed as the num-
ber of edit operations (insertions, deletions and
substitutions of single words and shifts of word se-
quences) to convert the system hypothesis into the
reference translation. BLEU computes a geomet-
ric mean of the precision of n-grams multiplied by
a factor to penalise short sentences.
3.2 Weighted Sum of Distances
In section 2, we define the median string of a set
as the string which minimises a weighted sum of
distances to the strings in the set (Eq. (1)). The
weights wn in the sum can be tuned. We compute
a weight value for each MT system as a whole, i.e.
all the hypotheses of a given MT system share the
same weight value. We study the performance of
different sets of weight looking for improvements
in the quality of the consensus translations. These
weight values are derived from different automatic
MT evaluation measures:
? BLEU score of each system.
? 1.0 minus TER score of each system.
? Number of times the hypothesis of each sys-
tem is the best TER-scoring translation.
We estimate these scores on the tuning corpora.
A normalisation is performed to transform these
scores into the range [0.0, 1.0]. After the normal-
isation, a weight value of 0.0 is assigned to the
lowest-scoring hypothesis, i.e. the lowest-scoring
hypothesis is not taking into account in the com-
putation of the median string.
3.3 System Combination Results
Our framework to compute consensus translations
allows multiple combinations varying the median
string algorithm or the set of weight values used
in the weighted sum of distances. To assure the
soundness of our submission to the WMT 2010
system combination task, the experiments on the
tuning corpora were carried out in a leaving-one-
out fashion dividing the tuning data into 5 parts
and averaging translation results over these 5 par-
titions. On each of the experiments, 4 of the par-
titions are devoted to obtain the weight values for
the weighted sum of distances while BLEU and
TER scores are calculated on the consensus trans-
lations of the remaining partition.
Table 2 shows, on each translation direction,
the performance of the consensus translations on
the tuning corpora. The consensus translations
were computed with the set median string and the
approximated median string using different sets
of weight values: Uniform, all weights are set
to 1.0, BLEU-based weights, TER-based weights
and oracle-based weights. In addition, we display
the performance of the best of the individual MT
systems for comparison purposes. The number of
MT systems combined for each translation direc-
tion is displayed between parentheses.
On all the translation directions under study, the
consensus translations improved the results of the
best individual systems. E.g. TER improved from
66.0 to 63.3 when translating from German into
English. On average, the set median strings per-
formed better than the best individual system, but
its results were always below the performance of
the approximate median string. The use of weight
values computed from MT quality measures al-
lows to improve the quality of the consensus trans-
lation computed. Specially, oracle-based weight
values that, except for the cz?en task, always per-
form equal or better than the other sets of weight
values. We have observed that no improvements
can be achieved with uniform weight values; it is
necessary to penalise low quality hypotheses.
To compute our primary submission to the
WMT 2010 system combination task we choose
the configurations that obtain consensus transla-
tions with highest BLEU score on the tuning cor-
pora. The approximate median string using oracle-
based scores is the chosen configuration for all
translation directions, except on the cz?en trans-
lation direction for which TER-based weights per-
formed better. As our secondary submission we
298
Single Set median Approximated median
best Uniform Bleu Ter Oracle Uniform Bleu Ter Oracle
cz?en (6) BLEU 17.6 16.5 17.8 18.2 17.6 17.1 18.5 18.5 18.0TER 64.5 68.7 67.6 65.2 64.5 67.0 65.9 65.4 64.4
en?cz (11) BLEU 11.4 10.1 10.9 10.7 11.0 10.1 10.7 10.7 11.0TER 75.3 75.1 74.3 74.2 74.2 73.9 73.4 73.3 73.0
de?en (16) BLEU 19.0 19.0 19.1 19.3 19.7 19.3 19.8 19.9 20.1TER 66.0 65.4 65.2 65.0 64.6 64.4 63.4 63.4 63.3
en?de (12) BLEU 11.9 11.6 11.7 11.7 12.0 11.6 11.8 11.8 12.0TER 74.3 74.1 74.1 74.0 73.7 72.7 72.9 72.7 72.6
es?en (8) BLEU 23.2 23.0 23.3 23.2 23.6 23.1 23.9 23.8 24.2TER 60.2 60.6 59.8 59.8 59.5 60.0 59.2 59.4 59.1
en?es (10) BLEU 23.3 23.0 23.3 23.4 24.0 23.6 23.8 23.8 24.2TER 60.1 60.1 59.9 59.7 59.5 59.0 59.1 58.9 58.6
fr?en (14) BLEU 23.3 22.9 23.2 23.2 23.4 23.4 23.8 23.8 23.9TER 61.1 61.2 60.9 60.9 60.7 60.6 60.0 60.1 59.9
en?fr (13) BLEU 22.7 23.4 23.5 23.6 23.8 23.3 23.6 23.7 23.8TER 62.3 61.0 61.0 60.9 60.6 60.2 60.1 60.0 60.0
Table 2: Consensus translation results (case-sensitive) on the tuning corpora with the set median string
and the approximate median string using different sets of weights: Uniform, BLEU-based, TER-based
and oracle-based. The number of systems being combined for each translation direction is in parentheses.
Best consensus translation scores are in bold.
Best Secondary Primary
BLEU TER BLEU TER BLEU TER
cz?en 18.2 63.9 18.3 66.7 19.0 65.1
en?cz 10.8 75.2 11.3 73.6 11.6 71.9
de?en 18.3 66.6 19.1 65.4 19.6 63.9
en?de 11.6 73.4 11.7 72.9 11.9 71.7
es?en 24.7 59.0 24.9 58.9 25.0 58.2
en?es 24.3 58.4 24.9 57.3 25.3 56.3
fr?en 23.7 59.7 23.6 59.8 23.9 59.4
en?fr 23.3 61.3 23.6 59.9 24.1 58.9
Table 3: Translation scores (case-sensitive) on the
test corpora of our primary and secondary submis-
sions to the WMT 2010 system combination task.
chose the set median string using the same set of
weight values chosen for the primary submission.
We compute MT quality scores on the WMT
2010 test corpora to verify the results on the tuning
data. Table 3 displays, on each translation direc-
tion, the results on the test corpora of our primary
and secondary submissions and of the best indi-
vidual system. These results confirm the results
on the tuning data. On all translation directions,
our submissions perform better than the best indi-
vidual systems as measured by BLEU and TER.
4 Summary
We have studied the performance of two consen-
sus translation algorithms that based in the compu-
tation of two different approximations to the me-
dian string. Our algorithms use a weighted sum of
distances whose weight values can be tuned. We
show that using weight values derived from auto-
matic MT quality measures computed on the tun-
ing corpora allow to improve the performance of
the best individual system on all the translation di-
rections under study.
Acknowledgements
This paper is based upon work supported
by the EC (FEDER/FSE) and the Spanish
MICINN under the MIPRCV ?Consolider In-
genio 2010? program (CSD2007-00018), the
iTransDoc (TIN2006-15694-CO2-01) and iTrans2
(TIN2009-14511) projects and the FPU scholar-
ship AP2006-00691. This work was also sup-
ported by the Spanish MITyC under the eru-
dito.com (TSI-020110-2009-439) project and by
the Generalitat Valenciana under grant Prom-
eteo/2009/014 and scholarships BFPI/2007/117
and ACIF/2010/226 and by the Mexican govern-
ment under the PROMEP-DGEST program.
299
References
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In IEEE Workshop on ASRU,
pages 351?354.
C. de la Higuera and F. Casacuberta. 2000. Topology
of strings: Median string is np-complete. Theoreti-
cal Computer Science, 230:39?48.
J. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting
error reduction (rover).
K.S. Fu. 1982. Syntactic Pattern Recognition and Ap-
plications. Prentice Hall.
J. Gonza?lez-Rubio and F. Casacuberta. 2010. On the
use of median string for multi-source translation.
In Proceedings of 20th International Conference on
Pattern Recognition, Istambul, Turkey, May 27-28.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of EAMT, pages 143?152.
A. Juan and E. Vidal. 1998. Fast Median Search in
Metric Spaces. In Proc. of SPR, volume 1451 of
Lecture Notes in Computer Science, pages 905?912.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000.
Use of Median String for Classification. In Proc. of
ICPR, volume 2, pages 907?910.
E. Matusov, N. Ueffing, and H-Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Proc. of EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
J. Schroeder, T. Cohn, and P. Koehn. 2009. Word lat-
tices for multi-source translation. In Proc. of EACL,
pages 719?727.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of TER with targeted
human annotation. In Proc. of AMTA, pages 223?
231.
E. Vidal, A. Marzal, and P. Aibar. 1995. Fast compu-
tation of normalized edit distances. IEEE Transac-
tions on PAMI, 17(9):899?902.
300
