Proceedings of NAACL-HLT 2013, pages 1100?1109,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Paving the Way to a Large-scale Pseudosense-annotated Dataset
Mohammad Taher Pilehvar and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,navigli}@di.uniroma1.it
Abstract
In this paper we propose a new approach to
the generation of pseudowords, i.e., artificial
words which model real polysemous words.
Our approach simultaneously addresses the
two important issues that hamper the gener-
ation of large pseudosense-annotated datasets:
semantic awareness and coverage. We eval-
uate these pseudowords from three different
perspectives showing that they can be used as
reliable substitutes for their real counterparts.
1 Introduction
A fundamental problem in computational linguis-
tics is the paucity of manually annotated data, such
as part-of-speech tagged sentences, treebanks, and
logical forms, which exist only for few languages
(Ide et al, 2010). A case in point is the lack of
abundant sense annotated data, which hampers the
performance and coverage of lexical semantic tasks
such as Word Sense Disambiguation (Navigli, 2009;
Navigli, 2012, WSD) and semantic role labeling
(Gildea and Jurafsky, 2002). A possible way to
break this bottleneck is to use pseudowords, i.e., arti-
ficial words constructed by conflating a set of unam-
biguous words, with the aim of modeling polysemy
in real ambiguous words. The idea of pseudowords
was originally proposed by Gale et al (1992) and
Schu?tze (1992) for WSD evaluation, but later found
application in other tasks such as selectional prefer-
ences (Erk, 2007; Bergsma et al, 2008; Chambers
and Jurafsky, 2010), Word Sense Induction (Bor-
dag, 2006; Di Marco and Navigli, 2013) or studies
concerning the effects of the amount of data on ma-
chine learning for natural language disambiguation
(Banko and Brill, 2001). Being made up of monose-
mous words, pseudowords can potentially be used to
create large amounts of pseudosense-annotated data
at virtually no cost, hence enabling large-scale stud-
ies in lexical semantics. Unfortunately, though, the
extent of their usability for such a purpose is ham-
pered by two main issues: semantic awareness and
wide coverage.
Semantic awareness corresponds to the constraint
that pseudowords, in order to be realistic, are ex-
pected to have senses which are in a semantic rela-
tionship (thus modeling systematic polysemy). Re-
cent work has focused on this issue and, by exploit-
ing either specific lexical hierarchies (Nakov and
Hearst, 2003; Lu et al, 2006), or the WordNet struc-
ture (Otrusina and Smrz, 2010), have succeeded in
generating pseudowords which are comparable to
real words in terms of disambiguation difficulty. The
second challenge is coverage, which corresponds to
the number of distinct pseudowords an algorithm
can generate. When coupled with the semantic
awareness issue, wide coverage is hampered by the
difficulty in generating thousands of pseudowords
which mimic existing polysemous words.
Unfortunately, none of the existing approaches to
the generation of pseudowords can meet both these
challenges simultaneously, and this has hindered
the generation of a large pseudosense-annotated
dataset. For instance, approaches which exploit the
monosemous neighbors of a target sense in Word-
Net (Otrusina and Smrz, 2010) can be used to gener-
ate pseudowords with good semantic awareness, but
1100
they have low coverage of ambiguous nouns when
many pseudosense-tagged sentences are needed (cf.
Section 2.1.1).
In this paper we propose a new approach, based
on Personalized PageRank, which simultaneously
addresses the two above-mentioned issues concern-
ing the generation of pseudowords (i.e., seman-
tic awareness and coverage), and hence enables
the generation of large-scale pseudosense-annotated
datasets. We perform three different experiments to
show that our pseudowords are good at modeling ex-
isting ambiguous words in terms of disambiguation
difficulty, representativeness of real senses and dis-
tinguishability of the artificial senses. As a byprod-
uct of this work, we generate a large dataset that pro-
vides 1000 tagged sentences for each of the 15,935
pseudowords modeled after real ambiguous nouns in
WordNet 3.0.
2 Pseudowords
A pseudoword p = w1*w2*. . . *wn is an artificially-
generated ambiguous word of polysemy degree n
which is usually created by conflating n unique un-
ambiguous words wi called pseudosenses. For in-
stance, airplane*river is a pseudoword with two
meanings explicitly identified by its pseudosenses:
airplane and river. Pseudowords are particularly in-
teresting as they can be used to introduce controlled
artificial ambiguity into a corpus. Given a pseu-
doword p and an untagged corpus C, this artificial
tagging is achieved by substituting all occurrences of
wi in C with p for each pseudosense i ? {1, . . . , n}.
As a result, each occurrence of the pseudoword p is
tagged with the underlying sense wi. As an example,
consider the following two sentences:
a1. The Wright brothers invented the airplane.
a2. The Nile is the longest river in the world.
If we replace the individual occurrences of air-
plane and river with the pseudoword airplane*river
while noting the replaced term as the corresponding
sense, we obtain the following pseudosense-tagged
sentences:
b1. The Wright brothers invented the airplane*river.
b2. The Nile is the longest airplane*river in the world.
As a result of this procedure, we obtain a corpus
of sentences containing the occurrences of an arti-
ficially ambiguous word p, for each of which we
know its correct sense annotation wi. Virtually any
number of pseudowords can be created, resulting in
a large pseudosense-annotated corpus. An obvious
restriction on the choice of pseudosenses is that they
need to be unambiguous, so as to avoid the introduc-
tion of uncontrolled ambiguity. Another constraint
is that the constituent wi must satisfy a minimum oc-
currence frequency in the corpus C. This minimum
frequency corresponds to the number of annotated
sentences that are requested for the task of interest
which will exploit the resulting annotated corpus.
An immediate way of generating a pseudoword
would be to randomly select its constituents from
the set of all monosemous words given by a lexi-
con (e.g., WordNet). However, constructing a pseu-
doword by merely combining a random set of unam-
biguous words selected on the basis of their falling
in the same range of occurrence frequency (Schu?tze,
1992), or leveraging homophones and OCR ambi-
guities (Yarowsky, 1993), does not provide a suit-
able model of a real polysemous word (Gaustad,
2001; Nakov and Hearst, 2003). This is because
in the real world different senses, unless they are
homonymous, share some semantic or pragmatic re-
lation. Therefore, random pseudowords will typ-
ically model only homonymous distinctions (such
as the centimeter vs. curium senses of cm), while
they will fall short of modeling systematic polysemy
(such as the lack vs. insufficiency senses of defi-
ciency).
2.1 Semantically-aware Pseudowords
In order to cope with the above-mentioned lim-
its of random pseudowords, an artificial word has
to model an existing word by providing a one-to-
one correspondence between each pseudosense and
a corresponding sense of the modeled word. For
instance, the pseudoword lack*shortfall is a good
model of the real word deficiency in that its pseu-
dosenses preserve the meanings of their correspond-
ing real word?s senses. We call this kind of artificial
words semantically-aware pseudowords.
In the next two subsections, we will describe two
techniques (the second of which is presented for
the first time in this paper) for the generation of
1101
Minimum Polysemy
Overall
Frequency 2 3 4 5 6 7 8 9 10 11 12 >12
0 87 82 74 71 67 70 60 64 45 46 44 28 83
500 41 31 24 15 12 13 10 7 7 0 0 0 35
1000 31 20 16 7 4 6 4 3 0 0 0 0 25
Table 1: Ambiguous noun coverage percentage of vicinity-based pseudowords by degree of polysemy for different
values of minimum pseudosense occurrence frequency in Gigaword.
semantically-aware pseudowords. In what follows
we focus on nominal pseudowords, and leave the ex-
tension to other parts of speech to future work.
2.1.1 Vicinity-based Pseudowords
A computational lexicon such as WordNet (Fell-
baum, 1998) can be used as the basis for the
automatic generation of semantically-aware pseu-
dowords, an idea which was first proposed by
Otrusina and Smrz (2010). WordNet can be viewed
as a graph in which synsets act as nodes and the lexi-
cal and semantic relationships among them as edges.
Given a sense, the approach looks into its surround-
ing synsets in the WordNet graph in order to find
a related monosemous term that can represent that
sense. As search space, the approach considers: the
other literals in the same synset, the genus phrase
from its textual definition, direct siblings, and di-
rect hyponyms. If no monosemous candidate can be
found, this space is further extended to hypernyms
and meronyms. Hereafter, we term this approach as
vicinity-based.
For example, consider the generation process of
the vicinity-based pseudoword corresponding to the
term coke, which has three senses in WordNet 3.0.
There exist multiple monosemous candidates for
each sense: dozens of candidates (such as biomass
and butane) in the direct siblings? vicinity of the
first sense, coca cola, pepsi, and pepsi cola for the
second sense, and nose candy and coca cola for
the third sense. Among these candidates Otrusina
and Smrz (2010) select those whose occurrence fre-
quency ratio in a given text corpus is most similar to
that of the senses of the corresponding real word as
given by a sense-annotated corpus. Clearly, a suffi-
ciently large sense-tagged corpus is required for cal-
culating the occurrence frequency of the individual
senses of a word. This is a limitation of the vicinity-
based approach.
In addition, as we mentioned earlier, we need
pseudowords that can enable the generation of large-
scale pseudosense-tagged corpora. For this to be
achieved, each pseudosense is required to occur with
a relatively high frequency in a given text corpus.
The vicinity-based approach can, however, identify
at best only a few representatives for each pseu-
dosense, thus undermining its ability to cover many
ambiguous nouns. Table 1 shows the percentage
of ambiguous nouns in WordNet that can be mod-
eled using the vicinity-based approach when differ-
ent minimum numbers of annotated sentences are
requested, i.e. each pseudosense is required to oc-
cur in at least 0 (i.e., no minimum frequency restric-
tion), 500, or 1000 unique sentences in the reference
corpus (we use Gigaword (Graff and Cieri, 2003)
in our experiments). In the Table, beside the over-
all coverage percentage, we present the coverage by
degree of polysemy and for three different values of
minimum pseudosense occurrence frequency. Even
though the overall coverage is over 80% when no re-
striction on minimum frequency is considered (first
row in the Table), this high coverage drops rapidly
when we request some hundred sentences per sense.
For instance, only 25% of the ambiguous nouns in
WordNet can be modeled using this approach when
a minimum frequency of 1000 noun occurrences is
required (last row of Table 1), with most of the cov-
ered words having low polysemy (in fact about 93%
of them are either 2- or 3-sense nouns). This se-
vere limitation of the vicinity-based approach hin-
ders a wide-coverage modeling of ambiguous nouns
in WordNet, thus preventing it from being an op-
tion for the generation of a large-scale pseudosense-
annotated dataset.
With a view to addressing the above-mentioned
issues and to enable wide coverage, in the next sub-
section we propose a flexible approach for the gen-
eration of semantically-aware pseudowords.
1102
2.1.2 Similarity-based Pseudowords
The vicinity-based pseudoword generation ap-
proach works on local subgraphs of WordNet, con-
sidering mostly all those candidates which are in a
direct relationship with a real sense si, and treating
them as potentially good representatives of si. We
propose an extension to this approach which exploits
the WordNet semantic network in its entirety, hence
enabling us to determine a graded degree of similar-
ity between si and all the senses of all other words
in WordNet.
We chose a graph-based similarity measure for
two reasons: firstly, it comes as a natural exten-
sion of the vicinity-based method, and, secondly, al-
ternative context-based methods such as Lin?s mea-
sure (Lin, 1998) have been shown to require a wide-
coverage sense-tagged dataset in order to calculate
similarities on a sense-by-sense basis for all words in
the lexicon (Otrusina and Smrz, 2010). As our sim-
ilarity measure we selected the Personalized PageR-
ank (Haveliwala, 2002, PPR) algorithm. PPR basi-
cally computes the probability according to which a
random walker at a specific node in a graph would
visit an arbitrary node in the same graph. The al-
gorithm estimates, for a specific node in a graph,
a probability distribution (called PPR vector) which
determines the importance of any given node in the
graph for that specific node. When applied to a
semantic graph, this importance can be interpreted
as semantic similarity. PPR has previously been
used as a core component for semantic similarity1
(Hughes and Ramage, 2007; Agirre et al, 2009)
and Word Sense Disambiguation (Agirre and Soroa,
2009).
Algorithm 1 shows the procedure for the genera-
tion of our similarity-based pseudowords. The algo-
rithm takes an ambiguous word w as input, and out-
puts its corresponding similarity-based pseudoword
Pw whose ith pseudosense models the ith sense of
w, together with a confidence score which we detail
below.
Given w, the algorithm iterates over the synsets
corresponding to its individual senses (lines 4-13)
and finds the most suitable pseudosenses for Pw. For
1Top-ranking synsets will contain words which are most
likely similar to the target sense, whereas we move to a graded
notion of relatedness as far as lower-ranking ones are concerned
(Agirre et al, 2009).
Algorithm 1 Generate a similarity-based pseudoword
Input: an ambiguous word w in WordNet
Output: a ?similarity-based? pseudoword Pw
a confidence score averageRank
1: Pw ? ?
2: totalRank? 0
3: i? 1
4: for each s ? Synsets(w)
5: similarSynsets? PersonalizedPageRank(s)
6: sort similarSynsets in descending order
7: for each s? ? similarSynsets
8: totalRank? totalRank + 1
9: for each w? ? SynsetLiterals(s?)
10: if |Synsets(w?)|=1 & Freq(w?)>minFreq then
11: Pw ? Pw ? {(i, w?)}
12: break
13: i? i + 1
14: averageRank? totalRank/|Synsets(w)|
15: return (Pw, averageRank)
each synset s of w, we start the PPR algorithm from
s (line 5) and collect the probability distribution vec-
tor output by PPR (similarSynsets in the algorithm),
which determines the probability of reaching each
synset in WordNet starting from s. We then sort
this vector (line 6) and check if each of its nomi-
nal synsets (s?) contains a monosemous word (line
10). This search continues until a suitable candi-
date is found that satisfies a certain minimum oc-
currence frequency minFreq. When this occurs, the
selected monosemous candidate w? is saved as the
corresponding pseudosense for the ith sense of Pw
(line 11). We iterate these steps for all synsets of w.
In line 14 we calculate the averageRank, a value
given by the average of synset positions in the simi-
larSynsets lists from which the pseudosenses of Pw
are picked out. We later use this value as a confi-
dence score while evaluating our pseudowords. Fi-
nally, the algorithm returns the corresponding pseu-
doword Pw along with its averageRank score (line
15). We show in Table 2 some examples of ambigu-
ous words together with their similarity-based pseu-
dowords.
Thanks to the large search space of our similarity-
based approach, we are always able to select a
monosemous candidate for each pseudosense, thus
resolving the coverage issue regarding vicinity-
based pseudowords. A question that arises here is
that of how often our algorithm needs to resort to
lower-ranking items in the similarSynsets list. To
1103
Word Similarity-based Pseudoword
bernoulli physicist*mathematician*astronomer
coach football coach*tutor*passenger car*clarence*
public transport
green greenery*common*labor leader*
green party*river*golf course*greens*max
horoscope forecast*diagram
sunray sunbeam*vine*sunlight
lifter athlete*thief
Table 2: Similarity-based pseudowords generated for
six different nouns in WordNet 3.0 (with minimum fre-
quency of 1000 occurrences in Gigaword). Pseudosenses
which could not be modeled using the vicinity-based ap-
proach are shown in bold.
verify this, we analyzed the averageRank values out-
put by Algorithm 1. Table 3 shows for each poly-
semy degree and for three different values of min-
Freq, the mean and mode statistics of the averageR-
ank scores of the generated similarity-based pseu-
dowords for all the 15,935 polysemous nouns in
WordNet 3.0. As expected, the higher the number of
required sentences per pseudosense (minFreq), the
further the algorithm descends through the list simi-
larSynsets to select a pseudosense. However, as can
be seen from the mode statistics in the Table, even
when minFreq is set to a large value, most of the
pseudosenses are picked from the highest-ranking
positions in the similarSynsets list.
3 Evaluation
Our novel similarity-based algorithm for the gen-
eration of pseudowords inherently tackles the cov-
erage issue. To test whether our generated pseu-
dowords also cope with the issue of semantic aware-
ness we carried out three separate evaluations so
as to assess their strength in modeling semantic
properties of their corresponding real senses from
different perspectives. These will be described in
the next three subsections. Since our aim was to
leverage pseudowords for the creation of a large-
scale pseudosense-annotated dataset, we performed
evaluations on pseudowords generated with minFreq
per pseudosense set to 1000 (i.e., we can gener-
ate at least 1000 annotated sentences for each pseu-
dosense).
minFreq 0 500 1000
poly. mean mode mean mode mean mode
2 2.0 1.0 14.8 2.0 25.4 4.0
3 2.3 1.7 13.4 2.7 21.0 5.5
4 2.3 1.8 12.3 5.8 19.8 6.8
5 2.3 1.8 12.9 5.6 20.0 10.0
6 2.4 2.0 13.7 4.5 18.7 8.8
7 2.3 2.1 11.5 6.3 16.0 6.1
8 2.2 1.8 11.3 9.6 17.2 10.8
9 2.4 2.0 10.7 10.9 15.6 15.1
10 2.2 2.0 10.1 7.0 14.3 12.1
11 2.4 2.1 10.2 7.1 14.2 17.3
12 2.5 2.4 11.0 4.4 14.4 14.4
>12 2.6 1.0 9.3 2.0 13.7 4.0
overall 2.1 1.0 14.1 2.0 23.4 4.0
Table 3: Statistics of averageRank scores for the full set
of 15,935 similarity-based pseudowords modeled after
ambiguous nouns in WordNet 3.0: we show mean and
mode statistics for three different values of minimum oc-
currence frequency (0, 500, and 1000). We show the av-
erage value in the case of multiple modes.
3.1 Disambiguation Difficulty of Pseudowords
Our first experiment is an extrinsic evaluation of
pseudowords. Ideally, pseudowords are expected to
show a similar degree of difficulty to real ambigu-
ous words in a disambiguation task (Otrusina and
Smrz, 2010; Lu et al, 2006). We thus experimen-
tally tested this assumption on similarity-based and
random pseudowords. Given its low coverage, we
excluded the vicinity-based approach from this ex-
periment.
Starting from a sense-tagged lexical sample
dataset for a set of ambiguous nouns, for each such
noun and for each kind of pseudoword, we automat-
ically generated a pseudosense-annotated dataset by
enforcing the same sense distribution as the cor-
responding real ambiguous noun. This constraint
was particularly important for random pseudowords
since they do not model the corresponding real am-
biguous words (see Section 2). An analysis was then
performed to compare the disambiguation perfor-
mance of a supervised WSD system on a given am-
biguous word against its corresponding pseudoword.
Specifically, for our manually sense-tagged cor-
pus we used the Senseval-3 English lexical sample
dataset (Mihalcea et al, 2004), which contains 3593
and 1807 sense-tagged sentences for 20 ambiguous
nouns (with an average polysemy degree of 5.8) in
its training and test sets, respectively. We generated,
1104
with minFreq = 1000, the similarity-based pseu-
dowords corresponding to these 20 nouns, as well
as a set of 20 random pseudowords with the same
polysemy degrees. We note that, in this setting, the
vicinity-based approach could only generate pseu-
dowords corresponding to 5 of the 20 nouns.
In order to create the datasets for our experiments,
for each of our similarity-based and random pseu-
dowords, we sampled unique sentences from the En-
glish Gigaword corpus (Graff and Cieri, 2003) ac-
cording to the same sense distributions given by the
Senseval-3 training and test datasets for the corre-
sponding real word. Next, we performed WSD on
our three datasets, namely: the Senseval-3 dataset
of real words, and the two artificially sense-tagged
datasets for the similarity-based and random pseu-
dowords. As our WSD system for this experiment,
we used It Makes Sense (IMS), a state-of-the-art su-
pervised WSD system (Zhong and Ng, 2010).
WSD recall2 performance values on the above-
mentioned datasets are shown in Table 4. For the
random setting, in order to ensure stability, the re-
sults are averaged on a set of 25 different pseu-
dowords modeling a given ambiguous noun. We
can see from the Table that the overall system
performance with the similarity-based pseudowords
(75.14%) is much closer to the real setting (73.26%)
than it is with random pseudowords (78.80%). For
random pseudowords, the overall recall over 25 runs
ranges from 75.40% to 80.80%.
Moreover, the similarity-based approach exhibits
a closer WSD recall performance to that of real data
(|RE?SB| column in the table) for 15 of the 20
nouns (shown in bold in the Table). Accordingly,
the overall sum of the differences (distance) between
the recall values is 129.3 for similarity-based pseu-
dowords, which is considerably lower than the 196.4
for random pseudowords (averaged over 25 runs
whose distances range from 158.3 to 262.0).
To further corroborate our findings, we calculated
the Pearson?s r correlation between recall values on
real words with those obtained on the corresponding
pseudowords. Similarity-based pseudowords obtain
the high correlation of 0.74, whereas this value drops
to 0.54 for random pseudowords. Even worse, we
2Since in our experiments the WSD system always provides
an answer for each item in the test set, the values of precision,
recall and F1 will be equal.
Word RE SB RND |RE?SB| |RE?RND|
argument 50.44 68.79 77.15 18.35 26.71
arm 92.30 85.69 88.11 6.61 4.19
atmosphere 70.52 69.15 80.44 1.37 10.32
audience 81.28 73.74 83.76 7.54 4.22
bank 85.76 83.07 82.46 2.69 3.99
degree 78.42 81.58 80.59 3.16 4.35
difference 62.46 61.43 75.17 1.03 12.90
difficulty 52.72 51.82 67.23 0.90 14.97
disc 78.62 76.48 78.07 2.14 6.18
image 71.78 75.76 81.50 3.98 10.02
interest 77.34 73.19 71.70 4.15 6.85
judgment 55.64 66.87 59.64 11.23 9.01
organization 80.36 72.86 78.65 7.50 3.65
paper 60.84 66.29 73.14 5.45 12.59
party 82.94 80.00 81.04 2.94 3.74
performance 58.56 64.76 73.86 6.20 15.52
plan 88.42 85.41 87.39 3.01 3.12
shelter 58.48 74.75 80.21 16.27 21.73
sort 67.64 88.15 77.37 20.51 9.73
source 63.46 67.74 66.26 4.28 7.03
overall 73.26 75.14 78.80 129.31 196.35
Table 4: Recall percentage of IMS on the 20 nouns of the
Senseval-3 lexical-sample test set (RE) compared to the
corresponding similarity-based (SB) and random (RND)
pseudowords. The last 2 columns show absolute differ-
ences between the real and the two pseudoword settings.
observed a high variation of correlation (in the range
of [0.18, 0.67]) over the 25 sets of random pseu-
dowords (0.54 being the average).
3.2 Representative Power of Pseudosenses
The ideal case for pseudosenses would be that of
being in a synonymous relationship with the cor-
responding real sense, i.e., selected from the same
WordNet synset. But given that many of the Word-
Net synsets do not contain monosemous terms, the
similarity-based approach often needs to look fur-
ther into other related synsets to find a suitable pseu-
dosense. To get a clear idea of the exact statistics, we
went through all our similarity-based pseudowords
and, for each pseudosense wi, checked the relation-
ship in WordNet between the synset containing wi
and the corresponding real sense. Table 5 shows
for three values of minFreq the distribution of pseu-
dosenses across different types of WordNet relation-
ships, also including indirect ones. As can be seen
in the Table, when minFreq is set to 0, a large por-
tion of pseudosenses (around 75%) are selected from
synonyms or generalization/specialization relations
1105
minFreq 0 500 1000
R
el
at
io
n
ty
pe
Synonyms 33.0 7.6 5.4
Hypernyms 33.4 16.1 13.0
Hyponyms 9.1 6.1 4.9
Meronyms 0.2 0.2 0.2
Siblings 8.2 17.2 16.6
Indirect relations 16.1 52.8 59.9
Table 5: Percentage of similarity-based pseudosenses ob-
tained from different types of WordNet relations.
(hypernym and hyponyms). However, this percent-
age drops to about 23% when minFreq = 1000. This
suggests that many of our pseudosenses are mod-
eled from indirect relations when higher values of
minFreq are used. This can potentially increase the
risk of an undesirable modeling in which meanings
are not properly preserved. For this reason, we car-
ried out another experiment to assess the representa-
tive power of similarity-based pseudosenses. To this
end, we randomly sampled 110 pseudowords (from
the entire set of 15,935 pseudowords generated with
minimum frequency of 1000), 10 for each degree of
polysemy, from 2 to 12, totaling 770 pseudosenses.
Then we presented each of these pseudowords3 to
two annotators who were asked to judge the degree
of representativeness of its pseudosenses based on
the following scores: 1: completely unrelated, 2:
somewhat related, 3: good substitute, or 4: perfect
substitute.
As an example, the scores assigned by the two
annotators to different pseudosenses of the pseu-
doword generated for the noun representative are
shown in Table 6. The overall representativeness
score for each pseudoword is calculated by aver-
aging the scores assigned to its individual pseu-
dosenses. For instance, the overall scores calculated
for the pseudoword representative are 3.75 and 3.50
(as given by the two annotators). The first row in
Table 7 shows the average representativeness scores
for each degree of polysemy on the full set of 770
pseudosenses. It can be seen that the score remains
around 3.0 for all polysemy degrees from 2 to 12.
Despite the fact that only one fifth of pseudosenses
are taken from synonyms, hypernyms and hyponyms
(when minFreq is 1000, cf. Table 5), the overall
3For each pseudoword, we provided annotators with the cor-
responding real word, as well as its synsets and glosses as given
by WordNet.
Sense Definition (in short)
Sc
or
e
1
Sc
or
e
2
{Synset}
> Corresponding Pseudosense
a person who represents others
3 3{representative}
> negotiator
an advocate who represents someone else?s policy
4 4{spokesperson, interpreter, representative, voice}
> spokesperson
a member of the U.S. House of Representatives
4 4{congressman, congresswoman, representative}
> congressman
an item of information that is typical of a group
4 3{example, illustration, instance, representative}
> case in point
average score 3.75 3.50
Table 6: Examples of representativeness scores assigned
by the annotators to pseudosenses of the term representa-
tive.
representativeness score of 3.12 shows that most of
these pseudosenses can be considered as good sub-
stitutes for their corresponding real senses. There-
fore we conclude that not only does our similarity-
based pseudoword generation approach extend the
coverage of the vicinity-based method from 25% to
100% (when minFreq = 1000), but also that the
pseudosenses coming from more distant synsets as
ranked by PPR are still good representatives on av-
erage.
3.3 Distinguishability of Pseudosenses
In addition to assessing the representativeness of
pseudosenses, their degree of distinguishability has
to be determined. In other words, we have to de-
termine how easily each pseudosense can be distin-
guished from the others in a pseudoword. Our rea-
son for having such an experiment is readily illus-
trated by way of an example: consider the similarity-
based pseudoword philanthropist*benefactor4 cor-
responding to the noun donor5. Even though both
pseudosenses are good representatives for their cor-
responding senses, the distinguishability of the two
4From WordNet: ?Philanthropist: someone who makes
charitable donations intended to increase human well-being?;
?Benefactor: a person who helps people or institutions (espe-
cially with financial help)?.
5donor has 2 senses according to WordNet 3.0: (1) ?person
who makes a gift of property?; (2) ?(medicine) someone who
gives blood or tissue or an organ to be used in another person?.
1106
Polysemy 2 3 4 5 6 7 8 9 10 11 12 Overall
Representativeness score 3.3 3.4 3.1 3.1 2.9 3.1 2.9 2.8 3.3 3.1 3.3 3.12
Distinguishability score 0.90 0.83 0.83 0.82 0.81 0.77 0.75 0.73 0.80 0.71 0.70 0.79
Table 7: Average representativeness and distinguishability scores for pseudosenses of different polysemy classes
(scores range from 1 to 4 for representativeness and from 0 to 1 for distinguishability evaluation).
real senses is not preserved in the pseudoword. For
instance, benefactor is a suitable pseudosense for
both senses of donor, whereas philanthropist cannot
be used in the blood donation sense.
Therefore we carried out another manual evalua-
tion to test the efficacy of pseudowords in preserving
the distinguishability of senses of real words. To this
end, for each pseudoword Pw (from the same set of
110 sampled pseudowords used in Section 3.2) we
presented its corresponding pseudosenses in random
order to two annotators and asked them to associate
each pseudosense with the most appropriate Word-
Net sense of the real word w. Then we calculated
a distinguishability score for each polysemy degree
by dividing the number of correct mappings by the
total number of senses.
For instance, for the similarity-based pseudoword
corresponding to the word representative (shown
in Table 6), we provided the shuffled list of pseu-
dosenses [spokesperson, case in point, negotiator,
congressman] to each annotator and asked them to
sort the list according to the WordNet sense inven-
tory of representative (i.e., map each pseudosense to
its most suitable real sense). Both annotators cor-
rectly mapped all pseudosenses of this pseudoword;
hence, the distinguishability score given by each an-
notator for this pseudoword was 4/4 = 1.
The average distinguishability scores for each de-
gree of polysemy, as well as the overall score, is
shown in Table 7 (second row). Each value is an
average of the scores obtained from the two an-
notators. It can be seen that the distinguishability
score decreases for higher degrees of polysemy. The
score, however, remains above 0.70 with highly-
polysemous pseudowords. The overall score of 0.79
shows that similarity-based pseudowords effectively
preserve the distinguishability of senses of their real
counterparts. In other words, they do not tend
to have over-generalized pseudosenses which cover
more than one sense.
4 Related Work
The idea of pseudowords dates back to 1992, when
it was first proposed as a means of generating large
amounts of artificially annotated evaluation data for
WSD algorithms (Gale et al, 1992; Schu?tze, 1992).
However, as mentioned earlier in Section 2, con-
structing a pseudoword by combining a random set
of unambiguous words, as was done in these early
works, can not model systematic polysemy (Gaus-
tad, 2001; Nakov and Hearst, 2003), since differ-
ent senses of a real ambiguous word, unless it is
homonymous, share some semantic or pragmatic re-
lation.
Several researchers addressed the issue of produc-
ing semantically-aware pseudowords that can model
semantic relationships between senses. Nakov
and Hearst (2003) used lexical category mem-
bership from a medical term hierarchy (extracted
from MeSH6 (Medical Subject Headings)) to cre-
ate ?more plausibly-motivated? pseudowords. By
considering the frequency distributions from lexi-
cal category co-occurrence, they produced a set of
pseudowords which were closer to real ambiguous
words in terms of disambiguation difficulty than
random pseudowords. However, this approach re-
quires a specific hierarchical lexicon and falls short
of creating many pseudowords with high polysemy
(the authors report generating pseudowords with two
senses only).
More recent work has focused on the identifica-
tion of monosemous representatives in the surround-
ing of a sense, i.e., selected among concepts directly
related to the given sense. Lu et al (2006) mod-
eled senses of a real ambiguous word by picking
out the most similar monosemous morpheme from a
Chinese hierarchical lexicon. Pseudowords are then
constructed by conflating these morphemes accord-
ingly. However, this method leverages a specific
Chinese hierarchical lexicon, in which different lev-
6http://www.nlm.nih.gov/mesh
1107
els of the hierarchy correspond to different levels of
sense granularity. A more flexible technique is pro-
posed by Otrusina and Smrz (2010) who model am-
biguous words in WordNet. Their vicinity-based ap-
proach searches the surroundings of each particular
sense in the WordNet graph in order to find an un-
ambiguous representative for that sense. However,
as we described in Section 2.1.1, while the approach
addresses the semantic awareness issue, it falls short
of providing a high coverage, an issue which we
tackle in our novel similarity-based approach.
5 Conclusion and Future Work
In this paper we proposed a new technique for the
generation of pseudowords which, in contrast to
existing work, can simultaneously tackle the two
major issues associated with pseudowords, i.e., se-
mantic awareness and coverage. Our approach can
be used to model any given ambiguous noun in
WordNet, hence enabling the generation of large-
scale pseudosense-annotated datasets for thousands
of pseudowords. We performed three experiments
to evaluate the reliability of our pseudowords. We
showed that the similarity-based pseudowords are
highly correlated with their real counterparts in
terms of disambiguation difficulty. Further evalua-
tions demonstrated that this approach is able to pro-
vide a good semantic modeling of individual senses
of real words while preserving their distinguishabil-
ity.
We are releasing to the research community
the entire set of 15,935 pseudowords, i.e., for
all WordNet polysemous nouns (http://lcl.
uniroma1.it/pseudowords/). This set of
pseudowords (together with the English Gigaword
corpus) can be used to generate a large pseudosense-
tagged dataset containing ?1000 annotated sen-
tences for every sense of all the pseudowords mod-
eled after real ambiguous nouns in WordNet. The
resulting dataset could be a good complement for
MASC (Ide et al, 2010) which, being human-
created, can provide 1000 sense-annotated sentences
for just a few words.
We hope that the availability of this resource will
enable large-scale experiments in tasks such as se-
mantic role labeling, semantic parsing, and Word
Sense Disambiguation. Specifically, as future work,
we plan to utilize the generated pseudosense-tagged
dataset to perform an in-depth study of different
WSD paradigms. We also plan to extend our work
to other part-of-speech tags.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ?09, pages 33?41, Athens, Greece.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
?09, pages 19?27, Boulder, Colorado.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting on
Association for Computational Linguistics, ACL ?01,
pages 26?33, Toulouse, France.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, Hawaii.
Stefan Bordag. 2006. Word Sense Induction: Triplet-
based clustering and automatic evaluation. In Pro-
ceedings of the 11th Conference on European chap-
ter of the Association for Computational Linguistics,
EACL ?06, pages 137?144, Trento, Italy.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selectional
preferences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?10, pages 445?453, Uppsala, Sweden.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-
based Word Sense Induction. Computational Linguis-
tics, 39(4).
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
1108
Annual Meeting of the Association of Computational
Linguistics, ACL ?07, pages 216?223, Prague, Czech
Republic.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
William Gale, Kenneth Church, and David Yarowsky.
1992. Work on statistical methods for Word Sense
Disambiguation. In Proceedings of the AAAI Fall
Symposium on Probabilistic Approaches to Natural
Language, pages 54?60, Menlo Park, CA.
Tanja Gaustad. 2001. Statistical corpus-based Word
Sense Disambiguation: Pseudowords vs real ambigu-
ous words. In Companion Volume to the Proceed-
ings of the 39th Annual Meeting of the Association
for Computational Linguistics Proceedings ot the Stu-
dent Research Workshop, ACL/EACL ?01, pages 61?
66, Toulouse, France.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW ?02, pages 517?526, Hon-
olulu, Hawaii, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, EMNLP-CoNLL
?07, pages 581?589, Prague, Czech Republic.
Nancy Ide, Collin F. Baker, Christiane Fellbaum, and Re-
becca J. Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (Short Pa-
pers), pages 68?73, Uppsala, Sweden.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, ICML ?98,
pages 296?304, Madison, USA.
Zhimao Lu, Haifeng Wang, Jianmin Yao, Ting Liu, and
Sheng Li. 2006. An equivalent pseudoword solution
to Chinese Word Sense Disambiguation. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL ?06,
pages 457?464, Sydney, Australia.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: The Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 25?28, Barcelona,
Spain.
Preslav I. Nakov and Marti A. Hearst. 2003. Category-
based pseudowords. In Proceedings of the Confer-
ence of the North American Chapter of the Association
of Computational Linguistics ? short papers, HLT-
NAACL ?03, pages 67?69, Edmonton, Canada.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of word sense
disambiguation, induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science, SOF-
SEM ?12, pages 115?129, Spindleruv Mlyn, Czech
Republic.
Lubomir Otrusina and Pavel Smrz. 2010. A new ap-
proach to pseudoword generation. In Proceedings of
the International Conference on Language Resources
and Evaluation, LREC?10, pages 1195?1199, Valletta,
Malta.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In Supercomputing ?92: Proceedings of the 1992
ACM/IEEE conference on Supercomputing, pages
787?796, Minneapolis, Minnesota, USA.
David Yarowsky. 1993. One sense per collocation.
In Proceedings of the 3rd DARPA Workshop on Hu-
man Language Technology, pages 266?271, Princeton,
New Jersey.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage Word Sense Disambiguation system
for free text. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL?10, pages 78?83, Uppsala, Sweden.
1109
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341?1351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Align, Disambiguate and Walk: A Unified Approach for
Measuring Semantic Similarity
Mohammad Taher Pilehvar, David Jurgens and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,jurgens,navigli}@di.uniroma1.it
Abstract
Semantic similarity is an essential com-
ponent of many Natural Language Pro-
cessing applications. However, prior meth-
ods for computing semantic similarity of-
ten operate at different levels, e.g., sin-
gle words or entire documents, which re-
quires adapting the method for each data
type. We present a unified approach to se-
mantic similarity that operates at multiple
levels, all the way from comparing word
senses to comparing text documents. Our
method leverages a common probabilistic
representation over word senses in order to
compare different types of linguistic data.
This unified representation shows state-of-
the-art performance on three tasks: seman-
tic textual similarity, word similarity, and
word sense coarsening.
1 Introduction
Semantic similarity is a core technique for many
topics in Natural Language Processing such as
Textual Entailment (Berant et al, 2012), Seman-
tic Role Labeling (Fu?rstenau and Lapata, 2012),
and Question Answering (Surdeanu et al, 2011).
For example, textual similarity enables relevant
documents to be identified for information re-
trieval (Hliaoutakis et al, 2006), while identifying
similar words enables tasks such as paraphrasing
(Glickman and Dagan, 2003), lexical substitution
(McCarthy and Navigli, 2009), lexical simplifica-
tion (Biran et al, 2011), and Web search result
clustering (Di Marco and Navigli, 2013).
Approaches to semantic similarity have often
operated at separate levels: methods for word sim-
ilarity are rarely applied to documents or even sin-
gle sentences (Budanitsky and Hirst, 2006; Radin-
sky et al, 2011; Halawi et al, 2012), while
document-based similarity methods require more
linguistic features, which often makes them in-
applicable at the word or microtext level (Salton
et al, 1975; Maguitman et al, 2005; Elsayed et
al., 2008; Turney and Pantel, 2010). Despite the
potential advantages, few approaches to semantic
similarity operate at the sense level due to the chal-
lenge in sense-tagging text (Navigli, 2009); for ex-
ample, none of the top four systems in the recent
SemEval-2012 task on textual similarity compared
semantic representations that incorporated sense
information (Agirre et al, 2012).
We propose a unified approach to semantic sim-
ilarity across multiple representation levels from
senses to documents, which offers two signifi-
cant advantages. First, the method is applicable
independently of the input type, which enables
meaningful similarity comparisons across differ-
ent scales of text or lexical levels. Second, by op-
erating at the sense level, a unified approach is able
to identify the semantic similarities that exist in-
dependently of the text?s lexical forms and any se-
mantic ambiguity therein. For example, consider
the sentences:
t1. A manager fired the worker.
t2. An employee was terminated from work by
his boss.
A surface-based approach would label the sen-
tences as dissimilar due to the minimal lexical
overlap. However, a sense-based representation
enables detection of the similarity between the
meanings of the words, e.g., fire and terminate.
Indeed, an accurate, sense-based representation is
essential for cases where different words are used
to convey the same meaning.
The contributions of this paper are threefold.
First, we propose a new unified representation of
the meaning of an arbitrarily-sized piece of text,
referred to as a lexical item, using a sense-based
probability distribution. Second, we propose a
novel alignment-based method for word sense dis-
1341
ambiguation during semantic comparison. Third,
we demonstrate that this single representation can
achieve state-of-the-art performance on three sim-
ilarity tasks, each operating at a different lexical
level: (1) surpassing the highest scores on the
SemEval-2012 task on textual similarity (Agirre
et al, 2012) that compares sentences, (2) achiev-
ing a near-perfect performance on the TOEFL syn-
onym selection task proposed by Landauer and
Dumais (1997), which measures word pair sim-
ilarity, and also obtaining state-of-the-art perfor-
mance in terms of the correlation with human
judgments on the RG-65 dataset (Rubenstein and
Goodenough, 1965), and finally (3) surpassing the
performance of Snow et al (2007) in a sense-
coarsening task that measures sense similarity.
2 A Unified Semantic Representation
We propose a representation of any lexical item as
a distribution over a set of word senses, referred
to as the item?s semantic signature. We begin
with a formal description of the representation at
the sense level (Section 2.1). Following this, we
describe our alignment-based disambiguation al-
gorithm which enables us to produce sense-based
semantic signatures for those lexical items (e.g.,
words or sentences) which are not sense annotated
(Section 2.2). Finally, we propose three methods
for comparing these signatures (Section 2.3). As
our sense inventory, we use WordNet 3.0 (Fell-
baum, 1998).
2.1 Semantic Signatures
The WordNet ontology provides a rich net-
work structure of semantic relatedness, connect-
ing senses directly with their hypernyms, and pro-
viding information on semantically similar senses
by virtue of their nearby locality in the network.
Given a particular node (sense) in the network, re-
peated random walks beginning at that node will
produce a frequency distribution over the nodes
in the graph visited during the walk. To ex-
tend beyond a single sense, the random walk may
be initialized and restarted from a set of senses
(seed nodes), rather than just one; this multi-seed
walk produces a multinomial distribution over all
the senses in WordNet with higher probability as-
signed to senses that are frequently visited from
the seeds. Prior work has demonstrated that multi-
nomials generated from random walks over Word-
Net can be successfully applied to linguistic tasks
such as word similarity (Hughes and Ramage,
2007; Agirre et al, 2009), paraphrase recogni-
tion, textual entailment (Ramage et al, 2009),
and pseudoword generation (Pilehvar and Navigli,
2013).
Formally, we define the semantic signature of
a lexical item as the multinomial distribution gen-
erated from the random walks over WordNet 3.0
where the set of seed nodes is the set of senses
present in the item. This representation encom-
passes both when the item is itself a single sense
and when the item is a sense-tagged sentence.
To construct each semantic signature, we use
the iterative method for calculating topic-sensitive
PageRank (Haveliwala, 2002). Let M be the ad-
jacency matrix for the WordNet network, where
edges connect senses according to the rela-
tions defined in WordNet (e.g., hypernymy and
meronymy). We further enrich M by connecting
a sense with all the other senses that appear in its
disambiguated gloss.1 Let ~v(0) denote the prob-
ability distribution for the starting location of the
random walker in the network. Given the set of
senses S in a lexical item, the probability mass
of ~v(0) is uniformly distributed across the senses
si ? S, with the mass for all sj /? S set to zero.
The PageRank may then be computed using:
~v (t) = (1? ?)M~v (t?1) + ? ~v (0) (1)
where at each iteration the random walker may
jump to any node si ? S with probability ?/|S|.
We follow standard convention and set ? to 0.15.
We repeat the operation in Eq. 1 for 30 itera-
tions, which is sufficient for the distribution to
converge. The resulting probability vector ~v(t) is
the semantic signature of the lexical item, as it
has aggregated its senses? similarities over the en-
tire graph. For our semantic signatures we used
the UKB2 off-the-shelf implementation of topic-
sensitive PageRank.
2.2 Alignment-Based Disambiguation
Commonly, semantic comparisons are between
word pairs or sentence pairs that do not have their
lexical content sense-annotated, despite the poten-
tial utility of sense annotation in making seman-
tic comparisons. However, traditional forms of
word sense disambiguation are difficult for short
texts and single words because little or no con-
textual information is present to perform the dis-
ambiguation task. Therefore, we propose a novel
1http://wordnet.princeton.edu
2http://ixa2.si.ehu.es/ukb/
1342
Figure 1: (a) Example alignments of the first sense of term manager (in sentence t1) to the two first
senses of the word types in sentence t2, along with the similarity of the two senses? semantic signatures;
(b) Alignments which maximize the similarities across words in t1 and t2 (the source side of an alignment
is taken as the disambiguated sense of its corresponding word).
alignment-based sense disambiguation that lever-
ages the content of the paired item in order to dis-
ambiguate each element. Leveraging the paired
item enables our approach to disambiguate where
traditional sense disambiguation methods can not
due to insufficient context.
We view sense disambiguation as an alignment
problem. Given two arbitrarily ordered texts, we
seek the semantic alignment that maximizes the
similarity of the senses of the context words in
both texts. To find this maximum we use an align-
ment procedure which, for each word type wi in
item T1, assigns wi to the sense that has the max-
imal similarity to any sense of the word types in
the compared text T2. Algorithm 1 formalizes the
alignment process, which produces a sense dis-
ambiguated representation as a result. Senses are
compared in terms of their semantic signatures,
denoted as function R. We consider multiple def-
initions ofR, defined later in Section 2.3.
As a part of the disambiguation procedure, we
leverage the one sense per discourse heuristic of
Yarowsky (1995); given all the word types in two
compared lexical items, each type is assigned a
single sense, even if it is used multiple times. Ad-
ditionally, if the same word type appears in both
sentences, both will always be mapped to the same
sense. Although such a sense assignment is poten-
tially incorrect, assigning both types to the same
sense results in a representation that does no worse
than a surface-level comparison.
We illustrate the alignment-based disambigua-
tion procedure using the two example sentences t1
and t2 given in Section 1. Figure 1(a) illustrates
example alignments of the first sense of manager
to the first two senses of the word types in sentence
t2 along with the similarity of the two senses?
Algorithm 1 Alignment-based Sense Disambiguation
Input: T1 and T2, the sets of word types being compared
Output: P , the set of disambiguated senses for T1
1: P ? ?
2: for each token ti ? T1
3: max sim? 0
4: best si? null
5: for each token tj ? T2
6: for each si ? Senses(ti), sj ? Senses(tj)
7: sim?R(si, sj)
8: if sim > max sim then
9: max sim = sim
10: best si = si
11: P ? P ? {best si}
12: return P
semantic signatures. For the senses of manager,
sense manager1n obtains the maximal similarity
value to boss1n among all the possible pairings of
the senses for the word types in sentence t2, and as
a result is selected as the sense labeling for man-
ager in sentence t1.3 Figure 1(b) shows the final,
maximally-similar sense alignment of the word
types in t1 and t2. The resulting alignment pro-
duces the following sets of senses:
Pt1 = {manager1n, fire4v, worker1n}
Pt2 = {employee1n, terminate4v, work3n, boss2n}
where Px denotes the corresponding set of senses
of sentence x.
2.3 Semantic Signature Similarity
Cosine Similarity. In order to compare seman-
tic signatures, we adopt the Cosine similarity mea-
sure as a baseline method. The measure is com-
puted by treating each multinomial as a vector and
then calculating the normalized dot product of the
two signatures? vectors.
3We follow Navigli (2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
1343
However, a semantic signature is, in essence,
a weighted ranking of the importance of Word-
Net senses for each lexical item. Given that the
WordNet graph has a non-uniform structure, and
also given that different lexical items may be of
different sizes, the magnitudes of the probabilities
obtained may differ significantly between the two
multinomial distributions. Therefore, for com-
puting the similarity of two signatures, we also
consider two nonparametric methods that use the
ranking of the senses, rather than their probability
values, in the multinomial.
Weighted Overlap. Our first measure provides
a nonparametric similarity by comparing the simi-
larity of the rankings for intersection of the senses
in both semantic signatures. However, we addi-
tionally weight the similarity such that differences
in the highest ranks are penalized more than differ-
ences in lower ranks. We refer to this measure as
the Weighted Overlap. Let S denote the intersec-
tion of all senses with non-zero probability in both
signatures and rji denote the rank of sense si ? S
in signature j, where rank 1 denotes the highest
rank. The sum of the two ranks r1i and r2i for a
sense is then inverted, which (1) weights higher
ranks more and (2) when summed, provides the
maximal value when a sense has the same rank in
both signatures. The unnormalized weighted over-
lap is then calculated as?|S|i=1(r1i + r2i )?1. Then,
to bound the similarity value in [0, 1], we normal-
ize the sum by its maximum value, ?|S|i=1(2i)?1,
which occurs when each sense has the same rank
in both signatures.
Top-k Jaccard. Our second measure uses the
ranking to identify the top-k senses in a signa-
ture, which are treated as the best representatives
of the conceptual associates. We hypothesize that
a specific rank ordering may be attributed to small
differences in the multinomial probabilities, which
can lower rank-based similarities when one of the
compared orderings is perturbed due to slightly
different probability values. Therefore, we con-
sider the top-k senses as an unordered set, with
equal importance in the signature. To compare two
signatures, we compute the Jaccard Index of the
two signatures? sets:
RJac(Uk, Vk) =
|Uk ? Vk|
|Uk ? Vk|
(2)
whereUk denotes the set of k senses with the high-
est probability in the semantic signature U .
Dataset MSRvid MSRpar SMTeuroparl OnWN SMTnews
Training 750 750 734 - -
Test 750 750 459 750 399
Table 1: Statistics of the provided datasets for the
SemEval-2012 Semantic Textual Similarity task.
3 Experiment 1: Textual Similarity
Measuring semantic similarity of textual items has
applications in a wide variety of NLP tasks. As
our benchmark, we selected the recent SemEval-
2012 task on Semantic Textual Similarity (STS),
which was concerned with measuring the seman-
tic similarity of sentence pairs. The task received
considerable interest by facilitating a meaningful
comparison between approaches.
3.1 Experimental Setup
Data. We follow the experimental setup used in
the STS task (Agirre et al, 2012), which provided
five test sets, two of which had accompanying
training data sets for tuning system performance.
Each sentence pair in the datasets was given a
score from 0 to 5 (low to high similarity) by hu-
man judges, with a high inter-annotator agreement
of around 0.90 when measured using the Pearson
correlation coefficient. Table 1 lists the number of
sentence pairs in training and test portions of each
dataset.
Comparison Systems. The top-ranking partic-
ipating systems in the SemEval-2012 task were
generally supervised systems utilizing a variety of
lexical resources and similarity measurement tech-
niques. We compare our results against the top
three systems of the 88 submissions: TLsim and
TLsyn, the two systems of S?aric? et al (2012), and
the UKP2 system (Ba?r et al, 2012). UKP2 utilizes
extensive resources among which are a Distribu-
tional Thesaurus computed on 10M dependency-
parsed English sentences. In addition, the sys-
tem utilizes techniques such as Explicit Semantic
Analysis (Gabrilovich and Markovitch, 2007) and
makes use of resources such as Wiktionary and
Wikipedia, a lexical substitution system based on
supervised word sense disambiguation (Biemann,
2013), and a statistical machine translation sys-
tem. The TLsim system uses the New York Times
Annotated Corpus, Wikipedia, and Google Book
Ngrams. The TLsyn system also uses Google
Book Ngrams, as well as dependency parsing and
named entity recognition.
1344
Ranking System Overall Dataset-specificALL ALLnrm Mean ALL ALLnrm Mean Mpar Mvid SMTe OnWN SMTn
1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604
2 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.493
3 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.468
4 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398
Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in
the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear-
son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),
OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua-
tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
System Configuration. Here we describe the
configuration of our approach, which we have
called Align, Disambiguate and Walk (ADW). The
STS task uses human similarity judgments on an
ordinal scale from 0 to 5. Therefore, in ADW we
adopted a similar approach to generating similar-
ity values to that adopted by other participating
systems, whereby a supervised system is trained
to combine multiple similarity judgments to pro-
duce a final rating consistent with the human an-
notators. We utilized the WEKA toolkit (Hall et
al., 2009) to train a Gaussian Processes regression
model for each of the three training sets (cf. Table
1). The features discussed hereafter were consid-
ered in our regression model.
Main features. We used the scores calculated
using all three of our semantic signature compar-
ison methods as individual features. Although the
Jaccard comparison is parameterized, we avoided
tuning and instead used four features for distinct
values of k: 250, 500, 1000, and 2500.
String-based features. Additionally, because
the texts often contain named entities which are
not present in WordNet, we incorporated the sim-
ilarity values produced by four string-based mea-
sures, which were used by other teams in the STS
task: (1) longest common substring which takes
into account the length of the longest overlap-
ping contiguous sequence of characters (substring)
across two strings (Gusfield, 1997), (2) longest
common subsequence which, instead, finds the
longest overlapping subsequence of two strings
(Allison and Dix, 1986), (3) Greedy String Tiling
which allows reordering in strings (Wise, 1993),
and (4) the character/word n-gram similarity pro-
posed by Barro?n-Ceden?o et al (2010).
We followed S?aric? et al (2012) and used the
models trained on the SMTeuroparl and MSRpar
datasets for testing on the SMTnews and OnWN
test sets, respectively.
3.2 STS Results
Three evaluation metrics are provided by the or-
ganizers of the SemEval-2012 STS task, all of
which are based on Pearson correlation r of human
judgments with system outputs: (1) the correla-
tion value for the concatenation of all five datasets
(ALL), (2) a correlation value obtained on a con-
catenation of the outputs, separately normalized
by least square (ALLnrm), and (3) the weighted
average of Pearson correlations across datasets
(Mean). Table 2 shows the scores obtained by
ADW for the three evaluation metrics, as well as
the Pearson correlation values obtained on each
of the five test sets (rightmost columns). We also
show the results obtained by the three top-ranking
participating systems (i.e., UKP2, TLsim, and TL-
syn). The leftmost three columns show the system
rankings according to the three metrics.
As can be seen from Table 2, our system (ADW)
outperforms all the 88 participating systems ac-
cording to all the evaluation metrics. Our sys-
tem shows a statistically significant improvement
on the SMTnews dataset, with an increase in the
Pearson correlation of over 0.10. MSRpar (MPar)
is the only dataset in which TLsim (S?aric? et al,
2012) achieves a higher correlation with human
judgments. Named entity features used by the TL-
sim system could be the reason for its better per-
formance on the MSRpar dataset, which contains
a large number of named entities.
3.3 Similarity Measure Analysis
To gain more insight into the impact of our
alignment-based disambiguation approach, we
carried out a 10-fold cross-validation on the three
training datasets (cf. Table 1) using the systems
described hereafter.
ADW-MF. To build this system, we utilized our
main features only; i.e., we did not make use of
additional string-based features.
1345
DW. Similarly to ADW-MF, this system utilized
the main features only. In DW, however, we re-
placed our alignment-based disambiguation phase
with a random walk-based WSD system that dis-
ambiguated the sentences separately, without per-
forming any alignment. As our WSD system,
we used UKB, a state-of-the-art knowledge-based
WSD system that is based on the same topic-
sensitive PageRank algorithm used by our ap-
proach. UKB initializes the algorithm from all
senses of the words in the context of a word to
be disambiguated. It then picks the most relevant
sense of the word according to the resulting prob-
ability vector. As the lexical knowledge base of
UKB, we used the same semantic network as that
utilized by our approach for calculating semantic
signatures.
Table 3 lists the performance values of the two
above-mentioned systems on the three training
sets in terms of Pearson correlation. In addition,
we present in the table correlation scores for four
other similarity measures reported by Ba?r et al
(2012):
? Pairwise Word Similarity that comprises of
a set of WordNet-based similarity measures
proposed by Resnik (1995), Jiang and Con-
rath (1997), and Lin (1998b). The aggre-
gation strategy proposed by Corley and Mi-
halcea (2005) has been utilized for extend-
ing these word-to-word similarity measures
for calculating text-to-text similarities.
? Explicit Semantic Analysis (Gabrilovich
and Markovitch, 2007) where the high-
dimensional vectors are obtained on Word-
Net, Wikipedia and Wiktionary.
? Distributional Thesaurus where a similarity
score is computed similarly to that of Lin
(1998a) using a distributional thesaurus ob-
tained from a 10M dependency-parsed sen-
tences of English newswire.
? Character n-grams which were also used as
one of our additional features.
As can be seen from Table 3, our alignment-
based disambiguation approach (ADW-MF) is
better suited to the task than a conventional WSD
approach (DW). Another interesting point is the
high scores achieved by the Character n-grams
Similarity measure DatasetMpar Mvid SMTe
DW 0.448 0.820 0.660
ADW-MF 0.485 0.842 0.721
Explicit Semantic Analysis 0.427 0.781 0.619
Pairwise Word Similarity 0.564 0.835 0.527
Distributional Thesaurus 0.494 0.481 0.365
Character n-grams 0.658 0.771 0.554
Table 3: Performance of our main-feature sys-
tem with conventional WSD (DW) and with the
alignment-based disambiguation approach (ADW-
MF) vs. four other similarity measures, using 10-
fold cross validation on the training datasets MSR-
par (Mpar), MSRvid (Mvid), and SMTeuroparl
(SMTe).
measure. This confirms that string-based meth-
ods are strong baselines for semantic textual sim-
ilarity. Except for the MSRpar (Mpar) dataset,
our system (ADW-MF) outperforms all other sim-
ilarity measures. The scores obtained by Explicit
Semantic Analysis and Distributional Thesaurus
are not competitive on any dataset. On the other
hand, Pairwise Word Similarity achieves a high
performance on MSRpar and MSRvid datasets,
but performs surprisingly low on the SMTeuroparl
dataset.
4 Experiment 2: Word Similarity
We now proceed from the sentence level to the
word level. Word similarity has been a key prob-
lem for lexical semantics, with significant efforts
being made by approaches in distributional se-
mantics to accurately identify synonymous words
(Turney and Pantel, 2010). Different evaluation
methods exist in the literature for evaluating the
performance of a word-level semantic similarity
measure; we adopted two well-established bench-
marks: synonym recognition and correlating word
similarity judgments with those from human an-
notators.
For synonym recognition, we used the TOEFL
dataset created by Landauer and Dumais (1997).
The dataset consists of 80 multiple-choice syn-
onym questions from the TOEFL test; a word is
paired with four options, one of which is a valid
synonym. Test takers with English as a second
language averaged 64.5% correct. Despite multi-
ple approaches, only recently has the test been an-
swered perfectly (Bullinaria and Levy, 2012), un-
derscoring the challenge of synonym recognition.
1346
Approach Accuracy
PPMIC (Bullinaria and Levy, 2007) 85.00%
GLSA (Matveeva et al, 2005) 86.25%
LSA (Rapp, 2003) 92.50%
ADWJac 93.75?2.5%
ADWWO 95.00%
ADWCos 96.25%
PR (Turney et al, 2003) 97.50%
PCCP (Bullinaria and Levy, 2012) 100.00%
Table 4: Accuracy on the 80-question TOEFL
Synonym test. ADWJac, ADWWO, and ADWCos
correspond to results with the Jaccard, Weighted
Overlap and Cosine signature comparison mea-
sures, respectively.
For the similarity judgment evaluation, we
used as benchmark the RG-65 dataset created by
Rubenstein and Goodenough (1965). The dataset
contains 65 word pairs judged by 51 human sub-
jects on a scale of 0 to 4 according to their seman-
tic similarity. Ideally, a measure?s similarity judg-
ments are expected to be highly correlated with
those of humans. To be consistent with the previ-
ous literature (Hughes and Ramage, 2007; Agirre
et al, 2009), we used Spearman?s rank correlation
in our experiment.
4.1 Experimental Setup
Our alignment-based sense disambiguation trans-
forms the task of comparing individual words
into that of calculating the similarity of the best-
matching sense pair across the two words. As
there is no training data we do not optimize the k
value for computing signature similarity with the
Jaccard index; instead, we report, for the synonym
recognition and the similarity judgment evalua-
tions, the respective range of accuracies and the
average correlation obtained upon using five val-
ues of k randomly selected in the range [50, 2500]:
678, 1412, 1692, 2358, 2387.
4.2 Word Similarity Results: TOEFL dataset
Table 4 lists the accuracy performance of the sys-
tem in comparison to the existing state of the
art on the TOEFL test. ADWWO, ADWCos,
and ADWJac correspond to our approach when
Weighted Overlap, Cosine, and Jaccard signa-
ture comparison measures are used, respectively.
Despite not being tuned for the task, our model
achieves near-perfect performance, answering all
but three questions correctly with the Cosine mea-
sure. Among the top-performing approaches, only
Word Synonym choices (correct in bold)
fanciful familiar apparent? imaginative? logical
verbal oral? overt fitting verbose?
resolved settled? forgotten? publicized examined
percentage volume sample proportion profit??
figure list solve? divide? express
highlight alter? imitate accentuate? restore
Table 5: Questions answered incorrectly by our
approach. Symbols ? and ? correspond to the
choices of our approach with the Weighted Over-
lap and Cosine signature comparisons respec-
tively. We do not include the mistakes made when
the Jaccard measure was used as they vary with
the k value.
that of Rapp (2003) uses word senses, an approach
that is outperformed by our method.
The errors produced by our system were largely
the result of sense locality in the WordNet net-
work. Table 5 highlights the incorrect responses.
The synonym mistakes reveal cases where senses
of the two words are close in WordNet, indicating
some relatedness. For example, percentage may
be interpreted colloquially as monetary value (e.g.,
?give me my percentage?) and elicits the synonym
of profit in the economic domain, which ADW in-
correctly selects as a synonym.
4.3 Word Similarity Results: RG-65 dataset
Table 6 shows the Spearman?s ? rank correlation
coefficients with human judgments on the RG-65
dataset. As can be seen from the Table, our ap-
proach with the Weighted Overlap signature com-
parison improves over the similar approach of
Hughes and Ramage (2007) which, however, does
not involve the disambiguation step and considers
a word as a whole unit as represented by the set of
its senses.
5 Experiment 3: Sense Similarity
WordNet is known to be a fine-grained sense in-
ventory with many related word senses (Palmer et
al., 2007). Accordingly, multiple approaches have
attempted to identify highly similar senses in or-
der to produce a coarse-grained sense inventory.
We adopt this task as a way of evaluating our sim-
ilarity measure at the sense level.
5.1 Coarse-graining Background
Earlier work on reducing the polysemy of sense
inventories has considered WordNet-based sense
relatedness measures (Mihalcea and Moldovan,
2001) and corpus-based vector representations of
1347
Approach Correlation
ADWCos 0.825
Agirre et al (2009) 0.830
Hughes and Ramage (2007) 0.838
Zesch et al (2008) 0.840
ADWJac 0.841
ADWWO 0.868
Table 6: Spearman?s ? correlation coefficients
with human judgments on the RG-65 dataset.
ADWJac, ADWWO, and ADWCos correspond to
results with the Jaccard, Weighted Overlap and
Cosine signature comparison measures respec-
tively.
word senses (Agirre and Lopez, 2003; McCarthy,
2006). Navigli (2006) proposed an automatic ap-
proach for mapping WordNet senses to the coarse-
grained sense distinctions of the Oxford Dictio-
nary of English (ODE). The approach leverages
semantic similarities in gloss definitions and the
hierarchical relations between senses in the ODE
to cluster WordNet senses. As current state of
the art, Snow et al (2007) developed a super-
vised SVM classifier that utilized, as its features,
several earlier sense relatedness techniques such
as those implemented in the WordNet::Similarity
package (Pedersen et al, 2004). The classifier
also made use of resources such as topic signatures
data (Agirre and de Lacalle, 2004), the WordNet
domain dataset (Magnini and Cavaglia`, 2000), and
the mappings of WordNet senses to ODE senses
produced by Navigli (2006).
5.2 Experimental Setup
We benchmark the accuracy of our similarity mea-
sure in grouping word senses against those of Nav-
igli (2006) and Snow et al (2007) on two datasets
of manually-labeled sense groupings of WordNet
senses: (1) sense groupings provided as a part of
the Senseval-2 English Lexical Sample WSD task
(Kilgarriff, 2001) which includes nouns, verbs and
adjectives; (2) sense groupings included in the
OntoNotes project4 (Hovy et al, 2006) for nouns
and verbs. Following the evaluation methodology
of Snow et al (2007), we combine the Senseval-2
and OntoNotes datasets into a third dataset.
Snow et al (2007) considered sense grouping as
a binary classification task whereby for each word
every possible pairing of senses has to be classified
4Sense groupings belong to a pre-version 1.0: http://
cemantix.org/download/sense/ontonotes-sense-groups.tar.gz
Onto SE-2 Onto + SE-2
Method Noun Verb Noun Verb Adj Noun Verb
RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485
RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503
RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493SVM 0.370 0.455 NA NA 0.473 0.423 0.432
ODE 0.218 0.396 NA NA 0.371 0.331 0.288
Table 7: F-score sense merging evaluation on
three hand-labeled datasets: OntoNotes (Onto),
Senseval-2 (SE-2), and combined (Onto+SE-2).
Results are reported for all three of our signature
comparison measures and also for two previous
works (last two rows).
as either merged or not-merged. We constructed
a simple threshold-based classifier to perform the
same binary classification. To this end, we cal-
culated the semantic similarity of each sense pair
and then used a threshold value t to classify the
pair as merged if similarity ? t and not-merged
otherwise. We sampled out 10% of the dataset for
tuning the value of t, thus adapting our classifier
to the fine granularity of the dataset. We used the
same held-out instances to perform a tuning of the
k value used for Jaccard index, over the same val-
ues of k as in Experiment 1 (cf. Section 3).
5.3 Sense Merging Results
For a binary classification task, we can directly
calculate precision, recall and F-score by con-
structing a contingency table. We show in Ta-
ble 7 the F-score performance of our classifier as
obtained by an averaged 10-fold cross-validation.
Results are presented for all three of the mea-
sures of semantic signature comparison and for
the three datasets: OntoNotes, Senseval-2, and
the two combined. In addition, we show in Ta-
ble 7 the F-score results provided by Snow et al
(2007) for their SVM-based system and for the
mapping-based approach of Navigli (2006), de-
noted by ODE.
Table 7 shows that our methodology yields im-
provements over previous work on both datasets
and for all parts of speech, irrespective of
the semantic signature comparison method used.
Among the three methods, Weighted Overlap
achieves the best performance, which demon-
strates that our transformation of semantic signa-
tures into ordered lists of concepts and calculating
similarity by rank comparison has been helpful.
1348
6 Related Work
Due to the wide applicability of semantic similar-
ity, significant efforts have been made at different
lexical levels. Early work on document-level sim-
ilarity was driven by information retrieval. Vector
space methods provided initial successes (Salton
et al, 1975), but often suffer from data spar-
sity when using small documents, or when doc-
uments use different word types, as in the case
of paraphrases. Later efforts such as LSI (Deer-
wester et al, 1990), PLSA (Hofmann, 2001) and
Topic Models (Blei et al, 2003; Steyvers and Grif-
fiths, 2007) overcame these sparsity issues using
dimensionality reduction techniques or modeling
the document using latent variables. However,
such methods were still most suitable for compar-
ing longer texts. Complementary approaches have
been developed specifically for comparing shorter
texts, such as those used in the SemEval-2012
STS task (Agirre et al, 2012). Most similar to
our approach are the methods of Islam and Inkpen
(2008) and Corley and Mihalcea (2005), who per-
formed a word-to-word similarity alignment; how-
ever, they did not operate at the sense level. Ram-
age et al (2009) used a similar semantic represen-
tation of short texts from random walks on Word-
Net, which was applied to paraphrase recognition
and textual entailment. However, unlike our ap-
proach, their method does not perform sense dis-
ambiguation prior to building the representation
and therefore potentially suffers from ambiguity.
A significant amount of effort has also been put
into measuring similarity at the word level, fre-
quently by approaches that use distributional se-
mantics (Turney and Pantel, 2010). These meth-
ods use contextual features to represent semantics
at the word level, whereas our approach represents
word semantics at the sense level. Most similar to
our approach are those of Agirre et al (2009) and
Hughes and Ramage (2007), which represent word
meaning as the multinomials produced from ran-
dom walks on the WordNet graph. However, un-
like our approach, neither of these disambiguates
the two words being compared, which potentially
conflates the meanings and lowers the similarity
judgment.
Measures of sense relatedness have frequently
leveraged the structural properties of WordNet
(e.g., path lengths) to compare senses. Budanit-
sky and Hirst (2006) provided a survey of such
WordNet-based measures. The main drawback
with these approaches lies in the WordNet struc-
ture itself, where frequently two semantically sim-
ilar senses are distant in the WordNet hierar-
chy. Possible solutions include relying on wider-
coverage networks such as WikiNet (Nastase and
Strube, 2013) or multilingual ones such as Babel-
Net (Navigli and Ponzetto, 2012b). Fewer works
have focused on measuring the similarity ? as op-
posed to relatedness ? between senses. The topic
signatures method of Agirre and Lopez (2003)
represents each sense as a vector over corpus-
derived features in order to build comparable sense
representations. However, topic signatures often
produce lower quality representations due to spar-
sity in the local structure of WordNet, especially
for rare senses. In contrast, the random walk
used in our approach provides a denser, and thus
more comparable, representation for all WordNet
senses.
7 Conclusions
This paper presents a unified approach for comput-
ing semantic similarity at multiple lexical levels,
from word senses to texts. Our method leverages
a common probabilistic representation at the sense
level for all types of linguistic data. We demon-
strate that our semantic representation achieves
state-of-the-art performance in three experiments
using semantic similarity at different lexical levels
(i.e., sense, word, and text), surpassing the per-
formance of previous similarity measures that are
often specifically targeted for each level.
In future work, we plan to explore the impact of
the sense inventory-based network used in our se-
mantic signatures. Specifically, we plan to investi-
gate higher coverage inventories such as BabelNet
(Navigli and Ponzetto, 2012a), which will handle
texts with named entities and rare senses that are
not in WordNet, and will also enable cross-lingual
semantic similarity. Second, we plan to evaluate
our method on larger units of text and formalize
comparison methods between different lexical lev-
els.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Sameer S. Pradhan
for providing us with an earlier version of the
OntoNotes dataset.
1349
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly
available topic signatures for all WordNet nominal senses.
In Proceedings of LREC, pages 1123?1126, Lisbon, Por-
tugal.
Eneko Agirre and Oier Lopez. 2003. Clustering WordNet
word senses. In Proceedings of RANLP, pages 121?130,
Borovets, Bulgaria.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, Marius Pas?ca, and Aitor Soroa. 2009. A study
on similarity and relatedness using distributional and
WordNet-based approaches. In Proceedings of NAACL,
pages 19?27, Boulder, Colorado.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of SemEval-2012, pages
385?393, Montreal, Canada.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Processing
Letters, 23(6):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual similar-
ity by combining multiple content similarity measures. In
Proceedings of SemEval-2012, pages 435?440, Montreal,
Canada.
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism detection across distant
language pairs. In Proceedings of COLING, pages 37?45,
Beijing, China.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012.
Learning entailment relations by global graph structure
optimization. Computational Linguistics, 38(1):73?111.
Chris Biemann. 2013. Creating a system for lexical sub-
stitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97?122.
Or Biran, Samuel Brody, and Noe?mie Elhadad. 2011.
Putting it simply: a context-aware approach to lexical sim-
plification. In Proceedings of ACL, pages 496?501, Port-
land, Oregon.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet Allocation. The Journal of Machine
Learning Research, 3:993?1022.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluat-
ing WordNet-based measures of Lexical Semantic Relat-
edness. Computational Linguistics, 32(1):13?47.
John A. Bullinaria and Joseph. P. Levy. 2007. Extracting
semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods,
(3):510.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
semantic representations from word co-occurrence statis-
tics: stop-lists, stemming, and SVD. Behavior Research
Methods, 44:890?907.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 13?18, Ann Arbor, Michigan.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of American
Society for Information Science, 41(6):391?407.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-based
Word Sense Induction. Computational Linguistics, 39(3).
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of ACL-HLT, pages 265?
268, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-supervised
Semantic Role Labeling via structural alignment. Compu-
tational Linguistics, 38(1):135?171.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of IJCAI, pages 1606?
1611, Hyderabad, India.
Oren Glickman and Ido Dagan. 2003. Acquiring lexi-
cal paraphrases from a single corpus. In Proceedings of
RANLP, pages 81?90, Borovets, Bulgaria.
Dan Gusfield. 1997. Algorithms on strings, trees, and se-
quences: computer science and computational biology.
Cambridge University Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda
Koren. 2012. Large-scale learning of word relatedness
with constraints. In Proceedings of KDD, pages 1406?
1414, Beijing, China.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10?18.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of WWW, pages 517?526, Hawaii, USA.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Thomas Hofmann. 2001. Unsupervised Learning by Prob-
abilistic Latent Semantic Analysis. Machine Learning,
42(1):177?196.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of NAACL, pages 57?60,
NY, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical semantic
relatedness with random graph walks. In Proceedings of
EMNLP-CoNLL, pages 581?589, Prague, Czech Repub-
lic.
Aminul Islam and Diana Inkpen. 2008. Semantic text sim-
ilarity using corpus-based word similarity and string sim-
ilarity. ACM Transactions on Knowledge Discovery from
Data, 2(2):10:1?10:25.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proceedings of ROCLING X, pages 19?30, Taiwan.
1350
Adam Kilgarriff. 2001. English lexical sample task descrip-
tion. In Proceedings of Senseval, pages 17?20, Toulouse,
France.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of COLING, pages 768?
774, Montreal, Quebec, Canada.
Dekang Lin. 1998b. An information-theoretic definition of
similarity. In Proceedings of ICML, pages 296?304, San
Francisco, CA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Integrat-
ing subject field codes into WordNet. In Proceedings of
LREC, pages 1413?1418, Athens, Greece.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and
Alessandro Vespignani. 2005. Algorithmic detection of
semantic similarity. In Proceedings of WWW, pages 107?
116, Chiba, Japan.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christiaan Royer. 2005. Terms representation with gener-
alized latent semantic analysis. In Proceedings of RANLP,
Borovets, Bulgaria.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Diana McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the Workshop on
Making Sense of Sense at EACL-06, pages 17?24, Trento,
Italy.
Rada Mihalcea and Dan Moldovan. 2001. Automatic gen-
eration of a coarse grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, USA.
Vivi Nastase and Michael Strube. 2013. Transforming
Wikipedia into a large scale multilingual concept network.
Artificial Intelligence, 194:62?85.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network.
Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b. Babel-
Relate! a joint multilingual approach to computing seman-
tic relatedness. In Proceedings of AAAI, pages 108?114,
Toronto, Canada.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of COLING-ACL, pages 105?112, Sydney,
Australia.
Roberto Navigli. 2009. Word Sense Disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):1?69.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2007.
Making fine-grained and coarse-grained sense distinc-
tions, both manually and automatically. Natural Lan-
guage Engineering, 13(2):137?163.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of AAAI, pages 144?152, San
Jose, CA.
Mohammad Taher Pilehvar and Roberto Navigli. 2013.
Paving the way to a large-scale pseudosense-annotated
dataset. In Proceedings of NAACL-HLT, pages 1100?
1109, Atlanta, USA.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and
Shaul Markovitch. 2011. A word at a time: comput-
ing word relatedness using temporal semantic analysis. In
Proceedings of WWW, pages 337?346, Hyderabad, India.
Daniel Ramage, Anna N. Rafferty, and Christopher D. Man-
ning. 2009. Random walks for text semantic similarity. In
Proceedings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 23?31, Sun-
tec, Singapore.
Reinhard Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. In Proceedings of the Ninth Ma-
chine Translation Summit, pages 315?322, New Orleans,
LA.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings of
IJCAI, pages 448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications of
the ACM, 18(11):613?620.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y.
Ng. 2007. Learning to merge word senses. In EMNLP-
CoNLL, pages 1005?1014, Prague, Czech Republic.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-factoid
questions from Web collections. Computational Linguis-
tics, 37(2):351?383.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent modules
to solve multiple-choice synonym and analogy problems.
In Proceedings of RANLP, pages 482?489, Borovets, Bul-
garia.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder, and
Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems for
measuring semantic text similarity. In Proceedings of
SemEval-2012, pages 441?448, Montreal, Canada.
Michael J. Wise. 1993. String similarity via greedy string
tiling and running Karp-Rabin matching. In Department
of Computer Science Technical Report, Sydney.
David Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation rivaling supervised methods. In Proceedings of
ACL, pages 189?196, Cambridge, Massachusetts.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych. 2008.
Using Wiktionary for computing semantic relatedness. In
Proceedings of AAAI, pages 861?866, Chicago, Illinois.
1351
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 468?478,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Robust Approach to Aligning Heterogeneous Lexical Resources
Mohammad Taher Pilehvar and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,navigli}@di.uniroma1.it
Abstract
Lexical resource alignment has been an
active field of research over the last
decade. However, prior methods for align-
ing lexical resources have been either spe-
cific to a particular pair of resources, or
heavily dependent on the availability of
hand-crafted alignment data for the pair of
resources to be aligned. Here we present a
unified approach that can be applied to an
arbitrary pair of lexical resources, includ-
ing machine-readable dictionaries with no
network structure. Our approach leverages
a similarity measure that enables the struc-
tural comparison of senses across lexical
resources, achieving state-of-the-art per-
formance on the task of aligning WordNet
to three different collaborative resources:
Wikipedia, Wiktionary and OmegaWiki.
1 Introduction
Lexical resources are repositories of machine-
readable knowledge that can be used in virtually
any Natural Language Processing task. Notable
examples are WordNet, Wikipedia and, more re-
cently, collaboratively-curated resources such as
OmegaWiki and Wiktionary (Hovy et al, 2013).
On the one hand, these resources are heteroge-
neous in design, structure and content, but, on
the other hand, they often provide complemen-
tary knowledge which we would like to see inte-
grated. Given the large scale this intrinsic issue
can only be addressed automatically, by means of
lexical resource alignment algorithms. Owing to
its ability to bring together features like multilin-
guality and increasing coverage, over the past few
years resource alignment has proven beneficial to
a wide spectrum of tasks, such as Semantic Pars-
ing (Shi and Mihalcea, 2005), Semantic Role La-
beling (Palmer et al, 2010), and Word Sense Dis-
ambiguation (Navigli and Ponzetto, 2012).
Nevertheless, when it comes to aligning textual
definitions in different resources, the lexical ap-
proach (Ruiz-Casado et al, 2005; de Melo and
Weikum, 2010; Henrich et al, 2011) falls short
because of the potential use of totally different
wordings to define the same concept. Deeper ap-
proaches leverage semantic similarity to go be-
yond the surface realization of definitions (Nav-
igli, 2006; Meyer and Gurevych, 2011; Niemann
and Gurevych, 2011). While providing good re-
sults in general, these approaches fail when the
definitions of a given word are not of adequate
quality and expressiveness to be distinguishable
from one another. When a lexical resource can be
viewed as a semantic graph, as with WordNet or
Wikipedia, this limit can be overcome by means
of alignment algorithms that exploit the network
structure to determine the similarity of concept
pairs. However, not all lexical resources provide
explicit semantic relations between concepts and,
hence, machine-readable dictionaries like Wik-
tionary have first to be transformed into semantic
graphs before such graph-based approaches can be
applied to them. To do this, recent work has pro-
posed graph construction by monosemous linking,
where a concept is linked to all the concepts asso-
ciated with the monosemous words in its definition
(Matuschek and Gurevych, 2013). However, this
alignment method still involves tuning of parame-
ters which are highly dependent on the character-
istics of the generated graphs and, hence, requires
hand-crafted sense alignments for the specific pair
of resources to be aligned, a task which has to be
replicated every time the resources are updated.
In this paper we propose a unified approach
to aligning arbitrary pairs of lexical resources
which is independent of their specific structure.
Thanks to a novel modeling of the sense entries
and an effective ontologization algorithm, our ap-
proach also fares well when resources lack rela-
tional structure or pair-specific training data is ab-
sent, meaning that it is applicable to arbitrary pairs
468
without adaptation. We report state-of-the-art per-
formance when aligning WordNet to Wikipedia,
OmegaWiki and Wiktionary.
2 Resource Alignment
Preliminaries. Our approach for aligning lexi-
cal resources exploits the graph structure of each
resource. Therefore, we assume that a lexical
resource L can be represented as an undirected
graph G = (V,E) where V is the set of nodes,
i.e., the concepts defined in the resource, and
E is the set of undirected edges, i.e., seman-
tic relations between concepts. Each concept
c ? V is associated with a set of lexicalizations
L
G
(c) = {w
1
, w
2
, ..., w
n
}. For instance, Word-
Net can be readily represented as an undirected
graph G whose nodes are synsets and edges are
modeled after the relations between synsets de-
fined in WordNet (e.g., hypernymy, meronymy,
etc.), and L
G
is the mapping between each synset
node and the set of synonyms which express the
concept. However, other resources such as Wik-
tionary do not provide semantic relations between
concepts and, therefore, have first to be trans-
formed into semantic networks before they can be
aligned using our alignment algorithm. We ex-
plain in Section 3 how a semi-structured resource
which does not exhibit a graph structure can be
transformed into a semantic network.
Alignment algorithm. Given a pair of lexical
resources L
1
and L
2
, we align each concept in L
1
by mapping it to its corresponding concept(s) in
the target lexicon L
2
. Algorithm 1 formalizes the
alignment process: the algorithm takes as input the
semantic graphs G
1
and G
2
corresponding to the
two resources, as explained above, and produces
as output an alignment in the form of a set A of
concept pairs. The algorithm iterates over all con-
cepts c
1
? V
1
and, for each of them, obtains the set
of concepts C ? V
2
, which can be considered as
alignment candidates for c
1
(line 3). For a concept
c
1
, alignment candidates in G
2
usually consist of
every concept c
2
? V
2
that shares at least one lex-
icalization with c
1
in the same part of speech tag,
i.e., L
G
1
(c
1
) ? L
G
2
(c
2
) 6= ? (Reiter et al, 2008;
Meyer and Gurevych, 2011). Once the set of target
candidates C for a source concept c
1
is obtained,
the alignment task can be cast as that of identifying
those concepts in C to which c
1
should be aligned.
To do this, the algorithm calculates the similarity
between c
1
and each c
2
? C (line 5). If their sim-
ilarity score exceeds a certain value denoted by ?
Algorithm 1 Lexical Resource Aligner
Input: graphs H = (V
H
, E
H
), G
1
= (V
1
, E
1
) and G
2
=
(V
2
, E
2
), the similarity threshold ?, and the combination
parameter ?
Output: A, the set of all aligned concept pairs
1: A? ?
2: for each concept c
1
? V
1
3: C ? getCandidates(c
1
, V
2
)
4: for each concept c
2
? C
5: sim? calculateSimilarity(H,G
1
, G
2
, c
1
, c
2
, ?)
6: if sim > ? then
7: A? A ? {(c
1
, c
2
)}
8: return A
(line 6), the two concepts c
1
and c
2
are aligned and
the pair (c
1
, c
2
) is added to A (line 7).
Different resource alignment techniques usually
vary in the way they compute the similarity of a
pair of concepts across two resources (line 5 in Al-
gorithm 1). In the following, we present our novel
approach for measuring the similarity of concept
pairs.
2.1 Measuring the Similarity of Concepts
Figure 1 illustrates the procedure underlying our
cross-resource concept similarity measurement
technique. As can be seen, the approach consists
of two main components: definitional similarity
and structural similarity. Each of these compo-
nents gets, as its input, a pair of concepts belong-
ing to two different semantic networks and pro-
duces a similarity score. These two scores are then
combined into an overall score (part (e) of Figure
1) which quantifies the semantic similarity of the
two input concepts c
1
and c
2
.
The definitional similarity component computes
the similarity of two concepts in terms of the simi-
larity of their definitions, a method that has also
been used in previous work for aligning lexical
resources (Niemann and Gurevych, 2011; Hen-
rich et al, 2012). In spite of its simplicity, the
mere calculation of the similarity of concept defi-
nitions provides a strong baseline, especially for
cases where the definitional texts for a pair of
concepts to be aligned are lexically similar, yet
distinguishable from the other definitions. How-
ever, as mentioned in the introduction, definition
similarity-based techniques fail at identifying the
correct alignments in cases where different word-
ings are used or definitions are not of high qual-
ity. The structural similarity component, instead,
is a novel graph-based similarity measurement
technique which calculates the similarity between
a pair of concepts across the semantic networks
of the two resources by leveraging the semantic
469
Figure 1: The process of measuring the similarity of a pair of concepts across two resources. The method
consists of two components: definitional and structural similarities, each measuring a similarity score for
the given concept pair. The two scores are combined by means of parameter ? in the last stage.
structure of those networks. This component goes
beyond the surface realization of concepts, thus
providing a deeper measure of concept similarity.
The two components share the same backbone
(parts (b) and (d) of Figure 1), but differ in some
stages (parts (a) and (c) in Figure 1). In the follow-
ing, we explain all the stages involved in the two
components (gray blocks in the figure).
2.1.1 Semantic signature generation
The aim of this stage is to model a given concept
or set of concepts through a vectorial semantic
representation, which we refer to as the seman-
tic signature of the input. We utilized Person-
alized PageRank (Haveliwala, 2002, PPR), a ran-
dom walk graph algorithm, for calculating seman-
tic signatures. The original PageRank (PR) algo-
rithm (Brin and Page, 1998) computes, for a given
graph, a single vector wherein each node is as-
sociated with a weight denoting its structural im-
portance in that graph. PPR is a variation of PR
where the computation is biased towards a set of
initial nodes in order to capture the notion of im-
portance with respect to those particular nodes.
PPR has been previously used in a wide variety of
tasks such as definition similarity-based resource
alignment (Niemann and Gurevych, 2011), textual
semantic similarity (Hughes and Ramage, 2007;
Pilehvar et al, 2013), Word Sense Disambigua-
tion (Agirre and Soroa, 2009; Faralli and Navigli,
2012) and semantic text categorization (Navigli et
al., 2011). When applied to a semantic graph by
initializing the random walks from a set of con-
cepts (nodes), PPR yields a vector in which each
concept is associated with a weight denoting its
semantic relevance to the initial concepts.
Formally, we first represent a semantic network
consisting of N concepts as a row-stochastic tran-
sition matrix M ? R
N?N
. The cell (i, j) in the
matrix denotes the probability of moving from a
concept i to j in the graph: 0 if no edge exists
from i to j and 1/degree(i) otherwise. Then the
PPR vector, hence the semantic signature S
v
of
vector v is the unique solution to the linear sys-
tem: S
v
= (1 ? ?)v + ?MS
v
, where v is the
personalization vector of size N in which all the
probability mass is put on the concepts for which
a semantic signature is to be computed and ? is the
damping factor, which is usually set to 0.85 (Brin
and Page, 1998). We used the UKB
1
off-the-shelf
implementation of PPR.
Definitional similarity signature. In the defini-
tional similarity component, the two concepts c
1
and c
2
are first represented by their corresponding
definitions d
1
and d
2
in the respective resourcesL
1
and L
2
(Figure 1(a), top). To improve expressive-
ness, we follow Niemann and Gurevych (2011)
and further extend d
i
with all the word forms asso-
ciated with concept c
i
and its neighbours, i.e., the
union of all lexicalizations L
G
i
(x) for all concepts
x ? {c
?
? V
i
: (c, c
?
) ? E
i
}?{c}, where E
i
is the
set of edges in G
i
. In this component the person-
alization vector v
i
is set by uniformly distributing
the probability mass over the nodes correspond-
ing to the senses of all the content words in the
extended definition of d
i
according to the sense
inventory of a semantic network H . We use the
same semantic graph H for computing the seman-
tic signatures of both definitions. Any semantic
network with a dense relational structure, provid-
ing good coverage of the words appearing in the
definitions, is a suitable candidate for H . For this
purpose we used the WordNet (Fellbaum, 1998)
graph which was further enriched by connecting
1
http://ixa2.si.ehu.es/ukb/
470
each concept to all the concepts appearing in its
disambiguated gloss.
2
Structural similarity signature. In the struc-
tural similarity component (Figure 1(b), bottom),
the semantic signature for each concept c
i
is com-
puted by running the PPR algorithm on its corre-
sponding graph G
i
, hence a different M
i
is built
for each of the two concepts.
2.1.2 Signature unification
As mentioned earlier, semantic signatures are vec-
tors with dimension equal to the number of nodes
in the semantic graph. Since the structural similar-
ity signatures S
v
1
and S
v
2
are calculated on differ-
ent graphs and thus have different dimensions, we
need to make them comparable by unifying them.
We therefore propose an approach (part (c) of Fig-
ure 1) that finds a common ground between the
two signatures: to this end we consider all the
concepts associated with monosemous words in
the two signatures as landmarks and restrict the
two signatures exclusively to those common con-
cepts. Leveraging monosemous words as bridges
between two signatures is a particularly reliable
technique as typically a significant portion of all
words in a lexicon are monosemous.
3
Formally, let I
G
(w) be an inventory mapping
function that maps a term w to the set of con-
cepts which are expressed by w in graph G. Then,
given two signatures S
v
1
and S
v
2
, computed on
the respective graphs G
1
and G
2
, we first obtain
the setM of words that are monosemous accord-
ing to both semantic networks, i.e., M = {w :
|I
G
1
(w)|=1 ? |I
G
2
(w)|=1}. We then transform
each of the two signatures S
v
i
into a new sub-
signature S
?
v
i
whose dimension is |M|: the k
th
component of S
?
v
i
corresponds to the weight in S
v
i
of the only concept ofw
k
in I
G
i
(w
k
). As an exam-
ple, assume we are given two semantic signatures
computed for two concepts in WordNet and Wik-
tionary. Also, consider the noun tradeoff which
is monosemous according to both these resources.
Then, each of the two unified sub-signatures will
contain a component whose weight is determined
by the weight of the only concept associated with
tradeoff
n
in the corresponding semantic signature.
As a result of the unification process, we obtain
a pair of equally-sized semantic signatures with
comparable components.
2
http://wordnet.princeton.edu
3
For instance, we calculated that more than 80% of the
words in WordNet are monosemous, with over 60% of all the
synsets containing at least one of them.
2.1.3 Signature comparison
Having at hand the semantic signatures for the
two input concepts, we proceed to comparing
them (part (d) in Figure 1). We leverage a non-
parametric measure proposed by Pilehvar et al
(2013) which first transforms each signature into
a list of sorted elements and then calculates the
similarity on the basis of the average ranking of
elements across the two lists:
Sim(S
v
1
,S
v
2
) =
?
|T |
i=1
(r
1
i
+ r
2
i
)
?1
?
|T |
i=1
(2i)
?1
(1)
where T is the intersection of all concepts with
non-zero probability in the two signatures and r
j
i
is the rank of the i
th
entry in the j
th
sorted list.
The denominator is a normalization factor to guar-
antee a maximum value of one. The method pe-
nalizes the differences in the higher rankings more
than it does for the lower ones. The measure was
shown to outperform the conventional cosine dis-
tance when comparing different semantic signa-
tures in multiple textual similarity tasks (Pilehvar
et al, 2013).
2.1.4 Score combination
Finally (part (e) of Figure 1), we calculate the
overall similarity between two concepts as a lin-
ear combination of their definitional and struc-
tural similarities: ? Sim
def
(S
v
1
,S
v
2
) + (1 ?
?)Sim
str
(S
v
1
,S
v
2
). In Section 4.2.1, we explain
how we set, in our experiments, the values of ?
and the similarity threshold ? (cf. alignment algo-
rithm in Section 2).
3 Lexical Resource Ontologization
In Section 2, we presented our approach for align-
ing lexical resources. However, the approach as-
sumes that the input resources can be viewed as
semantic networks, which seems to limit its ap-
plicability to structured resources only. In or-
der to address this issue and hence generalize our
alignment approach to any given lexical resource,
we propose a method for transforming a given
machine-readable dictionary into a semantic net-
work, a process we refer to as ontologization.
Our ontologization algorithm takes as input a
lexicon L and outputs a semantic graph G =
(V,E) where, as already defined in Section 2, V is
the set of concepts in L and E is the set of seman-
tic relations between these concepts. Introducing
relational links into a lexicon can be achieved in
different ways. A first option is to extract binary
471
relations between pairs of words from raw text.
Both words in these relations, however, should
be disambiguated according to the given lexicon
(Pantel and Pennacchiotti, 2008), making the task
particularly prone to mistakes due to the high num-
ber of possible sense pairings.
Here, we take an alternative approach which
requires disambiguation on the target side only,
hence reducing the size of the search space sig-
nificantly. We first create the empty undirected
graph G
L
= (V,E) such that V is the set of con-
cepts in L and E = ?. For each source con-
cept c ? V we create a bag of content words
W = {w
1
, . . . , w
n
} which includes all the con-
tent words in its definition d and, if available, ad-
ditional related words obtained from lexicon rela-
tions (e.g., synonyms in Wiktionary). The prob-
lem is then cast as a disambiguation task whose
goal is to identify the intended sense of each word
w
i
? W according to the sense inventory of L: if
w
i
is monosemous, i.e., |{I
G
L
(w
i
)}| = 1, we con-
nect our source concept c to the only sense c
w
i
of
w
i
and set E := E ? {{c, c
w
i
}}; else, w
i
has mul-
tiple senses in L. In this latter case, we choose the
most appropriate concept c
i
? I
G
L
(w
i
) by finding
the maximal similarity between the definition of c
and the definitions of each sense of w
i
. To do this,
we apply our definitional similarity measure intro-
duced in Section 2.1. Having found the intended
sense c?
w
i
of w
i
, we add the edge {c, c?
w
i
} to E.
As a result of this procedure, we obtain a semantic
graph representation G for the lexicon L.
As an example, consider the 4
th
sense of the
noun cone in Wiktionary (i.e., cone
4
n
) which is de-
fined as ?The fruit of a conifer?. The definition
contains two content words: fruit
n
and conifer
n
.
The latter word is monosemous in Wiktionary,
hence we directly connect cone
4
n
to the only sense
of conifer
n
. The noun fruit, however, has 5 senses
in Wiktionary. We therefore measure the similar-
ity between the definition of cone
4
n
and all the 5
definitions of fruit and introduce a link from cone
4
n
to the sense of fruit which yields the maximal
similarity value (defined as ?(botany) The seed-
bearing part of a plant...?).
4 Experiments
Lexical resources. To enable a comparison with
the state of the art, we followed Matuschek
and Gurevych (2013) and performed an align-
ment of WordNet synsets (WN) to three different
collaboratively-constructed resources: Wikipedia
(WP), Wiktionary (WT), and OmegaWiki (OW).
We utilized the DKPro software (Zesch et al,
2008; Gurevych et al, 2012) to access the infor-
mation in the foregoing three resources. For WP,
WT, OW we used the dump versions 20090822,
20131002, and 20131115, respectively.
Evaluation measures. We followed previous
work (Navigli and Ponzetto, 2012; Matuschek and
Gurevych, 2013) and evaluated the alignment per-
formance in terms of four measures: precision, re-
call, F1, and accuracy. Precision is the fraction of
correct alignment judgments returned by the sys-
tem and recall is the fraction of alignment judg-
ments in the gold standard dataset that are cor-
rectly returned by the system. F1 is the harmonic
mean of precision and recall. We also report re-
sults for accuracy which, in addition to true posi-
tives, takes into account true negatives, i.e., pairs
which are correctly judged as unaligned.
Lexicons and semantic graphs. Here, we de-
scribe how the four semantic graphs for our four
lexical resources (i.e., WN, WP, WT, OW) were
constructed. As mentioned in Section 2.1.1, we
build the WN graph by including all the synsets
and semantic relations defined in WordNet (e.g.,
hypernymy and meronymy) and further populate
the relation set by connecting a synset to all the
other synsets that appear in its disambiguated
gloss. For WP, we used the graph provided by
Matuschek and Gurevych (2013), constructed by
directly connecting an article (concept) to all the
hyperlinks in its first paragraph, together with the
category links. Our WN and WP graphs have 118K
and 2.8M nodes, respectively, with the average
node degree being roughly 9 in both resources.
The other two resources, i.e., WT and OW, do
not provide a reliable network of semantic rela-
tions, therefore we used our ontologization ap-
proach to construct their corresponding semantic
graphs. We report, in the following subsection,
the experiments carried out to assess the accuracy
of our ontologization method, together with the
statistics of the obtained graphs for WT and OW.
4.1 Ontologization Experiments
For ontologizing WT and OW, the bag of con-
tent words W is given by the content words in
sense definitions and, if available, additional re-
lated words obtained from lexicon relations (see
Section 3). In WT, both of these are in word sur-
face form and hence had to be disambiguated. For
OW, however, the encoded relations, though rela-
472
Source Type WT OW
Definition
Ambiguous 76.6% 50.7%
Unambiguous 18.3% 32.9%
Relation
Ambiguous 2.8% -
Unambiguous 2.3% 16.4%
Total number of edges 2.1M 255K
Table 1: The statistics of the generated graphs
for WT and OW. We report the distribution of
the edges across types (i.e., ambiguous and un-
ambiguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
tively small in number, are already disambiguated
and, therefore, the ontologization was just per-
formed on the definition?s content words.
The resulting graphs for WT and OW contain
430K and 48K nodes, respectively, each provid-
ing more than 95% coverage of concepts, with the
average node degree being around 10 for both re-
sources. We present in Table 1, for WT and OW,
the total number of edges together with their dis-
tribution across types (i.e., ambiguous and unam-
biguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
The edges obtained from unambiguous entries
are essentially sense disambiguated on both sides
whereas those obtained from ambiguous terms
are a result of our similarity-based disambigua-
tion. Hence, given that a large portion of edges
came from ambiguous words (see Table 1), we
carried out an experiment to evaluate the accu-
racy of our disambiguation method. To this end,
we took as our benchmark the dataset provided
by Meyer and Gurevych (2010) for evaluating re-
lation disambiguation in WT. The dataset con-
tains 394 manually-disambiguated relations. We
compared our similarity-based disambiguation ap-
proach against the state of the art on this dataset,
i.e., the WKTWSD system, which is a WT rela-
tion disambiguation algorithm based on a series of
rules (Meyer and Gurevych, 2012b).
Table 2 shows the performance of our disam-
biguation method, together with that of WKTWSD,
in terms of Precision (P), Recall (R), F1, and ac-
curacy. The ?Human? row corresponds to the
inter-rater F1 and accuracy scores, i.e., the upper-
bound performance on this dataset, as calculated
by Meyer and Gurevych (2010). As can be seen,
our method proves to be very accurate, surpassing
the performance of the WKTWSD system in terms
of precision, F1, and accuracy. This is particularly
Approach P R F1 A
WKTWSD 0.780 0.800 0.790 0.840
Our method 0.852 0.767 0.807 0.857
Human - - 0.890 0.910
Table 2: The performance of relation disam-
biguation for our similarity-based disambiguation
method, as well as for the WKTWSD system.
interesting as the WKTWSD system uses a rule-
based technique specific to relation disambigua-
tion in WT, whereas our method is resource inde-
pendent and can be applied to arbitrary words in
the definition of any concept. We also note that the
graph constructed by Meyer and Gurevych (2010)
had an average node degree of around 1.
More recently, Matuschek and Gurevych (2013)
leveraged monosemous linking (cf. Section 5) in
order to create denser semantic graphs for OW and
WT. Our approach, however, thanks to the con-
nections obtained through ambiguous words, can
provide graphs with significantly higher coverage.
As an example, for WT, Matuschek and Gurevych
(2013) generated a graph where around 30% of
the nodes were in isolation, whereas this number
drops to around 5% in our corresponding graph.
These results show that our ontologization ap-
proach can be used to obtain dense semantic graph
representations of lexical resources, while at the
same time preserving a high level of accuracy.
Now that all the four resources are transformed
into semantic graphs, we move to our alignment
experiments.
4.2 Alignment Experiments
4.2.1 Experimental setup
Datasets. As our benchmark we tested on
the gold standard datasets used in Matuschek
and Gurevych (2013) for three alignment
tasks: WordNet-Wikipedia (WN-WP), WordNet-
Wiktionary (WN-WT), and WordNet-OmegaWiki
(WN-OW). However, the dataset for WN-OW was
originally built for the German language and,
hence, was missing many English OW concepts
that could be considered as candidate target
alignments. We therefore fixed the dataset for the
English language and reproduced the performance
of previous work on the new dataset. The three
datasets contained 320, 484, and 315 WN concepts
that were manually mapped to their corresponding
concepts in WP, WT, and OW, respectively.
473
Approach Training type
WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
SB Cross-val. 0.780 0.780 0.780 0.950 0.670 0.650 0.660 0.910 0.749 0.691 0.716 0.886
DWSA Tuning on subset 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SB+DWSA Cross-val. + tuning 0.750 0.870 0.810 0.950 0.680 0.710 0.690 0.920 0.794 0.688 0.735 0.898
SemAlign
Unsupervised 0.709 0.929 0.805 0.943 0.642 0.799 0.712 0.923 0.664 0.761 0.709 0.872
Tuning on subset 0.877 0.792 0.833 0.960 0.672 0.799 0.730 0.930 0.750 0.717 0.733 0.893
Cross-val. 0.852 0.835 0.840 0.965 0.680 0.769 0.722 0.931 0.778 0.725 0.749 0.900
Tuning on WN-WP - - - - 0.754 0.627 0.684 0.931 0.825 0.584 0.684 0.889
Tuning on WN-WT 0.738 0.934 0.824 0.950 - - - - 0.805 0.677 0.736 0.900
Tuning on WN-OW 0.744 0.925 0.824 0.950 0.684 0.766 0.723 0.930 - - - -
Table 3: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP),
Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy
(A). We present results for different configurations of our system (SemAlign), together with the state of
the art in definition similarity-based alignment approaches (SB) and the best configuration of the state-
of-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA).
Configurations. Recall from Section 2 that our
resource alignment technique has two parameters:
the similarity threshold ? and the combination pa-
rameter ?, both defined in [0, 1]. We performed
experiments with three different configurations:
? Unsupervised, where the two parameters are
set to their middle values (i.e., 0.5), hence,
no tuning is performed for either of the pa-
rameters. In this case, both the definitional
and structural similarity scores are treated
as equally important and two concepts are
aligned if their overall similarity exceeds the
middle point of the similarity scale.
? Tuning, where we follow Matuschek and
Gurevych (2013) and tune the parameters on
a subset of the dataset comprising 100 items.
? Cross-validation, where a 5-fold cross vali-
dation is carried out to find the optimal values
for the parameters, a technique used in most
of the recent alignment methods (Niemann
and Gurevych, 2011; Meyer and Gurevych,
2012a; Matuschek and Gurevych, 2013).
4.2.2 Results
We show in Table 3 the alignment performance of
different systems on the task of aligning WN-WP,
WN-WT, and WN-OW in terms of Precision (P), Re-
call (R), F1, and Accuracy. The SB system corre-
sponds to the state-of-the-art definition similarity
approaches for WN-WP (Niemann and Gurevych,
2011), WN-WT (Meyer and Gurevych, 2011), and
WN-OW (Gurevych et al, 2012). DWSA stands
for Dijkstra-WSA, the state-of-the-art graph-based
alignment approach of Matuschek and Gurevych
(2013). The authors also provided results for
SB+Dijkstra-WSA, a hybrid system where DWSA
was tuned for high precision and, in the case when
no alignment target could be found, the algorithm
fell back on SB judgments. We also show the re-
sults for this system as SB+DWSA in the table.
For our approach (SemAlign) we show the re-
sults of six different runs each corresponding to a
different setting. The first three (middle part of the
table) correspond to the results obtained with the
three configurations of SemAlign: unsupervised,
with tuning on subset, and cross-validation (see
Section 4.2.1). In addition to these, we performed
experiments where the two parameters of SemA-
lign were tuned on pair-independent training data,
i.e., a training dataset for a pair of resources dif-
ferent from the one being aligned. For this setting,
we used the whole dataset of the corresponding re-
source pair to tune the two parameters of our sys-
tem. We show the results for this setting in the
bottom part of the table (last three lines).
The main feature worth remarking upon is the
consistency in the results across different resource
pairs: the unsupervised system gains the best re-
call among the three configurations (with the im-
provement over SB+DWSA being always statisti-
cally significant
4
) whereas tuning, both on a subset
or through cross-validation, consistently leads to
the best performance in terms of F1 and accuracy
(with the latter being statistically significant with
respect to SB+DWSA on WN-WP and WN-WT).
Moreover, the unsupervised system proves to be
very robust inasmuch as it provides competitive
results on all the three datasets, while it surpasses
the performance of SB+DWSA on WN-WT. This
4
All significance tests are done using z-test at p < 0.05.
474
Approach
WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
Dijkstra-WSA 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SemAlign
str
0.877 0.788 0.830 0.959 0.604 0.643 0.623 0.907 0.654 0.602 0.627 0.853
Table 4: Performance of SemAlign when using only the structural similarity component (SemAlign
str
)
compared to the state-of-the-art graph-based alignment approach, Dijkstra-WSA (Matuschek and
Gurevych, 2013) for our three resource pairs: WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT),
and OmegaWiki (WN-OW).
is particularly interesting as the latter system in-
volves tuning of several parameters, whereas Se-
mAlign, in its unsupervised configuration, does
not need any training data nor does it involve any
tuning. In addition, as can be seen in the table,
SemAlign benefits from pair-independent training
data in most cases across the three resource pairs
with performance surpassing that of SB+DWSA, a
system which is dependent on pair-specific train-
ing data. The consistency in the performance of
SemAlign in its different configurations and across
different resource pairs indicates its robustness
and shows that our system can be utilized effec-
tively for aligning any pair of lexical resources, ir-
respective of their structure or availability of train-
ing data.
The system performance is generally higher on
the alignment task for WP compared to WT and
OW. We attribute this difference to the dictionary
nature of the latter two, where sense distinctions
are more fine-grained, as opposed to the relatively
concrete concepts in the WP encyclopedia.
4.3 Similarity Measure Analysis
We explained in Section 2.1 that our concept sim-
ilarity measure consists of two components: the
definitional and the structural similarities. Mea-
suring the similarity of two concepts in terms of
their definitions has been investigated in previ-
ous work (Niemann and Gurevych, 2011; Hen-
rich et al, 2012). The structural similarity compo-
nent of our approach, however, is novel, but at the
same time one of the very few measures which en-
ables the computation of the similarity of concepts
across two resources directly and independently of
the similarity of their definitions. A comparable
approach is the Dijkstra-WSA proposed by Ma-
tuschek and Gurevych (2013) which, as also men-
tioned earlier in the Introduction, first connects the
two resources? graphs by leveraging monosemous
linking and then aligns two concepts across the
two graphs on the basis of their shortest distance.
To gain more insight into the effectiveness of our
structural similarity measure in comparison to the
Dijkstra-WSA method, we carried out an experi-
ment where our alignment system used only the
structural similarity component, a variant of our
system we refer to as SemAlign
str
. Both systems
(i.e., SemAlign
str
and Dijkstra-WSA) were tuned
on 100-item subsets of the corresponding datasets.
We show in Table 4 the performance of the two
systems on our three datasets. As can be seen in
the table, SemAlign
str
consistently improves over
Dijkstra-WSA according to recall, F1 and accu-
racy with all the differences in recall and accu-
racy being statistically significant (p < 0.05). The
improvement is especially noticeable for pairs in-
volving either WT or OW where, thanks to the rel-
atively denser semantic graphs obtained by means
of our ontologization technique, the gap in F1 is
about 0.23 (WN-WT) and 0.15 (WN-OW).
In addition, as we mentioned earlier, for WN-WP
we used the same graph as that of Dijkstra-WSA,
since both WN and WP provide a full-fledged se-
mantic network and thus neither needed to be
ontologized. Therefore, the considerable perfor-
mance improvement over Dijkstra-WSA on this
resource pair shows the effectiveness of our novel
concept similarity measure independently of the
underlying semantic network.
5 Related Work
Resource ontologization. Having lexical re-
sources represented as semantic networks is
highly beneficial. A good example is WordNet,
which has been exploited as a semantic network
in dozens of NLP tasks (Fellbaum, 1998). A re-
cent prominent case is Wikipedia (Medelyan et
al., 2009; Hovy et al, 2013) which, thanks to
its inter-article hyperlink structure, provides a rich
backbone for structuring additional information
(Auer et al, 2007; Suchanek et al, 2008; Moro
and Navigli, 2013; Flati et al, 2014). How-
ever, there are many large-scale resources, such
as Wiktionary for instance, which by their very
nature are not in the form of a graph. This is
475
usually the case with machine-readable dictionar-
ies, where structuring the resource involves the
arduous task of connecting lexicographic senses
by means of semantic relations. Surprisingly,
despite their vast potential, little research has
been conducted on the automatic ontologization of
collaboratively-constructed dictionaries like Wik-
tionary and OmegaWiki. Meyer and Gurevych
(2012a) and Matuschek and Gurevych (2013) pro-
vided approaches for building graph representa-
tions of Wiktionary and OmegaWiki. The result-
ing graphs, however, were either sparse or had a
considerable portion of the nodes left in isolation.
Our approach, in contrast, aims at transforming a
lexical resource into a full-fledged semantic net-
work, hence providing a denser graph with most
of its nodes connected.
Resource alignment. Aligning lexical resources
has been a very active field of research in the
last decade. One of the main objectives in this
area has been to enrich existing ontologies by
means of complementary information from other
resources. As a matter of fact, most efforts have
been concentrated on aligning the de facto com-
munity standard sense inventory, i.e. WordNet, to
other resources. These include: the Roget?s the-
saurus and Longman Dictionary of Contemporary
English (Kwong, 1998), FrameNet (Laparra and
Rigau, 2009), VerbNet (Shi and Mihalcea, 2005)
or domain-specific terminologies such as the Uni-
fied Medical Language System (Burgun and Bo-
denreider, 2001). More recently, the growth
of collaboratively-constructed resources has seen
the development of alignment approaches with
Wikipedia (Ruiz-Casado et al, 2005; Auer et al,
2007; Suchanek et al, 2008; Reiter et al, 2008;
Navigli and Ponzetto, 2012), Wiktionary (Meyer
and Gurevych, 2011) and OmegaWiki (Gurevych
et al, 2012). Last year Matuschek and Gurevych
(2013) proposed Dijkstra-WSA, a graph-based ap-
proach relying on shortest paths between two
concepts when the two corresponding resources
graphs were combined by leveraging monosemous
linking. Their method when backed off with other
definition similarity based approaches (Niemann
and Gurevych, 2011; Meyer and Gurevych, 2011),
achieved state-of-the-art results on the mapping of
WordNet to different collaboratively-constructed
resources. This approach, however, in addition to
setting the threshold for the definition similarity
component by means of cross validation, also re-
quired other parameters to be tuned, such as the
allowed path length (?) and the maximum num-
ber of edges in a graph. The optimal value for the
? parameter varied from one resource pair to an-
other, and even for a specific resource pair it had
to be tuned for each configuration. This made the
approach dependent on the training data for the
specific pair of resources that were to be aligned.
Instead of measuring the similarity of two con-
cepts on the basis of their distance in the com-
bined graph, our approach models each concept
through a rich vectorial representation we refer to
as semantic signature and compares the two con-
cepts in terms of the similarity of their semantic
signatures. This rich representation leads to our
approach having a good degree of robustness such
that it can achieve competitive results even in the
absence of training data. This enables our system
to be applied effectively for aligning new pairs of
resources for which no training data is available,
with state-of-the-art performance.
6 Conclusions
This paper presents a unified approach for align-
ing lexical resources. Our method leverages
a novel similarity measure which enables a di-
rect structural comparison of concepts across dif-
ferent lexical resources. Thanks to an effec-
tive ontologization method, our alignment ap-
proach can be applied to any pair of lexical re-
sources independently of whether they provide
a full-fledged network structure. We demon-
strate that our approach achieves state-of-the-
art performance on aligning WordNet to three
collaboratively-constructed resources with differ-
ent characteristics, i.e., Wikipedia, Wiktionary,
and OmegaWiki. We also show that our approach
is robust across its different configurations, even
when the training data is absent, enabling it to be
used effectively for aligning new pairs of lexical
resources for which no resource-specific training
data is available. In future work, we plan to ex-
tend our concept similarity measure across differ-
ent natural languages. We release all our data at
http://lcl.uniroma1.it/semalign.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Michael Matuschek for
providing us with Wikipedia graphs and alignment
datasets.
476
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?41, Athens, Greece.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735, Bu-
san, Korea.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual Web search engine. In Pro-
ceedings of the 7
th
Conference on World Wide Web,
pages 107?117, Brisbane, Australia.
Anita Burgun and Olivier Bodenreider. 2001. Compar-
ing terms, concepts and semantic classes in WordNet
and the Unified Medical Language System. In Pro-
ceedings of NAACL Workshop, WordNet and Other
Lexical Resources: Applications, Extensions and
Customizations, pages 77?82, Pittsburgh, USA.
Gerard de Melo and Gerhard Weikum. 2010. Pro-
viding multilingual, multimodal answers to lexical
database queries. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), pages 348?355, Val-
letta, Malta.
Stefano Faralli and Roberto Navigli. 2012. A
New Minimally-supervised Framework for Domain
Word Sense Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1411?
1422, Jeju, Korea.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and bet-
ter) than one: the Wikipedia Bitaxonomy Project.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014), Baltimore, Maryland.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 580?590, Avignon, France.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th international conference
on World Wide Web, pages 517?526, Hawaii, USA.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2011. Semi-automatic extension of GermaNet
with sense definitions from Wiktionary. In Pro-
ceedings of 5th Language & Technology Conference
(LTC 2011), pages 126?130, Pozna, Poland.
Verena Henrich, Erhard W. Hinrichs, and Klaus Sut-
tner. 2012. Automatically linking GermaNet to
Wikipedia for harvesting corpus examples for Ger-
maNet senses. In Journal for Language Technology
and Computational Linguistics (JLCL), 27(1):1?19.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing,
pages 581?589, Prague, Czech Republic.
Oi Yee Kwong. 1998. Aligning WordNet with
additional lexical resources. In COLING-ACL98
Workshop on Usage of WordNet in Natural Lan-
guage Processing Systems, pages 73?79, Montreal,
Canada.
Egoitzand Laparra and German Rigau. 2009. Inte-
grating WordNet and FrameNet using a knowledge-
based Word Sense Disambiguation algorithm. In
Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP09), pages 1?6, Borovets,
Bulgaria.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-WSA: A graph-based approach to word
sense alignment. Transactions of the Association for
Computational Linguistics (TACL), 1:151?164.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Christian M. Meyer and Iryna Gurevych. 2010. ?worth
its weight in gold or yet another resource?; a com-
parative study of Wiktionary, OpenThesaurus and
GermaNet. In Proceedings of the 11th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?10, pages 38?49,
Iasi, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
psycholinguists know about Chemistry: Aligning
Wiktionary and WordNet for increased domain cov-
erage. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
883?892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012a. On-
toWiktionary: Constructing an ontology from the
collaborative online dictionary Wiktionary. In Semi-
Automatic Ontology Development: Processes and
Resources, pages 131?161. IGI Global.
477
Christian M. Meyer and Iryna Gurevych. 2012b.
To exhibit is not to loiter: A multilingual, sense-
disambiguated Wiktionary for measuring verb sim-
ilarity. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING
2012), pages 1763?1780, Mumbai, India.
Andrea Moro and Roberto Navigli. 2013. Integrating
syntactic and semantic analysis into the Open Infor-
mation Extraction paradigm. In Proceedings of the
23
rd
International Joint Conference on Artificial In-
telligence (IJCAI 2013), pages 2148?2154, Beijing,
China.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier
de Lacalle, and Eneko Agirre. 2011. Two birds
with one stone: Learning semantic models for text
categorization and Word Sense Disambiguation. In
Proceedings of the 20th ACM Conference on Infor-
mation and Knowledge Management (CIKM), pages
2317?2320, Glasgow, UK.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21st International Conference on
Computational Linguistics (COLING-ACL 2006),
pages 105?112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people?s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the Ninth International Confer-
ence on Computational Semantics, pages 205?214,
Oxford, United Kingdom.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic Role Labeling. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Patrick Pantel and Marco Pennacchiotti. 2008. Auto-
matically harvesting and ontologizing semantic rela-
tions. In Proceedings of the 2008 Conference on On-
tology Learning and Population: Bridging the Gap
Between Text and Knowledge, pages 171?195, Am-
sterdam, The Netherlands.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1341?1351, Sofia, Bulgaria.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third International Conference on
Advances in Web Intelligence, pages 380?386, Lodz,
Poland.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the 6th International Conference on Computational
Linguistics and Intelligent Text Processing, pages
100?111, Mexico City, Mexico.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using Wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd national con-
ference on Artificial intelligence - Volume 2, pages
861?866, Chicago, Illinois.
478
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17?26,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 3: Cross-Level Semantic Similarity
David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{jurgens,pilehvar,navigli}@di.uniroma1.it
Abstract
This paper introduces a new SemEval
task on Cross-Level Semantic Similarity
(CLSS), which measures the degree to
which the meaning of a larger linguistic
item, such as a paragraph, is captured by
a smaller item, such as a sentence. High-
quality data sets were constructed for four
comparison types using multi-stage an-
notation procedures with a graded scale
of similarity. Nineteen teams submitted
38 systems. Most systems surpassed the
baseline performance, with several attain-
ing high performance for multiple com-
parison types. Further, our results show
that comparisons of semantic representa-
tion increase performance beyond what is
possible with text alone.
1 Introduction
Given two linguistic items, semantic similarity
measures the degree to which the two items have
the same meaning. Semantic similarity is an es-
sential component of many applications in Nat-
ural Language Processing (NLP), and similarity
measurements between all types of text as well
as between word senses lend themselves to a va-
riety of NLP tasks such as information retrieval
(Hliaoutakis et al., 2006) or paraphrasing (Glick-
man and Dagan, 2003).
Semantic similarity evaluations have largely fo-
cused on comparing similar types of lexical items.
Most recently, tasks in SemEval (Agirre et al.,
2012) and *SEM (Agirre et al., 2013) have intro-
duced benchmarks for measuring Semantic Tex-
tual Similarity (STS) between similar-sized sen-
tences and phrases. Other data sets such as that
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
of Rubenstein and Goodenough (1965) measure
similarity between word pairs, while the data sets
of Navigli (2006) and Kilgarriff (2001) offer a bi-
nary similar-dissimilar distinction between senses.
Notably, all of these evaluations have focused on
comparisons between a single type, in contrast to
application-based evaluations such as summariza-
tion and compositionality which incorporate tex-
tual items of different sizes, e.g., measuring the
quality of a paragraph?s sentence summarization.
Task 3 introduces a new evaluation where sim-
ilarity is measured between items of different
types: paragraphs, sentences, phrases, words and
senses. Given an item of the lexically-larger type,
a system measures the degree to which the mean-
ing of the larger item is captured in the smaller
type, e.g., comparing a paragraph to a sentence.
We refer to this task as Cross-Level Semantic Sim-
ilarity (CLSS). A major motivation of this task
is to produce semantic similarity systems that are
able to compare all types of text, thereby free-
ing downstream NLP applications from needing to
consider the type of text being compared. Task 3
enables assessing the extent to which the mean-
ing of the sentence ?do u know where i can watch
free older movies online without download?? is
captured in the phrase ?streaming vintage movies
for free?, or how similar is ?circumscribe? to the
phrase ?beating around the bush.? Furthermore,
by incorporating comparisons of a variety of item
sizes, Task 3 unifies in a single task multiple ob-
jectives from different areas of NLP such as para-
phrasing, summarization, and compositionality.
Because CLSS generalizes STS to items of dif-
ferent types, successful CLSS systems can directly
be applied to all STS-based applications. Fur-
thermore, CLSS systems can be used in other
similarity-based applications such as text simpli-
fication (Specia et al., 2012), keyphrase iden-
tification (Kim et al., 2010), lexical substitu-
tion (McCarthy and Navigli, 2009), summariza-
17
tion (Sp?arck Jones, 2007), gloss-to-sense mapping
(Pilehvar and Navigli, 2014b), and modeling the
semantics of multi-word expressions (Marelli et
al., 2014) or polysemous words (Pilehvar and Nav-
igli, 2014a).
Task 3 was designed with three main objectives.
First, the task should include multiple types of
comparison in order to assess each type?s difficulty
and whether specialized resources are needed for
each. Second, the task should incorporate text
from multiple domains and writing styles to en-
sure that system performance is robust across text
types. Third, the similarity methods should be able
to operate at the sense level, thereby potentially
uniting text- and sense-based similarity methods
within a single framework.
2 Task Description
2.1 Objective
Task 3 is intended to serve as an initial task for
evaluating the capabilities of systems at measuring
all types of semantic similarity, independently of
the size of the text. To accomplish this objective,
systems were presented with items from four com-
parison types: (1) paragraph to sentence, (2) sen-
tence to phrase, (3) phrase to word, and (4) word to
sense. Given a pair of items, a system must assess
the degree to which the meaning of the larger item
is captured in the smaller item. WordNet 3.0 was
chosen as the sense inventory (Fellbaum, 1998).
2.2 Rating Scale
Following previous SemEval tasks (Agirre et al.,
2012; Jurgens et al., 2012), Task 3 recognizes that
two items? similarity may fall within a range of
similarity values, rather than having a binary no-
tion of similar or dissimilar. Initially a six-point
(0?5) scale similar to that used in the STS tasks
was considered (Agirre et al., 2012); however, an-
notators found difficulty in deciding between the
lower-similarity options. After multiple revisions
and feedback from a group of initial annotators,
we developed a five-point Likert scale for rating a
pair?s similarity, shown in Table 1.
1
The scale was designed to systematically order
a broad range of semantic relations: synonymy,
similarity, relatedness, topical association, and un-
relatedness. Because items are of different sizes,
the highest rating is defined as very similar rather
1
Annotation materials along with all training and test
data are available on the task website http://alt.qcri.
org/semeval2014/task3/.
than identical to allow for some small loss in the
overall meaning. Furthermore, although the scale
is designed as a Likert scale, annotators were given
flexibility when rating items to use values between
the defined points in the scale, indicating a blend
of two relations. Table 2 provides examples of
pairs for each scale rating for all four comparison
type.
3 Task Data
Though several data sets exist for STS and com-
paring words and senses, no standard data set ex-
ists for CLSS. Therefore, we created a pilot data
set designed to test the capabilities of systems in a
variety of settings. The task data for all compar-
isons but word-to-sense was created using a three-
phase process. First, items of all sizes were se-
lected from publicly-available data sets. Second,
the selected items were used to produce a second
item of the next-smaller level (e.g., a sentence in-
spires a phrase). Third, the pairs of items were
annotated for their similarity. Because of the ex-
pertise required for working with word senses, the
word-to-sense data set was constructed by the or-
ganizers using a separate but similar process. In
the training and test data, each comparison type
had 500 annotated examples, for a total of 2000
pairs each for training and test. We first describe
the corpora used by Task 3 followed by the anno-
tation process. We then describe the construction
of the word-to-sense data set.
3.1 Corpora
Test and training data were constructed by draw-
ing from multiple publicly-available corpora and
then manually generating a paired item for com-
parison. To achieve our second objective for the
task, the data sets used to create item pairs in-
cluded texts from specific domains, social media,
and text with idiomatic or slang language. Table
3 summarizes the corpora and their distribution
across the test and training sets for each compari-
son type, with a high-level description of the genre
of the data. We briefly describe the corpora next.
The WikiNews, Reuters 21578, and Microsoft
Research (MSR) Paraphrase corpora are all drawn
from newswire text, with WikiNews being au-
thored by volunteer writers and the latter two cor-
pora written by professionals. Travel Guides was
drawn from the Berlitz travel guides data in the
Open American National Corpus (Ide and Suder-
man, 2004) and includes very verbose sentences
18
4 ? Very
Similar
The two items have very similar meanings and the most important ideas, concepts, or actions in the larger
text are represented in the smaller text. Some less important information may be missing, but the smaller
text is a very good summary of the larger text.
3 ? Somewhat
Similar
The two items share many of the same important ideas, concepts, or actions, but include slightly different
details. The smaller text may use similar but not identical concepts (e.g., car vs. vehicle), or may omit a
few of the more important ideas present in the larger text.
2 ? Somewhat
related but not
similar
The two items have dissimilar meaning, but share concepts, ideas, and actions that are related. The smaller
text may use related but not necessarily similar concepts (window vs. house) but should still share some
overlapping concepts, ideas, or actions with the larger text.
1 ? Slightly
related
The two items describe dissimilar concepts, ideas and actions, but may share some small details or domain
in common and might be likely to be found together in a longer document on the same topic.
0 ? Unrelated The two items do not mean the same thing and are not on the same topic.
Table 1: The five-point Likert scale used to rate the similarity of item pairs. See Table 2 for examples.
with many named entities. Wikipedia Science
was drawn from articles tagged with the cate-
gory Science on Wikipedia. Food reviews were
drawn from the SNAP Amazon Fine Food Re-
views data set (McAuley and Leskovec, 2013)
and are customer-authored reviews for a variety of
food items. Fables were taken from a collection of
Aesop?s Fables. The Yahoo! Answers corpus was
derived from the Yahoo! Answers data set, which
is a collection of questions and answers from the
Community Question Answering (CQA) site; the
data set is notable for having the highest degree of
ungrammaticality in our test set. SMT Europarl
is a collection of texts from the English-language
proceedings of the European parliament (Koehn,
2005); Europarl data was also used in the PPDB
corpus (Ganitkevitch et al., 2013), from which
phrases were extracted. Wikipedia was used to
generate two phrase data sets from (1) extracting
the definitional portion of an article?s initial sen-
tence, e.g., ?An [article name] is a [definition],?
and (2) captions for an article?s images. Web
queries were gathered from online sources of real-
world queries. Last, the first and second authors
generated slang and idiomatic phrases based on
expressions contained in Wiktionary.
For all comparison types, the test data included
one genre that was not seen in the training data
in order to test the generalizability of the systems
on data from a novel domain. In addition, we
included a new type of challenge genre with Fa-
bles; unlike other domains, the sentences paired
with the fable paragraphs were potentially seman-
tic interpretations of the intent of the fable, i.e.,
the moral of the story. These interpretations often
have little textual overlap with the fable itself and
require a deeper interpretation of the paragraph?s
meaning in order to make the correct similarity
judgment.
Prior to the annotation process, all content was
filtered to ensure its size and format matched the
desired text type. By average, a paragraph in our
dataset consists of 3.8 sentences. Typos and gram-
matical mistakes in the community-produced con-
tent were left unchanged.
3.2 Annotation Process
A two-phase process was used to produce the test
and training data sets for all but word-to-sense.
Phase 1 generates the item pairs from source texts
and Phase 2 rates the pairs? similarity.
Phase 1 In this phase, annotators were shown the
larger text of a comparison type and then asked
to produce the smaller text of the pair at a spec-
ified similarity; for example an annotator may be
shown a paragraph and asked to write a sentence
that is a ?3? rating. Annotators were instructed to
leave the smaller text blank if they had difficulty
understanding the larger text.
The requested similarity ratings were balanced
to create a uniform distribution of similarity val-
ues. Annotators were asked only to generate rat-
ings of 1?4; pairs with a ?0? rating were automat-
ically created by pairing the larger item with ran-
dom selections of text of the appropriate size from
the same corpus. The intent of Phase 1 is to pro-
duce varied item pairs with an expected uniform
distribution of similarity values along the rating
scale.
Four annotators participated in Phase 1 and
were paid a bulk rate of e110 for completing the
work. In addition to the four annotators, the first
two organizers also assisted in Phase 1: Both com-
pleted items from the SCIENTIFIC genre and the
first organizer produced 994 pairs, including all
19
PARAGRAPH TO SENTENCE
Paragraph: Teenagers take aerial shots of their neigh-
bourhood using digital cameras sitting in old bottles which
are launched via kites - a common toy for children liv-
ing in the favelas. They then use GPS-enabled smart-
phones to take pictures of specific danger points - such as
rubbish heaps, which can become a breeding ground for
mosquitoes carrying dengue fever.
Rating Sentence
4 Students use their GPS-enabled cellphones to
take birdview photographs of a land in order
to find specific danger points such as rubbish
heaps.
3 Teenagers are enthusiastic about taking aerial
photograph in order to study their neighbour-
hood.
2 Aerial photography is a great way to identify
terrestrial features that aren?t visible from the
ground level, such as lake contours or river
paths.
1 During the early days of digital SLRs, Canon
was pretty much the undisputed leader in
CMOS image sensor technology.
0 Syrian President Bashar al-Assad tells the US
it will ?pay the price? if it strikes against Syria.
SENTENCE TO PHRASE
Sentence: Schumacher was undoubtedly one of the very
greatest racing drivers there has ever been, a man who was
routinely, on every lap, able to dance on a limit accessible
to almost no-one else.
Rating Phrase
4 the unparalleled greatness of Schumacher?s
driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
PHRASE TO WORD
Phrase: loss of air pressure in a tire
Rating Word
4 flat-tire
3 deflation
2 wheel
1 parking
0 butterfly
WORD TO SENSE
Word: automobile
n
Rating Sense
4 car
1
n
(a motor vehicle with four wheels; usually
propelled by an internal combustion engine)
3 vehicle
1
n
(a conveyance that transports people
or objects)
2 bike
1
n
(a motor vehicle with two wheels and a
strong frame)
1 highway
1
n
(a major road for any form of motor
transport)
0 pen
1
n
(a writing implement with a point from
which ink flows)
Table 2: Example pairs and their ratings.
those for the METAPHORIC genre, and those that
the other annotators left blank.
Phase 2 Here, the item pairs produced in Phase
1 were rated for their similarity according to the
scale described in Section 2.2. An initial pilot
study showed that crowdsourcing was only mod-
erately effective for producing these ratings with
high agreement. Furthermore, the texts used in
Task 3 came from a variety of genres, such as
scientific domains, which some workers had dif-
ficulty understanding. While we note that crowd-
sourcing has been used in prior STS tasks for
generating similarity scores (Agirre et al., 2012;
Agirre et al., 2013), both tasks? efforts encoun-
tered lower worker score correlations on some por-
tions of the dataset (Diab, 2013), suggesting that
crowdsourcing may not be reliable for judging the
similarity of certain types of text. See Section 3.5
for additional details.
Therefore, to ensure high quality, the first two
organizers rated all items independently. Because
the sentence-to-phrase and phrase-to-word com-
parisons contain slang and idiomatic language, a
third American English mother tongue annotator
was added for those data sets. The third annotator
was compensated e250 for their assistance.
Annotators were allowed to make finer-grained
distinctions in similarity using multiples of 0.25.
For all items, when any two annotators disagreed
by one or more scale points, we performed an
adjudication to determine the item?s rating in the
gold standard. The adjudication process revealed
that nearly all disagreements were due to annota-
tor mistakes, e.g., where one annotator had over-
looked a part of the text or had misunderstood the
text?s meaning. The final similarity rating for an
unadjudicated item was the average of its ratings.
3.3 Word-to-Sense
Word-to-sense comparison items were generated
in three phases. To increase the diversity and
challenge of the data set, the word-to-sense was
created for four types of words: (1) a word and
its intended meaning are in WordNet, (2) a word
was not in the WordNet vocabulary, e.g., the verb
?zombify,? (3) the word is in WordNet, but has a
novel meaning that is not in WordNet, e.g., the ad-
jective ?red? referring to Communist, and (4) a set
of challenge words where one of the word?s senses
and a second sense are directly connected by an
edge in the WordNet network, but the two senses
are not always highly similar.
20
Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word
Corpus Genre Train Test Train Test Train Test
WikiNews Newswire 15.0 10.0 9.2 6.0
Reuters 21578 Newswire 20.2 15.0 5.0
Travel Guides Travel 15.2 10.0 15.0 9.8
Wikipedia Science Scientific ? 25.6 ? 14.8
Food Reviews Review 19.6 20.0
Fables Metaphoric 9.0 5.2
Yahoo! Answers CQA 21.0 14.2 17.6 17.4
SMT Europarl Newswire 35.4 14.4
MSR Paraphrase Newswire 10.0 10.0 8.8 6.0
Idioms Idiomatic 12.8 12.6 20.0 20.0
Slang Slang ? 15.0 ? 25.0
PPDB Newswire 10.0 10.0
Wikipedia Glosses Lexicographic 28.2 17.0
Wikipedia Image Captions Descriptive 23.0 17.0
Web Search Queries Search 5.0 5.0
Table 3: Percentages of the training and test data per source corpus.
In Phase 1, to select the first type of word,
lemmas in WordNet were ranked by frequency
in Wikipedia; the ranking was divided into ten
equally-sized groups, with words sampled evenly
from groups in order to control for word frequency
in the task data. For the second type, words not
present in WordNet were drawn from two sources:
examining words in Wikipedia, which we refer
to as out-of-vocabulary (OOV), and slang words.
For the third type, to identify words with a novel
sense, we examined Wiktionary entries and chose
novel, salient senses that were distinct from those
in WordNet. We refer to words with a novel mean-
ing as out-of-sense (OOS). Words of the fourth
type were chosen by hand. The part-of-speech dis-
tributions for all four types of items were balanced
as 50% noun, 25% verb, 25% adjective.
In Phase 2, each word was associated with a
particular WordNet sense for its intended mean-
ing, or the closest available sense in WordNet
for OOV or OOS items. To select a comparison
sense, we adopted a neighborhood search proce-
dure: All synsets connected by at most three edges
in the WordNet semantic network were shown.
Given a word and its neighborhood, the corre-
sponding sense for the item pair was selected by
matching the sense with an intended similarity for
the pair, much like how text items were gener-
ated in Phase 1. The reason behind using this
neighborhood-based selection process was to min-
imize the potential bias of consistently selecting
lower-similarity items from those further away in
the WordNet semantic network.
In Phase 3, given all word-sense pairs, annota-
tors were shown the definitions associated with the
intended meaning of the word and of the sense.
Definitions were drawn from WordNet or from
Wiktionary, if the word was OOV or OOS. An-
notators had access to the WordNet structure for
the compared sense in order to take into account
its parents and siblings.
3.4 Trial Data
The trial data set was created using a separate
process. Source text was drawn from WikiNews;
we selected the text for the larger item of each
level and then generated the text or sense of the
smaller. A total of 156 items were produced.
After, four fluent annotators independently rated
all items. Inter-annotator agreement rates varied
in 0.734?0.882, using Krippendorff?s ? (Krippen-
dorff, 2004) on the interval scale.
3.5 Data Set Discussion
The resulting annotation process produced a high-
quality data set. First, Table 4 shows the inter-
annotator agreement (IAA) statistics for each
comparison type on both the full and unadjudi-
cated portions of the data set. IAA was measured
using Krippendorff?s ? for interval data. Because
the disagreements that led to lower ? in the full
data were resolved via adjudication, the quality of
the full data set is expected to be on par with that
of the unadjudicated data. The annotation quality
for Task 3 was further improved by manually ad-
judicating all significant disagreements.
In contrast, the data sets of current STS tasks
aggregated data from annotators with moderate
correlation with each other (Diab, 2013); STS-
2012 (Agirre et al., 2012) saw inter-annotator
Pearson correlations of 0.530?0.874 per data set
and STS-2013 (Agirre et al., 2013) had average
21
Training Test
Data All Unadj. All Unadj.
Para.-to-Sent. 0.856 0.916 0.904 0.971
Sent.-to-Phr. 0.773 0.913 0.766 0.980
Phr.-to-Word 0.735 0.895 0.730 0.988
Word-to-Sense 0.681 0.895 0.655 0.952
Table 4: IAA rates for the task data.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Paragraph-to-Sentence
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sentence-to-Phrase
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Phrase-to-Word
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Word-to-Sense
TrainingTest
Figure 1: Similarity ratings distributions.
inter-annotator correlations of 0.377?0.832. How-
ever, we note that Pearson correlation and Krip-
pendorff?s ? are not directly comparable (Artstein
and Poesio, 2008), as annotators? scores may be
correlated, but completely disagree.
Second, the two-phase construction process
produced values that were evenly distributed
across the rating scale, shown in Figure 1 as the
distribution of the values for all data sets. How-
ever, we note that this creation procedure was very
resource intensive and, therefore, semi-automated
or crowdsourcing-based approaches for produc-
ing high-quality data will be needed to expand
the size of the data in future CLSS-based eval-
uations. Nevertheless, as a pilot task, the man-
ual effort was essential for ensuring a rigorously-
constructed data set for the initial evaluation.
4 Evaluation
Participation The ultimate goal of Task 3 is to
produce systems that can measure similarity for
multiple types of items. Therefore, we strongly
encouraged participating teams to submit systems
that were capable of generating similarity judg-
ments for multiple comparison types. However,
to further the analysis, participants were also per-
mitted to submit systems specialized to a single
domain. Teams were allowed at most three system
submissions, regardless of the number of compar-
ison types supported.
Scoring Systems were required to provide sim-
ilarity values for all items within a comparison
type. Following prior STS evaluations, systems
were scored for each comparison type using Pear-
son correlation. Additionally, we include a second
score using Spearman?s rank correlation, which is
only affected by differences in the ranking of items
by similarity, rather than differences in the similar-
ity values. Pearson correlation was chosen as the
official evaluation metric since the goal of the task
is to produce similar scores. However, Spearman?s
rank correlation provides an important metric for
assessing systems whose scores do not match hu-
man scores but whose rankings might, e.g., string-
similarity measures. Ultimately, a global ranking
was produced by ordering systems by the sum of
their Pearson correlation values for each of the
four comparison levels.
Baselines The official baseline system was
based on the Longest Common Substring (LCS),
normalized by the length of items using the
method of Clough and Stevenson (2011). Given
a pair, the similarity is reported as the normalized
length of the LCS. In the case of word-to-sense,
the LCS for a word-sense pair is measured be-
tween the sense?s definition in WordNet and the
definitions of each sense of the pair?s word, report-
ing the maximal LCS. Because OOV and slang
words are not in WordNet, the baseline reports the
average similarity value of non-OOV items. Base-
line scores were made public after the evaluation
period ended.
Because LCS is a simple procedure, a second
baseline based on Greedy String Tiling (GST)
(Wise, 1996) was added after the evaluation pe-
riod concluded. Unlike LCS, GST better handles
the transpositions of tokens across the two texts
and can still report high similarity when encoun-
tering reordered text. The minimum match length
for GST was set to 6.
5 Results
Nineteen teams submitted 38 systems. Of those
systems, 34 produced values for paragraph-to-
sentence and sentence-to-phrase comparisons, 22
for phrase-to-word, and 20 for word-to-sense.
Two teams submitted revised scores for their sys-
tems after the deadline but before the test set had
22
Team System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank Spearman Rank
Meerkat Mafia pairingWords? 0.794 0.704 0.457 0.389
SimCompass run1 0.811 0.742 0.415 0.356 1 1
ECNU run1 0.834 0.771 0.315 0.269 2 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3 6
SemantiKLUE run1 0.817 0.754 0.215 0.314 4 4
UNAL-NLP run1 0.817 0.739 0.252 0.249 5 7
UNIBA run2 0.784 0.734 0.255 0.180 6 8
RTM-DCU run1? 0.845 0.750 0.305
UNIBA run1 0.769 0.729 0.229 0.165 7 10
UNIBA run3 0.769 0.729 0.229 0.165 8 11
BUAP run1 0.805 0.714 0.162 0.201 9 13
BUAP run2 0.805 0.714 0.142 0.194 10 9
Meerkat Mafia pairingWords 0.794 0.704 -0.044 0.389 11 12
HULTECH run1 0.693 0.665 0.254 0.150 12 16
GST Baseline 0.728 0.662 0.146 0.185
HULTECH run3 0.669 0.671 0.232 0.137 13 15
RTM-DCU run2? 0.785 0.698 0.221
RTM-DCU run3 0.780 0.677 0.208 14 17
HULTECH run2 0.667 0.633 0.180 0.169 15 14
RTM-DCU run1 0.786 0.666 0.171 16 18
RTM-DCU run3? 0.786 0.663 0.171
Meerkat Mafia SuperSaiyan 0.834 0.777 17 19
Meerkat Mafia Hulk2 0.826 0.705 18 20
RTM-DCU run2 0.747 0.588 0.164 19 22
FBK-TR run3 0.759 0.702 20 23
FBK-TR run1 0.751 0.685 21 24
FBK-TR run2 0.770 0.648 22 25
Duluth Duluth2 0.501 0.450 0.241 0.219 23 21
AI-KU run1 0.732 0.680 24 26
LCS Baseline 0.527 0.562 0.165 0.109
UNAL-NLP run3 0.708 0.620 25 27
AI-KU run2 0.698 0.617 26 28
TCDSCSS run2 0.607 0.552 27 29
JU-Evora run1 0.536 0.442 0.090 0.091 28 31
TCDSCSS run1 0.575 0.541 29 30
Duluth Duluth1 0.458 0.440 0.075 0.076 30 5
Duluth Duluth3 0.455 0.426 0.075 0.079 31 3
OPI run1 0.433 0.213 0.152 32 36
SSMT run1 0.789 33 34
DIT run1 0.785 34 32
DIT run2 0.784 35 33
UMCC DLSI SelSim run1 0.760 36 35
UMCC DLSI SelSim run2 0.698 37 37
UMCC DLSI Prob run1 0.023 38 38
Table 5: Task results. Systems marked with a ? were submitted after the deadline but are positioned
where they would have ranked.
been released. These systems were scored and
noted in the results but were not included in the
official ranking.
Table 5 shows the performance of the participat-
ing systems across all the four comparison types in
terms of Pearson correlation. The two right-most
columns show system rankings by Pearson (Offi-
cial Rank) and Spearman?s ranks correlation.
The SimCompass system attained first place,
partially due to its superior performance on
phrase-to-word comparisons, providing an im-
provement of 0.10 over the second-best sys-
tem. The late-submitted version of the Meerkat
Mafia pairingWords? system corrected a bug in
the phrase-to-word comparison, which ultimately
would have attained first place due to large per-
formance improvements over SimCompass on
phrase-to-word and word-to-sense. ENCU and
UNAL-NLP systems rank respectively second and
third while the former being always in top-4 and
the latter being among the top-7 systems across the
four comparison types. Most systems were able
to surpass the naive LCS baseline; however, the
more sophisticated GST baseline (which accounts
for text transposition) outperforms two-thirds of
the systems. Importantly, both baselines perform
23
poorly on smaller text, highlighting the impor-
tance of performing a semantic comparison, as op-
posed to a string-based one.
Within the individual comparison types, spe-
cialized systems performed well for the larger
text sizes. In the paragraph-to-sentence type, the
run1 system of UNAL-NLP provides the best of-
ficial result, with the late RTM-DCU run1? sys-
tem surpassing its performance slightly. Meerkat
Mafia provides the best performance in sentence-
to-phrase with its SuperSaiyan system and the
best performances in phrase-to-word and word-to-
sense with its late pairingWords? system.
Comparison-Type Analysis Performance
across the comparison types varied considerably,
with systems performing best on comparisons
between longer textual items. As a general trend,
both the baselines? and systems? performances
tend to decrease with the size of lexical items
in the comparison types. A main contributing
factor to this is the reliance on textual similarity
measures (such as the baselines), which perform
well when two items? may share content. How-
ever, as the items? content becomes smaller, e.g.,
a word or phrase, the textual similarity does not
necessarily provide a meaningful indication of
the semantic similarity between the two. This
performance discrepancy suggests that, in order
to perform well, CLSS systems must rely on
comparisons between semantic representations
rather than textual representations. The two
top-performing systems on these smaller levels,
Meerkat Mafia and SimCompass, used additional
resources beyond WordNet to expand a word or
sense to its definition or to represent words with
distributional representations.
Per-genre results and discussions Task 3 in-
cludes multiple genres within the data set for each
comparison type. Figure 2 shows the correlation
of each system for each of these genres, with sys-
tems ordered left to right according to their official
ranking in Table 5. An interesting observation is
that a system?s official rank does not always match
the rank from aggregating its correlations for each
genre individually. This difference suggests that
some systems provided good similarity judgments
on individual genres, but their range of similarity
values was not consistent between genres leading
to lower overall Pearson correlation. For instance,
in the phrase-to-word comparison type, the ag-
gregated per-genre performance of Duluth-1 and
Duluth-3 are among the best whereas their over-
all Pearson performance puts these systems among
the worst-performing ones in the comparison type.
Among the genres, CQA, SLANG, and ID-
IOMATIC prove to be the more difficult for sys-
tems to interpret and judge. These genres in-
cluded misspelled, colloquial, or slang language
which required converting the text into semantic
form in order to meaningfully compare it. Fur-
thermore, as expected, the METAPHORIC genre
was the most difficult, with no system perform-
ing well; we view the METAPHORIC genre as an
open challenge for future systems to address when
interpreting larger text. On the other hand, SCI-
ENTIFIC, TRAVEL, and NEWSWIRE tend to be
the easiest genres for paragraph-to-sentence and
sentence-to-phrase. All three genres tend to in-
clude many named entities or highly-specific lan-
guage, which are likely to be more preserved in the
more-similar paired items. Similarly, DESCRIP-
TIVE and SEARCH genres were easiest in phrase-
to-word, which also often featured specific words
that were preserved in highly-similar pairs. In
the case of word-to-sense, REGULAR proves to be
the least difficult genre. Interestingly, in word-
to-sense, most systems attained moderate perfor-
mance for comparisons with words not in Word-
Net (i.e., OOV) but had poor performance for
slang words, which were also OOV. This differ-
ence suggests that systems could be improved with
additional semantic resources for slang.
Spearman Rank Analysis Although the goal of
Task 3 is to have systems produce similarity judg-
ments, some applications may benefit from simply
having a ranking of pairs, e.g., ranking summa-
rizations by goodness. The Spearman rank corre-
lation measures the ability of systems to perform
such a ranking. Surprisingly, with the Spearman-
based ranking, the Duluth1 and Duluth3 systems
attain the third and fifth ranks ? despite being
among the lowest ranked with Pearson. Both sys-
tems were unsupervised and produced similarity
values that did not correlate well with those of
humans. However, their Spearman ranks demon-
strate the systems ability to correctly identify rela-
tive similarity and suggests that such unsupervised
systems could improve their Pearson correlation
by using the training data to tune the range of sim-
ilarity values to match those of humans.
24
 0
 1
 2
 3
 4
 5
UNAL-NLP-2
ECNU-1
Meerkat_Mafia-SS
Meerkat_Mafia-H
SemantiKLUE-1
UNAL-NLP-1
SimCompass-1
BUAP-1
BUAP-2
Meerkat_Mafia-PW
SSMT-1
RTM-DCU-1
DIT-1DIT-2UNIBA-2
RTM-DCU-3
FBK-TR-2
UNIBA-1
UNIBA-3
FBK-TR-3
FBK-TR-1
RTM-DCU-2
AI-KU-1
UNAL-NLP-3
AI-KU-2
HULTECH-1
HULTECH-3
HULTECH-2
TCDSCSS-2
TCDSCSS-1
JU-Evora-1
Duluth-2
Duluth-1
Duluth-3
(a)
 Pa
rag
rap
h-t
o-S
en
ten
ce
  
Co
rre
lat
ion
s
pe
r g
en
re
CQAReview
TravelNewswire
ScientificMetaphoric
 0
 1
 2
 3
 4
 5
Meerkat_Mafia-SS
ECNU-1
UMCC_DLSI_SelSim-1
SemantiKLUE-1
SimCompass-1
UNAL-NLP-1
UNAL-NLP-2
UNIBA-2
UNIBA-1
UNIBA-3
BUAP-1
BUAP-2
Meerkat_Mafia-H
Meerkat_Mafia-PW
FBK-TR-3
UMCC_DLSI_SelSim-2
FBK-TR-1
AI-KU-1
RTM-DCU-3
HULTECH-3
RTM-DCU-1
HULTECH-1
FBK-TR-2
HULTECH-2
UNAL-NLP-3
AI-KU-2
RTM-DCU-2
TCDSCSS-2
TCDSCSS-1
Duluth-2
JU-Evora-1
Duluth-1
OPI-1
Duluth-3
(b)
 Se
nte
nc
e-t
o-P
hra
se
    
Co
rre
lat
ion
s
pe
r g
en
re
ScientificCQA
IdiomaticSlang
TravelNewswire
 0
 1
 2
 3
SimCompass-1
ECNU-1
UNAL-NLP-2
UNIBA-2
HULTECH-1
UNAL-NLP-1
Duluth-2
HULTECH-3
UNIBA-1
UNIBA-3
SemantiKLUE-1
OPI-1
RTM-DCU-3
HULTECH-2
RTM-DCU-1
RTM-DCU-2
BUAP-1
BUAP-2
JU-Evora-1
Duluth-1
Duluth-3
Meerkat_Mafia-PW
(c)
 Ph
ras
e-t
o-W
ord
    
  
Co
rre
lat
ion
s
pe
r g
en
re
SlangNewswire
IdiomaticDescriptive
LexicographicSearch
 0
 1
 2
Meerkat_Mafia-PW
SimCompass-1
SemantiKLUE-1
ECNU-1
UNAL-NLP-2
UNAL-NLP-1
Duluth-2
BUAP-1
BUAP-2
UNIBA-2
HULTECH-2
UNIBA-1
UNIBA-3
OPI-1
HULTECH-1
HULTECH-3
JU-Evora-1
Duluth-3
Duluth-1
UMCC_DLSI_Prob-1
(d)
 W
ord
-to
-Se
ns
e
Co
rre
lat
ion
s
pe
r g
en
re
OOSOOV
regularregular-challenge
Slang
Figure 2: A stacked histogram for each system, showing its Pearson correlations for genre-specific por-
tions of the gold-standard data, which may also be negative.
6 Conclusion
This paper introduces a new similarity task, Cross-
Level Semantic Similarity, for measuring the se-
mantic similarity of lexical items of different
sizes. Using a multi-phase annotation proce-
dure, we have produced a high-quality data set of
4000 items comprising of various genres, evenly-
split between training and test with four types of
comparison: paragraph-to-sentence, sentence-to-
phrase, phrase-to-word, and word-to-sense. Nine-
teen teams submitted 38 systems, with most teams
surpassing the baseline system and several sys-
tems achieving high performance in multiple types
of comparison. However, a clear performance
trend emerged where systems perform well only
when the text itself is similar, rather than its under-
lying meaning. Nevertheless, the results of Task 3
are highly encouraging and point to clear future
objectives for developing CLSS systems that op-
erate on more semantic representations rather than
text. In future work on CLSS evaluation, we first
intend to develop scalable annotation methods to
increase the data sets. Second, we plan to add new
evaluations where systems are tested according to
their performance in an application related to each
comparison-type, such as measuring the quality of
a paraphrase or summary.
Acknowledgments
We would like to thank Tiziano Flati, Marc Franco Salvador,
Maud Erhmann, and Andrea Moro for their help in preparing
the trial data; Gaby Ford, Chelsea Smith, and Eve Atkinson
for their help in generating the training and test data; and
Amy Templin for her help in generating and rating the train-
ing and test data.
The authors gratefully acknowledge the
support of the ERC Starting Grant Multi-
JEDI No. 259234.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval-2012), pages
385?393, Montr?eal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on typed-
similarity. In Proceedings of the Second Joint Confer-
25
ence on Lexical and Computational Semantics (*SEM),
Atlanta, Georgia.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational Lin-
guistics, 34(4):555?596.
Paul Clough and Mark Stevenson. 2011. Developing a cor-
pus of plagiarised short answers. Language Resources
and Evaluation, 45(1):5?24.
Mona Diab. 2013. Semantic textual similarity: past present
and future. In Joint Symposium on Semantic Process-
ing. Keynote address. http://jssp2013.fbk.eu/
sites/jssp2013.fbk.eu/files/Mona.pdf.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-
Burch. 2013. PPDB: The paraphrase database. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-HLT),
pages 758?764, Atlanta, Georgia.
Oren Glickman and Ido Dagan. 2003. Acquiring lexical
paraphrases from a single corpus. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing (RANLP), pages 81?90, Borovets,
Bulgaria.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Nancy Ide and K. Suderman. 2004. The American Na-
tional Corpus First Release. In Proceedings of the 4
th
Language Resources and Evaluation Conference (LREC),
pages 1681?1684, Lisbon, Portugal.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
Degrees of Relational Similarity. In Proceedings of
the 6th International Workshop on Semantic Evaluation
(SemEval-2012), pages 356?364, Montr?eal, Canada.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In The Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation Sys-
tems (SENSEVAL-2), pages 17?20, Toulouse, France.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. SemEval-2010 Task 5: Automatic
Keyphrase Extraction from Scientific Articles. In Pro-
ceedings of the 5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 21?26, Los Angeles,
California.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of Machine
Translation Summit X, pages 79?86, Phuket, Thailand.
Klaus Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology. Sage, Thousand Oaks, CA, sec-
ond edition.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben-
tivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014.
SemEval-2014 Task 1: Evaluation of compositional dis-
tributional semantic models on full sentences through se-
mantic relatedness and textual entailment. In Proceedings
of the 8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Julian John McAuley and Jure Leskovec. 2013. From ama-
teurs to connoisseurs: modeling the evolution of user ex-
pertise through online reviews. In Proceedings of the 22nd
International Conference on World Wide Web (WWW),
pages 897?908, Rio de Janeiro, Brazil.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (COLING-
ACL), pages 105?112, Sydney, Australia.
Mohammad Taher Pilehvar and Roberto Navigli. 2014a. A
large-scale pseudoword-based evaluation framework for
state-of-the-art Word Sense Disambiguation. Computa-
tional Linguistics, 40(4).
Mohammad Taher Pilehvar and Roberto Navigli. 2014b.
A robust approach to aligning heterogeneous lexical re-
sources. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, pages 468?
478, Baltimore, USA.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Karen Sp?arck Jones. 2007. Automatic summarising: The
state of the art. Information Processing and Management,
43(6):1449?1481.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval-2012), pages 347?355.
Michael J. Wise. 1996. YAP3: Improved detection of simi-
larities in computer program and other texts. In Proceed-
ings of the twenty-seventh SIGCSE technical symposium
on Computer science education, SIGCSE ?96, pages 130?
134, Philadelphia, Pennsylvania, USA.
26
