75
76
77
78
79
80
81
82
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?38,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Maximizing Component Quality in Bilingual Word-Aligned Segmentations
Spyros Martzoukos Christophe Costa Flor
?
encio Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.CostaFlorencio, C.Monz}@uva.nl
Abstract
Given a pair of source and target language
sentences which are translations of each other
with known word alignments between them,
we extract bilingual phrase-level segmenta-
tions of such a pair. This is done by identi-
fying two appropriate measures that assess the
quality of phrase segments, one on the mono-
lingual level for both language sides, and one
on the bilingual level. The monolingual mea-
sure is based on the notion of partition refine-
ments and the bilingual measure is based on
structural properties of the graph that repre-
sents phrase segments and word alignments.
These two measures are incorporated in a ba-
sic adaptation of the Cross-Entropy method
for the purpose of extracting an N -best list
of bilingual phrase-level segmentations. A
straight-forward application of such lists in
Statistical Machine Translation (SMT) yields
a conservative phrase pair extraction method
that reduces phrase-table sizes by 90% with
insignificant loss in translation quality.
1 Introduction
Given a pair of source and target language sen-
tences which are translations of each other with
known word alignments between them, the problem
of extracting high quality bilingual phrase segmen-
tations is defined as follows: Maximize the quality
of phrase segments, i.e., groupings of consecutive
words, in both language sides, subject to constraints
imposed by the underlying word alignments. The
purpose of this work is to provide a solution to this
maximization problem and investigate the effect of
the resulting high quality bilingual phrase segments
on SMT. For brevity, ?phrase-level sentence segmen-
tation? and ?phrase segment? will henceforth be sim-
ply referred to as ?segmentation? and ?segment? re-
spectively.
The exact definition of segments? quality depends
on the application. Our notion of a segmentation of
maximum quality is defined as the set of consecutive
words of the sentence that captures maximum col-
locational and/or grammatical characteristics. This
implies that a sequence of tokens is identified as a
segment if its fully compositional expressive power
is higher than the expressive power of any combina-
tion of partial compositions. Since this definition is
fairly general it is thus suitable for most NLP tasks.
In particular, it is tailored to the type of segments
that are suitable for the purposes of SMT and is in
line with previous work (Blackwood et al., 2008;
Paul et al., 2010).
With this definition in mind, we introduce a
monolingual segment quality measure that is based
on assessing the cost of converting one segmentation
into another by means of an elementary operation.
This operation, namely the ?splitting? of a segment
into two segments, together with all possible seg-
mentations of a sentence are known to form a par-
tially ordered set (Guo, 1997). Such a construction
is known as partition refinement and gives rise to the
desired monolingual surface quality measure.
The presence of word alignments between the
sentence pair provides additional structure which
should not be ignored. In the language of graph the-
ory, a segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
30
an edge between two words exists if and only if these
words are consecutive. Then, a bilingual segmenta-
tion is represented by the graph that is formed by all
its source and target language chains together with
edges induced by word alignments. Motivated by
the phrase pair extraction methods of SMT (Och et
al., 1999; Koehn et al., 2003), we focus on the con-
nected components, or simply components of such a
representation. We explain that the extent to which
we can delete word alignments from a component
without violating its component status, gives rise to
a bilingual, purely structural quality measure.
The surface and structural measures are incorpo-
rated in one algorithm that extracts an N -best list
of bilingual word-aligned segmentations. This algo-
rithm, which is an adaptation of the Cross-Entropy
method (Rubinstein, 1997), performs joint maxi-
mization of surface (in both languages) and struc-
tural quality measures. Components of graph repre-
sentations of the resulting N -best lists give rise to
high quality translation units. These units, which
form a small subset of all possible (continuous) con-
sistent phrase pairs, are used to construct SMT mod-
els. Results on Czech?English and German?English
datasets show a 90% reduction in phrase-table sizes
with insignificant loss in translation quality which
are in line with other pruning techniques in SMT
(Johnson et al., 2007; Zens et al., 2012).
2 Monolingual Surface Quality Measure
Given a sentence s
1
s
2
...s
k
that consists of words
s
i
, 1 ? i ? k, we introduce an empirical count-
based measure that assesses the quality of its seg-
mentations. By fixing a segmentation ?, we are in-
terested in assessing the cost of perturbing ? and
generating another segmentation ?
?
. A perturbation
of ? is achieved by splitting a segment of ? into
two new segments, while keeping all other segments
fixed. For example, for a sentence with five words, if
? : (s
1
s
2
)(s
3
s
4
s
5
), where brackets are used to dis-
tinguish the segments s
1
s
2
and s
3
s
4
s
5
, then ? can
be perturbed in three different ways:
? ?
?
: (s
1
)(s
2
)(s
3
s
4
s
5
), by splitting the first seg-
ment of ?.
? ?
??
: (s
1
s
2
)(s
3
)(s
4
s
5
), by splitting at the first
position of the second segment of ?.
? ?
???
: (s
1
s
2
)(s
3
s
4
)(s
5
), by splitting at the sec-
ond position of the second segment of ?,
so that ?
?
, ?
??
and ?
???
are the perturbations of ?.
Such perturbations are known as partition refine-
ments in the literature (Stanley, 1997). The set of all
segmentations of a sentence, equipped with the split-
ting operation forms a partially ordered set (Guo,
1997), and its visual representation is known as the
Hasse diagram. Figure 1 shows such a partially or-
dered set for a sentence with four words.
  
?s1 s2 s3 s4??s1??s2 s3 s4? ? s1 s2?? s3 s4? ?s1 s2 s3??s4?
?s1??s2?? s3 s4? ?s1??s2 s3?? s4? ?s1 s2??s3?? s4??s1??s2?? s3??s4?
Figure 1: Hasse diagram of segmentation refine-
ments for a sentence with four words.
The cost of perturbing a segmentation into an-
other, i.e., the weight of a directed edge in the Hasse
diagram, is calculated from n-gram counts that are
extracted from a monolingual training corpus. Let
n(s) be the empirical count of phrase s in the corpus.
Given a segmentation ? of a sentence, let seg(?) de-
note the set of ??s segments. In the above example
we have for instance seg(?
??
) = {s
1
s
2
, s
3
, s
4
s
5
}.
The probability of s in ? is given by relative fre-
quencies
p
?
(s) =
n(s)
?
s
?
?seg(?)
n(s
?
)
. (1)
The cost of perturbing ? into ?
?
by splitting a seg-
ment ss? of ? into segments s and s? is defined by
cost
???
?
(s, s?) = log
p
?
(ss?)
p
?
?
(s)p
?
?
(s?)
, (2)
and we say that s and s? are co-responsible for the
perturbation ? ? ?
?
. Intuitively, this cost function
yields the amount of energy (log of probability) that
is lost when performing a perturbation. On a more
31
technical level, it is closely related to metric spaces
on partially ordered sets (Monjardet, 1981; Orum
and Joslyn, 2009), but we do not go into further de-
tails here.
The cost function admits a measure for the seg-
ments that are co-responsible for perturbing ? into
?
?
and we define the gain of s from the perturbation
? ? ?
?
as
gain
???
?
(s) = ?cost
???
?
(s, s?). (3)
A segment smay be co-responsible for different per-
turbations, and we have to consider all such pertur-
bations. Let
R(s) = {? ? ?
?
: s /? seg(?), s ? seg(?
?
)} (4)
denote the set of perturbations for which s is co-
responsible. Then, the average gain of s in the sen-
tence is given by
gain(s) =
1
|R(s)|
?
{???
?
}?R(s)
gain
???
?
(s). (5)
Intuitively, gain(s) measures how difficult it is to
break phrase s into sub-phrases. Finally, the surface
quality measure of a segmentation ? of a sentence is
given by
g(?) =
?
s?seg(?)
gain(s). (6)
Note that g is a real number. The relation g(?) >
g(?
?
) implies that ? is a better segmentation than ?
?
.
We conclude this section with two remarks: (i)
The exact computation of gain(s) for each possi-
ble segment s is computationally expensive since
all perturbations need to be considered. In prac-
tice we can simply generate a random sample of no
more than 1500 segmentations and compute gain(?)
based on that sample only. (ii) Each sentence of
the monolingual training corpus (from which the n-
gram counts are extracted) should have the begin-
ning and end-of-sentence tokens. The count for each
of them is equal to the number of sentences in the
corpus, and they are treated as regular words. With-
out going into further details they provide the pur-
pose of normalization.
3 Bilingual Structural Quality Measure
Given a word-aligned sentence pair, we introduce a
purely structural measure that assesses the quality of
its bilingual segmentations. By ?purely structural?
we mean that the focus is entirely on combinatorial
aspects of the bilingual segmentations and the word
alignments. For that reason we turn to a graph theo-
retic framework.
A segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
an edge between two words exists if and only if these
words are consecutive. Then, a source segmentation
? and a target segmentation ? are graphs that con-
sist of source chains and target chains respectively.
The graph formed by ?, ? and the translation edges
induced by word alignments is thus a graph repre-
sentation of a bilingual word-aligned segmentation.
We focus on a particular type of subgraphs of this
representation, namely its connected components, or
simply components. A component is a graph such
that (a) there exists a path between any two of its
vertices, and (b) there does not exist a path between
a vertex of the component and a vertex outside the
component. Condition (a) means, both technically
and intuitively, that a component is connected and
Condition (b) requires connectivity to be maximal.
Components play a key role in SMT. The most
widely used strategy for extracting high quality
phrase-level translations without linguistic informa-
tion, namely the consistency method (Och et al.,
1999; Koehn et al., 2003) is entirely based on com-
ponents of word aligned unsegmented sentence pairs
(Martzoukos et al., 2013). In particular, each ex-
tracted translation is either a component or the union
of components. Since an unsegmented sentence
pair is just one possible configuration of all possi-
ble bilingual segmentations, we consequently have
no direct reason to investigate further than compo-
nents.
In order to get an intuition of the measure that will
be introduced in this section, we begin with an ex-
ample. Figure 2, shows two different configurations
of the pair (?, ?) for the same sentence pair with
known and fixed word alignments. Both configu-
rations have the same number of edges that connect
source vertices (3) and the same number of edges
that connect target vertices (2). However, one would
32
  
1 2 3 4 ? ? ?1 2 3 4 ? ?1 2 3 4 ? ? ?1 2 3 4 ? ?
Figure 2: Graph representations of two bilingual
segmentations with fixed word alignments. Source
and target vertices are shown with circles and
squares respectively.
expect the top configuration to represent a better
bilingual segmentation. This is because it has more
components (4 opposed to 2 for the bottom config-
uration) and because it consists of ?tighter? clusters,
i.e., ?tighter? components.
A general measure that would capture this obser-
vation requires a balance between the number of
edges of source and target chains, the number of
components and the number of translation edges, all
coupled with how these edges and vertices are con-
nected. This might seem as a daunting task that can
be tackled with a combination of heuristics, but there
is actually a graph-theoretic measure that can fully
describe the sought structure. We proceed with in-
troducing this measure.
Let C denote the set of components of the graph
representation of a bilingual word-aligned segmen-
tation. We are interested in measuring the extent to
which we can delete translation edges from c ? C,
while retaining its component status. Let a
c
denote
the subset of translation edges that are restricted to
the component c. We define the positive integer
gain(c) = number of ways of
deleting translation edges from a
c
,
while keeping c connected, (7)
where the option of deleting nothing is counted. In-
tuitively, by keeping the edges of the chains fixed
the quantity gain(c) measures how difficult it is to
perturb a component from its connected state to a
disconnected state.
Figure 3 shows two components c and c
?
that sat-
isfy gain(c) = gain(c
?
) = 3. Both components
are equally difficult to be perturbed into a discon-
nected state, but only superficially. The actual struc-
tural quality of c is revealed when it is ?compared? to
component c? that consists of the same source and tar-
get vertices, the same translation edges but its source
vertices form exactly one chain and similarly for its
target vertices; c? is essentially the ?upper bound? of
c. In general, the maximum value of gain(c), with
  
cc '?c
Figure 3: Superficially similar components c and c
?
.
Comparing c with c? yields c?s true structural quality.
respect to a fixed set of source and target vertices
and translation edges, is attained when it consists
of exactly one source chain and exactly one target
chain. It is not difficult to see that the desired max-
imum value is always 2
|a
c
|
? 1. In the example of
Figure 3, the structural quality of c and c
?
is thus
3/(2
5
? 1) = 9.7% and 3/(2
2
? 1) = 100% respec-
tively. Hence, the measure that evaluates the struc-
tural quality of a bilingual word-aligned segmenta-
tion (?, ?) is given by
f(?, ?) =
(
?
c?C
gain(c)
2
|a
c
|
? 1
)
1
|C|
, (8)
which takes values in (0, 1]. The relation f(?, ?) >
f(?
?
, ?
?
) implies that (?, ?) is a better bilingual seg-
mentation than (?
?
, ?
?
).
We conclude this section with two remarks: (i) A
component with no translation edges, i.e., a source
or target segment whose words are all unaligned, has
a contribution of 1/0 in (8). In practice we exclude
such components from C. (ii) In graph theory the
quantity gain(c) is known as the number of con-
nected spanning subgraphs (CSSGs) of graph c and
is the key quantity of network reliability (Valiant,
1979; Coulbourn, 1987). Finding the number of
CSSGs of a general graph is a known #P-hard prob-
lem (Welsh, 1997). In our setting, graphs have spe-
cific formation (source and target chains connected
via translation edges) and we are interested in the
deletion of translation edges only; it is possible to
33
compute gain(?) in polynomial time, but we do not
go into further details here.
4 Extracting Bilingual Segmentations with
the Cross-Entropy Method
Equipped with the measures of Sections 2 and 3 we
turn to extracting anN -best list of bilingual segmen-
tations for a given sentence pair. The search space is
exponential in the total number of words of the sen-
tence pair. We propose a new approach for this task,
by noting a direct connection with the combinato-
rial problems that can be solved efficiently and ef-
fectively with the Cross-Entropy (CE) method (Ru-
binstein, 1997).
The CE method is an iterative self-tuning sam-
pling method that has applications in various com-
binatorial and continuous global optimization prob-
lems as well as in rare event detection. A detailed
account on the CE method is beyond the scope of
this work, and we thus simply describe its applica-
tion to our problem.
In particular, we first establish the connection be-
tween the most basic form of the CE method and the
problem of finding the best monolingual segmen-
tation of a sentence, with respect to some scoring
function (not necessarily the one that was introduced
in Section 2). This connection yields a simple, ef-
ficient and effective algorithm for the monolingual
maximization problem. Then, the transition to the
bilingual level is done by incorporating the measure
of Section 3 in the algorithm, thus performing joint
maximization of surface and structural quality. Fi-
nally, the generation of theN -best list will be trivial.
A segmentation of a given sentence has a bit-
string representation in the following way: If two
consecutive words in the sentence belong to the
same segment in the segmentation, then this pair of
words is encoded by ?1?, otherwise by ?0?. Such a
representation is bijective and, thus, for the rest of
this section, we do not distinguish between a seg-
mentation and its bit-string representation. In this
setting, the CE method takes its most basic form
(De Boer et al., 2005). In a nutshell, it is a re-
peated application of (a) sampling bit-strings from
a parametrized probability mass function, (b) scor-
ing them and keeping only a small high-performing
subsample, and (c) updating the parameters of the
probability mass function based on that subsample
only.
We assume no prior knowledge on the quality
of bit-strings, so that they are all equally likely. In
other words, each position of a randomly chosen
bit-string can be either a ?0? or a ?1? with probability
1/2. The aim is to tune these position probabilities
towards the best bit-string, with respect to some
scoring function g. In particular, let the sentence
have n words and let ` = n ? 1 be the length of
bit-strings. A bit-string labeled by an integer i is
denoted by b
i
and its jth bit by b
ij
. The algorithm is
as follows:
0. Initialize the bit-string position probabilities
p
0
= (p
0
1
, ..., p
0
`
) = (1/2, ..., 1/2) and set M = 20`
(sample size), ? = d1%Me (keep top 1% of
samples), ? = 0.7 (smoothing parameter) and t = 1
(iteration).
1. Generate a sample b
1
, ..., b
M
of bit-strings, each
of length `, such that b
ij
?Bernoulli(p
t?1
j
), for all
i = 1, ...,M and j = 1, ..., `.
1.1 Compute scores g(b
1
), ..., g(b
M
).
1.2 Order them descendingly as g(b
pi(1)
) > ... >
g(b
pi(M)
).
2. Focus on the best performing ones: Compute
?
t
= g(b
pi(?)
); samples performing less than this
threshold will be ignored.
3. Use the best performing sub-sample of b
1
, ..., b
M
to update position probabilities:
p
t
j
=
?
M
i=1
I
i
(?
t
)b
ij
?
M
i=1
I
i
(?
t
)
, j = 1, ..., `, (9)
where the choice function I
i
is given by
I
i
(?
t
) =
{
1, if g(b
i
) > ?
t
0, otherwise.
4. Smooth the updated position probabilities as
p
t
j
:= ?p
t
j
+ (1 ? ?)p
t?1
j
, j = 1, ..., `. (10)
E. If for some t > 5we have ?
t
= ?
t?1
= ... = ?
t?5
then stop. Else, t := t + 1 and go to Step 1.
34
The values for the parameters M , ? and ? re-
ported here are in line with the ones suggested in the
literature (Rubinstein and Kroese, 2004) for combi-
natorial problems such as this one. After the execu-
tion of the algorithm, the updated vector of position
probabilities converges to sequence of ?0?s and ?1?s,
which corresponds to the best segmentation under g.
The extension to bilingual level is done by incor-
porating the structural quality measure of Section 3.
The setting is similar, i.e., samples are again bit-
strings, but of length ` = n + m ? 2, where n and
m are the number of words in the source and tar-
get sentence respectively. The first n ? 1 bits corre-
spond to the source sentence and the rest to the target
sentence. The surface quality score of such a bit-
string is given by the harmonic mean of its source
and target surface quality scores.
1
The bit-string
scoring function throughout Steps 1 ? 3 is given by
the harmonic mean of surface and structural quality
scores. Finally, N -best lists are trivially generated,
simply by collecting the top-N performing accumu-
lated samples of a maximization process.
5 Experiments
Given a sentence pair with known and fixed word
alignments, the result of the method described in
Section 4 is an N -best list of bilingual segmenta-
tions of such a pair. The objective function provides
a balance between compositional expressive power
of segments in both languages and synchronization
via word alignments. Thus, each (continuous) com-
ponent of such a bilingual segmentation leads to the
extraction of a high quality phrase pair.
As was mentioned in Section 3, each extracted
phrase pair of standard phrase-based SMT is con-
structed from a component or from the union of
components of an unsegmented word-aligned sen-
tence pair. For each sentence pair, all possible
(continuous) components and (continuous) unions
of components give rise to the extracted (contin-
uous) phrase pairs. In this section we investigate
the impact to SMT models and translation quality,
when extracting phrase pairs (from the N -best lists)
1
As it was mentioned in Section 2 the surface quality score
in (6) is a real number. At each iteration of the algorithm the
surface score of a segmentation can be converted into a number
in [0, 1] via Min-Max normalization. This holds for both source
and target sides of a bit-string (independently).
Cz?En De?En
Europarl (v7) 642,505 1,889,791
News Commentary (v8) 139,679 177,079
Total 782,184 2,066,870
Table 1: Number of filtered parallel sentences for
Czech?English and German?English.
that correspond to components only. A reduction
in phrase-table size is guaranteed because we are
essentially extracting only a subset of all possible
continuous phrase pairs. The challenge is to verify
whether this subset can provide a sufficient transla-
tion model.
Both the baseline and our system are standard
phrase-basedMT systems. Bidirectional word align-
ments are generated with GIZA++ (Och and Ney,
2003) and ?grow-diag-final-and?. These are used
to construct a phrase-table with bidirectional phrase
probabilities, lexical weights and a reordering model
with monotone, swap and discontinuous orienta-
tions, conditioned on both the previous and the next
phrase. 4-gram interpolated language models with
Kneser-Ney smoothing are built with SRILM (Stol-
cke, 2002). A distortion limit of 6 and a phrase-
penalty are also used. All model parameters are
tuned with MERT (Och, 2003). Decoding during
tuning and testing is done with Moses (Koehn et. al,
2007). Since our system only affects which phrases
are extracted, lexical weights and reordering orien-
tations are the same for both systems.
Datasets are from the WMT?13 translation task
(Bojar et al., 2013): Translation and reordering
models are trained on Czech?English and German?
English corpora (Table 1). Language models and
segment measures gain , as defined in (5), are trained
on 35.3M Czech, 50.0M German and 94.5M En-
glish sentences from the provided monolingual data.
Tuning is done on newstest2010 and performance
is evaluated on newstest2008, newstest2009, new-
stest2011 and newstest2012 with BLEU (Papineni
et al., 2001).
In our experiments the size of anN -best list varies
according to the total number of words in the sen-
tence pair, say w. For the purposes of phrase ex-
traction in SMT we would ideally require all local
maxima to be part of an N -best list. This would
35
Method
Czech?English English?Czech Czech?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 19.6 20.6 22.6 20.6 14.8 15.6 16.6 14.9 44.6M (100%)
N -best 19.7 20.4 22.4 20.3 14.4 15.2 16.3 14.3 4.4M (9.8%)
N -best & unseg. 19.6 20.5 22.6 20.7 14.6 15.4 16.8 14.7 4.6M (10.4%)
Table 2: BLEU scores and phrase-table (PT) sizes for Czech?English. Phrase-table of ?Baseline? is con-
structed from all consistent phrase pairs. Phrase-table of ?N -best? is constructed from consistent phrase
pairs that are components of the top-N bilingual word-aligned segmentations of each sentence pair. Simi-
larly for ?N -best & unseg.?, but consistent phrase pairs that are components of each (unsegmented) sentence
pair are also included.
Method
German?English English?German German?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 21.4 20.8 21.3 22.1 15.1 15.1 16.0 16.5 102.3M (100%)
N -best 21.3 20.6 21.3 21.8 15.0 15.0 15.6 16.0 9.4M (9.2%)
N -best & unseg. 21.5 20.8 21.5 22.0 15.4 15.2 15.7 16.2 9.9M (9.7%)
Table 3: Similar to Table 2, but for German?English.
guarantee the extraction of all high quality phrase
pairs, with (empirically) desired variations, while
keeping N small. Since the CE method performs
global optimization, the resulting members of an N -
best list are in the vicinity of the global maximum.
Consequently, we cannot guarantee the inclusion of
local maxima. We set N = d30%we so that at
least some variation from the global maximum is in-
cluded, but is not large enough to contaminate the
lists with noisy bilingual segmentations. The result-
ing lists have 22 bilingual segmentations on aver-
age for both language pairs. Figure 4 shows typical
German?English best performing bilingual segmen-
tations.
BLEU scores are reported in Tables 2 and 3 for
Czech?English and German?English respectively.
Methods ?Baseline? and ?N -best? are the ones de-
scribed above. Phrase-table sizes are reduced as
expected and performance when translating to En-
glish is comparable. The significant drops in new-
stest2012 when translating from the morphologi-
cally poorer language (English) prompts us to in-
clude more ?basic? phrase pairs in the phrase-tables.
This leads to augmenting each N -best list by its un-
segmented sentence pair. Consequently, method ?N -
best & unseg.? extracts the same phrase pairs as ?N -
best?, together with those from components of the
unsegmented sentence pairs. As a result, transla-
tion quality is comparable to ?Baseline? across all
language directions and small phrase-table sizes are
retained.
6 Discussion and Future Work
This work can also be viewed as an attempt to un-
derstand bilinguality as a generalization of mono-
linguality. There is conceptual common ground on
what gain(x) for phrase x (Section 2) or component
x (Section 3) computes. In both cases it measures
how ?stable? a unit is. The stability of a phrase x is
determined by how difficult it is to split x into multi-
ple phrases. The partially ordered set framework of
partition refinements is the natural setting for such
computations. In order to determine the stability
of a component we turn to empirical evidence from
SMT: ?good? phrase pairs are extracted from com-
ponents or unions of components of the graph that
represents word-aligned sentence pairs. The stabil-
ity of a component x is therefore determined by how
difficult it is to break x into multiple components. It
is thus interesting to investigate whether there exists
a general approach that unifies partition refinements
and network reliability for the purpose of identifying
highly stable multilingual units.
36
  
12 34??? ? ? ? ? 34 ??4 ?? ?? ?? ?2??? ?3? ? ?
3?? ? ? ? ?34??4 ?? ? ?
?? ? ? 2?4??
?? ?? ??4?41?
14 ?? ? 3?? ?23?? ? ? ? ? ? ? ? ? ? ? ? ? ?3? 3 ?? ? ? ? ?3? ? ?
?4??? ? ?34?14? 3???4? ?? ? ? ? ? ? ?
?3? ?? ? ? ?3? ?? ? ? ? ? ?4 12 432?4 ?? ?221??1?4
2? ?? ? ?4 ?3? ?? ? ? ? ? ? ? ?4 ??3?? ? ? ? ? ? 4?1??221?
14 ???34???4??4 ??1??? ?4 ?? ? ? ? ? ?1? ?4? ?? ? ? ? ? ? ?1?? ? ?4 ?41?4
4??? ?13?1?4? ?? ? ? ? ?4 ??1?? ? ? ?34? 34? ?? ? ? ? ? ? ? ?34 ?41?4 ???? ? ? ? ? 14
??? ? ?? ?1?421?1???1??? ?32?1?3?
?? ? ? ? ? ?4??21?1?3??32?1?3??
?? ?
?? ?4?
3???? ? ? ?4??? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ? ? ? ? ?4?4???? ?1??? ?4??1?
?? ? ? ??? ? ? ??? ?3143?????? ?2?4?3???34???? ? ? ?2?4??? ??4
??44?4
?34
?3???? ? ?4?1??? ? ? ? ? ? ?4?1??1??3??421??1?
??3???? ? ?? ?3???? ??1??? ? ?? ? ?2??? ? ?? ? ?
?
?? ?
3?
3???11?????? ? ?2???3?1????1?1??21??
??3412???34?2??1????2???3?1??? ? ??3??3213??4???4??
Figure 4: Typical fragments from best performing
German?English segmentations.
The focus has been on bilingual segmentations,
but as was mentioned in Section 2, it is possible
to apply the CE method for generating monolingual
segmentations. By using (6) as the objective func-
tion, we observed that the resulting segmentations
yield promising applications in n-gram topic model-
ing, named entity recognition and Chinese segmen-
tation. However, in the spirit of Ries et al. (1996),
attempts to minimize perplexity instead of maximiz-
ing (6), resulted in larger segments and the segment
quality definition of Section 1 was not met.
The sizes of the resulting phrase-tables together
with the type of phrase pairs that are extracted lead
to applications involving discontinuous phrase pairs.
In (Galley and Manning, 2010) there was evidence
that discontinuous phrase pairs that are extracted
from discontinuous components of word-aligned
sentence pairs can improve translation quality.
1
As
the number of such components is much bigger than
the continuous ones, (Gimpel and Smith, 2011) pro-
pose a Bayesian nonparametric model for finding the
most probable discontinuous phrase pairs. This can
also be done from the N -best lists that are generated
in Section 4, and it would be interesting to see the
effect of such phrase pairs in our existing models.
In a longer version of this work we intend to
study the effect in translation quality when varying
some of the parameters (size of N -best lists, sample
sizes for training gain in Section 2 and for the CE
method), as well as when extracting source-driven
bilingual segmentations as in (Sanchis-Trilles et al.,
2011).
7 Conclusions
In this work, we have presented a solution to the
problem of extracting bilingual segmentations in the
presence of word alignments. Two measures that as-
sess the quality of bilingual segmentations based on
the expressive power of segments in both languages
and their synchronization via word alignments have
been introduced. We have established the link be-
tween the CE method and finding the best monolin-
gual and bilingual segmentations. These measures
formed the objective function of the CE method
whose maximization resulted in an N -best list of
bilingual segmentations for a given sentence pair.
By extracting only phrase pairs that correspond to
components from bilingual segmentations of those
lists, we found that phrase table sizes can be reduced
with insignificant loss in translation quality.
Acknowledgements
This research was funded in part by the Euro-
pean Commission through the CoSyne project FP7-
ICT-4-248531 and the Netherlands Organisation
for Scientific Research (NWO) under project nr.
639.022.213.
1
By ?discontinuous component? we mean a component
whose source or target words (vertices) form a discontinuous
substring in the source or target sentence respectively.
37
References
Graeme Blackwood, Adria de Gispert, and William
Byrne. 2008. Phrasal Segmentation Models for Sta-
tistical Machine Translation. In COLING.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp Koehn,
Christof Monz, Matt Post, Radu Soricut, and Lucia
Specia. 2013. Findings of the 2013 Workshop on Sta-
tistical Machine Translation. In WMT.
Charlie J. Coulbourn. 1987. The Combinatorics of Net-
work Reliability. Oxford University Press.
Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and
Reuven Y. Rubinstein. 2005. A Tutorial on the Cross-
Entropy Method. Annals of Operations Research,
vol. 134, pages 19?67.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
NAACL.
Kevin Gimpel and Noah A. Smith. 2011. Generative
Models of Monolingual and Bilingual Gappy Patterns.
In WMT.
Jin Guo. 1997. Critical Tokenization and its Properties.
Computational Linguistics, vol. 23(4), pages 569?596.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by discard-
ing most of the phrase-table. In EMNLP-CoNLL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?rej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL,
demonstration session.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL.
Spyros Martzoukos, Christophe Costa Flor?encio, and
Christof Monz. 2013. Investigating Connectivity and
Consistency Criteria for Phrase Pair Extraction in Sta-
tistical Machine Translation. In Meeting on Mathe-
matics of Language.
Bernard Monjardet. 1981. Metrics on partially ordered
sets ? a survey. Discrete Mathematics, vol. 35, pages
173?184.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, vol. 29 (1), pages 19?51.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In EMNLP-VLC.
Chris Orum and Cliff A. Joslyn. 2009. Valuations and
Metrics on Partially Ordered Sets. Computing Re-
search Repository - CORR, vol. abs/0903.2.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2010.
Integration of Multiple Bilingually-Learned Segmen-
tation Schemes into Statistical Machine Translation.
In WMT and MetricsMATR.
Klaus Ries, Finn Dag Bu, and Alex Waibel. 1996. Class
phrase models for language modeling. In ICSLP.
Reuven Y. Rubinstein. 1997. Optimization of Computer
Simulation Models with Rare Events. European Jour-
nal of Operations Research, vol. 99, pages 89?112.
Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The
Cross-Entropy Method: A Unified Approach to Com-
binatorial Optimization, Monte-Carlo Simulation and
Machine Learning. Springer-Verlag, New York.
Germ?an Sanchis-Trilles, Daniel Ortiz-Mart??nez, Jes?us
Gonz?alez-Rubio, Jorge Gonz?alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in Statistical Machine Translation.
In EAMT.
Richard P. Stanley. 1997. Enumerative Combinatorics,
Volume 1. Cambridge University Press.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In ICSLP.
Leslie G. Valiant. 1979. The complexity of enumeration
and reliability problems. SIAM Journal on Comput-
ing, vol. 8, pages 410?421.
Dominic J. A. Welsh. 1997. Approximate counting.
Surveys in Combinatorics, London Math. Soc. Lecture
Notes Ser., 241, pages 287?324.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
Systematic Comparison of Phrase Table Pruning Tech-
niques. In EMNLP.
38
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 93?101,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Investigating Connectivity and Consistency Criteria for Phrase Pair
Extraction in Statistical Machine Translation
Spyros Martzoukos, Christophe Costa Flore?ncio and Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.Monz}@uva.nl, chriscostafl@gmail.com
Abstract
The consistency method has been estab-
lished as the standard strategy for extract-
ing high quality translation rules in statis-
tical machine translation (SMT). However,
no attention has been drawn to why this
method is successful, other than empiri-
cal evidence. Using concepts from graph
theory, we identify the relation between
consistency and components of graphs that
represent word-aligned sentence pairs. It
can be shown that phrase pairs of interest
to SMT form a sigma-algebra generated
by components of such graphs. This con-
struction is generalized by allowing seg-
mented sentence pairs, which in turn gives
rise to a phrase-based generative model. A
by-product of this model is a derivation of
probability mass functions for random par-
titions. These are realized as cases of con-
strained, biased sampling without replace-
ment and we provide an exact formula for
the probability of a segmentation of a sen-
tence.
1 Introduction
A parallel corpus, i.e., a collection of sentences in
a source and a target language, which are trans-
lations of each other, is a core ingredient of ev-
ery SMT system. It serves the purpose of training
data, i.e., data from which translation rules are ex-
tracted. In its most basic form, SMT does not re-
quire the parallel corpus to be annotated with lin-
guistic information, and human supervision is thus
restricted to the construction of the parallel corpus.
The extraction of translation rules is done by ap-
propriately collecting statistics from the training
data. The pioneering work of (Brown et al, 1993)
identified the minimum assumptions that should
be made in order to extract translation rules and
developed the relevant models that made such ex-
tractions possible.
These models, known as IBMmodels, are based
on standard machine learning techniques. Their
output is a matrix of word alignments for each sen-
tence pair in the training data. These word align-
ments provide the input for later approaches that
construct phrase-level translation rules which may
(Wu, 1997; Yamada and Knight, 2001) or may not
(Och et al, 1999; Marcu and Wong, 2002) rely on
linguistic information.
The method developed in (Och et al, 1999),
known as the consistency method, is a simple yet
effective method that has become the standard way
of extracting (source, target)-pairs of phrases as
translation rules. The development of consistency
has been done entirely on empirical evidence and
it has thus been termed a heuristic.
In this work we show that the method of (Och
et al, 1999) actually encodes a particular type of
structural information induced by the word align-
ment matrices. Moreover, we show that the way in
which statistics are extracted from the associated
phrase pairs is insufficient to describe the underly-
ing structure.
Based on these findings we suggest a phrase-
level model in the spirit of the IBM models. A key
aspect of the model is that it identifies the most
likely partitions, rather than alignment maps, asso-
ciated with appropriately chosen segments of the
training data. For that reason, we provide a gen-
eral construction of probability mass functions for
partitions and, in particular, an exact formula for
the probability of a segmentation of a sentence.
93
2 Definition of Consistency
In this section we provide the definition of consis-
tency, which was introduced in (Och et al, 1999),
refined in (Koehn et al, 2003), and we follow
(Koehn, 2009) in our description. We start with
some preliminary definitions.
Let S = s1...s|S| be a source sentence, i.e., a
string that consists of consecutive source words;
each word si is drawn from a source language vo-
cabulary and i indicates the position of the word
in S. The operation of string extraction from the
words of S is defined as the construction of the
string s = si1 ...sin from the words of S, with
1 ? i1 < ... < in ? |S|. If i1, ..., in are consecu-
tive, which implies that s is a substring of S, then
s is called a source phrase and we write s ? S.
As a shorthand we also write sini1 for the phrase
si1 ...sin . Similar definitions apply to the target
side and we denote by T, tj and t a target sen-
tence, word and phrase respectively.
Let (S = s1s2...s|S|, T = t1t2...t|T |) be a sen-
tence pair and letA denote the |S|?|T |matrix that
encodes the existence/absence of word alignments
in (S, T ) as
A(i, j) =
{
1, if si and tj are aligned
0, otherwise,
(1)
for all i = 1, ..., |S| and j = 1, ..., |T |. Un-
aligned words are allowed. A pair of strings (s =
si1 ...si|s| , t = tj1 ...tj|t|) that is extracted from
(S, T ) is termed consistent with A, if the follow-
ing conditions are satisfied:
1. s ? S and t ? T .
2. ?k ? {1, ..., |s|} such that A(ik, j) = 1, then
j ? {j1, ..., j|t|}.
3. ?l ? {1, ..., |t|} such that A(i, jl) = 1, then
i ? {i1, ..., i|s|}.
4. ?k ? {1, ..., |s|} and ?l ? {1, ..., |t|} such
that A(ik, jl) = 1.
Condition 1 guarantees that (s, t) is a phrase
pair and not just a pair of strings. Condition 2 says
that if a word in s is aligned to one or more words
in T , then all such target words must appear in t.
Condition 3 is the equivalent of Condition 2 for the
target words. Condition 4 guarantees the existence
of at least one word alignment in (s, t).
For a sentence pair (S, T ), the set of all consis-
tent pairs with an alignment matrix A is denoted
by P (S, T ). Figure 1(a) shows an example of a
sentence pair with an alignment matrix together
with all its consistent pairs.
In SMT the extraction of each consistent pair
(s, t) from (S, T ) is followed by a statistic
f(s, t;S, T ). Typically f(s, t;S, T ) counts the oc-
currences of (s, t) in (S, T ). By considering all
sentence pairs in the training data, the translation
probability is constructed as
p(t|s) =
?
(S,T ) f(s, t;S, T )
?
(S,T )
?
t? f(s, t
?;S, T )
, (2)
and similarly for p(s|t). Finally, the entries of the
phrase table consist of all extracted phrase pairs,
their corresponding translation probabilities and
other models which we do not discuss here.
3 Consistency and Components
For a given sentence pair (S, T ) and a fixed word
alignment matrixA, our aim is to show the equiva-
lence between consistency and connectivity prop-
erties of the graph formed by (S, T ) and A. More-
over, we explain that the way in which measure-
ments are performed is not compatible , in princi-
ple, with the underlying structure. We start with
some basic definitions from graph theory (see for
example (Harary, 1969)).
Let G = (V,E) be a graph with vertex set V
and edge set E. Throughout this work, vertices
represent words and edges represent word align-
ments, but the latter will be further generalized in
Section 4. A subgraph H = (V ?, E?) of G is a
graph with V ? ? V , E? ? E and the property
that for each edge in E?, both its endpoints are in
V ?. A path in G is a sequence of edges which con-
nect a sequence of distinct vertices. Two vertices
u, v ? V are called connected if G contains a path
from u to v. G is said to be connected if every pair
of vertices in G is connected.
A connected component, or simply component,
of G is a maximal connected subgraph of G. G
is called bipartite if V can be partitioned in sets
VS and VT , such that every edge in E connects a
vertex in VS to one in VT . The disjoint union of
graphs, or simply union, is an operation on graphs
defined as follows. For n graphs with disjoint ver-
tex sets V1, ..., Vn (and hence disjoint edge sets),
their union is the graph (?ni=1Vi,?
n
i=1Ei).
Consider the graph G whose vertices are the
words of the source and target sentences, and
whose edges are induced by the non-zero entries
94
  
t1 t2 t3 t4 t5 t6 t7s1s2s3s4s5 s1 s2
t1 t2 t3
s3 s4
t4 t5 t6
s5
t7s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7,{ }C1= G
s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7{ }C2= s1 s3t1 t4 t5 t6 s5t7 s2 s4t2 t3 t6 s2 s4t2 t3 s5t7s1 s3t1 t4 t5
s1 s3t1 t4 t5 s2 s4t2 t3 s5t7{ }C3= s1 s3t1 t4 t5 t6 s5t7s2 s4t2 t3 t6s2 s4t2 t3 s5t7s1 s3t1 t4 t5t6
s1 s3t1 t4 t5 s2 s4t2 t3{ }C 4= t6 s5t7
(s5 , t7) ,(s14 , t15) ,(s5 , t67) ,(s14 , t16) ,(S ,T )
(a)
(b)
P (S ,T )= { }
, ,, , , , ,, , ,
Figure 1: (a) Left: Sentence pair with an alignment matrix. Dots indicate existence of word alignments.
Right: All consistent pairs. (b) The graph representation of the matrix in (a), and the sets generated by
components of the graph. Dark shading indicates consistency.
of the matrix A. There are no edges between
any two source-type vertices nor between any two
target-type vertices. Moreover, the source and tar-
get language vocabularies are assumed to be dis-
joint and thus G is bipartite. The set of all com-
ponents of G is defined as C1 and let k denote its
cardinality, i.e., |C1| = k. From the members of
C1 we further construct sets C2, ..., Ck as follows:
For each i, 2 ? i ? k, any member ofCi is formed
by the union of any i distinct members of C1. In
other words, any member of Ci is a graph with i
components and each such component is a mem-
ber of C1. The cardinality of Ci is clearly
(k
i
)
, for
every i, 1 ? i ? k.
Note that Ck = {G}, since G is the union of
all members of C1. Moreover, observe that C? =
?ki=1Ci is the set of graphs that can be generated
by all possible unions of G?s components. In that
sense
C = {?} ? C? (3)
is the power set of G. Indeed we have |C| = 1 +
?k
i=1
(k
i
)
= 2k as required.1
Figure 1(b) shows the graph G and the associ-
ated sets Ci of (S, T ) and A in Figure 1(a). Note
the bijective correspondence between consistent
1Here we used the fact that for any set X with |X| =
n, the set of all subsets of X , i.e., the power set of X , has
cardinality
Pn
i=0
`n
i
?
= 2n.
pairs and the phrase pairs that can be extracted
from the vertices of the members of the sets Ci.
This is a consequence of consistency Conditions 2
and 3, since they provide the sufficient conditions
for component formation.
In general, if a pair of strings (s, t) satisfies the
consistency Conditions 2 and 3, then it can be ex-
tracted from the vertices of a graph inCi, for some
i. Moreover, if Conditions 1 and 4 are also satis-
fied, i.e., if (s, t) is consistent, then we can write
P (S, T ) =
k?
i=1
{
(SH , TH) : H ? Ci,
SH ? S, TH ? T
}
,
(4)
where SH denotes the extracted string from the
source-type vertices of H , and similarly for TH .
Having established this relationship, when refer-
ring to members of C, we henceforth mean either
consistent pairs or inconsistent pairs. The latter
are pairs (SH , TH) for some H ? C such that at
least either SH 6? S or TH 6? T .
The construction above shows that phrase pairs
of interest to SMT are part of a carefully con-
structed subclass of all possible string pairs that
can be extracted from (S, T ). The power set C
of G gives rise to a small, possibly minimal, set
95
in which consistent and inconsistent pairs can be
measured.1 In other words, since C is (by con-
struction) a sigma-algebra, the pair (C1, C) is a
measurable space. Furthermore, one can construct
a measure space (C1, C, f), with an appropriately
chosen measure f : C ? [0,?).
Is the occurrence-counting measure f of Sec-
tion 2 a good choice? Fix an ordering for Ci, and
let Ci,j denote the jth member of Ci, for all i,
1 ? i ? k. Furthermore, let ?(x, y) = 1, if x = y
and 0, otherwise. We argue by contradiction that
the occurrence-counting measure
f(H) =
?
{H?: H??C, H? is consistent}
?(H,H ?), (5)
fails to form a measure space. Suppose that more
than one component of G is consistent, i.e., sup-
pose that
1 <
k?
j=1
f(C1,j) ? k. (6)
By construction of C, it is guaranteed that
1 = f(G) = f(Ck,1) = f(?
k
j=1 C1,j). (7)
The members of C1 are pairwise disjoint, because
each of them is a component ofG. Thus, since f is
assumed to be a measure, sigma-additivity should
be satisfied, i.e., we must have
f(?kj=1 C1,j) =
k?
j=1
f(C1,j) > 1, (8)
which is a contradiction.
In practice, the deficiency of using eq. 5 as
a statistic could possibly be explained by the
fact that the so-called lexical weights are used as
smoothing.
4 Consistency, Components and
Segmentations
In Section 3 the only relation that was assumed
among source (target) words/vertices was the or-
der of appearance in the source (target) sentence.
As a result, the graph representation G of (S, T )
and A was bipartite. There are several, linguisti-
cally motivated, ways in which a general graph can
be obtained from the bipartite graph G. We ex-
plain that the minimal linguistic structure, namely
1See Appendix for definitions.
sentence segmentations, can provide a generaliza-
tion of the construction introduced in Section 3.
Let X be a finite set of consecutive integers. A
consecutive partition of X is a partition of X such
that each part consists of integers consecutive in
X . A segmentation ? of a source sentence S is a
consecutive partition of {1, ..., |S|}. A part of ?,
i.e., a segment, is intuitively interpreted as a phrase
in S. In the graph representation G of (S, T ) and
A, a segmentation ? of S is realised by the ex-
istence of edges between consecutive source-type
vertices whose labels, i.e., word positions in S, ap-
pear in the same segment of ?. The same argument
holds for a target sentence and its words; a target
segmentation is denoted by ? .
Clearly, there are 2|S|?1 possible ways to seg-
ment S and, given a fixed alignment matrix A,
the number of all possible graphs that can be con-
structed is thus 2|S|+|T |?2. The bipartite graph
of Section 3 is just one possible configuration,
namely the one in which each segment of ? con-
sists of exactly one word, and similarly for ? . We
denote this segmentation pair by (?0, ?0).
We now turn to extracting consistent pairs in
this general setting from all possible segmenta-
tions (?, ?) for a sentence pair (S, T ) and a fixed
alignment matrix A. As in Section 3, we con-
struct graphs G?,? , associated sets C?,?i , for all i,
1 ? i ? k?,? , and C?,? , for all (?, ?). Consistent
pairs are extracted in lieu of eq. 4, i.e.,
P ?,? (S, T ) =
k?,??
i=1
{
(SH , TH) : H ? C
?,?
i ,
SH ? S, TH ? T
}
, (9)
and it is trivial to see that
{(S, T )} ? P ?,? (S, T ) ? P (S, T ), (10)
for all (?, ?). Note that P (S, T ) = P ?0,?0(S, T )
and, depending on the details of A, it is possible
for other pairs (?, ?) to attain equality. Moreover,
each consistent pair in P (S, T ) can be be extracted
from a member of at least one C?,? .
We focus on the sets C?,?1 , i.e., the components
of G?,? , for all (?, ?). In particular, we are inter-
ested in the relation between P (S, T ) and C?,?1 ,
for all (?, ?). Each consistent H ? C?0,?0 can
be converted into a single component by appropri-
ately forming edges between consecutive source-
type vertices and/or between consecutive target-
type vertices. The resulting component will evi-
dently be a member of C?,?1 , for some (?, ?). It
96
is important to note that the conversion of a con-
sistent H ? C?0,?0 into a single component need
not be unique; see Figure 2 for a counterexam-
ple. Since (a) such conversions are possible for
all consistent H ? C?0,?0 and (b) P (S, T ) =
P ?0,?0(S, T ), it can be deduced that all possible
consistent pairs can be traced in the sets C?,?1 , for
all (?, ?). In other words, we have:
P (S, T ) =
?
?,?
{
(SH , TH) : H ? C
?,?
1 ,
SH ? S, TH ? T
}
. (11)
The above equation says that by taking sen-
tence segmentations into account, we can recover
all possible consistent pairs, by inspecting only the
components of the underlying graphs.
It would be interesting to investigate the re-
lation between measure spaces (C?,?1 , C
?,? , f?,? )
and different configurations for A. We leave that
for future work and focus on the advantages pro-
vided by eq. 11.
  
t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3
Figure 2: A graph with three components (top),
and four possible conversions into a single compo-
nent by forming edges between contiguous words.
5 Towards a phrase-level model that
respects consistency
The aim of this section is to exploit the relation
established in eq. 11 between consistent pairs and
components of segmented sentence pairs. It was
also shown in Section 2 that the computation of the
translation models is inappropriate to describe the
underlying structure. We thus suggest a phrase-
based generative model in the spirit of the IBM
word-based models, which is compatible with the
construction of the previous sections.
5.1 Hidden variables
All definitions from the previous sections are car-
ried over, and we introduce a new quantity that is
associated with components. Let G?,? and C?,?1 ,
for some (?, ?) be as in Section 4, then the set
K is defined as follows: Each member of K is
a pair of (source, target) sets of segments that cor-
responds to the pair of (source, target) vertices of
a consistent member of C?,?1 . In other words, K is
a bisegmentation of a pair of segmented sentences
that respects consistency.
Figure 3 shows three possible ways to con-
struct consistent graphs from (S, T ) = (s41, t
6
1),
? = {{1, 2}, {3}, {4}} ? {x1, x2, x3} and ? =
{{1}, {2, 3, 4}, {5}, {6}} ? {y1, y2, y3, y4}. In
each case the exact alignment information is un-
known and we have:
(a) K =
{ (
{x1}, {y1}
)
,
(
{x2}, {y2}
)
,
(
{x3}, {y3, y4}
) }
.
(b) K =
{ (
{x1, x2}, {y1, y2, y3}
)
,
(
{x3}, {y4}
)}
.
(c) K =
{ (
{x1}, {y3, y4}
)
,
(
{x2, x3}, {y1, y2}
)}
.
  
t1s1 s2 s3t 2 t3 t4 s5t 5 t6
t1 s1 s2 s3t 2 t3 t4t 5 t6s5
t1s1 s2 s3t 2 t3t4 t 5t6 s5
(a)
(b)
(c)
Figure 3: Three possible ways to construct con-
sistent graphs for (s41, t
6
1) and a given segmenta-
tion pair. Exact word alignment information is un-
known.
In the proposed phrase-level generative model
the random variables whose instances are ?, ? and
97
K are hidden variables. As with the IBM mod-
els, they are associated with the positions of words
in a sentence, rather than the words themselves.
Alignment information is implicitly identified via
the consistent bisegmentation K.
Suppose we have a corpus that consists of pairs
of parallel sentences (S, T ), and let fS,T denote
the occurrence count of (S, T ) in the corpus. Also,
let lS = |S| and lT = |T |. The aim is to maximize
the corpus log-likelihood function
` =
?
S,T
fS,T log p?(T |S)
=
?
S,T
fS,T log
?
?,?,K
p?(T, ?, ?,K|S), (12)
where ?, ? and K are hidden variables parameter-
ized by a vector ? of unknown weights, whose val-
ues are to be determined. The expectation max-
imization algorithm (Dempster et al, 1977) sug-
gests that an iterative application of
?n+1 = argmax
?
?
S,T
fS,T
?
?,?,K
p?n(?, ?,K|S, T )?
log p?(T, ?, ?,K|S),
(13)
provides a good approximation for the maximum
value of `. As with the IBM models we seek prob-
ability mass functions (PMFs) of the form
p?(T, ?, ?,K|S) = p?(lT |S)p?(?, ?,K|lT , S)?
p?(T |?, ?,K, lT , S),
(14)
and decompose further as
p?(?, ?,K|lT , S) = p?(?, ? |lT , S)p?(K|?, ?, lT , S)
(15)
A further simplification of p?(?, ? |lT , S) =
p?(?|S)p?(? |lT ) may not be desirable, but will
help us understand the relation between ? and the
PMFs. In particular, we give a formal description
of p?(?|S) and then explain that p?(K|?, ?, lT , S)
and p?(T |?, ?,K, lT , S) can be computed in a
similar way.
5.2 Constrained, biased sampling without
replacement
The probability of a segmentation given a sentence
can be realised in two different ways. We first pro-
vide a descriptive approach which is more intu-
itive, and we use the sentence S = s41 as an ex-
ample whenever necessary. The set of all possi-
ble segments of S is denoted by seg(S) and triv-
ially |seg(S)| = |S|
(
|S| + 1
)
/2. Each segment
x ? seg(S) has a nonnegative weight ?(x|lS) such
that ?
x?seg(S)
?(x|lS) = 1. (16)
Suppose we have an urn that consists of
|seg(S)| weighted balls; each ball corresponds to
a segment of S. We sample without replacement
with the aim of collecting enough balls to form a
segmentation of S. When drawing a ball x we si-
multaneously remove from the urn all other balls
x? such that x ? x? 6= ?. We stop when the urn
is empty. In our example, let the urn contain 10
balls and suppose that the first draw is {1, 2}. In
the next draw, we have to choose from {3}, {4}
and {3, 4} only, since all other balls contain a ?1?
and/or a ?2? and are thus removed. The sequence
of draws that leads to a segmentation is thus a path
in a decision tree. Since ? is a set, there are |?|!
different paths that lead to its formation. The set
of all possible segmentations, in all possible ways
that each segmentation can be formed, is encoded
by the collection of all such decision trees.
The second realisation, which is based on the
notions of cliques and neighborhoods, is more
constructive and will give rise to the desired PMF.
A clique in a graph is a subset U of the vertex set
such that for every two vertices u, v ? U , there ex-
ists an edge connecting u and v. For any vertex u
in a graph, the neighborhood of u is defined as the
set N(u) = {v : {u, v} is an edge}. A maximal
clique is a clique U that is not a subset of a larger
clique: For each u ? U and for each v ? N(u) the
set U ? {v} is not a clique.
Let G be the graph whose vertices are all seg-
ments of S and whose edges satisfy the condition
that any two vertices x and x? form an edge iff
x ? x? = ?; see Figure 4 for an example. G es-
sentially provides a compact representation of the
decision trees discussed above.
It is not difficult to see that a maximal clique
also forms a segmentation. Moreover, the set of all
maximal cliques in G is exactly the set of all pos-
sible segmentations for S. Thus, p?(?|S) should
satisfy
p?(?|S) = 0, if ? is not a clique in G, (17)
and ?
?
p?(?|S) = 1, (18)
98
  
{1}
{2}
{3}
{4}
{253}{154}
{351}{35154} {25351}
{2535154}
Figure 4: The graph whose vertices are the seg-
ments of s41 and whose edges are formed by non-
overlapping vertices.
where the sum is over all maximal cliques in G.
In our example p?
(
{ {1}, {1, 2} }|S
)
= 0, be-
cause there is no edge connecting segments {1}
and {1, 2} so they are not part of any clique.
In order to derive an explicit formula for
p?(?|S) we focus on a particular type of paths
in G. A path is called clique-preserving, if ev-
ery vertex in the path belongs to the same clique.
Our construction should be such that each clique-
preserving path has positive probability of occur-
ring, and all other paths should have probability
0. We proceed with calculating probabilities of
clique-preserving paths based on the structure of
G and the constraint of eq. 16.
The probability p?(?|S) can be viewed as
the probability of generating all clique-preserving
paths on the maximal clique ? in G. Since
? is a clique, there are |?|! possible paths that
span its vertices. Let ? = {x1, ..., x|?|},
and let pi denote a permutation of {1, ..., |?|}.
We are interested in computing the probabil-
ity q?(xpi(1), ..., xpi(|?|)) of generating a clique-
preserving path xpi(1), ..., xpi(|?|) in G. Thus,
p?(?|S) = p?({x1, ..., x|?|}|S)
=
?
pi
q?(xpi(1), ..., xpi(|?|))
=
?
pi
q?(xpi(1)) q?(xpi(2)|xpi(1))? ...
...? q?(xpi(|?|)|xpi(1), ..., xpi(|?|?1)).
(19)
The probabilities q?(?) can be explicitly calcu-
lated by taking into account the following ob-
servation. A clique-preserving path on a clique
? can be realised as a sequence of vertices
xpi(1), ..., xpi(i), ..., xpi(|?|) with the following con-
straint: If at step i ? 1 of the path we are at ver-
tex xpi(i?1), then the next vertex xpi(i) should be a
neighbor of all of xpi(1), ..., xpi(i?1). In other words
we must have
xpi(i) ? Npi,i ?
i?1?
l=1
N(xpi(l)). (20)
Thus, the probability of choosing xpi(i) as the next
vertex of the path is given by
q?(xpi(i)|xpi(1), ..., xpi(i?1)) =
?(xpi(i)|lS)
?
x?Npi,i
?(x|lS)
,
(21)
if xpi(i) ? Npi,i and 0, otherwise. When choosing
the first vertex of the path (the root in the deci-
sion tree) we have Npi,1 = seg(S), which gives
q?(xpi(1)) = ?(xpi(1)|lS), as required. Therefore
eq. 19 can be written compactly as
p?(?|S) =
?
?
|?|?
i=1
?(xi|lS)
?
?
?
pi
1
Q?(?, pi;S)
,
(22)
where
Q?(?, pi;S) =
|?|?
i=1
?
x?Npi,i
?(x|lS) . (23)
The construction above can be generalized in
order to derive a PMF for any random variable
whose values are partitions of a set. Indeed, by al-
lowing the vertices of G to be a subset of a power
set, and keeping the condition of edge formation
the same, probabilities of clique-preserving paths
can be calculated in the same way. Figure 5 shows
the graph G that represents all possible instances of
K with (S, T ) = (s41, t
5
1), ? =
{
{1, 2}, {3}, {4}
}
and ? =
{
{1}, {2, 3, 4}, {5}
}
. Again each maxi-
mal clique is a possible consistent bisegmentation.
In order for this model to be complete, one
should solve the maximization step of eq. 13 and
calculate the posterior p?n(?, ?,K|S, T ). We are
not bereft of hope, as relevant techniques have
been developed (see Section 6).
6 Related Work
To our knowledge, this is the first attempt to inves-
tigate formal motivations behind the consistency
method.
99
  
t 12 s, 23t 4 s,5 t 3 s, 5
t 12 s, 1
t 4 s,1 t 3 s, 1
t 12 s, 5
t 3 s, 23 t 4 s, 23t 14 s,13
t 14 s, 25
t 43 s, 25
t 13 s, 15
t 14 s,1 t 3 s, 25 t 14 s,5 t 3 s, 13
t 43 s, 13
Figure 5: Similar to Figure 4 but for consistent
bisegmentations with (S, T ) = (s41, t
5
1) and a
given segmentation pair (see text). For clarity, we
show the phrases that are formed from joining con-
tiguous segments in each pair, rather than the seg-
ments themselves.
Several phrase-level generative models have
been proposed, almost all relying on multinomial
distributions for the phrase alignments (Marcu and
Wong, 2002; Zhang et al, 2003; Deng and Byrne
2005; DeNero et al, 2006; Birch et al, 2006).
This is a consequence of treating alignments as
functions rather than partitions.
Word alignment and phrase extraction via In-
version Transduction Grammars (Wu, 1997), is a
linguistically motivated method that relies on si-
multaneous parsing of source and target sentences
(DeNero and Klein, 2010; Cherry and Lin 2007;
Neubig et al, 2012).
The partition probabilities we introduced in
Section 5.2 share the same tree structure discussed
in (Dennis III, 1991), which has found applica-
tions in Information Retrieval (Haffari and Teh,
2009).
7 Conclusions
We have identified the relation between consis-
tency and components of graphs that represent
word-aligned sentence pairs. We showed that
phrase pairs of interest to SMT form a sigma-
algebra generated by components of such graphs,
but the existing occurrence-counting statistics are
inadequate to describe this structure. A general-
ization of our construction via sentence segmenta-
tions lead to a realisation of random partitions as
cases of constrained, biased sampling without re-
placement. As a consequence, we derived an exact
formula for the probability of a segmentation of a
sentence.
Appendix: Measure Space
The following standard definitions can be found
in, e.g., (Feller, 1971). LetX be a set. A collection
B of subsets of X is called a sigma-algebra if the
following conditions hold:
1. ? ? B.
2. If E is in B, then so is its complement X \E.
3. If {Ei} is a countable collection of sets in B,
then so is their union ?iEi.
Condition 1 guarantees that B is non-empty and
Conditions 2 and 3 say thatB is closed under com-
plementation and countable unions respectively.
The pair (X,B) is called a measurable space.
A function f : B ? [0,?) is called a measure
if the following conditions hold:
1. f(?) = 0.
2. If {Ei} is a countable collection of pairwise
disjoint sets in B, then
f(?iEi) =
?
i
f(Ei).
Condition 2 is known as sigma-additivity. The
triple (X,B, f) is called a measure space.
Acknowledgments
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (GALATEAS) and by the EC funded
project CoSyne (FP7-ICT-4-24853).
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Proc. of the Workshop on Statistical
Machine Translation, pages 154?157.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation.
Computational Linguistics, vol.19(2), pages 263?
312.
100
Colin Cherry and Dekang Lin. 2007. Inversion Trans-
duction Grammar for Joint Phrasal TranslationMod-
eling. In Proc. of SSST, NAACL-HLT / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 17?24.
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B (Methodological) 39(1), pages 1?38.
John DeNero, Dan Gillick, James Zhang and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proc. of the Work-
shop on Statistical Machine Translation, pages 31?
38.
John DeNero and Dan Klein. 2010. Discriminative
Modeling of Extraction Sets for Machine Transla-
tion. In Proc. of the Association for Computational
Linguistics (ACL), pages 1453?1463.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proc. of the Conference on Empir-
ical Methods in Natural Language Processing and
Human Language Technology (HLT-EMNLP), pages
169?176.
Samuel Y. Dennis III. 1991. On the Hyper-Dirichlet
Type 1 and Hyper-Liouville Distributions. Commu-
nications in Statistics - Theory and Methods, 20(12),
pages 4069?4081.
William Feller. 1971. An Introduction to Probability
Theory and its Applications, Volume II. John Wiley,
New York.
Gholamreza Haffari and Yee Whye Teh. 2009. Hi-
erarchical Dirichlet Trees for Information Retrieval.
In Proc. of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL), pages 173?181.
Frank Harary. 1969. Graph Theory. Addison?Wesley,
Reading, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (HLT-NAACL), pages
48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133?139.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori and Tatsuya Kawahara. 2012. Joint
Phrase Alignment and Extraction for Statistical Ma-
chine Translation. Journal of Information Process-
ing, vol. 20(2), pages 512?523.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of the Joint Con-
ference of Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP-VLC),
pages 20?28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23, pages 377?
404.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Association for Computational Linguistics (ACL),
pages 523?530.
Ying Zhang, Stephan Vogel and Alex Waibel. 2003.
Integrated Phrase Segmentation and Alignment Al-
gorithm for Statistical Machine Translation. In
Proc. of the International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE).
101
