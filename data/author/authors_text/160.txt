Proceedings of the 12th European Workshop on Natural Language Generation, pages 1?8,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Using NLG to Help Language-Impaired Users Tell Stories and  
Participate in Social Dialogues 
 
 
Ehud Reiter, Ross Turner 
University of Aberdeen 
Aberdeen, UK 
e.reiter@abdn.ac.uk 
csc272@abdn.ac.uk 
Norman Alm, Rolf Black, 
Martin Dempster, Annalu Waller 
University of Dundee 
Dundee, UK 
{nalm,rolfblack,martindempster, 
awaller}@computing.dundee.ac.uk 
 
 
Abstract 
Augmentative and Alternative Communication 
(AAC) systems are communication aids for 
people who cannot speak because of motor or 
cognitive impairments.  We are developing 
AAC systems where users select information 
they wish to communicate, and this is ex-
pressed using an NLG system.  We believe 
this model will work well in contexts where 
AAC users wish to go beyond simply making 
requests or answering questions, and have 
more complex communicative goals such as 
story-telling and social interaction. 
1 Introduction 
Many people have difficulty in communicating 
linguistically because of cognitive or motor im-
pairments.  Such people typically use communi-
cation aids to help them interact with other peo-
ple.  Such communication aids range from sim-
ple tools that do not involve computers, such as 
picture cards, to complex software systems that 
attempt to ?speak? for the impaired user. 
From a technological perspective, even the 
most complex communication aids have typi-
cally been based on fixed (canned) texts or sim-
ple fill-in-the-blank templates; essentially the 
user selects a text or template from a set of pos-
sible utterances, and the system utters it.  We 
believe that while this may be adequate if the 
user is simply making a request (e.g., please give 
me a drink) or answering a question (e.g., I live 
at home), it is not adequate if the user has a more 
complex communicative goal, such as engaging 
in social interaction, or telling a story. 
We are exploring the idea of supporting such 
interactions by building a system which uses ex-
ternal data and/or knowledge sources, plus do-
main and conversational models, to dynamically 
suggest possible messages (event, facts, or opin-
ions, represented as ontology instances) which 
are appropriate to the conversation. The user se-
lects the specific message which he wishes the 
system to speak, and possibly adds simple anno-
tations (e.g., I like this) or otherwise edits the 
message.  The system then creates an appropriate 
linguistic utterance from the selected message, 
taking into consideration contextual factors. 
In this paper we describe two projects on 
which we are working within this framework.  
The goal of the first project is to help non-
speaking children tell stories about their day at 
school to their parents; the goal of the second 
project is to help non-speaking adults engage in 
social conversation. 
2 Background 
2.1 Augmentative and alternative commu-
nication 
Augmentative and alternative communication 
(AAC) is a term that describes a variety of meth-
ods of communication for non-speaking people 
which can supplement or replace speech.  The 
term covers techniques which require no equip-
ment, such as sign language and cards with im-
ages; and also more technologically complex 
systems which use speech synthesis and a variety 
of strategies to create utterances.  
The most flexible AAC systems allow users to 
specify arbitrary words, but communication rates 
are extremely low, averaging 2-10 words per 
minute. This is because many AAC users interact 
slowly with computers because of their impair-
ments.  For example, some of the children we 
work with cannot use their hands, so they use 
scanning interfaces with head switches.  In other 
words, the computer displays a number of op-
1
tions to them, and then scans through these, 
briefly highlighting each option.  When the de-
sired option is highlighted, the child selects it by 
pressing a switch with her head.   This is ade-
quate for communicating basic needs (such as 
hunger or thirst); the computer can display a 
menu of possible needs, and the child can select 
one of the items.  But creating arbitrary messages 
with such an interface is extremely slow, even if 
word prediction is used; and in general such in-
terfaces do not well support complex social in-
teractions such as story telling (Waller, 2006).  
A number of research projects in AAC have 
developed prototype systems which attempt to 
facilitate this type of human-human interaction.  
At their most basic, these systems provide users 
with a library of fixed ?conversational moves? 
which can be selected and uttered.  These moves 
are based on models of the usual shape and con-
tent of conversational encounters (Todman & 
Alm, 2003), and for example include standard 
conversational openings and closings, such as 
Hello and How are you. They also include back-
channel communication such as Uh-huh, Great!, 
and Sorry, can you repeat that. 
It would be very useful to go beyond standard 
openings, closings, and backchannel messages, 
and allow the user to select utterances which 
were relevant to the particular communicative 
context and goals.  Dye et al(1998) developed a 
system based on scripts of common interactions 
(Schank & Abelson, 1977).  For example, a user 
could activate the MakeAnAppointment script, 
and then could select utterances relevant to this 
script, such as I would like to make an appoint-
ment to see the doctor.  As the interaction pro-
gressed, the system would update the selections 
offered to the user based on the current stage of 
the script; for example during time negotiation a 
possible utterance would be I would like to see 
him next week. This system proved effective in 
trials, but needed a large number of scripts to be 
generally effective.  Users could author their own 
texts, which were added to the scripts, but this 
was time-consuming and had to be done in ad-
vance of the conversation. 
Another goal of AAC is to help users narrate 
stories. Narrative and storytelling play a very 
important part in the communicative repertoire of 
all speakers (Schank, 1990). In particular, the 
ability to draw on episodes from one?s life his-
tory in current conversation is vital to maintain-
ing a full impression of one?s personality in deal-
ing with others (Polkinghorne, 1991). Story tell-
ing tools for AAC users have been developed, 
which include ways to introduce a story, tell it at 
the pace required (with diversions) and give 
feedback to comments from listeners (Waller, 
2006); but again these tools are based on a li-
brary of fixed texts and templates. 
2.2 NLG and AAC 
Natural language generation (NLG) systems 
generate texts in English and other human lan-
guages from non-linguistic input (Reiter and 
Dale, 2000).  In their review of NLP and AAC, 
Newell, Langer, and Hickey (1998) suggest that 
NLG could be used to generate complete utter-
ances from the limited input that AAC users are 
able to provide.  For example, the Compansion 
project (McCoy, Pennington, Badman 1998) 
used NLP and NLG techniques to expand tele-
graphic user input, such as Mary go store?, into 
complete utterances, such as Did Mary go to the 
store?  Netzer and Elhadad (2006) allowed users 
to author utterances in the symbolic language 
BLISS, and used NLG to translate this to English 
and Hebrew texts. 
In recent years there has been growing interest 
in data-to-text NLG systems (Reiter, 2007); 
these systems generate texts based on sensor and 
other numerical data, supplemented with ontolo-
gies that specify domain knowledge.  In princi-
ple, it seems that data-to-text techniques should 
allow NLG systems to provide more assistance 
than the syntactic help provided by Compansion.  
For example, if the user wanted to talk about a 
recent football (soccer) match, a data-to-text sys-
tem could get actual data about the match from 
the web, and generate potential utterances from 
this data, such as Arsenal beat Chelsea 2-1 and 
Van Persie scored two goals; the user could then 
select one of these to utter. 
In addition to helping users interact with other 
people, NLG techniques can also be used to edu-
cate and encourage children with disabilities.  
The STANDUP system (Manurung, Ritchie et 
al., 2008), for example, used NLG and computa-
tional humour techniques to allow children who 
use AAC devices to generate novel punning 
jokes.  This provided the children with successful 
experiences of controlling language, gave them 
an opportunity to play with language and explore 
new vocabulary (Waller et al, in press). In a 
small study with nine children with cerebral 
palsy, the children used their regular AAC tools 
more and also performed better on a test measur-
ing linguistic abilities after they used STANDUP 
for ten weeks. 
2
3 Our Architecture 
Our goal is help AAC users engage in com-
plex social interaction by using NLG and data-
to-text technology to create potential utterances 
and conversational contributions for the users. 
The general architecture is shown in Figure 1, 
and Sections 4 and 5 describe two systems based 
on this architecture. 
 
 
The system has the following components: 
Data analysis: read in data, from sensors, 
web information sources, databases, and so forth.  
This module analyses this data and identifies 
messages (in the sense of Reiter and Dale 
(2000)) that the user is likely to want to commu-
nicate; this analysis is partially based on domain, 
conversation, and user models, which may be 
represented as ontologies. 
Editing: allow the user to edit the messages.  
Editing ranges from adding simple annotations to 
specify opinions (e.g., add BAD to Arsenal beat 
Chelsea 2-1 if the user is a Chelsea fan), to using 
an on-screen keyboard to type free-text com-
ments.  Users can also delete messages, specify 
which messages they are most likely to want to 
utter, and create new messages.  Editing is done 
before the actual conversation, so the user does 
not have to do this under time pressure.  The 
amount of editing which can be done partially 
depends on the extent of the user?s disabilities. 
Narration: allows the user to select mes-
sages, and perhaps conversational moves (e.g., 
Hello), in an actual conversational context.  Edit-
ing is possible, but is limited by the need to keep 
the conversation flowing. 
NLG and Speech Synthesis: Generates actual 
utterances from the selected messages, taking 
into account linguistic context, especially a dia-
logue model. 
4 Narrative for Children: How was 
School Today 
The goal of the How was School Today project is 
to enable non-speaking children with major mo-
tor disabilities but reasonable cognitive skills to 
tell a story about what they did at school during 
the day.  The particular children we are working 
with have cerebral palsy, and use wheelchairs.  A 
few of them can use touch screens, but most of 
them use a head switch and scanning interface, 
as described above.  By ?story?, we mean some-
thing similar to Labov?s (1972) conversational 
narrative, i.e., a series of linked real-world events 
which are unusual or otherwise interesting, pos-
sibly annotated with information about the 
child?s feelings, which can be narrated orally. 
We are not expecting stories in the literary sense, 
with character development and complex plots. 
The motivation of the project is to provide the 
children with successful narrative experience. 
Typically developing children develop narrative 
skills from an early age with adults scaffolding 
conversations to elicit narrative, e.g. ?What did 
you do at school today?? (Bruner, 1975). As the 
child?s vocabulary and language competence 
develops, scaffolding is reduced. This progres-
sion is seldom seen in children with complex 
communication needs ? they respond to closed 
questions but seldom take control of conversa-
Sensor 
data 
Web info 
sources 
Other 
external data 
Data analysis: 
select possible 
messages to 
communicate 
Conversation 
model 
Domain model 
User model 
Editing: User adds 
annotations 
User 
 
NLG: 
Generate 
utterance 
Dialogue 
model 
Speech 
synthesis 
Conversation 
partner 
 
Narration: User 
selects what to say 
Prepare content 
Narrate content 
Figure 1:  General architecture 
3
tion (von Tetzchner and Grove, 2003).  Many 
children who use AAC have very limited narra-
tive skills (Soto et al 2006). Research has shown 
that providing children who use AAC with suc-
cessful narrative experiences by providing full 
narrative text can help the development of writ-
ten and spoken narrative skills  (Waller, 2008).  
The system follows the architecture described 
above.  Input data comes from RFID sensors that 
track where the child went during the day; an 
RFID reader is mounted on the child?s wheel-
chair, and RFID tags are placed around the 
school, especially in doorways so we can moni-
tor children entering and leaving rooms.  Teach-
ers have also been given RFID swipe cards 
which they can swipe against a reader, to record 
that they are interacting with the child; this is 
more robust than attempting to infer interaction 
automatically by tracking teachers? position. 
Teachers can also record interactions with ob-
jects (toys, musical instruments, etc), by using 
special swipe cards associated with these objects. 
Last but not least, teachers can record spoken 
messages about what happened during the day. 
An example of how the child?s wheelchair is set 
up is shown in Figure 2. 
   
 
 
Figure 2: System configuration 
 
The data analysis module combines sensor-
derived location and interaction data with a time-
table which records what the child was expected 
to do during the day, and a domain knowledge 
base which includes information about typical 
activities (e.g., if the child?s location is Swim-
mingPool, the child?s activity is probably 
Swimming).  From this it creates a series of 
events (each of which contain a number of mes-
sages) which describe the child?s lessons and 
activities, including divergences from what is 
expected in the timetable.  Several messages may 
be associated with an event.  The data analysis 
module also infers which events and messages it 
believes are most interesting to the child; this is 
partially based on heuristics about what children 
are interested in (e.g., swimming is more inter-
esting than lunch), and partially based on the 
general principle that unexpected things (diver-
gences from the timetable) are more interesting 
than expected things.  No more than five events 
are flagged as interesting, and only these events 
are shown in the editing interface. 
The editing interface allows children to re-
move events they do not want to talk about (per-
haps for privacy reasons) from the list of interest-
ing events.  It also allows children to add mes-
sages that express simple opinions about events; 
i.e., I liked it or I didn?t like it.  The interface is 
designed to be used with a scanning interface, 
and is based on symbols that represent events, 
annotations, etc. 
The narration interface, shown in Figure 3, is 
similar to the editing interface. It allows children 
to choose a specific event to communicate, 
which must be one of the ones they selected dur-
ing the editing phase.  Children are encouraged 
to tell events in temporal order (this is one of the 
narration skills we are trying to teach), but this is 
not mandated, and they can deviate from tempo-
ral order if they wish.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Figure 3: Narration Interface 
 
The NLG system generates actual texts from 
the events selected by the children.  Most of this 
Tablet PC with NLG system and 
swipe-card RFID sensor  
long range 
RFID  
sensor for 
location 
tracking 
Events 
Opinion Annotations 
Messages  
for event 
4
is fairly simple, since the system deliberately 
uses simple ?child-like? language (Section 6).  
However, the system does need to make some 
decisions based on discourse context, including 
choosing appropriate referring expressions (es-
pecially pronouns), and temporal expressions 
(especially when children deviate from pure 
temporal order). 
4.1 Example 
For example, assume that the timetable speci-
fies the following information 
 
 
Assume that the sensors then recorded the fol-
lowing information 
 
Event 1 
      Location: CL_SEC2 
      Time: 13:23:00.0 - 14:07:00.0 
      Interactions: Mrs. Smith, Rolf, Ross 
 
Event 2 
      Location: HALL 
      Time: 14:10:00.0 ? 14:39:00.0 
      Interactions: none 
 
The data analysis module associates Event 1 with 
the Arts and Crafts timetable entry, since the lo-
cation is right, the timetabled teacher is present, 
and the times approximately match.  From this 
two messages are produced: one corresponding 
to I had Arts and Crafts this afternoon with Mrs. 
Smith (the core activity description), and the oth-
er corresponding to Rolf and Ross were there 
(additional information about people not time-
tabled to be there).  The child can add opinions 
using the editing interface; for example, if he 
added a positive annotation to the event, this 
would become an additional message corre-
sponding to It was great. 
For Event 2, the data analysis module notes 
that it does not match a timetabled event. The 
timetable indicates the child should be at Physio-
therapy after Art and Crafts; however, the sensor 
information indicates they were in the hall. The 
system generates a single message corresponding 
to Then I went to the Hall instead of Physiother-
apy to describe this event.  If the child added a 
negative annotation to this message, this would 
become an additional message expressed as I 
didn?t like it. 
4.2 Evaluation 
We conducted an initial evaluation of the How 
was School Today system in January, 2009.  
Two children used the system for four days: Ju-
lie, age 11, who had good cognitive skills but 
was non-verbal because of severe motor impair-
ments; and Jessica, age 13, who had less severe 
motor impairments but who had some cognitive 
and memory impairments (these are not the chil-
drens? real names).  Julie used the system as a 
communication and interaction aid, as described 
above; Jessica used the system partially as a 
memory aid.  The evaluation was primarily 
qualitative: we observed how Julie and Jessica 
used the system, and interviewed their teachers, 
speech therapists, care assistants, and Julie?s 
mother (Jessica?s parents were not available). 
The system worked very well for Julie; she 
learned it quickly, and was able to use it to have 
real conversations about her day with adults, al-
most for the first time in her life.  This validated 
our vision that our technology could help AAC 
users engage in real interaction, and go beyond 
simple question answering and communication 
of basic needs.  The system also worked rea-
sonably well as a memory aid for Jessica, but she 
had a harder time using it, perhaps because of her 
cognitive impairments. 
Staff and Julie?s mother were very supportive 
and pleased with the system.  They had sugges-
tions for improving the system, including a wider 
range of annotations; more phrases about the 
conversation itself, such as Guess what happened 
at school today; and allowing children to request 
teenager language (e.g., really cool). 
From a technical perspective, the system 
worked well overall.   School staff were happy to 
use the swipe cards, which worked well.  There 
were some problems with the location sensors, 
we need better techniques for distinguishing real 
readings from noise.  A surprising amount of 
effort was needed to enter up-to-date knowledge 
(e.g., daily lunch menus), this would need to be 
addressed if the system was used for a period of 
months as opposed to days. 
5 Social Conversation for Adults 
In our second project, we want to build a tool to 
help adults with cerebral palsy engage in social 
conversation about a football match, movie, 
weather, and so forth.  Many people with severe 
disabilities have great difficulty developing new 
interpersonal relationships, and indeed report that 
forming new relationships and taking part in new 
Time Activity Location Teacher 
?? ?? ?? ?? 
13.20 -14 Arts and 
Crafts 
CL_SEC2 Mrs Smith 
14 -14.40 Physiotherapy PHYSIO1 Mrs Jones 
?? ?? ?? ?? 
5
activities are major priorities in their lives (Datil-
lo et al, 2007).  Supporting these goals through 
the development of appropriate technologies is 
important as it could lead to improved social out-
comes. 
This project builds on the TALK system 
(Todman and Alm, 2003), which helped AAC 
users engage in active social conversation. 
TALK partially overcame the problem of low 
communication rate by requiring users to pre-
author their conversational material ahead of 
time, so that when it was needed it could simply 
be selected and output. TALK also used insights 
from Conversation Analysis (Sacks, 1995) to 
provide appropriate functionality in the system 
for social conversation. For example, it sup-
ported opening and closing statements, stepwise 
topic change, and the use of quick-fire utterances 
to provide fast, idiomatic responses to commonly 
encountered situations. This approach led to 
more dynamic AAC-facilitated interactions with 
higher communication rates, and had a positive 
impact on the perceived communicative compe-
tence of the user (Todman, Alm et al, 2007).   
TALK requires the user to spend a substantial 
amount of time pre-authoring material; this is 
perhaps its greatest weakness.  Our idea is to re-
duce the amount of pre-authoring needed, by us-
ing the architecture shown in Fig 1, where much 
of the material is automatically created from data 
sources, ontologies, etc, and the user?s role is 
largely to edit and annotate this material, not to 
create it from scratch. 
We developed an initial prototype system to 
demonstrate this concept in the domain of foot-
ball results (Dempster, 2008).  We are now 
working on another prototype, whose goal is to 
support social conversations about movies, mu-
sic, television shows, etc (which is a much 
broader domain than football).  We have created 
an ontology which can describe events such as 
watching a film, listening to a music track, or 
reading a book.  Each ?event? has both temporal 
and spatial properties which allow descriptions to 
be produced about where and when an event took 
place, and other particulars relating to that par-
ticular class of event.  For example, if the user 
listened to a radio show, we record the name of 
the show, the presenter and the station it was 
broadcast on.  Ultimately we plan to obtain in-
formation about movies, music tracks, etc from 
web-based databases such as IMDB (movies) 
and last.fm (music). 
Of course, databases such as IMDB do not 
contain information such as what the user 
thought of the movie, or who he saw it with.  
Hence we will allow users to add annotations 
with such information.  Some of these annota-
tions will be entered via a structured tool, such as 
a calendar interface that allows users to specify 
when they watched or listened to something. We 
would like to use NaturalOWL (Galanis and An-
droutsopoulos, 2007) as the NLG component of 
the system; it is well suited to describing objects, 
and is intended to be integrated with an ontology.  
As with the How Was School Today project, 
some of the main low-level NLG challenges are 
choosing appropriate referring expressions and 
temporal references, based on the current dis-
course context.  Speech output is done using Ce-
reproc (Aylett and Pidcock, 2007). 
An example of our current narration interface 
is shown in Figure 4.  In the editing interface, the 
user has specified that he went to a concert at 
8pm on Thursday, and that he rated it 8 out of 
10.  The narration interface gives the user a 
choice of a number of messages based on this 
information, together with some standard mes-
sages such as Thanks and Agree. 
 
 
 
Note that unlike the How Was School Today 
project, in this project we do not attempt to infer 
event information from sensors, but we allow 
(and expect) the user to enter much more infor-
mation at the editing stage.  We could in princi-
ple use sensors to pick up some information, 
such as the fact that the user was in the cinema 
from 12 to 2PM on Tuesday, but this is not the 
research focus of this project. 
We plan to evaluate the system using groups 
of both disabled and non-disabled users.  This 
has been shown in the past to be an effective ap-
proach for the evaluation of prototype AAC sys-
tems (Higginbotham, 1995). Initially pairs of 
non-disabled participants will be asked to pro-
duce short conversations with one person using 
the prototype and the other conversing normally.   
Quantitative measures of the communication rate 
6
will be taken as well as more qualitative observa-
tions relating to the usability of the system.  Af-
ter this evaluation we will improve the system 
based on our findings, and then conduct a final 
evaluation with a small group of AAC users. 
6 Discussion: Challenges for NLG 
From an NLG perspective, generating AAC texts 
of the sort we describe here presents different 
challenges from many other NLG applications. 
First of all, realization and even microplanning 
are probably not difficult, because in this context 
the AAC system should generate short simple 
sentences if possible.  This is because the system 
is speaking ?for? someone with limited or devel-
oping linguistic abilities, and it should try to pro-
duce something similar to what the user would 
say himself if he or she had the time to explicitly 
write a text using an on-screen keyboard. 
To take a concrete example, we had originally 
considered using past-perfect tense (a fairly 
complex linguistic construct) in the How was 
School project, when the narrative jumped to an 
earlier point in time.  For example I ate lunch at 
12.  I had gone swimming at 11.  But it was clear 
from corpora of child-written texts that these 
children never used perfect tenses, so instead we 
opted for I ate lunch at 12.  I went swimming at 
11.  This is less linguistically polished, but much 
more in line with what the children might actu-
ally produce. 
Given this desire for linguistic simplicity, re-
alisation is very simple, as is lexical choice (use 
simple words) and aggregation (keep sentences 
short).  The main microplanning challenges re-
late to discourse coherence, in particular refer-
ring expressions and temporal descriptions.   
On the other hand, there are major challenges 
in document planning.  In particular, in the How 
Was School project, we want the output to be a 
proper narrative, in the sense of Labov (1972).  
That is, not just a list of facts and events, but a 
structure with a beginning and end, and with ex-
planatory and other links between components 
(e.g., I had math in the afternoon because we 
went swimming in the morning, if the child nor-
mally has math in the morning).  We also wanted 
the narrative to be interesting and hold the inter-
est of the person the child is communicating 
with.  As pointed out by Reiter et al(2008), cur-
rent NLG systems do not do a good job of gener-
ating narratives.  
Similarly, in the Social Conversations project 
we want the system to generate a social dialogue, 
not just a list of facts about movies and songs.  
Little previous research has been done on gener-
ating social (as opposed to task-oriented) dia-
logues.  One exception is the NECA Socialite 
system (van Deemter et al 2008), but this fo-
cused on techniques for expressing affect, not on 
high-level conversational structure. 
For both stories and social conversations, it 
would be extremely useful to be able to monitor 
what the conversational partner is saying.  This is 
something we hope to investigate in the future.  
As most AAC users interact with a small number 
of conversational partners, it may be feasible to 
use a speech dictation system to detect at least 
some of what the conversational partner says. 
Last but not least, a major challenge implicit 
in our systems and indeed in the general architec-
ture is letting users control the NLG system.   
Our systems are intended to be speaking aids, 
ideally they should produce the same utterances 
as the user would if he was able to talk.  This 
means that users must be able to control the sys-
tems, so that it does what they want it to do, in 
terms of both content and expression.  To the 
best of our knowledge, little is known about how 
users can best control an NLG system. 
7 Conclusion 
Many people are in the unfortunate position of 
not being able to speak or type, due to cognitive 
and/or motor impairments.  Current AAC tools 
allow such people to engage in simple needs-
based communication, but they do not provide 
good support for richer use of language, such as 
story-telling and social conversation.  We are 
trying to develop more sophisticated AAC tools 
which support such interactions, by using exter-
nal data and knowledge sources to produce can-
didate messages, which can be expressed using 
NLG and speech synthesis technology.  Our 
work is still at an early stage, but we believe that 
it has the potential to help AAC users engage in 
richer interactions with other people.  
Acknowledgements 
We are very grateful to Julie, Jessica, and their 
teachers, therapists, carers, and parents for their 
help in building and evaluating the system de-
scribed in Section 4.  Many thanks to the anony-
mous referees and our colleagues at Aberdeen 
and Dundee for their very helpful comments.  
This research is supported by EPSRC grants 
EP/F067151/1 and EP/F066880/1, and by a 
Northern Research Partnership studentship. 
7
References 
Aylett, M. and C. Pidcock (2007). The CereVoice 
Characterful Speech Synthesiser SDK. Proceed-
ings of Proceedings of the 7th International Con-
ference on Intelligent Virtual Agents, pages 413-
414. 
Bruner, J. (1975). From communication to language: 
A psychological perspective. Cognition 3: 255-
289. 
Datillo, J., G. Estrella, L. Estrella, J. Light, D. 
McNaughton and M. Seabury (2007). "I have cho-
sen to live life abundantly": Perceptions of leisure 
by adults who use Augmentative and Alternative 
Communication. Augmentative & Alternative 
Communication 24(1): 16-28. 
van Deemter, K., B Krenn, P Piwek, M Klesen, M 
Schr?der and S Baumann. Fully generated scripted 
dialogue for embodied agents. Artificial Intelli-
gence 172: 1219?1244. 
Dempster, M. (2008). Using natural language genera-
tion to encourage effective communication in non-
speaking people. Proceedings of Young Research-
ers Consortium, ICCHP'08. 
Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-
son (1998). A script-based AAC system for trans-
actional interaction.  Natural Language Engineer-
ing, 4(1), 57-71. 
Galanis, D. and I. Androutsopoulos (2007). Generat-
ing Multilingual Descriptions from Linguistically 
Annotated OWL Ontologies: the NaturalOWL Sys-
tem. Proceedings of ENLG 2007. 
Higginbotham, D. J. (1995). Use of nondisabled sub-
jects in AAC Research : Confessions of a research 
infidel. Augmentative and Alternative Communica-
tion 11(1): 2-5. 
Labov, W (1972).  Language in the Inner City. Uni-
versity of Pennsylvania Press. 
Manurung, R., G. Ritchie, H. Pain, A. Waller, D. 
O'Mara and R. Black (2008). The Construction of a 
Pun Generator for Language Skills Development. 
Applied Artificial Intelligence 22(9): 841 ? 869. 
McCoy, K., C. Pennington and A. Badman (1998). 
Compansion: From research prototype to practical 
integration. Natural Language Engineering 4:73-
95. 
Netzer, Y and Elhadad, M (2006). Using Semantic 
Authoring for Blissymbols Communication 
Boards. In Proc of HLT-2006. 
Newell, A., S. Langer and M. Hickey (1998). The role 
of natural language processing in alternative and 
augmentative communication. Natural Language 
Engineering 4:1-16. 
Polkinghorne, D. (1991). Narrative and self-concept. 
Journal of Narrative and Life History, 1(2/3), 135-
153 
Reiter, E (2007). An Architecture for Data-to-Text 
Systems. In Proceedings of ENLG-2007, pages 
147-155. 
Reiter, E. and R. Dale (2000).  Building Natural Lan-
guage Generation Systems.  Cambridge University 
Press. 
Reiter, E,  A. Gatt, F Portet, and M van der Meulen 
(2008). The Importance of Narrative and Other 
Lessons from an Evaluation of an NLG System 
that Summarises Clinical Data (2007). In Proceed-
ings of INLG-2008, pages 97-104. 
Sacks, H. (1995). Lectures on Conversation. G. Jef-
ferson. Cambridge, MA, Blackwell. 
Schank, R. C. (1990). Tell me a story: A new look at 
real and artificial intelligence. New York, Macmil-
lan Publishing Co. 
Schank, R., and R. Abelson (1977).  Scripts, plans, 
goals, and understanding. New Jersey: Lawrence 
Erlbaum. 
Soto, G., E. Hartmann, and D. Wilkins (2006). Ex-
ploring the Elements of Narrative that Emerge in 
the Interactions between an 8-Year-Old Child who 
uses an AAC Device and her Teacher. Augmenta-
tive and Alternative Communication 4:231 ? 241. 
Todman, J. and N. A. Alm (2003). Modelling conver-
sational pragmatics in communication aids. Jour-
nal of Pragmatics 35: 523-538. 
Todman, J., N. A. Alm, D. J. Higginbotham and P. 
File (2007). Whole Utterance Approaches in AAC. 
Augmentative and Alternative Communication 
24(3): 235-254. 
von Tetzchner, S. and N. Grove (2003). The devel-
opment of alternative language forms. In S. von 
Tetzchner and N. Grove (eds), Augmentative and 
Alternative Communication: Developmental Issues, 
pages 1-27. Wiley. 
Waller, A. (2006). Communication Access to Conver-
sational Narrative. Topics in Language Disorders 
26(3): 221-239. 
Waller, A. (2008). Narrative-based Augmentative and 
Alternative Communication: From transactional to 
interactional conversation. Proceedings of ISAAC 
2008, pages 149-160.  
Waller, A., R. Black, D. A. O'Mara, H. Pain, G. Rit-
chie and R. Manurung (In Press). Evaluating the 
STANDUP Pun Generating Software with Chil-
dren with Cerebral Palsy. ACM Transactions on 
Accessible Computing. 
8
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic generation of conversational utterances and narrative for 
Augmentative and Alternative Communication: a prototype system 
Martin Dempster & Norman Alm Ehud Reiter 
School of Computing Computer Science Department 
University of Dundee University of Aberdeen 
Dundee, Scotland, DD1 4HN, UK Aberdeen, Scotland, AB24 3UE, UK 
m.k.dempster@dundee.ac.uk 
nalm@computing.dundee.ac.uk 
e.reiter@abdn.ac.uk 
 
 
 
Abstract 
We detail the design, development and evalua-
tion of Augmentative and Alternative Com-
munication (AAC) software which encourages 
rapid conversational interaction. The system 
uses Natural Language Generation (NLG) 
technology to automatically generate conver-
sational utterances from a domain knowledge 
base modelled from content suggested by a 
small AAC user group. Findings from this 
work are presented along with a discussion 
about how NLG might be successfully applied 
to conversational AAC systems in the future. 
1 Introduction 
Augmentative and Alternative Communication 
(AAC) systems assist non-speaking people communi-
cate. Reasons for lack of speech are varied and can be 
complex, but they are typically related to some pro-
found cognitive and/or motor impairment.  
Most AAC systems are computer based, utilize syn-
thesized speech output and employ a phrase-
construction approach to input. This approach requires 
the user to construct the majority of their utterances live 
during conversation. Undoubtedly this facilitated com-
munication is hugely important to those without natural 
speech. However, this process is often unacceptably 
slow and can lead to problematic and stilted interac-
tions, mostly due to the rapid nature of unimpeded face-
to-face communication. 
Previous work has shown that it is possible to hold 
mutually rewarding conversations using wholly pre-
stored material, known as the phrase-storage approach.  
Utterances are authored ahead of time and can be se-
lected and output immediately leading to quicker com-
munication rates. However, this approach suffers from 
several drawbacks which may have affected its more 
general adoption. 
Furthermore, Natural Language Processing (NLP) 
technology has proven to be a fruitful line of inquiry 
within the field. It has offered a powerful means to im-
prove system productivity and usability. We are current-
ly investigating how Natural Language Generation 
(NLG) might be applied in a useful way within an AAC 
device geared towards fast-paced and rewarding social 
interactions. It is hoped that the linguistic control and 
automaticity offered by NLG may go some way towards 
addressing the previous criticisms of pre-stored material 
regarding its inflexibility and cost in effort. 
2 Background 
2.1 Limitations of current AAC 
 High-tech AAC systems typically augment commu-
nication for non-speaking people by allowing live mes-
sage construction through some orthographic means. 
Completed messages are generally sent to a speech syn-
thesis engine for output during communication with 
others. Many people who require AAC have associated 
physical disabilities which reduce the speed achievable 
using input methods such as keyboards, pointing devic-
es or touch-screens. The rate achievable using most 
commercial AAC systems is highly dependent on the 
nature of the user?s disabilities but a generally accepted 
figure is in the region of 2-15 words per minute (Hig-
ginbotham, Shane et al 2007), at least an order of mag-
nitude slower than most natural speakers. 
This relatively slow rate of input is a crucial factor in 
some of the issues that arise in AAC-facilitated commu-
nication. Because of the effort and time required to 
create utterances, the user may not be able to construct 
messages quickly enough to take active roles in fast 
10
paced conversations. As a result users may become pas-
sive while also typically using a smaller communicative 
repertoire than natural speakers (Light 1988).  
Narrative, an important type of interpersonal com-
munication, is not well handled in most communication 
aids (Waller 1992). Delayed response and slow rate of 
aided-communication are correlated with higher inci-
dence of breakdown in communication and lesser per-
ceptions of the AAC user (Todman and Rzepecka 2003; 
McCarthy and Light 2005). This is primarily due to 
conflict between the relatively long time necessary to 
formulate an utterance and the fast paced nature of con-
versation.  
These problems are particularly critical in social con-
texts. AAC users typically have small social circles and 
are dependent on contact with families and carers. They 
often lack self-esteem and have negative self-image.  As 
a result, developing new relationships and experiencing 
new things can be difficult, despite being a major priori-
ty in their lives (Datillo, Estrella et al 2007). 
Some work has suggested that the use of pre-stored 
conversational material based on conversation models 
could help increase communication rate and conversa-
tion quality. Alm (1988) showed that it is possible to 
successfully model short ?chat? conversations involving 
greetings, personal enquiries and small-talk.  Further-
more, the TALK system allowed a user to pre-store a 
large volume of material on specific topics so that 
whole utterances could be selected and output. The sys-
tem also made heavy use of quick-fire phrases, classes 
of regularly used utterance which could be accessed 
quickly, and showed that communication using solely 
pre-stored material was viable (Todman and Alm 2003). 
Despite encouraging results and the development of a 
commercial product, the phrase-storage approach to 
social communication has not gained wide popularity. 
The reasons for this are complex, but include: the rela-
tive inflexibility of pre-stored material; the costs asso-
ciated with authoring the material and keeping the 
material up-to-date; and the vastly different nature of 
the approach and different training requirements neces-
sary to achieve success. 
2.2 The role of NLP in AAC 
NLP technology has provided many benefits to AAC 
system designers. Possibly the first technology to be 
included in many commercial systems to date was word 
prediction and completion. There have also been many 
research prototypes exploring the applicability of more 
emerging technologies such as named entity recognition 
from synthesized speech (Wisenburn and Higginbotham 
2008), the generation of well-formed utterances from 
telegraphic input (McCoy, Pennington et al 1998) and 
the automatic identification of contextual vocabulary 
from the web (Higginbotham, Bisantz et al 2008). 
Netzer and Elhadad (2006) used NLG to allow the se-
mantic authoring of utterances. 
However, NLG, in the sense of data-to-text (Reiter 
and Dale 2000), has had limited application within AAC 
thus far, although Reiter et al (2009) showed it is possi-
ble to generate stories from sensor data which allow a 
child using AAC to tell others about their day at school. 
2.3 System Rationale 
This project is exploring the use of NLG to produce 
conversational utterances in AAC systems designed for 
social interaction.  At the outset it was hoped that using 
NLG might address some of the difficulties observed in 
pre-storage systems. For instance, the generation com-
ponent could theoretically produce a range of utterances 
and speech act types automatically from the same un-
derlying data and adapt these somewhat to the interac-
tional context.  Using NLG would also have the benefit 
of offering control over the well-formedness of the out-
put, an important consideration given the difficulty 
some AAC users have in achieving literacy (Sandberg 
and Hjelmquist 1997). The fact that the system has an 
inherent awareness of the semantic content of the lin-
guistic output, rather than simply being stored as canned 
text, is also a potential benefit. In other words, NLG 
might offer a level of automaticity and flexibility that 
traditional pre-storage systems cannot offer, as well as 
potentially reducing the level of pre-authoring required 
from the user.  
3 System Development 
3.1 User-centered methodology 
To try to assess how useful NLG could be in this 
context we adopted a user-centered approach to the de-
sign of the system. A group of 3 AAC users has been 
recruited, all of whom currently use some form of high-
tech AAC. Literacy amongst the group is varied. Two of 
the individuals use the alphabetic keyboard-based 
Lightwriter communication device currently, and have 
normal cognitive and visual-perceptive skills.  All of the 
users have cerebral palsy and dysarthria, and have been 
involved in previous software evaluations.  
Weekly or twice weekly sessions were held with 
each user for several months while the software was 
being produced. Sessions consisted of various activities: 
11
discussion about the user?s ideas for the software and 
technology; the identification of topics and collation of 
input data to the system; demonstrations of the new fea-
tures or changes since the last session; system training; 
and dry-run test conversations between the investigators 
and the users. 
3.2 System Architecture 
A growing line of inquiry in the NLG community is 
the generation of language from ontologies (Mellish and 
Sun 2005).  An ontology is a logical and hierarchical 
model of the different concepts and the nature of rela-
tionships between concepts in a particular domain. 
These concepts and relationships can be mapped onto 
linguistic constructs to allow for the production of natu-
ral language descriptions (Karakatsiotis, Galanis et al 
2008) of parts of the ontology. 
In the case of our system, we are trying to model 
conversational topics that would be of interest in social 
conversation between users of the system and their co-
conversationalists. The current categories of topic we 
are experimenting with include travelling, listening to 
music, watching films and attending concerts.  Many 
categories are based on a simple event model which 
defines the basic characteristics common to all events, 
such as a time of occurrence (see Fig.1). We have also 
included concepts such as Person and Place which are 
associated with events to form a logical model of a par-
ticular event type.  
A separate file is created unique to each user which is 
linked to the original model. This is filled with individ-
uals consisting of data from the user. In other words, 
rather than defining the concept of an event as we did 
with the original ontology, here we are creating a de-
scription of an actual event and any other details, such 
as people or places, associated with it. We have defined 
our ontology in OWL, a standard language for the defi-
nition of ontologies, and each piece of knowledge is 
effectively stored as a RDF Triple consisting of a sub-
ject, predicate and object.   
 
 
 
 
 
 
 
 
 
 
Figure 1: The abstract event model 
The user?s knowledge base is turned into useful con-
versational utterances through a template-driven utter-
ance generation system (e.g. Van Deemter, Krahmer et 
al. 2005).  A large set of templates has been authored, 
using the SimpleNLG programming interface, which 
turn data from the onotlogy into natural language utter-
ances. The templates are created as concrete syntax 
trees containing unspecified ?slots? and parameters (See 
Fig.2).  These syntax trees map out the syntactic struc-
ture of the template), and are linked to a particular class 
in the ontology so that only appropriate templates are 
applied to each individual.   Slots are used to add con-
textually relevant clauses to our utterances. For exam-
ple, a template might contain a ?time?  slot, the contents 
of which are derived from the time of the event in ques-
tion.  For instance, the slot might be filled with ?next 
Tuesday evening?, ?a month ago? or ?this morning? 
depending on the context. Example parameters include 
the tense with which the utterance should be generated, 
and whether a pronoun or full noun phrase should be 
used to refer to the subject of the utterance. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: An example syntax tree with empty slots 
In addition to the language produced from the model 
and knowledge base we have included the ability to add 
canned text phrases to each individual.  
This is necessary because there may be things that 
you wish to be able say about a topic which it is not 
feasible to model.  Because we have a fairly diverse set 
of topics it is simply not possible to model all aspects of 
these topics in a reasonable time.  There is effectively a 
trade-off between complexity of the model and how 
maintainable and representative it is. A more complex 
model will lead to more expressive generated language, 
but will cost a great deal more to design and maintain. 
In the case of our system, a ?lowest common denomina-
tor? domain model combined with additional canned 
text has proven to be a relatively straightforward and 
inexpensive design. 
12
The system has also been designed to learn over time 
the sequences of utterances a user selects and suggest 
next moves based on past behavior.  The system does 
this by maintaining a directional weighted graph which 
records sequences of utterances as they are used.  The 
graph works by recording each individual utterance as a 
node in the graph and creating relationships between 
these nodes as they occur. The more often two utter-
ances appear in sequence the higher the value given to 
the edge between the two corresponding nodes. 
3.3 Conversation model and interface issues 
Perhaps the most challenging aspect of taking the 
system from initial concept to working prototype has 
been finding the most effective way of interfacing the 
technology. We have found that due to the complexity 
of the underlying technology, reaching the stage where 
generated utterances are both useful and accessible to 
the user during conversation has required careful con-
sideration and the trialing of several approaches with the 
user group. 
It was envisaged that the generative power of NLG 
would be its most powerful benefit. The system could 
realize the same piece of data as numerous speech act 
types and, within a speech type, in several different 
phrasings. This offered the ability to counteract the in-
flexibility and uniformity of pre-stored utterances 
somewhat.  However, we have had mixed success in 
achieving this goal as it has proven difficult to find an 
effective way to interface this enhanced choice and va-
riety to the user.  If there is a large volume of generated 
utterances available to choose from we must provide an 
efficient means by which the material is presented or 
organized so that the desired utterance can be located 
quickly. If a large choice results in a delayed selection 
and thus conversational turn, we may then lose any rate 
and speed of response benefits which would negate the 
need to use pre-stored and generated material at all. 
To address this problem, we attempted to design a 
conversational model which controlled the generation of 
utterances so that only the utterances deemed most like-
ly were presented to the user, thus reducing the cogni-
tive load required to search through a large set. This was 
done using a basic system where the templates were 
tagged according to where it might be most likely to be 
used in a conversation on a topic. For instance, a tem-
plate might produce a pre-sequence, an introduction, 
elaboration or concluding remark, or it may produce a 
interrogative. With the addition of historical sequential 
moves from our directional graph we could begin to 
present subsets of utterances to the user according to 
where they were in topic development. 
Another approach trialed was inspired by the Gricean 
maxim of quantity. Each template contains meta-data 
about the information it expresses. For each generated 
utterance selected, we can ?rule out? further generation 
of the same information. This is based on the assump-
tion that speakers will generally avoid repetition. We 
have found that this technique provides a useful way of 
supporting discourse coherence within conversations. 
Finally, using the logical model of topics we have 
created, it is possible to support and model stepwise 
topic progression. We can suggest, based on the model 
and the user knowledge base, other topics linked to the 
one currently selected. For example, if we were talking 
about an upcoming holiday to London with a friend 
called Bob we may want to the change topical perspec-
tive to related aspects of the trip. We might want to talk 
about London as a place, Bob as a friend, and other trips 
we have taken with Bob or to London. Because these 
concepts are all distinct within the model, they each 
have their own set of associated templates and result in 
sets of candidate utterances with differing perspectives. 
Navigating to related topics in this manner should be 
quicker since related topics do not have to be located 
manually. Although the users are still being trained in 
this approach to topic change, early evaluations are 
promising. It enables a ?one-click? transition to related 
topics, allowing the user to elaborate on certain aspects 
of a previous topic and respond quickly to questions 
from their conversational partners. 
Building on the last two mechanisms, we can also 
generate bridging phrases which allow for more cohe-
sive changes in topic. This allows for a more eloquent 
transition to a new topic and also aids the discourse co-
herence.  
All of these approaches in fact belie, to some degree, 
the complexity of conversation. By its very nature, con-
versation is unpredictable, and the purpose and meaning 
of sequential moves are highly dependent on their con-
text (Clark 1996). However, any form of context identi-
fication, such as speech recognition (Wisenburn and 
Higginbotham 2008), is likely to present a major tech-
nical challenge in any production AAC system at the 
current time. The above are simply at attempt to model, 
using the NLP/AI techniques available, aspects of 
communication process, to show the potential benefits 
when using NLG-produced utterances rather than sim-
ple canned text utterances. 
Application of some of the above techniques resulted 
in a highly fluid interface in which the utterances dis-
played changed rapidly according to the conversation 
model. This presented a major challenge to users learn-
ing the system, with all displaying a strong preference 
for a static interface where the same utterances could be 
found in the same location each time they were desired.  
13
Table 1: An example conversation produced 
using the system. Speaker A is the user and speaker 
B is an unaided speaker. The right-hand column 
shows the interface selections necessary prior to 
selecting the utterance from a set of possibilities. 
The marker G represents a generated utterance, C 
represents canned text. The remainder are quick-
fire utterances. 
We believe this does not suggest that use of such 
conversational models and semantic processing is not 
feasible, but simply that in the scope of the current 
work it has not been possible to fully evaluate their 
potential. Thus we have chosen to generate candidate 
phrases in a static manner without the predictive as-
pects described above. These changes have allowed for 
quicker achievement of proficiency and have lowered 
the cognitive effort required to navigate the interface.    
In the latest version of the software, we have de-
fined a set of templates for each topic which when 
realized in series produce a coherent narrative. They 
can still be selected individually by the user for output, 
so they retain ultimate control of what is said, but the 
utterances are presented in a natural order. This means 
that the user can easily make use of the utterances as a 
narrative or can choose according to the particular sit-
uation and context.  Any interrogative templates are 
displayed in a different part of the interface. We have 
set up a two column display so that interrogatives and 
other statements are clearly delineated.  
This approach has had very promising results as we 
have found that users no longer have to search through 
a list of suggestions which changes after each conver-
sational turn. They can also use the structured nature 
of the generated utterances to confidently introduce the 
different topics in conversation.  We are finding some 
evidence of increased self-selection at the end of their 
current turn as the user is easily able to continue their 
narrative automatically without having to worry about 
the location of their next turn in the interface. There is 
some other evidence of this structured application of 
NLG to narrative as being a promising area (Reiter, 
Turner et al 2009). 
We also believe that the passivity and lack of initia-
tion observed in AAC users could be positively ad-
dressed if AAC systems can better support a more 
varied communicative repertoire and suitable training 
is administered to show users how to confidently use 
these different constructs (e.g. Todman 2000). Early 
training sessions with our user group have again 
proved positive with increased use of the trained fea-
tures and interaction styles.  
 UTTERANCE USER 
SELECTION 
A: Hi Robert [GREET] 
B:      Oh, Hi. Nice to see you.  
A: And you. [GREET] 
A: How?s it going? [INTRO] 
B: Fine. And you?  
A: Not too bad. [INTRO] 
B: So you been keeping busy?  
A: Yeah [YES] 
A: I certainly have! [YES] 
 
A: 
 
I was out at a concert on Thursday 
night. (G) 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
B: Great. Who did you go to see?  
 
A: 
 
Have you heard of Martin Taylor? 
(G) 
[ARTIST] 
[Select ?Mar-
tin Taylor?] 
B: No.....I don?t think so.  
A: He is a Jazz guitarist. (G) [Select ?Mar-
tin Taylor?] 
B: Oh, great. I like jazz music.  
A: Me too. [AGREE] 
B: So how was the concert?  
 
A: 
 
It was really good. (G) 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
A: John and David came with me. (G)  
A: We all enjoyed it. (C)  
A: We had a bit of an interesting jour-
ney home because it was snowing 
heavily, but we made it back safe. 
(C) 
 
B:     Well that?s good news. Where was 
the concert? 
 
 
A: 
 
 
It was at the Tron Theatre in Glas-
gow. (G) 
I?ve been to Glasgow a few times 
lately. [G] 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
[Select ?Glas-
gow?] 
A: Anyway, I best be getting on. [WRAP UP] 
A: It was great talking to you. [WRAP UP] 
B:      Yes, likewise.  
B: See you soon.  
A: OK. Cheerio. [FINISH] 
B: Bye  
14
  
 
 
 
 
 
 
 
 
 
 
3.4 Authoring user content 
Currently we have not managed to produce a tool that 
the user can use to update their knowledge base them-
selves. The ontology editing tool used in the program, 
Prot?g?, is a free academic software package designed 
for knowledge engineers and thus has a high degree of 
internal complexity and takes time to learn. It is also not 
a particularly accessible piece of software. 
We have worked with the users to build up their 
knowledge bases over a series of meetings by allowing 
them to suggest individuals to add while entering the 
details for them into the system. The process of defining 
new individual is very quick, usually requiring the input 
of just a few words and selection of the associated indi-
viduals. However, one of the main criticisms on the part 
of the users is that for the system to be useful in the long 
term, it must be kept up to date, as old material will 
quickly become less relevant and useful in less frequent 
situations.  For this reason it is critical to the success of 
any NLG-driven communication system that the data 
input is as simple and seamless as possible. 
We have shown in our system that it is possible to get 
some limited data automatically from online sources, 
rather than having to input it manually. Many web ser-
vices are being made available which enable program-
mers to access data from online services in their 
applications. For instance, both Amazon and YouTube 
have their own APIs which allow 3rd party applications 
to request content information from these services.  
The notion of the semantic web is also related to this. 
There is a large effort underway to define how we might 
structure and link information on the web in such a way 
that more of it can be processed automatically by com-
puters and made available in interchangeable formats. 
Shared data and semantic web technologies such as 
these operate on the same premise as our proposed 
communication system in that they map out the basic 
vocabulary required to describe a domain, and allow 
people describe aspects of the domain in these terms.  
We have used an API provided by social music web-
site Last.fm to show that it is possible to create relevant 
conversational utterances without any authoring re-
quirement whatsoever. By supplying the users Last.fm 
username, we can use a web service supplied by the site 
to query the user?s recent activity, for instance the songs 
they have listened to, songs rated highly or events which 
they have signed up to attend. Because the output from 
these services comes as structured XML document we 
can simply map it?s schema onto our own vocabulary 
and feed the appropriate data to our templates to pro-
duce utterances. 
If web services are to be used we must have an 
equivalent local vocabulary to which we can map the 
data returned from any queries we send the service. 
However, in the case of semantic web sources, for ex-
ample the FOAF (friend-of-a-friend) vocabulary (Brick-
ley) describing online social networks, the process is 
simpler as we can simply use the pre-existing vocabu-
lary standard ourselves rather than having to develop 
our own. Despite the semantic web being in its infancy, 
the notion of shared data is growing in popularity and 
many popular websites and organizations are providing 
access to their information in a structured way. 
One problem with using these types of data acquisi-
tion methods for our purposes is that the data is largely 
generic and any personal opinion or evaluative informa-
tion personal to the user is limited. In some cases we 
may be able to query the data source for a rating 
awarded to a particular piece of content, for instance the 
star rating system on YouTube, but it is not clear how 
expressive the produced language will be since the 
process is likely to be a simple mapping from the rating 
to a suitable adjective. As in our system, the potential 
usefulness of the generated language is likely to be in-
creased if it is possible for the user to annotate the top-
ics with their own canned text expressions and 
evaluations. This will enable the system to express more 
of the individual?s personality and opinions. 
We believe this is an area of great interest for AAC. 
There is growing evidence of the importance of the in-
ternet in the lives of disabled people, particularly its role 
as a communication medium for people with communi-
cation impairments (Cohen 1999). By harnessing the 
large volumes of data created when using modern hard-
ware and software systems and transforming it into use-
ful utterances, we can begin to address one of the main 
criticisms of whole utterance approaches to AAC since 
there would be no authoring requirement on the part of 
the users.   This is certainly by no means a simple 
process and this approach will require further investiga-
tion, but as semantic web technologies reach maturity 
Figure 3 - System interface 
15
and gain wider adoption it should be clearer what the 
potential of the technology is. 
4 Formal Evaluation Methodology 
In our evaluations so far, we have concentrated on 
training the users in its operation, updating conversa-
tional material and implementing changes based on the 
user feedback. We have recently begun testing the sys-
tem in real conversational encounters and the results 
have been promising. We have found it is possible to 
hold pleasing conversations lasting up to 20 minutes 
with unfamiliar partners, with the aided communicator 
achieving a rate of upwards of 40 wpm. 
There also seems to be higher incidences of initia-
tions on the part of the user, with them making good use 
of both the scripted NLG material, the quick fire phrases 
and their own pre-stored material. The topic progression 
feature is currently being underused but subjects are 
responding well to training sessions on how to incorpo-
rate this to reduce their response time and expand on 
topics to extend the amount they are able to say. 
Formal evaluations are now being undertaken. An 
AB multiple-baseline study design is being conducted in 
which each aided communicator has a series of conver-
sations with 12 unknown and unaided conversation 
partners.  In the A condition, the aided participants use 
their existing AAC system, while in the B condition 
they use our prototype system. Each conversation will 
be limited to approximately 10 minutes, and the ses-
sions will be split across a three non-consecutive days to 
avoid user fatigue.  
There will be at least 3 conversations in both the A 
and B conditions, and the intervention point will be ran-
domized across the remaining 6 conversations to allow 
for valid inferences to be made despite the small n value 
(Todman and Dugard 2001). This also reduces the bias 
introduced by any training effects and avoids the need 
to use a response-guided intervention after baseline per-
formance has been established. The difficulty and ex-
pense of recruiting large numbers of subjects in AAC 
studies is a known problem (Higginbotham 1995) and 
therefore any findings from quantitative analysis per-
formed cannot be generalized across the AAC popula-
tion. However, we expect to be able to achieve a p value 
using the randomization design of <0.05 so the results 
should at least be internally robust and give a good indi-
cation of whether further investigation is warranted.   
The conversations will be audio-recorded and ana-
lyzed for a number of metrics. Primarily we are interest-
ed in measuring the rate at which people are able to 
communicate using the new system as this seems to be 
one of the clearest indicators of success when evaluating 
a new AAC intervention. We expect to the effect size 
observed across the conditions to be large.  
We are also particularly interested whether it is poss-
ible to use automatically generated material while main-
taining or enhancing the enjoyment and quality of the 
encounter for all participants.  It is still unclear how 
acceptable generated material will be to the user so we 
will measure the relative frequency of generated and 
canned-text utterances.  
In previous studies it has also been shown that the 
use of a whole-utterance approach can change the dy-
namics of communication, such as relative speech act 
distribution and number and type of initiation, so we are 
interested to see how the availability generated material 
might impact this and what role it might play. A coding 
schema based on Wang (2007) will be used to categor-
ize the utterances used. 
We are also asking the aided and unaided conversa-
tion partners to complete questionnaires regarding vari-
ous subjective ratings of the interactions and, in the case 
of the unaided speakers, impressions of the aided com-
municator. The questions will be based on a re-
evaluation of those suggested by Todman (2000) and 
answers will be requested on a 7-point rating scale. Pre-
vious work has shown that quicker, flowing interactions 
with less breakdowns or delays can lead to more re-
warding interactions for both participants. We expect to 
observe these effects in our system but it?s as yet un-
clear what impact the automatically generated phrases 
will have, if any, on perceptions of the user.  
Although the relatively small number of participants 
means it is unlikely that we will be able to make robust 
inferences from this data, we hope that results will be 
indicative of the naturalness and acceptability of auto-
matically generated utterances. 
5 Discussion & Future Plans 
One of the primary reasons that AAC systems featur-
ing NLP technology prove useful is that they go some 
way to leveling the playing field for many users. They 
have the potential to support the user in ways which 
reduce the effort required to communicate yet may im-
prove the quality of the communication. There are many 
NLP technologies, such as NLG, that deserve further 
attention within the field of AAC to determine what 
they can offer.  
Although our system has shown some encouraging 
preliminary results there are still many unanswered 
questions with regards to the role NLG can play.  For 
example, it is not clear how appealing NLG utterances 
16
are to use. Given that the user has not authored the form 
of the utterances themselves there is an argument that 
using them may feel unnatural. After the formal evalua-
tions we should be able provide analysis indicating 
whether NLG phrases are being used and in which sit-
uations they are proving most useful.  
One of the most challenging aspects of designing the 
system was the HCI challenge of incorporating some of 
these technologies. While it is obvious to the user that 
phrases are being generated automatically, and that 
these phrases are generated when a topic is selected, it is 
still important to note that the technologies have been 
intentionally kept largely transparent to the user. When 
using a communication system, the most important 
thing is the ability to say what you want to say, but is 
not yet clear whether the technical nature of the soft-
ware  may be an alienating factor since the user current-
ly has no access to the template construction or domain 
modeling aspects of the system. 
At the current time, the domain modeling and tem-
plate construction processes are quite complex and ex-
pensive. Tools are becoming available, from the NLG 
community, which go some way to addressing the diffi-
culty of interfacing these types of technology to non-
experts (Bilidas, Theologou et al 2007; Power, Stevens 
et al 2009) but these are largely unsolved problems. 
Domain modeling itself is problematic in that one 
persons notion of what defines a particular concept is 
often different to someone else?s. For instance, one per-
son?s idea of sport might encompass the sporting activi-
ties they take part in, while another person?s idea of 
sport is that which they follow or watch on the televi-
sion. This has clear implications for the general usabili-
ty of the system.  Using semantic web vocabularies may 
address this somewhat since they are likely to be more 
specific to a particular purpose and be more mature and 
interoperable than the ?home-brew? domain models we 
have used for the prototype. 
Using whole-utterance approaches to communication 
clearly requires the adoption of a different mindset. Ra-
ther than being able to construct a novel message the 
user has to ?make do? with whatever is available in the 
system. Despite the advantages observed while using 
such systems, they have still not become generally pop-
ular. It is likely that any NLG whole utterance system 
would similarly not gain immediate acceptance because 
it is vastly different to other systems and approaches to 
communication available. To some degree we are ask-
ing the user of our NLG system to think in an object 
orientated manner since they must understand the un-
derlying model and the way the concepts are structured 
to make the most of the system. Again it is not yet clear 
how natural this process is and how much training is 
required to become an expert user of such a system. 
However perhaps the major strength of these types of 
system is the way in which they help scaffold interac-
tion so the AAC user can be much more active in con-
versation and use an increased repertoire. The design of 
the software is such that it encourages the use of types 
of phrases often underused by AAC users, for example, 
initiations, elaborating moves, questions and the differ-
ent classes of quick-fire remarks. One interesting ques-
tion is whether the use of NLG might make it easier to 
encourage the user to use new types of conversational 
move. Since no full text-authoring is required the user 
does not even have feel confident authoring the utter-
ance, it is simply provided and can be used or experi-
mented with.  Scaffolding interactions in this way may 
be one of the most interesting avenues for NLP and AI 
technologies with AAC in the future. 
 The architecture of the prototype, although effective, 
lacks efficiency and may be difficult to reuse. A great 
deal of work is being done by NLG researchers investi-
gating how NLG architectures might be made more 
modular and reusable. This is an ongoing problem but it 
seems sensible to consider how a pipeline architecture 
(Reiter and Dale 2000) might work in practice for this 
type of system. 
At the moment, the system requires a reasonable lev-
el of literacy because the interface is mainly text based. 
However, semiotic systems are preferred because of the 
literacy problems observed in many AAC users. It is not 
clear how NLG may impact on semiotic message con-
struction but systems such as Compansion (McCoy, 
Pennington et al 1998) show there may useful applica-
tions in this area too. 
6 Conclusion 
Despite having only been able to perform informal 
evaluations so far, we believe we have seen some en-
couraging signals that NLG may have potential as an 
augmentative communication technology to assist in 
generating conversational utterances.  We believe that 
the rapid access to well-formed, contextually generated 
material offered in our system could lead to significant 
benefits for the AAC user and their interlocutors. 
There are further exciting possibilities with regards to 
the technology, particularly the ability to harvest per-
sonal data from the internet and other computer usage 
so that it can be transformed into useful phrases for in-
clusion in communication aids. We hope to have a rich-
er set of data and results in the coming months after the 
system training and formal evaluations have been com-
pleted.  
17
7 References 
Alm, N. A. (1988). Towards a Conversation Aid for 
Severely Phisically Disabled Non-Speaking People. 
Applied Computing Department. Dundee, University 
Of Dundee. Doctor Philosophy: 197. 
Bilidas, D., M. Theologou, et al (2007). Enriching 
OWL Ontologies with Linguistic and User-related 
Annotations: the ELEON system. 19th IEEE Interna-
tional Conference on Tools with Artificial Intelli-
gence, IEEE. 
Brickley, D. (25.2.10). "FOAF Vocabulary Specifica-
tion ", from http://xmlns.com/foaf/0.1/  
Clark, H. H. (1996). Using Language, Cambridge Uni-
versity Press. 
Cohen, K. J. (1999). Using the Internet to Empower 
Augmented Communicators. CSUN'99. 
Datillo, J., G. Estrella, et al (2007). ""I have chosen to 
live life abundantly": Perceptions of leisure by adults 
who use Augmentative and Alternative Communica-
tion." Augmentative & Alternative Communication 
24(1): 16-28. 
Higginbotham, D. J. (1995). "Use of nondisabled sub-
jects in AAC Research : Confessions of a research in-
fidel." Augmentative and Alternative Communication 
11(1): 2-5. 
Higginbotham, D. J., A. M. Bisantz, et al (2008). "The 
Effect of Context Priming and Task Type on Augmen-
tative Communication Performance." Augmentative & 
Alternative Communication. 
Higginbotham, D. J., H. Shane, et al (2007). "Access to 
AAC: Present, past, and future." Augmentative & Al-
ternative Communication 23(3): 243-257. 
Karakatsiotis, G., D. Galanis, et al (2008). Natura-
lOWL: Generating Texts from OWL Ontologies in 
Protege and in Second Life. 18th European Confe-
rence on Artificial Intelligence. 
Light, J. (1988). "Interaction Involving Individuals us-
ing Augmentative and Alternative Communication 
Systems: State of the Art and Future Directions." 
Augmentative and Alternative Communication 4(2): 
66-82. 
McCarthy, J. and J. Light (2005). "Attitudes towards 
individuals who use Augmentative and Alternative 
Communication: Research Review." Augmentative 
and Alternative Communication 21(1): 41-55. 
McCoy, K. F., C. A. Pennington, et al (1998). "Com-
pansion: From research prototype to practical integra-
tion." Natural Language Engineering 4(1): 73-95. 
Mellish, C. and X. Sun (2005). The Semantic Web as a 
Linguistic Resource: Opportunities for Natural Lan-
guage Generation. International Conference on theory, 
practical and application of Artificial Intelligence. M. 
Bramer, F. Coenen and T. Allen. Cambridge, UK, 
Springer: 77. 
Netzer, Y. and M. Elhadad (2006). Using Semantic Au-
thoring for Blissymbols Communication Boards. 
HLT-2006. 
Power, R., R. Stevens, et al (2009). Editing OWL 
through generated CNL. Workshop on Controlled 
Natural Language (CNL'09). Marettimo Island, Italy. 
Reiter, E. and R. Dale (2000). Building Natural Lan-
guage Generation Systems. Cambridge Cambridge 
University Press. 
Reiter, E., R. Turner, et al (2009). Using NLG to help 
language-impaired users tell stories and participate in 
social dialogues. Proceedings of the 12th European 
Workshop on Natural Language Generation. Athens, 
Greece, ACL. 
Sandberg, A. D. and E. Hjelmquist (1997). " Language 
and literacy in nonvocal children with cerebral palsy." 
Reading and Writing 9(2): 107-133. 
Todman, J. (2000). "Rate and quality of conversations 
using a text-storage AAC system: Single-case training 
study." Augmentative and Alternative Communication 
16: 164-179. 
Todman, J. and N. A. Alm (2003). "Modelling conver-
sational pragmatics in communication aids." Journal 
of Pragmatics(35): 523-538. 
Todman, J. and P. Dugard (2001). Single-case and 
small-n experimental designs: A practical guide to 
randomisation tests. Mahwah, NJ, Lawrence Erlbaum 
Associates. 
Todman, J. and H. Rzepecka (2003). "Effect of pre-
utterance pause length on perceptions of communica-
tive competence in AAC-aided social conversations." 
Augmentative and Alternative Communication 19(4): 
222-234. 
Van Deemter, K., E. Krahmer, et al (2005). "Plan-based 
vs. Template-based NLG: A false opposition?" Com-
putational Linguistics 31(1). 
Waller, A. (1992). Providing Narratives in an Augmen-
tative Communication System. Applied Computing. 
Dundee, University Of Dundee. Doctor of Philoso-
phy: 163. 
Wang, Y. (2007). A model of conversational structure 
for augmentative and alternative communication 
(AAC) systems. University of Dundee, Unpublished 
PhD Thesis. PhD. 
Wisenburn, B. and D. J. Higginbotham (2008). "An 
AAC Application Using Speaking Partner Speech 
Recognition to Automatically Produce Contextually 
Relevant Utterances: Objective Results." Augmenta-
tive and Alternative Communication 24(2): 100-109. 
 
 
 
18
