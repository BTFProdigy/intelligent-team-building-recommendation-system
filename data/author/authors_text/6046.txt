Data-driven Generation of Emphatic Facial Displays
Mary Ellen Foster
Department of Informatics, Technical University of Munich
Boltzmannstra?e 3, 85748 Garching, Germany
foster@in.tum.de
Jon Oberlander
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom
jon@inf.ed.ac.uk
Abstract
We describe an implementation of data-
driven selection of emphatic facial dis-
plays for an embodied conversational
agent in a dialogue system. A corpus of
sentences in the domain of the target dia-
logue system was recorded, and the facial
displays used by the speaker were anno-
tated. The data from those recordings was
used in a range of models for generating
facial displays, each model making use of
a different amount of context or choosing
displays differently within a context. The
models were evaluated in two ways: by
cross-validation against the corpus, and by
asking users to rate the output. The predic-
tions of the cross-validation study differed
from the actual user ratings. While the
cross-validation gave the highest scores to
models making a majority choice within a
context, the user study showed a signifi-
cant preference for models that produced
more variation. This preference was espe-
cially strong among the female subjects.
1 Introduction
It has long been documented that there are char-
acteristic facial displays that accompany the em-
phasised parts of spoken utterances. For example,
Ekman (1979) says that eyebrow raises ?appear to
coincide with primary vocal stress, or more sim-
ply with a word that is spoken more loudly.? Cor-
relations have also been found between prosodic
features and events such as head nodding and the
amplitude of mouth movements. When Krah-
mer and Swerts (2004) performed an empirical,
cross-linguistic evaluation of the influence of brow
movements on the perception of prosodic stress,
they found that subjects preferred eyebrow move-
ments to be correlated with the most prominent
word in an utterance and that eyebrow movements
boosted the perceived prominence of the word
they were associated with.
While many facial displays have been shown
to co-occur with prosodic accents, the converse
is not true: in normal embodied speech, many
pitch accents and other prosodic events are unac-
companied by any facial display, and when dis-
plays are used, the selection varies widely. Cas-
sell and Tho?risson (1999) demonstrated that ?en-
velope? facial displays related to the process of
conversation have a greater impact on successful
interaction with an embodied conversational agent
than do emotional displays. However, no descrip-
tion of face motion is sufficiently detailed that it
can be used as the basis for selecting emphatic fa-
cial displays for an agent. This is therefore a task
for which data-driven techniques are beneficial.
In this paper, we address the task of selecting
emphatic facial displays for the talking head in
the COMIC1 multimodal dialogue system. In the
basic COMIC process for generating multimodal
output (Foster et al, 2005), facial displays are se-
lected using simple rules based only on the pitch
accents specified by the text generation system. In
order to make a more sophisticated and naturalis-
tic selection of facial displays, we recorded a sin-
gle speaker reading a set of sentences drawn from
the COMIC domain, and annotated the facial dis-
plays that he used and the contexts in which he
used them. We then created models based on the
data from this corpus and used them to choose the
facial displays for the COMIC talking head.
1http://www.hcrc.ed.ac.uk/comic/
353
The rest of this paper is arranged as follows.
First, in Section 2, we describe previous ap-
proaches to selecting non-verbal behaviour for
embodied conversational agents. In Section 3, we
then show how we collected and annotated a cor-
pus of facial displays, and give some generalisa-
tions about the range of displays found in the cor-
pus. After that, in Section 4, we outline how we
implemented a range of models for selecting be-
haviours for the COMIC agent using the corpus
data, using varying amounts of context and differ-
ent selection strategies within a context. Next, we
give the results of two evaluation studies compar-
ing the quality of the output generated by the var-
ious models: a cross-validation study against the
corpus (Section 5) and a direct user evaluation of
the output (Section 6). In Section 7, we discuss the
results of these two evaluations. Finally, in Sec-
tion 8, we draw some conclusions from the current
study and outline potential follow-up work.
2 Choosing Non-Verbal Behaviour for
Embodied Conversational Agents
Embodied Conversational Agents (ECAs) are
computer interfaces that are represented as hu-
man bodies, and that use their face and body in
a human-like way in conversations with the user
(Cassell et al, 2000). The main benefit of ECAs
is that they allow users to interact with a computer
in the most natural possible setting: face-to-face
conversation. However, to realise this advantage
fully, the agent must produce high-quality output,
both verbal and non-verbal. A number of previous
systems have based the choice of non-verbal be-
haviours for an ECA on the behaviours of humans
in conversational situations. The implementations
vary as to how directly they use the human data.
In some systems, motion specifications for the
agent are created from scratch, using rules derived
from studying human behaviour. For the REA
agent (Cassell et al, 2001a), for example, ges-
turing behaviour was selected to perform particu-
lar communicative functions, using rules based on
studies of typical North American non-verbal dis-
plays. Similarly, the Greta agent (de Carolis et al,
2002) selected its performative facial displays us-
ing hand-crafted rules to map from affective states
to facial motions. Such implementations do not
make direct use of any recorded human motions;
this means that they generate average behaviours
from a range of people, but it is difficult to adapt
them to reproduce the behaviour of an individual.
In contrast, other ECA implementations have
selected non-verbal behaviour based directly on
motion-capture recordings of humans. Stone et al
(2004), for example, recorded an actor performing
scripted output in the domain of the target system.
They then segmented the recordings into coher-
ent phrases and annotated them with the relevant
semantic and pragmatic information, and com-
bined the segments at run-time to produce com-
plete performance specifications that were then
played back on the agent. Cunningham et al
(2004) and Shimodaira et al (2005) used similar
techniques to base the appearance and motions of
their talking heads directly on recordings of hu-
man faces. This technique is able to produce more
naturalistic output than the more rule-based sys-
tems described above; however, capturing the mo-
tion requires specialised hardware, and the agent
must be implemented in such a way that it can ex-
actly reproduce the human motions.
A middle ground is to use a purely synthetic
agent?one whose behaviour is controlled by
high-level instructions, rather than based directly
on human motions?but to create the instructions
for that agent using the data from an annotated cor-
pus of human behaviour. Like a motion-capture
implementation, this technique can also produce
increased naturalism in the output and also al-
lows choices to be based on the motions of a sin-
gle performer if necessary. However, annotating
a video corpus can be less technically demand-
ing than capturing and directly re-using real mo-
tions, especially when the corpus and the number
of features under consideration are small. This ap-
proach has been taken, for example, by Cassell
et al (2001b) to choose posture shifts for REA,
and by Kipp (2004) to select gestures for agents,
and it is also the approach that we adopt here.
3 Recording and Annotation
The recording script for the data collection con-
sisted of 444 sentences in the domain of the
COMIC multimodal dialogue system; all of the
sentences described one or more features of one or
more bathroom-tile designs. The sentences were
generated by the full COMIC output planner, and
were selected to provide coverage of all of the
syntactic patterns available to the system. In ad-
dition to the surface text, each sentence included
all of the contextual information from the COMIC
354
46. More about the current design
they dislike the first feature, but like the second one
There are GEOMETRIC SHAPES on the
decorative tiles, but the tiles ARE from the
ARMONIE series.
Figure 1: Sample prompt slide
planner: the predicted pitch accents?selected ac-
cording to Steedman?s (2000) theory of informa-
tion structure and intonation?along with any in-
formation from the user model and dialogue his-
tory. The sentences were presented one at a time
to the speaker, who was instructed to read each
sentence out loud as expressively as possible while
looking into a camera directed at his face. The seg-
ments for which the presentation planner specified
pitch accents were highlighted, and any applicable
user-model and dialogue-history information was
included. Figure 1 shows a sample prompt slide.
The recorded videos were annotated by the first
author, using a purpose-built tool that allowed any
set of facial displays to be associated with any seg-
ment of the sentence. First, the video was split into
clips corresponding to each sentence. After that,
the facial displays in each clip were annotated.
The following were the displays that were consid-
ered: eyebrow raising and lowering; eye squinting;
head nodding (up, small down, large down); head
leaning (left and right); and head turning (left and
right). Figure 2 shows examples of two typical
display combinations. Any combination of these
facial displays could be associated with any of the
relevant segments in the text. The relevant seg-
ments included all mentions of tile-design prop-
erties (e.g., colours, designers), modifiers such
as once again and also, deictic determiners (this,
these), and verbs in contrastive contexts (e.g., are
in Figure 1). The annotation scheme treated all fa-
cial displays as batons rather than underliners (Ek-
man, 1979); that is, each display was associated
with a single segment. If a facial display spanned
a longer phrase in the speech, it was annotated as a
series of identical batons on each of the segments.
Any predicted pitch accents and dialogue-
history and user-model information from the
COMIC presentation planner were also associated
with each segment, as appropriate. We chose not
to restrict our annotation to those segments with
predicted pitch accents, because the speaker also
made a large number of facial displays on seg-
ments with no predicted pitch accent; instead, we
incorporated the predicted accent as an additional
contextual factor. For the most part, the pitch ac-
cents used by the speaker followed the specifica-
tions on the slides. We did not explicitly consider
the rhetorical or syntactic structure, as did, e.g.,
de Carolis et al (2000); in general, the structure
was fully determined by the context.
There were a total of 1993 relevant segments in
the recorded sentences. Overall, the most frequent
display combination was a small downward nod
on its own, which occurred on just over 25% of the
segments. The second largest class was no motion
at all (20% of the segments), followed by down-
ward nods (large and small) accompanied by brow
raises. Further down the order, the various lateral
motions appear; for this speaker, these were pri-
marily turns to the right (Figure 2(a)) and leans to
the left (Figure 2(b)).
The distribution of facial displays in specific
contexts differed from the overall distribution. The
biggest influence was the user-model evaluation:
left leans, brow lowering, and eye squinting were
all relatively more frequent on objects with nega-
tive user-model evaluations, while right turns and
brow raises occurred more often in positive con-
texts. Other factors also had an influence: for ex-
ample, nodding and brow raises were both more
frequent on segments for which the COMIC plan-
ner specified a pitch accent. Foster (2006) gives a
detailed analysis of these recordings.
4 Modelling the Corpus Data
We built a range of models using the data from
the annotated corpus to select facial displays to
accompany generated text. For each segment in
the text, a model selected a display combination
from among the displays used by the speaker in a
similar context. All of the models used the corpus
counts of displays associated with the segments di-
rectly, with no back-off or smoothing.
The models differed from one another in two
ways: the amount of context that they used, and
the way in which they made a selection within a
context. There were three levels of context:
No context These models used the overall corpus
counts for all segments.
355
(a) Right turn + brow raise (b) Left lean + brow lower
Figure 2: Typical speaker motions from the recording
Surface only These models used only the context
provided by the word(s)?or, in some cases,
a domain-specific semantic class. For exam-
ple, a model would use the class DECORA-
TION rather than the specific word artwork.
Full context In addition to the surface form, these
models also used the pitch-accent specifica-
tions and contextual information supplied by
the COMIC presentation planner. The con-
textual information was associated with the
tile-design properties included in the sen-
tence and indicated (a) whether that property
had been mentioned before, (b) whether it
was explicitly contrasted with a property of
a previous design, and (c) the expected user
evaluation of that property.
Within a context, there were two strategies for se-
lecting a facial display:
Majority Choose the combination that occurred
the largest number of times in the context.
Weighted Make a random choice from all com-
binations seen in the context, weighting the
choice according to the relative frequency.
For example, in the no-context case, a majority-
choice model would choose the small downward
nod (the majority option) for every segment, while
a weighted-choice model would choose a small
downward nod with probability 0.25, no motion
with probability 0.2, and the other displays with
correspondingly decreasing probabilities.
These two factors produced a set of 6 models
in total (3 context levels ? 2 selection strategies).
Throughout the rest of this paper, we will use two-
character labels to refer to the models. The first
character of each label indicates the amount of
     


	
	











 

 











Figure 3: Mean F score for all models
context that was used, while the second indicates
the selection method within that context: for ex-
ample, SM corresponds to a model that used the
surface form only and made a majority choice.
5 Evaluation 1: Cross-validation
We first compared the performance of the models
using 10-fold cross-validation against the corpus.
For each fold, we built models using 90% of the
sentences in the corpus, and then used those mod-
els to predict the facial displays for the sentences
in the other 10% of the corpus. We measured the
recall and precision on a sentence by comparing
the predicted facial displays for each segment to
the actual displays used by the speaker and aver-
aging those scores across the sentence. We then
used the recall and precision scores for a sentence
to compute a sentence-level F score.
Averaged across all of the cross-validation
folds, the NM model had the highest recall score,
while the FM model scored highest for precision
and F score. Figure 3 shows the average sentence-
level F score for all of the models. All but one
of the differences shown are significant at the p <
356
(a) Neutral (b) Right turn + brow raise (c) Left lean + brow lower
Figure 4: Synthesised version of motions from Figure 2
0.01 level on a paired T test; the performance of
the NM and FW models was indistinguishable on
F score, although the FW model scored higher on
precision and the NM model on recall.
That the majority-choice models generally
scored better on this measure than the weighted-
choice models is not unexpected: a weighted-
choice model is more likely to choose a less-
common display, and if it chooses it in a context
where the speaker did not, the score for that sen-
tence is decreased. It is also not surprising that,
within a selection strategy, the models that take
into account more of the context did better than
those that use less of it; this is simply an indica-
tion that there are patterns in the corpus, and that
all of the contextual information contributes to the
selection of displays.
6 Evaluation 2: User Ratings
The majority-choice models performed better on
the cross-validation study than the weighted-
choice ones did; however, this does not does not
mean that users will necessarily like their output
in practice. A large amount of the lateral motion
and eyebrow movements occurs in the second- or
third-largest class in a number of contexts, and is
therefore less likely to be selected by a majority-
choice model. If users like to see motion other
than simple nodding, it might be that the sched-
ules generated by the weighted-choice models are
actually preferred. To address this question, we
performed a user evaluation.
6.1 Experiment Design
Materials For this study, we generated 30 new
sentences from the COMIC system. The sen-
tences were selected to ensure that they covered
the full range of syntactic structures available to
COMIC and that none of them was a duplicate
of anything from the recording script. We then
generated a facial schedule for each sentence us-
ing each of the six models. Note that, for some
of the sentences, more than one model produced
an identical sequence of facial displays, either be-
cause the majority choice in a broader context was
the same as in a more narrow context, or because
a weighted-choice model ended up selecting the
majority option in every case. All such identical
schedules were retained in the set of materials; in
Section 6.2, we discuss their impact on the results.
We then made videos of every schedule for ev-
ery sentence, using the Festival speech synthesiser
(Clark et al, 2004) and the RUTH talking head
(DeCarlo et al, 2004). Figure 4 shows synthesised
versions of the facial displays from Figure 2.
Procedure 33 subjects took part in the experi-
ment: 17 female subjects and 16 males. They
were primarily undergraduate students, between
20 and 24 years old, native speakers of English,
with an intermediate amount of computer experi-
ence. Each subject in the study was shown videos
of all 30 sentences in an individually-chosen ran-
dom order. For each sentence, the subject saw
two versions, each generated by a different model,
and was asked to choose which version they liked
better. The displayed versions were counterbal-
anced so that every subject performed each pair-
wise comparison of models twice, once in each
order. The study was run over the web.
6.2 Results2
Figure 5(a) shows the overall preference rates for
all of the models. For each model, the value shown
2 We do not include those trials where both videos were
identical; if these are included, the results are similar, but the
distinctions described here just fail to reach significance.
357
     


	
	














 










Evaluating Centering for Information
Ordering Using Corpora
Nikiforos Karamanis?
University of Cambridge
Chris Mellish??
University of Aberdeen
Massimo Poesio?
University of Essex
Jon Oberlander?
University of Edinburgh
In this article we discuss several metrics of coherence defined using centering theory and
investigate the usefulness of such metrics for information ordering in automatic text generation.
We estimate empirically which is the most promising metric and how useful this metric is using
a general methodology applied on several corpora. Our main result is that the simplest metric
(which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed
by other metrics which make use of additional centering-based features. This baseline can be used
for the development of both text-to-text and concept-to-text generation systems.
1. Introduction
Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to
present a set of preselected information-bearing items, has received much attention in
recent work in automatic text generation. This is because text generation systems need
to organize the content in a way that makes the output text coherent, that is, easy to read
and understand. The easiest way to exemplify coherence is by arbitrarily reordering the
sentences of a comprehensible text. This process very often gives rise to documents that
do not make sense although the information content is the same before and after the
reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).
Entity coherence, which is based on the way the referents of noun phrases (NPs)
relate subsequent clauses in the text, is an important aspect of textual organization.
Since the early 1980s, when it was first introduced, centering theory has been an
influential framework for modelling entity coherence. Seminal papers on centering such
as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and
Weinstein (1995, page 215) suggest that centering may provide solutions for information
ordering.
Indeed, following the pioneering work of McKeown (1985), recent work on text
generation exploits constraints on entity coherence to organize information (Mellish
et al 1998; Kibble and Power 2000, 2004; O?Donnell et al 2001; Cheng 2002; Lapata
? Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK.
Nikiforos.Karamanis@cl.cam.ac.uk.
?? Department of Computing Science, King?s College, Aberdeen AB24 3UE, UK.
? Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK.
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication:
7 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these
approaches often make use of heuristics related to centering, the features of entity
coherence they employ are usually defined informally. Additionally, centering-related
features are combined with other coherence-inducing factors in ways that are based
mainly on intuition, leaving many equally plausible options unexplored.
Thus, the answers to the following questions remain unclear: (i) How appropriate
is centering for information ordering in text generation? (ii) Which aspects of centering are
most useful for this purpose? These are the issues we investigate in this paper, which
presents the first systematic evaluation of centering for information ordering. To do this,
we define centering-based metrics of coherence which are compatible with several extant
information ordering approaches. An important insight of our work is that centering
can give rise to many such metrics of coherence. Hence, a general methodology
for identifying which of these metrics represent the most promising candidates for
information ordering is required.
We adopt a corpus-based approach to compare the metrics empirically and
demonstrate the portability and generality of our evaluation methods by experimenting
with several corpora. Our main result is that the simplest metric (which relies
exclusively on NOCB transitions) sets a baseline that cannot be outperformed by
other metrics that make use of additional centering-related features. Thus, we provide
substantial insight into the role of centering as an information ordering constraint and
offer researchers working on text generation a simple, yet robust, baseline to use against
their own information ordering approaches during system development.
The article is structured as follows: In Section 2 we discuss our information ordering
approach in relation to other work on text generation. After a brief introduction
to centering in Section 3, Section 4 demonstrates how we derived centering data
structures from existing corpora. Section 5 discusses how centering can be used to
define various metrics of coherence suitable for information ordering. Then, Section 6
outlines a corpus-based methodology for choosing among these metrics. Section 7
reports on the results of our experiments and Section 8 discusses their implications.
We conclude the paper with directions for future work and a summary of our main
contributions.1
2. Information Ordering
Information ordering has been investigated by substantial recent work in text-to-
text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and
Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and
Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al 2007,
among others) as well as concept-to-text generation (particularly Kan and McKeown
[2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work
by presenting approaches to information ordering based on a genetic algorithm
(Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and
Koller 2004) which can be applied to both concept-to-text and text-to-text generation.
These approaches use a metric of coherence defined using features derived from
1 Earlier versions of this work were presented in Karamanis et al (2004) and Karamanis (2006).
2 Concept-to-text generation is concerned with the automatic generation of text from some underlying
non-linguistic representation. By contrast, the input to text-to-text generation applications is text.
30
Karamanis et al Centering for Information Ordering
centering and will serve as the premises of our investigation of centering in this
article.
Metrics of coherence are used in other work on text generation, too (Mellish et al
1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and
Power?s work, the features of entity coherence used in these metrics are informally
defined using heuristics related to centering. Additionally, the metrics are further
specified by combining these features with other coherence-inducing factors such
as rhetorical relations (Mann and Thompson 1987). However, as acknowledged in
most of this work, these are preliminary computational investigations of the complex
interactions between different types of coherence which leave many other equally
plausible combinations unexplored.
Clearly, one would like to know what centering can achieve on its own before
devising more complicated metrics. To address this question, we define metrics which
are purely centering-based, placing any attempt to specify a more elaborate model of
coherence beyond the scope of this article. This strategy is similar to most work on
centering for text interpretation in which additional constraints on coherence are not
taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic
examples). This simplification makes it possible to assess for the first time how useful
the employed centering features are for information ordering.
Work on text generation which is solely based on rhetorical relations (Hovy 1988;
Marcu 1997, among others) typically masks entity coherence under the ELABORATION
relation. However, ELABORATION has been characterized as ?the weakest of all
rhetorical relations? (Scott and de Souza 1990, page 60). Knott et al (2001) identified
several theoretical problems all related to ELABORATION and suggested that this relation
be replaced by a theory of entity coherence for text generation. Our work builds on this
suggestion by investigating how appropriate centering is as a theory of entity coherence
for information ordering.
McKeown (1985, pages 60?75) also deployed features of entity coherence to
organize information for text generation. McKeown?s ?constraints on immediate focus?
(which are based on the model of entity coherence that was introduced by Sidner
[1979] and precedes centering) are embedded within the schema-driven approach to
generation which is rather domain-specific (Reiter and Dale 2000). By contrast, our
metrics are general and portable across domains and can be applied within information
ordering approaches which are applicable to both concept-to-text and text-to-text
generation.
3. Centering Overview
This section provides an overview of centering, focusing on the aspects which are most
closely related to our work. Poesio et al (2004) and Walker, Joshi, and Prince (1998)
discuss centering and its relation to other theories of coherence in more detail.
According to Grosz, Joshi, and Weinstein (1995), each utterance Un is assigned a
ranked list of forward looking centers (i.e., discourse entities) denoted as CF(Un). The
members of CF(Un) must be realized by the NPs in Un (Brennan, Friedman [Walker],
and Pollard 1987). The first member of CF(Un) is called the preferred center
CP(Un).
The backward looking center CB(Un) links Un to the previous utterance Un?1.
CB(Un) is defined as the most highly ranked member of CF(Un?1) which also belongs
to CF(Un). CF lists prior to CF(Un?1) are not taken into account for the computation
31
Computational Linguistics Volume 35, Number 1
Table 1
Centering transitions are defined according to whether the backward looking center, CB, is
the same in two subsequent utterances, Un?1 and Un, and whether the CB of the current
utterance, CB(Un), is the same as its preferred center, CP(Un). These identity checks are also
known as the principles of COHERENCE and SALIENCE, the violations of which are denoted
with an asterisk.
COHERENCE: COHERENCE?:
CB(Un)=CB(Un?1) CB(Un) =CB(Un?1)
or CB(Un?1) undef.
SALIENCE: CB(Un)=CP(Un) CONTINUE SMOOTH-SHIFT
SALIENCE?: CB(Un) =CP(Un) RETAIN ROUGH-SHIFT
of CB(Un). The original formulations of centering by Brennan, Friedman [Walker], and
Pollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniqueness
and the locality of the CB and will serve as the foundations of our work.
The CB and the CP are combined to define transitions across pairs of
adjacent utterances (Table 1). This definition of transitions is based on Brennan,
Friedman [Walker], and Pollard (1987) and has been popular with subsequent work.
There exist several variations, however, the most important of which comes from Grosz,
Joshi, and Weinstein (1995), who define only one SHIFT transition.3
Centering makes two major claims about textual coherence, the first of which
is known as Rule 2. Rule 2 states that CONTINUE is preferred to RETAIN, which
is preferred to SMOOTH-SHIFT, which is preferred to ROUGH-SHIFT. Although the
Rule was introduced within an algorithm for anaphora resolution, Brennan, Friedman
[Walker], and Pollard (1987, page 160) consider it to be relevant to text generation
too. Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest that
text generation systems should attempt to avoid unfavorable transitions such as
SHIFTs.
The second claim, which is implied by the definition of the CB (Poesio et al 2004),
is that CF(Un) should contain at least one member of CF(Un?1). This became known
as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz,
Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss
the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) define the
additional transition NOCB to account for this case. Different types of NOCB transitions
are introduced by Passoneau (1998) and Poesio et al (2004), among others. Other
researchers, however, consider the NOCB transition to be a type of ROUGH-SHIFT
(Miltsakaki and Kukich 2004).
Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and
SALIENCE, which correspond to the identity checks used to define the transitions
(see Table 1). To improve the way centering resolves pronominal anaphora, Strube
and Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it as
CB(Un)=CP(Un?1). They also redefined Rule 2 to favor transition pairs which satisfy
3 ?CB(Un?1) undef.? in Table 1 stands for the cases where Un?1 does not have a CB. Instead of classifying
the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT
is sometimes used (Kameyama 1998; Poesio et al 2004).
32
Karamanis et al Centering for Information Ordering
CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority
over every other centering principle in Strube and Hahn?s model.
In addition to the variability caused by the numerous definitions of transitions and
the introduction of the various principles, parameters such as ?utterance,? ?ranking,?
and ?realization? can also be specified in several ways giving rise to different
instantiations of centering (Poesio et al 2004). The following section discusses how these
parameters were defined in the corpora we deploy.
4. Experimental Data
We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME
corpus (Poesio et al 2004), and the two corpora that Barzilay and Lapata (2005)
experimented with. In this section, we discuss how the centering representations we
utilize were derived from each corpus.
4.1 The MPIRO-CF Corpus
Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the
database of the MPIRO concept-to-text generation system (Isard et al 2003), realized
them as sentences, and organized them in sets. Each set consisted of six facts which
were ordered by a domain expert. The orderings produced by this expert were shown
to be very close to those produced by two other archeologists (Karamanis and Mellish
2005b).
Our first corpus, MPIRO-CF, consists of 122 orderings that were made available
to us by D&A. We computed a CF list for each fact in each ordering by applying the
instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-
text generation. That is, we took each database fact to correspond to an ?utterance?
and specified the ?realization? parameter using the arguments of each fact as the
members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the
centering transitions, and the violations of CHEAPNESS for the following example from
MPIRO-CF:
(1) (a) This exhibit is an amphora.
(b) This exhibit was decorated by the Painter of Kleofrades.
(c) The Painter of Kleofrades used to decorate big vases.
(d) This exhibit depicts a warrior performing splachnoscopy before leaving for the
battle.
(e) This exhibit is currently displayed in the Martin von Wagner Museum.
(f) The Martin von Wagner Museum is in Germany.
MPIRO facts consist of two arguments, the first of which was specified as the CP
following the definition of ?CF ranking? in O?Donnell et al (2001).4 Notice that the
second argument can often be an entity such as en914 that is realized by a canned phrase
of significant syntactic complexity (a warrior performing splachnoscopy before leaving for
the battle). Moreover, the deployed definition of ?realization? is similar to what Grosz,
4 This is the main difference between our approach and that of Kibble and Power, who allow for more than
one potential CP in their CF lists.
33
Computational Linguistics Volume 35, Number 1
Table 2
The CF list, the CB, NOCB, or centering transition (see Table 1) and violations of CHEAPNESS
(denoted with an asterisk) for each fact in Example (1) from the MPIRO-CF corpus.
Fact CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(1a) {ex1, amphora} n.a. n.a. n.a.
(1b) {ex1, paint-of-kleofr} ex1 CONTINUE ?
(1c) {paint-of-kleofr, en404} paint-of-kleofr SMOOTH-SHIFT ?
(1d) {ex1, en914} ? NOCB n.a.
(1e) {ex1, wagner-mus} ex1 CONTINUE ?
(1f) {wagner-mus, germany} wagner-mus SMOOTH-SHIFT ?
Joshi, and Weinstein (1995) call ?direct realization,? which ignores potential bridging
relations (Clark 1977) between the members of two subsequent CF lists. These relations
are typically not taken into account for information ordering and were not considered
in any of the deployed corpora.
4.2 The GNOME-LAB Corpus
We also made use of the GNOME corpus (Poesio et al 2004), which contains object
descriptions (museum labels) reliably annotated with features relevant to centering.
The motivation for this study was to examine whether the phenomena observed in
MPIRO-CF (which is arguably somewhat artificial) also manifest in texts from the
same genre written by humans without the constraints imposed by a text generation
system.
Based on the definition of museum labels in Cheng (2002, page 65), we identified
20 such texts in GNOME, which were published in a book and a museum Web site (and
were thus taken to be coherent). The following example is a characteristic text from this
subcorpus (referred to here as GNOME-LAB):
(2) (a) Item 144 is a torc.
(b) Its present arrangement, twisted into three rings, may be a modern alteration;
(c) it should probably be a single ring, worn around the neck.
(d) The terminals are in the form of goats? heads.
The GNOME corpus provides us with reliable annotation of discourse units (i.e.,
clauses and sentences) that can be used for the computation of ?utterance? and of
NPs which introduce entities to the CF list. Each feature was marked up by at
least two annotators and agreement was checked using the ? statistic on part of the
corpus.
In order to avoid deviating too much from the MPIRO application domain, we
computed the CF lists from the units that seemed to correspond more closely to MPIRO
facts. So instead of using sentence for the definition of ?utterance,? we followed most
work on centering for English and computed CF lists from GNOME?s finite units.5 The
5 This definition includes titles which do not always have finite verbs, but excludes finite relative clauses,
the second element of coordinated VPs and clause complements which are often taken as not having their
own CF lists in the centering literature.
34
Karamanis et al Centering for Information Ordering
Table 3
First two members of the CF list, the CB, NOCB, or centering transition (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for each finite unit in Example (2) from the
GNOME-LAB corpus.
Unit CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(2a) {de374, de375} n.a. n.a. n.a.
(2b) {de376, de374, ... } de374 RETAIN ?
(2c) {de374, de379, ... } de374 CONTINUE ?
(2d) {de380, de381, ... } ? NOCB n.a.
text spans with the indexes (a) to (d) in Example (2) are examples of such units. Units
such as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appear
to be of similar syntactic complexity to (1d). On the other hand, the second sentence in
Example (2) consists of two finite units, namely (b) and (c), and appears to correspond
to higher degrees of aggregation than is typically seen in an MPIRO fact. The texts in
GNOME-LAB consist of 8.35 finite units on average.
Table 3 shows the first two members of the CF list, the CB, the transitions, and the
violations of CHEAPNESS for Example (2). Note that the same entity (i.e., de374) is used
to denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated as
coreferring with Item 144. All annotated NPs introduce referents to the CF list (which
often contains more entities than in MPIRO), but only direct realization is used for the
computation of the list. This means that, similarly to the MPIRO domain, bridging
relations between, for example, it in (2c) and the terminals in (2d), are not taken into
account.
The members of the CF list were ranked by combining grammatical function with
linear order, which is a robust way of estimating ?CF ranking? in English (Poesio et al
2004). In this instantiation, the CP corresponds to the referent of the first NP within the
unit that is annotated as a subject or as the post-copular NP in a there-clause.
4.3 The NEWS and ACCS Corpora
Barzilay and Lapata (2005) presented a probabilistic approach for information ordering
which is particularly suitable for text-to-text generation and is based on a new
representation called the entity grid. A collection of 200 articles from the North American
News Corpus (NEWS) and 200 narratives of accidents from the National Transportation
Safety Board database (ACCS) was used for training and evaluation. Example (3)
presents a characteristic text from the NEWS corpus:
(3) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [Microsoft
Corp.]X with [evidence]X that [the company]S is increasingly attempting to crush
[competitors]O.
(b) [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its
own products]S are not competitive enough to unseat [established brands]O.
(c) [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
(d) [Microsoft]S claims [its tactics]S are commonplace and good economically.
35
Computational Linguistics Volume 35, Number 1
Table 4
Fragment of the entity grid for Example (3). The grammatical function of the referents in each
sentence is reported using S, O, and X (for subject, object, and other). The symbol ??? is used for
referents which do not occur in the sentence.
Referents
Sentences department trial microsoft evidence ... products brands ...
(3a) S O S X ... ? ? ...
(3b) ? ? O ? ... S O ...
(3c) ? ? S O ... ? ? ...
(3d) ? ? S ? ... ? ? ...
(3e) ? ? ? ? ... ? ? ...
(3f) ? X S ? ... ? ? ...
(e) [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb
[competition]O through [collusion]X is [a violation]O of [the Sherman Act]X.
(f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
Barzilay and Lapata automatically annotated their corpora for the grammatical function
of the NPs in each sentence (denoted in the example by the subscripts S, O, and
X for subject, object, and other, respectively) as well as their coreferential relations
(which do not include bridging references). More specifically, they used a parser
(Collins 1997) to determine the constituent structure of the sentences from which the
grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft
Corp. and the company in (3a) were identified using the system of Ng and Cardie
(2002).
The entity grid is a two-dimensional array that captures the distribution of NP
referents across sentences in the text using the aforementioned symbols for their
grammatical role and the symbol ??? for a referent that does not occur in a sentence.
Table 4 illustrates a fragment of the grid for the sentences in Example (3).7
Barzilay and Lapata use the grid to compute models of coherence that are
considerably more elaborate than centering. To derive an appropriate instantiation of
centering for our investigation, we compute a CF list for each grid row using the
referents with the symbols S, O, and X. These referents are ranked according to their
grammatical function and their position in the text. This definition of ?CF ranking? is
similar to the one we use in GNOME-LAB. For instance, department is ranked higher
than microsoft in CF(3a) because the Justice Department is mentioned before Microsoft
Corp. in the text. The derived sequence of CF lists is used to compute the additional
centering data structures shown in Table 5.
The average number of sentences per text is 10.4 in NEWS and 11.5 in ACCS.
As we explain in the next section, our centering-based metrics of coherence can be
6 They also used a small set of patterns to recognize passive verbs and annotate arguments involved in
passive constructions with their underlying grammatical function. This is why Microsoft is marked with
the role O in sentence (3b).
7 If a referent such as microsoft is attested by several NPs in the same sentence, for example, Microsoft
Corp. and the company in (3a), the role with the highest priority (in this case S) is used to represent it.
36
Karamanis et al Centering for Information Ordering
Table 5
First two members of the CF list, the CB, NOCB, or centering transitions (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for Example (3) from the NEWS corpus.
Sentence CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(3a) {department, microsoft, ...} n.a. n.a. n.a.
(3b) {products, microsoft, ...} microsoft RETAIN ?
(3c) {microsoft, case, ...} microsoft CONTINUE ?
(3d) {microsoft, tactics} microsoft CONTINUE ?
(3e) {government, conspiracy, ...} ? NOCB n.a.
(3f) {microsoft, earnings, ... } ? NOCB n.a.
deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as test
data.8
5. Computing Centering-Based Metrics of Coherence
Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis,
and Koller 2004), the input to information ordering is an unordered set of information-
bearing items represented as CF lists. A set of candidate orderings is produced by
creating different permutations of these lists. A metric of coherence uses features from
centering to compute a score for each candidate ordering and select the highest scoring
ordering as the output.9
A wide range of metrics of coherence can be defined in centering?s terms, simply
on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume
that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5,
is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE,
SALIENCE, and CHEAPNESS, and the centering transitions for this ordering.10
The candidate ordering contains two NOCBs in sentences (3e) and (3f). Its score
according to M.NOCB, the metric used by Karamanis and Manurung (2002) and
Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should
such an ordering exist) will be preferred over this candidate as the selected output of
information ordering if M.NOCB is used to guide this process. M.NOCB relies only on
CONTINUITY. Because satisfying this principle is a prerequisite for the computation of
every other centering feature, M.NOCB is the simplest possible centering-based metric
and will be used as the baseline in our experiments.
According to Strube and Hahn (1999) the principle of CHEAPNESS is the most
important centering feature for anaphora resolution. We are interested in assessing how
suitable M.CHEAP, a metric which utilizes CHEAPNESS, is for information ordering.
CHEAPNESS is violated twice according to Table 6 so the score of the candidate ordering
8 By contrast, Barzilay and Lapata used 100 texts in each domain to train their models and reserved the
other 100 for testing them.
9 If the best coherence score is assigned to several candidate orderings, then the information ordering
algorithm will choose randomly between them.
10 Principles and transitions will be collectively referred to as ?features? from now on.
37
Computational Linguistics Volume 35, Number 1
Table 6
Violations of CONTINUITY (NOCB), COHERENCE, SALIENCE, and CHEAPNESS and centering
transitions for Example (3), based on the analysis in Table 5. The table reports the sentences
marked with each centering feature: That is, sentences (3e) and (3f) are classified as NOCBs, and
so on.
CONTINUITY? COHERENCE? SALIENCE? CHEAPNESS?
NOCB: CBn = CBn?1: CBn = CPn: CBn = CPn?1:
(3e), (3f) ? (3b) (3b), (3c)
CONTINUE: RETAIN: SMOOTH-SHIFT: ROUGH-SHIFT:
(3c), (3d) (3b) ? ?
according to M.CHEAP is 2.11 If another candidate ordering with fewer violations of
CHEAPNESS exists, it will be chosen as a preferred output according to M.CHEAP.
M.BFP employs the transition preferences of Rule 2 as specified by Brennan,
Friedman [Walker], and Pollard (1987). The first score to be computed by M.BFP is
the sum of CONTINUE transitions, which is 2 for the candidate ordering according to
Table 6. If this ordering is found to score higher than every other candidate ordering for
the number of CONTINUEs, it is selected as the output. If another ordering is found to
have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for
the other two types of centering transitions.12
M.KP, the metric deployed by Kibble and Power (2000) in their text generation
system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE,
and SALIENCE, preferring the ordering with the lowest total cost. In addition to
the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates
SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a
lower score (if any) will be preferred by this metric. Although Kibble and Power (2004)
introduced a weighted version of M.KP, the exact weighting of centering?s principles
remains an open question, as argued by Kibble (2001). This is why we decided to
experiment with M.KP instead of its weighted variant.
In the remainder of the paper, we take forward the four metrics motivated in this
section as the most appropriate starting point for experimentation. We would like to
emphasize, however, that these are not the only possible options. Indeed, similarly to
the various ways in which centering?s parameters can be specified, there exist many
other ways of using centering to define metrics of entity coherence for information
ordering. These possibilities arise from the numerous other definitions of centering?s
transitions and the various ways in which transitions and principles can be combined.
These are explored in more detail in Karamanis (2003, Chapter 3), which also provides
a formal definition of the metrics discussed previously.
6. Evaluation Methodology
Because using naturally occurring discourse in psycholinguistic studies to investigate
coherence effects is almost infeasible, computational corpus-based experiments are
11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS.
12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the
definition of transitions in M.BFP.
38
Karamanis et al Centering for Information Ordering
often the most viable alternative (Poesio et al 2004; Barzilay and Lee 2004). Corpus-
based evaluation can be usefully employed during system development and may
be later supplemented by less extended evaluation based on human judgments as
suggested by Lapata (2006).
The corpus-based methodology of Karamanis (2003) served as our experimental
framework. This methodology is based on the premise that the original sentence order
(OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other
ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it
has to be penalized.
Karamanis (2003) introduced a performance measure called the classification error
rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2.
Better(M,OSO) stands for the percentage of orderings that score better than the OSO
according to a metric M, and Equal(M,OSO) is the percentage of orderings that score
equal to the OSO.13 This measure provides an indication of how likely a metric is to lead
to an ordering different from the OSO. When comparing several metrics with each other,
the one with the lowest classification error rate is the most appropriate for ordering
the sentences that the OSO consists of. In other words, the smaller the classification
error rate, the better a metric is expected to perform for information ordering. The
average classification error rate is used to summarize the performance of each metric in
a corpus.
To compute the classification error rate we permute the CF lists of the OSO and
classify each alternative ordering as scoring better, equal, or worse than the OSO
according to M. When the number of CF lists in the OSO is fairly small, it is feasible
to search through all possible orderings. For OSOs consisting of more than 10 CF
lists, the classification error rate for the entire population of orderings can be reliably
estimated using a random sample of one million permutations (Karamanis 2003,
Chapter 5).
7. Results
Table 7 shows the average performance of each metric in the corpora employed in our
experiments. The smallest?that is, best?score in each corpus is printed in boldface.
The table indicates that the baseline M.NOCB performs best in three out of four corpora.
The experimental results of the pairwise comparisons of M.NOCB with each of
M.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8. The exact number
of texts for which the classification error rate of M.NOCB is lower than its competitor for
each comparison is reported in the columns headed by ?lower.? For instance, M.NOCB
has a lower classification error rate than M.CHEAP for 110 (out of 122) texts from
MPIRO-CF. M.CHEAP achieves a lower classification error rate for just 12 texts, and
there do not exist any ties, that is, cases in which the classification error rate of the two
metrics is the same.
The p value returned by the two-tailed Sign Test for the difference in the number
of texts in each corpus, rounded to the third decimal place, is also reported.14 With
13 Weighting Equal(M,OSO) by 0.5 is based on the assumption that, similarly to tossing a coin, the OSO will
on average do better than half of the orderings that score the same as it does when other coherence
constraints are considered.
14 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry
specific assumptions about population distributions and variance.
39
Computational Linguistics Volume 35, Number 1
Table 7
Average classification error rate for the centering-based metrics in each corpus.
Corpus
Metric MPIRO-CF GNOME-LAB NEWS ACCS Mean
M.NOCB 20.42 19.95 30.90 15.51 21.70
M.BFP 19.91 33.01 37.90 21.20 28.01
M.KP 53.15 58.22 57.70 55.60 56.12
M.CHEAP 81.04 57.23 64.60 76.29 69.79
No. of texts 122 20 200 200
Table 8
Comparing M.NOCB with M.CHEAP, M.KP, and M.BFP in each corpus.
MPIRO-CF GNOME-LAB
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 110 12 0 <0.001 18 2 0 <0.001
M.KP 103 16 3 <0.001 16 2 2 0.002
M.BFP 42 31 49 0.242 12 3 5 0.036
No. of texts 122 20
NEWS ACCS
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 155 44 1 <0.001 183 17 0 <0.001
M.KP 131 68 1 <0.001 167 33 0 <0.001
M.BFP 121 71 8 <0.001 100 100 0 1.000
No. of texts 200 200
respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF,
the p value is lower than 0.001 after rounding. This in turn means that M.NOCB
returns a better classification error rate for significantly more texts in MPIRO-CF
than M.CHEAP. In other words, M.NOCB outperforms M.CHEAP significantly in this
corpus.
Notably, M.NOCB performs significantly better than its competitor in 10 out of
12 cases.15 In the remaining two comparisons, the difference in performance between
M.NOCB and M.BFP is not significant (p > 0.05). However, this does not constitute
evidence against M.NOCB, the simplest of the investigated metrics. In fact, because
M.BFP fails to outperform the baseline, the latter may be considered as the most
promising solution for information ordering in these cases too by applying Occam?s
razor. Thus, M.NOCB is shown to be the best performing metric across all four
corpora.
15 This result is significant too according to the two-tailed Sign Test (p < 0.05).
40
Karamanis et al Centering for Information Ordering
8. Discussion
Our experiments show that M.NOCB is the most suitable metric for information
ordering among the metrics we experimented with. Despite the differences between our
corpora (in genre, average length, syntactic complexity, number of referents in the CF
list, etc.), M.NOCB proves robust across all four of them. It is also the most appropriate
metric to use in both application areas we relate our corpora to, namely concept-to-text
(MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.
These results indicate that when purely centering-based metrics are used, simply
avoiding NOCBs is more relevant to information ordering than the combinations of
additional centering features that the other metrics make use of.
In this section, we compare our work with other recent evaluation studies,
including the corpus-based investigation of centering by Poesio et al (2004); discuss
the implications of our findings for text generation; and summarize our contributions.
8.1 Recent Evaluation Studies in Information Ordering
There has been significant recent work on the corpus-based evaluation for information
ordering. In this section, we discuss the methodological differences between our work
and the studies which are most closely related to it.
Barzilay and Lee (2004) introduce a stochastic model for information ordering
which computes the probability of generating the OSO and every alternative ordering.
Then, all orderings are ranked according to this probability and the rank given to the
OSO is retrieved. Several evaluation measures are discussed, the most important of
which is the average OSO rank, that is, the average rank of the OSOs in their corpora.
This measure does not take into account that the OSOs differ in length. However, this
information is necessary to estimate reliably the performance of an information ordering
approach, as we discuss in Karamanis and Mellish (2005a) in more detail.
Barzilay and Lapata (2005) overcome this difficulty by introducing a performance
measure called ranking accuracywhich expresses the percentage of alternative orderings
that are ranked lower than the OSO. In Karamanis?s (2003) terms, ranking accuracy
equals 100% ? Better(M,OSO), assuming that no equally ranking orderings exist.16
Barzilay and Lapata (2005) compare the OSO with just 20 alternative orderings,
often sampled out of several millions. On the other hand, Barzilay and Lee (2004)
enumerate exhaustively each possible ordering, which might become impractical as the
search space grows factorially. We overcame these problems by using a large random
sample for the texts which consist of more than 10 sentences as suggested in Karamanis
(2003, Chapter 5). Equally important is the emphasis we placed on the use of statistical
tests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata.
Lapata (2003) presented a methodology for automatically evaluating generated
orderings on the basis of their distance from observed sentence orderings in a corpus.
A measure of rank correlation (called Kendall?s ?), which was subsequently shown to
correlate reliably with human ratings and reading times (Lapata 2006), was used to
estimate the distance between orderings.
16 Neither Barzilay and Lapata (2005) nor Barzilay and Lee (2004) appear to consider the possibility that two
orderings may be equally ranked.
41
Computational Linguistics Volume 35, Number 1
Whereas ? estimates how close the predictions of a metric are to several original
orderings, we measure how likely a metric is to lead to an ordering different than the
OSO. Taking into account more than one OSO for information ordering is the main
strength of Lapata?s method, but to do this one needs to ask several humans to order the
same set of sentences (Madnani et al 2007). Karamanis and Mellish (2005b) conducted
an experiment in the MPIRO domain using Lapata?s methodology which supplements
the work reported in this article. However, such an approach is less practical for much
larger collections of texts such as NEWS and ACCS. This is presumably the reason why
Barzilay and Lapata (2005) use ranking accuracy instead of ? in their evaluation.
8.2 Previous Corpus-Based Evaluations of Centering
Our work investigates how the coherence score of the OSO compares to the scores
of alternative orderings of the sentences that the OSO consists of. As Kibble (2001,
page 582) noticed, this question is crucial from an information ordering viewpoint, but
was not taken into account by any previous corpus-based study of centering. Grosz,
Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested by
examining ?alternative multi-utterance sequences that differentially realize the same
content.? We are the first to have pursued this research objective in the evaluation of
centering for information ordering.
Poesio et al (2004) observed that there remained a large number of NOCBs under
every instantiation of centering they tested and concluded that centering is inadequate
as a coherence model.17 However, the frequency of NOCBs does not necessarily provide
adequate indication of how appropriate NOCBs (and centering in general) are for
information ordering. Although over 50% of the transitions in GNOME-LAB are NOCBs,
the average classification error rate of approximately 20% for M.NOCB suggests that the
OSO tends to be in greater agreement with the preference to avoid NOCBs than 80% of
the alternative orderings. Thus, it appears that the observed ordering in the corpus does
optimize with respect to the number of potential NOCBs to a great extent.
8.3 A Simple and Robust Baseline for Text Generation
How likely is M.NOCB to come up with the attested ordering in the corpus (the OSO)
if it is actually used to guide an algorithm that orders the CF lists in our corpora?
The average classification error rates (Table 7) estimate exactly this variable. The
performance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9%
(NEWS). We attribute this variation to the aforesaid differences between the corpora.
Notice, however, that these differences affect all metrics in a similar way, not allowing
for another metric to significantly outperform M.NOCB.
Noticeably, even in ACCS, for which M.NOCB achieves its best performance,
approximately one out of six alternative orderings on average are taken to be more
coherent than the OSO. Given the average number of sentences per text in this corpus
17 We viewed the definition of the centering instantiation as being related to the application domain, as we
explained in Section 4. This is why, unlike Poesio et al, we did not experiment with different
instantiations of centering on the same data.
42
Karamanis et al Centering for Information Ordering
(11.5), this means that several millions of alternative orderings are often taken to be
more coherent than the gold standard.
Barzilay and Lapata (2005) report an average ranking accuracy of 87.3% for their
best sentence ordering method in ACCS. This corresponds to an average classification
error rate of 12.7% (assuming that there are no equally scoring orderings in their
evaluation; see Section 8.1). This is equal to an improvement of just 2.8% over
the performance of our baseline metric (15.5%) using a coherence model which is
substantially more elaborate than centering. However, it is in NEWS (for which
M.NOCB returns its worst performance of 30.9%) that this model shows its real strength,
approximating an average classification error rate of 9.6%, which corresponds to an
improvement of 21.3% over our baseline. We believe that the experiments reported in
this article put the studies of our colleagues in better perspective by providing a reliable
baseline to compare their metrics against.
8.4 Moving Beyond Centering-Based Metrics
Following McKeown (1985), Kibble and Power argue in favor of an integrated approach
for concept-to-text generation in which the same centering features are used at different
stages in the generation pipeline. However, our study suggests that features such as
CHEAPNESS and the centering transitions are not particularly relevant to information
ordering. The poor performance of these features can be explained by the fact that they
were originally introduced to account for pronoun resolution rather than information
ordering. CONTINUITY, on the other hand, captures a fundamental intuition about entity
coherence which constitutes part of several other discourse theories.18
CONTINUITY, however, captures just one aspect of coherence. This explains the
relatively high classification error rates for M.NOCB, which needs to be supplemented
with other coherence-inducing factors in order to be used in practice. This verifies the
premises of researchers such as Kibble and Power who a priori use features derived
from centering in combination with other factors in the definition of their metrics. Our
work should be quite helpful for that effort too, suggesting that M.NOCB is a better
starting point for defining such metrics than M.CHEAP or M.KP.
9. Conclusion
In conclusion, our analysis sheds more light on two previously unaddressed questions
in the corpus-based evaluation of centering: (i) which aspects of centering are most
relevant to information ordering and (ii) to what extent centering on its own can be
useful for this purpose. We have shown that the metric which relies exclusively on NOCB
transitions (M.NOCB) sets a baseline that cannot be outperformed by other coherence
metrics which make use of additional centering features. Although this metric does not
perform well enough to be used on its own, it constitutes a simple, yet robust, baseline
against which more elaborate information ordering approaches can be tested during
system development in both text-to-text and concept-to-text generation.
This work can be extended in numerous ways. For instance, given the abundance
of possible centering-based metrics one may investigate whether a different metric can
18 We thank one anonymous reviewer for suggesting this explanation of our results.
43
Computational Linguistics Volume 35, Number 1
outperform M.NOCB in any corpus or application domain. M.NOCB can also serve as
the starting point for the definition of more informed metrics which will incorporate
additional coherence-inducing factors. Finally, given that we used the instantiation
of centering which seemed to correspond more closely to the targeted application
domains, the extent to which computing the CF list in a different way may affect the
performance of the metrics is another question to explore in future work.
Acknowledgments
Many thanks to Aggeliki Dimitromanolaki,
Mirella Lapata, and Regina Barzilay for their
data; to David Schlangen, Ruli Manurung,
James Soutter, and Le An Ha for
programming solutions; and to Ruth Seal
and two anonymous reviewers for their
comments. Nikiforos Karamanis received
support from the Greek State Scholarships
Foundation (IKY) as a PhD student in
Edinburgh as well as the Rapid Item
Generation project and the BBSRC-funded
FlySlip grant (No 38688) as a postdoc in
Wolverhampton and Cambridge,
respectively.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of ACL
2004, pages 399?406, Barcelona.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of ACL 2005,
pages 141?148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models with applications to generation
and summarization. In Proceedings of
HLT-NAACL 2004, pages 113?120,
Boston, MA.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Bollegala, Danushka, Naoaki Okazaki, and
Mitsuru Ishizuka. 2006. A bottom-up
approach to sentence ordering for
multi-document summarization. In
Proceedings of ACL-COLING 2006,
pages 385?392, Sydney.
Brennan, Susan E., Marilyn A.
Friedman [Walker], and Carl J. Pollard.
1987. A centering approach to pronouns.
In Proceedings of ACL 1987, pages 155?162,
Stanford, CA.
Cheng, Hua. 2002. Modelling Aggregation
Motivated Interactions in Descriptive Text
Generation. Ph.D. thesis, Division of
Informatics, University of Edinburgh.
Clark, Herbert. H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science.
Cambridge University Press, Cambridge,
pages 9?27.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL 1997,
pages 16?23, Madrid.
Dimitromanolaki, Aggeliki and Ion
Androutsopoulos. 2003. Learning to order
facts for discourse planning in natural
language generation. In Proceedings of
ENLG 2003, pages 23?30, Budapest.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hovy, Eduard. 1988. Planning coherent
multisentential text. In Proceedings of ACL
1988, pages 163?169, Buffalo, NY.
Isard, Amy, Jon Oberlander, Ion
Androutsopoulos, and Colin Matheson.
2003. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45.
Ji, Paul and Stephen Pulman. 2006. Sentence
ordering with manifold-based
classification in multi-document
summarization. In Proceedings of EMNLP
2006, pages 526?533, Sydney.
Kameyama, Megumi. 1998. Intrasentential
centering: A case study. In Walker, Joshi,
and Prince 1998, pages 89?122.
Kan, Min-Yen and Kathleen McKeown. 2002.
Corpus-trained text generation for
summarization. In Proceedings of INLG
2002, pages 1?8, Harriman, NY.
Karamanis, N. 2006. Evaluating centering for
information ordering in two new domains.
In Proceedings of NAACL 2006, Companion
Volume, pages 65?68, New York.
Karamanis, N., M. Poesio, C. Mellish, and
J. Oberlander. 2004. Evaluating
centering-based metrics of coherence using
44
Karamanis et al Centering for Information Ordering
a reliably annotated corpus. In Proceedings
of ACL 2004, pages 391?398, Barcelona.
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Division of Informatics, University
of Edinburgh.
Karamanis, Nikiforos and Hisar Maruli
Manurung. 2002. Stochastic text
structuring using the principle of
continuity. In Proceedings of INLG 2002,
pages 81?88, Harriman, NY.
Karamanis, Nikiforos and Chris Mellish.
2005a. A review of recent corpus-based
methods for evaluating information
ordering in text production. In Proceedings
of Corpus Linguistics 2005 Workshop on
Using Corpora for NLG, pages 13?18,
Birmingham.
Karamanis, Nikiforos and Chris Mellish.
2005b. Using a corpus of sentence
orderings defined by many experts to
evaluate metrics of coherence for text
structuring. In Proceedings of ENLG 2005,
pages 174?179, Aberdeen.
Kibble, Rodger. 2001. A reformulation of rule
2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000.
An integrated framework for text
planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84,
Mitzpe Ramon.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects.
John Benjamins, Amsterdam, chapter 7,
pages 181?196.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of ACL 2003,
pages 545?552, Sapporo.
Lapata, Mirella. 2006. Automatic evaluation
of information ordering: Kendall?s tau.
Computational Linguistics, 32(4):1?14.
Madnani, Nitin, Rebecca Passonneau,
Necip Fazil Ayan, John Conroy, Bonnie
Dorr, Judith Klavans, Dianne O?Leary, and
Judith Schlesinger. 2007. Measuring
variability in sentence ordering for news
summarization. In Proceedings of ENLG
2007, pages 81?88, Schloss Dagstuhl.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organisation. Technical Report
RR-87-190, University of Southern
California / Information Sciences Institute.
Marcu, Daniel. 1997. The Rhetorical Parsing,
Summarization and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto.
McKeown, Kathleen. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Studies in Natural Language
Processing. Cambridge University Press,
Cambridge.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of INLG 1998,
pages 98?107, Niagara-on-the-Lake.
Miltsakaki, Eleni and Karen Kukich. 2004.
Evaluation of text coherence for electronic
essay scoring systems. Natural Language
Engineering, 10(1):25?55.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
to coreference resolution. In Proceedings of
ACL 2002, pages 104?111, Philadelphia,
PA.
O?Donnell, Mick, Chris Mellish, Jon
Oberlander, and Alistair Knott. 2001. ILEX:
An architecture for a dynamic hypertext
generation system. Natural Language
Engineering, 7(3):225?250.
Passoneau, Rebecca J. 1998. Interaction
of discourse structure with explicitness
of discourse anaphoric phrases. In
Walker, Joshi, and Prince 1998,
pages 327?358.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Technical Report
CSM-369, Department of Computer
Science, University of Essex. Extended
version of the paper that appeared in
Computational Linguistics 30(3):309?363,
2004.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press, San
Diego, CA, pages 47?74.
45
Computational Linguistics Volume 35, Number 1
Siddharthan, Advaith. 2006. Syntactic
simplification and text cohesion.
Research on Language and Computation,
4(1):77?109.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English. Ph.D. thesis, AI
Laboratory/MIT, Cambridge, MA. Also
available as Technical Report No.
AI-TR-537.
Soricut, Radu and Daniel Marcu. 2006.
Discourse generation using utility-trained
coherence models. In Proceedings of
ACL-COLING 2006 Poster Session,
pages 803?810, Sydney.
Strube, Michael and Udo Hahn. 1999.
Functional centering: Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford.
46
Evaluating Centering-based metrics of coherence for text
structuring using a reliably annotated corpus
Nikiforos Karamanis,? Massimo Poesio,? Chris Mellish,? and Jon Oberlander?
?School of Informatics, University of Edinburgh, UK, {nikiforo,jon}@ed.ac.uk
?Dept. of Computer Science, University of Essex, UK, poesio at essex dot ac dot uk
?Dept. of Computing Science, University of Aberdeen, UK, cmellish@csd.abdn.ac.uk
Abstract
We use a reliably annotated corpus to compare
metrics of coherence based on Centering The-
ory with respect to their potential usefulness for
text structuring in natural language generation.
Previous corpus-based evaluations of the coher-
ence of text according to Centering did not com-
pare the coherence of the chosen text structure
with that of the possible alternatives. A corpus-
based methodology is presented which distin-
guishes between Centering-based metrics taking
these alternatives into account, and represents
therefore a more appropriate way to evaluate
Centering from a text structuring perspective.
1 Motivation
Our research area is descriptive text generation
(O?Donnell et al, 2001; Isard et al, 2003), i.e.
the generation of descriptions of objects, typi-
cally museum artefacts, depicted in a picture.
Text (1), from the gnome corpus (Poesio et al,
2004), is an example of short human-authored
text from this genre:
(1) (a) 144 is a torc. (b) Its present arrangement,
twisted into three rings, may be a modern al-
teration; (c) it should probably be a single ring,
worn around the neck. (d) The terminals are
in the form of goats? heads.
According to Centering Theory (Grosz et al,
1995; Walker et al, 1998a), an important fac-
tor for the felicity of (1) is its entity coherence:
the way centers (discourse entities), such as
the referent of the NPs ?144? in clause (a) and
?its? in clause (b), are introduced and discussed
in subsequent clauses. It is often claimed in
current work on in natural language generation
that the constraints on felicitous text proposed
by the theory are useful to guide text struc-
turing, in combination with other factors (see
(Karamanis, 2003) for an overview). However,
how successful Centering?s constraints are on
their own in generating a felicitous text struc-
ture is an open question, already raised by the
seminal papers of the theory (Brennan et al,
1987; Grosz et al, 1995). In this work, we ex-
plored this question by developing an approach
to text structuring purely based on Centering,
in which the role of other factors is deliberately
ignored.
In accordance with recent work in the emerg-
ing field of text-to-text generation (Barzilay et
al., 2002; Lapata, 2003), we assume that the in-
put to text structuring is a set of clauses. The
output of text structuring is merely an order-
ing of these clauses, rather than the tree-like
structure of database facts often used in tradi-
tional deep generation (Reiter and Dale, 2000).
Our approach is further characterized by two
key insights. The first distinguishing feature is
that we assume a search-based approach to text
structuring (Mellish et al, 1998; Kibble and
Power, 2000; Karamanis and Manurung, 2002)
in which many candidate orderings of clauses
are evaluated according to scores assigned by
a given metric, and the best-scoring ordering
among the candidate solutions is chosen. The
second novel aspect is that our approach is
based on the position that the most straight-
forward way of using Centering for text struc-
turing is by defining a Centering-based metric
of coherence Karamanis (2003). Together, these
two assumptions lead to a view of text planning
in which the constraints of Centering act not
as filters, but as ranking factors, and the text
planner may be forced to choose a sub-optimal
solution.
However, Karamanis (2003) pointed out that
many metrics of coherence can be derived from
the claims of Centering, all of which could be
used for the type of text structuring assumed in
this paper. Hence, a general methodology for
identifying which of these metrics represent the
most promising candidates for text structuring
is required, so that at least some of them can
be compared empirically. This is the second re-
search question that this paper addresses, build-
ing upon previous work on corpus-based evalu-
ations of Centering, and particularly the meth-
ods used by Poesio et al (2004). We use the
gnome corpus (Poesio et al, 2004) as the do-
main of our experiments because it is reliably
annotated with features relevant to Centering
and contains the genre that we are mainly in-
terested in.
To sum up, in this paper we try to iden-
tify the most promising Centering-based metric
for text structuring, and to evaluate how useful
this metric is for that purpose, using corpus-
based methods instead of generally more expen-
sive psycholinguistic techniques. The paper is
structured as follows. After discussing how the
gnome corpus has been used in previous work
to evaluate the coherence of a text according to
Centering we discuss why such evaluations are
not sufficient for text structuring. We continue
by showing how Centering can be used to define
different metrics of coherence which might be
useful to drive a text planner. We then outline
a corpus-based methodology to choose among
these metrics, estimating how well they are ex-
pected to do when used by a text planner. We
conclude by discussing our experiments in which
this methodology is applied using a subset of the
gnome corpus.
2 Evaluating the coherence of a
corpus text according to Centering
In this section we briefly introduce Centering,
as well as the methodology developed in Poesio
et al (2004) to evaluate the coherence of a text
according to Centering.
2.1 Computing CF lists, CPs and CBs
According to Grosz et al (1995), each ?utter-
ance? in a discourse is assigned a list of for-
ward looking centers (CF list) each of which is
?realised? by at least one NP in the utterance.
The members of the CF list are ?ranked? in or-
der of prominence, the first element being the
preferred center CP.
In this paper, we used what we considered to
be the most common definitions of the central
notions of Centering (its ?parameters?). Poe-
sio et al (2004) point out that there are many
definitions of parameters such as ?utterance?,
?ranking? or ?realisation?, and that the setting
of these parameters greatly affects the predic-
tions of the theory;1 however, they found viola-
tions of the Centering constraints with any way
of setting the parameters (for instance, at least
25% of utterances have no CB under any such
setting), so that the questions addressed by our
work arise for all other settings as well.
Following most mainstream work on Center-
ing for English, we assume that an ?utterance?
corresponds to what is annotated as a finite unit
in the gnome corpus.2 The spans of text with
the indexes (a) to (d) in example (1) are exam-
ples. This definition of utterance is not optimal
from the point of view of minimizing Centering
violations (Poesio et al, 2004), but in this way
most utterances are the realization of a single
proposition; i.e., the impact of aggregation is
greatly reduced. Similarly, we use grammatical
function (gf) combined with linear order within
the unit (what Poesio et al (2004) call gfthere-
lin) for CF ranking. In this configuration, the
CP is the referent of the first NP within the unit
that is annotated as a subject for its gf.3
Example (2) shows the relevant annotation
features of unit u210 which corresponds to
utterance (a) in example (1). According to
gftherelin, the CP of (a) is the referent of ne410
?144?.
(2) <unit finite=?finite-yes? id=?u210?>
<ne id="ne410" gf="subj">144</ne>
is
<ne id="ne411" gf="predicate">
a torc</ne> </unit>.
The ranking of the CFs other than the
CP is defined according to the following pref-
erence on their gf (Brennan et al, 1987):
obj>iobj>other. CFs with the same gf are
ranked according to the linear order of the cor-
responding NPs in the utterance. The second
column of Table 1 shows how the utterances in
example (1) are automatically translated by the
scripts developed by Poesio et al (2004) into a
1For example, one could equate ?utterance? with sen-
tence (Strube and Hahn, 1999; Miltsakaki, 2002), use
indirect realisation for the computation of the CF list
(Grosz et al, 1995), rank the CFs according to their
information status (Strube and Hahn, 1999), etc.
2Our definition includes titles which are not always
finite units, but excludes finite relative clauses, the sec-
ond element of coordinated VPs and clause complements
which are often taken as not having their own CF lists
in the literature.
3Or as a post-copular subject in a there-clause.
CF list: cheapness
U {CP, other CFs} CB Transition CBn=CPn?1
(a) {de374, de375} n.a. n.a. n.a.
(b) {de376, de374, de377} de374 retain +
(c) {de374, de379} de374 continue ?
(d) {de380, de381, de382} - nocb +
Table 1: CP, CFs other than CP, CB, nocb or standard (see Table 2) transition and violations of
cheapness (denoted with an asterisk) for each utterance (U) in example (1)
coherence: coherence?:
CBn=CBn?1 CBn 6=CBn?1
or nocb in CFn?1
salience: CBn=CPn continue smooth-shift
salience?: CBn 6=CPn retain rough-shift
Table 2: coherence, salience and the table of standard transitions
sequence of CF lists, each decomposed into the
CP and the CFs other than the CP, according
to the chosen setting of the Centering param-
eters. Note that the CP of (a) is the center
de374 and that the same center is used as the
referent of the other NPs which are annotated
as coreferring with ne410.
Given two subsequent utterances Un?1 and
Un, with CF lists CFn?1 and CFn respectively,
the backward looking center of Un, CBn, is de-
fined as the highest ranked element of CFn?1
which also appears in CFn (Centering?s Con-
straint 3). For instance, the CB of (b) is de374.
The third column of Table 1 shows the CB for
each utterance in (1).4
2.2 Computing transitions
As the fourth column of Table 1 shows, each
utterance, with the exception of (a), is also
marked with a transition from the previous one.
When CFn and CFn?1 do not have any cen-
ters in common, we compute the nocb transi-
tion (Kibble and Power, 2000) (Poesio et als
null transition) for Un (e.g., utterance (d) in
Table 1).5
4In accordance with Centering, no CB is computed
for (a), the first utterance in the sequence.
5In this study we do not take indirect realisation into
account, i.e., we ignore the bridging reference (anno-
tated in the corpus) between the referent of ?it? de374
in (c) and the referent of ?the terminals? de380 in (d),
by virtue of which de374 might be thought as being a
member of the CF list of (d). Poesio et al (2004) showed
that hypothesizing indirect realization eliminates many
violations of entity continuity, the part of Constraint
1 that rules out nocb transitions. However, in this work
we are treating CF lists as an abstract representation
Following again the terminology in Kibble
and Power (2000), we call the requirement that
CBn be the same as CBn?1 the principle of co-
herence and the requirement that CBn be the
same as CPn the principle of salience. Each
of these principles can be satisfied or violated
while their various combinations give rise to the
standard transitions of Centering shown in Ta-
ble 2; Poesio et als scripts compute these vio-
lations.6 We also make note of the preference
between these transitions, known as Centering?s
Rule 2 (Brennan et al, 1987): continue is pre-
ferred to retain, which is preferred to smooth-
shift, which is preferred to rough-shift.
Finally, the scripts determine whether CBn
is the same as CPn?1, known as the principle
of cheapness (Strube and Hahn, 1999). The
last column of Table 1 shows the violations of
cheapness (denoted with an asterisk) in (1).7
2.3 Evaluating the coherence of a text
and text structuring
The statistics about transitions computed as
just discussed can be used to determine the de-
gree to which a text conforms with, or violates,
Centering?s principles. Poesio et al (2004)
found that nocbs account for more than 50%
of the atomic facts the algorithm has to structure, i.e.,
we are assuming that CFs are arguments of such facts;
including indirectly realized entities in CF lists would
violate this assumption.
6If the second utterance in a sequence U2 has a CB,
then it is taken to be either a continue or a retain,
although U1 is not classified as a nocb.
7As for the other two principles, no violation of
cheapness is computed for (a) or when Un is marked as
a nocb.
of the transitions in the gnome corpus in con-
figurations such as the one used in this pa-
per. More generally, a significant percentage of
nocbs (at least 20%) and other ?dispreferred?
transitions was found with all parameter config-
urations tested by Poesio et al (2004) and in-
deed by all previous corpus-based evaluations of
Centering such as Passoneau (1998), Di Eugenio
(1998), Strube and Hahn (1999) among others.
These results led Poesio et al (2004) to the
conclusion that the entity coherence as formal-
ized in Centering should be supplemented with
an account of other coherence inducing factors
to explain what makes texts coherent.
These studies, however, do not investigate
the question that is most important from the
text structuring perspective adopted in this pa-
per: whether there would be alternative ways of
structuring the text that would result in fewer
violations of Centering?s constraints (Kibble,
2001). Consider the nocb utterance (d) in (1).
Simply observing that this transition is ?dispre-
ferred? ignores the fact that every other ordering
of utterances (b) to (d) would result in more
nocbs than those found in (1). Even a text-
structuring algorithm functioning solely on the
basis of the Centering constraints might there-
fore still choose the particular order in (1). In
other words, a metric of text coherence purely
based on Centering principles?trying to mini-
mize the number of nocbs?may be sufficient to
explain why this order of clauses was chosen,
at least in this particular genre, without need
to involve more complex explanations. In the
rest of the paper, we consider several such met-
rics, and use the texts in the gnome corpus to
choose among them. We return to the issue of
coherence (i.e., whether additional coherence-
inducing factors need to be stipulated in addi-
tion to those assumed in Centering) in the Dis-
cussion.
3 Centering-based metrics of
coherence
As said previously, we assume a text structuring
system taking as input a set of utterances rep-
resented in terms of their CF lists. The system
orders these utterances by applying a bias in
favour of the best scoring ordering among the
candidate solutions for the preferred output.8
In this section, we discuss how the Centering
8Additional assumptions for choosing between the or-
derings that are assigned the best score are presented in
the next section.
concepts just described can be used to define
metrics of coherence which might be useful for
text structuring.
The simplest way to define a metric of coher-
ence using notions from Centering is to classify
each ordering of propositions according to the
number of nocbs it contains, and pick the or-
dering with the fewest nocbs. We call this met-
ric M.NOCB, following (Karamanis and Manu-
rung, 2002). Because of its simplicity, M.NOCB
serves as the baseline metric in our experiments.
We consider three more metrics. M.CHEAP
is biased in favour of the ordering with the
fewest violations of cheapness. M.KP sums
up the nocbs and the violations of cheapness,
coherence and salience, preferring the or-
dering with the lowest total cost (Kibble and
Power, 2000). Finally, M.BFP employs the
preferences between standard transitions as ex-
pressed by Rule 2. More specifically, M.BFP
selects the ordering with the highest number
of continues. If there exist several orderings
which have the most continues, the one which
has the most retains is favoured. The number
of smooth-shifts is used only to distinguish
between the orderings that score best for con-
tinues as well as for retains, etc.
In the next section, we present a general
methodology to compare these metrics, using
the actual ordering of clauses in real texts of
a corpus to identify the metric whose behav-
ior mimics more closely the way these actual
orderings were chosen. This methodology was
implemented in a program called the System for
Evaluating Entity Coherence (seec).
4 Exploring the space of possible
orderings
In section 2, we discussed how an ordering of
utterances in a text like (1) can be translated
into a sequence of CF lists, which is the repre-
sentation that the Centering-based metrics op-
erate on. We use the term Basis for Comparison
(BfC) to indicate this sequence of CF lists. In
this section, we discuss how the BfC is used in
our search-oriented evaluation methodology to
calculate a performance measure for each metric
and compare them with each other. In the next
section, we will see how our corpus was used
to identify the most promising Centering-based
metric for a text classifier.
4.1 Computing the classification rate
The performance measure we employ is called
the classification rate of a metric M on a cer-
tain BfC B. The classification rate estimates
the ability of M to produce B as the output of
text structuring according to a specific genera-
tion scenario.
The first step of seec is to search through
the space of possible orderings defined by the
permutations of the CF lists that B consists of,
and to divide the explored search space into sets
of orderings that score better, equal, or worse
than B according to M.
Then, the classification rate is defined accord-
ing to the following generation scenario. We
assume that an ordering has higher chances of
being selected as the output of text structuring
the better it scores for M. This is turn means
that the fewer the members of the set of better
scoring orderings, the better the chances of B
to be the chosen output.
Moreover, we assume that additional factors
play a role in the selection of one of the order-
ings that score the same for M. On average, B
is expected to sit in the middle of the set of
equally scoring orderings with respect to these
additional factors. Hence, half of the orderings
with the same score will have better chances
than B to be selected by M.
The classification rate ? of a metric M on
B expresses the expected percentage of order-
ings with a higher probability of being gener-
ated than B according to the scores assigned
by M and the additional biases assumed by the
generation scenario as follows:
(3) Classification rate:
?(M,B) = Better(M) + Equal(M)2
Better(M) stands for the percentage of order-
ings that score better than B according to M,
whilst Equal(M) is the percentage of order-
ings that score equal to B according to M. If
?(Mx, B) is the classification rate of Mx on B,
and ?(My, B) is the classification rate of My on
B, My is a more suitable candidate than Mx
for generating B if ?(My, B) is smaller than
?(Mx, B).
4.2 Generalising across many BfCs
In order for the experimental results to be re-
liable and generalisable, Mx and My should be
compared on more than one BfC from a corpus
C. In our standard analysis, the BfCs B1, ..., Bm
from C are treated as the random factor in a
repeated measures design since each BfC con-
tributes a score for each metric. Then, the clas-
sification rates for Mx and My on the BfCs are
compared with each other and significance is
tested using the Sign Test. After calculating the
number of BfCs that return a lower classifica-
tion rate for Mx than for My and vice versa, the
Sign Test reports whether the difference in the
number of BfCs is significant, that is, whether
there are significantly more BfCs with a lower
classification rate for Mx than the BfCs with a
lower classification rate for My (or vice versa).9
Finally, we summarise the performance of M
on m BfCs from C in terms of the average clas-
sification rate Y :
(4) Average classification rate:
Y (M,C) = ?(M,B1)+...+?(M,Bm)m
5 Using the gnome corpus for a
search-based comparison of
metrics
We will now discuss how the methodology
discussed above was used to compare the
Centering-based metrics discussed in Section
3, using the original ordering of texts in the
gnome corpus to compute the average classi-
fication rate of each metric.
The gnome corpus contains texts from differ-
ent genres, not all of which are of interest to us.
In order to restrict the scope of the experiment
to the text-type most relevant to our study, we
selected 20 ?museum labels?, i.e., short texts
that describe a concrete artefact, which served
as the input to seec together with the metrics
in section 3.10
5.1 Permutation and search strategy
In specifying the performance of the metrics we
made use of a simple permutation heuristic ex-
ploiting a piece of domain-specific communica-
tion knowledge (Kittredge et al, 1991). Like
Dimitromanolaki and Androutsopoulos (2003),
we noticed that utterances like (a) in exam-
ple (1), should always appear at the beginning
of a felicitous museum label. Hence, we re-
stricted the orderings considered by the seec
9The Sign Test was chosen over its parametric al-
ternatives to test significance because it does not carry
specific assumptions about population distributions and
variance. It is also more appropriate for small samples
like the one used in this study.
10Note that example (1) is characteristic of the genre,
not the length, of the texts in our subcorpus. The num-
ber of CF lists that the BfCs consist of ranges from 4 to
16 (average cardinality: 8.35 CF lists).
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 18 2 0 0.000 M.NOCB
M.NOCB vs M.KP 16 2 2 0.001 M.NOCB
M.NOCB vs M.BFP 12 3 5 0.018 M.NOCB
N 20
Table 3: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP in gnome
to those in which the first CF list of B, CF1,
appears in first position.11
For very short texts like (1), which give rise to
a small BfC, the search space of possible order-
ings can be enumerated exhaustively. However,
when B consists of many more CF lists, it is im-
practical to explore the search space in this way.
Elsewhere we show that even in these cases it
is possible to estimate ?(M,B) reliably for the
whole population of orderings using a large ran-
dom sample. In the experiments reported here,
we had to resort to random sampling only once,
for a BfC with 16 CF lists.
5.2 Comparing M.NOCB with other
metrics
The experimental results of the comparisons of
the metrics from section 3, computed using the
methodology in section 4, are reported in Ta-
ble 3.
In this table, the baseline metric M.NOCB is
compared with each of M.CHEAP, M.KP and
M.BFP. The first column of the Table identifies
the comparison in question, e.g. M.NOCB ver-
sus M.CHEAP. The exact number of BfCs for
which the classification rate of M.NOCB is lower
than its competitor for each comparison is re-
ported in the next column of the Table. For ex-
ample, M.NOCB has a lower classification rate
than M.CHEAP for 18 (out of 20) BfCs from
the gnome corpus. M.CHEAP only achieves a
lower classification rate for 2 BfCs, and there
are no ties, i.e. cases where the classification
rate of the two metrics is the same. The p value
returned by the Sign Test for the difference in
the number of BfCs, rounded to the third deci-
mal place, is reported in the fifth column of the
Table. The last column of the Table 3 shows
M.NOCB as the ?winner? of the comparison
with M.CHEAP since it has a lower classifica-
11Thus, we assume that when the set of CF lists serves
as the input to text structuring, CF1 will be identified
as the initial CF list of the ordering to be generated
using annotation features such as the unit type which
distinguishes (a) from the other utterances in (1).
tion rate than its competitor for significantly
more BfCs in the corpus.12
Overall, the Table shows that M.NOCB does
significantly better than the other three metrics
which employ additional Centering concepts.
This result means that there exist proportion-
ally fewer orderings with a higher probability of
being selected than the BfC when M.NOCB is
used to guide the hypothetical text structuring
algorithm instead of the other metrics.
Hence, M.NOCB is the most suitable among
the investigated metrics for structuring the CF
lists in gnome. This in turn indicates that sim-
ply avoiding nocb transitions is more relevant
to text structuring than the combinations of the
other Centering notions that the more compli-
cated metrics make use of. (However, these no-
tions might still be appropriate for other tasks,
such as anaphora resolution.)
6 Discussion: the performance of
M.NOCB
We already saw that Poesio et al (2004) found
that the majority of the recorded transitions in
the configuration of Centering used in this study
are nocbs. However, we also explained in sec-
tion 2.3 that what really matters when trying
to determine whether a text might have been
generated only paying attention to Centering
constraints is the extent to which it would be
possible to ?improve? upon the ordering chosen
in that text, given the information that the text
structuring algorithm had to convey. The av-
erage classification rate of M.NOCB is an esti-
12No winner is reported for a comparison when the p
value returned by the Sign Test is not significant (ns),
i.e. greater than 0.05. Note also that despite conduct-
ing more than one pairwise comparison simultaneously
we refrain from further adjusting the overall threshold
of significance (e.g. according to the Bonferroni method,
typically used for multiple planned comparisons that em-
ploy parametric statistics) since it is assumed that choos-
ing a conservative statistic such as the Sign Test already
provides substantial protection against the possibility of
a type I error.
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 110 12 0 0.000 M.NOCB
M.NOCB vs M.KP 103 16 3 0.000 M.NOCB
M.NOCB vs M.BFP 41 31 49 0.121 ns
N 122
Table 4: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP using the novel methodology
in MPIRO
mate of exactly this variable, indicating whether
M.NOCB is likely to arrive at the BfC during
text structuring.
The average classification rate Y for
M.NOCB on the subcorpus of gnome studied
here, for the parameter configuration of Cen-
tering we have assumed, is 19.95%. This means
that on average the BfC is close to the top 20%
of alternative orderings when these orderings
are ranked according to their probability of
being selected as the output of the algorithm.
On the one hand, this result shows that al-
though the ordering of CF lists in the BfC
might not completely minimise the number of
observed nocb transitions, the BfC tends to
be in greater agreement with the preference to
avoid nocbs than most of the alternative or-
derings. In this sense, it appears that the BfC
optimises with respect to the number of poten-
tial nocbs to a certain extent. On the other
hand, this result indicates that there are quite
a few orderings which would appear more likely
to be selected than the BfC.
We believe this finding can be interpreted in
two ways. One possibility is that M.NOCB
needs to be supplemented by other features in
order to explain why the original text was struc-
tured this way. This is the conclusion arrived at
by Poesio et al (2004) and those text structur-
ing practitioners who use notions derived from
Centering in combination with other coherence
constraints in the definitions of their metrics.
There is also a second possibility, however: we
might want to reconsider the assumption that
human text planners are trying to ensure that
each utterance in a text is locally coherent.
They might do all of their planning just on the
basis of Centering constraints, at least in this
genre ?perhaps because of resource limitations?
and simply accept a certain degree of incoher-
ence. Further research on this issue will require
psycholinguistic methods; our analysis never-
theless sheds more light on two previously un-
addressed questions in the corpus-based evalu-
ation of Centering ? a) which of the Centering
notions are most relevant to the text structur-
ing task, and b) to which extent Centering on
its own can be useful for this purpose.
7 Further results
In related work, we applied the methodology
discussed here to a larger set of existing data
(122 BfCs) derived from the MPIRO system
and ordered by a domain expert (Dimitro-
manolaki and Androutsopoulos, 2003). As Ta-
ble 4 shows, the results from MPIRO verify the
ones reported here, especially with respect to
M.KP and M.CHEAP which are overwhelm-
ingly beaten by the baseline in the new do-
main as well. Also note that since M.BFP fails
to overtake M.NOCB in MPIRO, the baseline
can be considered the most promising solution
among the ones investigated in both domains
by applying Occam?s logical principle.
We also tried to account for some additional
constraints on coherence, namely local rhetor-
ical relations, based on some of the assump-
tions in Knott et al (2001), and what Kara-
manis (2003) calls the ?PageFocus? which cor-
responds to the main entity described in a text,
in our example de374. These results, reported
in (Karamanis, 2003), indicate that these con-
straints conflict with Centering as formulated in
this paper, by increasing - instead of reducing
- the classification rate of the metrics. Hence,
it remains unclear to us how to improve upon
M.NOCB.
In our future work, we would like to experi-
ment with more metrics. Moreover, although we
consider the parameter configuration of Center-
ing used here a plausible choice, we intend to ap-
ply our methodology to study different instan-
tiations of the Centering parameters, e.g. by
investigating whether ?indirect realisation? re-
duces the classification rate for M.NOCB com-
pared to ?direct realisation?, etc.
Acknowledgements
Special thanks to James Soutter for writing the
program which translates the output produced by
gnome?s scripts into a format appropriate for seec.
The first author was able to engage in this research
thanks to a scholarship from the Greek State Schol-
arships Foundation (IKY).
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2002. Inferring strategies
for sentence ordering in multidocument news
summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. 1987. A
centering approach to pronouns. In Proceed-
ings of ACL 1987, pages 155?162, Stanford,
California.
Barbara Di Eugenio. 1998. Centering in Italian.
In Walker et al (Walker et al, 1998b), pages
115?137.
Aggeliki Dimitromanolaki and Ion Androut-
sopoulos. 2003. Learning to order facts for
discourse planning in natural language gen-
eration. In Proceedings of the 9th European
Workshop on Natural Language Generation,
Budapest, Hungary.
Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?225.
Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. 2003. Speaking the
users? languages. IEEE Intelligent Systems
Magazine, 18(1):40?45.
Nikiforos Karamanis and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring us-
ing the principle of continuity. In Proceedings
of INLG 2002, pages 81?88, Harriman, NY,
USA, July.
Nikiforos Karamanis. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D. the-
sis, Division of Informatics, University of Ed-
inburgh.
Rodger Kibble and Richard Power. 2000. An
integrated framework for text planning and
pronominalisation. In Proceedings of INLG
2000, pages 77?84, Israel.
Rodger Kibble. 2001. A reformulation of Rule
2 of Centering Theory. Computational Lin-
guistics, 27(4):579?587.
Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 1991. On the need for domain com-
munication knowledge. Computational Intel-
ligence, 7:305?314.
Alistair Knott, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001. Beyond
elaboration: The interaction of relations
and focus in coherent text. In T. Sanders,
J. Schilperoord, and W. Spooren, edi-
tors, Text Representation: Linguistic and
Psycholinguistic Aspects, chapter 7, pages
181?196. John Benjamins.
Mirella Lapata. 2003. Probabilistic text struc-
turing: Experiments with sentence ordering.
In Proceedings of ACL 2003, Saporo, Japan,
July.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments us-
ing stochastic search for text planning. In
Proceedings of the 9th International Work-
shop on NLG, pages 98?107, Niagara-on-the-
Lake, Ontario, Canada.
Eleni Miltsakaki. 2002. Towards an aposyn-
thesis of topic continuity and intrasenten-
tial anaphora. Computational Linguistics,
28(3):319?355.
Mick O?Donnell, Chris Mellish, Jon Oberlan-
der, and Alistair Knott. 2001. ILEX: An ar-
chitecture for a dynamic hypertext genera-
tion system. Natural Language Engineering,
7(3):225?250.
Rebecca J. Passoneau. 1998. Interaction of dis-
course structure with explicitness of discourse
anaphoric phrases. In Walker et al (Walker
et al, 1998b), pages 327?358.
Massimo Poesio, Rosemary Stevenson, Barbara
Di Eugenio, and Janet Hitzeman. 2004. Cen-
tering: a parametric theory and its instantia-
tions. Computational Linguistics, 30(3).
Ehud Reiter and Robert Dale. 2000. Building
Natural Language Generation Systems. Cam-
bridge.
Michael Strube and Udo Hahn. 1999. Func-
tional centering: Grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in nat-
urally occuring discourse: An overview. In
Walker et al (Walker et al, 1998b), pages
1?30.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Clarendon Press, Ox-
ford.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627?634,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Whose thumb is it anyway?
Classifying author personality from weblog text
Jon Oberlander
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
j.oberlander@ed.ac.uk
Scott Nowson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
s.nowson@ed.ac.uk
Abstract
We report initial results on the relatively
novel task of automatic classification of
author personality. Using a corpus of per-
sonal weblogs, or ?blogs?, we investigate
the accuracy that can be achieved when
classifying authors on four important per-
sonality traits. We explore both binary and
multiple classification, using differing sets
of n-gram features. Results are promising
for all four traits examined.
1 Introduction
There is now considerable interest in affective lan-
guage processing. Work focusses on analysing
subjective features of text or speech, such as sen-
timent, opinion, emotion or point of view (Pang
et al, 2002; Turney, 2002; Dave et al, 2003; Liu
et al, 2003; Pang and Lee, 2005; Shanahan et al,
2005). Discussing affective computing in general,
Picard (1997) notes that phenomena vary in du-
ration, ranging from short-lived feelings, through
emotions, to moods, and ultimately to long-lived,
slowly-changing personality characteristics.
Within computational linguistics, most work
has focussed on sentiment and opinion concern-
ing specific entities or events, and on binary clas-
sifications of these. For instance, both Pang and
Lee (2002) and Turney (2002) consider the thumbs
up/thumbs down decision: is a film review posi-
tive or negative? However, Pang and Lee (2005)
point out that ranking items or comparing re-
views will benefit from finer-grained classifica-
tions, over multiple ordered classes: is a film re-
view two- or three- or four-star? And at the same
time, some work now considers longer-term af-
fective states. For example, Mishne (2005) aims
to classify the primary mood of weblog post-
ings; the study encompasses both fine-grained
(but non-ordered) multiple classification (frus-
trated/loved/etc.) and coarse-grained binary clas-
sification (active/passive, positive/negative).
This paper is about the move to finer-grained
multiple classifications; and also about weblogs.
But it is also about even more persistent affec-
tive states; in particular, it focusses on classifying
author personality. We would argue that ongo-
ing work on sentiment analysis or opinion-mining
stands to benefit from progress on personality-
classification. The reason is that people vary in
personality, and they vary in how they appraise
events?and hence, in how strongly they phrase
their praise or condemnation. Reiter and Sripada
(2004) suggest that lexical choice may sometimes
be determined by a writer?s idiolect?their per-
sonal language preferences. We suggest that while
idiolect can be a matter of accident or experience,
it may also reflect systematic, personality-based
differences. This can help explain why, as Pang
and Lee (2005) note, one person?s four star re-
view is another?s two-star. To put it more bluntly,
if you?re not a very outgoing sort of person, then
your thumbs up might be mistaken for someone
else?s thumbs down. But how do we distinguish
such people? Or, if we spot a thumbs-up review,
how can we tell whose thumb it is, anyway?
The paper is structured as follows. It introduces
trait theories of personality, notes work to date on
personality classification, and raises some ques-
tions. It then outlines the weblog corpus and the
experiments, which compare classification accura-
cies for four personality dimensions, seven tasks,
and five feature selection policies. We discuss the
implications of the results, and related work, and
end with suggestions for next steps.
627
2 Background: traits and language
Cattell?s pioneering work led to the isolation of
16 primary personality factors, and later work on
secondary factors led to Costa and McCrae?s five-
factor model, closely related to the ?Big Five?
models emerging from lexical research (Costa and
McCrae, 1992). Each factor gives a continu-
ous dimension for personality scoring. These
are: Extraversion; Neuroticism; Openness; Agree-
ableness; and Conscientiousness (Matthews et al,
2003). Work has also investigated whether scores
on these dimensions correlate with language use
(Scherer, 1979; Dewaele and Furnham, 1999).
Building on the earlier work of Gottschalk and
Gleser, Pennebaker and colleagues secured signif-
icant results using the Linguistic Inquiry and Word
Count text analysis program (Pennebaker et al,
2001). This primarily counts relative frequencies
of word-stems in pre-defined semantic and syn-
tactic categories. It shows, for instance, that high
Neuroticism scorers use: more first person singu-
lar and negative emotion words; and fewer arti-
cles and positive emotion words (Pennebaker and
King, 1999).
So, can a text classifier trained on such features
predict the author personality? We know of only
one published study: Argamon et al (2005) fo-
cussed on Extraversion and Neuroticism, dividing
Pennebaker and King?s (1999) population into the
top- and bottom-third scorers on a dimension, and
discarding the middle third. For both dimensions,
using a restricted feature set, they report binary
classification accuracy of around 58%: an 8% ab-
solute improvement over their baseline. Although
mood is more malleable, work on it is also relevant
(Mishne, 2005). Using a more typical feature set
(including n-grams of words and parts-of-speech),
the best mood classification accuracy was 66%, for
?confused?. At a coarser grain, moods could be
classified with accuracies of 57% (active vs. pas-
sive), and 60% (positive vs. negative).
So, Argamon et al used a restricted feature set
for binary classification on two dimensions: Ex-
traversion and Neuroticism. Given this, we now
pursue three questions. (1) Can we improve per-
formance on a similar binary classification task?
(2) How accurate can classification be on the other
dimensions? (3) How accurate can multiple?
three-way or five-way?classification be?
3 The weblog corpus
3.1 Construction
A corpus of personal weblog (?blog?) text has been
gathered (Nowson, 2006). Participants were re-
cruited directly via e-mail to suitable candidates,
and indirectly by word-of-mouth: many partici-
pants wrote about the study in their blogs. Par-
ticipants were first required to answer sociobio-
graphic and personality questionnaires. The per-
sonality instrument has specifically been validated
for online completion (Buchanan, 2001). It was
derived from the 50-item IPIP implementation of
Costa and McCrae?s (1992) revised NEO person-
ality inventory; participants rate themselves on 41-
items using a 5-point Likert scale. This provides
scores for Neuroticism, Extraversion, Openness,
Agreeableness and Conscientiousness.
After completing this stage, participants were
requested to submit one month?s worth of prior
weblog postings. The month was pre-specified so
as to reduce the effects of an individual choos-
ing what they considered their ?best? or ?preferred?
month. Raw submissions were marked-up using
XML so as to automate extraction of the desired
text. Text was also marked-up by post type, such
as purely personal, commentary reporting of ex-
ternal matters, or direct posting of internet memes
such as quizzes. The corpus consisted of 71 par-
ticipants (47 females, 24 males; average ages 27.8
and 29.4, respectively) and only the text marked
as ?personal? from each weblog, approximately
410,000 words. To eliminate undue influence of
particularly verbose individuals, the size of each
weblog file was truncated at the mean word count
plus 2 standard deviations.
3.2 Personality distribution
It might be thought that bloggers are more Ex-
travert than most (because they express themselves
in public); or perhaps that they are less Extravert
(because they keep diaries in the first place). In
fact, plotting the Extraversion scores for the cor-
pus authors gives an apparently normal distribu-
tion; and the same applies for three other dimen-
sions. However, scores for Openness to experi-
ence are not normally distributed. Perhaps blog-
gers are more Open than average; or perhaps there
is response bias. Without a comparison sample of
matched non-bloggers, one cannot say, and Open-
ness is not discussed further in this paper.
628
4 Experiments
We are thus confined to classifying on four per-
sonality dimensions. However, a number of other
variables remain: different learning algorithms
can be employed; authors in the corpus can be
grouped in several ways, leading to various classi-
fication tasks; and more or less restricted linguistic
feature sets can be used as input to the classifier.
4.1 Algorithms
Support Vector Machines (SVM) appear to work
well for binary sentiment classification tasks, so
Argamon et al (2003) and Pang and Lee (2005)
consider One-vs-All, or All-vs-All, variants on
SVM, to permit multiple classifications. Choice
of algorithm is not our focus, but it remains to
be seen whether SVM outperforms Na??ve Bayes
(NB) for personality classification. Thus, we will
use both on the binary Tasks 1 to 3 (defined in sec-
tion 4.2.1), for each of the personality dimensions,
and each of the manually-selected feature sets
(Levels I to IV, defined in section 4.3). Whichever
performs better overall is then reported in full, and
used for the multiple Tasks 4 to 7 (defined in sec-
tion 4.2.2). Both approaches are applied as imple-
mented in the WEKA toolkit (Witten and Frank,
1999) and use 10-fold cross validation.
4.2 Tasks
For any blog, we have available the scores, on con-
tinuous scales, of its author on four personality di-
mensions. But for the classifier, the task can be
made more or less easy, by grouping authors on
each of the dimensions. The simplest tasks are, of
course, binary: given the sequence of words from
a blog, the classifier simply has to decide whether
the author is (for instance) high or low in Agree-
ableness. Binary tasks vary in difficulty, depend-
ing on whether authors scoring in the middle of a
dimension are left out, or not; and if they are left
out, what proportion of authors are left out.
More complex tasks will also vary in difficulty
depending on who is left out. But in the cases
considered here, middle authors are now included.
For a three-way task, the classifier must decide
if an author is high, medium or low; and those
authors known to score between these categories
may, or may not, be left out. In the most challeng-
ing five-way task, no-one is left out. The point of
considering such tasks is to gradually approximate
the most challenging task of all: continuous rating.
4.2.1 Binary classification tasks
In these task variants, the goal is to classify au-
thors as either high or low scorers on a dimension:
1. The easiest approach is to keep the high and
low groups as far apart as possible: high scor-
ers (H) are those whose scores fall above
1 SD above the mean; low scorers (L) are
those whose scores fall below 1 SD below the
mean.
2. Task-1 creates distinct groups, at the price of
excluding over 50% of the corpus from the
analysis. To include more of the corpus, pa-
rameters are relaxed: the high group (HH)
includes anyone whose score is above .5 SD
above the mean; the low group (LL) is simi-
larly placed below.
3. The most obvious task (but not the easiest)
arises from dividing the corpus in half about
the mean score. This creates high (HHH) and
low (LLL) groups, covering the entire pop-
ulation. Inevitably, some HHH scorers will
actually have scores much closer to those of
LLL scorers than to other HHH scorers.
These sub-groups are tabulated in Table 1, giv-
ing the size of each group within each trait. Note
that in Task-2, the standard-deviation-based divi-
sions contain very nearly the top third and bottom
third of the population for each dimension. Hence,
Task-2 is closest in proportion to the division by
thirds used in Argamon et al (2005).
Lowest . . . Highest
1 L ? H
2 LL ? HH
3 LLL HHH
N1 12 ? 13
N2 25 ? 22
N3 39 32
E1 11 ? 12
E2 23 ? 24
E3 32 39
A1 11 ? 13
A2 22 ? 21
A3 34 37
C1 11 ? 14
C2 17 ? 27
C3 30 41
Table 1: Binary task groups: division method and
author numbers. N = Neuroticism; E = Extraver-
sion; A = Agreeableness; C = Conscientiousness.
629
4.2.2 Multiple classification tasks
4. Takes the greatest distinction between high
(H) and low (L) groups from Task-1, and
adds a medium group, but attempts to reduce
the possibility of inter-group confusion by in-
cluding only the smaller medium (m) group
omitted from Task-2. Not all subjects are
therefore included in this analysis. Since the
three groups to be classified are completely
distinct, this should be the easiest of the four
multiple-class tasks.
5. Following Task-4, this uses the most distinct
high (H) and low (L) groups, but now consid-
ers all remaining subjects medium (M).
6. Following Task-2, this uses the larger high
(hH) and low (Ll) groups, with all those in
between forming the medium (m) group.
7. Using the distinction between the high and
low groups of Task-5 and -6, this creates a
5-way split: highest (H), relatively high (h),
medium (m), relatively low (l) and lowest
(L). With the greatest number of classes, this
task is the hardest.
These sub-groups are tabulated in Table 2, giving
the size of each group within each trait.
Lowest . . . Highest
4 L ? m ? H
5 L M H
6 Ll m hH
7 L l m h H
N4 12 ? 24 ? 13
N5 12 46 13
N6 25 24 22
N7 12 13 24 9 13
E4 11 ? 24 ? 12
E5 11 48 12
E6 23 24 24
E7 11 12 24 12 12
A4 11 ? 28 ? 13
A5 11 47 13
A6 22 28 21
A7 11 11 28 8 13
C4 11 ? 27 ? 14
C5 11 46 14
C6 17 27 27
C7 11 6 27 13 14
Table 2: 3-way/5-way task groups: division
method and author numbers. N = Neuroticism; E
= Extraversion; A = Agreeableness; C = Consci-
entiousness.
4.3 Feature selection
There are many possible features that can be
used for automatic text classification. These ex-
periments use essentially word-based bi- and tri-
grams. It should be noted, however, that some
generalisations have been made: all proper nouns
were identified via CLAWS tagging using the
WMatrix tool (Rayson, 2003), and replaced with
a single marker (NP1); punctuation was collapsed
into a single marker (<p>); and additional tags
correspond to non-linguistic features of blogs?
for instance, <SOP> and <EOP> were used the
mark the start and end of individual blogs posts.
Word n-gram approaches provide a large feature
space with which to work. But in the general
interest of computational tractability, it is useful
to reduce the size of the feature set. There are
many automatic approaches to feature selection,
exploiting, for instance, information gain (Quin-
lan, 1993). However, ?manual? methods can of-
fer principled ways of both reducing the size of
the set and avoiding overfitting. We therefore ex-
plore the effect of different levels of restriction on
the feature sets, and compare them with automatic
feature selection. The levels of restriction are as
follows:
I The least restricted feature set consists of the
n-grams most commonly occurring within the
blog corpus. Therefore, the feature set for
each personality dimension is to be drawn
from the same pool. The difference lies in the
number of features selected: the size of the set
will match that of the next level of restriction.
II The next set includes only those n-grams
which were distinctive for the two extremes
of each personality trait. Only features with
a corpus frequency ?5 are included. This al-
lows accurate log-likelihood G2 statistics to
be computed (Rayson, 2003). Distinct collo-
cations are identified via a three way compar-
ison between the H and L groups in Task-1
(see section 4.2.1) and a third, neutral group.
This neutral group contains all those individ-
uals who fell in the medium group (M) for
all four traits in the study; the resulting group
was of comparable size to the H and L groups
for each trait. Hence, this approach selects
features using only a subset of the corpus. N-
gram software was used to identify and count
collocations within a sub-corpus (Banerjee
630
and Pedersen, 2003). For each feature found,
its frequency and relative frequency are calcu-
lated. This permits relative frequency ratios
and log-likelihood comparisons to be made
between High-Low, High-Neutral and Low-
Neutral. Only features that prove distinctive
for the H or L groups with a significance of
p < .01 are included in the feature set.
III The next set takes into account the possibil-
ity that, for a group used in Level-II, an n-
gram may be used relatively frequently, but
only because a small number of authors in a
group use it very frequently, while others in
the same group use it not at all. To enter the
Level-III set, an n-gram meeting the Level-II
criteria must also be used by at least 50%1 of
the individuals within the subgroup for which
it is reported to be distinctive.
IV While Level-III guards against excessive indi-
vidual influence, it may abstract too far from
the fine-grained variation within a personality
trait. The final manual set therefore includes
only those n-grams that meet the Level-II cri-
teria with p < .001, meet the Level-III crite-
ria, and also correlate significantly (p < .05)
with individual personality trait scores.
V Finally, it is possible to allow the n-gram fea-
ture set to be selected automatically during
training. The set to be selected from is the
broadest of the manually filtered sets, those
n-grams that meet the Level-II criteria. The
approach adopted is to use the defaults within
the WEKA toolkit: Best First search with the
CfsSubsetEval evaluator (Witten and Frank,
1999).
Thus, a key question is when?if ever?a ?man-
ual? feature selection policy outperforms the auto-
matic selection carried out under Level-V. Levels-
II and -III are of particular interest, since they con-
tain features derived from a subset of the corpus.
Since different sub-groups are considered for each
personality trait, the feature sets which meet the
increasingly stringent criteria vary in size. Table 3
contains the size of each of the four manually-
determined feature sets for each of the four per-
sonality traits. Note again that the number of n-
grams selected from the most frequent in the cor-
1Conservatively rounded down in the case of an odd num-
ber of subjects.
I II III IV V
N 747 747 169 22 19
E 701 701 167 11 20
A 823 823 237 36 34
C 704 704 197 22 25
Table 3: Number of n-grams per set.
Low High
[was that] [this year]
N [NP1 <p> NP1] [to eat]
[<p> after] [slowly <p>]
[is that] [and buy]
[point in] [and he]
E [last night <p>] [cool <p>]
[it the] [<p> NP1]
[is to] [to her]
[thank god] [this is not]
A [have any] [<p> it is]
[have to] [<p> after]
[turn up] [not have]
[a few weeks] [by the way]
C [case <p>] [<p> i hope]
[okay <p>] [how i]
[the game] [kind of]
Table 4: Examples of significant Low and High
n-grams from the Level-IV set.
pus for Level-I matches the size of the set for
Level-II. In addition, the features automatically se-
lected are task-dependent, so the Level-V sets vary
in size; here, the Table shows the number of fea-
tures selected for Task-2.
To illustrate the types of n-grams in the feature
sets, Table 4 contains four of the most significant
n-grams from Level-IV for each personality class.
5 Results
For each of the 60 binary classification tasks (1
to 3), the performance of the two approaches was
compared. Na??ve Bayes outperformed Support
Vector Machines on 41/60, with 14 wins for SVM
and 5 draws. With limited space available, we
therefore discuss only the results for NB, and use
NB for Task-4 to -7. The results for the binary
tasks are displayed in Table 5. Those for the mul-
tiple tasks are displayed in Table 6. Baseline is the
majority classification. The most accurate perfor-
mance of a feature set for each task is highlighted
631
Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V
N1 52.0 52.0 92.0 84.0 96.0 92.0
N2 53.2 51.1 63.8 68.1 83.6 85.1
N3 54.9 54.9 60.6 53.5 71.9 83.1
E1 52.2 56.5 91.3 95.7 87.0 100.0
E2 51.1 44.7 74.5 72.3 66.0 93.6
E3 54.9 50.7 53.5 59.2 64.8 85.9
A1 54.2 62.5 100.0 100.0 95.8 100.0
A2 51.2 60.5 81.4 79.1 72.1 97.7
A3 52.1 53.5 60.6 69.0 66.2 93.0
C1 56.0 52.0 100.0 100.0 84.0 92.0
C2 61.2 54.5 77.3 81.8 72.7 93.2
C3 57.7 54.9 63.4 71.8 70.4 84.5
Table 5: Na??ve Bayes performance on binary
tasks. Raw % accuracy for 4 personality dimen-
sions, 3 tasks, and 5 feature selection policies.
in bold while the second most accurate is marked
italic.
6 Discussion
Let us consider the results as they bear in turn on
the three main questions posed earlier: Can we im-
prove on Argamon et al?s (2005) performance on
binary classification for the Extraversion and Neu-
roticism dimensions? How accurately can we clas-
sify on the four personality dimensions? And how
does performance on multiple classification com-
pare with that on binary classification?
Before addressing these questions, we note the
relatively good performance of NB compared with
?vanilla? SVM on the binary classification tasks.
We also note that automatic selection generally
outperforms ?manual? selection; however overfit-
ting is very likely when examining just 71 data
points. Therefore, we do not discuss the Level-V
results further.
6.1 Extraversion and Neuroticism
The first main question relates to the feature sets
chosen, because the main issue is whether word n-
grams can give reasonable results on the Extraver-
sion and Neuroticism classification tasks. Of the
current binary classification tasks, Task-2 is most
closely comparable to Argamon et al?s. Here, the
best performance for Extraversion was returned
by the ?manual? Level-II feature set, closely fol-
lowed by Level-III. The accuracy of 74.5% repre-
sents a 23.4% absolute improvement over baseline
Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V
N4 49.0 49.0 81.6 65.3 77.6 85.7
N5 64.8 60.6 76.1 67.6 67.6 94.4
N6 35.2 31.0 47.9 46.5 66.2 70.4
N7 33.8 31.0 49.3 38.0 42.3 47.9
E4 51.1 44.7 74.5 59.6 53.2 78.7
E5 67.6 60.6 83.1 67.6 54.9 90.1
E6 33.8 23.9 53.5 46.5 46.5 56.3
E7 33.8 44.7 39.4 29.6 38.0 40.8
A4 53.8 51.9 90.4 78.8 67.3 80.8
A5 66.2 59.2 83.1 84.5 74.6 80.3
A6 39.4 31.0 67.6 60.6 56.3 85.9
A7 39.4 33.8 69.8 60.6 50.7 47.9
C4 51.9 53.8 92.3 65.4 67.3 82.7
C5 64.8 62.0 74.6 69.0 62.0 83.1
C6 38.0 39.4 59.2 59.2 50.7 78.9
C7 38.0 36.6 62.0 45.1 45.1 49.3
Table 6: Na??ve Bayes performance on multiple
tasks. Raw % accuracy for 4 personality dimen-
sions, 4 tasks, and 5 feature selection policies.
(45.8% relative improvement; we report relative
improvement over baseline because baseline accu-
racies vary between tasks). The best performance
for Neuroticism was returned by Level-IV. The ac-
curacy of 83.6% represents a 30.4% absolute im-
provement over baseline (57.1% relative improve-
ment).
Argamon et al?s feature set combined in-
sights from computational stylometrics (Koppel et
al., 2002; Argamon et al, 2003) and systemic-
functional grammar. Their focus on function
words and appraisal-related features was intended
to provide more general and informative features
than the usual n-grams. Now, it is unlikely that
weblogs are easier to categorise than the genres
studied by Argamon et al So there are instead at
least two reasons for the improvement we report.
First, although we did not use systemic-
functional linguistic features, we did test n-grams
selected according to more or less strict policies.
So, considering the manual policies, it seems that
the Level-IV was the best-performing set for Neu-
roticism. This might be expected, given that
Level-IV potentially overfits, allowing features to
be derived from the full corpus. However, in
spite of this, Level-II pproved best for Extraver-
sion. Secondly, in classifying an individual as high
or low on some dimension, Argamon et al had
632
(for some of their materials) 500 words from that
individual, whereas we had approximately 5000
words. The availability of more words per indi-
vidual is to likely to help greatly in training. Ad-
ditionally, a greater volume of text increases the
chances that a long term ?property? such as per-
sonality will emerge
6.2 Binary classification of all dimensions
The second question concerns the relative ease
of classifying the different dimensions. Across
each of Task-1 to -3, we find that classification
accuracies for Agreeableness and Conscientious-
ness tend to be higher than those for Extraver-
sion and Neuroticism. In all but two cases, the
automatically generated feature set (V) performs
best. Putting this to one side, of the manually
constructed sets, the unrestricted set (I) performs
worst, often below the baseline, while Level-IV is
the best for classifying each task of Neuroticism.
Overall, II and III are better than IV, although the
difference is not large.
As tasks increase in difficulty?as high and low
groups become closer together, and the left-out
middle shrinks?performance drops. But accu-
racy is still respectable.
6.3 Beyond binary classification
The final question is about how classification ac-
curacy suffers as the classification task becomes
more subtle. As expected, we find that as we add
more categories, the tasks are harder: compare the
results in the Tables for Task-1, -5 and -7. And,
as with the binary tasks, if fewer mid-scoring in-
dividuals are left out, the task is typically harder:
compare results for Task-4 and 5. It does seem that
some personality dimensions respond to task dif-
ficulty more robustly than others. For instance, on
the hardest task, the best Extraversion classifica-
tion accuracy is 10.9% absolute over the baseline
(32.2% relative), while the best Agreeableness ac-
curacy is 30.4% absolute over the baseline (77.2%
relative). It is notable that the feature set which
return the best results?bar the automatic set V?
tends to be Level-II, excepting for Neuroticism on
Task-6, where Level-IV considerably outperforms
the other sets.
A supplementary question is how the best clas-
sifiers compare with human performance on this
task. Mishne (2005) reports that, for general
mood classification on weblogs, the accuracy of
his automatic classifier is comparable to human
performance. There are also general results on
human personality classification performance in
computer-mediated communication, which sug-
gest that at least some dimensions can be ac-
curately judged even when computer-mediated.
Vazire and Gosling (2004) report that for personal
websites, relative accuracy of judgment was, in de-
scending order: Openness > Extraversion > Neu-
roticism > Agreeableness > Conscientiousness.
Similarly, Gill et al (2006) report that for personal
e-mail, Extraversion is more accurately judged
than Neuroticism. The current study does not have
a set of human judgments to report. For now, it is
interesting to note that the performance profile for
the best classifiers, on the simplest tasks, appears
to diverge from the general human profile, instead
ranking on raw accuracy: Agreeableness > Con-
scientiousness > Neuroticism > Extraversion.
7 Conclusion and next steps
This paper has reported the first stages of our in-
vestigations into classification of author personal-
ity from weblog text. Results are quite promis-
ing, and comparable across all four personality
traits. It seems that even a small selection of fea-
tures found to exhibit an empirical relationship
with personality traits can be used to generate rea-
sonably accurate classification results. Naturally,
there are still many paths to explore. Simple re-
gression analyses are reported in Nowson (2006);
however, for classification, a more thorough com-
parison of different machine learning methodolo-
gies is required. A richer set of features besides
n-grams should be checked, and we should not ig-
nore the potential effectiveness of unigrams in this
task (Pang et al, 2002). A completely new test
set can be gathered, so as to further guard against
overfitting, and to explore systematically the ef-
fects of the amount of training data available for
each author. And as just discussed, comparison
with human personality classification accuracy is
potentially very interesting.
However, it does seem that we are making
progress towards being able to deal with a real-
istic task: if we spot a thumbs-up review in a we-
blog, we should be able to check other text in that
weblog, and tell whose thumb it is; or more accu-
rately, what kind of person?s thumb it is, anyway.
And that in turn should help tell us how high the
thumb is really being held.
633
8 Acknowledgements
We are grateful for the helpful advice of Mirella
Lapata, and our three anonymous reviewers. The
second author was supported by a studentship
from the Economic and Social Research Council.
References
Shlomo Argamon, Marin Saric, and Sterling S. Stein.
2003. Style mining of electronic messages for mul-
tiple authorship discrimination: first results. In Pro-
ceedings of SIGKDD, pages 475?480.
Shlomo Argamon, Sushant Dhawle, Moshe Koppel,
and James W. Pennebaker. 2005. Lexical predic-
tors of personality type. In Proceedings of the 2005
Joint Annual Meeting of the Interface and the Clas-
sification Society of North America.
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the ngram statistics
package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 370?381, Mexico City.
Tom Buchanan. 2001. Online implementation of an
IPIP five factor personality inventory [web page].
http://users.wmin.ac.uk/?buchant/wwwffi/
introduction.html [Accessed 25/10/05].
Paul T. Costa and Robert R. McCrae, 1992. Re-
vised NEO Personality Inventory (NEO-PI-R) and
NEO Five-Factor Inventory (NEO-FFI): Profes-
sional Manual. Odessa, FL: Psychological Assess-
ment Resources.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International Conference on
World Wide Web, pages 519?528. ACM Press.
Jean-Marc Dewaele and Adrian Furnham. 1999. Ex-
traversion: The unloved variable in applied linguis-
tic research. Language Learning, 49:509?544.
Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.
2006. Rating e-mail personality at zero acquain-
tance. Personality and Individual Differences,
40:497?507.
Moshe Koppel, Shlomo Argamon, and Arat Shimoni.
2002. Automatically categorizing written texts by
author gender. Literary and Linguistic Computing,
17(4):401?412.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 7th International
Conference on Intelligent User Interfaces.
Gerald Matthews, Ian J. Deary, and Martha C. White-
man. 2003. Personality Traits. Cambridge Univer-
sity Press, Cambridge, 2nd edition.
Gilad Mishne. 2005. Experiments with mood classifi-
cation in blog posts. In Proceedings of ACM SIGIR
2005 Workshop on Stylistic Analysis of Text for In-
formation Access.
Scott Nowson. 2006. The Language of Weblogs: A
study of genre and individual differences. Ph.D. the-
sis, University of Edinburgh.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the ACL, pages 115?124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
James W. Pennebaker and Laura King. 1999. Lin-
guistic styles: Language use as an individual differ-
ence. Journal of Personality and Social Psychology,
77:1296?1312.
James W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count
2001. Lawrence Erlbaum Associates, Mahwah, NJ.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, Ma.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Paul Rayson. 2003. Wmatrix: A statistical method and
software tool for linguistic analysis through corpus
comparison. Ph.D. thesis, Lancaster University.
Ehud Reiter and Somayajulu Sripada. 2004. Contex-
tual influences on near-synonym choice. In Pro-
ceedings of the Third International Conference on
Natural Language Generation.
Klaus Scherer. 1979. Personality markers in speech.
In K. R. Scherer and H. Giles, editors, Social Mark-
ers in Speech, pages 147?209. Cambridge Univer-
sity Press, Cambridge.
James G. Shanahan, Yan Qu, and Janyce Weibe, edi-
tors. 2005. Computing Attitude and Affect in Text.
Springer, Dordrecht, Netherlands.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unspervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the ACL, pages 417?424.
Simine Vazire and Sam D. Gosling. 2004. e-
perceptions: Personality impressions based on per-
sonal websites. Journal of Personality and Social
Psychology, 87:123?132.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
634
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
Optimising text quality in generation from relational databases 
Michael  O 'Donne l l t  (micko@dai .ed .ac .uk) ,  
A l i s ta i r  Knott:~ (a l i k@hermes .o tago .ac .nz ) ,  
Jon  Ober lander ,  ( jon@cogsc i .ed .ac .uk) ,  
Chr i s  Me l l i sh t (chr i sm@dai .ed .ac .uk)  
, D iv is ion  of  In fo rmat ics ,  Un ivers i ty  of  Ed inburgh .  
. . . . .  ~.:D.eparl~me~t.nf: Compulzer?c ience~ ~Otago Univers ity:  
Abst rac t  
This paper outlines a text generation system suited 
to a large class of information sources, relational 
databases. We focus on one aspect of the problem: 
the additional information which needs to be spe- 
cified to produce reasonable text quality when gen- 
erating from relational databases. We outline how 
databases need to be prepared, and then describe 
various types of domain semantics which can be used 
to improve text qualify. 
1 In t roduct ion  
As the problems of how we generate text are gradu- 
ally solved, a new problem is gaining prominence 
- where do we obtain the information which feeds 
the generation. Many domain models for existing 
generation systems are hand-crafted for the specific 
system. Other systems take advantage of existing 
information sources. 
A good information source for text generation 
resides in the vast number of relational databases 
which are in use around tile world. These resources 
have usually been provided for some reason other 
than text generation, such as inventory manage- 
ment, accounting, etc. However, given that the in- 
formation is on hand, it can be of value to conuect 
these databases to text generation facilities. 
The benefits include natural anguage access to in- 
formation which is usually accessed in tabular form, 
which can be difficult to interpret. Natural Lan- 
guage descriptions are easier to read, can be tailored 
to user types, and can be expressed in different lan- 
guages if properly represented. 
This paper outlines the domain specification lan- 
guage for the ILEX text g~neration system, (for 
Intelligent Labelling Explorer). 1
ILEX is a tool for ?dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
1Earlier ILEX papers have been based on Ilex 2.0, which 
was relatively domain-dependent.  This  paper is based around 
version 3.0 of ILEX, a re-draft to make the system domain- 
independent, and domain acquisition far easier. The ILEX 
project was supported by EPSRC grant GR/K53321.  
text. ILEX generates descriptions of database ob- 
jects on the fly, taking into account he user's con- 
text of browsing. Figure 1 shows the ILEX web in- 
terface, as applied to a museum domain, in this case 
the Twentieth Century Jewellery exhibition at the 
the National Museum of Scotland. 2 The links to 
related database objects are also automatically gen- 
erated. ILEX has been applied to other domains, in- 
cluding personnel (Nowson, 1999), and a sales cata- 
logue for computer systems and peripherals (Ander- 
son and Bradshaw, 1998). 
One of the advantages of using NLG for database 
browsing is that the system can keep track of what 
has already been said about objects, and not repeat 
that information on later pages. Appropriate refer- 
ring expressions can also be selected on the basis 
of the discourse history. The object descriptions can 
be tailored to the informational interests of the user. 
See Knott et al (1997) and Mellish et al (1998) for 
more information on these aspects of ILEX. 
In section 2, we consider some systems related to 
the ILEX system. Section 3 describes the form of 
relational database that ILEX accepts as input. Sec- 
tion 4 outlines what additional information - domain 
semantics - needs to be provided for coherent ext 
production from the database, while section 5 de- 
scribes additional information which can be provided 
to improve the quality of the text produced. 
2 Re la ted  Work  
It should be clear that the task we are discussing is 
very distinct from the task of response generation in 
a natural language interface to a database (e.g., see 
Androutsopoulos et al (1995)). ' In such systtems, 
the role of text planning is quite simple or absent, 
usually dealing with single sentences, or in the most 
? ? complex systems;~ a:single:sentence ,answer ~with an 
additional clause or two of supporting information. 
ILEX is not a query response generation system, 
it is an object description system. It composes a full 
text, at whatever size, with the goal of making that 
text a coherent discourse. 
2The authors thank the museum for making their database 
available: 
133 
Sflver.A~nd Ename . :  
!- S.~.v~ t !~s ,  w i~ blu~-~e~.~i/e.1 . ! 
' ) . :~v{ .  ,EX :~- - : :  . . . . . . . . . . . . . .  )' . . . . . . .  " t ' ': i:: ' :  :!:i 
? lessie-I~-X~g.,l~. Place of,. ; 
? This Jewel !s apel'l.d~mat-neckla~ ililitwaS . . 
I madebZ,aSa~h de:a,S~caUed-Jesae M , 
l<: gin,g:ilt~bnedlhe f~mrRemStn:tht~:case.,::_: , '  
:: ? / lowers reseri~A a~ai~t  i t - I t  is tn ~e Arts :~ud,  
. Crafts:style and was made t1~ :lgfl~ It has an . . . .  
elaborate aesign; specifically It h~ floral mows.  
: :;::anlllustrat~too, In fact., shg did qttite, a' l~ of,-: : 
" differei~tl~rpes of creative Wark;/cwdleiTls ? : i:. 
:. ; :  :i'; ~:t~n Arts. amt Craft#Style . ),.::.:):i:.: 2,} :,i.: 'i:' :" 
:'::' ; :-'? ;~.~,~t,~I,t~,,/~l.a~.~_~': ~" ; : "  : : ; " : " ,  -'; 7.; ...:. 
."  ' .:,L'~n Ai'ts aiid Crafts:s~lgne~iil~e -: :.::'..': " i 
J 
.; ... (; . 
Figure 1: Browsing Object Descriptions 
In this regard, ILEX should be more fruit- 
fully compared with text generation systems such 
as GOSSIP (Carcagno and Iordanskaja, 1993), 
PEBA (Milosavljevie, 1997; Milosavljevic, 1999), or 
POWER (Dale et al, 1998), systems which build an 
extended text fl'om an underlying database. 
ILEX 3.0 has been developed to be domain in- 
dependent, to handle relational databases from any 
domain, as long as the information is provided in the 
required format. The first two of the systems above 
are single domain systems. T:he third, POWER,  is 
an extension of PEBA to handle a new domain. It 
is not clear however whether the resulting system is 
.. itself domain-dependent or not. 
This last system is perhaps the best comparison 
for the ILEX system, since it also generates de- 
scriptions of museum objects from an underlying 
database. In that paper, the main focus is on the 
problem of extracting out usable information from 
badly structured databases (as often provided by 
museulns), and on generating texts using only only 
this information (plus some linguistic knowledge). 
The present paper differs from this approach by as- 
suming that information is already available in a nor- 
malised relational database. We observe, as do Dale 
et al (1998), that texts generated from this inform- 
ation alone are quite poor in quality. We go one 
step further by examining what additional informa- 
tion can be provided to improve the quality of the 
text to a reasonable l vel. 
The ILEX system has been implemented to be 
flexible in regards to the available domain inform- 
ation. With a bare minimum, the system provides 
poor quality texts, but as the domain developer ex- 
.tends-the domain semantics, the quahty of.texts im- 
proves, up to a point where users sometimes nfistake 
ILEX-generated texts for human-authored texts. 
3 The Structure of a Relational 
Database 
Databases vary widely in form, so we have assumed 
a fairly" standard relational database format. 
134 
3.1 Entity Files 
:.The database consists of .a number:.:of ~ntity files, 
each file providing the records for a different entity 
type. Each record (row) in the entity file defines a 
unique entity. The columns define attributes of the 
entities. In a museum domain, we might have an 
entity file for museum artifacts, another for people 
involved with the artifacts (designers, owners, etc.), 
another for locations, etc. See figure 2 for a sample 
entity file for the Jewellery domain. Given the wide 
.range of database formats..a~vailable, !LEX ~sumes 
a tab-delimited format for database files. 
ILEX imposes two requirements on the entity files 
it uses: 
1. Single field key: while relational databases of- 
ten use multiple attributes to form a unique key 
(e.g., name and birthdate), ILEX requires that 
each entity have a unique identifier in a single 
attribute. This identifier must be under a field 
labelled ID. 
2. Typing of entities: ILEX depends trongly on a 
type system. We require that each entity record 
provides a type for the entity in a field labelled 
Class. 
Some other attribute labels are reserved by the 
system, allowing ILEX to deal intelligently with 
them, including Name, Short-Name and Gender. 
3.2 L ink Fi les 
In some cases, an entity will have multiple fillers of 
an attribute, for instance, a jewellery piece may be 
made of any number of materials. Entity files, with 
fixed record structure, cannot handle such eases. 
The standard approach in relational databases i to 
provide a link file for each case where multiple fillers 
are possible. A link file consists of two columns only, 
one identifying the entity, the other identifying the 
filler (the name of the attribute is provided in the 
first line of the file, see figure 3). 
We are aware that the above specification repres- 
ents an impoverished view of relational databases. 
Many relational databases provide far more than 
simple entity and link files. However, by no means 
all relational databases provide more than this, so 
we have adopted the lowest common denominator. 
Most relational databases can be exported in a form 
which meets our requirements. 
3.3 Terminology 
In the following discussion, we will use the following 
terminology: 
* Predicate: each column of an entity file defines 
a predicate. Class, Designer and Date are thus 
predicates introduced in figure 2. Each link file 
also defines a predicate. 
? Record: each row of an entity table provides the 
attributes o f  a: single.,entity.: The row is termed 
a record in database terminology. 
? Fact: each entry in a record defines what we 
call a fact about that entity, a A fact consists o f  
three parts: its predicate name, and two argu- 
ments, being the entity of the record, and the 
filler of the slot. 
? ARC1: the first argument of a fact, the entity 
the  fact is about. 
. ARC2: the second argument of a fact, the filler 
of the attribute for the entity. 
4 Spec i fy ing  the  Semant ics  o f  the  
Database  
A database itself says nothing about the nature of 
the contents of each field in the database. It might 
be a name, a date, a price, etc. Similarly for the 
field label: the field label names a relation between 
the entity represented by the record and the entity 
represented by the filler. However, without further 
specification, we do not know what this relationship 
entails, apart from the label itself, e.g., 'Designer'. 
Before we can begin to process a database intelli- 
gently, we need to define the 'semantics' of the data- 
base. This section will outline how this is done in the 
ILEX case. There has been some work on automatic 
acquisition of database semantics, uch as in the con- 
struction of taxonomies of domain entity types (see 
Dale et al (1998) for instance). However, it is diffi- 
cult to perform this process reliably and in a domain- 
independent manner, so we have not attempted to 
in this case. The specification of domain semantics 
is still a manual process which has to be undertaken 
to link a database to the text generator. 
To use a database for generation, additional in- 
formation of several kinds needs to be provided: 
1. Taxonomic organisation: supplying of types for 
each database ntity, and organisation of these 
types into taxonomies; 
2. Taxonomic lexification: specif~'ing how each do- 
main type is lexified; 
3. Data type off attribute fillers: telling the system 
to expect the filler of a record slot to be an 
entity-id, a string, a date, etc. 
4. Domain type specification:specifying What do- 
main type the slot filler can be assumed to be. 
Each of these aspects of domain specification will 
be briefly described below. 
3Excepting the first column, which provides the entity-id 
for tile record. 
135 
 Class brooch -necklace necklace Designer KingO1 "KingO1 ChanelO1 Style J___190~ A-rt-Deco : ~_~_~ Art-Noveux London Paris 
L_ 
Sponsor 
Liberty01 
Figure 2: A Sample from an Entity file 
\ [ ~ .  Material 
Figure 3: A Sample from a Link file 
(def-basic-type 
:domain jewellery-domain 
:head jewellery 
:mn-link 3D-PHYS-0BJECT) 
(def-taxonomy 
:type jewellery 
:subtypes (neck-jewellery wrist-jewellery 
pin-jewellery pendant buckle 
earring earring-pair finger-ring 
ringset watch button dress-clip 
hat-pin)) 
Figure 4: Defining Taxonomic Knowledge 
4.1 Taxonomic  Organ isat ion  
ILEX requires that the entities of the domain are or- 
ganised under a domain taxonomy. The user defines 
a basic type (e.g., jewellery), and then defines the 
sub-types of the basic-type, and perhaps further sub- 
classification. Figure 4 shows the lisp forms defining 
a basic type in the jewellery domain, and the sub- 
classification of this type. The basic type is also 
mapped onto a type (or set of types) in the concept 
ontology used for sentence generation, a version of 
Penman's Upper Model (Bateman, 1990). This al- 
lows the sentence generator to reason about the ob- 
jects it expresses. 
Taxonomic organisation is important for several 
reasons, including among others: 
1. Expressing Entities: each type can be related to 
lexical i tems'to use,to-express that  type (e.g., 
linking the type brooch to a the lexical item for 
"brooch". If no lexical item is defined for a type, 
a lexical item associated with some super-type 
can be used instead. Other aspects of the ex- 
pression of entities may depend on the concep- 
tual type, for instance pronominalisation, deixis 
(e.g., mass or count entities), etc. 
2. Supporting Inferences and Generalisations: 
ILEX allows the user to assert generalisations 
about types, e.g., that Arts and Crafts jewellery 
tends to be made using enamel (see section 5.4). 
The type hierarchy is used to check whether a 
particular generalisation is appropriate for any 
given instance. 
The earlier version of ILEX, Ilex2.0, allowed the 
full representational power of the Systemic formal- 
ism for representing domain taxonomies, including 
cross-classification, and multiple inheritance (both 
disjunctive and conjunctive). However, our exper- 
iences with non-linguists trying to define domain 
models showed us that the more scope for expres- 
sion, the more direction was needed. We thus sim- 
plified the formalism, by requiring taxonomies to be 
simple, with no cross-classification r multiple inher- 
itance. We felt that the minor loss of expressivity 
was well balanced by the gain in simplicity for do- 
main developers. 
4.2 Type Lexi f icat ion 
To express each database ntity, it is essential to be 
able to map from its defined type, to a noun to use 
in a referring expression, e.g., this brooch. 
Ilex comes with a basic lexicon already provided. 
covering the commonly occurring words. Each entry 
defines the svntactic and morphological information 
required for sentence generation. For these items, 
the domain developer needs to provide a simpl e map- 
ping from domain type to lexical item, for instance, 
the following lisp form specifies that the domain type 
location should be lexified by the lexical item whose 
id is location=noun: 
(lexify location location-noun) 
For those lexical items not already defined, the do- 
main developer needs to provide in addition lexical 
item definitions for the nouns expressing the types 
in their domain. A typical entry has the form shown 
in figure 5. 
136 
(def-lexical-item 
:name professor-noun 
:spelling "professor" 
:grammatical-features (common-noun count-noun) 
) 
Figure 5: A Sample Lexical item Specification 
. . . .  (defobject-structurejewellery- " ..... 
:class :generic-type 
:subclass :generic-type 
:designer :entity-id 
:style :entity-id 
:material :generic-type 
:date :date 
:place :string 
:dimension :dimension) 
Figure 6: Specifying Field Semantics 
(def-predicateClass 
:expression (:verb be-verb) 
) 
Figure 8: Simple Fact Expression 
4.3 Data Type of Slot Fillers 
Each field in a database record contains a string of 
characters. It is not clear whether this string is an 
identifier for another domain entity, a string (e.g., 
someone's urname), a date, a number, a type in 
the type hierarchy, etc. 
ILEX requires, for each entity file, a statement as 
to how the field fillers should be interpreted. See 
figure 6 for an example. 
Some special filler types have been provided to 
facilitate the import of structured ata types. This 
includes both :date and :dimension in the current 
example. Special code has been written to convert 
the fillers of these slots into ILEX objects. Other 
special filler types are being added as needed. 
4.4 Domain  Type  o f  Slot Fi l lers 
The def-predicate form allows the domain developer 
to state what type the fillers of a particular field 
should be. This not only allows for type checking, 
but also allows the type of an entity to be inferred 
if not otherwise provided. For instance, by assert- 
ing that fillers of the Place field should of type city, 
the system can infer that "London" is a city even if 
London itself has no database record. See figure 7. 
(def-predicate Place 
:argl jewellery 
:arg2 city 
) 
Figure 7: Speci~'ing Predicate Fillers 
4.5 Summary  
..... '.:~With:just chisvmuch-semantics~specified,. ILEX e-an 
generate very poor texts, but texts which convey 
the content of the database records. In the next 
section, we will outline the extensions to the domain 
semantics which are needed to improve the quality 
of the text produced by ILEX. 
5 Extending Domain Semantics for 
Improved Text Quality 
So far we have discussed only the simplest level of 
domain semantics, which allows a fairly direct ex- 
pression of domain information. ILEX allows the 
domain developer to provide additional domain se- 
mantics to improve the quality of the text. 
5.1 Expression of Facts 
Unless told otherwise, ILEX will express each fact in 
a simple regular form, such as The designer of this 
brooch is Jessie M. King, using a template form4: 
The <predicate> of <entity-expression> 
is <filler-expression>. 
However, a text consisting solely of clauses of this 
form is unnatural, and depends on the predicate la- 
bel being appropriate to the task (labels like given-by 
will produce nonsense sentences). 
To produce better text, ILEX can be told how 
to express facts. The domain developer can provide 
an optional slot to the &f-predicate form as shown 
in figure 8. The expression specification first of all 
defines which verb to use in the expression. By de- 
fault, the ARG1 element is mapped onto the Sub- 
ject, and the ARG2 onto the Object. Default val- 
ues are assumed for tense, modality, polarity, voice. 
finiteness, quantification, etc., unless otherwise spe- 
cified. So, using the above expression specification, 
the Class fact of a jewel would be expressed by a 
clause like: This item is a brooch. 
To .produce less .standard expressions, we need to 
modify some of the defaults. A more complex ex- 
pression specification is shown in figure 9, which 
would result in the expression such as: For further 
information, see Liberty Style Guide No. 326: 
4ILEX3.0  borrowed this use of a default  express ion tem- 
p late  from the POWER system (Dale et al, 1998). In previ-  
ous vers ions of ILEX,  all facts were expressed by full NLG as 
exp la ined below. 
137 
(def-predicate Bib-Note 
:argl jewellery 
:expression ( 
:adjunctl "for further information" 
:mood imperative 
:verb see-verb 
:voice active) 
Figure 9: More Complex Fact Expression 
The expression form is used to construct a par- 
tial syntactic specification, which is then completed 
using the sentence generation module of the WAG 
sentence generator (O'Donnell, 1996). 
With the level of domain semantics pecified so 
far, ILEX is able to produce texts such as the two be- 
low, which provides an initial page describing data- 
base entity BUNDY01, and then a subsequent page 
when more information was requested (this from the 
Personnel domain (Nowson, 1999)): 
o Page  1: Alan Bundy is located in room F1, 
which is in South Bridge. He lectures a course 
called Advanced Automated Reasoning and is in 
the Institute for Representation and Reasoning. 
He is the Head of Division and is a professor. 
* Page  2: As already mentioned, Alan Bundy lec- 
tures Advanced Automated Reasoning. AAR is 
lectured to MSc and AI4. 
This expression specification form has been de- 
signed to limit the linguistic skills needed for domain 
developers working with the system. Given that the 
domain developers may be museum staff, not com- 
putational linguists, this is necessary. The notation 
however allows for a wide range of linguistic expres- 
sions if the full range of parameters are used. 
5.2 User  Adapt ion  
To enable the system to adapt its content to the 
type of user, the domain developers can associate 
information with each predicate indicating the sys- 
tem's view of the predicate's interest, importance, 
etc., to the user. This information is added to the 
d@predicate form, as shown in figure 10. 
The user annotations allowed by ILEX include: 
1. Interest: how interesting does the system judge 
the information to be to the user; 
2. Importance: how important is it to the system 
that the user reads the information; 
3. Assimilation: to what degree does the system 
judge the user to already know the infornlation: 
.<def~predicate Designer 
. o .  
:importance ((expert lO)(default 6)(child 5)) 
:interest ((expert lO)(default 6)(child 4)) 
:assimilation ((expert O)(default O)(child 0)) 
:assim-rate ((expert l)(default l)(child 0.5)) 
) 
Figure 10: Specifying User Parameters 
4. Assimilation Rate: How quickly does the sys- 
tem believe the user will absorb the information 
when presented (is one presentation enough?). 
This information influences what content will be 
expressed to a particular user, and in what or- 
der (more relevant on earlier pages). Information 
already assimilated will not be delivered, except 
when relevant for other purposes (e.g., when refer- 
ring to the entity). If no annotations are provided, 
no user customisation will occur. 
The values in ILEX's user models have been set 
intuitively by the implementers. While ideally these 
values would be derived through user studies, our 
purpose was purely to test the adaptive mechanism, 
and demonstrate that it works. We .leave the devel- 
opment of real user models for later work. 
ILEX has opted out of using adaptive user model- 
ling, whereby the user model attributes are adapted 
as a result of observed user choices in the web inter- 
face. We leave this for future research. 
5.3 Compar i sons  
When describing an object, it seems sometimes use- 
ful to compare it to similar articles already seen. 
With small addition to the domain specification, 
ILEX can compare items (an extension by Maria Mi- 
losavljevic), as demonstrated in the following text: 
This item is also a brooch. Like the previ- 
ous item, it was designed by King. How- 
ever, it differs from the previous item in 
that it is made of gold and enamel, while 
the previous brooch was made of silver and 
enamel. 
For ILEX to properly compare two entities, it 
needs to Mmw how the various.attributes of the en- 
tity can be compared (nominal, ordinal, scalar, etc.). 
Again, information can be added to the d@predicate 
for each predicate to define its scale of comparabil- 
ity. See Milosavljevic (1997) and (1999) for more de- 
tail. Figure 11 shows the additions for the Designer 
predicate. Comparisons introduce several RST re- 
lations to the text structure, including rst-contrast, 
rst-similarity and rst-whereas. 
138 
(def-predicate Designer 
:variation (string i) 
:scale nominal 
) 
Figure lh Specifying Predicate Comparability 
(def-defeasible-rule 
? :qv ($jewel jewellery) ....... 
:lhs (some ($X (style $jewel $X)) 
(arts-and-crafts SX))) 
:rhs (some ($X (made-of Sjewel SX)) 
(enamel SX))) 
Figure 12: Specifying Generalisations 
5 . 4  G e n e r a l i s a t i o n s  
We found it useful to allow facts about general types 
of entities to be asserted, for instance, that Arts and 
Crafts jewellery tend to be made of enamel. These 
generalisations can then be used to improve the qual- 
ity of text, producing object descriptions as in the 
following: 
This brooch is in the Arts and Crafts style. 
Arts and Crafts jewels tend to be made of 
enamel. However, this one is not. 
These generalisations are defined using defeasible 
implication - similar to the usual implication, but 
working in terms of few, many, or most rather than 
all or none. They are entered in a form derived 
from first order predicate calculus, for instance, see 
figure 12 which specifies that most Arts and Crafts 
jewellery uses enamel. 
ILEX find each instance which matches the gen- 
eral type (in this case, instances of type jewellery 
which have Arts and Crafts in the Style role). If 
the fact about the generic object has a correspond- 
ing fact on the instantial object, an exemplification 
relation is asserted between the facts. Otherwise, 
a ?concession relation is asserted. See Knott et al 
(1997) for more details on this procedure. 
6 Summary  
While observing people trying to convert an earlier 
ILEX system to a new domain, we noted the diffi- 
culty they had. To avoid these problems, we under- 
took to re-implement the domain specification as- 
pects of ILEX to simplify the task. 
Towards this end, we have followed a number of 
steps. Firstly, we reconstructed ILEX to be domain 
- Taxonomies 
- Lexification of Types 
- Filler Domain Type Information 
- Filler Data Type Information 
OBLIGATORY 
- Predicate Expression 
- Comparison Information 
- Generalisations 
- User Annotations 
OPTIONAL 
Figure 13: Obligatory and Optional Steps in Domain 
Specification 
independent, with all domain information defined in 
declarative resource files. This means that domain 
developers do not have to deal with code. 
Secondly, we built into ILEX the ability to import 
entity definitions directly from a relational database 
(although with some restrictions as to its form). 
A database by itself does not provide enough in- 
formation to produce text. Domain semantics is re- 
quired. We have provided a system of incremental 
specification of this semantics which allows a domain 
developer to hook up adynamic hypertext interface 
to a relational database quickly, although producing 
poor quality text. Minimally, the system requires 
a domain taxonomy, information on lexification of 
types, and specification of the data type of each re- 
cord field. 
Additional effort can then improve the quality of 
text up to a quite reasonable l vel. The additional 
information can include: specification of predicate 
expression, and specifications supporting comparis- 
ons, user adaption, and generalisations. 
Figure 13 summarises the obligatory and optional 
steps in domain specification in ILEX. 
Simplifying the domain specification task is a ne- 
cessity as text generation systems move outside of 
research labs and into the real world, where the 
domain developer may not be a computational lin- 
guist, but a museum curator, personnel officer or 
wine salesman. ~ have tried to take a step towards 
making their task easier. 
Re ferences  
Gail Anderson and Tim Bradshaw. 1998. ILEX: 
The intelligent labelling explorer: Experience of 
Building a Demonstrator for the Workstation Do- 
main. Internal Report, Artificial Intelligence Ap- 
plications tnstitute,University of Edinburgh. 
I. Androutsopoulos, G.D. Ritchie, and P. Thanisch. 
1995. Natural language interfaces to databases - 
an introduction. Natural Language Engineering, 1
(1):29-81. 
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
139 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Denis Carcagno and Lidija Iordanskaja. 1993. Con- 
tent determination a d text structuring: two in- 
terrelated processes. In Helmut Horocek and Mi- 
chael Zock, editors, New Concepts in Natural Lan- 
guage Generation, Communication i Artificial 
Intelligence Series, pages 10 - 26. Pinter: London. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In "Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
guage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
Maria Milosavljevic. 1997. Augmenting the user's 
knowledge via comparison. In Proceedings of the 
6th International Conference on User Modelling, 
pages 119-130, Sardinia, 2-5 June. 
Maria Milosavljevic. 1999. Maximising the Co- 
herence of Descriptions via Comparison. Ph.D. 
thesis, Macquarie University, Sydney, Australia. 
Scott Nowson. 1999. Acquiring ILEX for a Per- 
sonnel Domain. Honours Thesis, Artificial Intel- 
ligence, University of Edinburgh. 
Michael O'Donnell. 1996. Input specification i the 
wag sentence generation system. In Proceedings of 
the 8th International Workshop on Natural Lan- 
guage Generation, Herstmonceux Castle, UK, 13- 
15 June. 
140 - ' 
Demonstrat ion of ILEX 3.0 
Michae l  O 'Donne l l t  (micko@dai .ed.ac .uk) ,  
A l is ta i r  Knott:~ (a l ik@hermes.otago.ac .nz) ,  
Jon  Ober lander t  ( jon@cogsci .ed.ac.uk) ,  
Chr is  Mel l isht (chr ism@dai .ed.ac .uk)  
t Divis ion o f  Informat ics ,  Un ivers i ty  of  Ed inburgh .  
:~ Depar tment  of  Computer  Science ~ Otago  University.  
Abst rac t  
We will demonstrate the ILEX system, a system 
which dynamically generates descriptions of data- 
base objects for the web, adapting the description to 
the discourse context and user type. Among other 
improvements in version 3, the system now gener- 
ates from relational databases, and this demonstra- 
tion will focus on this ability. We will also show how 
incremental extensions to the domain semantics im- 
prove the quality of the text produced. 
1 In t roduct ion  
ILEX is a tool for dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
text. ILEX generates descriptions of a database ob- 
ject on the fly, taking into account he user's con- 
text of browsing. For more information on ILEX, 
see Knott et al (1997) and Mellish et al (1998). 
The demonstration will consist of generating a
series of texts, in each case adding in additional com- 
ponents of the domain semantics. This short paper 
should be read in conjunction with the full paper 
elsewhere in this volume. 
2 Generat ing  f rom Bare  Data  
We start initially with a relational database, as 
defined by a set of tab-delimited database files, plus 
some minimal semantics. As discussed in the paper, 
we use assume a relational database to consist of two 
types of files: 
1. Entity Files: each of which provides data for 
a particular entity type. Each row (or record) 
defines the attributes of a different entity. See 
figure 1. 
2. Link Files: where a particular attribute may 
have multiple fillers, we use link files to define 
the entity-entity relations. See figure 2. 
To generate from these files, the dolnain-editor 
needs to provide two additional resources: 
1. Data-type specification for each entity-file, a 
specification of what data-type the values in the 
~ Material 
silver 
enamel 
gold 
Figure 2: A Sample from a Link file 
. 
3. 
column are, e.g., string, entity-id, domain type, 
etc. 
Domain Taxonomy: detailing the taxonomic or- 
ganisation of the various classes of the entities. 
Mapping Domain taxonomy onto Upper Model: 
ILEX uses an Upper Model (a domain- 
independent semantic taxonomy, see Bateman 
(1990)), which supports the grammatical ex- 
pression of entities, e.g., selection of pronoun, 
differentiation between mass and count entities, 
between things and qualities, etc. We require 
that the basic types in the domain taxonomy 
are mapped onto the upper model, to allow the 
entities to be grammaticalised and lexicalised 
appropriately. 
With just this semantics, we can generate texts, 
although impoverished texts, such as: 
The class of J-997 is necklace. It's de- 
signer is Jessie M. King. It's date is 1905. 
Several tricks are needed to generate without a 
specified omain semantics: 
Use of standard clause templates: lacking any 
knowledge of how different attributes are to be 
expressed, the system-can only generate ach 
attribute using a standard template structures, 
such as the X of Y is Z or It's X is Z. The 
attribute names, e.g., Designer, Style, etc. can 
be assumed to work as the lexical head of the 
Subject. This ploy sometimes goes wrong, but 
in general works. (this approach borrowed from 
Dale et al (1998)). 
257 
ID Class 
J-997 brooch 
J~998: :neddace 
J-999 i necklace 
etc. I 
Designer Date Style Place Sponsor 
King01 11905 Art-Deco London Liberty01 
King01 '19116 - Art-Deco "London 
Chanel01 1910 Art-Noveau Paris 
Figure 1: A Sample from an Entity file 
* Referring to Entities: there are a number of 
strategies open for referring to entities. If the 
Name attribute.is.supplied-(a:defined- attribute 
within the ILEX system), then the system can 
use this for referring. Lacking a name, it is pos- 
sible for the system to form nominal references 
using the Class attr ibute of the entity (all en- 
tities in ILEX databases are required to have 
this attribute provided). We could thus gener- 
ate indefinite references such as a brooch as first 
mentions, and on subsequent mentions, gener- 
ate forms such as the brooch or the brooch whose 
designer is Jessie M. King. Without specifica- 
tion of which entities should be considered part 
of the general knowledge of the reader, we must 
assume all entities are initially unknown. 
* Fact Annotations: ILEX was designed to work 
with various extra information known about 
facts, such as the assumed level of interest to the 
current reader model, the importance of the fact 
to the system's educational agenda, and the as- 
sumed assimilation of the information (how well 
does the system believe the reader to already 
understand it). See the main paper for more 
details. 
Lacking this information, the system assumes 
an average value for interest and importance, 
and a 0 value for assimilation (totally un- 
known). 
With only default values, the system cannot 
customise the text to the particular user. It may 
provide information already well known by the 
user, and thus risking boring them. Also, there 
can be no selection of information to ensure that 
the more interesting and important information 
is provided on earlier pages (the reader may not 
bother to look at later pages). 
Other information (defeasible rules), which allows 
us to organise the material into complex rhetorical 
structure, is also missing. 
So, these tricks allow us to generate simple texts, 
consisting of a list of template-formatted clauses. 
3 Add ing  Express ion  in fo rmat ion  
In the next step, we will add in information about 
how the various attributes hould be expressed. This 
includes three main resources: 
1. Syntactic expression of attributes: for each at- 
tribute, we provide a specification of how the 
......... ~. ~.~ribu:te~should~be~-expressed. syntactically. 
2. Lexicalisation of domain types: by providing 
a lexicon, which maps domain types to lexical 
items, we avoid problems of using the domain 
type itself as the spelling. The lexical inform- 
ation allows correct generation of inflectional 
forms (e.g., of the plural for nouns, comparative 
or superlative forms for adjectives). 
3. Restrictive modifiers for referring expressions: 
In choosing restrictive modifiers for forming re- 
ferring expressions, ome facts work better than 
others. For instance, the brooch designed by 
King is more likely to refer adequately than the 
brooch which was 3 inches long. ILEX allows 
the user to state the preferential order for choos- 
ing restrictive modifiers. 
The addition of these resources will result in im- 
proved expression within the clauses, but not af- 
fect the text structure itself, which are still a list 
of clauses in random order. 
4 Add ing  User  Annotat ions  
In the next step, we add in the user model, which 
provides, for each attribute type, predicted user in- 
terest, importance for the system, and expected user 
assimilation. 
Using these values, ILEX can start to organise 
the text, placing important/interesting i formation 
on earlier pages, and avoiding information already 
known by tile user. 
5 Add ing  Defeas ib le  Ru les ,  S tor ies  
As a final step, we add in various resources which 
improve the texture of the text. 
o Defeasible Rules: ILEX allows the assertion 
of generalisations like most Art Deco jewels 
use enamel. These rules allow the genera- 
tion of complex rhetorical structures which in- 
dude Generalisation, Exemplification and Con- 
cession. The use of these relations improves tim 
quality of the text generated. 
* Stories: much of the information obtainable 
about tile domain is in natural language. Of- 
ten, the information is specific to a particular 
258 
entity, and as such, it would be a waste of time 
to reduce the in.formation i to ILEX's Pred-Arg 
knowledge structure, just to regenerate he text. 
Because of this, ILEX allows the association 
of canned text with a database ntity (e.g., J- 
999), or type of entity (e.g., jewels designed for 
Liberty). The text can then be included in the 
text when the entity or type of entity is men- 
tioned. 
The intermixing of generated and canned text 
improves the qual i ty of generated texts by 
providing more variety of structures, and al- 
lowing anecdotes, which would be difficult to 
model in terms of the knowledge representation 
system. 
6 Conc lus ion  
By showing incremental addition of domain spe- 
cification within the ILEX system, we have demon- 
strated that it is a system which can function with 
varying degrees of information. This allows domain 
developers to rapidly prototype a working system, 
after which they can concentrate on improving the 
quality of text in the directions they favour. 
Re ferences  
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
9uage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
259 
Proceedings of the Fourth International Natural Language Generation Conference, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Individuality and Alignment in Generated Dialogues
Amy Isard and Carsten Brockmann and Jon Oberlander
School of Informatics, University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
{Amy.Isard, Carsten.Brockmann, J.Oberlander}@ed.ac.uk
Abstract
It would be useful to enable dialogue
agents to project, through linguistic
means, their individuality or personality.
Equally, each member of a pair of agents
ought to adjust its language (to a greater or
lesser extent) to match that of its interlocu-
tor. We describe CRAG, which generates
dialogues between pairs of agents, who are
linguistically distinguishable, but able to
align. CRAG-2 makes use of OPENCCG
and an over-generation and ranking ap-
proach, guided by a set of language mod-
els covering both personality and align-
ment. We illustrate with examples of out-
put, and briefly note results from user stud-
ies with the earlier CRAG-1, indicating
how CRAG-2 will be further evaluated.
Related work is discussed, along with cur-
rent limitations and future directions.
1 Introduction
A computer agent should be individual. Nass
and collaborators find that users? responses
to computer-agents are influenced by whether
the agent?s linguistic personality matches?or
mismatches?the personality of the user (Moon
and Nass, 1996; Nass and Lee, 2000). Similarly,
characters in virtual environments should be dis-
tinctive (Ball and Breese, 2000; Rist et al, 2003).
But an aspect of personality is how well you adjust
to other people (and their language use): align-
ment. Pickering and Garrod?s Interactive Align-
ment Model suggests that people tend to automat-
ically converge on lexical and syntactic choices,
via a low-level mechanism of interpersonal prim-
ing (Pickering and Garrod, 2004), and Brennan
has shown that people will align their language to-
wards that of computer agents (Brennan, 1996).
But it is an open issue as to whether some peo-
ple are better ?aligners? than others. Conversely,
alignment is only visible and interesting (among
computer agents) if they start out being individual.
We therefore set out to simulate both individ-
uality and alignment. The paper briefly surveys
the evidence for linguistic personality, for inter-
personal alignment, and for interaction between
them. It then sketches the current version of
CRAG. CRAG-2 makes use of OPENCCG and
an over-generation and ranking approach, guided
by a set of language models for personality and
alignment. We illustrate the differing linguis-
tic behaviours that it generates, and briefly note
promising results from user studies with the ear-
lier CRAG-1 system, indicating how CRAG-2 will
be further evaluated. Related work is discussed,
along with possible directions for future work.
2 Background
2.1 Personality and Language
Current work on personality traits is dominated by
Costa and McCrae?s five-factor model (Costa and
McCrae, 1992). The five factors, or dimensions,
are: Extraversion; Neuroticism; Openness; Agree-
ableness; and Conscientiousness (Matthews et al,
2003). It has been shown that scores on these di-
mensions correlate with some aspects of language
use (Scherer, 1979; Dewaele and Furnham, 1999).
In studies of text, the focus has been on lexical
choice, and Pennebaker and colleagues have anal-
ysed relative frequencies of use of word-stems in
a dictionary structured into semantic and syntac-
tic categories (Pennebaker et al, 2001). Amongst
other results, they have shown that High Extraverts
25
use: more social process talk, positive emotion
words and inclusives; and fewer negations, ten-
tative words, exclusives, causation words, nega-
tive emotion words, and articles (Pennebaker and
King, 1999; Pennebaker et al, 2002).
Computational linguistic exploitation of such
empirically-derived features has been limited. On
the one hand, in generation, there has been work
on personality-based generation. For instance, in
developing embodied conversational agents, re-
searchers have designed agents or teams of agents
with distinguishable linguistic personalities (Ball
and Breese, 2000; Rist et al, 2003; Piwek and
van Deemter, 2003; Gebhard, 2005). However,
the linguistic behaviour is usually informed by
rules based on personality stereotypes, rather than
on language statistics themselves. On the other
hand, in interpretation, more empirical work has
recently been carried out, to enable text classifi-
cation. Argamon et al (2005) attempted to clas-
sify authors as High or Low Extravert and High
or Low Neurotic, using Pennebaker and King?s
(1999) data. They report classification accuracies
of around 58% (with a 50% baseline). Oberlander
and Nowson (2006) undertake a comparable task,
using weblog data. They report classification ac-
curacies of roughly 85% (Neuroticism) and 94%
(Extraversion), and comparable figures for Agree-
ableness and Conscientiousness. Such studies can
provide ordered lists of linguistic features which
are useful for distinguishing language producers,
and we will return to this, below.
2.2 Alignment and Language
People converge with their interlocutors in linguis-
tic choices at a number of levels (Pickering and
Garrod, 2004). The phenomena can be seen in
both social and cognitive terms. On the social side,
co-operative processes such as audience design
are usually considered to be conscious, at least in
part (Bell, 1984). But on the cognitive side, co-
ordinative processes such as alignment are usu-
ally considered to be largely automatic (Garrod
and Doherty, 1994). Alignment can be probed
by psycholinguistic tests for interpersonal prim-
ing, establishing the extent to which participants
are more likely to use a lexical item or syntac-
tic construction after hearing their conversational
partner use it. Syntactic priming experiments in-
volve constructions such as passives, and ditransi-
tives (Pickering and Branigan, 1998).
It is possible that some people are stronger
aligners than others. Gill et al (2004) probed
syntactic priming for passives, and investigated
whether levels of Extraversion or Neuroticism
would affect the strength of priming effects. It
was found that Extraversion has no effect, but that
Neuroticism has a non-linear effect: both High and
Low levels of Neuroticism led to weaker priming;
Mid levels led to significantly stronger priming.
Given this, if a generation system is going to simu-
late alignment, it is probably worth designing it so
that it can simulate agents with differing propensi-
ties to align.
3 The CRAG System Overview
The system described in the following sections
(CRAG-2) is the successor to CRAG-1 which is
detailed in Isard et al (2005). The system gener-
ates a dialogue between two computer agents on
the subject of opinions about a film. CRAG-2 uses
the OPENCCG parsing and generation framework
(White, 2004; White, 2006). The realiser com-
ponent takes a logical form as input and outputs
a list of candidate sentences ranked using one or
more language models. In CRAG-2, we use the
OPENCCG generator to massively over-generate
paraphrases, and the combination of n-gram mod-
els described in Section 4 to choose the best ut-
terance according to a character?s personality and
agenda, and the dialogue history.
4 N-Grams: Personality and Alignment
Modelling
4.1 N-Gram Language Models
The basic assumption underlying CRAG-2 is that
personality, as well as alignment behaviour, can
be modelled by the combination of a variety of n-
gram language models.
Language models are trained on a corpus and
subsequently used to compute probability scores
of word sequences. An n-gram language model
approximates the probability of a word given its
history of the preceding n? 1 words. According
to the chain rule, probabilities are then combined
by multiplication. Equation (1) shows a trigram
model that takes into account two words of context
to predict the probability of a word sequence wn1:
(1) P(wn1)?
n
?
i=1
P(wi|wi?1i?2)
26
4.2 Avoiding the Length Effect
Because word probabilities are always less than 1
and therefore each multiplication decreases the to-
tal, if we use this standard model, longer sentences
will always receive lower scores (this is known as
the length effect). We therefore calculate the prob-
ability of a sentence as the geometric mean of the
probability of each word in the sentence as shown
in (2):
(2) P(wn1)?
n
?
i=1
P(wi|wi?1i?2)
1/n
4.3 Linear Combination of Language Models
OPENCCG supports the linear combination of
language models, where each model is assigned a
weight. For uniform interpolation of two language
models Pa and Pb, each receives equal weight:
(3) P(wi|wi?1i?2) =
Pa(wi|wi?1i?2)+Pb(wi|w
i?1
i?2)
2
In the more general case, the language models
are assigned weights ?i, the sum of which has to
be 1:
(4) P(wi|wi?1i?2) = ?1Pa(wi|wi?1i?2)+?2Pb(wi|wi?1i?2)
For example, setting ?1 = 0.9 and ?2 = 0.1 assigns
a high weight to the first language model.
4.4 OPENCCG N-Gram Ranking
In the OPENCCG framework, language models
can be used to influence the chart-based realisation
process. The agenda of edges is re-sorted accord-
ing to the score an edge receives with respect to a
language model. For CRAG-2, many paraphrases
are generated from a given logical form, and they
are then ranked in order of probability according
to the combination of n-gram models appropriate
for the character and stage of the dialogue.
5 CRAG-2 Personality and Alignment
Models
We use the SRILM toolkit (Stolcke, 2002) to com-
pute our language models. All models (except
for the cache language model described in Sec-
tion 5.4) are trigram models with backoff to bi-
grams and unigrams.
We have experimented with two strategies for
creating personality models. Since we want to
study the effects of alignment as well as person-
ality, it is essential that the two characters in a di-
alogue be distinct from one another, so that the ef-
fects of alignment can be seen. The first strategy
involves using typical language for each personal-
ity trait, and the second uses the language of one
individual. In both cases, the language models de-
scribed in the following sections are combined as
described in Section 5.5.
5.1 Building a Personality
Nowson (2006) performed a study on language
use in weblogs. The weblog authors were asked to
complete personality questionnaires based on the
five-factor model (see Section 2.1). All weblog au-
thors scored High or Medium on the Openness di-
mension, so we have no data for typical Low Open
language.
We divided the data into High, Medium and
Low for each personality dimension, and trained
language models so that we would be able to as-
sess the probability of a word sequence given a
personality type. This means that each individual
weblog is used 5 times, once for each dimension.
For each personality dimension, the system sim-
plifies a character?s personality setting x by assign-
ing a value of High (x > 70), Medium (30 < x ?
70) or Low (x ? 30). The five models correspond-
ing to the character?s assigned personality are uni-
formly interpolated to give the final personality
model. If the character has been given a low Open-
ness score, since we do not have a model for this
personality type, we simply interpolate the other
four models.
5.2 Borrowing a Personality
Our second strategy was to train n-gram models
on language of the individuals from the CRAG-1
corpus (Isard et al, 2005) and to use one of these
models for each character in the dialogue.
5.3 Base Language Model
In the case of building a personality, a base lan-
guage model is obtained by combining a language
model computed from the corpus collected for the
CRAG-1 system and a general language model
based on data from the Switchboard corpus (Stol-
cke et al, 2000). The combined base model alone
would rank the utterances without any bias for per-
sonality or alignment. When we are borrowing a
personality, the base model is calculated from the
Switchboard corpus alone.
27
5.4 Cache Language Model
We simulate alignment by computing a cache lan-
guage model based on the utterance that was gen-
erated immediately before. This dialogue history
cache model is the uniform interpolation of word-
and class-based n-gram models, where classes act
as a backoff mechanism when there is no exact
word match. Classes group together lexical items
with similar semantic properties, e.g.:
? good, bad: quality-adjective
? loved, hated: opinion-verb
Details of this approach can be found in Brock-
mann et al (2005).
5.5 Combining the Language Models
The system uses weights to combine all the mod-
els described above. First the base and person-
ality models are interpolated to produce a base-
personality model, and finally the cache model is
introduced to add alignment effects.
6 Dialogue and Utterance Specifications
6.1 Character Specification
Two computer characters are parameterised for
their personality by specifying values (on a scale
from 0 to 100) for the five dimensions: Extraver-
sion (E), Neuroticism (N), Openness (O), Agree-
ableness (A), and Conscientiousness (C). Their
alignment behaviour is set to a value between 0
(low propensity to align) and 1 (high propensity
to align). Also, each character receives an agenda
of topics they wish to discuss, along with polari-
ties (positive/negative) that indicate their opinion
on the respective topic.
6.2 Utterance Design
The character with the higher Extraversion score
begins the dialogue, and their first topic is se-
lected. Once an utterance has been generated, the
other character is selected, and the system applies
the algorithm shown in (5) to decide which topic
should come next. This process continues until
there are no topics left on the agenda of the cur-
rent speaker.
(5) if (A < 46) or (C < 46) or
(no. of utts about this topic = 2)
then take next topic from own agenda
else continue on same topic
The system creates a simple XML representa-
tion of the character?s utterance, using the speci-
fied topic and polarity. An example using the topic
music and polarity negative is shown in Figure 1.
At this point the system also decides which dis-
course connectives may be appropriate, based on
the previous topic and polarity.
<utterance>
<utt topic="music" polarity="dislike"
opp-polarity="like" so="no" right="no"
also="no" well="yes" and="no" but="no">
<pred adj="bad"/>
<opp-pred adj="good"/>
</utt>
</utterance>
Figure 1: Simple Utterance Specification
6.3 OPENCCG Logical Forms
Following the method described in Foster and
White (2004), the basic utterance specification is
transformed, using stylesheets written in the XSL
transformation language, into an OPENCCG log-
ical form. We make use of the facility for defin-
ing optional and alternative inputs and underspec-
ified semantics to massively over-generate candi-
date utterances. A fragment of the logical form
which results from the transformation of Figure 1
is shown in Figure 2. We also include some frag-
ments of canned text from the CRAG corpus in our
OPENCCG lexicon.
We also add optional interjections (i mean, you
know, sort of ) and conversational markers (right,
but, and, well) where appropriate given the dis-
course history.
When the full logical form is processed by the
OPENCCG system, the output consists of sen-
tences of the types shown below:
(I think) the music was bad.
(I think) the music was not (wasn?t)
good.
I did not (didn?t) like the music.
I hated the music.
One thing I did not (didn?t) like was the
music.
One thing I hated was the music.
The fragmentary logical form in Figure 2 would
create all possible paraphrases from:
(well) (you know) I (kind of) [liked/loved] the
[music/score]
By using synonyms (e.g., plot=story, com-
edy=humour) and combining the sentence types
28
<node id="l1:opinion" pred="like" tense="past">
<rel name="Speaker">
<node id="p1:person" pred="pro1" num="sg"/>
</rel>
<rel name="Content">
<node id="f1:cragtopic" pred="music"
det="the" num="sg"/>
</rel>
<opt>
<rel name="Modifier">
<node id="w1:adv" pred="well"/>
</rel>
<opt>
<opt>
<rel name="HasProp">
<node id="a2:proposition" pred="kind-of"/>
</rel>
</opt>
<opt>
<rel name="Modifier">
<node id="a1:adv" pred="you-know"/>
</rel>
</opt>
</node>
Figure 2: Fragment of Logical Form
Stan: E:53 N:48 A:57 C:46 O:65
agenda: film(neg), dialogue(neg),
music(pos)
other opinions: plot(neg), comedy(neg)
Eddie: E:51 N:43 A:57 C:41 O:65
agenda: plot(neg), comedy(neg),
dialogue(neg)
other opinions: music(pos), film(neg)
Figure 3: Stan and Eddie
and optional expressions, we create up to 3000
possibilities per utterance, and the best candidate
is chosen by the specific combination of n-gram
models appropriate for the given personality and
dialogue history, as described in Section 4.
Our OPENCCG lexicon is based on the core
English lexicon included with the system and we
have added vocabulary appropriate to the movie
domain, and extended the range of grammatical
constructions where necessary.
7 Output and Evaluation
7.1 Output
In this section, we provide some example out-
puts from the CRAG-2 system, using characters
based on participants from our corpus (see Sec-
tion 5.2). Stan is higher on the Extraversion, Neu-
roticism, and Conscientiousness scales than Ed-
die. The characters? personalities and agendas are
summarised in Figure 3.
We show three example dialogues between Stan
and Eddie. In the first (Figure 4) neither charac-
ter aligns with the other at all, while in the sec-
ond (Figure 5) Stan has a slight tendency towards
alignment and in the third (Figure 6) a more pro-
nounced tendency. In system terms, this means
that in the first dialogue the cache model was given
weight 0, while in the second and third the cache
model was given weights 0.05 and 0.1 respectively
for Stan?s utterances. It can be seen that in the
first dialogue, Eddie?s use of humour in utterance
4 is followed by Stan?s comedy, and dialogue in
utterance 6 is followed by script. In the second
dialogue, Stan aligns his first lexical choice with
Eddie?s but not his second, while in the third di-
alogue he aligns both. The syntactic structures of
Stan?s utterances in the second and third dialogue
also mirror Eddie?s progressively more closely.
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: Yeah there were so many little pathways that they
didn?t take you know.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: Yeah you know the comedy was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the script was a bit dull.
Figure 4: Zero Alignment
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: Yeah there were so many little pathways that they
didn?t take you know.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: I mean the humour was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the script was a bit dull.
Figure 5: Little Alignment from Stan
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: I mean the story was a bit dull.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: I mean the humour was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the dialogue was a bit dull.
Figure 6: More Alignment from Stan
To further illustrate the differences between the
dialogues with and without alignment, we provide
some utterance rankings. We show candidates
for the fifth utterance in each dialogue. Table 1
shows sentences from the example generated with-
out alignment, corresponding to utterance 5 (Stan)
29
1 .03317 Yeah you know the comedy was a
bit dull.
3 .03210 Yeah you know the humour was a bit
dull.
6 .03083 Yeah to be honest I didn?t think that
the comedy was very good either.
15 .02938 I didn?t think much of the comedy
either.
24 .02861 I thought that the comedy was a bit
dull too you know.
Table 1: Ranked Sentences with Zero Alignment
1 .05384 I mean the humour was a bit dull.
8 .05239 The humour wasn?t really funny you
know.
15 .04748 I mean I didn?t think that the humour
was very good either.
19 .04518 I didn?t think much of the humour
either you know.
21 .04478 I thought the humour was a bit dull
too you know.
Table 2: Ranked Sentences with Little Alignment
from Stan
from Figure 4. We show the first five occurrences
of different sentence structures (see Section 6.3),
with their rank and their geometric mean adjusted
scores.
Table 2 shows the the top five sentences from
the fifth utterance from Figure 5 (little alignment),
and Table 3 those from Figure 6 (more align-
ment). It can be seen that when more alignment
is present, the syntactic structure used by the pre-
vious speaker rises higher in the rankings.
7.2 Evaluation
We have not evaluated CRAG-2. However, we
have evaluated CRAG-1. The method was to gen-
erate a set of dialogues, systematically contrasting
characters with extreme settings for the personal-
ity dimensions (High/Low Extraversion, Neuroti-
cism, and Psychoticism1).
1CRAG-1 used the simpler PEN three factor personality
model.
1 .07081 I mean the humour was a bit dull.
2 .06432 The humour wasn?t really funny you
know.
15 .05516 I mean I didn?t think that the humour
was really funny either.
27 .05000 I thought the humour was a bit dull
too you know.
36 .04884 I mean I didn?t think much of the hu-
mour either.
Table 3: Ranked Sentences with More Alignment
from Stan
Human subjects were asked to fill in a question-
naire to determine their personality. They were
then given a selection of dialogues to read. After
each dialogue, they were asked to rate their per-
ception of the interaction and of the characters in-
volved by assigning scores to a number of adjec-
tives related to the personality dimensions.
It was found that subjects could recognise dif-
ferences in the Extraversion level of the language.
Also, the personality setting of a character influ-
enced the perception of its and its dialogue part-
ner?s personality (Kahn, 2006).
We plan a similar evaluation for CRAG-2 to be
able to compare human raters? impressions of di-
alogues generated by the two systems. We also
plan to evaluate CRAG-2 internally by varying the
weight given to the underlying language models,
and observing the effects this has on the resulting
ranking of the generated utterances.
8 Related Work
Related work in NLG involves either personality
or alignment. So far as we can tell, there is little
work on the latter. Varges (2005) suggests that ?a
word similarity-based ranker could align the gen-
eration output (i.e. the highest-ranked candidate)
with previous utterances in the discourse context?,
but there is no report yet on an implementation of
this proposal. A rather different approach is sug-
gested by Bateman and Paris (2005), who discuss
initial work on alignment, mediated by a process
of register-recognition. Regarding generation with
personality, the most influential work is probably
Hovy?s PAULINE system, which varies both con-
tent selection and realisation according to an indi-
vidual speaker?s goals and attitudes (Hovy, 1990).
In her extremely useful survey of work on affective
(particularly, emotional) natural language gener-
ation, Belz (2003) notes that the complexity of
PAULINE?s rule system means that numerous rule
interactions can lead to unpredictable side effects.
In response, Paiva and Evans (2004) take a more
empirical line on style generation, which is closer
to that pursued here. Other relevant work includes
Loyall and Bates (1997), who explicitly propose
that personality and emotion could be used in
generation, but Belz observes that technical de-
scriptions of Hap and the Oz project suggest that
the proposals were not implemented. Walker et
al.?s (1997) system produces linguistic behaviour
which is much more varied than our current sys-
30
tem is capable of; but there, variation is driven by
a model of social relations (based on Brown and
Levinson), rather than on personality. The NECA
project subsequently developed methods for gen-
erating scripts for pairs of dialogue agents (Piwek
and van Deemter, 2003), supported by the MIAU
platform (Rist et al, 2003). The VIRTUALHU-
MAN project is a logical successor to this work,
and its ALMA platform provides an integrated ap-
proach to affective generation, covering emotion,
mood and personality (Gebhard, 2005).
9 Conclusion and Next Steps
Our current system takes a much coarser-grained
approach to semantics and discourse goals than
the recent projects described above, in order to
take advantage of empirically-derived relations
between language and personality. It should be
feasible in principle to move to a more sophisti-
cated semantics, but still retain the massive over-
generation and ranking method. However, to
support more perceptible variation, we need to
exploit much larger personality-corpus resources
than have been available up to now, and our cur-
rent priority is to obtain a corpus at least an order
of magnitude larger than what is currently avail-
able. This interest in individual differences and
what corpora can (and cannot) tell us about them
is one we share with Reiter and colleagues (Reiter
and Sripada, 2004).
We also plan to integrate techniques from
CRAG-1 and CRAG-2, by passing the ranked out-
put of CRAG-2 through further processing and
ranking stages. Furthermore, we intend to inves-
tigate longer-ranging alignment processes, taking
into account more than one previous utterance,
with reduced weight by distance, to emulate mem-
ory effects.
With these enhancements, we will take further
steps towards our goal of simulating both individu-
ality and alignment in believable computer agents.
10 Acknowledgements
This research has been funded by Scottish Enter-
prise through the Edinburgh-Stanford Link project
?Critical Agent Dialogue? (CRAG). We would
like to thank Michael White and Scott Nowson for
their assistance and our anonymous reviewers for
their helpful comments.
References
Shlomo Argamon, Sushant Dhawle, Moshe Koppel,
and James W. Pennebaker. 2005. Lexical predic-
tors of personality type. In Proceedings of the 2005
Joint Annual Meeting of the Interface and the Clas-
sification Society of North America.
Gene Ball and Jack Breese. 2000. Emotion and per-
sonality in a conversational agent. In J. Cassell,
J. Sullivan, S. Prevost, and E. Churchill, editors, Em-
bodied Conversational Agents, pages 189?219. MIT
Press, Cambridge, MA, USA.
John A. Bateman and Ce?cile L. Paris. 2005. Adap-
tation to affective factors: architectural impacts for
natural language generation and dialogue. In Pro-
ceedings of the Workshop on Adapting the Interac-
tion Style to Affective Factors at the 10th Interna-
tional Conference on User Modeling (UM-05), Ed-
inburgh, UK.
Allan Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
Anja Belz. 2003. And now with feeling: Develop-
ments in emotional language generation. Techni-
cal Report ITRI-03-21, Information Technology Re-
search Institute, University of Brighton, Brighton.
Susan E. Brennan. 1996. Lexical entrainment in spon-
taneous dialog. In Proceedings of the 1996 Inter-
national Symposium on Spoken Dialogue (ISSD-96),
pages 41?44, Philadelphia, PA.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proceedings of the Workshop on
Adapting the Interaction Style to Affective Factors at
the 10th International Conference on User Modeling
(UM-05), Edinburgh, UK.
Paul T. Costa and Robert R. McCrae, 1992. Re-
vised NEO Personality Inventory (NEO-PI-R) and
NEO Five-Factor Inventory (NEO-FFI): Profes-
sional Manual. Odessa, FL: Psychological Assess-
ment Resources.
Jean-Marc Dewaele and Adrian Furnham. 1999. Ex-
traversion: The unloved variable in applied linguis-
tic research. Language Learning, 49:509?544.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. of the
4th NLPXML Workshop.
Simon Garrod and Gwyneth Doherty. 1994. Conver-
sation, co-ordination and convention: an empirical
investigation of how groups establish linguistic con-
ventions. Cognition, 53(3):181?215.
Patrick Gebhard. 2005. Alma: a layered model of af-
fect. In AAMAS ?05: Proceedings of the Fourth In-
ternational Joint Conference on Autonomous Agents
and Multiagent Systems, pages 29?36, New York,
NY, USA. ACM Press.
31
Alastair J. Gill, Annabel J. Harrison, and Jon Ober-
lander. 2004. Interpersonality: Individual differ-
ences and interpersonal priming. In Proceedings of
the 26th Annual Conference of the Cognitive Science
Society, pages 464?469.
Eduard Hovy. 1990. Pragmatics and natural language
generation. Artificial Intelligence, 43.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2005. Re-creating dialogues from a corpus. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation at Corpus Linguistics
2005 (CL-05), pages 7?12, Birmingham, UK.
Adam S. Kahn. 2006. Master?s thesis, Stanford Uni-
versity.
A. Bryan Loyall and Joseph Bates. 1997. Personality-
rich believable agents that use language. In J. Lewis
and B. Hayes-Roth, editors, Proceedings of the
1st International Conference on Autonomous Agents
(Agents?97). ACM Press.
Gerald Matthews, Ian J. Deary, and Martha C. White-
man. 2003. Personality Traits. Cambridge Univer-
sity Press, Cambridge, 2nd edition.
Youngme Moon and Clifford Nass. 1996. How ?real?
are computer personalities? Communication Re-
search, 23:651?674.
Clifford Nass and Kwan Min Lee. 2000. Does
computer-generated speech manifest personality?
an experimental test of similarity-attraction. In
Proceedings of CHI 2000, The Hague, Amsterdam,
2000, pages 329?336.
Scott Nowson. 2006. The Language of Weblogs: A
study of genre and individual differences. Ph.D. the-
sis, University of Edinburgh.
Jon Oberlander and Scott Nowson. 2006. Whose
thumb is it anyway? Classifying author personality
from weblog text. In Proceedings of COLING/ACL-
06: 44th Annual Meeting of the Association for
Computational Linguistics and 21st International
Conference on Computational Linguistics, Sydney.
Daniel S. Paiva and Roger Evans. 2004. A framework
for stylistically controlled generation. In Proceed-
ings of the 3rd International Conference on Natural
Language Generation, pages 120?129.
James W. Pennebaker and Laura King. 1999. Lin-
guistic styles: Language use as an individual differ-
ence. Journal of Personality and Social Psychology,
77:1296?1312.
James W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count
2001. Lawrence Erlbaum Associates, Mahwah, NJ.
James W. Pennebaker, Matthias R. Mehl, and Kate G.
Neiderhoffer. 2002. Psychological aspects of nat-
ural language use: Our words, our selves. Annual
Review of Psychology, 54:547?577.
Martin J. Pickering and Holly P. Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633?651.
Martin J. Pickering and Simon Garrod. 2004. Towards
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27:169?225.
Paul Piwek and Kees van Deemter. 2003. Dialogue as
discourse: Controlling global properties of scripted
dialogue. In Proceedings of the AAAI Spring Sym-
posium on Natural Language Generation in Spoken
and Written Dialogue.
Ehud Reiter and Somayajulu Sripada. 2004. Contex-
tual influences on near-synonym choice. In Pro-
ceedings of the Third International Conference on
Natural Language Generation, pages 161?170.
Thomas Rist, Elisabeth Andre?, and Stephan Baldes.
2003. A flexible platform for building applications
with life-like characters. In IUI ?03: Proceedings of
the 8th International Conference on Intelligent User
Interfaces, pages 158?165, New York, NY, USA.
ACM Press.
Klaus Scherer. 1979. Personality markers in speech.
In K. R. Scherer and H. Giles, editors, Social Mark-
ers in Speech, pages 147?209. Cambridge Univer-
sity Press, Cambridge.
Andreas Stolcke, Harry Bratt, John Butzberger, Hora-
cio Franco, Venkata Ramana Rao Gadde, Madelaine
Plauche?, Colleen Richey, Elizabeth Shriberg, Kemal
So?nmez, Fuliang Weng, and Jing Zheng. 2000. The
SRI March 2000 Hub-5 conversational speech tran-
scription system. In Proceedings of the 2000 Speech
Transcription Workshop, College Park, MD.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
International Conference on Spoken Language Pro-
cessing (ICSLP-02), pages 901?904, Denver, CO.
Sebastian Varges. 2005. Spatial descriptions as refer-
ring expressions in the MapTask domain. In Pro-
ceedings of the 10th European Workshop on Natural
Language Generation.
Marilyn A. Walker, Janet E. Cahn, and Steve J. Whit-
taker. 1997. Improvising linguistic style: So-
cial and affective bases for agent personality. In
J. Lewis and B. Hayes-Roth, editors, Proceedings
of the 1st International Conference on Autonomous
Agents (Agents?97), pages 96?105. ACM Press.
Michael White. 2004. Reining in CCG Chart Re-
alization. In Proceedings of the 3rd International
Conference on Natural Language Generation, pages
182?191.
Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language & Computation, on-
line first, March.
32
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 507?516,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Feature-Based Selection of Dependency Paths
in Ad Hoc Information Retrieval
K. Tamsin Maxwell
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB, UK
t.maxwell@ed.ac.uk
Jon Oberlander
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB, UK
j.oberlander@ed.ac.uk
W. Bruce Croft
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
croft@cs.umass.edu
Abstract
Techniques that compare short text seg-
ments using dependency paths (or simply,
paths) appear in a wide range of automated
language processing applications including
question answering (QA). However, few
models in ad hoc information retrieval (IR)
use paths for document ranking due to
the prohibitive cost of parsing a retrieval
collection. In this paper, we introduce a
flexible notion of paths that describe chains
of words on a dependency path. These
chains, or catenae, are readily applied in
standard IR models. Informative catenae
are selected using supervised machine
learning with linguistically informed fea-
tures and compared to both non-linguistic
terms and catenae selected heuristically
with filters derived from work on paths.
Automatically selected catenae of 1-2
words deliver significant performance
gains on three TREC collections.
1 Introduction
In the past decade, an increasing number of
techniques have used complex and effective
syntactic and semantic features to determine the
similarity, entailment or alignment between short
texts. These approaches are motivated by the idea
that sentence meaning can be flexibly captured by
the syntactic and semantic relations between words,
and encoded in dependency parse tree fragments.
Dependency paths (or simply, paths) are compared
using techniques such as tree edit distance (Pun-
yakanok et al, 2004; Heilman and Smith, 2010),
relation probability (Gao et al, 2004) and parse tree
alignment (Wang et al, 2007; Park et al, 2011).
Much work on sentence similarity using
dependency paths focuses on question answering
(QA) where textual inference requires attention
to linguistic detail. Dependency-based techniques
can also be highly effective for ad hoc information
retrieval (IR) (Park et al, 2011). However, few
path-based methods have been explored for ad
hoc IR, largely because parsing large document
collections is computationally prohibitive.
In this paper, we explore a flexible application
of dependency paths that overcomes this difficulty.
We reduce paths to chains of words called catenae
(Osborne and Gro?, 2012) that capture salient
semantic content in an underspecified manner.
Catenae can be used as lexical units in a reformu-
lated query to explicitly indicate important word
relationships while retaining efficient and flexible
proximity matching. Crucially, this does not
require parsing documents. Moreover, catenae are
compatible with a variety of existing IR models.
We hypothesize that catenae identify most units
of salient knowledge in text. This is because
they are a condition for ellipsis, in which salient
knowledge can be successfully omitted from text
(Osborne and Gro?, 2012). To our knowledge, this
paper is the first time that catenae are proposed
as a means for term selection in IR, and where
ellipsis is considered as a means for identification
of semantic units.
We also extend previous work with development
of a linguistically informed, supervised machine
learning technique for selection of informative
catenae. Previous heuristic filters for dependency
paths (Lin and Pantel, 2001; Shen et al, 2005;
Cui et al, 2005) can exclude informative relations.
Alternatively, treating all paths as equally infor-
mative (Punyakanok et al, 2004; Park et al, 2011;
Moschitti, 2008) can generate noisy word relations
and is computationally intensive.
The challenge of path selection is that no
explicit information in text indicates which paths
are relevant. Consider the catenae captured by
heuristic filters for the TREC1 query, ?What
role does blood-alcohol level play in automobile
accident fatalities? (#358, Table 1). It may appear
obvious that the component words of ?role play?
1Text REtrieval Conference, see http://trec.nist.gov/
507
blood alcohol
level play
auto accident
accident fatal
role play
play fatal
blood alcohol play
play accident fatal
auto accident fatal
level play fatal
role play fatal
role level play
blood alcohol
level play
auto accident
accident fatal
role blood
alcohol level
play auto
blood alcohol
level play
auto accident
accident fatal
role play
play fatal
Catenae Sequential dependenceGovernor?ependent
Query: What role does blood-alcohol level play in automobile* accident fatalities*?    (*abbreviated to `auto', `fatal')
auto accident
accident fatal
play fatal
play accident fatal
auto accident fatal
Predicate?rgument
auto accident
accident fatal
auto accident fatal
level play fatal
role play fatal
Nominal end slots
Table 1: Catenae derived from dependency paths, as selected by heuristic methods. Selections are
compared to sequential bigrams that use no linguistic knowledge.
and ?level play? do not have an important semantic
relationship relative to the query, yet these catenae
are described by parent-child relations that are
commonly used to filter paths in text processing
applications. Alternative filters that avoid such
trivial word combinations also omit descriptions of
key entities such as ?blood alcohol?, and identify
longer catenae that may be overly restrictive.
These shortcomings suggest that an optimized
selection process may improve performance of
techniques that use dependency paths in ad hoc IR.
We identify three previously proposed selection
methods, and compare them on the task of catenae
selection for ad hoc IR. Selections are tested
using three TREC collections: Robust04, WT10G,
and GOV2. This provides a diverse platform for
experiments. We also develop a linguistically
informed machine learning technique for catenae
selection that captures both key aspects of heuristic
filters, and novel characteristics of catenae and
paths. The basic idea is that selection, or weighting,
of catenae can be improved by features that are
specific to paths, rather than generic for all terms.
Results show that our selection method is more
effective in identifying key catenae compared
to previously proposed filters. Integration of the
identified catenae in queries also improves IR ef-
fectiveness compared to a highly effective baseline
that uses sequential bigrams with no linguistic
knowledge. This model represents the obvious
alternative to catenae for term selection in IR.
The rest of this paper is organised as follows.
?2 reviews related work, ?3 describes catenae
and their linguistic motivation and ?4 describes
our selection method. ?5 evaluates classification
experiments using the supervised filter. ?6 presents
the results of experiments in ad hoc IR. Finally, ?7
concludes the paper.
2 Related work
Techniques that compare short text segments
using dependency paths are applied to a wide range
of automated language processing tasks, including
paraphrasing, summarization, entailment detection,
QA, machine translation and the evaluation of
word, phrase and sentence similarity. A generic
approach uses a matching function to compare a
dependency path between any two stemmed terms
x and y in a sentence A with any dependency path
between x and y in sentence B. The match score
for A and B is computed over all dependency
paths in A.
In QA this approach improves question repre-
sentation, answer selection and answer ranking
compared to methods that use bag-of-words
and ngram features (Surdeanu et al, 2011). For
example, Lin and Pantel (2001) present a method
to derive paraphrasing rules for QA using analysis
of paths that connect two nouns; Echihabi and
Marcu (2003) align all paths in questions with
trees for heuristically pruned answers; Cui et
al. (2005) score answers using a variation of the
IBM translation model 1; Wang et al (2007)
use quasi-synchronous translation to map all
parent-child paths in a question to any path in an
answer; and Moschitti (2008) explores syntactic
and semantic kernels for QA classification.
In ad hoc IR, most models of term dependence
use word co-occurrence and proximity (Song and
Croft, 1999; Metzler and Croft, 2005; Srikanth and
Srihari, 2002; van Rijsbergen, 1993). Syntactic
language models for IR are a significant departure
from this trend (Gao et al, 2004; Lee et al, 2006;
Cai et al, 2007; Maisonnasse et al, 2007) that
use dependency paths to address long-distance
dependencies and normalize spurious differences
in surface text. Paths are constrained in both
508
prd loc pmod loc pmod
Is    polio under control  in   China ?
X1 X2 X3 X4 X5 X6
poliopolio controlcontrolcontrol ChinaChinapolio control China
polio       under        control
control       in       China
polio       under        control       in        China 
loc pmod
loc pmod
loc pmod loc pmod
Catenae ?toplisted? Dependency paths
Figure 1: Catenae are an economical and intuitive
representation of dependency paths.
queries and documents to parent-child relations.
In contrast, (Park et al, 2011) present a quasi-
synchronous translation model for IR that does not
limit paths. This is based on the observation that
semantically related words have a variety of direct
and indirect relations. All of these models require
parsing of an entire document collection.
Techniques using dependency paths in both QA
and ad hoc IR show promising results, but there
is no clear understanding of which path constraints
result in the greatest IR effectiveness. We directly
compare selections of catenae as a simplified
representation of paths.
In addition, a vast number of methods have
been presented for term weighting and selection
in ad hoc IR. Our supervised selection extends the
successful method presented by Bendersky and
Croft (2008) for selection and weighting of query
noun phrases (NPs). It also extends work for deter-
mining the variability of governor-dependent pairs
(Song et al, 2008). In contrast to this work, we
apply linguistic features that are specific to catenae
and dependency paths, and select among units
containing more than two content-bearing words.
3 Catenae as semantic units
Catenae (Latin for ?chain?, singular catena) are
dependency-based syntactic units. This section
outlines their unique semantic properties.
A catena is defined on a dependency graph that
has lexical nodes (or words) linked by binary asym-
metrical relations called dependencies. Depen-
dencies hold between a governor and a dependent
and may be syntactic or semantic in nature (Nivre,
2005). A dependency graph is usually acyclic such
that each node has only one governor, and one root
node of the tree does not depend on any other node.
A catena is a word, or sequence of words that are
continuous with respect to a walk on a dependency
Is polio under control in China, and is polio under control in India?
Antecedent
First conjunct:Antecedent clause Second conjunct:Elliptical/target clause
Elided text Remnant
Figure 2: Ellipsis in a coordinated construct.
graph. For example, Fig. 1 shows a dependency
parse that generates 21 catenae in total: (using
i for Xi) 1, 2, 3, 4, 5, 6, 12, 23, 34, 45, 56, 123,
234, 345, 456, 1234, 2345, 3456, 12345, 23456,
123456. We process catenae to remove stop words
on the INQUERY stoplist (Allan et al, 2000) and
lexical units containing 18 TREC description stop
words such as ?describe?. This results in a reduced
set of catenae as shown in Fig. 1.
A dependency path is ordered and includes both
word tokens and the relations between them. In
contrast, a catena is a set of word types that may
be ordered or partially ordered. A catena is an
economical, intuitive lexical unit that corresponds
to a dependency path and is argued to play an
important role in syntax (Osborne et al, 2012).
In this paper, we explore catenae instead of paths
for ad hoc IR due to their suitability for efficient IR
models and flexible representation of language se-
mantics. Specifically, we note that catenae identify
words that can be omitted in elliptical constructions
(Osborne et al, 2012). They thus represent salient
semantic information in text. To clarify this insight,
we briefly review catenae in ellipsis.
3.1 Semantic units in ellipsis
Fig. 2 shows terminology for the phenomenon
of ellipsis. The omitted words are called elided
text, and words that could be omitted, but are not,
we call elliptical candidates.
Ellipsis relies on the logical structure of a
coordinated construction in which two or more
elements, such as sentences, are joined by a
conjunctive word or phrase such as ?and? or
?more than?. A coordinated structure is required
because the omitted words are ?filled in? by
assuming a parallel relation p between the first
and second conjunct. In ellipsis, p is omitted and
its arguments are retained in text. In order for
ellipsis to be successful and grammatically correct,
p must be salient shared knowledge at the time of
communication (Prince, 1986; Steedman, 1990). If
p is salient then the omitted text can be inferred. If
p is not salient then the omission of words merely
results in ungrammatical, or incoherent, sentences.
This framework is practically illustrated in Fig.
509
   Is polio under control in China, and ?is polio under control? in India ?   Is polio under control in China, and is cancer under observation ?in China? ?* Is polio under control in China, and ?is? cancer ?nder? observation ?in China? ?* Is polio under control in China, and ?is polio? under ?ontrol in? India ?
Caatentn ?optpslin ds?ip to tlsat?c lyih s?i ?liosi
a?                                         in India    ?b?   is cancer under observation        ?c? *    cancer            observation        ?d? *                under                 India   ?
Is polio under control in China, and...
Caatpip niolio?in
Figure 3: For ellipsis to be successful, elided words must be catenae. Ellipsis candidates are catenae2.
Is    polio under control  in   China ?
X1 X2
X3 X4
X5 X6
Figure 4: A parse in which ?polio China? is a
catena.
3 for the query, ?Is polio under control in China??.
Sentences marked by * are incoherent, and it is
evident that the omitted words do not form a salient
semantic unit. They also do not form catenae. In
contrast, the omitted words in successful ellipsis
do form catenae, and they represent informative
word combinations with respect to the query. This
observation leads us to an ellipsis hypothesis:
Ellipsis hypothesis: For queries formulated
into coordinated structures, the subset of
catenae that are elliptical candidates identify
the salient semantic units in the query.
3.2 Limitations of paths and catenae
The prediction of salient semantic units by cate-
nae is quite robust. However, there are two prob-
lems that can limit the effectiveness of any tech-
nique that uses catenae or dependency paths in IR.
1) Syntactic ambiguity: We make the simpli-
fying assumption that the most probable parse of
a query is accurate and sufficient for the extraction
of relevant catenae. However, this is not always
true. For example, the sentence ?Is polio under
control in China, and under observation ??
constitutes successful ellipsis. The elided words
?polio in china? are relevant to a base query, ?Is
polio under control in China??. Unfortunately,
in Fig. 1 the elided text does not qualify as a
catena. A parse with alternative prepositional
phrase attachment is shown in Fig. 4. Here, the
successfully elided text does qualify as a catena.
This highlights the fact that a single dependency
parse may only partially represent the ambiguous
semantics of a query. More accurate parsing does
not address this problem.
2) Rising: Automatic extraction of catenae is
limited by the phenomenon of rising. Let the
Is poolooiundeoer cdeltoolsooolooC lhua
X4X3X2
X1
X5
X6 X7
Standard structure
?ooiundeoer cdeltoIs poolsooolooC lhua
X3X2X1 X4g X5
X6 X7
Rising structure
Figure 5: A parse with and without rising. The
dashed dependency edge marks where a head is
not also the governor and the g-script marks the
governor of the risen catena.
governor of a catena be the word that licenses
it (in Fig. 5 ?used? licenses ?a toxic chemical?
e.g. ?used what??). Let the head of a catena be
its parent in a dependency tree. Rising occurs
when the head is not the same as the governor.
This is frequently seen with wh-fronting questions
that start who, what etc., as well as with many
other syntactic discontinuities (Osborne and Gro?,
2012). More specifically, rising occurs when a
catena is separated from its governor by words
that its governor does not dominate, or the catena
dominates the governor, as in Fig. 5. Note that
in the risen structure, the words for the catena
?chemical as a weapon? are discontinuous on the
surface, interrupted by the word ?used?.
4 Selection method for catenae
Catenae describe relatively few of the possible
word combinations in a sentence, but still include
many combinations that do not result in successful
ellipsis and are not informative for IR.
This section describes our supervised method
for selection of informative catenae. Candidate
catenae are identified using two constraints that
enable more efficient extraction: stopwords are
removed, and stopped catenae must contain fewer
than four words (single words are permitted). We
use a pseudo-projective joint dependency parse
and semantic role labelling system (Johansson and
510
Nugues, 2008) to generate the dependency parse.
This enables us to explore semantic classification
features and is highly accurate. However, any
dependency parser may be applied instead. For
comparison, catenae extracted from 500 queries
using the Stanford dependency parser (de Marneffe
et al, 2006) overlap with 77% of catenae extracted
from the same queries using the applied parser.
4.1 Feature Classes
Four feature classes are presented in Table 2:
Ellipsis candidates: The ellipsis hypothesis
suggests that informative catenae are elliptical
candidates. However, queries are not in the
coordinated structures required for ellipsis. To
enable extraction of characteristic features we (a)
construct a coordinated query by adding the query
to itself; and (b) elide catenae from the second
conjunct. For example, for the query, Is polio
under control in China? we have:
(a) Is polio under control in China, and is
polio under control in China?
(b) Is polio under control in China, and is
polio in China?
We refer to the words in (b) as the query remainder
and use this to identify features detailed in Table 2.
Dependency path features: Part-of-speech
tags and semantic roles have been used to filter
dependency paths. We identify several features that
use these characteristics from prior work (Table 2).
In addition, variability in the separation distance
in documents observed for words that have
governor-dependent relations in queries has been
proposed for identification of promising paths
(Song et al, 2008). We also observe that due to the
phenomenon of rising, words that form catenae can
be discontinuous in text, and the ability of catenae
to match similar word combinations is limited by
variability of how they appear in documents. Thus,
we propose features for separation distance, but use
efficient collection statistics rather than summing
statistics for every document in a collection.
Co-occurrence features: A governor w1 tends
to subcategorize for its dependents wn. This
means that w1 often determines the choice of wn.
We conclude that co-occurrence is an important
feature of dependency relations (Mel?c?uk, 2003).
In addition, term frequencies and inverse document
frequencies calculated using word co-occurrence
measures are commonly used in IR. We use
features previously proposed for filtering terms in
IR (Bendersky and Croft, 2008) with two methods
to normalize co-occurrence counts for catenae of
different lengths: a factor |c||c|, where |c| is the
number of words in catena c (Hagen et al, 2011),
and the average score for a feature type over all
pairwise word combinations in c.
IR performance predictors: Catenae take the
same form as typical IR search terms. For this
reason, we also use predictors of IR effectiveness
previously applied to IR terms.
In general, path and co-occurrence features are
similar to those applied by Surdeanu et al (2011)
but we do not parse documents. Path features
are also similar to Song et al (2008), but more
efficient and suited to units of variable length.
Ellipsis features have not been used before.
5 Experimental setup
5.1 Classification
Catenae selection is framed as a supervised
classification problem trained on binary human
judgments of informativeness: how well catenae
represent a query and discriminate between
relevant and non-relevant documents in a col-
lection. Kappa for two annotators on catenae
in 100 sample queries was 0.63, and test-retest
reliability for individual judges was similar (0.62)3.
Although this is low, human annotations produced
consistently better classification accuracy than
other labelling methods explored.
We use the Weka (Hall et al, 2009) Ad-
aBoost.M1 meta-classifier (Freund and Schapire,
1996) with unpruned C4.5 decision trees as base
learners to classify catenae as informative or
not. Adaboost.M1 boosts decisions over T weak
learners for T features using weighted majority
voting. At each round, predictions of a new learner
are focused on incorrectly classified examples
from the previous round. Adaboost.M1 was
selected in preference to other algorithms because
it performed better in preliminary experiments,
leverages many weak features to advantage, and
usually does not overfit (Schapire et al, 1997).
Predictions are made using 10-fold cross-
validation. There are roughly three times the
number of uninformative catenae compared to
informative catenae. In addition, the number of
training examples is small (1295 to 5163 per collec-
tion). To improve classifier accuracy, the training
data for each collection is supplemented and
balanced by generating examples from queries for
3Catenae, judgments and annotation details available at
ciir.cs.umass.edu/?tmaxwell
511
isSeq
Minimum perplexity of ngrams with length 2, 3, and 4 in a window of up to a 3 words around the site of catenae omission. This is the area where ungrammaticality may be introduced. For the remainder R=`ABCDE&ABE' we compute ppl1 for ?ABE, &AB, ABE, &A, AB, BE?
R_ppl1
R_strict
Compliance with strict hand?oded rules for grammaticality of a remainder. Rules include unlikely orderings of punctuation and part?f?speech ?OS? tags ?.g. ,, ?, poor placement of determiners and punctuation, and orphaned words, such as adjectives without the nouns they modify.
R_relax
A relaxed version of hand?oded rules for R_strict. Some rules were observed to be overly aggressive in detection of ungrammatical remainders.
Ellipsis candidate features (E)
Co-occurrence features (C)
IR performance prediction features (I)
c_ppl1
Dependency path features (D) (continued)
Dependency paths traverse nodes including stopwords and may be filtered based on POS tags. We use perplexity for the sequence of POS tags in catenae before removing stopwords. This is computed using a POS language model built on ukWaC parsed wikipedia data ?aroni et al, 2009?.
phClass
Phrasal class for a catena, with options NP, VP and Other. A catena has a NP or VP class if it is, or is entirely contained by, an NP or VP ?ong et al, 2008?.
NP_split
Unsuccessful ellipsis often results if elided words only partly describe a base NP. Boolean feature for presence of a partial NP in the remainder. NPs ?nd PPs? are identified using the MontyLingua toolkit.
PP_split As for NP_split, defined for prepositional phrases (PP). 
F_split As for NP_split, defined for finite clauses.
semRole
Boolean feature indicating whether a catena describes all, or part of, a predicate?rgument structure ?AS?. Previous work approximated PAS by using paths between head nouns and verbs, and all paths excluding those within base chunks.
c_len Length of a stopped catenae. Longer terms tend to reduce IR recall.
Boolean indicating if catena words are sequential in stoplisted surface text. 
cf_ow
Frequency of a catena in the retrieval collection, words appearing ordered in a window the length of the catena. 
cf_uw As for cf_ow, but words may appear unordered.
cf_uw8 As for cf_uw, but the window has a length of 8 words.
idf_ow
Inverse document frequency ?idf? where document 
frequency ?df? of a catena is calculated using cf_ow 
windows. Let N  be the number of documents in the retrieval collection, then:
                      idf(Ci) = log2
N
df(Ci)
and idf(Ci) = N  if df(Ci) = 0.
idf_uw As for idf_ow, but words may appear unordered.
idf_uw8 As for idf_uw, but the window has a length of 8 words.
gf
Google ngrams frequency ?rants and Franz, 2006? from a web crawl of approximately one trillion English word tokens. Counts from a large collection are expected to be more reliable than those from 
smaller test collections.
WIG
Normalized Weighted Information Gain ?WIG? is the change in information over top ranked documents between a random ranked list and an actual ranked list retrieved with a catena c ?hou and Croft, 2007?. 
    wig(c) =
1k
?
d?Dk(c) log p(c|d) ? log p(c|C)?log p(c|C)
where Dk are the top k=50 documents retrieved 
with catena c from collection C, and p(c|?) are maximum likelihood estimates. A second feature uses the average WIG score for all pairwise word combinations in c.
qf_in
Frequency of appearance in queries from the Live Search 2006 search query log ?pproximately 15 million queries?. Query log frequencies are a measure of the likelihood that a catena will appear in any query. 
wf_in As for qf_in, but using frequency counts in Wikipedia titles instead of queries.
sepMode
Most frequent separation distance of words in catena c in the retrieval collection, with possible 
values S = ?1, 2, 3, long?. 1 means that all words are 
adjacent, 2 means separation by 0-1 words, and long 
means containment in a window of size 4 ? |c|.
H_c
Entropy for separation distance s of words in catena 
c in the retrieval collection.fs is the frequency of c 
in window size s, and fS is the frequency of c in a 
window of size 4 ? |c| . All f are normalized for 
catena length using |c||c| ?agen et al, 2011?.
              Hc =
?
s?S
fs + 0.5fS + 0.5 log2
fs + 0.5fS + 0.5
sepRatio
Where fs and fS are defined as for H_c:
                        sepRatioc =
fs>2 + 0.5fS + 0.5
wRatio
For words w in catena c; fS is defined as for H_c.
                   wRatioc =
0.5 + 1|c|?w?c fw
fS + 0.5
nomEnd
Boolean indicating whether the words at each end of the catena are nouns ?r the catena is a single noun?.
Dependency path features (D)
Table 2: Classifier features.
512
Feature Classes
Pr
 
ROB04
WT10G
GOV2
 
D-CIE-CIE-DE-D-CI
R
86.2 72.8
79.3 67.1
77.0 68.0
Pr R
83.5 67.5
76.9 59.7
70.9 61.8
Pr R
86.2 71.7
77.2 65.6
72.8 63.9
Pr R
86.2 72.0
79.6 66.1
75.5 67.2
 
Table 3: Average classifier precision (Pr) and recall
(R) over 10 folds. Pr is % positive predictions
that are correct. R is % positive labeled instances
predicted as positive. A combination of all classes
marginally performs best.
other collections used in this paper, plus TREC8-
QA. For example, training data for Robust04
includes data from WT10G, GOV2 and TREC8-
QA. Any examples that replicate catenae in the test
collection are excluded. For Robust04, WT10G
and GOV2 respectively, 30%, 82% and 69% of the
training data is derived from other collections.
5.2 Classification results
Average classification precision and recall is
shown in Table 3. Co-occurrence and IR effective-
ness prediction features (CI) was the most influen-
tial class, and accounted for 70% of all features in
the model. Performance is marginally better using
all features (E-D-CI) with a moderate improvement
over human agreement on the annotation task. The
E-D-CI filter is used in subsequent experiments.
Catenae were predicted for all queries. Predic-
tions were more accurate for Robust04 than the
other two collections. One potential explanation
is that Robust04 queries are longer on average
(up to 32 content words per query, compared to
up to 16 words) so they generate a more diverse
set of catenae that are more easily distinguished
with respect to informativeness. The proportion
of training data specific to the retrieval collection
may also be a factor. Longer queries produce a
greater number of catenae, so less training data
from other collections is required.
6 Evaluation framework
6.1 Baseline IR models
Baselines are a unigram query likelihood (QL)
model (bag of words) and a highly effective
sequential dependence (SD) variant of the Markov
random field (MRF) model (Metzler and Croft,
2005). SD uses a linear combination of three
cliques of terms, where each clique is prioritized
by a weight ?c. The first clique contains individual
words (query likelihood QL), ?1 = 0.85. The
second clique contains query bigrams that match
document bigrams in 2-word ordered windows
(?#1?), ?2 = 0.1. The third clique uses the same
bigrams as clique 2 with an 8-word unordered
window (?#uw8?), ?3 = 0.05. For example, the
query new york city in Indri4 query language is:
#weight(
?1 #combine(new york city)
?2 #combine(#1(new york) #1(york city))
?3 #combine(#uw8(new york) #uw8(york city)))
SD is a competitive baseline in IR (Bendersky
and Croft, 2008; Park et al, 2011; Xue et al,
2010). Our reformulated model uses the same
query format as SD, but the second and third
cliques contain filtered catenae instead of query
bigrams. In addition, because catenae may be
multi-word units, we adjust the unordered window
size to 4 ? |c|. So, if two catenae ?york? and ?new
york city? are selected, the last clique has the form:
?3 #combine( york #uw12(new york city))
This query representation enables word relations
to be explicitly indicated while maintaining
efficient and flexible matching of catenae in
documents. Moreover, it does not use dependency
relations between words during retrieval, so there
is no need to parse a collection.
6.2 Baseline catenae selection
We explore four filters for catenae. Three are
based on previous work and describe heuristic
features of promising catenae. The fourth is our
novel supervised classifier.
NomEnd: Catenae starting and ending with
nouns, or containing only one word that is a noun.
Paths between nouns are used by Lin and Pantel
(2001).
SemRol: Catenae in which all component
words are either predicates or argument heads.
This is based on work that uses paths between head
nouns and verbs (Shen et al, 2005), semantic roles
(Moschitti, 2008), and all dependency paths except
those that occur between words in the same base
chunk (e.g. noun / verb phrase) (Cui et al, 2005).
GovDep: Cantenae containing words with a
governor-dependent relation. Many IR models
use this form of path filtering e.g. (Gao et al,
2004; Wang et al, 2007). Relations are ?collapsed?
by removing stopwords to reduce the distance
between content nodes in a dependency graph.
4http://www.lemurproject.org/
513
ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrQL 25.25 28.69 19.55 22.77 25.77 31.26SD 26.57? 30.02? 20.63 24.31? 28.00? 33.30?NomEnd 25.91? 29.35? 20.81? 24.27? 27.41? 32.94?GovDep 26.26? 29.63? 21.06 24.23? 27.87? 33.51?SemRol 25.70? 29.06 19.78 22.93 26.76 32.49?SFeat 27.04? 30.11? 20.84? 24.31? 28.43? 33.84?SF-12 27.03? 30.20? 21.62? 24.81? 28.57? 34.01?
Table 4: IR results using filtered catenae consistently improve over non-linguistic methods.
Significance(p < .05) shown compared to QL (?) and SD (?).
ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08
Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are more
effective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD).
6.3 Experiments
Experiments compare queries reformulated
using catenae selected by baseline filters and our
supervised selection method (SFeat) to SD and
a bag-of-words model (QL). We also compare IR
effectiveness of all catenae filtered using SFeat
with approaches that combine SFeat with baseline
filters. All models are implemented using the Indri
retrieval engine version 4.12.
6.4 Results
Results in Table 4 show significant improvement
in mean average precision (MAP) of queries using
catenae compared to QL. Consistent improvements
over SD are also demonstrated for supervised
selection applied to all catenae (SFeat) and catenae
with only 1-2 words (SF-12) across all collections
(Table 5). Overall, changes are small and fairly
robust, with one half to two thirds of all queries
showing less than 10% change in MAP.
Unlike sFeat, other filters tend to decrease per-
formance compared to SD. Governor-dependent
relations for WT10G are an exception and we spec-
ulate that this is due to a negative influence of 3-
word catenae for this collection. Manual inspection
suggests that WT10G queries are short and have
relatively simple syntactic structure (e.g. few PP
attachment ambiguities). This means that 3-word
catenae (in all models except GovDep) tend to in-
clude uninformative words, such as ?reasons? in
?fasting religious reasons?. In contrast, 3-word cate-
nae in other collections tend to identify query sub-
concepts or phrases, such as ?science plants water?.
Classification results for catenae separated by
length, such that the classifier for catenae with a
specific length are trained on examples of catenae
with the same length, confirm this intuition. The
rejection rate for 3-word catenae is twice as high
for WT10G as for other collections. It is also
more difficult to distinguish informative 3-word
catenae compared to catenae with 1-2 words. To
assess the impact of classification accuracy on IR
effectiveness, Table 5 shows results with oracle
knowledge of annotator judgments.
The SF-12 model combines catenae predicted for
lengths 1 and 2. Its strong performance across all
collections suggests that most of the benefit derived
from catenae in IR is found in governor-dependent
and single word units, where single words are
important (GovDep uses only 2-word catenae).
Another major observation (Table 5) is that mixing
baseline heuristic filters with a supervised ap-
proach is not as successful as supervised selection
alone. In particular, performance decreases for
filtered governor-dependent pairs. This suggests
that some important word relations in GovDep and
NomEnd are captured by triangulation.
Finally, we review selected catenae for queries
that perform significantly better or worse than SD
(> 75% change in MAP). The best IR effectiveness
occurs when selected catenae clearly focus on the
most important aspect of a query. Poor perfor-
514
mance is caused by a lack of focus in a catenae set,
even though selected catenae are reasonable, or an
emphasis on words that are not central to the query.
The latter can occur when words that are not es-
sential to query semantics appear in many catenae
due to their position in the dependency graph.
7 Conclusion
We presented a flexible implementation of
dependency paths for long queries in ad hoc IR that
does not require dependency parsing a collection.
Our supervised selection technique for catenae
addresses the need to balance a representation of
language expressiveness with effective, efficient
statistical methods. This is a core challenge in
computational linguistics.
It is not possible to directly compare perfor-
mance of our approach with ad hoc techniques in
IR that parse a retrieval collection. However, we
note that a recent result using query translation
based on dependency paths (Park et al, 2011)
reports 14% improvement over query likelihood
(QL). Our approach achieves 7% improvement
over QL on the same collection. We conclude that
catenae do not replace path-based techniques, but
may offer some insight into their application, and
have particular value when it is not practical to
parse target documents to determine text similarity.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval. Any opinions,
findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
References
James Allan, Margaret E. Connell, W. Bruce Croft,
Fang-Fang Feng, David Fisher, and Xiaoyan Li.
2000. INQUERY and TREC-9. In Proceedings of
TREC-9, pages 551?562.
Michael Bendersky and W. Bruce Croft. 2008.
Discovering key concepts in verbose queries. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?08, pages 491?498,
New York, NY, USA. ACM.
Keke Cai, Jiajun Bu, Chun Chen, and Guang Qiu.
2007. A novel dependency language model for in-
formation retrieval. Journal of Zhejiang University
SCIENCE A, 8(6):871?882.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan,
and Tat-Seng Chua. 2005. Question answering
passage retrieval using dependency relations. In
Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?05, pages 400?407,
New York, NY, USA. ACM.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-2006.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering.
In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume
1, ACL ?03, pages 16?23, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In ICML?96,
pages 148?156.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and
Guihong Cao. 2004. Dependence language model
for information retrieval. In Proceedings of the 27th
annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?04, pages 170?177, New York, NY, USA.
ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation
revisited. In Proceedings of the 20th international
conference on World wide web, WWW ?11, pages
97?106, New York, NY, USA. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an up-
date. SIGKDD Explorations Newsletter, 11:10?18,
November.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments,
paraphrases, and answers to questions. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT
?10, pages 1011?1019, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of
CoNNL 2008, pages 183?187.
Changki Lee, Gary Geunbae Lee, and Myung-Gil
Jang. 2006. Dependency structure language model
for information retrieval. In In ETRI journal,
volume 28, pages 337?346.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings
of ACM Conference on Knowledge Discovery
and Data Mining (KDD-01), pages 323?328, San
Francisco, CA.
515
Lo??c Maisonnasse, Eric Gaussier, and Jean-Pierre
Chevallet. 2007. Revisiting the dependence
language model for information retrieval. In
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?07, pages 695?696,
New York, NY, USA. ACM.
Igor A. Mel?c?uk. 2003. Levels of dependency in
linguistic description: Concepts and problems. In
V. Agel, L. Eichinger, H.-W. Eroms, P. Hellwig,
H. J. Herringer, and H. Lobin, editors, Dependency
and Valency. An International Handbook of Contem-
porary Research, volume 1, pages 188?229. Walter
De Gruyter, Berlin?New York.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In
Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?05, pages 472?479,
New York, NY, USA. ACM.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceeding of the 17th ACM conference on
Information and knowledge management, CIKM
?08, pages 253?262, New York, NY, USA. ACM.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical report, Va?xjo? University:
School of Mathematics and Systems Engineering.
Timothy Osborne and Thomas Gro?. 2012. Con-
structions are catenae: Construction grammar
meets dependency grammar. Cognitive Linguistics,
23(1):165?216.
Timothy Osborne, Michael Putnam, and Gro?. 2012.
Catenae: Introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396, December.
Jae Hyun Park, W. Bruce Croft, and David A. Smith.
2011. A quasi-synchronous dependence model for
information retrieval. In Proceedings of the 20th
ACM international conference on Information and
knowledge management, CIKM ?11, pages 17?26,
New York, NY, USA. ACM.
Ellen F. Prince. 1986. On the syntactic marking of
presupposed open propositions. In Proceedings of
the 22nd Annual Meeting of the Chicago Linguistic
Society, pages 208?222.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question
answering. In Proceedings of AI and MATH
Symposium 2004 (Special session: Intelligent Text
Processing).
Robert E. Schapire, Yoav Freund, Peter Bartlett, and
Wee Sun Lee. 1997. Boosting the margin: A new
explanation for the effectiveness of voting methods.
In Proceedings of ICML, pages 322?330.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for
question answering. In Proceedings of the Second
international joint conference on Natural Language
Processing, IJCNLP?05, pages 507?518, Berlin,
Heidelberg. Springer-Verlag.
Fei Song and W. Bruce Croft. 1999. A general
language model for information retrieval. In Pro-
ceedings of the 8th ACM international conference
on Information and knowledge management, CIKM
?99, pages 316?321, New York, NY, USA. ACM.
Young-In Song, Kyoung-Soo Han, Sang-Bum Kim,
So-Young Park, and Hae-Chang Rim. 2008. A
novel retrieval approach reflecting variability of syn-
tactic phrase representation. Journal of Intelligent
Information Systems, 31(3):265?286, December.
Munirathnam Srikanth and Rohini Srihari. 2002.
Biterm language models for document retrieval. In
Proceedings of the 25th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?02, pages 425?426,
New York, NY, USA. ACM.
Mark J. Steedman. 1990. Gapping as Constituent Co-
ordination. Linguistics and Philosophy, 13(2):207?
263, April.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers
to non-factoid questions from web collections.
Computational Linguistics, 37(2):351?383, June.
C. J. van Rijsbergen. 1993. A theoretical basis for the
use of co-occurrence data in information retrieval.
Journal of Documentation, 33(2):106?119.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 22?32, Prague, Czech Republic, June.
Association for Computational Linguistics.
Xiaobing Xue, Samuel Huston, and W. Bruce Croft.
2010. Improving verbose queries using subset
distribution. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10, pages 1059?1068, New
York, NY, USA. ACM.
516
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 409?414,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Determiner-Established Deixis to Communicative Artifacts in Pedagogical Text 
Shomir Wilson1,2 and Jon Oberlander1 1School of Informatics, University of Edinburgh, United Kingdom 2School of Computer Science, Carnegie Mellon University, USA shomir@cs.cmu.edu, jon@inf.ed.ac.uk    Abstract 
Pedagogical materials frequently contain deixis to communicative artifacts such as textual structures (e.g., sections and lists), discourse entities, and illustrations. By relating such artifacts to the prose, deixis plays an essential role in structuring the flow of information in informative writing. However, existing language technologies have largely overlooked this mechanism. We examine properties of deixis to communicative artifacts using a corpus rich in determiner-established instances of the phenomenon (e.g., ?this section?, ?these equations?, ?those reasons?) from Wikibooks, a collection of learning texts. We use this corpus in combination with WordNet to determine a set of word senses that are characteristic of the phenomenon, showing its diversity and validating intuitions about its qualities. The results motivate further research to extract the connections encoded by such deixis, with the goals of enhancing tools to present pedagogical e-texts to readers and, more broadly, improving language technologies that rely on deictic phenomena. 1 Introduction Deixis often appears in written language as an anaphoric mechanism to refer to communicative entities in a document. Such deixis can have a variety of referent types. For example, consider that idea in Sentence (1), those names in (2), this section in (3), and these figures in (4): (1) That idea has been challenged by many.  (2) Those names are Welsh in origin. (3) In this section, we cover some early work. (4) Quantities in these figures are approximate. The kinds of deixis represented in (1) and (2) are similar to discourse deixis (Webber, 1991) and textual deixis (Lyons, 1977), respectively. Sentence (3) contains deixis to a structural element of a document (Paraboni and Deemter, 
2006), and (4) contains an example of deixis to illustrative items such as figures or examples. We collectively term such deictic acts as communicative deixis (CD for brevity), recognizing their shared characteristics, and we name their referents communicative artifacts (CAs). Prior studies have focused on narrow varieties of CD (such as those identified above), leaving unknown their properties when viewed together as a whole. Moreover, efforts to automatically identify or resolve CD have been piecemeal at best. Given the complexity of the referents, conventional tools for coreference or anaphora resolution are poorly applicable. This paper describes analysis of the first collection of instances of deixis in English targeted to refer to a broad variety of CAs. Texts from the website Wikibooks are used, for the intuitive density of CD in pedagogical material and the potential value of augmenting them with interpretive metadata. The diversity of referents in this corpus enables new inferences on the composition and relative frequencies of CD varieties in text. We focus on determiner-established instances, i.e., anaphoric noun phrases that begin with determiners this, that, these, or those (e.g., (1)-(4)). This focus has the advantage of collecting instances that explicitly identify the relevant capacities of their referents (e.g., (1) reifies its referent as an ?idea?). The remainder of this paper is structured as follows. Section 2 surveys related work on deixis to specific types of CAs. Section 3 describes the text source for this study and the procedure used to collect and label instances. Section 4 describes our use of WordNet to characterize CAs, resulting in an ontology of such referents and inter-annotator agreement results for labeling of artifact types. Finally, Section 5 provides some conclusions and directions for future work. 
409
2 Related Work  The value of CD in pedagogical contexts has been established by studies such as those by Mayer (2009) and Buisine and Martin (2007). Those motivate our work to fill the present lack of corpus-based linguistic knowledge of the phenomenon. Also, although spatial deixis falls beyond the scope of this paper, we acknowledge the efforts of others such as Gergle et al (2013) to study its value in collaborative communication. Prior works have examined discourse deixis in text, though little attention has been given to CD as a phenomenon or deixis to other CAs. Seminal papers by Webber (1988, 1991) established the importance of discourse deixis, although they focused upon demonstrative pronouns such as ?this? or ?that?. Many efforts have addressed discourse deixis in the context of anaphora; these include Poesio and Artstein?s (2008), who created a corpus of anaphoric relations inclusive of (but not limited to) discourse. Their collection included 455 instances of discourse deixis, although they noted ambiguity in the set of markables. Dipper and Zinsmeister (2012) also addressed discourse deixis through anaphora resolution and produced a collection of 225 abstract anaphors out of 643 candidate instances. Prior studies of shell nouns revealed capacities of referents similar to a subset of those found in our work. Such nouns are used anaphorically to refer to complex, proposition-like pieces of information such as points, assumptions, or acts (Schmid, 2000). Kolhatkar et al (2013) noted the pervasiveness of shell nouns in text and their tendency to ?characterize and label? their antecedents. However, such antecedents only partly intersect with CAs. The set of shell nouns studied by Schmid did not include typical document entities such as section, figure, or list. Simultaneously, the set included many nouns with little or no relevance as CAs, such as fury, miracle, and pride.  The task of identifying CD in text and referent CAs bears some similarity to coreference resolution. However, coreference resolvers tried by the authors (namely CoreNLP (Recasens et al, 2013), ArkRef (O?Connor and Heilman, 2013) and the work of Roth and Bengston (2008)) were ineffective at this task. We posit that many CAs are not noun phrases, which makes them difficult or inappropriate to characterize as referring expressions. This limits the effectiveness of traditional approaches to coreference resolution toward the present problem. 
 Our results are further distinct from prior work by focusing on the communicative capacities of a variety of referents represented in documents. However, the present focus upon determiner-established phrases is more exclusive, and our results do not include demarcation of referents. We posit that the tradeoff is worthwhile, given limited prior work on identifying CD and the lack of prior efforts to study CAs other than discourse entities. 3 Corpus Creation  Textbooks from Wikibooks were chosen to supply pedagogical text. Among the alternatives, this source provided the largest volume of material with a license amenable to corpus redistribution. Moreover, the collection of English language textbooks on the site covers a diverse set of topics and contains samples from a variety of writers. Below we describe our text pre-processing and then explain how candidate instances of CD were identified.  3.1 Source Material To simplify collection and processing, 122 Wikibooks textbooks with printable versions were selected for use. Contained in this set are textbooks in eleven different subject areas, such as computing, humanities, and the sciences. In preparation for analysis, the documents were POS tagged and parsed by the Stanford CoreNLP suite (Socher et al, 2013; Toutanova et al, 2003). Table 1 presents some statistics on the texts in aggregate. They illustrate the substantial size of most texts, though a few were freshly started or incomplete. Overall, the corpus is comparable in size with corpora from efforts cited in Section 2, though text genera and sought markables vary. Next, potential instances of CD were identified. Such instances were noun phrases beginning with determiners this, that, these, or those. We include these and those to collect CD to sets of entities, a nuance absent from any previous work. 9252 sentences, or 8% of the corpus, contained at least one potential instance. 
Statistic Total Min. Median Mean Max. Words 2883178 1721 20337 23633 57465 Sentences 114474 71 832 938 2121 Candidates  10495 4 85 86 285 Table 1. Statistics for the 122 selected printable Wikibooks and the candidate instances of CD.  
410
This collection contained substantial boilerplate text, and sentences that appeared verbatim in at least ten different books were discarded. This filtering produced a set of 7613 candidate instances. Table 2 shows the most frequent head nouns in candidate instances. Some resemble the shell nouns of prior work, but the presence of others illustrates the diversity of CD. Diversity was expected from pedagogical texts and validates Wikibooks as a rich source of CD.   We conducted a preliminary survey of the corpus contents by reading a random selection of 10% of candidates and judging their statuses as instances of CA. Table 3 shows examples of candidate instances, categorized by the foci of prior studies (cited in the Introduction) of CD phenomena. The researchers estimated that 48% of candidates were instances of CD, although directly labeling large numbers of candidates was deemed impractical. Instead, we noted that the word sense of the noun in a candidate instance is an important (albeit not definitive) indication of its CD status. Accordingly, we shift our focus from individual candidate instances to words that appear in them (i.e., lemmas) and word senses.  
 3.2 Word Senses The noun in an instance of CD has a doubly salient role in CA, by providing a cue to the intended referent and also by reifying the referent. For example, an illustrating referent might be referred to as ?this example? or ?this ideal?, with divergent consequences. The noun choice semantically identifies the relevant capacity of the referent, affecting its message. To identify the varieties and characteristics of CD in pedagogical text, we examine in aggregate the senses of those words that appear in candidate phrases in the corpus. WordNet 3.0 (Fellbaum, 1998) was chosen to provide an ontological structure for relevant word senses and thus for CAs. First, synsets for the 27 most frequent nouns in candidate phrases were collected, irrespective of viability for CD. This covered 34% of candidate instances and resulted in a set of 200 synsets. Their glosses were labeled as viable or non-viable for CD by two expert annotators, who first worked separately and then collaborated to resolve differences in their annotations. 
Lemma Freq.  Lemma Freq. page 314  function 83 book 287  chapter 73 case 249  information 70 example 126  problem 69 point 121  value 62 section 116  type 59 way 112  process 56 option 102  feature 56 time 101  number 54 message 93  text 54 Table 2. The 20 most frequent head nouns in candidate instances. 
 
For each synset gloss, perform the following: Imagine instantiating the type represented by the gloss. Judge its suitability for the following statements. (1) [an instantiation of the type] is about a topic. (2) [an instantiation of the type] is intended to communicate an idea. (3) [an instantiation of the type] can be produced in a document or as a document to convey information. If at least two of the three statements above are coherent, mark 'y' for the gloss. Otherwise, mark 'n'. Figure 1. Instructions given to annotators.  
 Category Examples Structural Many of the resources listed elsewhere in this section have? In this chapter, we will show you how to draw? 
Illustrative Consider these sentences: [followed by example sentences] [following a source code fragment] ?the first time the computer sees this statement, ?a? is zero, so it is less than 10. Discourse Utilizing this idea, subunit analogies were invented? In this case, you?ve narrowed the topic down to ?Badges.? Non-CD Devices similar to resistors turn this energy into light, motion? What type of things does a person in that career field know?  Table 3. Examples of candidate instances. Bold text denotes the determiner and head noun in each instance. Sentences are truncated in the table for brevity.  
411
Figure 1 shows the annotation instructions, which were designed to address the combined range of CAs from prior work. To illustrate its application, consider the noun chapter. One gloss of chapter is ?a subdivision of a written work; usually numbered and titled?. This sense clearly satisfies the third numbered statement in Figure 1. Coherency arguments for the first and second statements are less definitional, but both annotators decided at least one was satisfactory, leading to a y mark. Another gloss of chapter is ?any distinct period in history or in a person?s life?. This sense fails to satisfy the second or third statement, leading to an n mark. 4 Results and Discussion Resolving differences between the annotators? labels produced a set of 62 synsets whose glosses characterized CAs. We refer to the sets of 200 synsets and 62 synsets as the CCS (candidates for communicative senses) and VCS (verified  communicative senses) sets, respectively. We offer the complete results of our annotations online 1  to encourage further research on this topic. In this section we present inter-annotator agreement statistics and describe the composition of the VCS set using the structure of WordNet. 4.1 Inter-Annotator Agreement The kappa statistic for category agreement between the two annotators was 0.70, with matching annotations on 174 of 200 senses. Although this metric is an imperfect indicator, this value is generally regarded as substantial (Viera and Garrett, 2005) albeit with some tentativeness (Carletta, 1996). The annotators respectively placed 33% and 30% of instances in the VCS set, suggesting general agreement on the distribution of labels irrespective of specific instances. The annotators agreed that some cases were difficult to label without context, and a combination of sense labeling and in-text instance labeling may be fruitful for future work. 4.2 Representation in WordNet We use the structure of WordNet to illustrate the properties of CAs that VCS senses represent. To do this, the hypernym closure (i.e., the sequence(s) of hypernyms from a given synset to the root synset) was computed for each VCS sense. These ?traces? were aggregated into a                                                             
1 http://www.cs.cmu.edu/~shomir/wb_cd_study/ 
reproduction of a subset of WordNet?s synsets and relations, resulting in a de facto ontology of CAs. The same procedure was performed for the CCS set to create an illustrative baseline. Table 4 shows the structure of the most general synsets in the ontologies constructed from VCS and CCS traces. Fractions illustrate the relative constituent weight of each synset, by virtue of the traces that include it. For example, 65 of the 72 traces for VCS synsets pass through abstraction.n.06, and 37 of those 65 traces pass through communication.n.02. The total quantities of traces for CCS and VCS are greater than their respective set sizes because of a small number of synsets in those sets with multiple hyponym paths to the root. The rightmost column of Table 4 shows the decimal result of subtracting the CCS constituent weight fraction from the VCS fraction. Positive numbers indicate that the manual labeling of senses magnified the weight of a synset over the CCS baseline. The constituent weights confirm some intuitions but also hold a few surprises. The vast majority of CAs are abstractions rather than physical entities, and most of the abstractions are ?something that is communicated by or to or between people or groups? (the gloss of communication.n.02). Psychological features are also a substantial constituency, with traces to VCS synsets that represent words such as method, plan, and question. Most of the few VCS physical entities are communicative artifacts in their complete form (e.g., a book or a periodical issue). Matter as a physical entity may seem out of place in Table 4. The VCS synset responsible for its inclusion is page.n.01, which 
Synset CCS VCS Chg. 0 entity.n.01   1 abstraction.n.06     2 psych._feature.n.01     2 communication.n.02     2 attribute.n.02     2 group.n.01     2 measure.n.02     2 relation.n.01   1 physical_entity.n.01     2 object.n.01     2 causal_agent.n.01     2 thing.n.12     2 process.n.06     2 matter.n.03 
217 / 217 166 / 217 51 / 166 47 / 166 24 / 166 18 / 166 15 / 166 11 / 166 51 / 217 38 / 51 7 / 51 4 / 51 1 / 51 1 / 51 
72 / 72 65 / 72 15 / 65 37 / 65 2 / 65 4 / 65 3 / 65 4 / 65 7 / 72 6 / 7 0 / 7 0 / 7 0 / 7 1 / 7 
0 .14 -.08 .29 -.11 -.05 -.04 .00 -.14 .11 -.14 -.08 -.02 .12 Table 4. Distributions of traces through the first two hyponym relations emanating from the root synset entity.n.01, for CCS and VCS. Fractions indicate the constituent weight of each synset.   
412
has the gloss ?one side of one leaf (of a book or magazine or newspaper or letter etc.) or the written or pictorial matter it contains.? Both annotators believed it merited inclusion in VCS. Finally, we observed that many VCS senses (58%) were not the first sense for their words, indicating different senses appear more often2. This likely hinders word sense disambiguation of nouns in CD instances: the common baseline of first sense tagging is futile in these cases, and their extra-topical nature means that appropriate CA senses are not implied by the surrounding words (Wilson, 2011). This suggests that identification of CD instances may require a dedicated approach to word sense tagging. 5 Conclusion The results of this study illustrate the significance of CD, both for the processing of pedagogical texts and for the broader project of understanding anaphora. Its pervasiveness and its diversity show its potential as a conduit for language technologies to enrich documents with pragmatic metadata. Our next effort will be to identify the referents of CD instances using knowledge from the present study of the character and distribution of those referents. CAs are represented by spans of content in a document (e.g., text or figures), and accordingly the identification of a CD referent will involve the selection of the correct span of content. We expect that the word sense of the noun in a CD phrase will limit the set of potentially relevant CAs, and that both localized features (such as paragraph position of a CD instance and the expected CA count) and document-level features (e.g., proximity of potential referents) will be valuable.  Acknowledgment This research was supported by grant #1159236 from the US National Science Foundation?s International Research Fellowship Program.  
                                                            
2 The WordNet manual advises that senses are ?generally? ordered by frequency. 
References  Bengtson, E. and Roth, D. (2008). Understanding the value of features for coreference resolution. In Proc. EMNLP. Buisine, S. and Martin, J.-C. (2007). The effects of speech?gesture cooperation in animated agents? behavior in multimedia presentations. Interacting with Computers, 19(4), 484?493. doi:10.1016/j.intcom.2007.04.002 Carletta, J. (1996). Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2), 249?254. Dipper, S. and Zinsmeister, H. (2012). Annotating abstract anaphora. In Proc. LREC, 46(1), 37?52. doi:10.1007/s10579-011-9160-1 Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge: MIT Press. Gergle, D., Kraut, R. E., and Fussell, S. R. (2013). Using visual information for grounding and awareness in collaborative tasks. Human-Computer Interaction, 28(1), 1?39. Kolhatkar, V., Zinsmeister, H., and Hirst, G. (2013). Interpreting anaphoric shell nouns using antecedents of cataphoric shell nouns as training data. In Proc. EMNLP (pp. 300?310). Lyons, J. (1977). Semantics. Cambridge University Press. Mayer, R. E. (2009). Multimedia Learning. Cambridge University Press. O'Connor, B. and Heilman, M. (2013). ARKref: A rule-based coreference resolution system. arXiv:1310.1975, Paraboni, I. and Deemter, K. (2006). Referring via document parts. In A. Gelbukh (Ed.), Computational Linguistics and Intelligent Text Processing (Vol. 3878, pp. 299?310). Springer Berlin Heidelberg. Retrieved from http://dx.doi.org/10.1007/11671299_31 Poesio, M. and Artstein, R. (2008). Anaphoric annotation in the ARRAU Corpus. In Proc. LREC. Marrakech, Morocco: European Language Resources Association (ELRA). Recasens, M., Catherine de Marneffe, M., and Potts, C. (2013). The life and death of discourse entities: Identifying singleton mentions. In Proc. NAACL. Schmid, H.-J. (2000). English Abstract Nouns as Conceptual Shells: From Corpus to Cognition. Walter de Gruyter. 
413
Socher, R., Bauer, J., Manning, C. D., and Ng, A. Y. (2013). Parsing with compositional vector grammars. In Proc. ACL (pp. 455?465). Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. NAACL. doi:10.3115/1073445.1073478 Viera, A. J., and Garrett, J. M. (2005). Understanding interobserver agreement: The kappa statistic. Family Medicine, 37(5), 360?363. 
Webber, B. L. (1988). Discourse deixis: Reference to discourse segments. In Proc. ACL (pp. 113?122). doi:10.3115/982023.982037 Webber, B. L. (1991). Structure and ostension in the interpretation of discourse deixis. In Natural Language and Cognitive Processes. Wilson, S. (2011). A Computational Theory of the Use-Mention Distinction in Natural Language. University of Maryland at College Park. PhD Thesis, College Park, MD, USA.   
414
Situated Reference in a Hybrid Human-Robot Interaction System
Manuel Giuliani1 and Mary Ellen Foster2 and Amy Isard3
Colin Matheson3 and Jon Oberlander3 and Alois Knoll1
1Informatik VI: Robotics and Embedded Systems, Technische Universita?t Mu?nchen
2School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
3Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh
Abstract
We present the situated reference genera-
tion module of a hybrid human-robot in-
teraction system that collaborates with a
human user in assembling target objects
from a wooden toy construction set. The
system contains a sub-symbolic goal in-
ference system which is able to detect the
goals and errors of humans by analysing
their verbal and non-verbal behaviour. The
dialogue manager and reference genera-
tion components then use situated refer-
ences to explain the errors to the human
users and provide solution strategies. We
describe a user study comparing the results
from subjects who heard constant refer-
ences to those who heard references gener-
ated by an adaptive process. There was no
difference in the objective results across
the two groups, but the subjects in the
adaptive condition gave higher subjective
ratings to the robot?s abilities as a conver-
sational partner. An analysis of the objec-
tive and subjective results found that the
main predictors of subjective user satisfac-
tion were the user?s performance at the as-
sembly task and the number of times they
had to ask for instructions to be repeated.
1 Introduction
When two humans jointly carry out a mutual task
for which both know the plan?for example, as-
sembling a new shelf?it frequently happens that
one makes an error, and the other has to assist
and to explain what the error was and how it can
be solved. Humans are skilled at spotting errors
committed by another, as well as errors which
they made themselves. Recent neurological stud-
ies have shown that error monitoring?i.e., ob-
serving the errors made by oneself or by others?
plays an important role in joint activity. For ex-
ample, Bekkering et al (2009) have demonstrated
that humans show the same brain activation pat-
terns when they make an error themselves and
when they observe someone else making an error.
In this paper, we describe a human-robot inter-
action (HRI) system that is able both to analyse
the actions and the utterances of a human part-
ner to determine if the human made an error in
the assembly plan, and to explain to the human
what went wrong and what to do to solve the prob-
lem. This robot combines approaches from sub-
symbolic processing and symbolic reasoning in a
hybrid architecture based on that described in Fos-
ter et al (2008b).
During the construction process, it is frequently
necessary to refer to an object which is being used
to assemble the finished product, choosing an un-
ambigious reference to distinguish the object from
the others available. The classic reference gen-
eration algorithm, on which most subsequent im-
plementations are based, is the incremental algo-
rithm of Dale and Reiter (1995), which selects
a set of attributes of a target object to single it
out from a set of distractor objects. In real-world
tasks, the speaker and hearer often have more con-
text in common than just the knowledge of object
attributes, and several extensions have been pro-
posed, dealing with visual and discourse salience
(Kelleher and Kruijff, 2006) and the ability to pro-
duce multimodal references including actions such
as pointing (van der Sluis, 2005; Kranstedt and
Wachsmuth, 2005).
Foster et al (2008a) noted another type of mul-
timodal reference which is particularly useful in
embodied, task-based contexts: haptic-ostensive
reference, in which an object is referred to as it
is being manipulated by the speaker. Manipulat-
ing an object, which must be done in any case as
part of the task, also makes an object more salient
and therefore affords linguistic references that in-
Figure 1: The dialogue robot
dicate the increased accessibility of the referent.
This type of reference is similar to the placing-for
actions noted by Clark (1996).
An initial approach for generating referring ex-
pressions that make use of haptic-ostensive refer-
ence was described in (Foster et al, 2009a). With
this system, a study was conducted comparing the
new reference strategy to the basic Dale and Reiter
incremental algorithm. Na??ve users reported that it
was significantly easier to understand the instruc-
tions given by the robot when it used references
generated by the more sophisticated algorithm. In
this paper, we perform a similar experiment, but
making use of a more capable human-robot in-
teraction system and a more complete process for
generating situated references.
2 Hybrid Human-Robot Dialogue
System
The experiment described in this paper makes use
of a hybrid human-robot dialogue system which
supports multimodal human-robot collaboration
on a joint construction task. The robot (Figure 1)
has a pair of manipulator arms with grippers,
mounted in a position to resemble human arms,
and an animatronic talking head (van Breemen,
2005) capable of producing facial expressions,
rigid head motion, and lip-synchronised synthe-
sised speech. The subject and the robot work to-
gether to assemble wooden construction toys on
a common workspace, coordinating their actions
through speech (English or German), gestures, and
facial expressions.
The robot can pick up and move objects in the
workspace and perform simple assembly tasks. In
the scenario considered here, both of the partici-
pants know the assembly plan and jointly execute
it. The robot assists the human, explains necessary
assembly steps in case the human makes an error,
and offers pieces as required. The workspace is di-
vided into two areas?one belonging to the robot
and one to the human?to make joint action nec-
essary for task success.
The system has components which use both
sub-symbolic and symbolic processing. It in-
cludes a goal inference module based on dynamic
neural fields (Erlhagen and Bicho, 2006; Bicho
et al, 2009), which selects the robot?s next actions
based on the human user?s actions and utterances.
Given a particular assembly plan and the knowl-
edge of which objects the user has picked up, this
module can determine when the user has made
an error. The system also incorporates a dialogue
manager based on the TrindiKit dialogue manage-
ment toolkit (Larsson and Traum, 2000), which
implements the information-state based approach
to dialogue management. This unique combina-
tion of abilities means that when the robot detects
that its human partner has made an error?for ex-
ample, picking up or requesting an assembly piece
that is not needed in the current step of the building
plan?it can explain to the human what the error
was and what can be done to correct the mistake?
for example by picking up or indicating the correct
assembly piece.
Messages from all of the system?s input chan-
nels (speech, object recognition, and gesture
recognition) are processed and combined by a
multimodal fusion component based on (Giuliani
and Knoll, 2008), which is the link between the
symbolic and the sub-symbolic parts of the sys-
tem. The fusion component then communicates
with the goal inference module, which calculates
the next action instructions for the robot and also
determines if the user made an error. From there,
fusion combines the information from goal infer-
ence with the input data and sends unified hy-
potheses to the dialogue manager.
When it receives the fusion hypotheses, the dia-
logue manager uses the dialogue history and the
physical and task context to choose a response.
It then sends a high-level specification of the de-
1. System First we will build a windmill.
2. User Okay.
3. User {picks up a yellow cube, unnecessary piece for a
windmill}
4. System You don?t need a yellow cube to build a windmill.
5. System To build a windmill, you first need to build a
tower.
6. System [picking up and holding out red cube] To build
the tower, insert the green bolt through the end of this
red cube and screw it into the blue cube.
7. User [takes cube, performs action] Okay.
Figure 2: Sample human-robot dialogue, showing
adaptively-generated situated references
sired response to the output planner, which in turn
sends commands to each output channel: linguis-
tic content (including multimodal referring ex-
pressions), facial expressions and gaze behaviours
of the talking head, and actions of the robot ma-
nipulators. The linguistic outputs are realised us-
ing the OpenCCG surface realiser (White, 2006).
3 Reference Generation
In this system, two strategies were implemented
for generating references to objects in the world:
a constant version that uses only the basic incre-
mental algorithm (Dale and Reiter, 1995) to se-
lect properties, and an adaptive version that uses
more of the physical, dialogue and task context
to help select the references. The constant sys-
tem can produce a definite or indefinite reference,
and the most appropriate combination of attributes
according to the incremental algorithm. The adap-
tive system also generates pronominal and deictic
references, and introduces the concept of multiple
types of distractor sets depending on context.
Figure 2 shows a fragment of a sample interac-
tion in which the user picks up an incorrect piece:
the robot detects the error and describes the correct
assembly procedure. The underlined references
show the range of output produced by the adap-
tive reference generation module; for the constant
system, the references would all have been ?the
red cube?. The algorithms used by the adaptive
reference generation module are described below.
3.1 Reference Algorithm
The module stores a history of the referring ex-
pressions spoken by both the system and the user,
and uses these together with distractor sets to se-
lect referring expressions. In this domain there are
two types of objects which we need to refer to:
concrete objects in the world (everything which is
on the table, or in the robot?s or user?s hand), and
objects which do not yet exist, but are in the pro-
cess of being created. For non-existent objects we
do not build a distractor set, but simply use the
name of the object. In all other cases, we use one
of three types of distractor set:
? all the pieces needed to build a target object;
? all the objects referred to since the last men-
tion of this object; or
? all the concrete objects in the world.
The first type of set is used if the object under
consideration (OUC) is a negative reference to a
piece in context of the creation of a target object.
In all other cases, the second type is used if the
OUC has been mentioned before and the third type
if it has not.
When choosing a referring expression, we first
process the distractor set, comparing the proper-
ties of the OUC with the properties of all distrac-
tors. If a distractor has a different type from the
OUC, it is removed from the distractor set. With
all other properties, if the distractor has a different
value from the OUC, it is removed from the dis-
tractor set, and the OUC?s property value is added
to the list of properties to use.
We then choose the type of referring expression.
We first look for a previous reference (PR) to the
OUC, and if one exists, determine whether it was
in focus. Depending on the case, we use one of the
following reference strategies.
No PR If the OUC does not yet exist or we are
making a negative reference, we use an indef-
inite article. If the robot is holding the OUC,
we use a deictic reference. If the OUC does
exist and there are no distractors, we use a
definite; if there are distractors we use an in-
definite.
PR was focal If the PR was within the same turn,
we choose a pronoun for our next reference.
If it was in focus but in a previous turn, if
the robot is holding the OUC we use a deictic
reference, and if the robot is not holding it,
we use a pronoun.
PR was not focal If the robot is holding the
OUC, we make a deictic reference. Other-
wise, if the PR was a pronoun, definite, or de-
ictic, we use a definite article. If the PR was
indefinite and there are no distractors, we use
a definite article, if there are distractors, we
use an indefinite article.
If there are any properties in the list, and the
reference chosen is not a pronoun, we add them.
3.2 Examples of the Reference Algorithm
We will illustrate the reference-selection strategy
with two cases from the dialogue in Figure 2.
Utterance 4 ?a yellow cube?
This object is going to be referred to in a negative
context as part of a windmill under construction,
so the distractor set is the set of objects needed to
make a windmill: {red cube, blue cube, small slat,
small slat, green bolt, red bolt}.
We select the properties to use in describing the
object under consideration, processing the distrac-
tor set. We first remove all objects which do not
share the same type as our object under considera-
tion, which leaves {red cube, blue cube}. We then
compare the other attributes of our new object with
the remaining distractors - in this case ?colour?.
Since neither cube shares the colour ?yellow? with
the target object, both are removed from the dis-
tractor set, and ?yellow? is added to the list of
properties to use.
There is no previous reference to this object,
and since we are making a negative reference,
we automatically choose an indefinite article. We
therefore select the reference ?a yellow cube?.
Utterance 6 ?it? (a green bolt)
This object has been referred to before, earlier in
the same utterance, so the distractor set is all the
references between the earlier one and this one?
{red cube}. Since this object has a different type
from the bolt we want to describe, the distractor
set is now empty, and nothing is added to the list
of properties to use.
There is a previous definite reference to the ob-
ject in the same utterance: ?the green bolt?. This
reference was focal, so we are free to use a pro-
noun if appropriate. Since the previous reference
was definite, and the object being referred to does
exist, we choose to use a pronoun. We therefore
select the reference ?it?.
4 Experiment Design
In the context of the HRI system, a constant refer-
ence strategy is sufficient in that it makes it possi-
ble for the robot?s partner to know which item is
needed. On the other hand, while the varied forms
produced by the more complex mechanism can in-
crease the naturalness of the system output, they
may actually be insufficient if they are not used
in appropriate current circumstances?for exam-
ple, ?this cube? is not a particularly helpful refer-
ence if a user has no way to tell which ?this? is.
As a consequence, the system for generating such
references must be sensitive to the current state
of joint actions and?in effect?of joint attention.
The difference between the two systems is a test of
the adaptive version?s ability to adjust expressions
to pertinent circumstances. It is known that peo-
ple respond well to reduced expressions like ?this
cube? or ?it? when another person uses them ap-
propriately (Bard et al, 2008); we need to see if
the robot system can also achieve the benefits that
situated reference could provide.
To address this question, the human-robot di-
alogue system was evaluated through a user study
in which subjects interacted with the complete sys-
tem. Using a between-subjects design, this study
compared the two reference strategies, measuring
the users? subjective reactions to the system along
with their overall performance in the interaction.
Based on the findings from the user evaluation de-
scribed in (Foster et al, 2009a)?in which the pri-
mary effect of varying the reference strategy was
on the users? subjective opinion of the robot?the
main prediction for this study was as follows:
? Subjects who interact with a system using
adaptive references will rate the quality of
the robot?s conversation more highly than the
subjects who hear constant references.
We made no specific prediction regarding the
effect of reference strategy on any of the objec-
tive measures: based on the results of the user
evaluation mentioned above, there is no reason to
expect an effect either way. Note that?as men-
tioned above?if the adaptive version makes in-
correct choices, that may have a negative impact
on users? ability to understand the system?s gener-
ated references. For this reason, even a finding of
(a) Windmill (b) Railway signal
Figure 3: Target objects for the experiment
no objective difference would demonstrate that the
adaptive references did not harm the users? ability
to interact with the system, as long as it was ac-
companied by the predicted improvement in sub-
jective judgements.
4.1 Subjects
41 subjects (33 male) took part in this experiment.
The mean age of the subjects was 24.5, with a min-
imum of 19 and a maximum of 42. Of the subjects
who indicated an area of study, the two most com-
mon areas were Mathematics (14 subjects) and In-
formatics (also 14 subjects). On a scale of 1 to 5,
subjects gave a mean assessment of their knowl-
edge of computers at 4.1, of speech-recognition
systems at 2.0, and of human-robot systems at 1.7.
Subjects were compensated for their participation
in the experiment.
4.2 Scenario
This study used a between-subjects design with
one independent variable: each subject interacted
either with a system that used a constant strategy
to generate referring expressions (19 subjects), or
else with a system that used an adaptive strategy
(22 subjects).1
Each subject built two objects in collaboration
with the system, always in the same order. The
first target object was the windmill (Figure 3a);
after the windmill was completed, the robot and
human then built a railway signal (Figure 3b). For
both target objects, the user was given a building
plan (on paper). To induce an error, both of the
plans given to the subjects instructed them to use
an incorrect piece: a yellow cube instead of a red
cube for the windmill, and a long (seven-hole) slat
instead of a medium (five-hole) slat for the rail-
1The results of an additional three subjects in the constant-
reference condition could not be analysed due to technical
difficulties.
way signal. The subjects were told that the plan
contained an error and that the robot would cor-
rect them when necessary, but did not know the
nature of the error.
When the human picked up or requested an in-
correct piece during the interaction, the system de-
tected the error and explained to the human what
to do in order to assemble the target object cor-
rectly. When the robot explained the error and
when it handed over the pieces, it used referring
expressions that were generated using the constant
strategy for half of the subjects, and the adaptive
strategy for the other half of the subjects.
4.3 Experimental Set-up and Procedure
The participants stood in front of the table facing
the robot, equipped with a headset microphone for
speech recognition. The pieces required for the
target object?plus a set of additional pieces in or-
der to make the reference task more complex?
were placed on the table, using the same layout
for every participant. The layout was chosen to
ensure that there would be points in the interaction
where the subjects had to ask the robot for build-
ing pieces from the robot?s workspace, as well as
situations in which the robot automatically handed
over the pieces. Along with the building plan men-
tioned above, the subjects were given a table with
the names of the pieces they could build the ob-
jects with.
4.4 Data Acquisition
At the end of a trial, the subject responded to
a usability questionnaire consisting of 39 items,
which fell into four main categories: Intelligence
of the robot (13 items), Task ease and task suc-
cess (12 items), Feelings of the user (8 items),
and Conversation quality (6 items). The items on
the questionnaire were based on those used in the
user evaluation described in (Foster et al, 2009b),
but were adapted for the scenario and research
questions of the current study. The questionnaire
was presented using software that let the subjects
choose values between 1 and 100 with a slider. In
addition to the questionnaire, the trials were also
video-taped, and the system log files from all tri-
als were kept for further analysis.
5 Results
We analysed the data resulting from this study in
three different ways. First, the subjects? responses
Table 1: Overall usability results
Constant Adaptive M-W
Intell. 79.0 (15.6) 74.9 (12.7) p = 0.19, n.s.
Task 72.7 (10.4) 71.1 (8.3) p = 0.69, n.s.
Feeling 66.9 (15.9) 66.8 (14.2) p = 0.51, n.s.
Conv. 66.1 (13.6) 75.2 (10.7) p = 0.036, sig.
Overall 72.1 (11.2) 71.8 (9.1) p = 0.68, n.s.
to the questionnaire items were compared to de-
termine if there was a difference between the re-
sponses given by the two groups. A range of sum-
mary objective measures were also gathered from
the log files and videos?these included the dura-
tion of the interaction measured both in seconds
and in system turns, the subjects? success at build-
ing each of the target objects, the number of times
that the robot had to explain the construction plan
to the user, and the number of times that the users
asked the system to repeat its instructions. Finally,
we compared the results on the subjective and ob-
jective measures to determine which of the objec-
tive factors had the largest influence on subjective
user satisfaction.
5.1 Subjective Measures
The subjects in this study gave a generally positive
assessment of their interactions with the system on
the questionnaire?with a mean overall satisfac-
tion score of 72.0 out of 100?and rated the per-
ceived intelligence of the robot particularly highly
(overall mean of 76.8). Table 1 shows the mean
results from the two groups of subjects for each
category on the user-satisfaction questionnaire, in
all cases on a scale from 0?100 (with the scores
for negatively-posed questions inverted).
To test the effect of reference strategy on the
usability-questionnaire responses, we performed a
Mann-Whitney test comparing the distribution of
responses from the two groups of subjects on the
overall results, as well as on each sub-category of
questions. For most categories, there was no sig-
nificant difference between the responses of the
two groups, with p values ranging from 0.19 to
0.69 (as shown in Table 1). The only category
where a significant difference was found was on
the questionnaire items that asked the subjects to
assess the robot?s quality as a conversational part-
ner; for those items, the mean score from sub-
jects who heard the adaptive references was sig-
nificantly higher (p < 0.05) than the mean score
from the subjects who heard references generated
by the constant reference module. Of the six ques-
Table 2: Objective results (all differences n.s.)
Measure Constant Adaptive M-W
Duration (s.) 404.3 (62.8) 410.5 (94.6) p = 0.90
Duration (turns) 29.8 (5.02) 31.2 (5.57) p = 0.44
Rep requests 0.26 (0.45) 0.32 (0.78) p = 0.68
Explanations 2.21 (0.63) 2.41 (0.80) p = 0.44
Successful trials 1.58 (0.61) 1.55 (0.74) p = 0.93
tions that were related to the conversation quality,
the most significant impact was on the two ques-
tions which assessed the subjects? understanding
of what they were able to do at various points dur-
ing the interaction.
5.2 Objective Measures
Based on the log files and video recordings, we
computed a range of objective measures. These
measures were divided into three classes, based
on those used in the PARADISE dialogue-system
evaluation framework (Walker et al, 2000):
? Two dialogue efficiency measures: the mean
duration of the interaction as measured both
in seconds and in system turns;
? Two dialogue quality measures: the number
of times that the robot gave explanations, and
the number of times that the user asked for
instructions to be repeated; and
? One task success measure: how many of the
(two) target objects were constructed as in-
tended (i.e., as shown in Figure 3).
For each of these measures, we tested whether the
difference in reference strategy had a significant
effect, again via a Mann-Whitney test. Table 2 il-
lustrates the results on these objective measures,
divided by the reference strategy.
The results from the two groups of subjects
were very similar on all of these measures: on
average, the experiment took 404 seconds (nearly
seven minutes) to complete with the constant strat-
egy and 410 seconds with the adaptive, the mean
number of system turns was close to 30 in both
cases, just over one-quarter of all subjects asked
for instructions to be repeated, the robot gave just
over two explanations per trial, and about three-
quarters of all target objects (i.e. 1.5 out of 2)
were correctly built. The Mann-Whitney test con-
firms that none of the differences between the two
groups even came close to significance on any of
the objective measures.
5.3 Comparing Objective and Subjective
Measures
In the preceding sections, we presented results on
a number of objective and subjective measures.
While the subjects generally rated their experi-
ence of using the system positively, there was
some degree of variation, most of which could not
be attributed to the difference in reference strat-
egy. Also, the results on the objective measures
varied widely across the subjects, but again were
not generally affected by the reference strategy.
In this section, we examine the relationship be-
tween these two classes of measures in order to
determine which of the objective measures had the
largest effect on users? subjective reactions to the
HRI system.
Being able to predict subjective user satisfac-
tion from more easily-measured objective proper-
ties can be very useful for developers of interac-
tive systems: in addition to making it possible to
evaluate systems based on automatically available
data without the need for extensive experiments
with users, such a performance function can also
be used in an online, incremental manner to adapt
system behaviour to avoid entering a state that is
likely to reduce user satisfaction (Litman and Pan,
2002), or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
We employed the procedure used in the PAR-
ADISE evaluation framework (Walker et al,
2000) to explore the relationship between the sub-
jective and objective factors. The PARADISE
model uses stepwise multiple linear regression to
predict subjective user satisfaction based on mea-
sures representing the performance dimensions of
task success, dialogue quality, and dialogue effi-
ciency, resulting in a predictor function of the fol-
lowing form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Table 3 shows the predictor functions that were
derived for each of the classes of subjective mea-
sures in this study, using all of the objective mea-
sures from Table 2 as initial factors. The R2 col-
umn indicates the percentage of the variance in the
target measure that is explained by the predictor
function, while the Significance column gives sig-
nificance values for each term in the function.
In general, the two factors with the biggest in-
fluence on user satisfaction were the number of
repetition requests (which had a uniformly neg-
ative effect on user satisfaction), and the num-
ber of target objects correctly built by the user
(which generally had a positive effect). Aside
from the questions on user feelings, the R2 values
are generally in line with those found in previous
PARADISE evaluations of other dialogue systems
(Walker et al, 2000; Litman and Pan, 2002), and
in fact are much higher than those found in a pre-
vious similar study (Foster et al, 2009b).
6 Discussion
The subjective responses on the relevant items
from the usability questionnaire suggest that
the subjects perceived the robot to be a bet-
ter conversational partner if it used contextually
varied, situationally-appropriate referring expres-
sions than if it always used a baseline, constant
strategy; this supports the main prediction for this
study. The result also agrees with the findings of
a previous study (Foster et al, 2009a)?this sys-
tem did not incorporate goal inference and had a
less-sophisticated reference strategy, but the main
effect of changing reference strategy was also on
the users? subjective opinions of the robot?s inter-
active ability. These studies together support the
current effort in the natural-language generation
community to devise more sophisticated reference
generation algorithms.
On the other hand, there was no significant dif-
ference between the two groups on any of the
objective measures: the dialogue efficiency, dia-
logue quality, and task success were nearly iden-
tical across the two groups of subjects. A de-
tailed analysis of the subjects? gaze and object-
manipulation behaviour immediately after various
forms of generated references from the robot also
failed to find any significant differences between
the various reference types. These overall results
are not particularly surprising: studies of human-
human dialogue in a similar joint construction task
(Bard et al, In prep.) have demonstrated that the
collaborators preserve quality of construction in
Table 3: PARADISE predictor functions for each category on the usability questionnaire
Measure Function R2 Significance
Intelligence 76.8+7.00?N (Correct)?5.51?N (Repeats) 0.39 Correct: p < 0.001,
Repeats: p < 0.005
Task 72.4+3.54?N (Correct)?3.45?N (Repeats)?2.17?N (Explain) 0.43 Correct: p < 0.005,
Repeats: p < 0.01,
Explain: p? 0.10
Feeling 66.9?6.54?N (Repeats)+4.28?N (Seconds) 0.09 Repeats: p < 0.05,
Seconds: p? 0.12
Conversation 71.0+5.28?N (Correct)?3.08?N (Repeats) 0.20 Correct: p < 0.01,
Repeats: p? 0.10
Overall 72.0+4.80?N (Correct)?4.27?N (Repeats) 0.40 Correct: p < 0.001,
Repeats: p < 0.005
all cases, though circumstances may dictate what
strategies they use to do this. Combined with the
subjective findings, this lack of an objective effect
suggests that the references generated by the adap-
tive strategy were both sufficient and more natural
than those generated by the constant strategy.
The analysis of the relationship between the
subjective and objective measures analysis has
also confirmed and extended the findings from a
similar analysis (Foster et al, 2009b). In that
study, the main contributors to user satisfaction
were user repetition requests (negative), task suc-
cess, and dialogue length (both positive). In the
current study, the primary factors were similar,
although dialogue length was less prominent as
a factor and task success was more prominent.
These findings are generally intuitive: subjects
who are able to complete the joint construction
task are clearly having more successful interac-
tions than those who are not able to complete the
task, while subjects who need to ask for instruc-
tions to be repeated are equally clearly not hav-
ing successful interactions. The findings add ev-
idence that, in this sort of task-based, embodied
dialogue system, users enjoy the experience more
when they are able to complete the task success-
fully and are able to understand the spoken contri-
butions of their partner, and also suggest that de-
signers should concentrate on these aspects of the
interaction when designing the system.
7 Conclusions
We have presented the reference generation mod-
ule of a hybrid human-robot interaction system
that combines a goal-inference component based
on sub-symbolic dynamic neural fields with a
natural-language interface based on more tradi-
tional symbolic techniques. This combination of
approaches results in a system that is able to work
together with a human partner on a mutual con-
struction task, interpreting its partner?s verbal and
non-verbal behaviour and responding appropri-
ately to unexpected actions (errors) of the partner.
We have then described a user evaluation of this
system, concentrating on the impact of different
techniques for generating situated references in
the context of the robot?s corrective feedback. The
results of this study indicate that using an adaptive
strategy to generate the references significantly in-
creases the users? opinion of the robot as a con-
versational partner, without having any effect on
any of the other measures. This result agrees with
the findings of the system evaluation described in
(Foster et al, 2009a), and adds evidence that so-
phisticated generation techniques are able to im-
prove users? experiences with interactive systems.
An analysis of the relationship between the ob-
jective and subjective measures found that the
main contributors to user satisfaction were the
users? task performance (which had a positive ef-
fect on most measures of satisfaction), and the
number of times the users had to ask for instruc-
tions to be repeated (which had a generally neg-
ative effect). Again, these results agree with the
findings of a previous study (Foster et al, 2009b),
and also suggest priorities for designers of this
type of task-based interactive system.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka and Levent Kent
for help in running the experiment and analysing
the data.
2http://www.jast-project.eu/
3http://www.ics.forth.gr/indigo/
References
E. G. Bard, R. Hill, and M. E. Foster. 2008. What
tunes accessibility of referring expressions in
task-related dialogue? In Proceedings of the
30th Annual Meeting of the Cognitive Science
Society (CogSci 2008). Chicago.
E. G. Bard, R. L. Hill, M. E. Foster, and M. Arai.
In prep. How do we tune accessibility in joint
tasks: Roles and regulations.
H. Bekkering, E.R.A. de Bruijn, R.H. Cuijpers,
R. Newman-Norlund, H.T. van Schie, and
R. Meulenbroek. 2009. Joint action: Neurocog-
nitive mechanisms supporting human interac-
tion. Topics in Cognitive Science, 1(2):340?
352.
E. Bicho, L. Louro, N. Hipolito, and W. Erlhagen.
2009. A dynamic field approach to goal infer-
ence and error monitoring for human-robot in-
teraction. In Proceedings of the Symposium on
?New Frontiers in Human-Robot Interaction?,
AISB 2009 Convention. Heriot-Watt University
Edinburgh.
H. H. Clark. 1996. Using Language. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(2):233?263.
W. Erlhagen and E. Bicho. 2006. The dynamic
neural field approach to cognitive robotics.
Journal of Neural Engineering, 3(3):R36?R54.
M. E. Foster, E. G. Bard, R. L. Hill, M. Guhe,
J. Oberlander, and A. Knoll. 2008a. The roles
of haptic-ostensive referring expressions in co-
operative, task-based human-robot dialogue. In
Proceedings of HRI 2008.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009a. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI-09.
M. E. Foster, M. Giuliani, and A. Knoll. 2009b.
Comparing objective and subjective measures
of usability in a human-robot dialogue system.
In Proceedings of ACL-IJCNLP 2009.
M. E. Foster, M. Giuliani, T. Mu?ller, M. Rickert,
A. Knoll, W. Erlhagen, E. Bicho, N. Hipo?lito,
and L. Louro. 2008b. Combining goal inference
and natural-language dialogue for human-robot
joint action. In Proceedings of the 1st Interna-
tional Workshop on Combinations of Intelligent
Methods and Applications at ECAI 2008.
M. Giuliani and A. Knoll. 2008. MultiML:
A general-purpose representation language for
multimodal human utterances. In Proceedings
of ICMI 2008.
J. D. Kelleher and G.-J. M. Kruijff. 2006. Incre-
mental generation of spatial referring expres-
sions in situated dialog. In Proceedings of
COLING-ACL 2006.
A. Kranstedt and I. Wachsmuth. 2005. Incremen-
tal generation of multimodal deixis referring to
objects. In Proceedings of ENLG 2005.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6(3&4):323?340.
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system.
User Modeling and User-Adapted Interaction,
12(2?3):111?137.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of AISB 2005
Creative Robotics Symposium.
I. F. van der Sluis. 2005. Multimodal Reference:
Studies in Automatic Generation of Multimodal
Referring Expressions. Ph.D. thesis, University
of Tilburg.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. White. 2006. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation, 4(1):39?75.
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 40?48,
Utica, May 2012. c?2012 Association for Computational Linguistics
Perceptions of Alignment and Personality in Generated Dialogue
Alastair J. Gill
University of Surrey
Guildford GU2 7XH, UK
A.Gill@surrey.ac.uk
Carsten Brockmann and Jon Oberlander
University of Edinburgh
Edinburgh EH8 9AB, UK
Carsten.Brockmann@gmx.net
J.Oberlander@ed.ac.uk
Abstract
Variation in language style can lead to differ-
ent perceptions of the interaction, and differ-
ent behaviour outcomes. Using the CRAG 2
language generation system we examine how
accurately judges can perceive character per-
sonality from short, automatically generated
dialogues, and how alignment (similarity be-
tween speakers) alters judge perceptions of the
characters? relationship. Whilst personality
perception of our dialogues is consistent with
perceptions of human behaviour, we find that
the introduction of alignment leads to nega-
tive perceptions of the dialogues and the inter-
locutors? relationship. A follow up evaluation
study of the perceptions of different forms of
alignment in the dialogues reveals that while
similarity at polarity, topic and construction
levels is viewed positively, similarity at the
word level is regarded negatively. We discuss
our findings in relation to the literature and in
the context of dialogue systems.
1 Introduction
Personality describes characteristics which are cen-
tral to human behaviour, and has implications for
social interactions: It can affect performance on col-
laborative processes, and can increase engagement
when incorporated within virtual agents (Hernault
et al, 2008). In addition, personality has also been
shown to influence linguistic style, both in written
and spoken language (Pennebaker and King, 1999;
Gill and Oberlander, 2002). Whilst individuals of-
ten possess individual styles of self-expression, such
as those influenced by personality, in a conversation
they may align or match the linguistic style of their
partner: For example, by entraining, or converging,
on a mutual vocabulary. Such alignment is associ-
ated with increased familiarity, trust, and task suc-
cess (Shepard et al, 2001). People also adjust their
linguistic styles when interacting with computers,
and this affects their perceptions of the interaction
(Porzel et al, 2006). However, when humans ? or
machines ? are faced with a choice of matching the
language of their conversational partner, this often
raises a conflict: matching the language of an in-
terlocutor may mean subduing one?s own linguistic
style. Better understanding these processes relating
to language choice and interpersonal perception can
inform our knowledge of human behaviour, but also
have important implications for the design of dia-
logue systems and user interfaces.
In this paper, we present and evaluate novel
automated natural language generation techniques,
via the Critical Agent Dialogue system version 2
(CRAG 2), which enable us to generate dynamic,
short-term alignment effects along with stable, long-
term personality effects. We use it to investigate
the following questions: Can personality be accu-
rately judged from short, automatically generated di-
alogues? What are the effects of alignment between
characters? How is the quality of the characters? re-
lationship perceived? Additionally, in our evaluation
study we examine perceptions of the different forms
of alignment present in the dialogues, for example at
the word, phrase or polarity levels. In the following
we review relevant literature, before describing the
CRAG 2 system and experimental method, and then
presenting our results and discussion.
40
2 Background
Researchers from several traditions have studied as-
pects of similarity in dialogue, naming it: entrain-
ment, alignment, priming, accommodation, coordi-
nation or convergence. For current purposes, we
gloss over some important differences, and borrow
the term ?alignment?, because we will go on to adopt
Pickering and Garrod?s theoretical mechanisms in
our system. Alignment usually means that if some-
thing has happened once in a dialogue (for instance,
referring to an object as a vase), it is likely to happen
again?and hence, alternatives become less likely
(for instance, referring to the same object as a jug)
(Pickering and Garrod, 2004). From this view, inter-
locutors align the representations they use in produc-
tion and comprehension and the process is an auto-
matic, labour-saving device, but there are of course
limits to periods over which alignment processes op-
erate; in corpus studies long-term adaptation pre-
dicts communicative success (Reitter, 2008). Al-
ternative approaches view similarity as a process of
negotiation leading to the establishment of common
ground (Brennan and Clark, 1996), or a relatively
conscious process resulting from attraction (Shepard
et al, 2001). Although increased similarity (conver-
gence) is generally regarded positively, it can some-
times arise during disagreement (Niederhoffer and
Pennebaker, 2002), with cultural differences influ-
encing both convergence and perceptions of others
(Bortfeld and Brennan, 1997). Wizard-of-Oz stud-
ies have also shown convergence with a natural lan-
guage interface (Brennan, 1996; Porzel et al, 2006).
Embodied conversational agents (Cassell et al,
2000) are implemented computer characters that ex-
hibit multimodal behaviour; the technology can be
exploited to give life to automatically generated
scripted dialogues and to make them more engag-
ing (van Deemter et al, 2008; Hernault et al, 2008).
Aspects of the agents? personalities and their inter-
ests can be pre-configured and affect their dialogue
strategies; the generation is template-based. A com-
mon way to describe personality is using the Big
Five traits: Extraversion (preference for, and behav-
ior in, social situations); Neuroticism (tendency to
experience negative thoughts and feelings); Open-
ness (reflects openness to new ideas); Agreeableness
(how we tend to interact with others); and Consci-
entiousness (how organised and persistent we are in
pursuing our goals). Relationships between person-
ality dimensions and language use appear to be ro-
bust: For instance, in monological writing (essays
and e-mails) high Extraverts use more social words,
positive emotion words, and express more certainty;
high Agreeableness scorers use more first person
singular and positive emotion words, and fewer ar-
ticles and negative emotion words (Pennebaker and
King, 1999; Gill and Oberlander, 2002).
Personality can not only be projected through, but
also perceived from, asynchronous textual commu-
nication. The extraversion dimension is generally
perceived most accurately in a variety of contexts,
while it was more difficult for raters to recognise
neuroticism (Gill et al, 2006; Li and Chignell, 2010)
Taking into account the difference between the lan-
guage actually used by people with certain person-
ality, and the language which others expect them
to use, natural language generation (NLG) systems
can exploit either to project personality. Perhaps the
closest previous work to what we present here is the
Personality Generator (PERSONAGE) (Mairesse and
Walker, 2010) which mapped psychological find-
ings relating to the personality to the components
of the NLG system (e.g., content planning, sen-
tence planning and realisation). Evaluation by hu-
man raters showed similar accuracy in perception
of extraversion in the generated language compared
with human-authored texts. There is evidence that
computer users attribute personality to interfaces,
and rate more highly those interfaces that exploit
language associated with the user?s own personal-
ity, and become more similar to the user over time
(Isbister and Nass, 2000).
We now turn to describing our automated natu-
ral language generation techniques, implemented in
CRAG 2, followed by a description of our experi-
mental method and evaluation.
3 Generation Method
Dialogues are composed by CRAG 2, a Java pro-
gram that provides a framework for generating dia-
logues between two computer characters discussing
a movie. For more details of this system, see Brock-
mann (2009). Within CRAG 2, linguistic personal-
ity and alignment are modelled using the OPENNLP
41
CCG Library (OPENCCG) natural language realiser
(White, 2006b). The realiser consults a grammar
adapted to the movie review domain to allow the
generation of utterances about the following top-
ics: Action scenes, characters, dialogue, film, music,
plot or special effects. The realiser also has access
to a set of n-gram language models, used to com-
pute probability scores of word sequences. The gen-
eral conversational language model (LM) is based
on data from the SWITCHBOARD corpus and a small
corpus of movie reviews. The general LM is used for
fallback probabilities, and is integrated with the per-
sonality and alignment language models (described
below) using linear interpolation.
3.1 Personality Models
Language models were trained on a corpus of web-
logs from authors of known personality (Nowson et
al., 2005). For each personality dimension, the lan-
guage data were divided up into high, medium and
low bands so that the probability of a word sequence
given a personality type could be derived; see Now-
son et al (2005) for further discussion of the pos-
itively skewed distribution of the openness dimen-
sion in bloggers. Each individual weblog was used
5 times, once for each dimension. The five models
corresponding to the character?s assigned personal-
ity are uniformly interpolated to give the final per-
sonality model, which is then combined with the
general model (respective weights, 0.7 and 0.3).
3.2 Alignment via Cache Language Models
Meanwhile, alignment is modelled via cache lan-
guage models (CLMs). For each utterance to be
generated, a language model is computed based on
the utterance that was generated immediately before
it. This CLM is then combined with the personality
LM. A character?s propensity to align corresponds
to the weight given to the CLM during this combi-
nation, and can be set to a value between 0 and 1.
3.3 Character Specification and Dialogue
Generation
The characters are parameterised for their per-
sonality by specifying values (on a scale from
0 to 100) for the five dimensions: extraver-
sion (E), neuroticism (N), agreeableness (A),
conscientiousness (C) and openness (O). This pa-
rameterisation determines the extent to which utter-
ances are weighted for their overlap with the per-
sonality generation model for each trait. Also, each
character receives an agenda of topics they wish
to discuss, along with polarities (POSITIVE/NEGA-
TIVE) that indicate their opinion on each topic.
The character with the higher E score begins the
dialogue, and their first topic is selected. Once
an utterance has been generated, the other charac-
ter is selected, and the system selects which topic
should come next. This process continues until
there are no topics left on the agenda of the cur-
rent speaker. The system creates a simple XML
representation of the character?s utterance, using
the specified topic and polarity. Following the
method described in Foster and White (2004), the
basic utterance specification is transformed, using
stylesheets written in the Extensible Stylesheet Lan-
guage Transformations (XSLT) language, into an
OPENCCG logical form. We make use of the fa-
cility for defining optional and alternative inputs
(White, 2006a) and underspecified semantics to
mildly overgenerate candidate utterances.
Optional interjections (I mean, you know, sort of )
and conversational markers (right, but, and, well)
are added where appropriate given the discourse his-
tory. Using synonyms (e.g., plot = story, comedy =
humour) and combining sentence types and optional
expressions, up to 3000 possibilities are created per
utterance, and the best candidate is chosen by the
specific combination of n-gram models appropriate
for dialogue history, personality and alignment.
4 Experimental Method
4.1 Participants
Data were collected from 80 participants with a va-
riety of educational and occupational backgrounds
using an online study (via the Language Experi-
ments Portal; www.language-experiments.org). To
ensure integrity of responses, submissions taking
less than five minutes (five cases), or more than 45
minutes (one case) were examined in relation to the
other responses before being included in the analy-
sis. The demographics were as follows: 43 partici-
pants (54%) were native, and 37 (46%) non-native,
speakers of English; 34 (42%) male, 46 (58%) fe-
42
Personality Par- Propen-
Dialogue ameter Setting sity to
Type Character E N A C O Align
1) High E I 75 50 25 25 50 0
vs. Low E II 25 50 75 75 50 0 or 0.7
2) Low E I 25 50 25 25 50 0
vs. High E II 75 50 75 75 50 0 or 0.7
3) High N I 50 75 25 25 50 0
vs. Low N II 50 25 75 75 50 0 or 0.7
4) Low N I 50 25 25 25 50 0
vs. High N II 50 75 75 75 50 0 or 0.7
Table 1: Dialogue type parameter settings.
male. Median age range was 25?29 (mode = 20?
24). Other demographic information (right/left-
handedness, area of upbringing, occupation) were
collected, but are not considered here.
4.2 Materials
To be able to compare human judges? perceptions
of characters demonstrating different personalities,
and dialogues without and with alignment, dialogues
were generated in four different dialogue types, as
shown in Table 1. Each dialogue type sets the two
computer characters to opposing extremes on either
the E or the N dimension, while keeping the respec-
tive other dimension at a middle, or neutral, level
(for example, in Dialogue Type 1, Character I is
High E, Character II is Low E, and both charac-
ters are Mid N). Furthermore, Character I is always
Low A and C, and Character II is always High A and
C. All characters are set to Mid O.
Two dialogues were generated per type, giving a
total of 8 dialogues, with aligning versions of each of
these dialogues subsequently generated (giving 16
dialogues in total). The movie under discussion and
the characters? respective agendas and their opinions
about the topics were randomly assigned. Each dia-
logue was eight utterances long, with characters tak-
ing turns, each of them producing four utterances
altogether. In each alignment dialogue, the High
A/High C Character II aligned. The weight for the
cache language model was set to 0.7. In both align-
ing and non-aligning versions of the dialogues, ut-
terances for the non-aligning speaker were the same.
The generation of utterances for the aligning speaker
was seeded with the respective previous utterance
functioning as the dialogue history. From the list
of generated possible utterances, the top-ranked ut-
terance was chosen.
4.2.1 Example Dialogue
To give an impression of the generated dialogues,
Table 2 shows an example of Dialogue Type 1
(High E versus Low E) where the characters dis-
cuss the movie Mystic River (the first row of Ta-
ble 1 gives the full parameter settings). The other
generation parameters are (valence of opinions fol-
lows each topic): Character I, agenda (PLOT/?,
CHARACTERS/?, MUSIC/?, FILM/?); further opin-
ions (SPECIAL EFFECTS/?, ACTION SCENES/+,
DIALOGUE/?); Character II, agenda (ACTION
SCENES/+, SPECIAL EFFECTS/+, PLOT/?, DI-
ALOGUE/?); further opinions (CHARACTERS/?,
FILM/?, MUSIC/?).
Alignment is switched on for Character II, with
examples of their utterances from the non-aligning
dialogues included beneath. As can be seen, the
aligned utterances closely match the surface form
of their primes. The personality language models?
influence on language choice can be seen when the
High E Character I uses the phrase I mean I didn?t
think, while the (non-aligning) Low E Character II
prefers I (kind of) thought throughout. Finally, the
Low A/Low C Character I has a strategy of switch-
ing topics in order to follow his own agenda.
4.3 Procedure
An introductory web page explained the task and de-
scribed the four personality dimensions of interest
(E, N, A and C; adjectives relating to each personal-
ity type were included for reference after each dia-
logue). The first page also contained an example di-
alogue similar to those that would be displayed dur-
ing the experiment. The participants were then pre-
sented with eight dialogues (two per type; one with,
one without alignment). The dialogues were pre-
sented in random order and randomly chosen from
the pool of available dialogues. One complete di-
alogue was shown at a time, and the participants
were asked to judge which of the two characters
scored higher on the E, N, A and C dimensions (us-
ing the previous personality descriptions and adjec-
tives). They also answered questions probing their
43
PLOT:
I ?I mean I didn?t think that the story was great.?
II ?I mean the story wasn?t great.?
Without alignment:
?Yes I mean I thought that it wasn?t nicely tied up.?
CHARACTERS:
I ?And I mean I thought there was no characterization at all.?
II ?I mean I thought there was no characterization at all.?
Without alignment:
?Yeah there was no characterization at all I mean.?
MUSIC:
I ?I mean I didn?t think that the music was great.?
II ?I mean the music wasn?t great.?
Without alignment:
?Yeah I kind of thought that the music was also very bad
you know.?
FILM:
I ?I mean I didn?t think the film was good.?
II ?I mean the film wasn?t good.?
Without alignment:
?I mean I kind of thought that it was a bit dull.?
Table 2: Example Dialogue.
perceptions of the characters? relationship. They as-
sessed on a seven-point Likert scale how well the
characters ?got on? with each other (very badly?very
well), interpreted as indicating positivity or rapport
between characters, and how smoothly the conver-
sation went (not at all smoothly?very smoothly), in-
dicating how natural and coherent the interactions
were. The participants were asked to rate each dia-
logue independently from the others.The experiment
was open to both native and non-native speakers of
English; upon supplying an email address, partici-
pants were entered into a draw for an Amazon gift
token. All data were analysed anonymously. Note
that this is a further evaluation of data previously
presented in Brockmann (2009).
5 Experimental Results
5.1 Personality perception
To study the perception of personality in our di-
alogues, a nominal logistic regression was run on
the perception ratings obtained from the judges.
Here agreement between generated personality and
rater judgements was coded as a binary value
(agreement=1; disagreement=0), and entered into
the regression model as the dependent variable
(DV). The following independent variables (IVs)
were entered into the model: Dialogue Alignment as
a binary variable (alignment=1; no alignment=0);
Personality Trait judged as a categorical variable
(?Extraversion?, ?Neuroticism??, ?Agreeableness?,
?Conscientiousness?). We also included an inter-
action variable, Generated Alignment ? Personality
Trait Rated. We ran this model in order to under-
stand how each of the independent variables, such
as Personality Trait judged, or combinations of vari-
ables (in the case of the interactions) best explain the
accuracy of the personality perception judgements
relative to our generated personality language (the
DV). Throughout this section we report the parame-
ter estimates and corresponding one degree of free-
dom for the more conservative Likelihood Ratio Chi
Square effect tests for N=1920 (with the exception
of the four-level variable, Personality Trait DF=3,
and Participant ID DF=79).
The whole model is significant (?2 = 128.22,
p < .0041, R Square (U)= .05; although note that
R Square (U) is not comparable to regular R Square,
and therefore cannot be interpreted as a percentage
of variance explained; model DF= 89). To investi-
gate effects of native/non-native speaker effects on
personality judgement accuracy, this variable was
included in earlier models as a binary variable (Na-
tive Speaker: native=1; non-native=0), but no sig-
nificant effect was found (?2 = 0.98, p = .3228).
Therefore data from all participants are included in
the analyses here, and the native/non-native variable
is not included in the model. For the interactions,
there is a significant relationship between Dialogue
Alignment and accuracy in judgement of Personal-
ity Trait (?2 = 13.67, p = .0034). Further exami-
nation of this relationship shows that in the case of
Agreeableness, accuracy decreases when alignment
is present in the dialogue (?2 = 10.90, p = .0010),
whereas in the case of Conscientiousness, percep-
tion accuracy significantly increases with alignment
(?2 = 4.38, p= .0364). This is shown in Figure 1.
There is a significant main effect for Personal-
ity Trait judged (?2 = 17.04, p = .0007): param-
eter estimates show that accuracy of judgement is
significantly more accurate for Extraversion (?2 =
7.21, p = .0073), but less accurate for Agreeable-
ness (?2 = 5.54, p = .0186) and Conscientiousness
(?2 = 8.09, p = .0044). No main effect was found
for Dialogue Alignment relative to accuracy of per-
sonality judgement (?2 = 2.16, p= .1420).
44
A C E N
Personality Trait
Agr
eem
ent 
(Ge
ner
ated
 Pe
rson
ality
 vs.
 Ra
ter 
Jud
gem
ents
)
0.0
0.2
0.4
0.6
0.8
1.0 ?
?
No AlignmentAlignment
Figure 1: Accuracy of personality judgements.
5.2 Ratings of ?Getting on? and ?Smoothness?
In the following we are interested in examining what
dialogue characteristics lead to the rater judgements
of ?getting on?. Using an ordinal logistic regression
(DV: how well the characters were judged to ?get
on?, seven point scale from ?very badly? to ?very
well?) the following independent variables, coded as
described in the previous section, were entered into
the model: Dialogue Alignment and Native Speaker
(Personality Trait was also entered into the model,
but did not reach significance). Participant ID was
included in the model to account for the repeated
measures design. Again, we use likelihood ratio
effect tests and note parameter estimates for one
degree of freedom (N=2560). The whole model
is significant (?2 = 1396.75, p < .0001, R Square
(U)= .15; model DF=89): A main effect for Dia-
logue Alignment (?2 = 244.94, p < .0001), shows
alignment decreased perceptions of ?getting on?.
Similarly, ordinal logistic regressions were used
to probe influencing factors in decisions of rating
dialogue smoothness (DV: smoothness rated on a
seven point scale from ?not at all smoothly? to ?very
smoothly?). The following independent variables,
coded as described in the previous section, were en-
tered into the model: Dialogue Alignment and Na-
tive Speaker (again Personality Trait did not reach
significance for inclusion). Again, Participant ID
was included in the model to account for the re-
peated measures design (parameter estimates and
likelihood ratio effect tests are for one degree of
freedom, N=2560, Condition, DF=3; Participant
ID, DF=78). The whole model is significant (?2 =
1291.28, p < .0001), with an R Square (U) of 0.13
(model DF=89). There are strong main effects for
Dialogue Alignment (?2 = 188.27, p < .0001), and
Native Speaker (?2= 110.00, p< .0001). Examina-
tion of the parameter estimates reveals negative rela-
tionships between ratings of smoothness and Native
Speaker, and Dialogue Alignment, implying that na-
tive speakers significantly rated the dialogues as be-
ing less smooth than the non-native speakers, and
also that dialogues with alignment were rated sig-
nificantly less smooth than those without alignment.
6 Evaluation Method
To better understand the linguistic alignment pro-
cesses which drive the participants? judgements in
the previous experiment, we performed further anal-
ysis. In particular, we coded the forms of alignment
present in each utterance of each dialogue, relative
to the previous utterance. The forms of alignment
were coded as follows: Polarity (matching a posi-
tive or negative opinion), Topic (whether the topic is
the same or shifts), Word (instances of alignment of
individual words of the previous utterance), Phrase
(alignment of phrases), Construction (alignment at
a grammatical construction level). Each instance of
alignment for a given utterance was counted, with
an overall score generated for the whole dialogue.
This coding procedure was performed by one re-
searcher and subsequently evaluated by a second,
with disputes resolved by mutual agreement. In the
following analysis we do not distinguish between di-
alogues intentionally generated with alignment and
those without, but instead include all dialogues in
the analysis to examine which objectively measured
forms of alignment relate to the judges? perceptions
for personality, ?getting on? and ?smoothness?.
7 Evaluation Results
7.1 Alignment Forms and Personality
Accuracy of judgements of personality ratings and
dialogue alignment was analysed for each of the four
45
personality traits (A, C, E, N) independently using
nominal logistic regression (DV: rater vs. gener-
ated personality agreement coded 0 or 1; IVs: occur-
rence scores for Polarity, Topic, Word, Phrase, and
Construction). For Agreeableness the whole model
is significant (?2 = 85.74, p < .0001, R Square
(U)= .10; model DF=5, N=640), with Topic align-
ment (?2 = 16.68, p < .0001), followed by Polar-
ity (?2 = 10.13, p= .0015) and Construction (?2 =
6.19, p = .0128) alignment all positively related to
perceptions of Agreeableness. For Conscientious-
ness (whole model ?2 = 11.26, p= .0465, R Square
(U)= .01; DF=5, N=640), Polarity alignment is in-
versely related to perceptions of Conscientiousness
(?2 = 5.12, p = .0236). In the case of Neuroti-
cism and Extraversion, the models are not significant
(?2 = 5.37, p = .3719, and ?2 = 1.49, p = .2226,
respectively; both DF=5, N=320).
7.2 Alignment Forms and ?Getting On? and
?Smoothness?
The relationship between the different forms of
alignment present in the dialogues and the judges?
ratings of ?getting on? and ?smoothness? were eval-
uated in two separate ordinal logistic models, in
which they were entered as the dependent variable.
The five alignment types (Polarity, Topic, Word,
Phrase, and Construction) were entered as indepen-
dent variables. Participant ID was also entered into
the model as an independent variable, since multiple
responses were collected from each participant.
Ratings of ?getting on? (whole model ?2 =
1595.10, p < .0001, R Square (U)= .17; DF=84,
N=2560) show that Polarity (?2 = 385.45, p <
.0001), Construction (?2 = 72.30, p < .0001) and
Topic (?2= 16.68, p= .0014) alignment all relate to
greater scores of perceived getting on. Conversely,
Word alignment leads to reduced scores of perceived
getting on (?2 = 14.13, p = .0002). For ratings of
dialogue ?smoothness? (?2 = 1519.31, p= .0014, R
Square (U)= .16; DF=84, N=2560), again Polarity
(?2 = 209.55, p < .0001), Topic (?2 = 39.39, p <
.0001) and Construction (?2 = 28.01, p < .0001)
alignment all lead to increased ratings of ?smooth-
ness?. Similarly, Word alignment has a negative
impact upon perceptions of dialogue ?smoothness?
(?2 = 29.24, p < .0001).
8 Discussion
We now discuss the perception and evaluation re-
sults of the CRAG 2 system in greater detail. In
terms of personality perception, extraversion is ac-
curately perceived, with agreeableness and consci-
entiousness less so, which matches findings from
personality perception studies in other contexts, in-
cluding text based computer-mediated communica-
tion (Li and Chignell, 2010; Gill et al, 2006). It
is interesting to note, however, that alignment helps
perception of conscientiousness, but hurts ratings of
agreeableness. Reduced accuracy in perception of
agreeableness, which is important to relationships,
may have a negative impact on the use of dialogues
in collaborative settings (Rammstedt and Schupp,
2008). Further work could usefully examine ways in
which these characteristics can be generated in more
readily perceptible ways. Interestingly, personality
perception is unaffected by whether the judges are
native English speakers or not. This is a notable
finding, and apparently implies that the social infor-
mation relating to personality is available in the text
only environment, or through the generation pro-
cess, it is equally accessible to native and non-native
English speakers. Native speaking judges were more
critical in rating dialogue smoothness and characters
getting on, perhaps indicating a finer-grained aware-
ness of linguistic cues in interpersonal interaction,
or else just greater confidence in making negative
judgements of their native language.
Our finding that our generated alignment actually
decreases the perceived positivity of the relationship
is contrary to what is generally predicted by the lit-
erature (Brennan and Clark, 1996; Shepard et al,
2001; Pickering and Garrod, 2004); but cf. Nieder-
hoffer and Pennebaker (2002). Likewise, we would
also have expected the dialogues with alignment to
have been perceived to have gone more smoothly.
However, in our evaluation of the different types
of alignment, we note that alignment per se is not
necessarily a bad thing: Generally alignment of Po-
larity, Topic, and Construction are seen positively
leading to higher ratings of ?getting on?, ?smooth-
ness?, and increased accurate perception of Agree-
ableness; repetition of individual words is however
viewed negatively, and leads to lower ratings of ?get-
ting on? and ?smoothness?.
46
There are a number of possible explanations for
these negative responses to our generated dialogue
alignment. They hinge on understanding what is
involved in generating alignment, or similar be-
haviour, in dialogue participants. First, it could be
that our dialogues encode the ?wrong? type of simi-
larity. For example, the alignment and entrainment
approaches to similarity usually study task-based di-
alogues, which often focus on establishing a shared
vocabulary for referencing objects (i.e., at the word
level). In such cases, the similarity arises either
through priming mechanisms, or the establishment
of common ground. Given that we used an align-
ment model to generate similarity in our dialogues,
this kind of repetition or similarity may seem incon-
gruent or out of place in dialogues that are not task-
based (cf. negative impact of word-level alignment).
A second explanation might be that similarity re-
lates to positive outcomes when it occurs over a
longer, rather than shorter, period of time (Reit-
ter, 2008). In the current study the dialogues con-
sisted of eight turns, thus similarity was not gener-
ated over a long period. Indeed, linguistic similarity
over a longer period of time may be more consis-
tent with perceptions of social similarity, such as in-
group, rather than outgroup, membership (Shepard
et al, 2001). Indeed, in such contexts word choice
is an important feature in dialogue and would be use-
ful to incorporate into a dialogue model to simulate
ingroup membership.
Third, in communication accommodation theory
it is ?convergence? ? the process of increasing sim-
ilarity between interlocutors ? which is important,
rather than similarity alone. In the current study,
convergence was not examined since the dialogues
were generated with static levels of alignment.
So how do these findings relate back to the area of
dialogue generation for applied contexts? Similarly
to findings for the PERSONAGE system (Mairesse
and Walker, 2010), personality in our generated di-
alogues is perceived with similar accuracy to the
way humans perceive personality of other humans.
This suggests that our CRAG 2 system can create
believable characters to whom the user can poten-
tially relate while auditing the dialogues, or using a
dialogue-based interface. That alignment can have
negative effects on dialogue perception we propose
is due to the form of alignment depicted in these gen-
erated dialogues (i.e., task-based nature emphasising
similarity at the word level), rather than alignment in
general. We do not take this result to necessarily in-
dicate that alignment in generated dialogues should
be avoided. Rather, its implementation should be
carefully considered, especially to ensure that the
form of similarity achieved makes sense in the com-
municative context. Indeed, as we show in the eval-
uation of the generated dialogues, alignment at the
Polarity, Topic, and Construction levels is gener-
ally viewed positively, however in contrast align-
ment at the Word level tends to be viewed more neg-
atively. One of the key suggestions arising from this
study is that the different forms of dialogue simi-
larity cannot simply be used interchangeably, with
alignment found in task-based dialogues which may
include many instances of word-level repetition and
alignment not necessarily appropriate in non-task
dialogues, and thus not automatically resulting in
perceptions of positivity. We note that non-native
speakers were more forgiving in their ratings of the
dialogues containing alignment. Given that they
were equally able to perceive the personality of the
characters, this may be due to non-native speakers
having fewer expectations of alignment behaviour
in dialogue. Indeed in some contexts, greater align-
ment, and thus repetition, may be beneficial for non-
native speakers auditing dialogues.
To conclude, personality in our generated dia-
logues was perceived with comparable accuracy to
human texts, but alignment or similarity between
speakers ? especially at the word level ? regarded
negatively. We would like to see future work exam-
ine further the responses to different forms of align-
ment, including convergence, in generated dialogue.
9 Acknowledgements
We acknowledge Edinburgh-Stanford Link funding,
and the partial support of the Future and Emerging
Technologies programme FP7-COSI-ICT of the Eu-
ropean Commission (project QLectives, grant no.:
231200). We thank Amy Isard, Scott Nowson and
Michael White for their assistance in this work. A
version of the paper was presented at the Twentieth
Society for Text and Discourse conference; thanks
to Herb Clark, Max Louwerse and Michael Schober
for their insights regarding linguistic similarity.
47
References
[Bortfeld and Brennan1997] H. Bortfeld and S. E. Bren-
nan. 1997. Use and acquisition of idiomatic expres-
sions in referring by native and non-native speakers.
Discourse Processes, 23:119?147.
[Brennan and Clark1996] Susan E. Brennan and Her-
bert H. Clark. 1996. Conceptual pacts and lexi-
cal choice in conversation. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
22(6):1482?1493, November.
[Brennan1996] Susan E. Brennan. 1996. Lexical entrain-
ment in spontaneous dialog. In International Sympo-
sium on Spoken Dialog, pages 41?44.
[Brockmann2009] Carsten Brockmann. 2009. Personal-
ity and Alignment Processes in Dialogue: Towards a
Lexically-Based Unified Model. Ph.D. thesis, Univer-
sity of Edinburgh, UK.
[Cassell et al2000] Justine Cassell, Joseph Sullivan, Scott
Prevost, and Elizabeth Churchill, editors. 2000. Em-
bodied Conversational Agents. MIT Press, Cam-
bridge, MA, USA.
[Foster and White2004] Mary Ellen Foster and Michael
White. 2004. Techniques for text planning with
XSLT. In Proceedings of the 4th Workshop on NLP
and XML (NLPXML-04) at the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 1?8, Barcelona, Spain.
[Gill and Oberlander2002] Alastair J. Gill and Jon Ober-
lander. 2002. Taking care of the linguistic features of
extraversion. In Proceedings of the 24th Annual Con-
ference of the Cognitive Science Society (CogSci2002),
pages 363?368, Fairfax, VA, USA.
[Gill et al2006] Alastair J. Gill, Jon Oberlander, and Eliz-
abeth Austin. 2006. Rating e-mail personality at zero
acquaintance. Personality and Individual Differences,
40(3):497?507.
[Hernault et al2008] Hugo Hernault, Paul Piwek, Helmut
Prendinger, and Mitsuru Ishizuka. 2008. Generating
dialogues for virtual agents using nested textual coher-
ence relations. In Proceedings of Intelligent Virtual
Agents, pages 139?145.
[Isbister and Nass2000] Katherine Isbister and Clifford
Nass. 2000. Consistency of personality in inter-
active characters: verbal cues, non-verbal cues, and
user characteristics. International Journal of Human?
Computer Studies, 53(2):251?267.
[Li and Chignell2010] J. Li and M. Chignell. 2010. Birds
of a feather: How personality influences blog writ-
ing and reading. Int. J. Human-Computer Studies,
68:589?602.
[Mairesse and Walker2010] Franc?ois Mairesse and Mari-
lyn Walker. 2010. Towards personality-based user
adaptation: Psychologically informed stylistic lan-
guage generation. User Modeling and User-Adapted
Interaction, 20(3):227?278.
[Niederhoffer and Pennebaker2002] Kate G. Niederhof-
fer and James W. Pennebaker. 2002. Linguistic style
matching in social interaction. Journal of Language
and Social Psychology, 21(4):337?360.
[Nowson et al2005] S. Nowson, J. Oberlander, and A.J.
Gill. 2005. Weblogs, genres and individual differ-
ences. In Proceedings of the 27th Annual Conference
of the Cognitive Science Society, pages 1666?1671.
[Pennebaker and King1999] James W. Pennebaker and
Laura A. King. 1999. Linguistic styles: Language
use as an individual difference. Journal of Personality
and Social Psychology, 77(6):1296?1312.
[Pickering and Garrod2004] Martin J. Pickering and Si-
mon Garrod. 2004. Toward a mechanistic psychol-
ogy of dialogue. Behavioral and Brain Sciences,
27(2):169?225.
[Porzel et al2006] Robert Porzel, Annika Scheffler, and
Rainer Malaka. 2006. How entrainment increases di-
alogical efficiency. In Proceedings of Workshop on on
Effective Multimodal Dialogue Interfaces.
[Rammstedt and Schupp2008] Beatrice Rammstedt and
Ju?rgen Schupp. 2008. Only the congruent survive ?
personality similarities in couples. Personality and In-
dividual Differences, 45(6):533?535.
[Reitter2008] David Reitter. 2008. Context Effects in
Language Production: Models of Syntactic Priming in
Dialogue Corpora. Ph.D. thesis, University of Edin-
burgh, UK.
[Shepard et al2001] Carolyn A. Shepard, Howard Giles,
and Beth A. Le Poire. 2001. Communication accom-
modation theory. In W. Peter Robinson and Howard
Giles, editors, The New Handbook of Language and
Social Psychology, chapter 1.2, pages 33?56. JohnWi-
ley & Sons, Chichester, UK.
[van Deemter et al2008] Kees van Deemter, Brigitte
Krenn, Paul Piwek, Martin Klesen, Marc Schro?der,
and Stefan Baumann. 2008. Fully generated scripted
dialogue for embodied agents. Artificial Intelligence,
172(10):1219?1244.
[White2006a] Michael White. 2006a. CCG chart real-
ization from disjunctive inputs. In Proceedings of the
4th International Natural Language Generation Con-
ference (INLG-06), pages 9?16, Sydney, Australia.
[White2006b] Michael White. 2006b. Efficient realiza-
tion of coordinate structures in Combinatory Catego-
rial Grammar. Research on Language and Computa-
tion, 4(1):39?75.
48
