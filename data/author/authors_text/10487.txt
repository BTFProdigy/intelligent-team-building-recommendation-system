Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385?392
Manchester, August 2008
Word Lattice Reranking for Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? ? Haitao Mi ? ? Qun Liu ?
?Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
?Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{jiangwenbin,htmi,liuqun}@ict.ac.cn
Abstract
In this paper, we describe a new rerank-
ing strategy named word lattice reranking,
for the task of joint Chinese word segmen-
tation and part-of-speech (POS) tagging.
As a derivation of the forest reranking
for parsing (Huang, 2008), this strategy
reranks on the pruned word lattice, which
potentially contains much more candidates
while using less storage, compared with
the traditional n-best list reranking. With a
perceptron classifier trained with local fea-
tures as the baseline, word lattice rerank-
ing performs reranking with non-local fea-
tures that can?t be easily incorporated into
the perceptron baseline. Experimental re-
sults show that, this strategy achieves im-
provement on both segmentation and POS
tagging, above the perceptron baseline and
the n-best list reranking.
1 Introduction
Recent work for Chinese word segmentation and
POS tagging pays much attention to discriminative
methods, such as Maximum Entropy Model (ME)
(Ratnaparkhi and Adwait, 1996), Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001), percep-
tron training algorithm (Collins, 2002), etc. Com-
pared to generative ones such as Hidden Markov
Model (HMM) (Rabiner, 1989; Fine et al, 1998),
discriminative models have the advantage of flexi-
bility in representing features, and usually obtains
almost perfect accuracy in two tasks.
Originated by Xue and Shen (2003), the typ-
ical approach of discriminative models conducts
c
? 2008. Licensed to the Coling 2008 Organizing Com-
mittee for publication in Coling 2008 and for re-publishing in
any form or medium.
segmentation in a classification style, by assign-
ing each character a positional tag indicating its
relative position in the word. If we extend these
positional tags to include POS information, seg-
mentation and POS tagging can be performed by a
single pass under a unify classification framework
(Ng and Low, 2004). In the rest of the paper, we
call this operation mode Joint S&T. Experiments
of Ng and Low (2004) shown that, compared with
performing segmentation and POS tagging one at
a time, Joint S&T can achieve higher accuracy not
only on segmentation but also on POS tagging.
Besides the usual local features such as the
character-based ones (Xue and Shen, 2003; Ng
and Low, 2004), many non-local features related
to POSs or words can also be employed to improve
performance. However, as such features are gener-
ated dynamically during the decoding procedure,
incorporating these features directly into the clas-
sifier results in problems. First, the classifier?s fea-
ture space will grow much rapidly, which is apt to
overfit on training corpus. Second, the variance of
non-local features caused by the model evolution
during the training procedure will hurt the param-
eter tuning. Last but not the lest, since the cur-
rent predication relies on the results of prior predi-
cations, exact inference by dynamic programming
can?t be obtained, and then we have to maintain a
n-best candidate list at each considering position,
which also evokes the potential risk of depress-
ing the parameter tuning procedure. As a result,
many theoretically useful features such as higher-
order word- or POS- grams can not be utilized ef-
ficiently.
A widely used approach of using non-local
features is the well-known reranking technique,
which has been proved effective in many NLP
tasks, for instance, syntactic parsing and machine
385
v0
v
1
v
2
v
3
v
4
v
5
v
6
v
7
C
1
:e
C
2
:? C
3
:U
C
4
:/ C
5
:? C
6
:?
C
7
:Y
NN
VV
NN
M
NN
NN
NN
NN
VV
NN
NN
Figure 1: Pruned word lattice as directed graph. The character sequence we choose is ?e-?-U-/-
?-?-Y?. For clarity, we represent each subsequence-POS pair as a single edge, while ignore the
corresponding scores of the edges.
translation (Collins, 2000; Huang, 2008), etc. Es-
pecially, Huang (2008) reranked the packed for-
est, which contains exponentially many parses.
Inspired by his work, we propose word lattice
reranking, a strategy that reranks the pruned word
lattice outputted by a baseline classifier, rather than
only a n-best list. Word lattice, a directed graph as
shown in Figure 1, is a packed structure that can
represent many possibilities of segmentation and
POS tagging. Our experiments on the Penn Chi-
nese Treebank 5.0 show that, reranking on word
lattice gains obvious improvement over the base-
line classifier and the reranking on n-best list.
Compared against the baseline, we obtain an error
reduction of 11.9% on segmentation, and 16.3%
on Joint S&T.
2 Word Lattice
Formally, a word lattice L is a directed graph
?V,E?, where V is the node set, and E is the
edge set. Suppose the word lattice is for sentence
C
1:n
= C
1
..C
n
, node v
i
? V (i = 1..n ? 1) de-
notes the position between C
i
and C
i+1
, while v
0
before C
1
is the source node, and v
n
after C
n
is
the sink node. An edge e ? E departs from v
b
and
arrives at v
e
(0 ? b < e ? n), it covers a subse-
quence of C
1:n
, which is recognized as a possible
word. Considering Joint S&T, we label each edge
a POS tag to represent a word-POS pair. A series
of adjoining edges forms a path, and a path con-
necting the source node and the sink node is called
diameter, which indicates a specific pattern of seg-
mentation and POS tagging. For a diameter d, |d|
denotes the length of d, which is the count of edges
contained in this diameter. In Figure 1, the path
p
?
= v?
0
v
3
? v?
3
v
5
? v?
5
v
7
is a diameter, and
|p
?
| is 3.
2.1 Oracle Diameter in Lattice
Given a sentence s, its reference r and pruned
word lattice L generated by the baseline classi-
fier, the oracle diameter d? of L is define as the
diameter most similar to r. With F-measure as the
scoring function, we can identify d? using the al-
gorithm depicted in Algorithm 1, which is adapted
to lexical analysis from the forest oracle computa-
tion of Huang (2008).
Before describe this algorithm in detail, we de-
pict the key point for finding the oracle diameter.
Given the system?s output y and the reference y?,
using |y| and |y?| to denote word counts of them
respectively, and |y ? y?| to denote matched word
count of |y| and |y?|, F-measure can be computed
by:
F (y, y
?
) =
2PR
P + R
=
2|y ? y
?
|
|y| + |y
?
|
(1)
Here, P = |y?y
?
|
|y|
is precision, and R = |y?y
?
|
|y
?
|
is recall. Notice that F (y, y?) isn?t a linear func-
tion, we need access the largest |y ? y?| for each
possible |y| in order to determine the diameter with
maximum F , or another word, we should know the
maximum matched word count for each possible
diameter length.
The algorithm shown in Algorithm 1 works in
a dynamic programming manner. A table node
T [i, j] is defined for sequence span [i, j], and it has
a structure S to remember the best |y
i:j
? y
?
i:j
| for
each |y
i:j
|, as well as the back pointer for this best
choice. The for-loop in line 2 ? 14 processes for
each node T [i, j] in a shorter-span-first order. Line
3? 7 initialize T [i, j] according to the reference r
and the word lattice?s edge set L ?E. If there exists
an edge e in L ?E covering the span [i, j], then we
386
Algorithm 1 Oracle Diameter, U la Huang (2008,
Sec. 4.1).
1: Input: sentence s, reference r and lattice L
2: for [i, j] ? [1, |s|] in topological order do
3: if ?e ? L ? E s.t. e spans from i to j then
4: if e ? label exists in r then
5: T [i, j] ? S[1]? 1
6: else
7: T [i, j] ? S[1]? 0
8: for k s.t. T [i, k ? 1] and T [k, j] defined do
9: for p s.t. T [i, k ? 1] ? S[p] defined do
10: for q s.t. T [k, j] ? S[q] defined do
11: n? T [i, k ? 1] ? S[p] + T [k, j] ? S[q]
12: if n > T [i, j] ? S[p + q] then
13: T [i, j] ? S[p + q]? n
14: T [i, j] ? S[p + q] ? bp? ?k, p, q?
15: t? argmax
t
2?T [1,|s|]?S[t]
t+|r|
16: d? ? Tr(T [1, |s|] ? S[t].bp)
17: Output: oracle diameter: d?
define T [i, j], otherwise we leave this node unde-
fined. In the first situation, we initialize this node?s
S structure according to whether the word-POS
pair of e is in the reference (line 4?7). Line 8?14
update T [i, j]?s S structure using the S structures
from all possible child-node pair, T [i, k ? 1] and
T [k, j]. Especially, line 9? 10 enumerate all com-
binations of p and q, where p and q each repre-
sent a kind of diameter length in T [i, k ? 1] and
T [k, j]. Line 12 ? 14 refreshes the structure S
of node T [i, j] when necessary, and meanwhile,
a back pointer ?k, p, q? is also recorded. When
the dynamic programming procedure ends, we se-
lect the diameter length t of the top node T [1, |s|],
which maximizes the F-measure formula in line
15, then we use function Tr to find the oracle di-
ameter d? by tracing the back pointer bp.
2.2 Generation of the Word Lattice
We can generate the pruned word lattice using the
baseline classifier, with a slight modification. The
classifier conducts decoding by considering each
character in a left-to-right fashion. At each consid-
ering position i, the classifier enumerates all can-
didate results for subsequence C
1:i
, by attaching
each current candidate word-POS pair p to the tail
of each candidate result at p?s prior position, as
the endmost of the new generated candidate. We
give each p a score, which is the highest, among
all C
1:i
?s candidates that have p as their endmost.
Then we select N word-POS pairs with the high-
est scores, and insert them to the lattice?s edge set.
This approach of selecting edges implies that, for
the lattice?s node set, we generate a node v
i
at each
position i. Because N is the limitation on the count
Algorithm 2 Lattice generation algorithm.
1: Input: character sequence C
1:n
2: E ? ?
3: for i? 1 .. n do
4: cands? ?
5: for l? 1 .. min(i, K) do
6: w ? C
i?l+1:i
7: for t ? POS do
8: p? ?w, t?
9: p ? score? Eval(p)
10: s? p ? score + Best[i? l]
11: Best[i]? max(s,Best[i])
12: insert ?s, p? into cands
13: sort cands according to s
14: E ? E ? cands[1..N ] ? p
15: Output: edge set of lattice: E
of edges that point to the node at position i, we call
this pruning strategy in-degree pruning. The gen-
eration algorithm is shown in Algorithm 2.
Line 3 ? 14 consider each character C
i
in se-
quence, cands is used to keep the edges closing at
position i. Line 5 enumerates the candidate words
ending with C
i
and no longer than K, where K
is 20 in our experiments. Line 5 enumerates all
POS tags for the current candidate word w, where
POS denotes the POS tag set. Function Eval in
line 9 returns the score for word-POS pair p from
the baseline classifier. The array Best preserve the
score for sequence C
1:i
?s best labelling results. Af-
ter all possible word-POS pairs (or edges) consid-
ered, line 13? 14 select the N edges we want, and
add them to edge set E.
Though this pruning strategy seems relative
rough ? simple pruning for edge set while no
pruning for node set, we still achieve a promising
improvement by reranking on such lattices. We be-
lieve more elaborate pruning strategy will results
in more valuable pruned lattice.
3 Reranking
A unified framework can be applied to describing
reranking for both n-best list and pruned word lat-
tices (Collins, 2000; Huang, 2008). Given the can-
didate set cand(s) for sentence s, the reranker se-
lects the best item y? from cand(s):
y? = argmax
y?cand(s)
w ? f(y) (2)
For reranking n-best list, cand(s) is simply the set
of n best results from the baseline classifier. While
for reranking word lattice, cand(s) is the set of
all diameters that are impliedly built in the lattice.
w ? f(y) is the dot product between a feature vec-
tor f and a weight vector w, its value is used to
387
Algorithm 3 Perceptron training for reranking
1: Input: Training examples{cand(s
i
), y
?
i
}
N
i=1
2: w? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: y? ? argmax
y?cand(s
i
)
w ? f(y)
6: if y? 6= y?
i
then
7: w? w + f(y?
i
)? f(y?)
8: Output: Parameters: w
Non-local Template Comment
W
0
T
0
current word-POS pair
W
?1
word 1-gram before W
0
T
0
T
?1
POS 1-gram before W
0
T
0
T
?2
T
?1
POS 2-gram before W
0
T
0
T
?3
T
?2
T
?1
POS 3-gram before W
0
T
0
Table 1: Non-local feature templates used for
reranking
rerank cand(s). Following usual practice in pars-
ing, the first feature f
1
(y) is specified as the score
outputted by the baseline classifier, and its value
is a real number. The other features are non-local
ones such as word- and POS- n-grams extracted
from candidates in n-best list (for n-best rerank-
ing) or diameters (for word lattice reranking), and
they are 0 ? 1 valued.
3.1 Training of the Reranker
We adopt the perceptron algorithm (Collins, 2002)
to train the reranker. as shown in Algorithm 3. We
use a simple refinement strategy of ?averaged pa-
rameters? of Collins (2002) to alleviate overfitting
on the training corpus and obtain more stable per-
formance.
For every training example {cand(s
i
), y
?
i
}, y
?
i
denotes the best candidate in cand(s
i
). For n-
best reranking, the best candidate is easy to find,
whereas for word lattice reranking, we should use
the algorithm in Algorithm 1 to determine the or-
acle diameter, which represents the best candidate
result.
3.2 Non-local Feature Templates
The non-local feature templates we use to train the
reranker are listed in Table 1. Notice that all fea-
tures generated from these templates don?t contain
?future? words or POS tags, it means that we only
use current or history word- or POS- n-grams to
evaluate the current considering word-POS pair.
Although it is possible to use ?future? information
in n-best list reranking, it?s not the same when we
rerank the pruned word lattice. As we have to tra-
verse the lattice topologically, we face difficulty in
Algorithm 4 Cube pruning for non-local features.
1: function CUBE(L)
2: for v ? L ? V in topological order do
3: NBEST(v)
4: return D
v
sink
[1]
5: procedure NBEST(v)
6: heap? ?
7: for v? topologically before v do
8: ?? all edges from v? to v
9: p? ?D
v
?
,??
10: ?p,1??score? Eval(p,1)
11: PUSH(?p,1?, heap)
12: HEAPIFY(heap)
13: buf ? ?
14: while |heap| > 0 and |buf | < N do
15: item? POP-MAX(heap)
16: append item to buf
17: PUSHSUCC(item, heap)
18: sort buf to D
v
19: procedure PUSHSUCC(?p, j?, heap)
20: p is ?vec
1
,vec
2
?
21: for i? 1..2 do
22: j? ? j+ bi
23: if |vec
i
| ? j
?
i
then
24: ?p, j???score? Eval(p, j?)
25: PUSH(?p, j??, heap)
utilizing the information ahead of the current con-
sidering node.
3.3 Reranking by Cube Pruning
Because of the non-local features such as word-
and POS- n-grams, the reranking procedure is sim-
ilar to machine translation decoding with inter-
grated language models, and should maintain a
list of N best candidates at each node of the lat-
tice. To speed up the procedure of obtaining the
N best candidates, following Huang (2008, Sec.
3.3), we adapt the cube pruning method from ma-
chine translation (Chiang, 2007; Huang and Chi-
ang 2007) which is based on efficient k-best pars-
ing algorithms (Huang and Chiang, 2005).
As shown in Algorithm 4, cube pruning works
topologically in the pruned word lattice, and main-
tains a list of N best derivations at each node.
When deducing a new derivation by attaching a
current word-POS pair to the tail of a antecedent
derivation, a function Eval is used to compute the
new derivation?s score (line 10 and 24). We use
a max-heap heap to hold the candidates for the
next-best derivation. Line 7 ? 11 initialize heap
to the set of top derivations along each deducing
source, the vector pair ?D
v
head
,??.Here, ? de-
notes the vector of current word-POS pairs, while
D
v
head
denotes the vector of N best derivations
at ??s antecedent node. Then at each iteration,
388
Non-lexical-target Instances
C
n
(n = ?2..2) C
?2
=e, C
?1
=?, C
0
=U, C
1
=/, C
2
=?
C
n
C
n+1
(n = ?2..1) C
?2
C
?1
=e?, C
?1
C
0
=?U, C
0
C
1
=U/, C
1
C
2
=/?
C
?1
C
1
C
?1
C
1
=?/
Lexical-target Instances
C
0
C
n
(n = ?2..2) C
0
C
?2
=Ue, C
0
C
?1
=U?, C
0
C
0
=UU, C
0
C
1
=U/, C
0
C
2
=U?
C
0
C
n
C
n+1
(n = ?2..1) C
0
C
?2
C
?1
=Ue?, C
0
C
?1
C
0
=U?U, C
0
C
0
C
1
=UU/, C
0
C
1
C
2
=U/?
C
0
C
?1
C
1
C
0
C
?1
C
1
= U?/
Table 2: Feature templates and instances. Suppose we consider the third character ?U? in the sequence
?e?U/??.
we pop the best derivation from heap (line 15),
and push its successors into heap (line 17), until
we get N derivations or heap is empty. In line 22
of function PUSHSUCC, j is a vector composed of
two index numbers, indicating the two candidates?
indexes in the two vectors of the deducing source
p, where the two candidates are selected to deduce
a new derivation. j? is a increment vector, whose
ith dimension is 1, while others are 0. As non-
local features (word- and POS- n-grams) are used
by function Eval to compute derivation?s score,
the derivations extracted from heap may be out of
order. So we use a buffer buf to keep extracted
derivations (line 16), then sort buf and put its first
N items to D
v
(line 18).
4 Baseline Perceptron Classifier
4.1 Joint S&T as Classification
Following Jiang et al (2008), we describe segmen-
tation and Joint S&T as below:
For a given Chinese sentence appearing as a
character sequence:
C
1:n
= C
1
C
2
.. C
n
the goal of segmentation is splitting the sequence
into several subsequences:
C
1:e
1
C
e
1
+1:e
2
.. C
e
m?1
+1:e
m
While in Joint S&T, each of these subsequences is
labelled a POS tag:
C
1:e
1
/t
1
C
e
1
+1:e
2
/t
2
.. C
e
m?1
+1:e
m
/t
m
Where C
i
(i = 1..n) denotes a character, C
l:r
(l ?
r) denotes the subsequence ranging from C
l
to C
r
,
and t
i
(i = 1..m,m ? n) denotes the POS tag of
C
e
i?1
+1:e
i
.
If we label each character a positional tag in-
dicating its relative position in an expected subse-
quence, we can obtain the segmentation result ac-
cordingly. As described in Ng and Low (2004) and
Jiang et al (2008), we use s indicating a single-
character word, while b, m and e indicating the be-
gin, middle and end of a word respectively. With
these positional tags, the segmentation transforms
to a classification problem. For Joint S&T, we
expand positional tags by attaching POS to their
tails as postfix. As each tag now contains both
positional- and POS- information, Joint S&T can
also be resolved in a classification style frame-
work. It means that, a subsequence is a word with
POS t, only if the positional part of the tag se-
quence conforms to s or bm?e pattern, and each
element in the POS part equals to t. For example,
a tag sequence b NN m NN e NN represents a
three-character word with POS tag NN .
4.2 Feature Templates
The features we use to build the classifier are gen-
erated from the templates of Ng and Low (2004).
For convenience of comparing with other, they
didn?t adopt the ones containing external knowl-
edge, such as punctuation information. All their
templates are shown in Table 2. C denotes a char-
acter, while its subscript indicates its position rela-
tive to the current considering character(it has the
subscript 0).
The table?s upper column lists the templates that
immediately from Ng and Low (2004). they
named these templates non-lexical-target because
predications derived from them can predicate with-
out considering the current character C
0
. Tem-
plates called lexical-target in the column below are
introduced by Jiang et al (2008). They are gener-
ated by adding an additional field C
0
to each non-
lexical-target template, so they can carry out pred-
ication not only according to the context, but also
according to the current character itself.
Notice that features derived from the templates
in Table 2 are all local features, which means all
features are determined only by the training in-
stances, and they can be generated before the train-
ing procedure.
389
Algorithm 5 Perceptron training algorithm.
1: Input: Training examples (x
i
, y
i
)
2: ?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: z
i
? argmax
z?GEN(x
i
)
?(x
i
, z) ? ?
6: if z
i
6= y
i
then
7: ?? ? +?(x
i
, y
i
)??(x
i
, z
i
)
8: Output: Parameters: ?
4.3 Training of the Classifier
Collins (2002)?s perceptron training algorithm
were adopted again, to learn a discriminative clas-
sifier, mapping from inputs x ? X to outputs
y ? Y . Here x is a character sequence, and y is
the sequence of classification result of each char-
acter in x. For segmentation, the classification re-
sult is a positional tag, while for Joint S&T, it is
an extended tag with POS information. X denotes
the set of character sequence, while Y denotes the
corresponding set of tag sequence.
According to Collins (2002), the function
GEN(x) generates all candidate tag sequences for
the character sequence x , the representation ?
maps each training example (x, y) ? X ? Y to
a feature vector ?(x, y) ? Rd, and the parameter
vector ? ? Rd is the weight vector corresponding
to the expected perceptron model?s feature space.
For a given input character sequence x, the mission
of the classifier is to find the tag sequence F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ? (3)
The inner product ?(x, y) ? ? is the score of the
result y given x, it represents how much plausibly
we can label character sequence x as tag sequence
y. The training algorithm is depicted in Algorithm
5. We also use the ?averaged parameters? strategy
to alleviate overfitting.
5 Experiments
Our experiments are conducted on the Penn Chi-
nese Treebank 5.0 (CTB 5.0). Following usual
practice of Chinese parsing, we choose chapters
1?260 (18074 sentences) as the training set, chap-
ters 301? 325 (350 sentences) as the development
set, and chapters 271 ? 300 (348 sentences) as
the final test set. We report the performance of
the baseline classifier, and then compare the per-
formance of the word lattice reranking against the
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
ur
e
number of iterations
Perceptron Learning Curves
Segmentation
Joint ST
Figure 2: Baseline averaged perceptron learning
curves for segmentation and Joint S&T.
n-best reranking, based on this baseline classifier.
For each experiment, we give accuracies on seg-
mentation and Joint S&T. Analogous to the situa-
tion in parsing, the accuracy of Joint S&T means
that, a word-POS is recognized only if both the
positional- and POS- tags are correctly labelled for
each character in the word?s span.
5.1 Baseline Perceptron Classifier
The perceptron classifier are trained on the train-
ing set using features generated from the templates
in Table 2, and the development set is used to
determine the best parameter vector. Figure 2
shows the learning curves for segmentation and
Joint S&T on the development set. We choose
the averaged parameter vector after 7 iterations for
the final test, this parameter vector achieves an F-
measure of 0.973 on segmentation, and 0.925 on
Joint S&T. Although the accuracy on segmentation
is quite high, it is obviously lower on Joint S&T.
Experiments of Ng and Low (2004) on CTB 3.0
also shown the similar trend, where they obtained
F-measure 0.952 on segmentation, and 0.919 on
Joint S&T.
5.2 Preparation for Reranking
For n-best reranking, we can easily generate n best
results for every training instance, by a modifica-
tion for the baseline classifier to hold n best can-
didates at each considering point. For word lattice
reranking, we use the algorithm in Algorithm 2 to
generate the pruned word lattice. Given a training
instance s
i
, its n best result list or pruned word
lattice is used as a reranking instance cand(s
i
),
the best candidate result (of the n best list) or or-
acle diameter (of the pruned word lattice) is the
reranking target y?
i
. We find the best result of the
n best results simply by computing each result?s
390
F-measure, and we determine the oracle diame-
ter of the pruned word lattice using the algorithm
depicted in Algorithm 1. All pairs of cand(s
i
)
and y?
i
deduced from the baseline model?s training
instances comprise the training set for reranking.
The development set and test set for reranking are
obtained in the same way. For the reranking train-
ing set {cand(s
i
), y
?
i
}
N
i=1
, {y
?
i
}
N
i=1
is called oracle
set, and the F-measure of {y?
i
}
N
i=1
against the ref-
erence set is called oracle F-measure. We use the
oracle F-measure indicating the utmost improve-
ment that an reranking algorithm can achieve.
5.3 Results and Analysis
The flows of the n-best list reranking and the
pruned word lattice reranking are similar to the
training procedure for the baseline classifier. The
training set for reranking is used to tune the param-
eter vector of the reranker, while the development
set for reranking is used to determine the optimal
number of iterations for the reranker?s training pro-
cedure.
We compare the performance of the word lat-
tice reranking against the n-best list reranking. Ta-
ble 3 shows the experimental results. The up-
per four rows are the experimental results for n-
best list reranking, while the four rows below are
for word lattice reranking. In n-best list rerank-
ing, with list size 20, the oracle F-measure on
Joint S&T is 0.9455, and the reranked F-measure
is 0.9280. When list size grows up to 50, the oracle
F-measure on Joint S&T jumps to 0.9552, while
the reranked F-measure becomes 0.9302. How-
ever, when n grows to 100, it brings tiny improve-
ment over the situation of n = 50. In word lat-
tice reranking, there is a trend similar to that in
n-best reranking, the performance difference be-
tween in degree = 2 and in degree = 5 is ob-
vious, whereas the setting in degree = 10 does
not obtain a notable improvement over the perfor-
mance of in degree = 5. We also notice that even
with a relative small in degree limitation, such as
in degree = 5, the oracle F-measures for seg-
mentation and Joint S&T both reach a quite high
level. This indicates the pruned word lattice con-
tains much more possibilities of segmentation and
tagging, compared to n-best list.
With the setting in degree = 5, the oracle F-
measure on Joint S&T reaches 0.9774, and the
reranked F-measure climbs to 0.9336. It achieves
an error reduction of 16.3% on Joint S&T, and an
error reduction of 11.9% on segmentation, over the
n-best Ora Seg Tst Seg Ora S&T Tst S&T
20 0.9827 0.9749 0.9455 0.9280
50 0.9903 0.9754 0.9552 0.9302
100 0.9907 0.9755 0.9558 0.9305
Degree Ora Seg Rnk Seg Ora S&T Rnk S&T
2 0.9898 0.9753 0.9549 0.9296
5 0.9927 0.9774 0.9768 0.9336
10 0.9934 0.9774 0.9779 0.9337
Table 3: Performance of n-best list reranking and
word lattice reranking. n-best: the size of the n-
best list for n-best list reranking; Degree: the in de-
gree limitation for word lattice reranking; Ora Seg:
oracle F-measure on segmentation of n-best lists or
word lattices; Ora S&T: oracle F-measure on Joint
S&T of n-best lists or word lattices; Rnk Seg: F-
measure on segmentation of reranked result; Rnk
S&T: F-measure on Joint S&T of reranked result
baseline classifier. While for n-best reranking with
setting n = 50, the Joint S&T?s error reduction is
6.9% , and the segmentation?s error reduction is
8.9%. We can see that reranking on pruned word
lattice is a practical method for segmentation and
POS tagging. Even with a much small data rep-
resentation, it obtains obvious advantage over the
n-best list reranking.
Comparing between the baseline and the two
reranking techniques, We find the non-local infor-
mation such as word- or POS- grams do improve
accuracy of segmentation and POS tagging, and
we also find the reranking technique is effective to
utilize these kinds of information. As even a small
scale n-best list or pruned word lattice can achieve
a rather high oracle F-measure, reranking tech-
nique, especially the word lattice reranking would
be a promising refining strategy for segmentation
and POS tagging. This is based on this viewpoint:
On the one hand, compared with the initial input
character sequence, the pruned word lattice has a
quite smaller search space while with a high ora-
cle F-measure, which enables us to conduct more
precise reranking over this search space to find the
best result. On the other hand, as the structure of
the search space is approximately outlined by the
topological directed architecture of pruned word
lattice, we have a much wider choice for feature se-
lection, which means that we would be able to uti-
lize not only features topologically before the cur-
rent considering position, just like those depicted
in Table 2 in section 4, but also information topo-
logically after it, for example the next word W
1
or
the next POS tag T
1
. We believe the pruned word
391
lattice reranking technique will obtain higher im-
provement, if we develop more precise reranking
algorithm and more appropriate features.
6 Conclusion
This paper describes a reranking strategy called
word lattice reranking. As a derivation of the for-
est reranking of Huang (2008), it performs rerank-
ing on pruned word lattice, instead of on n-best
list. Using word- and POS- gram information, this
reranking technique achieves an error reduction of
16.3% on Joint S&T, and 11.9% on segmentation,
over the baseline classifier, and it also outperforms
reranking on n-best list. It confirms that word lat-
tice reranking can effectively use non-local infor-
mation to select the best candidate result, from a
relative small representation structure while with a
quite high oracle F-measure. However, our rerank-
ing implementation is relative coarse, and it must
have many chances for improvement. In future
work, we will develop more precise pruning al-
gorithm for word lattice generation, to further cut
down the search space while maintaining the ora-
cle F-measure. We will also investigate the feature
selection strategy under the word lattice architec-
ture, for effective use of non-local information.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108. We show our special thanks to
Liang Huang for his valuable suggestions.
References
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
17th International Conference on Machine Learn-
ing, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Fine, Shai, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. In Machine Learning, pages 32?
41.
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics.
Jiang, Wenbin, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 23rd International Con-
ference on Machine Learning, pages 282?289, Mas-
sachusetts, USA.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Rabiner, Lawrence. R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?
286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
392
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206?214,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Forest-based Translation Rule Extraction
Haitao Mi1
1Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
htmi@ict.ac.cn
Liang Huang2,1
2Dept. of Computer & Information Science
University of Pennsylvania
3330 Walnut St., Levine Hall
Philadelphia, PA 19104, USA
lhuang3@cis.upenn.edu
Abstract
Translation rule extraction is a fundamental
problem in machine translation, especially for
linguistically syntax-based systems that need
parse trees from either or both sides of the bi-
text. The current dominant practice only uses
1-best trees, which adversely affects the rule
set quality due to parsing errors. So we pro-
pose a novel approach which extracts rules
from a packed forest that compactly encodes
exponentially many parses. Experiments show
that this method improves translation quality
by over 1 BLEU point on a state-of-the-art
tree-to-string system, and is 0.5 points better
than (and twice as fast as) extracting on 30-
best parses. When combined with our previous
work on forest-based decoding, it achieves a
2.5 BLEU points improvement over the base-
line, and even outperforms the hierarchical
system of Hiero by 0.7 points.
1 Introduction
Automatic extraction of translation rules is a funda-
mental problem in statistical machine translation, es-
pecially for many syntax-based models where trans-
lation rules directly encode linguistic knowledge.
Typically, these models extract rules using parse
trees from both or either side(s) of the bitext. The
former case, with trees on both sides, is often called
tree-to-tree models; while the latter case, with trees
on either source or target side, include both tree-
to-string and string-to-tree models (see Table 1).
Leveraging from structural and linguistic informa-
tion from parse trees, these models are believed
to be better than their phrase-based counterparts in
source target examples (partial)
tree-to-tree Ding and Palmer (2005)
tree-to-string Liu et al (2006); Huang et al (2006)
string-to-tree Galley et al (2006)
string-to-string Chiang (2005)
Table 1: A classification of syntax-based MT. The first
three use linguistic syntax, while the last one only formal
syntax. Our experiments cover the second type using a
packed forest in place of the tree for rule-extraction.
handling non-local reorderings, and have achieved
promising translation results.1
However, these systems suffer from a major limi-
tation, that the rule extractor only uses 1-best parse
tree(s), which adversely affects the rule set quality
due to parsing errors. To make things worse, mod-
ern statistical parsers are often trained on domains
quite different from those used in MT. By contrast,
formally syntax-based models (Chiang, 2005) do not
rely on parse trees, yet usually perform better than
these linguistically sophisticated counterparts.
To alleviate this problem, an obvious idea is to
extract rules from k-best parses instead. However, a
k-best list, with its limited scope, has too few vari-
ations and too many redundancies (Huang, 2008).
This situation worsens with longer sentences as the
number of possible parses grows exponentially with
the sentence length and a k-best list will only capture
a tiny fraction of the whole space. In addition, many
subtrees are repeated across different parses, so it is
1For example, in recent NIST Evaluations, some of these
models (Galley et al, 2006; Quirk et al, 2005; Liu et al, 2006)
ranked among top 10. See http://www.nist.gov/speech/tests/mt/.
206
IP
NP
x1:NPB CC
yu?
x2:NPB
x3:VPB ? x1 x3 with x2
Figure 1: Example translation rule r1. The Chinese con-
junction yu? ?and? is translated into English prep. ?with?.
also inefficient to extract rules separately from each
of these very similar trees (or from the cross-product
of k2 similar tree-pairs in tree-to-tree models).
We instead propose a novel approach that ex-
tracts rules from packed forests (Section 3), which
compactly encodes many more alternatives than k-
best lists. Experiments (Section 5) show that forest-
based extraction improves BLEU score by over 1
point on a state-of-the-art tree-to-string system (Liu
et al, 2006; Mi et al, 2008), which is also 0.5
points better than (and twice as fast as) extracting
on 30-best parses. When combined with our previ-
ous orthogonal work on forest-based decoding (Mi
et al, 2008), the forest-forest approach achieves a
2.5 BLEU points improvement over the baseline,
and even outperforms the hierarchical system of Hi-
ero, one of the best-performing systems to date.
Besides tree-to-string systems, our method is also
applicable to other paradigms such as the string-to-
tree models (Galley et al, 2006) where the rules are
in the reverse order, and easily generalizable to pairs
of forests in tree-to-tree models.
2 Tree-based Translation
We review in this section the tree-based approach to
machine translation (Liu et al, 2006; Huang et al,
2006), and its rule extraction algorithm (Galley et
al., 2004; Galley et al, 2006).
2.1 Tree-to-String System
Current tree-based systems perform translation in
two separate steps: parsing and decoding. The input
string is first parsed by a parser into a 1-best tree,
which will then be converted to a target language
string by applying a set of tree-to-string transforma-
tion rules. For example, consider the following ex-
ample translating from Chinese to English:
(a) Bu`sh?? yu? Sha?lo?ng ju?x??ng le hu?`ta?n
? 1-best parser(b) IP
NP
NPB
Bu`sh??
CC
yu?
NPB
Sha?lo?ng
VPB
VV
ju?x??ng
AS
le
NPB
hu?`ta?n
r1?
(c) NPB
Bu`sh??
VPB
VV
ju?x??ng
AS
le
NPB
hu?`ta?n
with NPB
Sha?lo?ng
r2 ? r3 ?
(d) Bush held NPB
hu?`ta?n
with NPB
Sha?lo?ng
r4 ? r5 ?
(e) Bush held a meeting with Sharon
r2 NPB(Bu`sh??)? Bush
r3 VPB(VV(ju?x??ng) AS(le) x1:NPB)? held x1
r4 NPB(Sha?lo?ng)? Sharon
r5 NPB(hu?`ta?n)? a meeting
Figure 2: Example derivation of tree-to-string translation,
with rules used. Each shaded region denotes a tree frag-
ment that is pattern-matched with the rule being applied.
(1) Bu`sh??
Bush
yu?
and/with
Sha?lo?ng
Sharon1
ju?x??ng
hold
le
past.
hu?`ta?n
meeting2
?Bush held a meeting2 with Sharon1?
Figure 2 shows how this process works. The Chi-
nese sentence (a) is first parsed into a parse tree (b),
which will be converted into an English string in 5
steps. First, at the root node, we apply rule r1 shown
in Figure 1, which translates the Chinese coordina-
tion construction (?... and ...?) into an English prepo-
sitional phrase. Then, from step (c) we continue ap-
plying rules to untranslated Chinese subtrees, until
we get the complete English translation in (e).2
2We swap the 1-best and 2-best parses of the example sen-
tence from our earlier paper (Mi et al, 2008), since the current
1-best parse is easier to illustrate the rule extraction algorithm.
207
IP
?Bush .. Sharon?
NP
?Bush ? with Sharon?
NPB
?Bush?
Bu`sh??
CC
?with?
yu?
NPB
?Sharon?
Sha?lo?ng
VPB
?held .. meeting?
VV
?held?
ju?x??ng
AS
?held?
le
NPB
?a meeting?
hu?`ta?n
(minimal) rules extracted
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? x1 x4 x2 x3
CC (yu?)? with
NPB (Bu`sh??)? Bush
NPB (Sha?lo?ng)? Sharon
VPB (VV(ju?x??ng) AS(le) x1:NPB)
? held x1
NPB (hu?`ta?n)? a meeting
Bush held a meeting with Sharon
Figure 3: Tree-based rule extraction (Galley et al, 2004). Each non-leaf node in the tree is annotated with its target
span (below the node), where ? denotes a gap, and non-faithful spans are crossed out. Shadowed nodes are admissible,
with contiguous and faithful spans. The first two rules can be ?composed? to form rule r1 in Figure 1.
IP0, 6
?Bush .. Sharon?
e2
NP0, 3
?Bush ? with Sharon?
e3
NPB0, 1
?Bush?
Bu`sh??
CC1, 2
?with?
yu?
VP1, 6
?held .. Sharon?
PP1, 3
?with Sharon?
P1, 2
?with?
NPB2, 3
?Sharon?
Sha?lo?ng
VPB3, 6
?held .. meeting?
VV3, 4
?held?
ju?x??ng
AS4, 5
?held?
le
NPB5, 6
?a meeting?
hu?`ta?n
e1
extra (minimal) rules extracted
IP (x1:NPB x2:VP)? x1 x2
VP (x1:PP x2:VPB)? x2 x1
PP (x1:P x2:NPB)? x1 x2
P (yu?)? with
Bush held a meeting with Sharon
Figure 4: Forest-based rule extraction. Solid hyperedges correspond to the 1-best tree in Figure 3, while dashed hyper-
edges denote the alternative parse interpreting yu? as a preposition in Figure 5.
More formally, a (tree-to-string) translation rule
(Galley et al, 2004; Huang et al, 2006) is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-
side tree fragment, whose internal nodes are la-
beled by nonterminal symbols (like NP and VP),
and whose frontier nodes are labeled by source-
language words (like ?yu??) or variables from a set
X = {x1, x2, . . .}; rhs(r) is the target-side string
expressed in target-language words (like ?with?) and
variables; and ?(r) is a mapping from X to nonter-
minals. Each variable xi ? X occurs exactly once in
lhs(r) and exactly once in rhs(r). For example, for
rule r1 in Figure 1,
lhs(r1) = IP ( NP(x1 CC(yu?) x2) x3),
rhs(r1) = x1 x3 with x2,
?(r1) = {x1: NPB, x2: NPB, x3: VPB}.
These rules are being used in the reverse direction of
the string-to-tree transducers in Galley et al (2004).
208
2.2 Tree-to-String Rule Extraction
We now briefly explain the algorithm of Galley et al
(2004) that can extract these translation rules from a
word-aligned bitext with source-side parses.
Consider the example in Figure 3. The basic idea
is to decompose the source (Chinese) parse into a se-
ries of tree fragments, each of which will form a rule
with its corresponding English translation. However,
not every fragmentation can be used for rule extrac-
tion, since it may or may not respect the alignment
and reordering between the two languages. So we
say a fragmentation is well-formed with respect to
an alignment if the root node of every tree fragment
corresponds to a contiguous span on the target side;
the intuition is that there is a ?translational equiva-
lence? between the subtree rooted at the node and
the corresponding target span. For example, in Fig-
ure 3, each node is annotated with its corresponding
English span, where the NP node maps to a non-
contiguous one ?Bush ? with Sharon?.
More formally, we need a precise formulation
to handle the cases of one-to-many, many-to-one,
and many-to-many alignment links. Given a source-
target sentence pair (?, ?) with alignment a, the (tar-
get) span of node v is the set of target words aligned
to leaf nodes yield(v) under node v:
span(v) , {?i ? ? | ??j ? yield(v), (?j , ?i) ? a}.
For example, in Figure 3, every node in the parse tree
is annotated with its corresponding span below the
node, where most nodes have contiguous spans ex-
cept for the NP node which maps to a gapped phrase
?Bush ? with Sharon?. But contiguity alone is not
enough to ensure well-formedness, since there might
be words within the span aligned to source words
uncovered by the node. So we also define a span s
to be faithful to node v if every word in it is only
aligned to nodes dominated by v, i.e.:
??i ? s, (?j , ?i) ? a? ?j ? yield(v).
For example, sibling nodes VV and AS in the tree
have non-faithful spans (crossed out in the Figure),
because they both map to ?held?, thus neither of
them can be translated to ?held? alone. In this case,
a larger tree fragment rooted at VPB has to be
extracted. Nodes with non-empty, contiguous, and
faithful spans form the admissible set (shaded nodes
IP0,6
NPB0,1
Bu`sh??
VP1,6
PP1,3
P1,2
yu?
NPB2,3
Sha?lo?ng
VPB3,6
ju?x??ng le hu?`ta?n
Figure 5: An alternative parse of the Chinese sentence,
with yu? as a preposition instead of a conjunction; com-
mon parts shared with 1-best parse in Fig. 3 are elided.
in the figure), which serve as potential cut-points for
rule extraction.3
With the admissible set computed, rule extraction
is as simple as a depth-first traversal from the root:
we ?cut? the tree at all admissible nodes to form tree
fragments and extract a rule for each fragment, with
variables matching the admissible descendant nodes.
For example, the tree in Figure 3 is cut into 6 pieces,
each of which corresponds to a rule on the right.
These extracted rules are called minimal rules,
which can be glued together to form composed rules
with larger tree fragments (e.g. r1 in Fig. 1) (Galley
et al, 2006). Our experiments use composed rules.
3 Forest-based Rule Extraction
We now extend tree-based extraction algorithm from
the previous section to work with a packed forest
representing exponentially many parse trees.
3.1 Packed Forest
Informally, a packed parse forest, or forest in
short, is a compact representation of all the deriva-
tions (i.e., parse trees) for a given sentence under
a context-free grammar (Earley, 1970; Billot and
Lang, 1989). For example, consider again the Chi-
nese sentence in Example (1) above, which has
(at least) two readings depending on the part-of-
speech of the word yu?: it can be either a conjunction
(CC ?and?) as shown in Figure 3, or a preposition
(P ?with?) as shown in Figure 5, with only PP and
VPB swapped from the English word order.
3Admissible set (Wang et al, 2007) is also known as ?fron-
tier set? (Galley et al, 2004). For simplicity of presentation, we
assume every target word is aligned to at least one source word;
see Galley et al (2006) for handling unaligned target words.
209
These two parse trees can be represented as a
single forest by sharing common subtrees such as
NPB0, 1 and VPB3, 6, as shown in Figure 4. Such a
forest has a structure of a hypergraph (Huang and
Chiang, 2005), where items like NP0, 3 are called
nodes, whose indices denote the source span, and
combinations like
e1 : IP0, 6 ? NPB0, 3 VP3, 6
we call hyperedges. We denote head(e) and tails(e)
to be the consequent and antecedant items of hyper-
edge e, respectively. For example,
head(e1) = IP0, 6, tails(e1) = {NPB0, 3,VP3, 6}.
We also denote BS (v) to be the set of incoming hy-
peredges of node v, being different ways of deriving
it. For example, in Figure 4, BS (IP0, 6) = {e1, e2}.
3.2 Forest-based Rule Extraction Algorithm
Like in tree-based extraction, we extract rules from
a packed forest F in two steps:
(1) admissible set computation (where to cut), and
(2) fragmentation (how to cut).
It turns out that the exact formulation developed
for admissible set in the tree-based case can be ap-
plied to a forest without any change. The fragmen-
tation step, however, becomes much more involved
since we now face a choice of multiple parse hyper-
edges at each node. In other words, it becomes non-
deterministic how to ?cut? a forest into tree frag-
ments, which is analogous to the non-deterministic
pattern-match in forest-based decoding (Mi et al,
2008). For example there are two parse hyperedges
e1 and e2 at the root node in Figure 4. When we fol-
low one of them to grow a fragment, there again will
be multiple choices at each of its tail nodes. Like in
tree-based case, a fragment is said to be complete
if all its leaf nodes are admissible. Otherwise, an in-
complete fragment can grow at any non-admissible
frontier node v, where following each parse hyper-
edge at v will split off a new fragment. For example,
following e2 at the root node will immediately lead
us to two admissible nodes, NPB0, 1 and VP1, 6
(we will highlight admissible nodes by gray shades
Algorithm 1 Forest-based Rule Extraction.
Input: forest F , target sentence ? , and alignment a
Output: minimal rule setR
1: admset ? ADMISSIBLE(F, ?, a) ? admissible set
2: for each v ? admset do
3: open ? ? ? queue of active fragments
4: for each e ? BS (v) do ? incoming hyperedges
5: front ? tails(e) \ admset ? initial frontier
6: open .append(?{e}, front?)
7: while open 6= ? do
8: ?frag , front? ? open .pop() ? active fragment
9: if front = ? then
10: generate a rule r using fragment frag
11: R.append(r)
12: else ? incomplete: further expand
13: u? front .pop() ? a frontier node
14: for each e ? BS (u) do
15: front ? ? front ? (tails(e) \ admset)
16: open .append(?frag ? {e}, front ??)
in this section like in Figures 3 and 4). So this frag-
ment, frag1 = {e2}, is now complete and we can
extract a rule,
IP (x1:NPB x2:VP)? x1 x2.
However, following the other hyperedge e1
IP0, 6 ? NP0, 3 VPB3, 6
will leave the new fragment frag2 = {e1} incom-
plete with one non-admissible node NP0, 3. We then
grow frag2 at this node by choosing hyperedge e3
NP0, 3 ? NPB0, 1 CC1, 2 NPB2, 3 ,
and spin off a new fragment frag3 = {e1, e3}, which
is now complete since all its four leaf nodes are ad-
missible. We then extract a rule with four variables:
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? x1 x4 x2 x3.
This procedure is formalized by a breadth-first
search (BFS) in Pseudocode 1. The basic idea is to
visit each frontier node v, and keep a queue open
of actively growing fragments rooted at v. We keep
expanding incomplete fragments from open , and ex-
tract a rule if a complete fragment is found (line 10).
Each fragment is associated with a frontier (variable
210
front in the Pseudocode), being the subset of non-
admissible leaf nodes (recall that expansion stops at
admissible nodes). So each initial fragment along
hyperedge e is associated with an initial frontier
(line 5), front = tails(e) \ admset .
A fragment is complete if its frontier is empty
(line 9), otherwise we pop one frontier node u to
expand, spin off new fragments by following hyper-
edges of u, and update the frontier (lines 14-16), un-
til all active fragments are complete and open queue
is empty (line 7).
A single parse tree can also be viewed as a triv-
ial forest, where each node has only one incoming
hyperedge. So the Galley et al (2004) algorithm for
tree-based rule extraction (Sec. 2.2) can be consid-
ered a special case of our algorithm, where the queue
open always contains one single active fragment.
3.3 Fractional Counts and Rule Probabilities
In tree-based extraction, for each sentence pair, each
rule extracted naturally has a count of one, which
will be used in maximum-likelihood estimation of
rule probabilities. However, a forest is an implicit
collection of many more trees, each of which, when
enumerated, has its own probability accumulated
from of the parse hyperedges involved. In other
words, a forest can be viewed as a virtual weighted
k-best list with a huge k. So a rule extracted from a
non 1-best parse, i.e., using non 1-best hyperedges,
should be penalized accordingly and should have a
fractional count instead of a unit one, similar to the
E-step in EM algorithms.
Inspired by the parsing literature on pruning
(Charniak and Johnson, 2005; Huang, 2008) we pe-
nalize a rule r by the posterior probability of its tree
fragment frag = lhs(r). This posterior probability,
notated ??(frag), can be computed in an Inside-
Outside fashion as the product of three parts: the out-
side probability of its root node, the probabilities of
parse hyperedges involved in the fragment, and the
inside probabilities of its leaf nodes,
??(frag) =?(root(frag))
?
?
e ? frag
P(e)
?
?
v ? yield(frag)
?(v)
(2)
where ?(?) and ?(?) denote the outside and inside
probabilities of tree nodes, respectively. For example
in Figure 4,
??({e2, e3}) = ?(IP0, 6) ? P(e2) ? P(e3)
? ?(NPB0, 1)?(CC1, 2)?(NPB2, 3)?(VPB3, 6).
Now the fractional count of rule r is simply
c(r) = ??(lhs(r))??(TOP) (3)
where TOP denotes the root node of the forest.
Like in the M-step in EM algorithm, we now
extend the maximum likelihood estimation to frac-
tional counts for three conditional probabilities re-
garding a rule, which will be used in the experi-
ments:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
, (4)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
, (5)
P(r |root(lhs(r)))
=
c(r)
?
r?:root(lhs(r?))=root(lhs(r)) c(r?)
. (6)
4 Related Work
The concept of packed forest has been previously
used in translation rule extraction, for example in
rule composition (Galley et al, 2006) and tree bina-
rization (Wang et al, 2007). However, both of these
efforts only use 1-best parses, with the second one
packing different binarizations of the same tree in a
forest. Nevertheless we suspect that their extraction
algorithm is in principle similar to ours, although
they do not provide details of forest-based fragmen-
tation (Algorithm 1) which we think is non-trivial.
The forest concept is also used in machine transla-
tion decoding, for example to characterize the search
space of decoding with integrated language models
(Huang and Chiang, 2007). The first direct appli-
cation of parse forest in translation is our previous
work (Mi et al, 2008) which translates a packed for-
est from a parser; it is also the base system in our
experiments (see below). This work, on the other
hand, is in the orthogonal direction, where we uti-
lize forests in rule extraction instead of decoding.
211
Our experiments will use both default 1-best decod-
ing and forest-based decoding. As we will see in the
next section, the best result comes when we combine
the merits of both, i.e., using forests in both rule ex-
traction and decoding.
There is also a parallel work on extracting rules
from k-best parses and k-best alignments (Venu-
gopal et al, 2008), but both their experiments and
our own below confirm that extraction on k-best
parses is neither efficient nor effective.
5 Experiments
5.1 System
Our experiments are on Chinese-to-English trans-
lation based on a tree-to-string system similar to
(Huang et al, 2006; Liu et al, 2006). Given a 1-
best tree T , the decoder searches for the best deriva-
tion d? among the set of all possible derivations D:
d? = arg max
d?D
?0 log P(d | T ) + ?1 log Plm(?(d))
+ ?2|d|+ ?3|?(d)|
(7)
where the first two terms are translation and lan-
guage model probabilities, ?(d) is the target string
(English sentence) for derivation d, and the last two
terms are derivation and translation length penalties,
respectively. The conditional probability P(d | T )
decomposes into the product of rule probabilities:
P(d | T ) =
?
r?d
P(r). (8)
Each P(r) is in turn a product of five probabilities:
P(r) =P(r | lhs(r))?4 ? P(r | rhs(r))?5
? P(r | root(lhs(r)))?6
? Plex(lhs(r) | rhs(r))?7
? Plex(rhs(r) | lhs(r))?8
(9)
where the first three are conditional probabilities
based on fractional counts of rules defined in Sec-
tion 3.3, and the last two are lexical probabilities.
These parameters ?1 . . . ?8 are tuned by minimum
error rate training (Och, 2003) on the dev sets. We
refer readers to Mi et al (2008) for details of the
decoding algorithm.
0.240
0.242
0.244
0.246
0.248
0.250
0.252
0.254
 0  1  2  3  4  5  6
B
L
E
U
 s
co
re
average extracting time (secs/1000 sentences)
1-best
pe=2
pe=5
pe=8
k=30
forest extraction
k-best extraction
Figure 6: Comparison of extraction time and BLEU
score: forest-based vs.1-best and 30-best.
rules from... extraction decoding BLEU
1-best trees 0.24 1.74 0.2430
30-best trees 5.56 3.31 0.2488
forest: pe=8 2.36 3.40 0.2533
Pharaoh - - 0.2297
Table 2: Results with different rule extraction methods.
Extraction and decoding columns are running times in
secs per 1000 sentences and per sentence, respectively.
We use the Chinese parser of Xiong et al (2005)
to parse the source side of the bitext. Following
Huang (2008), we also modify this parser to out-
put a packed forest for each sentence, which can
be pruned by the marginal probability-based inside-
outside algorithm (Charniak and Johnson, 2005;
Huang, 2008). We will first report results trained
on a small-scaled dataset with detailed analysis, and
then scale to a larger one, where we also combine the
technique of forest-based decoding (Mi et al, 2008).
5.2 Results and Analysis on Small Data
To test the effect of forest-based rule extraction, we
parse the training set into parse forests and use three
levels of pruning thresholds: pe = 2, 5, 8.
Figure 6 plots the extraction speed and transla-
tion quality of forest-based extraction with various
pruning thresholds, compared to 1-best and 30-best
baselines. Using more than one parse tree apparently
improves the BLEU score, but at the cost of much
slower extraction, since each of the top-k trees has to
be processed individually although they share many
212
rules from ... total # on dev new rules used
1-best trees 440k 90k -
30-best trees 1.2M 130k 8.71%
forest: pe=8 3.3M 188k 16.3%
Table 3: Statistics of rules extracted from small data. The
last column shows the ratio of new rules introduced by
non 1-best parses being used in 1-best derivations.
common subtrees. Forest extraction, by contrast, is
much faster thanks to packing and produces consis-
tently better BLEU scores. With pruning threshold
pe = 8, forest-based extraction achieves a (case in-
sensitive) BLEU score of 0.2533, which is an ab-
solute improvement of 1.0% points over the 1-best
baseline, and is statistically significant using the
sign-test of Collins et al (2005) (p < 0.01). This
is also 0.5 points better than (and twice as fast as)
extracting on 30-best parses. These BLEU score re-
sults are summarized in Table 2, which also shows
that decoding with forest-extracted rules is less than
twice as slow as with 1-best rules, and only fraction-
ally slower than with 30-best rules.
We also investigate the question of how often
rules extracted from non 1-best parses are used by
the decoder. Table 3 shows the numbers of rules
extracted from 1-best, 30-best and forest-based ex-
tractions, and the numbers that survive after filter-
ing on the dev set. Basically in the forest-based case
we can use about twice as many rules as in the 1-
best case, or about 1.5 times of 30-best extraction.
But the real question is, are these extra rules really
useful in generating the final (1-best) translation?
The last row shows that 16.3% of the rules used
in 1-best derivations are indeed only extracted from
non 1-best parses in the forests. Note that this is a
stronger condition than changing the distribution of
rules by considering more parses; here we introduce
new rules never seen on any 1-best parses.
5.3 Final Results on Large Data
We also conduct experiments on a larger training
dataset, FBIS, which contains 239K sentence pairs
with about 6.9M/8.9M words in Chinese/English,
respectively. We also use a bigger trigram model
trained on the first 1/3 of the Xinhua portion of Gi-
gaword corpus. To integrate with forest-based de-
coding, we use both 1-best trees and packed forests
extract. \ decoding 1-best tree forest: pd=10
1-best trees 0.2560 0.2674
30-best trees 0.2634 0.2767
forest: pe=5 0.2679 0.2816
Hiero 0.2738
Table 4: BLEU score results trained on large data.
during both rule extraction and decoding phases.
Since the data scale is larger than the small data, we
are forced to use harsher pruning thresholds, with
pe = 5 for extraction and pd = 10 for decoding.
The final BLEU score results are shown in Ta-
ble 4. With both tree-based and forest-based decod-
ing, rules extracted from forests significantly outper-
form those extracted from 1-best trees (p < 0.01).
The final result with both forest-based extraction
and forest-based decoding reaches a BLEU score of
0.2816, outperforming that of Hiero (Chiang, 2005),
one of the best performing systems to date. These re-
sults confirm that our novel forest-based rule extrac-
tion approach is a promising direction for syntax-
based machine translation.
6 Conclusion and Future Work
In this paper, we have presented a novel approach
that extracts translation rules from a packed forest
encoding exponentially many trees, rather than from
1-best or k-best parses. Experiments on a state-of-
the-art tree-to-string system show that this method
improves BLEU score significantly, with reasonable
extraction speed. When combined with our previ-
ous work on forest-based decoding, the final result
is even better than the hierarchical system Hiero.
For future work we would like to apply this ap-
proach to other types of syntax-based translation
systems, namely the string-to-tree systems (Galley
et al, 2006) and tree-to-tree systems.
Acknowledgement
This work was funded by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108 (H. M.), and by NSF ITR EIA-
0205456 (L. H.). We would also like to thank Qun
Liu for supporting this work, and the three anony-
mous reviewers for improving the earlier version.
213
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
of ACL ?89, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the 43rd ACL, Ann Arbor, MI.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd ACL, Ann Arbor, MI.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL,
Ann Arbor, MI.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA, August.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
Columbus, OH.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Ar-
bor, MI.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
of AMTA, Honolulu, Hawaii.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of EMNLP,
Prague, Czech Rep., July.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings of IJCNLP 2005, pages
70?81.
214
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Lattice-based System Combination for Statistical Machine Translation
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
Abstract
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
1 Introduction
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al, 2001; Matusov et al, 2006;
Sim et al, 2007; Rosti et al, 2007a; Rosti et al,
2007b; Rosti et al, 2008; He et al, 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called ?skeleton?
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, ?be capa-
ble of? and ?be able to? have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as ?be capable to? and ?be able of?.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as ?be capable to?.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al, 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
1105
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
He feels like ? apples
? prefer of
is fond
(c) confusion network
he feels like apples
? prefer
is fond of
(d) lattice
Figure 1: Comparison of a confusion network and a lat-
tice.
2 Background
2.1 Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
He feels like apples
He prefer apples
He is fond of apples
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although ?feels like? has the same meaning with
?is fond of?, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ? are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as ?He is like of apples?.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, ?like? is aligned to three words: ?is?,
?fond?, and ?of?. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
lations. Note that the phrase ?is fond of? is attached
to an edge. Now, it is unlikely to obtain a translation
like ?He is like of apples?.
A lattice G = ?V,E? is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al, 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
2.2 IHMM-based Alignment Method
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al, 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
p(e
?
1
J
|e
I
1
) =
?
a
J
1
J
?
j=1
[p(a
j
|a
j?1
, I)p(e
?
j
|e
a
j
)]l (1)
Where eI
1
= (e
1
, ..., e
I
) is the backbone, e?J
1
=
(e
?
1
, ..., e
?
J
) is a hypothesis aligned to eI
1
, and aJ
1
=
(a
1
, .., a
J
) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e?
j
|e
i
) is a linear in-
terpolation of semantic similarity p
sem
(e
?
j
|e
i
) and
surface similarity p
sur
(e
?
j
|e
i
) and ? is the interpo-
lation factor:
p(e
?
j
|e
i
) = ??p
sem
(e
?
j
|e
i
)+(1??)?p
sur
(e
?
j
|e
i
) (2)
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
1106
ilarity model is given by
p
sem
(e
?
j
|e
i
) =
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
, e
i
)
?
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
)
(3)
The surface similarity model is estimated by calcu-
lating the literal matching rate:
p
sur
(e
?
j
|e
i
) = exp{? ? [s(e
?
j
, e
i
)? 1]} (4)
where s(e?
j
, e
i
) is given by
s(e
?
j
, e
i
) =
M(e
?
j
, e
i
)
max(|e
?
j
|, |e
i
|)
(5)
where M(e?
j
, e
i
) is the length of the longest matched
prefix (LMP) and ? is a smoothing factor that speci-
fies the mapping.
The distortion probability p(a
j
= i|a
j?1
= i
?
, I)
is estimated by only considering the jump distance:
p(i|i
?
, I) =
c(i? i
?
)
?
I
i=1
c(l ? i
?
)
(6)
The distortion parameters c(d) are grouped into 11
buckets, c(? ?4), c(?3), ..., c(0), ..., c(5), c(? 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
c(d) = (1 + |d? 1|)
?K
, d = ?4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p
0
of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
p(i|i
?
, I) =
{
p
0
if i = null state
(1? p
0
) ? p(i|i
?
, I) otherwise
3 Lattice-based System Combination
Model
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
E
B
= argmin
E
?
?E
?
E?E
TER(E
?
, E) (8)
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
(6) Decode the lattice using a model similar to the
log-linear model.
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
1107
4 Lattice Construction
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e?
1
e
?
3
, e
2
) should be
a phrase pair, but /e?
1
0 and /e?
3
0 are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), /e?
1
0 and /e?
3
0 be-
come continuous and the phrase pair (e?
1
e
?
3
, e
2
) can
be extracted. The procedure of reordering is called
alignment normalization
E
h
: e?
1
e
?
2
e
?
3
E
B
:
e
1
e
2
e
3
(a)
E
h
: e?
2
e
?
1
e
?
3
E
B
:
e
1
e
2
e
3
(b)
Figure 2: An example of alignment units
4.1 Alignment Normalization
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e?
2
, e
1
), (e?
1
e
?
3
,
e
2
) and (?, e
3
). (e?
1
e
?
2
e
?
3
, e
1
e
2
) is an alignment unit,
but not a minimum alignment unit.
Let a?
i
= (e?
?
i
, e?
i
) denote a minimum alignment
unit, and assume that the word string e??
i
covers words
e
?
i
1
,..., e
?
i
m
on the hypothesis side, and the word
string e?
i
covers the consecutive words e
i
1
,..., e
i
n
on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit a?
i
= (e?
?
i
, e?
i
) must observe the
following rules:
E
B
: e
1
e
2
e
3
E
h
:
e
?
1
e
?
2 (a)
e
1
e
2
e
3
e
?
2
?
e
?
1
E
B
: e
1
e
2
E
h
: e
?
1
e
?
2
e
?
3
e
1
e
2
e
?
1
e
?
3
e
?
1
e
?
2
e
?
3
(b)
E
B
: e
1
e
2
E
h
:
e
?
1
e
?
2
e
?
3
e
1
?
e
2
e
?
1
e
?
2
e
?
3
(c)
Figure 3: Different cases of null insertion
? ? e
?
i
k
? e?
?
i
, e
a
?
i
k
? e?
i
? ? e
i
k
? e?
i
, e
?
a
i
k
= null or e?
a
i
k
? e?
?
i
? ? a?
j
= (e?
?
j
, e?
j
), e?
j
= e
i
1
, ..., e
i
k
or e?
j
=
e
i
k
, ..., e
i
n
, k ? [1, n]
Where a?
i
k
denotes the position of the word in the
backbone that e?
i
k
is aligned to, and a
i
k
denotes the
position of the word in the hypothesis that e
i
k
is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
? A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
? A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
? A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
1108
e1
e
2
?
e
3
e?
?
4
e?
?
5
e?
?
6
(a)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
(b)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
(c)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
(d)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
e?
?
6
(e)
Figure 4: A toy instance of lattice construction
are inserted to the backbone. This is shown in
Figure 3 (c).
4.2 Lattice Construction Algorithm
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
? e
1
is compared with e
1
. Since they are the
same, the hypothesis arc e??
4
, which comes from
the same node with e
1
in the alignment pair,
is compared with the hypothesis arc e??
1
, which
comes from the same node with e
1
in the lat-
tice. The two hypothesis arcs are not the same,
so e??
4
is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
? e
2
is compared with ?. The lattice remains the
same. The lattice goes to the next backbone
word e
2
.
? e
2
is compared with e
2
. There is no hypothesis
arc coming from the same node with the bone
arc e
2
in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
? ? is compared with e
3
. A null backbone arc is
inserted into the lattice between e
2
and e
3
. The
hypothesis arc e??
5
is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e
3
.
? e
3
is compared with e
3
. For they are the same
and there is no hypothesis arc e??
6
in the lattice,
e?
?
6
is inserted to the lattice as in Figure 4(e).
? Both arrive at the end and it is the turn of the
next alignment pair.
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
? The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
? The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
1109
Algorithm 1 Lattice construction algorithm.
1: Input: alignment pairs {p
n
}
N
n=1
2: L? p
1
3: Unique(L)
4: for n? 2 .. N do
5: pnode = p
n
? first
6: lnode = L ? first
7: while pnode ? barcnext 6= NULL do
8: if lnode ? barcnext = NULL or pnode ?
bword = null and lnode ? bword 6= null then
9: INSERTBARC(lnode, null)
10: pnode = pnode ? barcnext
11: else
12: if pnode ? bword 6= null and lnode ?
bword = null then
13: lnode = lnode ? barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode ? harc)
then
17: INSERTHARC(lnode, pnode ?
harc)
18: pnode = pnode ? barcnext
19: lnode = lnode ? barcnext
20: Output: lattice L
? The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harc
l
} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harc
h
de-
notes one of the corresponding hypothesis arcs
in the given alignment pair. In the {harc
l
},
if there is no arc which is the same with the
harc
h
, a hypothesis arc projecting to harc
h
is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 ? 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12?13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15?19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
5 Decoding
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
log p(e/f) =
N
arc
?
i=1
log (
N
s
?
s=1
?
s
p
s
(arc))
+ ?L(e) + ?N
nullarc
(e)
+ ?N
longarc
(e) + ?N
word
(e)
(9)
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
N
arc
is the number of arcs the path of e covers,
N
s
is the number of candidate systems and ?
s
is the
weight of system s. ? is the language model weight
and L(e) is the LM log-probability. N
nullarcs
(e) is
the number of the arcs which only contain a null
word, and N
longarc
(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. ? and ? are the
corresponding weights of the numbers, respectively.
N
word
(e) is the non-null word number and ? is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by p
s
(arc). p
s
(arc) is increased by
1110
1/(k+1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al, 2007a; He et al, 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
6 Experiments
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell?s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, ? = 3; the controlling
factor for the distortion model, K = 2.
6.1 Comparison with
Confusion-network-based model
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to ? = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
Table 1: Results on the MT02 and MT05 test sets
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
Table 2: Results on the MT06 and MT08 test sets
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, ?Russia? is aligned to
?Russian? and ??s? to ?null? in the IHMM-based
model, which leads to the error translation ?Russian
1111
Source: ?dIE?h?i??dIEd?i?1??
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas company
IHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas company
Lattice: Russia ?s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
?s?. Instead, ?Russia ?s? is together aligned to ?Rus-
sian? in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to ?is? is inserted.
As a result, ?is? is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, ?is working with? are aligned to ?is combined
with?, forming a phrase pair.
6.2 Effect of Dictionary Scale
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to ? = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
Table 4: Effect of dictionary scale
6.3 Effect of Semantic Alignments
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor ?.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with ? = 0.1. When we excluded the
semantic similarity model (? = 0.0) or excluded the
surface similarity model (? = 1.0), the performance
became worse.
7 Conclusion
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
1112
MT02 MT05
BLEU% BLEU%
? = 1.0 36.41 34.92
? = 0.7 37.21 35.65
? = 0.5 36.43 35.02
? = 0.4 37.14 35.55
? = 0.3 36.75 35.66
? = 0.2 36.81 35.55
? = 0.1 37.29 35.80
? = 0.0 36.45 35.14
Table 5: Effect of semantic alignments
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al, 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
8 Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351?354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL/HLT 2008, pages 1012?1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95?100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98?107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53?
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of IEEE EACL, pages 33?40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. of ICASSP, pages
105?108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836?841.
1113
Refinements in BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Haitao Mi, Qun Liu and Shouxun Lin
Key Lab of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing China, 100080
{htmi, liuqun, sxlin}@ict.ac.cn
Abstract
Bracketing Transduction Grammar (BTG)
has been well studied and used in statistical
machine translation (SMT) with promising
results. However, there are two major issues
for BTG-based SMT. First, there is no effec-
tive mechanism available for predicting or-
ders between neighboring blocks in the orig-
inal BTG. Second, the computational cost is
high. In this paper, we introduce two re-
finements for BTG-based SMT to achieve
better reordering and higher-speed decod-
ing, which include (1) reordering heuristics
to prevent incorrect swapping and reduce
search space, and (2) special phrases with
tags to indicate sentence beginning and end-
ing. The two refinements are integrated into
a well-established BTG-based Chinese-to-
English SMT system that is trained on large-
scale parallel data. Experimental results on
the NIST MT-05 task show that the proposed
refinements contribute significant improve-
ment of 2% in BLEU score over the baseline
system.
1 Introduction
Bracket transduction grammar was proposed by Wu
(1995) and firstly employed in statistical machine
translation in (Wu, 1996). Because of its good trade-
off between efficiency and expressiveness, BTG re-
striction is widely used for reordering in SMT (Zens
et al, 2004). However, BTG restriction does not
provide a mechanism to predict final orders between
two neighboring blocks.
To solve this problem, Xiong et al (2006)
proposed an enhanced BTG with a maximum en-
tropy (MaxEnt) based reordering model (MEBTG).
MEBTG uses boundary words of bilingual phrases
as features to predict their orders. Xiong et
al. (2006) reported significant performance im-
provement on Chinese-English translation tasks in
two different domains when compared with both
Pharaoh (Koehn, 2004) and the original BTG us-
ing flat reordering. However, error analysis of the
translation output of Xiong et al (2006) reveals
that boundary words predict wrong swapping, espe-
cially for long phrases although the MaxEnt-based
reordering model shows better performance than
baseline reordering models.
Another big problem with BTG-based SMT is the
high computational cost. Huang et al (2005) re-
ported that the time complexity of BTG decoding
with m-gram language model is O(n3+4(m?1)). If a
4-gram language model is used (common in many
current SMT systems), the time complexity is as
high as O(n15). Therefore with this time complexity
translating long sentences is time-consuming even
with highly stringent pruning strategy.
To speed up BTG decoding, Huang et al (2005)
adapted the hook trick which changes the time
complexity from O(n3+4(m?1)) to O(n3+3(m?1)).
However, the implementation of the hook trick with
pruning is quite complicated. Another method to in-
crease decoding speed is cube pruning proposed by
Chiang (2007) which reduces search space signifi-
cantly.
In this paper, we propose two refinements to ad-
dress the two issues, including (1) reordering heuris-
505
tics to prevent incorrect swapping and reduce search
space using swapping window and punctuation re-
striction, and (2) phrases with special tags to indicate
beginning and ending of sentence. Experimental re-
sults show that both refinements improve the BLEU
score significantly on large-scale data.
The above refinements can be easily implemented
and integrated into a baseline BTG-based SMT sys-
tem. However, they are not specially designed for
BTG-based SMT and can also be easily integrated
into other systems with different underlying trans-
lation strategies, such as the state-of-the-art phrase-
based system (Koehn et al, 2007), syntax-based sys-
tems (Chiang et al, 2005; Marcu et al, 2006; Liu et
al., 2006).
The rest of the paper is organized as follows. In
section 2, we review briefly the core elements of
the baseline system. In section 3 we describe our
proposed refinements in detail. Section 4 presents
the evaluation results on Chinese-to-English trans-
lation based on these refinements as well as results
obtained in the NIST MT-06 evaluation exercise. Fi-
nally, we conclude our work in section 5.
2 The Baseline System
In this paper, we use Xiong et al (2006)?s sys-
tem Bruin as our baseline system. Their system has
three essential elements which are (1) a stochastic
BTG, whose rules are weighted using different fea-
tures in log-linear form, (2) a MaxEnt-based reorder-
ing model with features automatically learned from
bilingual training data, (3) a CKY-style decoder us-
ing beam search similar to that of Wu (1996). We
describe the first two components briefly below.
2.1 Model
The translation process is modeled using BTG rules
which are listed as follows
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
The lexical rule (3) is used to translate source phrase
x into target phrase y and generate a block A. The
two rules (1) and (2) are used to merge two consec-
utive blocks into a single larger block in a straight or
inverted order.
To construct a stochastic BTG, we calculate rule
probabilities using the log-linear model (Och and
Ney, 2002). For the two merging rules (1) and (2),
the assigned probability Prm(A) is defined as fol-
lows
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where ?, the reordering score of block A1 and
A2, is calculated using the MaxEnt-based reordering
model (Xiong et al, 2006) described in the next sec-
tion, ?? is the weight of ?, and 4pLM (A1,A2) is the
increment of language model score of the two blocks
according to their final order, ?LM is its weight.
For the lexical rule (3), it is applied with a proba-
bility Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|y|)?6
?p?LMLM (y) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical translation
probabilities in both directions, exp(1) and exp(|y|)
are the phrase penalty and word penalty, respec-
tively and ?s are weights of features. These features
are commonly used in the state-of-the-art systems
(Koehn et al, 2005; Chiang et al, 2005).
2.2 MaxEnt-based Reordering Model
The MaxEnt-based reordering model is defined on
two consecutive blocks A1 and A2 together with
their order o ? {straight, inverted} according to
the maximum entropy framework.
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(6)
where the functions hi ? {0, 1} are model features
and ?i are weights of the model features trained au-
tomatically (Malouf, 2002).
There are three steps to train a MaxEnt-based re-
ordering model. First, we need to extract reordering
examples from unannotated bilingual data, then gen-
erate features from these examples and finally esti-
mate feature weights.
506
For extracting reordering examples, there are two
points worth mentioning:
1. In the extraction of useful reordering examples,
there is no length limitation over blocks com-
pared with extracting bilingual phrases.
2. When enumerating all combinations of neigh-
boring blocks, a good way to keep the number
of reordering examples acceptable is to extract
smallest blocks with the straight order while
largest blocks with the inverted order .
3 Refinements
In this section we describe two refinements men-
tioned above in detail. First, we present fine-
grained reordering heuristics using swapping win-
dow and punctuation restriction. Secondly, we inte-
grate special bilingual phrases with sentence begin-
ning/ending tags.
3.1 Reordering Heuristics
We conduct error analysis of the translation out-
put of the baseline system and observe that Bruin
sometimes incorrectly swaps two large neighboring
blocks on the target side. This happens frequently
when inverted order successfully challenges straight
order by the incorrect but strong support from the
language model and the MaxEnt-based reordering
model. The reason is that only boundary words
are used as evidences by both language model and
MaxEnt-based reordering model when the decoder
selects which merging rule (straight or inverted) to
be used 1. However, statistics show that bound-
ary words are not reliable for predicting the right
order between two larger neighboring blocks. Al-
Onaizan and Papineni (2006) also proved that lan-
guage model is insufficient to address long-distance
word reordering. If a wrong inverted order is se-
lected for two large consecutive blocks, incorrect
long-distance swapping happens.
Yet another finding is that many incorrect swap-
pings are related to punctuation marks. First, the
source sequence within a pair of balanced punctua-
tion marks (quotes and parentheses) should be kept
1In (Xiong et al, 2006), the language model uses the left-
most/rightmost words on the target side as evidences while the
MaxEnt-based reordering model uses the boundary words on
both sides.
Chinese: ?? : ??????????
?????????????????
????
Bruin: urgent action , he said : ?This is a very
serious situation , we can only hope that there
will be a possibility .?
Bruin+RH: he said : ?This is a very serious sit-
uation , we can only hope that there will be the
possibility to expedite action .?
Ref: He said: ?This is a very serious situa-
tion. We can only hope that it is possible to
speed up the operation.?
Figure 1: An example of incorrect long-distance
swap. The underlined Chinese words are incorrectly
swapped to the beginning of the sentence by the
original Bruin. RH means reordering heuristics.
within the punctuation after translation. However,
it is not always true when reordering is involved.
Sometime the punctuation marks are distorted with
the enclosed words sequences being moved out.
Secondly, it is found that a series of words is fre-
quently reordered from one side of a structural mark,
such as commas, semi-colons and colons, to the
other side of the mark for long sentences contain-
ing such marks. Generally speaking, on Chinese-
to-English translation, source words are translated
monotonously relative to their adjacent punctuation
marks, which means their order relative to punctua-
tion marks will not be changed. In summary, punctu-
ation marks place a strong constraint on word order
around them.
For example, in Figure 1, Chinese words ???
??? are reordered to sentence beginning. That is
an incorrect long-distance swapping, which makes
the reordered words moved out from the balanced
punctuation marks ??? and ???, and incorrectly
precede their previous mark ???.
These incorrect swappings definitely jeopardize
the quality of translation. Here we propose two
straightforward but effective heuristics to control
and adjust the reordering, namely swapping window
and punctuation restriction.
Swapping Window (SW): It constrains block
swapping in the following way
ACTIVATE A ? ?A1, A2? IF |A1s|+ |A2s| < sws
507
where |Ais| denotes the number of words on the
source side Ais of block Ai, sws is a pre-defined
swapping window size. Any inverted reordering be-
yond the pre-defined swapping window size is pro-
hibited.
Punctuation Restriction (PR): If two neighbor-
ing blocks include any of the punctuation marks p ?
{? ? ? ? ? ? ? ? ? ? ? ?}, the two
blocks will be merged with straight order.
Punctuation marks were already used in pars-
ing (Christine Doran, 2000) and statistical machine
translation (Och et al, 2003). In (Och et al,
2003), three kinds of features are defined, all re-
lated to punctuation marks like quotes, parentheses
and commas. Unfortunately, no statistically signifi-
cant improvement on the BLEU score was reported
in (Och et al, 2003). In this paper, we consider
this problem from a different perspective. We em-
phasize that words around punctuation marks are
reordered ungrammatically and therefore we posi-
tively use punctuation marks as a hard decision to
restrict such reordering around punctuations. This
is straightforward but yet results in significant im-
provement on translation quality.
The two heuristics described above can be used
together. If the following conditions are satisfied,
we can activate the inverted rule:
|A1s|+ |A2s| < sws && P
?
(A1s
?
A2s) = ?
where P is the set of punctuation marks mentioned
above.
The two heuristics can also speed up decoding be-
cause decoding will be monotone within those spans
which are not in accordance with both heuristics.
For a sentence with n words, the total number of
spans is O(n2). If we set sws = m (m < n),
then the number of spans with monotone search is
O((n?m)2). With punctuation restriction, the non-
monotone search space will reduce further.
3.2 Phrases with Sentence Beginning/Ending
Tags
We observe that in a sentence some phrases are more
likely to be located at the beginning, while other
phrases are more likely to be at the end. This kind of
location information with regard to the phrase posi-
tion could be used for reordering. A straightforward
way to use this information is to mark the begin-
ning and ending of word-aligned sentences with ?s?
and ?/s? respectively. This idea is borrowed from
language modeling (Stolcke, 2002). The corre-
sponding tags at the source and target sentences are
aligned to each other, i.e, the beginning tag of source
sentences is aligned to the beginning tag of target
sentences, similarly for the ending tag. Figure 2
shows a word-aligned sentence pair annotated with
the sentence beginning and ending tag.
During training, the sentence beginning and end-
ing tags (?s? and ?/s?) are treated as words. There-
fore the phrase extraction and MaxEnt-based re-
ordering training algorithm need not to be modified.
Phrases with the sentence beginning/ending tag will
be extracted and MaxEnt-based reordering features
with such tags will also be generated. For example,
from the word-aligned sentence pair in Figure 2, we
can extract tagged phrases like
?s??? ||| ?s? Tibet ?s
?? ?/s? ||| achievements ?/s?
and generate MaxEnt-based reordering features with
tags like
hi(o, b1, b2) =
{ 1, b2.t1 = ?/s?, o = s
0, otherwise
where b1, b2 are blocks, t1 denotes the last source
word, o = s means the order between two blocks
is straight. To avoid wrong alignments, we remove
tagged phrases where only the beginning/ending tag
is extracted on either side of the phrases, such as
?s? ||| ?s? Those?
?/s? ||| ?/s?
During decoding, we first annotate source sen-
tences with the beiginning/ending tags, then trans-
late them as what Bruin does. Note that phrases
with sentence beginning/ending tags will be used in
the same way as ordinary phrases without such tags
during decoding. With the additional support of lan-
guage model and MaxEnt-based reordering model,
we observe that phrases with such tags are always
moved to the beginning or ending of sentences cor-
rectly.
508
?s? ?? ?? ?? ?? ?? ?? ?/s?
?s? Tibet ?s financial work has gained remarkable achievements ?/s?
Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.
4 Evaluation
In this section, we report the performance of the en-
hanced Bruin on the NIST MT-05 and NIST MT-06
Chinese-to-English translation tasks. We describe
the corpus, model training, and experiments related
to the refinements described above.
4.1 Corpus
The bilingual training data is derived from the fol-
lowing various sources: the FBIS (LDC2003E14),
Hong Kong Parallel Text (Hong Kong News and
Hong Kong Hansards, LDC2004T08), Xinhua News
(LDC2002E18), Chinese News Translation Text
Part1 (LDC2005T06), Translations from the Chi-
nese Treebank (LDC2003E07), Chinese English
News Magazine (LDC2005E47). It contains 2.4M
sentence pairs in total (68.1M Chinese words and
73.8M English words).
For the efficiency of minimum-error-rate training,
we built our development set using sentences not ex-
ceeding 50 characters from the NIST MT-02 evalu-
ation test data (580 sentences).
4.2 Training
We use exactly the same way and configuration de-
scribed in (He et al, 2006) to preprocess the training
data, align words and extract phrases.
We built two four-gram language models using
Xinhua section of the English Gigaword corpus
(181.1M words) and the English side of the bilin-
gual training data described above respectively. We
applied modified Kneser-Ney smoothing as imple-
mented in the SRILM toolkit (Stolcke, 2002).
The MaxEnt-based reordering model is trained
using the way of (Xiong et al, 2006). The difference
is that we only use lexical features generated by tail
words of blocks, instead of head words, removing
features generated by the combination of two bound-
ary words.
Bleu(%) Secs/sent
Bruin 29.96 54.3
sws RH1 RH12 RH1 RH12
5 29.65 29.95 42.6 41.2
10 30.55 31.27 46.2 41.8
15 30.26 31.40 48.0 42.2
20 30.19 31.42 49.1 43.2
Table 1: Effect of reordering heuristics. RH1 de-
notes swapping window while RH12 denotes swap-
ping window with the addition of punctuation re-
striction.
4.3 Translation Results
Table 1 compares the BLEU scores 2 and the speed
in seconds/sentence of the baseline system Bruin
and the enhanced system with reordering heuristics
applied. The second row gives the BLEU score and
the average decoding time of Bruin. The rows be-
low row 3 show the BLEU scores and speed of the
enhanced Bruin with different combinations of re-
ordering heuristics. We can clearly see that the re-
ordering heuristics proposed by us have a two-fold
effect on the performance: improving the BLEU
score and decreasing the average decoding time.
The example in Figure 1 shows how reordering
heuristics prevent incorrect long-distance swapping
which is not in accordance with the punctuation re-
striction.
Table 1 also shows that a 15-word swapping win-
dow is an inflexion point with the best tradeoff be-
tween the decoding time and the BLEU score. We
speculate that in our corpus most reorderings hap-
pen within a 15-word window. We use the FBIS
corpus to testify this hypothesis. In this corpus, we
extract all reordering examples using the algorithm
of Xiong et al (2006). Figure 3 shows the reorder-
ing length distribution curve in this corpus. Accord-
2In this paper, all BLEU scores are case-sensitive and evalu-
ated on the NIST MT-05 Chinese-to-English translation task if
there is no special note.
509
0 10 20 30 40 50 60 70 80
0
5
10
15
20
25
Pe
rce
nt 
(%
)
Reordering Length
Figure 3: Reordering length distribution. The hor-
izontal axis (reordering length) indicates the num-
ber of words on the source side of two neighboring
blocks which are to be swapped. The vertical axis
represents what proportion of reorderings with a cer-
tain length is likely to be in all reordering examples
with an inverted order.
Bleu(%)
Without Special Phrases 31.40
With Special Phrases 32.01
Table 2: Effect of integrating special phrases with
the sentence beginning/ending tag.
ing to our statistics, reorderings within a window
not exceeding 15 words have a very high proportion,
97.29%. Therefore we set sws = 15 for later exper-
iments.
Table 2 shows the effect of integrating special
phrases with sentence beginning/ending tags into
Bruin. As special phrases accounts for only 1.95%
of the total phrases used, an improvement of 0.6%
in BLEU score is well worthwhile. Further, the im-
provement is statistically significant at the 99% con-
fidence level according to Zhang?s significant tester
(Zhang et al, 2004). Figure 4 shows several exam-
ples translated with special phrases integrated. We
can see that phrases with sentence beginning/ending
tags are correctly selected and located at the right
place.
Table 3 shows the performance of two systems on
the NIST MT-05 Chinese test data, which are (1)
System Refine MT-05 MT-06
Bruin - 29.96 -
EBruin RH 31.40 30.22
EBruin RH+SP 32.01 -
Table 3: Results of different systems. The refine-
ments RH, SP represent reordering heuristics and
special phrases with the sentence beginning/ending
tag, respectively.
Bruin, trained on the large data described above; and
(2) enhanced Bruin (EBruin) with different refine-
ments trained on the same data set. This table also
shows the evaluation result of the enhanced Bruin
with reordering heuristics, obtained in the NIST MT-
06 evaluation exercise. 3
5 Conclusions
We have described in detail two refinements for
BTG-based SMT which include reordering heuris-
tics and special phrases with tags. The refinements
were integrated into a well-established BTG-based
system Bruin introduced by Xiong et al (2006). Re-
ordering heuristics proposed here achieve a twofold
improvement: better reordering and higher-speed
decoding. To our best knowledge, we are the first
to integrate special phrases with the sentence be-
ginning/ending tag into SMT. Experimental results
show that the above refinements improve the base-
line system significantly.
For further improvements, we will investigate
possible extensions to the BTG grammars, e.g.
learning useful nonterminals using unsupervised
learning algorithm.
Acknowledgements
We would like to thank the anonymous review-
ers for useful comments on the earlier version of
this paper. The first author was partially sup-
ported by the National Science Foundations of
China (No. 60573188) and the High Technology
Research and Development Program of China (No.
2006AA010108) while he studied in the Institute of
Computing Technology, Chinese Academy of Sci-
ences.
3Full results are available at http://www.nist.gov/
speech/tests/mt/doc/mt06eval official results.html.
510
With Special Phrases Without Special Phrases
?s? Japan had already pledged to provide 30 mil-
lion US dollars of aid due to the tsunami victims of
the country . ?/s?
originally has pledged to provide 30 million US
dollars of aid from Japan tsunami victimized coun-
tries .
?s? the results of the survey is based on the re-
sults of the chiefs of the Ukrainian National 50.96%
cast by chiefs . ?/s?
is based on the survey findings Ukraine 50.96% cast
by the chiefs of the chiefs of the country .
?s? and at the same time , the focus of the world have
been transferred to other areas . ?/s?
and at the same time , the global focus has shifted
he.
Figure 4: Examples translated with special phrases integrated. The bold underlined words are special phrases
with the sentence beginning/ending tag.
References
Yaser Al-Onaizan, Kishore Papineni. 2006. Distortion
Models for Statistical Machine Translation. In Pro-
ceedings of ACL-COLING 2006.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, Michael Subotin. 2005. The
Hiero Machine Translation System: Extensions, Eval-
uation, and Analysis. In Proceedings of HLT/EMNLP,
pages 779?786, Vancouver, October 2005.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. In computational linguistics, 33(2).
Christine Doran. 2000. Punctuation in a Lexicalized
Grammar. In Proceedings of Workshop TAG+5, Paris.
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, Qun
Liu. 2006. ICT System Description for the 2006
TC-STAR Run #2 SLT Evaluation. In Proceedings of
TC-STAR Workshop on Speech-to-Speech Translation,
Barcelona, Spain.
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Ma-
chine Translation as Lexicalized Parsing with Hooks.
In Proceedings of the 9th International Workshop
on Parsing Technologies (IWPT-05), Vancouver, BC,
Canada, October 2005.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, demonstration session, Prague, Czech Republic,
June 2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL-COLING 2006.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phraases. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2003. Final
Report of Johns Hopkins 2003 Summer Workshop on
Syntax for Statistical Machine Translation.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAL 1995, pages 1328-1334, Montreal, August.
511
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL
1996.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006, pages 521?528.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC 2004, pages 2051? 2054.
512
Proceedings of ACL-08: HLT, pages 192?199,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Forest-Based Translation
Haitao Mi? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
{htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
Among syntax-based translation models, the
tree-based approach, which takes as input a
parse tree of the source sentence, is a promis-
ing direction being faster and simpler than
its string-based counterpart. However, current
tree-based systems suffer from a major draw-
back: they only use the 1-best parse to direct
the translation, which potentially introduces
translation mistakes due to parsing errors. We
propose a forest-based approach that trans-
lates a packed forest of exponentially many
parses, which encodes many more alternatives
than standard n-best lists. Large-scale exper-
iments show an absolute improvement of 1.7
BLEU points over the 1-best baseline. This
result is also 0.8 points higher than decoding
with 30-best parses, and takes even less time.
1 Introduction
Syntax-based machine translation has witnessed
promising improvements in recent years. Depend-
ing on the type of input, these efforts can be di-
vided into two broad categories: the string-based
systems whose input is a string to be simultane-
ously parsed and translated by a synchronous gram-
mar (Wu, 1997; Chiang, 2005; Galley et al, 2006),
and the tree-based systems whose input is already a
parse tree to be directly converted into a target tree
or string (Lin, 2004; Ding and Palmer, 2005; Quirk
et al, 2005; Liu et al, 2006; Huang et al, 2006).
Compared with their string-based counterparts, tree-
based systems offer some attractive features: they
are much faster in decoding (linear time vs. cubic
time, see (Huang et al, 2006)), do not require a
binary-branching grammar as in string-based mod-
els (Zhang et al, 2006), and can have separate gram-
mars for parsing and translation, say, a context-free
grammar for the former and a tree substitution gram-
mar for the latter (Huang et al, 2006). However, de-
spite these advantages, current tree-based systems
suffer from a major drawback: they only use the 1-
best parse tree to direct the translation, which po-
tentially introduces translation mistakes due to pars-
ing errors (Quirk and Corston-Oliver, 2006). This
situation becomes worse with resource-poor source
languages without enough Treebank data to train a
high-accuracy parser.
One obvious solution to this problem is to take as
input k-best parses, instead of a single tree. This k-
best list postpones some disambiguation to the de-
coder, which may recover from parsing errors by
getting a better translation from a non 1-best parse.
However, a k-best list, with its limited scope, of-
ten has too few variations and too many redundan-
cies; for example, a 50-best list typically encodes
a combination of 5 or 6 binary ambiguities (since
25 < 50 < 26), and many subtrees are repeated
across different parses (Huang, 2008). It is thus inef-
ficient either to decode separately with each of these
very similar trees. Longer sentences will also aggra-
vate this situation as the number of parses grows ex-
ponentially with the sentence length.
We instead propose a new approach, forest-based
translation (Section 3), where the decoder trans-
lates a packed forest of exponentially many parses,1
1There has been some confusion in the MT literature regard-
ing the term forest: the word ?forest? in ?forest-to-string rules?
192
VP
PP
P
yu?
x1:NPB
VPB
VV
ju?x??ng
AS
le
x2:NPB
? held x2 with x1
Figure 1: An example translation rule (r3 in Fig. 2).
which compactly encodes many more alternatives
than k-best parses. This scheme can be seen as
a compromise between the string-based and tree-
based methods, while combining the advantages of
both: decoding is still fast, yet does not commit to
a single parse. Large-scale experiments (Section 4)
show an improvement of 1.7 BLEU points over the
1-best baseline, which is also 0.8 points higher than
decoding with 30-best trees, and takes even less time
thanks to the sharing of common subtrees.
2 Tree-based systems
Current tree-based systems perform translation in
two separate steps: parsing and decoding. A parser
first parses the source language input into a 1-best
tree T , and the decoder then searches for the best
derivation (a sequence of translation steps) d? that
converts source tree T into a target-language string
among all possible derivations D:
d? = argmax
d?D
P(d|T ). (1)
We will now proceed with a running example
translating from Chinese to English:
(2) ?
Bu`sh??
Bush
?
yu?
with/and
??
Sha?lo?ng
Sharon1
>L
ju?x??ng
hold
?
le
pass.
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576?584,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Joint Decoding with Multiple Translation Models
Yang Liu and Haitao Mi and Yang Feng and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,htmi,fengyang,liuqun}@ict.ac.cn
Abstract
Current SMT systems usually decode with
single translation models and cannot ben-
efit from the strengths of other models in
decoding phase. We instead propose joint
decoding, a method that combines multi-
ple translation models in one decoder. Our
joint decoder draws connections among
multiple models by integrating the trans-
lation hypergraphs they produce individu-
ally. Therefore, one model can share trans-
lations and even derivations with other
models. Comparable to the state-of-the-art
system combination technique, joint de-
coding achieves an absolute improvement
of 1.5 BLEU points over individual decod-
ing.
1 Introduction
System combination aims to find consensus trans-
lations among different machine translation sys-
tems. It proves that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
Recent several years have witnessed the rapid
development of system combination methods
based on confusion networks (e.g., (Rosti et al,
2007; He et al, 2008)), which show state-of-the-
art performance in MT benchmarks. A confusion
network consists of a sequence of sets of candidate
words. Each candidate word is associated with a
score. The optimal consensus translation can be
obtained by selecting one word from each set of
candidates to maximizing the overall score. While
it is easy and efficient to manipulate strings, cur-
rent methods usually have no access to most infor-
mation available in decoding phase, which might
be useful for obtaining further improvements.
In this paper, we propose a framework for com-
bining multiple translation models directly in de-
coding phase. 1 Based on max-translation decod-
ing and max-derivation decoding used in conven-
tional individual decoders (Section 2), we go fur-
ther to develop a joint decoder that integrates mul-
tiple models on a firm basis:
? Structuring the search space of each model
as a translation hypergraph (Section 3.1),
our joint decoder packs individual translation
hypergraphs together by merging nodes that
have identical partial translations (Section
3.2). Although such translation-level combi-
nation will not produce new translations, it
does change the way of selecting promising
candidates.
? Two models could even share derivations
with each other if they produce the same
structures on the target side (Section 3.3),
which we refer to as derivation-level com-
bination. This method enlarges the search
space by allowing for mixing different types
of translation rules within one derivation.
? As multiple derivations are used for finding
optimal translations, we extend the minimum
error rate training (MERT) algorithm (Och,
2003) to tune feature weights with respect
to BLEU score for max-translation decoding
(Section 4).
We evaluated our joint decoder that integrated
a hierarchical phrase-based model (Chiang, 2005;
Chiang, 2007) and a tree-to-string model (Liu et
al., 2006) on the NIST 2005 Chinese-English test-
set. Experimental results show that joint decod-
1It might be controversial to use the term ?model?, which
usually has a very precise definition in the field. Some
researchers prefer to saying ?phrase-based approaches? or
?phrase-based systems?. On the other hand, other authors
(e.g., (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2007))
do use the expression ?phrase-based models?. In this paper,
we use the term ?model? to emphasize that we integrate dif-
ferent approaches directly in decoding phase rather than post-
processing system outputs.
576
S ? ?X1,X1?
X ? ?fabiao X1, give a X1?
X ? ?yanjiang, talk?
Figure 1: A derivation composed of SCFG rules
that translates a Chinese sentence ?fabiao yan-
jiang? into an English sentence ?give a talk?.
ing with multiple models achieves an absolute im-
provement of 1.5 BLEU points over individual de-
coding with single models (Section 5).
2 Background
Statistical machine translation is a decision prob-
lem where we need decide on the best of target
sentence matching a source sentence. The process
of searching for the best translation is convention-
ally called decoding, which usually involves se-
quences of decisions that translate a source sen-
tence into a target sentence step by step.
For example, Figure 1 shows a sequence of
SCFG rules (Chiang, 2005; Chiang, 2007) that
translates a Chinese sentence ?fabiao yanjiang?
into an English sentence ?give a talk?. Such se-
quence of decisions is called a derivation. In
phrase-based models, a decision can be translating
a source phrase into a target phrase or reordering
the target phrases. In syntax-based models, deci-
sions usually correspond to transduction rules. Of-
ten, there are many derivations that are distinct yet
produce the same translation.
Blunsom et al (2008) present a latent vari-
able model that describes the relationship between
translation and derivation clearly. Given a source
sentence f , the probability of a target sentence e
being its translation is the sum over all possible
derivations:
Pr(e|f) =
?
d??(e,f)
Pr(d, e|f) (1)
where ?(e, f) is the set of all possible derivations
that translate f into e and d is one such derivation.
They use a log-linear model to define the con-
ditional probability of a derivation d and corre-
sponding translation e conditioned on a source
sentence f :
Pr(d, e|f) = exp
?
m ?mhm(d, e, f)
Z(f) (2)
where hm is a feature function, ?m is the asso-
ciated feature weight, and Z(f) is a constant for
normalization:
Z(f) =
?
e
?
d??(e,f)
exp
?
m
?mhm(d, e, f) (3)
A feature value is usually decomposed as the
product of decision probabilities: 2
h(d, e, f) =
?
d?d
p(d) (4)
where d is a decision in the derivation d.
Although originally proposed for supporting
large sets of non-independent and overlapping fea-
tures, the latent variable model is actually a more
general form of conventional linear model (Och
and Ney, 2002).
Accordingly, decoding for the latent variable
model can be formalized as
e? = argmax
e
{
?
d??(e,f)
exp
?
m
?mhm(d, e, f)
}
(5)
where Z(f) is not needed in decoding because it
is independent of e.
Most SMT systems approximate the summa-
tion over all possible derivations by using 1-best
derivation for efficiency. They search for the 1-
best derivation and take its target yield as the best
translation:
e? ? argmax
e,d
{
?
m
?mhm(d, e, f)
}
(6)
We refer to Eq. (5) as max-translation decoding
and Eq. (6) as max-derivation decoding, which are
first termed by Blunsom et al (2008).
By now, most current SMT systems, adopting
either max-derivation decoding or max-translation
decoding, have only used single models in decod-
ing phase. We refer to them as individual de-
coders. In the following section, we will present
a new method called joint decoding that includes
multiple models in one decoder.
3 Joint Decoding
There are two major challenges for combining
multiple models directly in decoding phase. First,
they rely on different kinds of knowledge sources
2There are also features independent of derivations, such
as language model and word penalty.
577
Sgive
0-1
talk
1-2
give a talk
0-2
give talks
0-2
S
give
0-1
speech
1-2
give a talk
0-2
make a speech
0-2
S
give
0-1
talk
1-2
speech
1-2
give a talk
0-2
give talks
0-2
make a speech
0-2
packing(a) (b)
(c)
Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced by
another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote
the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating
that the two models produce the same translations.
and thus need to collect different information dur-
ing decoding. For example, taking a source parse
as input, a tree-to-string decoder (e.g., (Liu et al,
2006)) pattern-matches the source parse with tree-
to-string rules and produces a string on the tar-
get side. On the contrary, a string-to-tree decoder
(e.g., (Galley et al, 2006; Shen et al, 2008)) is a
parser that applies string-to-tree rules to obtain a
target parse for the source string. As a result, the
hypothesis structures of the two models are funda-
mentally different.
Second, translation models differ in decoding
algorithms. Depending on the generating order
of a target sentence, we distinguish between two
major categories: left-to-right and bottom-up. De-
coders that use rules with flat structures (e.g.,
phrase pairs) usually generate target sentences
from left to right while those using rules with hier-
archical structures (e.g., SCFG rules) often run in
a bottom-up style.
In response to the two challenges, we first ar-
gue that the search space of an arbitrary model can
be structured as a translation hypergraph, which
makes each model connectable to others (Section
3.1). Then, we show that a packed translation hy-
pergraph that integrates the hypergraphs of indi-
vidual models can be generated in a bottom-up
topological order, either integrated at the transla-
tion level (Section 3.2) or the derivation level (Sec-
tion 3.3).
3.1 Translation Hypergraph
Despite the diversity of translation models, they all
have to produce partial translations for substrings
of input sentences. Therefore, we represent the
search space of a translation model as a structure
called translation hypergraph.
Figure 2(a) demonstrates a translation hyper-
graph for one model, for example, a hierarchical
phrase-based model. A node in a hypergraph de-
notes a partial translation for a source substring,
except for the starting node ?S?. For example,
given the example source sentence
0 fabiao 1 yanjiang 2
the node ??give talks?, [0, 2]? in Figure 2(a) de-
notes that ?give talks? is one translation of the
source string f21 = ?fabiao yanjiang?.
The hyperedges between nodes denote the deci-
sion steps that produce head nodes from tail nodes.
For example, the incoming hyperedge of the node
??give talks?, [0, 2]? could correspond to an SCFG
rule:
X ? ?X1 yanjiang,X1 talks?
Each hyperedge is associated with a number of
weights, which are the feature values of the corre-
sponding translation rules. A path of hyperedges
constitutes a derivation.
578
Hypergraph Decoding
node translation
hyperedge rule
path derivation
Table 1: Correspondence between translation hy-
pergraph and decoding.
More formally, a hypergraph (Klein and Man-
ning., 2001; Huang and Chiang, 2005) is a tuple
?V,E,R?, where V is a set of nodes, E is a set
of hyperedges, and R is a set of weights. For a
given source sentence f = fn1 = f1 . . . fn, each
node v ? V is in the form of ?t, [i, j]?, which de-
notes the recognition of t as one translation of the
source substring spanning from i through j (that
is, fi+1 . . . fj). Each hyperedge e ? E is a tuple
e = ?tails(e), head(e), w(e)?, where head(e) ?
V is the consequent node in the deductive step,
tails(e) ? V ? is the list of antecedent nodes, and
w(e) is a weight function from R|tails(e)| to R.
As a general representation, a translation hyper-
graph is capable of characterizing the search space
of an arbitrary translation model. Furthermore,
it offers a graphic interpretation of decoding pro-
cess. A node in a hypergraph denotes a translation,
a hyperedge denotes a decision step, and a path
of hyperedges denotes a derivation. A translation
hypergraph is formally a semiring as the weight
of a path is the product of hyperedge weights and
the weight of a node is the sum of path weights.
While max-derivation decoding only retains the
single best path at each node, max-translation de-
coding sums up all incoming paths. Table 1 sum-
marizes the relationship between translation hy-
pergraph and decoding.
3.2 Translation-Level Combination
The conventional interpretation of Eq. (1) is that
the probability of a translation is the sum over all
possible derivations coming from the same model.
Alternatively, we interpret Eq. (1) as that the
derivations could come from different models.3
This forms the theoretical basis of joint decoding.
Although the information inside a derivation
differs widely among translation models, the be-
ginning and end points (i.e., f and e, respectively)
must be identical. For example, a tree-to-string
3The same for all d occurrences in Section 2. For exam-
ple, ?(e, f) might include derivations from various models
now. Note that we still use Z for normalization.
model first parses f to obtain a source tree T (f)
and then transforms T (f) to the target sentence
e. Conversely, a string-to-tree model first parses
f into a target tree T (e) and then takes the surface
string e as the translation. Despite different inside,
their derivations must begin with f and end with e.
This situation remains the same for derivations
between a source substring f ji and its partial trans-
lation t during joint decoding:
Pr(t|f ji ) =
?
d??(t,fji )
Pr(d, t|f ji ) (7)
where d might come from multiple models. In
other words, derivations from multiple models
could be brought together for computing the prob-
ability of one partial translation.
Graphically speaking, joint decoding creates a
packed translation hypergraph that combines in-
dividual hypergraphs by merging nodes that have
identical translations. For example, Figure 2 (a)
and (b) demonstrate two translation hypergraphs
generated by two models respectively and Fig-
ure 2 (c) is the resulting packed hypergraph. The
solid lines denote the hyperedges of the first model
and the dashed lines denote those of the second
model. The shaded nodes are shared by both mod-
els. Therefore, the two models are combined at the
translation level. Intuitively, shared nodes should
be favored in decoding because they offer consen-
sus translations among different models.
Now the question is how to decode with multi-
ple models jointly in just one decoder. We believe
that both left-to-right and bottom-up strategies can
be used for joint decoding. Although phrase-based
decoders usually produce translations from left to
right, they can adopt bottom-up decoding in prin-
ciple. Xiong et al (2006) develop a bottom-up de-
coder for BTG (Wu, 1997) that uses only phrase
pairs. They treat reordering of phrases as a binary
classification problem. On the other hand, it is
possible for syntax-based models to decode from
left to right. Watanabe et al (2006) propose left-
to-right target generation for hierarchical phrase-
based translation. Although left-to-right decod-
ing might enable a more efficient use of language
models and hopefully produce better translations,
we adopt bottom-up decoding in this paper just for
convenience.
Figure 3 demonstrates the search algorithm of
our joint decoder. The input is a source language
sentence fn1 , and a set of translation models M
579
1: procedure JOINTDECODING(fn1 , M )
2: G? ?
3: for l ? 1 . . . n do
4: for all i, j s.t. j ? i = l do
5: for all m ?M do
6: ADD(G, i, j,m)
7: end for
8: PRUNE(G, i, j)
9: end for
10: end for
11: end procedure
Figure 3: Search algorithm for joint decoding.
(line 1). After initializing the translation hyper-
graph G (line 2), the decoder runs in a bottom-
up style, adding nodes for each span [i, j] and for
each model m. For each span [i, j] (lines 3-5),
the procedure ADD(G, i, j,m) add nodes gener-
ated by the model m to the hypergraph G (line 6).
Each model searches for partial translations inde-
pendently: it uses its own knowledge sources and
visits its own antecedent nodes, just running like
a bottom-up individual decoder. After all mod-
els finishes adding nodes for span [i, j], the pro-
cedure PRUNE(G, i, j) merges identical nodes and
removes less promising nodes to control the search
space (line 8). The pruning strategy is similar to
that of individual decoders, except that we require
there must exist at least one node for each model
to ensure further inference.
Although translation-level combination will not
offer new translations as compared to single mod-
els, it changes the way of selecting promising can-
didates in a combined search space and might po-
tentially produce better translations than individ-
ual decoding.
3.3 Derivation-Level Combination
In translation-level combination, different models
interact with each other only at the nodes. The
derivations of one model are unaccessible to other
models. However, if two models produce the same
structures on the target side, it is possible to com-
bine two models within one derivation, which we
refer to as derivation-level combination.
For example, although different on the source
side, both hierarchical phrase-based and tree-to-
string models produce strings of terminals and
nonterminals on the target side. Figure 4 shows
a derivation composed of both hierarchical phrase
IP(x1:VV, x2:NN) ? x1 x2
X ? ?fabiao, give?
X ? ?yanjiang, a talk?
Figure 4: A derivation composed of both SCFG
and tree-to-string rules.
pairs and tree-to-string rules. Hierarchical phrase
pairs are used for translating smaller units and
tree-to-string rules for bigger ones. It is appealing
to combine them in such a way because the hierar-
chical phrase-based model provides excellent rule
coverage while the tree-to-string model offers lin-
guistically motivated non-local reordering. Sim-
ilarly, Blunsom and Osborne (2008) use both hi-
erarchical phrase pairs and tree-to-string rules in
decoding, where source parse trees serve as condi-
tioning context rather than hard constraints.
Depending on the target side output, we dis-
tinguish between string-targeted and tree-targeted
models. String-targeted models include phrase-
based, hierarchical phrase-based, and tree-to-
string models. Tree-targeted models include
string-to-tree and tree-to-tree models. All models
can be combined at the translation level. Models
that share with same target output structure can be
further combined at the derivation level.
The joint decoder usually runs as max-
translation decoding because multiple derivations
from various models are used. However, if all
models involved belong to the same category, a
joint decoder can also adopt the max-derivation
fashion because all nodes and hyperedges are ac-
cessible now (Section 5.2).
Allowing derivations for comprising rules from
different models and integrating their strengths,
derivation-level combination could hopefully pro-
duce new and better translations as compared with
single models.
4 Extended Minimum Error Rate
Training
Minimum error rate training (Och, 2003) is widely
used to optimize feature weights for a linear model
(Och and Ney, 2002). The key idea of MERT is
to tune one feature weight to minimize error rate
each time while keep others fixed. Therefore, each
580
xf(x)
t1
t2
t3
(0, 0) x1 x2
Figure 5: Calculation of critical intersections.
candidate translation can be represented as a line:
f(x) = a? x + b (8)
where a is the feature value of current dimension,
x is the feature weight being tuned, and b is the
dotproduct of other dimensions. The intersection
of two lines is where the candidate translation will
change. Instead of computing all intersections,
Och (2003) only computes critical intersections
where highest-score translations will change. This
method reduces the computational overhead sig-
nificantly.
Unfortunately, minimum error rate training can-
not be directly used to optimize feature weights of
max-translation decoding because Eq. (5) is not a
linear model. However, if we also tune one dimen-
sion each time and keep other dimensions fixed,
we obtain a monotonic curve as follows:
f(x) =
K
?
k=1
eak?x+bk (9)
where K is the number of derivations for a can-
didate translation, ak is the feature value of cur-
rent dimension on the kth derivation and bk is the
dotproduct of other dimensions on the kth deriva-
tion. If we restrict that ak is always non-negative,
the curve shown in Eq. (9) will be a monotoni-
cally increasing function. Therefore, it is possible
to extend the MERT algorithm to handle situations
where multiple derivations are taken into account
for decoding.
The key difference is the calculation of criti-
cal intersections. The major challenge is that two
curves might have multiple intersections while
two lines have at most one intersection. Fortu-
nately, as the curve is monotonically increasing,
we need only to find the leftmost intersection of
a curve with other curves that have greater values
after the intersection as a candidate critical inter-
section.
Figure 5 demonstrates three curves: t1, t2, and
t3. Suppose that the left bound of x is 0, we com-
pute the function values for t1, t2, and t3 at x = 0
and find that t3 has the greatest value. As a result,
we choose x = 0 as the first critical intersection.
Then, we compute the leftmost intersections of t3
with t1 and t2 and choose the intersection closest
to x = 0, that is x1, as our new critical intersec-
tion. Similarly, we start from x1 and find x2 as the
next critical intersection. This iteration continues
until it reaches the right bound. The bold curve de-
notes the translations we will choose over different
ranges. For example, we will always choose t2 for
the range [x1, x2].
To compute the leftmost intersection of two
curves, we divide the range from current critical
intersection to the right bound into many bins (i.e.,
smaller ranges) and search the bins one by one
from left to right. We assume that there is at most
one intersection in each bin. As a result, we can
use the Bisection method for finding the intersec-
tion in each bin. The search process ends immedi-
ately once an intersection is found.
We divide max-translation decoding into three
phases: (1) build the translation hypergraphs, (2)
generate n-best translations, and (3) generate n?-
best derivations. We apply Algorithm 3 of Huang
and Chiang (2005) for n-best list generation. Ex-
tended MERT runs on n-best translations plus n?-
best derivations to optimize the feature weights.
Note that feature weights of various models are
tuned jointly in extended MERT.
5 Experiments
5.1 Data Preparation
Our experiments were on Chinese-to-English
translation. We used the FBIS corpus (6.9M +
8.9M words) as the training corpus. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT Evaluation test
set as our development set, and used the NIST
2005 test set as test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
Our joint decoder included two models. The
581
Max-derivation Max-translationModel Combination Time BLEU Time BLEU
hierarchical N/A 40.53 30.11 44.87 29.82
tree-to-string N/A 6.13 27.23 6.69 27.11
translation N/A N/A 55.89 30.79both derivation 48.45 31.63 54.91 31.49
Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-
onds/sentence) and BLEU score (case-insensitive).
first model was the hierarchical phrase-based
model (Chiang, 2005; Chiang, 2007). We obtained
word alignments of training data by first running
GIZA++ (Och and Ney, 2003) and then applying
the refinement rule ?grow-diag-final-and? (Koehn
et al, 2003). About 2.6M hierarchical phrase pairs
extracted from the training corpus were used on
the test set.
Another model was the tree-to-string model
(Liu et al, 2006; Liu et al, 2007). Based on
the same word-aligned training corpus, we ran a
Chinese parser on the source side to obtain 1-best
parses. For 15,157 sentences we failed to obtain
1-best parses. Therefore, only 93.7% of the train-
ing corpus were used by the tree-to-string model.
About 578K tree-to-string rules extracted from the
training corpus were used on the test set.
5.2 Individual Decoding Vs. Joint Decoding
Table 2 shows the results of comparing individ-
ual decoding and joint decoding on the test set.
With conventional max-derivation decoding, the
hierarchical phrase-based model achieved a BLEU
score of 30.11 on the test set, with an average de-
coding time of 40.53 seconds/sentence. We found
that accounting for all possible derivations in max-
translation decoding resulted in a small negative
effect on BLEU score (from 30.11 to 29.82), even
though the feature weights were tuned with respect
to BLEU score. One possible reason is that we
only used n-best derivations instead of all possi-
ble derivations for minimum error rate training.
Max-derivation decoding with the tree-to-string
model yielded much lower BLEU score (i.e.,
27.23) than the hierarchical phrase-based model.
One reason is that the tree-to-string model fails
to capture a large amount of linguistically unmo-
tivated mappings due to syntactic constraints. An-
other reason is that the tree-to-string model only
used part of the training data because of pars-
ing failure. Similarly, accounting for all possible
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
 0  1  2  3  4  5  6  7  8  9  10  11
pe
rc
en
ta
ge
span width
Figure 6: Node sharing in max-translation de-
coding with varying span widths. We retain at
most 100 nodes for each source substring for each
model.
derivations in max-translation decoding failed to
bring benefits for the tree-to-string model (from
27.23 to 27.11).
When combining the two models at the trans-
lation level, the joint decoder achieved a BLEU
score of 30.79 that outperformed the best result
(i.e., 30.11) of individual decoding significantly
(p < 0.05). This suggests that accounting for
all possible derivations from multiple models will
help discriminate among candidate translations.
Figure 6 demonstrates the percentages of nodes
shared by the two models over various span widths
in packed translation hypergraphs during max-
translation decoding. For one-word source strings,
89.33% nodes in the hypergrpah were shared by
both models. With the increase of span width, the
percentage decreased dramatically due to the di-
versity of the two models. However, there still ex-
ist nodes shared by two models even for source
substrings that contain 33 words.
When combining the two models at the deriva-
tion level using max-derivation decoding, the joint
decoder achieved a BLEU score of 31.63 that out-
performed the best result (i.e., 30.11) of individ-
582
Method Model BLEU
hierarchical 30.11individual decoding
tree-to-string 27.23
system combination both 31.50
joint decoding both 31.63
Table 3: Comparison of individual decoding, sys-
tem combination, and joint decoding.
ual decoding significantly (p < 0.01). This im-
provement resulted from the mixture of hierarchi-
cal phrase pairs and tree-to-string rules. To pro-
duce the result, the joint decoder made use of
8,114 hierarchical phrase pairs learned from train-
ing data, 6,800 glue rules connecting partial trans-
lations monotonically, and 16,554 tree-to-string
rules. While tree-to-string rules offer linguistically
motivated non-local reordering during decoding,
hierarchical phrase pairs ensure good rule cover-
age. Max-translation decoding still failed to sur-
pass max-derivation decoding in this case.
5.3 Comparison with System Combination
We re-implemented a state-of-the-art system com-
bination method (Rosti et al, 2007). As shown
in Table 3, taking the translations of the two indi-
vidual decoders as input, the system combination
method achieved a BLEU score of 31.50, slightly
lower than that of joint decoding. But this differ-
ence is not significant statistically.
5.4 Individual Training Vs. Joint Training
Table 4 shows the effects of individual training and
joint training. By individual, we mean that the two
models are trained independently. We concatenate
and normalize their feature weights for the joint
decoder. By joint, we mean that they are trained
together by the extended MERT algorithm. We
found that joint training outperformed individual
training significantly for both max-derivation de-
coding and max-translation decoding.
6 Related Work
System combination has benefited various NLP
tasks in recent years, such as products-of-experts
(e.g., (Smith and Eisner, 2005)) and ensemble-
based parsing (e.g., (Henderson and Brill, 1999)).
In machine translation, confusion-network based
combination techniques (e.g., (Rosti et al, 2007;
He et al, 2008)) have achieved the state-of-the-
art performance in MT evaluations. From a dif-
Training Max-derivation Max-translation
individual 30.70 29.95
joint 31.63 30.79
Table 4: Comparison of individual training and
joint training.
ferent perspective, we try to combine different ap-
proaches directly in decoding phase by using hy-
pergraphs. While system combination techniques
manipulate only the final translations of each sys-
tem, our method opens the possibility of exploit-
ing much more information.
Blunsom et al (2008) first distinguish between
max-derivation decoding and max-translation de-
coding explicitly. They show that max-translation
decoding outperforms max-derivation decoding
for the latent variable model. While they train the
parameters using a maximum a posteriori estima-
tor, we extend the MERT algorithm (Och, 2003)
to take the evaluation metric into account.
Hypergraphs have been successfully used in
parsing (Klein and Manning., 2001; Huang and
Chiang, 2005; Huang, 2008) and machine trans-
lation (Huang and Chiang, 2007; Mi et al, 2008;
Mi and Huang, 2008). Both Mi et al (2008) and
Blunsom et al (2008) use a translation hyper-
graph to represent search space. The difference is
that their hypergraphs are specifically designed for
the forest-based tree-to-string model and the hier-
archical phrase-based model, respectively, while
ours is more general and can be applied to arbi-
trary models.
7 Conclusion
We have presented a framework for including mul-
tiple translation models in one decoder. Repre-
senting search space as a translation hypergraph,
individual models are accessible to others via shar-
ing nodes and even hyperedges. As our decoder
accounts for multiple derivations, we extend the
MERT algorithm to tune feature weights with re-
spect to BLEU score for max-translation decod-
ing. In the future, we plan to optimize feature
weights for max-translation decoding directly on
the entire packed translation hypergraph rather
than on n-best derivations, following the lattice-
based MERT (Macherey et al, 2008).
583
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60873167
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done while
Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Yajuan Lu?, Liang Huang, Nguyen
Bach, Andreas Zollmann, Vamshi Ambati, and
Kevin Gimpel for their helpful feedback.
References
Phil Blunsom and Mile Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP08.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL08.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL05.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP94.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL06.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proc. of
EMNLP08.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proc. of EMNLP99.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL07.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL08.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proc. of ACL08.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL03.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL06.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL07.
Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proc. of EMNLP08.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP08.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL08.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL02.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL02.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proc. of ACL07.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL08.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL05.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP02.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL06.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL06.
584
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137?140,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Sub-Sentence Division for Tree-Based Machine Translation 
 
Hao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu* 
*Key Lab. of Intelligent Information Processing 
+Key Lab. of Computer System and Architecture 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cn
 
Abstract 
Tree-based statistical machine translation 
models have made significant progress in re-
cent years, especially when replacing 1-best 
trees with packed forests. However, as the 
parsing accuracy usually goes down dramati-
cally with the increase of sentence length, 
translating long sentences often takes long 
time and only produces degenerate transla-
tions. We propose a new method named sub-
sentence division that reduces the decoding 
time and improves the translation quality for 
tree-based translation. Our approach divides 
long sentences into several sub-sentences by 
exploiting tree structures. Large-scale ex-
periments on the NIST 2008 Chinese-to-
English test set show that our approach 
achieves an absolute improvement of 1.1 
BLEU points over the baseline system in 
50% less time. 
1 Introduction 
Tree-based statistical machine translation 
models in days have witness promising progress 
in recent years, such as tree-to-string models (Liu 
et al, 2006; Huang et al, 2006), tree-to-tree 
models (Quirk et al,2005;Zhang et al, 2008). 
Especially, when incorporated with forest, the 
correspondent forest-based tree-to-string models 
(Mi et al, 2008; Zhang et al, 2009), tree-to-tree 
models (Liu et al, 2009) have achieved a prom-
ising improvements over correspondent tree-
based systems. However, when we translate long 
sentences, we argue that two major issues will be 
raised. On one hand, parsing accuracy will be 
lower as the length of sentence grows. It will in-
evitably hurt the translation quality (Quirk and 
Corston-Oliver, 2006; Mi and Huang, 2008). On 
the other hand, decoding on long sentences will 
be time consuming, especially for forest ap-
proaches. So splitting long sentences into sub- 
 
Figure 1. Main framework of our method 
 
sentences becomes a natural way in MT litera-
ture.  
A simple way is to split long sentences by 
punctuations. However, without concerning 
about the original whole tree structures, this ap-
proach will result in ill-formed sub-trees which 
don?t respect to original structures. In this paper, 
we present a new approach, which pays more 
attention to parse trees on the long sentences. We 
firstly parse the long sentences into trees, and 
then divide them accordingly into sub-sentences, 
which will be translated independently (Section 
3). Finally, we combine sub translations into a 
full translation (Section 4). Large-scale experi-
ments (Section 5) show that the BLEU score 
achieved by our approach is 1.1 higher than di-
rect decoding and 0.3 higher than always split-
ting on commas on the 2008 NIST MT Chinese-
English test set. Moreover, our approach has re-
duced decoding time significantly. 
2 Framework  
Our approach works in following steps. 
(1) Split a long sentence into sub-sentences.  
(2) Translate all the sub-sentences respectively. 
(3) Combine the sub-translations.   
Figure 1 illustrates the main idea of our ap-
proach. The crucial issues of our method are how 
to divide long sentences and how to combine the 
sub-translations.  
3 Sub Sentence Division  
Long sentences could be very complicated in 
grammar and sentence structure, thereby creating 
an obstacle for translation. Consequently, we 
need to break them into shorter and easier 
clauses. To divide sentences by punctuation is 
137
 
 
Figure 2. An undividable parse tree 
 
 
Figure 3. A dividable parse tree 
 
one of the most commonly used methods. How-
ever, simply applying this method might damage 
the accuracy of parsing. As a result, the strategy 
we proposed is to operate division while con-
cerning the structure of parse tree. 
As sentence division should not influence the 
accuracy of parsing, we have to be very cautious 
about sentences whose division might decrease 
the accuracy of parsing. Figure 2(a) shows an 
example of the parse tree of an undividable sen-
tence. 
As can be seen in Figure 2, when we divide 
the sentence by comma, it would break the struc-
ture of ?VP? sub-tree and result in a ill-formed 
sub-tree ?VP? (right sub-tree), which don?t have 
a subject and don?t respect to original tree struc-
tures. 
Consequently, the key issue of sentence divi-
sion is finding the sentences that can be divided 
without loosing parsing accuracy. Figure 2(b) 
shows the parse tree of a sentence that can be 
divided by punctuation, as sub-sentences divided 
by comma are independent. The reference trans-
lation of the sentence in figure 3 is 
 
Less than two hours earlier, a Palestinian took 
on a shooting spree on passengers in the town of 
Kfar Saba in northern Israel. 
Pseudocode 1 Check Sub Sentence Divi-
sion Algorithm 
1: procedure CheckSubSentence(sent) 
2: for each word i in sent 
3:    if(i is a comma) 
4:       left={words in left side of i}; 
          //words between last comma and cur-
rent comma i 
5:       right={words in right side of i}; 
         //words between i and next comma or
 semicolon, period, question mark 
6:       isDividePunct[i]=true; 
7:       for each j in left 
8:          if(( LCA(j, i)!=parent[i]) 
9:             isDividePunct[i]=false; 
10:           break; 
11:     for each j in right 
12:        if(( LCA(j, i)!=parent[i]) 
13:           isDividePunct[i]=false; 
14:           break; 
15: function LCA(i, j) 
16:    return lowest common ancestor(i, j);
 
It demonstrates that this long sentence can be 
divided into two sub-sentences, providing a good 
support to our division. 
In addition to dividable sentences and non-
dividable sentences, there are sentences contain-
ing more than one comma, some of which are 
dividable and some are not. However, this does 
not prove to be a problem, as we process each 
comma independently. In other words, we only 
split the dividable part of this kind of sentences, 
leaving the non-dividable part unchanged.  
To find the sentences that can be divided, we 
present a new method and provide its pseudo 
code. Firstly, we divide a sentence by its commas. 
For each word in the sub-sentence on the left 
side of a comma, we compute its lowest common 
ancestor (LCA) with the comma. And we process 
the words in the sub-sentence on the right side of 
the comma in the same way. Finally, we check if 
all the LCA we have computed are comma?s par-
ent node.  If all the LCA are the comma?s parent 
node, the sub-sentences are independent.  
As shown in figure 3, the LCA (AD ?? , 
PU ?),  is ?IP? ,which is the parent node of 
?PU ??; and the LCA (NR ??? , PU ?) is 
also ?IP?.  Till we have checked all the LCA of 
each word and comma, we finally find that all 
the LCA are ?IP?. As a result, this sentence can 
be divided without loosing parsing accuracy. 
LCA can be computed by using union-set (Tar-
jan, 1971) in lineal time. Concerning the  
138
sub-sentence 1: ???? 
Translation 1: Johndroe said                   A1
Translation 2: Johndroe pointed out       A2
Translation 3: Qiang Zhuo said              A3
comma 1: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 2: ???????????
??????????????? 
Translation 1: the two presidents also wel-
comed the US-South Korea free trade 
agreement that was signed yesterday       B1
Translation 2: the two presidents also ex-
pressed welcome to the US ? South Korea 
free trade agreement signed yesterday     B2
comma 2: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 3:???????????
?????? 
Translation 1: and would work to ensure 
that the congresses of both countries ap-
prove this agreement.                               C1
Translation 2: and will make efforts to en-
sure the Congress to approve this agreement 
of the two countries.                                C2
 
Table 1. Sub translation example 
 
implementation complexity, we have reduced the 
problem to range minimum query problem 
(Bender et al, 2005) with a time complexity of  
(1)?  for querying.  
Above all, our approach for sub sentence 
works as follows: 
(1)Split a sentence by semi-colon if there is 
one. 
(2)Parse a sentence if it contains a comma, 
generating k-best parses (Huang Chiang, 2005) 
with k=10.  
 (3)Use the algorithm in pseudocode 1 to 
check the sentence and divide it if there are 
more than 5 parse trees indicates that the sen-
tence is dividable.  
4 Sub Translation Combining  
For sub translation combining, we mainly use the 
best-first expansion idea from cube pruning 
(Huang and Chiang, 2007) to combine sub- 
translations and generate the whole k-best trans-
lations. We first select the best translation from 
sub translation sets, and then use an interpolation 
 
Test Set 02 05 08 
No Sent Division 34.56 31.26 24.53 
Split by Comma 34.59 31.23 25.39 
Our Approach 34.86 31.23 25.69 
 
Table 2. BLEU results (case sensitive) 
 
Test Set 02 05 08 
No Sent Division 28 h 36 h 52 h 
Split by Comma 18h 23h 29h 
Our Approach 18 h 22 h 26 h 
 
Table 3. Decoding time of our experiments 
(h means hours) 
 
language model for rescoring (Huang and Chiang, 
2007).  
For example, we split the following sentence ??
???,??????????????????
????????,?????????????
????? into three sub-sentences and generate 
some translations, and the results are displayed in 
Table 1.  
As seen in Table 1, for each sub-sentence, 
there are one or more versions of translation. For 
convenience, we label the three translation ver-
sions of sub-sentence 1 as A1, A2, and A3, re-
spectively. Similarly, B1, B2, C1, C2 are also 
labels of translation. We push the A1, white 
space, B1, white space, C1 into the cube, and 
then generate the final translation. 
According to cube pruning algorithm, we will 
generate other translations until we get the best 
list we need. Finally, we rescore the k-best list 
using interpolation language model and find the 
best translation which is A1 that B1 white space 
C1. 
5 Experiments  
5.1 Data preparation 
We conduct our experiments on Chinese-English 
translation, and use the Chinese parser of Xiong 
et al (2005) to parse the source sentences. And 
our decoder is based on forest-based tree-to-
string translation model (Mi et al 2008). 
Our training corpus consists of 2.56 million 
sentence pairs. Forest-based rule extractor (Mi 
and Huang 2008) is used with a pruning thresh-
old p=3. And we use SRI Language Modeling 
Toolkit (Stolcke, 2002) to train two 5-gram lan-
guage models with Kneser-Ney smoothing on the 
English side of the training corpus and the Xin-
hua portion of Gigaword corpora respectively. 
139
We use 2006 NIST MT Evaluation test set as 
development set, and 2002, 2005 and 2008 NIST 
MT Evaluation test sets as test sets. We also use 
minimum error-rate training (Och, 2003) to tune 
our feature weights. We evaluate our results with 
case-sensitive BLEU-4 metric (Papineni et al, 
2002). The pruning threshold p for parse forest in 
decoding time is 12. 
5.2 Results 
The final BLEU results are shown in Table 2, our 
approach has achieved a BLEU score that is 1.1 
higher than direct decoding and 0.3 higher than 
always splitting on commas. 
The decoding time results are presented in Ta-
ble 3. The search space of our experiment is ex-
tremely large due to the large pruning threshold 
(p=12), thus resulting in a long decoding time. 
However, our approach has reduced the decoding 
time by 50% over direct decoding, and 10% over 
always splitting on commas. 
6 Conclusion & Future Work  
We have presented a new sub-sentence division 
method and achieved some good results. In the 
future, we will extend our work from decoding to 
training time, where we divide the bilingual sen-
tences accordingly.  
Acknowledgement 
The authors were supported by National Natural 
Science Foundation of China, Contracts 0873167 
and 60736014, and 863 State Key Project 
No.2006AA010108. We thank Liang Huang for 
his insightful suggestions.  
References  
Bender, Farach-Colton, Pemmasani, Skiena, Sumazin, 
Lowest common ancestors in trees and di- 
rected acyclic graphs. J. Algorithms 57(2), 75?
94 (2005) 
Liang Huang and David Chiang. 2005. Better kbest 
Parsing. In Proceedings of IWPT-2005. 
Liang Huang and David Chiang. 2007. Forest res-
coring: Fast decoding with integrated lan-
guage models. In Proceedings of ACL. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proceedings of 
AMTA 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127-133. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignments template for statistical ma-
chine translation. In Proceedings of ACL. 
Yang Liu, Yajuan Lv and Qun Liu.2009. Improving 
Tree-to-Tree Translation with Packed Forests.To 
appear in Proceedings of ACL/IJCNLP.. 
Daniel Marcu, Wei Wang, AbdessamadEchihabi, and 
Kevin Knight. 2006. Statistical Machine Trans-
lation with syntactifiedtarget language 
phrases. In Proceedings of EMNLP. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT. 
Haitao Mi and Liang Huang. 2008. Forest-based 
translation rule extraction. In Proceedings of 
EMNLP. 
Franz J. Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceed-
ings of ACL, pages 160?167. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In 
Proceedings of ACL, pages 311?318,. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proceedings of ACL. 
Chris Quirk and Simon Corston-Oliver. 2006. The 
impact of parse quality on syntactically-
informed statistical machine translation. In 
Proceedings of EMNLP. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, volume 30, pages 901?904. 
Georgianna Tarjan, Depth First Search and Linear 
Graph Algorithms. SIAM J. Comp. 1:2, pp. 146?
160, 1972. 
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun 
Lin.2005. Parsing the Penn Chinese Treebank 
with semantic knowledge. In Proceedings of 
IJCNLP. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree transla-
tion model. In Proceedings of ACL. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009. Forest-based Tree Sequence 
to String Translation Model. To appear in Proceed-
ings of ACL/IJCNLP 
140
Coling 2010: Poster Volume, pages 285?293,
Beijing, August 2010
An Efficient Shift-Reduce Decoding Algorithm for Phrased-Based
Machine Translation
Yang Feng, Haitao Mi, Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{fengyang,htmi,yliu,liuqun}@ict.ac.cn
Abstract
In statistical machine translation, decod-
ing without any reordering constraint is
an NP-hard problem. Inversion Transduc-
tion Grammars (ITGs) exploit linguistic
structure and can well balance the needed
flexibility against complexity constraints.
Currently, translation models with ITG
constraints usually employs the cube-time
CYK algorithm. In this paper, we present
a shift-reduce decoding algorithm that can
generate ITG-legal translation from left to
right in linear time. This algorithm runs
in a reduce-eager style and is suited to
phrase-based models. Using the state-of-
the-art decoder Moses as the baseline, ex-
periment results show that the shift-reduce
algorithm can significantly improve both
the accuracy and the speed on different
test sets.
1 Introduction
In statistical machine translation, for the diver-
sity of natural languages, the word order of
source and target language may differ and search-
ing through all possible translations is NP-hard
(Knight, 1999). So some measures have to be
taken to reduce search space: either using a search
algorithm with pruning technique or restricting
possible reorderings.
Currently, beam search is widely used (Till-
mann and Ney, 2003; Koehn, 2004) to reduce
search space. However, the pruning technique
adopted by this algorithm is not risk-free. As a
result, the best partial translation may be ruled out
during pruning. The more aggressive the prun-
ing is, the more likely the best translation escapes.
There should be a tradeoff between the speed and
the accuracy. If some heuristic knowledge is em-
ployed to guide the search, the search algorithm
can discard some implausible hypotheses in ad-
vance and focus on more possible ones.
Inversion Transduction Grammars (ITGs) per-
mit a minimal extra degree of ordering flexibility
and are particularly well suited to modeling or-
dering shifts between languages (Wu, 1996; Wu,
1997). They can well balance the needed flex-
ibility against complexity constraints. Recently,
ITG has been successfully applied to statistical
machine translation (Zens and Ney, 2003; Zens
et al, 2004; Xiong et al, 2006). However, ITG
generally employs the expensive CYK parsing al-
gorithm which runs in cube time. In addition, the
CYK algorithm can not calculate language model
exactly in the process of decoding, as it can not
catch the full history context of the left words in a
hypothesis.
In this paper, we introduce a shift-reduce de-
coding algorithm with ITG constraints which runs
in a left-to-right manner. This algorithm parses
source words in the order of their corresponding
translations on the target side. In the meantime,
it gives all candidate ITG-legal reorderings. The
shift-reduce algorithm is different from the CYK
algorithm, in particular:
? It produces translation in a left-to-right man-
ner. As a result, language model probability
can be calculated more precisely in the light
of full history context.
? It decodes much faster. Applied with distor-
285
target side target side target side
(a) straight (b) inverted (c) discontinuous
Figure 1: Orientation of two blocks.
tion limit, shift-reduce decoding algorithm
can run in linear time, while the CYK runs
in cube time.
? It holds ITG structures generated during de-
coding. That is to say, it can directly give
ITG-legal spans, which leads to faster de-
coding. Furthermore, it can be extended to
syntax-based models.
We evaluated the performance of the shift-
reduce decoding algorithm by adding ITG con-
straints to the state-of-the-art decoder Moses. We
did experiments on three data sets: NIST MT08
data set, NIST MT05 data set and China Work-
shop on Machine Translation 2007 data set. Com-
pared to Moses, the improvements of the accuracy
are 1.59, 0.62, 0.8 BLEU score, respectively, and
the speed improvements are 15%, 24%, 30%, re-
spectively.
2 Decoding with ITG constraints
In this paper, we employ the shift-reduce algo-
rithm to add ITG constraints to phrase-based ma-
chine translation model. It is different from the
traditional shift-reduce algorithm used in natural
language parsing. On one hand, as natural lan-
guage parsing has to cope with a high degree of
ambiguity, it need take ambiguity into considera-
tion. As a result, the traditional one often suffers
shift-reduce divergence. Nonetheless, the shift-
reduce algorithm in this paper does not pay atten-
tion to ambiguity and acts in a reduce-eager man-
ner. On the other hand, the traditional algorithm
can not ensure that all reorderings observe ITG
constraints, so we have to modify the traditional
algorithm to import ITG constraints.
We will introduce the shift-reduce decoding al-
gorithm in the following two steps: First, we
1\1
zairu1
??2
shijian2
N3
diaocha3
]4
ziliaode4
>M5
diannao5
;?6
zaoqie6
The laptop with inquiry data on the event was stolen
(a)
A1
The laptop
diannao5
with
A2
zairu1
inquiry
A3
diaocha3
data
A4
ziliaode4
A5
on the event
shijian2
A6
was stolen
zaoqie6
A7
A8
A9
A10
A11
(b)
Figure 2: A Chinese-to-English sentence pair and
its corresponding ITG tree.
will deduce how to integrate the shift-reduce al-
gorithm and ITG constraints and show its correct-
ness (Section 2.1). Second, we will describe the
shift-reduce decoding algorithm in details (Sec-
tion 2.2).
2.1 Adding ITG constraints
In the process of decoding, a source phrase is re-
garded as a block and a source sentence is seen
as a sequence of blocks. The orientation of two
blocks whose translations are adjacent on the tar-
get side can be straight, inverted or discontinu-
ous, as shown in Figure 1. According to ITG,
two blocks which are straight or inverted can be
merged into a single block. For parsing, differ-
ent mergence order of a sequence of continuous
blocks may yield different derivations. In con-
trast, the phrase-based machine translation does
not compute reordering probabilities hierarchi-
cally, so the mergence order will not impact the
computation of reordering probabilities. As a
result, the shift-reduce decoding algorithm need
not take into consideration the shift-reduce diver-
gence. It merges two continuous blocks as soon
as possible, acting in a reduce-eager style.
Every ITG-legal sentence pair has a corre-
286
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry
(a) (b) (c)
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(d) (e) (f)
 
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(g) (h) (i)
Figure 3: The partial translation procedure of the sentence in Figure 2.
sponding ITG tree, and source words covered
by every node (eg. A1, ..., A11 in Figure 2(b))
in the ITG tree can be seen as a block. By
watching the tree in Figure 2, we can find that
a block must be adjacent to the block either on
its left or on its right, then they can be merged
into a larger block. For example, A2 matches
the block [zairu1] and A8 matches the block
[shijian2 diaocha3 ziliaode4].1 The two blocks
are adjacent and they are merged into a larger
block [zairu1 shijian2 diaocha3 ziliaode4],
covered by A9. The procedure of translating
zairu1 shijian2 diaocha3 ziliaode4 diannao5
is illustrated in Figure 3.
For a hypothesis during decoding, we assign it
three factors: the current block, the left neigh-
boring uncovered span and the right neighbor-
ing uncovered span. For example, in Figure
3(c), the current block is [diaocha3] and the left
neighboring uncovered span is [shijian2] and the
right neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-
ing block, for it is not adjacent to [diaocha3]. The
next covered block is [ziliaode4] (as shown in
Figure 3(d)). For [diaocha3] and [ziliaode4] are
adjacent, they are merged. In Figure 3(e), the cur-
rent block is [diaocha3 ziliaode4].
A sentence is translated with ITG constraints iff
1The words within a block are sorted by their order in the
source sentence.
its source side can be covered by an ITG tree. That
is to say, for every hypothesis during decoding, the
next block to cover must be selected from the left
or right neighboring uncovered span.
First, we show that if the next block to cover is
selected in this way, the translation must observe
ITG constraints. For every hypothesis during de-
coding, the immediate left and right words of the
current block face the following three conditions:
(1) The immediately left word is not covered
and the immediately right word is covered, then
the next block to cover must be selected from the
left neighboring uncovered span, eg. for the cur-
rent block [diaocha3 ziliaode4] in Figure 3(e). In
this condition, the ITG tree can be constructed in
the following two ways: either all words in the left
neighboring uncovered span are translated first,
then this span is merged with the current span
(taking three nodes as an example, this case is
shown in Figure 4(a)), or the right part of the left
neighboring uncovered span is merged with the
current block first, then the new block is merged
with the rest part of the left neighboring uncov-
ered span (shown in Figure 4(b)). In a word, only
after all words in the left neighboring uncovered
span are covered, other words can be covered.
(2) The immediately right word is not covered
and the immediately left word is covered. Simi-
larly, only after all words in the right neighboring
uncovered span are covered, other words can be
287
(a) (b)
Figure 4: The two ways that the current block is
merged with its left neighboring uncovered span.
The third node in the first row denotes the current
block, the first and second nodes in the first row
denote left and right parts of the left neighboring
uncovered span, respectively.
covered.
(3) The immediately left and right words are
neither covered. The next block can be selected
from either the left or the right neighboring uncov-
ered span until the immediate left or right word is
covered.
The above operations can be performed recur-
sively until the whole source sentence is merged
into a single block, so the reordering observes ITG
constraints.
Now, we show that translation which is not gen-
erated in the above way must violate ITG con-
straints.
If the next block is selected out of the neighbor-
ing uncovered spans, the current block can be nei-
ther adjacent to the last covered block nor adjacent
to the selected next block, so the current block can
not be merged with any block and the whole sen-
tence can not be covered by an ITG tree. As in
Figure 3(b), if the next block to cover is [zaoqie6],
then [zairu1] is neither adjacent to [diannao5]
nor adjacent to [zaoqie6].
We can conclude that if we select the next block
from the left or right neighboring uncovered span
of the current block, then the translation must ob-
serve ITG constraints.
2.2 Shift-Reduce Decoding Algorithm
In order to generate the translation with ITG con-
straints, the shift-reduce algorithm have to keep
trace of covered blocks, left and right neighboring
uncovered spans. Formally, the shift-reduce de-
coding algorithm uses the following three stacks:
? St: the stack for covered blocks. The blocks
are pushed in the order that they are covered,
not the order that they are in the source sen-
tence.
? Sl : the stack for the left uncovered spans of
the current block. When a block is pushed
into St, its corresponding left neighboring
uncovered span is pushed into Sl.
? Sr :the stack for the right uncovered spans of
the current block. When a block is pushed
into St, its corresponding right neighboring
uncovered span is pushed into Sr.
A translation configuration is a triple c =
?St, Sl, Sr?. Given a source sentence f =
f1, f2, ..., fm, we import a virtual start word and
the whole translation procedure can be seen as
a sequence of transitions from cs to ct, where
cs = ?[0], ?, [1,m]? is the initial configura-
tion, ct = ?[0,m], ?, ?? is the terminal con-
figuration. The configuration for Figure 3 (e) is
?[0][5][1][3, 4], [2], [6]?.
We define three types of transitions from
a configuration to another . Assume the cur-
rent configuration c = ? [ft11, ft12]...[ftk1, ftk2],
[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] ?,
then :
? Transitions LShift pop the top element
[flu1, flu2] from Sl and select a block [i, j]
from [flu1, flu2] to translate. In addition,
they push [i, j] into St, and if i 6= flu1, they
push [flu1, i ? 1] into Sl, and if j 6= flu2,
they push [j+1, flu2] into Sr. The precondi-
tion to operate the transition is that Sl is not
null and the top span of Sl is adjacent to the
top block of St. Formally, the precondition
is flu2 + 1 = ftk1.
? Transitions RShift pop the top element
[frv1, frv2] of Sr and select a block [i, j]
from [frv1, frv2] to translate. In addition,
they push [i, j] into St, and if i 6= frv1, they
push [frv1, i?1] into Sl, and if j 6= frv2, they
push [j + 1, frv2] into Sr. The precondition
is that Sr is not null and the top span of Sr is
288
adjacent to the top block of St. Formally, the
precondition is ftk2 + 1 = frv1.
? Transitions Reduce pop the top two blocks
[ftk?11, ftk?12], [ftk1, ftk2] from St and push
the merged span [ftk?11, ftk2] into St. The
precondition is that the top two blocks are ad-
jacent. Formally, the precondition is ftk?12+
1 = ftk1
The transition sequence of the example in Fig-
ure 2 is listed in Figure 5. For the purpose of
efficiency, transitions Reduce are integrated with
transitions LShift and RShift in practical imple-
mentation. Before transitions LShift and RShift
push [i, j] into St, they check whether [i, j] is ad-
jacent to the top block of St. If so, they change
the top block into the merged block directly.
In practical implementation, in order to further
restrict search space, distortion limit is applied be-
sides ITG constraints: a source phrase can be cov-
ered next only when it is ITG-legal and its distor-
tion does not exceed distortion limit. The distor-
tion d is calculated by d = |starti ? endi?1 ? 1|,
where starti is the start position of the current
phrase and endi?1 is the last position of the last
translated phrase.
3 Related Work
Galley and Manning (2008) present a hierarchi-
cal phrase reordering model aimed at improving
non-local reorderings. Via the hierarchical mer-
gence of two blocks, the orientation of long dis-
tance words can be computed. Their shift-reduce
algorithm does not import ITG constraints and ad-
mits the translation violating ITG constraints.
Zens et al (2004) introduce a left-to-
right decoding algorithm with ITG constraints
on the alignment template system (Och et al,
1999). Their algorithm processes candidate
source phrases one by one through the whole
search space and checks if the candidate phrase
complies with ITG constraints. Besides, their al-
gorithm checks validity via cover vector and does
not formalize ITG structure. The shift-reduce de-
coding algorithm holds ITG structure via three
stacks. As a result, it can offer ITG-legal spans
directly and decode faster. Furthermore, with
Transition St Sl Sr
[0] ? [1, 6]
RShift [0][5] [1, 4] [6]
LShift [0][5][1] ? [2, 4][6]
RShift [0][5][1][3] [2] [4][6]
RShift [0][5][1][3][4] [2] [6]
Reduce [0][5][1][3, 4] [2] [6]
LShift [0][5][1][3, 4][2] ? [6]
Reduce [0][5][1][2, 4] ? [6]
Reduce [0][5][1, 4] ? [6]
Reduce [0][1, 5] ? [6]
Reduce [0, 5] ? [6]
RShift [0, 5][6] ? ?
Reduce [0, 6] ? ?
Figure 5: Transition sequence for the example in
Figure 2. The top nine transitions correspond to
Figure 3 (a), ... , Figure 3 (i), respectively.
the help of ITG structure, it can be extended to
syntax-based models easily.
Xiong et al (2006) propose a BTG-based
model, which uses the context to determine the
orientation of two adjacent spans. It employs the
cube-time CYK algorithm.
4 Experiments
We compare the shift-reduce decoder with the
state-of-the-art decoder Moses (Koehn et al,
2007). The shift-reduce decoder was imple-
mented by modifying the normal search algo-
rithm of Moses to our shift-reduce algorithm,
without cube pruning (Huang and Chiang, 2005).
We retained the features of Moses: four trans-
lation features, three lexical reordering features
(straight, inverted and discontinuous), linear dis-
tortion, phrase penalty, word penalty and language
model, without importing any new feature. The
decoding configurations used by all the decoders,
including beam size, phrase table limit and so on,
were the same, so the performance was compared
fairly.
First, we will show the performance of shift-
reduce algorithm on three data sets with large
training data sets (Section 4.1). Then, we will
analyze the performance elaborately in terms of
accuracy, speed and search ability with a smaller
289
training data set (Section 4.2). All experiments
were done on Chinese-to-English translation tasks
and all results are reported with case insensitive
BLEU score. Statistical significance were com-
puted using the sign-test described in Collins et
al. (Collins et al, 2005).
4.1 Performance Evaluation
We did three experiments to compare the perfor-
mance of the shift-reduce decoder, Moses and the
decoder with ITG constraints using cover vector
(denoted as CV). 2 The shift-reduce decoder de-
coded with two sets of parameters: one was tuned
by itself (denoted as SR) and the other was tuned
by Moses (denoted as SR-same), using MERT
(Och, 2003). Two searching algorithms of Moses
are considered: one is the normal search algorithm
without cubing pruning (denoted as Moses), the
other is the search algorithm with cube pruning
(denoted as Moses-cb). For all the decoders, the
distortion limit was set to 6, the nbest size was set
to 100 and the phrase table limit was 50.
In the first experiment, the development set is
part of NIST MT06 data set including 862 sen-
tences, the test set is NIST MT08 data set and
the training data set contains 5 million sentence
pairs. We used a 5-gram language model which
were trained on the Xinhua and AFP portion of
the Gigaword corpus. The results are shown in
Table 1(a).
In the second experiment, the development data
set is NIST MT02 data set and the test set is NIST
MT05 data set. Language model and the training
data set are the same to that of the first experiment.
The result is shown in Table 1(b).
In the third experiment, the development set
is China Workshop on Machine Translation 2008
data set (denoted as CWMT08) and the test set
is China Workshop on Machine Translation 2007
data set (denoted as CWMT07). The training set
contains 2 Million sentence pairs and the language
model are a 6-gram language model trained on
the Reuter corpus and English corpus. Table 1(c)
gives the results.
In the above three experiments, SR decoder
2The decoder CV is implemented by adding the ITG con-
straints to Moses using the algorithm described in (Zens et
al., 2004).
NIST06 NIST08 speed
Moses 30.24 25.08 4.827
Moses-cb 30.27 23.80 1.501
CV 30.35 26.23** 4.335
SR-same ?? 25.09 3.856
SR 30.47 26.67** 4.126
(a)
NIST02 NIST05 speed
Moses 35.68 35.80 7.142
Moses-cb 35.42 35.03 1.811
CV 35.45 36.56** 6.276
SR-same ?? 35.84 5.008
SR 35.99* 36.42** 5.432
(b)
CWMT08 CWMT07 speed
Moses 27.75 25.91 3.061
Moses-cb 27.82 25.16 0.548
CV 27.71 26.58** 2.331
SR-same ?? 25.97 1.988
SR 28.14* 26.71** 2.106
(c)
Table 1: Performance comparison. Moses: Moses
without cube pruning, Moses-cb: Moses with
cube pruning, CV: the decoder using cover vector,
SR-same: the shift-reduce decoder decoding with
parameters tunes by Moses, SR: the shift-reduce
decoder with parameters tuned by itself. The sec-
ond column stands for develop set, the third col-
umn stands for test set and speed column shows
the average time (seconds) of translating one sen-
tence in the test set. **: significance at the .01
level.
improves the accuracy by 1.59, 0.62, 0.8 BLEU
score (p < .01), respectively, and improves the
speed by 15%, 24%, 30%, respectively. we can
see that SR can improve both the accuracy and
the speed while SR-same can increase the speed
significantly with a slight improvement on the ac-
curacy. As both SR and CV decode with ITG
constraints, they match each other on the accu-
290
27.00
27.50
28.00
28.50
29.00
29.50
30.00
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17
B
LE
U
average decoding speed (s)
d=-1
d=-1
d=-1
SR
SR-same
Moses
Figure 6: Performance comparison on NIST05.
For a curve, the dots correspond to distortion limit
4, 6, 8, 10, 14 and no distortion from left to right.
d = ?1 stands for no distortion limit.
racy. However, the speed of SR is faster than CV.
Cube pruning can improve decoding speed dra-
matically, but it is not risk-free pruning technol-
ogy, so the BLEU score declines obviously.
4.2 Performance Analysis
We make performance analysis with the same ex-
periment configuration as the second experiment
in Section 4.1, except that the training set in
the analysis experiment is FBIS corpus, includ-
ing 289k sentence pairs. In the following exper-
iments, Moses employs the normal search algo-
rithm without cube pruning.
For the decoders employ the linear distortion
feature, the distortion limit will influence the
translation accuracy. Besides, with different dis-
tortion limit, the proportion of ITG-legal transla-
tion generated by Moses will differ. The smaller
the distortion limit is, the greater the proportion is.
So we first compare the performance with differ-
ent distortion limit.
We compare the shift-reduce decoder with
Moses using different distortion limit. The re-
sults are shown in Figure 6. When distortion limit
is set to 6, every decoder gets a peak value and
SR has an improvement of 0.66 BLEU score over
Moses. From the curves, we can see that the
BLEU score of SR-same with distortion limit 8
28.00
28.50
29.00
29.50
30.00
30.50
31.00
31.50
32.00
32.50
33.00
33.50
34.00
34.50
35.00
35.50
36.00
36.50
37.00
 4  6  8  10  12  14  16
B
LE
U
distortion limit
SR
SR-same
Moses
(a) ITG set
25.00
25.50
26.00
26.50
27.00
27.50
28.00
 4  6  8  10  12  14  16
BL
EU
distortion limit
SR
SR-same
Moses
(b) rest set
Figure 7: Accuracy comparison on the ITG set
and rest set of NIST05. The ITG set includes the
sentences the translations of which generated by
Moses are ITG-legal, and the rest set contains the
rest sentences. distortion limit = 16 denotes no
distortion limit.
is lower than that of Mose with distortion limit
6. This is because the decoding speed of SR-
same with distortion limit 8 is not faster than that
of Moses with distortion limit 6. On the whole,
compared to Moses, SR-same can improve the ac-
curacy slightly with much faster decoding speed,
and SR can obtain improvements on both the ac-
curacy and the speed.
We split the test set into two sets: one contains
the sentences, the translations of which generated
by Moses are ITG-legal (denoted as ITG set) and
the other contains the rest (denoted as rest set).
From Figure 7, we can see that no matter on the
ITG set or on the rest set, SR decoder can gain ob-
vious accuracy improvements with all distortion
291
ITG restd
Moses SR-same total < = > Moses SR-same total < = >
4 28.67 28.68 1050 8 1042 0 25.61 25.82 32 0 0 32
6 31.34 31.42 758 51 705 2 25.78 25.72 324 32 2 290
8 32.59 32.93* 594 72 516 6 25.68 25.65 488 82 3 403
10 34.36 34.99** 456 80 365 11 26.04 26.50* 626 147 3 476
12 33.16 33.61** 454 63 380 11 27.01 27.13 628 165 1 462
14 35.98 36.25* 383 60 316 7 26.35 26.67* 699 203 1 495
-1 34.13 34.96** 351 39 308 4 26.17 26.78** 731 154 0 577
Table 2: Search ability comparison. The ITG set and the rest set of NIST05 were tested, respectively.
On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEU
score of SR-same, the number of sentences in the ITG set, the number of sentences the translation
probabilities of which computed by Moses, compared to that computed by SR, is lower, equal and
greater. The rest set goes similarly. *: significance at the .05 level, **: significance at the .01 level.
limit. While SR-same decoder only gets better re-
sults on the ITG set with all distortion limit. This
may result from the use of the linear distortion
feature. Moses may generate hypotheses the dis-
tortion of which is forbidden in the shift-reduce
decoder. This especially sharpens on the rest set.
So SR-same may suffer from an improper linear
distortion parameter.
The search ability of Moses and the shift-
reduce decoder are evaluated, too. The translation
must be produced with the same set of parameters.
In our experiments, we employed the parameters
tuned by Moses. The test was done on the ITG and
the rest set, respectively. The results are shown in
Table 2. As the distortion limit becomes greater,
the number of the ITG-legal translation generated
by Moses becomes smaller. On the ITG set, trans-
lation probabilities from the shift-reduce decoder
is either greater or equal to that from Moses on
most sentences, and BLEU scores of shift-reduce
decoder is greater than that of Moses with all
distortion limit. Although the search space of
shift-reduce decoder is smaller than that of Moses,
shift-reduce decoder can give the translation that
Moses can not reach. On the rest set, for most sen-
tences, the translation probabilities from Moses is
greater than that from shift-reduce decoder. But
only when distortion limit is 6 and 8, the BLEU
score of Moses is greater than that of the shift-
reduce decoder. We may conclude that greater
score does not certainly lead to greater BLEU
score.
5 Conclusions and Future Work
In this paper, we present a shift-reduce decod-
ing algorithm for phrase-based translation model
that can generate the ITG-legal translation in lin-
ear time. The algorithm need not consider shift-
reduce divergence and performs reduce operation
as soon as possible. We compare the performance
of the shift-reduce decoder with the state-of-the-
art decoder Moses. Experiment results show that
the shift-reduce algorithm can improve both the
accuracy and the speed significantly on different
test sets. We further analyze the performance and
find that on the ITG set, the shift-reduce decoder
is superior over Moses in terms of accuracy, speed
and search ability, while on the rest set, it does
not display advantage, suffering from improper
parameters.
Next, we will extend the shift-reduce algorithm
to syntax-based translation models, to see whether
it works.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China Contract 60736014,
National Natural Science Foundation of China
Contract 60873167 and High Technology R&D
Program Project No. 2006AA010108. We are
grateful to the anonymous reviewers for their
valuable comments.
292
References
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Galley, Michel and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages
53?64.
Knight, Kevin. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607?615.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstra-
tion Session.
Koehn, Philipp. 2004. Pharaoh: A beam search de-
coder for phrased-based statistical machine transla-
tion. In Proc. of AMTA, pages 115?124.
Och, Frans J., Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proc. of EMNLP, pages 20?
28.
Och, Frans J. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL,
pages 160?167.
Tillmann, Chirstoph and Hermann Ney. 2003.
Word reordering and a dynamic programming beam
search algorithm for statistical machine translation.
Computational Linguistics, 29:97?133.
Wu, Dekai. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL,
pages 152?158.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL, pages
521?528.
Zens, Richard and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proc. of ACL, pages 144?151.
Zens, Richard, Hermann Ney, Taro Watanable, and
Eiichiro Sumita. 2004. Reordering constraints
for phrase-based statistical machine translation. In
Proc. of COLING, pages 205?211.
293
Coling 2010: Poster Volume, pages 837?845,
Beijing, August 2010
Machine Translation with Lattices and Forests
Haitao Mi?? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Information Sciences Institute
Institute of Computing Technology Viterbi School of Engineering
Chinese Academy of Sciences University of Southern California
{htmi,liuqun}@ict.ac.cn {lhuang,haitaomi}@isi.edu
Abstract
Traditional 1-best translation pipelines
suffer a major drawback: the errors of 1-
best outputs, inevitably introduced by each
module, will propagate and accumulate
along the pipeline. In order to alleviate
this problem, we use compact structures,
lattice and forest, in each module instead
of 1-best results. We integrate both lat-
tice and forest into a single tree-to-string
system, and explore the algorithms of lat-
tice parsing, lattice-forest-based rule ex-
traction and decoding. More importantly,
our model takes into account all the proba-
bilities of different steps, such as segmen-
tation, parsing, and translation. The main
advantage of our model is that we can
make global decision to search for the best
segmentation, parse-tree and translation in
one step. Medium-scale experiments show
an improvement of +0.9 BLEU points over
a state-of-the-art forest-based baseline.
1 Introduction
Statistical machine translation (SMT) has wit-
nessed promising progress in recent years. Typi-
cally, conventional SMT is characterized as a 1-
best pipeline system (Figure 1(a)), whose mod-
ules are independent of each other and only take
as input 1-best results from the previous module.
Though this assumption is convenient to reduce
the complexity of SMT systems. It also bring a
major drawback of error propagation. The errors
of 1-best outputs, introduced inevitably in each
phase, will propagate and accumulate along the
pipeline. Not recoverable in the final decoding
(b)source segmentation lattice
parse forest target
source 1-best segmentation
1-best tree target
(a)
Figure 1: The pipeline of tree-based system: (a) 1-
best (b) lattice-forest.
step. These errors will severely hurt the translation
quality. For example, if the accuracy of each mod-
ule is 90%, the final accuracy will drop to 73%
after three separate phases.
To alleviate this problem, an obvious solution
is to widen the pipeline with k-best lists rather
than 1-best results. For example Venugopal et
al. (2008) use k-best alignments and parses in the
training phase. However, with limited scope and
too many redundancies, it is inefficient to search
separately on each of these similar lists (Huang,
2008).
Another efficient method is to use compact data
structures instead of k-best lists. A lattice or forest,
compactly encoded exponentially many deriva-
tions, have proven to be a promising technique.
For example, Mi and Huang (2008), Mi et al
(2008), Liu et al (2009) and Zhang et al (2009)
use forests in rule extraction and decoding phases
to extract more general rules and weaken the influ-
ence of parsing errors; Dyer et al (2008) use word
lattice in Chinese word segmentation and Arabic
morphological variation phases to weaken the in-
fluence of segmentation errors; Huang (2008) and
837
0 1 2 3 4 5 6 7 8 9c0:Bu` c1:sh?? c2:yu? c3:Sha? c4:lo?ng c5:ju? c6:x??ng c7:ta?o c8:lu`n
(0, 2, NR) (2, 3, CC) (3, 5, NR) (5, 6, VV) (6, 8, NN) (8, 9, NN)
(5, 7, VV) (7, 9, NN)(2, 3, P)
Figure 2: The lattice of the example:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n.? The solid lines show the 1-best
result, which is wrong.
Jiang et al (2008b) stress the problems in re-
ranking phase. Both lattices and forests have be-
come popular in machine translation literature.
However, to the best of our knowledge, previous
work only focused on one module at a time. In this
paper, we investigate the combination of lattice
and forest (Section 2), as shown in Figure 1(b).
We explore the algorithms of lattice parsing (Sec-
tion 3.2), rule extraction (Section 4) and decod-
ing (Section 5). More importantly, in the decoding
step, our model can search among not only more
parse-trees but also more segmentations encoded
in the lattice-forests and can take into account all
the probabilities of segmentations and parse-trees.
In other words, our model postpones the disambi-
guition of segmentation and parsing into the final
translation step, so that we can do global search
for the best segmentation, parse-tree and transla-
tion in one step. When we integrate a lattice into
a forest system, medium-scale experiments (Sec-
tion 6) show another improvement of +0.9 BLEU
points over a state-of-the-art forest-based system.
2 Compact Structures
A word lattice (Figure 2) is a compact representa-
tion of all the possible of segmentations and POS
tags, while a parse forest (Figure 5) is a compact
representation of all parse trees.
2.1 Word Lattice
For a given input sentence C = c0..cn?1, where
ci denotes a character at position i, and n is the
length of the sentence.
A word lattice (Figure 2), or lattice in short, is
a set of edges L, where each edge is in the form
of (i, j, X), which denotes a word of tag X , cov-
ering characters ci through cj?1. For example, in
Figure 2, (7, 9, NN) is a noun ?ta?olu`n? of two char-
acters.
The lattice in Figure 2 shows result of the ex-
ample:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n ?.
One ambiguity comes from the POS tag of word
?yu?? (preposition (P) or conjunction (CC)). The
other one is the segmentation ambiguity of the last
four characters, we can segment into either ?ju?
x??ngta?o lu`n? (solid lines), which means lift, beg-
ging and argument separately for each word or
?ju?x??ng ta?olu`n? (dashed lines), which means hold
a discussion.
lift begging argument
5 ju? 6 x??ng 7 ta?o 8 lu`n 9
hold a discussion
The solid lines above (and also in Figure 2)
show the 1-best result, which is obviously wrong.
If we feed it into the next modules in the SMT
pipeline, parsing and translation will be become
much more difficult, since the segmentation is not
recoverable. So it is necessary to postpone er-
ror segmentation decisions to the final translation
step.
2.2 Parse Forest
In parsing scenario, a parse forest (Figrure 5), or
forest for short, can be formalized as a hyper-
graph H , a pair ?V, E?, where node v ? V is in
the form of Xi,j , which denotes the recognition of
nonterminal X spanning the substring ci:j?1 from
positions ci through cj?1. Each hyperedge e ? E
is a pair ?tails(e), head(e)?, where head(e) ? V
is the consequent node in an instantiated deduc-
tive step, and tails(e) ? (V )? is the list of an-
tecedent nodes.
For the following deduction:
NR0,2 CC2,3 NR3,5
NP0,5 (*)
838
its hyperedge e? is notated:
?(NR0,2, CC2,3, NR3,5), NP0,5?.
where
head(e?) = {NP0,5}, and
tails(e?) = {NR0,2,CC2,3,NR3,5}.
We also denote IN (v) to be the set of incoming
hyperedges of node v, which represents the dif-
ferent ways of deriving v. For simplicity, we only
show a tree in Figure 5(a) over 1-best segmenta-
tion and POS tagging result in Figure 2. So the
IN (NP0,5) is {e?}.
3 Lattice Parsing
In this section, we first briefly review the con-
ventional CYK parsing, and then extend to lattice
parsing. More importantly, we propose a more ef-
ficient parsing paradigm in Section 3.3.
3.1 Conventional Parsing
The conventional CYK parsing algorithm in Fig-
ure 3(a) usually takes as input a single sequence of
words, so the CYK cells are organized over words.
This algorithm consists of two steps: initialization
and parsing. The first step is to initialize the CYK
cells, whose span size is one, with POS tags pro-
duced by a POS tagger or defined by the input
string1. For example, the top line in Figure 3(a)
is initialized with a series of POS tags in 1-best
segmentation. The second step is to search for the
best syntactic tree under a context-free grammar.
For example, the tree composed by the solid lines
in Figure 5(a) shows the parsing tree for the 1-best
segmentation and POS tagging results.
3.2 Lattice Parsing
The main differences of our lattice parsing in Fig-
ure 3(b) from conventional approach are listed in
following: First, the CYK cells are organized over
characters rather than words. Second, in the ini-
tialization step, we only initialize the cells with
all edges L in the lattice. Take the edge (7, 9,
NN) in Figure 2 for example, the corresponding
cell should be (7, 9), then we add a leaf node
v = NN7,9 with a word ta?olu`n. The final initial-
ization is shown in Figure 3(b), which shows that
1For simplicity, we assume the input of a parser is a seg-
mentation and POS tagging result
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC NR VV NN NN
NP VPB
IP
O(n3w)
(a): Parsing over 1-best segmentation
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR
CC,P
NR
VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3)
(b): Parsing over characters
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC,P NR VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3r)
(c): Parsing over most-refined segmentation
Figure 3: CKY parsing charts (a): Conventional
parsing over 1-best segmentation. (b): Lattice
parsing over characters of input sentence. (c): Lat-
tice parsing over most-refined segmentation of lat-
tice. nw and nr denotes the number of tokens over
the 1-best segmentation and the most-refined seg-
menation respectively, and nw ? nr ? n.
lattice parsing can initialize the cells, whose span
size is larger than one. Third, in the deduction step
of the parsing algorithm i, j, k are the indexes be-
tween characters rather than words.
We formalize our lattice parser as a deductive
proof system (Shieber et al, 1994) in Figure 4.
Following the definitions of the previous Sec-
839
tion, given a set of edges L of a lattice for an in-
put sentence C = c0..cn?1 and a PCFG grammar:
a 4-tuple ?N, ?, P, S?, where N is a set of non-
terminals, ? is a set of terminal symbols, P is a
set of inference rules, each of which is in the form
of X ? ? : p for X ? N , ? ? (N ? ?)? and p is
the probability, and S ? N is the start symbol. The
deductive proof system (Figure 4) consists of ax-
ioms, goals and inference rules. The axioms are
converted by edges in L. Take the (5, 7, NN) as-
sociated with a weight p1 for example, the corre-
sponding axiom is NN ? ta?olu`n : p1. All axioms
converted from the lattice are shown in Figure 3(b)
exclude the italic non-terminals. Please note that
all the probabilities of the edges L in a lattice are
taken into account in the parsing step. The goals
are the recognition X0,n ? S of the whole sen-
tence. The inference rules are the deductions in
parsing. Take the deduction (*) for example, it will
prove a new item NP0,5 (italic NP in Figure 3(b))
and generate a new hyper-edge e? (in Figure 5(b)).
So the parsing algorithm starts with the axioms,
and then applies the inference rules to prove new
items until a goal item is proved. The final whole
forest for the input lattice (Figure 2) is shown in
Figure 5(b). The extra hyper-edges of lattice-forest
are highlighted with dashed lines, which can in-
ference the input sentence correctly. For example:
?yu?? is tagged into P rather than CC.
3.3 Faster Parsing with Most-refined Lattice
However, our statistics show that the average num-
ber of characters n in a sentence is 1.6 times than
the number of words nw in its 1-best segmenta-
tion. As a result, the parsing time over the charac-
ters will grow more than 4 times than parsing over
the 1-best segmentation, since the time complexity
is O(n3). In order to alleviate this problem, we re-
duce the parsing time by using most-refined seg-
mentation for a lattice, whose number of tokens
is nr and has the property nw ? nr ? n.
Given a lattice with its edges L over indexes
(0, .., n), a index i is a split point, if and only if
there exists some edge (i, j, X) ? L or (k, i, X) ?
L. The most-refined segmentation, or ms for
short, is the segmentation result by using all split
points in a lattice. For example, the corresponding
ms of the example is ?Bu`sh?? yu? Sha?lo?ng ju? x??ng
ta?o lu`n? since points 1 and 4 are not split points.
Item form: Xi,j
Axioms: Xi,j : p(i, j, X)
(i, j, X) ? L
Infer. rules:
Xi,k : p1 Yk,j : p2
Zi,j : pp1p2
Z ? XY : p ? P
Goals: X0,n
Figure 4: Lattice parsing as deductive proof sys-
tem. The i, j, k are the indexes between characters.
Figure 3(c) shows the CKY parsing cells over
most-refined segmentation, the average number
of tokens nr is reduced by combining columns,
which are shown with red dashed boxes. As a re-
sult, the search space is reduced without losing any
derivations. Theoretically, the parsing over fs will
speed up in O((n/nr)3). And our experiments in
Section 6 show the efficiency of our new approach.
It turns out that the parsing algorithm developed
in lattice-parsing Section 3.2 can be used here
without any change. The non-terminals inducted
are also shown in Figure 3(c) in italic style.
4 Rule Extraction with Lattice & Forest
We now explore the extraction algorithm from
aligned source lattice-forest and target string2,
which is a tuple ?F, ?, a? in Figure 5(b). Following
Mi and Huang (2008), we extract minimal rules
from a lattice-forest also in two steps:
(1) frontier set computation
(2) fragmentation
Following the algorithms developed by Mi and
Huang (2008) in Algorithm 1, all the nodes in
frontier set (fs) are highlighted with gray in Fig-
ure 5(b).
Our process of fragmentation (lines 1- 13) is
to visit each frontier node v and initial a queue
(open) of growing fragments with a pair of empty
fragment and node v (line 3). Each fragment is as-
sociated with a list of expansion sites (front) being
2For simplicity and consistency, we use character-based
lattice-forest for the running example. The ?Bu`? and ?sh???
are aligned to the same word ?Bush?. In our experiment,
we use most-refined segmentation to run lattice-parsing and
word alignment.
840
IP0,9
NP0,5 VPB5,9
(a)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
.NR0,2 .CC2,3 .NR3,5 .VV5,6 .NN6,8 .NN8,9
e?
IP0,9
NP0,5 VP2,9
PP2,5 VPB5,9
(b)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
. NR0,2 . CC2,3 . NR3,5 .VV5,6 .NN6,8 .NN8,9. VV5,7 . NN7,9. P2,3
e?
Bush held a discussion with Sharon
Forest only (Minimal rules) Lattice & forest (Extra minimal rules)
(c)
IP(NP(x1:NR x2:CC x3:NR) x4:VPB) IP(x1:NR x2:VP) ? x1 x2
? x1 x4 x2 x3 VP(x1:PP x2:VPB) ? x2 x1
CC(yu?) ?with PP(x1:P x2:NR) ? x1 x2
NR(Sha?lo?ng) ?Sharon P(yu?) ?with
NR(Bu`sh??) ?Bush VPB(x1:VV x2:NN) ? x1 x2
VPB(VV(ju?) NN(x??ngta?o) NN(lu`n)) VV(ju?x??ng) ?held
?held a discussion NN(ta?olu`n) ?a discussion
Figure 5: (a): The parse forest over the 1-best segmentation and POS tagging result. (b): Word-aligned
tuple ?F, ?, a?: the lattice-forest F , the target string ? and the word alingment a. The solid hyperedges
form the forest in (a). The dashed hyperedges are the extra hyperedges introduced by the lattice-forest.
(c): The minimal rules extracted on forest-only (left column), and the extra minimal rules extracted on
lattice-forest (right column).
the subset of leaf nodes of the current fragment
that are not in the fs except for the initial node
v. Then we keep expanding fragments in open in
following way. If current fragment is complete,
whose expansion sites is empty, we extract rule
corresponding to the fragment and its target string
841
Code 1 Rule Extraction (Mi and Huang, 2008).
Input: lattice-forest F , target sentence ? , and
alignment a
Output: minimal rule set R
1: fs ? FROSET(F, ?, a)  frontier set
2: for each v ? fs do
3: open ? {??, {v}?}  initial queue
4: while open 6= ? do
5: ?frag , front? ? open.pop()
6: if front = ? then  finished?
7: generate a rule r using frag
8: R.append(r)
9: else  incomplete: further expand
10: u ? front .pop()  expand frontier
11: for each e ? IN (u) do
12: f ? front ? (tails(e) \ fs)
13: open .append(?frag ? {e}, f ?)
(line 7) . Otherwise we pop one expansion node
u to grow and spin-off new fragments by IN (u),
adding new expansion sites (lines 11- 13), until all
active fragments are complete and open queue is
empty.
The extra minimal rules extracted on lattice-
forest are listed at the right bottom of Figure 5(c).
Compared with the forest-only approach, we can
extract smaller and more general rules.
After we get al the minimal rules, we com-
pose two or more minimal rules into composed
rules (Galley et al, 2006), which will be used in
our experiments.
For each rule r extracted, we also assign a frac-
tional count which is computed by using inside-
outside probabilities:
c(r) =
?(root(r)) ? P(lhs(r)) ? Qv?yield(root(r)) ?(v)
?(TOP) ,
(1)
where root(r) is the root of the rule, lhs(r) is
the left-hand-side of rule, rhs(r) is the right-
hand-side of rule, P(lhs(r)) is the product of
all probabilities of hyperedges involved in lhs(r),
yield(root(r)) is the leave nodes, TOP is the root
node of the forest, ?(v) and ?(v) are outside and
inside probabilities, respectively.
Then we compute three conditional probabili-
ties for each rule:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(2)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(3)
P(r | root(r)) = c(r)?
r?:root(r?)=root(r) c(r?)
. (4)
All these probabilities are used in decoding step
(Section 5). For more detail, we refer to the algo-
rithms of Mi and Huang (2008).
5 Decoding with Lattice & Forest
Given a source-side lattice-forest F , our decoder
searches for the best derivation d? among the set of
all possible derivation D, each of which converts
a tree in lattice-forest into a target string ? :
d? = argmax
d?D,T?F
P (d|T )?0 ? e?1|d|
? LM(?(d))?2 ? e?3|?(d)|,
(5)
where |d| is the penalty term on the number of
rules in a derivation, LM(?(d)) is the language
model and e?3|?(d)| is the length penalty term on
target translation. The P (d|T ) decomposes into
the product of rule probabilities P (r), each of
which is decomposed further into
P (d|T ) =
?
r?d
P (r). (6)
Each P (r) in Equation 6 is decomposed further
into the production of five probabilities:
P(r) = P(r|lhs(r))?4
? P(r|rhs(r))?5
? P(r|root(lhs(r))?6
? Plex(lhs(r)|rhs(r))?7
? Plex(rhs(r)|lhs(r))?8 ,
(7)
where the last two are the lexical probabilities be-
tween the terminals of lhs(r) and rhs(r). All the
weights of those features are tuned by using Min-
imal Error Rate Training (Och, 2003).
Following Mi et al (2008), we first convert the
lattice-forest into lattice translation forest with the
conversion algorithm proposed byMi et al (2008),
842
and then the decoder finds the best derivation on
the lattice translation forest. For 1-best search, we
use the cube pruning technique (Chiang, 2007;
Huang and Chiang, 2007) which approximately
intersects the translation forest with the LM. For
k-best search after getting 1-best derivation, we
use the lazy Algorithm 3 of Huang and Chiang
(2005) to incrementally compute the second, third,
through the kth best alternatives.
For more detail, we refer to the algorithms of
Mi et al (2008).
6 Experiments
6.1 Data Preparation
Our experiments are on Chinese-to-English trans-
lation. Our training corpus is FBIS corpus with
about 6.9M/8.9M words in Chinese/English re-
spectively.
We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
Kneser-Ney smoothing on the first 1/3 of the Xin-
hua portion of Gigaword corpus.
We use the 2002 NIST MT Evaluation test set
as development set and the 2005 NIST MT Eval-
uation test set as test set. We evaluate the trans-
lation quality using the case-insensitive BLEU-4
metric (Papineni et al, 2002). We use the standard
MERT (Och, 2003) to tune the weights.
6.1.1 Baseline Forest-based System
We first segment the Chinese sentences into the
1-best segmentations using a state-of-the-art sys-
tem (Jiang et al, 2008a), since it is not necessary
for a conventional parser to take as input the POS
tagging results. Then we parse the segmentation
results into forest by using the parser of Xiong et
al. (2005). Actually, the parser will assign multiple
POS tags to each word rather than one. As a result,
our baseline system has already postponed the
POS tagging disambiguition to the decoding step.
Forest is pruned by using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold are pf = 5 and pf = 10 at
rule extraction and decoding steps respectively.
We word-align the strings of 1-best segmenta-
tions and target strings with GIZA++ (Och and
Ney, 2000) and apply the refinement method
?grow-diag-final-and? (Koehn et al, 2003) to get
the final alignments. Following Mi and Huang
(2008) and Mi et al (2008), we also extract rules
from forest-string pairs and translate forest to
string.
6.1.2 Lattice-forest System
We first segment and POS tag the Chinese sen-
tences into word lattices using the same sys-
tem (Jiang et al, 2008a), and prune each lat-
tice into a reasonable size using the marginal
probability-based pruning algorithm.
Then, as current GIZA++ (Och and Ney, 2000)
can only handle alignment between string-string
pairs, and word-alingment with the pairs of Chi-
nese characters and target-string will obviously re-
sult in worse alignment quality. So a much better
way to utilize GIZA++ is to use the most-refined
segmentation for each lattice instead of the char-
acter sequence. This approach can be viewed as a
compromise between character-string and lattice-
string word-alignment paradigms. In our exper-
iments, we construct the most-refined segmen-
tations for lattices and word-align them against
the English sentences. We again apply the refine-
ment method ?grow-diag-final-and? (Koehn et al,
2003) to get the final alignments.
In order to get the lattice-forests, we modi-
fied Xiong et al (2005)?s parser into a lattice
parser, which produces the pruned lattice forests
for both training, dev and test sentences. Finally,
we apply the rule extraction algorithm proposed in
this paper to obtain the rule set. Both lattices and
forests are pruned using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold of lattice is pl = 20 at both
the rule extraction and decoding steps, the thresh-
olds for the latice-forests are pf = 5 and pf = 10
at rule extraction and decoding steps respectively.
6.2 Results and Analysis
Table 1 shows results of two systems. Our lattice-
forest (LF) system achieves a BLEU score of
29.65, which is an absolute improvement of 0.9
points over the forest (F) baseline system, and the
improvement is statistically significant at p < 0.01
using the sign-test of Collins et al (2005).
The average number of tokens for the 1-best
and most-refined segmentations are shown in sec-
ond column. The average number of characters
is 46.7, which is not shown in Table 1. Com-
843
Sys Avg # of Rules BLEU
tokens links All dev&tst
F 28.7 35.1 29.6M 3.3M 28.75
LF 37.1 37.1 23.5M 3.4M 29.65
Table 1: Results of forest (F) and lattice-forest
(LF) systems. Please note that lattice-forest system
only extracts 23.5M rules, which is only 79.4% of
the rules extracted by forest system. However, in
decoding step, lattice-forest system can use more
rules after filtered on dev and test sets.
pared with the characters-based lattice parsing, our
most-refined lattice parsing speeds up parsing by
(37.1/46.7)3 ? 2 times, since parsing complexity
is O(n3).
More interestingly, our lattice-forest model only
extracts 23.5M rules, which is 79.4% percent of
the rules extracted by the baseline system. The
main reason lies in the larger average number
of words for most-refined segmentations over lat-
tices being 37.1 words vs 28.7 words over 1-best
segmentations. With much finer granularity, more
word aligned links and restrictions are introduced
during the rule extraction step by GIZA++. How-
ever, more rules can be used in the decoding step
for the lattice-forest system, since the lattice-forest
is larger than the forest over 1-best segmentation.
We also investigate the question of how often
the non 1-best segmentations are picked in the fi-
nal translation. The statistic on our dev set sug-
gests 33% of sentences choose non 1-best segmen-
tations. So our lattice-forest model can do global
search for the best segmentation and parse-tree to
direct the final translation. More importantly, we
can use more translation rules in the translation
step.
7 Related Works
Compactly encoding exponentially many deriva-
tions, lattice and forest have been used in some
previous works on SMT. To alleviate the prob-
lem of parsing error in 1-best tree-to-string trans-
lation model, Mi et al (2008) first use forest to
direct translation. Then Mi and Huang (2008) use
forest in rule extraction step. Following the same
direction, Liu et al (2009) use forest in tree-
to-tree model, and improve 1-best system by 3
BLEU points. Zhang et al (2009) use forest in
tree-sequence-to-string model and also achieve a
promising improvement. Dyer et al (2008) com-
bine multiple segmentations into word lattice and
then use lattice to direct a phrase-based transla-
tion decoder. Then Dyer (2009) employ a single
Maximum Entropy segmentation model to gen-
erate more diverse lattice, they test their model
on the hierarchical phrase-based system. Lattices
and forests can also be used in Minimal Error
Rate Training and Minimum Bayes Risk Decod-
ing phases (Macherey et al, 2008; Tromble et al,
2008; DeNero et al, 2009; Kumar et al, 2009; Li
and Eisner, 2009). Different from the works listed
above, we mainly focus on how to combine lattice
and forest into a single tree-to-string system.
8 Conclusion and Future Work
In this paper, we have proposed a lattice-forest
based model to alleviate the problem of error prop-
agation in traditional single-best pipeline frame-
work. Unlike previous works, which only focus on
one module at a time, our model successfully in-
tegrates lattice into a state-of-the-art forest tree-to-
string system. We have explored the algorithms of
lattice parsing, rule extraction and decoding. Our
model postpones the disambiguition of segmenta-
tion and parsing into the final translation step, so
that we can make a more global decision to search
for the best segmentation, parse-tree and transla-
tion in one step. The experimental results show
that our lattice-forest approach achieves an abso-
lute improvement of +0.9 points in term of BLEU
score over a state-of-the-art forest-based model.
For future work, we would like to pay more
attention to word alignment between lattice pairs
and forest pairs, which would be more principled
than our current method of word alignment be-
tween most-refined segmentation and string.
Acknowledgement
We thank Steve DeNeefe and the three anony-
mous reviewers for comments. The work is sup-
ported by National Natural Science Foundation
of China, Contracts 90920004 and 60736014,
and 863 State Key Project No. 2006AA010108
(H. M and Q. L.), and in part by DARPA GALE
Contract No. HR0011-06-C-0022, and DARPA
under DOI-NBC Grant N10AP20031 (L. H and
H. M).
844
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL/IJCNLP.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, June.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney,
Australia, July.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling
2008.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133, Edmon-
ton, Canada, May.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the ACL/IJCNLP 2009.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of EMNLP, pages 40?51, Singapore,
August. Association for Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of EMNLP 2008.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206?214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311?318, Philadephia, USA, July.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of de-
ductive parsing.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP 2008.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in MT training. In Proceed-
ings of AMTA.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
845
Coling 2010: Poster Volume, pages 1185?1193,
Beijing, August 2010
Dependency-Based Bracketing Transduction Grammar
for Statistical Machine Translation
Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan Lu?, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{sujinsong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn
Abstract
In this paper, we propose a novel
dependency-based bracketing transduc-
tion grammar for statistical machine
translation, which converts a source sen-
tence into a target dependency tree. Dif-
ferent from conventional bracketing trans-
duction grammar models, we encode tar-
get dependency information into our lex-
ical rules directly, and then we employ
two different maximum entropy models
to determine the reordering and combi-
nation of partial dependency structures,
when we merge two neighboring blocks.
By incorporating dependency language
model further, large-scale experiments on
Chinese-English task show that our sys-
tem achieves significant improvements
over the baseline system on various test
sets even with fewer phrases.
1 Introduction
Bracketing transduction grammar (BTG) (Wu,
1995) is an important subclass of synchronous
context free grammar, which employs a special
synchronous rewriting mechanism to parse paral-
lel sentence of both languages.
Due to the prominent advantages such as the
simplicity of grammar and the good coverage of
syntactic diversities in different language pairs,
BTG has attracted increasing attention in statis-
tical machine translation (SMT). In flat reorder-
ing model (Wu, 1996; Zens et al, 2004), which
assigns constant reordering probabilities depend-
ing on the language pairs, BTG constraint proves
to be very effective for reducing the search space
of phrase reordering. To pursue a better method
to predict the order between two neighboring
blocks1, Xiong et al (2006) present an enhanced
BTG with a maximum entropy (ME) based re-
ordering model. Along this line, source-side syn-
tactic knowledge is introduced into the reorder-
ing model to improve BTG-based translation (Se-
tiawan et al, 2007; Zhang et al, 2007; Xiong et
al., 2008; Zhang and Li, 2009). However, these
methods mainly focus on the utilization of source
syntactic knowledge, while ignoring the modeling
of the target-side syntax that directly influences
the translation quality. As a result, how to ob-
tain better translation by exploiting target syntac-
tic knowledge is somehow neglected. Thus, we
argue that it is important to model the target-side
syntax in BTG-based translation.
Recently, modeling syntactic information on
the target side has progressed significantly. De-
pending on the type of output, these models can
be divided into two categories: the constituent-
output systems (Galley et al, 2006; Zhang et
al., 2008; Liu et al, 2009) and dependency-
output systems (Eisner, 2003; Lin, 2004; Ding
and Palmer, 2005; Quirk et al, 2005; Shen et
al., 2008). Compared with the constituent-output
systems, the dependency-output systems provide a
simpler platform to capture the target-side syntac-
tic information, while also having the best inter-
lingual phrasal cohesion properties (Fox, 2002).
Typically, Shen et al (2008) propose a string-to-
dependency model, which integrates the target-
side well-formed dependency structure into trans-
lation rules. With the dependency structure, this
system employs a dependency language model
(LM) to exploit long distance word relations, and
achieves a significant improvement over the hier-
archical phrase-based system (Chiang, 2007). So
1A block is a bilingual phrase without maximum length
limitation.
1185
we think it will be a promising way to integrate the
target-side dependency structure into BTG-based
translation.
In this paper, we propose a novel dependency-
based BTG (DepBTG) for SMT, which represents
translation in the form of dependency tree. Ex-
tended from BTG, our grammars operate on two
neighboring blocks with target dependency struc-
ture. We integrate target syntax into bilingual
phrases and restrict target phrases to the well-
formed structures inspired by (Shen et al, 2008).
Then, we adopt two ME models to predict how to
reorder and combine partial structures into a target
dependency tree, which gives us access to captur-
ing the target-side syntactic information. To the
best of our knowledge, this is the first effort to
combine the translation generation with the mod-
eling of target syntactic structure in BTG-based
translation.
The remainder of this paper is structured as fol-
lows: In Section 2, we give brief introductions to
the bases of our research: BTG and dependency
tree. In Section 3, we introduce DepBTG in detail.
In Section 4, we further illustrate how to create
two ME models to predict the reordering and de-
pendency combination between two neighboring
blocks. Section 5 describes the implementation
of our decoder. Section 6 shows our experiments
on Chinese-English task. Finally, we end with a
summary and future research in Section 7.
2 Background
2.1 BTG
BTG is a special case of synchronous context free
grammar. There are three rules utilized in BTG:
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
where the reordering rules (1) and (2) are used
to merge two neighboring blocks A1 and A2 in
a straight or inverted order, respectively. The lex-
ical rule (3) is used to translate the source phrase
x into the target phrase y.
 



	




	
 
		

	
 


	


 








Figure 1: The dependency tree for sentence The
UN will provide abundant financial aid to Haiti
next week.
2.2 Dependency Tree
In a given sentence, each word depends on a par-
ent word, except for the root word. The depen-
dency tree for a given sentence reflects the long
distance dependency and grammar relations be-
tween words. Figure 1 shows an example of a de-
pendency tree, where a black arrow points from a
child word to its parent word.
Compared with constituent tree, dependency
tree directly models semantic structure of a sen-
tence in a simpler form. Thus, it provides a desir-
able platform for us to utilize the target-side syn-
tactic knowledge.
3 Dependency-based BTG
3.1 Grammars
In this section, we extend the original BTG into
DepBTG. The rules of DepBTG, which derive
from that of BTG, merge blocks with target de-
pendency structure into a larger one. These rules
take the following forms:
Ad ? [A1d, A2d]CC (4)
Ad ? [A1d, A2d]LA (5)
Ad ? [A1d, A2d]RA (6)
Ad ? ?A1d, A2d?CC (7)
Ad ? ?A1d, A2d?LA (8)
Ad ? ?A1d, A2d?RA (9)
Ad ? x/y (10)
where A1d and A2d represent two neighboring
blocks with target dependency structure. Rules
(4)?(9) are used to determine the reordering and
combination of two dependency structures, when
1186

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1133?1143, Dublin, Ireland, August 23-29 2014.
A Structured Language Model for Incremental Tree-to-String Translation
Heng Yu1
1Institute of Computing Technology. CAS
University of Chinese Academy of Sciences
yuheng@ict.ac.cn
Haitao Mi
T.J. Watson Research Center
IBM
hmi@us.ibm.com
Liang Huang
Queens College & Grad. Center
City University of New York
huang@cs.qc.cuny.edu
Qun Liu1,2
2Centre for Next Generation Localisation.
Faculty of Engineering and Computing
Dublin City University
qliu@computing.dcu.ie
Abstract
Tree-to-string systems have gained significant popularity thanks to their simplicity and efficien-
cy by exploring the source syntax information, but they lack in the target syntax to guarantee
the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate
a structured language model, a left-to-right shift-reduce parser in specific, into an incremental
tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integra-
tion. Large-scale experiments on various Chinese-English test sets show that with a reasonable
speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
1 Introduction
Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained
significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et
al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster
by proposing an incremental tree-to-string model, which generates the target translation exactly in a left-
to-right manner. Although, tree-to-string models have made those progresses, they can not utilize the
target syntax information to guarantee the grammaticality of the output, as they only generate strings on
the target side.
One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree
models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches
still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al.,
2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010).
Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Char-
niak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved
better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is inde-
pendent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the
problems that tree-to-tree models have. However, integration is not easy, as the following two questions
arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures.
Second, hypotheses in the same bin may not be comparable, since their syntactic structures may not be
comparable, and the future costs are hard to estimate. Hassan et al. (2009) skip those problems by only
keeping the best parsing structure for each hypothesis.
In this paper, we integrate a shift-reduce parser into an incremental tree-to-string model, and intro-
duce an efficient grouping and pruning method to handle the growing search space and incomparable
hypotheses problems. Large-scale experiments on various Chinese-English test sets show that with a rea-
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1133
sonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
2 Linear-time Shift-reduce Parsing
parsing
action signature dependency structure
s1 s0 q0
Bush S 0
sh Bush held S 1: Bush
sh Bush held a S 2: Bush held
re
x
held
Bush
a S 3: Bush held
sh held
Bush
a meeting S 4: Bush held a
sh a meeting with S 5: Bush held a meeting
re
x
held
Bush
meeting
a
with S 6: Bush held a meeting
re
y
held
Bush meeting
with S 7: Bush held a meeting
sh held
Bush meeting
with Sharon S 8: Bush held a meeting with
sh with Sharon S 9: Bush held a meeting with Sharon
re
y
held
Bush meeting
with
Sharon
S 10: Bush held a meeting with Sharon
re
y
held
Bush meeting with
S 11: Bush held a meeting with Sharon
Figure 1: Linear-time left-to-right dependency parsing.
A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step,
chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re)
the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing
scenario, the reduce action is further divided into two cases: left-reduce (re
x
) and right-reduce (re
y
),
depending on which one of the two items becomes the head after reduction. Each parsing derivation can
be represented by a sequence of parsing actions.
1134
2.1 Shift-reduce Dependency Parsing
We will use the following sentence as the running example:
Bush held a meeting with Sharon
Given an input sentence e, where ei is the ith token, ei...e j is the substring of e from i to j, a shift-reduce
parser searches for a dependency tree with a sequence of shift-reduce moves (see Figure 1). Starting
from an initial structure S 0, we first shift (sh) a word e1, ?Bush?, onto the parsing stack s0, and form a
structure S 1 with a singleton tree. Then e2, ?held?, is shifted, and there are two or more structures in the
parsing stack, we can use re
x
or re
y
step to combine the top two trees on the stack, replace them with
dependency structure e1 x e0 or e1 y e0 (shown as S 3), and add one more dependency edge between
e0 and e1.
Note that the shade nodes are exposed heads on which re
x
or re
y
parsing actions can be performed.
The middle columns in Figure 1 are the parsing signatures: q0 (parsing queue), s0 and s1 (parsing stack),
where s0 and s1 only have one level dependency. Take the line of S 11 for example, ?a? is not in the
signature. As each action results in an update of cost, we can pick the best one (or few, with beam) after
each action. Costs are accumulated in each step by extracting contextual features from the structure and
the action. As the sentence gets longer, the number of partial structures generated at each steps grows
exponentially, which makes it impossible to search all of the hypothesis. In practice, we usually use beam
search instead.
(a) atomic features
s0.w s0.t
s1.w s1.t
s0.lc.t s0.rc.t
q0.w q0.t
(b) feature templates
unigram
s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
bigram
s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.w ? s1.w ? s1.t s0.t ? s1.w ? s1.t
s0.w ? s0.t ? s1.w
trigram s0.t ? s1.t ? q0.t s1.t ? s0.t ? s0.lc.t
s1.t ? s0.t ? q0.t s1.t ? s0.t ? s0.rc.t
(c) ?? parsing stack parsing queue ??
. . . s1 s0
s0.lc ? ? ? s0.rc
q0
Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from
Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree,
x.lc and x.rc denote x?s leftmost and rightmost child respectively. (c) the feature window.
2.2 Features
We view features as ?abstractions? or (partial) observations of the current structure. Feature templates f
are functions that draw information from the feature window, consisting of current partial tree and first
word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic
1135
IP
NP
Bu`sh??
VP
PP
P
yu?
NP
Sha?lo?ng
VP
VV
ju?x??ng
AS
le
NP
hu?`ta?n
Figure 2: A parse tree
features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way
classification based on f, and conjoin these feature instances with each action:
[f ? (action=sh/re
x
/re
y
)]
We extract all the feature templates from training data, and use the average perceptron algorithm and
early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model.
3 Incremental Tree-to-string Translation with Slm
The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps:
parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and
the linear incremental decoder then searches for the best derivation that generates a target-language string
in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the
following section.
3.1 Decoding with Slm
Since the incremental tree-to-string model generates translation in strictly left-to-right fashion, and the
shift-reduce dependency parser also processes an input sentence in left-to-right order, it is intuitive to
combine them together. The last two columns in Figure 3 show the dependency structures for the corre-
sponding hypotheses. Start at the root translation stack with a dot  before the root node IP:
[ IP ],
we first predict (pr) with rule r1,
(r1) IP (x1:NP x2:VP)? x1 x2,
and push its English-side to the translation stack, with variables replaced by matched tree nodes, here
x1 for NP and x2 for VP. Since this translation action does not generate any translation string, we don?t
perform any dependency parsing actions. So we have the following translation stack
[ IP ][ NP VP],
where the dot  indicates the next symbol to process in the English word-order. Since node NP is the next
symbol, we then predict with rule r2,
(r2) NP(Bu`sh??)? Bush,
and add it to the translation stack:
[ IP ] [ NP VP ] [ Bush]
Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current
translation, which results in the new translation stack
[ IP ] [ NP VP ] [Bush  ]
1136
translation parsing
stack string dependency structure Slm
[  IP ] S 0
1 pr [  IP ] [  NP VP] S 0
2 pr [  IP ] [ NP VP ] [  Bush ] S 0
3 sc [  IP ] [ NP VP] [Bush  ] Bush S 1: Bush P(Bush | S 0)
4 co [  IP ] [NP  VP] S 1:
5 pr [  IP ] [NP  VP] [ held NP with NP] S 1:
6 sc [  IP ] [NP  VP] [held  NP with NP] held S 3: Bush held P(held | S 1)
7 pr [ IP] [NP VP] [held NP with NP] [ a meeting] S 3
8 sc [ IP] [NP VP] [held  NP with NP] [a meeting  ] a meeting S 7: Bush held a meeting P(a meeting | S 3)
9 co [ IP ] [NP VP] [held NP  with NP] S 7
10 sc [ IP] [NP VP] [held NP with  NP] with S 8: Bush held a meeting with P(with | S 7)
S ?8: Bush held a meeting with P
? (with | S 7)
11 pr [ IP] [NP VP] [held NP with  NP] [ Sharon] S 8
S 8?
12 sc [ IP ] [NP  VP] [held NP with  NP] [Sharon ] Sharon S 11: Bush held a meeting with Sharon P(Sharon | S 8)
S ?11? : Bush held a meeting with Sharon P
? (Sharon | S ?8)
13 co [  IP ] [NP  VP] [held NP with NP ] S 11
14 co [  IP ] [NP VP ] S 11
15 co [ IP  ] S 11
Figure 3: Simulation of the integraton of an Slm into an incremental tree-to-string decoding. The first
column is the line number. The second column shows the translation actions: predict (pr), scan (sc), and
complete (co). S i denotes a dependency parsing structure. The shaded nodes are exposed roots of S i.
Immediately after each sc translation action, our shift-reduce parser is triggered. Here, our parser applies
the parsing action sh, and shift ?Bush? into a partial dependency structure S 1 as a root ?Bush? (shaded
node) in Figure 3. Now the top rule on the translation stack has finished (dot is at the end), so we complete
(co) it, pop the top rule and advance the dot in the second-to-top rule, denoting that NP is completed:
[ IP ] [NP  VP].
Following this procedure, we have a dependency structure S 3 after we scan (sc) the word ?held? and
take a shift (sh) and a left reduce (re
x
) parsing actions. The shaded node ?held? means exposed roots,
that the shift-reduce parser takes actions on.
Following Huang and Mi (2010), the hypotheses with same translation step1 fall into the same bin.
Thus, only the prediction (pr) actions actually make a jump from a bin to another. Here line 2 to 4 fall
into one bin (translation step = 4, as there are 4 nodes, IP, NP, VP and Bu`sh??, in the source tree are
covered). Similarly, lines from 7 to 10 fall into another bin (translation step = 15).
1The step number is defined by the number of tree nodes covered in the source tree, and it is not equal to the number of
translation actions taken so far.
1137
Noted that as we number the bins by the translation step, only pr actions make progress, the sc and
co actions are treated as ?closure? operators in practice. Thus we always do as many sc/co actions as
possible immediately after a pr step until the symbol after the dot is another non-terminal. The total
number of bins is equal to the size of the parse tree, and each hypothesis has a constant number of
outgoing hyper-edges to predict, so the time complexity is linear in the sentence length.
After adding our Slm to this translation, an interesting branch occurs after we scan the word ?with?,
we have two different partial dependency structures S 8 and S
?
8 for the same translation. If we denote
N(S i) as the number of re actions that S i takes, N(S 8) is 3, while N(S ?8) is 4. Here N(S i) does not take
into account the number of sh parsing actions, since all partial structures with same translations should
shift the same number of translations. Thus, N(S i) determines the score of dependency structures, and
only the hypotheses with same N(S i) are comparable to each other. In this case, we should distinguish
S 8 with S
?
8, and if we make a prediction over the hypothesis of S 8, we can reach the correct parsing state
S 11 (shown in the red dashed line in Figure 3).
So the key problem of our integration is that, after each translation step, we will apply different se-
quences of parsing actions, which result in different and incomparable dependency structures with the
same translation. In the following two Sections, we introduce three ways for this integration.
3.2 Na??ve: Adding Parsing Signatures into Translation Signatures
One straightforward approach is to add the parsing signatures (in Figure 1) of each dependency structure
(in Figure 1 and Figure 3) to translation signatures. Here, we only take into account of the s0 and s1 in
the parsing stack, as the q0 is the future word that is not available in translation strings. For example, the
dependency structure S 8 has parsing signatures:
held
Bush meeting
with
We add those information to its translation signature, and only the hypothesis that have same translation
and parsing signatures can be recombined.
So, in each translation bin, different dependency structures with same translation strings are treated as
different hypothesis, and all the hypothesis are sorted and ranked in the same way. For example, S 8 and
S ?8 are compared in the bin, and we only keep top b (the beam size) hypothesis for each bin.
Obviously, this simple approach suffers from the incomparable problem for those hypothesis that have
different number of parsing actions (e.g. S 8 and S ?8). Moreover, it may result in very low translation
variance in each beam.
3.3 Best-parse: Keeping the Best Dependency Structure for Each Translation
Following Hassan et al. (2009), we only keep the best parsing tree for each translation. That means after
a consecutive translation sc actions, our shift-reduce parser applies all the possible parsing actions, and
generates a set of new partial dependency structures. Then we only choose the best one with the highest
Slm score, and only use this dependency structure for future predictions.
For example, for the translation in line 10 in Figure 3, we only keep S 8, if the parsing score of S 8 is
higher than S ?8, although they are not comparable. Another complicate example is shown in Figure 4,
within the translation step 15, there are many alternatives with different parsing structures for the same
translation (?a meeting with?) in the third column, but we can only choose the top one in the final.
3.4 Grouping: Regrouping Hypothesis by N(S ) in Each Bin
In order to do comparable sorting and pruning, our basic idea is to regroup those hypotheses in a same
bin into small groups by N(S ). For each translation, we first apply all the possible parsing actions,
and generate all dependency structures. Then we regroup all the hypothesis with different dependency
structures based on the size of N(S ).
1138
Bush held al Bush held a meetingl i
sh
Bush held al
re
Bush held a meetingl i
Bush held a meetingl i
re
sh
Bush held a meetingl i
re
Bush held a meeting withl i i
sh
Bush held a meeting withl i i
sh
sh
Bush held a meeting withl i i
Bush held a meeting withl i i
sh
re
re
Bush held a meeting withl i i
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
Bush held a meeting with Sharonl i i
re
sh
Bush held a meeting with Sharonl i i
sh
......
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
re
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i ish
......
Bush held a meeting with Sharonl i i
re
Step 15 Step 16
G1: N(S)=1
......
Bush held a meeting withl i i
G2: N(S)=2
G3: N(S)=3
G4: N(S)=4
Figure 4: Multi-beam structures of two bins with different translation steps (15 and 16). The first three
columns show the parsing movements in bin 15. Each dashed box is a group based on the number of
reduce actions over the new translation strings (?a meeting with? for bin 15, and ?Sharon? for bin 16).
G2 means two reduce actions have been applied. After this regrouping, we perform the pruning in two
phases: 1) keep top b states in each group, and labeled each group with the state with the highest parsing
score in this group; 2) sort the different groups, and keep top g groups.
For example, Figure 4 shows two bins with two different translation steps (15 and 16). In bin 15, the
graph shows the parsing movements after we scan three new words (?a?, ?meeting?, and ?with?). The
parsing sh action happens from a parsing state in one column to another state in the next column, while
re happens from a state to another state in the same column. The third column in bin 15 lists some partial
dependency structures that have all new words parsed. Here each dashed box is a group of hypothesis
with a same N(S ), e.g. the G2 contains all the dependency structures that have two reduce actions after
parsed all the new words. Then, we sort and prune each group by the beam size b, and each group labeled
as the highest hypothesis in this group. Finally, we sort those groups and only keep top g groups for the
future predictions. Again, in Figure 4, we can keep the whole group G3 and partial group of G2 if b = 2.
In our experiments, we set the group size g to 5.
3.5 Log-linear Model
We integrate our dependency parser into the log-linear model as an additional feature. So the decoder
searches for the best translation e? with a latent tree structure (evaluated by our Slm) according to the
following equation:
e? = argmax
e?E
exp(Slm(e) ? ws +
?
i
fi ? wi) (1)
where Slm(e) is the dependency parsing score calculated by our parser, ws is the weight of Slm(e), fi are
the features in the baseline model and wi are the weights.
1139
4 Experiments
4.1 Data Preparation
The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respec-
tively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08
(newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement
option ?grow-diag-and? (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley
parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string
translation rules. Our trigram word language model was trained on the target side of the training corpus
using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we
again parse the input sentences using the Berkeley parser, and convert them into translation forests using
rule pattern-matching (Mi et al., 2008).
Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the
same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate
training (Och, 2003) to maximize the Bleu score on the development set.
Our dependency parser is an implementation of the ?arc-standard? shift-reduce parser (Nivre, 2004),
and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training
set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and
Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We
add the structured language model as an additional feature into the baseline system.
We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the scrip-
t mteval-v13a.pl. We also report the Ter scores.
4.2 Complete Comparisons on MT08
To explore the soundness of our approach, we carry out some experiments in Table 2. With a beam size
100, the baseline decoder achieves a Bleu score of 21.06 with a speed of 1.7 seconds per sentence.
Since our dependency parser is trained on the English PTB, which is not included in the MT training
set, there is a chance that the gain of Bleu score is due to the increase of new n-grams in the PTB data.
In order to rule out this possibility, we use the tool SRILM to train another tri-gram language model on
English PTB and use it as a secondary language model for the decoder. The Bleu score is 21.10, which
is similar to the baseline result. Thus we can conclude that any gain of the following +Slm experiments
is not because of the using of the additional English PTB.
Our second experiment re-ranks the 100-best translations of the baseline with our structured language
model trained on PTB. The improvement is less than 0.2 Bleu, which is not statistically significant, as
the search space for re-ranking is relatively small compared with the decoding space.
As shown in Section 3, we have three different ways to integrate an Slm to the baseline system:
? na??ve: adding the parsing signature to the translation signature;
? best-parse: keeping the best dependency structure for each translation;
? grouping: regrouping the hypothesis by N(S ) in each bin.
The na??ve approach achieves a Bleu score of 19.12, which is significantly lower than the baseline. The
main reason is that adding parsing signatures leads to very restricted translation variance in each beam.
We also tried to increase the beam size to 1000, but we do not see any improvement.
The fourth line in Table 2 shows the result of the best-parse (Hassan et al., 2009). This approach only
slows the speed by a factor of two, but the improvement is not statistically significant. We manually
looked into some dependency trees this approach generates, and found this approach always introduce
local parsing errors.
The last line shows our efficient beam grouping scheme with a grouping size 5, it achieves a significant
improvement with an acceptable speed, which is about 6 times slower than the baseline system.
1140
System Bleu Speed
baseline 21.06 1.7
+Slm
re-ranking 21.23 1.73
na??ve 19.12 2.6
best-parse 21.30 3.4
grouping (g=5) 21.64 10.6
Table 2: Results on MT08. The bold score is significantly better than the baseline result at level p < 0.05.
System MT03 MT04 MT05 MT08 Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
baseline 19.94 10.73 22.03 18.63 19.92 11.45 21.06 10.37 12.80
+Slm 21.49 9.44 22.33 18.38 20.51 10.71 21.64 9.88 12.10
Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p < 0.5).
4.3 Final Results on All Test Sets
Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points
in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p < 0.05, using
bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times
slower than the baseline.
5 Related Work
The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways.
First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to
a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can
only tune their system on short sentences less than 20 words. Furthermore, their results are from a much
bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the
larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-the-
art tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions
beyond their work.
Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant
improvement. Different from their work, we pay more attention to the dependency parser, and we also
test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they
are suffering from the local parsing errors.
Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005)
to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental
parser remains in quadratic time.
Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008)
and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and
tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in
translation rules. Both papers integrate dependency structures into translation model, we instead model
the dependency structures with a monolingual parsing model over translation strings.
6 Conclusion
In this paper, we presented an efficient algorithm to integrate a structured language model (an incremen-
tal shift-reduce parser in specific) into an incremental tree-to-string system. We calculate the structured
language model scores incrementally at the decoding step, rather than re-scoring a complete transla-
tion. Our experiments suggest that it is important to design efficient pruning strategies, which have been
1141
overlooked in previous work. Experimental results on large-scale data set show that our approach signif-
icantly improves the translation quality at a reasonable slower speed than a state-of-the-art tree-to-string
system.
The structured language model introduced in our work only takes into account the target string, and
ignores the reordering information in the source side. Thus, our future work seeks to incorporate more
source side syntax information to guide the parsing of the target side, and tune a structured language
model for both Bleu and paring accuracy. Another potential work lies in the more efficient searching and
pruning algorithms for integration.
Acknowledgments
We thank the three anonymous reviewers for helpful suggestions, and Dan Gildea and Licheng Fang for
discussions. Yu and Liu were supported in part by CAS Action Plan for the Development of Western
China (No. KGZD-EW-501) and a grant from Huawei Noah?s Ark Lab, Hong Kong. Liu was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin C-
ity University. Huang was supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty Research
Award, and a PSC-CUNY Award, and Mi by DARPA HR0011-12-C-0015. The views and findings in
this paper are those of the authors and are not endorsed by the US or Chinese governments.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. Parsing of series in automatic computation. In The Theory of Parsing,
Translation, and Compiling, page Volume I.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine
translation. In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. volume 14, pages 283 ? 332.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of
ACL.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of ACL 2009 and AFNLP, pages 773?781, Suntec, Singapore, August.
Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In Proceed-
ings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-
ACL, pages 961?968.
Hany Hassan, Khalil Sima?an, and Andy Way. 2009. A syntactified direct translation model with linear-time de-
coding. In Proceedings of EMNLP 2009, pages 1182?1191, Singapore, August. Association for Computational
Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of ACL 2010, pages 1077?1086, Uppsala, Sweden, July. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings
of NAACL 2012, Montreal, Quebec.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of NAACL, pages 127?133.
1142
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP,
pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of COLING-ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of ACL/IJCNLP, pages 558?566, Suntec, Singapore, August.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523?530, Vancouver, British Columbia,
Canada, October.
Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of ACL, pages
1433?1442, Uppsala, Sweden, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL: HLT, pages
192?199.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational Linguis-
tics.
Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411.
Matt Post and Daniel Gildea. 2008. Language modeling with tree substitution grammars. In Proceedings of
AMTA.
Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In Proceedings of NIPS
workshop on Grammar Induction, Representation of Language, and Language Learning.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Arbor, MI, June.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of ACL 2011, pages 620?631, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
1143
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273?283,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Efficient Incremental Decoding for Tree-to-String Translation
Liang Huang 1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
{lhuang,haitaomi}@isi.edu
Haitao Mi 2,1
2Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
htmi@ict.ac.cn
Abstract
Syntax-based translation models should in
principle be efficient with polynomially-sized
search space, but in practice they are often
embarassingly slow, partly due to the cost
of language model integration. In this paper
we borrow from phrase-based decoding the
idea to generate a translation incrementally
left-to-right, and show that for tree-to-string
models, with a clever encoding of deriva-
tion history, this method runs in average-
case polynomial-time in theory, and linear-
time with beam search in practice (whereas
phrase-based decoding is exponential-time in
theory and quadratic-time in practice). Exper-
iments show that, with comparable translation
quality, our tree-to-string system (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++).
1 Introduction
Most efforts in statistical machine translation so far
are variants of either phrase-based or syntax-based
models. From a theoretical point of view, phrase-
based models are neither expressive nor efficient:
they typically allow arbitrary permutations and re-
sort to language models to decide the best order. In
theory, this process can be reduced to the Traveling
Salesman Problem and thus requires an exponential-
time algorithm (Knight, 1999). In practice, the de-
coder has to employ beam search to make it tractable
(Koehn, 2004). However, even beam search runs in
quadratic-time in general (see Sec. 2), unless a small
distortion limit (say, d=5) further restricts the possi-
ble set of reorderings to those local ones by ruling
out any long-distance reorderings that have a ?jump?
in theory in practice
phrase-based exponential quadratic
tree-to-string polynomial linear
Table 1: [main result] Time complexity of our incremen-
tal tree-to-string decoding compared with phrase-based.
In practice means ?approximate search with beams.?
longer than d. This has been the standard prac-
tice with phrase-based models (Koehn et al, 2007),
which fails to capture important long-distance re-
orderings like SVO-to-SOV.
Syntax-based models, on the other hand, use
syntactic information to restrict reorderings to
a computationally-tractable and linguistically-
motivated subset, for example those generated by
synchronous context-free grammars (Wu, 1997;
Chiang, 2007). In theory the advantage seems quite
obvious: we can now express global reorderings
(like SVO-to-VSO) in polynomial-time (as opposed
to exponential in phrase-based). But unfortunately,
this polynomial complexity is super-linear (being
generally cubic-time or worse), which is slow in
practice. Furthermore, language model integration
becomes more expensive here since the decoder now
has to maintain target-language boundary words at
both ends of a subtranslation (Huang and Chiang,
2007), whereas a phrase-based decoder only needs
to do this at one end since the translation is always
growing left-to-right. As a result, syntax-based
models are often embarassingly slower than their
phrase-based counterparts, preventing them from
becoming widely useful.
Can we combine the merits of both approaches?
While other authors have explored the possibilities
273
of enhancing phrase-based decoding with syntax-
aware reordering (Galley and Manning, 2008), we
are more interested in the other direction, i.e., can
syntax-based models learn from phrase-based de-
coding, so that they still model global reordering, but
in an efficient (preferably linear-time) fashion?
Watanabe et al (2006) is an early attempt in
this direction: they design a phrase-based-style de-
coder for the hierarchical phrase-based model (Chi-
ang, 2007). However, this algorithm even with the
beam search still runs in quadratic-time in prac-
tice. Furthermore, their approach requires grammar
transformation that converts the original grammar
into an equivalent binary-branching Greibach Nor-
mal Form, which is not always feasible in practice.
We take a fresh look on this problem and turn our
focus to one particular syntax-based paradigm, tree-
to-string translation (Liu et al, 2006; Huang et al,
2006), since this is the simplest and fastest among
syntax-based approaches. We develop an incremen-
tal dynamic programming algorithm and make the
following contributions:
? we show that, unlike previous work, our in-
cremental decoding algorithm runs in average-
case polynomial-time in theory for tree-to-
string models, and the beam search version runs
in linear-time in practice (see Table 1);
? large-scale experiments on a tree-to-string sys-
tem confirm that, with comparable translation
quality, our incremental decoder (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++) (Koehn
et al, 2007);
? furthermore, on the same tree-to-string system,
incremental decoding is slightly faster than the
standard cube pruning method at the same level
of translation quality;
? this is also the first linear-time incremental de-
coder that performs global reordering.
We will first briefly review phrase-based decod-
ing in this section, which inspires our incremental
algorithm in the next section.
2 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English to explain both phrase-based and
syntax-based decoding throughout this paper:
0 Bu`sh?? 1
Bush
yu? 2
with
Sha?lo?ng 3
Sharon
ju?x??ng 4
hold
le
-ed
5 hu?`ta?n 6
meeting
?Bush held talks with Sharon?
2.1 Basic Dynamic Programming Algorithm
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (Koehn, 2004). Each hypothesis has
a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment. This process can be formalized as
a deductive system. For example, the following de-
duction step grows a hypothesis by the phrase-pair
?yu? Sha?lo?ng, with Sharon? covering Chinese span
[1-3]:
(? ???6) : (w, ?Bush held talks?)
(???3???) : (w?, ?Bush held talks with Sharon?) (1)
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where w and
w? = w+c+d are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair,
and d being the distortion cost. To compute d we
also need to maintain the ending position of the last
phrase (the 3 and 6 in the coverage vector).
To add a bigram model, we split each ?LM item
above into a series of +LM items; each +LM item
has the form (v,a ) where a is the last word of the
hypothesis. Thus a +LM version of (1) might be:
(? ???6,talks ) : (w, ?Bush held talks?)
(???3???,Sharon ) : (w?, ?Bush held talks with Sharon?)
where the score of the resulting +LM item
w? = w + c + d? logPlm(with | talk)
now includes a combination cost due to the bigrams
formed when applying the phrase-pair. The com-
plexity of this dynamic programming algorithm for
g-gram decoding is O(2nn2|V |g?1) where n is the
sentence length and |V | is the English vocabulary
size (Huang and Chiang, 2007).
274
1 2 3 4 5
Figure 1: Beam search in phrase-based decoding expands
the hypotheses in the current bin (#2) into longer ones.
VP
PP
P
yu?
x1:NP
VP
VV
ju?x??ng
AS
le
x2:NP
? held x2 with x1
Figure 2: Tree-to-string rule r3 for reordering.
2.2 Beam Search in Practice
To make the exponential algorithm practical, beam
search is the standard approximate search method
(Koehn, 2004). Here we group +LM items into n
bins, with each bin Bi hosting at most b items that
cover exactly i Chinese words (see Figure 1). The
complexity becomes O(n2b) because there are a to-
tal of O(nb) items in all bins, and to expand each
item we need to scan the whole coverage vector,
which costs O(n). This quadratic complexity is still
too slow in practice and we often set a small distor-
tion limit of dmax (say, 5) so that no jumps longer
than dmax are allowed. This method reduces the
complexity to O(nbdmax) but fails to capture long-
distance reorderings (Galley and Manning, 2008).
3 Incremental Decoding for Tree-to-String
Translation
We will first briefly review tree-to-string translation
paradigm and then develop an incremental decoding
algorithm for it inspired by phrase-based decoding.
3.1 Tree-to-string Translation
A typical tree-to-string system (Liu et al, 2006;
Huang et al, 2006) performs translation in two
steps: parsing and decoding. A parser first parses the
source language input into a 1-best tree T , and the
decoder then searches for the best derivation (a se-
(a) Bu`sh?? [yu? Sha?lo?ng ]1 [ju?x??ng le hu?`ta?n ]2
? 1-best parser
(b) IP@?
NP@1
Bu`sh??
VP@2
PP@2.1
P
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP@2.2.3
hu?`ta?n
r1 ?
(c) NP@1
Bu`sh??
VP@2
PP@2.1
P
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP@2.2.3
hu?`ta?n
r2 ? r3 ?
(d) Bush held NP@2.2.3
hu?`ta?n
with NP@2.1.2
Sha?lo?ng
r4 ? r5 ?
(e) Bush [held talks]2 [with Sharon]1
Figure 3: An example derivation of tree-to-string trans-
lation (much simplified from Mi et al (2008)). Shaded
regions denote parts of the tree that matches the rule.
quence of translation steps) d? that converts source
tree T into a target-language string.
Figure 3 shows how this process works. The Chi-
nese sentence (a) is first parsed into tree (b), which
will be converted into an English string in 5 steps.
First, at the root node, we apply rule r1 preserving
the top-level word-order
(r1) IP (x1:NP x2:VP) ? x1 x2
which results in two unfinished subtrees, NP@1 and
VP@2 in (c). Here X@? denotes a tree node of la-
bel X at tree address ? (Shieber et al, 1995). (The
root node has address ?, and the first child of node ?
has address ?.1, etc.) Then rule r2 grabs the Bu`sh??
subtree and transliterate it into the English word
275
in theory in practice
phrase* O(2nn2 ? |V |g?1) O(n2b)
tree-to-str O(nc ? |V |4(g?1)) O(ncb2)
this work* O(nk log2(cr) ? |V |g?1) O(ncb)
Table 2: Summary of time complexities of various algo-
rithms. b is the beam width, V is the English vocabulary,
and c is the number of translation rules per node. As a
special case, phrase-based decoding with distortion limit
dmax is O(nbdmax). *: incremental decoding algorithms.
?Bush?. Similarly, rule r3 shown in Figure 2 is ap-
plied to the VP subtree, which swaps the two NPs,
yielding the situation in (d). Finally two phrasal
rules r4 and r5 translate the two remaining NPs and
finish the translation.
In this framework, decoding without language
model (?LM decoding) is simply a linear-time
depth-first search with memoization (Huang et al,
2006), since a tree of n words is also of size
O(n) and we visit every node only once. Adding
a language model, however, slows it down signifi-
cantly because we now have to keep track of target-
language boundary words, but unlike the phrase-
based case in Section 2, here we have to remember
both sides the leftmost and the rightmost boundary
words: each node is now split into +LM items like
(? a ? b) where ? is a tree node, and a and b are left
and right English boundary words. For example, a
bigram +LM item for node VP@2 might be
(VP@2 held ? Sharon).
This is also the case with other syntax-based models
like Hiero or GHKM: language model integration
overhead is the most significant factor that causes
syntax-based decoding to be slow (Chiang, 2007). In
theory +LM decoding is O(nc|V |4(g?1)), where V
denotes English vocabulary (Huang, 2007). In prac-
tice we have to resort to beam search again: at each
node we would only allow top-b +LM items. With
beam search, tree-to-string decoding with an inte-
grated language model runs in time O(ncb2), where
b is the size of the beam at each node, and c is (max-
imum) number of translation rules matched at each
node (Huang, 2007). See Table 2 for a summary.
3.2 Incremental Decoding
Can we borrow the idea of phrase-based decoding,
so that we also grow the hypothesis strictly left-
to-right, and only need to maintain the rightmost
boundary words?
The key intuition is to adapt the coverage-vector
idea from phrase-based decoding to tree-to-string
decoding. Basically, a coverage-vector keeps track
of which Chinese spans have already been translated
and which have not. Similarly, here we might need
a ?tree coverage-vector? that indicates which sub-
trees have already been translated and which have
not. But unlike in phrase-based decoding, we can
not simply choose any arbitrary uncovered subtree
for the next step, since rules already dictate which
subtree to visit next. In other words what we need
here is not really a tree coverage vector, but more of
a derivation history.
We develop this intuition into an agenda repre-
sented as a stack. Since tree-to-string decoding is a
top-down depth-first search, we can simulate this re-
cursion with a stack of active rules, i.e., rules that are
not completed yet. For example we can simulate the
derivation in Figure 3 as follows. At the root node
IP@?, we choose rule r1, and push its English-side
to the stack, with variables replaced by matched tree
nodes, here x1 for NP@1 and x2 for VP@2. So we
have the following stack
s = [ NP@1 VP@2],
where the dot  indicates the next symbol to process
in the English word-order. Since node NP@1 is the
first in the English word-order, we expand it first,
and push rule r2 rooted at NP to the stack:
[ NP@1 VP@2 ] [ Bush].
Since the symbol right after the dot in the top rule is
a word, we immediately grab it, and append it to the
current hypothesis, which results in the new stack
[ NP@1 VP@2 ] [Bush  ].
Now the top rule on the stack has finished (dot is at
the end), so we trigger a ?pop? operation which pops
the top rule and advances the dot in the second-to-
top rule, denoting that NP@1 is now completed:
[NP@1  VP@2].
276
stack hypothesis
[<s>  IP@? </s>] <s>
p [<s>  IP@? </s>] [ NP@1 VP@2] <s>
p [<s>  IP@? </s>] [ NP@1 VP@2] [ Bush] <s>
s [<s>  IP@? </s>] [ NP@1 VP@2] [Bush  ] <s> Bush
c [<s>  IP@? </s>] [NP@1  VP@2] <s> Bush
p [<s>  IP@? </s>] [NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2] <s> Bush
s [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] <s> Bush held
p [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [ talks] <s> Bush held
s [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [talks  ] <s> Bush held talks
c [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3  with NP@2.1.2] <s> Bush held talks
s [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] <s> Bush held talks with
p [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [ Sharon] <s> Bush held talks with
s [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [Sharon ] <s> Bush held talks with Sharon
c [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with NP@2.1.2 ] <s> Bush held talks with Sharon
c [<s>  IP@? </s>] [NP@1 VP@2 ] <s> Bush held talks with Sharon
c [<s> IP@?  </s>] <s> Bush held talks with Sharon
s [<s> IP@? </s> ] <s> Bush held talks with Sharon </s>
Figure 4: Simulation of tree-to-string derivation in Figure 3 in the incremental decoding algorithm. Actions: p, predict;
s, scan; c, complete (see Figure 5).
Item ? : ?s, ?? : w; ?: step, s: stack, ?: hypothesis, w: weight
Equivalence ? : ?s, ?? ? ? : ?s?, ??? iff. s = s? and lastg?1(?) = lastg?1(??)
Axiom 0 : ?[<s>g?1  ? </s>], <s>g?1? : 0
Predict
? : ?... [?  ? ?], ?? : w
? + |C(r)| : ?... [?  ? ?] [ f(?,E(r))], ?? : w + c(r) match(?, C(r))
Scan
? : ?... [?  e ?], ?? : w
? : ?... [? e  ?], ?e? : w ? log Pr(e | lastg?1(?))
Complete
? : ?... [?  ? ?] [?], ?? : w
? : ?... [? ?  ?], ?? : w
Goal |T | : ?[<s>g?1 ? </s>], ?</s>? : w
Figure 5: Deductive system for the incremental tree-to-string decoding algorithm. Function lastg?1(?) returns the
rightmost g ? 1 words (for g-gram LM), and match(?, C(r)) tests matching of rule r against the subtree rooted at
node ?. C(r) and E(r) are the Chinese and English sides of rule r, and function f(?,E(r)) = [xi 7? ?.var(i)]E(r)
replaces each variable xi on the English side of the rule with the descendant node ?.var(i) under ? that matches xi.
277
The next step is to expand VP@2, and we use rule r3
and push its English-side ?VP ? held x2 with x1?
onto the stack, again with variables replaced by
matched nodes:
[NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2]
Note that this is a reordering rule, and the stack al-
ways follows the English word order because we
generate hypothesis incrementally left-to-right. Fig-
ure 4 works out the full example.
We formalize this algorithm in Figure 5. Each
item ?s, ?? consists of a stack s and a hypothesis
?. Similar to phrase-based dynamic programming,
only the last g?1 words of ? are part of the signature
for decoding with g-gram LM. Each stack is a list of
dotted rules, i.e., rules with dot positions indicting
progress, in the style of Earley (1970). We call the
last (rightmost) rule on the stack the top rule, which
is the rule being processed currently. The symbol af-
ter the dot in the top rule is called the next symbol,
since it is the symbol to expand or process next. De-
pending on the next symbol a, we can perform one
of the three actions:
? if a is a node ?, we perform a Predict action
which expands ? using a rule r that can pattern-
match the subtree rooted at ?; we push r is to
the stack, with the dot at the beginning;
? if a is an English word, we perform a Scan ac-
tion which immediately adds it to the current
hypothesis, advancing the dot by one position;
? if the dot is at the end of the top rule, we
perform a Complete action which simply pops
stack and advance the dot in the new top rule.
3.3 Polynomial Time Complexity
Unlike phrase-based models, we show here
that incremental decoding runs in average-case
polynomial-time for tree-to-string systems.
Lemma 1. For an input sentence of n words and
its parse tree of depth d, the worst-case complex-
ity of our algorithm is f(n, d) = c(cr)d|V |g?1 =
O((cr)dng?1), assuming relevant English vocabu-
lary |V | = O(n), and where constants c, r and g are
the maximum number of rules matching each tree
node, the maximum arity of a rule, and the language-
model order, respectively.
Proof. The time complexity depends (in part) on the
number of all possible stacks for a tree of depth d. A
stack is a list of rules covering a path from the root
node to one of the leaf nodes in the following form:
R1
? ?? ?
[... ?1...]
R2
? ?? ?
[... ?2...] ...
Rs
? ?? ?
[... ?s...],
where ?1 = ? is the root node and ?s is a leaf node,
with stack depth s ? d. Each rule Ri(i > 1) ex-
pands node ?i?1, and thus has c choices by the defi-
nition of grammar constant c. Furthermore, each rule
in the stack is actually a dotted-rule, i.e., it is associ-
ated with a dot position ranging from 0 to r, where r
is the arity of the rule (length of English side of the
rule). So the total number of stacks is O((cr)d).
Besides the stack, each state also maintains (g?1)
rightmost words of the hypothesis as the language
model signature, which amounts to O(|V |g?1). So
the total number of states is O((cr)d|V |g?1). Fol-
lowing previous work (Chiang, 2007), we assume
a constant number of English translations for each
foreign word in the input sentence, so |V | = O(n).
And as mentioned above, for each state, there are c
possible expansions, so the overall time complexity
is f(n, d) = c(cr)d|V |g?1 = O((cr)dng?1).
We do average-case analysis below because the
tree depth (height) for a sentence of n words is a
random variable: in the worst-case it can be linear in
n (degenerated into a linear-chain), but we assume
this adversarial situation does not happen frequently,
and the average tree depth is O(log n).
Theorem 1. Assume for each n, the depth of a
parse tree of n words, notated dn, distributes nor-
mally with logarithmic mean and variance, i.e.,
dn ? N (?n, ?2n), where ?n = O(logn) and ?2n =
O(logn), then the average-case complexity of the
algorithm is h(n) = O(nk log2(cr)+g?1) for constant
k, thus polynomial in n.
Proof. From Lemma 1 and the definition of average-
case complexity, we have
h(n) = Edn?N (?n,?2n)[f(n, dn)],
where Ex?D[?] denotes the expectation with respect
278
to the random variable x in distribution D.
h(n) = Edn?N (?n,?2n)[f(n, dn)]
= Edn?N (?n,?2n)[O((cr)
dnng?1)],
= O(ng?1Edn?N (?n,?2n)[(cr)
dn ]),
= O(ng?1Edn?N (?n,?2n)[exp(dn log(cr))]) (2)
Since dn ? N (?n, ?2n) is a normal distribution,
dn log(cr) ? N (??, ??2) is also a normal distribu-
tion, where ?? = ?n log(cr) and ?? = ?n log(cr).
Therefore exp(dn log(cr)) is a log-normal distribu-
tion, and by the property of log-normal distribution,
its expectation is exp (?? + ??2/2). So we have
Edn?N (?n,?2/2)[exp(dn log(cr))]
= exp (?? + ??2/2)
= exp (?n log(cr) + ?2n log2(cr)/2)
= exp (O(log n) log(cr) + O(log n) log2(cr)/2)
= exp (O(log n) log2(cr))
? exp (k(log n) log2(cr)), for some constant k
= exp (log nk log2(cr))
= nk log2(cr). (3)
Plug it back to Equation (2), and we have the
average-case complexity
Edn [f(n, dn)] ? O(ng?1nk log
2(cr))
= O(nk log2(cr)+g?1). (4)
Since k, c, r and g are constants, the average-case
complexity is polynomial in sentence length n.
The assumption dn ? N (O(logn), O(logn))
will be empirically verified in Section 5.
3.4 Linear-time Beam Search
Though polynomial complexity is a desirable prop-
erty in theory, the degree of the polynomial,
O(log cr) might still be too high in practice, depend-
ing on the translation grammar. To make it linear-
time, we apply the beam search idea from phrase-
based again. And once again, the only question to
decide is the choice of ?binning?: how to assign each
item to a particular bin, depending on their progress?
While the number of Chinese words covered is a
natural progress indicator for phrase-based, it does
not work for tree-to-string because, among the three
actions, only scanning grows the hypothesis. The
prediction and completion actions do not make real
progress in terms of words, though they do make
progress on the tree. So we devise a novel progress
indicator natural for tree-to-string translation: the
number of tree nodes covered so far. Initially that
number is zero, and in a prediction step which ex-
pands node ? using rule r, the number increments by
|C(r)|, the size of the Chinese-side treelet of r. For
example, a prediction step using rule r3 in Figure 2
to expand VP@2 will increase the tree-node count by
|C(r3)| = 6, since there are six tree nodes in that
rule (not counting leaf nodes or variables).
Scanning and completion do not make progress
in this definition since there is no new tree node
covered. In fact, since both of them are determin-
istic operations, they are treated as ?closure? op-
erators in the real implementation, which means
that after a prediction, we always do as many scan-
ning/completion steps as possible until the symbol
after the dot is another node, where we have to wait
for the next prediction step.
This method has |T | = O(n) bins where |T | is
the size of the parse tree, and each bin holds b items.
Each item can expand to c new items, so the overall
complexity of this beam search is O(ncb), which is
linear in sentence length.
4 Related Work
The work of Watanabe et al (2006) is closest in
spirit to ours: they also design an incremental decod-
ing algorithm, but for the hierarchical phrase-based
system (Chiang, 2007) instead. While we leave de-
tailed comparison and theoretical analysis to a future
work, here we point out some obvious differences:
1. due to the difference in the underlying trans-
lation models, their algorithm runs in O(n2b)
time with beam search in practice while ours
is linear. This is because each prediction step
now has O(n) choices, since they need to ex-
pand nodes like VP[1, 6] as:
VP[1,6] ? PP[1, i] VP[i, 6],
where the midpoint i in general has O(n)
choices (just like in CKY). In other words, their
grammar constant c becomes O(n).
2. different binning criteria: we use the number of
tree nodes covered, while they stick to the orig-
279
inal phrase-based idea of number of Chinese
words translated;
3. as a result, their framework requires gram-
mar transformation into the binary-branching
Greibach Normal Form (which is not always
possible) so that the resulting grammar always
contain at least one Chinese word in each rule
in order for a prediction step to always make
progress. Our framework, by contrast, works
with any grammar.
Besides, there are some other efforts less closely
related to ours. As mentioned in Section 1, while
we focus on enhancing syntax-based decoding with
phrase-based ideas, other authors have explored the
reverse, but also interesting, direction of enhancing
phrase-based decoding with syntax-aware reorder-
ing. For example Galley and Manning (2008) pro-
pose a shift-reduce style method to allow hiearar-
chical non-local reorderings in a phrase-based de-
coder. While this approach is certainly better than
pure phrase-based reordering, it remains quadratic
in run-time with beam search.
Within syntax-based paradigms, cube pruning
(Chiang, 2007; Huang and Chiang, 2007) has be-
come the standard method to speed up +LM de-
coding, which has been shown by many authors to
be highly effective; we will be comparing our incre-
mental decoder with a baseline decoder using cube
pruning in Section 5. It is also important to note
that cube pruning and incremental decoding are not
mutually exclusive, rather, they could potentially be
combined to further speed up decoding. We leave
this point to future work.
Multipass coarse-to-fine decoding is another pop-
ular idea (Venugopal et al, 2007; Zhang and Gildea,
2008; Dyer and Resnik, 2010). In particular, Dyer
and Resnik (2010) uses a two-pass approach, where
their first-pass, ?LM decoding is also incremental
and polynomial-time (in the style of Earley (1970)
algorithm), but their second-pass, +LM decoding is
still bottom-up CKY with cube pruning.
5 Experiments
To test the merits of our incremental decoder we
conduct large-scale experiments on a state-of-the-art
tree-to-string system, and compare it with the stan-
dard phrase-based system of Moses. Furturemore we
also compare our incremental decoder with the stan-
dard cube pruning approach on the same tree-to-
string decoder.
5.1 Data and System Preparation
Our training corpus consists of 1.5M sentence pairs
with about 38M/32M words in Chinese/English, re-
spectively. We first word-align them by GIZA++ and
then parse the Chinese sentences using the Berke-
ley parser (Petrov and Klein, 2007), then apply
the GHKM algorithm (Galley et al, 2004) to ex-
tract tree-to-string translation rules. We use SRILM
Toolkit (Stolcke, 2002) to train a trigram language
model with modified Kneser-Ney smoothing on the
target side of training corpus. At decoding time,
we again parse the input sentences into trees, and
convert them into translation forest by rule pattern-
matching (Mi et al, 2008).
We use the newswire portion of 2006 NIST MT
Evaluation test set (616 sentences) as our develop-
ment set and the newswire portion of 2008 NIST
MT Evaluation test set (691 sentences) as our test
set. We evaluate the translation quality using the
BLEU-4 metric, which is calculated by the script
mteval-v13a.pl with its default setting which is case-
insensitive matching of n-grams. We use the stan-
dard minimum error-rate training (Och, 2003) to
tune the feature weights to maximize the system?s
BLEU score on development set.
We first verify the assumptions we made in Sec-
tion 3.3 in order to prove the theorem that tree depth
(as a random variable) is normally-distributed with
O(logn) mean and variance. Qualitatively, we veri-
fied that for most n, tree depth d(n) does look like a
normal distribution. Quantitatively, Figure 6 shows
that average tree height correlates extremely well
with 3.5 log n, while tree height variance is bounded
by 5.5 log n.
5.2 Comparison with Cube pruning
We implemented our incremental decoding algo-
rithm in Python, and test its performance on the de-
velopment set. We first compare it with the stan-
dard cube pruning approach (also implemented in
Python) on the same tree-to-string system.1 Fig-
1Our implementation of cube pruning follows (Chiang,
2007; Huang and Chiang, 2007) where besides a beam size b
of unique +LM items, there is also a hard limit (of 1000) on the
280
 0
 1
 2
 3
 4
 5
 0  10  20  30  40  50  60  70
Av
er
ag
e 
De
co
di
ng
 T
im
e 
(S
ec
s)
Sentence Length
incremental
cube pruning
 29.5
 29.6
 29.7
 29.8
 29.9
 30
 30.1
 0  0.2  0.4  0.6  0.8  1  1.2  1.4
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
incremental
cube pruning
(a) decoding time against sentence length (b) BLEU score against decoding time
Figure 7: Comparison with cube pruning. The scatter plot in (a) confirms that our incremental decoding scales linearly
with sentence length, while cube pruning super-linearly (b = 50 for both). The comparison in (b) shows that at the
same level of translation quality, incremental decoding is slightly faster than cube pruning, especially at smaller beams.
 0
 5
 10
 15
 20
 25
 0  10  20  30  40  50
Tr
ee
 D
ep
th
 d
(n)
Sentence Length (n)
Avg Depth
Variance
3.5 log n
Figure 6: Mean and variance of tree depth vs. sentence
length. The mean depth clearly scales with 3.5 log n, and
the variance is bounded by 5.5 log n.
ure 7(a) is a scatter plot of decoding times versus
sentence length (using beam b = 50 for both sys-
tems), where we confirm that our incremental de-
coder scales linearly, while cube pruning has a slight
tendency of superlinearity. Figure 7(b) is a side-by-
side comparison of decoding speed versus transla-
tion quality (in BLEU scores), using various beam
sizes for both systems (b=10?70 for cube pruning,
and b=10?110 for incremental). We can see that in-
cremental decoding is slightly faster than cube prun-
ing at the same levels of translation quality, and the
difference is more pronounced at smaller beams: for
number of (non-unique) pops from priority queues.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60  70
Av
er
ag
e 
De
co
di
ng
 T
im
e 
(S
ec
s)
Sentence Length
M +?
 M 10
M 6
M 0
t2s
Figure 8: Comparison of our incremental tree-to-string
decoder with Moses in terms of speed. Moses is shown
with various distortion limits (0, 6, 10, +?; optimal: 10).
example, at the lowest levels of translation quality
(BLEU scores around 29.5), incremental decoding
takes only 0.12 seconds, which is about 4 times as
fast as cube pruning. We stress again that cube prun-
ing and incremental decoding are not mutually ex-
clusive, and rather they could potentially be com-
bined to further speed up decoding.
5.3 Comparison with Moses
We also compare with the standard phrase-based
system of Moses (Koehn et al, 2007), with stan-
dard settings except for the ttable limit, which we set
to 100. Figure 8 compares our incremental decoder
281
system/decoder BLEU time
Moses (optimal dmax=10) 29.41 10.8
tree-to-str: cube pruning (b=10) 29.51 0.65
tree-to-str: cube pruning (b=20) 29.96 0.96
tree-to-str: incremental (b=10) 29.54 0.32
tree-to-str: incremental (b=50) 29.96 0.77
Table 3: Final BLEU score and speed results on the test
data (691 sentences), compared with Moses and cube
pruning. Time is in seconds per sentence, including pars-
ing time (0.21s) for the two tree-to-string decoders.
with Moses at various distortion limits (dmax=0, 6,
10, and +?). Consistent with the theoretical anal-
ysis in Section 2, Moses with no distortion limit
(dmax = +?) scale quadratically, and monotone
decoding (dmax = 0) scale linearly. We use MERT
to tune the best weights for each distortion limit, and
dmax = 10 performs the best on our dev set.
Table 3 reports the final results in terms of BLEU
score and speed on the test set. Our linear-time
incremental decoder with the small beam of size
b = 10 achieves a BLEU score of 29.54, compara-
ble to Moses with the optimal distortion limit of 10
(BLEU score 29.41). But our decoding (including
source-language parsing) only takes 0.32 seconds a
sentences, which is more than 30 times faster than
Moses. With a larger beam of b = 50 our BLEU
score increases to 29.96, which is a half BLEU point
better than Moses, but still about 15 times faster.
6 Conclusion
We have presented an incremental dynamic pro-
gramming algorithm for tree-to-string translation
which resembles phrase-based based decoding. This
algorithm is the first incremental algorithm that runs
in polynomial-time in theory, and linear-time in
practice with beam search. Large-scale experiments
on a state-of-the-art tree-to-string decoder confirmed
that, with a comparable (or better) translation qual-
ity, it can run more than 30 times faster than the
phrase-based system of Moses, even though ours is
in Python while Moses in C++. We also showed that
it is slightly faster (and scale better) than the popular
cube pruning technique. For future work we would
like to apply this algorithm to forest-based transla-
tion and hierarchical system by pruning the first-pass
?LM forest. We would also combine cube pruning
with our incremental algorithm, and study its perfor-
mance with higher-order language models.
Acknowledgements
We would like to thank David Chiang, Kevin
Knight, and Jonanthan Graehl for discussions and
the anonymous reviewers for comments. In partic-
ular, we are indebted to the reviewer who pointed
out a crucial mistake in Theorem 1 and its proof
in the submission. This research was supported in
part by DARPA, under contract HR0011-06-C-0022
under subcontract to BBN Technologies, and under
DOI-NBC Grant N10AP20031, and in part by the
National Natural Science Foundation of China, Con-
tracts 60736014 and 90920004.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?208.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proceedings of
NAACL.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of EMNLP 2008.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA, August.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proc. NAACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL:
demonstration sesion.
282
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
Columbus, OH.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Ashish Venugopal, Andreas Zollmann, and Stephen Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL.
Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of COLING-
ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In Proceedings of ACL.
283
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 216?226,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Novel Dependency-to-String Model for Statistical Machine Translation
Jun Xie, Haitao Mi and Qun Liu
Key Laboratory of Intelligent Information Processiong
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{junxie,htmi,liuqun}@ict.ac.cn
Abstract
Dependency structure, as a first step towards
semantics, is believed to be helpful to improve
translation quality. However, previous works
on dependency structure based models typi-
cally resort to insertion operations to complete
translations, which make it difficult to spec-
ify ordering information in translation rules.
In our model of this paper, we handle this
problem by directly specifying the ordering
information in head-dependents rules which
represent the source side as head-dependents
relations and the target side as strings. The
head-dependents rules require only substitu-
tion operation, thus our model requires no
heuristics or separate ordering models of the
previous works to control the word order of
translations. Large-scale experiments show
that our model performs well on long dis-
tance reordering, and outperforms the state-
of-the-art constituency-to-string model (+1.47
BLEU on average) and hierarchical phrase-
based model (+0.46 BLEU on average) on two
Chinese-English NIST test sets without resort
to phrases or parse forest. For the first time,
a source dependency structure based model
catches up with and surpasses the state-of-the-
art translation models.
1 Introduction
Dependency structure represents the grammatical
relations that hold between the words in a sentence.
It encodes semantic relations directly, and has the
best inter-lingual phrasal cohesion properties (Fox,
2002). Those attractive characteristics make it pos-
sible to improve translation quality by using depen-
dency structures.
Some researchers pay more attention to use de-
pendency structure on the target side. (Shen et al,
2008) presents a string-to-dependency model, which
restricts the target side of each hierarchical rule to be
a well-formed dependency tree fragment, and em-
ploys a dependency language model to make the out-
put more grammatically. This model significantly
outperforms the state-of-the-art hierarchical phrase-
based model (Chiang, 2005). However, those string-
to-tree systems run slowly in cubic time (Huang et
al., 2006).
Using dependency structure on the source side
is also a promising way, as tree-based systems run
much faster (linear time vs. cubic time, see (Huang
et al, 2006)). Conventional dependency structure
based models (Lin, 2004; Quirk et al, 2005; Ding
and Palmer, 2005; Xiong et al, 2007) typically
employ both substitution and insertion operation to
complete translations, which make it difficult to
specify ordering information directly in the transla-
tion rules. As a result, they have to resort to either
heuristics (Lin, 2004; Xiong et al, 2007) or sepa-
rate ordering models (Quirk et al, 2005; Ding and
Palmer, 2005) to control the word order of transla-
tions.
In this paper, we handle this problem by di-
rectly specifying the ordering information in head-
dependents rules that represent the source side as
head-dependents relations and the target side as
string. The head-dependents rules have only one
substitution operation, thus we don?t face the prob-
lems appeared in previous work and get rid of the
216
heuristics and ordering model. To alleviate data
sparseness problem, we generalize the lexicalized
words in head-dependents relations with their cor-
responding categories.
In the following parts, we first describe the moti-
vation of using head-dependents relations (Section
2). Then we formalize our grammar (Section 3),
present our rule acquisition algorithm (Section 4),
our model (Section 5) and decoding algorithm (Sec-
tion 6). Finally, large-scale experiments (Section 7)
show that our model exhibits good performance on
long distance reordering, and outperforms the state-
of-the-art tree-to-string model (+1.47 BLEU on av-
erage) and hierarchical phrase-based model (+0.46
BLEU on average) on two Chinese-English NIST
test sets. For the first time, a source dependency tree
based model catches up with and surpasses the state-
of-the-art translation models.
2 Dependency Structure and
Head-Dependents Relation
2.1 Dependency Sturcture
A dependency structure for a sentence is a directed
acyclic graph with words as nodes and modification
relations as edges. Each edge direct from a head to
a dependent. Figure 1 (a) shows an example depen-
dency structure of a Chinese sentence.
2010? FIFA??????????
2010 FIFA [World Cup] in/at [South Africa]
successfully hold
Each node is annotated with the part-of-speech
(POS) of the related word.
For convenience, we use the lexicon dependency
grammar (Hellwig, 2006) which adopts a bracket
representation to express a projective dependency
structure. The dependency structure of Figure 1 (a)
can be expressed as:
((2010?) (FIFA)???) (?(??)) (??)??
where the lexicon in brackets represents the depen-
dents, while the lexicon out the brackets is the head.
To construct the dependency structure of a sen-
tence, the most important thing is to establish de-
pendency relations and distinguish the head from the
dependent. Here are some criteria (Zwicky, 1985;
x2:?2:x1:???1: x3:AD3:
??
x1 was held x3 x21  s l  3 2
?/P/???/NR/
??/NR/
??/AD/
2010?/NT/ FIFA/NRI /
??/VV/
/P?/???/NR/ ??/AD/
??/VV/
(a)
(b)
(c)
?? successfullys ssf ll(d)
Figure 1: Examples of dependency structure (a), head-
dependents relation (b), head-dependents rule (r1 of Fig-
ure 2) and head rule (d). Where ?x1:???? and
?x2:?? indicate substitution sites which can be replaced
by a subtree rooted at ????? and ??? respectively.
?x3:AD?indicates a substitution site that can be replaced
by a subtree whose root has part-of-speech ?AD?. The
underline denotes a leaf node.
Hudson, 1990) for identifying a syntactic relation
between a head and a dependent between a head-
dependent pair:
1. head determines the syntactic category of C,
and can often replace C;
2. head determines the semantic category of C;
dependent gives semantic specification.
2.2 Head-Dependents Relation
A head-dependents relation is composed of a head
and all its dependents as shown in Figure 1(b).
Since all the head-dependent pairs satisfy crite-
ria 1 and 2, we can deduce that a head-dependents
relation L holds the property that the head deter-
mines the syntactic and semantic categories of L,
and can often replace L. Therefore, we can recur-
217
sively replace the bottom level head-dependent re-
lations of a dependency structure with their heads
until the root. This implies an representation of the
generation of a dependency structure on the basis of
head-dependents relation.
Inspired by this, we represent the translation rules
of our dependency-to-string model on the founda-
tion of head-dependents relations.
3 Dependency-to-String Grammar
Figure 1 (c) and (d) show two examples of the trans-
lation rules used in our dependency-to-string model.
The former is an example of head-dependent rules
that represent the source side as head-dependents re-
lations and act as both translation rules and reorder-
ing rules. The latter is an example of head rules
which are used for translating words.
Formally, a dependency-to-string grammar is de-
fined as a tuple ??, N,?, R?, where ? is a set of
source language terminals, N is a set of categories
for the terminals in ? , ? is a set of target language
terminals, and R is a set of translation rules. A rule
r in R is a tuple ?t, s, ??, where:
- t is a node labeled by terminal from ?; or a
head-dependents relation of the source depen-
dency structures, with each node labeled by a
terminal from ? or a variable from a set X =
{x1, x2, ...} constrained by a terminal from ?
or a category from N ;
- s ? (X ??)? is the target side string;
- ? is a one-to-one mapping from nonterminals
in t to variables in s.
For example, the head-dependents rule shown in
Figure 1 (c) can be formalized as:
t = ((x1:???) (x2:?) (x3:AD)??)
s = x1 was held x3 x2
? = {x1:???? x1, x2:?? x2, x3:AD? x3}
where the underline indicates a leaf node, and
xi:letters indicates a pair of variable and its con-
straint.
A derivation is informally defined as a sequence
of steps converting a source dependency structure
into a target language string, with each step apply-
ing one translation rule. As an example, Figure 2
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
??/VV
2010? FIFAI ??? ? ?? ?? ??
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
was held l
?/P???/NR
??/NR2010?/NT FIFA/NRI
was held successfully l  f ll
?/P
??/NR
2010 FIFA [World Cup] was held successfully  I   [ rl  ]    l   f ll   
??2010 FIFA World Cup was held successfully in  I   rl       l   f ll   i
2010 FIFA World Cup was held successfully in [South Africa] I  rl    l  f ll  i  [ t  fri ]
parser
(a)
(b)
(c)
(d)
(e)
(f)
(g)
r3: (2010?) (FIFA) ??? 
 ?2010 FIFA World Cup
r2: ???successfully
r1: (x1:???)(x2 :?)(x3:AD)??
  ?x1 was held x3 x2
r4: ? (x2:NR)?in x2
r5: ???South Africa
Figure 2: An example derivation of dependency-to-string
translation. The dash lines indicate the reordering when
employing a head-dependents rule.
shows the derivation for translating a Chinese (CH)
sentence into an English (EN) string.
CH 2010? FIFA??????????
EN 2010 FIFA World Cup was held successfully in
South Africa
218
The Chinese sentence (a) is first parsed into a de-
pendency structure (b), which is converted into an
English string in five steps. First, at the root node,
we apply head-dependents rule r1 shown in Figure
1(c) to translate the top level head-dependents rela-
tion and result in three unfinished substructures and
target string in (c). The rule is particular interesting
since it captures the fact: in Chinese prepositional
phrases and adverbs typically modify verbs on the
left, whereas in English prepositional phrases and
adverbs typically modify verbs on the right. Second,
we use head rule r2 translating ???? into ?success-
fully? and reach situation (d). Third, we apply head-
dependents rule r3 translating the head-dependents
relation rooted at ????? and yield (e). Fourth,
head-dependents rules r5 partially translate the sub-
tree rooted at ??? and arrive situation in (f). Finally,
we apply head rule r5 translating the residual node
???? and obtain the final translation in (g).
4 Rule Acquisition
The rule acquisition begins with a word-aligned cor-
pus: a set of triples ?T, S,A?, where T is a source
dependency structure, S is a target side sentence,
and A is an alignment relation between T and S.
We extract from each triple ?T, S,A? head rules that
are consistent with the word alignments and head-
dependents rules that satisfy the intuition that syn-
tactically close items tend to stay close across lan-
guages. We accomplish the rule acquisition through
three steps: tree annotation, head-dependents frag-
ments identification and rule induction.
4.1 Tree Annotation
Given a triple ?T, S,A? as shown in Figure 3, we
first annotate each node n of T with two attributes:
head span and dependency span, which are defined
as follows.
Definition 1. Given a node n, its head span hsp(n)
is a set of index of the target words aligned to n.
For example, hsp(2010?)={1, 5}, which corre-
sponds to the target words ?2010? and ?was?.
Definition 2. A head span hsp(n) is consistent if it
satisfies the following property:
?n? ?=nhsp(n?) ? hsp(n) = ?.
?/P
{5,8}{9,10}{ , }{ , }
???/NR
{3,4}{2-4}
??/NR
{9,10}{9,10}
??/AD
{7}{7}
2010?/NT
{1,5}{}{ , }{}
FIFA/NR
{2,2}{2,2}
??/VV
{6}{2-10}
2010
1
FIFA
2
I World
3
rl held
6
l successfully
7
f ll in
8
i South
9
tCup
4
was
5
Africa
10
fri
Figure 3: An annotated dependency structure. Each node
is annotated with two spans, the former is head span and
the latter dependency span. The nodes in acceptable head
set are displayed in gray, and the nodes in acceptable de-
pendent set are denoted by boxes. The triangle denotes
the only acceptable head-dependents fragment.
For example, hsp(??) is consistent, while
hsp(2010?) is not consistent since hsp(2010?) ?
hsp(?) = 5.
Definition 3. Given a head span hsp(n), its closure
cloz(hsp(n)) is the smallest contiguous head span
that is a superset of hsp(n).
For example, cloz(hsp(2010?)) = {1, 2, 3, 4, 5},
which corresponds to the target side word sequence
?2010 FIFA World Cup was?. For simplicity, we use
{1-5} to denotes the contiguous span {1, 2, 3, 4, 5}.
Definition 4. Given a subtree T ? rooted at n, the
dependency span dsp(n) of n is defined as:
dsp(n) = cloz(
?
n??T ?
hsp(n?) is consistent
hsp(n?)).
If the head spans of all the nodes of T ? is not consis-
tent, dsp(n) = ?.
For example, since hsp(?) is not consistent,
dsp(?)=dsp(??)={9, 10}, which corresponds to
the target words ?South? and ?Africa?.
The tree annotation can be accomplished by a sin-
gle postorder transversal of T . The extraction of
head rules from each node can be readily achieved
with the same criteria as (Och and Ney, 2004). In
219
the following, we focus on head-dependents rules
acquisition.
4.2 Head-Dependents Fragments Identification
We then identify the head-dependents fragments that
are suitable for rule induction from the annotated de-
pendency structure.
To facilitate the identification process, we first de-
fine two sets of dependency structure related to head
spans and dependency spans.
Definition 5. A acceptable head set ahs(T) of a de-
pendency structure T is a set of nodes, each of which
has a consistent head span.
For example, the elements of the acceptable head
set of the dependency structure in Figure 3 are dis-
played in gray.
Definition 6. A acceptable dependent set adt(T) of
a dependency structure T is a set of nodes, each of
which satisfies: dep(n) ?= ?.
For example, the elements of the acceptable de-
pendent set of the dependency structure in Figure 3
are denoted by boxes.
Definition 7. We say a head-dependents fragments
is acceptable if it satisfies the following properties:
1. the root falls into acceptable head set;
2. all the sinks fall into acceptable dependent set.
An acceptable head-dependents fragment holds
the property that the head span of the root and the de-
pendency spans of the sinks do not overlap with each
other, which enables us to determine the reordering
in the target side.
The identification of acceptable head-dependents
fragments can be achieved by a single preorder
transversal of the annotated dependency structure.
For each accessed internal node n, we check
whether the head-dependents fragment f rooted at
n is acceptable. If f is acceptable, we output an
acceptable head-dependents fragment; otherwise we
access the next node.
Typically, each acceptable head-dependents frag-
ment has three types of nodes: internal nodes, inter-
nal nodes of the dependency structure; leaf nodes,
leaf nodes of the dependency structure; head node, a
special internal node acting as the head of the related
head-dependents relation.
?/P
{5,8}{9,10}
/
{ , }{ , }
???/NR
{3,4}{2-4}
??/AD
{7}{7}
??/VV
{6}{2-10}
heldl successfullys ssf ll[FIFA World Cup][ I  rl  ] South Africa][ t  fri ]
Input:
Output:
x2:?2:x1:???1: ??
??
x1 held successfully x21  l  s ssf ll  2
(x1:???)(x2:?)(??) ??
      ?  x1  held successfully x2
(a)
(b)
Figure 4: A lexicalized head-dependents rule (b) induced
from the only acceptable head-dependents fragment (a)
of Figure 3.
4.3 Rule Induction
From each acceptable head-dependents fragment,
we induce a set of lexicalized and unlexicalized
head-dependents rules.
4.3.1 Lexicalized Rule
We induce a lexicalized head-dependents rule
from an acceptable head-dependents fragment by
the following procedure:
1. extract the head-dependents relation and mark
the internal nodes as substitution sites. This
forms the input of a head-dependents rule;
2. place the nodes in order according to the head
span of the root and the dependency spans of
the sinks, then replace the internal nodes with
variables and the other nodes with the target
words covered by their head spans. This forms
the output of a head-dependents rule.
Figure 4 shows an acceptable head-dependents
fragment and a lexicalized head-dependents rule in-
220
duced from it.
4.3.2 Unlexicalized Rules
Since head-dependents relations with verbs as
heads typically consist of more than four nodes, em-
ploying only lexicalized head-dependents rules will
result in severe sparseness problem. To alleviate
this problem, we generalize the lexicalized head-
dependents rules and induce rules with unlexicalized
nodes.
As we know, the modification relation of a head-
dependents relation is determined by the edges.
Therefore, we can replace the lexical word of each
node with its categories (i.e. POS) and obtain new
head-dependents relations with unlexicalized nodes
holding the same modification relation. Here we call
the lexicalized and unlexicalized head-dependents
relations as instances of the modification relation.
For a head-dependents relation with m node, we can
produce 2m ? 1 instances with unlexicalized nodes.
Each instance represents the modification relation
with a different specification.
Based on this observation, from each lexical-
ized head-dependent rule, we generate new head-
dependents rules with unlexicalized nodes according
to the following principles:
1. change the aligned part of the target string into
a new variable when turning a head node or a
leaf node into its category;
2. keep the target side unchanged when turning a
internal node into its category.
Restrictions: Since head-dependents relations
with verbs as heads typically consists of more than
four nodes, enumerating all the instances will re-
sult in a massive grammar with too many kinds of
rules and inflexibility in decoding. To alleviate these
problems, we filter the grammar with the following
principles:
1. nodes of the same type turn into their categories
simultaneously.
2. as for leaf nodes, only those with open class
words can be turned into their categories.
In our experiments of this paper, we only
turn those dependents with POS tag in the
set of {CD,DT,OD,JJ,NN,NR,NT,AD,FW,PN}
into their categories.
x2:?2:x1:???1: ??
heldl successfullyf ll
??
x11 x22
(x1:???)(x2:?)(??) ??
 ? x1  held successfully x2
x2:?2:x1:???1: x3:AD:
heldl x33
??
x11 x22
(x1:???)(x2:?)(x3:AD) ??
 ? x1  held x3 x2
x2:P2:x1:NR1: ??
heldl successfullyf ll
??
x11 x22
(x1:NR)(x2:P)(??) ??
? x1  held successfully x2
x2:P2:x1:NR1: x3:AD3:
heldl x33
??
x11 x22
(x1:NR)(x2:P)(x3:AD) ??
 ? x1  held x3 x2
x2:?2:x1:???1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:???)(x2:?)(??) x4:VV
 ? x1  x4 successfully x2
x2:?2:x1:???1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:???)(x2:?)(x3:AD) x4:VV
 ?   x1  x4  x3 x2
x2:P2:x1:NR1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:NR)(x2:P)(??) x4:VV
 ? x1 x4 successfully x2
x2:P2:x1:NR1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:NR)(x2:P)(x3:AD) x4:VV
 ? x1  x4 x3 x2
generalize leaf generalize leaf
generalize internalgeneralize internal
generalize leaf generalize leaf
generalize
head
Figure 5: An illustration of rule generalization. Where
?x1:???? and ?x2:?? indicate substitution sites
which can be replaced by a subtree rooted at ?????
and ??? respectively. ?x3:AD?indicates a substitution
site that can be replaced by a subtree whose root has part-
of-speech ?AD?. The underline denotes a leaf node. The
box indicates the starting lexicalized head-dependents
rule.
Figure 5 illustrates the rule generalization process
under these restrictions.
4.3.3 Unaligned Words
We handle the unaligned words of the target side
by extending the head spans of the lexicalized head
and leaf nodes on both left and right directions.
This procedure is similar with the method of (Och
and Ney, 2004) except that we might extend several
221
Algorithm 1: Algorithm for Rule Acquisition
Input: Source dependency structure T , target string S, alignment A
Output: Translation rule set R
1 HSet? ACCEPTABLE HEAD(T ,S,A)
2 DSet? ACCEPTABLE DEPENDENT(T ,S,A)
3 for each node n ? HSet do
4 extract head rules
5 append the extracted rules to R
6 if ?n? ? child(n) n? ? DSet
7 then
8 obtain a head-dependent fragment f
9 induce lexicalized and unlexicalized head-dependents rules from f
10 append the induced rules to R
11 end
12 end
spans simultaneously. In this process, we might ob-
tain m(m ? 1) head-dependents rules from a head-
dependent fragment in handling unaligned words.
Each of these rules is assigned with a fractional
count 1/m.
4.4 Algorithm for Rule Acquisition
The rule acquisition is a three-step process, which is
summarized in Algorithm 1.
We take the extracted rule set as observed data and
make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
5 The model
Following (Och and Ney, 2002), we adopt a general
log-linear model. Let d be a derivation that convert
a source dependency structure T into a target string
e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
we used seven features as follows:
- translation probabilities P (t|s) and P (s|t);
- lexical translation probabilities Plex(t|s) and
Plex(s|t);
- rule penalty exp(?1);
- language model Plm(e);
- word penalty exp(|e|).
6 Decoding
Our decoder is based on bottom up chart parsing.
It finds the best derivation d? that convert the input
dependency structure into a target string among all
possible derivations D:
d? = argmaxd?DP (D) (2)
Given a source dependency structure T , the decoder
transverses T in post-order. For each accessed in-
ternal node n, it enumerates all instances of the re-
lated modification relation of the head-dependents
relation rooted at n, and checks the rule set for
matched translation rules. If there is no matched
rule, we construct a pseudo translation rule accord-
ing to the word order of the head-dependents rela-
tion. For example, suppose that we can not find
any translation rule about to ?(2010?) (FIFA) ?
???, we will construct a pseudo translation rule
?(x1:2010?) (x2:FIFA) x3:??? ? x1 x2 x3?.
A larger translation is generated by substituting the
variables in the target side of a translation rule with
the translations of the corresponding dependents.
We make use of cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to find the k-best items with inte-
grated language model for each node.
To balance performance and speed, we prune the
search space in several ways. First, beam thresh-
222
old ? , items with a score worse than ? times of the
best score in the same cell will be discarded; sec-
ond, beam size b, items with a score worse than the
bth best item in the same cell will be discarded. The
item consist of the necessary information used in de-
coding. Each cell contains all the items standing for
the subtree rooted at it. For our experiments, we set
? = 10?3 and b = 300. Additionally, we also prune
rules that have the same source side (b = 100).
7 Experiments
We evaluated the performance of our dependency-
to-string model by comparison with replications of
the hierarchical phrase-based model and the tree-to-
string models on Chinese-English translation.
7.1 Data preparation
Our training corpus consists of 1.5M sentence
pairs from LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, Hansards portion of
LDC2004T07, LDC2004T08 and LDC2005T06.
We parse the source sentences with Stanford
Parser (Klein and Manning, 2003) into projective
dependency structures, whose nodes are annotated
by POS tags and edges by typed dependencies. In
our implementation of this paper, we make use of
the POS tags only.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) on the corpus in
both directions and applying ?grow-diag-and? re-
finement(Koehn et al, 2003).
We apply SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
modified Kneser-Ney smoothing on the Xinhua por-
tion of the Gigaword corpus.
We use NIST MT Evaluation test set 2002 as our
development set, NIST MT Evaluation test set 2004
(MT04) and 2005 (MT05) as our test set. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric (Papineni et al, 2002).1
We make use of the standard MERT (Och, 2003)
to tune the feature weights in order to maximize the
system?s BLEU score on the development set.
1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
System Rule # MT04(%) MT05(%)
cons2str 30M 34.55 31.94
hiero-re 148M 35.29 33.22
dep2str 56M 35.82+ 33.62+
Table 1: Statistics of the extracted rules on training cor-
pus and the BLEU scores on the test sets. Where ?+?
means dep2str significantly better than cons2str with p <
0.01.
7.2 The baseline models
We take a replication of Hiero (Chiang, 2007) as
the hierarchical phrase-based model baseline. In
our experiments of this paper, we set the beam size
b = 200 and the beam threshold ? = 0. The maxi-
mum initial phrase length is 10.
We use constituency-to-string model (Liu et al,
2006) as the syntax-based model baseline which
make use of composed rules (Galley et al, 2006)
without handling the unaligned words. In our exper-
iments of this paper, we set the tatTable-limit=20,
tatTable-threshold=10?1, stack-limit=100, stack-
threshold=10?1,hight-limit=3, and length-limit=7.
7.3 Results
We display the results of our experiments in Table
1. Our dependency-to-string model dep2str signif-
icantly outperforms its constituency structure-based
counterpart (cons2str) with +1.27 and +1.68 BLEU
on MT04 and MT05 respectively. Moreover, with-
out resort to phrases or parse forest, dep2str sur-
passes the hierarchical phrase-based model (hiero-
re) over +0.53 and +0.4 BLEU on MT04 and MT05
respectively on the basis of a 62% smaller rule set.
Furthermore, We compare some actual transla-
tions generated by cons2str, hiero-re and dep2str.
Figure 6 shows two translations of our test sets
MT04 and MT05, which are selected because each
holds a long distance dependency commonly used in
Chinese.
In the first example, the Chinese input holds
a complex long distance dependencies ?? ?
? ?... ?...? ???. This dependency cor-
responds to sentence pattern ?noun+prepostional
phrase+prepositional phrase+verb?, where the for-
mer prepositional phrase specifies the position and
the latter specifies the time. Both cons2str and
hiero-re are confused by this sentence and mistak-
223
??? ? ?? ??? ? ?? ?? ?? ? ?? ?
Afterft r briefri f talkst l withit Powellll ,, thet US State Departmentt t  rt t Barnierr i r ,,saidi
MT05----Segment 163
Reference: After a brief talk with 
Powell at the US State 
Department, Barnier said:
Cons2str: Barnier after brief 
talks in US State Department 
and Powell  said:
Hiero-re: After a short meeting 
with Barnier on the US State 
Department, Powell said:
Dep2str: After brief talks with 
Powell, the US State 
Department Barnier said,
?? ?? ?? ??? ? ?? ?? 1373 2001 ?
Chinai appreciatesr i t efforts
 
ff rt off Anti -ti - Terrorismrr ri Committee itt tot promoter t allll inicountriestri
MT04----Segment 1096
Reference: China appreciates the 
efforts of the Counter-Terrorism 
Committee to promote the 
implementation of the resolution 
1373(2001) in all states and to 
help enhance the anti-terrorist 
capabilities of developing 
countries.
Cons2str: China appreciates 
Anti - Terrorist Committee for 
promoting implementation of 
the resolution No. 1373(2001) 
and help developing countries 
strength counter-terrorism  
capability building for the 
efforts,
Hiero-re: China appreciates 
Anti - Terrorism Committee to 
promote countries implement 
resolution No . 1373 ( 2001 ) 
and help developing countries 
strengthen anti-terrorism 
capacity building support for 
efforts
Dep2str: China appreciates 
efforts of Anti - Terrorism 
Committee to promote all 
countries in the implementation 
of resolution  1373 ( 2001 )  , to 
help strengthen the anti-
terrorism capability building of 
developing countries
?? ? ? ? ?? ??? ????? ? ? ?
nsubj             
prep prep
????? ? ?? ?
thet implementationi l t ti off ......
nsubj dobj
Figure 6: Actual translations produced by the baselines and our system. For our system, we also display the long
distance dependencies correspondence in Chinese and English. Here we omit the edges irrelevant to the long distance
dependencies.
enly treat ???(Powell)? as the subjective, thus
result in translations with different meaning from
the source sentence. Conversely, although ??? is
falsely translated into a comma, dep2str captures
this complex dependency and translates it into ?Af-
ter ... ,(should be at) Barnier said?, which accords
with the reordering of the reference.
In the second example, the Chinese input holds
a long distance dependency ??? ?? ... ?
?? which corresponds to a simple pattern ?noun
phrase+verb+noun phrase?. However, due to the
modifiers of ???? which contains two sub-
sentences including 24 words, the sentence looks
rather complicated. Cons2str and hiero-re fail to
capture this long distance dependency and provide
monotonic translations which do not reflect the
meaning of the source sentence. In contrast, dep2str
successfully captures this long distance dependency
and translates it into ?China appreciates efforts of
...?, which is almost the same with the reference
?China appreciates the efforts of ...?.
All these results prove the effectiveness of our
dependency-to-string model in both translation and
long distance reordering. We believe that the ad-
vantage of dep2str comes from the characteristics of
dependency structures tending to bring semantically
related elements together (e.g., verbs become adja-
cent to all their arguments) and are better suited to
lexicalized models (Quirk et al, 2005). And the in-
capability of cons2str and hiero-re in handling long
distance reordering of these sentences does not lie in
the representation of translation rules but the com-
promises in rule extraction or decoding so as to bal-
ance the speed or grammar size and performance.
The hierarchical phrase-based model prohibits any
nonterminal X from spanning a substring longer
than 10 on the source side to make the decoding al-
gorithm asymptotically linear-time (Chiang, 2005).
224
While constituency structure-based models typically
constrain the number of internal nodes (Galley et
al., 2006) and/or the height (Liu et al, 2006) of
translation rules so as to balance the grammar size
and performance. Both strategies limit the ability of
the models in processing long distance reordering of
sentences with long and complex modification rela-
tions.
8 Related Works
As a first step towards semantics, dependency struc-
tures are attractive to machine translation. And
many efforts have been made to incorporating this
desirable knowledge into machine translation.
(Lin, 2004; Quirk et al, 2005; Ding and Palmer,
2005; Xiong et al, 2007) make use of source depen-
dency structures. (Lin, 2004) employs linear paths
as phrases and view translation as minimal path cov-
ering. (Quirk et al, 2005) extends paths to treelets,
arbitrary connected subgraphs of dependency struc-
tures, and propose a model based on treelet pairs.
Both models require projection of the source depen-
dency structure to the target side via word alignment,
and thus can not handle non-isomorphism between
languages. To alleviate this problem, (Xiong et al,
2007) presents a dependency treelet string corre-
spondence model which directly map a dependency
structure to a target string. (Ding and Palmer, 2005)
presents a translation model based on Synchronous
Dependency Insertion Grammar(SDIG), which han-
dles some of the non-isomorphism but requires both
source and target dependency structures. Most im-
portant, all these works do not specify the ordering
information directly in translation rules, and resort
to either heuristics (Lin, 2004; Xiong et al, 2007) or
separate ordering models(Quirk et al, 2005; Ding
and Palmer, 2005) to control the word order of
translations. By comparison, our model requires
only source dependency structure, and handles non-
isomorphism and ordering problems simultaneously
by directly specifying the ordering information in
the head-dependents rules that represent the source
side as head-dependents relations and the target side
as strings.
(Shen et al, 2008) exploits target dependency
structures as dependency language models to ensure
the grammaticality of the target string. (Shen et al,
2008) extends the hierarchical phrase-based model
and present a string-to-dependency model, which
employs string-to-dependency rules whose source
side are string and the target as well-formed depen-
dency structures. In contrast, our model exploits
source dependency structures, as a tree-based sys-
tem, it run much faster (linear time vs. cubic time,
see (Huang et al, 2006)).
9 Conclusions and future work
In this paper, we present a novel dependency-to-
string model, which employs head-dependents rules
that represent the source side as head-dependents
relations and the target side as string. The head-
dependents rules specify the ordering information
directly and require only substitution operation.
Thus, our model does not need heuristics or order-
ing model of the previous works to control the word
order of translations. Large scale experiments show
that our model exhibits good performance in long
distance reordering and outperforms the state-of-
the-art constituency-to-string model and hierarchi-
cal phrase-based model without resort to phrases and
parse forest. For the first time, a source dependency-
based model shows improvement over the state-of-
the-art translation models.
In our future works, we will exploit the semantic
information encoded in the dependency structures
which is expected to further improve the transla-
tions, and replace 1-best dependency structures with
dependency forests so as to alleviate the influence
caused by parse errors.
Acknowledgments
This work was supported by National Natural Sci-
ence Foundation of China, Contract 60736014,
60873167, 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions. We appreciate Yajuan Lv,
Wenbin Jiang, Hao Xiong, Yang Liu, Xinyan Xiao,
Tian Xia and Yun Huang for the insightful advices in
both experiments and writing. Special thanks goes
to Qian Chen for supporting my pursuit all through.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
225
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of ACL 2005.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of EMNLP 2002,
pages 304?311.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL 2006, pages 961?968, Sydney, Australia,
July. Association for Computational Linguistics.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. In Dependenz und Valenz / Dependency and Va-
lency, volume 2, pages 1081?1109. Berlin, New York.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proceedings of ACL 2007, pages 144?151, Prague,
Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing, pages 1?8, New York City,
New York, June. Association for Computational Lin-
guistics.
Richard Hudson. 1990. English Word Grammar. Black-
ell.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of Coling 2004,
pages 625?630, Geneva, Switzerland, Aug 23?Aug
27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL 2006, pages 609?616,
Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL-
2003, pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
ACL 2002, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271?
279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL 2008: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computational
Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A depen-
dency treelet string correspondence model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
40?47, Prague, Czech Republic, June.
Arnold M. Zwicky. 1985. Heads. Journal of Linguistics,
21:1?29.
226
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 545?555,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and
Forest-to-String Decoding?
Martin C?mejrek??
?IBM Prague Research Lab
V Parku 2294/4
Prague, Czech Republic, 148 00
martin.cmejrek@us.ibm.com
Haitao Mi? and Bowen Zhou?
?IBM T. J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10598
{hmi,zhou}@us.ibm.com
Abstract
Machine translation benefits from system
combination. We propose flexible interaction
of hypergraphs as a novel technique combin-
ing different translation models within one de-
coder. We introduce features controlling the
interactions between the two systems and ex-
plore three interaction schemes of hiero and
forest-to-string models?specification, gener-
alization, and interchange. The experiments
are carried out on large training data with
strong baselines utilizing rich sets of dense
and sparse features. All three schemes signif-
icantly improve results of any single system
on four testsets. We find that specification?a
more constrained scheme that almost entirely
uses forest-to-string rules, but optionally uses
hiero rules for shorter spans?comes out as
the strongest, yielding improvement up to 0.9
(Ter-Bleu)/2 points. We also provide a de-
tailed experimental and qualitative analysis of
the results.
1 Introduction
Recent years have witnessed the success of var-
ious statistical machine translation (SMT) mod-
els using different levels of linguistic knowledge?
phrase (Koehn et al, 2003), hiero (Chiang, 2005),
and syntax-based (Liu et al, 2006; Galley et al,
2006). System combination became a promising
way of building up synergy from different SMT sys-
tems and their specific merits.
Numerous efforts that have been proposed in this
field recently can be broadly divided into two cat-
?M. C? and H. M. contributed equally to this work.
egories: Offline system combination (Rosti et al,
2007; He et al, 2008; Watanabe and Sumita, 2011;
Denero et al, 2010) aims at producing consensus
translations from the outputs of multiple individ-
ual systems. Those outputs usually contain k-best
lists of translations, which only explore a small por-
tion of the entire search space of each system. This
issue is well addressed in joint decoding (Liu et
al., 2009), or online system combination, showing
comparable improvements to the offline combina-
tion methods. Rather than finding consensus trans-
lations from the outputs of individual systems, joint
decoding works with different grammars at the de-
coding time. Although limited to individual systems
sharing the same search paradigm (e.g. left-to-right
or bottom-up), joint decoding offers many poten-
tial advatages: search through a larger space, bet-
ter efficiency, features designed once for all subsys-
tems, potential cross-system features, online sharing
of partial hypotheses, and many others.
Different approaches have different strengths in
general?hiero rules are believed to provide reliable
lexical coverage, while tree-to-string rules are good
at non-local reorderings. Different contexts present
different challenges?noun phrases usually follow
the adjacency principle, while verb phrases require
more challenging reorderings. In this work, we study
different schemes of interaction between translation
models, reflecting their specific strengths at differ-
ent (syntactic) contexts. We make five new contribu-
tions:
First, we propose a framework for joint decod-
ing by means of flexible combination of trans-
lation hypergraphs, allowing for detailed con-
545
trol of interactions between the different sys-
tems using soft constraints (Section 3).
Second, we study three interaction schemes?
special cases of joint decoding: generalization,
specification, and interchange (Section 3.3).
Third, instead of using a tree-to-string system,
we use a much stronger forest-to-string sys-
temwith fuzzy match of nonterminal categories
(Section 2.1).
Fourth, we train strong systems on a large-
scale data set, and test all methods on four test
sets. Experimental results (Section 6) show that
our new approach brings improvement of up to
0.9 points in terms of (Ter ? Bleu)/2 over the
best single system.
Fifth, we conduct a comprehensive experimen-
tal analysis, and find that joint decoding actu-
ally prefers tree-to-string rules in both shorter
and longer spans. (Section 6.3).
The paper is organized as follows: We briefly re-
view the individual models in Section 2, describe
the method of joint decoding using three alternative
interaction schemes in Section 3, describe the fea-
tures controlling the interactions and fuzzy match in
Section 4, review the related work in Section 5, and
finally, describe our experiments and give detailed
discussion of the results in Section 6.
2 Individual Models
Our individual models are two state-of-the-art sys-
tems: a hiero model (Chiang, 2005), and a forest-to-
string model (Mi et al, 2008; Mi and Huang, 2008).
We will use the following example from Chinese
to English to explain both individual and joint de-
coding algorithms throughout this paper.
SS ta?olu`nSSSSSSSS hu`iSSSSS ze?nmeya`ng
discussion/NN SSS will/VV how/VV
S discuss/VV SS meeting/NN
There are several possible meanings based on the
different POS tagging sequences:
1: NN VV VV: How is the discussion going?
2: VV NN VV: Discuss about the meeting.
3: NN NN VV: How was the discussion meeting?
4: VV VV VV: Discuss what will happen.
id rule
r1 VV(ta?olu`n) ? discuss
r2 NP(ta?olu`n) ? the discussion
r3 NP(hu`i) ? the meeting
r4 VP(ze?nmeya`ng) ? how
r?4 VP(ze?nmeya`ng) ? about
r5 IP(x1:NP x2:VP) ? x2 x1
r6 IP(x1:VV x2:IP) ? x1 x2
r7 IP(x1:NP VP(VV(hu`i) x2:VP)) ? x2 is x1 going
r11 X(x1:X ze?nmeya`ng) ? how was x1
r12 X(ze?nmeya`ng) ? what
r13 X(ta?olu`n hu`i) ? the discussion meeting
r14 X(hu`i x1:X) ? x1 will happen
r15 S(x1:S x2:X) ? x1 x2
Table 1: Translation rules. Tree-to-string (r1?r7), hiero
(r11?r14), vanilla glue (r15).
IP
x1:NP VP
VV
hu`i
x2:VP
? x2 is x1 going
Figure 1: Tree-to-string rule r7.
Table 1 shows translation rules that can generate
all four translations. We will use those rules in the
following sections.
2.1 Forest-to-string
Forest-to-string translation (Mi et al, 2008) is a lin-
guistic syntax-based system, which significantly im-
proves the translation quality of the tree-to-string
model (Liu et al, 2006; Huang et al, 2006) by using
a packed parse forest as the input instead of a single
parse tree.
Figure 1 shows a tree-to-string translation
rule (Huang et al, 2006), which is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-side
tree fragment, whose internal nodes are labeled by
nonterminal symbols (like NP and VP), and whose
frontier nodes are labeled by source-language words
(like ?hu`i?) or variables from a set X = {x1, x2, . . .};
rhs(r) is the target-side string expressed in target-
language words (like ?going?) and variables; and
?(r) is a mapping from X to nonterminals. Each
546
(a)
IP0, 3
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
VP1, 3
VP2, 3
ze?nmeya`ng
Rt
? (b)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X0, 3
e5
e6
e7
? Rh ?
(b?)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X2, 3
X0, 3
e11
e14
? (c)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X2, 3
X0, 3
Figure 2: Parse and translation hypergraphs. (a) The parse forest of the example sentence. Solid hyperedges denote
the 1-best parse. (b) The corresponding translation forest F t after applying the tree-to-string translation rule set Rt.
Target lexical content is not shown. Each translation hyperedge (e.g. e7) has the same index as the corresponding rule
(r7). Gray nodes (e.g. VP1,3) became inaccessible due to the insufficient rule coverage. (b?) The translation forest Fh
after applying the hierarchical rule set Rh to the input sentence. (c) The combined translation forest Hm obtained by
superimposing b and b?. The nodes within each solid box share the same span. See Figure 3 for an example of the
internal structure of a box. The forest-to-string system can produce the translation 1 (dashed derivation: r2, r4 and r7)
and 2 (solid derivation: r1, r3, r?4, r5, and r6). Hierarchical rules generate the translation 3 (r11 and r13). The translation
4 is available by using joint decoding at X1, 3 ? IP1, 3 with the derivation: r1, r6, r12, and r14.
variable xi ? X occurs exactly once in lhs(r) and
exactly once in rhs(r). Take the rule r7 in Figure 1
for example, we have:
lhs(r7) = IP(x1:NP VP(VV(hu`i) x2:VP)),
rhs(r7) = x2 is x1 going,
?(r7) = {x1 7? NP, x2 7? VP}.
Typically, a forest-to-string system performs
translation in two steps (shown in Figure 2): pars-
ing and decoding. In the parsing step, we convert the
source language input into a parse forest (a). In the
decoding step, we first convert the parse forest into a
translation forest Ft in (b) by using the fast pattern-
matching technique (Zhang et al, 2009). For exam-
ple, we pattern-match the rule r7 rooted at IP0, 3, in
such a way that x1 spans NP0, 1 and x2 spans VP2, 3,
and add a translation hyperedge e7 in (b). Then the
decoder searches for the best derivation on the trans-
lation forest and outputs the target string.
2.2 Hiero
Hiero (hierarchical phrase-based) model (Chiang,
2005) acquires rules of synchronous context-free
grammars (SCFGs) from word-aligned parallel data,
and uses plain sequences of words as the input, with-
out any syntactic information.
547
FN
IP?1, 3
IP1, 3
BBBBSN
X?1, 3
X1, 3
EEEE
scheme interaction edges in supernode
Generalization
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Specification
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Interchange
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Figure 3: Three interaction schemes for joint decoding.
Details of the interaction supernode for span (1, 3) shown
in Figure 2 (c). Soft constraints control the transitions.
SCFG can be formalized as a set of tuples
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-side
one-level CFG, whose root is X or S, and whose
frontier nodes are labeled by source-language words
(like ?hu`i?) or variables from a set X = {x1, x2, . . .};
rhs(r) is the target-side string expressed in target-
language words (like ?going?) and variables; and
?(r) is a mapping from X to nonterminals. Table 1
shows examples of hiero rules r11?r15.
Although different on source side, hiero decod-
ing can be formalized equally as forest-to-string de-
coding: First, pattern-match the input sentence into
a translation forest Fh. For example, since the rule
r11 matches ?ze?nmeya`ng? such that x1 spans the first
two words, add a hyperedge e11 in Figure 2 (b?).
Then search for the best derivation over the trans-
lation forest.
3 Joint Decoding
The goal of joint decoding is to let different MT
models collaborate within the framework of a single
decoder. This can be done by combining translation
hypergraphs of the different models at the decod-
ing time, so that online sharing of partial hypotheses
overcomes weaknesses and boosts strengths of the
systems combined.
As both forest-to-string and hiero produce trans-
lation forests that share the same hypergraph struc-
ture, we first formalize the hypergraph, then we in-
troduce an algorithm to combine different hyper-
graphs, and finally we describe three joint decoding
schemes over the merged hypergraph.
3.1 Hypergraphs
More formally, a hypergraph H is a pair ?V, E?,
where V is the set of nodes, and E the set of hyper-
edges. For a given sentence w1:l = w1 . . .wl, each
node v ? V is in the form of Y i, j, where Y is a
nonterminal in the context-free grammar1 and i, j,
0 ? i < j ? l, are string positions in the sentence
w1:l, which denote the recognition of nonterminal
Y spanning the substring from positions i through j
(that is, wi+1 . . .w j). Each hyperedge e ? E is a tuple
?tails(e), head(e), target(e)?, where head(e) ? V is
the consequent node in the deductive step, tails(e) ?
V? is the list of antecedent nodes, and target(e) is
a list of rhs(r) for rules r such that each rule r has
the same lhs(r) pattern-matched at the node head(e).
For example, the hyperedge e7 in Figure 2 (b) is
e7 = ?(NP0, 1,VP2, 3), IP0, 3, (x2 is x1 going)?,
where we can infer the mapping to be
{x1 7? NP0, 1, x2 7? VP2, 3 }.
We also denote BS(v) to be the set of incoming
hyperedges of node v, which represent the different
ways of deriving v. For example, BS(IP0, 3) is a set
of e7 and e6.
There is also a distinguished root node TOP in
each hypergraph, denoting the goal item in transla-
tion, which is simply TOP0, l.
3.2 Combining Hypergraphs
We enable interaction between translation hyper-
graphs, such as hiero Fh = ?Vh, Eh? and forest-to-
string Ft = ?V t, Et?, on nodes covering the same
span (e.g. IP1, 3 and X1, 3 in Figure 2 (c) grouped in
a box). We call such groups interaction supernodes
and show a detailed example of a supernode for span
(1, 3) in Figure 3.
The combination runs in four steps:
1In this paper, nonterminal labels X and S denote hiero
derivations, other labels are tree-to-string labels.
548
1. For each node v = Y i, j, v ? Vh ? V t, we create
a new interaction node v? = Y ?i, j with empty
BS (v?). For example, we create two nodes,
IP?1, 3 and X?1, 3, at the top of Figure 3.
2. For each hyperedge e ? BS(v), v ? V t ? Vh,
we replace each v in tails(e) with v?. For exam-
ple, e7 becomes ?(NP?0, 1,VP?2, 3), IP0, 3, (x2 is
x1 going)?.
3. All the nodes and hyperedges form the merged
hypergraph Fm, such as in Figure 2 (c).
4. Insert interaction hyperedges connecting nodes
within each interaction supernode to make Fm
connected again.
In the following subsection we present details of in-
teractions and introduce three alternative schemes.
3.3 Three Schemes of Joint Decoding
Interaction hyperedges within each supernode allow
the decoder either to stay within the same system
(e.g. in hiero using X1, 3 ? X?1, 3 in Figure 3), or to
switch to the other (e.g. to forest-to-string using X1, 3
? IP?1, 3).
For example, translation 4 can be produced as
follows: The source string ?ze?nmeya`ng? is trans-
lated by the phrase rule r12. The hiero hyperedge
e14 combines it with the translation of ?hu`i?, reach-
ing the hiero node X1, 3. Using the interaction edge
X1, 3 ? IP?1, 3 will switch into the tree-to-string
model, so that the translation can be completed with
the tree-to-string edge e6 that connects it with a par-
tial tree-to string translation of ?ta?olu`n? done by r1.
In order to achieve more precise control over the
interaction between tree-to-string and hiero deriva-
tions, we propose the following three basic inter-
action schemes: generalization, specification, in-
terchange. The schemes control the interaction be-
tween hiero and tree-to-string models by means of
soft constraints. Some schemes may even restrict
certain types of transitions. The schemes are de-
picted in Figure 3 and their details are discussed in
the following three subsections.
3.3.1 Specification
The specification decoding scheme reflects the in-
tuition of using hiero rules to translate shorter spans
and tree-to-string rules to reorder higher-level sen-
tence structures. In other words, the scheme allows
one-way switching from the hiero general nontermi-
nal into the more specific nonterminal of a tree-to-
string rule. Transitions in reverse directions are not
allowed. This is achieved by inserting specification
interaction hyperedges e leading from hiero nodes
Xi, j or Si, j into all tree-to-string interaction nodes
Y?i, j within the same supernode.
3.3.2 Generalization
In some translation domains, hiero outperforms
tree-to-string systems, as was shown in experiments
in Section 6. While local hiero or tree-to-string re-
orderings perform well, long distance reorderings
proposed by tree-to-string may be too risky (e.g. due
to parsing errors), so that monotone concatenation
of long sequences2 is the more reliable strategy. The
generalization decoding scheme, complementary to
the specification, is motivated by the idea of incorpo-
rating reliable tree-to-string translations for some se-
quences into a strong hiero translation system. This
is achieved by inserting generalization interaction
hyperedges e leading from tree-to-string nodes Yi, j
nodes into general hiero interaction nodes X?i, j and
S?i, j within the same supernode.
3.3.3 Interchange
The interchange decoding scheme is a union of
the two previous approaches. Any derivation can
freely combine hiero and tree-to-string productions.
Both specification and generalization interaction
hyperedges are inserted leading from all hiero and
tree-to-string nodes Xi, j, Si, j, and Yi, j into all inter-
action nodes X?i, j, S?i, j, and Y?i, j.
3.4 Fuzzy match
The translation rule set cannot usually cover all
hyperedges in the parse forest, thus some nodes
become inaccessible in the translation forest (e.g.
VP1, 3 in Figure 2). However, in the parse forest, as
opposed to a 1-best tree, we can find other nodes
spanning the same sequence wi: j (e.g. node IP1, 3).
In order to re-enable inaccessible nodes and to in-
crease the variability of the translation forest, we
allow reaching them from the other tree-to-string
2Monotone glue is the only possibility for very long spans
exceeding the hiero maxParse treshold.
549
nodes within the same interaction node. This can
be achieved by adding fuzzy hyperedges between
every tree-to-string state Y i, j and a differently la-
beled tree-to-string interaction state Z?i, j. For exam-
ple, in the span (0,1), we have a fuzzy hyperedge
VV0, 1 ? NP?0, 1.
While interaction hyperedges combine different
translation models, fuzzy hyperedges combine dif-
ferent derivations within the same (tree-to-string)
model.
4 Interaction Features
Our baseline systems use the log-linear framework
to estimate the probability P(D) of a derivation D
from features ?i and their weights ?i as P(D) ?
exp
(?
i ?i?i
)
. Similarly as Chiang et al (2009), our
systems use tens of dense (e.g. language models,
translation probabilities) and thousands of sparse
(e.g. lexical, fertility) features.
The features related to the joint decoding experi-
ments are the costs for specification, generalization,
interchange, and the fuzzy match. Let Lt be the set
of the labels used by the source language parser and
Lh = {S,X} be the labels used by hiero.
The generalization feature
?Y?Z = |{e; e ? D,?i, j tails(e) = {Yi, j} (1)
?head(e) = Z?i, j}|
is the total number of generalization hyperedges in
D going from tree-to-string states Y ? Lt to hiero
states Z? ? Lh.
The specification feature
?Z?Y = |{e; e ? D,?i, j tails(e) = {Zi, j} (2)
?head(e) = Y?i, j}|
is the total number of specification hyperedges in D
going from hiero states Z ? Lh to tree-to-string states
Y ? ? Lt.
The interchange feature is implemented by en-
abling the generalization and specification features
at the same time for both tuning and testing.
The fuzzy match feature
?U?W = |{e; e ? D,?i, j tails(e) = {Ui, j} (3)
?head(e) = W?i, j}|
is the total number of fuzzy match hyperedges in D
going from tree-to-tree statesU ? Lt to tree-to-string
states W? ? Lt. 3
We use MIRA to obtain weights for the new fea-
tures by tuning on the development set. The num-
ber of new parameters to tune can be estimated as
|Lh| ? |Lt| for generalization and specification, and
2 ? |Lh| ? |Lt| for interchange. For the fuzzy match
of tree-to-string nonterminals we have |Lt| ? |Lt| pa-
rameters organized as a sparse matrix, since we only
consider combinations on nonterminal labels that
cooccur in the data.4
5 Related Work
From the previous explorations of online translation
model combination, we see the work of Liu et al
(2009) proposing an unconstrained combination of
hiero and tree-to-string models as a special configu-
ration of our framework, and we also replicate it.
Denero et al (2010) combine translation mod-
els even with different search paradigms. Their ap-
proach is different, since their component systems
do not interact at decoding time, instead, each of
them provides its weighted translation forest first,
the forests are then combined to infer a new com-
bination model.
6 Experiment
In this section we describe the setup, present results,
and analyze the experiments. Finally, we propose fu-
ture directions of research.
3Here we allow U = W, which can be viewed in such a way
that exact match is a special case of fuzzy match.
4We also carried out an alternative experiment with only
three fuzzy match features estimated from the training data
parse forest by Na??ve Bayes by observing all spans in the train-
ing data, accumulating counts Cs(U) and Cs(U,W) of nonter-
minals (or pairs of nonterminals) heading the same span s. The
first two features (one for each direction) are based on condi-
tional probabilities:
?(U |W) = ? log
(
?
s?spans Cs (U,W)
?
s?spans Cs(W)
)
. (4)
The third feature is based on joint probability:
?(U,W) = ? log
(
?
s?spans Cs(U,W)
?
s?spans,A,B?Lt Cs(A, B)
)
. (5)
The average performance drops by 0.1 (Ter-Bleu)/2 points,
compared to the interchange eperiment.
550
System
GALE-web P1R6-web MT08 news MT08 web Avg.
Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
Single
T2S 32.6 11.6 16.9 23.5 37.7 7.8 28.1 14.5 14.4
Hiero 33.7 10.2 17.0 23.1 39.2 6.3 28.8 13.7 13.3
F2S 34.0 10.3 17.3 23.2 39.6 6.3 29.2 13.6 13.4
Joint
Liu:09 34.1 9.7 17.0 23.0 38.8 6.7 29.0 13.2 13.2
Gen. 34.4 9.7 17.8 22.6 40.0 6.1 29.6 13.1 12.9
Spe. 35.1 9.4 18.1 22.2 40.2 5.8 29.6 12.9 12.6
Int. 34.9 9.4 17.9 22.3 40.0 6.2 29.6 12.9 12.7
Table 2: All results of single and joint decoding systems.
6.1 Setup
The training corpus consists of 16 million sen-
tence pairs available within the DARPA BOLT
Chinese-English task. The corpus includes a mix
of newswire, broadcast news, webblog and comes
from various sources such as LDC, HK Law, HK
Hansard and UN data. The Chinese text is seg-
mented with a segmenter trained on CTB data using
conditional random fields (CRF). Language models
are trained on the English side of the parallel cor-
pus, and on monolingual corpora, such as Gigaword
(LDC2011T07) and Google News, altogether com-
prising around 10 billion words.
We use a modified version of the Berkeley parser
(Petrov and Klein, 2007) to obtain a parse forest
for each training sentence, then we prune it with
the marginal probability-based inside-outside algo-
rithm to contain only 3n CFG nodes, where n is the
sentence length. Finally, we apply the forest-based
GHKM algorithm (Mi and Huang, 2008; Galley et
al., 2004) to extract tree-to-string translation rules
from forest-string pairs.
In the decoding step, we prune the input hyper-
graphs to 10n nodes before we use fast pattern-
matching (Zhang et al, 2009) to convert the parse
forest into the translation forest.
We tune on 1275 sentences, each with 4 refer-
ences, from the LDC2010E30 corpus, initially re-
leased under the DARPA GALE program.
All MT experiments are optimized with
MIRA (Crammer et al, 2006) to maximize
(Ter-Bleu)/2.
We test on four different test sets: GALE-web test
set from LDC2010E30 corpus (1239 sentences, 4
references), P1R6-web test set from LDC2012E124
corpus (1124 sentences, 1 reference), NIST MT08
newswire portion (691 sentences, 4 references), and
NIST MT08 web portion (666 sentences, 4 refer-
ences).
6.2 Results
Table 2 shows all results of single and joint decoding
systems. The Bleu score of the single hiero baseline
is 39.2 on MT08-news, showing that it is a strong
system. The single F2S baseline achieves compara-
ble scores on all four test sets.
Then, for reference, we present results of joint Hi-
ero and T2S decoding, which is, to our knowledge, a
strong and competitive reimplementaion of the work
described by Liu et al (2009). Finally, we present re-
sults of joint decoding of hiero and F2S in three in-
teraction schemes: generalization, specification, and
interchange.
All three combination schemes significantly im-
prove results of any single system on all four test-
sets. On average and measured in (Ter-Bleu)/2,
our systems improve the best single system by 0.4
(generalization), 0.7 (specification), and 0.6 (inter-
change).
The specification comes out as the strongest inter-
action scheme, beating the second interchange on 2
testsets by 0.1 and 0.4 (Ter-Bleu)/2 points and on 3
testsets by 0.2 Bleu points.
6.3 Discussion of Results
Interpretations of model behavior with thousands of
parameters that may possibly overlap and interfere
should be always attempted with caution. In this sec-
tion we highlight some interesting observations, ac-
551
Specification Generalization Interchange
X ? ? ? ? X X ? ? ? ? X
VP
IP
VV
NR
ADVP
QP
CC
DVP
NP
P
...
CS
CP
AD
VRD
PU
ADJP
DNP
PP
PRN
DP
0.069
0.059
0.053
0.032
0.025
0.023
0.017
0.017
0.017
0.012
...
-0.005
-0.007
-0.011
-0.012
-0.028
-0.028
-0.045
-0.064
-0.069
-0.092
QP
PP
NN
DP
NR
DNP
NP
LC
DEC
DEG
...
VV
PRN
PN
BA
VP
VRD
JJ
VC
DFL
PU
0.057
0.054
0.048
0.044
0.034
0.032
0.030
0.025
0.023
0.023
...
-0.010
-0.011
-0.013
-0.015
-0.015
-0.028
-0.035
-0.037
-0.054
-0.073
VV
VP
NN
QP
ADVP
LCP
NP
P
IP
NR
...
VSB
PN
PU
M
VRD
DNP
ADJP
PP
DP
PRN
0.062
0.044
0.034
0.025
0.022
0.021
0.018
0.017
0.016
0.016
...
-0.004
-0.004
-0.004
-0.007
-0.014
-0.023
-0.039
-0.058
-0.070
-0.080
NN
PP
CP
LCP
DEG
DP
DEC
QP
LC
NP
...
FLR
DVP
BA
JJ
AS
VRD
ADVP
PN
DFL
PU
0.048
0.041
0.035
0.035
0.031
0.028
0.027
0.027
0.021
0.019
...
-0.006
-0.009
-0.010
-0.011
-0.014
-0.017
-0.021
-0.033
-0.038
-0.103
Table 3: Examples of specification, generalization, and interchange weights. POS tags in italics.
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
 18
AD
VP CL
P
AD
JP
FL
R
D
FL D
P
VC
P
VS
B
VR
D
VC
D QP NP DV
P
D
N
P
LC
P PP VP CP
PR
N IP
FR
AG
Av
er
ag
e 
sp
an
 le
ng
th
Figure 4: Average span length for selected syntactic la-
bels on GALE-web test set.
companying them with our subjective judgements
and speculations.
Table 3 shows the specification and generalization
features tuned for the three combination schemes,
then sorted by their weights ?X?Y or ?Y?X . Features
shown at the top of the table are very expensive (the
#Interactions Generalization Inter. gen.
F2S ? glue 5557 4202
F2S ? hiero 695 1178
total gen. 6252 5380
Specification Inter. spec.
phrase ? F2S 2763 2235
glue ? F2S 946 841
hiero ? F2S 683 839
total spec. 4392 3915
Table 5: Rule interactions on GALE-web test set.
system tries to avoid them), while inexpensive fea-
tures are at the bottom (the system is encouraged to
use them).
The most expensive interactions for the specifi-
cation belong to constituents (IP, VP) that usually
occur higher in a syntactic tree (see Figure 4 for av-
erage span lengths of selected syntactic labels), and
often require non-local reorderings. This indicates
that the decoder is discouraged from switching from
hiero into F2S derivation at these higher-level spans.
552
rule type Generalization Specification Interchange
F2S 18,807 58% 19,399 70% 18,400 61%
Hiero 3,730 12% 2,330 8% 3,133 10%
Glue 7,367 23% 571 2% 4,714 16%
Phrase 2,274 7% 5,484 20% 3,868 13%
total 32,178 27,784 30,115
Table 4: Rule counts on GALE-web test set.
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Generalization
F2S
Hiero
Glue
Phrase
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Specification
F2S
Hiero
Glue
Phrase
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Interchange
F2S
Hiero
Glue
Phrase
Figure 5: Rule distributions on GALE-web test set.
The third most expensive feature belongs to a
part-of-speech tag?the preterminal VV. We may
hypothesize that it shows the importance of lexical
information for the precision of reordering typically
carried out within (parent) VP nodes, and/or the im-
portance of POS information for succesful disam-
biguation of word senses in translation. Ideally, the
system can use a VP rule with a lexicalized VV. Less
preferably, the VV part has to be translated by an-
other T2S rule (losing the lexical constraint). In the
worst case, the system has to use a hiero hypothe-
sis to translate the VV part (losing the syntactic con-
straint), risking imprecise translation, since the hiero
rule is not constrained to senses corresponding to the
source POS VV. Again, the high penalty discourages
from using the hiero derivation in this context.
On the other hand, the bottom of the table shows
labels that encourage using hiero?DP, PP, DNP,
ADJP, etc.?shorter phrases that tend to be monotone
and less ambiguous.
Similar interpretations seem plausible when ex-
amining the generalization experiment. Expensive
features related to preterminals (NR, NN, CD) may
suggest two alternative principles: First, using F2S
rules for thes POS categories and then switching to
hiero is discouraged, since these contexts are more
reliably handled by hiero due to better lexical cover-
age and common adjacency in nominal categories.
Second, since there is only one attempt to switch
from F2S derivation to hiero, letting F2S complete
even larger spans (and maybe switching to hiero
later) is favorable.
The tail of generalization feature weights is more
difficult to interpret. The discount on VP encourages
decoder to use F2S for entire verb phrases before
switching to hiero, on the other hand, other verb-
related preterminals occupy the tail as well, hurrying
into early switching from F2S to hiero.
553
Finally, the feature weights tuned for the in-
terchange experiment are divided into two sub-
columns. Both generalization and specification
weights show similar trends as in the previous two
interaction schemes, although blurred (VP and IP
descending from the absolute top). Since transitions
in both ways are allowed, the search space is big-
ger and the system may behave differently. It is even
possible for a path in the hypergraph to zigzag be-
tween F2S and hiero nodes to collect interaction dis-
counts, ?diluting? the syntactic homogeneity of the
hypothesis.
Figure 5 and Tables 4 and 5 show rule distribu-
tions, total rule counts, and numbers of interactions
of different types for the three interaction schemes
on the GALE-web test set. The scope of phrase rules
is limited to 6 words. The scope of hiero rules is lim-
ited to 20 words by the commonly used maxParse
parameter, leaving longer spans to the glue rule.
The trends of F2S and glue rules show the most
obvious difference. In the generalization, F2S rules
translate spans of up to 50 words. Glue rules pre-
vail on spans longer then 7 words. The specification
is reversed, pushing the longest scope of hiero and
glue rules down to 40 words, completing the longest
sentences entirely with F2S. The interchange comes
out as a mixture of the previous two trends.
All three schemes prefer using F2S rules at
shorter spans, to the contrary of our original assump-
tion of phrasal and hiero rules being stronger on lo-
cal contexts in general. Here we may refer again
to the specification feature weights for preterminals
VV, NR, CC and P in Table 3 and to our previously
stated hypothesis about the importance of preserving
lexical and syntactic context.
Hiero rules usage on longer spans drops fastest
for specification, slowest for generalization, and in
between for interchange.
It is also interesting to notice the trends on very
short spans (2?4 words) shown by rule distributions
and reflected in numbers of interaction types. While
specification often transitions from a single phrase
rule directly into F2S, the interchange has relatively
higher counts of hiero rules, another sign of the hiero
and F2S interaction.
Synthesizing from several sources of indications
is difficult, however, we arrive at the conclusion that
joint decoding of hiero and F2S significantly im-
proves the performance. While the single systems
show similar performance, their roles are not bal-
anced in joint decoding. It seems that the role of hi-
ero consists in enabling F2S in most contexts.
We have focused on three special cases of inter-
action. We see a great potential in further studies
of other schemes, allowing more flexible interaction
than simple specification, but still more constrained
than the interchange. It seems also promising to re-
fine the interaction modeling with features taking
into account more information than a single syntac-
tic label, and to explore additional ways of parame-
ter estimation.
7 Conclusion
We have proposed flexible interaction of hyper-
graphs as a novel technique combining hiero
and forest-to-string translation models within one
decoder. We have explored three basic interac-
tion schemes?specification, generalization, and
interchange?and described soft constraints control-
ling the interactions. We have carried out experi-
ments on large training data and with strong base-
lines. Of the three schemes, the specification shows
the highest gains, achieving improvements from 0.5
to 0.9 (Ter-Bleu)/2 points over the best single sys-
tem. We have conducted a detailed analysis of each
system output based on different indications of inter-
actions, discussed possible interpretations of results,
and finally offered our conclusion and proposed fu-
ture lines of research.
Acknowledgments
We thank Jir??? Havelka for proofreading and help-
ful suggestions. We would like to acknowledge the
support of DARPA under Grant HR0011-12-C-0015
for funding part of this work. The views, opinions,
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
References
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of HLT-NAACL, pages 218?226.
554
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
John Denero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In In Proceedings NAACL-HLT, pages 975?983.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney, Aus-
tralia, July.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-HMM-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
pages 98?107, October.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL, pages 127?133.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
ACL, pages 312?319, Prague, Czech Republic, June.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
translation system combination by confusion forest. In
Proceedings of ACL 2011, pages 1249?1257.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim
Tan. 2009. Fast translation rule matching for syntax-
based statistical machine translation. In Proceedings
of EMNLP, pages 1037?1045, Singapore, August.
555
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112?1123,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Violation Perceptron and Forced Decoding for Scalable MT Training
Heng Yu1?
1Institute of Computing Tech.
Chinese Academy of Sciences
yuheng@ict.ac.cn
Liang Huang2? Haitao Mi3
2Queens College & Grad. Center
City University of New York
{huang@cs.qc,kzhao@gc}.cuny.edu
Kai Zhao2
3T.J. Watson Research Center
IBM
hmi@us.ibm.com
Abstract
While large-scale discriminative training has
triumphed in many NLP problems, its defi-
nite success on machine translation has been
largely elusive. Most recent efforts along this
line are not scalable (training on the small
dev set with features from top ?100 most fre-
quent words) and overly complicated. We in-
stead present a very simple yet theoretically
motivated approach by extending the recent
framework of ?violation-fixing perceptron?,
using forced decoding to compute the target
derivations. Extensive phrase-based transla-
tion experiments on both Chinese-to-English
and Spanish-to-English tasks show substantial
gains in BLEU by up to +2.3/+2.0 on dev/test
over MERT, thanks to 20M+ sparse features.
This is the first successful effort of large-scale
online discriminative training for MT.
1 Introduction
Large-scale discriminative training has witnessed
great success in many NLP problems such as pars-
ing (McDonald et al, 2005) and tagging (Collins,
2002), but not yet for machine translation (MT) de-
spite numerous recent efforts. Due to scalability is-
sues, most of these recent methods can only train
on a small dev set of about a thousand sentences
rather than on the full training set, and only with
2,000?10,000 rather ?dense-like? features (either
unlexicalized or only considering highest-frequency
words), as in MIRA (Watanabe et al, 2007; Chiang
et al, 2008; Chiang, 2012), PRO (Hopkins and May,
2011), and RAMP (Gimpel and Smith, 2012). How-
ever, it is well-known that the most important fea-
tures for NLP are lexicalized, most of which can not
?Work done while visiting City University of New York.
?Corresponding author.
be seen on a small dataset. Furthermore, these meth-
ods often involve complicated loss functions and
intricate choices of the ?target? derivations to up-
date towards or against (e.g. k-best/forest oracles, or
hope/fear derivations), and are thus hard to replicate.
As a result, the classical method of MERT (Och,
2003) remains the default training algorithm for MT
even though it can only tune a handful of dense fea-
tures. See also Section 6 for other related work.
As a notable exception, Liang et al (2006) do
train a structured perceptron model on the train-
ing data with sparse features, but fail to outperform
MERT. We argue this is because structured percep-
tron, like many structured learning algorithms such
as CRF and MIRA, assumes exact search, and search
errors inevitably break theoretical properties such as
convergence (Huang et al, 2012). Empirically, it
is now well accepted that standard perceptron per-
forms poorly when search error is severe (Collins
and Roark, 2004; Zhang et al, 2013).
To address the search error problem we propose a
very simple approach based on the recent framework
of ?violation-fixing perceptron? (Huang et al, 2012)
which is designed specifically for inexact search,
with a theoretical convergence guarantee and excel-
lent empirical performance on beam search pars-
ing and tagging. The basic idea is to update when
search error happens, rather than at the end of the
search. To adapt it to MT, we extend this framework
to handle latent variables corresponding to the hid-
den derivations. We update towards ?gold-standard?
derivations computed by forced decoding so that
each derivation leads to the exact reference transla-
tion. Forced decoding is also used as a way of data
selection, since those reachable sentence pairs are
generally more literal and of higher quality, which
the training should focus on. When the reachable
subset is small for some language pairs, we augment
1112
it by including reachable prefix-pairs when the full
sentence pair is not.
We make the following contributions:
1. Our work is the first successful effort to scale
online structured learning to a large portion of
the training data (as opposed to the dev set).
2. Our work is the first to use a principled learning
method customized for inexact search which
updates on partial derivations rather than full
ones in order to fix search errors. We adapt it
to MT using latent variables for derivations.
3. Contrary to the common wisdom, we show that
simply updating towards the exact reference
translation is helpful, which is much simpler
than k-best/forest oracles or loss-augmented
(e.g. hope/fear) derivations, avoiding sentence-
level BLEU scores or other loss functions.
4. We present a convincing analysis that it is the
search errors and standard perceptron?s inabil-
ity to deal with them that prevent previous
work, esp. Liang et al (2006), from succeed-
ing.
5. Scaling to the training data enables us to engi-
neer a very rich feature set of sparse, lexical-
ized, and non-local features, and we propose
various ways to alleviate overfitting.
For simplicity and efficiency reasons, in this paper
we use phrase-based translation, but our method has
the potential to be applicable to other translation
paradigms. Extensive experiments on both Chinese-
to-English and Spanish-to-English tasks show statis-
tically significant gains in BLEU by up to +2.3/+2.0
on dev/test over MERT, and up to +1.5/+1.5 over
PRO, thanks to 20M+ sparse features.
2 Phrase-Based MT and Forced Decoding
We first review the basic phrase-based decoding al-
gorithm (Koehn, 2004), which will be adapted for
forced decoding.
2.1 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English from Mi et al (2008):
0 1 2 3 4 5 6
Figure 1: Standard beam-search phrase-based decoding.
Bu`sh??
Bush
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
-ed
hu?`ta?n
meeting
?Bush held a meeting with Sharon?
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (or states) (Koehn, 2004). Each hy-
pothesis has a coverage vector capturing the source-
language words translated so far, and can be ex-
tended into a longer hypothesis by a phrase-pair
translating an uncovered segment. For example, the
following is one possible derivation:
(0 ) : (0, ??)
(?1 ) : (s1, ?Bush?)
r1
(? ???6) : (s2, ?Bush held talks?)
r2
(???3???) : (s3, ?Bush held talks with Sharon?)
r3
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where each
si is the score of each state, each adding the rule
score and the distortion cost (dc) to the score of the
previous state. To compute the distortion cost we
also need to maintain the ending position of the last
phrase (e.g., the 3 and 6 in the coverage vectors).
In phrase-based translation there is also a distortion-
limit which prohibits long-distance reorderings.
The above states are called?LM states since they
do not involve language model costs. To add a bi-
gram model, we split each ?LM state into a series
of +LM states; each +LM state has the form (v,a)
where a is the last word of the hypothesis. Thus a
+LM version of the above derivation might be:
(0 ,<s>) : (0, ?<s>?)
(?1 ,Bush) : (s?1, ?<s> Bush?)
r1
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
(???3???,Sharon) : (s?3, ?<s> Bush held ... with Sharon?)
r3
1113
0 1 2 3 4 5 6
Bush held
held talks
talks with
with Sharon
Sharon
Figure 2: Forced decoding and y-good derivation lattice.
where the score of applying each rule now also in-
cludes a combination cost due to the bigrams formed
when applying the phrase-pair, e.g.
s?3 = s?2 + s(r3) +dc(|6?3|)? logPlm(with | talk)
To make this exponential-time algorithm practi-
cal, beam search is the standard approximate search
method (Koehn, 2004). Here we group +LM states
into n bins, with each bin Bi hosting at most b states
that cover exactly i Chinese words (see Figure 1).
2.2 Forced Decoding
The idea of forced decoding is to consider only those
(partial) derivations that can produce (a prefix of)
the exact reference translation (assuming single ref-
erence). We call these partial derivations ?y-good?
derivations (Daume?, III and Marcu, 2005), and those
that deviate from the reference translation ?y-bad?
derivations. The forced decoding algorithm is very
similar to +LM decoding introduced above, with the
new ?forced decoding LM? to be defined as only
accepting two consecutive words on the reference
translation, ruling out any y-bad hypothesis:
Pforced (b | a) =
{
1 if ?j, s.t. a = yj and b = yj+1
0 otherwise
In the +LM state, we can simply replace the
boundary word by the index on the reference trans-
lation:
(0 ,0) : (0, ?<s>?)
(?1 ,1) : (w?1, ?<s> Bush?)
r1
(? ???6,3) : (w?2, ?<s> Bush held talks?)
r2
(???3???,5) : (w?3, ?<s> Bush held talks with Sharon?)
r3
The complexity of this forced decoding algorithm
is reduced to O(2nn3) where n is the source sen-
tence length, without the expensive bookkeeping for
English boundary words.
Lia?
nhe?
guo?
pa`i
qia?
n
50 gua?
nch
a?iy
ua?n
jia?n
du?
Bo?l
?`we?
iya`
hu??
fu`
m??n
zhu?
zhe`
ngz
h?`
y??la?
i
sho?
uc?`
qua?
ngu?
o
da`x
ua?n
P            
U.N.
 P           
sent
  P          
50
   P         
observers
             
to
    P        
monitor
          P  
the
          P  
1st
           PP
election
         P   
since
     P       
Bolivia
      P      
restored
       PP    
democracy
5
33
4
1
Figure 3: Example of unreachable sentence pair and
reachable prefix-pair. The first big jump is disallowed for
a distortion limit of 4, but we can still extract the top-left
box as a reachable prefix-pair. Note that this example is
perfectly reachable in syntax-based MT.
2.3 Reachable Prefix-Pairs
In practice, many sentence pairs in the parallel text
fail in forced decoding due to two reasons:
1. distortion limit: long-distance reorderings are
disallowed but are very common between lan-
guages with very different word orders such as
English and Chinese.
2. noisy alignment and phrase limit: the word-
alignment quality (typically from GIZA++) are
usually very noisy, which leads to unnecessar-
ily big chunks of rules beyond the phrase limit.
If we only rely on the reachable whole sentence
pairs, we will not be able to use much of the training
set for Chinese-English. So we propose to augment
the set of reachable examples by considering reach-
able prefix-pairs (see Figure 3 for an example).
3 Violation-Fixing Perceptron for MT
Huang et al (2012) establish a theoretical frame-
work called ?violation-fixing perceptron? which is
tailored for structured learning with inexact search
and has provable convergence properties. The high-
level idea is that standard full update does not fix
search errors; to do that we should instead up-
date when search error occurs, e.g., when the gold-
1114
standard derivation falls below the beam. Huang et
al. (2012) show dramatic improvements in the qual-
ity of the learned model using violation-fixing per-
ceptron (compared to standard perceptron) on incre-
mental parsing and part-of-speech tagging.
Since phrase-based decoding is also an incremen-
tal search problem which closely resembles beam-
search incremental parsing, it is very natural to em-
ploy violation-fixing perceptron here for MT train-
ing. Our goal is to produce the exact reference trans-
lation, or in other words, we want at least one y-good
derivation to survive in the beam search.
To adapt the violation-fixing perceptron frame-
work to MT we need to extend the framework
to handle latent variables since the gold-standard
derivation is not observed. This is done in a way
similar to the latent variable structured perceptron
(Zettlemoyer and Collins, 2005; Liang et al, 2006;
Sun et al, 2009) where each update is from the best
(y-bad) derivation towards the best y-good deriva-
tion in the current model; the latter is a constrained
search which is exactly forced decoding in MT.
3.1 Notations
We first establish some necessary notations. Let
?x, y? be a sentence pair in the training data, and
d = r1 ? r2 ? . . . ? r|d|
be a (partial) derivation, where each ri =
?c(ri), e(ri)? is a rule, i.e., a Chinese-English
phrase-pair. Let |c(d)| ?= ?i |c(ri)| be the num-
ber of Chinese words covered by this derivation, and
e(d) ?= e(r1) ? e(r2) . . . ? e(r|d|) be the English pre-
fix generated so far. Let D(x) be the set of all pos-
sible partial derivations translating part of the input
sentence x. Let pre(y) ?= {y[0:j] | 0 ? j ? |y|}
be the set of prefixes of the reference translation y,
and good i(x, y) be the set of partial y-good deriva-
tions whose English side is a prefix of the reference
translation y, and whose Chinese projection covers
exactly i words on the input sentence x, i.e.,
good i(x, y)
?
= {d ? D(x) | e(d)?pre(y), |c(d)|= i}.
Conversely, we define the set of y-bad partial deriva-
tions covering i Chinese words to be:
bad i(x, y) ?= {d ? D(x) | e(d) /?pre(y), |c(d)|= i}.
Basically, at each bin Bi, y-good derivations
good i(x, y) and y-bad ones bad i(x, y) compete for
the b slots in the bin:
B0 = {} (1)
Bi = topb
?
j=1..l
{d ? r | d ? Bi?j , |c(r)| = j} (2)
where r is a rule covering j Chinese words, l is
the phrase-limit, and topb S is a shorthand for
argtopbd?S w ? ?(x, d) which selects the top b
derivations according to the current model w.
3.2 Algorithm 1: Early Update
As a special case of violation-fixing perceptron,
early update (Collins and Roark, 2004) stops decod-
ing whenever the gold derivation falls off the beam,
makes an update on the prefix so far and move on
to the next example. We adapt it to MT as fol-
lows: if at a certain bin Bi, all y-good derivations
in good i(x, y) have fallen off the bin, then we stop
and update, rewarding the best y-good derivation in
good i(x, y) (with respect to current model w), and
penalizing the best y-bad derivation in the same step:
d+i (x, y)
?
= argmax
d?goodi(x,y)
w ??(x, d) (3)
d?i (x, y)
?
= argmax
d?badi(x,y)?Bi
w ??(x, d) (4)
w? w + ??(x, d+i (x, y), d?i (x, y)) (5)
where ??(x, d, d?) ?= ?(x, d)??(x, d?) is a short-
hand notation for the difference of feature vectors.
Note that the set good i(x, y) is independent of the
beam search and current model and is instead pre-
computed in the forced decoding phase, whereas the
negative signal d?i (x, y) depends on the beam.
In practice, however, there are exponentially
many y-good derivations for each reachable sen-
tence pair, and our goal is just to make sure (at least)
one y-good derivation triumphs at the end. So it
is possible that at a certain bin, all y-good partial
derivations fall off the bin, but the search can still
continue and produce the exact reference translation
through some other y-good path that avoids that bin.
For example, in Figure 1, the y-good states in steps
3 and 5 are not critical; it is totally fine to miss them
in the search as long as we save the y-good states
1115
ea
r
l
y
m
a
x
-
v
i
o
l
a
t
i
o
n
best in the beam
worst in the beam
d i
d+i d+i?
d i?
d+|x|
dy|x|
s
t
d
l
o
c
a
l
standard update 
is invalid
m
o
d
e
l
w
d |x|
Figure 4: Illustration of four update methods. The blue
paths denote (possibly lots of) gold-standard derivations
from forced decoding. Standard update in this case is
invalid as it reinforces the error of w (Huang et al, 2012).
in bins 1, 4 and 6. So we actually use a ?softer?
version of the early update algorithm: only stop and
update when there is no hope to continue. To be
more concrete, let l denote the phrase-limit then we
stop where there are l consecutive bins without any
y-good states, and update on the first among them.
3.3 Algorithm 2: Max-Violation Update
While early update learns substantially better mod-
els than standard perceptron in the midst of inex-
act search, it is also well-known to be converging
much slower than the latter, since each update is
on a (short) prefix. Huang et al (2012) propose an
improved method ?max-violation? which updates at
the worst mistake instead of the first, and converges
much faster than early update with similar or better
accuracy. We adopt this idea here as follows: decode
the whole sentence, and find the step i? where the
difference between the best y-good derivation and
the best y-bad one is the biggest. This amount of dif-
ference is called the amount of ?violation? in Huang
et al (2012), and the place of maximum violation is
intuitively the site of the biggest mistake during the
search. More formally, the update rule is:
i? ?= argmin
i
w ???(x, d+i (x, y), d?i (x, y)) (6)
w? w + ??(x, d+i?(x, y), d?i?(x, y)) (7)
3.4 Previous Work: Standard and Local Updates
We compare the above new update methods with the
two existing ones from Liang et al (2006).
Standard update (also known as ?bold update?
in Liang et al (2006)) simply updates at the very
end, from the best derivation in the beam towards the
best gold-standard derivation (regardless of whether
it survives the beam search):
w? w + ??(x, d+|x|(x, y), d
?
|x|(x, y)) (8)
Local update, however, updates towards the
derivation in the final bin that is most similar to the
reference y, denoted dy|x|(x, y):
dy|x|(x, y) = argmax
d?B|x|
Bleu+1(y, e(d)) (9)
w? w + ??(x, dy|x|(x, y), d
?
|x|(x, y))
(10)
where Bleu+1(?, ?) returns the sentence-level BLEU.
Liang et al (2006) observe that standard update
performs worse than local update, which they at-
tribute to the fact that the former often update to-
wards a gold derivation made up of ?unreasonable?
rules. Here we give a very different but theoreti-
cally more reasonable explanation based on the the-
ory of Huang et al (2012), who define an update
??(x, d+, d?) to be invalid if d+ scores higher
than d? (i.e., w ? ??(x, d+, d?) > 0, or update
?w points to the same direction as w in Fig. 4), in
which case there is no ?violation? or mistake to fix.
Perceptron is guaranteed to converge if all updates
are valid. Clearly, early and max-violation updates
are valid. But standard update is not: it is possible
that at the end of search, the best y-good derivation
d+|x|(x, y), though pruned earlier in the search, rankseven higher in the current model than anything in the
final bin (see Figure 4). In other words, there is no
mistake at the final step, while there must be some
search error in earlier steps which expels the y-good
subderivation. We will see in Section 5.3 that invalid
updates due to search errors are indeed the main rea-
son why standard update fails. Local update, how-
ever, is always valid in that definition.
Finally, it is worth noting that in terms of imple-
mentation, standard and max-violation are the easi-
est, while early update is more involved.
4 Feature Design
Our feature set includes the following 11 dense fea-
tures: LM, four conditional and lexical translation
probabilities (pc(e|f), pc(f |e), pl(e|f), pl(f |e)),
length and phrase penalties, distortion cost, and
three lexicalized reordering features. All these fea-
tures are inherited from Moses (Koehn et al, 2007).
1116
(?1 ,Bush) : (s?1, ?<s> Bush?)
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
</s>ju?x??ng le hu?`ta?nyu? Sha?lo?ngBu`sh??<s>
held talksBush<s>
r1 r2
features for applying r2 on span x[3:6]
WordEdges
c(r2)[0] = ju?x??ng, c(r2)[?1] = hu?`ta?n
e(r2)[0] = held, e(r2)[?1] = talks
x[2:3] = Sha?lo?ng, x[6:7] = </s>, |c(r2)| = 3
... (combos of the above atomic features) ...
non-local e(r0 ? r1)[?2:] ? id(r2)id(r1) ? id(r2)
Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax.
4.1 Local Sparse Features: Ruleid & WordEdges
We first add the rule identification feature for each
rule: id(ri). We also introduce lexicalized Word-
Edges features, which are shown to be very effec-
tive in parsing (Charniak and Johnson, 2005) and
MT (Liu et al, 2008; He et al, 2008) literatures.
We use the following atomic features when apply-
ing a rule ri = ?c(ri), e(ri)?: the source-side length
|c(ri)|, the boundary words of both c(ri) and e(ri),
and the surrounding words of c(ri) on the input sen-
tence x. See Figure 5 for examples. These atomic
features are concatenated to generate all kinds of
combo features.
Chinese English class size budget
word 52.9k 64.2k 5
characters - 3.7k - 3
Brown cluster, full string 200 3
Brown cluster, prefix 6 6 8 2
Brown cluster, prefix 4 4 4 2
POS tag 52 36 2
word type - 4 - 1
Table 1: Various levels of backoff for WordEdges fea-
tures. Class size is estimated on the small Chinese-
English dataset (Sec. 5.3). The POS tagsets are ICT-
CLAS for Chinese (Zhang et al, 2003) and Penn Tree-
bank for English (Marcus et al, 1993).
4.2 Addressing Overfitting
With large numbers of lexicalized combo features
we will face the overfitting problem, where some
combo features found in the training data are too
rare to be seen in the test data. Thus we propose
three ways to alleviate this problem.
First, we introduce various levels of backoffs for
each word w (see Table 1). We include w?s Brown
cluster and its prefixes of lengths 4 and 6 (Brown et
al., 1992), and w?s part-of-speech tag. If w is Chi-
nese we also include its word type (punctuations,
digits, alpha, or otherwise) and (leftmost or right-
most) character. In such a way, we significantly in-
crease the feature coverage on unseen data.
However, if we allow arbitrary combinations, we
can extract a hexalexical feature (4 Chinese + 2 En-
glish words) for a local window in Figure 5, which
is unlikely to be seen at test time. To control model
complexity we introduce a feature budget for each
level of backoffs, shown in the last column in Ta-
ble 1. The total budget for a combo feature is the
sum of the budgets of all atomic features. In our ex-
periments, we only use the combo features with a
total budget of 10 or less, i.e., we can only include
bilexical but not trilexical features, and we can in-
clude for example combo features with one Chinese
word plus two English tags (total budget: 9).
Finally, we use two methods to alleviate overfit-
ting due to one-count rules: for large datasets, we
simply remove all one-count rules, but for small
datasets where out-of-vocabulary words (OOVs)
abound, we use a simple leave-one-out method:
when training on a sentence pair (x, y), do not use
the one-count rules extracted from (x, y) itself.
4.3 Non-Local Features
Following the success of non-local features in pars-
ing (Huang, 2008) and MT (Vaswani et al, 2011),
we also introduce them to capture the contextual in-
formation in MT. Our non-local features, shown in
Figure 5, include bigram rule-ids and the concatena-
tion of a rule id with the translation history, i.e. the
last two English words. Note that we also use back-
offs (Table 1) for the words included. Experiments
(Section 5.3) show that although the set of non-local
features is just a tiny fraction of all features, it con-
tributes substantially to the improvement in BLEU.
1117
Scale Language Training Data Reachability ?BLEU SectionsPair # sent. # words sent. words # feats # refs dev/test
small CH-EN 30K 0.8M/1.0M 21.4% 8.8% 7M 4 +2.2/2.0 5.2, 5.3large 230K 6.9M/8.9M 32.1% 12.7% 23M +2.3/2.0 5.2, 5.4
large SP-EN 174K 4.9M/4.3M 55.0% 43.9% 21M 1 +1.3/1.1 5.5
Table 2: Overview of all experiments. The ?BLEU column shows the absolute improvements of our method MAX-
FORCE on dev/test sets over MERT. The Chinese datasets also use prefix-pairs in training (see Table 3).
5 Experiments
In order to test our approach in different language
pairs, we conduct three experiments, shown in Ta-
ble 2, on two significantly different language pairs
(long vs. short distance reorderings), Chinese-to-
English (CH-EN) and Spanish-to-English (SP-EN).
5.1 System Preparation and Data
We base our experiments on Cubit, a state-of-art
phrase-based system in Python (Huang and Chiang,
2007).1 We set phrase-limit to 7 in rule extraction,
and beam size to 30 and distortion limit 6 in de-
coding. We compare our violation-fixing percep-
tron with two popular tuning methods: MERT (Och,
2003) and PRO (Hopkins and May, 2011).
For word alignments we use GIZA++-`0
(Vaswani et al, 2012) which produces sparser align-
ments, alleviating the garbage collection problem.
We use the SRILM toolkit (Stolcke, 2002) to train a
trigram language model with modified Kneser-Ney
smoothing on 1.5M English sentences.
Our dev and test sets for CH-EN task are from the
newswire portion of 2006 and 2008 NIST MT Eval-
uations (616/691 sentences, 18575/18875 words),
with four references.2 The dev and test sets for SP-
EN task are from newstest2012 and newstest2013,
with only one reference. Below both MERT and PRO
tune weights on the dev set, while our method on the
training set. Specifically, our method only uses the
dev set to know when to stop training.
5.2 Forced Decoding Reachability on Chinese
As mentioned in Section 2.2, we perform forced de-
coding to select reachable sentences from the train-
1http://www.cis.upenn.edu/?lhuang3/cubit/. We
will release the new version at http://acl.cs.qc.edu.
2We use the ?average? reference length to compute the
brevity penalty factor, which does not decrease with more ref-
erences unlike the ?shortest? heuristic.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 10  20  30  40  50  60  70
R
at
io
 o
f c
om
pl
et
e 
co
ve
ra
ge
Sentence length
dist-unlimited
dist-6
dist-4
dist-2
dist-0
Figure 6: Reachability ratio vs. sentence length on the
small CH-EN training set.
small large
sent. words sent. words
full 21.4% 8.8% 32.1% 12.7%
+prefix 61.3% 24.6% 67.3% 32.8%
Table 3: Ratio of sentence reachability and word cover-
age on the two CH-EN training data (distortion limit: 6).
ing data; this part is done with exact search with-
out any beam pruning. Figure 6 shows the reacha-
bility ratio vs. sentence length on the small CH-EN
training data, where the ratio decreases sharply with
sentence length, and increases with distortion limit.
We can see that there are a lot of long distance re-
orderings beyond small distortion limits. In the ex-
treme case of unlimited distortion, a large amount of
sentences will be reachable, but at the cost of much
slower decoding (O(n2V 2) in beam search decod-
ing, andO(2nn3) in forced decoding). In fact forced
decoding is too slow in the unlimited mode that we
only plot reachability for sentences up to 30 words.
Table 3 shows the statistics of forced decoding on
both small and large CH-EN training sets. In the
1118
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
 5  10  15  20  25  30  35  40  45  50
A
ve
ra
ge
 n
um
be
r 
of
 d
er
iv
at
io
ns
Sentence length
dist-6
dist-4
dist-2
dist-0
Figure 7: Average number of derivations in gold lattices.
small data-set, 21.4% sentences are fully reachable
which only contains 8.8% words (since shorter sen-
tences are more likely to be reachable). Larger data
improves reachable ratios significantly thanks to bet-
ter alignment quality, but still only 12.7% words can
be used. In order to add more examples for per-
ceptron training, we pick all non-trivial reachable
prefix-pairs (with 5 or more Chinese words) as addi-
tional training examples (see Section 2.2). As shown
in Table 3, with prefix-pairs we can use about 1/4 of
small data and 1/3 of large data for training, which is
10x and 120x bigger than the 616-sentence dev set.
After running forced decoding, we obtain gold
translation lattice for each reachable sentence (or
prefix) pair. Figure 7 shows, as expected, the av-
erage number of gold derivations in these lattices
grows exponentially with sentence length.
5.3 Analysis on Small Chinese-English Data
Figure 8 shows the BLEU scores of different learn-
ing algorithms on the dev set. MAXFORCE3 per-
forms the best, peaking at iteration 13 while early
update learns much slower (the first few iterations
are faster than other methods due to early stopping
but this difference is immaterial later). The local and
standard updates, however, underperform MERT; in
particular, the latter gets worse as training goes on.
As analysized in Section 3.4, the reason why stan-
dard update (or ?bold update? in Liang et al (2006))
fails is that inexact search leads to many invalid up-
dates. This is confirmed by Figure 9, where more
3Stands for Max-Violation Perceptron w/ Forced Decoding
17
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16  18  20
B
LE
U
Number of iteration
Max
Forc
e
MERT
early
local
standard
Figure 8: BLEU scores on the heldout dev set for different
update methods (trained on small CH-EN data).
50%
60%
70%
80%
90%
 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
R
at
io
beam size
+non-local features
standard perceptron
Figure 9: Ratio of invalid updates in standard update.
than half of the updates remain invalid even at a
beam of 30. These analyses provide an alternative
but theoretically more reasonable explanation to the
findings of Liang et al (2006): while they blame
?unreasonable? gold derivations for the failure of
standard update, we observe that it is the search er-
rors that make the real difference, and that an up-
date that respects search errors towards a gold sub-
derivation is indeed helpful, even if that subderiva-
tion might be ?unreasonable?.
In order to speedup training, we use mini-batch
parallelization of Zhao and Huang (2013) which has
been shown to be much faster than previous paral-
lelization methods. We set the mini-batch size to
24 and train MAXFORCE with 1, 6, and 24 cores
on a small subset of the our original reachable sen-
1119
 22
 23
 24
 0  0.5  1  1.5  2  2.5  3  3.5  4
B
LE
U
Time
MERT PRO-dense
minibatch(24-core)
minibatch(6-core)
minibatch(1 core)
single processor
Figure 10: Minibatch parallelization speeds up learning.
10
12
14
16
18
20
22
24
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MaxForce
MERT
PRO-dense
PRO-medium
PRO-large
Figure 11: Comparison between different training meth-
ods. Ours trains the training set while others on dev set.
tences. The number of sentence pairs in this subset
is 1,032, which contains similar number of words to
our 616-sentence dev set (since reachable sentences
are much shorter). Thus, it is reasonable to compare
different learning algorithms in terms of speed and
performance. Figure 10 shows that first of all, mini-
batch improves BLEU even in the serial setting, and
when run on 24 cores, it leads to a speedup of about
7x. It is also interesting to know that on 1 CPU,
minibatch perceptron takes similar amount of time
to reach the same performance as MERT and PRO.
Figure 11 compares the learning curves of MAX-
FORCE, MERT, and PRO. We test PRO in three
different ways: PRO-dense (dense features only),
PRO-medium (dense features plus top 3K most fre-
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MERT
+non-local
+word-edges
+ruleid
dense
Figure 12: Incremental contributions of different feature
sets (dense features, ruleid, WordEdges, and non-local).
type count % BLEU
dense 11 - 22.3
+ruleid +9,264 +0.1% +0.8
+WordEdges +7,046,238 +99.5% +2.0
+non-local +22,536 +0.3% +0.7
all 7,074,049 100% 25.8
Table 4: Feature counts and incremental BLEU improve-
ments. MAXFORCE with all features is +2.2 over MERT.
quent sparse features4), and PRO-large (dense fea-
tures plus all sparse features). The results show that
PRO-dense performs almost the same as MERT but
with a stabler learning curve while PRO-medium im-
proves by +0.6. However, PRO-large decreases the
performance significantly, which indicates PRO is
not scalable to truly sparse features. By contrast,
our method handles large-scale sparse features well
and outperforms all other methods by a large margin
and with a stable learning curve.
We also investigate the individual contribution
from each group of features (ruleid, WordEdges, and
non-local features). So we perform experiments by
adding each group incrementally. Figure 12 shows
the learning curves and Table 4 lists the counts and
incremental contributions of different feature sets.
With dense features alone MAXFORCE does not do
4To prevent overfitting we remove all lexicalized features
and only use Brown clusters. It is difficult to engineer the right
feature set for PRO, whereas MAXFORCE is much more robust.
1120
system algorithm # feat. dev test
Moses MERT 11 25.5 22.5
Cubit
MERT 11 25.4 22.5
PRO
11 25.6 22.6
3K 26.3 23.0
36K 17.7 14.3
MAXFORCE 23M 27.8 24.5
Table 5: BLEU scores (with four references) using the
large CH-EN data. Our approach is +2.3/2.0 over MERT.
well because perceptron is known to suffer from fea-
tures of vastly different scales. Adding ruleid helps,
but still not enough. WordEdges (which is the vast
majority of features) improves BLEU by +2.0 points
and outperforms MERT, when sparse features totally
dominate dense features. Finally, the 0.3% non-local
features contribute a final +0.7 in BLEU.
5.4 Results on Large Chinese-English Data
Table 5 shows all BLEU scores for different learn-
ing algorithms on the large CH-EN data. The MERT
baseline on Cubit is essentially the same as Moses.
Our MAXFORCE activates 23M features on reach-
able sentences and prefixes in the training data, and
takes 35 hours to finish 15 iterations on 24 cores,
peaking at iteration 13. It achieves significant im-
provements over other approaches: +2.3/+2.0 points
over MERT and +1.5/+1.5 over PRO-medium on de-
v/test sets, respectively.
5.5 Results on Large Spanish-English Data
In SP-EN translation, we first run forced decod-
ing on the training set, and achieve a very high
reachability of 55% (with the same distortion limit
of 6), which is expected since the word order be-
tween Spanish and English are more similar than
than between Chinese and English, and most SP-
EN reorderings are local. Table 6 shows that MAX-
FORCE improves the translation quality over MERT
by +1.3/+1.1 BLEU on dev/test. These gains are
comparable to the improvements on the CH-EN task,
since it is well accepted in MT literature that a
change of ? in 1-reference BLEU is roughly equiva-
lent to a change of 2? with 4 references.
system algorithm # feat. dev test
Moses MERT 11 27.4 24.4
Cubit MAXFORCE 21M 28.7 25.5
Table 6: BLEU scores (with one reference) on SP-EN.
6 Related Work
Besides those discussed in Section 1, there are also
some research on tuning sparse features on the train-
ing data, but they integrate those sparse features into
the MT log-linear model as a single feature weight,
and tune its weight on the dev set (e.g. (Liu et al,
2008; He et al, 2008; Wuebker et al, 2010; Simi-
aner et al, 2012; Flanigan et al, 2013; Setiawan
and Zhou, 2013; He and Deng, 2012; Gao and He,
2013)). By contrast, our approach learns sparse fea-
tures only on the training set, and use dev set as held-
out to know when to stop.
Forced decoding has been used in the MT litera-
ture. For example, open source MT systems Moses
and cdec have implemented it. Liang et al (2012)
also use the it to boost the MERT tuning by adding
more y-good derivations to the standard k-best list.
7 Conclusions and Future Work
We have presented a simple yet effective approach
of structured learning for machine translation which
scales, for the first time, to a large portion of the
whole training data, and enables us to tune a rich set
of sparse, lexical, and non-local features. Our ap-
proach results in very significant BLEU gains over
MERT and PRO baselines. For future work, we will
consider other translation paradigms such as hierar-
chical phrase-based or syntax-based MT.
Acknowledgement
We thank the three anonymous reviewers for helpful sug-
gestions. We are also grateful to David Chiang, Dan
Gildea, Yoav Goldberg, Yifan He, Abe Ittycheriah, and
Hao Zhang for discussions, and Chris Callison-Burch,
Philipp Koehn, Lemao Liu, and Taro Watanabe for help
with datasets. Huang, Yu, and Zhao are supported by
DARPA FA8750-13-2-0041 (DEFT), a Google Faculty
Research Award, and a PSC-CUNY Award, and Mi by
DARPA HR0011-12-C-0015. Yu is also supported by the
China 863 State Key Project (No. 2011AA01A207). The
views and findings in this paper are those of the authors
and are not endorsed by the US or Chinese governments.
1121
References
Peter Brown, Peter Desouza, Robert Mercer, Vincent
Pietra, and Jenifer Lai. 1992. Class-based n-gram
models of natural language. Computational linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of NAACL 2013.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient ascent.
In Proceedings of NAACL:HLT, pages 450?459, At-
lanta, Georgia, June.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proceedings of ACL.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of COLING, pages
321?328, Manchester, UK, August.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of ACL:
Demonstrations.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
COLING-ACL, Sydney, Australia, July.
Huashen Liang, Min Zhang, and Tiejun Zhao. 2012.
Forced decoding for minimum error rate training in
statistical machine translation. Journal of Computa-
tional Information Systems, (8):861868.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of EMNLP, pages 89?97, Honolulu,
Hawaii, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Hendra Setiawan and Bowen Zhou. 2013. Discrimi-
native training of 150 million translation parameters
and its application to pruning. In Proceedings of
NAACL:HLT, pages 335?341, Atlanta, Georgia, June.
ACL.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proceedings of ACL, Jeju Island, Korea.
1122
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Xu Sun, Takuya Matsuzaki, and Daisuke Okanohara.
2009. Latent variable perceptron algorithm for struc-
tured classification. In Proceedings of IJCAI.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of ACL 2011, Port-
land, OR.
Ashish Vaswani, Liang Huang, and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of ACL.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of EMNLP-
CoNLL.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of ACL, pages 475?484, Uppsala,
Sweden, July.
Luke Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In Pro-
ceedings of UAI.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. In Proceedings of the second SIGHAN workshop
on Chinese language processing, pages 184?187.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-
ald. 2013. Online learning with inexact hypergraph
search. In Proceedings of EMNLP 2013.
Kai Zhao and Liang Huang. 2013. Minibatch and paral-
lelization for online large margin structured learning.
In Proceedings of NAACL 2013.
1123
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433?1442,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Constituency to Dependency Translation with Forests
Haitao Mi and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{htmi,liuqun}@ict.ac.cn
Abstract
Tree-to-string systems (and their forest-
based extensions) have gained steady pop-
ularity thanks to their simplicity and effi-
ciency, but there is a major limitation: they
are unable to guarantee the grammatical-
ity of the output, which is explicitly mod-
eled in string-to-tree systems via target-
side syntax. We thus propose to com-
bine the advantages of both, and present
a novel constituency-to-dependency trans-
lation model, which uses constituency
forests on the source side to direct the
translation, and dependency trees on the
target side (as a language model) to en-
sure grammaticality. Medium-scale exper-
iments show an absolute and statistically
significant improvement of +0.7 BLEU
points over a state-of-the-art forest-based
tree-to-string system even with fewer
rules. This is also the first time that a tree-
to-tree model can surpass tree-to-string
counterparts.
1 Introduction
Linguistically syntax-based statistical machine
translation models have made promising progress
in recent years. By incorporating the syntactic an-
notations of parse trees from both or either side(s)
of the bitext, they are believed better than phrase-
based counterparts in reorderings. Depending on
the type of input, these models can be broadly di-
vided into two categories (see Table 1): the string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar, and the tree-based systems whose input
is already a parse tree to be directly converted into
a target tree or string. When we also take into ac-
count the type of output (tree or string), the tree-
based systems can be divided into tree-to-string
and tree-to-tree efforts.
tree on examples (partial) fast gram. BLEU
source Liu06, Huang06 + - +
target Galley06, Shen08 - + +
both Ding05, Liu09 + + -
both our work + + +
Table 1: A classification and comparison of lin-
guistically syntax-based SMT systems, where
gram. denotes grammaticality of the output.
On one hand, tree-to-string systems (Liu et al,
2006; Huang et al, 2006) have gained significant
popularity, especially after incorporating packed
forests (Mi et al, 2008; Mi and Huang, 2008; Liu
et al, 2009; Zhang et al, 2009). Compared with
their string-based counterparts, tree-based systems
are much faster in decoding (linear time vs. cu-
bic time, see (Huang et al, 2006)), do not re-
quire a binary-branching grammar as in string-
based models (Zhang et al, 2006; Huang et al,
2009), and can have separate grammars for pars-
ing and translation (Huang et al, 2006). However,
they have a major limitation that they do not have a
principled mechanism to guarantee grammatical-
ity on the target side, since there is no linguistic
tree structure of the output.
On the other hand, string-to-tree systems ex-
plicitly model the grammaticality of the output
by using target syntactic trees. Both string-to-
constituency system (e.g., (Galley et al, 2006;
Marcu et al, 2006)) and string-to-dependency
model (Shen et al, 2008) have achieved signif-
icant improvements over the state-of-the-art for-
mally syntax-based system Hiero (Chiang, 2007).
However, those systems also have some limita-
tions that they run slowly (in cubic time) (Huang
et al, 2006), and do not utilize the useful syntactic
information on the source side.
We thus combine the advantages of both tree-to-
string and string-to-tree approaches, and propose
1433
a novel constituency-to-dependency model, which
uses constituency forests on the source side to di-
rect translation, and dependency trees on the tar-
get side to guarantee grammaticality of the out-
put. In contrast to conventional tree-to-tree ap-
proaches (Ding and Palmer, 2005; Quirk et al,
2005; Xiong et al, 2007; Zhang et al, 2007;
Liu et al, 2009), which only make use of a sin-
gle type of trees, our model is able to combine
two types of trees, outperforming both phrase-
based and tree-to-string systems. Current tree-to-
tree models (Xiong et al, 2007; Zhang et al, 2007;
Liu et al, 2009) still have not outperformed the
phrase-based system Moses (Koehn et al, 2007)
significantly even with the help of forests.1
Our new constituency-to-dependency model
(Section 2) extracts rules from word-aligned pairs
of source constituency forests and target depen-
dency trees (Section 3), and translates source con-
stituency forests into target dependency trees with
a set of features (Section 4). Medium data exper-
iments (Section 5) show a statistically significant
improvement of +0.7 BLEU points over a state-
of-the-art forest-based tree-to-string system even
with less translation rules, this is also the first time
that a tree-to-tree model can surpass tree-to-string
counterparts.
2 Model
Figure 1 shows a word-aligned source con-
stituency forest Fc and target dependency tree De,
our constituency to dependency translation model
can be formalized as:
P(Fc, De) =
?
Cc?Fc
P(Cc, De)
=
?
Cc?Fc
?
o?O
P(O)
=
?
Cc?Fc
?
o?O
?
r?o
P(r),
(1)
where Cc is a constituency tree in Fc, o is a deriva-
tion that translates Cc to De, O is the set of deriva-
tion, r is a constituency to dependency translation
rule.
1According to the reports of Liu et al (2009), their forest-
based constituency-to-constituency system achieves a com-
parable performance against Moses (Koehn et al, 2007), but
a significant improvement of +3.6 BLEU points over the 1-
best tree-based constituency-to-constituency system.
2.1 Constituency Forests on the Source Side
A constituency forest (in Figure 1 left) is a com-
pact representation of all the derivations (i.e.,
parse trees) for a given sentence under a context-
free grammar (Billot and Lang, 1989).
More formally, following Huang (2008), such
a constituency forest is a pair Fc = Gf =
?V f , Hf ?, where V f is the set of nodes, and Hf
the set of hyperedges. For a given source sen-
tence c1:m = c1 . . . cm, each node vf ? V f is
in the form of X i,j , which denotes the recognition
of nonterminal X spanning the substring from po-
sitions i through j (that is, ci+1 . . . cj). Each hy-
peredge hf ? Hf is a pair ?tails(hf ), head(hf )?,
where head(hf ) ? V f is the consequent node in
the deductive step, and tails(hf ) ? (V f )? is the
list of antecedent nodes. For example, the hyper-
edge hf0 in Figure 1 for deduction (*)
NPB0,1 CC1,2 NPB2,3
NP0,3 , (*)
is notated:
?(NPB0,1, CC1,2, NPB2,3), NP0,3?.
where
head(hf0) = {NP0,3},
and
tails(hf0) = {NPB0,1,CC1,2,NPB2,3}.
The solid line in Figure 1 shows the best parse
tree, while the dashed one shows the second best
tree. Note that common sub-derivations like those
for the verb VPB3,5 are shared, which allows the
forest to represent exponentially many parses in a
compact structure.
We also denote IN (vf ) to be the set of in-
coming hyperedges of node vf , which represents
the different ways of deriving vf . Take node IP0,5
in Figure 1 for example, IN (IP0,5) = {hf1 , h
f
2}.
There is also a distinguished root node TOP in
each forest, denoting the goal item in parsing,
which is simply S0,m where S is the start symbol
and m is the sentence length.
2.2 Dependency Trees on the Target Side
A dependency tree for a sentence represents each
word and its syntactic dependents through directed
arcs, as shown in the following examples. The
main advantage of a dependency tree is that it can
explore the long distance dependency.
1434
1: talk
blank a blan blan
2: held
Bush bla blk talk
a bl
with
b Sharon
We use the lexicon dependency grammar (Hell-
wig, 2006) to express a projective dependency
tree. Take the dependency trees above for exam-
ple, they will be expressed:
1: ( a ) talk
2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )
where the lexicons in brackets represent the de-
pendencies, while the lexicon out the brackets is
the head.
More formally, a dependency tree is also a pair
De = Gd = ?V d, Hd?. For a given target sen-
tence e1:n = e1 . . . en, each node vd ? V d is
a word ei (1 6 i 6 n), each hyperedge hd ?
Hd is a directed arc ?vdi , vdj ? from node vdi to
its head node vdj . Following the formalization of
the constituency forest scenario, we denote a pair
?tails(hd), head(hd)? to be a hyperedge hd, where
head(hd) is the head node, tails(hd) is the node
where hd leaves from.
We also denote Ll(vd) and Lr(vd) to be the left
and right children sequence of node vd from the
nearest to the farthest respectively. Take the node
vd2 = ?held? for example:
Ll(vd2) ={Bush},
Lr(vd2) ={talk, with}.
2.3 Hypergraph
Actually, both the constituency forest and the de-
pendency tree can be formalized as a hypergraph
G, a pair ?V,H?. We use Gf and Gd to distinguish
them. For simplicity, we also use Fc and De to de-
note a constituency forest and a dependency tree
respectively. Specifically, the size of tails(hd) of
a hyperedge hd in a dependency tree is a constant
one.
IP
NP
x1:NPB CC
yu?
x2:NPB
x3:VPB? (x1) x3 (with (x2))
Figure 2: Example of the rule r1. The Chinese con-
junction yu? ?and? is translated into English prepo-
sition ?with?.
3 Rule Extraction
We extract constituency to dependency rules from
word-aligned source constituency forest and target
dependency tree pairs (Figure 1). We mainly ex-
tend the tree-to-string rule extraction algorithm of
Mi and Huang (2008) to our scenario. In this sec-
tion, we first formalize the constituency to string
translation rule (Section 3.1). Then we present
the restrictions for dependency structures as well
formed fragments (Section 3.2). Finally, we de-
scribe our rule extraction algorithm (Section 3.3),
fractional counts computation and probabilities es-
timation (Section 3.4).
3.1 Constituency to Dependency Rule
More formally, a constituency to de-
pendency translation rule r is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the
source side tree fragment, whose internal nodes
are labeled by nonterminal symbols (like NP and
VP), and whose frontier nodes are labeled by
source language words ci (like ?yu??) or variables
from a set X = {x1, x2, . . .}; rhs(r) is expressed
in the target language dependency structure with
words ej (like ?with?) and variables from the set
X ; and ?(r) is a mapping from X to nontermi-
nals. Each variable xi ? X occurs exactly once in
lhs(r) and exactly once in rhs(r). For example,
the rule r1 in Figure 2,
lhs(r1) = IP(NP(x1 CC(yu?) x2) x3),
rhs(r1) = (x1) x3 (with (x2)),
?(r1) = {x1 7? NPB, x2 7? NPB, x3 7? VPB}.
3.2 Well Formed Dependency Fragment
Following Shen et al (2008), we also restrict
rhs(r) to be well formed dependency fragment.
The main difference between us is that we use
more flexible restrictions. Given a dependency
1435
IP0,5
?(Bush) .. Sharon))?
hf1
NP0,3
?(Bush) unionsq (with (Sharon))?
NPB0,1
?Bush?
Bu`sh??
hf0
CC1,2
?with?
yu?
VP1,5
?held .. Sharon))?
PP1,3
?with (Sharon)?
P1,2
?with?
NPB2,3
?Sharon?
Sha?lo?ng
VPB3,5
?held ((a) talk)?
VV3,4
?held ((a)*)?
ju?x??ngle
NPB4,5
?talk?
hu?`ta?n
hf2
Minimal rules extracted
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? (x1) x4 (x2 (x3) )
IP (x1:NPB x2:VP)? (x1) x2
VP (x1:PP x2:VPB)? x2 (x1)
PP (x1:P x2:NPB)? x1 (x2)
VPB (VV(ju?x??ngle)) x1:NPB)
? held ((a) x1)
NPB (Bu`sh??)? Bush
NPB (hu?`ta?n)? talk
CC (yu?)? with
P (yu?)? with
NPB (Sha?lo?ng)? Sharon
( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )
Figure 1: Forest-based constituency to dependency rule extraction.
fragment di:j composed by the words from i to j,
two kinds of well formed structures are defined as
follows:
Fixed on one node vdone, fixed for short, if it
meets the following conditions:
? the head of vdone is out of [i, j], i.e.: ?hd, if
tails(hd) = vdone ? head(hd) /? ei:j .
? the heads of other nodes except vdone are in
[i, j], i.e.: ?k ? [i, j] and vdk 6= vdone,?hd if
tails(hd) = vdk ? head(hd) ? ei:j .
Floating with multi nodes M , floating for
short, if it meets the following conditions:
? all nodes in M have a same head node,
i.e.: ?x /? [i, j],?hd if tails(hd) ? M ?
head(hd) = vhx .
? the heads of other nodes not in M are in
[i, j], i.e.: ?k ? [i, j] and vdk /? M, ?hd if
tails(hd) = vdk ? head(hd) ? ei:j .
Take the ? (Bush) held ((a) talk))(with (Sharon))
? for example: partial fixed examples are ? (Bush)
held ? and ? held ((a) talk)?; while the partial float-
ing examples are ? (talk) (with (Sharon)) ? and ?
((a) talk) (with (Sharon)) ?. Please note that the
floating structure ? (talk) (with (Sharon)) ? can not
be allowed in Shen et al (2008)?s model.
The dependency structure ? held ((a))? is not a
well formed structure, since the head of word ?a?
is out of scope of this structure.
3.3 Rule Extraction Algorithm
The algorithm shown in this Section is mainly ex-
tended from the forest-based tree-to-string extrac-
tion algorithm (Mi and Huang, 2008). We extract
rules from word-aligned source constituency for-
est and target dependency tree pairs (see Figure 1)
in three steps:
(1) frontier set computation,
(2) fragmentation,
(3) composition.
The frontier set (Galley et al, 2004) is the po-
tential points to ?cut? the forest and dependency
tree pair into fragments, each of which will form a
minimal rule (Galley et al, 2006).
However, not every fragment can be used for
rule extraction, since it may or may not respect
to the restrictions, such as word alignments and
well formed dependency structures. So we say a
fragment is extractable if it respects to all re-
strictions. The root node of every extractable tree
fragment corresponds to a faithful structure on
the target side, in which case there is a ?transla-
tional equivalence? between the subtree rooted at
the node and the corresponding target structure.
For example, in Figure 1, every node in the forest
is annotated with its corresponding English struc-
ture. The NP0,3 node maps to a non-contiguous
structure ?(Bush) unionsq (with (Sharon))?, the VV3,4
node maps to a contiguous but non-faithful struc-
ture ?held ((a) *)?.
1436
Algorithm 1 Forest-based constituency to dependency rule extraction.
Input: Source constituency forest Fc, target dependency tree De, and alignment a
Output: Minimal rule setR
1: fs ? FRONTIER(Fc, De, a) . compute frontier set
2: for each vf ? fs do
3: open ? {??, {vf}?} . initial queue of growing fragments
4: while open 6= ? do
5: ?hs, exps? ? open.pop() . extract a fragment
6: if exps = ? then . nothing to expand?
7: generate a rule r using fragment hs . generate a rule
8: R.append(r)
9: else . incomplete: further expand
10: v? ? exps .pop() . a non-frontier node
11: for each hf ? IN (v?) do
12: newexps ? exps ? (tails(hf ) \ fs) . expand
13: open .append(?hs ? {hf},newexps?)
Following Mi and Huang (2008), given a source
target sentence pair ?c1:m, e1:n? with an alignment
a, the span of node vf on source forest is the set
of target words aligned to leaf nodes under vf :
span(vf ) , {ei ? e1:n | ?cj ? yield(vf ), (cj , ei) ? a}.
where the yield(vf ) is all the leaf nodes un-
der vf . For each span(vf ), we also denote
dep(vf ) to be its corresponding dependency struc-
ture, which represents the dependency struc-
ture of all the words in span(vf ). Take the
span(PP1,3) ={with, Sharon} for example, the
corresponding dep(PP1,3) is ?with (Sharon)?. A
dep(vf ) is faithful structure to node vf if it meets
the following restrictions:
? all words in span(vf ) form a continuous sub-
string ei:j ,
? every word in span(vf ) is only aligned to leaf
nodes of vf , i.e.: ?ei ? span(vf ), (cj , ei) ?
a? cj ? yield(vf ),
? dep(vf ) is a well formed dependency struc-
ture.
For example, node VV3,4 has a non-faithful
structure (crossed out in Figure 1), since its
dep(VV3,4 = ? held ((a) *)? is not a well formed
structure, where the head of word ?a? lies in the
outside of its words covered. Nodes with faithful
structure form the frontier set (shaded nodes in
Figure 1) which serve as potential cut points for
rule extraction.
Given the frontier set, fragmentation step is to
?cut? the forest at all frontier nodes and form
tree fragments, each of which forms a rule with
variables matching the frontier descendant nodes.
For example, the forest in Figure 1 is cut into 10
pieces, each of which corresponds to a minimal
rule listed on the right.
Our rule extraction algorithm is formalized in
Algorithm 1. After we compute the frontier set
fs (line 1). We visit each frontier node vf ? fs
on the source constituency forest Fc, and keep a
queue open of growing fragments rooted at vf . We
keep expanding incomplete fragments from open ,
and extract a rule if a complete fragment is found
(line 7). Each fragment hs in open is associated
with a list of expansion sites (exps in line 5) being
the subset of leaf nodes of the current fragment
that are not in the frontier set. So each fragment
along hyperedge h is associated with
exps = tails(hf ) \ fs.
A fragment is complete if its expansion sites is
empty (line 6), otherwise we pop one expansion
node v? to grow and spin-off new fragments by
following hyperedges of v?, adding new expansion
sites (lines 11-13), until all active fragments are
complete and open queue is empty (line 4).
After we get al the minimal rules, we glue them
together to form composed rules following Galley
et al (2006). For example, the composed rule r1
in Figure 2 is glued by the following two minimal
rules:
1437
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) r2? (x1) x4 (x2 (x3) )
CC (yu?)? with r3
where x2:CC in r2 is replaced with r3 accordingly.
3.4 Fractional Counts and Rule Probabilities
Following Mi and Huang (2008), we penalize a
rule r by the posterior probability of the corre-
sponding constituent tree fragment lhs(r), which
can be computed in an Inside-Outside fashion, be-
ing the product of the outside probability of its
root node, the inside probabilities of its leaf nodes,
and the probabilities of hyperedges involved in the
fragment.
??(lhs(r)) =?(root(r))
?
?
hf ? lhs(r)
P(hf )
?
?
vf ? leaves(lhs(r))
?(vf )
(2)
where root(r) is the root of the rule r, ?(v) and
?(v) are the outside and inside probabilities of
node v, and leaves(lhs(r)) returns the leaf nodes
of a tree fragment lhs(r).
We use fractional counts to compute three con-
ditional probabilities for each rule, which will be
used in the next section:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
, (3)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
, (4)
P(r | root(r)) = c(r)?
r?:root(r?)=root(r) c(r?)
. (5)
4 Decoding
Given a source forest Fc, the decoder searches for
the best derivation o? among the set of all possible
derivations O, each of which forms a source side
constituent tree Tc(o), a target side string e(o), and
a target side dependency tree De(o):
o? = arg max
Tc?Fc,o?O
?1 log P(o | Tc)
+?2 log Plm(e(o))
+?3 log PDLMw(De(o))
+?4 log PDLMp(De(o))
+?5 log P(Tc(o))
+?6ill(o) + ?7|o|+ ?8|e(o)|,
(6)
where the first two terms are translation and lan-
guage model probabilities, e(o) is the target string
(English sentence) for derivation o, the third and
forth items are the dependency language model
probabilities on the target side computed with
words and POS tags separately, De(o) is the target
dependency tree of o, the fifth one is the parsing
probability of the source side tree Tc(o) ? Fc, the
ill(o) is the penalty for the number of ill-formed
dependency structures in o, and the last two terms
are derivation and translation length penalties, re-
spectively. The conditional probability P(o | Tc)
is decomposes into the product of rule probabili-
ties:
P(o | Tc) =
?
r?o
P(r), (7)
where each P(r) is the product of five probabili-
ties:
P(r) =P(r | lhs(r))?9 ? P(r | rhs(r))?10
? P(r | root(lhs(r)))?11
? Plex(lhs(r) | rhs(r))?12
? Plex(rhs(r) | lhs(r))?13 ,
(8)
where the first three are conditional probabilities
based on fractional counts of rules defined in Sec-
tion 3.4, and the last two are lexical probabilities.
When computing the lexical translation probabili-
ties described in (Koehn et al, 2003), we only take
into accout the terminals in a rule. If there is no
terminal, we set the lexical probability to 1.
The decoding algorithm works in a bottom-up
search fashion by traversing each node in forest
Fc. We first use pattern-matching algorithm of Mi
et al (2008) to convert Fc into a translation for-
est, each hyperedge of which is associated with a
constituency to dependency translation rule. How-
ever, pattern-matching failure2 at a node vf will
2Pattern-matching failure at a node vf means there is no
translation rule can be matched at vf or no translation hyper-
edge can be constructed at vf .
1438
cut the derivation path and lead to translation fail-
ure. To tackle this problem, we construct a pseudo
translation rule for each parse hyperedge hf ?
IN (vf ) by mapping the CFG rule into a target de-
pendency tree using the head rules of Magerman
(1995). Take the hyperedge hf0 in Figure1 for ex-
ample, the corresponding pseudo translation rule
is:
NP(x1:NPB x2:CC x3:NPB)? (x1) (x2) x3,
since the x3:NPB is the head word of the CFG
rule: NP? NPB CC NPB.
After the translation forest is constructed, we
traverse each node in translation forest also in
bottom-up fashion. For each node, we use the
cube pruning technique (Chiang, 2007; Huang
and Chiang, 2007) to produce partial hypotheses
and compute all the feature scores including the
dependency language model score (Section 4.1).
If all the nodes are visited, we trace back along
the 1-best derivation at goal item S0,m and build
a target side dependency tree. For k-best search
after getting 1-best derivation, we use the lazy Al-
gorithm 3 of Huang and Chiang (2005) that works
backwards from the root node, incrementally com-
puting the second, third, through the kth best alter-
natives.
4.1 Dependency Language Model Computing
We compute the score of a dependency language
model for a dependency tree De in the same way
proposed by Shen et al (2008). For each nonter-
minal node vdh = eh in De and its children se-
quences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj ,
the probability of a trigram is computed as fol-
lows:
P(Ll, Lr | eh?) = P(Ll | eh?) ?P(Lr | eh?), (9)
where the P(Ll | eh?) is decomposed to be:
P(Ll | eh?) =P(ell | eh?)
? P(el2 | el1 , eh?)
...
? P(eln | eln?1 , eln?2).
(10)
We use the suffix ??? to distinguish the head
word and child words in the dependency language
model.
In order to alleviate the problem of data sparse,
we also compute a dependency language model
for POS tages over a dependency tree. We store
the POS tag information on the target side for each
constituency-to-dependency rule. So we will also
generate a POS taged dependency tree simulta-
neously at the decoding time. We calculate this
dependency language model by simply replacing
each ei in equation 9 with its tag t(ei).
5 Experiments
5.1 Data Preparation
Our training corpus consists of 239K sentence
pairs with about 6.9M/8.9M words in Chi-
nese/English, respectively. We first word-align
them by GIZA++ (Och and Ney, 2000) with re-
finement option ?grow-diag-and? (Koehn et al,
2003), and then parse the Chinese sentences using
the parser of Xiong et al (2005) into parse forests,
which are pruned into relatively small forests with
a pruning threshold 3. We also parse the English
sentences using the parser of Charniak (2000) into
1-best constituency trees, which will be converted
into dependency trees using Magerman (1995)?s
head rules. We also store the POS tag informa-
tion for each word in dependency trees, and com-
pute two different dependency language models
for words and POS tags in dependency tree sepa-
rately. Finally, we apply translation rule extraction
algorithm described in Section 3. We use SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
4-gram language model with Kneser-Ney smooth-
ing on the first 1/3 of the Xinhua portion of Giga-
word corpus. At the decoding step, we again parse
the input sentences into forests and prune them
with a threshold 10, which will direct the trans-
lation (Section 4).
We use the 2002 NIST MT Evaluation test set
as our development set and the 2005 NIST MT
Evaluation test set as our test set. We evaluate the
translation quality using the BLEU-4 metric (Pap-
ineni et al, 2002), which is calculated by the script
mteval-v11b.pl with its default setting which is
case-insensitive matching of n-grams. We use the
standard minimum error-rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem?s BLEU score on development set.
5.2 Results
Table 2 shows the results on the test set. Our
baseline system is a state-of-the-art forest-based
constituency-to-string model (Mi et al, 2008), or
forest c2s for short, which translates a source for-
est into a target string by pattern-matching the
1439
constituency-to-string (c2s) rules and the bilin-
gual phrases (s2s). The baseline system extracts
31.9M c2s rules, 77.9M s2s rules respectively and
achieves a BLEU score of 34.17 on the test set3.
At first, we investigate the influence of differ-
ent rule sets on the performance of baseline sys-
tem. We first restrict the target side of transla-
tion rules to be well-formed structures, and we
extract 13.8M constituency-to-dependency (c2d)
rules, which is 43% of c2s rules. We also extract
9.0M string-to-dependency (s2d) rules, which is
only 11.6% of s2s rules. Then we convert c2d and
s2d rules to c2s and s2s rules separately by re-
moving the target-dependency structures and feed
them into the baseline system. As shown in the
third line in the column of BLEU score, the per-
formance drops 1.7 BLEU points over baseline
system due to the poorer rule coverage. However,
when we further use all s2s rules instead of s2d
rules in our next experiment, it achieves a BLEU
score of 34.03, which is very similar to the base-
line system. Those results suggest that restrictions
on c2s rules won?t hurt the performance, but re-
strictions on s2s will hurt the translation quality
badly. So we should utilize all the s2s rules in or-
der to preserve a good coverage of translation rule
set.
The last two lines in Table 2 show the results of
our new forest-based constituency-to-dependency
model (forest c2d for short). When we only use
c2d and s2d rules, our system achieves a BLEU
score of 33.25, which is lower than the baseline
system in the first line. But, with the same rule set,
our model still outperform the result in the sec-
ond line. This suggests that using dependency lan-
guage model really improves the translation qual-
ity by less than 1 BLEU point.
In order to utilize all the s2s rules and increase
the rule coverage, we parse the target strings of
the s2s rules into dependency fragments, and con-
struct the pseudo s2d rules (s2s-dep). Then we
use c2d and s2s-dep rules to direct the translation.
With the help of the dependency language model,
our new model achieves a significant improvement
of +0.7 BLEU points over the forest c2s baseline
system (p < 0.05, using the sign-test suggested by
3According to the reports of Liu et al (2009), with a more
larger training corpus (FBIS plus 30K) but no name entity
translations (+1 BLEU points if it is used), their forest-based
constituency-to-constituency model achieves a BLEU score
of 30.6, which is similar to Moses (Koehn et al, 2007). So our
baseline system is much better than the BLEU score (30.6+1)
of the constituency-to-constituency system and Moses.
System Rule Set BLEUType #
forest c2s
c2s 31.9M 34.17
s2s 77.9M
c2d 13.8M 32.48(?1.7)
s2d 9.0M
c2d 13.8M 34.03(?0.1)
s2s 77.9M
forest c2d
c2d 13.8M 33.25(?0.9)
s2d 9.0M
c2d 13.8M 34.88(?0.7)
s2s-dep 77.9M
Table 2: Statistics of different types of rules ex-
tracted on training corpus and the BLEU scores
on the test set.
Collins et al (2005)). For the first time, a tree-to-
tree model can surpass tree-to-string counterparts
significantly even with fewer rules.
6 Related Work
The concept of packed forest has been used in
machine translation for several years. For exam-
ple, Huang and Chiang (2007) use forest to char-
acterize the search space of decoding with in-
tegrated language models. Mi et al (2008) and
Mi and Huang (2008) use forest to direct trans-
lation and extract rules rather than 1-best tree in
order to weaken the influence of parsing errors,
this is also the first time to use forest directly
in machine translation. Following this direction,
Liu et al (2009) and Zhang et al (2009) apply
forest into tree-to-tree (Zhang et al, 2007) and
tree-sequence-to-string models(Liu et al, 2007)
respectively. Different from Liu et al (2009), we
apply forest into a new constituency tree to de-
pendency tree translation model rather than con-
stituency tree-to-tree model.
Shen et al (2008) present a string-to-
dependency model. They define the well-formed
dependency structures to reduce the size of
translation rule set, and integrate a dependency
language model in decoding step to exploit long
distance word relations. This model shows a
significant improvement over the state-of-the-art
hierarchical phrase-based system (Chiang, 2005).
Compared with this work, we put fewer restric-
tions on the definition of well-formed dependency
structures in order to extract more rules; the
1440
other difference is that we can also extract more
expressive constituency to dependency rules,
since the source side of our rule can encode
multi-level reordering and contain more variables
being larger than two; furthermore, our rules can
be pattern-matched at high level, which is more
reasonable than using glue rules in Shen et al
(2008)?s scenario; finally, the most important one
is that our model runs very faster.
Liu et al (2009) propose a forest-based
constituency-to-constituency model, they put
more emphasize on how to utilize parse forest
to increase the tree-to-tree rule coverage. By
contrast, we only use 1-best dependency trees
on the target side to explore long distance rela-
tions and extract translation rules. Theoretically,
we can extract more rules since dependency
tree has the best inter-lingual phrasal cohesion
properties (Fox, 2002).
7 Conclusion and Future Work
In this paper, we presented a novel forest-based
constituency-to-dependency translation model,
which combines the advantages of both tree-to-
string and string-to-tree systems, runs fast and
guarantees grammaticality of the output. To learn
the constituency-to-dependency translation rules,
we first identify the frontier set for all the
nodes in the constituency forest on the source
side. Then we fragment them and extract mini-
mal rules. Finally, we glue them together to be
composed rules. At the decoding step, we first
parse the input sentence into a constituency for-
est. Then we convert it into a translation for-
est by patter-matching the constituency to string
rules. Finally, we traverse the translation forest
in a bottom-up fashion and translate it into a tar-
get dependency tree by incorporating string-based
and dependency-based language models. Using all
constituency-to-dependency translation rules and
bilingual phrases, our model achieves +0.7 points
improvement in BLEU score significantly over a
state-of-the-art forest-based tree-to-string system.
This is also the first time that a tree-to-tree model
can surpass tree-to-string counterparts.
In the future, we will do more experiments
on rule coverage to compare the constituency-to-
constituency model with our model. Furthermore,
we will replace 1-best dependency trees on the
target side with dependency forests to further in-
crease the rule coverage.
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60736014
and 90920004, and 863 State Key Project No.
2006AA010108. We thank the anonymous review-
ers for their insightful comments. We are also
grateful to Liang Huang for his valuable sugges-
tions.
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
of ACL ?89, pages 143?151.
Eugene Charniak. 2000. A maximum-entropy inspired
parser. In Proceedings of NAACL, pages 132?139.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL, pages
541?548, June.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In In Proceedings of EMNLP-
02.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, July.
Peter Hellwig. 2006. Parsing with Dependency Gram-
mars, volume II. An International Handbook of
Contemporary Research.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
1441
Liang Huang, Hao Zhang, Daniel Gildea, , and Kevin
Knight. 2009. Binarization of synchronous context-
free grammars. Comput. Linguist.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133, Edmon-
ton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, pages 177?180, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proceedings of ACL, pages 704?711, June.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283, June.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP, pages 44?52, July.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206?214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadephia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL, pages
271?279, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, June.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A
dependency treelet string correspondence model for
statistical machine translation. In Proceedings of
SMT, pages 40?47.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. of HLT-NAACL.
Min Zhang, Hongfei Jiang, Aiti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT-Summit.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
1442
Proceedings of the ACL 2010 Conference Short Papers, pages 12?16,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Lexicalized Reordering Models from Reordering Graphs
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn
Abstract
Lexicalized reordering models play a crucial
role in phrase-based translation systems. They
are usually learned from the word-aligned
bilingual corpus by examining the reordering
relations of adjacent phrases. Instead of just
checking whether there is one phrase adjacent
to a given phrase, we argue that it is important
to take the number of adjacent phrases into
account for better estimations of reordering
models. We propose to use a structure named
reordering graph, which represents all phrase
segmentations of a sentence pair, to learn lex-
icalized reordering models efficiently. Exper-
imental results on the NIST Chinese-English
test sets show that our approach significantly
outperforms the baseline method.
1 Introduction
Phrase-based translation systems (Koehn et al,
2003; Och and Ney, 2004) prove to be the state-
of-the-art as they have delivered translation perfor-
mance in recent machine translation evaluations.
While excelling at memorizing local translation and
reordering, phrase-based systems have difficulties in
modeling permutations among phrases. As a result,
it is important to develop effective reordering mod-
els to capture such non-local reordering.
The early phrase-based paradigm (Koehn et al,
2003) applies a simple distance-based distortion
penalty to model the phrase movements. More re-
cently, many researchers have presented lexicalized
reordering models that take advantage of lexical
information to predict reordering (Tillmann, 2004;
Xiong et al, 2006; Zens and Ney, 2006; Koehn et
Figure 1: Occurrence of a swap with different numbers
of adjacent bilingual phrases: only one phrase in (a) and
three phrases in (b). Black squares denote word align-
ments and gray rectangles denote bilingual phrases. [s,t]
indicates the target-side span of bilingual phrase bp and
[u,v] represents the source-side span of bilingual phrase
bp.
al., 2007; Galley and Manning, 2008). These mod-
els are learned from a word-aligned corpus to pre-
dict three orientations of a phrase pair with respect
to the previous bilingual phrase: monotone (M ),
swap (S), and discontinuous (D). Take the bilingual
phrase bp in Figure 1(a) for example. The word-
based reordering model (Koehn et al, 2007) ana-
lyzes the word alignments at positions (s?1, u?1)
and (s ? 1, v + 1). The orientation of bp is set
to D because the position (s ? 1, v + 1) contains
no word alignment. The phrase-based reordering
model (Tillmann, 2004) determines the presence
of the adjacent bilingual phrase located in position
(s? 1, v+1) and then treats the orientation of bp as
S. Given no constraint on maximum phrase length,
the hierarchical phrase reordering model (Galley and
Manning, 2008) also analyzes the adjacent bilingual
phrases for bp and identifies its orientation as S.
However, given a bilingual phrase, the above-
mentioned models just consider the presence of an
adjacent bilingual phrase rather than the number of
adjacent bilingual phrases. See the examples in Fig-
12
Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.
ure 1 for illustration. In Figure 1(a), bp is in a swap
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized
reordering models do not distinguish different num-
bers of adjacent phrase pairs, and just give bp the
same count in the swap orientation.
In this paper, we propose a novel method to better
estimate the reordering probabilities with the con-
sideration of varying numbers of adjacent bilingual
phrases. Our method uses reordering graphs to rep-
resent all phrase segmentations of parallel sentence
pairs, and then gets the fractional counts of bilin-
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results
indicate that our method achieves significant im-
provements over the traditional lexicalized reorder-
ing model (Koehn et al, 2007).
This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional
lexicalized reordering model. Then we introduce
our method to estimate the reordering probabilities
from reordering graphs. The experimental results
are reported in Section 3. Finally, we end with a
conclusion and future work in Section 4.
2 Estimation of Reordering Probabilities
Based on Reordering Graph
In this section, we first describe the traditional lexi-
calized reordering model, and then illustrate how to
construct reordering graphs to estimate the reorder-
ing probabilities.
2.1 Lexicalized Reordering Model
Given a phrase pair bp = (ei, fai), where ai de-
fines that the source phrase fai is aligned to the
target phrase ei, the traditional lexicalized reorder-
ing model computes the reordering count of bp in
the orientation o based on the word alignments of
boundary words. Specifically, the model collects
bilingual phrases and distinguishes their orientations
with respect to the previous bilingual phrase into
three categories:
o =
?
??
??
M ai ? ai?1 = 1
S ai ? ai?1 = ?1
D |ai ? ai?1| 6= 1
(1)
Using the relative-frequency approach, the re-
ordering probability regarding bp is
p(o|bp) = Count(o, bp)?
o? Count(o?, bp)
(2)
2.2 Reordering Graph
For a parallel sentence pair, its reordering graph in-
dicates all possible translation derivations consisting
of the extracted bilingual phrases. To construct a
reordering graph, we first extract bilingual phrases
using the way of (Och, 2003). Then, the adjacent
13
bilingual phrases are linked according to the target-
side order. Some bilingual phrases, which have
no adjacent bilingual phrases because of maximum
length limitation, are linked to the nearest bilingual
phrases in the target-side order.
Shown in Figure 2(b), the reordering graph for
the parallel sentence pair (Figure 2(a)) can be rep-
resented as an undirected graph, where each rect-
angle corresponds to a phrase pair, each link is the
orientation relationship between adjacent bilingual
phrases, and two distinguished rectangles bs and be
indicate the beginning and ending of the parallel sen-
tence pair, respectively. With the reordering graph,
we can obtain all reordering examples containing
the given bilingual phrase. For example, the bilin-
gual phrase ?zhengshi huitan, formal meetings? (see
Figure 2(a)), corresponding to the rectangle labeled
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase
and in a discontinuous order with two subsequent
phrases (see Figure 2(b)).
2.3 Estimation of Reordering Probabilities
We estimate the reordering probabilities from re-
ordering graphs. Given a parallel sentence pair,
there are many translation derivations correspond-
ing to different paths in its reordering graph. As-
suming all derivations have a uniform probability,
the fractional counts of bilingual phrases for orien-
tations can be calculated by utilizing an algorithm in
the inside-outside fashion.
Given a phrase pair bp in the reordering graph,
we denote the number of paths from bs to bp with
?(bp). It can be computed in an iterative way
?(bp) = ?bp? ?(bp?), where bp? is one of the pre-
vious bilingual phrases of bp and ?(bs)=1. In a sim-
ilar way, the number of paths from be to bp, notated
as ?(bp), is simply ?(bp) = ?bp?? ?(bp??), where
bp?? is one of the subsequent bilingual phrases of bp
and ?(be)=1. Here, we show the ? and ? values of
all bilingual phrases of Figure 2 in Table 1. Espe-
cially, for the reordering example consisting of the
bilingual phrases bp1=?jiang juxing, will hold? and
bp2=?zhengshi huitan, formal meetings?, marked in
the gray color in Figure 2, the ? and ? values can be
calculated: ?(bp1) = 1, ?(bp2) = 1+1 = 2, ?(bs) =
8+1 = 9.
Inspired by the parsing literature on pruning
src span trg span ? ?
[0, 0] [0, 0] 1 9
[1, 1] [1, 1] 1 8
[1, 7] [1, 7] 1 1
[4, 4] [2, 2] 1 1
[4, 5] [2, 3] 1 3
[4, 6] [2, 4] 1 1
[4, 7] [2, 5] 1 2
[2, 7] [2, 7] 1 1
[5, 5] [3, 3] 1 1
[6, 6] [4, 4] 2 1
[6, 7] [4, 5] 1 2
[7, 7] [5, 5] 3 1
[2, 2] [6, 6] 5 1
[2, 3] [6, 7] 2 1
[3, 3] [7, 7] 5 1
[8, 8] [8, 8] 9 1
Table 1: The ? and ? values of the bilingual phrases
shown in Figure 2.
(Charniak and Johnson, 2005; Huang, 2008), the
fractional count of (o, bp?, bp) is
Count(o, bp?, bp) = ?(bp
?) ? ?(bp)
?(bs) (3)
where the numerator indicates the number of paths
containing the reordering example (o, bp?, bp) and
the denominator is the total number of paths in the
reordering graph. Continuing with the reordering
example described above, we obtain its fractional
count using the formula (3): Count(M, bp1, bp2) =
(1? 2)/9 = 2/9.
Then, the fractional count of bp in the orientation
o is calculated as described below:
Count(o, bp) =
?
bp?
Count(o, bp?, bp) (4)
For example, we compute the fractional count of
bp2 in the monotone orientation by the formula (4):
Count(M, bp2) = 2/9.
As described in the lexicalized reordering model
(Section 2.1), we apply the formula (2) to calculate
the final reordering probabilities.
3 Experiments
We conduct experiments to investigate the effec-
tiveness of our method on the msd-fe reorder-
ing model and the msd-bidirectional-fe reordering
model. These two models are widely applied in
14
phrase-based system (Koehn et al, 2007). The msd-
fe reordering model has three features, which rep-
resent the probabilities of bilingual phrases in three
orientations: monotone, swap, or discontinuous. If a
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.
3.1 Experiment Setup
Two different sizes of training corpora are used in
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-
gual sentence pairs, the other is a large-scale corpus
that includes 1.55M bilingual sentence pairs from
LDC. The 2002 NIST MT evaluation test data is
used as the development set and the 2003, 2004,
2005 NIST MT test data are the test sets. We
choose the MOSES1 (Koehn et al, 2007) as the ex-
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics ?grow-diag-final-and? are used to
generate a word-aligned corpus, where we extract
bilingual phrases with maximum length 7. We use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
language model on the Xinhua portion of Gigaword
corpus.
In exception to the reordering probabilities, we
use the same features in the comparative experi-
ments. During decoding, we set ttable-limit = 20,
stack = 100, and perform minimum-error-rate train-
ing (Och, 2003) to tune various feature weights. The
translation quality is evaluated by case-insensitive
BLEU-4 metric (Papineni et al, 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to
test the significance in BLEU scores differences.
3.2 Experimental Results
Table 2 shows the results of experiments with the
small training corpus. For the msd-fe model, the
BLEU scores by our method are 30.51 32.78 and
29.50, achieving absolute improvements of 0.89,
0.66 and 0.62 on the three test sets, respectively. For
the msd-bidirectional-fe model, our method obtains
BLEU scores of 30.49 32.73 and 29.24, with abso-
lute improvements of 1.11, 0.73 and 0.60 over the
baseline method.
1The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
the limit of time and space, we only use Moses-style reordering
model (Koehn et al, 2007) as our baseline.
model method MT-03 MT-04 MT-05
baseline 29.62 32.12 28.88m-f RG 30.51?? 32.78?? 29.50?
baseline 29.38 32.00 28.64m-b-f RG 30.49?? 32.73?? 29.24?
Table 2: Experimental results with the small-scale cor-
pus. m-f: msd-fe reordering model. m-b-f: msd-
bidirectional-fe reordering model. RG: probabilities esti-
mation based on Reordering Graph. * or **: significantly
better than baseline (p < 0 .05 or p < 0 .01 ).
model method MT-03 MT-04 MT-05
baseline 31.58 32.39 31.49m-f RG 32.44?? 33.24?? 31.64
baseline 32.43 33.07 31.69m-b-f RG 33.29?? 34.49?? 32.79??
Table 3: Experimental results with the large-scale cor-
pus.
Table 3 shows the results of experiments with
the large training corpus. In the experiments of
the msd-fe model, in exception to the MT-05 test
set, our method is superior to the baseline method.
The BLEU scores by our method are 32.44, 33.24
and 31.64, which obtain 0.86, 0.85 and 0.15 gains
on three test set, respectively. For the msd-
bidirectional-fe model, the BLEU scores produced
by our approach are 33.29, 34.49 and 32.79 on the
three test sets, with 0.86, 1.42 and 1.1 points higher
than the baseline method, respectively.
4 Conclusion and Future Work
In this paper, we propose a method to improve the
reordering model by considering the effect of the
number of adjacent bilingual phrases on the reorder-
ing probabilities estimation. Experimental results on
NIST Chinese-to-English tasks demonstrate the ef-
fectiveness of our method.
Our method is also general to other lexicalized
reordering models. We plan to apply our method
to the complex lexicalized reordering models, for
example, the hierarchical reordering model (Galley
and Manning, 2008) and the MEBTG reordering
model (Xiong et al, 2006). In addition, how to fur-
ther improve the reordering model by distinguishing
the derivations with different probabilities will be-
come another study emphasis in further research.
15
Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China, Contracts 60873167 and
60903138. We thank the anonymous reviewers for
their insightful comments. We are also grateful to
Hongmei Zhao and Shu Cai for their helpful feed-
back.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173?180.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008, pages 848?856.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL 2008,
pages 586?594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, pages 417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages 901?
904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
ACL 2004, Short Papers, pages 101?104.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proc. of ACL 2006, pages
521?528.
Richard Zens and Hermann Ney. 2006. Discriminvative
reordering models for statistical machine translation.
In Proc. of Workshop on Statistical Machine Transla-
tion 2006, pages 521?528.
16
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856?864,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Rule Markov Models for Fast Tree-to-String Translation
Ashish Vaswani
Information Sciences Institute
University of Southern California
avaswani@isi.edu
Haitao Mi
Institute of Computing Technology
Chinese Academy of Sciences
htmi@ict.ac.cn
Liang Huang and David Chiang
Information Sciences Institute
University of Southern California
{lhuang,chiang}@isi.edu
Abstract
Most statistical machine translation systems
rely on composed rules (rules that can be
formed out of smaller rules in the grammar).
Though this practice improves translation by
weakening independence assumptions in the
translation model, it nevertheless results in
huge, redundant grammars, making both train-
ing and decoding inefficient. Here, we take the
opposite approach, where we only use min-
imal rules (those that cannot be formed out
of other rules), and instead rely on a rule
Markov model of the derivation history to
capture dependencies between minimal rules.
Large-scale experiments on a state-of-the-art
tree-to-string translation system show that our
approach leads to a slimmer model, a faster
decoder, yet the same translation quality (mea-
sured using Bleu) as composed rules.
1 Introduction
Statistical machine translation systems typically
model the translation process as a sequence of trans-
lation steps, each of which uses a translation rule,
for example, a phrase pair in phrase-based transla-
tion or a tree-to-string rule in tree-to-string transla-
tion. These rules are usually applied independently
of each other, which violates the conventional wis-
dom that translation should be done in context.
To alleviate this problem, most state-of-the-art sys-
tems rely on composed rules, which are larger rules
that can be formed out of smaller rules (includ-
ing larger phrase pairs that can be formerd out of
smaller phrase pairs), as opposed to minimal rules,
which are rules that cannot be formed out of other
rules. Although this approach does improve trans-
lation quality dramatically by weakening the inde-
pendence assumptions in the translation model, they
suffer from two main problems. First, composition
can cause a combinatorial explosion in the number
of rules. To avoid this, ad-hoc limits are placed dur-
ing composition, like upper bounds on the number
of nodes in the composed rule, or the height of the
rule. Under such limits, the grammar size is man-
ageable, but still much larger than the minimal-rule
grammar. Second, due to large grammars, the de-
coder has to consider many more hypothesis transla-
tions, which slows it down. Nevertheless, the advan-
tages outweigh the disadvantages, and to our knowl-
edge, all top-performing systems, both phrase-based
and syntax-based, use composed rules. For exam-
ple, Galley et al (2004) initially built a syntax-based
system using only minimal rules, and subsequently
reported (Galley et al, 2006) that composing rules
improves Bleu by 3.6 points, while increasing gram-
mar size 60-fold and decoding time 15-fold.
The alternative we propose is to replace composed
rules with a rule Markov model that generates rules
conditioned on their context. In this work, we re-
strict a rule?s context to the vertical chain of ances-
tors of the rule. This ancestral context would play
the same role as the context formerly provided by
rule composition. The dependency treelet model de-
veloped by Quirk and Menezes (2006) takes such
an approach within the framework of dependency
translation. However, their study leaves unanswered
whether a rule Markov model can take the place
of composed rules. In this work, we investigate the
use of rule Markov models in the context of tree-
856
to-string translation (Liu et al, 2006; Huang et al,
2006). We make three new contributions.
First, we carry out a detailed comparison of rule
Markov models with composed rules. Our experi-
ments show that, using trigram rule Markov mod-
els, we achieve an improvement of 2.2 Bleu over
a baseline of minimal rules. When we compare
against vertically composed rules, we find that our
rule Markov model has the same accuracy, but our
model is much smaller and decoding with our model
is 30% faster. When we compare against full com-
posed rules, we find that our rule Markov model still
often reaches the same level of accuracy, again with
savings in space and time.
Second, we investigate methods for pruning rule
Markov models, finding that even very simple prun-
ing criteria actually improve the accuracy of the
model, while of course decreasing its size.
Third, we present a very fast decoder for tree-to-
string grammars with rule Markov models. Huang
and Mi (2010) have recently introduced an efficient
incremental decoding algorithm for tree-to-string
translation, which operates top-down and maintains
a derivation history of translation rules encountered.
This history is exactly the vertical chain of ancestors
corresponding to the contexts in our rule Markov
model, which makes it an ideal decoder for our
model.
We start by describing our rule Markov model
(Section 2) and then how to decode using the rule
Markov model (Section 3).
2 Rule Markov models
Our model which conditions the generation of a rule
on the vertical chain of its ancestors, which allows it
to capture interactions between rules.
Consider the example Chinese-English tree-to-
string grammar in Figure 1 and the example deriva-
tion in Figure 2. Each row is a derivation step; the
tree on the left is the derivation tree (in which each
node is a rule and its children are the rules that sub-
stitute into it) and the tree pair on the right is the
source and target derived tree. For any derivation
node r, let anc1(r) be the parent of r (or  if it has no
parent), anc2(r) be the grandparent of node r (or  if
it has no grandparent), and so on. Let ancn1(r) be the
chain of ancestors anc1(r) ? ? ? ancn(r).
The derivation tree is generated as follows. With
probability P(r1 | ), we generate the rule at the root
node, r1. We then generate rule r2 with probability
P(r2 | r1), and so on, always taking the leftmost open
substitution site on the English derived tree, and gen-
erating a rule ri conditioned on its chain of ancestors
with probability P(ri | ancn1(ri)). We carry on until
no more children can be generated. Thus the proba-
bility of a derivation tree T is
P(T ) =
?
r?T
P(r | ancn1(r)) (1)
For the minimal rule derivation tree in Figure 2, the
probability is:
P(T ) = P(r1 | ) ? P(r2 | r1) ? P(r3 | r1)
? P(r4 | r1, r3) ? P(r6 | r1, r3, r4)
? P(r7 | r1, r3, r4) ? P(r5 | r1, r3) (2)
Training We run the algorithm of Galley et al
(2004) on word-aligned parallel text to obtain a sin-
gle derivation of minimal rules for each sentence
pair. (Unaligned words are handled by attaching
them to the highest node possible in the parse tree.)
The rule Markov model
can then be trained on the path set of these deriva-
tion trees.
Smoothing We use interpolation with absolute
discounting (Ney et al, 1994):
Pabs(r | ancn1(r)) =
max
{
c(r | ancn1(r)) ? Dn, 0
}
?
r? c(r? | ancn1(r?))
+ (1 ? ?n)Pabs(r | ancn?11 (r)), (3)
where c(r | ancn1(r)) is the number of times we have
seen rule r after the vertical context ancn1(r), Dn is
the discount for a context of length n, and (1 ? ?n) is
set to the value that makes the smoothed probability
distribution sum to one.
We experiment with bigram and trigram rule
Markov models. For each, we try different values of
D1 and D2, the discount for bigrams and trigrams,
respectively. Ney et al (1994) suggest using the fol-
lowing value for the discount Dn:
Dn =
n1
n1 + n2
(4)
857
rule id translation rule
r1 IP(x1:NP x2:VP) ? x1 x2
r2 NP(Bu`sh??) ? Bush
r3 VP(x1:PP x2:VP) ? x2 x1
r4 PP(x1:P x2:NP) ? x1 x2
r5 VP(VV(ju?x??ng) AS(le) NPB(hu?`ta?n)) ? held talks
r6 P(yu?) ? with
r?6 P(yu?) ? and
r7 NP(Sha?lo?ng) ? Sharon
Figure 1: Example tree-to-string grammar.
derivation tree derived tree pair
 IP@ : IP@
r1
IP@
NP@1 VP@2 : NP
@1 VP@2
r1
r2 r3
IP@
NP@1
Bu`sh??
VP@2
PP@2.1 VP@2.2
: Bush VP@2.2 PP@2.1
r1
r2 r3
r4 r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1 NP@2.1.2
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks P@2.1.1 NP@2.1.2
r1
r2 r3
r4
r6 r7
r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks with Sharon
Figure 2: Example tree-to-string derivation. Each row shows a rewriting step; at each step, the leftmost nonterminal
symbol is rewritten using one of the rules in Figure 1.
858
Here, n1 and n2 are the total number of n-grams with
exactly one and two counts, respectively. For our
corpus, D1 = 0.871 and D2 = 0.902. Additionally,
we experiment with 0.4 and 0.5 for Dn.
Pruning In addition to full n-gram Markov mod-
els, we experiment with three approaches to build
smaller models to investigate if pruning helps. Our
results will show that smaller models indeed give a
higher Bleu score than the full bigram and trigram
models. The approaches we use are:
? RM-A: We keep only those contexts in which
more than P unique rules were observed. By
optimizing on the development set, we set P =
12.
? RM-B: We keep only those contexts that were
observed more than P times. Note that this is a
superset of RM-A. Again, by optimizing on the
development set, we set P = 12.
? RM-C: We try a more principled approach
for learning variable-length Markov models in-
spired by that of Bejerano and Yona (1999),
who learn a Prediction Suffix Tree (PST). They
grow the PST in an iterative manner by start-
ing from the root node (no context), and then
add contexts to the tree. A context is added if
the KL divergence between its predictive distri-
bution and that of its parent is above a certain
threshold and the probability of observing the
context is above another threshold.
3 Tree-to-string decoding with rule
Markov models
In this paper, we use our rule Markov model frame-
work in the context of tree-to-string translation.
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) have gained popularity in recent
years due to their speed and simplicity. The input to
the translation system is a source parse tree and the
output is the target string. Huang and Mi (2010) have
recently introduced an efficient incremental decod-
ing algorithm for tree-to-string translation. The de-
coder operates top-down and maintains a derivation
history of translation rules encountered. The history
is exactly the vertical chain of ancestors correspond-
ing to the contexts in our rule Markov model. This
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV@2.2.1
ju?x??ng
AS@2.2.2
le
NP@2.2.3
hu?`ta?n
Figure 3: Example input parse tree with tree addresses.
makes incremental decoding a natural fit with our
generative story. In this section, we describe how
to integrate our rule Markov model into this in-
cremental decoding algorithm. Note that it is also
possible to integrate our rule Markov model with
other decoding algorithms, for example, the more
common non-incremental top-down/bottom-up ap-
proach (Huang et al, 2006), but it would involve
a non-trivial change to the decoding algorithms to
keep track of the vertical derivation history, which
would result in significant overhead.
Algorithm Given the input parse tree in Figure 3,
Figure 4 illustrates the search process of the incre-
mental decoder with the grammar of Figure 1. We
write X@? for a tree node with label X at tree address
? (Shieber et al, 1995). The root node has address ,
and the ith child of node ? has address ?.i. At each
step, the decoder maintains a stack of active rules,
which are rules that have not been completed yet,
and the rightmost (n ? 1) English words translated
thus far (the hypothesis), where n is the order of the
word language model (in Figure 4, n = 2). The stack
together with the translated English words comprise
a state of the decoder. The last column in the fig-
ure shows the rule Markov model probabilities with
the conditioning context. In this example, we use a
trigram rule Markov model.
After initialization, the process starts at step 1,
where we predict rule r1 (the shaded rule) with prob-
ability P(r1 | ) and push its English side onto the
stack, with variables replaced by the correspond-
ing tree nodes: x1 becomes NP@1 and x2 becomes
VP@2. This gives us the following stack:
s = [ NP@1 VP@2]
The dot () indicates the next symbol to process in
859
stack hyp. MR prob.
0 [<s>  IP@ </s>] <s>
1 [<s>  IP@ </s>] [ NP@1 VP@2] <s> P(r1 | )
2 [<s>  IP@ </s>] [ NP@1 VP@2] [ Bush] <s> P(r2 | r1)
3 [<s>  IP@ </s>] [ NP@1 VP@2] [Bush  ] . . . Bush
4 [<s>  IP@ </s>] [NP@1  VP@2] . . . Bush
5 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] . . . Bush P(r3 | r1)
6 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks] . . . Bush P(r5 | r1, r3)
7 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held  talks] . . . held
8 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks  ] . . . talks
9 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] . . . talks
10 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] . . . talks P(r4 | r1, r3)
11 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ with] . . . with P(r6 | r3, r4)
12 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [with  ] . . . with
13 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . with
14 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . with P(r7 | r3, r4)
11? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ and] . . . and P(r?6 | r3, r4)
12? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [and  ] . . . and
13? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . and
14? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . and P(r7 | r3, r4)
15 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [Sharon  ] . . . Sharon
16 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1 NP@2.1.2  ] . . . Sharon
17 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2 PP@2.1  ] . . . Sharon
18 [<s>  IP@ </s>] [NP@1 VP@2  ] . . . Sharon
19 [<s> IP@  </s>] . . . Sharon
20 [<s> IP@ </s>  ] . . . </s>
Figure 4: Simulation of incremental decoding with rule Markov model. The solid arrows indicate one path and the
dashed arrows indicate an alternate path.
860
VP@2
VP@2.2 PP@2.1
P@2.1.1
yu?
NP@2.1.2
Figure 5: Vertical context r3 r4 which allows the model
to correctly translate yu? as with.
the English word order. We expand node NP@1 first
with English word order. We then predict lexical rule
r2 with probability P(r2 | r1) and push rule r2 onto
the stack:
[ NP@1 VP@2 ] [ Bush]
In step 3, we perform a scan operation, in which
we append the English word just after the dot to the
current hypothesis and move the dot after the word.
Since the dot is at the end of the top rule in the stack,
we perform a complete operation in step 4 where we
pop the finished rule at the top of the stack. In the
scan and complete steps, we don?t need to compute
rule probabilities.
An interesting branch occurs after step 10 with
two competing lexical rules, r6 and r?6. The Chinese
word yu? can be translated as either a preposition with
(leading to step 11) or a conjunction and (leading
to step 11?). The word n-gram model does not have
enough information to make the correct choice, with.
As a result, good translations might be pruned be-
cause of the beam. However, our rule Markov model
has the correct preference because of the condition-
ing ancestral sequence (r3, r4), shown in Figure 5.
Since VP@2.2 has a preference for yu? translating to
with, our corpus statistics will give a higher proba-
bility to P(r6 | r3, r4) than P(r?6 | r3, r4). This helps
the decoder to score the correct translation higher.
Complexity analysis With the incremental decod-
ing algorithm, adding rule Markov models does not
change the time complexity, which is O(nc|V |g?1),
where n is the sentence length, c is the maximum
number of incoming hyperedges for each node in the
translation forest, V is the target-language vocabu-
lary, and g is the order of the n-gram language model
(Huang and Mi, 2010). However, if one were to use
rule Markov models with a conventional CKY-style
bottom-up decoder (Liu et al, 2006), the complexity
would increase to O(nCm?1|V |4(g?1)), where C is the
maximum number of outgoing hyperedges for each
node in the translation forest, and m is the order of
the rule Markov model.
4 Experiments and results
4.1 Setup
The training corpus consists of 1.5M sentence pairs
with 38M/32M words of Chinese/English, respec-
tively. Our development set is the newswire portion
of the 2006 NIST MT Evaluation test set (616 sen-
tences), and our test set is the newswire portion of
the 2008 NIST MT Evaluation test set (691 sen-
tences).
We word-aligned the training data using GIZA++
followed by link deletion (Fossum et al, 2008),
and then parsed the Chinese sentences using the
Berkeley parser (Petrov and Klein, 2007). To extract
tree-to-string translation rules, we applied the algo-
rithm of Galley et al (2004). We trained our rule
Markov model on derivations of minimal rules as
described above. Our trigram word language model
was trained on the target side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing. The base feature
set for all systems is similar to the set used in Mi et
al. (2008). The features are combined into a standard
log-linear model, which we trained using minimum
error-rate training (Och, 2003) to maximize the Bleu
score on the development set.
At decoding time, we again parse the input
sentences using the Berkeley parser, and convert
them into translation forests using rule pattern-
matching (Mi et al, 2008). We evaluate translation
quality using case-insensitive IBM Bleu-4, calcu-
lated by the script mteval-v13a.pl.
4.2 Results
Table 1 presents the main results of our paper. We
used grammars of minimal rules and composed rules
of maximum height 3 as our baselines. For decod-
ing, we used a beam size of 50. Using the best
bigram rule Markov models and the minimal rule
grammar gives us an improvement of 1.5 Bleu over
the minimal rule baseline. Using the best trigram
rule Markov model brings our gain up to 2.3 Bleu.
861
grammar rule Markov max parameters (?10
6) Bleu time
model rule height full dev+test test (sec/sent)
minimal None 3 4.9 0.3 24.2 1.2
RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8
RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0
vertical composed None 7 176.8 1.3 26.5 2.9
composed None 3 17.5 1.6 26.4 2.2
None 7 448.7 3.3 27.5 6.8
RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2
Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same
level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for
both the full model and the model filtered for the concatenation of the development and test sets (dev+test).
These gains are statistically significant with p <
0.01, using bootstrap resampling with 1000 samples
(Koehn, 2004). We find that by just using bigram
context, we are able to get at least 1 Bleu point
higher than the minimal rule grammar. It is interest-
ing to see that using just bigram rule interactions can
give us a reasonable boost. We get our highest gains
from using trigram context where our best perform-
ing rule Markov model gives us 2.3 Bleu points over
minimal rules. This suggests that using longer con-
texts helps the decoder to find better translations.
We also compared rule Markov models against
composed rules. Since our models are currently lim-
ited to conditioning on vertical context, the closest
comparison is against vertically composed rules. We
find that our approach performs equally well using
much less time and space.
Comparing against full composed rules, we find
that our system matches the score of the base-
line composed rule grammar of maximum height 3,
while using many fewer parameters. (It should be
noted that a parameter in the rule Markov model is
just a floating-point number, whereas a parameter in
the composed-rule system is an entire rule; there-
fore the difference in memory usage would be even
greater.) Decoding with our model is 0.2 seconds
faster per sentence than with composed rules.
These experiments clearly show that rule Markov
models with minimal rules increase translation qual-
ity significantly and with lower memory require-
ments than composed rules. One might wonder if
the best performance can be obtained by combin-
ing composed rules with a rule Markov model. This
rule Markov D1
Bleu time
model dev (sec/sent)
RM-A 0.871 29.2 1.8
RM-B 0.4 29.9 1.8
RM-C 0.871 29.8 1.8
RM-Full 0.4 29.7 1.9
Table 2: For rule bigrams, RM-B with D1 = 0.4 gives the
best results on the development set.
rule Markov D1 D2
Bleu time
model dev (sec/sent)
RM-A 0.5 0.5 30.3 2.0
RM-B 0.5 0.5 29.9 2.0
RM-C 0.5 0.5 30.1 2.0
RM-Full 0.4 0.5 30.1 2.2
Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 gives
the best results on the development set.
is straightforward to implement: the rule Markov
model is still defined over derivations of minimal
rules, but in the decoder?s prediction step, the rule
Markov model?s value on a composed rule is cal-
culated by decomposing it into minimal rules and
computing the product of their probabilities. We find
that using our best trigram rule Markov model with
composed rules gives us a 0.5 Bleu gain on top of
the composed rule grammar, statistically significant
with p < 0.05, achieving our highest score of 28.0.1
4.3 Analysis
Tables 2 and 3 show how the various types of rule
Markov models compare, for bigrams and trigrams,
1For this experiment, a beam size of 100 was used.
862
parameters (?106) Bleu dev/test time (sec/sent)
dev/test without RMM with RMM without/with RMM
2.6 31.0/27.0 31.1/27.4 4.5/7.0
2.9 31.5/27.7 31.4/27.3 5.6/8.1
3.3 31.4/27.5 31.4/28.0 6.8/9.2
Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.
D2
D1
0.4 0.5 0.871
0.4 30.0 30.0
0.5 29.3 30.3
0.902 30.0
Table 4: RM-A is robust to different settings of Dn on the
development set.
parameters (?106) Bleu time
dev+test dev test (sec/sent)
1.2 30.2 26.1 2.8
1.3 30.1 26.5 2.9
1.3 30.1 26.2 3.2
Table 5: Comparison of vertically composed rules using
various settings (maximum rule height 7).
respectively. It is interesting that the full bigram and
trigram rule Markov models do not give our high-
est Bleu scores; pruning the models not only saves
space but improves their performance. We think that
this is probably due to overfitting.
Table 4 shows that the RM-A trigram model does
fairly well under all the settings of Dn we tried. Ta-
ble 5 shows the performance of vertically composed
rules at various settings. Here we have chosen the
setting that gives the best performance on the test
set for inclusion in Table 1.
Table 6 shows the performance of fully composed
rules and fully composed rules with a rule Markov
Model at various settings.2 In the second line (2.9
million rules), the drop in Bleu score resulting from
adding the rule Markov model is not statistically sig-
nificant.
5 Related Work
Besides the Quirk and Menezes (2006) work dis-
cussed in Section 1, there are two other previous
2For these experiments, a beam size of 100 was used.
efforts both using a rule bigram model in machine
translation, that is, the probability of the current rule
only depends on the immediate previous rule in the
vertical context, whereas our rule Markov model
can condition on longer and sparser derivation his-
tories. Among them, Ding and Palmer (2005) also
use a dependency treelet model similar to Quirk and
Menezes (2006), and Liu and Gildea (2008) use a
tree-to-string model more like ours. Neither com-
pared to the scenario with composed rules.
Outside of machine translation, the idea of weak-
ening independence assumptions by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. However, be-
sides the difference between parsing and translation,
there are still two major differences. First, our work
conditions rule probabilities on parent and grandpar-
ent rules, not just nonterminals. Second, we com-
pare against a composed-rule system, which is anal-
ogous to the Data Oriented Parsing (DOP) approach
in parsing (Bod, 2003). To our knowledge, there has
been no direct comparison between a history-based
PCFG approach and DOP approach in the parsing
literature.
6 Conclusion
In this paper, we have investigated whether we can
eliminate composed rules without any loss in trans-
lation quality. We have developed a rule Markov
model that captures vertical bigrams and trigrams of
minimal rules, and tested it in the framework of tree-
to-string translation. We draw three main conclu-
sions from our experiments. First, our rule Markov
models dramatically improve a grammar of minimal
rules, giving an improvement of 2.3 Bleu. Second,
when we compare against vertically composed rules
we are able to get about the same Bleu score, but
our model is much smaller and decoding with our
863
model is faster. Finally, when we compare against
full composed rules, we find that we can reach the
same level of performance under some conditions,
but in order to do so consistently, we believe we
need to extend our model to condition on horizon-
tal context in addition to vertical context. We hope
that by modeling context in both axes, we will be
able to completely replace composed-rule grammars
with smaller minimal-rule grammars.
Acknowledgments
We would like to thank Fernando Pereira, Yoav
Goldberg, Michael Pust, Steve DeNeefe, Daniel
Marcu and Kevin Knight for their comments.
Mi?s contribution was made while he was vis-
iting USC/ISI. This work was supported in part
by DARPA under contracts HR0011-06-C-0022
(subcontract to BBN Technologies), HR0011-09-1-
0028, and DOI-NBC N10AP20031, by a Google
Faculty Research Award to Huang, and by the Na-
tional Natural Science Foundation of China under
contracts 60736014 and 90920004.
References
Gill Bejerano and Golan Yona. 1999. Modeling pro-
tein families using probabilistic suffix trees. In Proc.
RECOMB, pages 15?24. ACM Press.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of EACL, pages 19?26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of ACL, pages 541?
548.
Victoria Fossum, Kevin Knight, and Steve Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-
string transducer for machine translation. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 62?69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of NAACL
HLT, pages 9?16.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
864
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 785?790,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical MT Training using Max-Violation Perceptron
Kai Zhao
?
Liang Huang
?
?
Graduate Center & Queens College
City University of New York
{kzhao@gc,huang@cs.qc}.cuny.edu
Haitao Mi
?
Abe Ittycheriah
?
?
T. J. Watson Research Center
IBM
{hmi,abei}@us.ibm.com
Abstract
Large-scale discriminative training has be-
come promising for statistical machine
translation by leveraging the huge train-
ing corpus; for example the recent effort
in phrase-based MT (Yu et al, 2013) sig-
nificantly outperforms mainstream meth-
ods that only train on small tuning sets.
However, phrase-based MT suffers from
limited reorderings, and thus its training
can only utilize a small portion of the bi-
text due to the distortion limit. To address
this problem, we extend Yu et al (2013)
to syntax-based MT by generalizing their
latent variable ?violation-fixing? percep-
tron from graphs to hypergraphs. Exper-
iments confirm that our method leads to
up to +1.2 BLEU improvement over main-
stream methods such as MERT and PRO.
1 Introduction
Many natural language processing problems in-
cluding part-of-speech tagging (Collins, 2002),
parsing (McDonald et al, 2005), and event extrac-
tion (Li et al, 2013) have enjoyed great success us-
ing large-scale discriminative training algorithms.
However, a similar success on machine translation
has been elusive, where the mainstream methods
still tune on small datasets.
What makes large-scale MT training so hard
then? After numerous attempts by various re-
searchers (Liang et al, 2006; Watanabe et al,
2007; Arun and Koehn, 2007; Blunsom et al,
2008; Chiang et al, 2008; Flanigan et al, 2013;
Green et al, 2013), the recent work of Yu et al
(2013) finally reveals a major reason: it is the vast
amount of (inevitable) search errors in MT decod-
ing that astray learning. To alleviate this prob-
lem, their work adopts the theoretically-motivated
framework of violation-fixing perceptron (Huang
et al, 2012) tailed for inexact search, yielding
great results on phrase-based MT (outperforming
Collins (02)
inexact
??
search
Huang et al (12)
latent
??
variable
Yu et al (13)
? hypergraph ?
Zhang et al (13) ??
variable
this work
Figure 1: Relationship with previous work.
small-scale MERT/PRO by a large margin for the
first time). However, the underlying phrase-based
model suffers from limited distortion and thus can
only employ a small portion (about 1/3 in their Ch-
En experiments) of the bitext in training.
To better utilize the large training set, we
propose to generalize from phrase-based MT to
syntax-based MT, in particular the hierarchical
phrase-based translation model (HIERO) (Chiang,
2005), in order to exploit sentence pairs beyond
the expressive capacity of phrase-based MT.
The key challenge here is to extend the latent
variable violation-fixing perceptron of Yu et al
(2013) to handle tree-structured derivations and
translation hypergraphs. Luckily, Zhang et al
(2013) have recently generalized the underlying
violation-fixing perceptron of Huang et al (2012)
from graphs to hypergraphs for bottom-up parsing,
which resembles syntax-based decoding. We just
need to further extend it to handle latent variables.
We make the following contributions:
1. We generalize the latent variable violation-
fixing perceptron framework to inexact
search over hypergraphs, which subsumes
previous algorithms for PBMT and bottom-
up parsing as special cases (see Fig. 1).
2. We show that syntax-based MT, with its bet-
ter handling of long-distance reordering, can
exploit a larger portion of the training set,
which facilitates sparse lexicalized features.
3. Experiments show that our training algo-
rithm outperforms mainstream tuning meth-
ods (which optimize on small devsets) by
+1.2 BLEU over MERT and PRO on FBIS.
785
id rule
r
0
S? ?X
1
,X
1
?
r
1
S? ?S
1
X
2
,S
1
X
2
?
r
2
X? ?B`ush??,Bush?
r
3
X? ?Sh?al?ong,Sharon?
r
4
X? ?hu`?t?an, talks?
r
5
X? ?y?u X
1
j?ux??ng X
2
,
held X
2
with X
1
?
r
6
X? ?y?u Sh?al?ong, with Sharon?
r
7
X? ?X
1
j?ux??ng X
2
,
X
1
held X
2
?
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[2:3]
Sh?al?ong
3
|
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
Sharon
5
with
4
X
talks
3
held
2
S
X
0
Bush
1
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[1:3]
Sh?al?ong
3
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
talks
5
held
4
X
Sharon
3
with
2
S
X
0
Bush
1
(a) HIERO rules (b) gold derivation (c) Viterbi derivation
Figure 2: An example of HIERO translation.
X[0:1] X[2:3] X[4:5]
X[1:5]
X[1:3]
S[0:1]
S[0:5]
Figure 3: A ?LM hypergraph with two deriva-
tions: the gold derivation (Fig. 2b) in solid lines,
and the Viterbi derivation (Fig. 2c) in dashed lines.
2 Review: Syntax-based MT Decoding
For clarity reasons we will describe HIERO decod-
ing as a two-pass process, first without a language
model, and then integrating the LM. This section
mostly follows Huang and Chiang (2007).
In the first, ?LM phase, the decoder parses the
source sentence using the source projection of the
synchronous grammar (see Fig. 2 (a) for an ex-
ample), producing a?LM hypergraph where each
node has a signature N
[i:j]
, where N is the nonter-
minal type (either X or S in HIERO) and [i : j] is
the span, and each hyperedge e is an application
of the translation rule r(e) (see Figure 3).
To incorporate the language model, each node
also needs to remember its target side boundary
words. Thus a ?LM node N
[i:j]
is split into mul-
tiple +LM nodes of signature N
a?b
[i:j]
, where a and
b are the boundary words. For example, with a bi-
gram LM, X
held?Sharon
[1:5]
is a node whose translation
starts with ?held? and ends with ?Sharon?.
More formally, the whole decoding process can
be cast as a deductive system. Take the partial
translation of ?held talks with Sharon? in Figure 2
(b) for example, the deduction is
X
Sharon?Sharon
[2:3]
: s
1
X
talks?talks
[4:5]
: s
2
X
held?Sharon
[1:5]
: s
1
+ s
2
+ s(r
5
) + ?
r
5
,
where s(r
5
) is the score of rule r
5
, and the LM
combo score ? is log P
lm
(talks | held)P
lm
(with |
talks)P
lm
(Sharon | with).
3 Violation-Fixing Perceptron for HIERO
As mentioned in Section 1, the key to the success
of Yu et al (2013) is the adoption of violation-
fixing perceptron of Huang et al (2012) which
is tailored for vastly inexact search. The general
idea is to update somewhere in the middle of the
search (where search error happens) rather than at
the very end (standard update is often invalid). To
adapt it to MT where many derivations can output
the same translation (i.e., spurious ambiguity), Yu
et al (2013) extends it to handle latent variables
which correspond to phrase-based derivations. On
the other hand, Zhang et al (2013) has generalized
Huang et al (2012) from graphs to hypergraphs
for bottom-up parsing, which resembles HIERO
decoding. So we just need to combine the two
generalizing directions (latent variable and hyper-
graph, see Fig. 1).
3.1 Latent Variable Hypergraph Search
The key difference between bottom-up parsing
and MT decoding is that in parsing the gold tree
for each input sentence is unique, while in MT
many derivations can generate the same reference
translation. In other words, the gold derivation to
update towards is a latent variable.
786
Here we formally define the latent variable
?max-violation? perceptron over a hypergraph for
MT training. For a given sentence pair ?x, y?, we
denote H(x) as the decoding hypergraph of HI-
ERO without any pruning. We say D ? H(x) if
D is a full derivation of decoding x, and D can be
derived from the hypergraph. Let good(x, y) be
the set of y-good derivations for ?x, y?:
good(x, y)
?
= {D ? H(x) | e(D) = y},
where e(D) is the translation from derivation D.
We then define the set of y-good partial derivations
that cover x
[i:j]
with root N
[i:j]
as
good
N
[i:j]
(x, y)
?
= {d ? D | D ? good(x, y),
root(d) = N
[i:j]
}
We further denote the real decoding hypergraph
with beam-pruning and cube-pruning as H
?
(x).
The set of y-bad derivations is defined as
bad
N
[i:j]
(x, y)
?
= {d ? D | D ? H
?
(x, y),
root(d) = N
[i:j]
, d 6? good
N
[i:j]
(x, y)}.
Note that the y-good derivations are defined over
the unpruned whole decoding hypergraph, while
the y-bad derivations are defined over the real de-
coding hypergraph with pruning.
The max-violation method performs the update
where the model score difference between the
incorrect Viterbi partial derivation and the best
y-good partial derivation is maximal, by penaliz-
ing the incorrect Viterbi partial derivation and re-
warding the y-good partial derivation.
More formally, we first find the Viterbi partial
derivation d
?
and the best y-good partial deriva-
tion d
+
for each N
[i:j]
group in the pruned +LM
hypergraph:
d
+
N
[i:j]
(x, y)
?
= argmax
d?good
N
[i:j]
(x,y)
w ??(x, d),
d
?
N
[i:j]
(x, y)
?
= argmax
d?bad
N
[i:j]
(x,y)
w ??(x, d),
where ?(x, d) is the feature vector for derivation
d. Then it finds the group N
?
[i
?
:j
?
]
with the max-
imal score difference between the Viterbi deriva-
tion and the best y-good derivation:
N
?
[i
?
:j
?
]
?
= argmax
N
[i:j]
w ???(x, d
+
N
[i:j]
(x, y), d
?
N
[i:j]
(x, y)),
and update as follows:
w? w + ??(x, d
+
N
?
[i
?
:j
?
]
(x, y), d
?
N
?
[i
?
:j
?
]
(x, y)),
where ??(x, d, d
?
)
?
= ?(x, d)??(x, d
?
).
3.2 Forced Decoding for HIERO
We now describe how to find the gold derivations.
1
Such derivations can be generated in way similar
to Yu et al (2013) by using a language model tai-
lored for forced decoding:
P
forced
(q | p) =
{
1 if q = p+ 1
0 otherwise
,
where p and q are the indices of the boundary
words in the reference translation. The +LM node
now has signature N
p?q
[i:j]
, where p and q are the in-
dexes of the boundary words. If a boundary word
does not occur in the reference, its index is set to
? so that its language model score will always be
??; if a boundary word occurs more than once in
the reference, its ?LM node is split into multiple
+LM nodes, one for each such index.
2
We have a similar deductive system for forced
decoding. For the previous example, rule r
5
in
Figure 2 (a) is rewritten as
X? ?y?u X
1
j?ux??ng X
2
, 1 X
2
4 X
1
?,
where 1 and 4 are the indexes for reference words
?held? and ?with? respectively. The deduction for
X
[1:5]
in Figure 2 (b) is
X
5?5
[2:3]
: s
1
X
2?3
[4:5]
: s
2
X
1?5
[1:5]
: s(r
5
) + ?+ s
1
+ s
2
r
5
,
where ? = log
?
i?{1,3,4}
P
forced
(i+ 1 | i) = 0.
4 Experiments
Following Yu et al (2013), we call our max-
violation method MAXFORCE. Our implemen-
tation is mostly in Python on top of the cdec
system (Dyer et al, 2010) via the pycdec in-
terface (Chahuneau et al, 2012). In addition, we
use minibatch parallelization of (Zhao and Huang,
1
We only consider single reference in this paper.
2
Our formulation of index-based language model fixes a
bug in the word-based LM of Yu et al (2013) when a sub-
string appears more than once in the reference (e.g. ?the
man...the man...?); thanks to Dan Gildea for pointing it out.
787
2013) to speedup perceptron training. We evalu-
ate MAXFORCE for HIERO over two CH-EN cor-
pora, IWSLT09 and FBIS, and compare the per-
formance with vanilla n-best MERT (Och, 2003)
from Moses (Koehn et al, 2007), Hypergraph
MERT (Kumar et al, 2009), and PRO (Hopkins
and May, 2011) from cdec.
4.1 Features Design
We use all the 18 dense features from cdec, in-
cluding language model, direct translation prob-
ability p(e|f), lexical translation probabilities
p
l
(e|f) and p
l
(f |e), length penalty, counts for the
source and target sides in the training corpus, and
flags for the glue rules and pass-through rules.
For sparse features we use Word-Edges fea-
tures (Charniak and Johnson, 2005; Huang, 2008)
which are shown to be extremely effective in
both parsing and phrase-based MT (Yu et al,
2013). We find that even simple Word-Edges
features boost the performance significantly, and
adding complex Word-Edges features from Yu et
al. (2013) brings limited improvement and slows
down the decoding. So in the following experi-
ments we only use Word-Edges features consisting
of combinations of English and Chinese words,
and Chinese characters, and do not use word clus-
ters nor word types. For simplicity and efficiency
reasons, we also exclude all non-local features.
4.2 Datasets and Preprocessing
Our first corpus, IWSLT09, contains ?30k
short sentences collected from spoken language.
IWSLT04 is used as development set in MAX-
FORCE training, and as tuning set for n-best
MERT, Hypergraph MERT, and PRO. IWSLT05
is used as test set. Both IWSLT04 and IWSLT05
contain 16 references.We mainly use this corpus
to investigate the properties of MAXFORCE.
The second corpus, FBIS, contains ?240k sen-
tences. NIST06 newswire is used as development
set for MAXFORCE training, and as tuning set
for all other tuning methods. NIST08 newswire
is used as test set. Both NIST06 newswire
and NIST08 newswire contain 4 references. We
mainly use this corpus to demonstrate the perfor-
mance of MAXFORCE in large-scale training.
For both corpora, we do standard tokeniza-
tion, alignment and rule extraction using the cdec
tools. In rule extraction, we remove all 1-count
rules but keep the rules mapping from one Chi-
nese word to one English word to help balancing
sent. words
phrase-based MT 32% 12%
HIERO 35% 30%
HIERO (all rules) 65% 55%
Table 1: Reachability comparison (on FBIS) be-
tween phrase-based MT reported in Yu et al
(2013) (without 1-count rules) and HIERO (with
and without 1-count rules).
 0
 0.2
 0.4
 0.6
 0.8
 1
 20  40  60  80  100
fo
rc
ed
 d
ec
od
ab
le
 ra
tio
sentence length
loose
tight
Figure 4: Reachability vs. sent. length on FBIS.
See text below for ?loose? and ?tight?.
between overfitting and coverage. We use a tri-
gram language model trained from the target sides
of the two corpora respectively.
4.3 Forced Decoding Reachability
We first report the forced decoding reachability for
HIERO on FBIS in Table 1. With the full rule set,
65% sentences and 55% words of the whole cor-
pus are forced decodable in HIERO. After pruning
1-count rules, our forced decoding covers signif-
icantly more words than phrase-based MT in Yu
et al (2013). Furthermore, in phrase-based MT,
most decodable sentences are very short, while
in HIERO the lengths of decodable sentences are
more evenly distributed.
However, in the following experiments, due to
efficiency considerations, we use the ?tight? rule
extraction in cdec that is more strict than the
standard ?loose? rule extraction, which generates
a reduced rule set and, thus, a reduced reachabil-
ity. We show the reachability distributions of both
tight and loose rule extraction in Figure 4.
4.4 Evaluation on IWSLT
For IWSLT, we first compare the performance
from various update methods in Figure 5. The
max-violation method is more than 15 BLEU
788
 30
 35
 40
 45
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
Max-Violation
local update
skip
standard update
Figure 5: Comparison of various update methods.
 42
 43
 44
 45
 46
 47
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
sparse features
dense features
Hypergraph MERT
PRO
n-best MERT
Figure 6: Sparse features (Word-Edges) contribute
?2 BLEU points, outperforming PRO and MERT.
points better than the standard perceptron (also
known as ?bold-update? in Liang et al (2006))
which updates at the root of the derivation tree.
3,4
This can be explained by the fact that in train-
ing ?58% of the standard updates are invalid (i.e.,
they do not fix any violation). We also use the
?skip? strategy of Zhang et al (2013) which up-
dates at the root of the derivation only when it fixes
a search error, avoiding all invalid updates. This
achieves ?10 BLEU better than the standard up-
date, but is still more than ?5 BLEU worse than
Max-Violation update. Finally we also try the
?local-update? method from Liang et al (2006)
which updates towards the derivation with the best
Bleu
+1
in the root group S
[0:|x|]
. This method is
about 2 BLEU points worse than max-violation.
We further investigate the contribution of sparse
features in Figure 6. On the development set,
max-violation update without Word-Edges fea-
tures achieves BLEU similar to n-best MERT and
3
We find that while MAXFORCE generates translations of
length ratio close to 1 during training, the length ratios on
dev/test sets are significantly lower, due to OOVs. So we
run a binary search for the length penalty weight after each
training iteration to tune the length ratio to ?0.97 on dev set.
4
We report BLEU with averaged reference lengths.
algorithm # feats dev test
n-best MERT 18 44.9 47.9
Hypergraph MERT 18 46.6 50.7
PRO 18 45.0 49.5
local update perc. 443K 45.6 49.1
MAXFORCE 529K 47.4 51.5
Table 2: BLEU scores (with 16 references) of var-
ious training algorithms on IWSLT09.
algorithm # feats dev test
Hypergraph MERT 18 27.3 23.0
PRO 18 26.4 22.7
MAXFORCE 4.5M 27.7 23.9
Table 3: BLEU scores (with 4 references) of vari-
ous training algorithms on FBIS.
PRO, but lower than Hypergraph MERT. Adding
simple Word-Edges features improves BLEU by
?2 points, outperforming the very strong Hyper-
graph MERT baseline by?1 point. See Table 2 for
details. The results of n-best MERT, Hypergraph
MERT, and PRO are averages from 3 runs.
4.5 Evaluation on FBIS
Table 3 shows BLEU scores of Hypergraph MERT,
PRO, and MAXFORCE on FBIS. MAXFORCE ac-
tives 4.5M features, and achieves +1.2 BLEU over
PRO and +0.9 BLEU over Hypergraph MERT. The
training time (on 32 cores) for Hypergraph MERT
and PRO is about 30 min. on the dev set, and is
about 5 hours for MAXFORCE on the training set.
5 Conclusions
We have presented a latent-variable violation-
fixing framework for general structured predic-
tion problems with inexact search over hyper-
graphs. Its application on HIERO brings signif-
icant improvement in BLEU, compared to algo-
rithms that are specially designed for MT tuning
such as MERT and PRO.
Acknowledgment
Part of this work was done during K. Z.?s intern-
ship at IBM. We thank Martin
?
Cmejrek and Lemao
Liu for discussions, David Chiang for pointing
us to pycdec, Dan Gildea for Footnote 2, and
the anonymous reviewers for comments. This
work is supported by DARPA FA8750-13-2-0041
(DEFT), DARPA HR0011-12-C-0015 (BOLT),
and a Google Faculty Research Award.
789
References
Abhishek Arun and Philipp Koehn. 2007. On-
line learning methods for discriminative training of
phrase based statistical machine translation. Proc.
of MT Summit XI, 2(5):29.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Victor Chahuneau, Noah Smith, and Chris Dyer. 2012.
pycdec: A python interface to cdec. Prague Bulletin
of Mathematical Linguistics, (98).
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of NAACL 2013.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D Manning. 2013. Fast and adaptive online
training of feature-rich translation models. to ap-
pear) ACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of ACL and AFNLP.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of ACL.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan Mc-
Donald. 2013. Online learning with inexact hyper-
graph search. In Proceedings of EMNLP.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL 2013.
790
