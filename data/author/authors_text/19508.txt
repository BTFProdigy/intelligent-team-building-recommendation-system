Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 99?108, Dublin, Ireland, August 23-29 2014.
Multi-Objective Search Results Clustering
Sudipta Acharya Sriparna Saha
Indian Institute of Technology Patna
Kurji, Patna, Bihar, India
{sudipta.pcs13,sriparna}@iitp.ac.in
Jose G. Moreno Ga
?
el Dias
Normandie University - CNRS GREYC
Caen, France
first.last@unicaen.fr
Abstract
Most web search results clustering (SRC) strategies have predominantly studied the definition of
adapted representation spaces to the detriment of new clustering techniques to improve perfor-
mance. In this paper, we define SRC as a multi-objective optimization (MOO) problem to take
advantage of most recent works in clustering. In particular, we define two objective functions
(compactness and separability), which are simultaneously optimized using a MOO-based simu-
lated annealing technique called AMOSA. The proposed algorithm is able to automatically detect
the number of clusters for any query and outperforms all state-of-the-art text-based solutions in
terms of F
?
-measure and F
b
3-measure over two gold standard data sets.
1 Introduction
Web search results clustering (SRC), also known as post-retrieval clustering or ephemeral clustering has
received much attention for the past twenty years for easing up user?s effort in web browsing. The key
idea behind SRC systems is to return some meaningful labeled clusters from a set of web documents (or
web snippets) retrieved from a search engine for a given query.
Recently, SRC strategies have been focusing on the introduction of external (exogenous) knowledge to
better capture semantics between documents (Scaiella et al., 2012; Marco and Navigli, 2013). Although
this research direction has evidenced competitive results, the proposed clustering techniques are based
on a single cluster quality measure, which must reflect alone the goodness of a given partitioning. These
techniques are usually referred to as single objective optimizations (SOO).
In this paper, we hypothesize that improved clustering can be achieved by defining different objective
functions over well-known data representations. As such, our study aims to focus on new clustering
issues for SRC instead of defining new representation spaces.
Recent studies (Maulik et al., 2011) have shown that clustering can be defined as a multi-objective
optimization (MOO) problem. Within the context of SRC, we propose to define two objective functions
(compactness and separability), which are simultaneously optimized using a MOO-based simulated an-
nealing technique called AMOSA (Bandyopadhyay et al., 2008).
In order to draw conclusive remarks, we present an exhaustive evaluation where our MOO algorithm
(MOO-clus) is compared to the most competitive text-based (endogenous) SRC algorithms: STC (Zamir
and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and
GK-means (Moreno et al., 2013). Experiments are run over two different gold standard data sets (ODP-
239 and MORESQUE) for two clustering evaluation metrics (F
?
-measure and F
b
3
-measure). Results
show that MOO-clus outperforms all text-based solutions and approaches performances of knowledge
driven strategies (Scaiella et al., 2012). In this paper, our main contributions are:
? The first
1
attempt to solve SRC by defining multiple objective functions,
? A new MOO clustering algorithm for SRC, which automatically determines the number of clusters,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/.
1
As far as we know.
99
? An exhaustive evaluation of SRC algorithms with recent data sets and evaluation metrics over the
most competitive state-of-the-art text-based SRC algorithms.
2 Related Work
2.1 SRC Algorithms
One of the most cited SRC solutions is the Suffix Tree Clustering (STC) algorithm proposed by (Zamir
and Etzioni, 1998). They propose a monothetic clustering technique, which merges base clusters with
high string overlap based on web snippets represented as compact tries. Their evaluation shows improve-
ments over agglomerative hierarchical clustering, K-Means, Buckshot, Fractionation and Single-Pass
algorithms, and is still a hard baseline to beat (Moreno and Dias, 2014).
Later, (Osinski and Weiss, 2005) proposed a polythetic solution called LINGO based on the same
string representation as of (Zamir and Etzioni, 1998). They first extract frequent phrases based on suffix-
arrays and match group descriptions with topics obtained with latent semantic analysis. Documents are
then assigned straightforwardly to their corresponding groups. Their evaluation does not allow conclu-
sive remarks but they propose an open source implementation, which is an important contribution.
More recently, (Carpineto and Romano, 2010) showed that the characteristics of the outputs returned
by SRC algorithms suggest the adoption of a meta clustering approach. The underlying idea is that dif-
ferent SOO solutions lead to complementary results that must be combined. So, they introduce a novel
criterion to measure the concordance of two partitions of objects into different clusters based on the infor-
mation content associated to the series of decisions made by the partitions on single pairs of objects. The
results of OPTIMSRC demonstrate that meta clustering is superior over individual clustering techniques.
The latest work, exclusively based on endogenous information (i.e. web snippets returned by the
search engine), is proposed by (Moreno et al., 2013). They adapt the K-means algorithm to a third-order
similarity measure and propose a stopping criterion to automatically determine the ?optimal? number of
clusters. Experiments are run over two gold standard data sets, ODP-239 (Carpineto and Romano, 2010)
and MORESQUE (Navigli and Crisafulli, 2010), and show improved results over all state-of-the-art
text-based SRC techniques so far.
A great deal of works have also proposed to include exogenous information to solve the SRC problem.
One important work is proposed by (Scaiella et al., 2012) who use Wikipedia articles to build a bipartite
graph and apply spectral clustering over it to discover relevant clusters. More recently, (Marco and
Navigli, 2013) proposed to include word sense induction based on the Web1T corpus (Brants and Franz,
2006) to improve SRC. In this paper, we exclusively focus on endogenous solutions.
2.2 MOO-based Clustering
Many works have been proposed where the problem of clustering is posed as one of multi-objective op-
timization (Deb, 2009; Maulik et al., 2011). One important work is proposed by (Handl and Knowles,
2007) who define a multi-objective clustering technique with automatic K-determination called MOCK.
Their algorithm outperforms several standard single-objective clustering algorithms (K-means, agglom-
erative hierarchical clustering and ensemble clustering) on artificial data sets.
In parallel, a multi-objective evolutionary algorithm for fuzzy clustering is proposed by (Bandyopad-
hyay et al., 2007) for clustering gene expressions. Here, two objectives are simultaneously optimized.
The first one is the objective function optimized in the fuzzy C-means algorithm (Bezdek, 1981) and the
other one is the Xie-Beni index (Xie and Beni, 1991).
Later, (Mukhopadhyay and Maulik, 2009) proposed a novel approach that combines the multi-
objective fuzzy clustering method of (Bandyopadhyay et al., 2007) with a Support Vector Machines
(SVM) classifier. Performance results are provided for remote sensing data.
As far as we know, within text applications, (Morik et al., 2012) is the first work, which formulates
text clustering a multi-objective optimization problem. In particular, they express desired properties
of frequent termset clustering in terms of multiple conflicting objective functions. The optimization is
solved by a genetic algorithm and the result is a set of Pareto-optimal solutions. Note that this effort is
100
defined for large text colllections with high dimensional data, which is contradictory to the specific task
of SRC (Carpineto et al., 2009)
2
.
2.3 Our Motivation
Recent works have focused on the introduction of external (exogenous) knowledge to solve the SRC
task. However, this research direction higly depends on existing resources, which are not available for a
great deal of languages. Moreover, (Carpineto and Romano, 2010) has suggested an interesting research
direction, which has still remained unexplored. Indeed, (Carpineto and Romano, 2010) showed that meta
clustering leads to improved results in the context of text-based (endogenous) SRC. This suggests that
better clustering can be obtained by combining different SOO solutions. However, their algorithm is
casted to a SOO problem of the concordance between the clustering combination and a meta partition.
As a consequence, we hypothesize that improved performances can be obtained by defining the SRC
task as a MOO clustering problem. For that purpose, we (1) take advantage of the recent advances in the
field of multi-objective clustering (Saha and Bandyopadhyay, 2010), (2) define new objective functions
in a non euclidean space and (3) adapt a MOO-based simulated annealing technique called AMOSA
(Bandyopadhyay et al., 2008) to take into account third-order similarity metrics (Moreno et al., 2013).
3 Clustering as a MOO Problem
3.1 Formal Definition of MOO Clustering
Multi-objective optimization can be formally stated as finding the vector x
?
= [x
?
1
, x
?
2
, . . . , x
?
n
]
T
of
decision variables that simultaneously optimize M objective function values {f
1
(x), f
2
(x), . . . , f
M
(x)}
while satisfying user-defined constraints, if any.
An important concept in MOO is that of domination. Within the context of a maximization prob-
lem, a solution x
i
is said to dominate x
j
if ?k ? 1, 2, . . . ,M, f
k
(x
i
) ? f
k
(x
j
) and ?k ?
1, 2, . . . ,M, such that f
k
(x
i
) > f
k
(x
j
).
Among a set of solutions R, the non-dominated set of solutions R
?
are those that are not dominated by
any member of the set R and is called the globally Pareto-optimal set or Pareto front. In general, a MOO
algorithm outputs a set of solutions not dominated by any solution encountered by it. These notions can
be illustrated by considering an optimization problem with two objective functions (f
1
and f
2
) with six
different solutions, as shown in Figure 1. Here target is to maximize both objective functions f
1
and f
2
.
1
3
4
5
Pareto Front
2
6
f1(maximize)
f2(maximize)
Figure 1: Example of dominance and Pareto optimal front.
In this example, solutions 3, 4 and 5 dominate all the other three solutions 1, 2 and 6. Solutions 3, 4
and 5 are nondominating to each other. Because 3 is better than 4 w.r.t. function f
1
, but 4 is better than
3 w.r.t. f
2
. Similarly 4 is better than 5 w.r.t. f
1
but 5 is better than 4 w.r.t. f
2
. The same happens for
solutions 3 and 5. So, the Pareto front is made of solutions 3, 4 and 5.
Within the specific context of clustering, two objective functions are usually defined, which must be
optimized simultaneously. These functions are based on two intrinsic properties of the data space and
are defined as follows.
2
SRC is usually referred to as text clustering in the ?small?: i.e. small list of short text documents.
101
Compactness: This objective function measures the proximity among the various elements of a given
cluster and must be maximized.
Separability: This objective function measures the similarity between two cluster centroids and must
be minimized.
3.2 AMOSA Optimization Strategy
Clustering is viewed as a search problem, where optimal partitions satisfying the given set of objective
functions must be discovered. As such, an optimization strategy must be defined. Here, we propose to
use archived multi-objective simulated annealing (AMOSA) proposed by (Bandyopadhyay et al., 2008).
AMOSA incorporates the concept of an archive where the non-dominated solutions seen so far are stored.
Two limits are kept on the size of the archive: a hard limit denoted by HL and a soft limit denoted by
SL. Given ? > 1, the algorithm begins with the initialization of a number (? ? SL) of solutions each of
which representing a state in the search space. Thereafter, the non-dominated solutions are determined
and stored in the archive.
Then, one point is randomly selected from the archive. This is taken as the current point, or the initial
solution, at temperature T = T
max
. The current point is perturbed/mutated to generate a new solution
named new-pt and its objective functions are computed. The domination status of the new-pt is checked
w.r.t. the current point and the solutions in the archive. Based on domination status, different cases may
arise: (i) accept the new-pt, (ii) accept the current-pt or (iii) accept a solution from the archive. In case
of overflow of the archive, its size is reduced to HL.
The process is repeated iter times for each temperature that is annealed with a cooling rate of ? (<1)
till the minimum temperature T
min
is attained. The process thereafter stops and the archive contains the
final non-dominated solutions i.e. the Pareto front.
4 SRC as MOO Problem: MOO-clus
4.1 Archive Initialization
As we follow an endogenous approach, only the information returned by a search engine is used. In
particular, we only deal with web snippets and each one is represented as a word feature vector. So, our
solution called MOO-clus starts its execution after initializing the archive with some random solutions
as archive members. Here, a particular solution refers to a complete assignment of web snippets (or data
points) in several clusters. So, the first step is to represent a solution compatible with AMOSA, which
represents each individual solution as a string. In order to encode the clustering problem in the form of
a string, a center-based representation is used. Note that the use of a string representation facilitates the
definition of individuals and mutation functions (Bandyopadhyay et al., 2008).
Let us assume that the archive member i represents the centroids of K
i
clusters and the number of
tokens in a centroid is p
3
, then the archive member (or string) has length l
i
where l
i
= p?K
i
. To initialize
the number of centroids K
i
encoded in the string i, a random value between 2 and K
max
is chosen and
each K
i
cluster centroid is initialized by randomly generated tokens from the global vocabulary.
4.2 Assignment of Web Snippets
As for any classical clustering algorithms, web snippets (or data points) must be assigned to their respec-
tive clusters. In MOO-clus, this assignment is computed as in (Moreno et al., 2013), to take advantage
of recent advances in similarity measures. For two word feature vectors d
i
and d
j
, their similarity is
evaluated by the similarity of their constituents as defined in Equation 1.
S(d
i
, d
j
) =
1
?d
i
??d
j
?
?d
i
?
?
r=1
?d
j
?
?
b=1
SCP (w
r
i
, w
b
j
), with SCP (w
1
, w
2
) =
P (w
1
, w
2
)
2
P (w
1
) ? P (w
2
)
(1)
3
A centroid is represented by a p word feature vector (w
1
k
, w
2
k
, w
3
k
, . . . , w
p
k
).
102
Here, w
r
i
(resp. w
b
j
) corresponds to the token at the r
th
(resp. b
th
) position of the word feature vector d
i
(resp. d
j
). ?d
i
? and ?d
j
? respectively denote the total number of tokens in word feature vectors d
i
and
d
j
. SCP (w
r
i
, w
b
j
) is the Symmetric Conditional Probability (da Silva et al., 1999) where P (., .) is the
joint probability of two tokens (w
1
and w
2
) appearing in the same word feature vector and P (.) is the
marginal probability of any token appearing in a word feature vector.
Note that each cluster centroid is a word feature vector of varying number of tokens. Thus, Equation 2
is used to assign any data point (web snippet) d
j
to a cluster twhose centroid has the maximum similarity
value to d
j
.
t = argmax
k=1,...K
S(d
j
,m
pi
k
) (2)
K denotes the total number of clusters, d
j
is the j
th
web snippet, m
pi
k
is the centroid of the k
th
cluster pi
k
and S(d
j
,m
pi
k
) denotes similarity measurement between the point d
j
and cluster centroid
m
pi
k
defined in Equation 1.
4.3 Definition of Objective Functions
A string i represents a set of centroids to which web snippets can be assigned as seen in Section 4.2. As a
consequence, each string i corresponds to a candidate partition of the data space. Now, in order to verify
the domination of different solutions over other ones, objective functions must be defined. Compactness
and separability are usually used in MOO clustering solutions. Here, compactness can be defined as the
informational density of each cluster. This can be straightforwardly formulated as in Equation 3.
Compactness =
K
?
k=1
?
d
i
?pi
k
S(d
i
,m
pi
k
) (3)
Note that if tokens in a particular cluster are very similar to the cluster centroid then the corresponding
Compactness value would be maximized. Here our target is to form good clusters whose compactness
in terms of similarity should be maximum.
The second objective function is cluster separability, which measures the dissimilarity between two
cluster centroids. Indeed, the purpose of any clustering algorithm is to obtain compact similar typed
clusters, which are dissimilar to each other. Here, we define separability as the minimization of the
summation of similarities between each pair of cluster centroids. This is defined in Equation 4, where
m
pi
k
and m
pi
o
are the centroids of clusters pi
k
and pi
o
, respectively.
Separability =
K
?
k=1
K
?
o=k+1
S(m
pi
k
,m
pi
o
) (4)
Finally, for a particular string, the following objectives {Compactness,
1
Separability
} are maximized
using the search capability of AMOSA.
4.4 Search Operators
In MOO-clus, AMOSA is used as the optimization strategy. For that purpose, three different types of
mutation operations have been defined to suit the framework.
Mutation 1: This mutation operation is used to update the cluster center representation. Each token of
cluster centroid is replaced by one token from the global vocabulary according to highest SCP similarity.
This is applied individually to all tokens of a particular centroid if it is selected for mutation.
Mutation 2: This mutation operation is used to reduce the size of the string by 1. We randomly select a
cluster centroid and thereafter all the tokens of this centroid are deleted from the string.
Mutation 3: This mutation is for increasing the size of string by 1 i.e. one new centroid is inserted in
the string. For that purpose, we randomly choose p number of tokens from the global vocabulary and
add it to the string.
103
Let be a string < w
1
w
2
w
3
w
4
w
5
w
6
> representing three cluster centroids (w
1
, w
2
), (w
3
, w
4
) and
(w
5
, w
6
)
4
. For mutation 1, let position 2 be selected randomly. Each token of the word vector (w
3
, w
4
)
will be changed by some token from the global vocabulary using SCP. Then, after change, the string
will look like < w
1
w
2
w
new
3
w
new
4
w
5
w
6
>. If mutation 2 is selected, a centroid will be removed from
the string. Let centroid 3 be selected for deletion. The new string will look like < w
1
w
2
w
3
w
4
>.
In case of mutation 3, a new centroid will be added to the string. A new cluster centroid is generated
choosing p=2 number of tokens from the global vocabulary. Let the randomly generated new clus-
ter centroid to be added to the string be (w
7
, w
8
). After inclusion of this centroid, the string will be
< w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
>. In our experiments, we have associated equal probability to each of
these mutation operations. Thus, each mutation is applied in 33% cases of the cases.
5 Experimental Setup
5.1 Datasets
The main gold standards used for the evaluation of SRC algorithms are ODP-239 and MORESQUE
5
.
In ODP-239 (Carpineto and Romano, 2010), each document is represented by a title and a web snip-
pet and the subtopics are chosen from the top levels of DMOZ
6
. On the other hand, the subtopics in
MORESQUE (Navigli and Crisafulli, 2010) follow a more natural distribution as they are defined based
on the disambiguation pages of Wikipedia. As such, the subtopics cover most of the query-related senses.
However, not all queries are Wikipedia related or ambiguous (e.g. ?Olympic Games?, which Wikipedia
entry is not ambiguous, although there are many events related to this topic). As a consequence, it is
clear that different results can be obtained from one data set to another. A quick summary of both data
sets is presented in Table 1.
# of # of Subtopics # of
Dataset queries Avg / Min / Max Snippets
ODP-239 239 10 / 10 / 10 25580
MORESQUE 114 6.7 / 2 / 38 11402
Table 1: SRC gold standard data sets.
5.2 Evaluation Metrics
A successful SRC systemmust evidence high quality level clustering. Each query subtopic should ideally
be represented by a unique cluster containing all the relevant web pages inside. However, determining a
unique and complete metric to evaluate the performance of a clustering algorithm is still an open problem
(Amig?o et al., 2013).
In this paper, we propose to use the F
b
3
-measure (Amig?o et al., 2009) to explore the Pareto front.
In particular, F
b
3
has been defined to evaluate cluster homogeneity, completeness, rag-bag and size-vs-
quantity constraints. F
b
3
is a function of Precision
b
3
(P
b
3
) and Recall
b
3
(R
b
3
). All metrics are defined
in Equation 5
F
b
3
=
2 ? P
b
3
?R
b
3
P
b
3
+ R
b
3
, P
b
3
=
1
N
K
?
i=1
?
d
j
?pi
i
1
|pi
i
|
?
d
l
?pi
i
g
?
(d
j
, d
l
), R
b
3
=
1
N
K
?
i=1
?
d
j
?pi
?
i
1
|pi
?
i
|
?
d
l
?pi
?
i
g(d
j
, d
l
) (5)
where pi
i
is i
th
cluster, pi
?
i
is the gold standard of the category i, and g
?
(., .) and g(., .) are defined as
follows:
g
?
(d
i
, d
j
) =
{
1 ? ?l : d
i
? pi
?
l
? d
j
? pi
?
l
0 otherwise
and g(d
i
, d
j
) =
{
1 ? ?l : d
i
? pi
l
? d
j
? pi
l
0 otherwise
.
4
with p=2.
5
AMBIENT has received less attention since the creation of ODP-239.
6
http://www.dmoz.org [Last access: 14/03/2014].
104
Most SRC studies have also used the F
?
-measure (F
?
), which is defined in Equation 6.
F
?
=
(?
2
+ 1) ? P ?R
?
2
? P + R
, P =
TP
TP + FP
, R =
TP
TP + FN
(6)
where
TP =
K
?
i=1
?
d
j
?pi
?
i
?
d
l
? pi
?
i
l 6= j
g(d
i
, d
j
), FP =
K
?
i=1
?
d
j
?pi
i
?
d
l
? pi
i
l 6= j
(1? g
?
(d
i
, d
j
)), FN =
K
?
i=1
?
d
j
?pi
?
i
?
d
l
? pi
?
i
l 6= j
(1? g(d
i
, d
j
)).
6 Results and Discussion
In this evaluation, we used the open source framework GATE (Cunningham et al., 2013) without stop-
word removal for web snippet tokenization
7
. We executed MOO-clus over ODP-239 and MORESQUE.
The parameters of MOO-clus are: T
min
= 0.01, T
max
= 100, ? = 0.85, HL = 10, SL = 20 and
iter = 15. Note that, they have been determined after conducting a thorough sensitivity study. A first
set of experiments have been conducted for different p values of tokens present in the centroid, namely
in the range 2 to 5 in order to understand the behavior of MOO-clus w.r.t. centroid size
8
. Note that the
partition with maximum F
b
3 is choosen for each size of p
9
. Overall results are shown in Table 2.
MORESQUE ODP-239
MOO-clus MOO-clus
2 3 4 5 2 3 4 5
F
b
3
0.477 0.491 0.497 0.502 0.478 0.481 0.484 0.481
F
1
0.661 0.666 0.675 0.658 0.379 0.379 0.384 0.381
F
2
0.750 0.768 0.764 0.742 0.534 0.536 0.537 0.535
F
5
0.831 0.862 0.846 0.820 0.717 0.720 0.716 0.715
Table 2: Evaluation results of MOO-clus over MORESQUE and ODP239 data sets.
Results show that for MORESQUE, MOO-clus obtains the highest F
b
3 value for p=5. In particular,
performance increases for higher values of p. For ODP-239, best results are reported for p=4, but evi-
dence less sensitivity to the number of words in the centroids. Indeed, a marginal difference is obtained
between all runs. In terms of F
?
, the same behaviour is obtained for ODP-239. But, for MORESQUE,
best results are provided for smaller values of p, namely p=3.
Two important comments must be pointed at. In the first place, F
b
3
shows a steady behaviour compared
to F
?
when the data set changes. The conclusions drawn in (Amig?o et al., 2009) reporting the superiority
of F
b
3
over F
?
seem to be verified for the specific case of SRC. In the second place, MOO-clus evidences
a marginal sensitivity to different p values. Indeed, for ODP-239, changing p between 2 and 5 words has
a negligible impact on F
b
3 . The figures show a different behaviour for MORESQUE but this can easily
be explained. In MORESQUE, less queries are provided for test and the number of reference clusters
varies between 2 and 38, with a majority of queries containing very few clusters (the average cluster size
is 6.7). As such, small clustering errors may result in high deviations in the evaluation metrics. So, p
can be seen as a non influent parameter for clustering purposes. In fact, increasing the value of p may
exclusively allow a more descriptive power for cluster labeling.
We also compared MOO-clus to the current state-of-the-art text-based (endogenous) SRC algorithms:
STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Ro-
mano, 2010), Bisecting Incremental K-means (BIK), GK-means (Moreno et al., 2013) and the combi-
nation STC-LINGO (Moreno and Dias, 2014). The results are illustrated in Table 3 where we provide
values for all the metrics for open source implementations and reported values in the literature for the
7
Note that keeping stop words is a challenging task as most methodologies withdraw these elements as they are hard to
handle. This decision is supported by the fact that we aim to produce as much as possible language-independent solutions.
8
Note that to ease the user effort in searching for information, the cluster label must be small and expressive. Typical
configurations range between 3 to 5 to include multiword expressions.
9
F
?
metrics are calculated over the partition with highest F
b
3
value.
105
other experiments i.e. OPTIMSRC, GK-means and STC-LINGO. In particular, the Min (resp. Max)
column refers to the worst (resp. best) performance when varying p, the size of the centroid.
The results of Table 3 clearly show the performance improvements of our proposed methodology over
existing text-based techniques for both data sets and most evaluation metrics. For ODP-239, MOO-clus
attains the highest values with respect to F
1
, F
2
, F
5
and F
b
3
metrics against all existing endogenous algo-
rithms. For MORESQUE, our algorithm reaches highest performance over all state-of-the-art algorithms
for F
1
and F
b
3 metrics but marginally fails for F
2
and F
5
against GK-means.
MOO-clus SOO SRC Combination of SOO SRC
Min Max GK-means STC LINGO BIK OPTIMSRC STC-LINGO
MORESQUE F
1
0.658 0.675 0.665 0.455 0.326 0.317 N/A 0.561
F
2
0.742 0.768 0.770 0.392 0.260 0.269 N/A N/A
F
5
0.820 0.862 0.872 0.370 0.237 0.255 N/A N/A
F
b
3
0.477 0.502 0.482 0.460 0.399 0.315 N/A 0.498
ODP-239 F
1
0.379 0.384 0.366 0.324 0.273 0.200 0.313 0.362
F
2
0.534 0.537 0.416 0.319 0.167 0.173 0.341 N/A
F
5
0.715 0.720 0.462 0.322 0.153 0.165 0.380 N/A
F
b
3
0.478 0.484 0.452 0.403 0.346 0.307 N/A 0.425
Table 3: Comparative results with respect to F
?
and F
b
3 metrics over the ODP-239 and MORESQUE
datasets obtained by different SRC techniques.
It is important to notice that OPTIMSRC and STC-LINGO can be viewed as a combination of different
SRC SOO solutions but still casted to a SOO solution. These previous results report interesting issues
for SRC and confort the idea that the combination of different objective functions may lead to enhanced
SRC algorithms. But, MOO-clus is capable to find better partitions than OPTIMSRC and STC-LINGO
for all data sets and all evaluation metrics as reported in Table 3.
It is important to notice that the MOO-clus provides a set of partitions with automatic definition of
the number of clusters. So, defining one unique solution is an important issue for SRC. So far, we have
provided results for the best partition evaluated by F
b
3
. However, deeper analysis of all the partitions
on the Pareto front must be endeavoured. Results are reported for F
b
3
only as all other metrics behave
correspondingly and are reported in Table 4.
MORESQUE ODP-239
2 3 4 5 2 3 4 5
Min 0.428 0.464 0.464 0.462 0.396 0.401 0.403 0.408
Max 0.477 0.491 0.497 0.502 0.478 0.481 0.484 0.481
Avg. 0.454 0.479 0.482 0.486 0.443 0.447 0.448 0.449
Table 4: F
b
3 evaluation results of the Pareto front.
Figures show the validity of each individual solution of the Pareto front. In the worst case, MOO-clus
produces similar results compared to the hard baseline STC. On average, it reaches the results of GK-
means and the highest performance values can be found on the Pareto front. The correct identification
of the best partition is still an open issue and can be compared to the automatic selection of K clusters,
which is a hard task as shown in recent studies (Scaiella et al., 2012; Marco and Navigli, 2013).
7 Conclusions
In this paper, we proposed the first attempt
10
to define the SRC task as a multi-objective problem. For that
purpose, we defined two objective functions, which are simultaneously optimized through the archived
multi-objective simulated annealing framework called AMOSA. A correct definition of the task allowed
to take advantage of the most recent advances in terms of endogenous SRC algorithms as well as the most
powerful techniques for multi-objective clustering. The performance of MOO-clus has been evaluated
over two gold standard data sets, ODP-239 andMORESQUE for different evaluation metrics, F
1
and F
b
3 .
10
As far as we know.
106
Results showed that our proposal steadily outperforms all existing state-of-the-art text-based endogenous
SRC algorithms and approaches recent knowledge-driven exogenous strategies (Scaiella et al., 2012),
which reach F
1
=0.413 for ODP-239
11
.
As future works, we propose to use MOO clustering in a strict meta learning way, where any labeled-
based SOO solution is defined by specific Compactness and Separability functions. Another research
direction is the definition of the Dual representation proposed by (Moreno et al., 2014) as a MOO prob-
lem. Finally, new objective functions can be defined to measure the quality of the labels, which may
integrate meaningful multiword expressions or named entities.
Acknowledgement
We would like to thank the CNRS to provide Sriparna Saha with a 6 months internship at the GREYC
Laboratory of the Normandie University.
References
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Information Retrieval, 12(4):461?486.
Enrique Amig?o, Julio Gonzalo, and Felisa Verdejo. 2013. A general evaluation measure for document organization
tasks. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 643?652.
Sanghamitra Bandyopadhyay, Anirban Mukhopadhyay, and Ujjwal Maulik. 2007. An improved algorithm for
clustering gene expression data. Bioinformatics, 23(21):2859?2865.
Sanghamitra Bandyopadhyay, Sriparna Saha, Ujjwal Maulik, and Kalyanmoy Deb. 2008. A simulated annealing-
based multiobjective optimization algorithm: Amosa. In IEEE transactions on evolutionary computation, pages
269?283.
James C. Bezdek. 1981. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum, New York.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram.
Claudio Carpineto and Giovanni Romano. 2010. Optimal meta search results clustering. In 33rd International
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 170?177.
Claudio Carpineto, Stanislaw Osinski, Giovanni Romano, and Dawid Weiss. 2009. A survey of web clustering
engines. ACM Computing Surveys, 41(3):1?38.
Hamish Cunningham, Valentin Tablan, Angus Roberts, and Kalina Bontcheva. 2013. Getting more out of
biomedical documents with gate?s full lifecycle open source text analytics. PLoS Computational Biology,
9(2):e1002854.
Joaquim Ferreira da Silva, Ga?el Dias, Sylvie Guillor?e, and Jos?e Gabriel Pereira Lopes. 1999. Using localmaxs
algorithm for the extraction of contiguous and non-contiguous multiword lexical units. In Proceedings of 9th
Portuguese Conference in Artificial Intelligence (EPIA), pages 113?132.
Kalyanmoy Deb. 2009. Multi-Objective Optimization Using Evolutionary Algorithms. Wiley.
Julia Handl and Joshua Knowles. 2007. An evolutionary approach to multiobjective clustering. IEEE Transactions
on Evolutionary Computation, 11:56?76.
Antonio D. Marco and Roberto Navigli. 2013. Clustering and diversifying web search results with graph-based
word sense induction. Computational Linguistics, 39(4):1?43.
Ujjwal Maulik, Sanghamitra Bandyopadhyay, and Anirban Mukhopadhyay. 2011. Multiobjective Genetic Algo-
rithms for Clustering - Applications in Data Mining and Bioinformatics. Springer.
Jos?e G. Moreno and Ga?el Dias. 2014. Easy web search results clustering: When baselines can reach state-of-
the-art algorithms. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 1?5.
11
Note that results of (Marco and Navigli, 2013) are not reported in this paper as the authors do not use the standard versions
of MORESQUE and do not provide experiments for ODP-239.
107
Jos?e G. Moreno, Ga?el Dias, and Guillaume Cleuziou. 2013. Post-retrieval clustering using third-order similarity
measures. In 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 153?158.
Jos?e G. Moreno, Ga?el Dias, and Guillaume Cleuziou. 2014. Query log driven web search results clustering. In
Proceedings of the 37th Annual ACM SIGIR Conference (SIGIR).
Katharina Morik, Andreas Kaspari, Michael Wurst, and Marcin Skirzynsk. 2012. Multi-objective frequent termset
clustering. Knowledge Information Systems, 30(3):715?738.
Anirban Mukhopadhyay and Ujjwal Maulik. 2009. Unsupervised pixel classification in satellite imagery using
multiobjective fuzzy clustering combined with SVM classifier. IEEE Transactions on Geoscience and Remote
Sensing, pages 1132?1138.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
116?126.
Stanislaw Osinski and Dawid Weiss. 2005. A concept-driven algorithm for clustering search results. IEEE
Intelligent Systems, 20(3):48?54.
Sriparna Saha and Sanghamitra Bandyopadhyay. 2010. A symmetry based multiobjective clustering technique for
automatic evolution of clusters. Pattern Recognition, 43(3):738?751.
Ugo Scaiella, Paolo Ferragina, Andrea Marino, and Massimiliano Ciaramita. 2012. Topical clustering of search
results. In 5th ACM International Conference on Web Search and Data Mining (WSDM), pages 223?232.
Xuanli L. Xie and Gerardo Beni. 1991. A validity measure for fuzzy clustering. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 13:841?847.
Oren Zamir and Oren Etzioni. 1998. Web document clustering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages
46?54.
108
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?5,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Easy Web Search Results Clustering:
When Baselines Can Reach State-of-the-Art Algorithms
Jose G. Moreno
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
jose.moreno@unicaen.fr
Gae?l Dias
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
gael.dias@unicaen.fr
Abstract
This work discusses the evaluation of
baseline algorithms for Web search re-
sults clustering. An analysis is performed
over frequently used baseline algorithms
and standard datasets. Our work shows
that competitive results can be obtained by
either fine tuning or performing cascade
clustering over well-known algorithms. In
particular, the latter strategy can lead to
a scalable and real-world solution, which
evidences comparative results to recent
text-based state-of-the-art algorithms.
1 Introduction
Visualizing Web search results remains an open
problem in Information Retrieval (IR). For exam-
ple, in order to deal with ambiguous or multi-
faceted queries, many works present Web page re-
sults using groups of correlated contents instead
of long flat lists of relevant documents. Among
existing techniques, Web Search Results Cluster-
ing (SRC) is a commonly studied area, which
consists in clustering ?on-the-fly? Web page re-
sults based on their Web snippets. Therefore,
many works have been recently presented includ-
ing task adapted clustering (Moreno et al., 2013),
meta clustering (Carpineto and Romano, 2010)
and knowledge-based clustering (Scaiella et al.,
2012).
Evaluation is also a hot topic both in Natural
Language Processing (NLP) and IR. Within the
specific case of SRC, different metrics have been
used such as F
1
-measure (F
1
), kSSL1 and F
b
3-
measure (F
b
3) over different standard datasets:
ODP-239 (Carpineto and Romano, 2010) and
Moresque (Navigli and Crisafulli, 2010). Unfor-
tunately, comparative results are usually biased as
1This metric is based on subjective label evaluation and as
such is out of the scope of this paper.
baseline algorithms are run with default parame-
ters whereas proposed methodologies are usually
tuned to increase performance over the studied
datasets. Moreover, evaluation metrics tend to cor-
relate with the number of produced clusters.
In this paper, we focus on deep understand-
ing of the evaluation task within the context of
SRC. First, we provide the results of baseline algo-
rithms with their best parameter settings. Second,
we show that a simple cascade strategy of base-
line algorithms can lead to a scalable and real-
world solution, which evidences comparative re-
sults to recent text-based algorithms. Finally, we
draw some conclusions about evaluation metrics
and their bias to the number of output clusters.
2 Related Work
Search results clustering is an active research area.
Two main streams have been proposed so far:
text-based strategies such as (Hearst and Peder-
sen, 1996; Zamir and Etzioni, 1998; Zeng et al.,
2004; Osinski et al., 2004; Carpineto and Romano,
2010; Carpineto et al., 2011; Moreno et al., 2013)
and knowledge-based ones (Ferragina and Gulli,
2008; Scaiella et al., 2012; Di Marco and Nav-
igli, 2013). Successful results have been obtained
by recent works compared to STC (Zamir and Et-
zioni, 1998) and LINGO (Osinski et al., 2004)
which provide publicly available implementations,
and as a consequence, are often used as state-
of-the-art baselines. On the one hand, STC pro-
poses a monothetic methodology which merges
base clusters with high string overlap relying on
suffix trees. On the other hand, LINGO is a poly-
thetic solution which reduces a term-document
matrix using single value decomposition and as-
signs documents to each discovered latent topic.
All solutions have been evaluated on differ-
ent datasets and evaluation measures. The well-
known F
1
has been used as the standard evaluation
metric. More recently, (Carpineto and Romano,
1
Moresque ODP-239
F
1
F
b
3
F
1
F
b
3
Algo. Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k
STC 0.4550 12.7 0.6000 2.9 0.4602 12.7 0.4987 2.9 0.3238 12.4 0.3350 3.0 0.4027 12.4 0.4046 14.5
LINGO 0.3258 26.7 0.6034 3.0 0.3989 26.7 0.5004 5.8 0.2029 27.7 0.3320 3.0 0.3461 27.7 0.4459 8.7
BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2
Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2
Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets.
2010) evidenced more complete results with the
general definition of the F
?
-measure for ? =
{1, 2, 5}, (Navigli and Crisafulli, 2010) introduced
the Rand Index metric and (Moreno et al., 2013)
used F
b
3 introduced by (Amigo? et al., 2009) as a
more adequate metric for clustering.
Different standard datasets have been built such
as AMBIENT2 (Carpineto and Romano, 2009),
ODP-2393 (Carpineto and Romano, 2010) and
Moresque4 (Navigli and Crisafulli, 2010). ODP-
239, an improved version of AMBIENT, is based
on DMOZ5 where each query, over 239 ones, is a
selected category in DMOZ and its associated sub-
categories are considered as the respective clus-
ter results. The small text description included in
DMOZ is considered as a Web snippet. Moresque
is composed by 114 queries selected from a list
of ambiguous Wikipedia entries. For each query, a
set of Web results have been collected from a com-
mercial search engine and manually classified into
the disambiguation Wikipedia pages which form
the reference clusters.
In Table 2, we report the results obtained so
far in the literature by text-based and knowledge-
based strategies for the standard F
1
over ODP-239
and Moresque datasets.
F
1
ODP239 Moresque
Text
STC 0.324 0.455
LINGO 0.273 0.326
(Carpineto and Romano, 2010) 0.313 -
(Moreno et al., 2013) 0.390 0.665
Know. (Scaiella et al., 2012) 0.413 -(Di Marco and Navigli, 2013) - 0.7204*
Table 2: State-of-the-art Results for SRC. (*) The
result of (Di Marco and Navigli, 2013) is based
on a reduced version of AMBIENT + Moresque.
3 Baseline SRC Algorithms
Newly proposed algorithms are usually tuned to-
wards their maximal performance. However, the
results of baseline algorithms are usually run with
2http://credo.fub.it/ambient/ [Last acc.: Jan., 2014]
3http://credo.fub.it/odp239/ [Last acc.: Jan., 2014]
4http://lcl.uniroma1.it/moresque/ [Last acc.: Jan., 2014]
5http://www.dmoz.org [Last acc.: Jan., 2014]
default parameters based on available implemen-
tations. As such, no conclusive remarks can be
drawn knowing that tuned versions might provide
improved results.
In particular, available implementations6 of
STC, LINGO and the Bisection K-means (BiKm)
include a fixed stopping criterion. However, it
is well-known that tuning the number of output
clusters may greatly impact the clustering perfor-
mance. In order to provide fair results for base-
line algorithms, we evaluated a k-dependent7 ver-
sion for all baselines. We ran all algorithms for
k = 2..20 and chose the best result as the ?op-
timal? performance. Table 1 sums up results for
all the baselines in their different configurations
and shows that tuned versions outperform standard
(available) ones both for F
1
and F
b
3 over ODP-
239 and Moresque.
4 Cascade SRC Algorithms
In the previous section, our aim was to claim that
tunable versions of existing baseline algorithms
might evidence improved results when faced to
the ones reported in the literature. And these
values should be taken as the ?real? baseline re-
sults within the context of controllable environ-
ments. However, exploring all the parameter space
is not an applicable solution in a real-world situa-
tion where the reference is unknown. As such, a
stopping criterion must be defined to adapt to any
dataset distribution. This is the particular case for
the standard implementations of STC and LINGO.
Previous results (Carpineto and Romano, 2010)
showed that different SRC algorithms provide dif-
ferent results and hopefully complementary ones.
For instance, STC demonstrates high recall and
low precision, while LINGO inversely evidences
high precision for low recall. Iteratively apply-
ing baseline SRC algorithms may thus lead to
improved results by exploiting each algorithm?s
strengths.
6http://carrot2.org [Last acc.: Jan., 2014]
7Carrot2 parameters maxClusters, desiredClusterCount-
Base and clusterCount were used to set k value.
2
In a cascade strategy, we first cluster the ini-
tial set of Web page snippets with any SRC al-
gorithm. Then, the input of the second SRC al-
gorithm is the set of meta-documents built from
the documents belonging to the same cluster8. Fi-
nally, each clustered meta-document is mapped to
the original documents generating the final clus-
ters. This process can iteratively be applied, al-
though we only consider two-level cascade strate-
gies in this paper.
This strategy can be viewed as an easy, re-
producible and parameter free baseline SRC im-
plementation that should be compared to existing
state-of-the-art algorithms. Table 3 shows the re-
sults obtained with different combinations of SRC
baseline algorithms for the cascade strategy both
for F
1
and F
b
3 over ODP-239 and Moresque. The
?Stand.? column corresponds to the performance
of the cascade strategy and k to the automatically
obtained number of clusters. Results show that
the combination STC-STC achieves the best per-
formance overall for the F
1
and STC-LINGO is
the best combination for the F
b
3 in both datasets.
In order to provide a more complete evaluation,
we included in column ?Equiv.? the performance
that could be obtained by the tunable version of
each single baseline algorithm based on the same
k. Interestingly, the cascade strategy outperforms
the tunable version for any k for F
1
but fails to
compete (not by far) with F
b
3 . This issue will be
discussed in the next section.
5 Discussion
In Table 1, one can see that when using the tuned
version and evaluating with F
1
, the best perfor-
mance for each baseline algorithm is obtained for
the same number of output clusters independently
of the dataset (i.e. around 3 for STC and LINGO
and 2 for BiKm). As such, a fast conclusion would
be that the tuned versions of STC, LINGO and
BiKm are strong baselines as they show similar
behaviour over datasets. Then, in a realistic situa-
tion, k might be directly tuned to these values.
However, when comparing the output number
of clusters based on the best F
1
value to the refer-
ence number of clusters, a huge difference is ev-
idenced. Indeed, in Moresque, the ground-truth
average number of clusters is 6.6 and exactly 10
in ODP-239. Interestingly, F
b
3 shows more accu-
rate values for the number of output clusters for
8Fused using concatenation of strings.
the best tuned baseline performances. In particu-
lar, the best F
b
3 results are obtained for LINGO
with 5.8 clusters for Moresque and 8.7 clusters
for ODP-239 which most approximate the ground-
truths.
In order to better understand the behaviour of
each evaluation metric (i.e. F
?
and F
b
3) over dif-
ferent k values, we experienced a uniform random
clustering over Moresque and ODP-239. In Fig-
ure 1(c), we illustrate these results. The important
issue is that F
?
is more sensitive to the number
of output clusters than F
b
3 . On the one hand, all
F
?
measures provide best results for k = 2 and
a random algorithm could reach F
1
=0.5043 for
Moresque and F
1
=0.2980 for ODP-239 (see Ta-
ble 1), thus outperforming almost all standard im-
plementations of STC, LINGO and BiKm for both
datasets. On the other hand, F
b
3 shows that most
standard baseline implementations outperform the
random algorithm.
Moreover, in Figures 1(a) and 1(b), we illus-
trate the different behaviours between F
1
and F
b
3
for k = 2..20 for both standard and tuned ver-
sions of STC, LINGO and BiKm. One may clearly
see that F
b
3 is capable to discard the algorithm
(BiKm) which performs worst in the standard ver-
sion while this is not the case for F
1
. And, for
LINGO, the optimal performances over Moresque
and ODP-239 are near the ground-truth number of
clusters while this is not the case for F
1
which ev-
idences a decreasing tendency when k increases.
In section 4, we showed that competitive results
could be achieved with a cascade strategy based on
baseline algorithms. Although results outperform
standard and tunable baseline implementations for
F
1
, it is wise to use F
b
3 to better evaluate the SRC
task, based on our previous discussion. In this
case, the best values are obtained by STC-LINGO
with F
b
3=0.4980 for Moresque and F
b
3=0.4249
for ODP-239, which highly approximate the val-
ues reported in (Moreno et al., 2013): F
b
3=0.490
(Moresque) and F
b
3=0.452 (ODP-239). Addition-
ally, when STC is performed first and LINGO later
the cascade algorithm scale better due to LINGO
and STC scaling properties9.
6 Conclusion
This work presents a discussion about the use of
baseline algorithms in SRC and evaluation met-
9http://carrotsearch.com/lingo3g-comparison [Last acc.:
Jan., 2014]
3
Moresque ODP-239
F
1
F
b
3
F
1
F
b
3
Level 1 Level 2 Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k
STC
STC 0.6145 0.5594 3.1 0.4550 0.4913 3.1 0.3629 0.3304 3.2 0.3982 0.4023 3.2
LINGO 0.5611 0.4932 7.3 0.4980 0.4716 7.3 0.3624 0.3258 6.9 0.4249 0.4010 6.9
BiKm 0.5413 0.5160 4.5 0.4395 0.4776 4.5 0.3319 0.3276 4.3 0.3845 0.4020 4.3
LINGO
STC 0.5696 0.5176 6.7 0.4602 0.4854 6.7 0.3457 0.3029 7.2 0.4229 0.4429 7.2
LINGO 0.4629 0.4371 13.7 0.4447 0.4566 13.7 0.2789 0.2690 13.6 0.3931 0.4237 13.6
BiKm 0.4038 0.4966 8.6 0.3801 0.4750 8.6 0.2608 0.2953 8.5 0.3510 0.4423 8.5
BiKm
STC 0.5873 0.5891 2.7 0.4144 0.4069 2.7 0.3425 0.3381 2.7 0.3787 0.3677 2.7
LINGO 0.4773 0.5186 5.4 0.3832 0.3869 5.4 0.2819 0.3191 6.3 0.3546 0.3644 6.3
BiKm 0.4684 0.5764 3.5 0.3615 0.4114 3.5 0.2767 0.3322 4.3 0.3328 0.3693 4.3
Table 3: Cascade Results for Moresque and ODP-239 datasets.
(a) F
1
for Moresque (Left) and ODP-239 (Right).
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
2 4 6 8 10 12 14 16 18 20
F1
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
0.36
2 4 6 8 10 12 14 16 18 20
F1
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
(b) F
b
3
for Moresque (Left) and ODP-239 (Right).
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0.46
0.48
0.5
0.52
2 4 6 8 10 12 14 16 18 20
Fb
cu
be
d
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0.46
2 4 6 8 10 12 14 16 18 20
Fb
cu
be
d
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
(c) Evaluation Metrics for Random Clustering for Moresque (Left) and ODP-239 (Right).
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
2 4 6 8 10 12 14 16 18 20
Pe
rf
or
m
an
ce
k
F1
F2
F5
Fbcubed
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
2 4 6 8 10 12 14 16 18 20
Pe
rf
or
m
an
ce
k
F1
F2
F5
Fbcubed
Figure 1: F
1
and F
b
3 for Moresque and ODP-239 for Standard, Tuned and Random Clustering.
rics. Our experiments show that F
b
3 seems more
adapted to evaluate SRC systems than the com-
monly used F
1
over the standard datasets avail-
able so far. New baseline values which approxi-
mate state-of-the-art algorithms in terms of clus-
tering performance can also be obtained by an
easy, reproducible and parameter free implemen-
tation (the cascade strategy) and could be consid-
ered as the ?new? baseline results for future works.
4
References
E. Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Re-
trieval, 12(4):461?486.
C. Carpineto and G. Romano. 2009. Mobile infor-
mation retrieval with search results clustering : Pro-
totypes and evaluations. Journal of the American
Society for Information Science, 60:877?895.
C. Carpineto and G. Romano. 2010. Optimal meta
search results clustering. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 170?177.
C. Carpineto, M. D?Amico, and A. Bernardini. 2011.
Full discrimination of subtopics in search results
with keyphrase-based clustering. Web Intelligence
and Agent Systems, 9(4):337?349.
A. Di Marco and R. Navigli. 2013. Clustering and
diversifying web search results with graph-based
word sense induction. Computational Linguistics,
39(3):709?754.
P. Ferragina and A. Gulli. 2008. A personalized search
engine based on web-snippet hierarchical clustering.
Software: Practice and Experience, 38(2):189?225.
M.A. Hearst and J.O. Pedersen. 1996. Re-examining
the cluster hypothesis: Scatter/gather on retrieval re-
sults. In 19th Annual International Conference on
Research and Development in Information Retrieval
(SIGIR), pages 76?84.
J.G. Moreno, G. Dias, and G. Cleuziou. 2013. Post-
retrieval clustering using third-order similarity mea-
sures. In 51st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 153?158.
R. Navigli and G. Crisafulli. 2010. Inducing word
senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 116?126.
S. Osinski, J. Stefanowski, and D. Weiss. 2004. Lingo:
Search results clustering algorithm based on singu-
lar value decomposition. In Intelligent Information
Systems Conference (IIPWM), pages 369?378.
U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.
2012. Topical clustering of search results. In 5th
ACM International Conference on Web Search and
Data Mining (WSDM), pages 223?232.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 46?54.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma.
2004. Learning to cluster web search results. In
27th Annual International Conference on Research
and Development in Information Retrieval (SIGIR),
pages 210?217.
5
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153?158,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Post-Retrieval Clustering Using Third-Order Similarity Measures
Jose? G. Moreno
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
jose.moreno@unicaen.fr
Gae?l Dias
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
gael.dias@unicaen.fr
Guillaume Cleuziou
University of Orle?ans
LIFO
F-45067 Orle?ans, France
cleuziou@univ-orleans.fr
Abstract
Post-retrieval clustering is the task of clus-
tering Web search results. Within this
context, we propose a new methodology
that adapts the classical K-means algo-
rithm to a third-order similarity measure
initially developed for NLP tasks. Results
obtained with the definition of a new stop-
ping criterion over the ODP-239 and the
MORESQUE gold standard datasets evi-
dence that our proposal outperforms all re-
ported text-based approaches.
1 Introduction
Post-retrieval clustering (PRC), also known as
search results clustering or ephemeral clustering,
is the task of clustering Web search results. For
a given query, the retrieved Web snippets are au-
tomatically clustered and presented to the user
with meaningful labels in order to minimize the
information search process. This technique can
be particularly useful for polysemous queries but
it is hard to implement efficiently and effectively
(Carpineto et al, 2009). Indeed, as opposed to
classical text clustering, PRC must deal with small
collections of short text fragments (Web snippets)
and be processed in run-time.
As a consequence, most of the successful
methodologies follow a monothetic approach (Za-
mir and Etzioni, 1998; Ferragina and Gulli, 2008;
Carpineto and Romano, 2010; Navigli and Crisa-
fulli, 2010; Scaiella et al, 2012). The underlying
idea is to discover the most discriminant topical
words in the collection and group together Web
snippets containing these relevant terms. On the
other hand, the polythetic approach which main
idea is to represent Web snippets as word feature
vectors has received less attention, the only rele-
vant work being (Osinski and Weiss, 2005). The
main reasons for this situation are that (1) word
feature vectors are hard to define in small collec-
tions of short text fragments (Timonen, 2013), (2)
existing second-order similarity measures such as
the cosine are unadapted to capture the seman-
tic similarity between small texts, (3) Latent Se-
mantic Analysis has evidenced inconclusive re-
sults (Osinski and Weiss, 2005) and (4) the la-
beling process is a surprisingly hard extra task
(Carpineto et al, 2009).
This paper is motivated by the fact that the poly-
thetic approach should lead to improved results if
correctly applied to small collections of short text
fragments. For that purpose, we propose a new
methodology that adapts the classical K-means
algorithm to a third-order similarity measure ini-
tially developed for Topic Segmentation (Dias et
al., 2007). Moreover, the adapted K-means algo-
rithm allows to label each cluster directly from its
centroids thus avoiding the abovementioned extra
task. Finally, the evolution of the objective func-
tion of the adapted K-means is modeled to auto-
matically define the ?best? number of clusters.
Finally, we propose different experiments over
the ODP-239 (Carpineto and Romano, 2010)
and MORESQUE (Navigli and Crisafulli, 2010)
datasets against the most competitive text-based
PRC algorithms: STC (Zamir and Etzioni, 1998),
LINGO (Osinski and Weiss, 2005), OPTIMSRC
(Carpineto and Romano, 2010) and the classical
bisecting incremental K-means (which may be
seen as a baseline for the polythetic paradigm)1.
A new evaluation measure called the b-cubed F -
measure (Fb3) and defined in (Amigo? et al, 2009)
is then calculated to evaluate both cluster homo-
geneity and completeness. Results evidence that
our proposal outperforms all state-of-the-art ap-
proaches with a maximum Fb3 = 0.452 for ODP-
239 and Fb3 = 0.490 for MORESQUE.
1The TOPICAL algorithm proposed by (Scaiella et
al., 2012) is a knowledge-driven methodology based on
Wikipedia.
153
2 Polythetic Post-Retrieval Clustering
The K-means is a geometric clustering algorithm
(Lloyd, 1982). Given a set of n data points, the
algorithm uses a local search approach to partition
the points into K clusters. A set of K initial clus-
ter centers is chosen. Each point is then assigned
to the center closest to it and the centers are recom-
puted as centers of mass of their assigned points.
The process is repeated until convergence. To as-
sure convergence, an objective function Q is de-
fined which decreases at each processing step. The
classical objective function is defined in Equation
(1) where pik is a cluster labeled k, xi ? pik is
an object in the cluster, mpik is the centroid of the
cluster pik and E(., .) is the Euclidean distance.
Q =
K?
k=1
?
xi?pik
E(xi,mpik )2. (1)
Within the context of PRC, the K-means algo-
rithm needs to be adapted to integrate third-order
similarity measures (Mihalcea et al, 2006; Dias
et al, 2007). Third-order similarity measures,
also called weighted second-order similarity mea-
sures, do not rely on exact matches of word fea-
tures as classical second-order similarity measures
(e.g. the cosine metric), but rather evaluate simi-
larity based on related matches. In this paper, we
propose to use the third-order similarity measure
called InfoSimba introduced in (Dias et al, 2007)
for Topic Segmentation and implement its simpli-
fied version S3s in Equation 2.
S3s (Xi, Xj) =
1
p2
p?
k=1
p?
l=1
Xik ?Xjl ? S(Wik,Wjl). (2)
Given two Web snippets Xi and Xj , their sim-
ilarity is evaluated by the similarity of its con-
stituents based on any symmetric similarity mea-
sure S(., .) where Wik (resp. Wjl) corresponds to
the word at the kth (resp. lth) position in the vector
Xi (resp. Xj) and Xik (resp. Xjl) is the weight of
word Wik (resp. Wjl) in the set of retrieved Web
snippets. A direct consequence of the change in
similarity measure is the definition of a new ob-
jective function QS3s to ensure convergence. Thisfunction is defined in Equation (3) and must be
maximized2.
2A maximization process can easily be transformed into a
minimization one
QS3s =
K?
k=1
?
xi?pik
S3s (xi,mpik ). (3)
A cluster centroid mpik is defined by a vector of
p words (wpik1 , . . . , wpikp ). As a consequence, each
cluster centroid must be instantiated in such a way
that QS3s increases at each step of the clusteringprocess. The choice of the best p words repre-
senting each cluster is a way of assuring conver-
gence. For that purpose, we define a procedure
which consists in selecting the best p words from
the global vocabulary V in such a way that QS3sincreases. The global vocabulary is the set of all
words which appear in any context vector.
So, for each word w ? V and any symmet-
ric similarity measure S(., .), its interestingness
?k(w) is computed as regards to cluster pik. This
operation is defined in Equation (4) where si ? pik
is any Web snippet from cluster pik. Finally, the p
words with higher ?k(w) are selected to construct
the cluster centroid. In such a way, we can easily
prove that QS3s is maximized. Note that a wordwhich is not part of cluster pik may be part of the
centroid mpik .
?k(w) = 1p
?
si?pik
?
wiq?si
S(wiq, w). (4)
Finally, we propose to rely on a modified ver-
sion of the K-means algorithm called Global K-
means (Likasa et al, 2003), which has proved to
lead to improved results. To solve a clustering
problem with M clusters, all intermediate prob-
lems with 1, 2, ...,M ? 1 clusters are sequentially
solved. The underlying idea is that an optimal so-
lution for a clustering problem with M clusters
can be obtained using a series of local searches us-
ing the K-means algorithm. At each local search,
the M ? 1 cluster centers are always initially
placed at their optimal positions corresponding to
the clustering problem with M ? 1 clusters. The
remaining M th cluster center is initially placed at
several positions within the data space. In addi-
tion to effectiveness, the method is deterministic
and does not depend on any initial conditions or
empirically adjustable parameters. Moreover, its
adaptation to PRC is straightforward.
3 Stopping Criterion
Once clustering has been processed, selecting the
best number of clusters still remains to be decided.
154
For that purpose, numerous procedures have been
proposed (Milligan and Cooper, 1985). However,
none of the listed methods were effective or adapt-
able to our specific problem. So, we proposed
a procedure based on the definition of a ratio-
nal function which models the quality criterion
QS3s . To better understand the behaviour of QS3sat each step of the adapted GK-means algorithm,
we present its values for K = 10 in Figure 1.
Figure 1: QS3s and its modelisation.
QS3s can be modelled as in Equation (5) whichconverges to a limit ? whenK increases and starts
from Q1S3s (i.e. QS3s at K = 1). The underlyingidea is that the best number of clusters is given by
the ? value which maximizes the difference with
the average ?mean. So, ?, ? and ? need to be
expressed independently of unknown variables.
?K, f(K) = ?? ?K? . (5)
As ? can theoretically or operationally be de-
fined and it can easily be proved that ? = ??Q1S3s ,? needs to be defined based on ? or ?. This can
also be easily proved and the given result is ex-
pressed in Equation (6).
? =
log(??Q1S3s )? log(??Q
K
S3s
)
log(K) . (6)
Now, the value of ? which best approximates
the limit of the rational function must be defined.
For that purpose, we computed its maximum theo-
retical and experimental values as well as its ap-
proximated maximum experimental value based
on the ?2-Aitken (Aitken, 1926) procedure to ac-
celerate convergence as explained in (Kuroda et
al., 2008). Best results were obtained with the
maximum experimental value which is defined as
building the cluster centroid mpik for each Web
snippet individually. Finally, the best number of
clusters is defined as in Algorithm (1) and each
one receives its label based on the p words with
greater interestingness of its centroid mpik .
Algorithm 1 The best K selection procedure.
1. Calculate ?K for each K
2. Evaluate the mean of all ?K i.e. ?mean
3. Select ?K which maximizes ?K ? ?mean
4. Return K as the best number of partitions
This situation is illustrated in Figure (1) where
the red line corresponds to the rational functional
for ?mean and the blue line models the best ?
value (i.e. the one which maximizes the difference
with ?mean). In this case, the best number would
correspond to ?6 and as a consequence, the best
number of clusters would be 6. In order to illus-
trate the soundness of the procedure, we present
the different values for ? at each K iteration and
the differences between consecutive values of ? at
each iteration in Figure 2. We clearly see that the
highest inclination of the curve is between clus-
ter 5 and 6 which also corresponds to the highest
difference between two consecutive values of ?.
Figure 2: Values of ? (on the left) and differences
between consecutive values of ? (on the right).
4 Evaluation
Evaluating PRC systems is a difficult task as stated
in (Carpineto et al, 2009). Indeed, a successful
PRC system must evidence high quality level clus-
tering. Ideally, each query subtopic should be rep-
resented by a unique cluster containing all the rel-
evant Web pages inside. However, this task is far
from being achievable. As such, this constraint
is reformulated as follows: the task of PRC sys-
tems is to provide complete topical cluster cov-
erage of a given query, while avoiding excessive
155
Fb3 K Stop Criterion2 3 4 5 6 7 8 9 10 Fb3 Avg. K
SCP p
2 0.387 0.396 0.398 0.396 0.391 0.386 0.382 0.378 0.374 0.395 4.799
3 0.400 0.411 0.412 0.409 0.406 0.400 0.397 0.391 0.388 0.411 4.690
4 0.405 0.416 0.423 0.425 0.423 0.420 0.416 0.414 0.411 0.441 4.766
5 0.408 0.422 0.431 0.431 0.429 0.429 0.423 0.422 0.421 0.452 4.778
PMI p
2 0.391 0.399 0.397 0.393 0.388 0.383 0.377 0.373 0.366 0.393 4.778
3 0.408 0.418 0.422 0.418 0.414 0.410 0.405 0.398 0.392 0.416 4.879
4 0.420 0.434 0.439 0.439 0.435 0.430 0.425 0.420 0.412 0.436 4.874
5 0.423 0.444 0.451 0.451 0.451 0.445 0.441 0.434 0.429 0.450 4.778
Table 1: Fb3 for SCP and PMI for the global search and the stopping criterion for the ODP-239 dataset.
Adapated GK-means
STC LINGO BIK OPTIMSRCSCP PMI
ODP-239
p p
2 3 4 5 2 3 4 5
F1 0.312 0.341 0.352 0.366 0.332 0.358 0.378 0.390 0.324 0.273 0.200 0.313
F2 0.363 0.393 0.404 0.416 0.363 0.395 0.421 0.435 0.319 0.167 0.173 0.341
F5 0.411 0.441 0.453 0.462 0.390 0.430 0.459 0.476 0.322 0.153 0.165 0.380
Fb3 0.395 0.411 0.441 0.452 0.393 0.416 0.436 0,450 0.403 0.346 0.307 N/A
MORESQUE
F1 0.627 0.649 0.665 0.664 0.615 0.551 0.543 0.571 0.455 0.326 0.317 N/A
F2 0.685 0.733 0.767 0.770 0.644 0.548 0.521 0.551 0.392 0.260 0.269 N/A
F5 0.747 0.817 0.865 0.872 0.679 0.563 0.519 0.553 0.370 0.237 0.255 N/A
Fb3 0.482 0.482 0.473 0.464 0.490 0.465 0.462 0.485 0.460 0.399 0.315 N/A
Table 2: PRC comparative results for F? and Fb3 over the ODP-239 and MORESQUE datasets.
redundancy of the subtopics in the result list of
clusters. So, in order to evaluate our methodol-
ogy, we propose two different evaluations. First,
we want to evidence the quality of the stopping
criterion when compared to an exhaustive search
over all tunable parameters. Second, we propose a
comparative evaluation with existing state-of-the-
art algorithms over gold standard datasets and re-
cent clustering evaluation metrics.
4.1 Text Processing
Before the clustering process takes place, Web
snippets are represented as word feature vectors.
In order to define the set of word features, the
Web service proposed in (Machado et al, 2009) is
used3. In particular, it assigns a relevance score to
any token present in the set of retrieved Web snip-
pets based on the analysis of left and right token
contexts. A specific threshold is then applied to
withdraw irrelevant tokens and the remaining ones
form the vocabulary V . Then, each Web snippet is
represented by the set of its p most relevant to-
kens in the sense of the W (.) value proposed in
(Machado et al, 2009). Note that within the pro-
posed Web service, multiword units are also iden-
tified. They are exclusively composed of relevant
individual tokens and their weight is given by the
arithmetic mean of their constituents scores.
3Access to this Web service is available upon request.
4.2 Intrinsic Evaluation
The first set of experiments focuses on understand-
ing the behaviour of our methodology within a
greedy search strategy for different tunable param-
eters defined as a tuple < p,K, S(Wik,Wjl) >.
In particular, p is the size of the word feature vec-
tors representing both Web snippets and centroids
(p = 2..5), K is the number of clusters to be
found (K = 2..10) and S(Wik,Wjl) is the col-
location measure integrated in the InfoSimba sim-
ilarity measure. In these experiments, two asso-
ciation measures which are known to have dif-
ferent behaviours (Pecina and Schlesinger, 2006)
are tested. We implement the Symmetric Condi-
tional Probability (Silva et al, 1999) in Equation
(7) which tends to give more credits to frequent as-
sociations and the Pointwise Mutual Information
(Church and Hanks, 1990) in Equation (8) which
over-estimates infrequent associations. Then, best
< p,K, S(Wik,Wjl) > configurations are com-
pared to our stopping criterion.
SCP (Wik,Wjl) =
P (Wik,Wjl)2
P (Wik)? P (Wjl)
. (7)
PMI(Wik,Wjl) = log2
P (Wik,Wjl)
P (Wik)? P (Wjl)
. (8)
In order to perform this task, we evaluate per-
formance based on the Fb3 measure defined in
(Amigo? et al, 2009) over the ODP-239 gold stan-
dard dataset proposed in (Carpineto and Romano,
156
2010). In particular, (Amigo? et al, 2009) indi-
cate that common metrics such as the F?-measure
are good to assign higher scores to clusters with
high homogeneity, but fail to evaluate cluster com-
pleteness. First results are provided in Table 1 and
evidence that the best configurations for different
< p,K, S(Wik,Wjl) > tuples are obtained for
high values of p, K ranging from 4 to 6 clusters
and PMI steadily improving over SCP . How-
ever, such a fuzzy configuration is not satisfac-
tory. As such, we proposed a new stopping cri-
terion which evidences coherent results as it (1)
does not depend on the used association measure
(FSCPb3 = 0.452 and FPMIb3 = 0.450), (2) discov-ers similar numbers of clusters independently of
the length of the p-context vector and (3) increases
performance with high values of p.
4.3 Comparative Evaluation
The second evaluation aims to compare our
methodology to current state-of-the-art text-based
PRC algorithms. We propose comparative exper-
iments over two gold standard datasets (ODP-239
(Carpineto and Romano, 2010) and MORESQUE
(Di Marco and Navigli, 2013)) for STC (Za-
mir and Etzioni, 1998), LINGO (Osinski and
Weiss, 2005), OPTIMSRC (Carpineto and Ro-
mano, 2010) and the Bisecting Incremental K-
means (BIK) which may be seen as a baseline for
the polythetic paradigm. A brief description of
each PRC algorithm is given as follows.
STC: (Zamir and Etzioni, 1998) defined the
Suffix Tree Clustering algorithm which is still a
difficult standard to beat in the field. In partic-
ular, they propose a monothetic clustering tech-
nique which merges base clusters with high string
overlap. Indeed, instead of using the classical Vec-
tor Space Model (VSM) representation, they pro-
pose to represent Web snippets as compact tries.
LINGO: (Osinski and Weiss, 2005) proposed a
polythetic solution called LINGO which takes into
account the string representation proposed by (Za-
mir and Etzioni, 1998). They first extract frequent
phrases based on suffix-arrays. Then, they reduce
the term-document matrix (defined as a VSM) us-
ing Single Value Decomposition to discover latent
structures. Finally, they match group descriptions
with the extracted topics and assign relevant doc-
uments to them.
OPTIMSRC: (Carpineto and Romano, 2010)
showed that the characteristics of the outputs re-
turned by PRC algorithms suggest the adoption of
a meta clustering approach. As such, they intro-
duce a novel criterion to measure the concordance
of two partitions of objects into different clusters
based on the information content associated to the
series of decisions made by the partitions on single
pairs of objects. Then, the meta clustering phase
is casted to an optimization problem of the concor-
dance between the clustering combination and the
given set of clusterings.
With respect to implementation, we used the
Carrot2 APIs4 which are freely available for STC,
LINGO and the classical BIK. It is worth notic-
ing that all implementations in Carrot2 are tuned
to extract exactly 10 clusters. For OPTIMSRC,
we reproduced the results presented in the paper
of (Carpineto and Romano, 2010) as no imple-
mentation is freely available. The results are il-
lustrated in Table 2 including both F?-measure
and Fb3 . They evidence clear improvements of
our methodology when compared to state-of-the-
art text-based PRC algorithms, over both datasets
and all evaluation metrics. But more important,
even when the p-context vector is small (p = 3),
the adapted GK-means outperforms all other ex-
isting text-based PRC which is particularly impor-
tant as they need to perform in real-time.
5 Conclusions
In this paper, we proposed a new PRC ap-
proach which (1) is based on the adaptation of
the K-means algorithm to third-order similar-
ity measures and (2) proposes a coherent stop-
ping criterion. Results evidenced clear improve-
ments over the evaluated state-of-the-art text-
based approaches for two gold standard datasets.
Moreover, our best F1-measure over ODP-239
(0.390) approximates the highest ever-reached F1-
measure (0.413) by the TOPICAL knowledge-
driven algorithm proposed in (Scaiella et al,
2012)5. These results are promising and in future
works, we propose to define new knowledge-based
third-order similarity measures based on studies in
entity-linking (Ferragina and Scaiella, 2010).
4http://search.carrot2.org/stable/search [Last access:
15/05/2013].
5Notice that the authors only propose the F1-measure al-
though different results can be obtained for different F?-
measures and Fb3 as evidenced in Table 2.
157
References
A.C. Aitken. 1926. On bernoulli?s numerical solution
of algebraic equations. Research Society Edinburgh,
46:289?305.
E. Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Re-
trieval, 12(4):461?486.
C. Carpineto and G. Romano. 2010. Optimal meta
search results clustering. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 170?177.
C. Carpineto, S. Osinski, G. Romano, and D. Weiss.
2009. A survey of web clustering engines. ACM
Computer Survey, 41(3):1?38.
K. Church and P. Hanks. 1990. Word association
norms mutual information and lexicography. Com-
putational Linguistics, 16(1):23?29.
A. Di Marco and R. Navigli. 2013. Clustering and
diversifying web search results with graph-based
word sense induction. Computational Linguistics,
39(4):1?43.
G. Dias, E. Alves, and J.G.P. Lopes. 2007. Topic
segmentation algorithms for text summarization and
passage retrieval: An exhaustive evaluation. In Pro-
ceedings of 22nd Conference on Artificial Intelli-
gence (AAAI), pages 1334?1339.
P. Ferragina and A. Gulli. 2008. A personalized search
engine based on web-snippet hierarchical clustering.
Software: Practice and Experience, 38(2):189?225.
P. Ferragina and U. Scaiella. 2010. Tagme: On-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 1625?1628.
M. Kuroda, M. Sakakihara, and Z. Geng. 2008. Ac-
celeration of the em and ecm algorithms using the
aitken ?2 method for log-linear models with par-
tially classified data. Statistics & Probability Let-
ters, 78(15):2332?2338.
A. Likasa, Vlassis. N., and J. Verbeek. 2003.
The global k-means clustering algorithm. Pattern
Recognition, 36:451?461.
S.P. Lloyd. 1982. Least squares quantization in
pcm. IEEE Transactions on Information Theory,
28(2):129?137.
D. Machado, T. Barbosa, S. Pais, B. Martins, and
G. Dias. 2009. Universal mobile information re-
trieval. In Proceedings of the 5th International Con-
ference on Universal Access in Human-Computer
Interaction (HCI), pages 345?354.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the
21st National Conference on Artificial Intelligence
(AAAI), pages 775?780.
G.W. Milligan and M.C. Cooper. 1985. An exami-
nation of procedures for determining the number of
clusters in a data set. Psychometrika, 50(2):159?
179.
R. Navigli and G. Crisafulli. 2010. Inducing word
senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 116?126.
S. Osinski and D. Weiss. 2005. A concept-driven algo-
rithm for clustering search results. IEEE Intelligent
Systems, 20(3):48?54.
P. Pecina and P. Schlesinger. 2006. Combining as-
sociation measures for collocation extraction. In
Proceedings of the Joint Conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL), pages 651?658.
U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.
2012. Topical clustering of search results. In Pro-
ceedings of the 5th ACM International Conference
on Web Search and Data Mining (WSDM), pages
223?232.
J. Silva, G. Dias, S. Guillore?, and J.G.P. Lopes. 1999.
Using localmaxs algorithm for the extraction of con-
tiguous and non-contiguous multiword lexical units.
In Proceedings of 9th Portuguese Conference in Ar-
tificial Intelligence (EPIA), pages 113?132.
M. Timonen. 2013. Term Weighting in Short Docu-
ments for Document Categorization, Keyword Ex-
traction and Query Expansion. Ph.D. thesis, Uni-
versity of Helsinki, Finland.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 46?54.
158
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 305?308,
Dublin, Ireland, August 23-24, 2014.
HulTech: A General Purpose System for Cross-Level Semantic Similarity
based on Anchor Web Counts
Jose G. Moreno Rumen Moraliyski Asma Berrezoug Ga
?
el Dias
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
firstname.lastname@unicaen.fr
Abstract
This paper describes the HULTECH team par-
ticipation in Task 3 of SemEval-2014. Four
different subtasks are provided to the partici-
pants, who are asked to determine the semantic
similarity of cross-level test pairs: paragraph-
to-sentence, sentence-to-phrase, phrase-to-
word and word-to-sense. Our system adopts
a unified strategy (general purpose system) to
calculate similarity across all subtasks based
on word Web frequencies. For that purpose,
we define ClueWeb InfoSimba, a cross-level
similarity corpus-based metric. Results show
that our strategy overcomes the proposed base-
lines and achieves adequate to moderate re-
sults when compared to other systems.
1 Introduction
Similarity between text documents is considered a
challenging task. Recently, many works concentrate on
the study of semantic similarity for multi-level text doc-
uments (Pilehvar et al., 2013), but skipping the cross-
level similarity task. In the later, the underlying idea is
that text similarity can be considered between pairs of
text documents at different granularities levels: para-
graph, sentence, phrase or word. One obvious partic-
ularity of this task is that text pairs may not share the
same characteristics of size, context or structure, i.e.,
the granularity level.
In task 3 of SemEval-2014, two different strategies
have been proposed to solve this issue. On the one
hand, participants may propose a combination of indi-
vidual systems, each one solving a particular subtask.
On the other hand, a general purpose system may be
proposed, which deals with all the subtasks following
the exact same strategy.
In this paper, we describe a language-independent
corpus-based general purpose system, which relies on
a huge freely available Web collection called Anchor-
ClueWeb12 (Hiemstra and Hauff, 2010). In particular,
we calculate ClueWeb InfoSimba
1
a cross-level seman-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
It is a Web version of InfoSimba (Dias et al., 2007).
tic similarity based on word-word frequencies. Indeed,
these frequencies are captured by the use of a colloca-
tion metric called SCP
2
(Silva et al., 1999), which has
similar properties as the well studied PMI-IR (Turney,
2001) but does not over-evaluate rare events.
Our system outputs a normalized (between 0 and 1)
similarity value between two pieces of texts. However,
the subtasks proposed in task 3 of SemEval-2014 in-
clude a different scoring scale between 0 and 4. To
solve this issue, we applied linear, polynomial and ex-
ponential regressions as three different runs. Results
show that our strategy overcomes the proposed base-
lines and achieves adequate to moderate results when
compared to other systems.
2 System Description
Our system is based on a reduced version of the
ClueWeb12 dataset called Anchor ClueWeb12 and an
informative attributional similarity measure called In-
foSimba (Dias et al., 2007) adapted to this dataset.
2.1 Anchor ClueWeb12 Dataset
The Anchor ClueWeb12 dataset contains 0.5 billion
Web pages, which cover about 64% of the total num-
ber of Web pages in ClueWeb12. The particularity of
Anchor ClueWeb12 is that each Web page is repre-
sented by the anchor texts of the links pointing to it
in ClueWeb12. Web pages are indexed not on their
content but on their references. As such, the size of
the index is drastically reduced and the overall results
are consistent with full text indexing as discussed in
(Hiemstra and Hauff, 2010).
For development purposes, this dataset was indexed
in Solr 4.4 on a desktop computer using a batch in-
dexing script. Particularly, each compressed part file
of the Anchor ClueWeb12 was uncompressed, prepro-
cessed and indexed in a sequential way using the fea-
tures of incremental indexing offered by Solr (Smiley
and Pugh, 2009).
2.2 InfoSimba
In (Dias et al., 2007), the authors proposed the hypothe-
sis that two texts are similar if they share related (even-
tually different) constituents. So, their concept of simi-
2
Symmetric Conditional Probability.
305
larity is not any more based on the exact match of con-
stituents but relies on related constituents (e.g. words).
For example, it is clear that the following text pieces
extracted from the sentence-to-phrase subtask are re-
lated
3
although they do not share any word.
1. he is a nose-picker
2. an uncouth young man
The InfoSimba similarity measure models this phe-
nomenon evaluating individual similarities between all
possible words pairs. Indeed, each piece of text is rep-
resented by the vector of its words. So, given two
pieces of texts X
i
and X
j
, their similarity is defined
in Equation 1 where SCP (., .) is the Symmetric Con-
ditional Probability association measure proposed in
(Silva et al., 1999) and defined in Equation 2.
IS(X
i
, X
j
) =
1
pq
p
?
k=1
q
?
l=1
SCP (w
ik
, w
jl
). (1)
SCP (w
ik
, w
jl
) =
P (w
ik
, w
jl
)
2
P (w
ik
)? P (w
jl
)
. (2)
Following the previous example, the In-
foSimba value between the two vectors
X
1
= {?he?, ?is?, ?a?, ?nose-picker?} and
X
2
= {?an?, ?uncouth?, ?young?, ?man?} is
an average weight formed by all possible words pairs
associations as illustrated in Figure 1. Note that each
vertex is a word of a X
l
vector and each edge is
weighted by the SCP (., .) value of the connected
words. In particular, each w
ij
corresponds to the
word at the j
th
position in vector X
i
, P (., .) is the
joint probability of two words appearing in the same
document, P (.) is the marginal probability of any
word appearing in a document and p (resp. q) is the
size of the vector X
i
(resp. X
j
).
Figure 1: Pairs of words evaluated when InfoSimba is
calculated.
In the case of task 3 of SemEval-2014, each text
pair is represented by two word vectors for which a
modified version of InfoSimba, ClueWeb InfoSimba,
is computed.
3
The score of this pair (#85) in the training set is the max-
imum value 4.
2.3 ClueWeb InfoSimba
The final similarity metric, called ClueWeb InfoSimba
(CWIS), between two pieces of texts is defined in
Equation 3, where hits(w) returns the number of doc-
uments retrieved by Solr over Anchor ClueWeb12 for
the query w and hits(w
a
? w
b
) is the number of doc-
uments retrieved when both words are present simul-
taneously. In this case, SCP is modified into SCP-IR
similarly as PMI is to PMI-IR, i.e., using hits counts
instead of probability values (see Equation 4).
CWIS(X
i
, X
j
) =
1
pq
p
?
k=1
q
?
l=1
SCP ? IR(w
ik
, w
jl
).
(3)
SCP ? IR(w
ik
, w
jl
) =
hits(w
ik
? w
jl
)
2
hits(w
ik
).hits(w
jl
)
. (4)
2.4 System Input
The task 3 of SemEval-2014 consists of (1) paragraph-
to-sentence, (2) sentence-to-phrase, (3) phrase-to-word
and (4) word-to-sense subtasks. Before submitting the
pieces of texts to our system, we first performed simple
stop-words removal with the NLTK toolkit (Bird et al.,
2009). Note that in the case of the word-to-sense sub-
task, the similarity is performed over the word itself
and the gloss of the corresponding sense
4
.
2.5 Output Values Transformations
The CWIS(., .) similarity metric returns a value be-
tween 0 and 1. However, the subtasks suppose that
each pair must be attributed a score between 0 and 4.
As such, an adequate scale transformation must be per-
formed. For that purpose, we proposed linear, polyno-
mial and exponential regressions and submitted three
different runs, one for each regression
5
. Note that the
regressions have been tuned on the training dataset us-
ing the respective R regression functions with default
parameters:
? lm(y ? x),
? lm(y ? x + I(x
2
) + I(x
3
)),
? lm(log(y + ) ? x),
where 
6
is a small value included to avoid undefined
log values. The regression results on the test datasets
are presented in Figure 2.
4
Glosses are obtained from WordNet using the sense id
provided for the task by the organizers.
5
In the case of linear and exponential, these are mono-
thetic functions therefore ranking-based evaluation metrics
give the same score before and after this step.
6
In our experiments, this value was set to 0.001.
306
Figure 2: Linear, polynomial and exponential predic-
tions for the test dataset of the paragraph-to-sentence
subtask (colored dots). Black dots correspond to the
obtained ClueWeb InfoSimba value versus the manu-
ally assigned score in the training dataset.
3 Evaluation and Results
For evaluation purposes, two metrics have been se-
lected by the organizers: Pearson correlation (Pearson,
1895) and Spearman?s rank correlation (Hollander and
Wolfe, 1973). Detailed information about the evalu-
ation setup can be found in the task discussion paper
(Jurgens et al., 2014).
All results are given in Tables 1 and 2 for each
run. Note that the baseline metric is calculated for the
longest common string (LCS) and that each regression
has been tuned on the training dataset for each one of
the four tasks.
First, in almost all cases, the results outperform the
baseline. Second, performances show that with a cer-
tain amount of information (longer pieces of texts), in-
teresting results can be obtained. However, when the
size decreases, the performance diminishes and extra
information is certainly needed to better capture the se-
mantics between two pieces of text. Third, the poly-
nomial regression provides better results for the Pear-
son correlation evaluation, while for the Rho test, linear
and polynomial regressions get the lead. Note that this
situation depends on the data distribution and cannot
be seen as a conclusive remark. However, it is cer-
tainly an important subject of study for our unsuper-
vised methodology.
Another key point is that training examples were
used only for evaluation purposes
7
. In the case of
Spearman?s rank correlation, the linear and exponen-
7
For Pearson correlation, valid interval was fixed to [0,4].
tial transformations obviously show exact same values
(See Table 2).
4 Conclusions
In this paper, we proposed a general purpose system
to deal with cross-level text similarity. The aim of
our research was to push as far as possible the lim-
its of language-independent corpus-based solutions in
a general context of text similarity. We were also con-
cerned with reproducibility and as such we exclusively
used publicly available datasets and tools
8
. The results
clearly show the limits of a simple solution based on
word statistics. Nevertheless, the framework can easily
be empowered with the straightforward introduction of
more competitive resources.
Acknowledgement
The authors would like to thank the University of
Mostaganem (Algeria) for providing an internship to
Asma Berrezoug at the Normandie University.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc., 1st edition.
Ga?el Dias, Elsa Alves, and Jos?e Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text sum-
marization and passage retrieval: An exhaustive
evaluation. In Proceedings of AAAI, pages 1334?
1339.
Djoerd Hiemstra and Claudia Hauff. 2010. Mirex:
Mapreduce information retrieval experiments. In
CTIT Technical Report TR-CTIT-10-15, Centre for
Telematics and Information Technology, University
of Twente, pages 1?8.
Myles Hollander and Douglas A. Wolfe. 1973. Non-
parametric Statistical Methods. John Wiley and
Sons, New York.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Task 3: Cross-level seman-
tic similarity. In Proceedings of SemEval-2014.
Karl Pearson. 1895. Note on regression and inheri-
tance in the case of two parents. Proceedings of the
Royal Society of London, 58(347-352):240?242.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of ACL, pages 1341?
1351.
Joaquim Ferreira da Silva, Ga?el Dias, Sylvie Guillor?e,
and Jos?e Gabriel Pereira Lopes. 1999. Using local-
maxs algorithm for the extraction of contiguous and
8
Scripts to Index the Anchor ClueWeb12 Dataset are
available under request.
307
Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2Sense
Linear (run 3) 0.669 0.671 0.232 0.137
Polynomial (run 1) 0.693 0.665 0.254 0.150
Exponential (run 2) 0.667 0.633 0.180 0.169
Baseline (LCS) 0.527 0.562 0.165 0.109
Table 1: Overall results for the Pearson correlation.
Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2Sense
Linear (run 3) 0.688 0.633 0.260 0.124
Polynomial (run 1) 0.666 0.633 0.260 0.126
Exponential (run 2) 0.688 0.633 0.260 0.124
Baseline (LCS) 0.613 0.626 0.162 0.130
Table 2: Overall results for the Spearman?s rank correlation.
non-contiguous multiword lexical units. In Proceed-
ings of EPIA, pages 113?132.
David Smiley and Eric Pugh. 2009. Solr 1.4 Enter-
prise Search Server. Packt Publishing.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of ECML,
pages 491?502.
308
