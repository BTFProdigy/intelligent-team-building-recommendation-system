Lattice-Based Search for Spoken Utterance Retrieval
Murat Saraclar
AT&T Labs ? Research
180 Park Ave. Florham Park, NJ 07932
murat@research.att.com
Richard Sproat
University of Illinois at Urbana-Champaign
Urbana, IL 61801
rws@uiuc.edu
Abstract
Recent work on spoken document retrieval has
suggested that it is adequate to take the single-
best output of ASR, and perform text retrieval
on this output. This is reasonable enough for
the task of retrieving broadcast news stories,
where word error rates are relatively low, and
the stories are long enough to contain much
redundancy. But it is patently not reasonable
if one?s task is to retrieve a short snippet of
speech in a domain where WER?s can be as
high as 50%; such would be the situation with
teleconference speech, where one?s task is to
find if and when a participant uttered a certain
phrase.
In this paper we propose an indexing proce-
dure for spoken utterance retrieval that works
on lattices rather than just single-best text. We
demonstrate that this procedure can improve F
scores by over five points compared to single-
best retrieval on tasks with poor WER and low
redundancy. The representation is flexible so
that we can represent both word lattices, as
well as phone lattices, the latter being impor-
tant for improving performance when search-
ing for phrases containing OOV words.
1 Introduction
Automatic systems for indexing, archiving, searching and
browsing of large amounts of spoken communications
have become a reality in the last decade. Most such sys-
tems use an automatic speech recognition (ASR) compo-
nent to convert speech to text which is then used as an
input to a standard text based information retrieval (IR)
component. This strategy works reasonably well when
speech recognition output is mostly correct or the docu-
ments are long enough so that some occurrences of the
query terms are recognized correctly.
Most of the research has concentrated on retrieval of
Broadcast News type of spoken documents where speech
is relatively clean and the documents are relatively long.
In addition it is possible to find large amounts of text with
similar content in order to build better language models
and enhance retrieval through use of similar documents.
We are interested in extending this to telephone con-
versations and teleconferences. Our task is locating oc-
currences of a query in spoken communications to aid
browsing. This is not exactly spoken document retrieval.
In fact, it is more similar to word spotting. Each docu-
ment is a short segment of audio.
Although reasonable retrieval performance can be ob-
tained using the best ASR hypothesis for tasks with
moderate (? 20%) word error rates, tasks with higher
(40? 50%) word error rates require use of multiple ASR
hypotheses. Use of ASR lattices makes the system more
robust to recognition errors.
Almost all ASR systems have a closed vocabulary.
This restriction comes from run-time requirements as
well as the finite amount of data used for training the
language models of the ASR systems. Typically the
recognition vocabulary is taken to be the words appear-
ing in the language model training corpus. Sometimes
the vocabulary is further reduced to only include the
most frequent words in the corpus. The words that are
not in this closed vocabulary ? the out of vocabulary
(OOV) words ? will not be recognized by the ASR sys-
tem, contributing to recognition errors. The effects of
OOV words in spoken document retrieval are discussed
by Woodland et al (2000). Using phonetic search helps
retrieve OOV words.
This paper is organized as follows. In Section 2 we
give an overview of related work, focusing on methods
dealing with speech recognition errors and OOV queries.
We present the methods used in this study in Section 3.
Experimental setup and results are given in Section 4. Fi-
nally, our conclusions are presented in Section 5.
2 Related Work
There are commercial systems including Nexidia/Fast-
Talk (www.nexidia.com), Virage/AudioLogger
(www.virage.com), Convera (www.convera.com)
as well as research systems like AT&T DVL (Cox et
al., 1998), AT&T ScanMail (Hirschberg et al, 2001),
BBN Rough?n?Ready (Makhoul et al, 2000), CMU
Informedia (www.informedia.cs.cmu.edu),
SpeechBot (www.speechbot.com), among others.
Also between 1997 and 2000, the Test REtrieval Con-
ference (TREC) had a spoken document retrieval (SDR)
track with many participants (Garofolo et al, 2000).
NIST TREC-9 SDR Web Site (2000) states that:
The results of the TREC-9 2000 SDR eval-
uation presented at TREC on November 14,
2000 showed that retrieval performance for
sites on their own recognizer transcripts was
virtually the same as their performance on the
human reference transcripts. Therefore, re-
trieval of excerpts from broadcast news using
automatic speech recognition for transcription
was deemed to be a solved problem - even with
word error rates of 30%.
PhD Theses written on this topic include James (1995),
Wechsler (1998), Siegler (1999) and Ng (2000).
Jones et al (1996) describe a system that com-
bines a large vocabulary continuous speech recognition
(LVCSR) system and a phone-lattice word spotter (WS)
for retrieval of voice and video mail messages (Brown
et al, 1996). Witbrock and Hauptmann (1997) present
a system where a phonetic transcript is obtained from
the word transcript and retrieval is performed using
both word and phone indices. Wechsler et al (1998)
present new techniques including a new method to
detect occurrences of query features, a new method
to estimate occurrence probabilities, a collection-wide
probability re-estimation technique and feature length
weighting. Srinivasan and Petkovic (2000) introduce a
method for phonetic retrieval based on the probabilis-
tic formulation of term weighting using phone confu-
sion data. Amir et al (2001) use indexing based on con-
fusable phone groups and a Bayesian phonetic edit dis-
tance for phonetic speech retrieval. Logan et al (2002)
compare three indexing methods based on words,
syllable-like particles, and phonemes to study the
problem of OOV queries in audio indexing systems.
Logan and Van Thong (2002) give an alternate approach
to the OOV query problem by expanding query words
into in-vocabulary phrases while taking acoustic confus-
ability and language model scores into account.
Of the previous work, the most similar approach to the
one proposed here is that of Jones et al (1996), in that
they used phone lattices to aid in word spotting, in ad-
dition to single-best output from LVCSR. Our proposal
might be thought of as a generalization of their approach
in that we use lattices as the sole representation over
which retrieval is performed. We believe that lattices are
a more natural representation for retrieval in cases where
there is a high degree of uncertainty about what was said,
which is typically the case in LVCSR systems for con-
versational speech. We feel that our results, presented
below, bear out this belief. Also novel in our approach is
the use of indexed lattices allowing for efficient retrieval.
As we note below, in the limit where one is using one-best
output, the indexed lattices reduce to the normal inverted
index used in text retrieval.
3 Methods
In this section we describe the overall structure of our
system and give details of the techniques used in our
investigations. The system consists of three main com-
ponents. First, the ASR component is used to convert
speech into a lattice representation, together with timing
information. Second, this representation is indexed for
efficient retrieval. These two steps are performed off-line.
Finally, when the user enters a query the index is searched
and matching audio segments are returned.
3.1 Automatic Speech Recognition
We use a state-of-the-art HMM based large vocabulary
continuous speech recognition (LVCSR) system. The
acoustic models consist of decision tree state clustered
triphones and the output distributions are mixtures of
Gaussians. The language models are pruned backoff tri-
gram models. The pronunciation dictionaries contain few
alternative pronunciations. Pronunciations that are not
in our baseline pronunciation dictionary (including OOV
query words) are generated using a text-to-speech (TTS)
frontend. The TTS frontend can produce multiple pro-
nunciations. The ASR systems used in this study are
single pass systems. The recognition networks are rep-
resented as weighted finite state machines (FSMs).
The output of the ASR system is also represented as an
FSM and may be in the form of a best hypothesis string
or a lattice of alternate hypotheses. The labels on the arcs
of the FSM may be words or phones, and the conversion
between the two can easily be done using FSM composi-
tion. The costs on the arcs are negative log likelihoods.
Additionally, timing information can also be present in
the output.
3.2 Lattice Indexing and Retrieval
In the case of lattices, we store a set of indices, one for
each arc label (word or phone) l, that records the lat-
tice number L[a], input-state k[a] of each arc a labeled
with l in each lattice, along with the probability mass
f(k[a]) leading to that state, the probability of the arc
itself p(a|k[a]) and an index for the next state. To re-
trieve a single label from a set of lattices representing a
speech corpus one simply retrieves all arcs in each lattice
from the label index. The lattices are first normalized by
weight pushing (Mohri et al, 2002) so that the probabil-
ity of the set of all paths leading from the arc to the final
state is 1. After weight pushing, for a given arc a, the
probability of the set of all paths containing that arc is
given by
p(a) =
?
pi?L:a?pi
p(pi) = f(k[a])p(a|k[a])
namely the probability of all paths leading into that arc,
multiplied by the probability of the arc itself. For a lattice
L we construct a ?count? C(l|L) for a given label l using
the information stored in the index I(l) as follows,
C(l|L) =
?
pi?L
p(pi)C(l|pi)
=
?
pi?L
(
p(pi)
?
a?pi
?(a, l)
)
=
?
a?L
(
?(a, l)
?
pi?L:a?pi
p(pi)
)
=
?
a?I(l):L[a]=L
p(a)
=
?
a?I(l):L[a]=L
f(k[a])p(a|k[a])
where C(l|pi) is the number of times l is seen on path pi
and ?(a, l) is 1 if arc a has the label l and 0 otherwise. Re-
trieval can be thresholded so that matches below a certain
count are not returned.
To search a multilabel expression (e.g. a multi-
word phrase) w1w2 . . . wn we seek on each label in
the expression, and then for each (wi, wi+1) join the
output states of wi with the matching input states of
wi+1; in this way we retrieve just those path seg-
ments in each lattice that match the entire multi-label
expression. The probability of each match is de-
fined as f(k[a1])p(a1|k[a1])p(a2|k[a2]) . . . p(an|k[an]),
where p(ai|k[ai]) is the probability of the ith arc in the
expression starting in arc a1. The total ?count? for the
lattice is computed as defined above.
Note that in the limit case where each lattice is an un-
weighted single path ? i.e. a string of labels ? the above
scheme reduces to a standard inverted index.
The count C(l|L) can be interpreted as a lattice-based
confidence measure. Although it may be possible to use
more sophisticated confidence measures, use of (poste-
rior) probabilities allows for a simple factorization which
makes indexing efficient.
3.3 Indexing Using Sub-word Units
In order to deal with queries that contain OOV words we
investigate the use of sub-word units for indexing. In this
study we use phones as the sub-word units. There are two
methods for obtaining phonetic representation of an input
utterance.
1. Phone recognition using an ASR system where
recognition units are phones. This is achieved by
using a phone level language model instead of the
word level language model used in the baseline ASR
system.
2. Converting the word level representation of the ut-
terance into a phone level representation. This is
achieved by using the baseline ASR system and re-
placing each word in the output by its pronuncia-
tion(s) in terms of phones.
Both methods have their shortcomings. Phone recogni-
tion is known to be less accurate than word recognition.
On the other hand, the second method can only generate
phone strings that are substrings of the pronunciations of
in-vocabulary word strings. An alternative is to use hy-
brid language models used for OOV word detection (Yaz-
gan and Saraclar, 2004).
For retrieval, each query word is converted into phone
string(s) by using its pronunciation(s). The phone index
can then be searched for each phone string. Note that this
approach will generate many false alarms, particularly for
short query words, which are likely to be substrings of
longer words. In order to control for this a bound on min-
imum pronunciation length can be utilized. Since most
short words are in vocabulary this bound has little effect
on recall.
3.4 Using Both Word and Sub-word Indices
Given a word index and a sub-word index, it is possible to
improve the retrieval performance of the system by using
both indices. There are many strategies for doing this.
1. combination:
Search both the word index and the sub-word index,
combine the results.
2. vocabulary cascade:
Search the word index for in-vocabulary queries,
search the sub-word index for OOV queries.
3. search cascade:
Search the word index,
if no result is returned search the sub-word index.
In the first case, if the indices are obtained from ASR
best hypotheses, then the result combination is a simple
union of the separate sets of results. However, if indices
are obtained from lattices, then in addition to taking a
union of results, retrieval can be done using a combined
score. Given a query q, let Cw(q) and Cp(q) be the lattice
counts obtained from the word index and the phone index
respectively. We also define the normalized lattice count
for the phone index as
Cnormp (q) = (Cp(q))
1
|pron(q)|
where |pron(q)| is the length of the pronunciation of
query q. We then define the combined score to be
Cwp(q) = Cw(q) + ?C
norm
p (q)
where ? is an empirically determined scaling factor.
In the other cases, instead of using two different thresh-
olds we use a single threshold on Cw(q) and Cnormp (q)
during retrieval.
4 Experiments
4.1 Evaluation Metrics
For evaluating ASR performance we use the standard
word error rate (WER) as our metric. Since we are in-
terested in retrieval we use OOV rate by type to measure
the OOV word characteristics. For evaluating retrieval
performance we use precision and recall with respect to
manual transcriptions. Let Correct(q) be the number of
times the query q is found correctly, Answer(q) be the
number of answers to the query q, and Reference(q) be
the number of times q is found in the reference.
Precision(q) =
Correct(q)
Answer(q)
Recall(q) =
Correct(q)
Reference(q)
We compute precision and recall rates for each query and
report the average over all queries. The set of queries Q
consists of all the words seen in the reference except for
a stoplist of 100 most common words. The measurement
is not weighted by frequency ? i.e. each query q ? Q
is presented to the system only once, independent of the
number of occurences of q in the transcriptions.
Precision =
1
|Q|
?
q?Q
Precision(q)
Recall =
1
|Q|
?
q?Q
Recall(q)
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve.
In addition to individual precision-recall values we
also compute the F-measure defined as
F =
2? Precision? Recall
Precision + Recall
and report the maximum F-measure (maxF) to summa-
rize the information in a precision-recall curve.
4.2 Corpora
We use three different corpora to assess the effectiveness
of different retrieval techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types. For ASR we use a real-time system (Saraclar
et al, 2002). Since the system was designed for SDR,
the recognition vocabulary of the system has over 200K
words. The pronunciation dictionary has 1.25 pronuncia-
tions per word.
The second corpus is the Switchboard corpus consist-
ing of two party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types. For ASR we use the first pass of the evalua-
tion system (Ljolje et al, 2002). The recognition vocab-
ulary of the system has over 45K words. For these words
the average number of pronunciations per word is 1.07.
The third corpus is named Teleconferences since it con-
sists of multiparty teleconferences on various topics. The
audio from the legs of the conference are summed and
recorded as a single channel. A test set of six telecon-
ferences (about 3.5 hours) was transcribed. It contains
31106 word tokens and 2779 word types. Calls are auto-
matically segmented into a total of 1157 segments prior
to ASR, using an algorithm that detects changes in the
acoustics. We again use the first pass of the Switchboard
evaluation system for ASR.
In Table 1 we present the ASR performance on these
three tasks as well as the OOV Rate by type of the cor-
pora. It is important to note that the recognition vocab-
ulary for the Switchboard and Teleconferences tasks are
the same and no data from the Teleconferences task was
used while building the ASR systems. The mismatch be-
tween the Teleconference data and the models trained on
the Switchboard corpus contributes to the significant in-
crease in WER.
4.3 Using ASR Best Word Hypotheses
As a baseline, we use the best word hypotheses of the
ASR system for indexing and retrieval. The performance
Task WER OOV Rate by Type
Broadcast News ?20% 0.6%
Switchboard ?40% 6%
Teleconferences ?50% 12%
Table 1: Word Error Rate (WER) and OOV Rate (by
type) of various LVCSR tasks
of this baseline system is given in Table 2. As ex-
pected, we obtain very good performance on the Broad-
cast News corpus. It is interesting to note that when mov-
ing from Switchboard to Teleconferences the degradation
in precision-recall is the same as the degradation in WER.
Task WER Precision Recall
Broadcast News ?20% 92% 77%
Switchboard ?40% 74% 47%
Teleconferences ?50% 65% 37%
Table 2: Precision Recall for ASR 1-best
4.4 Using ASR Word Lattices
In the second set of experiments we investigate the use
of ASR word lattices. In order to reduce storage require-
ments, lattices can be pruned to contain only the paths
whose costs (i.e. negative log likelihood) are within a
threshold with respect to the best path. The smaller this
cost threshold is, the smaller the lattices and the index
files are. In Figure 1 we present the precision-recall
curves for different pruning thresholds on the Telecon-
ferences task.
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall on Teleconferences
Precision
Reca
ll
1?best word hypothesisword latticesword lattices (prune=6)word lattices (prune=4)word lattices (prune=2)
Figure 1: Precision Recall using word lattices for tele-
conferences
In Table 3 the resulting index sizes and maximum F-
measure values are given. On the teleconferences task we
observed that cost=6 yields good results, and used this
value for the rest of the experiments. Note that this in-
creases the index size with respect to the ASR 1-best case
by 3 times for Broadcast News, by 5 times for Switch-
board and by 9 times for Teleconferences.
Task Pruning Size (MB) maxF
Broadcast News nbest=1 29 84.0
Broadcast News cost=6 91 84.8
Switchboard nbest=1 18 57.1
Switchboard cost=6 90 58.4
Teleconferences nbest=1 16 47.4
Teleconferences cost=2 29 49.5
Teleconferences cost=4 62 50.0
Teleconferences cost=6 142 50.3
Teleconferences cost=12 3100 50.1
Table 3: Comparison of index sizes
4.5 Using ASR Phone Lattices
Next, we compare using the two methods of phonetic
transcription discussed in Section 3.3 ? phone recogni-
tion and word-to-phone conversion ? for retrieval using
only phone lattices. In Table 4 the precision and recall
values that yield the maximum F-measure as well as the
maximum F-measure values are presented. These results
clearly indicate that phone recognition is inferior for our
purposes.
Source for Indexing Precision Recall maxF
Phone Recognition 25.6 37.3 30.4
Conversion from Words 43.1 48.5 45.6
Table 4: Comparison of different sources for the phone
index on the Teleconferences corpus
4.6 Using ASR Word and Phone Lattices
We investigated using the strategies mentioned in Sec-
tion 3.4, and found strategy 3 ? search the word index, if
no result is returned search the phone index ? to be su-
perior to others. We give a comparison of the maximum
F-values for the three strategies in Table 5.
Strategy maxF
1.combination 50.5
2.vocabulary cascade 51.0
3.search cascade 52.8
Table 5: Comparison of different strategies for using
word and phone indices
In Figure 2 we present results for this strategy on the
Teleconferences corpus. The phone indices used in these
experiments were obtained by converting the word lat-
tices into phone lattices. Using the phone indices ob-
tained by phone recognition gave significantly worse re-
sults.
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall on Teleconferences
Precision
Reca
ll
1?best word hypothesisword latticesword and phone lattices
Figure 2: Comparison of word lattices and word/phone
hybrid strategies for teleconferences
4.7 Effect of Minimum Pronunciation Length for
Queries
When searching for words with short pronunciations in
the phone index the system will produce many false
alarms. One way of reducing the number of false alarms
is to disallow queries with short pronunciations. In Fig-
ure 3 we show the effect of imposing a minimum pronun-
ciation length for queries. For a query to be answered its
pronunciation has to have more than minphone phones,
otherwise no answers are returned. Best maximum F-
measure result is obtained using minphone=3.
4.8 Effects of Recognition Vocabulary Size
In Figure 4 we present results for different recognition
vocabulary sizes (5k, 20k, 45k) on the Switchboard cor-
pus. The OOV rates by type are 32%, 10% and 6% re-
spectively. The word error rates are 41.5%, 40.1% and
40.1% respectively. The precision recall curves are al-
most the same for 20k and 45k vocabulary sizes.
4.9 Using Word Pair Queries
So far, in all the experiments the query list consisted of
single words. In order to observe the behavior of various
methods when faced with longer queries we used a set of
0 20 40 60 80 1000
20
40
60
80
100 Effect of Minimum Pronunciation Length
Precision
Reca
ll
1?best word hypothesisword latticesminphone=0minphone=3minphone=5
Figure 3: Effect of minimum pronunciation length using
a word/phone hybrid strategy for teleconferences
word pair queries. Instead of using all the word pairs seen
in the reference transcriptions, we chose the ones which
were more likely to occur together than with other words.
For this, we sorted the word pairs (w1, w2) according to
their pointwise mutual information
log
p(w1, w2)
p(w1)p(w2)
and used the top pairs as queries in our experiments. Note
that in these experiments only the query set is changed
and the indices remain the same as before.
As it turns out, the precision of the system is very high
on this type of queries. For this reason, it is more in-
teresting to look at the operating point that achieves the
maximum F-measure for each technique, which in this
case coincides with the point that yields the highest re-
call. In Table 6 we present results on the Switchboard
corpus using 1004 word pair queries. Using word lat-
tices it is possible to increase the recall of the system by
16.4% while degrading the precision by only 2.2%. Us-
ing phone lattices we can get another 3.7% increase in
recall for 1.2% loss in precision. The final system still
has 95% precision.
System Precision Recall maxF
Word 1-best 98.3 29.7 45.6
Word lattices 96.1 46.1 62.3
Word+Phone lattices 94.9 49.8 65.4
Table 6: Results for word pair queries on Switchboard
0 20 40 60 80 1000
20
40
60
80
100 Effect of Recognition Vocabulary Size
Precision
Reca
ll
word (45k)word (20k)word (5k)word+phone (45k)word+phone (20k)word+phone (5k)
Figure 4: Comparison of various recognition vocabulary
sizes for Switchboard
4.10 Summary of Results on Different Corpora
Finally, we make a comparison of various techniques on
different tasks. In Table 7 maximum F-measure (maxF)
is given. Using word lattices yields a relative gain of 3-
5% in maxF over using best word hypotheses. For the
final system that uses both word and phone lattices, the
relative gain over the baseline increases to 8-12%.
Task System
1-best W Lats W+P Lats
Broadcast News 84.0 84.8 86.0
Switchboard 57.1 58.4 60.5
Teleconferences 47.4 50.3 52.8
Table 7: Maximum F-measure for various systems and
tasks
In Figure 5 we present the precision recall curves.
The gain from using better techniques utilizing word
and phone lattices increases as retrieval performance gets
worse.
5 Conclusion
We proposed an indexing procedure for spoken utter-
ance retrieval that works on ASR lattices rather than just
single-best text. We demonstrated that this procedure can
improve maximum F-measure by over five points com-
pared to single-best retrieval on tasks with poor WER
and low redundancy. The representation is flexible so
that we can represent both word lattices, as well as phone
lattices, the latter being important for improving per-
formance when searching for phrases containing OOV
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall Comparison
Precision
Reca
ll
Teleconferences
SwitchboardBroadcast News
Figure 5: Precision Recall for various techniques on dif-
ferent tasks. The tasks are Broadcast News (+), Switch-
board (x), and Teleconferences (o). The techniques are
using best word hypotheses (single points), using word
lattices (solid lines), and using word and phone lattices
(dashed lines).
words. It is important to note that spoken utterance re-
trieval for conversational speech has different properties
than spoken document retrieval for broadcast news. Al-
though consistent improvements were observed on a va-
riety of tasks including Broadcast News, the procedure
proposed here is most beneficial for more difficult con-
versational speech tasks like Switchboard and Telecon-
ferences.
References
A. Amir, A. Efrat, and S. Srinivasan. 2001. Advances
in phonetic word spotting. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management, pages 580?582, Atlanta, Georgia,
USA.
M. G. Brown, J. T. Foote, G. J. F. Jones, K. Sparck Jones,
and S. J. Young. 1996. Open-vocabulary speech in-
dexing for voice and video mail retrieval. In Proc.
ACM Multimedia 96, pages 307?316, Boston, Novem-
ber.
R. V. Cox, B. Haskell, Y. LeCun, B. Shahraray, and L. Ra-
biner. 1998. On the application of multimedia pro-
cessing to telecommunications. Proceedings of the
IEEE, 86(5):755?824, May.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of the Recherche d?Informations
Assiste par Ordinateur: Content Based Multimedia In-
formation Access Conference.
J. Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and
G. Zamchick. 2001. Scanmail: Browsing and search-
ing speech data by content. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), Aalborg, Denmark.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph.D. thesis, University of Cambridge, Down-
ing College.
G. J. F. Jones, J. T. Foote, K. Sparck Jones, and S. J.
Young. 1996. Retrieving spoken documents by com-
bining multiple index sources. In Proc. SIGIR 96,
pages 30?38, Zu?rich, August.
A. Ljolje, M. Saraclar, M. Bacchiani, M. Collins, and
B. Roark. 2002. The AT&T RT-02 STT system. In
Proc. RT02 Workshop, Vienna, Virginia.
B. Logan and JM Van Thong. 2002. Confusion-based
query expansion for OOV words in spoken document
retrieval. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, Colorado, USA.
B. Logan, P. Moreno, and O. Deshmukh. 2002. Word
and sub-word indexing approaches for reducing the ef-
fects of OOV queries on spoken audio. In Proc. HLT.
J. Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,
R. Schwartz, and A. Srivastava. 2000. Speech and
language technologies for audio indexing and retrieval.
Proceedings of the IEEE, 88(8):1338?1353, August.
M. Mohri, F. Pereira, and M. Riley. 2002. Weighted
finite-state transducers in speech recognition. Com-
puter Speech and Language, 16(1):69?88.
Kenney Ng. 2000. Subword-Based Approaches for Spo-
ken Document Retrieval. Ph.D. thesis, Massachusetts
Institute of Technology.
NIST TREC-9 SDR Web Site. 2000.
www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm.
M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002.
Towards automatic closed captioning: Low latency
real time broadcast news transcription. In Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), Denver, Colorado, USA.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrieval for Mu-
tually Optimal Performance. Ph.D. thesis, Carnegie
Mellon University.
S. Srinivasan and D. Petkovic. 2000. Phonetic confu-
sion matrix based spoken document retrieval. In Pro-
ceedings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 81?87.
M. Wechsler, E. Munteanu, and P. Sca?uble. 1998. New
techniques for open-vocabulary spoken document re-
trieval. In Proceedings of the 21st Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 20?27, Mel-
bourne, Australia.
Martin Wechsler. 1998. Spoken Document Retrieval
Based on Phoneme Recognition. Ph.D. thesis, Swiss
Federal Institute of Technology (ETH), Zurich.
M. Witbrock and A. Hauptmann. 1997. Using words and
phonetic strings for efficient information retrieval from
imperfectly transcribed spoken documents. In 2nd
ACM International Conference on Digital Libraries
(DL?97), pages 30?35, Philadelphia, PA, July.
P.C. Woodland, S.E. Johnson, P. Jourlin, and K.Sparck
Jones. 2000. Effects of out of vocabulary words
in spoken document retrieval. In Proc. SIGIR, pages
372?374, Athens, Greece.
A. Yazgan and M. Saraclar. 2004. Hybrid language mod-
els for out of vocabulary word detection in large vocab-
ulary conversational speech recognition. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), Mon-
treal, Canada.
Language model adaptation with MAP estimation
and the perceptron algorithm
Michiel Bacchiani, Brian Roark and Murat Saraclar
AT&T Labs-Research, 180 Park Ave., Florham Park, NJ 07932, USA
{michiel,roark,murat}@research.att.com
Abstract
In this paper, we contrast two language model
adaptation approaches: MAP estimation and
the perceptron algorithm. Used in isolation, we
show that MAP estimation outperforms the lat-
ter approach, for reasons which argue for com-
bining the two approaches. When combined,
the resulting system provides a 0.7 percent ab-
solute reduction in word error rate over MAP
estimation alone. In addition, we demonstrate
that, in a multi-pass recognition scenario, it is
better to use the perceptron algorithm on early
pass word lattices, since the improved error rate
improves acoustic model adaptation.
1 Introduction
Most common approaches to language model adapta-
tion, such as count merging and model interpolation, are
special cases of maximum a posteriori (MAP) estima-
tion (Bacchiani and Roark, 2003). In essence, these ap-
proaches involve beginning from a smoothed language
model trained on out-of-domain observations, and adjust-
ing the model parameters based on in-domain observa-
tions. The approach ensures convergence, in the limit, to
the maximum likelihood model of the in-domain obser-
vations. The more in-domain observations, the less the
out-of-domain model is relied upon. In this approach, the
main idea is to change the out-of-domain model parame-
ters to match the in-domain distribution.
Another approach to language model adaptation would
be to change model parameters to correct the errors
made by the out-of-domain model on the in-domain data
through discriminative training. In such an approach,
the baseline recognizer would be used to recognize in-
domain utterances, and the parameters of the model ad-
justed to minimize recognition errors. Discriminative
training has been used for language modeling, using vari-
ous estimation techniques (Stolcke and Weintraub, 1998;
Roark et al, 2004), but language model adaptation to
novel domains is a particularly attractive scenario for dis-
criminative training, for reasons we discuss next.
A key requirement for discriminative modeling ap-
proaches is training data produced under conditions that
are close to testing conditions. For example, (Roark et al,
2004) showed that excluding an utterance from the lan-
guage model training corpus of the baseline model used
to recognize that utterance is essential to getting word
error rate (WER) improvements with the perceptron al-
gorithm in the Switchboard domain. In that paper, 28
different language models were built, each omitting one
of 28 sections, for use in generating word lattices for the
omitted section. Without removing the section, no benefit
was had from models built with the perceptron algorithm;
with removal, the approach yielded a solid improvement.
More time consuming is controlling acoustic model train-
ing. For a task such as Switchboard, on which the above
citation was evaluated, acoustic model estimation is ex-
pensive. Hence building multiple models, omitting var-
ious subsections is a substantial undertaking, especially
when discriminative estimation techniques are used.
Language model adaptation to a new domain, how-
ever, can dramatically simplify the issue of controlling
the baseline model for producing discriminative training
data, since the in-domain training data is not used for
building the baseline models. The purpose of this paper is
to compare a particular discriminative approach, the per-
ceptron algorithm, which has been successfully applied
in the Switchboard domain, with MAP estimation, for
adapting a language model to a novel domain. In addi-
tion, since the MAP and perceptron approaches optimize
different objectives, we investigate the benefit from com-
bination of these approaches within a multi-pass recogni-
tion system.
The task that we focus upon, adaptation of a general
voicemail recognition language model to a customer ser-
vice domain, has been shown to benefit greatly from
MAP estimation (Bacchiani and Roark, 2003). It is an
attractive test for studying language model adaptation,
since the out-of-domain acoustic model is matched to
the new domain, and the domain shift does not raise the
OOV rate significantly. Using 17 hours of in-domain
observations, versus 100 hours of out-of-domain utter-
ances, (Bacchiani and Roark, 2003) reported a reduction
in WER from 28.0% using the baseline system to 20.3%
with the best performing MAP adapted model. In this pa-
per, our best scenario, which uses MAP adaptation and
the perceptron algorithm in combination, achieves an ad-
ditional 0.7% reduction, to 19.6% WER.
The rest of the paper is structured as follows. In the
next section, we provide a brief background for both
MAP estimation and the perceptron algorithm. This is
followed by an experimental results section, in which we
present the performance of each approach in isolation, as
well as several ways of combining them.
2 Background
2.1 MAP language model adaptation
To build an adapted n-gram model, we use a count
merging approach, much as presented in (Bacchiani and
Roark, 2003), which is shown to be a special case of max-
imum a posteriori (MAP) adaptation. Let wO be the out-
of-domain corpus, and wI be the in-domain sample. Let
h represent an n-gram history of zero or more words. Let
ck(hw) denote the raw count of an n-gram hw in wk,
for k ? {O, I}. Let p?k(hw) denote the standard Katz
backoff model estimate of hw given wk. We define the
corrected count of an n-gram hw as:
c?k(hw) = |wk| p?k(hw) (1)
where |wk| denotes the size of the sample wk. Then:
p?(w | h) =
?hc?O(hw) + c?I(hw)
?h
?
w? c?O(hw
?) +
?
w? c?I(hw
?)
(2)
where ?h is a state dependent parameter that dictates how
much the out-of-domain prior counts should be relied
upon. The model is then defined as:
p?(w | h) =
{
p?(w | h) if cO(hw) + cI(hw) > 0
?p?(w | h?) otherwise
(3)
where ? is the backoff weight and h? the backoff history
for history h.
The principal difficulty in MAP adaptation of this sort
is determining the mixing parameters ?h in Eq. 2. Follow-
ing (Bacchiani and Roark, 2003), we chose a single mix-
ing parameter for each model that we built, i.e. ?h = ?
for all states h in the model.
2.2 Perceptron algorithm
Our discriminative n-gram model training approach uses
the perceptron algorithm, as presented in (Roark et al,
2004), which follows the general approach presented in
(Collins, 2002). For brevity, we present the algorithm,
not in full generality, but for the specific case of n-gram
model training.
The training set consists of N weighted word lattices
produced by the baseline recognizer, and a gold-standard
transcription for each of the N lattices. Following (Roark
et al, 2004), we use the lowest WER hypothesis in the
lattice as the gold-standard, rather than the reference tran-
scription. The perceptron model is a linear model with k
feature weights, all of which are initialized to 0. The al-
gorithm is incremental, i.e. the parameters are updated at
each example utterance in the training set in turn, and the
updated parameters are used for the next utterance. Af-
ter each pass over the training set, the model is evaluated
on a held-out set, and the best performing model on this
held-out set is the model used for testing.
For a given path pi in a weighted word lattice L, let
w[pi] be the cost of that path as given by the baseline rec-
ognizer. Let GL be the gold-standard transcription for
L. Let ?(pi) be the K-dimensional feature vector for pi,
which contains the count within the path pi of each fea-
ture. In our case, these are unigram, bigram and trigram
feature counts. Let ??t ? RK be the K-dimensional fea-
ture weight vector of the perceptron model at time t. The
perceptron model feature weights are updated as follows
1. For the example lattice L at time t, find p?it such that
p?it = argmin
pi?L
(w[pi] + ??(pi) ? ??t) (4)
where ? is a scaling constant.
2. For the 0 ? k ? K features in the feature weight
vector ??t,
??t+1[k] = ??t[k] + ?(p?it)[k] ? ?(GL)[k] (5)
Note that if p?it = GL, then the features are left un-
changed.
As shown in (Roark et al, 2004), the perceptron fea-
ture weight vector can be encoded in a deterministic
weighted finite state automaton (FSA), so that much of
the feature weight update involves basic FSA operations,
making the training relatively efficient in practice. As
suggested in (Collins, 2002), we use the averaged per-
ceptron when applying the model to held-out or test data.
After each pass over the training data, the averaged per-
ceptron model is output as a weighted FSA, which can be
used by intersecting with a lattice output from the base-
line system.
3 Experimental Results
We evaluated the language model adaptation algorithms
by measuring the transcription accuracy of an adapted
voicemail transcription system on voicemail messages re-
ceived at a customer care line of a telecommunications
network center. The initial voicemail system, named
Scanmail, was trained on general voicemail messages
collected from the mailboxes of people at our research
site in Florham Park, NJ. The target domain is also com-
posed of voicemail messages, but for a mailbox that re-
ceives messages from customer care agents regarding
network outages. In contrast to the general voicemail
messages from the training corpus of the Scanmail sys-
tem, the messages from the target domain, named SS-
NIFR, will be focused solely on network related prob-
lems. It contains frequent mention of various network
related acronyms and trouble ticket numbers, rarely (if at
all) found in the training corpus of the Scanmail system.
To evaluate the transcription accuracy, we used a multi-
pass speech recognition system that employs various
unsupervised speaker and channel normalization tech-
niques. An initial search pass produces word-lattice out-
put that is used as the grammar in subsequent search
passes. The system is almost identical to the one de-
scribed in detail in (Bacchiani, 2001). The main differ-
ences in terms of the acoustic model of the system are
the use of linear discriminant analysis features; use of a
100 hour training set as opposed to a 60 hour training set;
and the modeling of the speaker gender which in this sys-
tem is identical to that described in (Woodland and Hain,
1998). Note that the acoustic model is appropriate for ei-
ther domain as the messages are collected on a voicemail
system of the same type. This parallels the experiments
in (Lamel et al, 2002), where the focus was on AM adap-
tation in the case where the LM was deemed appropriate
for either domain.
The language model of the Scanmail system is a Katz
backoff trigram, trained on hand-transcribed messages of
approximately 100 hours of voicemail (1 million words).
The model contains 13460 unigram, 175777 bigram, and
495629 trigram probabilities. The lexicon of the Scan-
mail system contains 13460 words and was compiled
from all the unique words found in the 100 hours of tran-
scripts of the Scanmail training set.
For every experiment, we report the accuracy of the
one-best transcripts obtained at 2 stages of the recog-
nition process: after the first pass lattice construction
(FP), and after vocal tract length normalization and gen-
der modeling (VTLN), Constrained Model-space Adap-
tation (CMA), and Maximum Likelihood Linear regres-
sion adaptation (MLLR). Results after FP will be denoted
FP; results after VTLN, CMA and MLLR will be denoted
MP.
For the SSNIFR domain we have available a 1 hour
manually transcribed test set (10819 words) and approx-
imately 17 hours of manually transcribed adaptation data
(163343 words). In all experiments, the vocabulary of
the system is left unchanged. Generally, for a domain
shift this can raise the error rate significantly due to an
increase in the OOV rate. However, this increase in error
rate is limited in these experiments, because the majority
of the new domain-dependent vocabulary are acronyms
System FP MP
Baseline 32.7 28.0
MAP estimation 23.7 20.3
Perceptron (FP) 26.8 23.0
Perceptron (MP) ? 23.9
Table 1: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the two methods, versus the
baseline out-of-domain system.
which are covered by the Scanmail vocabulary through
individual letters. The OOV rate of the SSNIFR test set,
using the Scanmail vocabulary is 2%.
Following (Bacchiani and Roark, 2003), ?h in Eq. 2 is
set to 0.2 for all reported MAP estimation trials. Follow-
ing (Roark et al, 2004), ? in Eq. 4 is also (coincidentally)
set to 0.2 for all reported perceptron trials. For the percep-
tron algorithm, approximately 10 percent of the training
data is reserved as a held-out set, for deciding when to
stop the algorithm.
Table 1 shows the results using MAP estimation and
the perceptron algorithm independently. For the percep-
tron algorithm, the baseline Scanmail system was used to
produce the word lattices used in estimating the feature
weights. There are two ways to do this. One is to use the
lattices produced after FP; the other is to use the lattices
produced after MP.
These results show two things. First, MAP estimation
on its own is clearly better than the perceptron algorithm
on its own. Since the MAP model is used in the ini-
tial search pass that produces the lattices, it can consider
all possible hypotheses. In contrast, the perceptron algo-
rithm is limited to the hypotheses available in the lattice
produced with the unadapted model.
Second, training the perceptron model on FP lattices
and applying that perceptron at each decoding step out-
performed training on MP lattices and only applying the
perceptron on that decoding step. This demonstrates the
benefit of better transcripts for the unsupervised adapta-
tion steps.
The benefit of MAP adaptation that leads to its supe-
rior performance in Table 1 suggests a hybrid approach,
that uses MAP estimation to ensure that good hypotheses
are present in the lattices, and the perceptron algorithm
to further reduce the WER. Within the multi-pass recog-
nition approach, several scenarios could be considered to
implement this combination. We investigate two here.
For each scenario, we split the 17 hour adaptation set
into four roughly equi-sized sets. In a first scenario, we
produced a MAP estimated model on the first 4.25 hour
subset, and produced word lattices on the other three sub-
sets, for use with the perceptron algorithm. Table 2 shows
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
MAP estimation 25 25.6 21.5
Perceptron (FP) 25 23.8 20.5
Perceptron (MP) 25 ? 20.8
Table 2: Recognition on the 1 hour SSNIFR test set using
systems obtained by supervised LM adaptation on the 17
hour adaptation set using the first method of combination
of the two methods, versus the baseline out-of-domain
system.
the results for this training scenario.
A second scenario involves making use of all of the
adaptation data for both MAP estimation and the percep-
tron algorithm. As a result, it requires a more compli-
cated control of the baseline models used for producing
the word lattices for perceptron training. For each of the
four sub-sections of the adaptation data, we produced a
baseline MAP estimated model using the other three sub-
sections. Using these models, we produced training lat-
tices for the perceptron algorithm for the entire adaptation
data set. At test time, we used the MAP estimated model
trained on the entire adaptation set, as well as the percep-
tron model trained on the entire set. The results for this
training scenario are shown in table 3.
Both of these hybrid training scenarios demonstrate a
small improvement by using the perceptron algorithm on
FP lattices rather than MP lattices. Closely matching the
testing condition for perceptron training is important: ap-
plying a perceptron trained on MP lattices to FP lattices
hurts performance. Iterative training did not produce fur-
ther improvements: training a perceptron on MP lattices
produced by using both MAP estimation and a perceptron
trained on FP lattices, achieved no improvement over the
19.6 percent WER shown above.
4 Discussion
This paper has presented a series of experimental re-
sults that compare using MAP estimation for language
model domain adaptation to a discriminative modeling
approach for correcting errors produced by an out-of-
domain model when applied to the novel domain. Be-
cause the MAP estimation produces a model that is used
during first pass search, it has an advantage over the
perceptron algorithm, which simply re-weights paths al-
ready in the word lattice. In support of this argument, we
showed that, by using a subset of the in-domain adapta-
tion data for MAP estimation, and the rest for use in the
perceptron algorithm, we achieved results at nearly the
same level as MAP estimation on the entire adaptation
set.
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
Perceptron (FP) 100 22.9 19.6
Perceptron (MP) 100 ? 19.9
Table 3: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the second method of com-
bination of the two methods, versus the baseline out-of-
domain system.
With a more complicated training scenario, which used
all of the in-domain adaptation data for both methods
jointly, we were able to improve WER over MAP estima-
tion alone by 0.7 percent, for a total improvement over
the baseline of 8.4 percent.
Studying the various options for incorporating the per-
ceptron algorithm within the multi-pass rescoring frame-
work, our results show that there is a benefit from incor-
porating the perceptron at an early search pass, as it pro-
duces more accurate transcripts for unsupervised adapta-
tion. Furthermore, it is important to closely match testing
conditions for perceptron training.
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
language model adaptation. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP), pages 224?227.
Michiel Bacchiani. 2001. Automatic transcription of
voicemail at AT&T. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?8.
L. Lamel, J.-L. Gauvain, and G. Adda. 2002. Unsuper-
vised acoustic model training. In Proceedings of the
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pages 877?880.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
A. Stolcke and M. Weintraub. 1998. Discriminitive lan-
guage modeling. In Proceedings of the 9th Hub-5
Conversational Speech Recog nition Workshop.
P.C. Woodland and T. Hain. 1998. The September 1998
HTK Hub 5E System. In The Proceedings of the 9th
Hub-5 Conversational Speech Recognition Workshop.
Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm
Brian Roark Murat Saraclar
AT&T Labs - Research
{roark,murat}@research.att.com
Michael Collins Mark Johnson
MIT CSAIL Brown University
mcollins@csail.mit.edu Mark Johnson@Brown.edu
Abstract
This paper describes discriminative language modeling
for a large vocabulary speech recognition task. We con-
trast two parameter estimation methods: the perceptron
algorithm, and a method based on conditional random
fields (CRFs). The models are encoded as determin-
istic weighted finite state automata, and are applied by
intersecting the automata with word-lattices that are the
output from a baseline recognizer. The perceptron algo-
rithm has the benefit of automatically selecting a rela-
tively small feature set in just a couple of passes over the
training data. However, using the feature set output from
the perceptron algorithm (initialized with their weights),
CRF training provides an additional 0.5% reduction in
word error rate, for a total 1.8% absolute reduction from
the baseline of 39.2%.
1 Introduction
A crucial component of any speech recognizer is the lan-
guage model (LM), which assigns scores or probabilities
to candidate output strings in a speech recognizer. The
language model is used in combination with an acous-
tic model, to give an overall score to candidate word se-
quences that ranks them in order of probability or plau-
sibility.
A dominant approach in speech recognition has been
to use a ?source-channel?, or ?noisy-channel? model. In
this approach, language modeling is effectively framed
as density estimation: the language model?s task is to
define a distribution over the source ? i.e., the possible
strings in the language. Markov (n-gram) models are of-
ten used for this task, whose parameters are optimized
to maximize the likelihood of a large amount of training
text. Recognition performance is a direct measure of the
effectiveness of a language model; an indirect measure
which is frequently proposed within these approaches is
the perplexity of the LM (i.e., the log probability it as-
signs to some held-out data set).
This paper explores alternative methods for language
modeling, which complement the source-channel ap-
proach through discriminatively trained models. The lan-
guage models we describe do not attempt to estimate a
generative model P (w) over strings. Instead, they are
trained on acoustic sequences with their transcriptions,
in an attempt to directly optimize error-rate. Our work
builds on previous work on language modeling using the
perceptron algorithm, described in Roark et al (2004).
In particular, we explore conditional random field meth-
ods, as an alternative training method to the perceptron.
We describe how these models can be trained over lat-
tices that are the output from a baseline recognizer. We
also give a number of experiments comparing the two ap-
proaches. The perceptron method gave a 1.3% absolute
improvement in recognition error on the Switchboard do-
main; the CRF methods we describe give a further gain,
the final absolute improvement being 1.8%.
A central issue we focus on concerns feature selection.
The number of distinct n-grams in our training data is
close to 45 million, and we show that CRF training con-
verges very slowly even when trained with a subset (of
size 12 million) of these features. Because of this, we ex-
plore methods for picking a small subset of the available
features.1 The perceptron algorithm can be used as one
method for feature selection, selecting around 1.5 million
features in total. The CRF trained with this feature set,
and initialized with parameters from perceptron training,
converges much more quickly than other approaches, and
also gives the optimal performance on the held-out set.
We explore other approaches to feature selection, but find
that the perceptron-based approach gives the best results
in our experiments.
While we focus on n-gram models, we stress that our
methods are applicable to more general language mod-
eling features ? for example, syntactic features, as ex-
plored in, e.g., Khudanpur and Wu (2000). We intend
to explore methods with new features in the future. Ex-
perimental results with n-gram models on 1000-best lists
show a very small drop in accuracy compared to the use
of lattices. This is encouraging, in that it suggests that
models with more flexible features than n-gram models,
which therefore cannot be efficiently used with lattices,
may not be unduly harmed by their restriction to n-best
lists.
1.1 Related Work
Large vocabulary ASR has benefitted from discrimina-
tive estimation of Hidden Markov Model (HMM) param-
eters in the form of Maximum Mutual Information Es-
timation (MMIE) or Conditional Maximum Likelihood
Estimation (CMLE). Woodland and Povey (2000) have
shown the effectiveness of lattice-based MMIE/CMLE in
challenging large scale ASR tasks such as Switchboard.
In fact, state-of-the-art acoustic modeling, as seen, for
example, at annual Switchboard evaluations, invariably
includes some kind of discriminative training.
Discriminative estimation of language models has also
been proposed in recent years. Jelinek (1995) suggested
an acoustic sensitive language model whose parameters
1Note also that in addition to concerns about training time, a lan-
guage model with fewer features is likely to be considerably more effi-
cient when decoding new utterances.
are estimated by minimizing H(W |A), the expected un-
certainty of the spoken text W, given the acoustic se-
quence A. Stolcke and Weintraub (1998) experimented
with various discriminative approaches including MMIE
with mixed results. This work was followed up with
some success by Stolcke et al (2000) where an ?anti-
LM?, estimated from weighted N-best hypotheses of a
baseline ASR system, was used with a negative weight
in combination with the baseline LM. Chen et al (2000)
presented a method based on changing the trigram counts
discriminatively, together with changing the lexicon to
add new words. Kuo et al (2002) used the generalized
probabilistic descent algorithm to train relatively small
language models which attempt to minimize string error
rate on the DARPA Communicator task. Banerjee et al
(2003) used a language model modification algorithm in
the context of a reading tutor that listens. Their algorithm
first uses a classifier to predict what effect each parame-
ter has on the error rate, and then modifies the parameters
to reduce the error rate based on this prediction.
2 Linear Models, the Perceptron
Algorithm, and Conditional Random
Fields
This section describes a general framework, global linear
models, and two parameter estimation methods within
the framework, the perceptron algorithm and a method
based on conditional random fields. The linear models
we describe are general enough to be applicable to a di-
verse range of NLP and speech tasks ? this section gives
a general description of the approach. In the next section
of the paper we describe how global linear models can
be applied to speech recognition. In particular, we focus
on how the decoding and parameter estimation problems
can be implemented over lattices using finite-state tech-
niques.
2.1 Global linear models
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . We assume the following compo-
nents: (1) Training examples (xi, yi) for i = 1 . . . N .
(2) A function GEN which enumerates a set of candi-
dates GEN(x) for an input x. (3) A representation
? mapping each (x, y) ? X ? Y to a feature vector
?(x, y) ? Rd. (4) A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping
from an input x to an output F (x) through
F (x) = argmax
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the y that maximizes
Eq. 1.
2.2 The Perceptron algorithm
We now turn to methods for training the parameters
?? of the model, given a set of training examples
Inputs: Training examples (xi, yi)
Initialization: Set ?? = 0
Algorithm:
For t = 1 . . . T , i = 1 . . . N
Calculate zi = argmaxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Output: Parameters ??
Figure 1: A variant of the perceptron algorithm.
(x1, y1) . . . (xN , yN ). This section describes the per-
ceptron algorithm, which was previously applied to lan-
guage modeling in Roark et al (2004). The next section
describes an alternative method, based on conditional
random fields.
The perceptron algorithm is shown in figure 1. At
each training example (xi, yi), the current best-scoring
hypothesis zi is found, and if it differs from the refer-
ence yi , then the cost of each feature2 is increased by
the count of that feature in zi and decreased by the count
of that feature in yi. The features in the model are up-
dated, and the algorithm moves to the next utterance.
After each pass over the training data, performance on
a held-out data set is evaluated, and the parameterization
with the best performance on the held out set is what is
ultimately produced by the algorithm.
Following Collins (2002), we used the averaged pa-
rameters from the training algorithm in decoding held-
out and test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 Conditional Random Fields
Conditional Random Fields have been applied to NLP
tasks such as parsing (Ratnaparkhi et al, 1994; Johnson
et al, 1999), and tagging or segmentation tasks (Lafferty
et al, 2001; Sha and Pereira, 2003; McCallum and Li,
2003; Pinto et al, 2003). CRFs use the parameters ??
to define a conditional distribution over the members of
GEN(x) for a given input x:
p??(y|x) =
1
Z(x, ??)
exp (?(x, y) ? ??)
where Z(x, ??) =
?
y?GEN(x) exp (?(x, y) ? ??) is a
normalization constant that depends on x and ??.
Given these definitions, the log-likelihood of the train-
ing data under parameters ?? is
LL(??) =
N?
i=1
log p??(yi|xi)
=
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)] (2)
2Note that here lattice weights are interpreted as costs, which
changes the sign in the algorithm presented in figure 1.
Following Johnson et al (1999) and Lafferty et al
(2001), we use a zero-mean Gaussian prior on the pa-
rameters resulting in the regularized objective function:
LLR(??) =
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)]?
||??||2
2?2
(3)
The value ? dictates the relative influence of the log-
likelihood term vs. the prior, and is typically estimated
using held-out data. The optimal parameters under this
criterion are ??? = argmax?? LLR(??).
We use a limited memory variable metric method
(Benson and More?, 2002) to optimize LLR. There is a
general implementation of this method in the Tao/PETSc
software libraries (Balay et al, 2002; Benson et al,
2002). This technique has been shown to be very effec-
tive in a variety of NLP tasks (Malouf, 2002; Wallach,
2002). The main interface between the optimizer and the
training data is a procedure which takes a parameter vec-
tor ?? as input, and in turn returns LLR(??) as well as
the gradient of LLR at ??. The derivative of the objec-
tive function with respect to a parameter ?s at parameter
values ?? is
?LLR
??s
=
N?
i=1
?
??s(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?s(xi, y)
?
??
?s
?2
(4)
Note that LLR(??) is a convex function, so that there is
a globally optimal solution and the optimization method
will find it. The use of the Gaussian prior term ||??||2/2?2
in the objective function has been found to be useful in
several NLP settings. It effectively ensures that there is a
large penalty for parameter values in the model becoming
too large ? as such, it tends to control over-training. The
choice ofLLR as an objective function can be justified as
maximum a-posteriori (MAP) training within a Bayesian
approach. An alternative justification comes through a
connection to support vector machines and other large
margin approaches. SVM-based approaches use an op-
timization criterion that is closely related to LLR ? see
Collins (2004) for more discussion.
3 Linear models for speech recognition
We now describe how the formalism and algorithms in
section 2 can be applied to language modeling for speech
recognition.
3.1 The basic approach
As described in the previous section, linear models re-
quire definitions of X , Y , xi, yi, GEN, ? and a param-
eter estimation method. In the language modeling setting
we take X to be the set of all possible acoustic inputs; Y
is the set of all possible strings, ??, for some vocabu-
lary ?. Each xi is an utterance (a sequence of acous-
tic feature-vectors), and GEN(xi) is the set of possible
transcriptions under a first pass recognizer. (GEN(xi)
is a huge set, but will be represented compactly using a
lattice ? we will discuss this in detail shortly). We take
yi to be the member of GEN(xi) with lowest error rate
with respect to the reference transcription of xi.
All that remains is to define the feature-vector repre-
sentation, ?(x, y). In the general case, each component
?i(x, y) could be essentially any function of the acous-
tic input x and the candidate transcription y. The first
feature we define is ?0(x, y) as the log-probability of y
given x under the lattice produced by the baseline recog-
nizer. Thus this feature will include contributions from
the acoustic model and the original language model. The
remaining features are restricted to be functions over the
transcription y alone and they track all n-grams up to
some length (say n = 3), for example:
?1(x, y) = Number of times ?the the of? is seen in y
At an abstract level, features of this form are introduced
for all n-grams up to length 3 seen in some training data
lattice, i.e., n-grams seen in any word sequence within
the lattices. In practice, we consider methods that search
for sparse parameter vectors ??, thus assigning many n-
grams 0 weight. This will lead to more efficient algo-
rithms that avoid dealing explicitly with the entire set of
n-grams seen in training data.
3.2 Implementation using WFA
We now give a brief sketch of how weighted finite-state
automata (WFA) can be used to implement linear mod-
els for speech recognition. There are several papers de-
scribing the use of weighted automata and transducers
for speech in detail, e.g., Mohri et al (2002), but for clar-
ity and completeness this section gives a brief description
of the operations which we use.
For our purpose, a WFA A = (?, Q, qs, F, E, ?),
where ? is the vocabulary, Q is a (finite) set of states,
qs ? Q is a unique start state, F ? Q is a set of final
states, E is a (finite) set of transitions, and ? : F ? R
is a function from final states to final weights. Each tran-
sition e ? E is a tuple e = (l[e], p[e], n[e], w[e]), where
l[e] ? ? is a label (in our case, words), p[e] ? Q is the
origin state of e, n[e] ? Q is the destination state of e,
and w[e] ? R is the weight of the transition. A suc-
cessful path pi = e1 . . . ej is a sequence of transitions,
such that p[e1] = qs, n[ej ] ? F , and for 1 < k ? j,
n[ek?1] = p[ek]. Let ?A be the set of successful paths pi
in a WFA A. For any pi = e1 . . . ej , l[pi] = l[e1] . . . l[ej ].
The weights of the WFA in our case are always in the
log semiring, which means that the weight of a path pi =
e1 . . . ej ? ?A is defined as:
wA[pi] =
(
j?
k=1
w[ek]
)
+ ?(ej) (5)
By convention, we use negative log probabilities as
weights, so lower weights are better. All WFA that we
will discuss in this paper are deterministic, i.e. there are
no  transitions, and for any two transitions e, e? ? E,
if p[e] = p[e?], then l[e] 6= l[e?]. Thus, for any string
w = w1 . . . wj , there is at most one successful path
pi ? ?A, such that pi = e1 . . . ej and for 1 ? k ? j,
l[ek] = wk, i.e. l[pi] = w. The set of strings w such that
there exists a pi ? ?A with l[pi] = w define a regular
language LA ? ?.
We can now define some operations that will be used
in this paper.
? ?A. For a set of transitions E and ? ? R, define
?E = {(l[e], p[e], n[e], ?w[e]) : e ? E}. Then, for
any WFA A = (?, Q, qs, F, E, ?), define ?A for ? ? R
as follows: ?A = (?, Q, qs, F, ?E, ??).
? A ?A?. The intersection of two deterministic WFAs
A ? A? in the log semiring is a deterministic WFA
such that LA?A? = LA
?
LA? . For any pi ? ?A?A? ,
wA?A? [pi] = wA[pi1] + wA? [pi2], where l[pi] = l[pi1] =
l[pi2].
?BestPath(A). This operation takes a WFA A, and
returns the best scoring path p?i = argminpi??A wA[pi].
? MinErr(A, y). Given a WFA A, a string y, and
an error-function E(y,w), this operation returns p?i =
argminpi??A E(y, l[pi]). This operation will generally be
used with y as the reference transcription for a particular
training example, and E(y,w) as some measure of the
number of errors in w when compared to y. In this case,
the MinErr operation returns the path pi ? ?A such
l[pi] has the smallest number of errors when compared to
y.
? Norm(A). Given a WFA A, this operation yields
a WFA A? such that LA = LA? and for every pi ? ?A
there is a pi? ? ?A? such that l[pi] = l[pi?] and
wA? [pi
?] = wA[pi] + log
(
?
p?i??A
exp(?wA[p?i])
)
(6)
Note that
?
pi?Norm(A)
exp(?wNorm(A)[pi]) = 1 (7)
In other words the weights define a probability distribu-
tion over the paths.
? ExpCount(A,w). Given a WFA A and an n-gram
w, we define the expected count of w in A as
ExpCount(A,w) =
?
pi??A
wNorm(A)[pi]C(w, l[pi])
where C(w, l[pi]) is defined to be the number of times
the n-gram w appears in a string l[pi].
Given an acoustic input x, let Lx be a deterministic
word-lattice produced by the baseline recognizer. The
lattice Lx is an acyclic WFA, representing a weighted set
of possible transcriptions of x under the baseline recog-
nizer. The weights represent the combination of acoustic
and language model scores in the original recognizer.
The new, discriminative language model constructed
during training consists of a deterministic WFA which
we will denote D, together with a single parameter ?0.
The parameter ?0 is the weight for the log probability
feature ?0 given by the baseline recognizer. The WFA
D is constructed so that LD = ?? and for all pi ? ?D
wD[pi] =
d?
j=1
?j(x, l[pi])?j
Recall that ?j(x,w) for j > 0 is the count of the j?th n-
gram in w, and ?j is the parameter associated with that
w  wi-2     i-1 w   wi-1     iwi
wi-1
?
wi
?wi
?
? wi
Figure 2: Representation of a trigram model with failure transitions.
n-gram. Then, by definition, ?0L ? D accepts the same
set of strings as L, but
w?0L?D[pi] =
d?
j=0
?j(x, l[pi])?j
and argmin
pi?L
?(x, l[pi]) ? ?? = BestPath(?0L ? D).
Thus decoding under our new model involves first pro-
ducing a lattice L from the baseline recognizer; second,
scaling L with ?0 and intersecting it with the discrimi-
native language model D; third, finding the best scoring
path in the new WFA.
We now turn to training a model, or more explicitly,
deriving a discriminative language model (D, ?0) from a
set of training examples. Given a training set (xi, ri) for
i = 1 . . . N , where xi is an acoustic sequence, and ri is
a reference transcription, we can construct lattices Li for
i = 1 . . . N using the baseline recognizer. We can also
derive target transcriptions yi = MinErr(Li, ri). The
training algorithm is then a mapping from (Li, yi) for
i = 1 . . . N to a pair (D, ?0). Note that the construction
of the language model requires two choices. The first
concerns the choice of the set of n-gram features ?i for
i = 1 . . . d implemented by D. The second concerns
the choice of parameters ?i for i = 0 . . . d which assign
weights to the n-gram features as well as the baseline
feature ?0.
Before describing methods for training a discrimina-
tive language model using perceptron and CRF algo-
rithms, we give a little more detail about the structure
of D, focusing on how n-gram language models can be
implemented with finite-state techniques.
3.3 Representation of n-gram language models
An n-gram model can be efficiently represented in a de-
terministic WFA, through the use of failure transitions
(Allauzen et al, 2003). Every string accepted by such an
automaton has a single path through the automaton, and
the weight of the string is the sum of the weights of the
transitions in that path. In such a representation, every
state in the automaton represents an n-gram history h,
e.g. wi?2wi?1, and there are transitions leaving the state
for every word wi such that the feature hwi has a weight.
There is also a failure transition leaving the state, labeled
with some reserved symbol ?, which can only be tra-
versed if the next symbol in the input does not match any
transition leaving the state. This failure transition points
to the backoff state h?, i.e. the n-gram history h minus
its initial word. Figure 2 shows how a trigram model can
be represented in such an automaton. See Allauzen et al
(2003) for more details.
Note that in such a deterministic representation, the
entire weight of all features associated with the word
wi following history h must be assigned to the transi-
tion labeled with wi leaving the state h in the automa-
ton. For example, if h = wi?2wi?1, then the trigram
wi?2wi?1wi is a feature, as is the bigram wi?1wi and
the unigram wi. In this case, the weight on the transi-
tion wi leaving state h must be the sum of the trigram,
bigram and unigram feature weights. If only the trigram
feature weight were assigned to the transition, neither the
unigram nor the bigram feature contribution would be in-
cluded in the path weight. In order to ensure that the cor-
rect weights are assigned to each string, every transition
encoding an order k n-gram must carry the sum of the
weights for all n-gram features of orders ? k. To ensure
that every string in ?? receives the correct weight, for
any n-gram hw represented explicitly in the automaton,
h?w must also be represented explicitly in the automaton,
even if its weight is 0.
3.4 The perceptron algorithm
The perceptron algorithm is incremental, meaning that
the language model D is built one training example at
a time, during several passes over the training set. Ini-
tially, we build D to accept all strings in ?? with weight
0. For the perceptron experiments, we chose the param-
eter ?0 to be a fixed constant, chosen by optimization on
the held-out set. The loop in the algorithm in figure 1 is
implemented as:
For t = 1 . . . T, i = 1 . . . N :
? Calculate zi = argmaxy?GEN(x) ?(x, y) ? ??
= BestPath(?0Li ? D)
? If zi 6= MinErr(Li, ri), then update the feature
weights as in figure 1 (modulo the sign, because of
the use of costs), and modify D so as to assign the
correct weight to all strings.
In addition, averaged parameters need to be stored
(see section 2.2). These parameters will replace the un-
averaged parameters in D once training is completed.
Note that the only n-gram features to be included in
D at the end of the training process are those that oc-
cur in either a best scoring path zi or a minimum error
path yi at some point during training. Thus the percep-
tron algorithm is in effect doing feature selection as a
by-product of training. Given N training examples, and
T passes over the training set,O(NT ) n-grams will have
non-zero weight after training. Experiments in Roark et
al. (2004) suggest that the perceptron reaches optimal
performance after a small number of training iterations,
for example T = 1 or T = 2. Thus O(NT ) can be very
small compared to the full number of n-grams seen in
all training lattices. In our experiments, the perceptron
method chose around 1.4 million n-grams with non-zero
weight. This compares to 43.65 million possible n-grams
seen in the training data.
This is a key contrast with conditional random fields,
which optimize the parameters of a fixed feature set. Fea-
ture selection can be critical in our domain, as training
and applying a discriminative language model over all
n-grams seen in the training data (in either correct or in-
correct transcriptions) may be computationally very de-
manding. One training scenario that we will consider
will be using the output of the perceptron algorithm (the
averaged parameters) to provide the feature set and the
initial feature weights for use in the CRF algorithm. This
leads to a model which is reasonably sparse, but has the
benefit of CRF training, which as we will see gives gains
in performance.
3.5 Conditional Random Fields
The CRF methods that we use assume a fixed definition
of the n-gram features ?i for i = 1 . . . d in the model.
In the experimental section we will describe a number of
ways of defining the feature set. The optimization meth-
ods we use begin at some initial setting for ??, and then
search for the parameters ??? which maximize LLR(??)
as defined in Eq. 3.
The optimization method requires calculation of
LLR(??) and the gradient of LLR(??) for a series of val-
ues for ??. The first step in calculating these quantities is
to take the parameter values ??, and to construct an ac-
ceptor D which accepts all strings in ??, such that
wD[pi] =
d?
j=1
?j(x, l[pi])?j
For each training lattice Li, we then construct a new lat-
tice L?i = Norm(?0Li ? D). The lattice L?i represents
(in the log domain) the distribution p??(y|xi) over strings
y ? GEN(xi). The value of log p??(yi|xi) for any i can
be computed by simply taking the path weight of pi such
that l[pi] = yi in the new lattice L?i. Hence computation
of LLR(??) in Eq. 3 is straightforward.
Calculating the n-gram feature gradients for the CRF
optimization is also relatively simple, once L?i has been
constructed. From the derivative in Eq. 4, for each i =
1 . . . N, j = 1 . . . d the quantity
?j(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?j(xi, y) (8)
must be computed. The first term is simply the num-
ber of times the j?th n-gram feature is seen in yi. The
second term is the expected number of times that the
j?th n-gram is seen in the acceptor L?i. If the j?th
n-gram is w1 . . . wn, then this can be computed as
ExpCount(L?i, w1 . . . wn). The GRM library, which
was presented in Allauzen et al (2003), has a direct im-
plementation of the function ExpCount, which simul-
taneously calculates the expected value of all n-grams of
order less than or equal to a given n in a lattice L.
The one non-ngram feature weight that is being esti-
mated is the weight ?0 given to the baseline ASR nega-
tive log probability. Calculation of the gradient of LLR
with respect to this parameter again requires calculation
of the term in Eq. 8 for j = 0 and i = 1 . . . N . Com-
putation of
?
y?GEN(xi)
p??(y|xi)?0(xi, y) turns out to
be not as straightforward as calculating n-gram expec-
tations. To do so, we rely upon the fact that ?0(xi, y),
the negative log probability of the path, decomposes to
the sum of negative log probabilities of each transition
in the path. We index each transition in the lattice Li,
and store its negative log probability under the baseline
model. We can then calculate the required gradient from
L?i, by calculating the expected value in L?i of each in-
dexed transition in Li.
We found that an approximation to the gradient of
?0, however, performed nearly identically to this exact
gradient, while requiring substantially less computation.
Let wn1 be a string of n words, labeling a path in word-
lattice L?i. For brevity, let Pi(wn1 ) = p??(wn1 |xi) be the
conditional probability under the current model, and let
Qi(wn1 ) be the probability of wn1 in the normalized base-
line ASR lattice Norm(Li). Let Li be the set of strings
in the language defined by Li. Then we wish to compute
Ei for i = 1 . . . N , where
Ei =
?
wn1 ?Li
Pi(w
n
1 ) log Qi(w
n
1 )
=
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
1 ) (9)
The approximation is to make the following Markov
assumption:
Ei ?
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
k?2)
=
?
xyz?Si
ExpCount(L?i, xyz) log Qi(z|xy)(10)
where Si is the set of all trigrams seen in Li. The term
log Qi(z|xy) can be calculated once before training for
every lattice in the training set; the ExpCount term is
calculated as before using the GRM library. We have
found this approximation to be effective in practice, and
it was used for the trials reported below.
When the gradients and conditional likelihoods are
collected from all of the utterances in the training set, the
contributions from the regularizer are combined to give
an overall gradient and objective function value. These
values are provided to the parameter estimation routine,
which then returns the parameters for use in the next it-
eration. The accumulation of gradients for the feature set
is the most time consuming part of the approach, but this
is parallelizable, so that the computation can be divided
among many processors.
4 Empirical Results
We present empirical results on the Rich Transcription
2002 evaluation test set (rt02), which we used as our de-
velopment set, as well as on the Rich Transcription 2003
Spring evaluation CTS test set (rt03). The rt02 set con-
sists of 6081 sentences (63804 words) and has three sub-
sets: Switchboard 1, Switchboard 2, Switchboard Cel-
lular. The rt03 set consists of 9050 sentences (76083
words) and has two subsets: Switchboard and Fisher.
We used the same training set as that used in Roark
et al (2004). The training set consists of 276726 tran-
scribed utterances (3047805 words), with an additional
20854 utterances (249774 words) as held out data. For
0 500 100037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticePerceptron, Feat=PN, N=1000CRF, ? = ?, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=PN, N=1000
Figure 3: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
each utterance, a weighted word-lattice was produced,
representing alternative transcriptions, from the ASR
system. From each word-lattice, the oracle best path
was extracted, which gives the best word-error rate from
among all of the hypotheses in the lattice. The oracle
word-error rate for the training set lattices was 12.2%.
We also performed trials with 1000-best lists for the same
training set, rather than lattices. The oracle score for the
1000-best lists was 16.7%.
To produce the word-lattices, each training utterance
was processed by the baseline ASR system. However,
these same utterances are what the acoustic and language
models are built from, which leads to better performance
on the training utterances than can be expected when the
ASR system processes unseen utterances. To somewhat
control for this, the training set was partitioned into 28
sets, and baseline Katz backoff trigram models were built
for each set by including only transcripts from the other
27 sets. Since language models are generally far more
prone to overtrain than standard acoustic models, this
goes a long way toward making the training conditions
similar to testing conditions.
There are three baselines against which we are com-
paring. The first is the ASR baseline, with no reweight-
ing from a discriminatively trained n-gram model. The
other two baselines are with perceptron-trained n-gram
model re-weighting, and were reported in Roark et al
(2004). The first of these is for a pruned-lattice trained
trigram model, which showed a reduction in word er-
ror rate (WER) of 1.3%, from 39.2% to 37.9% on rt02.
The second is for a 1000-best list trained trigram model,
which performed only marginally worse than the lattice-
trained perceptron, at 38.0% on rt02.
4.1 Perceptron feature set
We use the perceptron-trained models as the starting
point for our CRF algorithm: the feature set given to
the CRF algorithm is the feature set selected by the per-
ceptron algorithm; the feature weights are initialized to
those of the averaged perceptron. Figure 3 shows the
performance of our three baselines versus three trials of
0 500 1000 1500 2000 250037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=E,  ?=0.01CRF, ? = 0.5, Feat=E,  ?=0.9
Figure 4: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
the CRF algorithm. In the first two trials, the training
set consists of the pruned lattices, and the feature set
is from the perceptron algorithm trained on pruned lat-
tices. There were 1.4 million features in this feature set.
The first trial set the regularizer constant ? =?, so that
the algorithm was optimizing raw conditional likelihood.
The second trial is with the regularizer constant ? = 0.5,
which we found empirically to be a good parameteriza-
tion on the held-out set. As can be seen from these re-
sults, regularization is critical.
The third trial in this set uses the feature set from the
perceptron algorithm trained on 1000-best lists, and uses
CRF optimization on these on these same 1000-best lists.
There were 0.9 million features in this feature set. For
this trial, we also used ? = 0.5. As with the percep-
tron baselines, the n-best trial performs nearly identically
with the pruned lattices, here also resulting in 37.4%
WER. This may be useful for techniques that would be
more expensive to extend to lattices versus n-best lists
(e.g. models with unbounded dependencies).
These trials demonstrate that the CRF algorithm can
do a better job of estimating feature weights than the per-
ceptron algorithm for the same feature set. As mentioned
in the earlier section, feature selection is a by-product of
the perceptron algorithm, but the CRF algorithm is given
a set of features. The next two trials looked at selecting
feature sets other than those provided by the perceptron
algorithm.
4.2 Other feature sets
In order for the feature weights to be non-zero in this ap-
proach, they must be observed in the training set. The
number of unigram, bigram and trigram features with
non-zero observations in the training set lattices is 43.65
million, or roughly 30 times the size of the perceptron
feature set. Many of these features occur only rarely
with very low conditional probabilities, and hence cannot
meaningfully impact system performance. We pruned
this feature set to include all unigrams and bigrams, but
only those trigrams with an expected count of greater
than 0.01 in the training set. That is, to be included, a
Trial Iter rt02 rt03
ASR Baseline - 39.2 38.2
Perceptron, Lattice - 37.9 36.9
Perceptron, N-best - 38.0 37.2
CRF, Lattice, Percep Feats (1.4M) 769 37.4 36.5
CRF, N-best, Percep Feats (0.9M) 946 37.4 36.6
CRF, Lattice, ? = 0.01 (12M) 2714 37.6 36.5
CRF, Lattice, ? = 0.9 (1.5M) 1679 37.5 36.6
Table 1: Word-error rate results at convergence iteration for
various trials, on both Switchboard 2002 test set (rt02), which
was used as the dev set, and Switchboard 2003 test set (rt03).
trigram must occur in a set of paths, the sum of the con-
ditional probabilities of which must be greater than our
threshold ? = 0.01. This threshold resulted in a feature
set of roughly 12 million features, nearly 10 times the
size of the perceptron feature set. For better comparabil-
ity with that feature set, we set our thresholds higher, so
that trigrams were pruned if their expected count fell be-
low ? = 0.9, and bigrams were pruned if their expected
count fell below ? = 0.1. We were concerned that this
may leave out some of the features on the oracle paths, so
we added back in all bigram and trigram features that oc-
curred on oracle paths, giving a feature set of 1.5 million
features, roughly the same size as the perceptron feature
set.
Figure 4 shows the results for three CRF trials versus
our ASR baseline and the perceptron algorithm baseline
trained on lattices. First, the result using the perceptron
feature set provides us with a WER of 37.4%, as pre-
viously shown. The WER at convergence for the big
feature set (12 million features) is 37.6%; the WER at
convergence for the smaller feature set (1.5 million fea-
tures) is 37.5%. While both of these other feature sets
converge to performance close to that using the percep-
tron features, the number of iterations over the training
data that are required to reach that level of performance
are many more than for the perceptron-initialized feature
set.
Table 1 shows the word-error rate at the convergence
iteration for the various trials, on both rt02 and rt03. All
of the CRF trials are significantly better than the percep-
tron performance, using the Matched Pair Sentence Seg-
ment test for WER included with SCTK (NIST, 2000).
On rt02, the N-best and perceptron initialized CRF trials
were were significantly better than the lattice perceptron
at p < 0.001; the other two CRF trials were significantly
better than the lattice perceptron at p < 0.01. On rt03,
the N-best CRF trial was significantly better than the lat-
tice perceptron at p < 0.002; the other three CRF tri-
als were significantly better than the lattice perceptron at
p < 0.001.
Finally, we measured the time of a single iteration over
the training data on a single machine for the perceptron
algorithm, the CRF algorithm using the approximation to
the gradient of ?0, and the CRF algorithm using an exact
gradient of ?0. Table 2 shows these times in hours. Be-
cause of the frequent update of the weights in the model,
the perceptron algorithm is more expensive than the CRF
algorithm for a single iteration. Further, the CRF algo-
rithm is parallelizable, so that most of the work of an
CRF
Features Percep approx exact
Lattice, Percep Feats (1.4M) 7.10 1.69 3.61
N-best, Percep Feats (0.9M) 3.40 0.96 1.40
Lattice, ? = 0.01 (12M) - 2.24 4.75
Table 2: Time (in hours) for one iteration on a single Intel
Xeon 2.4Ghz processor with 4GB RAM.
iteration can be shared among multiple processors. Our
most common training setup for the CRF algorithm was
parallelized between 20 processors, using the approxi-
mation to the gradient. In that setup, using the 1.4M fea-
ture set, one iteration of the perceptron algorithm took
the same amount of real time as approximately 80 itera-
tions of CRF.
5 Conclusion
We have contrasted two approaches to discriminative
language model estimation on a difficult large vocabu-
lary task, showing that they can indeed scale effectively
to handle this size of a problem. Both algorithms have
their benefits. The perceptron algorithm selects a rela-
tively small subset of the total feature set, and requires
just a couple of passes over the training data. The CRF
algorithm does a better job of parameter estimation for
the same feature set, and is parallelizable, so that each
pass over the training set can require just a fraction of
the real time of the perceptron algorithm.
The best scenario from among those that we investi-
gated was a combination of both approaches, with the
output of the perceptron algorithm taken as the starting
point for CRF estimation.
As a final point, note that the methods we describe do
not replace an existing language model, but rather com-
plement it. The existing language model has the benefit
that it can be trained on a large amount of text that does
not have speech transcriptions. It has the disadvantage
of not being a discriminative model. The new language
model is trained on the speech transcriptions, meaning
that it has less training data, but that it has the advan-
tage of discriminative training ? and in particular, the ad-
vantage of being able to learn negative evidence in the
form of negative weights on n-grams which are rarely
or never seen in natural language text (e.g., ?the of?),
but are produced too frequently by the recognizer. The
methods we describe combines the two language models,
allowing them to complement each other.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized
algorithms for constructing language models. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F.
Smith. 2002. Petsc users manual. Technical Report ANL-95/11-
Revision 2.1.2, Argonne National Laboratory.
Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam.
2003. Improving language models by learning from speech recog-
nition errors in a reading tutor that listens. In Proceedings of the
Second International Conference on Applied Artificial Intelligence,
Fort Panhala, Kolhapur, India.
Steven J. Benson and Jorge J. More?. 2002. A limited memory vari-
able metric method for bound constrained minimization. Preprint
ANL/ACSP909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J. More?, and Jason
Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM-
242-Revision 1.4, Argonne National Laboratory.
Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative
training on language model. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing (ICSLP), Bei-
jing, China.
Michael Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algo-
rithms. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods. In Harry
Bunt, John Carroll, and Giorgio Satta, editors, New Developments
in Parsing Technology. Kluwer.
Yoav Freund and Robert Schapire. 1999. Large margin classification
using the perceptron algorithm. Machine Learning, 3(37):277?296.
Frederick Jelinek. 1995. Acoustic sensitive language modeling. Tech-
nical report, Center for Language and Speech Processing, Johns
Hopkins University, Baltimore, MD.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan
Riezler. 1999. Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 535?541.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques
for exploiting syntactic, semantic and collocational dependencies in
language modeling. Computer Speech and Language, 14(4):355?
372.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-
Hui Lee. 2002. Discriminative training of language models for
speech recognition. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), Orlando,
Florida.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Con-
ditional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289, Williams
College, Williamstown, MA, USA.
Robert Malouf. 2002. A comparison of algorithms for maximum en-
tropy parameter estimation. In Proc. CoNLL, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction and
web-enhanced lexicons. In Proc. CoNLL.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition. Computer
Speech and Language, 16(1):69?88.
NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c.
Available at http://www.nist.gov/speech/tools.
David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003.
Table extraction using conditional random fields. In Proc. ACM SI-
GIR.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A max-
imum entropy model for parsing. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (ICSLP), pages
803?806.
Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective
language modeling for large vocabulary ASR with the perceptron al-
gorithm. In Proceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages 749?752.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL, Edmonton, Canada.
A. Stolcke and M. Weintraub. 1998. Discriminitive language model-
ing. In Proceedings of the 9th Hub-5 Conversational Speech Recog-
nition Workshop.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and
J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech
transcription system. In Proceedings of the NIST Speech Transcrip-
tion Workshop.
Hanna Wallach. 2002. Efficient training of conditional random fields.
Master?s thesis, University of Edinburgh.
P.C. Woodland and D. Povey. 2000. Large scale discriminative training
for speech recognition. In Proc. ISCA ITRW ASR2000, pages 7?16.
A System for Searching and Browsing Spoken Communications
Lee Begeja
Bernard Renger
Murat Saraclar
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
{lee, renger, murat}
@research.att.com
David Gibbon
Zhu Liu
Behzad Shahraray
AT&T Labs ? Research
200 Laurel Ave S
Middletown, NJ 07748
{dcg, zliu, behzad}
@research.att.com
Abstract
As the amount of spoken communications ac-
cessible by computers increases, searching and
browsing is becoming crucial for utilizing such
material for gathering information. It is desir-
able for multimedia content analysis systems
to handle various formats of data and to serve
varying user needs while presenting a simple
and consistent user interface. In this paper,
we present a research system for searching and
browsing spoken communications. The system
uses core technologies such as speaker segmen-
tation, automatic speech recognition, transcrip-
tion alignment, keyword extraction and speech
indexing and retrieval to make spoken commu-
nications easy to navigate. The main focus is
on telephone conversations and teleconferences
with comparisons to broadcast news.
1 Introduction
Archiving and organizing multimedia communications
for easy user access is becoming more important as such
information sources are becoming available in amounts
that can easily overwhelm a user. As storage and ac-
cess become cheaper, the types of multimedia communi-
cations are also becoming more diverse. Therefore, it is
necessary for multimedia content analysis and navigation
systems to handle various forms of data.
In this paper we present SpeechLogger, a research sys-
tem for searching and browsing spoken communications,
or the spoken component of multimedia communications.
In general, the information contained in a spoken com-
munication consists of more than just words. Our goal is
to make use of all the information within a spoken com-
munication. Our system uses automatic speech recogni-
tion (ASR) to convert speech into a format which makes
word and phonetic searching of the material possible. It
also uses speaker segmentation to aid navigation.
We are interested in a wide range of spoken communi-
cations with different characteristics, including broadcast
material, lectures, meetings, interviews, telephone con-
versations, call center recordings, and teleconferences.
Each of these communication types presents interesting
opportunities, requirements and challenges. For example,
lectures might have accompanying material that can aid
ASR and navigation. Prior knowledge about the speakers
and the topic may be available for meetings. Call center
recordings may be analyzed to create aggregate reports.
Spoken document retrieval (SDR) for Broadcast News
type of content has been well studied and there are many
research and commercial systems. There has also been
some interest in the Voicemail domain (Hirschberg et al,
2001) which consists of typically short duration human-
to-machine messages. Our focus here is on telephone
conversations and teleconferences with comparisons to
broadcast news.
The paper is organized as follows. In Section 2, we
motivate our approach by describing the user needs un-
der various conditions. Then we describe our system in
Section 3, giving the details of various components. Ex-
perimental results for some components are given in Sec-
tion 4. Finally, in Section 5 we present a summary.
2 User Needs
We are primarily interested in situations in which a per-
son needs to gather information from audio data but the
quality of that data is not always sufficient to produce
good ASR results. In the case of telephone conversations,
the information gatherer needs to know who was on the
call, how long the call was, what was said, a summary of
the call, the ability to listen to any part of the call based
on search parameters that s/he specifies, etc. Our users
want to be able to scan a database of many calls, across a
long period of time to look for specific phrases, speakers,
or patterns of speech.
In many cases, it is difficult to gather this type of infor-
mation from teleconference calls since the audio quality
is poor because of speaker phones, cell phones and line
noise. All of these combine to lower ASR results to a
point where the text of the call is not fully representative
of the conversation. Thus, using standard information re-
trieval techniques may not provide sufficient information
to the user. We focus on the navigation aspect of infor-
mation gathering with the goal of compensating for lower
ASR accuracy by presenting user interface elements rel-
evant to the specific task at hand (Stark et al, 2000).
Rather than looking at the recorded conversation as
merely audio information, we view it as a source of lin-
guistic information to which we can apply information
retrieval and data mining techniques. We use all avail-
able metadata to enhance the search and the presentation.
We wanted to have a set of interface elements that
would be useful no matter what the ASR accuracy was.
The main interface elements are:
? Timeline with tick marks indicates search hits within
the spoken document which allows for many search
results to be displayed without overwhelming the
user. This is particularly useful for cases where there
are many false positives.
? Keyword extraction summarizes a given communi-
cation, enables differentiation among a collection of
many spoken documents, and detects subtopics in a
large spoken document.
? Speaker segmentation and speaker identification
separate a long spoken document into inherently
useful pieces.
? Lattice search and phoneme search expand the pos-
sible search space.
In this paper we examine three classes of spoken doc-
uments and consider what tasks a user might want to per-
form on them.
? Broadcast News - excellent ASR conditions, one
speaker at a time, good audio quality and gener-
ally a good speaker. Task involves primarily inter-
document navigation. User needs to search text for
information with metadata possibly used to enhance
the search.
? Telephone Conversations - fair ASR conditions, two
speakers, decent quality audio. User needs to search
text but also wants speaker identification and some
classification (call type, urgency, importance).
? Teleconferences - poor ASR conditions, multiple
speakers, mixed to poor audio quality. Most time
is spent in intra-document navigation. User needs to
navigate through the calls and find relevant informa-
tion in the audio.
3 System Description
The system overview is shown in Figure 1. Our sys-
tem is flexible enough to support various forms of live
(via a VoiceXML Gateway) or prerecorded spoken com-
munications including the three classes of spoken docu-
ments discussed above. It can record the audio via tele-
phone for two-party or multi-party calls. Alternatively,
the system can support prerecorded audio input from var-
ious sources including telephone conversations or video
content in which case the audio is extracted from the
video. Once various speech processing techniques are ap-
plied and the speech is indexed, it is possible to search
and browse the audio content. Our system is scalable
and supports open source/industry standard components
(J2EE, VXML, XML, Microsoft SAMI, Microsoft Media
Player). It is also flexible enough to support other forms
of audio as input or to support new speech processing
techniques as they become available. The system was de-
signed with modularity in mind. For instance, it should
be possible to add a speaker identification module to the
processing.
Figure 1: System Overview
Once a new audio recording is available on the
File Server, the following processing steps can begin:
speaker segmentation, speech recognition, transcription
alignment, keyword extraction, audio compression, and
speech indexing. Each step will be described in more de-
tail below. We attempt to distinguish the different speak-
ers from each other in the speaker segmentation compo-
nent. The speech recognition component converts the au-
dio into a word or phone based representation including
alternative hypotheses in the form of a lattice. If a tran-
script is available, the transcript can be synchronized (or
aligned) in time with the speech recognition output. The
keyword extraction component generates the most salient
words found in the speech recognition output (one-best
word) or transcript (if available) and can be used to de-
termine the nature of the spoken communications. The
audio compression component compresses the audio file
and creates an MP3 audio file which is copied to the Me-
dia Server. The final step in the processing is text and
lattice indexing. This includes creating indices based on
one-best word and one-best phone strings or word and
phone lattices.
After processing, the user can search and browse the
audio using either the text index or the lattice index. The
audio is played back via media streaming. Alternatively,
the user can playback the audio file over the phone using
the VoiceGenie VoiceXML Gateway.
3.1 Speaker Segmentation
Speaker-based segmentation of multi-speaker audio data
has received considerable attention in recent years. Ap-
plications that have been considered include: indexing
archived recorded spoken documents by speaker to facil-
itate browsing and retrieval of desired portions; tagging
speaker specific portions of data to be used for adapt-
ing speech models in order to improve the quality of
automatic speech recognition transcriptions, and track-
ing speaker specific segments in audio streams to aid in
surveillance applications. In our system, speaker segmen-
tation is used for more effective visualization of the audio
document and speaker-based audio playback.
Figure 2 gives an overview of the speaker segmenta-
tion algorithm we developed. It consists of two steps:
preprocessing and iterative speaker segmentation. Dur-
ing the preprocessing step, the input audio stream is seg-
mented into frames and acoustic features are computed
for each frame. The features we extracted are energy, 12
Mel-frequency cepstral coefficients (MFCC), pitch, and
the first and second order temporal derivatives. Then,
all speaker boundary candidates are located, which in-
clude silent frames and frames with minimum energy in
a window of neighboring frames. The preprocessing step
generates a set of over-segmented audio segments, whose
durations may be as short as a fraction of a second to as
long as a couple of seconds.
The iterative speaker segmentation step, as depicted in
the bigger dotted rectangle in Figure 2, detects all seg-
ments of each speaker in an iterative way and then marks
the boundaries where speakers change. At the beginning,
all segments produced by the preprocessing step are un-
labeled. Assuming that the features within each segment
follow a Gaussian distribution, we compute the distances
between each pair of segments using the Kullback Leibler
distance (KLD) (Cover and Thomas, 1991). Here, we just
consider features extracted from voiced frames since only
voiced frames have pitch information. Based on the seg-
ment distance matrix, a hierarchical agglomerative clus-
tering (HAC) (Jain and Dubes, 1988) algorithm is applied
Figure 2: Overview of the Speaker Segmentation Algo-
rithm.
to all unlabeled segments. The biggest cluster will be hy-
pothesized as the set of segments for a new speaker and
the rest of the segments will be considered as background
audio. Accordingly, each unlabeled segment is labeled as
either the target speaker or background. Then an embed-
ded speaker segment refinement substep is activated to
iteratively refine the segments of the target speaker.
The refinement substep is depicted in the smaller dot-
ted rectangle in Figure 2. For each iteration, two Gaus-
sian mixture models (GMM) are built based on current
segment labels, one for the target speaker, one for back-
ground audio. Then all segments are relabeled as either
the target speaker or background audio using the maxi-
mum likelihood method based on the two GMM models.
If the set of segments for the target speaker converges
or the refinement iteration number reaches its maximum,
the refinement iteration stops. Otherwise, a new itera-
tion starts. Before the refinement substep terminates, it
assigns a new speaker label for all segments of the tar-
get speaker, and sets the background audio as unlabeled.
Then the iterative speaker segmentation step needs to test
for more speakers or needs to stop. The termination cri-
teria could be the given number of speakers (or major
speakers) in an audio document, the percentage of unla-
beled segments to the number of all segments, or the max-
imum distance among all pairs of unlabeled segments. If
any of the criteria are met, the speaker segmentation algo-
rithm merges all adjacent segments if their speaker labels
are the same, and then outputs a list of audio segments
with corresponding speaker labels.
Obviously, one advantage of our speaker segmentation
method is that the speaker labels are also extracted. Al-
though the real speaker identities are not available, the
Figure 3: Presentation of Speaker Segmentation Results.
labels are very useful for presenting, indexing, and re-
trieving audio documents. For more detailed description
of the speaker segmentation algorithm, please refer to
Rosenberg et al (2002).
Figure 3 illustrates a graphic interface for presenting
the speaker segmentation results. The audio stream is
shown in colored blocks along a timeline which goes
from top to bottom, and from left to right. Color is used to
differentiate the speaker labels. There are two layers for
each line: the bottom layer shows the manually labeled
speaker segments and the top layer displays the automat-
ically generated segments. This allows the segmentation
performance to be clearly observed.
3.2 Automatic Speech Recognition
We use two different state-of-the-art HMM based large
vocabulary continuous speech recognition (LVCSR) sys-
tems for telephone and microphone recordings. In both
cases the front-end uses 9 frames of 12 MFCC compo-
nents and energy summarized into a feature vector via
linear discriminant analysis. The acoustic models consist
of decision tree state clustered triphones and the output
distributions are mixtures of Gaussians. The models are
discriminatively trained using maximum mutual informa-
tion estimation. The language models are pruned backoff
trigram models.
For narrow-band telephone recordings we use the first
pass of the Switchboard evaluation system developed
by Ljolje et al (2002). The calls are automatically seg-
mented prior to ASR. The acoustic models are trained on
265 hours of speech. The recognition vocabulary of the
system has 45K words.
For wide-band recordings, we use the real-time
Broadcast News transcription system developed by
Saraclar et al (2002). The acoustic models are trained on
140 hours of speech. The language models are estimated
on a mixture of newspaper text, closed captions and high-
accuracy transcriptions from LDC. Since the system was
designed for SDR, the recognition vocabulary of the sys-
tem has over 200K words.
Both systems use the same Finite State Machine (FSM)
based LVCSR decoder (Allauzen et al, 2003). The out-
put of the ASR system is represented as a FSM and may
be in the form of a one-best hypothesis string or a lattice
of alternate hypotheses. The lattices are normalized so
that the probability of the set of all paths leading from
any state to the final state is 1. The labels on the arcs of
the FSM may be words or phones and the conversion be-
tween the two can easily be done using FSM composition
using the AT&T FSM Library (Mohri et al, 1997). The
costs on the arcs of the FSM are negative log likelihoods.
Additionally, timing information can also be present in
the output.
3.3 Alignment with Transcripts
Manual transcriptions of spoken communications are
available for certain application domains such as medical
diagnosis, legal depositions, television and radio broad-
casts. Most audio and video teleconferencing providers
offer transcription as an optional service. In these
cases, we can take advantage of this additional informa-
tion to create high quality multimedia representations of
the archived spoken communications using parallel text
alignment techniques (Gibbon, 1998). The obvious ad-
vantage is increased retrieval accuracy due to the lower
word error rate (manual transcriptions are seldom com-
pletely error free.) What is more compelling, however, is
that we can construct much more evolved user interfaces
for browsing speech by leveraging the fact that the tran-
scription is by its nature readable whereas the one-best
hypothesis from ASR is typically useful only in small
segments to establish context for a search term occur-
rence.
There are several methods for aligning text with
speech. We use dynamic programming techniques to
maximize the number or word correspondences between
the manual transcription and the one-best ASR word hy-
pothesis. For most applications, finding the start and end
times of the transcript sentences is sufficient; but we do
alignment at the word level and then derive the sentence
alignment from that. In cases where the first or last word
of a sentence is not recognized, we expand to the near-
est recognized word to avoid cropping even though we
may include small segments from neighboring sentences
during playback. The accuracy of the resulting align-
ment is directly related to the ASR word error rate; more
precisely it can be thought of as a sentence error rate
where we impose a minimum percentage of correspond-
ing words per sentence (typically 20%) before declar-
ing a sentence a match to avoid noise words triggering
false matches. For sentences without correspondences,
we must fall back to deriving the timings from the near-
est neighboring sentences with correspondences.
Figure 4: Illustration of Keyword Extraction.
3.4 Keyword Extraction
Playing back a spoken document or linearly skimming
the corresponding text transcript, either from automatic
speech recognition or manual transcription, is not an ef-
ficient way for a user to grasp the central topics of the
document within a short period of time. A list of repre-
sentative keywords, which serve as a dense summary for a
document, can effectively convey the essence of the docu-
ment to the user. The keywords have been widely used for
indexing and retrieval of documents in large databases. In
our system, we extract a list of keywords for each audio
document based on its transcript (ASR or manual tran-
script).
There are different ways to automatically extract key-
words for a text document within a corpus. A popular
approach is to select keywords that frequently occur in
one document but do not frequently occur in other doc-
uments based on the term frequency - inverse document
frequency (TF-IDF) feature. Our task is slightly differ-
ent. We are interested in choosing keywords for a sin-
gle document, independent of the remaining documents
in the database. Accordingly, we adopt a different fea-
ture, which is term frequency - inverse term probability
(TF-ITP) to serve our purpose. The term probability mea-
sures the probability that a term may appear in a general
document and it is a language dependent characteristic.
Assuming that a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of Tk is
defined as wk = tfk/tpk.
Figure 4 illustrates the keyword extraction method that
we have developed. For the transcript of a given doc-
ument, we first apply the Porter stemming algorithm
(Porter, 1980) to remove word variations. Then, the stop
words, which are common words that have no impact on
the document content (also called noise words), are re-
moved. Here we use two lists of noise words, one for gen-
eral purposes, which apply to all varieties of documents,
and one for specific domains, which can be customized
by the user when prior knowledge about the document is
available. For each remaining term in the document, a
value of TF-ITP is calculated. A vocabulary is created
based on the transcripts of 600 hours of broadcast news
data and corresponding term probabilities are estimated
using the same corpus. If a term in the document is not
in the vocabulary, and its term frequency is more than
2, then a default term probability value tpd will be used.
The tpd we use is the minimum term probability in the
vocabulary. After we get a list of terms and their TF-ITP
values, we sort the terms based on their TF-ITP values,
such that the most representative terms (highest TF-ITP
values) are on the top of the list. Depending on certain
criteria, for example, the number of keywords desired or
the minimum TF-ITP value required, a list of keywords
can be chosen from the top of the term list. In our sys-
tem, we choose the top ten terms as the keywords for a
document.
3.5 Speech Indexing and Retrieval
Two different indexing and retrieval modules are uti-
lized depending on the type of ASR output. In
the case of one-best word or phone strings, we use
an off-the-shelf text-based index server called Lucene
(http://jakarta.apache.org/lucene). In the case of word
and phone lattices, we use the method described in
Saraclar and Sproat (2004). Here we give a brief descrip-
tion of the latter.
The lattice output is a compact representation of likely
alternative hypotheses of an ASR system. Each path in
the lattice corresponds to a word (or phone) string and
has a probability attached to it. The expected count for
a substring can be defined as the sum of the probabilities
of all paths which contain that substring. Lattice based
retrieval makes the system more robust to recognition er-
rors, whereas phonetic search allows for retrieving words
that are not in the vocabulary of the recognizer.
The lattice index is similar to a standard inverted index
but contains enough information to compute the expected
count of an arbitrary substring for each lattice. This can
be achieved by storing a set of index files, one for each
label (word or phone) l. For each arc labeled with l in a
lattice, the index file for l records the lattice number, the
previous and next states of the arc, along with the prob-
ability mass leading to the previous state of the arc and
the probability of the arc itself. For a lattice, which is
normalized so that the probability of the set of all paths
leading from any state to the final state is 1, the poste-
rior probability of an arc is given by the multiplication of
the probability mass leading to the previous state and the
probability of the arc itself. The expected count of a label
given a lattice is equal to the sum of the posterior proba-
bilities of all arcs in the index for that label with the same
lattice number.
To search for a multi-label expression (e.g., a multi-
word phrase) w1w2 . . . wn we seek on each label in the
expression and then for each (wi, wi+1) join the next
states of wi with the matching previous states of wi+1.
In this way, we retrieve just those path segments in each
lattice that match the entire multi-label expression. The
probability of each match is defined as the multiplication
of the probability mass leading to the previous state of
the first arc and the probabilities of all the arcs in the path
segment. The expected count of a multi-label expression
for the lattice is computed as above.
The answer to a query contains an audio segment only
if the expected count of the query for the lattice corre-
sponding to that audio segment is higher than a threshold.
3.6 User Interface
The user interface description will apply for the three
types of spoken communications (Telephone Conversa-
tions, Teleconferences, Broadcast News) although the au-
dio and speaker quality do vary for each of these types
of spoken communications. Once the user has found the
desired call (or spoken communication) using one of the
retrieval modules (one-best word, one-best phone string,
word lattice, phone lattice, or both word and phone lat-
tice), the user can navigate the call using the user inter-
face elements described below.
For the one-best word index, the Web page in Fig-
ure 5 shows the user interface for searching, browsing,
and playing back this call. The user can browse the call
at any time by clicking on the timeline to start playing at
that location on the timeline. The compressed audio file
(MP3) that was created during the processing would be
streamed to the user. The user can at any time either enter
a word (or word phrase) in the Search box or use one of
the common keywords generated during the keyword ex-
traction process. The text index would be queried and the
results of the search would be shown. The timeline plot
at the top would show all the hits or occurrences of the
word as thin tick marks. The list of hits would be found
under the keyword list. In this case, the word ?chap-
ter? was found 4 times and the time stamps are shown.
The time stamps come from the results of the automatic
speech recognition process when the one-best words and
time stamps were generated. The search term ?chapter?
is shown in bold with 5 context words on either side. The
user can click on any of these 4 hits to start playing where
the hit occurred. The solid band in the timeline indicates
the current position of the audio being played back. The
entire call, in this case, is 9:59 minutes long and the au-
dio is playing at the beginning of the fourth hit at 5:20
minutes. As part of the processing, caption data is gener-
ated in Microsoft?s SAMI (Synchronized Accessible Me-
dia Interchange) format from the one-best word output in
order to show caption text during the playback. The cap-
tion text under the timeline will be updated as the audio
is played. At this point in the call, the caption text is ?but
i did any chapter in a?. This caption option can be dis-
Figure 5: User Interface for ASR One-Best Word Search.
Figure 6: User Interface for Lattice Search.
abled by clicking on the CC icon and can be enabled by
clicking on the CC icon again. The user can also speed
up or slow down the playback at any time by using the
?Speed? button. The speed will toggle from 50% (slow)
to 100% to 150% (fast) to 200% (faster) and then start
over at 50%. The speed, which is currently ?fast?, will be
shown next to the current time above the ?Stop? button.
This allows the user to more quickly peruse the audio file.
A similar Web page in Figure 6 shows the user inter-
face for searching a lattice index. Note that for the same
audio file (or call) and the same search term ?chapter?,
the results of the query show 6 hits compared to the 4
hits in the text index in Figure 5. In this particular case,
the manual transcript does indeed contain these 6 occur-
rences of the word ?chapter?. The search terms were
found in audio segments, which is why the time of the
hit is a time range. The information in brackets is the ex-
pected count and can exceed 1.0 if the search term occurs
more than once in the audio segment. The time range is
reflected in the timeline since the thin tick marks have
been replaced with colored segments. The colors of the
segments correspond to the colors of the hits in the list.
The darker the color, the higher the count and the lighter
the color, the lower the count. Finally, the search can be
refined by altering the threshold using the ?Better Hits?
and ?More Hits? buttons. In this example, the threshold
is set to 0.2 as can be seen under the timeline. If the
user clicks on the ?Better Hits? button, the threshold is
increased so that only better hits are shown. If the ?More
Hits? button is used, the threshold is decreased so more
hits are shown although the hits may not be as good. The
lattice index only returns hits where each hit has a count
above the threshold.
The lattice search user interface allows the user to more
easily find what the user wants and has additional controls
(threshold adjustments) and visual feedback (colored seg-
ments/hits) that are not possible for the text search user
interface.
4 Experimental Results
We used three different corpora to assess the effectiveness
of different techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types.
The second corpus is the Switchboard corpus consist-
ing of two-party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types.
The third corpus is named Teleconference since it con-
sists of multi-party teleconferences on various topics. A
test set of six teleconferences (about 3.5 hours) was tran-
scribed. It contains 31106 word tokens and 2779 word
types. Calls are automatically segmented into a total of
1157 segments prior to ASR.
4.1 Speaker Segmentation
The performance of the speaker segmentation is evalu-
ated as follows. For an audio document, assume there
are N true boundaries, and the algorithm generates M
speaker boundaries. If a detected boundary is within
1 second of a true boundary, it is a correctly detected
boundary, otherwise it is a falsely detected boundary. Let
C denote the number of correctly detected boundaries,
the recall and precision of the boundary detection can be
computed as R = C/N and P = C/M , respectively.
We can combine these two values using the F-measure
F = 2 ? P ? R/(P + R) to measure the speaker seg-
mentation performance.
We evaluated the developed method on three different
types of audio documents: Broadcast News recordings
(16KHz sampling rate, 16 bits/sample), two-party tele-
phone conversations (8KHz, 16bps), and multi-party tele-
conference recordings (8KHz, 16bps). Due to the high
audio quality and well controlled structure of the broad-
cast news program, the achieved F-measure for broadcast
news data is 91%. Teleconference data has the worst au-
dio quality given the various devices (headset, speaker-
phone, etc.) used and different channels (wired and wire-
less) involved. There are also a lot of spontaneous speech
segments less than 1 second long, for example, ?Yes?,
?No?, ?Uh?, etc. These characteristics make the telecon-
ference data the most challenging one to segment. The
F-measure we achieved for this type of data is 70%. The
F-measure for two-party telephone conversations is in the
middle at 82%.
4.2 Automatic Speech Recognition
For evaluating ASR performance, we use the standard
word error rate (WER) as our metric. Since we are in-
terested in retrieval, we use OOV (Out Of Vocabulary)
rate by type to measure the OOV word characteristics.
In Table 1, we present the ASR performance on these
three tasks as well as the OOV Rate by type of the cor-
pora. It is important to note that the recognition vocabu-
lary for the Switchboard and Teleconference tasks are the
same and no data from the Teleconference task was used
while building the ASR systems.
Task WER OOV Rate by Type
Broadcast News ?20% 0.6%
Switchboard ?40% 6%
Teleconference ?50% 12%
Table 1: Word Error Rate and OOV Rate Comparison.
4.3 Retrieval
Our task is to retrieve the audio segments in which the
user query appears. For evaluating retrieval performance,
we use precision and recall with respect to manual tran-
scriptions. Let C(q) be the number of times the query
q is found correctly, M(q) be the number of answers
to the query q, and N(q) be the number of times q is
found in the reference. We compute precision and re-
call rates for each query as P (q) = C(q)/M(q) and
R(q) = C(q)/N(q). We report the average of these
quantities over a set of queries Q, P =
?
q?Q P (q)/|Q|
and R =
?
q?Q R(q)/|Q|. The set of queries Q includes
all the words seen in the reference except for a stop list of
the 100 most common words.
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve. In addition to individual precision-recall
values we also compute the F-measure defined above and
report the maximum F-measure (maxF) to summarize the
information in a precision-recall curve.
In Table 2, a comparison of the maximum F-measure
(maxF) is given for various corpora. Using word lattices
yields a relative gain of 3-5% in maxF over using one-
best word hypotheses. Using both word and phone lat-
tices, the relative gain over the baseline increases to 8-
12%. In this approach, we first search the word index;
if no matches are found then we search the phone index.
This allows the system to return matches even if the user
query is not in the ASR vocabulary.
Task System
1-best W Lats W+P Lats
Broadcast News 84.0 84.8 86.0
Switchboard 57.1 58.4 60.5
Teleconference 47.4 50.3 52.8
Table 2: Maximum F-measure Comparison.
In Figure 7, we present the precision-recall curves.
The gain from using better techniques utilizing word
and phone lattices increases as retrieval performance gets
worse.
0 20 40 60 80 1000
20
40
60
80
100
Precision
Reca
ll
Teleconferences
SwitchboardBroadcast News
1?best Word HypothesesWord LatticesWord and Phone Lattices
Figure 7: Precision vs Recall Comparison.
5 Summary
We presented a system for searching and browsing spo-
ken communications. The system is flexible enough to
support various forms of spoken communications. In this
paper, our focus was on telephone conversations and tele-
conferences. We also presented experimental results for
the speaker segmentation, ASR and retrieval components
of the system.
Acknowledgments
We would like to thank Richard Sproat for useful dis-
cussions and for making his lattice indexing software
(lctools) available for our system.
References
C. Allauzen, M. Mohri, and M. Riley.
2003. DCD Library ? Decoder Library.
http://www.research.att.com/sw/tools/dcd.
T. M. Cover and J. A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley & Sons.
D. Gibbon. 1998. Generating hypermedia documents
from transcriptions of television programs using paral-
lel text alignment. In B. Furht, editor, Handbook of In-
ternet and Multimedia Systems and Applications. CR-
CPress.
J. Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and
G. Zamchick. 2001. Scanmail: Browsing and search-
ing speech data by content. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), Aalborg, Denmark.
A. K. Jain and R. C. Dubes. 1988. Algorithms for Clus-
tering Data. Prentice-Hall.
A. Ljolje, M. Saraclar, M. Bacchiani, M. Collins, and
B. Roark. 2002. The AT&T RT-02 STT system. In
Proc. RT02 Workshop, Vienna, Virginia.
M. Mohri, F. C. N. Pereira, and M. Riley. 1997.
AT&T FSM Library ? Finite-State Machine Library.
http://www.research.att.com/sw/tools/fsm.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
A. Rosenberg, A. Gorin, Z. Liu, and S. Parthasarathy.
2002. Unsupervised speaker segmentation of tele-
phone conversations. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), Denver, Colorado, USA.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002.
Towards automatic closed captioning: Low latency
real time broadcast news transcription. In Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), Denver, Colorado, USA.
L. Stark, S. Whittaker, and J. Hirschberg. 2000. ASR
satisficing: the effects of ASR accuracy on speech re-
trieval. In Proceedings of the International Conference
on Spoken Language Processing (ICSLP).
General Indexation of Weighted Automata ?
Application to Spoken Utterance Retrieval
Cyril Allauzen and Mehryar Mohri and Murat Saraclar
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932
{allauzen, mohri, murat}@research.att.com
Abstract
Much of the massive quantities of digitized
data widely available, e.g., text, speech, hand-
written sequences, are either given directly,
or, as a result of some prior processing, as
weighted automata. These are compact rep-
resentations of a large number of alternative
sequences and their weights reflecting the un-
certainty or variability of the data. Thus,
the indexation of such data requires indexing
weighted automata.
We present a general algorithm for the index-
ation of weighted automata. The resulting in-
dex is represented by a deterministic weighted
transducer that is optimal for search: the search
for an input string takes time linear in the sum
of the size of that string and the number of
indices of the weighted automata where it ap-
pears. We also introduce a general framework
based on weighted transducers that general-
izes this indexation to enable the search for
more complex patterns including syntactic in-
formation or for different types of sequences,
e.g., word sequences instead of phonemic se-
quences. The use of this framework is illus-
trated with several examples.
We applied our general indexation algorithm
and framework to the problem of indexation of
speech utterances and report the results of our
experiments in several tasks demonstrating that
our techniques yield comparable results to pre-
vious methods, while providing greater gener-
ality, including the possibility of searching for
arbitrary patterns represented by weighted au-
tomata.
1 Motivation
Much of the massive quantities of digitized data widely
available is highly variable or uncertain. This uncertainty
affects the interpretation of the data and its computational
processing at various levels, e.g., natural language texts
are abundantly ambiguous, speech and hand-written se-
quences are highly variable and hard to recognize in pres-
ence of noise, biological sequences may be altered or in-
complete.
Searching or indexing such data requires dealing with
a large number of ranked or weighted alternatives. These
may be for example the different parses of an input text,
the various responses to a search engine or information
extraction query, or the best hypotheses of a speech or
hand-written recognition system. In most cases, alterna-
tive sequences can be compactly represented by weighted
automata. The weights may be probabilities or some
other weights used to rank different hypotheses.
This motivates our study of the general problem of
indexation of weighted automata. This is more general
than the classical indexation problems since, typically,
there are many distinct hypotheses or alternatives asso-
ciated with the same index, e.g., a specific input speech
or hand-written sequence may have a large number of dif-
ferent transcriptions according to the system and models
used. Moreover, the problem requires taking into con-
sideration the weight of each alternative, which does not
have a counterpart in classical indexation problems.
We describe a general indexation algorithm for
weighted automata. The resulting index is represented
by a deterministic weighted transducer that is optimal for
search: the search for an input string takes time linear
in the sum of the size of that string and the number of
indices of the weighted automata where it appears.
In some cases, one may wish to search using sequences
in some level, e.g. word sequences, different from the
level of the sequences of the index, e.g. phonemic se-
quences. One may also wish to search for complex se-
quences including both words and parts-of-speech, or re-
strict the search by either restricting the weights or proba-
bilities or the lengths or types of sequences. We describe
a general indexation framework covering all these cases.
Our framework is based on the use of filtering weighted
transducers for restriction or other transducers mapping
between distinct information levels or knowledge struc-
tures. We illustrate the use of this framework with sev-
eral examples that demonstrate its relevance to a number
of indexation tasks.
We applied our framework and algorithms to the par-
ticular problem of speech indexation. In recent years,
spoken document retrieval systems have made large
archives of broadcast news searchable and browsable.
Most of these systems use automatic speech recognition
to convert speech into text, which is then indexed us-
ing standard methods. When a user presents the system
with a query, documents that are relevant to the query are
found using text-based information retrieval techniques.
As speech indexation and retrieval systems move be-
yond the domain of broadcast news to more challenging
spoken communications, the importance for the indexed
material to contain more than just a simple text represen-
tation of the communication is becoming clear. Index-
ation and retrieval techniques must be extended to han-
dle more general representations including for example
syntactic information. In addition to the now familiar re-
trieval systems or search engines, other applications such
as data mining systems can be used to automatically iden-
tify useful patterns in large collections of spoken commu-
nications. Information extraction systems can be used to
gather high-level information such as named-entities.
For a given input speech utterance, a large-vocabulary
speech recognition system often generates a lattice, a
weighted automaton representing a range of alternative
hypotheses with some associated weights or probabilities
used to rank them. When the accuracy of a system is rel-
atively low as in many conversational speech recognition
tasks, it is not safe to rely only on the best hypothesis out-
put by the system. It is then preferable to use instead the
full lattice output by the recognizer.
We report the results of our experiments in sev-
eral tasks demonstrating that our techniques yield
comparable results to the previous methods of
Saraclar and Sproat (2004), while providing greater
generality, including the possibility of searching for
arbitrary patterns represented by weighted automata.
The paper is organized as follows. Section 2 introduces
the notation and the definitions used in the rest of the pa-
per. Section 3 describes our general indexation algorithm
for weighted automata. The algorithm for searching that
index is presented in Section 4 and our general indexa-
tion framework is described and illustrated in Section 5.
Section 6 reports the results of our experiments in several
tasks.
2 Preliminaries
Definition 1 A system (K,?,?, 0, 1) is a semiring
(Kuich and Salomaa, 1986) if: (K,?, 0) is a commuta-
tive monoid with identity element 0; (K,?, 1) is a monoid
with identity element 1; ? distributes over ?; and 0 is an
annihilator for ?: for all a ? K, a? 0 = 0? a = 0.
Thus, a semiring is a ring that may lack negation. Two
semirings often used in speech processing are: the log
semiring L = (R ? {?},?log,+,?, 0) (Mohri, 2002)
which is isomorphic to the familiar real or probability
semiring (R+,+,?, 0, 1) via a log morphism with, for
all a, b ? R ? {?}:
a?log b = ? log(exp(?a) + exp(?b))
and the convention that: exp(??) = 0 and
? log(0) = ?, and the tropical semiring T = (R+ ?
{?},min,+,?, 0) which can be derived from the log
semiring using the Viterbi approximation.
Definition 2 A weighted finite-state transducer T over a
semiring K is an 8-tuple T = (?,?, Q, I, F,E, ?, ?)
where: ? is the finite input alphabet of the transducer;
? is the finite output alphabet; Q is a finite set of states;
I ? Q the set of initial states; F ? Q the set of final
states; E ? Q? (?? {})? (?? {})?K?Q a finite
set of transitions; ? : I ? K the initial weight function;
and ? : F ? K the final weight function mapping F to
K.
A Weighted automaton A = (?, Q, I, F,E, ?, ?) is de-
fined in a similar way by simply omitting the output la-
bels. We denote by L(A) ? ?? the set of strings ac-
cepted by an automaton A and similarly by L(X) the
strings described by a regular expression X . We denote
by |A| = |Q|+ |E| the size of A.
Given a transition e ? E, we denote by i[e] its input
label, p[e] its origin or previous state and n[e] its desti-
nation state or next state, w[e] its weight, o[e] its output
label (transducer case). Given a state q ? Q, we denote
by E[q] the set of transitions leaving q.
A path pi = e1 ? ? ? ek is an element of E? with con-
secutive transitions: n[ei?1] = p[ei], i = 2, . . . , k. We
extend n and p to paths by setting: n[pi] = n[ek] and
p[pi] = p[e1]. A cycle pi is a path whose origin and
destination states coincide: n[pi] = p[pi]. We denote by
P (q, q?) the set of paths from q to q? and by P (q, x, q?)
and P (q, x, y, q?) the set of paths from q to q? with in-
put label x ? ?? and output label y (transducer case).
These definitions can be extended to subsets R,R? ? Q,
by: P (R, x,R?) = ?q?R, q??R?P (q, x, q?). The label-
ing functions i (and similarly o) and the weight func-
tion w can also be extended to paths by defining the la-
bel of a path as the concatenation of the labels of its
constituent transitions, and the weight of a path as the
?-product of the weights of its constituent transitions:
i[pi] = i[e1] ? ? ? i[ek], w[pi] = w[e1] ? ? ? ? ? w[ek]. We
also extend w to any finite set of paths ? by setting:
w[?] = ?pi?? w[pi]. The output weight associated by
A to each input string x ? ?? is:
[[A]](x) =
?
pi?P (I,x,F )
?(p[pi]) ? w[pi]? ?(n[pi])
[[A]](x) is defined to be 0 when P (I, x, F ) = ?. Simi-
larly, the output weight associated by a transducer T to a
pair of input-output string (x, y) is:
[[T ]](x, y) =
?
pi?P (I,x,y,F )
?(p[pi]) ? w[pi]? ?(n[pi])
[[T ]](x, y) = 0 when P (I, x, y, F ) = ?. A successful
path in a weighted automaton or transducer M is a path
from an initial state to a final state. M is unambiguous if
for any string x ? ?? there is at most one successful path
labeled with x. Thus, an unambiguous transducer defines
a function.
For any transducer T , denote by ?2(T ) the automaton
obtained by projecting T on its output, that is by omitting
its input labels.
Note that the second operation of the tropical semiring
and the log semiring as well as their identity elements are
identical. Thus the weight of a path in an automaton A
over the tropical semiring does not change if A is viewed
as a weighted automaton over the log semiring or vice-
versa.
Given two strings u and v in ??, v is a factor of u if
u = xvy for some x and y in ??; if y =  then v is also
a suffix of u. More generally, v is a factor (resp. suffix) of
L ? ?? if v is a suffix (resp. factor) of some u ? L. We
denote by |x| the length of a string x ? ??.
3 Indexation Algorithm
This section presents an algorithm for the construction of
an efficient index for a large set of speech utterances.
We assume that for each speech utterance ui of the
dataset considered, i = 1, . . . , n, a weighted automaton
Ai over the alphabet ? and the log semiring, e.g., phone
or word lattice output by an automatic speech recognizer,
is given. The problem consists of creating a full index,
that is one that can be used to search directly any factor
of any string accepted by these automata. Note that this
problem crucially differs from classical indexation prob-
lems in that the input data is uncertain. Our algorithm
must make use of the weights associated to each string
by the input automata.
The main idea behind the design of the algorithm de-
scribed is that the full index can be represented by a
weighted finite-state transducer T mapping each factor
x to the set of indices of the automata in which x appears
and the negative log of the expected count of x. More
precisely, let Pi be the probability distribution defined by
the weighted automaton Ai over the set of strings ?? and
let Cx(u) denote the number of occurrences of a factor
x in u, then, for any factor x ? ?? and automaton index
i ? {1, . . . , n}:
[[T ]](x, i) = ? log(EPi [Cx]) (1)
Our algorithm for the construction of the index is simple,
it is based on general weighted automata and transducer
algorithms. We describe the consecutive stages of the al-
gorithm.
This algorithm can be seen as a generalization to
weighted automata of the notion of suffix automaton and
factor automaton for strings. The suffix (factor) automa-
ton of a string u is the minimal deterministic finite au-
tomata recognizing exactly the set of suffixes (resp. fac-
tors) of u (Blumer et al, 1985; Crochemore, 1986). The
size of both automata is linear in the length of u and both
can be built in linear time. These are classical repre-
sentations used in text indexation (Blumer et al, 1987;
Crochemore, 1986).
3.1 Preprocessing
When the automata Ai are word or phone lattices out-
put by a speech recognition or other natural language
processing system, the path weights correspond to joint
probabilities. We can apply to Ai a general weight-
pushing algorithm in the log semiring (Mohri, 1997)
which converts these weights into the desired (negative
log of) posterior probabilities. More generally, the path
weights in the resulting automata can be interpreted as
log-likelihoods. We denote by Pi the corresponding
probability distribution. When the input automaton Ai is
acyclic, the complexity of the weight-pushing algorithm
is linear in its size (O(|Ai|)). Figures 1(b)(d) illustrates
the application of the algorithm to the automata of Fig-
ures 1(a)(c).
3.2 Construction of Transducer Index T
Let Bi = (?, Qi, Ii, Fi, Ei, ?i, ?i) denote the result of
the application of the weight pushing algorithm to the au-
tomaton Ai. The weight associated by Bi to each string
it accepts can be interpreted as the log-likelihood of that
string for the utterance ui given the models used to gen-
erate the automata. More generally, Bi defines a proba-
bility distribution Pi over all strings x ? ?? which is just
the sum of the probability of all paths of Bi in which x
appears.
For each state q ? Qi, denote by d[q] the shortest dis-
tance from Ii to q (or -log of the forward probability) and
by f [q] the shortest distance from q to F (or -log of the
backward probability):
d[q] =
?
log
pi?P (Ii,q)
(?i(p[pi]) + w[pi]) (2)
0
1a
2b
b
3a 0
1a/0.5
2b/0.5
b/1
3/1a/1
(a) (b)
0
1b/1
2a/2
a/1
3/1b/1 0
1b/0.333
2a/0.666
a/1
3/1b/1
(c) (d)
Figure 1: Weighted automata over the real semiring (a) A1, (b) B1 obtained by applying weight pushing to A1, (c) A2
and (d) B2 obtained by applying weight pushing to A2.
f [q] =
?
log
pi?P (q,Fi)
(w[pi] + ?i(n[pi])) (3)
The shortest distances d[q] and f [q] can be computed for
all states q ? Qi in linear time (O(|Bi|)) when Bi is
acyclic (Mohri, 2002). Then,
? log(EPi [Cx]) =
?
log
i[pi]=x
d[p[pi]] + w[pi] + f [n[pi]] (4)
From the weighted automaton Bi, one can derive a
weighted transducer Ti in two steps:
1. Factor Selection. In the general case we select all
the factors to be indexed in the following way:
? Replace each transition (p, a, w, q) ? Qi???
R?Qi by (p, a, a, w, q) ? Qi?????R?Qi;
? Create a new state s 6? Qi and make s the
unique initial state;
? Create a new state e 6? Qi and make e the
unique final state;
? Create a new transition (s, , , d[q], q) for each
state q ? Qi;
? Create a new transition (q, , i, f [q], e) for each
state q ? Qi;
2. Optimization. The resulting transducer can be op-
timized by applying weighted -removal, weighted
determinization, and minimization over the log
semiring by viewing it as an acceptor, i.e., input-
output labels are encoded a single labels.
It is clear from Equation 4 that for any factor x ? ??:
[[Ti]](x, i) = ? log(EPi [Cx]) (5)
This construction is illustrated by Figures 2(a)(b). Our
full index transducer T is the constructed by
? taking the ?log-sum (or union) of all the transducers
Ti, i = 1, . . . , n;
? defining T as the result of determinization (in the
log semiring) applied to that transducer.
Figure 3 is illustrating this construction and optimization.
0
1
a:?/0.5
2
b:?/0.5
5/1
?:1/1
b:?/1
?:1/1
?:1/1
3a:?/1
?:1/1
4
?:?/1
?:?/0.5
?:?/1
?:?/1
(a)
0 1/1
?:1/3.5
2a:?/1.5
3
b:?/1
?:1/1
b:?/0.333 ?:1/1
4
a:?/1 ?:1/1
(b)
Figure 2: Construction of T1 index of the weighted au-
tomata B1 given Figure 1(b): (a) intermediary result after
factor selection and (b) resulting weighted transducer T1.
4 Search
The full index represented by the weighted finite-state
transducer T is optimal. Indeed, T contains no transi-
tion with input  other than the final transitions labeled
with an output index and it is deterministic. Thus, the
set of indices Ix of the weighted automata containing a
factor x can be obtained in O(|x|+ |Ix|) by reading in T
the unique path with input label x and then the transitions
with input  which have each a distinct output label.
The user?s query is typically an unweighted string, but
it can be given as an arbitrary weighted automaton X .
This covers the case of Boolean queries or regular expres-
sions which can be compiled into automata. The response
to a query X is computed using the general algorithm of
composition of weighted transducers (Mohri et al, 1996)
followed by projection on the output:
?2(X ? T ) (6)
which is then -removed and determinized to give di-
rectly the list of all indices and their corresponding log-
01
a:?/2.5
2b:?/2.333
3/1?:1/3.5
?:2/3.333
?:1/0.600
?:2/0.400
4b:?/0.600
?:1/0.428
?:2/0.571
5a:?/0.571
?:1/0.333
?:2/0.666
6
a:?/0.333
?:1/0.75
?:2/0.25
7b:?/0.25
8/1?:1/1
9/1?:2/1
Figure 3: Weighted transducer T obtained by index-
ing the weighted automata B1 and B2 given in Fig-
ures 1(b)(d)
likelihoods. The final result can be pruned to include only
the most likely responses. The pruning threshold may be
used to vary the number of responses.
5 General Indexation Framework
The indexation technique just outlined can be easily ex-
tended to include many of the techniques used for speech
indexation. This can be done by introducing a transducer
F that converts between different levels of information
sources or structures, or that filters out or reweights index
entries. The filter F can be applied (i) before, (ii) during
or (iii) after the construction of the index. For case (i), the
filter is used directly on the input and the indexation algo-
rithm is applied to the weighted automata (F ?Ai)1?i?n.
For case (ii), filtering is done after the factor selection
step of the algorithm and the filter applies to the factors,
typically to restrict the factors that will be indexed. For
case (iii), the filter is applied to the index. Obviously
different filters can be used in combination at different
stages.
When such a filter is used, the response to a query X is
obtained using another transducer F ? 1 and the following
composition and projection:
?2(X ? F ? ? T ) (7)
Since composition is associative, it does not impose a
specific order to its application. However, in practice,
it is often advantageous to compute X ? F ? before appli-
cation of T . The following are examples of some filter
transducers that can be of interest in many applications.
1In most cases, F ? is the inverse of F .
? Pronunciation Dictionary: a pronunciation dic-
tionary can be used to map word sequences into
their phonemic transcriptions, thus transform word
lattices into equivalent phone lattices. This map-
ping can represented by a weighted transducer F .
Using an index based on phone lattices allows a
user to search for words that are not in the ASR
vocabulary. In this case, the inverse transduc-
tion F ? is a grapheme to phoneme converter, com-
monly present in TTS front-ends. Among others,
Witbrock and Hauptmann (1997) present a system
where a phonetic transcript is obtained from the
word transcript and retrieval is performed using both
word and phone indices.
? Vocabulary Restriction: in some cases using a full
index can be prohibitive and unnecessary. It might
be desirable to do partial indexing by ignoring some
words (or phones) in the input. For example, we
might wish to index only ?named entities?, or just
the consonants. This is mostly motivated by the
reduction of the size of the index while retaining
the necessary information. A similar approach is
to apply a many to one mapping to index groups of
phones, or metaphones (Amir et al, 2001), to over-
come phonetic errors.
? Reweighting: a weighted transducer can be used
to emphasize some words in the input while de-
emphasizing other. The weights, for example might
correspond to TF-IDF weights. Another reweight-
ing method might involve edit distance or confusion
statistics.
? Classification: an extreme form of summarizing the
information contained in the indexed material is to
assign a class label, such as a topic label, to each
input. The query would also be classified and all
answers with the same class label would be returned
as relevant.
? Length Restriction: a common way of indexing
phone strings is to index fixed length overlapping
phone strings (Logan et al, 2002). This results in a
partial index with only fixed length strings. More
generally a minimum and maximum string length
may be imposed on the index. An example restric-
tion automaton is given in Figure 4. In this case,
the filter applies to the factors and has to be applied
during or after indexation. The restricted index will
be smaller in size but contains less information and
may result in degradation in retrieval performance,
especially for long queries.
The length restriction filter requires a modification of
the search procedure. Assume a fixed ? say r ? length
restriction filter and a string query of length k. If k < r,
0 1ab 2
a
b
(a)
0
1a:?/2.5
4
b:?/2.333
2b:?/0.600
5a:?/0.571
3/1
?:1/0.333
?:2/0.666
?:1/0.75
?:2/0.25
(b)
Figure 4: (a) Filter F restricting to strings of length 2. (b)
Restricted index F ? T , where T is the weighted trans-
ducer given in Figure 3(b).
then we need to pad the input to length r with ?r?k. If
k ? r, then we must search for all substrings of length r
in the index. A string is present in a certain lattice if all its
substrings are (and not vice versa). So, the results of each
substring search must be intersected. The probability of
each substring xi+r?1i for i ? {1, . . . , k + 1 ? r} is an
upper bound on the probability of the string xk1 , and the
count of each substring is an upper bound on the count of
the string, so for i ? {1, . . . , k + 1? r}
EP [C(xk1)] ? EP [C(xi+r?1i )].
Therefore, the intersection operation must use minimum
for combining the expected counts of substrings. In other
words, the expected count of the string is approximated
by the minimum of the probabilities of each of its sub-
strings,
EP [C(xk1)] ? min1?i?k+1?r EP [C(x
i+r?1
i )].
In addition to a filter transducer, pruning can be ap-
plied at different stages of the algorithm to reduce the
size of the index. Pruning eliminates least likely paths in
a weighted automaton or transducer. Applying pruning
to Ai can be seen as part of the process that generates the
uncertain input data. When pruning is applied to Bi, only
the more likely alternatives will be indexed. If pruning is
applied to Ti, or to T , pruning takes the expected counts
into consideration and not the probabilities. Note that the
threshold used for this type of pruning is directly compa-
rable to the threshold used for pruning the search results
in Section 4 since both are thresholds on expected counts.
6 Experimental Results
Our task is retrieving the utterances (or short audio seg-
ments) that a given query appears in. The experimental
setup is identical to that of Saraclar and Sproat (2004).
Since, we take the system described there as our base-
line, we give a brief review of the basic indexation al-
gorithm used there. The algorithm uses the same pre-
processing step. For each label in ?, an index file is
constructed. For each arc a that appears in the prepro-
cessed weighted automaton Bi, the following informa-
tion is stored: (i, p[a], n[a], d[p[a]], w[a]). Since the pre-
processing ensures that f [q] = 0 for all q in Bi, it is pos-
sible to compute ? log(EPi [Cx]) as in Equation 4 using
the information stored in the index.
6.1 Evaluation Metrics
For evaluating retrieval performance we use precision
and recall with respect to manual transcriptions. Let
Correct(q) be the number of times the query q is found
correctly, Answer(q) be the number of answers to the
query q, and Reference(q) be the number of times q is
found in the reference.
Precision(q) = Correct(q)Answer(q)
Recall(q) = Correct(q)Reference(q)
We compute precision and recall rates for each query and
report the average over all queries. The set of queries Q
includes all the words seen in the reference except for a
stoplist of 100 most common words.
Precision = 1|Q|
?
q?Q
Precision(q)
Recall = 1|Q|
?
q?Q
Recall(q)
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve.
In addition to individual precision-recall values we
also compute the F-measure defined as
F = 2? Precision? RecallPrecision + Recall
and report the maximum F-measure (maxF) to summa-
rize the information in a precision-recall curve.
6.2 Corpora
We use three different corpora to assess the effectiveness
of different retrieval techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types. For ASR we use a real-time system (Saraclar
et al, 2002). Since the system was designed for SDR,
the recognition vocabulary of the system has over 200K
words.
The second corpus is the Switchboard corpus consist-
ing of two party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types. For ASR we use the first pass of the evalua-
tion system (Ljolje et al, 2002). The recognition vocab-
ulary of the system has over 45K words.
The third corpus is named Teleconferences since it con-
sists of multi-party teleconferences on various topics. A
test set of six teleconferences (about 3.5 hours) was tran-
scribed. It contains 31106 word tokens and 2779 word
types. Calls are automatically segmented into a total of
1157 segments prior to ASR. We again use the first pass
of the Switchboard evaluation system for ASR.
We use the AT&T DCD Library (Allauzen et al, 2003)
as our ASR decoder and our implementation of the algo-
rithm is based on the AT&T FSM Library (Mohri et al,
2000), both of which are available for download.
6.3 Results
We implemented some of the proposed techniques and
made comparisons with the previous method used by
Saraclar and Sproat (2004). The full indexing method
consumed too much time while indexing Broadcast News
lattices and used too much memory while indexing phone
lattices for Teleconferences. In the other cases, we con-
firmed that the new method yields identical results. In
Table 1 we compare the index sizes for full indexing and
partial indexing with the previous method. In both cases,
the input lattices are pruned so that the cost (negative log
probability) difference between two paths is less than six.
Although the new method results in much smaller index
sizes for the string case (i.e. nbest=1), it can result in very
large index sizes for full indexing of lattices (cost=6).
However, partial indexing by length restriction solves this
problem. For the results reported in Table 1, the length of
the word strings to be indexed was restricted to be less
than or equal to four, and the length of the phone strings
to be indexed was restricted to be exactly four.
In Saraclar and Sproat (2004), it was shown that using
word lattices yields a relative gain of 3-5% in maxF over
using best word hypotheses. Furthermore, it was shown
that a ?search cascade? strategy for using both word and
phone indices increases the relative gain over the baseline
to 8-12%. In this strategy, we first search the word index
for the given query, if no matches are found we search
the phone index. Using the partial indices, we obtained
a precision recall performance that is almost identical to
the one obtained with the previous method. Comparison
of the maximum F-measure for both methods is given in
Table 2.
Task Previous Method Partial Index
Broadcast News 86.0 86.1
Switchboard 60.5 60.8
Teleconferences 52.8 52.7
Table 2: Comparison of maximum F-measure for three
corpora.
As an example, we used a filter that indexes only con-
sonants (i.e. maps the vowels to ). The resulting index
was used instead of the full phone index. The size of
the consonants only index was 370MB whereas the size
of the full index was 431MB. In Figure 5 we present the
precision recall performance of this consonant only in-
dex.
30 40 50 60 70 8030
40
50
60
70
80
Precision
R
ec
al
l
Word Index
Word and Phone Index
Word and Phone (consonants only) Index
Figure 5: Comparison of Precision vs Recall Perfor-
mance for Switchboard.
7 Conclusion
We described a general framework for indexing uncer-
tain input data represented as weighted automata. The
indexation algorithm utilizes weighted finite-state algo-
rithms to obtain an index represented as a weighted finite-
state transducer. We showed that many of the techniques
used for speech indexing can be implemented within this
framework. We gave comparative results to a previous
method for lattice indexing.
The same idea and framework can be used for indexa-
tion in natural language processing or other areas where
uncertain input data is given as weighted automata. The
complexity of the index construction algorithm can be
improved in some general cases using techniques simi-
lar to classical string matching ones (Blumer et al, 1985;
Task Type Pruning Previous Method Full Index Partial Index
Broadcast News word nbest=1 29 2.7 ?
Broadcast News word cost=6 91 ? 25
Broadcast News phone cost=6 27 ? 14
Switchboard word nbest=1 18 4.7 ?
Switchboard word cost=6 90 99 88
Switchboard phone cost=6 97 431 41
Teleconferences word nbest=1 16 2.6 ?
Teleconferences word cost=6 142 352 184
Teleconferences phone cost=6 146 ? 69
Table 1: Comparison of Index Sizes in MegaBytes.
Crochemore, 1986; Blumer et al, 1987). Various prun-
ing techniques can be applied to reduce the size of the
index without significantly degrading performance. Fi-
nally, other types of filters that make use of the general
framework can be investigated.
Acknowledgments
We wish to thank our colleague Richard Sproat for useful
discussions and the use of the lattice indexing software
(lctools) used in our baseline experiments.
References
Cyril Allauzen, Mehryar Mohri, and Michael Ri-
ley. 2003. DCD Library - Decoder Library.
http://www.research.att.com/sw/tools/dcd.
Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001.
Advances in phonetic word spotting. In Proceedings
of the Tenth International Conference on Information
and Knowledge Management, pages 580?582, Atlanta,
Georgia, USA.
Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht,
David Haussler, and Joel Seiferas. 1985. The smallest
automaton recognizing the subwords of a text. Theo-
retical Computer Science, 40(1):31?55.
Anselm Blumer, Janet Blumer, David Haussler, Ross Mc-
Connel, and Andrzej Ehrenfeucht. 1987. Complete
inverted files for efficient text retrieval and analysis.
Journal of the ACM, 34(3):578?595.
Maxime Crochemore. 1986. Transducers and repeti-
tions. Theoretical Computer Science, 45(1):63?86.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Andrej Ljolje, Murat Saraclar, Michiel Bacchiani,
Michael Collins, and Brian Roark. 2002. The AT&T
RT-02 STT system. In Proc. RT02 Workshop, Vienna,
Virginia.
Beth Logan, Pedro Moreno, and Om Deshmukh. 2002.
Word and sub-word indexing approaches for reducing
the effects of OOV queries on spoken audio. In Proc.
HLT.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 1996. Weighted Automata in Text and Speech
Processing. In Proceedings of the 12th biennial Euro-
pean Conference on Artificial Intelligence (ECAI-96),
Workshop on Extended finite state models of language,
Budapest, Hungary.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The Design Principles of a
Weighted Finite-State Transducer Library. The-
oretical Computer Science, 231:17?32, January.
http://www.research.att.com/sw/tools/fsm.
Mehryar Mohri. 1997. Finite-State Transducers in Lan-
guage and Speech Processing. Computational Lin-
guistics, 23:2.
Mehryar Mohri. 2002. Semiring Frameworks and Algo-
rithms for Shortest-Distance Problems. Journal of Au-
tomata, Languages and Combinatorics, 7(3):321?350.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In Proc. HLT-
NAACL.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real-time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, Colorado, USA.
Michael Witbrock and Alexander Hauptmann. 1997. Us-
ing words and phonetic strings for efficient informa-
tion retrieval from imperfectly transcribed spoken doc-
uments. In 2nd ACM International Conference on Dig-
ital Libraries (DL?97), pages 30?35, Philadelphia, PA,
July.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 487?494,
New York, June 2006. c?2006 Association for Computational Linguistics
Unlimited vocabulary speech recognition for agglutinative languages
Mikko Kurimo1, Antti Puurula1, Ebru Arisoy2, Vesa Siivola1,
Teemu Hirsim?ki1, Janne Pylkk?nen1, Tanel Alum?e3, Murat Saraclar2
1 Adaptive Informatics Research Centre, Helsinki University of Technology
P.O.Box 5400, FIN-02015 HUT, Finland
{Mikko.Kurimo,Antti.Puurula,Vesa.Siivola}@tkk.fi
2 Bogazici University, Electrical and Electronics Eng. Dept.
34342 Bebek, Istanbul, Turkey
{arisoyeb,murat.saraclar}@boun.edu.tr
3 Laboratory of Phonetics and Speech Technology,
Institute of Cybernetics, Tallinn Technical University, Estonia
tanel.alumae@phon.ioc.ee
Abstract
It is practically impossible to build a
word-based lexicon for speech recogni-
tion in agglutinative languages that would
cover all the relevant words. The prob-
lem is that words are generally built by
concatenating several prefixes and suffixes
to the word roots. Together with com-
pounding and inflections this leads to mil-
lions of different, but still frequent word
forms. Due to inflections, ambiguity and
other phenomena, it is also not trivial to
automatically split the words into mean-
ingful parts. Rule-based morphological
analyzers can perform this splitting, but
due to the handcrafted rules, they also suf-
fer from an out-of-vocabulary problem. In
this paper we apply a recently proposed
fully automatic and rather language and
vocabulary independent way to build sub-
word lexica for three different agglutina-
tive languages. We demonstrate the lan-
guage portability as well by building a
successful large vocabulary speech recog-
nizer for each language and show superior
recognition performance compared to the
corresponding word-based reference sys-
tems.
1 Introduction
Speech recognition for dictation or prepared radio
and television broadcasts has had huge advances
during the last decades. For example, broadcast
news (BN) in English can now be recognized with
about ten percent word error rate (WER) (NIST,
2000) which results in mostly quite understandable
text. Some rare and new words may be missing but
the result has proven to be sufficient for many im-
portant applications, such as browsing and retrieval
of recorded speech and information retrieval from
the speech (Garofolo et al, 2000). However, besides
the development of powerful computers and new al-
gorithms, a crucial factor in this development is the
vast amount of transcribed speech and suitable text
data that has been collected for training the mod-
els. The problem faced in porting the BN recogni-
tion systems to conversational speech or to other lan-
guages is that almost as much new speech and text
data have to be collected again for the new task.
The reason for the need for a vast amount of train-
ing texts is that the state-of-the-art statistical lan-
guage models contain a huge amount of parameters
to be estimated in order to provide a proper probabil-
ity for any possible word sequence. The main reason
for the huge model size is that for an acceptable cov-
erage in an English BN task, the vocabulary must
be very large, at least 50,000 words, or more. For
languages with a higher degree of word inflections
than English, even larger vocabularies are required.
This paper focuses on the agglutinative languages in
which words are frequently formed by concatenat-
ing one or more stems, prefixes, and suffixes. For
these languages in which the words are often highly
inflected as well as formed from several morphemes,
even a vocabulary of 100,000 most common words
would not give sufficient coverage (Kneissler and
487
Klakow, 2001; Hirsim?ki et al, 2005). Thus, the
solution to the language modeling clearly has to in-
volve splitting of words into smaller modeling units
that could then be adequately modeled.
This paper focuses on solving the vocabulary
problem for several languages in which the speech
and text database resources are much smaller than
for the world?s main languages. A common fea-
ture for the agglutinative languages, such as Finnish,
Estonian, Hungarian and Turkish is that the large
vocabulary continuous speech recognition (LVCSR)
attempts so far have not resulted comparable perfor-
mance to the English systems. The reason for this
is not only the language modeling difficulties, but,
of course, the lack of suitable speech and text train-
ing data resources. In (Geutner et al, 1998; Sii-
vola et al, 2001) the systems aim at reducing the
active vocabulary and language models to a feasi-
ble size by clustering and focusing. In (Szarvas and
Furui, 2003; Alum?e, 2005; Hacioglu et al, 2003)
the words are split into morphemes by language-
dependent hand-crafted morphological rules. In
(Kneissler and Klakow, 2001; Arisoy and Arslan,
2005) different combinations of words, grammati-
cal morphemes and endings are utilized to decrease
the OOV rate and optimize the speech recognition
accuracy. However, constant large improvements
over the conventional word-based language models
in LVCSR have been rare.
The approach presented in this paper relies on a
data-driven algorithm called Morfessor (Creutz and
Lagus, 2002; Creutz and Lagus, 2005) which is a
language independent unsupervised machine learn-
ing method to find morpheme-like units (called sta-
tistical morphs) from a large text corpus. This
method has several advantages over the rule-based
grammatical morphemes, e.g. that no hand-crafted
rules are needed and all words can be processed,
even the foreign ones. Even if good grammatical
morphemes are available, the language modeling re-
sults by the statistical morphs seem to be at least as
good, if not better (Hirsim?ki et al, 2005). In this
paper we evaluate the statistical morphs for three
agglutinative languages and describe three different
speech recognition systems that successfully utilize
the n-gram language models trained for these units
in the corresponding LVCSR tasks.
2 Building the lexicon and language
models
2.1 Unsupervised discovery of morph units
Naturally, there are many ways to split the words
into smaller units to reduce a lexicon to a tractable
size. However, for a subword lexicon suitable
for language modeling applications such as speech
recognition, several properties are desirable:
1. The size of the lexicon should be small enough
that the n-gram modeling becomes more feasi-
ble than the conventional word based modeling.
2. The coverage of the target language by words
that can be built by concatenating the units
should be high enough to avoid the out-of-
vocabulary problem.
3. The units should be somehow meaningful, so
that the previously observed units can help in
predicting the next one.
4. In speech recognition one should be able to de-
termine the pronunciation for each unit.
A common approach to find the subword units
is to program the language-dependent grammatical
rules into a morphological analyzer and utilize that
to then split the text corpus into morphemes as in
e.g. (Hirsim?ki et al, 2005; Alum?e, 2005; Ha-
cioglu et al, 2003). There are some problems re-
lated to ambiguous splits and pronunciations of very
short inflection-type units, but also the coverage in,
e.g., news texts may be poor because of many names
and foreign words.
In this paper we have adopted a similar approach
as (Hirsim?ki et al, 2005). We use unsupervised
learning to find the best units according to some cost
function. In the Morfessor algorithm the minimized
cost is the coding length of the lexicon and the words
in the corpus represented by the units of the lexicon.
This minimum description length based cost func-
tion is especially appealing, because it tends to give
units that are both as frequent and as long as possi-
ble to suit well for both training the language models
and also decoding of the speech. Full coverage of
the language is also guaranteed by splitting the rare
words into very short units, even to single phonemes
if necessary. For language models utilized in speech
488
recognition, the lexicon of the statistical morphs can
be further reduced by omitting the rare words from
the input of the Morfessor algorithm. This operation
does not reduce the coverage of the lexicon, because
it just splits the rare words then into smaller units,
but the smaller lexicon may offer a remarkable speed
up of the recognition.
The pronunciation of, especially, the short units
may be ambiguous and may cause severe problems
in languages like English, in which the pronuncia-
tions can not be adequately determined from the or-
thography. In most agglutinative languages, such as
Finnish, Estonian and Turkish, rather simple letter-
to-phoneme rules are, however, sufficient for most
cases.
2.2 Building the lexicon for open vocabulary
The whole training text corpus is first passed through
a word splitting transformation as in Figure 1. Based
on the learned subword unit lexicon, the best split
for each word is determined by performing a Viterbi
search with the unigram probabilities of the units. At
this point the word break symbols are added between
each word in order to incorporate that information in
the statistical language models, as well. Then the n-
gram models are trained similarly as if the language
units were words including word and sentence break
symbols as additional units.
2.3 Building the n-gram model over morphs
Even though the required morph lexicon is much
smaller than the lexicon for the corresponding word
n-gram estimation, the data sparsity problem is still
important. Interpolated Kneser-Ney smoothing is
utilized to tune the language model probabilities in
the same way as found best for the word n-grams.
The n-grams that are not very useful for modeling
the language can be discarded from the model in
order to keep the model size down. For Turkish,
we used the entropy based pruning (Stolcke, 1998),
where the n-grams, that change the model entropy
less than a given treshold, are discarded from the
model. For Finnish and Estonian, we used n-gram
growing (Siivola and Pellom, 2005). The n-grams
that increase the training set likelihood enough with
respect to the corresponding increase in the model
size are accepted into the model (as in the minimum
description length principle). After the growing pro-
Morph lexicon
+ probabilities
word forms
Distinct
Text with words
segmented into
morphs
model
Language
Text corpus
segmentation
Viterbi
segmentation
MorphExtract
vocabulary
Train
n?grams
Figure 1: The steps in the process of estimating a
language model based on statistical morphs from a
text corpus (Hirsim?ki et al, 2005).
cess the model is further pruned with entropy based
pruning. The method allows us to train models with
higher order n-grams, since the memory consump-
tion is lower and also gives somewhat better mod-
els. Both methods can also be viewed as choosing
the correct model complexity for the training data to
avoid over-learning.
3 Statistical properties of Finnish,
Estonian and Turkish
Before presenting the speech recognition results,
some statistical properties are presented for the three
agglutinative languages studied. If we consider
choosing a vocabulary of the 50k-70k most common
words, as usual in English broadcast news LVCSR
systems, the out-of-vocabulary (OOV) rate in En-
glish is typically smaller than 1%. Using the lan-
guage model training data the following OOV rates
can be found for a vocabulary including only the
most common words: 15% OOV for 69k in Finnish
(Hirsim?ki et al, 2005), 10% for 60k in Estonian
and 9% for 50k in Turkish. As shown in (Hacioglu et
al., 2003) this does not only mean the same amount
of extra speech recognition errors, but even more,
because the recognizer tends to lose track when un-
known words get mapped to those that are in the vo-
cabulary. Even doubling the vocabulary is not a suf-
489
0 1 2 3
x 106
0
2
4
6
8 x 10
5
Number of sentences
N
um
be
r o
f d
ist
in
ct
 u
ni
ts
0 1 2 3
x 106
2.6
2.8
3
3.2
3.4
3.6 x 10
4
Number of sentences
N
um
be
r o
f d
ist
in
ct
 m
or
ph
s
Morphs
Words Morphs
Figure 2: Vocabulary growth of words and morphs
for Turkish language
ficient solution, because a vocabulary twice as large
(120k) would only reduce the OOV rate to 6% in
Estonian and 5% in Turkish. In Finnish even a 400k
vocabulary of the most common words still gives 5%
OOV in the language model training material.
Figure 2 illustrates the vocabulary explosion en-
countered when using words and how using morphs
avoids this problem for Turkish. The figure on the
left shows the vocabulary growth for both words and
morphs. The figure on the right shows the graph
for morphs in more detail. As seen in the figure,
the number of new words encountered continues to
increase as the corpus size gets larger whereas the
number of new morphs encountered levels off.
4 Speech recognition experiments
4.1 About selection of the recognition tasks
In this work the morph-based language models have
been applied in speech recognition for three differ-
ent agglutinative languages, Finnish, Estonian and
Turkish. The recognition tasks are speaker depen-
dent and independent fluent dictation of sentences
taken from newspapers and books, which typically
require very large vocabulary language models.
4.2 Finnish
Finnish is a highly inflected language, in which
words are formed mainly by agglutination and com-
pounding. Finnish is also the language for which the
algorithm for the unsupervised morpheme discovery
(Creutz and Lagus, 2002) was originally developed.
The units of the morph lexicon for the experiments
in this paper were learned from a joint corpus con-
taining newspapers, books and newswire stories of
totally about 150 million words (CSC, 2001). We
obtained a lexicon of 25k morphs by feeding the
learning algorithm with the word list containing the
160k most common words. For language model
training we used the same text corpus and the re-
cently developed growing n-gram training algorithm
(Siivola and Pellom, 2005). The amount of resulted
n-grams are listed in Table 4. The average length
of a morph is such that a word corresponds to 2.52
morphs including a word break symbol.
The speech recognition task consisted of a book
read aloud by one female speaker as in (Hirsim?ki et
al., 2005). Speaker dependent cross-word triphone
models were trained using the first 12 hours of data
and evaluated by the last 27 minutes. The models
included tied state hidden Markov models (HMMs)
of totally 1500 different states, 8 Gaussian mixtures
(GMMs) per state, short-time mel-cepstral features
(MFCCs), maximum likelihood linear transforma-
tion (MLLT) and explicit phone duration models
(Pylkk?nen and Kurimo, 2004). The real-time fac-
tor of recognition speed was less than 10 xRT with
a 2.2 GHz CPU. However, with the efficient LVCSR
decoder utilized (Pylkk?nen, 2005) it seems that by
making an even smaller morph lexicon, such as 10k,
the decoding speed could be optimized to only a few
times real-time without an excessive trade-off with
recognition performance.
4.3 Estonian
Estonian is closely related to Finnish and a similar
language modeling approach was directly applied
to the Estonian recognition task. The text corpus
used to learn the morph units and train the statis-
tical language model consisted of newspapers and
books, altogether about 55 million words (Segakor-
pus, 2005). At first, 45k morph units were obtained
as the best subword unit set from the list of the 470k
most common words in the corpora. For speed-
ing up the recognition, the morph lexicon was after-
wards reduced to 37k by splitting the rarest morphs
(occurring in only one or two words) further into
smaller ones. Corresponding growing n-gram lan-
guage models as in Finnish were trained from the
Estonian corpora resulting the n-grams in Table 4.
The speech recognition task in Estonian consisted
of long sentences read by 50 randomly picked held-
out test speakers, 7 sentences each (a part of (Meister
490
et al, 2002)). Unlike the Finnish and Turkish micro-
phone data, this data was recorded from telephone,
i.e. 8 kHz sampling rate and narrow band data in-
stead of 16 kHz and normal (full) bandwidth. The
phoneme models were trained for speaker indepen-
dent recognition using windowed cepstral mean sub-
traction and significantly more data (over 200 hours
and 1300 speakers) than for the Finnish task. The
speaker independence, together with the telephone
quality and occasional background noises, made this
task still a considerably more difficult one. Other-
wise the acoustic models were similar cross-word
triphone GMM-HMMs with MFCC features, MLLT
transformation and the explicit phone duration mod-
eling, except larger: 5100 different states and 16
GMMs per state. Thus, the recognition speed is
also slower than in Finnish, about 20 xRT (2.2GHz
CPU).
4.4 Turkish
Turkish is another a highly-inflected and agglutina-
tive language with relatively free word order. The
same Morfessor tool (Creutz and Lagus, 2005) as in
Finnish and Estonian was applied to Turkish texts
as well. Using the 360k most common words from
the training corpus, 34k morph units were obtained.
The training corpus consists of approximately 27M
words taken from literature, law, politics, social
sciences, popular science, information technology,
medicine, newspapers, magazines and sports news.
N-gram language models for different orders with
interpolated Kneser-Ney smoothing as well as en-
tropy based pruning were built for this morph lexi-
con using the SRILM toolkit (Stolcke, 2002). The
number of n-grams for the highest order we tried (6-
grams without entropy-based pruning) are reported
in Table 4. In average, there are 2.37 morphs per
word including the word break symbol.
The recognition task in Turkish consisted of ap-
proximately one hour of newspaper sentences read
by one female speaker. We used decision-tree state
clustered cross-word triphone models with approx-
imately 5000 HMM states. Instead of using letter
to phoneme rules, the acoustic models were based
directly on letters. Each state of the speaker inde-
pendent HMMs had a GMM with 6 mixture compo-
nents. The HTK frontend (Young et al, 2002) was
used to get the MFCC based acoustic features. The
explicit phone duration models were not applied.
The training data contained 17 hours of speech from
over 250 speakers. Instead of the LVCSR decoder
used in Finnish and Estonian (Pylkk?nen, 2005), the
Turkish evaluation was performed using another de-
coder (AT&T, 2003), Using a 3.6GHz CPU, the real-
time factor was around one.
5 Results
The recognition results for the three different tasks:
Finnish, Estonian and Turkish, are provided in Ta-
bles 1 ? 3. In each task the word error rate (WER)
and letter error rate (LER) statistics for the morph-
based system is compared to a corresponding word-
based system. The resulting morpheme strings are
glued to words according to the word break symbols
included in the language model (see Section 2.2) and
the WER is computed as the sum of substituted, in-
serted and deleted words divided by the correct num-
ber of words. LER is included here as well, because
although WER is a more common measure, it is not
comparable between languages. For example, in ag-
glutinative languages the words are long and contain
a variable amount of morphemes. Thus, any incor-
rect prefix or suffix would make the whole word in-
correct. The n-gram language model statistics are
given in Table 4.
Finnish lexicon WER LER
Words 400k 8.5 1.20
Morphs 25k 7.0 0.95
Table 1: The LVCSR performance for the speaker-
dependent Finnish task consisting of book-reading
(see Section 4.2). For a reference (word-based) lan-
guage model a 400k lexicon was chosen.
Estonian lexicon WER LER
Words 60k 56.3 22.4
Morphs 37k 47.6 18.9
Table 2: The LVCSR performance for the speaker-
independent Estonian task consisting of read sen-
tences recorded via telephone (see Section 4.3). For
a reference (word-based) language model a 60k lex-
icon was used here.
491
Turkish lexicon WER LER
Words
3-gram 50k 38.8 15.2
Morphs
3-gram 34k 39.2 14.8
4-gram 34k 35.0 13.1
5-gram 34k 33.9 12.4
Morphs, rescored by morph 6-gram
3-gram 34k 33.8 12.4
4-gram 34k 33.2 12.3
5-gram 34k 33.3 12.2
Table 3: The LVCSR performance for the speaker-
independent Turkish task consisting of read news-
paper sentences (see Section 4.4). For the refer-
ence 50k (word-based) language model the accuracy
given by 4 and 5-grams did not improve from that of
3-grams.
In the Turkish recognizer the memory constraints
during network optimization (Allauzen et al, 2004)
allowed the use of language models only up to 5-
grams. The language model pruning thresholds were
optimized over a range of values and the best re-
sults are shown in Table 3. We also tried the same
experiments with two-pass recognition. In the first
pass, instead of the best path, lattice output was gen-
erated with the same language models with prun-
ing. Then these lattices were rescored using the non-
pruned 6-gram language models (see Table 4) and
the best path was taken as the recognition output.
For the word-based reference model, the two-pass
recognition gave no improvements. It is likely that
the language model training corpus was too small to
train proper 6-gram word models. However, for the
morph-based model, we obtained a slight improve-
ment (0.7 % absolute) by two-pass recognition.
6 Discussion
The key result of this paper is that we can success-
fully apply the unsupervised statistical morphs in
large vocabulary language models in all the three ex-
perimented agglutinative languages. Furthermore,
the results show that in all the different LVCSR
tasks, the morph-based language models perform
very well and constantly dominate the reference lan-
guage model based on words. The way that the lexi-
# morph-based models
ngrams Finnish Estonian Turkish
1grams 24,833 37,061 34,332
2grams 2,188,476 1,050,127 655,621
3grams 17,064,072 7,133,902 1,936,263
4grams 25,200,308 8,201,543 3,824,362
5grams 7,167,021 3,298,429 4,857,125
6grams 624,832 691,899 5,523,922
7grams 23,851 55,363 -
8grams 0 1045 -
Sum 52,293,393 20,469,369 16,831,625
Table 4: The amount of different n-grams in each
language model based on statistical morphs. Note
that the Turkish language model was not prepared
by the growing n-gram algorithm as the others and
the model was limited to 6-grams.
con is built from the word fragments allows the con-
struction of statistical language models, in practice,
for almost an unlimited vocabulary by a lexicon that
still has a convenient size.
The recognition was here restricted to agglutina-
tive languages and tasks in which the language used
is both rather general and matches fairly well with
the available training texts. Significant performance
variation in different languages can be observed
here, because of the different tasks and the fact that
comparable recognition conditions and training re-
sources have not been possible to arrange. However,
we believe that the tasks are still both difficult and
realistic enough to illustrate the difference of per-
formance when using language models based on a
lexicon of morphs vs. words in each task. There are
no directly comparable previous LVCSR results on
the same tasks and data, but the closest ones which
can be found are slightly over 20% WER for the
Finnish task (Hirsim?ki et al, 2005), slightly over
40 % WER for the Estonian task (Alum?e, 2005)
and slightly over 30 % WER for the Turkish task
(Erdogan et al, 2005).
Naturally, it is also possible to prepare a huge lex-
icon and still succeed in recognition fairly well (Sar-
aclar et al, 2002; McTait and Adda-Decker, 2003;
Hirsim?ki et al, 2005), but this is not a very con-
venient approach because of the resulting huge lan-
guage models or the heavy pruning required to keep
492
them still tractable. The word-based language mod-
els that were constructed in this paper as reference
models were trained as much as possible in the same
way as the corresponding morph language models.
For Finnish and Estonian the growing n-grams (Sii-
vola and Pellom, 2005) were used including the op-
tion of constructing the OOV words from phonemes
as in (Hirsim?ki et al, 2005). For Turkish a con-
ventional n-gram was built by SRILM similarly as
for the morphs. The recognition approach taken for
Turkish involves a static decoding network construc-
tion and optimization resulting in near real time de-
coding. However, the memory requirements of net-
work optimization becomes prohibitive for large lex-
icon and language models as presented in this paper.
In this paper the recognition speed was not a ma-
jor concern, but from the application point of view
that is a very important factor to be taken into a ac-
count in the comparison. It seems that the major fac-
tors that make the recognition slower are short lexi-
cal units, large lexicon and language models and the
amount of Gaussian mixtures in the acoustic model.
7 Conclusions
This work presents statistical language models
trained on different agglutinative languages utilizing
a lexicon based on the recently proposed unsuper-
vised statistical morphs. To our knowledge this is
the first work in which similarly developed subword
unit lexica are developed and successfully evaluated
in three different LVCSR systems in different lan-
guages. In each case the morph-based approach con-
stantly shows a significant improvement over a con-
ventional word-based LVCSR language models. Fu-
ture work will be the further development of also
the grammatical morph-based language models and
comparison of that to the current approach, as well
as extending this evaluation work to new languages.
8 Acknowledgments
We thank the Finnish Federation of the Visually Im-
paired for providing the Finnish speech data and the
Finnish news agency (STT) and the Finnish IT cen-
ter for science (CSC) for the text data. Our work was
supported by the Academy of Finland in the projects
New information processing principles, Adaptive In-
formatics and New adaptive and learning methods in
speech recognition. This work was supported in part
by the IST Programme of the European Community,
under the PASCAL Network of Excellence, IST-
2002-506778. The authors would like to thank Sa-
banci and ODTU universities for the Turkish acous-
tic and text data and AT&T Labs ? Research for
the software. This research is partially supported
by SIMILAR Network of Excellence and TUBITAK
BDP (Unified Doctorate Program of the Scientific
and Technological Research Council of Turkey).
References
Cyril Allauzen, Mehryar Mohri, Michael Riley, and Brian
Roark. 2004. A generalized construction of integrated
speech recognition transducers. In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), Montreal, Canada.
Tanel Alum?e. 2005. Phonological and morphologi-
cal modeling in large vocabulary continuous Estonian
speech recognition system. In Proceedings of Second
Baltic Conference on Human Language Technologies,
pages 89?94.
Mehryar Mohri and Michael D. Riley. DCD Library ?
Speech Recognition Decoder Library. AT&T Labs ?
Research. http://www.research.att.com/
sw/tools/dcd/.
Ebru Arisoy and Levent Arslan. 2005. Turkish dictation
system for broadcast news applications. In 13th Euro-
pean Signal Processing Conference - EUSIPCO 2005,
Antalya, Turkey, September.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of the Work-
shop on Morphological and Phonological Learning of
ACL-02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Unsuper-
vised morpheme segmentation and morphology in-
duction from text corpora using Morfessor. Techni-
cal Report A81, Publications in Computer and Infor-
mation Science, Helsinki University of Technology.
URL: http://www.cis.hut.fi/projects/
morpho/.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of Content Based Multimedia
Information Access Conference, April 12-14.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adap-
tive vocabularies for transcribing multilingual broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), Seattle, WA, USA, May.
493
H. Erdogan, O. Buyuk, K. Oflazer. 2005. Incorporating
language constraints in sub-word based speech recog-
nition. IEEE Automatic Speech Recognition and Un-
derstanding Workshop, Cancun, Mexico.
Kadri Hacioglu, Brian Pellom, Tolga Ciloglu, Ozlem Oz-
turk, Mikko Kurimo, and Mathias Creutz. 2003. On
lexicon creation for Turkish LVCSR. In Proceedings
of 8th European Conference on Speech Communica-
tion and Technology, pages 1165?1168.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen. 2005.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language. (accepted for publication).
Jan Kneissler and Dietrich Klakow. 2001. Speech recog-
nition for huge vocabularies by using optimized sub-
word units. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology
(Eurospeech), pages 69?72, Aalborg, Denmark.
CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-
guage Text Bank: Corpora Books, Newspapers,
Magazines and Other. http://www.csc.fi/
kielipankki/.
Kevin McTait and Martine Adda-Decker. 2003. The
300k LIMSI German Broadcast News Transcription
System. In Proceedings of 8th European Conference
on Speech Communication and Technology.
Einar Meister, J?rgen Lasn, and Lya Meister. 2002. Esto-
nian SpeechDat: a project in progress. In Proceedings
of the Fonetiikan P?iv?t ? Phonetics Symposium 2002
in Finland, pages 21?26.
NIST. 2000. Proceedings of DARPA workshop on Auto-
matic Transcription of Broadcast News. NIST, Wash-
ington DC, May.
Janne Pylkk?nen. 2005. New pruning criteria for effi-
cient decoding. In Proceedings of 9th European Con-
ference on Speech Communication and Technology.
Janne Pylkk?nen and Mikko Kurimo. 2004. Duration
modeling techniques for continuous speech recogni-
tion. In Proceedings of the International Conference
on Spoken Language Processing.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, CO, USA.
Segakorpus ? Mixed Corpus of Estonian. Tartu Uni-
versity. http://test.cl.ut.ee/korpused/
segakorpus/.
Vesa Siivola and Bryan Pellom. 2005. Growing an n-
gram language model. In Proceedings of 9th European
Conference on Speech Communication and Technol-
ogy.
Vesa Siivola, Mikko Kurimo, and Krista Lagus. 2001.
Large vocabulary statistical language modeling for
continuous speech recognition. In Proceedings of 7th
European Conference on Speech Communication and
Technology, pages 737?747, Aalborg, Copenhagen.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Mate Szarvas and Sadaoki Furui. 2003. Evaluation of the
stochastic morphosyntactic language model on a one
million word Hungarian task. In Proceedings of the
8th European Conference on Speech Communication
and Technology (Eurospeech), pages 2297?2300.
S. Young, D. Ollason, V. Valtchev, and P. Woodland.
2002. The HTK book (for HTK version 3.2.), March.
494
Proceedings of NAACL HLT 2007, pages 380?387,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis of Morph-Based Speech Recognition and the Modeling of
Out-of-Vocabulary Words Across Languages
Mathias Creutz?, Teemu Hirsima?ki?, Mikko Kurimo?, Antti Puurula?, Janne Pylkko?nen?,
Vesa Siivola?, Matti Varjokallio?, Ebru Ar?soy?, Murat Sarac?lar?, and Andreas Stolcke?
? Helsinki University of Technology, <firstname>.<lastname>@tkk.fi,
? Bog?azic?i University, arisoyeb@boun.edu.tr, murat.saraclar@boun.edu.tr,
? SRI International / International Computer Science Institute, stolcke@speech.sri.com
Abstract
We analyze subword-based language
models (LMs) in large-vocabulary
continuous speech recognition across
four ?morphologically rich? languages:
Finnish, Estonian, Turkish, and Egyptian
Colloquial Arabic. By estimating n-gram
LMs over sequences of morphs instead
of words, better vocabulary coverage
and reduced data sparsity is obtained.
Standard word LMs suffer from high
out-of-vocabulary (OOV) rates, whereas
the morph LMs can recognize previously
unseen word forms by concatenating
morphs. We show that the morph LMs
generally outperform the word LMs and
that they perform fairly well on OOVs
without compromising the accuracy
obtained for in-vocabulary words.
1 Introduction
As automatic speech recognition systems are being
developed for an increasing number of languages,
there is growing interest in language modeling ap-
proaches that are suitable for so-called ?morpholog-
ically rich? languages. In these languages, the num-
ber of possible word forms is very large because
of many productive morphological processes; words
are formed through extensive use of, e.g., inflection,
derivation and compounding (such as the English
words ?rooms?, ?roomy?, ?bedroom?, which all stem
from the noun ?room?).
For some languages, language modeling based on
surface forms of words has proven successful, or at
least satisfactory. The most studied language, En-
glish, is not characterized by a multitude of word
forms. Thus, the recognition vocabulary can sim-
ply consist of a list of words observed in the training
text, and n-gram language models (LMs) are esti-
mated over word sequences. The applicability of the
word-based approach to morphologically richer lan-
guages has been questioned. In highly compounding
languages, such as the Germanic languages German,
Dutch and Swedish, decomposition of compound
words can be carried out to reduce the vocabulary
size. Highly inflecting languages are found, e.g.,
among the Slavic, Romance, Turkic, and Semitic
language families. LMs incorporating morphologi-
cal knowledge about these languages can be applied.
A further challenging category comprises languages
that are both highly inflecting and compounding,
such as the Finno-Ugric languages Finnish and Es-
tonian.
Morphology modeling aims to reduce the out-
of-vocabulary (OOV) rate as well as data sparsity,
thereby producing more effective language mod-
els. However, obtaining considerable improvements
in speech recognition accuracy seems hard, as is
demonstrated by the fairly meager improvements
(1?4 % relative) over standard word-based models
accomplished by, e.g., Berton et al (1996), Ordel-
man et al (2003), Kirchhoff et al (2006), Whit-
taker and Woodland (2000), Kwon and Park (2003),
and Shafran and Hall (2006) for Dutch, Arabic, En-
glish, Korean, and Czech, or even the worse perfor-
mance reported by Larson et al (2000) for German
and Byrne et al (2001) for Czech. Nevertheless,
clear improvements over a word baseline have been
achieved for Serbo-Croatian (Geutner et al, 1998),
Finnish, Estonian (Kurimo et al, 2006b) and Turk-
ish (Kurimo et al, 2006a).
In this paper, subword language models in the
recognition of speech of four languages are ana-
380
lyzed: Finnish, Estonian, Turkish, and the dialect
of Arabic spoken in Egypt, Egyptian Colloquial
Arabic (ECA). All these languages are considered
?morphologically rich?, but the benefits of using
subword-based LMs differ across languages. We at-
tempt to discover explanations for these differences.
In particular, the focus is on the analysis of OOVs:
A perceived strength of subword models, when con-
trasted with word models, is that subword models
can generalize to previously unseen word forms by
recognizing them as sequences of shorter familiar
word fragments.
2 Morfessor
Morfessor is an unsupervised, data-driven, method
for the segmentation of words into morpheme-like
units. The general idea is to discover as com-
pact a description of the input text corpus as possi-
ble. Substrings occurring frequently enough in sev-
eral different word forms are proposed as morphs,
and the words in the corpus are then represented
as a concatenation of morphs, e.g., ?hand, hand+s,
left+hand+ed, hand+ful?. Through maximum a pos-
teriori optimization (MAP), an optimal balance is
sought between the compactness of the inventory of
morphs, i.e., the morph lexicon, versus the compact-
ness of the representation of the corpus.
Among others, de Marcken (1996), Brent (1999),
Goldsmith (2001), Creutz and Lagus (2002), and
Creutz (2006) have shown that models based on
the above approach produce segmentations that re-
semble linguistic morpheme segmentations, when
formulated mathematically in a probabilistic frame-
work or equivalently using the Minimum Descrip-
tion Length (MDL) principle (Rissanen, 1989).
Similarly, Goldwater et al (2006) use a hierarchical
Dirichlet model in combination with morph bigram
probabilities.
The Morfessor model has been developed over
the years, and different model versions exist. The
model used in the speech recognition experiments of
the current paper is the original, so-called Morfes-
sor Baseline algorithm, which is publicly available
for download.1. The mathematics of the Morfessor
Baseline model is briefly outlined in the following;
consult Creutz (2006) for details.
1http://www.cis.hut.fi/projects/morpho/
2.1 MAP Optimization Criterion
In slightly simplified form, the optimization crite-
rion utilized in the model corresponds to the maxi-
mization of the following posterior probability:
P (lexicon | corpus) ?
P (lexicon) ? P (corpus | lexicon) =
?
letters ?
P (?) ?
?
morphs ?
P (?). (1)
The lexicon consists of all distinct morphs spelled
out; this forms a long string of letters ?, in which
each morph is separated from the next morph using
a morph boundary character. The probability of the
lexicon is the product of the probability of each let-
ter in this string. Analogously, the corpus is repre-
sented as a sequence of morphs, which corresponds
to a particular segmentation of the words in the cor-
pus. The probability of this segmentation equals the
product of the probability of each morph token ?.
Letter and morph probabilities are maximum likeli-
hood estimates (empirical Bayes).
2.2 From Morphs to n-Grams
As a result of the probabilistic (or MDL) approach,
the morph inventory discovered by the Morfessor
Baseline algorithm is larger the more training data
there is. In some speech recognition experiments,
however, it has been desirable to restrict the size of
the morph inventory. This has been achieved by set-
ting a frequency threshold on the words on which
Morfessor is trained, such that the rarest words will
not affect the learning process. Nonetheless, the
rarest words can be split into morphs in accordance
with the model learned, by using the Viterbi algo-
rithm to select the most likely segmentation. The
process is depicted in Figure 1.
2.3 Grapheme-to-Phoneme Mapping
The mapping between graphemes (letters) and
phonemes is straightforward in the languages stud-
ied in the current paper. More or less, there is
a one-to-one correspondence between letters and
phonemes. That is, the spelling of a word indicates
the pronunciation of the word, and when splitting the
word into parts, the pronunciation of the parts in iso-
lation does not differ much from the pronunciation
of the parts in context. However, a few exceptions
381
Morph
inventory
+ probs
n?grams
Train
cut?off
Frequency
Viterbi
segm.
Text with words
segmented into
LM
morphs
MorfessorExtractwords
Text corpus
Figure 1: How to train a segmentation model using
the Morfessor Baseline algorithm, and how to fur-
ther train an n-gram model based on morphs.
have been treated more rigorously in the Arabic ex-
periments: e.g., in some contexts the same (spelled)
morph can have multiple possible pronunciations.
3 Experiments and Analysis
The goal of the conducted experiments is to com-
pare n-gram language models based on morphs to
standard word n-gram models in automatic speech
recognition across languages.
3.1 Data Sets and Recognition Systems
The results from eight different tests have been an-
alyzed. Some central properties of the test config-
urations are shown in Table 1. The Finnish, Esto-
nian, and Turkish test configurations are slight vari-
ations of experiments reported earlier in Hirsima?ki
et al (2006) (Fin1: ?News task?, Fin2: ?Book task?),
Kurimo et al (2006a) (Fin3, Tur1), and Kurimo et
al. (2006b) (Fin4, Est, Tur2).
Three different recognition platforms have been
used, all of which are state-of-the-art large vocab-
ulary continuous speech recognition (LVCSR) sys-
tems. The Finnish and Estonian experiments have
been run on the HUT speech recognition system de-
veloped at Helsinki University of Technology.
The Turkish tests were performed using the
AT&T decoder (Mohri and Riley, 2002); the acous-
tic features were produced using the HTK front end
(Young et al, 2002). The experiments on Egyptian
Colloquial Arabic (ECA) were carried out using the
SRI DecipherTM speech recognition system.
3.1.1 Speech Data and Acoustic Models
The type and amount of speech data vary from
one language to another. The Finnish data con-
sists of news broadcasts read by one single female
speaker (Fin1), as well as an audio book read by an-
other female speaker (Fin2, Fin3, Fin4). The Finnish
acoustic models are speaker dependent (SD). Mono-
phones (mon) were used in the earlier experiments
(Fin1, Fin2), but these were later replaced by cross-
context triphones (tri).
The Estonian speech data has been collected from
a large number of speakers and consists of sen-
tences from newspapers as well as names and dig-
its read aloud. The acoustic models are speaker-
independent triphones (SI tri) adapted online using
Cepstral Mean Subtraction and Constrained Maxi-
mum Likelihood Linear Regression. Also the Turk-
ish acoustic training data contains speech from hun-
dreds of speakers. The test set is composed of news-
paper text read by one female speaker. Speaker-
independent triphones are used as acoustic models.
The Finnish, Estonian, and Turkish data sets con-
tain planned speech, i.e., written text read aloud.
By contrast, the Arabic data consists of transcribed
spontaneous telephone conversations,2 which are
characterized by disfluencies and by the presence
of ?non-speech?, such as laugh and cough sounds.
There are multiple speakers in the Arabic data, and
online speaker adaptation has been performed.
3.1.2 Text Data and Language Models
The n-gram language models are trained using
the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2,
Tur1, Tur2, ECA) or similar software developed
at HUT (Siivola and Pellom, 2005) (Fin3, Fin4,
Est). All models utilize the Modified Interpolated
Kneser-Ney smoothing technique (Chen and Good-
man, 1999). The Arabic LM is trained on the
same corpus that is used for acoustic training. This
data set is regrettably small (160 000 words), but it
matches the test set well in style, as it consists of
transcribed spontaneous speech. The LM training
corpora used for the other languages contain fairly
large amounts of mainly news and book texts and
conceivably match the style of the test data well.
In the morph-based models, words are split into
morphs using Morfessor, and statistics are collected
for morph n-grams. As the desired output of the
2LDC CallHome corpus of Egyptian Colloquial Ara-
bic: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC97S45
382
Table 1: Test configurations
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
Recognizer HUT HUT HUT HUT HUT AT&T AT&T SRI
Speech data
Type of speech read read read read read read read spont.
Training set [kwords] 20 49 49 49 790 230 110 160
Speakers in training set 1 1 1 1 1300 550 250 310
Test set [kwords] 4.3 1.9 1.9 1.9 3.7 7.0 7.0 16
Speakers in test set 1 1 1 1 50 1 1 57
Text data
LM training set [Mwords] 36 36 32 150 53 17 27 0.16
Models
Acoustic models SD mon SD mon SD tri SD tri SI tri SI tri SI tri SI tri
Morph lexicon [kmorphs] 66 66 120 25 37 52 34 6.1
Word lexicon [kwords] 410 410 410 ? 60 120 50 18
Out-of-vocabulary words
OOV LM training set [%] 5.0 5.0 5.9 ? 14 5.3 9.6 0.61
OOV test set [%] 5.0 7.2 7.3 ? 19 5.5 12 9.9
New words in test set [%] 2.7 3.0 3.1 1.5 3.4 1.6 1.5 9.8
speech recognizer is a sequence of words rather than
morphs, the LM explicitly models word breaks as
special symbols occurring in the morph sequence.
For comparison, word n-gram models have been
tested. The vocabulary cannot typically include ev-
ery word form occurring in the training set (because
of the large number of different words), so the most
frequent words are given priority; the actual lexicon
sizes used in each experiment are shown in Table 1.
Any word not contained in the lexicon is replaced by
a special out-of-vocabulary symbol.
As words and morphs are units of different length,
their optimal performance may occur at different or-
ders of the n-gram. The best order of the n-gram
has been optimized on development test sets in the
following cases: Fin1, Fin2, Tur1, ECA (4-grams
for both morphs and words) and Tur2 (5-grams for
morphs, 3-grams for words). The models have ad-
ditionally been pruned using entropy-based pruning
(Tur1, Tur2, ECA) (Stolcke, 1998). In the other
experiments (Fin3, Fin4, Est), no fixed maximum
value of n was selected. n-Gram growing was per-
formed (Siivola and Pellom, 2005), such that those
n-grams that maximize the training set likelihood
are gradually added to the model. The unrestricted
growth of the model is counterbalanced by an MDL-
type complexity term. The highest order of n-grams
accepted was 7 for Finnish and 8 for Estonian.
Note that the optimization procedure is neutral
with respect to morphs vs. words. Roughly the
same number of parameters are allowed in the result-
ing LMs, but typically the morph n-gram LMs are
smaller than the corresponding word n-gram LMs.
3.1.3 Out-of-Vocabulary Words
Table 1 further shows statistics on out-of-
vocabulary rates in the data sets. This is relevant
for the assessment of the word models, as the OOV
rates define the limits of these models.
The OOV rate for the LM training set corresponds
to the proportion of words replaced by the OOV
symbol in the LM training data, i.e., words that were
not included in the recognition vocabulary. The high
OOV rates for Estonian (14 %) and Tur2 (9.6 %) in-
dicate that the word lexicons have poor coverage of
these sets. By contrast, the ECA word lexicon cov-
ers virtually the entire training set vocabulary.
Correspondingly, the test set OOV rate is the pro-
portion of words that occur in the data sets used
for running the speech recognition tests, but that are
missing from the recognition lexicons. This value
is thus the minimum error that can be obtained by
the word models, or put differently, the recognizer
is guaranteed to get at least this proportion of words
wrong. Again, the values are very high for Estonian
(19 %) and Tur2 (12 %), but also for Arabic (9.9 %)
because of the insufficient amount of training data.
Finally, the figures labeled ?new words in test set?
denote the proportion of words in the test set that do
not occur in the LM training set. Thus, these values
indicate the minimum error achievable by any word
model trained on the training sets available.
383
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
76.6
71.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
W
or
d 
ac
cu
ra
cy
 [%
]
 
 
Morphs
Words
Figure 2: Word accuracies for the different speech
recognition test configurations.
3.2 Results and Analysis
The morph-based and word-based results of the con-
ducted speech recognition experiments are shown in
Figure 2 (for Fin4, no comparable word experiment
has been carried out). The evaluation measure used
is word accuracy (WAC): the number of correctly
recognized words minus the number of incorrectly
inserted words divided by the number of words in
the reference transcript. (Another frequently used
measure is the word error rate, WER, which relates
to word accuracy as WER = 100 % ? WAC.)
Figure 2 shows that the morph models perform
better than the word models, with the exception
of the Arabic experiment (ECA), where the word
model outperforms the morph model. The statisti-
cal significance of these differences is confirmed by
one-tailed paired Wilcoxon signed-rank tests at the
significance level of 0.05.
Overall, the best performance is observed for the
Finnish data sets, which is explained by the speaker-
dependent acoustic models and clean noise condi-
tions. The Arabic setup suffers from the insufficient
amount of LM training data.
3.2.1 In-Vocabulary Words
For a further investigation of the outcome of the
experiments, the test sets have been partitioned into
regions based on the types of words they contain.
The recognition output is aligned with the refer-
ence transcript, and the regions aligned with in-
vocabulary (IV) reference words (words contained
in the vocabulary of the word model) are put in
one partition and the remaining words (OOVs) are
put in another partition. Word accuracies are then
computed separately for the two partitions. Inserted
words, i.e., words that are not aligned with any word
in the reference, are put in the IV partition, unless
they are adjacent to an OOV region, in which case
they are put in the OOV partition.
Figure 3a shows word accuracies for the in-
vocabulary words. Without exception, the accuracy
for the IVs is higher than that of the entire test set vo-
cabulary. One could imagine that the word models
would do better than the morph models on the IVs,
since the word models are totally focused on these
words, whereas the morph models reserve modeling
capacity for a much larger set of words. The word
accuracies in Fig. 3a also partly seem to support this
view. However, Wilcoxon signed-rank tests (level
0.05) show that the superiority of the word model is
statistically significant only for Arabic and for Fin3.
With few exceptions, it is thus possible to draw
the conclusion that morph models are capable of
modeling a much larger set of words than word
models without, however, compromising the perfor-
mance on the limited vocabulary covered by the
word models in a statistically significant way.
3.2.2 Out-of-Vocabulary Words
Since the word model and morph model perform
equally well on the subset of words that are included
in the lexicon of the word model, the overall supe-
riority of the morph model needs to come from its
successful coping with out-of-vocabulary words.
In Figure 3b, word accuracies have been plot-
ted for the out-of-vocabulary words contained in the
test set. It is clear that the recognition accuracy for
the OOVs is much lower than the overall accuracy.
Also, negative accuracy values are observed. This
happens when the number of insertions exceeds the
number of correctly recognized units.
In Figure 3b, if speaker-dependent and speaker-
independent setups are considered separately (and
Arabic is left out), there is a tendency for the morph
models to recognize the OOVs more accurately, the
higher the OOV rate is. One could say that a morph
model has a double advantage over a correspond-
ing word model: the larger the proportion of OOVs
384
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
79.979.781.977.9
92.594.6
73.374.771.872.671.771.9
45.6
48.1
W
or
d 
ac
cu
ra
cy
 fo
r i
n?
vo
ca
bu
la
ry
 w
or
ds
 [%
]
 
 
Morphs
Words
(a)
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
15.1
?74.8
43.2
?62.6
55.1
?76.1
38.0
?47.7
13.2
?21.8
29.2
?19.4
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r O
O
Vs
 [%
]
 
 
Morphs
Words
(b)
Figure 3: Word accuracies computed separately for those words in the test sets that are (a) included in and
(b) excluded from the vocabularies of the word vocabulary; cf. figures listed on the row ?OOV test set? in
Table 1. Together these two partitions make up the entire test set vocabulary. For comparison, the results for
the entire sets are shown using gray-shaded bars (also displayed in Figure 2).
in the word model is, the larger the proportion of
words that the morph model can recognize but the
word model cannot, a priori. In addition, the larger
the proportion of OOVs, the more frequent and more
?easily modelable? words are left out of the word
model, and the more successfully these words are
indeed learned by the morph model.
3.2.3 New Words in the Test Set
All words present in the training data (some of
which are OOVs in the word models) ?leave some
trace? in the morph models, in the n-gram statistics
that are collected for morph sequences. How, then,
about new words that occur only in the test set, but
not in the training set? In order to recognize such
words correctly, the model must combine morphs in
ways it has not observed before.
Figure 4 demonstrates that the new unseen words
are very challenging. Now, also the morph mod-
els mostly obtain negative word accuracies, which
means that the number of insertions adjacent to new
words exceeds the number of correctly recognized
new words. The best results are obtained in clean
acoustic conditions (Fin2, Fin3, Fin4) with only few
foreign names, which are difficult to get right using
typical Finnish phoneme-to-grapheme mappings (as
the negative accuracy of Fin1 suggests).
3.3 Vocabulary Growth and Arabic
Figure 5 shows the development of the size of
the vocabulary (unique word forms) for growing
amounts of text in different corpora. The corpora
used for Finnish, Estonian, and Turkish (planned
speech/text), as well as Arabic (spontaneous speech)
are the LM training sets used in the experiments.
Additional sources have been provided for Arabic
and English: Arabic text (planned) from the FBIS
corpus of Modern Standard Arabic (a collection
of transcribed radio newscasts from various radio
stations in the Arabic speaking world), as well as
text from the New York Times magazine (English
planned) and spontaneous transcribed English tele-
phone conversations from the Fisher corpus.
The figure illustrates two points: (1) The faster
the vocabulary growth is, the larger the potential ad-
vantage of morph models is in comparison to stan-
dard word models, because of OOV and data spar-
sity problems. The obtained speech recognition re-
sults seem to support this hypothesis; the applied
morph LMs are clearly beneficial for Finnish and
Estonian, mostly beneficial for Turkish, and slightly
detrimental for ECA. (2) A more slowly growing
vocabulary is used in spontaneous speech than in
planned speech (or written text). Moreover, the
Arabic ?spontaneous? curve is located fairly close
385
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
?8.7
?93.0
20.7
?75.9
31.0
?81.0
17.9
?32.3
?64.6
?6.1
?24.6
?5.9
?24.5
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r u
ns
ee
n 
wo
rd
s 
[%
]
 
 
Morphs
Words
Figure 4: Word accuracies computed for the words
in the test sets that do not occur at all in the train-
ing sets; cf. figures listed on the row ?new words
in test set? in Table 1. For comparison, the gray-
shaded bars show the corresponding results for the
entire test sets (also displayed in Figure 2).
to the English ?planned? curve and much below
the Finnish, Estonian, and Turkish curves. Thus,
even though Arabic is considered a ?morphologi-
cally rich? language, this is not manifested through
a considerable vocabulary growth (and high OOV
rate) in the Egyptian Colloquial Arabic data used in
the current speech recognition experiments. Conse-
quently, it may not be that surprising that the morph
model did not work particularly well for Arabic.
Arabic words consist of a stem surrounded by pre-
fixes and suffixes, which are fairly successfully seg-
mented out by Morfessor. However, Arabic also
has templatic morphology, i.e., the stem is formed
through the insertion of a vowel pattern into a ?con-
sonantal skeleton?.
Additional experiments have been performed us-
ing the ECA data and Factored Language Models
(FLMs) (Kirchhoff et al, 2006). The FLM is a
powerful model that makes use of several sources
of information, in particular a morphological lexi-
con of ECA. The FLM incorporates mechanisms for
handling templatic morphology, but despite its so-
phistication, it barely outperforms the standard word
model: The word accuracy of the FLM is 42.3 % and
that of the word model is 41.8 %. The speech recog-
nition implementation of both the FLM and the word
0 20 40 60 80 100 120 140 160 1800
5
10
15
20
25
30
35
40
45
50
Corpus size [1000 words]
Un
iq
ue
 w
or
ds
 [1
00
0 w
ord
s]
Fin
nish
 (pla
nned
)
Est
onia
n (pl
anne
d)
Turk
ish (pl
anned
)
Arabic 
(spontan
eous)Ara
bic (plann
ed)
English (plann
ed)
English (spontaneous)
Figure 5: Vocabulary growth curves for the differ-
ent corpora of spontaneous and planned speech (or
written text). For growing amounts of text (word
tokens) the number of unique different word forms
(word types) occurring in the corpus are plotted.
model is based on whole words (although subword
units are used for assigning probabilities to word
forms in the FLM). This contrasts these models with
the morph model, which splits words into subword
units also in the speech recognition implementation.
It seems that the splitting is a source of errors in this
experimental setup with very little data available.
4 Discussion
Alternative morph-based and word-based ap-
proaches exist. We have tried some, but none of
them has outperformed the described morph models
for Finnish, Estonian, and Turkish, or the word and
FLM models for Egyptian Arabic (in a statistically
significant way). The tested models comprise
more linguistically accurate morph segmentations
obtained using later Morfessor versions (Categories-
ML and Categories-MAP) (Creutz, 2006), as well
as analyses obtained from morphological parsers.
Hybrids, i.e., word models augmented with
phonemes or other subword units have been pro-
posed (Bazzi and Glass, 2000; Galescu, 2003;
Bisani and Ney, 2005). In our experiments, such
models have outperformed the standard word mod-
els, but not the morph models.
Simply growing the word vocabulary to cover the
386
entire vocabulary of large training corpora could be
one (fairly ?brute-force?) approach, but this is hardly
feasible for languages such as Finnish. The en-
tire Finnish LM training data of 150 million words
(used in Fin4) contains more than 4 million unique
word forms, a value ten times the size of the rather
large word lexicon currently used. And even if a 4-
million-word lexicon were to be used, the OOV rate
of the test set would still be relatively high: 1.5 %.
Judging by the Arabic experiments, there seems
to be some potential in Factored Language Models.
The FLMs might work well also for the other lan-
guages, and in fact, to do justice to the more ad-
vanced morph models from later versions of Mor-
fessor, FLMs or some other refined techniques may
be necessary as a complement to the currently used
standard n-grams.
Acknowledgments
We are most grateful to Katrin Kirchhoff and Dimitra Vergyri
for their valuable help on issues related to Arabic, and to the EU
AMI training program for funding part of this work. The work
was also partly funded by DARPA under contract No. HR0011-
06-C-0023 (approved for public release, distribution is unlim-
ited). The views herein are those of the authors and do not nec-
essarily reflect the views of the funding agencies.
References
I. Bazzi and J. R. Glass. 2000. Modeling out-of-vocabulary
words for robust speech recognition. In Proc. ICSLP, Bei-
jing, China.
A. Berton, P. Fetter, and P. Regel-Brietzmann. 1996. Com-
pound words in large-vocabulary German speech recognition
systems. In Proc. ICSLP, pp. 1165?1168, Philadelphia, PA,
USA.
M. Bisani and H. Ney. 2005. Open vocabulary speech recog-
nition with flat hybrid models. In Proc. Interspeech, Lisbon,
Portugal.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34:71?105.
W. Byrne, J. Hajic?, P. Ircing, F. Jelinek, S. Khudanpur, P. Kr-
bec, and J. Psutka. 2001. On large vocabulary continuous
speech recognition of highly inflectional language ? Czech.
In Proc. Eurospeech, pp. 487?489, Aalborg, Denmark.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 13:359?394.
M. Creutz and K. Lagus. 2002. Unsupervised discovery of
morphemes. In Proc. ACL SIGPHON, pp. 21?30, Philadel-
phia, PA, USA.
M. Creutz. 2006. Induction of the Morphology of Natural
Language: Unsupervised Morpheme Segmentation with Ap-
plication to Automatic Speech Recognition. Ph.D. thesis,
Helsinki University of Technology. http://lib.tkk.fi/
Diss/2006/isbn9512282119/.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
L. Galescu. 2003. Recognition of out-of-vocabulary words
with sub-lexical language models. In Proc. Eurospeech, pp.
249?252, Geneva, Switzerland.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adaptive vocabu-
laries for transcribing multilingual broadcast news. In Proc.
ICASSP, pp. 925?928, Seattle, WA, USA.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153?198.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. Coling/ACL, pp. 673?680, Sydney, Australia.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and
J. Pylkko?nen. 2006. Unlimited vocabulary speech recogni-
tion with morph language models applied to Finnish. Com-
puter Speech and Language, 20(4):515?541.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for Ara-
bic speech recognition. Computer Speech and Language,
20(4):589?608.
M. Kurimo, M. Creutz, M. Varjokallio, E. Ar?soy, and
M. Sarac?lar. 2006a. Unsupervised segmentation of words
into morphemes ? Morpho Challenge 2005, Application to
automatic speech recognition. In Proc. Interspeech, Pitts-
burgh, PA, USA.
M. Kurimo, A. Puurula, E. Ar?soy, V. Siivola, T. Hirsima?ki,
J. Pylkko?nen, T. Aluma?e, and M. Sarac?lar. 2006b. Un-
limited vocabulary speech recognition for agglutinative lan-
guages. In Proc. NAACL-HLT, New York, USA.
O.-W. Kwon and J. Park. 2003. Korean large vocabulary con-
tinuous speech recognition with morpheme-based recogni-
tion units. Speech Communication, 39(3?4):287?300.
M. Larson, D. Willett, J. Koehler, and G. Rigoll. 2000. Com-
pound splitting and lexical unit recombination for improved
performance of a speech recognition system for German par-
liamentary speeches. In Proc. ICSLP.
M. Mohri and M. D. Riley. 2002. DCD library, Speech
recognition decoder library. AT&T Labs Research. http:
//www.research.att.com/sw/tools/dcd/.
R. Ordelman, A. van Hessen, and F. de Jong. 2003. Compound
decomposition in Dutch large vocabulary speech recogni-
tion. In Proc. Eurospeech, pp. 225?228, Geneva, Switzer-
land.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15:79?93.
I. Shafran and K. Hall. 2006. Corrective models for speech
recognition of inflected languages. In Proc. EMNLP, Syd-
ney, Australia.
V. Siivola and B. Pellom. 2005. Growing an n-gram model. In
Proc. Interspeech, pp. 1309?1312, Lisbon, Portugal.
A. Stolcke. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA BNTU Workshop, pp. 270?274,
Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. ICSLP, pp. 901?904. http://www.speech.
sri.com/projects/srilm/.
E. W. D. Whittaker and P. C. Woodland. 2000. Particle-based
language modelling. In Proc. ICSLP, pp. 170?173, Beijing,
China.
S. Young, D. Ollason, V. Valtchev, and P. Woodland. 2002.
The HTK book (for version 3.2 of HTK). University of Cam-
bridge.
387
Proceedings of NAACL HLT 2009: Short Papers, pages 269?272,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Score Distribution Based Term Specific Thresholding
for
Spoken Term Detection
Dog?an Can and Murat Sarac?lar
Electrical & Electronics Engineering Department
Bog?azic?i University
I?stanbul, Turkey
{dogan.can, murat.saraclar}@boun.edu.tr
Abstract
The spoken term detection (STD) task aims
to return relevant segments from a spoken
archive that contain the query terms. This pa-
per focuses on the decision stage of an STD
system. We propose a term specific threshold-
ing (TST) method that uses per query poste-
rior score distributions. The STD system de-
scribed in this paper indexes word-level lat-
tices produced by an LVCSR system using
Weighted Finite State Transducers (WFSTs).
The target application is a sign dictionary
where precision is more important than recall.
Experiments compare the performance of dif-
ferent thresholding techniques. The proposed
approach increases the maximum precision at-
tainable by the system.
1 Introduction
The availability of vast multimedia archives calls
for solutions to efficiently search this data. Multi-
media content also enables interesting applications
which utilize multiple modalities, such as speech
and video. Spoken term detection (STD) is a sub-
field of speech retrieval, which locates occurrences
of a query in a spoken archive. In this work, STD
is used as a tool to segment and retrieve the signs
in news videos for the hearing impaired based on
speech information. After the location of the query
is extracted with STD, the sign video correspond-
ing to that time interval is displayed to the user.
In addition to being used as a sign language dic-
tionary this approach can also be used to automat-
ically create annotated sign databases that can be
utilized for training sign recognizers (Aran et al,
2008). For these applications the precision of the
system is more important than its recall.
The classical STD approach consists of convert-
ing the speech to word transcripts using large vocab-
ulary continuous speech recognition (LVCSR) tools
and extending classical information retrieval tech-
niques to word transcripts. However, retrieval per-
formance is highly dependent on the recognition er-
rors. In this context, lattice indexing provides a
means of reducing the effect of recognition errors
by incorporating alternative transcriptions in a prob-
abilistic framework. A system using lattices can also
return the posterior probability of a query as a de-
tection score. Various operating points can be ob-
tained by comparing the detection scores to a thresh-
old. In addition to using a global detection thresh-
old, choosing term specific thresholds that optimize
the STD evaluation metric known as Term-Weighted
Value (TWV) was recently proposed (Miller et al,
2007). A similar approach which trains a neural net-
work mapping various features to the target classes
was used in (Vergyri et al, 2007).
The rest of the paper is organized as follows.
In Section 2 we explain the methods used for spo-
ken term detection. These include the indexing and
search framework based on WFSTs and the detec-
tion framework based on posterior score distribu-
tions. In Section 3 we describe our experimental
setup and present the results. Finally, in Section 4
we summarize our contributions and discuss possi-
ble future directions.
269
2 Methods
The STD system used in this study consists of four
stages. In the first stage, an LVCSR system is used
to generate lattices from speech. In the second stage
the lattices are indexed for efficient retrieval. When
a query is presented to the system a set of candidates
ranked by posterior probabilities are obtained from
the index. In the final stage, the posterior probabil-
ities are compared to a threshold to decide which
candidates should be returned.
2.1 Indexing and Retrieval using Finite-State
Automata
General indexation of weighted automata (Allauzen
et al, 2004) provides an efficient means of index-
ing for STD (Parlak and Sarac?lar, 2008; Can et al,
2009), where retrieval is based on the posterior prob-
ability of a term in a given time interval. In this
work, the weighted automata to be indexed are the
preprocessed lattice outputs of the ASR system. The
input labels are phones, the output labels are quan-
tized time-intervals and the weights are normalized
negative log probabilities. The index is represented
as a WFST where each substring (factor) leads to a
successful path over the input labels whenever that
particular substring was observed. Output labels of
these paths carry the time interval information fol-
lowed by the utterance IDs. The path weights give
the probability of each factor occurring in the spe-
cific time interval of that utterance. The index is op-
timized by WFST determinization and minimization
so that the search complexity is linear in the sum of
the query length and the number of times the query
appears in the index.
2.2 Decision Mechanism
Once a list of candidates ranked with respect to their
posterior probabilities are determined using the in-
dex, the candidates exceeding a threshold are re-
turned by the system. The threshold is computed
to minimize the Bayes risk. In this framework, we
need to specify a cost function, prior probabilities
and likelihood functions for each class. We choose
the cost of a miss to be 1 and the cost of a false alarm
to be a free parameter, ?. The prior probabilities and
the likelihood functions are estimated from the pos-
terior scores of the candidate results for each query.
The likelihood functions are found by fitting para-
metric models to the score distributions (Manmatha
et al, 2001). In this study, the score distributions
are modeled by exponential distributions. When the
system returns a score, we do not know whether
it belongs to the correct or incorrect group, so we
use a mixture of two exponential distributions to
model the posterior scores returned by the system.
The exponential mixture model (EMM) parameters
are determined via unsupervised estimation using
the Expectation-Maximization (EM) algorithm. Fig-
ure 1 shows the normalized histogram of posterior
scores and the EM estimate given by our method for
an example query.
0 0.2 0.4 0.6 0.8 10
5
10
15
Posterior Score
n
 
 Incorrect Class DistributionCorrect Class DistributionIncorrect Class EM EstimateCorrect Class EM Estimate
Figure 1: The normalized histogram of posterior scores
and the EM estimates for correct and incorrect detections
given an example query.
If we denote the posterior score of each candidate
by x, incorrect class by c0 and correct class by c1,
we have
p(x) = P (c0)p(x|c0) + P (c1)p(x|c1)
where the incorrect class likelihood
p(x|c0) = ?0e??0x and correct class like-
lihood p(x|c1) = ?1e??1(1?x). The model
parameters ?0, ?1, P (c0), P (c1) are estimated
using the EM algorithm given the scores xi for
i = 1, . . . , N . Each iteration consists of first
computing P (cj |xi) = P (cj)p(xi|cj)/p(xi) for
j = 1, 2 and then updating
P (cj) = 1N
?
i
P (cj |xi),
?0 =
?
i P (c0|xi)?
i P (c0|xi)xi ,
270
?1 =
?
i P (c1|xi)?
i P (c1|xi)(1? xi) .
After the mixture parameters are estimated, we as-
sume that each mixture represents a class and mix-
ture weights correspond to class priors. Then, the
Minimum Bayes Risk (MBR) detection threshold
for x is given as:
?1 + log(?0/?1) + log(P (c0)/P (c1)) + log?
?0 + ?1 .
3 Experiments
3.1 Data and Application
Turkish Radio and Television Channel 2 (TRT2)
broadcasts a news program for the hearing impaired
which contains speech as well as signs. We have
collected 11 hours (total speech time) of test ma-
terial from this broadcast and performed our ex-
periments on this data with a total of 10229 sin-
gle word queries extracted from the reference tran-
scriptions. We used IBM Attila speech recognition
toolkit (Soltau et al, 2007) at the back-end of our
system to produce recognition lattices. The ASR
system is trained on 100 hours of speech and tran-
scription data collected from various TV and radio
broadcasts including TRT2 hearing impaired news,
and a general text corpus of size 100 million words.
Our application uses the speech modality to re-
trieve the signs corresponding to a text query. Re-
trieved results are displayed as video demonstrations
to support the learning of sign language. Since the
application acts like an interactive dictionary of sign
language, primary concern is to return correct results
no matter how few they are. Thus high precision is
appreciated much more than high recall rates.
3.2 Evaluation Measures
In our experiments, we use precision and recall as
the primary evaluation metrics. For a set of queries
qk, k = 1, . . . , Q,
Precision = 1Q
?
k
C(qk)
A(qk) Recall =
1
Q
?
k
C(qk)
R(qk)
where:
R(qk): Number of occurences of query qk,
A(qk): Total no. of retrieved documents for qk,
C(qk): No. of correctly retrieved documents for qk.
We obtain a precision/recall curve by changing
the free parameter associated with each thresholding
method to simulate different decision cost settings.
Right end of these curves fall into the high precision
region which is the main concern in our application.
For the case of global thresholding (GT), the same
threshold ? is used for all queries. TWV based
term specific thresholding (TWV-TST) (Miller et al,
2007) aims to maximize the TWV metric introduced
during NIST 2006 STD Evaluations (NIST, 2006).
TWV = 1? 1Q
Q?
k=1
{Pmiss(qk) + ?.PFA(qk)}
Pmiss(qk) = 1?C(qk)R(qk) ,PFA(qk) =
A(qk)? C(qk)
T ? C(qk)
where T is the total duration of the speech archive
and ? is a weight assigned to false alarms that is
proportional to the prior probability of occurence of
a specific term and its cost-value ratio. This method
sets individual thresholds for each query term con-
sidering per query expected counts and the tuning
parameter ?. In the proposed method ? plays the
same role as ? and allows us to control the decision
threshold for different cost settings.
3.3 Results
0.8 0.85 0.9 0.95 10.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
Rec
all
 
 
Global ThresholdingTerm Specific Thresholding (TWV)EMM + EM + MBR DetectionCheat + EMM + MBR Detection
Figure 2: The precision and recall curves for various
thresholding techniques.
Figure 2 compares GT, TWV-TST, and the pro-
posed method that utilizes score distributions to de-
rive an optimal decision threshold. For GT and
TWT-TST, last precision/recall point in the figure
corresponds to the limit threshold value which is 1.0.
Both the TWV-TST and the proposed method out-
perform GT over the entire region of interest. While
TWV-TST provides better performance around the
271
knees of the curves, proposed method achieves
higher maximum precision values which coincides
with the primary objective of our application.
Figure 2 also provides a curve of what happens
when the correct class labels are used to estimate
the parameters of the exponential mixture model in a
supervised manner instead of using EM. This curve
provides an upper bound on the performance of the
proposed method.
4 Discussion
In this paper, we proposed a TST scheme for STD
which works almost as good as TWV-TST. Extrapo-
lating from the cheating experiment, we believe that
the proposed method has potential for outperform-
ing the TWV-TST over the entire region of interest
given better initial estimates for the correct and in-
correct classes.
A special remark goes to the performance in the
high precision region where our method clearly out-
performs the rest. While GT and TWV-TST meth-
ods are bounded around 96.5% precision value, our
method reaches at higher precision figures. For GT,
this behavior is due to the inability to set differ-
ent thresholds for different queries. For TWT-TST,
in the high precision region where ? is large, the
threshold is very close to 1.0 value no matter what
the expected count of the query term is, thus it es-
sentially acts like a global threshold.
Our current implementation of the proposed
method does not make use of training data to es-
timate the initial parameters for the EM algorithm.
Instead, it relies on some loose assumptions about
the initial parameters of the likelihood functions and
uses uninformative prior distributions. The signifi-
cant difference between the upper bound and the ac-
tual performance of the proposed method indicates
that the current implementation can be improved by
better initial estimates.
Our assumption about the parametric form of the
likelihood function may not be valid at all times.
Maximizing the likelihood with mismatched mod-
els degrades the performance even when initial
parameters are close to the optimal values. In the
future, other parametric forms can be utilized to bet-
ter model the posterior score distributions.
Maximum likelihood estimation with insufficient
data is prone to overtraining. This is a common sit-
uation with the STD task at hand. With the current
data, three or less results are returned for half of the
queries. Bayesian methods can be used to introduce
priors on the model parameters in order to make the
estimation more robust.
Acknowledgments
This study was supported in part by Bog?azic?i Uni-
versity Research Fund (BAP) under the project num-
ber 05HA202, TU?BI?TAK under the project number
105E102 and Turkish State Planning Organization
(DPT) under the project number DPT2007K120610.
References
C. Allauzen, M. Mohri, and M. Sarac?lar. 2004. General-
indexation of weighted automata-application to spo-
ken utterance retrieval. In Proc. Workshop on Inter-
disciplinary Approaches to Speech Indexing and Re-
trieval at HLT-NAACL, pages 33?40, March.
O. Aran, I. Ar?, E. Dikici, S. Parlak, P. Campr, M. Hruz,
L. Akarun, and M. Sarac?lar. 2008. Speech and slid-
ing text aided sign retrieval from hearing impaired sign
news videos. Journal on Multimodal User Interfaces,
2(1):117?131, November.
D. Can, E. Cooper, A. Sethy, C.M. White, B. Ramabhad-
ran, and M. Saraclar. 2009. Effect of pronunciations
on oov queries in spoken term detection. In ICASSP,
April.
R. Manmatha, T. Rath, and F. Feng. 2001. Modeling
score distributions for combining the outputs of search
engines. In SIGIR ?01, pages 267?275, New York, NY,
USA. ACM.
D. R. H. Miller, M. Kleber, C. Kao, O. Kimball,
T. Colthurst, S. A. Lowe, R. M. Schwartz, and H. Gish.
2007. Rapid and accurate spoken term detection. In
Proc. Interspeech, pages 314?317, August.
NIST. 2006. The spoken term detection (STD) 2006
evaluation plan http://www.nist.gov/speech/tests/std/.
S. Parlak and M. Sarac?lar. 2008. Spoken term detection
for Turkish broadcast news. In Proc. ICASSP, pages
5244?5247, April.
H. Soltau, G. Saon, D. Povey, L. Mangu, J. Kuo,
M. Omar, and G. Zweig. 2007. The IBM 2006 GALE
Arabic ASR system. In Proc. ICASSP 2007, Hon-
olulu, HI, USA.
D. Vergyri, I. Shafran, A. Stolcke, R. R. Gadde, M. Ak-
bacak, B. Roark, and W. Wang. 2007. The SRI/OGI
2006 spoken term detection system. In Proc. Inter-
speech, pages 2393?2396, August.
272
Proceedings of the 43rd Annual Meeting of the ACL, pages 507?514,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discriminative Syntactic Language Modeling for Speech Recognition
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Brian Roark
OGI/OHSU
roark@cslu.ogi.edu
Murat Saraclar
Bogazici University
murat.saraclar@boun.edu.tr
Abstract
We describe a method for discriminative
training of a language model that makes
use of syntactic features. We follow
a reranking approach, where a baseline
recogniser is used to produce 1000-best
output for each acoustic input, and a sec-
ond ?reranking? model is then used to
choose an utterance from these 1000-best
lists. The reranking model makes use of
syntactic features together with a parame-
ter estimation method that is based on the
perceptron algorithm. We describe exper-
iments on the Switchboard speech recog-
nition task. The syntactic features provide
an additional 0.3% reduction in test?set
error rate beyond the model of (Roark et
al., 2004a; Roark et al, 2004b) (signifi-
cant at p < 0.001), which makes use of
a discriminatively trained n-gram model,
giving a total reduction of 1.2% over the
baseline Switchboard system.
1 Introduction
The predominant approach within language model-
ing for speech recognition has been to use an n-
gram language model, within the ?source-channel?
or ?noisy-channel? paradigm. The language model
assigns a probability Pl(w) to each string w in the
language; the acoustic model assigns a conditional
probability Pa(a|w) to each pair (a,w) where a is a
sequence of acoustic vectors, and w is a string. For
a given acoustic input a, the highest scoring string
under the model is
w? = arg max
w
(? log Pl(w) + log Pa(a|w)) (1)
where ? > 0 is some value that reflects the rela-
tive importance of the language model; ? is typi-
cally chosen by optimization on held-out data. In
an n-gram language model, a Markov assumption
is made, namely that each word depends only on
the previous (n ? 1) words. The parameters of the
language model are usually estimated from a large
quantity of text data. See (Chen and Goodman,
1998) for an overview of estimation techniques for
n-gram models.
This paper describes a method for incorporating
syntactic features into the language model, using
discriminative parameter estimation techniques. We
build on the work in Roark et al (2004a; 2004b),
which was summarized and extended in Roark et al
(2005). These papers used discriminative methods
for n-gram language models. Our approach reranks
the 1000-best output from the Switchboard recog-
nizer of Ljolje et al (2003).1 Each candidate string
w is parsed using the statistical parser of Collins
(1999) to give a parse tree T (w). Information from
the parse tree is incorporated in the model using
a feature-vector approach: we define ?(a,w) to
be a d-dimensional feature vector which in princi-
ple could track arbitrary features of the string w
together with the acoustic input a. In this paper
we restrict ?(a,w) to only consider the string w
and/or the parse tree T (w) for w. For example,
?(a,w) might track counts of context-free rule pro-
ductions in T (w), or bigram lexical dependencies
within T (w). The optimal string under our new
model is defined as
w? = arg max
w
(? log Pl(w) + ???, ?(a,w)?+
log Pa(a|w)) (2)
where the arg max is taken over all strings in the
1000-best list, and where ?? ? Rd is a parameter
vector specifying the ?weight? for each feature in
? (note that we define ?x, y? to be the inner, or dot
1Note that (Roark et al, 2004a; Roark et al, 2004b) give
results for an n-gram approach on this data which makes use of
both lattices and 1000-best lists. The results on 1000-best lists
were very close to results on lattices for this domain, suggesting
that the 1000-best approximation is a reasonable one.
507
product, between vectors x and y). For this paper,
we train the parameter vector ?? using the perceptron
algorithm (Collins, 2004; Collins, 2002). The per-
ceptron algorithm is a very fast training method, in
practice requiring only a few passes over the train-
ing set, allowing for a detailed comparison of a wide
variety of feature sets.
A number of researchers have described work
that incorporates syntactic language models into a
speech recognizer. These methods have almost ex-
clusively worked within the noisy channel paradigm,
where the syntactic language model has the task
of modeling a distribution over strings in the lan-
guage, in a very similar way to traditional n-gram
language models. The Structured Language Model
(Chelba and Jelinek, 1998; Chelba and Jelinek,
2000; Chelba, 2000; Xu et al, 2002; Xu et al, 2003)
makes use of an incremental shift-reduce parser to
enable the probability of words to be conditioned on
k previous c-commanding lexical heads, rather than
simply on the previous k words. Incremental top-
down and left-corner parsing (Roark, 2001a; Roark,
2001b) and head-driven parsing (Charniak, 2001)
approaches have directly used generative PCFG
models as language models. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar were used to exploit syn-
tactic dependencies.
Our approach differs from previous work in a cou-
ple of important respects. First, through the feature-
vector representations ?(a,w) we can essentially
incorporate arbitrary sources of information from
the string or parse tree into the model. We would ar-
gue that our method allows considerably more flexi-
bility in terms of the choice of features in the model;
in previous work features were incorporated in the
model through modification of the underlying gen-
erative parsing or tagging model, and modifying a
generative model is a rather indirect way of chang-
ing the features used by a model. In this respect, our
approach is similar to that advocated in Rosenfeld et
al. (2001), which used Maximum Entropy modeling
to allow for the use of shallow syntactic features for
language modeling.
A second contrast between our work and previ-
ous work, including that of Rosenfeld et al (2001),
is in the use of discriminative parameter estimation
techniques. The criterion we use to optimize the pa-
rameter vector ?? is closely related to the end goal
in speech recognition, i.e., word error rate. Previ-
ous work (Roark et al, 2004a; Roark et al, 2004b)
has shown that discriminative methods within an n-
gram approach can lead to significant reductions in
WER, in spite of the features being of the same type
as the original language model. In this paper we ex-
tend this approach, by including syntactic features
that were not in the baseline speech recognizer.
This paper describe experiments using a variety
of syntactic features within this approach. We tested
the model on the Switchboard (SWB) domain, using
the recognizer of Ljolje et al (2003). The discrim-
inative approach for n-gram modeling gave a 0.9%
reduction in WER on this domain; the syntactic fea-
tures we describe give a further 0.3% reduction.
In the remainder of this paper, section 2 describes
previous work, including the parameter estimation
methods we use, and section 3 describes the feature-
vector representations of parse trees that we used in
our experiments. Section 4 describes experiments
using the approach.
2 Background
2.1 Previous Work
Techniques for exploiting stochastic context-free
grammars for language modeling have been ex-
plored for more than a decade. Early approaches
included algorithms for efficiently calculating string
prefix probabilities (Jelinek and Lafferty, 1991; Stol-
cke, 1995) and approaches to exploit such algo-
rithms to produce n-gram models (Stolcke and Se-
gal, 1994; Jurafsky et al, 1995). The work of Chelba
and Jelinek (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000; Chelba, 2000) involved the use of a
shift-reduce parser trained on Penn treebank style
annotations, that maintains a weighted set of parses
as it traverses the string from left-to-right. Each
word is predicted by each candidate parse in this set
at the point when the word is shifted, and the con-
ditional probability of the word given the previous
words is taken as the weighted sum of the condi-
tional probabilities provided by each parse. In this
approach, the probability of a word is conditioned
by the top two lexical heads on the stack of the par-
508
ticular parse. Enhancements in the feature set and
improved parameter estimation techniques have ex-
tended this approach in recent years (Xu et al, 2002;
Xu et al, 2003).
Roark (2001a; 2001b) pursued a different deriva-
tion strategy from Chelba and Jelinek, and used the
parse probabilities directly to calculate the string
probabilities. This work made use of a left-to-right,
top-down, beam-search parser, which exploits rich
lexico-syntactic features from the left context of
each derivation to condition derivation move proba-
bilities, leading to a very peaked distribution. Rather
than normalizing a prediction of the next word over
the beam of candidates, as in Chelba and Jelinek,
in this approach the string probability is derived by
simply summing the probabilities of all derivations
for that string in the beam.
Other work on syntactic language modeling in-
cludes that of Charniak (2001), which made use of
a non-incremental, head-driven statistical parser to
produce string probabilities. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar, were used to exploit syn-
tactic dependencies. The processing advantages of
the finite-state encoding of the model has allowed
for the use of probabilities calculated off-line from
this model to be used in the first pass of decoding,
which has provided additional benefits. Finally, Och
et al (2004) use a reranking approach with syntactic
information within a machine translation system.
Rosenfeld et al (2001) investigated the use of
syntactic features in a Maximum Entropy approach.
In their paper, they used a shallow parser to anno-
tate base constituents, and derived features from se-
quences of base constituents. The features were in-
dicator features that were either (1) exact matches
between a set or sequence of base constituents with
those annotated on the hypothesis transcription; or
(2) tri-tag features from the constituent sequence.
The generative model that resulted from their fea-
ture set resulted in only a very small improvement
in either perplexity or word-error-rate.
2.2 Global Linear Models
We follow the framework of Collins (2002; 2004),
recently applied to language modeling in Roark et
al. (2004a; 2004b). The model we propose consists
of the following components:
? GEN(a) is a set of candidate strings for an
acoustic input a. In our case, GEN(a) is a set of
1000-best strings from a first-pass recognizer.
? T (w) is the parse tree for string w.
? ?(a,w) ? Rd is a feature-vector representation
of an acoustic input a together with a string w.
? ?? ? Rd is a parameter vector.
? The output of the recognizer for an input a is
defined as
F (a) = argmax
w?GEN(a)
??(a,w), ??? (3)
In principle, the feature vector ?(a,w) could take
into account any features of the acoustic input a to-
gether with the utterance w. In this paper we make
a couple of restrictions. First, we define the first fea-
ture to be
?1(a,w) = ? log Pl(w) + log Pa(a|w)
where Pl(w) and Pa(a|w) are language and acous-
tic model scores from the baseline speech recog-
nizer. In our experiments we kept ? fixed at the
value used in the baseline recogniser. It can then
be seen that our model is equivalent to the model
in Eq. 2. Second, we restrict the remaining features
?2(a,w) . . . ?d(a,w) to be sensitive to the string
w alone.2 In this sense, the scope of this paper is
limited to the language modeling problem. As one
example, the language modeling features might take
into account n-grams, for example through defini-
tions such as
?2(a,w) = Count of the the in w
Previous work (Roark et al, 2004a; Roark et al,
2004b) considered features of this type. In this pa-
per, we introduce syntactic features, which may be
sensitive to the parse tree for w, for example
?3(a,w) = Count of S ? NP VP in T (w)
where S ? NP VP is a context-free rule produc-
tion. Section 3 describes the full set of features used
in the empirical results presented in this paper.
2Future work may consider features of the acoustic sequence
a together with the string w, allowing the approach to be ap-
plied to acoustic modeling.
509
2.2.1 Parameter Estimation
We now describe how the parameter vector ?? is
estimated from a set of training utterances. The
training set consists of examples (ai,wi) for i =
1 . . .m, where ai is the i?th acoustic input, and wi
is the transcription of this input. We briefly review
the two training algorithms described in Roark et al
(2004b), the perceptron algorithm and global condi-
tional log-linear models (GCLMs).
Figure 1 shows the perceptron algorithm. It is an
online algorithm, which makes several passes over
the training set, updating the parameter vector after
each training example. For a full description of the
algorithm, see Collins (2004; 2002).
A second parameter estimation method, which
was used in (Roark et al, 2004b), is to optimize
the log-likelihood under a log-linear model. Sim-
ilar approaches have been described in Johnson et
al. (1999) and Lafferty et al (2001). The objective
function used in optimizing the parameters is
L(??) =
?
i
log P (si|ai, ??) ? C
?
j
?2j (4)
where P (si|ai, ??) = e
??(ai,si),???
?
w?GEN(ai) e
??(ai,w),??? .
Here, each si is the member of GEN(ai) which
has lowest WER with respect to the target transcrip-
tion wi. The first term in L(??) is the log-likelihood
of the training data under a conditional log-linear
model. The second term is a regularization term
which penalizes large parameter values. C is a con-
stant that dictates the relative weighting given to the
two terms. The optimal parameters are defined as
??? = arg max
??
L(??)
We refer to these models as global conditional log-
linear models (GCLMs).
Each of these algorithms has advantages. A num-
ber of results?e.g., in Sha and Pereira (2003) and
Roark et al (2004b)?suggest that the GCLM ap-
proach leads to slightly higher accuracy than the per-
ceptron training method. However the perceptron
converges very quickly, often in just a few passes
over the training set?in comparison GCLM?s can
take tens or hundreds of gradient calculations before
convergence. In addition, the perceptron can be used
as an effective feature selection technique, in that
Input: A parameter specifying the number of iterations over
the training set, T . A value for the first parameter, ?. A
feature-vector representation ?(a,w) ? Rd. Training exam-
ples (ai,wi) for i = 1 . . . m. An n-best list GEN(ai) for each
training utterance. We take si to be the member of GEN(ai)
which has the lowest WER when compared to wi.
Initialization: Set ?1 = ?, and ?j = 0 for j =
2 . . . d.
Algorithm: For t = 1 . . . T, i = 1 . . . m
?Calculate yi = arg maxw?GEN(ai) ??(ai,w), ???
? For j = 2 . . .m, set ??j = ??j + ?j(ai, si) ?
?j(ai,yi)
Output: Either the final parameters ??, or the averaged pa-
rameters ??avg defined as ??avg =
?
t,i ??t,i/mT where ??t,i is
the parameter vector after training on the i?th training example
on the t?th pass through the training data.
Figure 1: The perceptron training algorithm. Following
Roark et al (2004a), the parameter ?1 is set to be some con-
stant ? that is typically chosen through optimization over the
development set. Recall that ?1 dictates the weight given to the
baseline recognizer score.
at each training example it only increments features
seen on si or yi, effectively ignoring all other fea-
tures seen on members of GEN(ai). For example,
in the experiments in Roark et al (2004a), the per-
ceptron converged in around 3 passes over the train-
ing set, while picking non-zero values for around 1.4
million n-gram features out of a possible 41 million
n-gram features seen in the training set.
For the present paper, to get a sense of the relative
effectiveness of various kinds of syntactic features
that can be derived from the output of a parser, we
are reporting results using just the perceptron algo-
rithm. This has allowed us to explore more of the po-
tential feature space than we would have been able
to do using the more costly GCLM estimation tech-
niques. In future we plan to apply GLCM parameter
estimation methods to the task.
3 Parse Tree Features
We tagged each candidate transcription with (1)
part-of-speech tags, using the tagger documented in
Collins (2002); and (2) a full parse tree, using the
parser documented in Collins (1999). The models
for both of these were trained on the Switchboard
510
SNP
PRP
we
VP
VBD
helped
NP
PRP
her
VP
VB
paint
NP
DT
the
NN
house
Figure 2: An example parse tree
treebank, and applied to candidate transcriptions in
both the training and test sets. Each transcription
received one POS-tag annotation and one parse tree
annotation, from which features were extracted.
Figure 2 shows a Penn Treebank style parse tree
that is of the sort produced by the parser. Given such
a structure, there is a tremendous amount of flexibil-
ity in selecting features. The first approach that we
follow is to map each parse tree to sequences encod-
ing part-of-speech (POS) decisions, and ?shallow?
parsing decisions. Similar representations have been
used by (Rosenfeld et al, 2001; Wang and Harper,
2002). Figure 3 shows the sequential representations
that we used. The first simply makes use of the POS
tags for each word. The latter representations make
use of sequences of non-terminals associated with
lexical items. In 3(b), each word in the string is asso-
ciated with the beginning or continuation of a shal-
low phrase or ?chunk? in the tree. We include any
non-terminals above the level of POS tags as poten-
tial chunks: a new ?chunk? (VP, NP, PP etc.) begins
whenever we see the initial word of the phrase dom-
inated by the non-terminal. In 3(c), we show how
POS tags can be added to these sequences. The final
type of sequence mapping, shown in 3(d), makes a
similar use of chunks, but preserves only the head-
word seen with each chunk.3
From these sequences of categories, various fea-
tures can be extracted, to go along with the n-gram
features used in the baseline. These include n-tag
features, e.g. ti?2ti?1ti (where ti represents the
3It should be noted that for a very small percentage of hy-
potheses, the parser failed to return a full parse tree. At the
end of every shallow tag or category sequence, a special end of
sequence tag/word pair ?</parse> </parse>? was emit-
ted. In contrast, when a parse failed, the sequence consisted of
solely ?<noparse> <noparse>?.
(a)
we/PRP helped/VBD her/PRP paint/VB the/DT
house/NN
(b)
we/NPb helped/VPb her/NPb paint/VPb the/NPb
house/NPc
(c)
we/PRP-NPb helped/VBD-VPb her/PRP-NPb
paint/VB-VPb the/DT-NPb house/NN-NPc
(d)
we/NP helped/VP her/NP paint/VP house/NP
Figure 3: Sequences derived from a parse tree: (a) POS-tag
sequence; (b) Shallow parse tag sequence?the superscripts b
and c refer to the beginning and continuation of a phrase re-
spectively; (c) Shallow parse tag plus POS tag sequence; and
(d) Shallow category with lexical head sequence
tag in position i); and composite tag/word features,
e.g. tiwi (where wi represents the word in posi-
tion i) or, more complicated configurations, such as
ti?2ti?1wi?1tiwi. These features can be extracted
from whatever sort of tag/word sequence we pro-
vide for feature extraction, e.g. POS-tag sequences
or shallow parse tag sequences.
One variant that we performed in feature extrac-
tion had to do with how speech repairs (identified as
EDITED constituents in the Switchboard style parse
trees) and filled pauses or interjections (labeled with
the INTJ label) were dealt with. In the simplest ver-
sion, these are simply treated like other constituents
in the parse tree. However, these can disrupt what
may be termed the intended sequence of syntactic
categories in the utterance, so we also tried skipping
these constituents when mapping from the parse tree
to shallow parse sequences.
The second set of features we employed made
use of the full parse tree when extracting features.
For this paper, we examined several features tem-
plates of this type. First, we considered context-free
rule instances, extracted from each local node in the
tree. Second, we considered features based on lex-
ical heads within the tree. Let us first distinguish
between POS-tags and non-POS non-terminal cate-
gories by calling these latter constituents NTs. For
each constituent NT in the tree, there is an associ-
ated lexical head (HNT) and the POS-tag of that lex-
ical head (HPNT). Two simple features are NT/HNT
and NT/HPNT for every NT constituent in the tree.
511
Feature Examples from figure 2
(P,HCP,Ci,{+,-}{1,2},HP,HCi ) (VP,VB,NP,1,paint,house)
(S,VP,NP,-1,helped,we)
(P,HCP,Ci,{+,-}{1,2},HP,HPCi ) (VP,VB,NP,1,paint,NN)
(S,VP,NP,-1,helped,PRP)
(P,HCP,Ci,{+,-}{1,2},HPP,HCi ) (VP,VB,NP,1,VB,house)
(S,VP,NP,-1,VBD,we)
(P,HCP,Ci,{+,-}{1,2},HPP,HPCi ) (VP,VB,NP,1,VB,NN)
(S,VP,NP,-1,VBD,PRP)
Table 1: Examples of head-to-head features. The examples
are derived from the tree in figure 2.
Using the heads as identified in the parser, example
features from the tree in figure 2 would be S/VBD,
S/helped, NP/NN, and NP/house.
Beyond these constituent/head features, we can
look at the head-to-head dependencies of the sort
used by the parser. Consider each local tree, con-
sisting of a parent node (P), a head child (HCP), and
k non-head children (C1 . . . Ck). For each non-head
child Ci, it is either to the left or right of HCP, and is
either adjacent or non-adjacent to HCP. We denote
these positional features as an integer, positive if to
the right, negative if to the left, 1 if adjacent, and 2 if
non-adjacent. Table 1 shows four head-to-head fea-
tures that can be extracted for each non-head child
Ci. These features include dependencies between
pairs of lexical items, between a single lexical item
and the part-of-speech of another item, and between
pairs of part-of-speech tags in the parse.
4 Experiments
The experimental set-up we use is very similar to
that of Roark et al (2004a; 2004b), and the exten-
sions to that work in Roark et al (2005). We make
use of the Rich Transcription 2002 evaluation test
set (rt02) as our development set, and use the Rich
Transcription 2003 Spring evaluation CTS test set
(rt03) as test set. The rt02 set consists of 6081 sen-
tences (63804 words) and has three subsets: Switch-
board 1, Switchboard 2, Switchboard Cellular. The
rt03 set consists of 9050 sentences (76083 words)
and has two subsets: Switchboard and Fisher.
The training set consists of 297580 transcribed
utterances (3297579 words)4. For each utterance,
4Note that Roark et al (2004a; 2004b; 2005) used 20854 of
these utterances (249774 words) as held out data. In this work
we simply use the rt02 test set as held out and development data.
a weighted word-lattice was produced, represent-
ing alternative transcriptions, from the ASR system.
The baseline ASR system that we are comparing
against then performed a rescoring pass on these first
pass lattices, allowing for better silence modeling,
and replaces the trigram language model score with
a 6-gram model. 1000-best lists were then extracted
from these lattices. For each candidate in the 1000-
best lists, we identified the number of edits (inser-
tions, deletions or substitutions) for that candidate,
relative to the ?target? transcribed utterance. The or-
acle score for the 1000-best lists was 16.7%.
To produce the word-lattices, each training utter-
ance was processed by the baseline ASR system. In
a naive approach, we would simply train the base-
line system (i.e., an acoustic model and language
model) on the entire training set, and then decode
the training utterances with this system to produce
lattices. We would then use these lattices with the
perceptron algorithm. Unfortunately, this approach
is likely to produce a set of training lattices that are
very different from test lattices, in that they will have
very low word-error rates, given that the lattice for
each utterance was produced by a model that was
trained on that utterance. To somewhat control for
this, the training set was partitioned into 28 sets, and
baseline Katz backoff trigram models were built for
each set by including only transcripts from the other
27 sets. Lattices for each utterance were produced
with an acoustic model that had been trained on the
entire training set, but with a language model that
was trained on the 27 data portions that did not in-
clude the current utterance. Since language mod-
els are generally far more prone to overtraining than
standard acoustic models, this goes a long way to-
ward making the training conditions similar to test-
ing conditions. Similar procedures were used to
train the parsing and tagging models for the training
set, since the Switchboard treebank overlaps exten-
sively with the ASR training utterances.
Table 2 presents the word-error rates on rt02 and
rt03 of the baseline ASR system, 1000-best percep-
tron and GCLM results from Roark et al (2005)
under this condition, and our 1000-best perceptron
results. Note that our n-best result, using just n-
gram features, improves upon the perceptron result
of (Roark et al, 2005) by 0.2 percent, putting us
within 0.1 percent of their GCLM result for that
512
WER
Trial rt02 rt03
ASR system output 37.1 36.4
Roark et al (2005) perceptron 36.6 35.7
Roark et al (2005) GCLM 36.3 35.4
n-gram perceptron 36.4 35.5
Table 2: Baseline word-error rates versus Roark et al (2005)
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS (1) perceptron 36.1
n-gram + POS (1,2) perceptron 36.1
n-gram + POS (1,3) perceptron 36.1
Table 3: Use of POS-tag sequence derived features
condition. (Note that the perceptron?trained n-gram
features were trigrams (i.e., n = 3).) This is due to
a larger training set being used in our experiments;
we have added data that was used as held-out data in
(Roark et al, 2005) to the training set that we use.
The first additional features that we experimented
with were POS-tag sequence derived features. Let
ti and wi be the POS tag and word at position i,
respectively. We experimented with the following
three feature definitions:
1. (ti?2ti?1ti), (ti?1ti), (ti), (tiwi)
2. (ti?2ti?1wi)
3. (ti?2wi?2ti?1wi?1tiwi), (ti?2ti?1wi?1tiwi),
(ti?1wi?1tiwi), (ti?1tiwi)
Table 3 summarizes the results of these trials on
the held out set. Using the simple features (num-
ber 1 above) yielded an improvement beyond just
n-grams, but additional, more complicated features
failed to yield additional improvements.
Next, we considered features derived from shal-
low parsing sequences. Given the results from the
POS-tag sequence derived features, for any given se-
quence, we simply use n-tag and tag/word features
(number 1 above). The first sequence type from
which we extracted features was the shallow parse
tag sequence (S1), as shown in figure 3(b). Next,
we tried the composite shallow/POS tag sequence
(S2), as in figure 3(c). Finally, we tried extract-
ing features from the shallow constituent sequence
(S3), as shown in figure 3(d). When EDITED and
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS perceptron 36.1
n-gram + POS + S1 perceptron 36.1
n-gram + POS + S2 perceptron 36.0
n-gram + POS + S3 perceptron 36.0
n-gram + POS + S3-E perceptron 36.0
n-gram + POS + CF perceptron 36.1
n-gram + POS + H2H perceptron 36.0
Table 4: Use of shallow parse sequence and full parse derived
features
INTJ nodes are ignored, we refer to this condition
as S3-E. For full-parse feature extraction, we tried
context-free rule features (CF) and head-to-head fea-
tures (H2H), of the kind shown in table 1. Table 4
shows the results of these trials on rt02.
Although the single digit precision in the table
does not show it, the H2H trial, using features ex-
tracted from the full parses along with n-grams and
POS-tag sequence features, was the best performing
model on the held out data, so we selected it for ap-
plication to the rt03 test data. This yielded 35.2%
WER, a reduction of 0.3% absolute over what was
achieved with just n-grams, which is significant at
p < 0.001,5 reaching a total reduction of 1.2% over
the baseline recognizer.
5 Conclusion
The results presented in this paper are a first step in
examining the potential utility of syntactic features
for discriminative language modeling for speech
recognition. We tried two possible sets of features
derived from the full annotation, as well as a va-
riety of possible feature sets derived from shallow
parse and POS tag sequences, the best of which
gave a small but significant improvement beyond
what was provided by the n-gram features. Future
work will include a further investigation of parser?
derived features. In addition, we plan to explore the
alternative parameter estimation methods described
in (Roark et al, 2004a; Roark et al, 2004b), which
were shown in this previous work to give further im-
provements over the perceptron.
5We use the Matched Pair Sentence Segment test for WER,
a standard measure of significance, to calculate this p-value.
513
References
Eugene Charniak. 2001. Immediate-head parsing for language
models. In Proc. ACL.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computa-
tional Linguistics, pages 225?231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for Nat-
ural Language Modeling. Ph.D. thesis, The Johns Hopkins
University.
Stanley Chen and Joshua Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Technical
Report, TR-10-98, Harvard University.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proc. EMNLP, pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer
Academic Publishers, Dordrecht.
Frederick Jelinek and John Lafferty. 1991. Computation of
the probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proc. ACL, pages 535?541.
Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas
Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan.
1995. Using a stochastic context-free grammar as a lan-
guage model for speech recognition. In Proceedings of the
IEEE Conference on Acoustics, Speech, and Signal Process-
ing, pages 189?192.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML, pages
282?289, Williams College, Williamstown, MA, USA.
Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark,
Murat Saraclar, and Izhak Shafran. 2003. The AT&T 1xRT
CTS system. In Rich Transcription Workshop.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004.
Brian Roark, Murat Saraclar, and Michael Collins. 2004a. Cor-
rective language modeling for large vocabulary ASR with the
perceptron algorithm. In Proc. ICASSP, pages 749?752.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004b. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Proc.
ACL.
Brian Roark, Murat Saraclar, and Michael Collins. 2005. Dis-
criminative n-gram language modeling. Computer Speech
and Language. submitted.
Brian Roark. 2001a. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2):249?
276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. 2001.
Whole-sentence exponential language models: a vehicle for
linguistic-statistical integration. In Computer Speech and
Language.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of the Human
Language Technology Conference and Meeting of the North
American Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), Edmonton, Canada.
Andreas Stolcke and Jonathan Segal. 1994. Precise n-gram
probabilities from stochastic context-free grammars. In Pro-
ceedings of the 32nd Annual Meeting of the Association for
Computational Linguistics, pages 74?79.
Andreas Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities. Com-
putational Linguistics, 21(2):165?202.
Wen Wang and Mary P. Harper. 2002. The superARV language
model: Investigating the effectiveness of tightly integrating
multiple knowledge sources. In Proc. EMNLP, pages 238?
247.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The
use of a linguistically motivated language model in conver-
sational speech recognition. In Proc. ICASSP.
Wen Wang. 2003. Statistical parsing and language model-
ing based on constraint dependency grammar. Ph.D. thesis,
Purdue University.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A
study on richer syntactic dependencies for structured lan-
guage modeling. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics, pages
191?198.
Peng Xu, Ahmad Emami, and Frederick Jelinek. 2003. Train-
ing connectionist models for the structured language model.
In Proc. EMNLP, pages 160?167.
514
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 273?276,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
A Stochastic Finite-State Morphological Parser for Turkish
Has?im Sak & Tunga G?ung
?
or
Dept. of Computer Engineering
Bo?gazic?i University
TR-34342, Bebek,
?
Istanbul, Turkey
hasim.sak@boun.edu.tr
gungort@boun.edu.tr
Murat Sarac?lar
Dept. of Electrical & Electronics Engineering
Bo?gazic?i University
TR-34342, Bebek,
?
Istanbul, Turkey
murat.saraclar@boun.edu.tr
Abstract
This paper presents the first stochastic
finite-state morphological parser for Turk-
ish. The non-probabilistic parser is a
standard finite-state transducer implemen-
tation of two-level morphology formal-
ism. A disambiguated text corpus of
200 million words is used to stochas-
tize the morphotactics transducer, then it
is composed with the morphophonemics
transducer to get a stochastic morpho-
logical parser. We present two applica-
tions to evaluate the effectiveness of the
stochastic parser; spelling correction and
morphology-based language modeling for
speech recognition.
1 Introduction
Turkish is an agglutinative language with a highly
productive inflectional and derivational morphol-
ogy. The computational aspects of Turkish mor-
phology have been well studied and several mor-
phological parsers have been built (Oflazer, 1994),
(G?ung?or, 1995).
In language processing applications, we may
need to estimate a probability distribution over all
word forms. For example, we need probability es-
timates for unigrams to rank misspelling sugges-
tions for spelling correction. None of the previ-
ous studies for Turkish have addressed this prob-
lem. For morphologically complex languages, es-
timating a probability distribution over a static vo-
cabulary is not very desirable due to high out-of-
vocabulary rates. It would be very convenient for a
morphological parser as a word generator/analyzer
to also output a probability estimate for a word
generated/analyzed. In this work, we build such a
stochastic morphological parser for Turkish
1
and
give two example applications for evaluation.
1
The stochastic morphological parser is available for re-
search purposes at http://www.cmpe.boun.edu.tr/?hasim
2 Language Resources
We built a morphological parser using the two-
level morphology formalism of Koskenniemi
(1984). The two-level phonological rules and the
morphotactics were adapted from the PC-KIMMO
implementation of Oflazer (1994). The rules were
compiled using the twolc rule compiler (Karttunen
and Beesley, 1992). A new root lexicon of 55,278
words based on the Turkish Language Institution
dictionary
2
was compiled. For finite-state opera-
tions and for running the parser, we used the Open-
FST weighted finite-state transducer library (Al-
lauzen et al, 2007). The parser can analyze about
8700 words per second on a 2.33 GHz Intel Xeon
processor.
We need a text corpus for estimating the param-
eters of a statistical model of morphology. For this
purpose, we compiled a text corpus of 200 million-
words by collecting texts from online newspa-
pers. The morphological parser can analyze about
96.7% of the tokens.
The morphological parser may output more
than one possible analysis for a word due to am-
biguity. For example, the parser returns four
analyses for the word kedileri as shown below.
The morphological representation is similar to
the one used by Oflazer and Inkelas (2006).
kedi[Noun]+lAr[A3pl]+SH[P3sg]+[Nom] (his/her cats)
kedi[Noun]+lAr[A3pl]+[Pnon]+YH[Acc] (the cats)
kedi[Noun]+lAr[A3pl]+SH[P3pl]+[Nom] (their cats)
kedi[Noun]+[A3sg]+lArH[P3pl]+[Nom] (their cat)
We need to resolve this ambiguity to train a prob-
abilistic morphology model. For this purpose, we
used our averaged perceptron-based morphologi-
cal disambiguator (Sak et al, 2008). The disam-
biguation system achieves about 97.05% disam-
biguation accuracy on the test set.
2
http://www.tdk.gov.tr
273
0 1k:?/2.34 2e:?/1.76 3d:?/5.68 4i:kedi[Noun] 6l:+lAr[A3pl]/1.19 5?:+[A3sg]
8e:?
7l:+lArH[P3pl]/5.73 9e:?
10r:?
11r:?
14i:+SH[P3pl]/2.89
13i:+SH[P3sg]/0.62
12?:+[Pnon] 15/3.83i:+[Nom]/1.06
?:+[Nom]
?:+[Nom]i:+YH[Acc]/1.66
Figure 1: Finite-state transducer for the word kedileri.
3 Stochastic Morphological Parser
The finite-state transducer of the morphological
parser is obtained as the composition of the mor-
phophonemics transducer mp and the morphotac-
tics transducer mt; mp ? mt. The morphotac-
tics transducer encodes the morphosyntax of the
language. If we can estimate a statistical mor-
phosyntactic model, we can convert the morpho-
logical parser to a probabilistic one by composing
the probabilistic morphotactics transducer with the
morphophonemics transducer. Eisner (2002) gives
a general EM algorithm for parameter estimation
in probabilistic finite-state transducers. The algo-
rithm uses a bookkeeping trick (expectation semir-
ing) to compute the expected number of traversals
of each arc in the E step. The M step reestimates
the probabilities of the arcs from each state to be
proportional to the expected number of traversals
of each arc - the arc probabilities are normalized
at each state to make the finite-state transducer
Markovian. However, we do not need this general
method of training. Since we can disambiguate
the possible morphosyntactic tag sequences of a
word, there is a single path in the morphotactics
transducer that matches the chosen morphosyntac-
tic tag sequence. Then the maximum-likelihood
estimates of the weights of the arcs in the morpho-
tactics transducer are found by setting the weights
proportional to the number of traversals of each
arc. We can use a specialized semiring to cleanly
and efficiently count the number of traversals of
each arc.
Weights in finite-state transducers are elements
of a semiring, which defines two binary operations
? and ?, where ? is used to combine the weights
of arcs on a path into a path weight and ? is used
to combine the weights of alternative paths (Bers-
tel and Reutenauer, 1988). We define a counting
semiring to keep track of the number of traver-
sals of each arc. The weights in the mt trans-
ducer are converted to the counting semiring. In
this semiring, the weigths are vectors of integers
having dimension as the total number of arcs in
the mt transducer. We number the arcs in the mt
transducer and set the weight of the n
th
arc as the
n
th
basis vector. The binary plus ? and the times
? operations of the counting semiring are defined
as the sum of the weight vectors. Thus, the n
th
value of the vector in the counting semiring just
counts the appearances of the n
th
arc of mt in a
path.
To estimate the weights of the stochastic model
of the mt transducer, we use the text corpus col-
lected from the web. First we parse the words
in the corpus to get al the possible analyses of
the words. Then we disambiguate the morpho-
logical analyses of the words to select one of the
morphosyntactic tag sequences x
i
for each word.
We build a finite-state transducer ? x
i
that maps
 symbol to x
i
in the counting semiring. The
weights of this transducer are zero vectors having
the same dimension as themt transducer. Then the
finite-state transducer (?x
i
)?(mt?) having all
 :  arcs can be minimized to get a one-state FST
which has the weight vector that keeps the number
of traversals of each arc in mt. The weight vec-
tor is accumulated for all the x
i
morphosyntactic
tag sequences in the corpus. The final accumu-
lated weight vector is used to assign probabilities
to each arc in the mt transducer proportional to
the traversal count of the arc, hence resulting in
the stochastic morphotactics transducer
?
mt. We
use add-one smoothing to prevent the arcs having
zero probability. The
?
mt transducer is composed
with the morphophonemics transducer mp to get a
stochastic morphological parser.
The stochastic parser now returns probabilities
with the possible analyses of a word. Figure 1
shows the weighted paths for the four possible
analyses of the word kedileri as represented in the
stochastic parser. The weights are negative log
probabilities.
4 Spelling Correction
The productive morphology of Turkish allows
one to generate very long words such as
274
?ol?ums?uzles?tirdi?gimizden. Therefore, the detection
and the correction of spelling errors by present-
ing the user with a ranked list of spelling sugges-
tions are highly desired. There have been some
previous studies for spelling checking (Solak and
Oflazer, 1993) and spelling correction (Oflazer,
1996). However there has been no study to ad-
dress the problem of ranking spelling suggestions.
One can use a stochastic morphological parser to
do spelling checking and correction, and present
spelling suggestions ranked with the parser output
probabilities. We assume that a word is misspelled
if the parser fails to return an analysis of the word.
Our method for spelling correction is to enumerate
all the valid and invalid candidates that resemble
the incorrect input word and filter the invalid ones
with the morphological parser.
To enumerate the alternative spellings for a mis-
spelled word, we generate all the words in one-
character edit distance with the input word, where
we consider one symbol insertion, deletion or sub-
stitution, or transposition of adjacent symbols.
The Turkish alphabet includes six special letters
(c?, ?g, ?, ?o, s?, ?u) that do not exist in English.
These characters may not be supported in some
keyboards and message transfer protocols; thus
people frequently use their nearest ASCII equiv-
alents (c, g, i, o, s, u, respectively) instead of the
correct forms, e.g., spelling nas?ls?n as nasilsin.
Therefore, in addition to enumerating words in
one edit distance, we also enumerate all the words
from which the misspelled word can be obtained
by replacing these special Turkish characters with
their ASCII counterparts. For instance, for the
word nasilsin, the alternative spellings nas?lsin,
nasils?n, and nas?ls?n will also be generated.
Note that although the context is important for
spelling correction, we use only unigrams. One
can build a morpheme based language model to
incorporate the context information. We also lim-
ited the edit distance to 1, but it is straightfor-
ward to allow longer edit distances. We can build
a finite-state transducer to enumerate and repre-
sent efficiently all the valid and invalid word forms
that can be obtained by these edit operations on
a word. For example, the deletion of a charac-
ter can be represented by the regular expression
?
?
(? : )?
?
which can be compiled as a finite-
state transducer, where ? is the alphabet. The
union of the transducers encoding one-edit dis-
tance operations and the restoration of the special
Turkish characters is precompiled and optimized
with determinization and minimization algorithms
for efficiency. A misspelled input word transducer
can be composed with the resulting transducer and
in turn with the morphological parser to filter out
the invalid word forms. The words with their es-
timated probabilities can be read from the output
transducer and constitute the list of spelling sug-
gestions for the word. The probabilities are used
to rank the list to show to the user. We also handle
the spelling errors where omission of a space char-
acter causes joining of two correct words by split-
ting the word into all combinations of two strings
and checking if the string pieces are valid word
forms. An example list of suggestions with the as-
signed negative log probabilities and their English
glosses for the misspelled word nasilsin is given
below.
nas?ls?n (14.2) (How are you), nakilsin (15.3) (You are
a transfer), nesilsin (21.0) (You are a generation), nasipsin
(21.2) (You are a share), basilsin (23.9) (You are a bacillus)
On a manually chosen test set containing 225 cor-
rect words which have relatively more complex
morphology and 43 commonly misspelled words,
the Precision and the Recall scores for the detec-
tion of spelling errors are 0.81 and 0.93, respec-
tively.
5 Morphology-based Language
Modeling
The closure of the transducer for the stochastic
parser can be considered as a morphology-based
unigram language model. Different than standard
unigram word language models, this morphology-
based model can assign probabilities to words not
seen in the training corpus. It can also achieve
lower out-of-vocabulary (OOV) rates than models
that use a static vocabulary by employing a rela-
tively smaller number of root words in the lexicon.
We compared the performances of the
morphology-based unigram language model
and the unigram word language model on a broad-
cast news transcription task. The acoustic model
uses Hidden Markov Models (HMMs) trained on
183.8 hours of broadcast news speech data. The
test set contains 3.1 hours of speech data (2,410
utterances). A text corpus of 1.2 million words
from the transcriptions of the news recordings was
used to train the stochastic parser as explained in
Section 3 and unigram word language models.
We experimented with four different language
275
0.5 1.0 1.5 2.0 2.5
43
44
45
46
47
48
Real?time factor (cpu time/audio time)
WER
 (%)
Morphology?basedWord?50KWord+MorphologyWord?100K
Figure 2: Word error rate versus real-time factor
obtained by changing the pruning beam width.
models. Figure 2 shows the word error rate ver-
sus run-time factor for these models. In this fig-
ure the Word-50K and Word-100K are unigram
word models with the specified vocabulary size
and have the OOV rates 7% and 4.7% on the test
set, respectively. The morphology-based model is
based on the stochastic parser and has the OOV
rate 2.8% . The ?word+morphology? model is the
union of the morphology-based model and the un-
igram word model.
Even though the morphology-based model has
a better OOV rate than the word models, the word
error rate (WER) is higher. One of the reasons is
that the transducer for the morphological parser is
ambiguous and cannot be optimized for recogni-
tion in contrast to the word models. Another rea-
son is that the probability estimates of this model
are not as good as the word models since proba-
bility mass is distributed among ambiguous parses
of a word and over the paths in the transducer.
The ?word+morphology? model seems to allevi-
ate most of the shortcomings of the morphology
model. It performs better than 50K word model
and is very close to the 100K word model. The
main advantage of morphology-based models is
that we have at hand the morphological analyses
of the words during recognition. We plan to train
a language model over the morphological features
and use this model to rescore the hypothesis gener-
ated by the morphology-based models on-the-fly.
6 Conclusion
We described the first stochastic morphological
parser for Turkish and gave two applications. The
first application is a very efficient spelling correc-
tion system where probability estimates are used
for ranking misspelling suggestions. We also gave
the preliminary results for incorporating the mor-
phology as a knowledge source in speech recogni-
tion and the results look promising.
Acknowledgments
This work was supported by the Bo?gazic?i Uni-
versity Research Fund under the grant numbers
06A102 and 08M103, the Scientific and Techno-
logical Research Council of Turkey (T
?
UB
?
ITAK)
under the grant number 107E261, the Turk-
ish State Planning Organization (DPT) under
the TAM Project, number 2007K120610 and
T
?
UB
?
ITAK B
?
IDEB 2211.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11?23. Springer. http://www.openfst.org.
Jean Berstel and Christophe Reutenauer. 1988. Ratio-
nal Series and their Languages. Springer-Verlag.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Tunga G?ung?or. 1995. Computer Processing of
Turkish: Morphological and Lexical Investigation.
Ph.D. thesis, Bo?gazic?i University.
Lauri Karttunen and Kenneth R. Beesley. 1992. Two-
level rule compiler. Technical report, Xerox Palo
Alto Research Center, Palo Alto, CA.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
ACL, pages 178?181.
Kemal Oflazer and Sharon Inkelas. 2006. The archi-
tecture and the implementation of a finite state pro-
nunciation lexicon for Turkish. Computer Speech
and Language, 20(1):80?106.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137?148.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73?89.
Has?im Sak, Tunga G?ung?or, and Murat Sarac?lar. 2008.
Turkish language resources: Morphological parser,
morphological disambiguator and web corpus. In
GoTAL 2008, volume 5221 of LNCS, pages 417?
427. Springer.
Ays?in Solak and Kemal Oflazer. 1993. Design and im-
plementation of a spelling checker for turkish. Lit-
erary and Linguistic Computing, 8(3):113?130.
276
Proceedings of NAACL-HLT 2013, pages 727?732,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Semi-Supervised Discriminative Language Modeling
with Out-of-Domain Text Data
Arda C?elebi1 and Murat Sarac?lar2
1Department of Computer Engineering
2Department of Electrical and Electronics Engineering
Bog?azic?i University, Istanbul, Turkey
{arda.celebi, murat.saraclar}@boun.edu.tr
Abstract
One way to improve the accuracy of auto-
matic speech recognition (ASR) is to use dis-
criminative language modeling (DLM), which
enhances discrimination by learning where
the ASR hypotheses deviate from the uttered
sentences. However, DLM requires large
amounts of ASR output to train. Instead,
we can simulate the output of an ASR sys-
tem, in which case the training becomes semi-
supervised. The advantage of using simu-
lated hypotheses is that we can generate as
many hypotheses as we want provided that we
have enough text material. In typical scenar-
ios, transcribed in-domain data is limited but
large amounts of out-of-domain (OOD) data
is available. In this study, we investigate how
semi-supervised training performs with OOD
data. We find out that OOD data can yield im-
provements comparable to in-domain data.
1 Introduction
Discriminative language modeling (DLM) helps
ASR systems to discriminate between acoustically
similar word sequences in the process of choos-
ing the most accurate transcription of an utterance.
DLM characterizes and learns from ASR errors by
comparing the reference transcription of the utter-
ance and the candidate hypotheses generated by the
ASR system. Although previous studies based on
this supervised setting have been successful (Roark
et al, 2007; Ar?soy et al, 2009; Ar?soy et al, 2012;
Sak et al, 2012), they require large amounts of tran-
scribed speech data and a well-trained in-domain
ASR system, both of which are hard to obtain. To
overcome this difficulty, instead of training with the
real ASR output, we can use simulated output, in
which case the training becomes semi-supervised.
Semi-supervised training for discriminative lan-
guage modeling has been shown to achieve as good
word error rate (WER) reduction as the training done
with real ASR output (Sagae et al, 2012; C?elebi et
al., 2012). In this approach, first a confusion model
(CM) is estimated from supervised data. This CM
contains all seen confusions and their occurrence
probabilities in hypotheses generated by an ASR
system. Then, the CM is used to generate a num-
ber of alternative-but-incorrect hypotheses, or simu-
lated hypotheses, for a given sentence. Since the CM
characterizes the errors that the ASR system makes,
simulated hypotheses carry these characteristics. At
the end, the DLM is trained on the reference sen-
tences and their simulated hypotheses. Although be-
ing able to simulate the output of the ASR system
allows us to generate as much output as we need
for the DLM training, there is not always enough
text data that is in the same domain as the ASR sys-
tem. Yet, it is easier to find large amounts of out-of-
domain (OOD) text data. In this study, we extend the
previous studies where in-domain text data was used
for hypothesis simulation. Instead of using limited
in-domain data, we experiment with larger amounts
of OOD data for hypothesis simulation.
The rest of the paper is organized as follows. In
Section 2, we summarize the related work. In Sec-
tion 3, we explain the methods to simulate the hy-
potheses and to train the DLM. We give the exper-
imental results in Section 4 before concluding with
Section 5.
727
2 Related Work
The earliest work on hypothesis simulation for DLM
was done by Kurata et al (2009; 2012). They gen-
erate the probable n-best lists that an ASR system
may output for a hypothetical input utterance given
a word sequence. In another study, Tan et al (2010)
propose a system for channel modeling of ASR
for simulating the ASR corruption using a phrase-
based machine translation system trained between
the reference and output phoneme sequences from
a phoneme recognizer. Jyothi and Fosler-Lussier
(2010) also model the phonetic confusions using a
confusion matrix that takes into account word-based
phone confusion log likelihoods and distances be-
tween the phonetic acoustic models. This model
is then used to generate confusable word graphs
for training a DLM using the perceptron algorithm.
Xu et al (2009) propose the concept of cohorts
and report significant WER improvement for self-
supervised DLM. Similarly, Sagae et al (2012) use
phrasal cohorts to simulate ASR output and the per-
ceptron algorithm for training. They observe half of
the WER reduction that the fully supervised meth-
ods achieve. In another parallel study, C?elebi et al
(2012) work on a Turkish ASR system and consider
various confusion models at four different granular-
ities (word, morph, syllable, and phone) and differ-
ent sampling methods to choose from a large list of
simulated hypotheses. They observe that the strat-
egy that matches the word error (WE) distribution
of the simulated hypotheses to the WE distribution
of the ASR outputs yields the best WER reduction.
While the previous studies use in-domain data
sets for simulation, it is quite common to collect
large amounts of OOD text data from the web. How-
ever, given the nature of web data, some kind of se-
lection mechanism is needed to ensure quality. Bu-
lyko et al (2007) use perplexity-based filtering to
select a relevant subset from vast amounts of web
data in order to increase the training data of the gen-
erative LM used by the ASR system. There are also
studies that use a relative-entropy based selection
mechanism in order to match the n-gram distribution
of the selected data against the in-domain data by
Sethy et al (2006; 2009). In this study, we consider
the perplexity-based selection method for a start.
3 Method
3.1 Sentence Selection from OOD Data
In order to select sentences from the OOD data,
we use three methods in addition to random selec-
tion. We calculate the perplexity of each sentence
with SRILM toolkit, which gives normalized scores
with respect to the length of the sentence. Then,
we order sentences based on their perplexity scores
in increasing order. Perplexity is calculated by a
LM trained on in-domain data. After ordering, the
top of the list contains those sentences that resem-
ble the in-domain data the most whereas the sen-
tences at the bottom resemble the in-domain data the
least. We apply the three methods on this ordered
list of sentences. The first two methods, TOP-N and
BOTTOM-N , simply get the top and bottom N sen-
tences, respectively. The third method, RC-NxM ,
picks uniformly separated N clusters of M consec-
utive sentences, while making sure that top and bot-
tom M sentences are among the selected ones.
3.2 Hypothesis Simulation
Semi-supervised DLM training uses artificially gen-
erated hypotheses which mimic the ASR system
output. To generate the hypotheses, we follow
the three-step finite state transducer based pipeline
given in C?elebi et al (2012) and summarized by the
following composition sequence:
sample(N -best(prune(W?LW?CM)?LM
?1?GM))
In the first step of the pipeline, we use the confusion
model transducer (CM) to generate all possible con-
fusions that the ASR system can make for a given
reference sentenceW . We consider syllable, morph
and word based confusion models, and convert W
to these units using the lexicon LW . The generated
alternatives are pruned for efficiency reasons.
As the output of the first step may include many
implausible sequences, the second step converts
them to morphs using LM?1 and reweights them
with a morph-based language model GM to favor
the meaningful sequences. For this, we use three ap-
proaches. The first approach is to use the LM that is
used by the ASR system, called GEN-LM. The sec-
ond LM called ASR-LM is trained from the output
of the ASR system, whereas the third approach is
not to use any language model, denoted by NO-LM,
728
in which case we just use the scores coming from
the confusion model in the first step. A large list of
of N -best (N = 1000) hypotheses are produced at
this stage.
The third step, called sampling, involves picking a
subset of the hypotheses from a larger set with broad
variety. This step is done in order to pick samples so
as to make sure that they include error variety in-
stead of just high scoring hypotheses. As done by
C?elebi et al (2012), we use four sampling meth-
ods to pick 50 hypotheses out of the highest scoring
1000 hypotheses. The simplest of them is Top50,
where we select the highest scoring 50 hypotheses.
Another method is Uniform Sampling (US) which
selects instances from the WER-ordered list in uni-
form intervals. Third method, called RC5x10, forms
5 clusters separated uniformly, each containing 10
hypotheses. Lastly, ASRdist-50 selects 50 hypothe-
ses in such a way that the WE distribution of selected
hypotheses resembles the WE distribution of the real
ASR output as much as it can. We accomplish this
by filling the WE bins with the hypotheses having
required number of WEs.
3.3 DLM Estimation
The training of the DLM involves representing the
training data as feature vectors and processing via
a discriminative learning algorithm. We represent
the simulated N -best lists using unigram features as
described by Dikici et al (2012). As the learning
algorithm, we apply the WER-sensitive perceptron
algorithm proposed by Sak et al (2011b), which has
been shown to perform better for reranking ASR hy-
potheses as it minimizes an objective function based
on the WER rather than the number of misclassifi-
cations.
4 Experiments
4.1 Experimental Setup
We employ DLM on a Turkish broadcast news tran-
scription data set (Ar?soy et al, 2009), which com-
prises disjoint training (105356 sentences), held-out
(1947 sentences) and test (1784 sentences) subsets
consisting of ASR outputs represented as N -best
lists. We use Morfessor (Creutz and Lagus, 2005)
to obtain the morph level word segmentations from
which we build the LMs. For semi-supervised ex-
periments, we use the first half of the training sub-
set (t1: 53992 sentences, 965K morphs) to learn
the confusion models, and the reference transcrip-
tions of the second half (t2: 51364 sentences, 935K
morphs) to generate in-domain simulated n-best lists
to be compared against OOD simulated ones. For
this setup, the generative baseline WER and oracle
WER on the held-out set are 22.9% and 14.2% and
on the test set are 22.4% and 13.9%, respectively.
When we use ASR 50-best from t1 for DLM train-
ing, WERs drop to 22.2% and 21.8% on the held-out
and the test sets, respectively.
For OOD data, we use a data set of 10.8M
sentences (140M morphs) from newspaper articles
downloaded from the Internet (Sak et al, 2011a).
To calculate the perplexity of OOD sentences for se-
lection, we use a language model trained over the
reference transcripts and 50-best lists of t1 and t2.
4.2 Results on Out-of-Domain Data
We start our experiments with 500K randomly se-
lected OOD sentences, or RAND-500K. We run
the simulation pipeline with four sampling methods,
three confusion and three language models, giving
36 experiments in total. We choose among the pro-
posed sampling approaches and confusion models
using a rank-based comparison as done by Dikici et
al. (2012).
We look at which sampling method performs the
best by first dividing experiments into 9 groups, each
having 4 results from all sampling methods. Within
each group, we rank the sampling methods based on
the WER they achieve in increasing order and take
the average of assigned ranks. ASRdist-50 gets the
lowest average rank of 1.8, while RC5x10, US-50,
and TOP-50 come after with the averages of 2.1, 2.4,
and 3.4, respectively. This shows that ASRdist-50
gives the best WER reduction on OOD data, which
is also true for in-domain data (C?elebi et al, 2012).
Doing the same rank-based comparison for the
CMs this time, we observe that the syllable and
morph-based models have the same average rank of
1.5, whereas the word-based model has 2.8. How-
ever, a closer look reveals that the syllable-based
CM paired with NO-LM is an outlier because NO-
LM approach allows variety at the output but when
the unit of the confusion model is as small as syl-
lables, it produces too much variety that deterio-
729
rates the discriminative model. If we don?t consider
the ranks coming from NO-LM, the average rank of
syllable- and morph-based models become 1.1 and
1.8, respectively. Thus, we use syllable-based mod-
els over the others for the rest of the experiments.
Knowing that the ASRdist-50 sampling method
and syllable-based CM together give the best re-
sults for RAND-500K, we experiment with three
more sentence selection methods described in Sec-
tion 3.1. Table 1 shows all the results obtained from
four 500K OOD data sets.
OOD Data sets GEN-LM ASR-LM NO-LM
TOP-500K 22.6 22.6 22.6
BOTTOM-500K 22.4 22.2 22.5
RAND-500K 22.2 22.5 22.6
RC-5x100K 22.4 22.6 22.5
Table 1: WER (%) on held-out set obtained with syllable-
based CMs and ASRdist-50 sampling method
According to Table 1, the highest WER reduc-
tion is achieved with BOTTOM-500K+ASR-LM
and RAND-500K+GEN-LM combinations. While
ASR-LM exceeds the other two LMs only in the
case of BOTTOM-500K, for other three OOD data
sets GEN-LM gives the best results. More interest-
ingly, using OOD sentences resembling in-domain
data (or TOP-500K) is outperformed in all cases,
especially by BOTTOM-500K. To understand this,
we look at the number of morphs in each data set
given in Table 2. Even though each OOD data set
has 500K sentences, BOTTOM-500K has the high-
est number of morphs (?6.5M) and TOP-500K had
the lowest (?3.5M), while the other two have around
5.5M morphs. We also look at the morph unigram
distribution (M) of all four data sets and calculat-
ing the KL divergence KL(M || U)1 of each M to
uniform distribution (U). We observe that the uni-
gram morph distribution of the TOP-500K data set
is the least uniform with KL distance of 6.6, whereas
BOTTOM-500K has KL distance of 2.7 and the
other two have KL distances of around 4.3. In
other words, this shows that TOP-500K has the low-
est content variation, especially when compared to
BOTTOM-500K. Note also the slightly high value
of KL distance for t2, which can be attributed to the
1KL(M || U) =
?
i pilog(
pi
1/V ) = log(V ) ? H(p), where
V = 61294 and H(p) is the entropy of p.
relatively low number of unique morphs (types).
Data set KLD Types Tokens
t2 (50K) 4.65 22,107 935,137
TOP-500K 6.63 20,689 3,519,012
BOTTOM-500K 2.71 54,458 6,474,385
RAND-500K 4.36 50,422 5,559,763
RC-5x100K 4.35 50,561 5,343,342
Table 2: KL distance, KL(M || U), between uniform dis-
tribution (U) and unigram morph distribution (M); num-
ber of unique morphs and tokens.
4.3 Out-of-Domain vs In-Domain Data
In this section, we compare the results for in-domain
data with the results for four OOD data sets in
Table 3. In order to see how the size of OOD
data set affects the WER reduction, we start with
50K sentences and increase the size gradually up
to 500K. The first row of Table 3 shows the WER
obtained with the in-domain data t2, containing ap-
proximately 50K sentences.
Data 50K 100K 200K 500K
t2 22.4 - - -
TOP 22.8 22.7 22.7 22.6
BOTTOM 22.6 22.4 22.3 22.2
RAND 22.5 22.3 22.3 22.2
RC-5 22.5 22.5 22.3 22.4
Table 3: WER (%) on held-out set for in-domain
(Syllable+ASR-LM+ASRdist-50) and four OOD data
sets in increasing sizes
According to Table 3, even though 50K OOD sen-
tences yield worse results than the same amount of
in-domain sentences, as the size of OOD data set
increases, the amount of WER reduction increases
and surpasses the level obtained by using in-domain
data. What is more interesting is that RAND outper-
forms in-domain data starting from 100K, whereas
BOTTOM starts at a higher WER but drops rela-
tively fast, leveling with RAND starting at 200K.
Note that the best WER achieved with the simulated
data matches the supervised DLM performance us-
ing ASR 50-best from t1, reported in Section 4.1.
Then we go one step further and expand the BOT-
TOM data set to 1M sentences and we observe WER
of 22.1% on the held-out set. This further supports
730
the observation that the more OOD data we use, the
lower WER we can achieve.
As a side observation, when we calculate the
WER of five 100K-blocks from the RAND-500K
set, we find that the standard deviation of WER is
0.06%, which gives and idea about the significance
level of the WER differences.
4.4 Merging Real and Simulated Hypotheses
We also evaluate whether merging simulated hy-
potheses with real ASR hypotheses yields further
WER reductions. The result of merging the real hy-
potheses from t1 with the simulated ones from in-
domain and OOD data are shown in Table 4. The
first row shows the WER of the combination with
the simulated hypotheses from in-domain data t2.
Real Simulated WER (%)
t1 t2 (50K) 22.0
t1 TOP-500K 22.3
t1 BOTTOM-500K 22.1
t1 RAND-500K 22.0
t1 RC-5x100K 22.1
t1 BOTTOM-1M 21.9
Table 4: WER (%) on held-out set obtained by merging
real and simulated hypotheses
When combined with the real hypotheses from t1,
RAND500K achieves the same level of WER re-
duction as the simulated hypotheses from t2 on the
heldout set. The results on the test set are also sim-
ilar. On the test set, the combination of the real
hypotheses from t1 and the simulated hypotheses
from t2 achieve 21.5% WER, whereas the WER is
21.6% when the simulated hypotheses from t2 are
replaced by those from RAND500K. This indicates
that enough OOD data can replace the in-domain
data and yield similar performance, even in combi-
nation with in-domain real data.
Moreover, we further expand the OOD data to 1M
for BOTTOM, however even though it reduces the
WER on the heldout set, it achieves slightly higher
WER on the test set (21.7%).
Next, we combine the in-domain real hypotheses
from t1, simulated hypotheses from t2 and simulated
ones from the OOD data sets. However, compared
to the combination of t1 and t2, adding extra 500K
OOD hypotheses on top of those two gives similar
WERs on the held-out set while WERs on the test
set increases slightly. From another point of view,
adding in-domain simulated hypotheses from t2 on
top of real ones from t1 and 500K OOD data (rows
2-5 in Table 4) provides slight WER improvement
on the held-out set but not on the test set.
5 Conclusion
In this study, we investigate whether we can
achieve the same level of WER reduction for semi-
supervised DLM with the large amounts of OOD
data instead of in-domain data. We observe that
ASRdist-50 sampling method and syllable-based
CMs yield the best results with the OOD data. More-
over, selecting OOD sentences randomly rather than
using perplexity-based methods is enough to achieve
the best WER reduction. We also observe that sim-
ulated hypotheses from the OOD data is almost as
good as in-domain simulated hypotheses or even real
ones. As a future work, we will increase the size of
the OOD data and examine other methods like rela-
tive entropy based OOD selection.
Acknowledgments
This research is supported in part by TU?BI?TAK un-
der the project number 109E142.
References
Ebru Ar?soy, Dog?an Can, S?dd?ka Parlak, Has?im Sak,
and Murat Sarac?lar. 2009. Turkish broadcast news
transcription and retrieval. IEEE Transactions on Au-
dio, Speech, and Language Processing, 17(5):874?
883, July.
Ebru Ar?soy, Murat Sarac?lar, Brian Roark, and Izhak
Shafran. 2012. Discriminative language modeling
with linguistic and statistically derived features. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 20(2):540?550, February.
Ivan Bulyko, Mari Ostendorf, Man-Hung Siu, Tim Ng,
Andreas Stolcke, and O?zgu?r C?etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Transactions on Speech and
Language Processing, 5(1):1?25, December.
Arda C?elebi, Has?im Sak, Erinc? Dikici, Murat Sarac?lar,
Maider Lehr, Emily T. Prud?hommeaux, Puyang Xu,
Nathan Glenn, Damianos Karakos, Sanjeev Khudan-
pur, Brian Roark, Kenji Sagae, Izhak Shafran, Daniel
Bikel, Chris Callison-Burch, Yuan Cao, Keith Hall,
731
Eva Hasler, Philipp Koehn, Adam Lopez, Matt Post,
and Darcey Riley. 2012. Semi-supervised discrimi-
native language modeling for Turkish ASR. In Proc.
ICASSP, pages 5025?5028.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical re-
port, Helsinki University of Technology. Publications
in Computer and Information Science Report A81.
Erinc? Dikici, Arda C?elebi, and Murat Sarac?lar. 2012.
Performance comparison of training algorithms for
semi-supervised discriminative language modeling. In
Proc. Interspeech, Oregon, Portland, September.
Preethi Jyothi and Eric Fosler-Lussier. 2010. Discrimi-
native language modeling using simulated ASR errors.
In Proc. Interspeech, pages 1049?1052.
Gakuto Kurata, Nobuyasu Itoh, and Masafumi
Nishimura. 2009. Acoustically discriminative
training for language models. In Proc. ICASSP, pages
4717?4720.
Gakuto Kurata, Abhinav Sethy, Bhuvana Ramabhad-
ran, Ariya Rastrow, Nobuyasu Itoh, and Masafumi
Nishimura. 2012. Acoustically discriminative lan-
guage model training with pseudo-hypothesis. Speech
Communication, 54(2):219?228.
Brian Roark, Murat Sarac?lar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392, April.
Kenji Sagae, Maider Lehr, Emily T. Prud?hommeaux,
Puyang Xu, Nathan Glenn, Damianos Karakos, San-
jeev Khudanpur, Brian Roark, Murat Sarac?lar, Izhak
Shafran, Daniel Bikel, Chris Callison-Burch, Yuan
Cao, Keith Hall, Eva Hasler, Philipp Koehn, Adam
Lopez, Matt Post, and Darcey Riley. 2012. Halluci-
nated n-best lists for discriminative language model-
ing. In Proc. ICASSP.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2011a.
Resources for turkish morphological processing. Lan-
guage Resources and Evaluation, 45(2):249?261.
Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r. 2011b.
Discriminative reranking of ASR hypotheses with
morpholexical and n-best-list features. In Proc. ASRU,
pages 202?207.
Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r. 2012.
Morpholexical and discriminative language models for
Turkish automatic speech recognition. IEEE Trans-
actions on Audio, Speech, and Language Processing,
20(8):2341?2351, October.
Abhinav Sethy, Panayiotis G. Georgiou, and Shrikanth
Narayanan. 2006. Text data acquisition for domain-
specific language models. In EMNLP ?06 Proceedings
of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 382?389.
Abhinav Sethy, Panayiotis G. Georgiou, Bhuvana Ram-
abhadran, and Shrikanth Narayanan. 2009. An itera-
tive relative entropy minimization-based data selection
approach for n-gram model adaptation. IEEE Trans-
actions on Audio, Speech and Language Processing,
17(1):13?23, January.
Qun Feng Tan, Kartik Audhkhasi, Panayiotis G. Geor-
giou, Emil Ettelaie, and Shrikanth Narayanan. 2010.
Automatic speech recognition system channel model-
ing. In Proc. Interspeech, pages 2442?2445.
Puyang Xu, Damianos Karakos, and Sanjeev Khudanpur.
2009. Self-supervised discriminative training of statis-
tical language models. In Proc. ASRU, pages 317?322.
732
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182?187,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bayesian Word Alignment for Statistical Machine Translation
Cos?kun Mermer1,2
1BILGEM
TUBITAK
Gebze 41470 Kocaeli, Turkey
coskun@uekae.tubitak.gov.tr
Murat Sarac?lar2
2Electrical and Electronics Eng. Dept.
Bogazici University
Bebek 34342 Istanbul, Turkey
murat.saraclar@boun.edu.tr
Abstract
In this work, we compare the translation
performance of word alignments obtained
via Bayesian inference to those obtained via
expectation-maximization (EM). We propose
a Gibbs sampler for fully Bayesian inference
in IBM Model 1, integrating over all possi-
ble parameter values in finding the alignment
distribution. We show that Bayesian inference
outperforms EM in all of the tested language
pairs, domains and data set sizes, by up to 2.99
BLEU points. We also show that the proposed
method effectively addresses the well-known
rare word problem in EM-estimated models;
and at the same time induces a much smaller
dictionary of bilingual word-pairs.
1 Introduction
Word alignment is a crucial early step in the training
of most statistical machine translation (SMT) sys-
tems, in which the estimated alignments are used for
constraining the set of candidates in phrase/grammar
extraction (Koehn et al, 2003; Chiang, 2007; Galley
et al, 2006). State-of-the-art word alignment mod-
els, such as IBM Models (Brown et al, 1993), HMM
(Vogel et al, 1996), and the jointly-trained symmet-
ric HMM (Liang et al, 2006), contain a large num-
ber of parameters (e.g., word translation probabili-
ties) that need to be estimated in addition to the de-
sired hidden alignment variables.
The most common method of inference in such
models is expectation-maximization (EM) (Demp-
ster et al, 1977) or an approximation to EM when
exact EM is intractable. However, being a maxi-
mization (e.g., maximum likelihood (ML) or max-
imum a posteriori (MAP)) technique, EM is gen-
erally prone to local optima and overfitting. In
essence, the alignment distribution obtained via EM
takes into account only the most likely point in the
parameter space, but does not consider contributions
from other points.
Problems with the standard EM estimation of
IBM Model 1 was pointed out by Moore (2004) and
a number of heuristic changes to the estimation pro-
cedure, such as smoothing the parameter estimates,
were shown to reduce the alignment error rate, but
the effects on translation performance was not re-
ported. Zhao and Xing (2006) note that the param-
eter estimation (for which they use variational EM)
suffers from data sparsity and use symmetric Dirich-
let priors, but they find the MAP solution.
Bayesian inference, the approach in this paper,
have recently been applied to several unsupervised
learning problems in NLP (Goldwater and Griffiths,
2007; Johnson et al, 2007) as well as to other tasks
in SMT such as synchronous grammar induction
(Blunsom et al, 2009) and learning phrase align-
ments directly (DeNero et al, 2008).
Word alignment learning problem was addressed
jointly with segmentation learning in Xu et al
(2008), Nguyen et al (2010), and Chung and Gildea
(2009). The former two works place nonparametric
priors (also known as cache models) on the param-
eters and utilize Gibbs sampling. However, align-
ment inference in neither of these works is exactly
Bayesian since the alignments are updated by run-
ning GIZA++ (Xu et al, 2008) or by local maxi-
mization (Nguyen et al, 2010). On the other hand,
182
Chung and Gildea (2009) apply a sparse Dirichlet
prior on the multinomial parameters to prevent over-
fitting. They use variational Bayes for inference, but
they do not investigate the effect of Bayesian infer-
ence to word alignment in isolation. Recently, Zhao
and Gildea (2010) proposed fertility extensions to
IBM Model 1 and HMM, but they do not place any
prior on the parameters and their inference method is
actually stochastic EM (also known as Monte Carlo
EM), a ML technique in which sampling is used to
approximate the expected counts in the E-step. Even
though they report substantial reductions in align-
ment error rate, the translation BLEU scores do not
improve.
Our approach in this paper is fully Bayesian in
which the alignment probabilities are inferred by
integrating over all possible parameter values as-
suming an intuitive, sparse prior. We develop a
Gibbs sampler for alignments under IBM Model 1,
which is relevant for the state-of-the-art SMT sys-
tems since: (1) Model 1 is used in bootstrapping
the parameter settings for EM training of higher-
order alignment models, and (2) many state-of-the-
art SMT systems use Model 1 translation probabil-
ities as features in their log-linear model. We eval-
uate the inferred alignments in terms of the end-to-
end translation performance, where we show the re-
sults with a variety of input data to illustrate the gen-
eral applicability of the proposed technique. To our
knowledge, this is the first work to directly investi-
gate the effects of Bayesian alignment inference on
translation performance.
2 Bayesian Inference with IBM Model 1
Given a sentence-aligned parallel corpus (E,F), let
ei (fj) denote the i-th (j-th) source (target)1 word
in e (f ), which in turn consists of I (J) words and
denotes the s-th sentence in E (F).2 Each source
sentence is also hypothesized to have an additional
imaginary ?null? word e0. Also let VE (VF ) denote
the size of the observed source (target) vocabulary.
In Model 1 (Brown et al, 1993), each target word
1We use the ?source? and ?target? labels following the gen-
erative process, in which E generates F (cf. Eq. 1).
2Dependence of the sentence-level variables e, f , I , J (and
a and n, which are introduced later) on the sentence index s
should be understood even though not explicitly indicated for
notational simplicity.
fj is associated with a hidden alignment variable aj
whose value ranges over the word positions in the
corresponding source sentence. The set of align-
ments for a sentence (corpus) is denoted by a (A).
The model parameters consist of a VE ? VF ta-
ble T of word translation probabilities such that
te,f = P (f |e).
The joint distribution of the Model-1 variables is
given by the following generative model3:
P (E,F,A;T) =
?
s
P (e)P (a|e)P (f |a, e;T) (1)
=
?
s
P (e)
(I + 1)J
J?
j=1
teaj ,fj (2)
In the proposed Bayesian setting, we treat T as a
random variable with a prior P (T). To find a suit-
able prior for T, we re-write (2) as:
P (E,F,A|T) =
?
s
P (e)
(I + 1)J
VE?
e=1
VF?
f=1
(te,f )
ne,f (3)
=
VE?
e=1
VF?
f=1
(te,f )
Ne,f
?
s
P (e)
(I + 1)J
(4)
where in (3) the count variable ne,f denotes the
number of times the source word type e is aligned
to the target word type f in the sentence-pair s, and
in (4) Ne,f =
?
s ne,f . Since the distribution over
{te,f} in (4) is in the exponential family, specifically
being a multinomial distribution, we choose the con-
jugate prior, in this case the Dirichlet distribution,
for computational convenience.
For each source word type e, we assume the prior
distribution for te = te,1 ? ? ? te,VF , which is itself
a distribution over the target vocabulary, to be a
Dirichlet distribution (with its own set of hyperpa-
rameters ?e = ?e,1 ? ? ? ?e,VF ) independent from the
priors of other source word types:
te ? Dirichlet(te;?e)
fj |a, e,T ? Multinomial(fj ; teaj )
We choose symmetric Dirichlet priors identically
for all source words e with ?e,f = ? = 0.0001 to
obtain a sparse Dirichlet prior. A sparse prior favors
3We omit P (J |e) since both J and e are observed and so
this term does not affect the inference of hidden variables.
183
distributions that peak at a single target word and
penalizes flatter translation distributions, even for
rare words. This choice addresses the well-known
problem in the IBM Models, and more severely in
Model 1, in which rare words act as ?garbage col-
lectors? (Och and Ney, 2003) and get assigned ex-
cessively large number of word alignments.
Then we obtain the joint distribution of all (ob-
served + hidden) variables as:
P (E,F,A,T;?) = P (T;?) P (E,F,A|T) (5)
where ? = ?1 ? ? ??VE .
To infer the posterior distribution of the align-
ments, we use Gibbs sampling (Geman and Ge-
man, 1984). One possible method is to derive the
Gibbs sampler from P (E,F,A,T;?) obtained in
(5) and sample the unknowns A and T in turn, re-
sulting in an explicit Gibbs sampler. In this work,
we marginalize out T by:
P (E,F,A;?) =
?
T
P (E,F,A,T;?) (6)
and obtain a collapsed Gibbs sampler, which sam-
ples only the alignment variables.
Using P (E,F,A;?) obtained in (6), the Gibbs
sampling formula for the individual alignments is
derived as:4
P (aj = i|E,F,A?j ;?)
=
N?jei,fj + ?ei,fj
?VF
f=1N
?j
ei,f
+
?VF
f=1 ?ei,f
(7)
where the superscript ?j denotes the exclusion of
the current value of aj .
The algorithm is given in Table 1. Initialization
of A in Step 1 can be arbitrary, but for faster conver-
gence special initializations have been used, e.g., us-
ing the output of EM (Chiang et al, 2010). Once the
Gibbs sampler is deemed to have converged after B
burn-in iterations, we collect M samples of A with
L iterations in-between5 to estimate P (A|E,F). To
obtain the Viterbi alignments, which are required for
phrase extraction (Koehn et al, 2003), we select for
each aj the most frequent value in the M collected
samples.
4The derivation is quite standard and similar to other
Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik
and Hardisty, 2010).
5A lag is introduced to reduce correlation between samples.
Input: E, F; Output: K samples of A
1 Initialize A
2 for k = 1 to K do
3 for each sentence-pair s in (E,F) do
4 for j = 1 to J do
5 for i = 0 to I do
6 Calculate P (aj = i| ? ? ? )
according to (7)
7 Sample a new value for aj
Table 1: Gibbs sampling algorithm for IBM Model 1 (im-
plemented in the accompanying software).
3 Experimental Setup
For Turkish?English experiments, we used the
20K-sentence travel domain BTEC dataset (Kikui
et al, 2006) from the yearly IWSLT evaluations6
for training, the CSTAR 2003 test set for develop-
ment, and the IWSLT 2004 test set for testing7. For
Czech?English, we used the 95K-sentence news
commentary parallel corpus from the WMT shared
task8 for training, news2008 set for development,
news2009 set for testing, and the 438M-word En-
glish and 81.7M-word Czech monolingual news cor-
pora for additional language model (LM) training.
For Arabic?English, we used the 65K-sentence
LDC2004T18 (news from 2001-2004) for training,
the AFP portion of LDC2004T17 (news from 1998,
single reference) for development and testing (about
875 sentences each), and the 298M-word English
and 215M-word Arabic AFP and Xinhua subsets of
the respective Gigaword corpora (LDC2007T07 and
LDC2007T40) for additional LM training. All lan-
guage models are 4-gram in the travel domain exper-
iments and 5-gram in the news domain experiments.
For each language pair, we trained standard
phrase-based SMT systems in both directions (in-
cluding alignment symmetrization and log-linear
model tuning) using Moses (Koehn et al, 2007),
SRILM (Stolcke, 2002), and ZMERT (Zaidan,
2009) tools and evaluated using BLEU (Papineni et
al., 2002). To obtain word alignments, we used the
accompanying Perl code for Bayesian inference and
6International Workshop on Spoken Language Translation.
http://iwslt2010.fbk.eu
7Using only the first English reference for symmetry.
8Workshop on Machine Translation.
http://www.statmt.org/wmt10/translation-task.html
184
Method TE ET CE EC AE EA
EM-5 38.91 26.52 14.62 10.07 15.50 15.17
EM-80 39.19 26.47 14.95 10.69 15.66 15.02
GS-N 41.14 27.55 14.99 10.85 14.64 15.89
GS-5 40.63 27.24 15.45 10.57 16.41 15.82
GS-80 41.78 29.51 15.01 10.68 15.92 16.02
M4 39.94 27.47 15.47 11.15 16.46 15.43
Table 2: BLEU scores in translation experiments. E: En-
glish, T: Turkish, C: Czech, A: Arabic.
GIZA++ (Och and Ney, 2003) for EM.
For each translation task, we report two EM es-
timates, obtained after 5 and 80 iterations (EM-5
and EM-80), respectively; and three Gibbs sampling
estimates, two of which were initialized with those
two EM Viterbi alignments (GS-5 and GS-80) and a
third was initialized naively9 (GS-N). Sampling set-
tings were B = 400 for T?E, 4000 for C?E and
8000 for A?E; M = 100, and L = 10. For refer-
ence, we also report the results with IBM Model 4
alignments (M4) trained in the standard bootstrap-
ping regimen of 15H53343.
4 Results
Table 2 compares the BLEU scores of Bayesian in-
ference and EM estimation. In all translation tasks,
Bayesian inference outperforms EM. The improve-
ment range is from 2.59 (in Turkish-to-English)
up to 2.99 (in English-to-Turkish) BLEU points in
travel domain and from 0.16 (in English-to-Czech)
up to 0.85 (in English-to-Arabic) BLEU points in
news domain. Compared to the state-of-the-art IBM
Model 4, the Bayesian Model 1 is better in all travel
domain tasks and is comparable or better in the news
domain.
Fertility of a source word is defined as the num-
ber of target words aligned to it. Table 3 shows the
distribution of fertilities in alignments obtained from
different methods. Compared to EM estimation, in-
cluding Model 4, the proposed Bayesian inference
dramatically reduces ?questionable? high-fertility (4
? fertility? 7) alignments and almost entirely elim-
9Each target word was aligned to the source candidate that
co-occured the most number of times with that target word in
the entire parallel corpus.
Method TE ET CE EC AE EA
All 140K 183K 1.63M 1.78M 1.49M 1.82M
EM-80 5.07K 2.91K 52.9K 45.0K 69.1K 29.4K
M4 5.35K 3.10K 36.8K 36.6K 55.6K 36.5K
GS-80 755 419 14.0K 10.9K 47.6K 18.7K
EM-80 426 227 10.5K 18.6K 21.4K 24.2K
M4 81 163 2.57K 10.6K 9.85K 21.8K
GS-80 1 1 39 110 689 525
EM-80 24 24 28 30 44 46
M4 9 9 9 9 9 9
GS-80 8 8 13 18 20 19
Table 3: Distribution of inferred alignment fertilities. The
four blocks of rows from top to bottom correspond to (in
order) the total number of source tokens, source tokens
with fertilities in the range 4?7, source tokens with fertil-
ities higher than 7, and the maximum observed fertility.
The first language listed is the source in alignment (Sec-
tion 2).
Method TE ET CE EC AE EA
EM-80 52.5K 38.5K 440K 461K 383K 388K
M4 57.6K 40.5K 439K 441K 422K 405K
GS-80 23.5K 25.4K 180K 209K 158K 176K
Table 4: Sizes of bilingual dictionaries induced by differ-
ent alignment methods.
inates ?excessive? alignments (fertility ? 8)10.
The number of distinct word-pairs induced by an
alignment has been recently proposed as an objec-
tive function for word alignment (Bodrumlu et al,
2009). Small dictionary sizes are preferred over
large ones. Table 4 shows that the proposed in-
ference method substantially reduces the alignment
dictionary size, in most cases by more than 50%.
5 Conclusion
We developed a Gibbs sampling-based Bayesian in-
ference method for IBM Model 1 word alignments
and showed that it outperforms EM estimation in
terms of translation BLEU scores across several lan-
guage pairs, data sizes and domains. As a result
of this increase, Bayesian Model 1 alignments per-
form close to or better than the state-of-the-art IBM
10The GIZA++ implementation of Model 4 artificially limits
fertility parameter values to at most nine.
185
Model 4. The proposed method learns a compact,
sparse translation distribution, overcoming the well-
known ?garbage collection? problem of rare words
in EM-estimated current models.
Acknowledgments
Murat Sarac?lar is supported by the TU?BA-GEBI?P
award.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
782?790, Suntec, Singapore, August.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing,
pages 28?35, Boulder, Colorado, June. Association for
Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for finite-state transducers. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 447?455, Los Angeles, Cali-
fornia, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 718?726,
Singapore, August.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, Hawaii, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions On Pattern Analy-
sis And Machine Intelligence, 6(6):721?741, Novem-
ber.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751, Prague, Czech Republic, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 139?146, Rochester, New York, April.
Genichiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Com-
parative study on corpora for speech translation.
IEEE Transactions on Audio, Speech and Language
Processing, 14(5):1674?1682.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, Main Papers, pages 48?54,
Edmonton, May-June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04),
Main Volume, pages 518?525, Barcelona, Spain, July.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for ma-
186
chine translation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 815?823, Beijing, China, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling
for the uninitiated. University of Maryland Computer
Science Department; CS-TR-4956, June.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing, volume 3.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised Chinese
word segmentation for statistical machine translation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
1017?10124, Manchester, UK, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79?88.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden Markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 596?605, Cambridge, MA, October.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 969?976, Sydney, Australia,
July. Association for Computational Linguistics.
187
