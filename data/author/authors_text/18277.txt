Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688?1699,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Boosting Cross-Language Retrieval by Learning
Bilingual Phrase Associations from Relevance Rankings
Artem Sokolov and Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to learning bilin-
gual n-gram correspondences from relevance
rankings of English documents for Japanese
queries. We show that directly optimizing
cross-lingual rankings rivals and complements
machine translation-based cross-language in-
formation retrieval (CLIR). We propose an ef-
ficient boosting algorithm that deals with very
large cross-product spaces of word correspon-
dences. We show in an experimental evalu-
ation on patent prior art search that our ap-
proach, and in particular a consensus-based
combination of boosting and translation-based
approaches, yields substantial improvements
in CLIR performance. Our training and test
data are made publicly available.
1 Introduction
The central problem addressed in Cross-Language
Information Retrieval (CLIR) is that of translating
or projecting a query into the language of the docu-
ment repository across which retrieval is performed.
There are two main approaches to tackle this prob-
lem: The first approach leverages the standard Sta-
tistical Machine Translation (SMT) machinery to
produce a single best translation that is used as
search query in the target language. We will hence-
forth call this the direct translation approach. This
technique is particularly useful if large amounts of
data are available in domain-specific form.
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-idf
scheme. Darwish and Oard (2003) termed this
method the probabilistic structured query approach.
The advantage of this technique is an implicit query
expansion effect due to the use of probability distri-
butions over term translations (Xu et al, 2001). Re-
cent research has shown that leveraging query con-
text by extracting term translation probabilities from
n-best direct translations of queries instead of using
context-free translation tables outperforms both di-
rect translation and context-free projection (Ture et
al., 2012b; Ture et al, 2012a).
While direct translation as well as probabilistic
structured query approaches use machine learning to
optimize the SMT module, retrieval is done by stan-
dard search algorithms in both approaches. For ex-
ample, Google?s CLIR approach uses their standard
proprietary search engine (Chin et al, 2008). Ture et
al. (2012b; 2012a) use standard retrieval algorithms
such as BM25 (Robertson et al, 1998). That means,
machine learning in SMT-based approaches concen-
trates on the cross-language aspect of CLIR and is
agnostic of the ultimate ranking task.
In this paper, we present a method to project
search queries into the target language that is com-
plementary to SMT-based CLIR approaches. Our
method learns a table of n-gram correspondences by
direct optimization of a ranking objective on rele-
vance rankings of English documents for Japanese
queries. Our model is similar to the approach of
Bai et al (2010) who characterize their technique as
?Learning to rank with (a Lot of) Word Features?.
Given a set of search queries q ? IRQ and docu-
1688
ments d ? IRD, where the jth dimension of a vector
indicates the occurrence of the jth word for dictio-
naries of size Q and D, we want to learn a score
f(q,d) between a query and a given document us-
ing the model1
f(q,d) = q>Wd =
Q?
i=1
D?
j=1
qiWijdj .
We take a pairwise ranking approach to optimiza-
tion. That is, given labeled data in the form of a
set R of tuples (q,d+,d?), where d+ is a relevant
(or higher ranked) document and d? an irrelevant
(or lower ranked) document for query q, the goal
is to find a weight matrix W ? IRQ?D such that
f(q,d+) > f(q,d?) for all data tuples from R.
The scoring model learns weights for all possible
correspondences of query terms and document terms
by directly optimizing the ranking objective at hand.
Such a phrase table contains domain-specific word
associations that are useful to discern relevant from
irrelevant documents, something that is orthogonal
and complementary to standard SMT models.
The challenge of our approach can be explained
by constructing a joint feature map ? from the outer
product of the vectors q and d where
?((i?1)D+j)(q,d) = (q? d)ij = (qd
>)ij . (1)
Using this feature map, we see that the score func-
tion f can be written in the standard form of a lin-
ear model that computes the inner product between
a weight vector w and a feature vector ? where
w, ? ? IRQ?D and
f(q,d) = ?w, ?(q,d)?. (2)
While various standard algorithms exist to optimize
linear models, the difficulty lies in the memory foot-
print and capacity of the word-based model. A full-
sized model includes Q ? D parameters which is
easily in the billions even for moderately sized dic-
tionaries. Clearly, an efficient implementation and
remedies against overfitting are essential.
The main contribution of our paper is the pre-
sentation of algorithms that make learning a phrase
1With bold letters we denote vectors for query q and docu-
ment d. Vector components are denoted with normal font letters
and indices (e.g., qi).
table by direct rank optimization feasible, and an
experimental verification of the benefits of this ap-
proach, especially with regard to a combination
of the orthogonal information sources of ranking-
based and SMT-based CLIR approaches. Our ap-
proach builds upon a boosting framework for pair-
wise ranking (Freund et al, 2003) that allows the
model to grow incrementally, thus avoiding having
to deal with the full matrix W . Furthermore, we
present an implementation of boosting that utilizes
parallel estimation on bootstrap samples from the
training set for increased efficiency and reduced er-
ror (Breiman, 1996). Our ?bagged boosting? ap-
proach allows to combine incremental feature selec-
tion, parallel training, and efficient management of
large data structures.
We show in an experimental evaluation on large-
scale retrieval on patent abstracts that our boosting
approach is comparable in MAP and improves sig-
nificantly by 13-15 PRES points over very competi-
tive translation-based CLIR systems that are trained
on 1.8 million parallel sentence pairs from Japanese-
English patent documents. Moreover, a combination
of the orthogonal information learned in ranking-
based and translation-based approaches improves
over 7 MAP points and over 15 PRES points over the
respective translation-based system in a consensus-
based voting approach following the Borda Count
technique (Aslam and Montague, 2001).
2 Related Work
Recent research in CLIR follows the two main
paradigms of direct translation and probabilistic
structured query approaches. An example for the
first approach is the work of Magdy and Jones
(2011) who presented an efficient technique to adapt
off-the-shelf SMT systems for CLIR by training
them on data pre-processed for retrieval (case fold-
ing, stopword removal, stemming). Nikoulina et al
(2012) presented an approach to direct translation-
based CLIR where the n-best list of an SMT system
is re-ranked according to the MAP performance of
the translated queries. The probabilistic structured
query approach has seen a lot of work on context-
aware query expansion across languages, based on
various similarity statistics (Ballesteros and Croft,
1998; Gao et al, 2001; Lavrenko et al, 2002; Gao
1689
et al, 2007). At the time of writing this paper, the
most recent extension to this paradigm is Ture et
al. (2012a). In addition to projecting terms from
n-best translations, they propose a projection ex-
tracted from the hierarchical phrase- based grammar
models, and a scoring method based on multi-token
terms. Since the latter techniques achieved only
marginal improvements over the context-sensitive
query translation from n-best lists, we did not pur-
sue them in our work.
CLIR in the context of patent prior art search
was done as extrinsic evaluation at the NTCIR
PatentMT2 workshops until 2010, and has been on-
going in the CLEF-IP3 benchmarking workshops
since 2009. However, most workshop participants
did either not make use of automatic translation at
all, or they used an off-the-shelf translation tool.
This is due to the CLEF-IP data collection where
parts of patent documents are provided as man-
ual translations into three languages. In order to
evaluate CLIR in a truly cross-lingual scenario, we
created a large patent CLIR dataset where queries
and documents are Japanese and English patent ab-
stracts, respectively.
Ranking approaches to CLIR have been presented
by Guo and Gomes (2009) who use pairwise rank-
ing for patent retrieval. Their method is a classical
learning-to-rank setup where retrieval scores such as
tf-idf or BM25 are combined with domain knowl-
edge on patent class, inventor, date, location, etc.
into a dense feature vector of a few hundred fea-
tures. Methods to learn word-based translation cor-
respondences from supervised ranking signals have
been presented by Bai et al (2010) and Chen et
al. (2010). These approaches tackle the problem of
complexity and capacity of the cross product matrix
of word correspondences from different directions.
The first proposes to learn a low rank representa-
tion of the matrix; the second deploys sparse online
learning under `1 regularization to keep the matrix
small. Both approaches are mainly evaluated in a
monolingual setting. The cross-lingual evaluation
presented in Bai et al (2010) uses weak translation-
based baselines and non-public data such that a di-
rect comparison is not possible.
2http://research.nii.ac.jp/ntcir/ntcir/
3http://www.ifs.tuwien.ac.at/?clef-ip/
A combination of bagging and boosting in the
context of retrieval has been presented by Pavlov et
al. (2010) and Ganjisaffar et al (2011). This work
is done in a standard learning-to-rank setup using a
few hundred dense features trained on hundreds of
thousands of pairs. Our setup deals with billions of
sparse features (from the cross-product of the un-
restricted dictionaries) trained on millions of pairs
(sampled from a much larger space). Parallel boost-
ing where all feature weights are updated simultane-
ously has been presented by Collins et al (2002) and
Canini et al (2010). The first method distributes the
gradient calculation for different features among dif-
ferent compute nodes. This is not possible in our ap-
proach because we construct the cross-product ma-
trix on-the-fly. The second approach requires sub-
stantial efforts in changing the data representation
to use the MapReduce framework. Overall, one of
the goals of our work is sequential updating for im-
plicit feature selection, something that runs contrary
to parallel boosting.
3 CLIR Approaches
3.1 Direct translation approach
For direct translation, we use the SCFG decoder
cdec (Dyer et al, 2010)4 and build grammars us-
ing its implementation of the suffix array extraction
method described in Lopez (2007). Word align-
ments are built from all parallel data using mgiza5
and the Moses scripts6. SCFG models use the same
settings as described in Chiang (2007). Training
and querying of a modified Kneser-Ney smoothed 5-
gram language model are done on the English side
of the training data using KenLM (Heafield, 2011)7.
Model parameters were optimized using cdec?s im-
plementation of MERT (Och (2003)).
At retrieval time, all queries are translated
sentence-wise and subsequently re-joined to form
one query per patent. Our baseline retrieval system
uses the Okapi BM25 scores for document ranking.
4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.php/
mgiza:overview
6http://www.statmt.org/moses/?n=Moses.
SupportTools
7http://kheafield.com/code/kenlm/
estimation/
1690
3.2 Probabilistic structured query approach
Early Probabilistic Structured Query approaches
(Xu et al, 2001; Darwish and Oard, 2003) represent
translation options by lexical, i.e., token-to-token
translation tables that are estimated using standard
word alignment techniques (Och and Ney, 2000).
Later approaches (Ture et al, 2012b; Ture et al,
2012a) extract translation options from the decoder?s
n-best list for translating a particular query. The
central idea is to let the language model choose flu-
ent, context-aware translations for each query term
during decoding. This retains the desired query-
expansion effect of probabilistic structured models,
but it reduces query drift by filtering translations
with respect to the context of the full query.
A projection of source language query terms f ?
F into the target language is achieved by repre-
senting each source token f by its probabilistically
weighted translations. The score of target document
E, given source language query F , is computed by
calculating the BM25 rank over projected term fre-
quency and document frequency weights as follows:
score(E|F ) =
?
f?F
BM25(tf(f,E), df(f)) (3)
tf(f,E) =
?
e?Ef
tf(e, E)p(e|f)
df(f) =
?
e?Ef
df(e)p(e|f)
where Ef = {e ? E|p(e|f) > pL} is the set of
translation options for query term f with probability
greater than pL. We also use a cumulative threshold
pC so that only the most probable options are added
until pC is reached.
Ture et al (2012b; 2012a) achieved best retrieval
performance by interpolating between (context-free)
lexical translation probabilities plex estimated on
symmetrized word alignments, and (context-aware)
translation probabilities pnbest estimated on the n-
best list of an SMT decoder:
p(e|f) = ?pnbest(e|f) + (1? ?)plex(e|f) (4)
pnbest(e|f) is estimated by calculating expectations
of term translations from k-best translations:
pnbest(e|f) =
?n
k=1 ak(e, f)D(k, F )?n
k=1
?
e? ak(e
?, f)D(k, F )
where ak(e, f) is a function indicating an alignment
of target term e to source term f in the kth derivation
of queryF , andD(k, F ) is the model score of the kth
derivation in the n-best list for query F .
We use the same hierarchical phrase-based sys-
tem that was used for direct translation to calcu-
late n-best translations for the probabilistic struc-
tured query approach. This allows us to extract
word alignments between source and target text for
F from the SCFG rules used in the derivation. The
concept of self-translation is covered by the de-
coder?s ability to use pass-through rules if words or
phrases cannot be translated.
Probabilistic structured queries that include
context-aware estimates of translation probabilities
require a preservation of sentence-wise context-
sensitivity also in retrieval. Thus, unlike the direct
translation approach, we compute weighted term
and document frequencies for each sentence s in
query F separately. The scoring (3) of a target doc-
ument for a multiple sentence query then becomes:
score(E|F ) =
?
s in F
?
f?s
BM25(tf(f,E), df(f))
3.3 Direct Phrase Table Learning from
Relevance Rankings
Pairwise Ranking using Boosting The general
form of the RankBoost algorithm (Freund et al,
2003; Collins and Koo, 2005) defines a scoring
function f(q,d) on query q and document d as a
weighted linear combination of T weak learners ht
such that f(q,d) =
?T
t=1wtht(q,d). Weak learn-
ers can belong to an arbitrary family of functions,
but in our case they are restricted to the simplest
case of unparameterized indicator functions select-
ing components of the feature vector ?(q,d) in (1)
such that f is of the standard linear form (2). In our
experiments, these features indicate the presence of
pairs of uni- and bi-grams from the source-side vo-
cabulary of query terms and the target-side vocabu-
lary of document-terms, respectively. Furthermore,
in order to simulate the pass-through behavior of
SMT, we introduce additional features to the model
that indicate the identity of terms in source and tar-
get. All identity features have the same fixed weight
?, which is found on the development set.
For training, we are given labeled data in the form
1691
of a set R of tuples (q,d+,d?), where d+ is a rel-
evant (or higher ranked) document and d? an ir-
relevant (or lower ranked) document for query q.
RankBoost?s objective is to correctly rank query-
document pairs such that f(q,d+) > f(q,d?) for
all data tuples from R. RankBoost achieves this by
optimizing the following convex exponential loss:
Lexp =
?
(q,d+,d?)?R
D(q,d+,d?)ef(q,d
?)?f(q,d+),
where D(q,d+,d?) is a non-negative importance
function on pairs of documents for a given q.
We optimize Lexp in a greedy iterative fash-
ion, which closely follows an efficient algorithm of
Collins and Koo (2005) for the case of binary-valued
h. In each step, the single feature h is selected that
provides the largest decrease of Lexp, i.e., that has
the largest projection on the direction of the gradi-
ent ?hLexp. Because of the sequential nature of
the algorithm, RankBoost implicitly performs auto-
matic feature selection and regularization (Rosset et
al., 2004), which is crucial to reduce complexity and
capacity for our application.
Parallelization and Bagging To achieve paral-
lelization we use a variant of bagging (Breiman,
1996) on top of boosting, which has been observed
to improve performance, reduce variance and is
trivial to parallelize. The procedure is described
as part of Algorithm 1: From the set of prefer-
ence pairs R, draw S equal-sized samples with
replacement and distribute to nodes. Then, us-
ing each of the samples as a training set, sep-
arate boosting models {wst , h
s
t}, s = 1 . . . S are
trained that contain the same number of features
t = 1 . . . T . Finally the models are averaged:
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d).
Algorithm The entire training procedure is out-
lined in Algorithm 1. For each possible feature h
we maintain auxiliary variables W+h and W
?
h :
W?h =
?
(q,d+,d?):h(q,d+)?h(q,d?)=?1
D(q,d+,d?),
which are the cumulative weights of correctly and
incorrectly ranked instances by a candidate feature
h. The absolute value of ?Lexp/?h can be ex-
pressed as
?
?
?
W+h ?
?
W?h
?
? which is used as fea-
ture selection criterion (Collins and Koo, 2005).
The optimum of minimizing Lexp over w (with
fixed h) can be shown to be w = 12 ln
W+h +Z
W?h +Z
,
where  is a smoothing parameter to avoid prob-
lems with small W?h (Schapire and Singer, 1999),
and Z =
?
(q,d+,d?)?RD(q,d
+,d?). Further-
more, for each step t of the learning process, values
of D are updated to concentrate on pairs that have
not been correctly ranked so far:
Dt+1 = Dt ? e
wt
(
ht(q,d?)?ht(q,d+)
)
. (5)
Finally, to speed up learning, on iteration t we
recalculate W?h only for those h that cooccur
with previously selected ht and keep the rest un-
changed (Collins and Koo, 2005).
Algorithm 1: Bagged Boosting
Input: training tuplesR, max number of features
T , initial D0, smoothing param.  ' 10?5
Initialize:
fromR draw S samples with replacement and
distribute to nodes
Learn:
for all samples s = 1 . . . S in parallel do
calculate W+h ,W
?
h , Z on sample?s data
for all t = 1 . . . T do
choose ht = argmaxh
?
?
?
W+h ?
?
W?h
?
?
and wt = 12 ln
W+h +Z
W?h +Z
update Dt according to (5)
update W?h for all h that cooccur with ht
end
return to master {hst , w
s
t }, t = 1 . . . T
end
Bagging:
return scoring function
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d)
Implementation Because of the total number of
features (billions) there are several obstacles for the
straight-forward implementation of Algorithm 1.
First, we cannot directly access all pairs (q,d)
containing a particular feature h needed for calcu-
lating W?h . Building an inverted index is compli-
cated as it needs to fit into memory for fast fre-
1692
quent access8. We resort to the on-the-fly creation of
the cross-product space of features, following prior
work by Grangier and Bengio (2008) and Goel et al
(2008). That is, while processing a pair (q,d), we
update W?h for all h found for the pair.
Second, even if the explicit representation of all
features is avoided by on-the-fly feature construc-
tion, we still need to keep all W?h in addressable
RAM. To achieve that we use hash kernels (Shi et
al., 2009) and map original features into b-bit integer
hashes. The values W?h? for new, ?hashed?, features
h? become W?h? =
?
h:HASH(h)=h?W
?
h . We used
the MurmurHash3 function on the UTF-8 represen-
tations of features and b = 30 (resulting in more
than 1 billion distinct hashes).
4 Model Combination by Borda Counts
SMT-based approaches to CLIR and our boosting
approach have different strengths. The SMT-based
approaches produce fluent translations that are use-
ful for matching general passages written in natu-
ral language. Both baseline SMT-based approaches
presented above are agnostic of the ultimate retrieval
task and are not specifically adapted for it. The
boosting method, on the other hand, learns domain-
specific word associations that are useful to discern
relevant from irrelevant documents. In order to com-
bine these orthogonal sources of information in a
way that democratically respects each approach we
use Borda Counts, i.e., a consensus-based voting
procedure that has been successfully employed to
aggregate ranked lists of documents for metasearch
(Aslam and Montague, 2001).
We implemented a weighted version of the Borda
Count method where each voter has a fixed amount
of voting points which she is free to distribute among
the candidates to indicate the amount of preference
she is giving to each of them. In the case of retrieval,
for each q, the candidates are the scored documents
in the retrieved subset of the whole document set.
The aggregate score fagg for two rankings f1(q,d)
8It is possible to construct separate query and document in-
verted indices and intersect them on the fly to determine the
set of documents that contains some pair of words. In practice,
however, we found the overhead of set intersection during each
feature access prohibitive.
and f2(q,d) for all (q,d) in the test set is then:
fagg(q,d) = ?
f1(q,d)
?
d f1(q,d)
+(1??)
f2(q,d)
?
d f2(q,d)
.
In practice, the normalizations sum over the top K
retrieved documents. If a document is present only
in the top-K list of one system, its score is con-
sidered zero for the other system. The aggregated
scores fagg(q,d) are sorted in descending order and
top K scores are kept for evaluation.
Using the terminology proposed by Belkin et
al. (1995), combining several systems? scores with
Borda Counts can be viewed as the ?data fusion?
approach to IR, that merges outputs of the systems,
while the PSQ baseline is an example of the ?query
combination? approach that extends the query at the
input. Both techniques were earlier found to have
similar performance in CLIR tasks based on direct
translation, with a preference for the data fusion ap-
proach (Jones and Lam-Adesina, 2002).
5 Translation and Ranking Data
5.1 Parallel Translation Data
For Japanese-to-English patent translation we used
data provided by the organizers of the NTCIR9
workshop for the JP-EN PatentMT subtask. In par-
ticular, we used the data provided for NTCIR-7 (Fu-
jii et al, 2008), consisting of 1.8 million parallel
sentence pairs from the years 1993-2002 for train-
ing. For parameter tuning we used the develop-
ment set of the NTCIR-8 test collection, consisting
of 2,000 sentence pairs. The data were extracted
from the description section of patents published
by the Japanese Patent Office (JPO) and the United
States Patent and Trademark Office (USPTO) by the
method described in Utiyama and Isahara (2007).
Japanese text was segmented using the MeCab10
toolkit. Following Feng et al (2011), we applied
a modified version of the compound splitter de-
scribed in Koehn and Knight (2003) to katakana
terms, which are often transliterations of English
compound words. As these are usually not split by
MeCab, they can cause a large number of out-of-
vocabulary terms.
9http://research.nii.ac.jp/ntcir/ntcir/
10https://code.google.com/p/mecab/
1693
#queries #relevant #unique docs
train 107,061 1,422,253 888,127
dev 2,000 26,478 25,669
test 2,000 25,173 24,668
Table 1: Statistics of ranking data.
For the English side of the training data, we ap-
plied a modified version of the tokenizer included in
the Moses scripts. This tokenizer relies on a list of
non-breaking prefixes which mark expressions that
are usually followed by a ?.? (period). We cus-
tomized the list of prefixes by adding some abbrevi-
ations like ?Chem?, ?FIG? or ?Pat?, which are spe-
cific to patent documents.
5.2 Ranking Data from Patent Citations
Graf and Azzopardi (2008) describe a method to ex-
tract relevance judgements for patent retrieval from
patent citations. The key idea is to regard patent doc-
uments that are cited in a query patent, either by the
patent applicant, or by the patent examiner or in a
patent office?s search report, as relevant for the query
patent. Furthermore, patent documents that are re-
lated to the query patent via a patent family relation-
ship, i.e., patents granted by different patent author-
ities but related to the same invention, are regarded
as relevant. We assign three integer relevance levels
to these three categories of relationships, with high-
est relevance (3) for family patents, lower relevance
for patents cited in search reports by patent examin-
ers (2), and lowest relevance level (1) for applicants?
citations. We also include all patents which are in
the same patent family as an applicant or examiner
citation to avoid false negatives. This methodol-
ogy has been used to create patent retrieval data at
CLEF-IP11 and proved very useful to automatically
create a patent retrieval dataset for our experiments.
For the creation of our dataset, we used the
MAREC12 citation graph to extract patents in cita-
tion or family relation. Since the Japanese portion
of the MAREC corpus only contains English ab-
stracts, but not the Japanese full texts, we merged
the patent documents in the NTCIR-10 test collec-
tion described above with the Japanese (JP) section
11http://www.ifs.tuwien.ac.at/?clef-ip/
12http://www.ifs.tuwien.ac.at/imp/marec.
shtml
of MAREC. Title, abstract, description and claims
were added to the MAREC-JP data if the docu-
ment was available in NTCIR. In order to keep par-
allel data for SMT training separate from ranking
data, we used only data from the years 2003-2005
to extract training data for ranking, and two small
datasets of 2,000 queries each from the years 2006-
2007 for development and testing. Table 1 gives an
overview over the data used for ranking. For de-
velopment and test data, we randomly added irrele-
vant documents from the NTCIR-10 collection until
we obtained two pools of 100,000 documents. The
necessary information to reproduce the exact train,
development and test data samples is downloadable
from authors? webpage13.
The experiments reported here use only the ab-
stract of the Japanese and English patents in our
training, development and test collection.
6 Experiments
6.1 System Development
System development and evaluation in our exper-
iments was done on the ranking data described
in the previous section (see Table 1). We report
Mean Average Precision (MAP) scores, using the
trec eval (ver. 8.1) script from the TREC evalu-
ation campaign14, with a limit of top K = 1, 000 re-
trieved documents for each query. Furthermore, we
use the Patent Retrieval Evaluation Score (PRES)15
introduced by Magdy and Jones (2010). This met-
ric accounts for both precision and recall. In the
study by Magdy and Jones (2010), PRES agreed
with MAP in almost 80% of cases, and both agreed
on the ranks of the best and the worst IR system.
Both MAP and PRES scores are reported in the same
range [0, 1], and 0.01 stands for 1 MAP (PRES)
point. Statistical significance of pairwise system
comparisons was assessed using the paired random-
ization test (Noreen, 1989; Smucker et al, 2007).
For each system, optimal meta-parameter settings
were found by choosing the configuration with high-
est MAP score on the development set. These results
13http://www.cl.uni-heidelberg.de/
statnlpgroup/boostclir
14http://trec.nist.gov/trec_eval
15http://www.computing.dcu.ie/?wmagdy/
Scripts/PRESeval.htm
1694
method MAP PRESdev test dev test
1 DT 0.2636 0.2555 0.5669 0.5681
2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498
3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851
Boost-1g 0.2064 1230.1982 0.5850 120.6122
Boost-2g 0.2526 30.2474 0.6900 1230.7196
Table 2: MAP and PRES scores for CLIR methods (best
configurations) on the development and test sets. Prefixed
numbers denote statistical significance of a pairwise com-
parison with the baseline indicated by the superscript. For
example, the bottom right result shows that Boost-2g is
significantly better than DT (method 1), PSQ lexical ta-
ble (method 2) and PSQ n-best table (method 3).
(together with PRES results) are shown in the sec-
ond and fourth column of Table 2.
The direct translation approach (DT) was devel-
oped in three configurations: no stopword filtering,
small stopword list (52 words) and a large stopword
list (543 words). The last configuration achieved the
highest score (MAP 0.2636).
The probabilistic structured query (PSQ) ap-
proach was developed using the lexical translation
table and the translation table estimated on the de-
coder?s n-best list, both optionally pruned with a
variable lower pL and cumulative pC threshold on
the word pair probability in the table (Section 3.2).
A further meta-parameter of PSQ was whether to use
standard or unique n-best lists. Finally, all variants
were coupled with the same stopword filters as in
the DT approach. The configurations that achieved
the highest scores were: MAP 0.2520 for PSQ with
a lexical table (pL = 0.01, pC = 0.95, no stop-
word filtering), and MAP 0.2698 for PSQ with a
translation table estimated on the n-best list (pL =
0.005, pC = 0.95, large stopword list). Interpolat-
ing between lexical and n-best tables did not im-
prove results in our experiments, thus we set ? = 1
in equation (4).
Each SMT-based system was run with 4 different
MERT optimizations, leading to variations of less
than 1 MAP point for each system. The best con-
figurations for DT and PSQ on the development set
were fixed and used for evaluation on the test set.
Training of the boosting approach (Boost) was
done in parallel on bootstrap samples from the train-
ing data. First, a query q (i.e., a Japanese abstract)
was sampled uniformly from all training queries.
method MAP PRESdev test dev test
DT + PSQ n-best 0.2778 ?0.2726 0.5884 ?0.5942
DT + Boost-1g 0.2778 ?0.2728 0.6157 ?0.6225
DT + Boost-2g 0.3309 ?0.3300 0.7132 ?0.7279
PSQ lexical + Boost-1g 0.2695 ?0.2653 0.6068 ?0.6131
PSQ lexical + Boost-2g 0.3215 ?0.3187 0.7071 ?0.7240
PSQ n-best + Boost-1g 0.2863 ?0.2850 0.6309 ?0.6402
PSQ n-best + Boost-2g 0.3439 ?0.3416 0.7212 ?0.7376
Table 3: MAP and PRES scores of the aggregated mod-
els on the development and test sets. Development scores
correspond to peaks in Figures 1 and 3, respectively, for
MAP and PRES; test scores are given for the ??s deliv-
ering these peaks on the development set. Prefixed ? in-
dicates statistical significance of the result difference be-
tween aggregated system and the respective translation-
based system used in the aggregation.
Then we sampled independently and uniformly a
relevant document d+ (i.e., an English abstract)
from the English patents marked relevant for the
Japanese patent, and a random document d? from
the whole pool of English patent abstracts. If d?
had a relevance score greater or equal to the rele-
vance score of d+, it was resampled. The initial im-
portance weight D0 for a triplet (q,d+,d?) was set
to the positive difference in relevance scores for d+
and d?. Each bootstrap sample consisted of 10 pairs
of documents for each of 10, 000 queries, resulting
in 100, 000 training instances per sample.
The Boost approach was developed for uni-gram
and combined uni- and bi-gram versions. We ob-
served that the performance of the Boost method
continuously improved with the number of iterations
T and with the number of samples S, but saturated
at about 15-20 samples without visible over-fitting
in the tested range of T . Therefore we arbitrarily
stopped training after obtaining 5, 000 features per
sample, and used 35 samples for uni-gram version
and 65 samples for the combined bi-gram version,
resulting in models with 104K and 172K unique fea-
tures, respectively. The optimal values for the pass-
through weight ? were found to be 0.3 and 0.2 for
the uni-gram and bi-gram models on the develop-
ment set. The best configuration of uni-gram and
bi-gram model achieved MAP scores of 0.2064 and
0.2526 the development set. Using stopword filters
during training did not improve the results here.
1695
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.2636PSQ lexical, 0.2520PSQ n-best, 0.2698Boost-2g, 0.2526PSQ n-best + DT
Figure 1: MAP rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
dev, PSQ n-best + Boost-2gtest, PSQ n-best + Boost-2gdev, PSQ n-best, 0.2698test, PSQ n-best, 0.2659dev, Boost-2g, 0.2526test, Boost-2g, 0.2474
Figure 2: MAP rank aggregation for the bi-gram boosting
and the ?PSQ n-best table? approach on dev and test sets.
0.54
0.56
0.58
0.60
0.62
0.64
0.66
0.68
0.70
0.72
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
P
R
E
S
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.5669PSQ lexical, 0.5445PSQ n-best, 0.5789Boost-2g, 0.6900PSQ n-best + DT
Figure 3: PRES rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
6.2 Testing and Model Combination
The third and the fifth columns of Table 2 give a
comparison of the MAP scores of the baseline ap-
proaches and the Boost model evaluated individu-
ally on the test set. Each score corresponds to the
best configuration found on the development set. We
see that the PSQ approach using n-best lists for pro-
jection outperforms all other methods in terms of
MAP, but loses to both Boost approaches when eval-
uated with PRES. Direct translation is about 1 MAP
point lower than PSQ n-best; Boost with combined
uni- and bi-grams is another 0.8 MAP points worse,
but is better in terms of PRES, especially for the bi-
gram version. Given the fact that the complex SMT
system behind the direct translation and PSQ ap-
proach is trained and tuned on very large in-domain
datasets, the performance of the bare phrase table
induced by the Boost method is respectable.
Our best results are obtained by a combination
of the orthogonal information sources of the SMT
and the Boost approaches. We evaluated the Borda
Count aggregation scheme on the development data
in order to find the optimal value for ? ? [0, 1]. The
interpolation was done for the best combined uni-
and bi-gram boosting model with the best variants of
the DT and PSQ approaches. As can be seen from
Figures 1 and 3, rank aggregation by Borda Count
outperforms both individual approaches by a large
margin. Figure 2 verifies that the results are trans-
ferable from the development set to the test set. The
best performing system combination on the develop-
ment data is also optimal on the test data.
Table 3 shows the retrieval performance of the
best baseline model (PSQ n-best) combined with
the best Boost model (bi-gram), with an impres-
sive gain of over 7 MAP points (15 PRES points)
over the best individual baseline result from Table 2.
Even when, according to the PRES measure (Fig-
ure 3), the Boost-2g system is better on its own, in-
jecting complementary information from the PSQ or
DT approach still contributes several points. Simi-
lar gains are obtained by model combination of the
DT approach with the best Boost model. However,
a combination of the SMT-based CLIR approaches
DT and PSQ barely improved results over the best
input model. In summary, aggregating rankings is
helpful for orthogonal systems, but not for systems
including similar information.
1696
6.3 Analysis
Table 4 lists some of the top-200 selected features
for the boosting approach (the most common trans-
lation of the Japanese term is put in subscript).
We see that the direct ranking approach is able
to penalize uni- and bi-gram cooccurrences that are
harmful for retrieval by assigning them a negative
weight, e.g., the pairing of??resolution with image.
Pairs of uni- and bi-grams that are useful for re-
trieval are boosted by positive weights, e.g., the pair
??compression,?machine and compressor captures an
important compound. Further examples, not shown
in the table, are matches of the same source (tar-
get) n-gram with several different target (source) n-
grams, e.g., the Japanese term ??image is paired
not only with its main translation, but also with
dozens of related notions: video, picture, scanning,
printing, photosensitive, pixel, background etc. This
has a query expansion effect that is not possible in
systems that use one translation or a small list of n-
best translations. In addition, associations of source
n-grams with overlapping target n-grams help boost
the final score: e.g., the same term??image is pos-
itively paired with target bi-grams as {an,original},
{original,image} and {image,for}. This has the ef-
fect of compensating for the lack of handling phrase
overlaps in an SMT decoder.
7 Conclusion
We presented a boosting approach to induce a table
of bilingual n-gram correspondences by direct pref-
erence learning on relevance rankings. This table
can be seen as a phrase table that encodes word-
based information that is orthogonal and comple-
mentary to the information in standard translation-
based CLIR approaches. We compared our boosting
approach to very competitive CLIR baselines that
use a complex SMT system trained and tuned on
large in-domain datasets. Furthermore, our patent
retrieval setup gives SMT-based approaches an ad-
vantage in that queries consist of several normal-
length sentences, as opposed to the short queries
common to web search. Despite this and despite the
tiny size (about 170K parameters) of the boosting
phrase table, compared to standard SMT phrase ta-
bles, this approach reached performance similar to
direct translation using a full SMT model in terms
t ht (uni- & bi-grams) wt
1 ?layer - layer 1.29
2 ???data - data 1.13
3 ??circuit - circuit 1.13
76 ?in - voltage -0.39
77 ?guide,?power - conductive 1.25
81 ??resolution - image -0.25
99 ??speed - transmission 1.68
100 ??LCD - liquid,crystal 1.73
123 ?power - force 0.91
124 ??compression,?machine - compressor 2.83
132 ????cable - cable 1.81
133 ?hyper,??sound wave - ultrasonic 3.34
169 ??particle - particles 1.57
170 ??calculation - for,each 1.14
184 ???rotor - rotor 2.01
185 ??detection,?vessel - detector 1.43
Table 4: Examples of the features found by boosting.
of MAP, and was significantly better in terms of
PRES. Overall, we obtained the best results by a
model combination using consensus- based voting
where the best SMT-based approach was combined
with the boosting phrase table (gaining more than 7
MAP or 15 PRES points). We attribute this to the
fact that the boosting approach augments SMT ap-
proaches with valuable information that is hard to
get in approaches that are agnostic about the rank-
ing data and the ranking task at hand.
The experimental setup presented in this paper
uses relevance links between patent abstracts as
ranking data. While this technique is useful to de-
velop patent retrieval systems, it would be interest-
ing to see if our results transfer to patent retrieval
scenarios where full patent documents are used in-
stead of only abstracts, or to standard CLIR scenar-
ios that use short search queries in retrieval.
Acknowledgements
The research presented in this paper was supported
in part by DFG grant ?Cross-language Learning-to-
Rank for Patent Retrieval?. We would like to thank
Eugen Ruppert for his contribution to the ranking
data construction.
1697
References
Javed A. Aslam and Mark Montague. 2001. Models for
metasearch. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning to
rank with (a lot of) word features. Information Re-
trieval Journal, 13(3):291?314.
Lisa Ballesteros and W. Bruce Croft. 1998. Resolving
ambiguity for cross-language retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?98), Mel-
bourne, Australia.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information re-
trieval. Inf. Process. Manage., 31(3):431?448.
Leo Breiman. 1996. Bagging predictors. Journal of Ma-
chine Learning Research, 24:123?140.
Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-
den, Ken Goldman, Mike Gunter, Jeremiah Harm-
sen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, Indraneel Mukherjee, Fernando Pereira, Josh
Redstone, Tal Shaked, and Yoram Singer. 2010.
Sibyl: A system for large scale machine learning.
In LADIS: The 4th ACM SIGOPS/SIGACT Workshop
on Large Scale Distributed Systems and Middleware,
Zurich, Switzerland.
Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime
Carbonell. 2010. Learning preferences with millions
of parameters by enforcing sparsity. In Proceedings
of the IEEE International Conference on Data Mining
(ICDM?10), Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,
Jocelyn Lin, and Hui Tan. 2008. Cross-language
information retrieval. Patent Application. US
2008/0288474 A1.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, AdaBoost and Bregman
distances. Journal of Machine Learning Research,
48(1-3):253?285.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings. of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?03), Toronto,
Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Minwei Feng, Christoph Schmidt, Joern Wuebker,
Stephan Peitz, Markus Freitag, and Hermann Ney.
2011. The RWTH Aachen system for NTCIR-9
PatentMT. In Proceedings of the NTCIR-9 Workshop,
Tokyo, Japan.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933?969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of NTCIR-7 Workshop Meeting, Tokyo, Japan.
Yasser Ganjisaffar, Rich Caruana, and Cristina Videira
Lopes. 2011. Bagging gradient-boosted trees for
high precision, low variance ranking models. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?11),
Beijing, China.
Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,
Ming Zhou, and Changning Huang. 2001. Improv-
ing query translation for cross-language information
retrieval using statistical models. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?01), New Or-
leans, LA.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
lingual query suggestion using query logs of different
languages. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?07), Amsterdam, The Netherlands.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Advances
in Neural Information Processing Systems, Vancouver,
Canada.
Erik Graf and Leif Azzopardi. 2008. A methodology
for building a patent test collection for prior art search.
In Proceedings of the 2nd International Workshop on
Evaluating Information Access (EVIA), Tokyo, Japan.
David Grangier and Samy Bengio. 2008. A discrimi-
native kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 30(8):1371?1384.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
1698
patent prior art search. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT?11), Edinburgh, UK.
Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002.
Combination methods for improving the reliability of
machine translation based cross-language information
retrieval. In Proceedings of the 13th Irish Interna-
tional Conference on Artificial Intelligence and Cog-
nitive Science (AICS?02), Limerick, Ireland.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Conference on European Chapter of the Association
for Computational Linguistics (EACL?03), Budapest,
Hungary.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR?02), Tampere,
Finland.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Walid Magdy and Gareth J.F. Jones. 2010. PRES: a
score metric for evaluating recall-oriented information
retrieval applications. In Proceedings of the ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR?10), New York, NY.
Walid Magdy and Gareth J. F. Jones. 2011. An efficient
method for using machine translation technologies in
cross-language patent search. In Proceedings of the
20th ACM Conference on Informationand Knowledge
Management (CIKM?11), Glasgow, Scotland, UK.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,
and Christof Monz. 2012. Adaptation of statistical
machine translation model for cross-lingual informa-
tion retrieval in a service context. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?12),
Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hongkong, China.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting on Association for Computational Lin-
guistics (ACL?03), Sapporo, Japan.
Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.
2010. Bagboo: a scalable hybrid bagging-the-
boosting model. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management (CIKM?10), Toronto, Canada.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD.
Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boost-
ing as a regularized path to a maximum margin clas-
sifier. Journal of Machine Learning Research, 5:941?
973.
Robert E. Schapire and Yoram Singer. 1999. Im-
proved boosting algorithms using confidence-rated
predictions. Journal of Machine Learning Research,
37(3):297?336.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alexander J. Smola, Alexander L. Strehl, and
Vishy Vishwanathan. 2009. Hash Kernels. In Pro-
ceedings of the 12th Int. Conference on Artificial In-
telligence and Statistics (AISTATS?09), Irvine, CA.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of the
16th ACM conference on Conference on Information
and Knowledge Management (CIKM ?07), New York,
NY.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Combining statistical translation techniques for cross-
language information retrieval. In Proceedings of the
International Conference on Computational Linguis-
tics (COLING 2012), Bombay, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b.
Looking inside the box: Context-sensitive translation
for cross-language information retrieval. In Proceed-
ings of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR.
Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-
English patent parallel corpus. In Proceedings of MT
Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR?01), New York, NY.
1699
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323?327,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Task Alternation in Parallel Sentence Retrieval for Twitter Translation
Felix Hieber and Laura Jehl and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to mine com-
parable data for parallel sentences us-
ing translation-based cross-lingual infor-
mation retrieval (CLIR). By iteratively al-
ternating between the tasks of retrieval
and translation, an initial general-domain
model is allowed to adapt to in-domain
data. Adaptation is done by training the
translation system on a few thousand sen-
tences retrieved in the step before. Our
setup is time- and memory-efficient and of
similar quality as CLIR-based adaptation
on millions of parallel sentences.
1 Introduction
Statistical Machine Translation (SMT) crucially
relies on large amounts of bilingual data (Brown et
al., 1993). Unfortunately sentence-parallel bilin-
gual data are not always available. Various ap-
proaches have been presented to remedy this prob-
lem by mining parallel sentences from comparable
data, for example by using cross-lingual informa-
tion retrieval (CLIR) techniques to retrieve a target
language sentence for a source language sentence
treated as a query. Most such approaches try to
overcome the noise inherent in automatically ex-
tracted parallel data by sheer size. However, find-
ing good quality parallel data from noisy resources
like Twitter requires sophisticated retrieval meth-
ods. Running these methods on millions of queries
and documents can take weeks.
Our method aims to achieve improvements sim-
ilar to large-scale parallel sentence extraction ap-
proaches, while requiring only a fraction of the ex-
tracted data and considerably less computing re-
sources. Our key idea is to extend a straightfor-
ward application of translation-based CLIR to an
iterative method: Instead of attempting to retrieve
in one step as many parallel sentences as possible,
we allow the retrieval model to gradually adapt to
new data by using an SMT model trained on the
freshly retrieved sentence pairs in the translation-
based retrieval step. We alternate between the
tasks of translation-based retrieval of target sen-
tences, and the task of SMT, by re-training the
SMT model on the data that were retrieved in the
previous step. This task alternation is done itera-
tively until the number of newly added pairs stabi-
lizes at a relatively small value.
In our experiments on Arabic-English Twitter
translation, we achieved improvements of over 1
BLEU point over a strong baseline that uses in-
domain data for language modeling and parameter
tuning. Compared to a CLIR-approach which ex-
tracts more than 3 million parallel sentences from
a noisy comparable corpus, our system produces
similar results in terms of BLEU using only about
40 thousand sentences for training in each of a
few iterations, thus being much more time- and
resource-efficient.
2 Related Work
In the terminology of semi-supervised learning
(Abney, 2008), our method resembles self-training
and co-training by training a learning method on
its own predictions. It is different in the aspect of
task alternation: The SMT model trained on re-
trieved sentence pairs is not used for generating
training data, but for scoring noisy parallel data
in a translation-based retrieval setup. Our method
also incorporates aspects of transductive learning
in that candidate sentences used as queries are fil-
tered for out-of-vocabulary (OOV) words and sim-
ilarity to sentences in the development set in or-
der to maximize the impact of translation-based
retrieval.
Our work most closely resembles approaches
that make use of variants of SMT to mine com-
parable corpora for parallel sentences. Recent
work uses word-based translation (Munteanu and
323
Marcu, 2005; Munteanu and Marcu, 2006), full-
sentence translation (Abdul-Rauf and Schwenk,
2009; Uszkoreit et al, 2010), or a sophisticated
interpolation of word-based and contextual trans-
lation of full sentences (Snover et al, 2008; Jehl
et al, 2012; Ture and Lin, 2012) to project source
language sentences into the target language for re-
trieval. The novel aspect of task alternation in-
troduced in this paper can be applied to all ap-
proaches incorporating SMT for sentence retrieval
from comparable data.
For our baseline system we use in-domain lan-
guage models (Bertoldi and Federico, 2009) and
meta-parameter tuning on in-domain development
sets (Koehn and Schroeder, 2007).
3 CLIR for Parallel Sentence Retrieval
3.1 Context-Sensitive Translation for CLIR
Our CLIR model extends the translation-based re-
trieval model of Xu et al (2001). While transla-
tion options in this approach are given by a lexical
translation table, we also select translation options
estimated from the decoder?s n-best list for trans-
lating a particular query. The central idea is to let
the language model choose fluent, context-aware
translations for each query term during decoding.
For mapping source language query terms to
target language query terms, we follow Ture et
al. (2012a; 2012). Given a source language query
Q with query terms qj , we project it into the tar-
get language by representing each source token qj
by its probabilistically weighted translations. The
score of target documentD, given source language
query Q, is computed by calculating the Okapi
BM25 rank (Robertson et al, 1998) over projected
term frequency and document frequency weights
as follows:
score(D|Q) =
|Q|?
j=1
bm25(tf(qj , D), df(qj))
tf(q,D) =
|Tq|?
i=1
tf(ti, D)P (ti|q)
df(q) =
|Tq|?
i=1
df(ti)P (ti|q)
where Tq = {t|P (t|q) > L} is the set of trans-
lation options for query term q with probability
greater than L. Following Ture et al (2012a;
2012) we impose a cumulative thresholdC, so that
only the most probable options are added until C
is reached.
Like Ture et al (2012a; 2012) we achieved best
retrieval performance when translation probabil-
ities are calculated as an interpolation between
(context-free) lexical translation probabilities Plex
estimated on symmetrized word alignments, and
(context-aware) translation probabilities Pnbest es-
timated on the n-best list of an SMT decoder:
P (t|q) = ?Pnbest(t|q) + (1? ?)Plex(t|q) (1)
Pnbest(t|q) is the decoder?s confidence to trans-
late q into t within the context of query Q. Let
ak(t, q) be a function indicating an alignment of
target term t to source term q in the k-th deriva-
tion of query Q. Then we can estimate Pnbest(t|q)
as follows:
Pnbest(t|q) =
?n
k=1 ak(t, q)D(k,Q)?n
k=1 ak(?, q)D(k,Q)
(2)
D(k,Q) is the model score of the k-th derivation
in the n-best list for query Q.
In our work, we use hierarchical phrase-based
translation (Chiang, 2007), as implemented in the
cdec framework (Dyer et al, 2010). This allows
us to extract word alignments between source and
target text for Q from the SCFG rules used in the
derivation. The concept of self-translation is cov-
ered by the decoder?s ability to use pass-through
rules if words or phrases cannot be translated.
3.2 Task Alternation in CLIR
The key idea of our approach is to iteratively al-
ternate between the tasks of retrieval and trans-
lation for efficient mining of parallel sentences.
We allow the initial general-domain CLIR model
to adapt to in-domain data over multiple itera-
tions. Since our set of in-domain queries was
small (see 4.2), we trained an adapted SMT model
on the concatenation of general-domain sentences
and in-domain sentences retrieved in the step be-
fore, rather than working with separate models.
Algorithm 1 shows the iterative task alternation
procedure. In terms of semi-supervised learning,
we can view algorithm 1 as non-persistent as we
do not keep labels/pairs from previous iterations.
We have tried different variations of label persis-
tency but did not find any improvements. A sim-
ilar effect of preventing the SMT model to ?for-
get? general-domain knowledge across iterations
is achieved by mixing models from current and
previous iterations. This is accomplished in two
ways: First, by linearly interpolating the transla-
tion option weights P (t|q) from the current and
324
Algorithm 1 Task Alternation
Require: source language TweetsQsrc, target language TweetsDtrg , general-domain parallel sentences Sgen, general-domain
SMT model Mgen, interpolation parameter ?
procedure TASK-ALTERNATION(Qsrc, Dtrg, Sgen,Mgen, ?)
t? 1
while true do
Sin ? ? . Start with empty parallel in-domain sentences
if t == 1 then
M (t)clir ?Mgen . Start with general-domain SMT model for CLIRelse
M (t)clir ? ?M
(t?1)
smt + (1? ?)M (t)smt . Use mixture of previous and current SMT model for CLIRend if
Sin ? CLIR(Qsrc, Dtrg,M (t)clir) . Retrieve top 1 target language Tweets for each source language query
M (t+1)smt ? TRAIN(Sgen + Sin) . Train SMT model on general-domain and retrieved in-domain data
t? t+ 1
end while
end procedure
BLEU (test) # of in-domain sents
Standard DA 14.05 -
Full-scale CLIR 14.97 3,198,913
Task alternation 15.31 ?40k
Table 1: Standard Domain Adaptation with in-domain LM
and tuning; Full-scale CLIR yielding over 3M in-domain par-
allel sentences; Task alternation (? = 0.1, iteration 7) using
?40k parallel sentences per iteration.
previous model with interpolation parameter ?.
Second, by always using Plex(t|q) weights esti-
mated from word alignments on Sgen.
We experimented with different ways of using
the ranked retrieval results for each query and
found that taking just the highest ranked docu-
ment yielded the best results. This returns one pair
of parallel Twitter messages per query, which are
then used as additional training data for the SMT
model in each iteration.
4 Experiments
4.1 Data
We trained the general domain model Mgen on
data from the NIST evaluation campaign, includ-
ing UN reports, newswire, broadcast news and
blogs. Since we were interested in relative im-
provements rather than absolute performance, we
sampled 1 million parallel sentences Sgen from the
originally over 5.8 million parallel sentences.
We used a large corpus of Twitter messages,
originally created by Jehl et al (2012), as com-
parable in-domain data. Language identification
was carried out with an off-the-shelf tool (Lui and
Baldwin, 2012). We kept only Tweets classified
as Arabic or English with over 95% confidence.
After removing duplicates, we obtained 5.5 mil-
lion Arabic Tweets and 3.7 million English Tweets
(Dtrg). Jehl et al (2012) also supply a set of 1,022
Arabic Tweets with 3 English translations each for
evaluation purposes, which was created by crowd-
sourcing translation on Amazon Mechanical Turk.
We randomly split the parallel sentences into 511
sentences for development and 511 sentences for
testing. All URLs and user names in Tweets were
replaced by common placeholders. Hashtags were
kept, since they might be helpful in the retrieval
step. Since the evaluation data do not contain any
hashtags, URLs or user names, we apply a post-
processing step after decoding in which we re-
move those tokens.
4.2 Transductive Setup
Our method can be considered transductive in two
ways. First, all Twitter data were collected by
keyword-based crawling. Therefore, we can ex-
pect a topical similarity between development, test
and training data. Second, since our setup aims
for speed, we created a small set of queries Qsrc,
consisting of the source side of the evaluation data
and similar Tweets. Similarity was defined by
two criteria: First, we ranked all Arabic Tweets
with respect to their term overlap with the devel-
opment and test Tweets. Smoothed per-sentence
BLEU (Lin and Och, 2004) was used as a similar-
ity metric. OOV-coverage served as a second cri-
terion to remedy the problem of unknown words
in Twitter translation. We first created a general
list of all OOVs in the evaluation data under Mgen
(3,069 out of 7,641 types). For each of the top 100
BLEU-ranked Tweets, we counted OOV-coverage
with respect to the corresponding source Tweet
and the general OOV list. We only kept Tweets
325
0 1 2 3 4 5 6 7 8iteration
14.05
14.97
15.31
16.00
BLE
U (t
est)
(a)
?=0.0?=0.1?=0.5?=0.9
1 2 3 4 5 6 7 8iteration 0
10000
20000
30000
40000
50000
60000
70000
# n
ew p
airs
(b)
?=0.0?=0.1?=0.5?=0.9
Figure 1: Learning curves for varying ? parameters. (a) BLEU scores and (b) number of new pairs added per iteration.
containing at least one OOV term from the corre-
sponding source Tweet and two OOV terms from
the general list, resulting in 65,643 Arabic queries
covering 86% of all OOVs. Our query set Qsrc
performed better (14.76 BLEU) after one iteration
than a similar-sized set of random queries (13.39).
4.3 Experimental Results
We simulated the full-scale retrieval approach by
Jehl et al (2012) with the CLIR model described
in section 3. It took 14 days to run 5.5M Arabic
queries on 3.7M English documents. In contrast,
our iterative approach completed a single iteration
in less than 24 hours.1
In the absence of a Twitter data set for re-
trieval, we selected the parameters ? = 0.6 (eq.1),
L = 0.005 and C = 0.95 in a mate-finding
task on Wikipedia data. The n-best list size for
Pnbest(t|q) was 1000. All SMT models included
a 5-gram language model built from the English
side of the NIST data plus the English side of the
Twitter corpus Dtrg. Word alignments were cre-
ated using GIZA++ (Och and Ney, 2003). Rule
extraction and parameter tuning (MERT) was car-
ried out with cdec, using standard features. We
ran MERT 5 times per iteration, carrying over the
weights which achieved median performance on
the development set to the next iteration.
Table 1 reports median BLEU scores on test of
our standard adaptation baseline, the full-scale re-
trieval approach and the best result from our task
alternation systems. Approximate randomization
tests (Noreen, 1989; Riezler and Maxwell, 2005)
showed that improvements of full-scale retrieval
and task alternation over the baseline were statis-
1Retrieval was done in 4 batches on a Hadoop cluster us-
ing 190 mappers at once.
tically significant. Differences between full-scale
retrieval and task alternation were not significant.2
Figure 1 illustrates the impact of ?, which con-
trols the importance of the previous model com-
pared to the current one, on median BLEU (a) and
change of Sin (b) over iterations. For all ?, few
iterations suffice to reach or surpass full-scale re-
trieval performance. Yet, no run achieved good
performance after one iteration, showing that the
transductive setup must be combined with task al-
ternation to be effective. While we see fluctuations
in BLEU for all ?-values, ? = 0.1 achieves high
scores faster and more consistently, pointing to-
wards selecting a bolder updating strategy. This
is also supported by plot (b), which indicates that
choosing ? = 0.1 leads to faster stabilization in
the pairs added per iteration (Sin). We used this
stabilization as a stopping criterion.
5 Conclusion
We presented a method that makes translation-
based CLIR feasible for mining parallel sentences
from large amounts of comparable data. The key
of our approach is a translation-based high-quality
retrieval model which gradually adapts to the tar-
get domain by iteratively re-training the underly-
ing SMT model on a few thousand parallel sen-
tences retrieved in the step before. The number
of new pairs added per iteration stabilizes to a
few thousand after 7 iterations, yielding an SMT
model that improves 0.35 BLEU points over a
model trained on millions of retrieved pairs.
2Note that our full-scale results are not directly compara-
ble to those of Jehl et al (2012) since our setup uses less than
one fifth of the NIST data, a different decoder, a new CLIR
approach, and a different development and test split.
326
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?09), Athens, Greece.
Steven Abney. 2008. Semisupervised Learning for
Computational Linguistics. Chapman and Hall.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Transla-
tion (WMT?09), Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions (ACL?10), Uppsala, Sweden.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), Montreal, Quebec, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings the 20th In-
ternational Conference on Computational Linguis-
tics (COLING?04).
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, Demo Session
(ACL?12), Jeju, Republic of Korea.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4).
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics (COLING-ACL?06), Sydney,
Australia.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1).
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Honolulu, Hawaii.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT?12),
Montreal, Canada.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING?12), Mumbai, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Looking inside the box: Context-sensitive transla-
tion for cross-language information retrieval. In
Proceedings of the ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?12), Portland, OR.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), Beijing,
China.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?01), New York, NY.
327
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 488?494,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Translational and Knowledge-based Similarities
from Relevance Rankings for Cross-Language Retrieval
Shigehiko Schamoni and Felix Hieber and Artem Sokolov and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{schamoni,hieber,sokolov,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to cross-language
retrieval that combines dense knowledge-
based features and sparse word transla-
tions. Both feature types are learned di-
rectly from relevance rankings of bilin-
gual documents in a pairwise ranking
framework. In large-scale experiments for
patent prior art search and cross-lingual re-
trieval in Wikipedia, our approach yields
considerable improvements over learning-
to-rank with either only dense or only
sparse features, and over very competitive
baselines that combine state-of-the-art ma-
chine translation and retrieval.
1 Introduction
Cross-Language Information Retrieval (CLIR) for
the domain of web search successfully lever-
ages state-of-the-art Statistical Machine Transla-
tion (SMT) to either produce a single most prob-
able translation, or a weighted list of alternatives,
that is used as search query to a standard search
engine (Chin et al, 2008; Ture et al, 2012). This
approach is advantageous if large amounts of in-
domain sentence-parallel data are available to train
SMT systems, but relevance rankings to train re-
trieval models are not.
The situation is different for CLIR in special
domains such as patents or Wikipedia. Paral-
lel data for translation have to be extracted with
some effort from comparable or noisy parallel data
(Utiyama and Isahara, 2007; Smith et al, 2010),
however, relevance judgments are often straight-
forwardly encoded in special domains. For ex-
ample, in patent prior art search, patents granted
at any patent office worldwide are considered rel-
evant if they constitute prior art with respect to
the invention claimed in the query patent. Since
patent applicants and lawyers are required to list
relevant prior work explicitly in the patent appli-
cation, patent citations can be used to automati-
cally extract large amounts of relevance judgments
across languages (Graf and Azzopardi, 2008). In
Wikipedia search, one can imagine a Wikipedia
author trying to investigate whether a Wikipedia
article covering the subject the author intends to
write about already exists in another language.
Since authors are encouraged to avoid orphan arti-
cles and to cite their sources, Wikipedia has a rich
linking structure between related articles, which
can be exploited to create relevance links between
articles across languages (Bai et al, 2010).
Besides a rich citation structure, patent docu-
ments and Wikipedia articles contain a number
of further cues on relatedness that can be ex-
ploited as features in learning-to-rank approaches.
For monolingual patent retrieval, Guo and Gomes
(2009) and Oh et al (2013) advocate the use of
dense features encoding domain knowledge on
inventors, assignees, location and date, together
with dense similarity scores based on bag-of-word
representations of patents. Bai et al (2010) show
that for the domain of Wikipedia, learning a sparse
matrix of word associations between the query and
document vocabularies from relevance rankings is
useful in monolingual and cross-lingual retrieval.
Sokolov et al (2013) apply the idea of learning
a sparse matrix of bilingual phrase associations
from relevance rankings to cross-lingual retrieval
in the patent domain. Both show improvements
of learning-to-rank on relevance data over SMT-
based approaches on their respective domains.
The main contribution of this paper is a thor-
ough evaluation of dense and sparse features
for learning-to-rank that have so far been used
only monolingually or only on either patents or
Wikipedia. We show that for both domains,
patents and Wikipedia, jointly learning bilingual
sparse word associations and dense knowledge-
based similarities directly on relevance ranked
488
data improves significantly over approaches that
use either only sparse or only dense features, and
over approaches that combine query translation
by SMT with standard retrieval in the target lan-
guage. Furthermore, we show that our approach
can be seen as supervised model combination
that allows to combine SMT-based and ranking-
based approaches for further substantial improve-
ments. We conjecture that the gains are due to
orthogonal information contributed by domain-
knowledge, ranking-based word associations, and
translation-based information.
2 Related Work
CLIR addresses the problem of translating or pro-
jecting a query into the language of the document
repository across which retrieval is performed. In
a direct translation approach (DT), a state-of-the-
art SMT system is used to produce a single best
translation that is used as search query in the target
language. For example, Google?s CLIR approach
combines their state-of-the-art SMT system with
their proprietary search engine (Chin et al, 2008).
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-
idf scheme. Darwish and Oard (2003) termed
this method the probabilistic structured query ap-
proach (PSQ). The advantage of this technique
is an implicit query expansion effect due to the
use of probability distributions over term trans-
lations (Xu et al, 2001). Ture et al (2012)
brought SMT back into this paradigm by pro-
jecting terms from n-best translations from syn-
chronous context-free grammars.
Ranking approaches have been presented by
Guo and Gomes (2009) and Oh et al (2013).
Their method is a classical learning-to-rank setup
where pairwise ranking is applied to a few hun-
dred dense features. Methods to learn sparse
word-based translation correspondences from su-
pervised ranking signals have been presented by
Bai et al (2010) and Sokolov et al (2013). Both
approaches work in a cross-lingual setting, the for-
mer on Wikipedia data, the latter on patents.
Our approach extends the work of Sokolov et
al. (2013) by presenting an alternative learning-
to-rank approach that can be used for supervised
model combination to integrate dense and sparse
features, and by evaluating both approaches on
cross-lingual retrieval for patents and Wikipedia.
This relates our work to supervised model merg-
ing approaches (Sheldon et al, 2011).
3 Translation and Ranking for CLIR
SMT-based Models. We will refer to DT and
PSQ as SMT-based models that translate a query,
and then perform monolingual retrieval using
BM25. Translation is agnostic of the retrieval task.
Linear Ranking for Word-Based Models. Let
q ? {0, 1}
Q
be a query and d ? {0, 1}
D
be a doc-
ument where the j
th
vector dimension indicates the
occurrence of the j
th
word for dictionaries of size
Q and D. A linear ranking model is defined as
f(q,d) = q
>
Wd =
Q
?
i=1
D
?
j=1
q
i
W
ij
d
j
,
where W ? IR
Q?D
encodes a matrix of ranking-
specific word associations (Bai et al, 2010) . We
optimize this model by pairwise ranking, which
assumes labeled data in the form of a set R of tu-
ples (q,d
+
,d
?
), where d
+
is a relevant (or higher
ranked) document and d
?
an irrelevant (or lower
ranked) document for query q. The goal is to
find a weight matrix W such that an inequality
f(q,d
+
) > f(q,d
?
) is violated for the fewest
number of tuples from R. We present two meth-
ods for optimizing W in the following.
Pairwise Ranking using Boosting (BM). The
Boosting-based Ranking baseline (Freund et al,
2003) optimizes an exponential loss:
L
exp
=
?
(q,d
+
,d
?
)?R
D(q,d
+
,d
?
)e
f(q,d
?
)?f(q,d
+
)
,
whereD(q,d
+
,d
?
) is a non-negative importance
function on tuples. The algorithm of Sokolov et
al. (2013) combines batch boosting with bagging
over a number of independently drawn bootstrap
data samples fromR. In each step, the single word
pair feature is selected that provides the largest de-
crease of L
exp
. The found corresponding models
are averaged. To reduce memory requirements we
used random feature hashing with the size of the
hash of 30 bits (Shi et al, 2009). For regulariza-
tion we rely on early stopping.
Pairwise Ranking with SGD (VW). The sec-
ond objective is an `
1
-regularized hinge loss:
L
hng
=
?
(q,d
+
,d
?
)?R
(
f(q,d
+
)? f(q,d
?
)
)
+
+ ?||W ||
1
,
489
where (x)
+
= max(0, 1 ? x) and ? is the regu-
larization parameter. This newly added model uti-
lizes the standard implementation of online SGD
from the Vowpal Wabbit (VW) toolkit (Goel et al,
2008) and was run on a data sample of 5M to 10M
tuples from R. On each step, W is updated with
a scaled gradient vector ?
W
L
hng
and clipped to
account for `
1
-regularization. Memory usage was
reduced using the same hashing technique as for
boosting.
Domain Knowledge Models. Domain knowl-
edge features for patents were inspired by Guo
and Gomes (2009): a feature fires if two patents
share similar aspects, e.g. a common inventor. As
we do not have access to address data, we omit
geolocation features and instead add features that
evaluate similarity w.r.t. patent classes extracted
from IPC codes. Documents within a patent sec-
tion, i.e. the topmost hierarchy, are too diverse
to provide useful information but more detailed
classes and the count of matching classes do.
For Wikipedia, we implemented features that
compare the relative length of documents, num-
ber of links and images, the number of common
links and common images, and Wikipedia cat-
egories: Given the categories associated with a
foreign query, we use the language links on the
Wikipedia category pages to generate a set of
?translated? English categories S. The English-
side category graph is used to construct sets of
super- and subcategories related to the candidate
document?s categories. This expansion is done in
both directions for two levels resulting in 5 cat-
egory sets. The intersection between target set
T
n
and the source category set S reflects the cat-
egory level similarity between query and docu-
ment, which we calculate as a mutual containment
score s
n
=
1
2
(|S ? T
n
|/|S| + |S ? T
n
|/|T
n
|) for
n ? {?2,?1, 0,+1,+2} (Broder, 1997).
Optimization for these additional models in-
cluding domain knowledge features was done by
overloading the vector representation of queries q
and documents d in the VW linear learner: Instead
of sparse word-based features, q and d are rep-
resented by real-valued vectors of dense domain-
knowledge features. Optimization for the over-
loaded vectors is done as described above for VW.
4 Model Combination
Combination by Borda Counts. The baseline
consensus-based voting Borda Count procedure
endows each voter with a fixed amount of voting
points which he is free to distribute among the
scored documents (Aslam and Montague, 2001;
Sokolov et al, 2013). The aggregate score for
two rankings f
1
(q,d) and f
2
(q,d) for all (q,d)
in the test set is then a simple linear interpolation:
f
agg
(q,d) = ?
f
1
(q,d)?
d
f
1
(q,d)
+(1??)
f
2
(q,d)?
d
f
2
(q,d)
. Pa-
rameter ? was adjusted on the dev set.
Combination by Linear Learning. In order to
acquire the best combination of more than two
models, we created vectors of model scores along
with domain knowledge features and reused the
VW pairwise ranking approach. This means
that the vector representation of queries q and
documents d in the VW linear learner is over-
loaded once more: In addition to dense domain-
knowledge features, we incorporate arbitrary
ranking models as dense features whose value is
the score of the ranking model. Training data was
sampled from the dev set and processed with VW.
5 Data
Patent Prior Art Search (JP-EN). We use
BoostCLIR
1
, a Japanese-English (JP-EN) corpus
of patent abstracts from the MAREC and NTCIR
data (Sokolov et al, 2013). It contains automati-
cally induced relevance judgments for patent ab-
stracts (Graf and Azzopardi, 2008): EN patents
are regarded as relevant with level (3) to a JP query
patent, if they are in a family relationship (e.g.,
same invention), cited by the patent examiner (2),
or cited by the applicant (1). Statistics on the rank-
ing data are given in Table 1. On average, queries
and documents contain about 5 sentences.
Wikipedia Article Retrieval (DE-EN). The in-
tuition behind our Wikipedia retrieval setup is as
follows: Consider the situation where the German
(DE) Wikipedia article on geological sea stacks
does not yet exist. A native speaker of Ger-
man with profound knowledge in geology intends
to write it, naming it ?Brandungspfeiler?, while
seeking to align its structure with the EN counter-
part. The task of a CLIR engine is to return rele-
vant EN Wikipedia articles that may describe the
very same concept (Stack (geology)), or relevant
instances of it (Bako National Park, Lange Anna).
The information need may be paraphrased as a
high-level definition of the topic. Since typically
the first sentence of any Wikipedia article is such
1
www.cl.uni-heidelberg.de/boostclir
490
#q #d #d
+
/q #words/q
Patents (JP-EN)
train 107,061 888,127 13.28 178.74
dev 2,000 100,000 13.24 181.70
test 2,000 100,000 12.59 182.39
Wikipedia (DE-EN)
train 225,294 1,226,741 13.04 25.80
dev 10,000 113,553 12.97 25.75
test 10,000 115,131 13.22 25.73
Table 1: Ranking data statistics: number of queries and doc-
uments, avg. number of relevant documents per query, avg.
number of words per query.
a well-formed definition, this allows us to extract
a large set of one sentence queries from Wikipedia
articles. For example: ?Brandungspfeiler sind vor
einer Kliffk?uste aufragende Felsent?urme und ver-
gleichbare Formationen, die durch Brandungsero-
sion gebildet werden.?
2
Similar to Bai et al (2010)
we induce relevance judgments by aligning DE
queries with their EN counterparts (?mates?) via
the graph of inter-language links available in arti-
cles and Wikidata
3
. We assign relevance level (3)
to the EN mate and level (2) to all other EN ar-
ticles that link to the mate, and are linked by the
mate. Instead of using all outgoing links from the
mate, we only use articles with bidirectional links.
To create this data
4
we downloaded XML and
SQL dumps of the DE and EN Wikipedia from,
resp., 22
nd
and 4
th
of November 2013. Wikipedia
markup removal and link extraction was carried
out using the Cloud9 toolkit
5
. Sentence extrac-
tion was done with NLTK
6
. Since Wikipedia arti-
cles vary greatly in length, we restricted EN doc-
uments to the first 200 words after extracting the
link graph to reduce the number of features for BM
and VW models. To avoid rendering the task too
easy for literal keyword matching of queries about
named entities, we removed title words from the
German queries. Statistics are given in Table 1.
Preprocessing Ranking Data. In addition to
lowercasing and punctuation removal, we applied
Correlated Feature Hashing (CFH), that makes
collisions more likely for words with close mean-
ing (Bai et al, 2010). For patents, vocabularies
contained 60k and 365k words for JP and EN.
Filtering special symbols and stopwords reduced
the JP vocabulary size to 50k (small enough not
to resort to CFH). To reduce the EN vocabulary
2
de.wikipedia.org/wiki/Brandungspfeiler
3
www.wikidata.org/
4
www.cl.uni-heidelberg.de/wikiclir
5
lintool.github.io/Cloud9/index.html
6
www.nltk.org/
to a comparable size, we applied similar prepro-
cessing and CFH with F=30k and k=5. Since for
Wikipedia data, the DE and EN vocabularies were
both large (6.7M and 6M), we used the same filter-
ing and preprocessing as for the patent data before
applying CFH with F=40k and k=5 on both sides.
Parallel Data for SMT-based CLIR. For both
tasks, DT and PSQ require an SMT baseline
system trained on parallel corpora that are dis-
junct from the ranking data. A JP-EN sys-
tem was trained on data described and prepro-
cessed by Sokolov et al (2013), consisting of
1.8M parallel sentences from the NTCIR-7 JP-EN
PatentMT subtask (Fujii et al, 2008) and 2k par-
allel sentences for parameter development from
the NTCIR-8 test collection. For Wikipedia, we
trained a DE-EN system on 4.1M parallel sen-
tences from Europarl, Common Crawl, and News-
Commentary. Parameter tuning was done on 3k
parallel sentences from the WMT?11 test set.
6 Experiments
Experiment Settings. The SMT-based models
use cdec (Dyer et al, 2010). Word align-
ments were created with mgiza (JP-EN) and
fast align (Dyer et al, 2013) (DE-EN). Lan-
guage models were trained with the KenLM
toolkit (Heafield, 2011). The JP-EN system uses
a 5-gram language model from the EN side of the
training data. For the DE-EN system, a 4-gram
model was built on the EN side of the training
data and the EN Wikipedia documents. Weights
for the standard feature set were optimized using
cdec?s MERT (JP-EN) and MIRA (DE-EN) im-
plementations (Och, 2003; Chiang et al, 2008).
PSQ on patents reuses settings found by Sokolov
et al (2013); settings for Wikipedia were adjusted
on its dev set (n=1000, ?=0.4, L=0, C=1).
Patent retrieval for DT was done by sentence-
wise translation and subsequent re-joining to form
one query per patent, which was ranked against the
documents using BM25. For PSQ, BM25 is com-
puted on expected term and document frequencies.
For ranking-based retrieval, we compare several
combinations of learners and features (Table 2).
VW denotes a sparse model using word-based fea-
tures trained with SGD. BM denotes a similar
model trained using Boosting. DK denotes VW
training of a model that represents queries q and
documents d by dense domain-knowledge fea-
tures instead of by sparse word-based vectors. In
491
order to simulate pass-through behavior of out-of-
vocabulary terms in SMT systems, additional fea-
tures accounting for source and target term iden-
tity were added to DK and BM models. The pa-
rameter ? for VW was found on dev set. Statis-
tical significance testing was performed using the
paired randomization test (Smucker et al, 2007).
Borda denotes model combination by Borda
Count voting where the linear interpolation pa-
rameter is adjusted for MAP on the respective de-
velopment sets with grid search. This type of
model combination only allows to combine pairs
of rankings. We present a combination of SMT-
based CLIR, DT+PSQ, a combination of dense
and sparse features, DK+VW, and a combination
of both combinations, (DT+PSQ)+(DK+VW).
LinLearn denotes model combination by over-
loading the vector representation of queries q and
documents d in the VW linear learner by incor-
porating arbitrary ranking models as dense fea-
tures. In difference to grid search for Borda, opti-
mal weights for the linear combination of incorpo-
rated ranking models can be learned automatically.
We investigate the same combinations of rank-
ing models as described for Borda above. We do
not report combination results including the sparse
BM model since they were consistently lower than
the ones with the sparse VW model.
Test Results. Experimental results on test data
are given in Table 2. Results are reported
with respect to MAP (Manning et al, 2008),
NDCG (J?arvelin and Kek?al?ainen, 2002), and
PRES (Magdy and Jones, 2010). Scores were
computed on the top 1,000 retrieved documents.
As can be seen from inspecting the two blocks
of results, one for patents, one for Wikipedia, we
find the same system rankings on both datasets. In
both cases, as standalone systems, DT and PSQ
are very close and far better than any ranking ap-
proach, irrespective of the objective function or the
choice of sparse or dense features. Model combi-
nation of similar models, e.g., DT and PSQ, gives
minimal gains, compared to combining orthogo-
nal models, e.g. DK and VW. The best result is
achieved by combining DT and PSQ with DK and
VW. This is due to the already high scores of the
combined models, but also to the combination of
yet other types of orthogonal information. Borda
voting gives the best result under MAP which is
probably due to the adjustment of the interpola-
tion parameter for MAP on the development set.
combination models MAP NDCG PRES
P
a
t
e
n
t
s
(
J
P
-
E
N
)
s
t
a
n
d
a
l
o
n
e
DT 0.2554 0.5397 0.5680
PSQ 0.2659 0.5508 0.5851
DK 0.2203 0.4874 0.5171
VW 0.2205 0.4989 0.4911
BM 0.1669 0.4167 0.4665
B
o
r
d
a
DT+PSQ
?
0.2747
?
0.5618
?
0.5988
DK+VW
?
0.3023
?
0.5980
?
0.6137
(DT+PSQ)+(DK+VW)
?
0.3465
?
0.6420
?
0.6858
L
i
n
L
e
a
r
n
DT+PSQ
??
0.2707
??
0.5578
??
0.5941
DK+VW
??
0.3283
??
0.6366
??
0.7104
DT+PSQ+DK+VW
??
0.3739
??
0.6755
??
0.7599
W
i
k
i
p
e
d
i
a
(
D
E
-
E
N
)
s
t
a
n
d
a
l
o
n
e
DT 0.3678 0.5691 0.7219
PSQ 0.3642 0.5671 0.7165
DK 0.2661 0.4584 0.6717
VW 0.1249 0.3389 0.6466
BM 0.1386 0.3418 0.6145
B
o
r
d
a
DT+PSQ
?
0.3742
?
0.5777
?
0.7306
DK+VW
?
0.3238
?
0.5484
?
0.7736
(DT+PSQ)+(DK+VW)
?
0.4173
?
0.6333
?
0.8031
L
i
n
L
e
a
r
n
DT+PSQ
??
0.3718
??
0.5751
??
0.7251
DK+VW
??
0.3436
??
0.5686
??
0.7914
DT+PSQ+DK+VW
?
0.4137
??
0.6435
??
0.8233
Table 2: Test results for standalone CLIR models using di-
rect translation (DT), probabilistic structured queries (PSQ),
sparse model with CFH (VW), sparse boosting model (BM),
dense domain knowledge features (DK), and model combi-
nations using Borda Count voting (Borda) or linear super-
vised model combination (LinLearn). Significant differences
(at p=0.01) between aggregated systems and all its compo-
nents are indicated by ?, between LinLearn and the respective
Borda system by ?.
Under NDCG and PRES, LinLearn achieves the
best results, showing the advantage of automati-
cally learning combination weights that leads to
stable results across various metrics.
7 Conclusion
Special domains such as patents or Wikipedia of-
fer the possibility to extract cross-lingual rele-
vance data from citation and link graphs. These
data can be used to directly optimizing cross-
lingual ranking models. We showed on two differ-
ent large-scale ranking scenarios that a supervised
combination of orthogonal information sources
such as domain-knowledge, translation knowl-
edge, and ranking-specific word associations by
far outperforms a pipeline of query translation and
retrieval. We conjecture that if these types of in-
formation sources are available, a supervised rank-
ing approach will yield superior results in other re-
trieval scenarios as well.
Acknowledgments
This research was supported in part by DFG
grant RI-2221/1-1 ?Cross-language Learning-to-
Rank for Patent Retrieval?.
492
References
Javed A. Aslam and Mark Montague. 2001. Models
for metasearch. In Proceedings of the ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR?01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning
to rank with (a lot of) word features. Information
Retrieval Journal, 13(3):291?314.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In Compression and Com-
plexity of Sequences (SEQUENCES?97), pages 21?
29. IEEE Computer Society.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?08), Waikiki, Hawaii.
Jeffrey Chin, Maureen Heymans, Alexandre Ko-
joukhov, Jocelyn Lin, and Hui Tan. 2008. Cross-
language information retrieval. Patent Application.
US 2008/0288474 A1.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings.
of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?03),
Toronto, Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, Uppsala, Sweden.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Atlanta, GA.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learn-
ing Research, 4:933?969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Pro-
ceedings of NTCIR-7 Workshop Meeting, Tokyo,
Japan.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Ad-
vances in Neural Information Processing Systems,
Vancouver, Canada.
Erik Graf and Leif Azzopardi. 2008. A methodol-
ogy for building a patent test collection for prior
art search. In Proceedings of the 2nd Interna-
tional Workshop on Evaluating Information Access
(EVIA?08), Tokyo, Japan.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
patent prior art search. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJCAI?09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation (WMT?11), Edinburgh, UK.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions in Information Systems, 20(4):422?
446.
Walid Magdy and Gareth J.F. Jones. 2010. PRES:
a score metric for evaluating recall-oriented infor-
mation retrieval applications. In Proceedings of the
ACM SIGIR conference on Research and develop-
ment in information retrieval (SIGIR?10), New York,
NY.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting on Association for Computational
Linguistics (ACL?03), Sapporo, Japan.
Sooyoung Oh, Zhen Lei, Wang-Chien Lee, Prasenjit
Mitra, and John Yen. 2013. CV-PCR: A context-
guided value-driven framework for patent citation
recommendation. In Proceedings of the Interna-
tional Conference on Information and Knowledge
Management (CIKM?13), San Francisco, CA.
Daniel Sheldon, Milad Shokouhi, Martin Szummer,
and Nick Craswell. 2011. Lambdamerge: Merging
the results of query reformulations. In Proceedings
of WSDM?11, Hong Kong, China.
Qinfeng Shi, James Petterson, Gideon Dror, John
Langford, Alexander J. Smola, Alexander L. Strehl,
and Vishy Vishwanathan. 2009. Hash Kernels. In
Proceedings of the 12th Int. Conference on Artifi-
cial Intelligence and Statistics (AISTATS?09), Irvine,
CA.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT?10), Los Angeles, CA.
493
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the 16th ACM conference on Conference on Infor-
mation and Knowledge Management (CIKM ?07),
New York, NY.
Artem Sokolov, Laura Jehl, Felix Hieber, and Stefan
Riezler. 2013. Boosting cross-language retrieval
by learning bilingual phrase associations from rele-
vance rankings. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP?13).
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING?12), Bombay, India.
Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR?01), New York, NY.
494
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 410?421,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Twitter Translation using Translation-Based Cross-Lingual Retrieval
Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
Microblogging services such as Twitter have
become popular media for real-time user-
created news reporting. Such communica-
tion often happens in parallel in different lan-
guages, e.g., microblog posts related to the
same events of the Arab spring were written
in Arabic and in English. The goal of this
paper is to exploit this parallelism in order
to eliminate the main bottleneck in automatic
Twitter translation, namely the lack of bilin-
gual sentence pairs for training SMT systems.
We show that translation-based cross-lingual
information retrieval can retrieve microblog
messages across languages that are similar
enough to be used to train a standard phrase-
based SMT pipeline. Our method outper-
forms other approaches to domain adaptation
for SMT such as language model adaptation,
meta-parameter tuning, or self-translation.
1 Introduction
Among the various social media platforms, mi-
croblogging services such as Twitter1 have become
popular communication tools. This is due to the easy
accessibility of microblogging platforms via inter-
net or mobile phones, and due to the need for a fast
mode of communication that microblogging satis-
fies: Twitter messages are short (limited to 140 char-
acters) and simultaneous (due to frequent updates by
prolific microbloggers). Twitter users form a social
network by ?following? the updates of other users,
either reciprocal or one-way. The topics discussed
in Twitter messages range from private chatter to im-
portant real-time witness reports.
1http://twitter.com/
Events such as the Arab spring have shown the
power and also the shortcomings of this new mode
of communication. Microblogging services played a
crucial role in quickly spreading the news about im-
portant events, furthermore they were useful in help-
ing organizers plan their protest. The fact that news
on microblogging platforms is sometimes ahead of
newswire is one of the most interesting facets of
this new medium. However, while Twitter messag-
ing is happening in multiple languages, most net-
works of ?friends? and ?followers? are monolingual
and only about 40% of all messages are in English2.
One solution to sharing news quickly and interna-
tionally was crowdsourcing manual translations, for
example at Meedan3, a nonprofit organization built
to share news and opinion between the Arabic and
English speaking world, by translating articles and
blogs, using machine translation and human expert
corrections.
The goal of our research is to automate this trans-
lation process, with a further aim of providing rapid
crosslingual data access for downstream applica-
tions. The automated translation of microblogging
messages is facing two main problems. First, there
are no bilingual sentence pair data from microblog-
ging domains available. Second, the colloquial, non-
standard language of many microblogging messages
makes it very difficult to adapt a machine translation
system trained on any of the available bilingual re-
sources such as transcriptions from political organi-
zations or news text.
The approach presented in this paper aims to ex-
ploit the fact that microblogging often happens in
2http://semiocast.com/publications/2011_
11_24_Arabic_highest_growth_on_Twitter
3http://news.meedan.net
410
parallel in different languages, e.g., microblog posts
related to the same events of the Arab spring were
published in parallel in Arabic and in English. The
central idea is to crawl a large set of topically related
Arabic and English microblogging messages, and
use Arabic microblog messages as search queries in
a cross-lingual information retrieval (CLIR) setup.
We use the probabilistic translation-based retrieval
technique of Xu et al (2001) that naturally inte-
grates translation tables for cross-lingual retrieval.
The retrieval results are then used as input to a stan-
dard SMT pipeline to train translation models, start-
ing from unsupervised induction of word alignments
(Och and Ney, 2000) to phrase-extraction (Och and
Ney, 2004) and phrase-based decoding (Koehn et al,
2007). We investigate several filtering techniques
for retrieval and phrase extraction (Munteanu and
Marcu, 2006; Snover et al, 2008) and find a straight-
forward application of phrase extraction from sym-
metrized alignments to be optimal. Furthermore, we
compare our approach to related domain adaptation
techniques for SMT and find our approach to yield
large improvements over all related techniques.
Finally, a side-product of our research is a cor-
pus of around 1,000 Arabic Twitter messages with
3 manual English translations each, which were cre-
ated using crowdsourcing techniques. This corpus
is used for development and testing in our experi-
ments.
2 Related Work
SMT for user-generated noisy data has been pio-
neered at the 2011 Workshop on Statistical Ma-
chine Translation that featured a translation task of
Haitian Creole emergency SMS messages4. This
task is very similar to the problem of Twitter transla-
tion since SMS contain noisy, abbreviated language.
The research papers related to the featured transla-
tion task deploy several approaches to domain adap-
tation, including crowdsourcing (Hu et al, 2011)
or extraction of parallel sentences from comparable
data (Hewavitharana et al, 2011).
The use of crowdsourcing to evaluate machine
translation and to build development sets was pi-
oneered by Callison-Burch (2009) and Zaidan and
4http://www.statmt.org/wmt11/
featured-translation-task.html
Callison-Burch (2009). Crowdsourcing has its lim-
its when it comes to generating parallel training data
on the scale of millions of parallel sentences. In
our work, we use crowdsourcing via Amazon Me-
chanical Turk5 to create a development and test cor-
pus that includes 3 English translations for each of
around 1,000 Arabic microblog messages.
There is a substantial amount of previous work on
extracting parallel sentences from comparable data
such as newswire text (Fung and Cheung, 2004;
Munteanu and Marcu, 2005; Tillmann and ming Xu,
2009) and on finding parallel phrases in non-parallel
sentences (Munteanu and Marcu, 2006; Quirk et al,
2007; Cettolo et al, 2010; Vogel and Hewavitha-
rana, 2011). The approach that is closest to our
work is that of Munteanu and Marcu (2006): They
use standard information retrieval together with sim-
ple word-based translation for CLIR, and extract
phrases from the retrieval results using a clean bilin-
gual lexicon and an averaging filter. In this ap-
proach, filtering and cleaning techniques in align-
ment and phrase extraction have to compensate for
low-quality retrieval results. In our approach, the fo-
cus is on high-quality retrieval.
As our experimental results show, the main im-
provement of our technique is a decrease in out-of-
vocabulary (OOV) rate at an increase of the per-
centage of correctly translated unigrams and bi-
grams. Similar work on solving domain adaptation
for SMT by mining unseen words has been pre-
sented by Snover et al (2008) and Daum? and Ja-
garlamudi (2011). Both approaches show improve-
ments by adding new phrase tables; however, both
approaches rely on techniques that require larger
comparable texts for mining unseen words. Since
in our case documents are very short (they consist
of 140 character sequences), these techniques are
not applicable. However, the advantage of the fact
that microblog messages resemble sentences is that
we can apply standard word- and phrase-alignment
techniques directly to the retrieval results.
Further approaches to domain adaptation for SMT
include adaptation using in-domain language mod-
els (Bertoldi and Federico, 2009), meta-parameter
tuning on in-domain development sets (Koehn and
Schroeder, 2007), or translation model adaptation
5http://www.turk.com
411
using self-translations of in-domain source language
texts (Ueffing et al, 2007). In our experiments we
compare our approach to these domain adaptation
techniques.
3 Cross-Lingual Retrieval via Statistical
Translation
3.1 Retrieval Model
In our approach, comparable candidates for domain
adaptation are selected via cross-lingual retrieval.
In a probabilistic retrieval framework, we estimate
the probability of a relevant document microblog
message D given a query microblog message Q,
P (D|Q). Following Bayes rule, this can be sim-
plified to ranking documents according to the like-
lihood P (Q|D) if we assume a uniform prior over
documents.
score(Q,D) = P (D|Q) = P (D)P (Q|D)P (Q) (1)
Our model is defined as follows:
score(Q,D) = P (Q|D) =
?
q?Q
P (q|D) (2)
P (q|D) = ?Pmix(q|D)
? ?? ?
mixture model
+(1? ?) PML(q|C)
? ?? ?
query collection backoff
(3)
Pmix(q|D) = ?
?
d?D
T (q|d)PML(d|D)
? ?? ?
translation model
(4)
+(1? ?)PML(q|D)
? ?? ?
self-translation
Our retrieval model is related to monolingual re-
trieval models such as the language-modeling ap-
proach of Ponte and Croft (1998) and the monolin-
gual statistical translation approach of Berger and
Lafferty (1999). Xu et al (2001) extend the former
approaches to the cross-lingual setting by adding a
term translation table. They describe their model in
terms of a Hidden Markov Model with two states
that generate query terms: First, a document state
generates terms d in the document language and then
translates them into a query term q. Second, a back-
off state generates query terms q directly in the query
language. In the document state the probability of
emitting q depends on all d that translate to q, ac-
cording to a translation distribution T . This is esti-
mated by marginalizing out d as
?
d T (q|d)P (d|D).
In the backoff state the probability PML(q|C) of
emitting a query term is estimated as the relative
frequency of this term within a corpus in the query
language. The probability of transitioning into the
document state or the backoff state is given by ? and
1? ?.
We view this model from a smoothing perspective
where the backoff state is linearly interpolated with
the translation probability using a mixture weight
? to control the weighting between both terms.
Furthermore, we expand Xu et al (2001)?s gen-
erative model to incorporate the concept of ?self-
translation?, introduced by Xue et al (2008) in a
monolingual question-answering context: Twitter
messages across languages usually share relevant
terms such as hashtags, named entities or user men-
tions. Therefore, we model the event of a query
term literally occurring in the document in a sepa-
rate model that is itself linearly interpolated with a
parameter ? with the translation model.
We implemented the model based on a Lucene6
index, which allows efficient storage of term-
document and document-term vectors. To mini-
mize retrieval time, we consider only those doc-
uments as retrieval candidates where at least one
term translates to a query term, according to the
translation table T . Stopwords were removed for
both queries and documents. Compared to com-
mon inverted index retrieval implementations, our
model is quite slow since the document-term vectors
have to be loaded. However, multi-threading sup-
port and batch retrieval on a Hadoop cluster made
the model tractable. On the upside, the translation-
based model allows greater precision in finding
the candidates for comparable microblog messages
than simpler approaches that use a combination of
tfidf matching and n-best query term expansion:
The translation-based retrieval exploits all possi-
ble alignments between query and document terms
which is particularly important for short documents
such as microblog messages.
3.2 In-Domain Phrase Extraction
To prepare the extraction of phrases from retrieval
results, we conducted cross-lingual retrieval in both
directions: retrieving Arabic documents using En-
glish microblog messages as queries and vice versa.
6http://lucene.apache.org/core/
412
For each run we kept the top N retrieved documents.
Each document was then paired with its query to
generate pseudo-parallel data.
We tried two approaches for using this data to
improve our translations. The first, more restric-
tive method makes use of the word alignments we
obtained from 5.8 million clean parallel training
data from the NIST evaluation campaign. The re-
trieval step generates word-alignments in the direc-
tion D ? Q. After retrieval, the reverse alignment
for each query-document pair is also generated by
using a translation table in the direction Q ? D. An
alignment point between a query term q and a docu-
ment term d is created, iff T (q|d) or T (d|q) exist in
the translation tables D ? Q or Q ? D. Based on
these word-alignments, we extract phrases by apply-
ing the grow-diag-final-and heuristic and using Och
and Ney (2004)?s phrase extraction algorithm as im-
plemented in Moses7 (Koehn et al, 2007). We con-
ducted experiments using different constraints on
the number of alignment points required for a pair
to be considered as well as the value of N . Our first
technique resembles the technique of Munteanu and
Marcu (2006) who also perform phrase extraction
by combining clean alignment lexica for initial sig-
nals with heuristics to smooth alignments for final
fragment extraction.
While we obtained some gains using our heuris-
tics, we are aware that our method is severely re-
stricted in that it only learns new words which are
in the vicinity of known words. We therefore also
tried the bolder approach of treating our data as
parallel and running unsupervised word alignment8
(Och and Ney, 2000) directly on the query-document
pairs to obtain new world alignments and build a
phrase table. In contrast to previous work (Snover
et al, 2008; Daum? and Jagarlamudi, 2011), we can
take advantage of the sentence-like character of mi-
croblog messages and treat queries and retrieval re-
sults similar to sentence aligned data.
For both extraction methods, the standard five
translation features from the new phrase table
(phrase translation probability and lexical weight-
ing in both directions, phrase penalty) were added to
the translation features in Moses. We tried different
7http://statmt.org/moses/
8http://code.google.com/p/giza-pp/
al-Gaddafi, al-Qaddhafi, assad, babrain, bahrain,
egypt, gadaffi, gaddaffi, gaddafi, Gheddafi, homs,
human rights, human-rights, humanrights, libia, li-
bian, libya, libyan, lybia, lybian, lybya, lybyan,
manama, Misrata, nabeelrajab, nato, oman, Pos-
itiveLibyaTweets, Qaddhafi, sirte, syria, tripoli,
tripolis, yemen;
Table 1: Keywords used for Twitter crawl.
modes of combining new and original phrase table,
namely using either one or using the new phrase ta-
ble as backoff in case no phrase translation is found
in the original phrase table.
4 Data
4.1 Twitter Crawl
We crawled Twitter messages from September 20,
2011 until January 23, 2012 via the Streaming API9
in keyword-tracking mode, obtaining 25.5M Twit-
ter messages (tweets) in various languages. Table 1
shows the list of keywords that were chosen to re-
trieve microblog messages related to the events of
the Arab spring.10
In order to separate the microblog message cor-
pus by languages, we applied a Naive Bayes lan-
guage identifier11. This yielded a distribution with
the six most common languages (of 52) being Ara-
bic (57%), English (33%), Somali (2%), Spanish
(2%), Indonesian (1.5%), German (0.7%). We kept
only microblog messages classified as English or
Arabic with confidence greater 0.9. Keyword-based
crawling creates a strong bias towards the domain
of the keywords and it does not guarantee that all
microblog messages regarding a certain topic or re-
gion are retrieved or that all retrieved messages are
related to the Arab Spring and human righs in the
middle east. Additionally, retweets artificially in-
9https://dev.twitter.com/docs/
streaming-api/
10The Twitter Streaming API allows up to 400 tracking key-
words that are matched to uppercase, lowercase and quoted
variations of the keywords. Partial matching such as ?tripolis?
matching ?tripoli? as well as Arabic Unicode characters are not
supported. We extended our keywords over time by analyzing
the crawl, e.g., by introducing spelling variants and hashtags.
11Language Detection Library for Java, by
Shuyo Nakatani (http://code.google.com/p/
language-detection/).
413
Arabic English
tweets + retweets 14,565,513 8,501,788
tweets 6,614,126 5,129,829
avg. retweet/tweet 11.62 7.27
unique users 180,271 865,202
avg. tweets/user 36.6 5.9
Table 2: Twitter corpus statistics
flate the size of the data, although there are no new
terms added. Therefore, we removed all duplicate
retweets that did not introduce additional terms to
the original tweet. Table 2 explains the shrinkage
of the dataset after removing retweets - compared
to English users, a smaller number of Arabic users
produced a much larger number of retweets. Inter-
estingly, 56,087 users tweet a substantial amount in
both languages. This suggests that users spread mes-
sages simultaneously in Arabic and English.
4.2 Creating a Small Parallel Twitter Corpus
using Crowdsourcing
For the evaluation of our method, a small amount
of parallel in-domain data was required. Since there
are no corpora of translated microblog messages, we
decided to use Amazon Mechanical Turk12 to cre-
ate our own evaluation set, following the exploratory
work of Zaidan and Callison-Burch (2011b). We
randomly selected 2,000 Arabic microblog mes-
sages. Hashtags, user mentions and URLs were re-
moved from each microblog message beforehand,
because they do not need to be translated and would
just artificially inflate scores at test time. The mi-
croblog messages were then manually cleaned and
pruned. We discarded messages which contained
almost no text or large portions of other languages
and removed remaining Twitter markup. In the end,
1,022 microblog messages were used in the Me-
chanical Turk task. We split the data into batches
of ten sentences which comprised one HIT (human
intelligence task). Each HIT had to be completed by
three workers. In order to have some control over
translation quality, we inserted one control sentence
per HIT, taken from the LDC-GALE Phase 1 Arabic
Blog Parallel Text. Turkers were rewarded 10 cents
per translation. Following Zaidan and Callison-
Burch (2011b), all Arabic sentences were converted
12http://www.turk.com
into images in order to prevent turkers from past-
ing them into online machine translation engines.
Our final corpus consists of 1,022 translated mi-
croblog messages with three translations each. An
example containing translations for one of the sen-
tences which we inserted for quality checking pur-
poses, along with the reference translation, is given
in table 3. It can be seen that translators sometimes
made grammar mistakes or odd word choices. They
also tended to omit punctuation marks. However,
translations also contained reasonable translation al-
ternatives (such as ?gathered? or ?collected?). We
also asked translators to insert an ?unknown? token
whenever they were unable to translate a word. Our
HIT setup did not allow workers to skip a sentence,
forcing them to complete an entire batch. In order to
account for translation variants we decided to use all
three translations obtained via Mechanical Turk as
multiple references instead of just keeping the top
translation. We randomly split our small parallel
corpus, using half of the microblog messages for de-
velopment and half for testing.
4.3 Preprocessing
Besides removal of Twitter markup, several addi-
tional preprocessing steps such as digit normaliza-
tion were applied to the data. We also decided to ap-
ply the Buckwalter Arabic transliteration scheme13
to avoid encoding difficulties. Habash and Sadat
(2006) have shown that tokenization is helpful for
translating Arabic. We therefore decided to ap-
ply a more involved tokenization scheme than sim-
ple whitespace splitting to our data. As the re-
trieval relies on translation tables, all data need
to be tokenized the same way. We are aware
of the MADA+TOKAN Arabic morphological an-
alyzer and tokenizer (Habash and Rambow, 2005),
however, this toolkit produces very in-depth analy-
ses of the data and thus led to difficulties when we
tried to scale it to millions of sentences/microblog
messages. That is why we only used MADA for
transliteration and chose to implement the simpler
approach by Lee et al (2003) for tokenization. This
approach only requires a small set of annotated data
to obtain a list of prefixes and suffixes and uses n-
13http://www.qamus.org/transliteration.
htm
414
REFERENCE breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers.
TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli
TRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier.
TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiers
Table 3: Example turker translations.
gram-models to determine the most likely prefix?-
stem-suffix? split of a word.14
5 Twitter Translation Experiments
We conducted a series of experiments to evaluate
our strategy of using CLIR and phrase-extraction to
extract comparable data in the Twitter domain. We
also explored more standard ways of domain adap-
tation such as using English microblog messages to
build an in-domain language model, or generating
synthetic bilingual corpora from monolingual data.
All experiments were conducted using the Moses
machine translation system15 (Koehn et al, 2007)
with standard settings. Language models were
built using the SRILM toolkit16 (Stolcke, 2002).
For all experiments, we report lowercased BLEU-
4 scores (Papineni et al, 2001) as calculated by
Moses? multi-bleu script. For assessing signifi-
cance, we apply the approximate randomization test
(Noreen, 1989; Riezler and Maxwell, 2005). We
consider pairwise differing results scoring a p-value
< 0.05 as significant.
Our baseline model was trained using 5,823,363
million parallel sentences in Modern Standard
Arabic (MSA) (198,500,436 tokens) and English
(193,671,201 tokens) from the NIST evaluation
campaign. This data contains parallel text from dif-
ferent domains, including UN reports, newsgroups,
newswire, broadcast news and weblogs.
5.1 Domain Adaption using Monolingual
Resources
As a first step, we used the available in-domain
data for a combination of domain adaptation tech-
14The n-gram-model required for tokenization was trained on
5.8 million Modern Standard Arabic sentences from the NIST
evaluation campaign. This data had previously been tokenized
with the same method, trained to match the Penn Arabic Tree-
bank, v3.
15http://statmt.org/moses/
16http://www.speech.sri.com/projects/
srilm/
niques similar to Bertoldi and Federico (2009).
There were three different adaptation measures:
First, the turker-generated development set was used
for optimizing the weights of the decoding meta-
parameters, as introduced by Koehn and Schroeder
(2007). Second, the English microblog messages in
our crawl were used to build an in-domain language
model. This adaptation technique was first proposed
by Zhao et al (2004). Third, the Arabic portion of
our crawl was used to synthetically generate addi-
tional parallel training data. This was accomplished
by machine-translating the Arabic microblog mes-
sages with the best system after performing the first
two adaptation steps. Since decoding is very time-
intensive, only 1 million randomly selected Ara-
bic microblog messages were used to generate syn-
thetic parallel data. This new data was then used
to train another phrase table. Such self-translation
techniques have been introduced by Ueffing et al
(2007). All results were evaluated against a base-
line of using only NIST data for translation model,
language model and weight optimization.
Our results are shown in table 4. Using an in-
domain development set while leaving everything
else untouched led to an improvement of approxi-
mately 1 BLEU point. Three experiments involv-
ing the Twitter language model confirm Bertoldi
and Federico (2009)?s findings that the language
model was most helpful. The BLEU-score could
be improved by 1.5 to 2 points in all experiments.
When using an in-domain language model, there
was no significant difference between deploying an
in-domain or out-of-domain development set. We
also compared the effect of using only the in-domain
language model to that of adding the in-domain
language model as an extra feature while keeping
the NIST language model.17 There was no signif-
17The weights for both language models were optimized
along with all other translation feature weights, rather than run-
ning an extra optimization step to interpolate between both lan-
guage models, since Koehn and Schroeder (2007) showed that
415
Run Translation Model Language Model Dev Set BLEU %
1 NIST NIST NIST 13.90
2 NIST NIST Twitter 14.83?
3 NIST Twitter NIST 15.98?
4 NIST Twitter Twitter 15.68?
5 NIST Twitter & NIST Twitter 16.04?
6 self-train Twitter & NIST Twitter 15.79?
7 self-train & NIST Twitter & NIST Twitter 15.94?
Table 4: Domain adaptation experiments. Asterisks indicate significant improvements over baseline (1).
Run Twitter Phrases extraction method # sentence pairs # extracted phrases BLEU %
8 top 3 retrieval results heuristics 14,855,985 6,508,141 17.04?
9 top 1 retrieval results GIZA++ 5,141,065 54,260,537 18.73??
10 retrieval intersection GIZA++ 3,452,566 29,091,009 18.85??
11 retrieval intersection as backoff GIZA++ 3,452,566 29,091,009 18.93??
Table 5: CLIR domain adaptation experiments. All weights were optimized on the Twitter dev set and used
the Twitter and NIST language models. One Asterisk indicates a significant improvement over baseline run
(5) from table 4. Two Asterisks indicate a significant improvement over run (8).
icant difference between both runs. However, for
further adaptation experiments we used the system
with the highest absolute BLEU score. In our case,
using synthetically generated data was not help-
ful, yielding similar results as the language model
experiments above. As has been observed before
by Bertoldi and Federico (2009), it did not matter
whether the synthetic data were used on their own
or in addition to the original training data.
5.2 Domain Adaptation using
Translation-based CLIR
Meta-parameters ?, ? ? [0, 1] of the retrieval model
were tuned in a mate-finding experiment: Mate-
finding refers to the task of retrieving the single rel-
evant document for a query. In our case, each source
tweet in the crowdsourced development set had ex-
actly one ?mate?, namely the crowdsourced transla-
tion that was ranked best in a further crowdsourced
ranking task. Using the retrieval model described
in section 3 we achieved precision@1 scores above
95% in finding the translations of a tweet when ?
and ? were set to 0.9. We fixed these parameter set-
tings for all following experiments. The translation
table was taken from the baseline experiments in ta-
ble 4. During retrieval, we kept up to 10 highest
scoring documents per query.
both strategies yielded the same results.
We first employed heuristic phrase extraction
based on the word alignments generated from the
NIST data as described above. To avoid learning
too much noise, maximum phrase length was re-
stricted to 3 (the default is 7). To evaluate the effects
of choosing more restrictive or more lax settings,
we ran experiments varying the following configu-
rations:
1. Constraints on alignment points:
? no constraints,
? 3+ alignment points in each direction,
? 3+ alignment points in both directions,
? 5+ alignment points in both directions.
2. Constraints on retrieval ranking:
? top 10 results,
? top 3 results,
? top 1 results,
? retrieval intersection (results found in both
retrieval directions)
We obtained improvements for all combinations
of these configurations. However, we observed that
requiring 5 common alignment points was too strict,
since few pairs met this constraint. We also noticed
that using only the top 3 retrieval results was benefi-
cial to performance, suggesting that more compara-
ble microblog messages were indeed ranked higher.
416
Using extraction heuristics we gained maximally 1.0
BLEU using the top 3 retrieval results and requiring
at least 3 alignment points in both alignment direc-
tions (see first line in table 5). However, other con-
figurations produced very similar results.
While heuristics led to small incremental im-
provements, we achieved a much larger improve-
ment by training a new phrase table from scratch us-
ing GIZA++. Again, we restricted maximum phrase
length to 3 words. In order to keep phrase table
size manageable, we had to restrict retrieval to top-
1 results or only use retrieval results in the inter-
section of retrieval directions. Best results are ob-
tained when combining phrase tables extracted from
GIZA++ alignments in the intersection of retrieval
results with NIST phrase tables in backoff mode (see
last line in table 5).
6 Error Analysis
Our cross-lingual retrieval approach succeeded in
finding nearly parallel tweets, confirming our hy-
pothesis that such data actually exists. Examples are
given in table 6.
Table 7 shows a more detailed breakdown of our
translation scores. First, standard adaptation meth-
ods increased n-gram precision, suggesting that us-
ing in-domain adaptation data caused the system to
choose more suitable words. As expected, there was
no reduction in OOVs, since using an in-domain
language model and development set does not in-
troduce new vocabulary. Heuristic phrase extrac-
tion again produced small improvements in n-gram
precision while reducing the number of unknown
words. Learning a new phrase table with GIZA++
produced substantial improvements both in OOV-
rate and in n-gram precision.
Nevertheless, even the scores of the adapted sys-
tem are still fairly low and translation quality as
judged by inspection of the output can be very poor.
This suggests that the language used on Twitter still
poses a great challenge, due to its variety of styles
as well as the users? tendency to use non-standard
spelling and colloquial or dialectal expressions. Our
development set contained many different genres,
from Qu?ran verses over news headlines to personal
chatter. Another difficulty was posed by dialectal
Arabic content. To gain an impression of the amount
of dialectal content in our data, we used the Arabic
Online Commentary Dataset created by Zaidan and
Callison-Burch (2011a) to classify our test set. Ta-
ble 8 shows the distribution of dialects in our test
data according to language model probability. This
distribution should be viewed with a grain of salt,
since the shortness of tweets might cause unreliable
results when using a model based on word frequen-
cies for classification. Still, the results suggest that
there is a high proportion of dialectal content and
spelling variation in our data, causing a large num-
ber of OOVs. For example, the preposition ??,
meaning ?in? is often written as Y?. Our phrase
table trained only on standard Arabic data as well as
our extraction heuristic failed to translate this fre-
quently occurring word. Only when retraining a
phrase table with GIZA++ did we translate it cor-
rectly.
Dialect # Sentences
Egyptian 141
Levantine 147
Gulf 78
Modern Standard Arabic 145
Table 8: Dialectal content in our test set as classified
by the AOC dataset.
Table 9 gives examples of translations generated
using different adaptation methods in comparison to
the references and the Google translation service to
illustrate strengths and weaknesses of our approach.
Example 1 shows a case where unknown words were
learned through translation model adaptation. Note
that even the Google translator did not recognize
the word ?ys? which was transliterated as
?Msellat?. Zaidan and Callison-Burch (2011a) point
out that dialectal variants are often transliterated
by Google. Note also, that the unadapted transla-
tion erroneously translated the place name ?sitra? as
?jacket?, a mistake which was also made in two of
the references and by Google. The same happened
to the place name ?wadyan?, which could also be
taken as meaning ?and religions?. This error was
enforced by our preprocessing step incorrectly split-
ting off the prefix ?w? which often carries the mean-
ing ?and?. In addition to that, the two runs which
used translation model adaptation each dropped a
part of the input sentence (?in sitra?, ?firing?). We
417
ARABIC TWEET fO?  Y?  ?yybyl?   w?d?? ??AyF ?? @q?  ?  d??? ?s?rf?  Hy?r?  
 ?  
GOOGLE TRANSLATION AFP confirms that the French President Gaddafi Libyans tried to call and forgiveness
ENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each other
ARABIC TWEET Hym?  ??   ? rO? Y? ?wmm?  A?rJ ?ym ??C ? A?E Crq? ?AO?  ?y\n EAh
GOOGLE TRANSLATION NTRA decide to increase the number of all mobile operators in Egypt a commencement from Thursday
ENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursday
ARABIC TWEET ?CA? ?lV ??rV ?? r?An? ?w? dm  Yl? ?y?  dyhK? 
GOOGLE TRANSLATION Shahid Amin AA Day January through gunshot
ENGLISH TWEET martyr amin ali ahmed on jan by gunshot
Table 6: Examples of nearly parallel tweets found by our retrieval method.
Adaptation method OOV-rate %/absolute unigram precision %/absolute bigram precision %/absolute output length (words)
None 22.56/2216 51.1/5020 20.2/1882 9832
LM and Dev 20.05/2220 51.4/5442 22.1/2227 10595
Retrieval (heuristic) 17.47/1790 53.5/5484 23.6/2299 10246
Retrieval (GIZA++) 4.22/439 56.1/5834 26.1/2575 10395
Table 7: OOV-rate and precision for different adaptation methods.
attribute this to that fact that the phrase table extrac-
tion often produced one-to-many alignments when
only one alignment point was known. In Example 2
GIZA++ extraction clearly outperformed heuristic
phrase extraction. This example also shows that our
method is good at learning proper names. While
the first two examples resemble news text, Exam-
ple 3 is a more informal message. It is particularly
interesting to note that with GIZA++ extraction the
term ?shabiha? is learned, which is commonly used
in Syria to mean ?thugs? and specifically refers to
armed civilians who assault protesters against Bashir
Al-Assad?s regime. Example 4 also shows substan-
tial OOV reduction. However, the term ? rtns
 r??  (?in Opera Central?, the location of Telecom
Egypt) is incorrectly translated as ?really opera?.
7 Conclusion
We presented an approach to translation of mi-
croblog messages from the Twitter domain. The
main obstacle to state-of-the-art SMT of such data
is the complete lack of sentence-parallel training
data. We presented a technique that uses translation-
based CLIR to find relevant Arabic Twitter messages
given English Twitter queries, and applies a standard
pipeline for unsupervised training of phrase-based
SMT to retrieval results. We found this straight-
forward technique to outperform more conservative
techniques to extract phrases from comparable data
and also to outperform techniques using monolin-
gual resources for language model adaptation, meta-
parameter tuning, or self-translation.
The greatest benefit of our approach is a signifi-
cant reduction of OOV terms at a simultaneous im-
provement of correct unigram and bigram transla-
tions. Despite this positive net effect, we still find
a considerable amount of noise in the automati-
cally extracted phrase tables. Noise reduction by
improved pre-processing and by more sophisticated
training will be subject to future work. Furthermore,
we would like to investigate a tighter integration of
CLIR and SMT training by using forced decoding
techniques for CLIR and by a integrating a feedback
loop into retrieval and training.
Acknowledgments
We would like to thank Julia Ostertag for several it-
erations of manual error analysis of Arabic transla-
tion output.
418
EXAMPLE 1
SRC ?w?d?  ?ys? ?lW? Tlrt? ?A?  ? ?tq 	?K?   w? ?rtF
GOOGLE Riot troops stormed the jacket and religions foot and launches Msellat tears
NO ADAPTATION jacket riot forces storm and religions foot ?ys? ?lW? tears
LM AND DEV sitra and religions of the foot of the riot forces storm ?ys? ?lW? tears
RETRIEVAL (HEURISTIC) in sitra riot police storming and religions of tear gas on foot
RETRIEVAL (GIZA++) the riot police stormed and religions of the foot firing tear gas
REF0 vest riot forces break into wadyan by foot and trough gas tear
REF1 sotra the riot forces enter on foot and shoot tear bombs
REF2 the cover for riot police enters wadian walking and shoot tear bombs
EXAMPLE 2
SRC Yq?w`?  ?tq? ?? ?wy?  dtyF A?A?
GOOGLE Obama will speak today the death of al-Awlaki
NO ADAPTATION dtyF A?A? today killed Yq?w`? 
LM AND DEV dtyF A?A? friday for the killing of Yq?w`? 
RETRIEVAL (HEURISTIC) A?A? today on the killing of
RETRIEVAL (GIZA++) obama today on the al awlaki killing
REF0 obama will talk today about the killing of al - awlaki
REF1 obama is talking today about el awlaqi death
REF2 obama will speak today about the killing of al - awlaqi
EXAMPLE 3
SRC (: ?wy?ts? ?Am ?? TybK? 
GOOGLE Cbihh in Hama are crying :)
NO ADAPTATION TybK?  mired in calling for help : )
LM AND DEV TybK?  in hama calling for help : )
RETRIEVAL (HEURISTIC) inside the protectors of the calling for help : )
RETRIEVAL (GIZA++) shabiha in hama calling for help : )
REF0 the gangsters in hama are asking for help
REF1 the gangs in hamah are peading :)
REF2 the thugs in hama are calling for help :)
EXAMPLE 4
SRC  r??  ? rtns T?r? Y? T?rK?  Hy?C ??zt? ?AO?? T?rOm?A ?wl?A? :: ???r?
GOOGLE Freedom :: Telecom Egypt workers holding company?s president in a room Psontral Opera
NO ADAPTATION : : free workers ?AO?? T?rOm?A holding company chairman  r??  ? rtns Y? chamber
LM AND DEV : : workers free ?AO?? T?rOm?A holding company chairman Y? r??  ? rtns room
RETRIEVAL (HEURISTIC) free : : afcd T?rOm?A hold ceo hostage ppl is the president of the chamber of  r??  ? rtns
RETRIEVAL (GIZA++) egypt : : workers telecom workers are holding the head of the company in the chamber of really opera
REF0 freedom :: workers in the egyptian for communication are holding the company president in a room in the opera central
REF1 freedom , workers in egypt for calls detain the head of the company in a room in opera central
REF2 hurriya :: workers in telecom egypt detaining the president of the company in a room in the opera central
Table 9: Example output using different adaptation methods.
References
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of the
22nd ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?99), Berkeley,
CA.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
EACL Workshop on Statistical Machine Translation,
Athens, Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
M. Cettolo, M. Federico, and N. Bertoldi. 2010. Min-
ing parallel fragments from comparable texts. In Pro-
ceedings of the 7th International Workshop on Spoken
419
Language Translation, Paris, France.
Hal Daum? and Jagadeesh Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT?11), Portland,
OR.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), Ann Arbor, MI.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference - North American Chapter of the Association
for Computational Linguistics annual meeting (HLT-
NAACL?06), New York, NY.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU haitian
creole-english translation system for WMT 2011. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland, UK.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency SMS messages. In Proceed-
ings of the 6th Workshop on Statistical Machine Trans-
lation, Edinburgh, Scotland, UK.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
Prague, Czech Republic.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based arabic word segmentation. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics (ACL?03), Sapporo, Japan.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), Sydney, Aus-
tralia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hongkong, China.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Jay M. Ponte and Bruce W. Croft. 1998. A language
modeling approach to information retrieval. Proceed-
ings of the 21st annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR?98).
Chris Quirk, Raghavendra Udupa U, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen , Denmark.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?08), Honolulu, Hawaii.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Christoph Tillmann and Jian ming Xu. 2009. A sim-
ple sentence-level extraction algorithm for comparable
data. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
420
can Chapter of the Association for Computational Lin-
guistic (NAACL-HLT?09), Boulder, CO.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL?07), Prague, Czech Republic.
Stephan Vogel and Sanjika Hewavitharana. 2011. Ex-
tracting parallel phrases from comparable data. In
Proceedings of the 4th Workshop on Building and Us-
ing Comparable Corpora: Comparable Corpora and
the Web, Portland, OR.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the 24th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?01),
New York, NY.
Xiaobing Xue, Jiwoon Jeon, and Bruce Croft. 2008. Re-
trieval models for question and answer archives. In
Proceedings of the 31st Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR?08), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?11), Port-
land, OR.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?11), Portland, OR.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING?04), Geneva, Switzer-
land.
421
