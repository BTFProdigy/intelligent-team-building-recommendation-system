High Precision Extraction of Grammatical Relations
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton
BN1 9QH, UK
Ted Briscoe
Computer Laboratory
University of Cambridge
JJ Thomson Avenue
Cambridge CB3 0FD, UK
Abstract
A parsing system returning analyses in the form of
sets of grammatical relations can obtain high pre-
cision if it hypothesises a particular relation only
when it is certain that the relation is correct. We
operationalise this technique?in a statistical parser
using a manually-developed wide-coverage gram-
mar of English?by only returning relations that
form part of all analyses licensed by the grammar.
We observe an increase in precision from 75% to
over 90% (at the cost of a reduction in recall) on a
test corpus of naturally-occurring text.
1 Introduction1
Head-dependent relationships (possibly labelled
with a relation type) have been advocated as a use-
ful level of representation for grammatical struc-
ture in a number of different large-scale language-
processing tasks. For instance, in recent work on
statistical treebank grammar parsing (e.g. Collins,
1999) high levels of accuracy have been reached
using lexicalised probabilistic models over head-
dependent tuples. Bouma, van Noord and Mal-
ouf (2001) create dependency treebanks semi-auto-
matically in order to induce dependency-based sta-
tistical models for parse selection. Lin (1998),
Srinivas (2000) and others have evaluated the ac-
curacy of both phrase structure-based and depen-
dency parsers by matching head-dependent rela-
tions against ?gold standard? relations, rather than
matching (labelled) phrase structure bracketings.
Research on unsupervised acquisition of lexical in-
formation from corpora, such as argument structure
of predicates (Briscoe and Carroll, 1997; McCarthy,
2000), word classes for disambiguation (Clark and
Weir, 2001), and collocations (Lin 1999), has used
grammatical relation/head/dependent tuples. Such
1A previous version of this paper was presented at
IWPT?01; this version contains new experiments and results.
tuples also constitute a convenient intermediate rep-
resentation in applications such as information ex-
traction (Palmer et al, 1993; Yeh, 2000), and docu-
ment retrieval on the Web (Grefenstette, 1997).
A variety of different approaches have been taken
for robust extraction of relation/head/dependent tu-
ples, or grammatical relations, from unrestricted
text. Dependency parsing is a natural technique
to use, and there has been some work in that area
on robust analysis and disambiguation (e.g. Laf-
ferty, Sleator and Temperley, 1992; Srinivas, 2000).
Finite-state approaches (e.g. Karlsson et al, 1995;
A??t-Mokhtar and Chanod, 1997; Grefenstette, 1998)
have used hand-coded transducers to recognise lin-
ear configurations of words and part of speech la-
bels associated with, for example, subject/object-
verb relationships. An intermediate step may be to
mark nominal, verbal etc. ?chunks? in the text and to
identify the head word of each of the chunks. Sta-
tistical finite-state approaches have also been used:
Brants, Skut and Krenn (1997) train a cascade of
Hidden Markov Models to tag words with their
grammatical functions. Approaches based on mem-
ory based learning have also used chunking as a
first stage, before assigning grammatical relation la-
bels to heads of chunks (Argamon, Dagan and Kry-
molowski, 1998; Buchholz, Veenstra and Daele-
mans, 1999). Blaheta and Charniak (2000) assume
a richer input representation consisting of labelled
trees produced by a treebank grammar parser, and
use the treebank again to train a further procedure
that assigns grammatical function tags to syntac-
tic constituents in the trees. Alternatively, a hand-
written grammar can be used that produces ?shal-
low? and perhaps partial phrase structure analyses
from which grammatical relations are extracted (e.g.
Carroll, Minnen and Briscoe, 1998; Lin, 1998).
Recently, Schmid and Rooth (2001) have de-
scribed an algorithm for computing expected gov-
ernor labels for terminal words in labelled headed
parse trees produced by a probabilistic context-free
grammar. A governor label (implicitly) encodes a
grammatical relation type (such as subject or ob-
ject) and a governing lexical head. The labels are
expected in the sense that each is weighted by the
sum of the probabilities of the trees giving rise to
it, and are computed efficiently by processing the
entire parse forest rather than individual trees. The
set of terminal/relation/governing-head tuples will
not typically constitute a globally coherent analy-
sis, but may be useful for interfacing to applications
that primarily accumulate fragments of grammati-
cal information from text (such as for instance in-
formation extraction, or systems that acquire lexical
data from corpora). The approach is not so suit-
able for applications that need to interpret complete
and consistent sentence structures (such as the anal-
ysis phase of transfer-based machine translation).
Schmid and Rooth have implemented the algorithm
for parsing with a lexicalised probabilistic context-
free grammar of English and applied it in an open
domain question answering system, but they do not
give any practical results or an evaluation.
In the paper we investigate empirically Schmid
and Rooth?s proposals, using a wide-coverage pars-
ing system applied to a test corpus of naturally-
occurring text, extend it with various thresholding
techniques, and observe the trade-off between pre-
cision and recall in grammatical relations returned.
Using the most conservative threshold results in a
parser that returns only grammatical relations that
form part of all analyses licensed by the grammar.
In this case, precision rises to over 90%, as com-
pared with a baseline of 75%.
2 The Analysis System
In this investigation we extend a statistical shallow
parsing system for English developed originally by
Carroll, Minnen and Briscoe (1998). Briefly, the
system works as follows: input text is labelled with
part-of-speech (PoS) tags by a tagger, and these
are parsed using a wide-coverage unification-based
?phrasal? grammar of English PoS tags and punctu-
ation. For disambiguation, the parser uses a prob-
abilistic LR model derived from parse tree struc-
tures in a treebank, augmented with a set of lexical
entries for verbs, acquired automatically from a 10
million word sample of the British National Corpus
(Leech, 1992), each entry containing subcategori-
sation frame information and an associated proba-
bility. The parser is therefore ?semi-lexicalised? in
that verbal argument structure is disambiguated lex-
ically, but the rest of the disambiguation is purely
structural.
The coverage of the grammar?the proportion of
sentences for which at least one complete spanning
analysis is found?is around 80% when applied to
the SUSANNE corpus (Sampson, 1995). In addition,
the system is able to perform parse failure recov-
ery, finding the highest scoring sequence of phrasal
fragments (following the approach of Kiefer et al,
1999), and the system has produced at least partial
analyses for over 98% of the sentences in the written
part of the British National Corpus.
The parsing system reads off grammatical rela-
tion tuples (GRs) from the constituent structure tree
that is returned from the disambiguation phase. In-
formation is used about which grammar rules in-
troduce subjects, complements, and modifiers, and
which daughter(s) is/are the head(s), and which the
dependents. In Carroll et al?s evaluation the system
achieves GR accuracy that is comparable to pub-
lished results for other systems: extraction of non-
clausal subject relations with 83% precision, com-
pared with Grefenstette?s (1998) figure of 80%; and
overall F-score2 of unlabelled head-dependent pairs
of 80%, as opposed to Lin?s (1998) 83%3 and Srini-
vas?s (2000) 84% (this with respect only to binary
relations, and omitting the analysis of control rela-
tionships). Blaheta and Charniak (2000) report an
F-score of 87% for assigning grammatical function
tags to constituents, but the task, and therefore the
scoring method, is rather different.
For the work reported in this paper we have ex-
tended Carroll et al?s basic system, implementing
a version of Schmid and Rooth?s expected gover-
nor technique (see section 1 above) but adapted for
unification-based grammar and GR-based analyses.
Each sentence is analysed as a set of weighted GRs
where the weight associated with each grammati-
cal relation is computed as the sum of the proba-
bilities of the parses that relation was derived from,
divided by the sum of the probabilities of all parses.
So, if we assume that Schmid and Rooth?s example
sentence Peter reads every paper on markup has 2
parses, one where on markup attaches to the preced-
ing noun having overall probability     and the
other where it has verbal attachment with probabil-
ity     , then some of the weighted GRs would be
2We use the F 	 measure defined as 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 257?264
Manchester, August 2008
Statistical anaphora resolution in biomedical texts
Caroline Gasperin Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
{cvg20,ejb}@cl.cam.ac.uk
Abstract
This paper presents a probabilistic model
for resolution of non-pronominal anaphora
in biomedical texts. The model seeks to
find the antecedents of anaphoric expres-
sions, both coreferent and associative ones,
and also to identify discourse-new expres-
sions. We consider only the noun phrases
referring to biomedical entities. The model
reaches state-of-the art performance: 56-
69% precision and 54-67% recall on coref-
erent cases, and reasonable performance
on different classes of associative cases.
1 Introduction
Inspired by Ge et al (1998) probabilistic model for
pronoun resolution, we have developed a model for
resolution of non-pronominal anaphora in biomed-
ical texts.
The probabilistic model results from a simple
decomposition process applied to a conditional
probability equation that involves several param-
eters (features). The decomposition makes use
of Bayes? theorem and independence assumptions,
and aims to decrease the impact of data sparse-
ness on the model, so that even small training cor-
pora can be viable. The decomposed model can
be understood as a more sophisticated version of
the naive-Bayes algorithm, since we consider the
dependence among some of the features instead
of full independence as in naive Bayes. Proba-
bilistic models can return a confidence measure
(probability) for each decision they make, while
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
decision trees, for example, cannot. Another ad-
vantage of this type of model is the fact that they
consider the prior probability of each class, while
other machine-learning techniques such as SVMs
and neural networks do not.
Our model seeks to classify the relation between
an anaphoric expression and an antecedent candi-
date as coreferent, associative or neither. It com-
putes the probability of each pair of anaphor and
candidate for each class. The candidate with the
highest overall probability for each class is se-
lected as the antecedent for that class, or no an-
tecedent is selected if the probability of no relation
overcomes the positive probabilities; in this case,
the expression is considered to be new to the dis-
course.
Coreferent cases are the ones in which the
anaphoric expression and its antecedent refer to the
same entity in the real world, as below:
(1) ?The expression of reaper has been
shown ... the gene encodes ...?
Associative cases are the ones in which the
anaphoric expression and its antecedent refer to
different entities, but where the antecedent plays
a role in defining the meaning of the anaphoric ex-
pression, as in Example 2:
(2) ?Drosophila gene Bok interacts with
... expression of Bok protein
promotes apoptosis ...?
Discourse new cases usually consist of the first
mention of an entity in the text, so no antecedent
can be found for it.
We have focused on the biomedical domain for
two reasons. Firstly, there is a vast demand from
the biomedical field for information extraction ef-
forts (which require NLP processing, including
257
anaphora resolution), in order to process the ex-
ponentially increasing number of journal publica-
tions, which are the major source of novel knowl-
edge to be extracted and condensed into domain
databases for easy access. Secondly, anaphora res-
olution can benefit from the several sources of re-
fined semantic knowledge that are not commonly
available for other domains, such as biomedical
databases, ontologies, and terminologies.
In the next section, we describe the anaphoric
relations that we found in biomedical texts, which
we are considering for the resolution process. In
Section 3 we describe our probabilistic model, and
in Section 4 the corpus created for training it.
In Section 5 we present and discuss the results
achieved by the model, and compare it with a base-
line system and a decision-tree-based system.
2 Anaphora cases in biomedical text
Biomedical texts differ from other genres of text
(e.g. newswire, fiction) in several points. Differ-
ent types of NPs have a particular distribution in
biomedical articles. For example, pronouns are
very rare, accounting for a very small percentage
of the noun phrases
1
; on the other hand, proper
names occur very often, given the frequent men-
tion of gene and protein names and the names of
other biomedical entities. A system for anaphora
resolution in the biomedical domain can benefit
from focusing on the most common types of noun
phrases, that is, non-pronominal ones.
In biomedical articles, the reader needs back-
ground knowledge to understand the underlying
relation between the entities mentioned in the text
in order to understand the text. For instance, in Ex-
ample 2 the reader is expected to know that a gene
encodes a protein (which usually carries its name),
so that he/she can capture the anaphoric relation
and understand the sentence. This aspect empha-
sises the need for semantic information as part of
the anaphora resolution process.
Another aspect affecting the anaphoric relations
in biomedical texts are the writing conventions
adopted in the biomedical domain to distinguish
between the mention of a gene name and the men-
tion of the protein encoded by that gene. The
most usual convention is writing gene names with
lowercase italicised letters and protein names with
non-italicised uppercase letters. The existence of
1
About 3% of the noun phrases according to the corpus
presented in Section 4.
such conventions allows for constructions where
the reader keeps the conventions in mind to under-
stand the text, as below.
(3) ?Drosophila has recently been
shown also to have a CED-4/Apaf-1
homolog, named Dark/HAC-1/Dapaf-1.
... Loss of function mutations in
dark/hac-1/dapaf-1 result in ...?
Among the associative cases present in biomed-
ical text, we were able to distinguish two main
types of relations. The first, which we call ?bio-
type? relations, are associative relations between
biomedical entities with different semantic types,
which we call biotypes (e.g. gene, gene product,
part of gene). This is the case of Example 2 and 3
2
.
If we take into account the specific biotype of the
entities that are involved in a biotype relation, it is
possible to determine a WordNet-like semantic re-
lation behind the anaphora relation. For example,
a biotype relation between a ?gene? and a ?variant?
of gene can be considered an hyponymy relation,
the relation between a ?gene? and a transcript (part
of gene) can be seen as a meronymy relation.
The second type of associative relation is more
common to other domains as well, we call it ?set-
member? relation. It consists of cases where the
anaphor refers to a set that contains the antecedent,
or vice-versa, as in Examples 4 and 5.
(4) ...ced-4 and ced-9...the genes...
(5) ...the mammalian anti-apoptotic
protein Bcl-2...Bcl-2 family...
3 The resolution model
Our probabilistic model aims to find the clos-
est coreferent and/or associative antecedent for all
non-pronominal NPs that refer to biomedical enti-
ties. Among non-pronominal NPs we distinguish
proper names, definite NPs (e.g. ?the gene?),
demonstrative NPs (e.g. ?this gene?), indefinite
NPs (e.g. ?a gene?), quantified NPs (e.g. ?four
genes?, ?some genes?) and other NPs.
We consider the three classes of anaphoric rela-
tions mentioned above: coreferent, associative bio-
type, and associative set-member.
We have chosen 11 features to describe the
anaphoric relations between two noun phrases.
The features are presented in Table 1. Most fea-
tures are domain-independent, while one, gp, is
2
Associative relations between proper names are not
known to happen in other domains, and are made possible
in the biomedical domain given the existence of naming con-
ventions.
258
specific for the biomedical domain. Our feature set
covers the basic aspects that influence anaphoric
relations: the form of the anaphor?s NP, string
matching, semantic class matching, number agree-
ment, and distance.
Given these features, for each antecedent candi-
date a of an anaphor A, we compute the probabil-
ity P of an specific class of anaphoric relation C
between a and A. P is defined as follows:
P (C = ?class?|f
A
, f
a
, hm
a,A
, hmm
a,A
,mm
a,A
,
num
a,A
, sr
a
, bm
a,A
, gp
a,A
, d
a,A
, dm
a,A
)
For each pair of a given anaphor and an an-
tecedent candidate we compute P for C=?coreferent?,
C=?biotype?, and C=?set-member?. We also compute
C=?none?, that represents the probability of no rela-
tion between the NPs.
We decompose the probability P and assume in-
dependence among some of the features in order
to handle the sparseness of the training data. In
the following equations, we omit the subscripted
indexes of the relational features for clarity.
P (C|f
A
, f
a
, hm, hmm,mm,num, sr, bm, gp, d, dm)
=
P (C)P (f
A
, f
a
, hm, hmm,mm,
num, sr, bm, gp, d, dm|C)
P (f
A
, f
a
, hm, hmm,mm,num, sr, bm, gp, d, dm)
(1)
Equation 1 is obtained by applying Bayes? theo-
rem to the initial equation. P (C) is the prior prob-
ability of each class, it will encode the distribution
of the classes in the training data. As the denom-
inator contains feature values that change accord-
ing to the candidate being considered, we cannot
eliminate it in the usual fashion, so we keep it in
order to normalise P across all candidates. From
this equation, we then selectively apply the chain
rule to both numerator and denominator until we
get to the following equation:
=
P (C) P (f
A
|C) P (f
a
|C, f
A
) P (d, dm|C, f
A
, f
a
)
P (sr|C, f
A
, f
a
, d, dm) P (bm, gp|C, f
A
, f
a
, d, dm, sr)
P (num|C, f
A
, f
a
,d, dm, sr, bm, gp)
P (hm, hmm,mm|C, f
A
,f
a
, d, dm, sr, bm, gp, num)
P (f
A
) P (f
a
|f
A
) P (d, dm|f
A
, f
a
)
P (sr|f
A
, f
a
, d, dm) P (bm, gp|f
A
, f
a
, d, dm, sr)
P (num|f
A
, f
a
, d, dm, sr, bm, gp)
P (hm, hmm,mm|f
A
, f
a
, d, dm, sr, bm, gp, num)
(2)
Following the decomposition, we eliminate
some of the dependencies among the features that
we consider unnecessary
3
. We consider that the
lexical features hm, hmm, andmm are not depen-
dent on distance d or dm, nor on sr, gp or num,
so:
P (hm, hmm,mm|C, f
A
, f
a
, d, dm, sr, bm, gp, num) ?
P (hm, hmm,mm|C, f
A
, f
a
, bm)
We model num as independent from d, dm, sr,
bm, and gp, so:
P (num|C, f
A
, f
a
, d, dm, sr, bm, gp) ?
P (num|C, f
A
, f
a
)
We also assume the semantic features bm, and
gp as independent from all features but C:
P (bm, gp|C, f
A
, f
a
, d, dm, sr) ? P (bm, gp|C)
We also assume sr to be independent of f
A
and
f
a
:
P (sr|C, f
A
, f
a
, d, dm) ? P (sr|C, d, dm)
The final equation then becomes:
P (C|f
A
, f
a
, hm, hmm,mm,num, sr, bm, gp, d, dm) =
P (C) P (f
A
|C) P (f
a
|C, f
A
) P (d, dm|C, f
A
, f
a
)
P (sr|C, d, dm) P (bm, gp|C) P (num|C, f
A
, f
a
)
P (hm, hmm,mm|C, f
A
, f
a
, bm)
P (f
A
) P (f
a
|f
A
) P (d, dm|f
A
, f
a
)
P (sr|d, dm) P (bm, gp) P (num|f
A
, f
a
)
P (hm, hmm,mm|f
A
, f
a
, bm)
(3)
4 Training
There are very few biomedical corpora annotated
with anaphora information, and all of them are
built from paper abstracts (Cohen et al, 2005), in-
stead of full papers. As anaphora is a phenomenon
that develops through a text, we believe that short
abstracts are not the best source to work with and
decided to concentrate on full papers.
In order to collect the statistics to train our
model, we have manually annotated anaphoric re-
lations between biomedical entities in 5 full-text
articles (approx. 33,300 words)
4
, which are part of
the Drosophila molecular biology literature. The
corpus and annotation process are described in
(Gasperin et al, 2007). To the best of our knowl-
edge, this corpus is the first corpus of biomedical
full-text articles to be annotated with anaphora in-
formation.
3
For brevity, we only show this process for the numerator,
although the same is assumed for the denominator.
4
Corpus available via the FlySlip project website
http://www.wiki.cl.cam.ac.uk/rowiki/NaturalLanguage/FlySlip
259
Feature Possible values
f
A
Form of noun phrase of the anaphor A: ?pn?, ?defnp?, ?demnp?, ?indefnp?, ?quantnp?, or ?np?.
f
a
Form of noun phrase of the antecedent candidate a: same values as for f
A
.
hm
a,A
Head-noun matching: ?yes? if the anaphor?s and the candidate?s head nouns match, ?no? otherwise.
hmm
a,A
Head-modifier matching: ?yes? if the anaphor?s head noun matches any of the candidate?s pre-modifiers, or
vice-versa, ?no? otherwise.
mm
a,A
Modifier matching: ?yes? if anaphor and candidate have at least one head modifier in common, ?no? otherwise.
num
a,A
Number agreement: ?yes? if anaphor and candidate agree in number, ?no? otherwise.
sr
a,A
Syntactic relation between anaphor and candidate: ?none?, ?apposition?, ?subj-obj?, ?pp?, and few others.
bm
a,A
Biotype matching: ?yes? if anaphor?s and candidate?s biotype (semantic class) match, ?no? otherwise.
gp
a,A
is biotype gene or product? ?yes? if the anaphor biotype or candidate biotype is gene or product, ?no? otherwise.
This feature is mainly to distinguish which pairs can hold biotype relations.
d
a,A
Distance in sentences between the anaphor and the candidate.
dm
a,A
Distance in number of entities (markables) between the anaphor and the candidate.
Table 1: Feature set
Before annotating anaphora, we have prepro-
cessed the articles in order to (1) tag gene names,
(2) identify all NPs, and (3) classify the NPs ac-
cording to their domain type, which we call bio-
type. To tag all gene names in the corpus, we
have applied the gene name recogniser developed
by Vlachos et al (2006). To identify all NPs, their
subconstituents (head, modifiers, determiner) and
broader pre- and post-modification patterns, we
have used the RASP parser (Briscoe et al, 2006).
To classify the NPs according to their type in
biomedical terms, we have adopted the Sequence
Ontology (SO)
5
(Eilbeck and Lewis, 2004). SO
is a fine-grained ontology, which contains the
names of practically all entities that participate in
genomic sequences, besides the relations among
these entities (e.g. is-a, part-of, derived-from re-
lations). We derived from SO seven biotypes to
be used to classify the entities in the text, namely:
?gene?, ?gene product?, ?part of gene?, ?part of
product?, ?gene variant?, ?gene subtype?, and
?gene supertype?. We also created the biotype
?other-bio? to be associated with noun phrases that
contain a gene name (identified by the gene name
recogniser) but whose head noun does not fit any
of the other biotypes. All NPs were tagged with
their biotypes, and NPs for which no biotypes were
found were excluded.
The gene-name tags, NP boundaries and bio-
types resulting from the preprocessing phase were
revised and corrected by hand before the anaphoric
relations were annotated.
For each biotyped NP we annotated its closest
coreferent antecedent (if found) and its closest as-
sociative antecedent (if found), from one of the as-
sociative classes. From our annotation, we can in-
5
http://www.sequenceontology.org/
fer coreference chains by merging the coreferent
links between mentions of a same entity.
The annotated relations, and the features de-
rived from them, are used as training data for the
probabilistic model above. We have also consid-
ered negative training samples, which result from
the absence of an anaphoric relation between a
NP that precedes an anaphoric expression and was
not marked as its antecedent (nor marked as part
of the same coreference chain of its antecedent).
The negative samples outnumber considerably the
number of positive samples (annotated cases). Ta-
ble 2 presents the distribution of the cases among
the classes of anaphora relations.
We note that around 80% of the definite NPs are
anaphoric in our corpus, instead of the 50% pre-
sented in (Vieira and Poesio, 2000) for newspa-
per texts. Nearly all demonstrative NPs (93%) are
anaphoric. More than 70% of the proper names
take part in coreference relations, as they inher-
ently refer to a specific named entity, but never-
theless 5% of them take part in associative biotype
relations, due to the fact that a gene and the protein
it synthesizes usually share the same name. 44% of
quantified NPs take part in set-member relations,
as they usually refer to more than one entity. Fi-
nally 51% of indefinite NPs are discourse new.
To balance the ratio between positive and nega-
tive training samples, we have clustered the neg-
ative samples and kept only a portion of each
cluster, proportional to its size. All negative
samples that have the same values for all fea-
tures are grouped together (consequently, a clus-
ter is formed by a set of identical samples) and
only
1
10
of each cluster members is kept, re-
sulting in 85,314 negative samples. This way,
small clusters (with less than 10 members), which
260
Class/NPs pn defnp demnp indefnp quantnp other np Total
coreferent 689 429 70 40 54 396 1678
biotype 43 102 3 8 4 114 274
set-member 151 126 26 14 68 158 543
discourse new 63 107 0 72 38 156 436
none 873,731
Table 2: Training instances, according to anaphoric class and to NP form
are likely to represent noisy samples (similar to
positive ones), are eliminated, and bigger clus-
ters are shrunk; however the shape of the dis-
tribution of the negative samples is preserved.
For example, our biggest cluster (feature values
are: f
A
=?pn?, f
a
=?pn?, hm=?no?, hmm=?no?,
mm=?no?, bm=?yes?, gp=?yes?, num=?yes?,
sr=?none?, d=?16<?, dm=?50<?) with 33,998 in-
stances is reduced to 3,399 ? still considerably
more numerous than any positive sample.
Other works have used a different strategy to re-
duce the imbalance between positive and negative
samples (Soon et al, 2001; Ng and Cardie, 2002;
Strube et al, 2002), where only samples composed
by a negative antecedent that is closer than the
annotated one are considered. We compare the
performance of both strategies in Section 5.1 and
show that ours is more effective. The higher the
number of negative samples, the higher the preci-
sion of the resolution, but the lower the recall.
5 Results
Given the small size of our corpus, we did not hold
out a test set. Instead, we have measured the av-
erage performance achieved by the model in a 10-
fold cross-validation setting, using the whole of the
annotated corpus.
We consider as antecedent candidates all noun
phrases that precede the anaphor. For a given
anaphor, we first select as antecedent according to
each anaphora class the candidate with the high-
est value for P for that class. We also compute
P(C=?none?) for all candidates. If P(C=?coreferent?) >
P(C=?none?) for the selected coreferent antecedent,
it is kept as the resulting antecedent. The same is
tested for the selected associative antecedent with
the highest probability, independent of the type of
associative class. For set-member cases, where
an anaphor can have multiple antecedents, if more
than one candidate has an equally high probabil-
ity, all these candidates are kept. When no coref-
erent or associative antecedent is found (or when
P(C=?none?) is higher on both cases) the anaphor is
classified as discourse new.
Table 3 presents the performance scores we
achieved for each anaphora class. The first col-
umn, ?perfect?, shows the result of a strict evalu-
ation, where we consider as correct all pairs that
match exactly an antecedent-anaphor pair in the
annotated data. On the other hand, column ?re-
laxed? treats as correct also the pairs where the
assigned antecedent is not the exact match in the
annotated data but is coreferent with it.
It is clear that the results for coreferent cases
are much better than for associative cases, but the
latter are known to be more challenging. On top
of that, the ?relaxed? column shows considerable
improvements in comparison to ?perfect?. That
means that several anaphors are being linked to the
correct coreference chain, despite not being linked
to the closest antecedent. This happens mainly in
cases where there is no string matching between
the closest antecedent and the anaphor, causing an
earlier mention of the same entity with matching
head and/or modifiers to get higher probability. We
believe we can approximate ?perfect? to ?relaxed?
results if we extend the string matching features to
represent the whole coreference chain, that is, con-
sider a positive matching when the anaphor match
any of the elements in a chain, similarly to the idea
presented in (Yang et al, 2004).
We believe that the lower overall performance
for associative cases is due to the difficulty of se-
lecting features that capture all aspects involved in
associative relations. Our set of features is clearly
failing to cover some of these aspects, and a deeper
feature study should be the best way to boost the
scores. However, despite lower, these performance
Perfect Relaxed
Class
P R F P R F
coreferent 56.3 54.7 55.5 69.4 67.4 68.3
biotype 28.5 35.0 31.4 31.2 37.9 34.2
set-member 35.4 38.2 36.7 38.5 41.5 40.0
discourse new 44.3 53.4 48.4 44.3 53.4 48.4
Table 3: Performance of the probabilistic model
261
scores are higher that the ones from previous ap-
proaches for newspaper texts, which used for in-
stance the WordNet (Poesio et al, 1997) or the
Internet (Bunescu, 2003) as source of semantic
knowledge.
We have analysed our features and observed
that the string matching features hm, hmm, and
mm, the number agreement feature num, bio-
type matching bm, and distance in markables dm
are the core features and achieve reasonable per-
formance. However, f
A
and f
a
play an impor-
tant role, they increase the precision of corefer-
ent cases and boost considerably the performance
of the associative ones. This is due to the differ-
ent distribution of NP types across the relations as
shown is Table 2. The remaining features focused
on specific cases: gp improved biotype recall, by
boosting the probability of a biotype relation when
anaphor or candidate had specific biotypes; and sr
improved precision and recall of coreferent cases.
Table 4 shows the ?perfect? performance scores
according to each class of NP. The resolution of
proper names achieves the highest scores among
all types of NPs for most classes. That is due to
their limited structure, since proper names usually
do not have elaborated pre-modification or modifi-
cation at all, so our string matching features car-
ried simpler patterns for these NPs. Indefinite
and quantified NPs achieved the lowest scores for
coreferent cases, since the highest percentage of
training instances for these NPs are not coreferent
(as seen in Table 2). Indefinite NPs, as expected,
have the best scores for discourse new cases.
5.1 Comparing to other approaches
We have tried training our probabilistic model us-
ing a different strategy than the one described in
Section 4 for selecting negative samples. This
strategy consists of selecting only the negative
samples that occur between the anaphor and its
coreferent antecedent, not considering candidates
that are further away than the antecedent. This
strategy was first used for anaphora resolution by
Soon et al (2001). Column ?prob+closest? on Ta-
ble 5 shows the performance scores. In our dataset,
this strategy was able to reduce the number of neg-
ative samples to about
1
3
of its size, while our strat-
egy reduces it to
1
10
. The larger number of neg-
ative samples increases the precision scores and
reduces the recall scores for all positive classes,
while the opposite happens for the negative class,
which defines the discourse new scores. We reckon
that the considerable drop on recall numbers for
the associative cases would make the system less
viable, while the low precision for discourse new
cases shows that many anaphoric cases are left un-
resolved. We view our strategy, based on the clus-
tering of negative samples and consecutive cluster
size reduction, to be more effective at proportion-
ally eliminating negative samples that are less fre-
quent and that are more likely to be noisy.
We compare our model to a rule-based base-
line system that we have previously developed.
The baseline system (Gasperin, 2006) for each
anaphoric expression: 1) selects as coreferent an-
tecedent the closest preceding NP that has the same
head noun, same biotype and agrees in number
with the anaphor, and 2) selects as associative an-
tecedent the closest preceding NP that has the same
head noun, same biotype but disagrees in num-
ber with the anaphor, or that has the same head
noun or a modifier matching the anaphor head (or
vice-versa) or matching modifiers, agrees in num-
ber but has different biotypes. The baseline sys-
tem does not distinguish between different types
of associative cases, although it aims to cover bio-
type and set-member cases. If no antecedent that
matches these criteria is found, the anaphor is con-
sidered discourse new. Column ?baseline? on Ta-
ble 5 shows the performance scores for the base-
line system. The scores for coreferent cases are
reasonable, despite being below our probabilistic
model, while the scores for associative cases, es-
pecially recall, are considerably lower. The base-
line system relies on some sort of string matching
between anaphor and antecedent, and is not able
to infer a relation between expressions when the
matching does not happen. That is one of the main
aspects that the probabilistic system tries to over-
come by weighting the contribution of all features.
We also compared our model to a system based
on decision trees, since this approach has been
taken by several corpus-based anaphora resolution
systems (Soon et al, 2001; Ng and Cardie, 2002;
Strube et al, 2002). We have induced a decision
tree using the C4.5 algorithm implemented in the
Weka tool (Witten and Frank, 2005); we have used
the same features used for our probabilistic model.
We selected as the antecedent the candidate which
is the closest one to the anaphor for which a class
other then ?none? is assigned by the decision tree.
The ?perfect? and ?relaxed? scores for C4.5 are pre-
262
coreferent biotype set-member discourse new
Class
P R F P R F P R F P R F
pn 77.5 71.9 74.6 26.8 25.5 26.1 53.7 65.7 59.1 35.1 59.3 44.1
defnp 48.0 47.3 47.6 26.3 28.1 27.2 29.2 26.1 27.6 38.8 51.8 44.4
demnp 57.8 48.5 52.8 - - - 71.4 57.6 63.8 - - -
indefnp 27.0 34.2 30.2 14.2 12.5 13.3 21.0 28.5 24.2 63.4 54.7 58.8
quantnp 11.2 12.9 12.0 - - - 28.5 37.6 32.5 37.1 34.2 35.6
other np 41.3 41.4 41.4 30.9 48.2 37.7 19.3 19.4 19.4 49.7 56.0 52.6
Table 4: Performance of the probabilistic model (?perfect?) per NP form
sented in the last two columns of Table 5. We note
that the difference between ?perfect? and ?relaxed?
scores is not as large as for our probabilistic model;
that shows that decision trees are more often get-
ting even the coreference chain wrong, not just the
closest antecedent. We assume this is due to the
lack of ranking among the candidates, since we
adopt the default strategy of selecting the closest
candidate that gets a positive class according to the
tree.
The main disadvantage of both the baseline and
decision tree systems when compared to the prob-
abilistic model, besides the lower performance, is
that they do not provide a probability assigned to
each decision they make, which makes it impos-
sible to know how confident the model is for dif-
ferent cases and to take advantage of that infor-
mation to improve the system. This aspect also
makes it difficult to develop a consistent strategy
for returning multiple antecedents for set-member
cases, since there is no obvious way to do it.
6 Related work
We are not aware of any learning-based system
which has dealt with coreferent as well as asso-
ciative cases of anaphora.
Viera and Poesio (2000) have developed a
heuristic-based system for coreferent and asso-
ciative anaphora resolution of definite NPs in
newspaper texts, and have reached 62% recall
and 83% precision for direct anaphora (coreferent
cases with same head noun), but poor performance
for bridging cases (associative cases + coreferent
cases with different head nouns) using WordNet as
souce of semantic knowledge.
Ng and Cardie (2002), extending the work of
Soon et al (2001), have developed a machine-
learning system just for coreference resolution of
all types of NPs, also on newspaper texts. Their
best results were 64.2% recall and 78.0% preci-
sion.
The best-known system to resolve anaphora in
the biomedical domain is the work of Casta?no et
al. (2002), who developed a salience-based sys-
tem for resolution of coreferent cases. It seeks
to resolve pronouns and nominal (which they call
sortal) anaphora. As a source of semantic knowl-
edge, they have used the UMLS Semantic Net-
work types
6
, which they report to be too coarse
grained, and assume that a finer-grained typing
strategy would help to increase the precision of the
resolution system. They achieved 74% precision
and 75% recall on a very small test set.
Yang et al (2004) implemented a machine-
learning approach to coreference resolution similar
to Ng and Cardie?s, and evaluated it on a portion of
the GENIA corpus, which is tagged with semantic
information based on the GENIA Ontology
7
. They
achieved recall of 80.2% and precision of 77.4%.
Both the Casta?no et al and Yang et al systems
have been developed based on abstracts of biomed-
ical articles, instead of full-text articles, which in-
volve only restricted use of anaphora.
7 Conclusion and future work
We have presented a probabilistic model for re-
solving anaphoric NPs in biomedical texts. We are
not aware of previous works which have dealt with
coreferent and associative anaphora in the biomed-
ical domain. Our model, despite being simple and
being trained on a very small corpus, coped well
with its task of finding antecedents for coreferent
and associative cases of anaphora, and was able
to achieve state-of-the-art performance. It has out-
performed our baseline system and a decision-tree-
based system using the same set of features.
Our model returns a probability for each classi-
fication it makes, and this can be used as a confi-
dence measure that can be exploited to improve the
system itself or by external applications.
Due to our small corpus, we had to limit the
6
http://www.nlm.nih.gov/research/umls/
7
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
263
Prob+Closest Baseline C4.5 C4.5 relaxed
Class
P R F P R F P R F P R F
coreferent 66.2 50.0 56.9 47.0 57.6 51.8 49.6 58.1 53.5 52.7 61.6 56.8
biotype 31.1 10.1 15.2
28.6 10.7 15.6
21.7 28.5 24.6 22.9 29.9 26.0
set-member 46.3 17.5 25.4 28.5 31.3 29.8 30.4 33.3 31.8
discourse new 31.3 88.1 46.2 37.3 30.2 33.4 48.5 32.5 38.9 48.5 32.5 38.9
Table 5: Performance of other models
number and the complexity of the features we
use, since the more features, the more sparse the
data, and the more training data needed. However,
we aim to expand the feature set with more fine-
grained features.
Our current work involves using the probabilis-
tic model presented here as part of an active learn-
ing framework. The confidence of the model for
each decision (probability) is used to selectively
gather more samples from unlabelled data and it-
eratively improve the performance of the system.
The probabilistic model is intended to replace
the baseline system in a tool designed to help bi-
ology researchers to curate scientific papers (Kara-
manis et al, 2008).
Acknowledgements
This work is part of the BBSRC-funded FlySlip
project. Caroline Gasperin is funded by a CAPES
award from the Brazilian government.
References
Briscoe, Edward J., John Carroll, and Rebecca Watson.
2006. The second release of the RASP system. In
Proceedings of ACL-COLING 06, Sydney, Australia.
Bunescu, Razvan. 2003. Associative anaphora reso-
lution: A web-based approach. In Proceedings of
EACL 2003 - Workshop on The Computational Treat-
ment of Anaphora, Budapest.
Casta?no, Jos?e, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
In Proceedings of International Symposium on Ref-
erence Resolution for NLP 2002, Alicante, Spain.
Cohen, K. Bretonnel, Lynne Fox, Philip Ogren, and
Lawrence Hunter. 2005. Corpus design for biomed-
ical natural language processsing. In Proceedings of
the ACL-ISMB Workshop on Linking Biological Lit-
erature, Ontologies and Databases, Detroit.
Eilbeck, Karen and Suzanna E. Lewis. 2004. Sequence
ontology annotation guide. Comparative and Func-
tional Genomics, 5:642?647.
Gasperin, Caroline, Nikiforos Karamanis, and Ruth
Seal. 2007. Annotation of anaphoric relations in
biomedical full-text articles using a domain-relevant
scheme. In Proceedings of DAARC 2007, Lagos,
Portugal.
Gasperin, Caroline. 2006. Semi-supervised anaphora
resolution in biomedical texts. In Proceedings of
BioNLP?06, New York.
Ge, Niyu, John Hale, and Eugene Charniak. 1998. A
statistical approach to anaphora resolution. In Pro-
ceedings of the Sixth Workshop on Very Large Cor-
pora - COLING-ACL?98, Montreal, Canada.
Karamanis, Nikiforos, Ruth Seal, Ian Lewin, Peter
McQuilton, Andreas Vlachos, Caroline Gasperin,
Rachel Drysdale, and Ted Briscoe. 2008. Natural
language processing in aid of flybase curators. BMC
Bioinformatics, 9(193).
Ng, Vincent and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of ACL 2002, Philadelphia.
Poesio, Massimo, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in unre-
stricted texts. In Proceedings of the Workshop on
Operational Factors In Practical, Robust Anaphora
Resolution for Unrestricted Texts, Madrid.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Strube, Michael, Stefan Rapp, and Christoph M?uller.
2002. The influence of minimum edit distance on
reference resolution. In Proceedings of the EMNLP
2002, Philadelphia.
Vieira, Renata and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):525?
579.
Vlachos, Andreas and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of BioNLP
at HLT-NAACL 2006, pages 138?145, New York.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
Yang, X., J. Su, G. Zhou, and C. L. Tan. 2004. An NP-
cluster based approach to coreference resolution. In
Proceedings of COLING 2004, Geneva, Switzerland.
264
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1170?1174,
Prague, June 2007. c?2007 Association for Computational Linguistics
Adapting the RASP System for the CoNLL07 Domain-Adaptation Task
Rebecca Watson and Ted Briscoe
Computer Laboratory
University of Cambridge
FirstName.LastName@cl.cam.ac.uk
Abstract
We describe our submission to the domain
adaptation track of the CoNLL07 shared
task in the open class for systems using ex-
ternal resources. Our main finding was that
it was very difficult to map from the annota-
tion scheme used to prepare training and de-
velopment data to one that could be used to
effectively train and adapt the RASP system
unlexicalized parse ranking model. Never-
theless, we were able to demonstrate a sig-
nificant improvement in performance utiliz-
ing bootstrapping over the PBIOTB data.
1 Introduction
The CoNLL07 domain adaptation task was created
to explore how a parser trained in one domain might
be adapted to a new one. The training data were
drawn from the PTB (Marcus et al, 1993) rean-
notated with dependency relations (Johansson and
Nugues, 2007, hereafter DRs). The test data were
taken from a corpus of biomedical articles (Kulick
et al, 2004) and the CHILDES database (Brown,
1973; MacWhinney, 2000) also reannotated with
DRs (see Nivre et al, 2007) for further details of
the task, annotation format, and evaluation scheme.
The development data consisted of a small amount
of annotated and unannotated biomedical and con-
versational data.
The RASP system (Briscoe et al, 2006) utilizes
a manually-developed grammar and outputs gram-
matical bilexical dependency relations (see Briscoe,
2006 for a detailed description, hereafter GRs). Wat-
son et al (2007) describe a semi-supervised, boot-
strapping approach to training the parser which uti-
lizes unlabelled partially-bracketed input with re-
spect to the system derivations. For the domain
adaptation task we retrained RASP by mapping our
GR scheme to the DR scheme and annotation for-
mat, and used this mapping to select a derivation
to train the unlexicalized parse ranking model from
the annotated PTB training data. We also performed
similar partially-supervised bootstrapping over the
200 annotated biomedical sentences in the develop-
ment data. We then tried unsupervised bootstrap-
ping from the unannotated development data based
on these initial models.
As the parser requires input to consist of a se-
quence of one of 150 CLAWS PoS tags, we also uti-
lize a first-order HMM PoS tagger which has been
trained on manually-annotated data from the LOB,
BNC and Susanne Corpora (see Briscoe, 2006 for
further details). Accordingly, we submitted our re-
sults in the open class.
2 Training and Adaptation
The RASP parser is a generalized LR parser which
builds a non-deterministic generalized LALR(1)
parse table from the grammar (Tomita, 1987). A
context-free ?backbone? is automatically derived
from a unification grammar. The residue of fea-
tures not incorporated into the backbone are unified
on each reduce action and if unification fails the as-
sociated derivation paths also fail. The parser cre-
ates a packed parse forest represented as a graph-
structured stack.
Inui et al (1997) describe the probability model
1170
utilized in the system where a transition is repre-
sented by the probability of moving from one stack
state, ?i?1, (an instance of the graph structured
stack) to another, ?i. They estimate this probability
using the stack-top state si?1, next input symbol li
and next action ai. This probability is conditioned
on the type of state si?1. Ss and Sr are mutually
exclusive sets of states which represent those states
reached after shift or reduce actions, respectively.
The probability of an action is estimated as:
P (li, ai, ?i|?i?1) ?
{
P (li, ai|si?1) si?1 ? Ss
P (ai|si?1, li) si?1 ? Sr
}
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead for the
state depending on whether the state is a member of
Ss or Sr, respectively. In addition, Laplace estima-
tion can be used to ensure that all actions in the table
are assigned a non-zero probability.
These probabilities are estimated from counts
of actions which yield derivations compatible with
training data. We use a confidence-based self-
training approach to select derivations compatible
with the annotation of the training and development
data to train the model. In Watson et al (2007), we
utilized unlabelled partially-bracketed training data
as the starting point for this semi-supervised train-
ing process. Here we start from the DR-annotated
training data, map it to GRs, and then find the one
or more derivations in our grammar which yield GR
output consistent with the GRs recovered from the
DR scheme. Following Watson et al (2007), we
utilize the subset of sentences in the training data
for which there is a single derivation consistent with
this mapping to build an initial trained parse ranking
model. Then we use this model to rank the deriva-
tions consistent with the mapping in the portion of
the training data which remains ambiguous given
the mapping. We then train a new model based on
counts from these consistent derivations which are
weighted in somemanner by our confidence in them,
given both the degree of remaining ambiguity and
also the ranking and/or derivation probabilities pro-
vided by the initial model.
Thus, the first and hardest step was to map the
DR scheme to our GR scheme. Issues concerning
this mapping are discussed in section 4. Given this
mapping, we determined the subset of sentences in
the (PTB) training data for which there was a sin-
gle derivation in the grammar compatible with the
set of mapped GRs. These derivations were used
to create the initial trained model (B) from the uni-
form model (A). To evaluate the performance of
these and subsequent models, we tested them using
our own GR-based evaluation scheme over 560 sen-
tences from our reannotated version of DepBank, a
subset of section 23 of the WSJ PTB (see Briscoe
& Carroll, 2006). Table 1 gives the unlabelled pre-
cision, recall and microaveraged F1 score of these
models over this data. Model B was used to rerank
derivations compatible with the mapped GRs recov-
ered for the PTB training data. Model C was built
from the weighted counts of actions in the initial set
of unambiguous data and from the highest-ranked
derivations over the training data (i.e. we do not in-
clude duplicate counts from the unambiguous data).
Counts were weighted with scores ranging [0 ? 1]
corresponding to the overall probability of the rel-
evant derivation. The evaluation shows a steady
increase in performance for these successive mod-
els. We also explored other variants of this boot-
strapping approach involving use of weighted counts
from the top n ranked parses derived from the initial
model (see Watson et al, 2007, for details), but none
performed better than simply selecting the highest-
ranked derivation.
To adapt the trained parser, we used the same
technique for the 200 in-domain biomedical sen-
tences (PBIOTB), using Model C to find the highest-
ranked parse compatible with the annotation, and
derived Model D from the combined counts from
this and the previous training data. We then used
Model D to rank the parses for the unannotated
in-domain data (PBIOTB unsupervised), and de-
rived Model E from the combined counts from the
highest-ranked parses for all of the training and de-
velopment data. We then iterated this process two
more times over the unannotated datasets (each with
an increasing number of examples though increas-
ingly less relevant to the test data). The performance
over our out-of-domain PTB-derived test data re-
mains approximately the same for all these models.
Therefore, we chose to use Model G for the blind
test as it incorporates most information from the in-
1171
Mdl. Data Init. Prec. Rec. F1
A Uniform - 71.06 69.00 70.01
PTB
B Unambig. A 75.94 73.16 74.53
C Ambig. B 77.88 75.11 76.47
PBIOTB
D Supervised C 77.86 75.09 76.45
E Unsup. 1 D 77.98 75.25 76.59
F Unsup. 2 E 77.85 75.19 76.50
G Unsup. 3 F 77.76 75.09 76.41
CHILDES
H Unsup. 1 C 78.34 75.59 76.94
Table 1: Performance of Successive Bootstrapping
Models
Score Avg. Std
PCHEMTB - labelled 55.47 65.11 09.64
PCHEMTB - unlab.ed 62.79 70.24 08.14
CHILDES - unlab.ed 45.61 56.12 09.17
Table 2: Official Scores
domain data. For the CHILDES data we performed
one iteration of unsupervised adaptation in the same
manner starting from Model C.
3 Evaluation
For the blind test submission we used Models G and
H to parse the PCHEMTB and CHILDES data, re-
spectively. We then mapped our GR output from
the highest-ranked parses to the DR scheme and an-
notation format required by the CoNLL evaluation
script. Our reported results are given in Table 2.
We used the annotated versions of the blind test
data supplied after the official evaluation to assess
the degree of adaptation of the parser to the in-
domain data. We mapped from the DR scheme and
annotation format to our GR format and used our
evaluation script to calculate the precision, recall
and microaveraged F1 score for the unadapted mod-
els and their adapted counterparts on the blind test
data, given in Table 3. The results for CHILDES
show no evidence of adaptation to the domain. How-
ever, those for PCHEMTB show a statistically sig-
nificant (Wilcoxin Signed Ranks) improvement over
the initial model. The generally higher scores in
Model Test Data Prec. Rec. F1
C PCHEMTB 71.58 73.69 72.62
G PCHEMTB 72.32 74.56 73.42
C CHILDES 82.64 65.18 72.88
H CHILDES 81.71 64.58 72.14
Table 3: Performance of (Un)Adapted Models
Table 3, as compared to Table 2, reflect the differ-
ences between the task annotation scheme and our
GR representation as well as those of the evaluation
schemes, which we discuss in the next section.
4 Discussion
The biggest issue for us participating in the shared
task was the difficulty of reconciling the DR an-
notation scheme with our GR scheme, given the
often implicit and sometimes radical underlying
differences in linguistic assumptions between the
schemes.
Firstly, the PoS tagsets are different and ours con-
tains three times the number of tags. Given that the
grammar uses these tags as preterminal categories,
this puts us at a disadvantage in mapping the anno-
tated training and development data to optimal input
to train the (semi-)supervised models.
Secondly, there are 17 main types of GR rela-
tion and a total of 46 distinctions when GR sub-
types are taken into account ? for instance the GR
ncsubj has two subtypes depending on whether the
surface subject is the underlying object of a passive
clause. The DR scheme has far fewer distinctions
creating similar difficulties when creating optimal
(semi-)supervised training data.
Thirdly, the topology of the dependency graphs
is often significantly different because of reversed
head-dependent bilexical relations and their knock-
on effects ? for instance, the DR AUX relation treats
the (leftmost) auxiliary as head and modifiers of the
verb group attach to the leftmost auxiliary, while the
GR scheme treats the main verb as (semantic) head
and modifiers of the verb group attach to it.
Fourthly, the treatment of punctuation is very dif-
ferent. The DR scheme includes punctuation mark-
ers in DRs which attach to the root of the subgraph
over which they have scope. By contrast, the GR
scheme does not output punctuation marks directly
1172
but follows Nunberg?s (1990) linguistic analysis of
punctuation as delimiting and typing text units or
adjuncts (at constituent boundaries). Thus the GR
scheme includes text (adjunct) relations and treats
punctuation marks as indicators of such relations ?
for instance, for the example The subject GRs ? nc-
subj, xsubj and csubj ? all have subtypes., RASP
outputs the GR (ta dash GRs and) indicating that
the dash-delimited parenthetical is a text adjunct of
GRs with head and, while the DR scheme gives
(DEP GRs and), and two (P and ?) relations cor-
responding to each dash mark.
Although we attempted to derive an optimal and
error-free mapping between the schemes, this was
hampered by the lack of information concerning the
DR scheme, lack of time, and the very different ap-
proaches to punctuation. This undoubtedly limited
our ability to train effectively from the PTB data and
to adapt the trained parser using the in-domain data.
For instance, the mean average unlabelled F1 score
between the GRs mapped from the annotated PTB
training data and closest matching set of GRs output
by RASP for this data is 84.56 with a standard de-
viation of 12.41. This means that the closest match-
ing derivation which is used for training the initial
model is on average only around 85% similar even
by the unlabelled measure. Thus, the mapping pro-
cedure required to relate the annotated data to RASP
derivations is introducing considerable noise into the
training process.
Mapping difficulties also depressed our official
scores very significantly. In training and adapting
we found that bootstrapping based on unlabelled de-
pendencies worked better in all cases than utilizing
the labelled mapping we derived. For the official
submission, we removed all ta, quote and passive
GRs and mapped all punctuation marks to the P re-
lation with head 0. Furthermore, we do not generate
a root relation, though we assumed any word that
was not a dependent in other GRs to have the depen-
dent ROOT. In our own evaluations based on map-
ping the annotated training and development data to
our GR scheme, we remove all P relations and map
ROOT relations to the type root which we added
to our GR hierarchy. We determined the semantic
head of each parse during training so as to compare
against the root GR and better utilize this additional
information. In the results given in Table 1 over our
DepBank test set, the effect of removing the P de-
pendencies is to depress the F1 scores by over 20%.
For the CHILDES and PCHEMTB blind test data,
our F1 scores improve by over 7% and just under 9%
respectively when we factor out the effect of P rela-
tions. These figures give an indication of the scale
of the problem caused by these representional differ-
ences.
5 Conclusions
The main conclusion that we draw from this experi-
ence is that it is very difficult to effectively relate lin-
guistic annotations even when these are inspired by
a similar (dependency-based) theoretical tradition.
The scores we achieved were undoubtedly further
depressed by the need to use a partially-supervised
boostrapping approach to training because the DR
scheme is less informative than the GR one, and by
our decision to use an entirely unlexicalized parse
ranking model for these experiments. Despite these
difficulties, performance on the PCHEMTB dataset
using the adapted model improved significantly over
that of the unadapted model, suggesting that boot-
strapping using confidence-based self-training is a
viable technique.
Acknowledgements
This research has been partially supported by the
EPSRC via the RASP project (grants GR/N36462
and GR/N36493) and the ACLEX project (grant
GR/T19919). The first author is funded by the
Overseas Research Students Awards Scheme and the
Poynton Scholarship appointed by the Cambridge
Australia Trust in collaboration with the Cambridge
Commonwealth Trust.
References
E. Briscoe (2006) An introduction to tag sequence
grammars and the RASP system parser, Univer-
sity of Cambridge, Computer Laboratory Techni-
cal Report, UCAM-CL-TR-662.
E. Briscoe and J. Carroll (2006) ?Evaluating the Ac-
curacy of an Unlexicalized
Statistical Parser on the PARC DepBank?, Pro-
ceedings of the ACL-Coling?06, Sydney, Aus-
tralia.
1173
Briscoe, E.J., J. Carroll and R. Watson (2006) ?The
Second Release of the RASP System?, Proceed-
ings of the ACL-Coling?06, Sydney, Australia.
R. Brown (1973) A First Language: The Early
Stages, Harvard University Press.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) ?A new formalization of prob-
abilistic GLR parsing?, Proceedings of the 5th
International Workshop on Parsing Technologies,
MIT, Cambridge, Massachusetts, pp. 123?134.
R. Johansson and P. Nugues (2007) Extended
Constituent-to-Dependency Conversion for En-
glish, NODALIDA16.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R.
McDonald, M. Palmer, A. Schein and L. Ungar
(2004) ?Integrated Annotation for Biomedical In-
formation Extraction?, Proceedings of the HLT-
NAACL2004, Boston, MA..
B. MacWhinney (2000) The CHILDES Project:
Tools for Analyzing Talk, Lawrence Erlbaum.
M. Marcus, B. Santorini and M. Marcinkiewicz
(1993) ?Building a Large Annotated Corpus of
English: the Penn Treebank?, Computational Lin-
guistics, vol.19.2, 313?330.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel and D. Yuret (2007) ?The CoNLL
2007 Shared Task on Dependency Parsing?, Pro-
ceedings of the EMNLP-CoNLL2007, Prague.
G. Nunberg (1990) The Linguistics of Punctuation,
CSLI Publications.
Tomita, M. (1987) ?An Efficient Augmented
Context-Free Parsing Algorithm?, Computational
Linguistics, vol.13(1?2), 31?46.
R. Watson, E. Briscoe and J. Carroll (2007) ?Semi-
supervised Training of a Statistical Parser from
Unlabeled Partially-bracketed Data?, Proceedings
of the IWPT07, Prague.
1174
Proceedings of the 43rd Annual Meeting of the ACL, pages 614?621,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Acquisition of Adjectival Subcategorization from Corpora
Jeremy Yallop?, Anna Korhonen, and Ted Briscoe
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 OFD, UK
yallop@cantab.net, {Anna.Korhonen, Ted.Briscoe}@cl.cam.ac.uk
Abstract
This paper describes a novel system
for acquiring adjectival subcategorization
frames (SCFs) and associated frequency
information from English corpus data.
The system incorporates a decision-tree
classifier for 30 SCF types which tests
for the presence of grammatical relations
(GRs) in the output of a robust statisti-
cal parser. It uses a powerful pattern-
matching language to classify GRs into
frames hierarchically in a way that mirrors
inheritance-based lexica. The experiments
show that the system is able to detect SCF
types with 70% precision and 66% recall
rate. A new tool for linguistic annotation
of SCFs in corpus data is also introduced
which can considerably alleviate the pro-
cess of obtaining training and test data for
subcategorization acquisition.
1 Introduction
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
?Part of this research was conducted while this author was
at the University of Edinburgh Laboratory for Foundations of
Computer Science.
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al, 1998b),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) (Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from English corpora emerged over
a decade ago (Brent, 1991; Manning, 1993). Subse-
quent research has yielded systems for English (Car-
roll and Rooth, 1998; Briscoe and Carroll, 1997; Ko-
rhonen, 2002) capable of detecting comprehensive
sets of SCFs with promising accuracy and demon-
strated success in application tasks (e.g. (Carroll et
al., 1998b; Korhonen et al, 2003)), besides systems
for a number of other languages (e.g. (Kawahara and
Kurohashi, 2002; Ferrer, 2004)).
While there has been considerable research into
acquisition of verb subcategorization, we are not
aware of any systems built for adjectives. Al-
though adjectives are syntactically less multivalent
than verbs, and although verb subcategorization dis-
tribution data appears to offer the greatest potential
boost in parser performance, accurate and compre-
hensive knowledge of the many adjective SCFs can
improve the accuracy of parsing at several levels
614
(from tagging to syntactic and semantic analysis).
Automatic SCF acquisition techniques are particu-
larly important for adjectives because extant syntax
dictionaries provide very limited coverage of adjec-
tive subcategorization.
In this paper we propose a method for automatic
acquisition of adjectival SCFs from English corpus
data. Our method has been implemented using a
decision-tree classifier which tests for the presence
of grammatical relations (GRs) in the output of the
RASP (Robust Accurate Statistical Parsing) system
(Briscoe and Carroll, 2002). It uses a powerful task-
specific pattern-matching language which enables
the frames to be classified hierarchically in a way
that mirrors inheritance-based lexica. As reported
later, the system is capable of detecting 30 SCFs
with an accuracy comparable to that of best state-of-
art verbal SCF acquisition systems (e.g. (Korhonen,
2002)).
Additionally, we present a novel tool for linguistic
annotation of SCFs in corpus data aimed at alleviat-
ing the process of obtaining training and test data for
subcategorization acquisition. The tool incorporates
an intuitive interface with the ability to significantly
reduce the number of frames presented to the user
for each sentence.
We discuss adjectival subcategorization in sec-
tion 2 and introduce the system for SCF acquisition
in section 3. Details of the annotation tool and the
experimental evaluation are supplied in section 4.
Section 5 provides discussion on our results and fu-
ture work, and section 6 summarises the paper.
2 Adjectival Subcategorization
Although the number of SCF types for adjectives
is smaller than the number reported for verbs
(e.g. (Briscoe and Carroll, 1997)), adjectives never-
theless exhibit rich syntactic behaviour. Besides the
common attributive and predicative positions there
are at least six further positions in which adjec-
tives commonly occur (see figure 1). Adjectives in
predicative position can be further classified accord-
ing to the nature of the arguments with which they
combine ? finite and non-finite clauses and noun
phrases, phrases with and without complementisers,
etc. ? and whether they occur as subject or ob-
ject. Additional distinctions can be made concern-
Attributive ?The young man?
Predicative ?He is young?
Postpositive ?Anyone [who is] young can do it?
Predeterminer ?such a young man?;
?so young a man?
Fused modifier-head ?the younger of them?; ?the young?
Predicative adjunct ?he died young?
Supplementive clause ?Young, he was plain
in appearance?
Contingent clause ?When young, he was lonely?
Figure 1: Fundamental adjectival frames
ing such features as the mood of the complement
(mandative, interrogative, etc.), preferences for par-
ticular prepositions and whether the subject is extra-
posed.
Even ignoring preposition preference, there are
more than 30 distinguishable adjectival SCFs. Some
fairly extensive frame sets can be found in large syn-
tax dictionaries, such as COMLEX (31 SCFs) (Wolff
et al, 1998) and ANLT (24 SCFs) (Boguraev et al,
1987). While such resources are generally accu-
rate, they are disappointingly incomplete: none of
the proposed frame sets in the well-known resources
subsumes the others, the coverage of SCF types for
individual adjectives is low, and (accurate) informa-
tion on the relative frequency of SCFs for each ad-
jective is absent.
The inadequacy of manually-created dictionaries
and the difficulty of adequately enhancing and main-
taining the information by hand was a central moti-
vation for early research into automatic subcatego-
rization acquisition. The focus heretofore has re-
mained firmly on verb subcategorization, but this is
not sufficient, as countless examples show. Knowl-
edge of adjectival subcategorization can yield fur-
ther improvements in tagging (e.g. distinguishing
between ?to? as an infinitive marker and as a true
preposition), parsing (e.g. distinguishing between
PP-arguments and adjuncts), and semantic analysis.
For example, if John is both easy and eager to please
then we know that he is the recipient of pleasure in
the first instance and desirous of providing it in the
second, but a computational system cannot deter-
mine this without knowledge of the subcategoriza-
tion of the two adjectives. Likewise, a natural lan-
guage generation system can legitimately apply the
extraposition transformation to the first case, but not
to the second: It is ?easy to please John?, but not
615
?eager? to do so, at least if ?it? be expletive. Similar
examples abound.
Many of the difficulties described in the litera-
ture on acquiring verb subcategorization also arise
in the adjectival case. The most apparent is data
sparsity: among the 100M-word British National
Corpus (BNC) (Burnard, 1995), the RASP tools find
124,120 distinct adjectives, of which 70,246 occur
only once, 106,464 fewer than ten times and 119,337
fewer than a hundred times. There are fewer than
1,000 adjectives in the corpus which have more than
1,000 occurrences. Both adjective and SCF frequen-
cies have Zipfian distributions; consequently, even
the largest corpora may contain only single instances
of a particular adjective-SCF combination, which is
generally insufficient for classification.
3 Description of the System
Besides focusing on adjectives, our approach to SCF
acquisition differs from earlier work in a number
of ways. A common strategy in existing systems
(e.g. (Briscoe and Carroll, 1997)) is to extract SCFs
from parse trees, introducing an unnecessary depen-
dence on the details of a particular parser. In our ap-
proach the patterns are extracted from GRs ? repre-
sentations of head-complement relations which are
designed to be largely parser-independent ? mak-
ing the techniques more widely applicable and al-
lowing classification to operate at a higher level.
Further, most existing systems work by classifying
corpus occurrences into individual, mutually inde-
pendent SCFs. We adopt instead a hierarchical ap-
proach, viewing frames that share features as de-
scendants of a common parent frame. The benefits
are severalfold: specifying each feature only once
makes the system both more efficient and easier to
understand and maintain, and the multiple inheri-
tance hierarchy reflects the hierarchy of lexical types
found in modern grammars where relationships be-
tween similar frames are represented explicitly1 .
Our acquisition process consists of two main
steps: 1) extracting GRs from corpus data, and 2)
feeding the GRs as input to the classifier which in-
crementally matches parts of the GR sets to decide
which branches of a decision-tree to follow. The
1Compare the cogent argument for a inheritance-based lexi-
con in (Flickinger and Nerbonne, 1992), much of which can be
applied unchanged to the taxonomy of SCFs.
dependent
mod arg mod arg aux conj
subj or dobjncmod xmod cmod detmod
subj comp
ncsubj xsubj csubj obj clausal
dobj obj2 iobj xcomp ccomp
Figure 2: The GR hierarchy used by RASP
leaves of the tree correspond to SCFs. The details of
these two steps are provided in the subsequent sec-
tions, respectively2 .
3.1 Obtaining Grammatical Relations
Attempts to acquire verb subcategorization have
benefited from increasingly sophisticated parsers.
We have made use of the RASP toolkit (Briscoe and
Carroll, 2002) ? a modular statistical parsing sys-
tem which includes a tokenizer, tagger, lemmatiser,
and a wide-coverage unification-based tag-sequence
parser. The parser has several modes of operation;
we invoked it in a mode in which GRs with asso-
ciated probabilities are emitted even when a com-
plete analysis of the sentence could not be found. In
this mode there is wide coverage (over 98% of the
BNC receives at least a partial analysis (Carroll and
Briscoe, 2002)) which is useful in view of the in-
frequent occurrence of some of the SCFs, although
combining the results of competing parses may in
some cases result in an inconsistent or misleading
combination of GRs.
The parser uses a scheme of GRs between lemma-
tised lexical heads (Carroll et al, 1998a; Briscoe et
al., 2002). The relations are organized as a multiple-
inheritance subsumption hierarchy where each sub-
relation extends the meaning, and perhaps the argu-
ment structure, of its parents (figure 2). For descrip-
tions and examples of each relation, see (Carroll et
al., 1998a).
The dependency relationships which the GRs em-
body correspond closely to the head-complement
2In contrast to almost all earlier work, there was no filtering
stage involved in SCF acquisition. The classifier was designed
to operate with high precision, so filtering was less necessary.
616
26
6
6
6
6
6
4
SUBJECT NP 1 ,
ADJ-COMPS
*
PP
"
PVAL ?for?
NP 3
#
,
VP
2
6
6
4
MOOD to-infinitive
SUBJECT 3
OMISSION 1
3
7
7
5
+
3
7
7
7
7
7
7
5
Figure 3: Feature structure for SCF
adj-obj-for-to-inf
(|These:1_DD2| |example+s:2_NN2| |of:3_IO|
|animal:4_JJ| |senses:5_NN2| |be+:6_VBR|
|relatively:7_RR| |easy:8_JJ| |for:9_IF|
|we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xmod(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
Figure 4: GRs from RASP for adj-obj-for-to-inf
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of ?easy?
in the sentence:
These examples of animal senses are rel-
atively easy for us to comprehend as they
are not too far removed from our own ex-
perience.
According to the COMLEX classification, this is an
example of the frame adj-obj-for-to-inf, shown
in figure 3, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence (the full output includes 87 weighted GRs)
is shown in figure 43.
Each instantiated GR in figure 4 corresponds to
one or more parts of the feature structure in figure
3. xcomp( be[6] easy[8]) establishes be[6] as
the head of the VP in which easy[8] occurs as a
complement. The first (PP)-complement is ?for us?,
as indicated by ncmod(for[9] easy[8] we+[10]),
with ?for? as PFORM and we+ (?us?) as NP. The
second complement is represented by xmod(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
NP headed by ?examples? is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) corresponds to
the coindexation marked by 3 : the subject of the
3The format is slightly more complicated than that shown
in (Carroll et al, 1998a): each argument that corresponds to a
word consists of three parts: the lexeme, the part of speech tag,
and the position (index) of the word in the sentence.
xcomp(_, [*;1;be-verb], ?)
xmod([to;*;to], 1, [*;2;vv0])
ncsubj(1, [*;3;noun/pronoun], _)
ncmod([for;*;if], ?, [*;4;noun/pronoun])
ncsubj(2, 4)
Figure 5: A pattern to match the frame
adj-obj-for-to-inf
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
3.2 SCF Classifier
3.2.1 SCF Frames
We used for our classifier a modified version of
the fairly extensive COMLEX frameset, including 30
SCFs. The COMLEX frameset includes mutually in-
consistent frames, such as sentential complement
with obligatory complementiser that and sentential
complement with optional that. We modified the
frameset so that an adjective can legitimately instan-
tiate any combination of frames, which simplifies
classification. We also added simple-predicative
and attributive SCFs to the set, since these ac-
count for a substantial proportion of frame instances.
Finally, frames which could only be distinguished
by information not retained in the GRs scheme of the
current version of the shallow parser were merged
(e.g. the COMLEX frames adj-subj-to-inf-rs
(?She was kind to invite me?) and adj-to-inf (?She
was able to climb the mountain?)).
3.2.2 Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against var-
ious patterns. The patterns were developed by a
combination of knowledge of the GRs and examin-
ing a set of training sentences to determine which re-
lations were actually emitted by the parser for each
SCF. The data used during development consisted
of the sentences in the BNC in which one of the 23
adjectives4 given as examples for SCFs in (Macleod
4The adjectives used for training were: able, anxious, ap-
parent, certain, convenient, curious, desirable, disappointed,
easy, happy, helpful, imperative, impractical, insistent, kind,
obvious, practical, preferable, probable, ridiculous, unaware,
uncertain and unclear.
617
et al, 1998) occur.
In our pattern matching language a pattern is a
disjunction of sets of partially instantiated GRs with
logic variables (slots) in place of indices, augmented
by ordering constraints that restrict the possible in-
stantiations of slots. A match is considered success-
ful if the set of GRs can be unified with any of the
disjuncts. Unification of a sentence-relation and a
pattern-relation occurs when there is a one-to-one
correspondence between sentence elements and pat-
tern elements that includes a mapping from slots to
indices (a substitution), and where atomic elements
in corresponding positions share a common subtype.
Figure 5 shows a pattern for matching the SCF
adj-obj-for-to-inf. For a match to suc-
ceed there must be GRs associated with the sen-
tence that match each part of the pattern. Each ar-
gument matches either anything at all (*), the ?cur-
rent? adjective (?), an empty GR argument ( ), a
[word;id;part-of-speech] 3-tuple or a nu-
meric id. In a successful match, equal ids in different
parts of the pattern must match the same word posi-
tion, and distinct ids must match different positions.
The various patterns are arranged in a tree, where
a parent node contains the elements common to all
of its children. This kind of once-only representa-
tion of particular features, together with the succes-
sive refinements provided by child nodes reflects the
organization of inheritance-based lexica. The inher-
itance structure naturally involves multiple inheri-
tance, since each frame typically includes multiple
features (such as the presence of a to-infinitive
complement or an expletive subject argument) inher-
ited from abstract parent classes, and each feature is
instantiated in several frames.
The tree structure also improves the efficiency of
the pattern matching process, which then occurs in
stages: at each matching node the classifier attempts
to match a set of relations with each child pattern
to yield a substitution that subsumes the substitution
resulting from the parent match.
Both the patterns and the pattern language itself
underwent successive refinements after investigation
of the performance on training data made it increas-
ingly clear what sort of distinctions were useful to
express. The initial pattern language had no slots; it
was easy to understand and implement, but insuffi-
ciently expressive. The final refinement was the ad-
unspecified 285 improbable 350
unsure 570 doubtful 1147
generous 2052 sure 13591
difficult 18470 clear 19617
important 33303
Table 1: Test adjectives and frequencies in the BNC
dition of ordering constraints between instantiated
slots, which are indispensable for detecting, e.g., ex-
traposition.
4 Experimental Evaluation
4.1 Data
In order to evaluate the system we selected a set of
9 adjectives which between them could instantiate
all of the frames. The test set was intentionally kept
fairly small for these first experiments with adjec-
tival SCF acquisition so that we could carry out a
thorough evaluation of all the test instances. We ex-
cluded the adjectives used during development and
adjectives with fewer than 200 instances in the cor-
pus. The final test set, together with their frequen-
cies in the tagged version of the BNC, is shown in ta-
ble 1. For each adjective we extracted 200 sentences
(evenly spaced throughout the BNC) which we pro-
cessed using the SCF acquisition system described in
the previous section.
4.2 Method
4.2.1 Annotation Tool and Gold Standard
Our gold standard was human-annotated data.
Two annotators associated a SCF with each sen-
tence/adjective pair in the test data. To alleviate the
process we developed a program which first uses re-
liable heuristics to reduce the number of SCF choices
and then allows the annotator to select the preferred
choice with a single mouse click in a browser win-
dow. The heuristics reduced the average number
of SCFs presented alongside each sentence from 30
to 9. Through the same browser interface we pro-
vided annotators with information and instructions
(with links to COMLEX documentation), the ability
to inspect and review previous decisions and deci-
sion summaries5 and an option to record that partic-
5The varying number of SCFs presented to the user and the
ability to revisit previous decisions precluded accurate measure-
618
Figure 6: Sample classification screen for web an-
notation tool
ular sentences could not be classified (which is use-
ful for further system development, as discussed in
section 5). A screenshot is shown in figure 6. The
resulting annotation revealed 19 of the 30 SCFs in
the test data.
4.2.2 Evaluation Measures
We use the standard evaluation metrics: type and
token precision, recall and F-measure. Token recall
is the proportion of annotated (sentence, frame) pairs
that the system recovered correctly. Token precision
is the proportion of classified (sentence, frame) pairs
that were correct. Type precision and type recall are
analogously defined for (adjective, frame) pairs. The
F-measure (? = 1) is a weighted combination of
precision and recall.
4.3 Results
Running the system on the test data yielded the re-
sults summarised in table 2. The greater expres-
siveness of the final pattern language resulted in a
classifier that performed better than the ?regression?
versions which ignored either ordering constraints,
or both ordering constraints and slots. As expected,
removing features from the classifier translated di-
rectly into degraded accuracy. The performance of
the best classifier (67.8% F-measure) is quite simi-
lar to that of the best current verbal SCF acquisition
systems (e.g. (Korhonen, 2002)).
Results for individual adjectives are given in table
3. The first column shows the number of SCFs ac-
quired for each adjective, ranging from 2 for unspec-
ments of inter-annotator agreement, but this was judged less im-
portant than the enhanced ease of use arising from the reduced
set of choices.
Type performance
System Precision Recall F
Final 69.6 66.1 67.8
No order constraints 67.3 62.7 64.9
No slots 62.7 51.4 56.5
Token performance
System Precision Recall F
Final 63.0 70.5 66.5
No order constraints 58.8 68.3 63.2
No slots 58.3 67.6 62.6
Table 2: Overall performance of the classifier and of
regression systems with restricted pattern-matching
ified to 11 for doubtful. Looking at the F-measure,
the best performing adjectives are unspecified, diffi-
cult and sure (80%) and the worst performing unsure
(50%) and and improbable (60%).
There appears to be no obvious connection be-
tween performance figures and the number of ac-
quired SCF types; differences are rather due to the
difficulty of detecting individual SCF types ? an is-
sue directly related to data sparsity.
Despite the size of the BNC, 5 SCFs were not
seen at all, either for the test adjectives or for any
others. Frames involving to-infinitive complements
were particularly rare: 4 such SCFs had no exam-
ples in the corpus and a further 3 occurred 5 times or
fewer in the test data. It is more difficult to develop
patterns for SCFs that occur infrequently, and the few
instances of such SCFs are unlikely to include a set
of GRs that is adequate for classification. The ef-
fect on the results was clear: of the 9 SCFs which
the classifier did not correctly recognise at all, 4 oc-
curred 5 times or fewer in the test data and a further
2 occurred 5?10 times.
The most common error made by the clas-
sifier was to mistake a complex frame (e.g.
adj-obj-for-to-inf, or to-inf-wh-adj)
for simple-predicative, which subsumes all
such frames. This occurred whenever the GRs emit-
ted by the parser failed to include any information
about the complements of the adjective.
5 Discussion
Data sparsity is perhaps the greatest hindrance both
to recovering adjectival subcategorization and to
lexical acquisition in general. In the future, we plan
to carry out experiments with a larger set of adjec-
619
Adjective SCFs Precision Recall F-measure
unspecified 2 66.7 100.0 80.0
generous 3 60.0 100.0 75.0
improbable 5 60.0 60.0 60.0
unsure 6 50.0 50.0 50.0
important 7 55.6 71.4 62.5
clear 8 83.3 62.5 71.4
difficult 8 85.7 75.0 80.0
sure 9 100.0 66.7 80.0
doubtful 11 66.7 54.5 60.0
Table 3: SCF count and classifier performance for
each adjective.
tives using more data (possibly from several corpora
and the web) to determine how severe this problem
is for adjectives. One possible way to address the
problem is to smooth the acquired SCF distributions
using SCF ?back-off? (probability) estimates based
on lexical classes of adjectives in the manner pro-
posed by (Korhonen, 2002). This helps to correct the
acquired distributions and to detect low frequency
and unseen SCFs.
However, our experiment also revealed other
problems which require attention in the future.
One such is that GRs output by RASP (the ver-
sion we used in our experiments) do not re-
tain certain distinctions which are essential for
distinguishing particular SCFs. For example,
a sentential complement of an adjective with
a that-complementiser should be annotated with
ccomp(that, adjective, verbal-head), but this
relation (with that as the type argument) does not
occur in the parsed BNC. As a consequence the clas-
sifier is unable to distinguish the frame.
Another problem arises from the fact that our cur-
rent classifier operates on a predefined set of SCFs.
The COMLEX SCFs, from which ours were derived,
are extremely incomplete. Almost a quarter (477 of
1931) of sentences were annotated as ?undefined?.
For example, while there are SCFs for sentential
and infinitival complement in subject position with
what6, there is no SCF for the case with a what-
prefixed complement in object position, where the
subject is an NP. The lack is especially perplexing,
because COMLEX does include the corresponding
SCFs for verbs. There is a frame for ?He wondered
6(adj-subj-what-s: ?What he will do is uncertain?;
adj-subj-what-to-inf: ?What to do was unclear?), to-
gether with the extraposed versions (extrap-adj-what-s
and extrap-adj-what-to-inf).
what to do? (what-to-inf), but none for ?He was
unsure what to do?.
While we can easily extend the current frame-
set by looking for further SCF types from dictio-
naries and from among the corpus occurrences la-
belled by our annotators as unclassified, we also plan
to extend the classifier to automatically induce pre-
viously unseen frames from data. A possible ap-
proach is to use restricted generalization on sets of
GRs to group similar sentences together. General-
ization (anti-unification) is an intersection operation
on two structures which retains the features common
to both; generalization over the sets of GRs associ-
ated with the sentences which instantiate a particular
frame can produce a pattern such as we used for clas-
sification in the experiments described above. This
approach also offers the possibility of associating
confidence levels with each pattern, corresponding
to the degree to which the generalized pattern cap-
tures the features common to the members of the
associated class. It is possible that frames could
be induced by grouping sentences according to the
?best? (e.g. most information-preserving) general-
izations for various combinations, but it is not clear
how this can be implemented with acceptable effi-
ciency.
The hierarchical approach described in this paper
may also helpful in the discovery of new frames:
missing combinations of parent classes can be ex-
plored readily, and it may be possible to combine the
various features in an SCF feature structure to gen-
erate example sentences which a human could then
inspect to judge grammaticality.
6 Conclusion
We have described a novel system for automati-
cally acquiring adjectival subcategorization and as-
sociated frequency information from corpora, along
with an annotation tool for producing training and
test data for the task. The acquisition system, which
is capable of distinguishing 30 SCF types, performs
sophisticated pattern matching on sets of GRs pro-
duced by a robust statistical parser. The informa-
tion provided by GRs closely matches the structure
that subcategorization acquisition seeks to recover.
The figures reported demonstrate the feasibility of
the approach: our classifier achieved 70% type pre-
620
cision and 66% type recall on the test data. The dis-
cussion suggests several ways in which the system
may be improved, refined and extended in the fu-
ture.
Acknowledgements
We would like to thank Ann Copestake for all her
help during this work.
References
B. Boguraev, J. Carroll, E. Briscoe, D. Carter, and
C. Grover. 1987. The derivation of a grammatically-
indexed lexicon from the Longman Dictionary of Con-
temporary English. In Proceedings of the 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 193?200, Stanford, CA.
Michael R. Brent. 1991. Automatic acquisition of sub-
categorization frames from untagged text. In Meet-
ing of the Association for Computational Linguistics,
pages 209?214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction
of Subcategorization from Corpora. In Proceedings
of the 5th Conference on Applied Natural Language
Processing, Washington DC, USA.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 1499?1504, Las Pal-
mas, Canary Islands, May.
E. Briscoe, J. Carroll, Jonathan Graham, and Ann Copes-
take. 2002. Relational evaluation schemes. In Pro-
ceedings of the Beyond PARSEVAL Workshop at the
3rd International Conference on Language Resources
and Evaluation, pages 4?8, Las Palmas, Gran Canaria.
Lou Burnard, 1995. The BNC Users Reference Guide.
British National Corpus Consortium, Oxford, May.
J. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 134?140, Taipei, Taiwan.
Glenn Carroll and Mats Rooth. 1998. Valence induction
with a head-lexicalized pcfg. In Proc. of the 3rd Con-
ference on Empirical Methods in Natural Language
Processing, Granada, Spain.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998a. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, pages 447?454, Granada,
Spain.
John Carroll, Guido Minnen, and Edward Briscoe.
1998b. Can Subcategorisation Probabilities Help
a Statistical Parser? In Proceedings of the 6th
ACL/SIGDAT Workshop on Very Large Corpora, pages
118?126, Montreal, Canada. Association for Compu-
tational Linguistics.
Eva Esteve Ferrer. 2004. Towards a Semantic Clas-
sification of Spanish Verbs Based on Subcategorisa-
tion Information. In ACL Student Research Workshop,
Barcelona, Spain.
Dan Flickinger and John Nerbonne. 1992. Inheritance
and complementation: A case study of easy adjec-
tives and related nouns. Computational Linguistics,
18(3):269?309.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of Case Frame Dictionary for Robust Japanese
Case Analysis. In 19th International Conference on
Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering Polysemic Subcategorization Frame
Distributions Semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
Anna Korhonen. 2002. Subcategorization acquisition.
Ph.D. thesis, University of Cambridge Computer Lab-
oratory, February.
Catherine Macleod, Ralph Grishman, and Adam Meyers,
1998. COMLEX Syntax Reference Manual. Computer
Science Department, New York University.
Christopher D. Manning. 1993. Automatic Acquisition
of a Large Subcategorization Dictionary from Cor-
pora. In Meeting of the Association for Computational
Linguistics, pages 235?242.
S. Schulte im Walde and C. Brew. 2002. Inducing
german semantic verb classes from purely syntactic
subcategorisation information. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadephia, USA.
Mihai Surdeanu, Sanda Harabagiu, JohnWilliams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics, Sapporo.
Susanne Rohen Wolff, Catherine Macleod, and Adam
Meyers, 1998. COMLEX Word Classes Manual. Com-
puter Science Department, New York University ,
June.
621
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating the Accuracy of an Unlexicalized
Statistical Parser on the PARC DepBank
Ted Briscoe
Computer Laboratory
University of Cambridge
John Carroll
School of Informatics
University of Sussex
Abstract
We evaluate the accuracy of an unlexi-
calized statistical parser, trained on 4K
treebanked sentences from balanced data
and tested on the PARC DepBank. We
demonstrate that a parser which is compet-
itive in accuracy (without sacrificing pro-
cessing speed) can be quickly tuned with-
out reliance on large in-domain manually-
constructed treebanks. This makes it more
practical to use statistical parsers in ap-
plications that need access to aspects of
predicate-argument structure. The com-
parison of systems using DepBank is not
straightforward, so we extend and validate
DepBank and highlight a number of repre-
sentation and scoring issues for relational
evaluation schemes.
1 Introduction
Considerable progress has been made in accu-
rate statistical parsing of realistic texts, yield-
ing rooted, hierarchical and/or relational repre-
sentations of full sentences. However, much
of this progress has been made with systems
based on large lexicalized probabilistic context-
free like (PCFG-like) models trained on the Wall
Street Journal (WSJ) subset of the Penn Tree-
Bank (PTB). Evaluation of these systems has been
mostly in terms of the PARSEVAL scheme using
tree similarity measures of (labelled) precision and
recall and crossing bracket rate applied to section
23 of the WSJ PTB. (See e.g. Collins (1999) for
detailed exposition of one such very fruitful line
of research.)
We evaluate the comparative accuracy of an un-
lexicalized statistical parser trained on a smaller
treebank and tested on a subset of section 23 of
the WSJ using a relational evaluation scheme. We
demonstrate that a parser which is competitive
in accuracy (without sacrificing processing speed)
can be quickly developed without reliance on large
in-domain manually-constructed treebanks. This
makes it more practical to use statistical parsers in
diverse applications needing access to aspects of
predicate-argument structure.
We define a lexicalized statistical parser as one
which utilizes probabilistic parameters concerning
lexical subcategorization and/or bilexical relations
over tree configurations. Current lexicalized sta-
tistical parsers developed, trained and tested on
PTB achieve a labelled F1-score ? the harmonic
mean of labelled precision and recall ? of around
90%. Klein and Manning (2003) argue that such
results represent about 4% absolute improvement
over a carefully constructed unlexicalized PCFG-
like model trained and tested in the same man-
ner.1 Gildea (2001) shows that WSJ-derived bilex-
ical parameters in Collins? (1999) Model 1 parser
contribute less than 1% to parse selection accu-
racy when test data is in the same domain, and
yield no improvement for test data selected from
the Brown Corpus. Bikel (2004) shows that, in
Collins? (1999)Model 2, bilexical parameters con-
tribute less than 0.5% to accuracy on in-domain
data while lexical subcategorization-like parame-
ters contribute just over 1%.
Several alternative relational evaluation
schemes have been developed (e.g. Carroll et al,
1998; Lin, 1998). However, until recently, no
WSJ data has been carefully annotated to support
relational evaluation. King et al (2003) describe
the PARC 700 Dependency Bank (hereinafter
DepBank), which consists of 700 WSJ sentences
randomly drawn from section 23. These sentences
have been annotated with syntactic features and
with bilexical head-dependent relations derived
from the F-structure representation of Lexical
Functional Grammar (LFG). DepBank facilitates
1Klein and Manning retained some functional tag infor-
mation from PTB, so it could be argued that their model re-
mains ?mildly? lexicalized since functional tags encode some
subcategorization information.
41
comparison of PCFG-like statistical parsers
developed from the PTB with other parsers whose
output is not designed to yield PTB-style trees,
using an evaluation which is closer to the protypi-
cal parsing task of recovering predicate-argument
structure.
Kaplan et al (2004) compare the accuracy and
speed of the PARC XLE Parser to Collins? Model
3 parser. They develop transformation rules for
both, designed to map native output to a subset of
the features and relations in DepBank. They com-
pare performance of a grammatically cut-down
and complete version of the XLE parser to the
publically available version of Collins? parser.
One fifth of DepBank is held out to optimize the
speed and accuracy of the three systems. They
conclude from the results of these experiments that
the cut-down XLE parser is two-thirds the speed
of Collins? Model 3 but 12% more accurate, while
the complete XLE system is 20% more accurate
but five times slower. F1-score percentages range
from the mid- to high-70s, suggesting that the re-
lational evaluation is harder than PARSEVAL.
Both Collins? Model 3 and the XLE Parser use
lexicalized models for parse selection trained on
the rest of the WSJ PTB. Therefore, although Ka-
plan et al demonstrate an improvement in accu-
racy at some cost to speed, there remain questions
concerning viability for applications, at some re-
move from the financial news domain, for which
substantial treebanks are not available. The parser
we deploy, like the XLE one, is based on a
manually-defined feature-based unification gram-
mar. However, the approach is somewhat differ-
ent, making maximal use of more generic struc-
tural rather than lexical information, both within
the grammar and the probabilistic parse selection
model. Here we compare the accuracy of our
parser with Kaplan et al?s results, by repeating
their experiment with our parser. This compari-
son is not straightforward, given both the system-
specific nature of some of the annotation in Dep-
Bank and the scoring reported. We, therefore, ex-
tend DepBank with a set of grammatical relations
derived from our own system output and highlight
how issues of representation and scoring can affect
results and their interpretation.
In ?2, we describe our development method-
ology and the resulting system in greater detail.
?3 describes the extended Depbank that we have
developed and motivates our additions. ?2.4 dis-
cusses how we trained and tuned our current sys-
tem and describes our limited use of information
derived from WSJ text. ?4 details the various ex-
periments undertaken with the extended DepBank
and gives detailed results. ?5 discusses these re-
sults and proposes further lines of research.
2 Unlexicalized Statistical Parsing
2.1 System Architecture
Both the XLE system and Collins? Model 3 pre-
process textual input before parsing. Similarly,
our baseline system consists of a pipeline of mod-
ules. First, text is tokenized using a deterministic
finite-state transducer. Second, tokens are part-of-
speech and punctuation (PoS) tagged using a 1st-
order Hidden Markov Model (HMM) utilizing a
lexicon of just over 50K words and an unknown
word handling module. Third, deterministic mor-
phological analysis is performed on each token-
tag pair with a finite-state transducer. Fourth, the
lattice of lemma-affix-tags is parsed using a gram-
mar over such tags. Finally, the n-best parses are
computed from the parse forest using a probabilis-
tic parse selection model conditioned on the struc-
tural parse context. The output of the parser can be
displayed as syntactic trees, and/or factored into a
sequence of bilexical grammatical relations (GRs)
between lexical heads and their dependents.
The full system can be extended in a variety of
ways ? for example, by pruning PoS tags but al-
lowing multiple tag possibilities per word as in-
put to the parser, by incorporating lexical subcate-
gorization into parse selection, by computing GR
weights based on the proportion and probability
of the n-best analyses yielding them, and so forth
? broadly trading accuracy and greater domain-
dependence against speed and reduced sensitivity
to domain-specific lexical behaviour (Briscoe and
Carroll, 2002; Carroll and Briscoe, 2002; Watson
et al, 2005; Watson, 2006). However, in this pa-
per we focus exclusively on the baseline unlexical-
ized system.
2.2 Grammar Development
The grammar is expressed in a feature-based, uni-
fication formalism. There are currently 676 phrase
structure rule schemata, 15 feature propagation
rules, 30 default feature value rules, 22 category
expansion rules and 41 feature types which to-
gether define 1124 compiled phrase structure rules
in which categories are represented as sets of fea-
42
tures, that is, attribute-value pairs, possibly with
variable values, possibly bound between mother
and one or more daughter categories. 142 of the
phrase structure schemata are manually identified
as peripheral rather than core rules of English
grammar. Categories are matched using fixed-
arity term unification at parse time.
The lexical categories of the grammar consist
of feature-based descriptions of the 149 PoS tags
and 13 punctuation tags (a subset of the CLAWS
tagset, see e.g. Sampson, 1995) which constitute
the preterminals of the grammar. The number
of distinct lexical categories associated with each
preterminal varies from 1 for some function words
through to around 35 as, for instance, tags for main
verbs are associated with a VSUBCAT attribute tak-
ing 33 possible values. The grammar is designed
to enumerate possible valencies for predicates by
including separate rules for each pattern of pos-
sible complementation in English. The distinc-
tion between arguments and adjuncts is expressed
by adjunction of adjuncts to maximal projections
(XP ? XP Adjunct) as opposed to government of
arguments (i.e. arguments are sisters within X1
projections; X1 ? X0 Arg1. . . ArgN).
Each phrase structure schema is associated with
one or more GR specifications which can be con-
ditioned on feature values instantiated at parse
time and which yield a rule-to-rule mapping from
local trees to GRs. The set of GRs associated with
a given derivation define a connected, directed
graph with individual nodes representing lemma-
affix-tags and arcs representing named grammati-
cal relations. The encoding of this mapping within
the grammar is similar to that of F-structure map-
ping in LFG. However, the connected graph is not
constructed and completeness and coherence con-
straints are not used to filter the phrase structure
derivation space.
The grammar finds at least one parse rooted in
the start category for 85% of the Susanne treebank,
a 140K word balanced subset of the Brown Cor-
pus, which we have used for development (Samp-
son, 1995). Much of the remaining data consists
of phrasal fragments marked as independent text
sentences, for example in dialogue. Grammati-
cal coverage includes the majority of construction
types of English, however the handling of some
unbounded dependency constructions, particularly
comparatives and equatives, is limited because of
the lack of fine-grained subcategorization infor-
mation in the PoS tags and by the need to balance
depth of analysis against the size of the deriva-
tion space. On the Susanne corpus, the geometric
mean of the number of analyses for a sentence of
length n is 1.31n. The microaveraged F1-score for
GR extraction on held-out data from Susanne is
76.5% (see section 4.2 for details of the evaluation
scheme).
The system has been used to analyse about 150
million words of English text drawn primarily
from the PTB, TREC, BNC, and Reuters RCV1
datasets in connection with a variety of projects.
The grammar and PoS tagger lexicon have been
incrementally improved by manually examining
cases of parse failure on these datasets. How-
ever, the effort invested amounts to a few days?
effort for each new dataset as opposed to the main
grammar development effort, centred on Susanne,
which has extended over some years and now
amounts to about 2 years? effort (see Briscoe, 2006
for further details).
2.3 Parser
To build the parsing module, the unification gram-
mar is automatically converted into an atomic-
categoried context free ?backbone?, and a non-
deterministic LALR(1) table is constructed from
this, which is used to drive the parser. The residue
of features not incorporated into the backbone
are unified on each rule application (reduce ac-
tion). In practice, the parser takes average time
roughly quadratic in the length of the input to cre-
ate a packed parse forest represented as a graph-
structured stack. The statistical disambiguation
phase is trained on Susanne treebank bracketings,
producing a probabilistic generalized LALR(1)
parser (e.g. Inui et al, 1997) which associates
probabilities with alternative actions in the LR ta-
ble.
The parser is passed as input the sequence of
most probable lemma-affix-tags found by the tag-
ger. During parsing, probabilities are assigned
to subanalyses based on the the LR table actions
that derived them. The n-best (i.e. most proba-
ble) parses are extracted by a dynamic program-
ming procedure over subanalyses (represented by
nodes in the parse forest). The search is effi-
cient since probabilities are associated with single
nodes in the parse forest and no weight function
over ancestor or sibling nodes is needed. Proba-
bilities capture structural context, since nodes in
43
the parse forest partially encode a configuration of
the graph-structured stack and lookahead symbol,
so that, unlike a standard PCFG, the model dis-
criminates between derivations which only differ
in the order of application of the same rules and
also conditions rule application on the PoS tag of
the lookahead token.
When there is no parse rooted in the start cat-
egory, the parser returns a connected sequence
of partial parses which covers the input based
on subanalysis probability and a preference for
longer and non-lexical subanalysis combinations
(e.g. Kiefer et al, 1999). In these cases, the GR
graph will not be fully connected.
2.4 Tuning and Training Method
The HMM tagger has been trained on 3M words
of balanced text drawn from the LOB, BNC and
Susanne corpora, which are available with hand-
corrected CLAWS tags. The parser has been
trained from 1.9K trees for sentences from Su-
sanne that were interactively parsed to manually
obtain the correct derivation, and also from 2.1K
further sentences with unlabelled bracketings de-
rived from the Susanne treebank. These brack-
etings guide the parser to one or possibly sev-
eral closely-matching derivations and these are
used to derive probabilities for the LR table us-
ing (weighted) Laplace estimation. Actions in the
table involving rules marked as peripheral are as-
signed a uniform low prior probability to ensure
that derivations involving such rules are consis-
tently lower ranked than those involving only core
rules.
To improve performance onWSJ text, we exam-
ined some parse failures from sections other than
section 23 to identify patterns of consistent fail-
ure. We then manually modified and extended the
grammar with a further 6 rules, mostly to handle
cases of indirect and direct quotation that are very
common in this dataset. This involved 3 days?
work. Once completed, the parser was retrained
on the original data. A subsequent limited inspec-
tion of top-ranked parses led us to disable 6 ex-
isting rules which applied too freely to the WSJ
text; these were designed to analyse auxiliary el-
lipsis which appears to be rare in this genre. We
also catalogued incorrect PoS tags fromWSJ parse
failures and manually modified the tagger lexicon
where appropriate. These modifications mostly
consisted of adjusting lexical probabilities of ex-
tant entries with highly-skewed distributions. We
also added some tags to extant entries for infre-
quent words. These modifications took a further
day. The tag transition probabilities were not rees-
timated. Thus, we have made no use of the PTB
itself and only limited use of WSJ text.
This method of grammar and lexicon devel-
opment incrementally improves the overall per-
formance of the system averaged across all the
datasets that it has been applied to. It is very
likely that retraining the PoS tagger on the WSJ
and retraining the parser using PTB would yield
a system which would perform more effectively
on DepBank. However, one of our goals is to
demonstrate that an unlexicalized parser trained
on a modest amount of annotated text from other
sources, coupled to a tagger also trained on
generic, balanced data, can perform competitively
with systems which have been (almost) entirely
developed and trained using PTB, whether or not
these systems deploy hand-crafted grammars or
ones derived automatically from treebanks.
3 Extending and Validating DepBank
DepBank was constructed by parsing the selected
section 23 WSJ sentences with the XLE system
and outputting syntactic features and bilexical re-
lations from the F-structure found by the parser.
These features and relations were subsequently
checked, corrected and extended interactively with
the aid of software tools (King et al, 2003).
The choice of relations and features is based
quite closely on LFG and, in fact, overlaps sub-
stantially with the GR output of our parser. Fig-
ure 1 illustrates some DepBank annotations used
in the experiment reported by Kaplan et al and
our hand-corrected GR output for the example
Ten of the nation?s governors meanwhile called
on the justices to reject efforts to limit abortions.
We have kept the GR representation simpler and
more readable by suppressing lemmatization, to-
ken numbering and PoS tags, but have left the
DepBank annotations unmodified.
The example illustrates some differences be-
tween the schemes. For instance, the subj and
ncsubj relations overlap as both annotations con-
tain such a relation between call(ed) and Ten), but
the GR annotation also includes this relation be-
tween limit and effort(s) and reject and justice(s),
while DepBank links these two verbs to a variable
pro. This reflects a difference of philosophy about
44
DepBank: obl(call?0, on?2)
stmt_type(call?0, declarative)
subj(call?0, ten?1)
tense(call?0, past)
number_type(ten?1, cardinal)
obl(ten?1, governor?35)
obj(on?2, justice?30)
obj(limit?7, abortion?15)
subj(limit?7, pro?21)
obj(reject?8, effort?10)
subj(reject?8, pro?27)
adegree(meanwhile?9, positive)
num(effort?10, pl)
xcomp(effort?10, limit?7)
GR: (ncsubj called Ten _)
(ncsubj reject justices _)
(ncsubj limit efforts _)
(iobj called on)
(xcomp to called reject)
(dobj reject efforts)
(xmod to efforts limit)
(dobj limit abortions)
(dobj on justices)
(det justices the)
(ta bal governors meanwhile)
(ncmod poss governors nation)
(iobj Ten of)
(dobj of governors)
(det nation the)
Figure 1: DepBank and GR annotations.
resolution of such ?understood? relations in differ-
ent constructions. Viewed as output appropriate to
specific applications, either approach is justifiable.
However, for evaluation, these DepBank relations
add little or no information not already specified
by the xcomp relations in which these verbs also
appear as dependents. On the other hand, Dep-
Bank includes an adjunct relation between mean-
while and call(ed), while the GR annotation treats
meanwhile as a text adjunct (ta) of governors, de-
limited by balanced commas, following Nunberg?s
(1990) text grammar but conveying less informa-
tion here.
There are also issues of incompatible tokeniza-
tion and lemmatization between the systems and
of differing syntactic annotation of similar infor-
mation, which lead to problems mapping between
our GR output and the current DepBank. Finally,
differences in the linguistic intuitions of the an-
notators and errors of commission or omission
on both sides can only be uncovered by manual
comparison of output (e.g. xmod vs. xcomp for
limit efforts above). Thus we reannotated the Dep-
Bank sentences with GRs using our current sys-
tem, and then corrected and extended this anno-
tation utilizing a software tool to highlight dif-
ferences between the extant annotations and our
own.2 This exercise, though time-consuming, un-
covered problems in both annotations, and yields
a doubly-annotated and potentially more valuable
resource in which annotation disagreements over
complex attachment decisions, for instance, can be
inspected.
The GR scheme includes one feature in Dep-
Bank (passive), several splits of relations in Dep-
Bank, such as adjunct, adds some of DepBank?s
featural information, such as subord form, as a
subtype slot of a relation (ccomp), merges Dep-
Bank?s oblique with iobj, and so forth. But it
does not explicitly include all the features of Dep-
Bank or even of the reduced set of semantically-
relevant features used in the experiments and eval-
uation reported in Kaplan et al. Most of these
features can be computed from the full GR repre-
sentation of bilexical relations between numbered
lemma-affix-tags output by the parser. For in-
stance, num features, such as the plurality of jus-
tices in the example, can be computed from the
full det GR (det justice+s NN2:4 the AT:3)
based on the CLAWS tag (NN2 indicating ?plu-
ral?) selected for output. The few features that can-
not be computed from GRs and CLAWS tags di-
rectly, such as stmt type, could be computed from
the derivation tree.
4 Experiments
4.1 Experimental Design
We selected the same 560 sentences as test data as
Kaplan et al, and all modifications that we made
to our system (see ?2.4) were made on the basis
of (very limited) information from other sections
of WSJ text.3 We have made no use of the further
140 held out sentences in DepBank. The results
we report below are derived by choosing the most
probable tag for each word returned by the PoS
tagger and by choosing the unweighted GR set re-
turned for the most probable parse with no lexical
information guiding parse ranking.
4.2 Results
Our parser produced rooted sentential analyses for
84% of the test items; actual coverage is higher
2The new version of DepBank along with evaluation
software is included in the current RASP distribution:
www.informatics.susx.ac.uk/research/nlp/rasp
3The PARC group kindly supplied us with the experimen-
tal data files they used to facilitate accurate reproduction of
this experiment.
45
Relation Precision Recall F1 P R F1 Relation
mod 75.4 71.2 73.3
ncmod 72.9 67.9 70.3
xmod 47.7 45.5 46.6
cmod 51.4 31.6 39.1
pmod 30.8 33.3 32.0
det 88.7 91.1 89.9
arg mod 71.9 67.9 69.9
arg 76.0 73.4 74.6
subj 80.1 66.6 72.7 73 73 73
ncsubj 80.5 66.8 73.0
xsubj 50.0 28.6 36.4
csubj 20.0 50.0 28.6
subj or dobj 82.1 74.9 78.4
comp 74.5 76.4 75.5
obj 78.4 77.9 78.1
dobj 83.4 81.4 82.4 75 75 75 obj
obj2 24.2 38.1 29.6 42 36 39 obj-theta
iobj 68.2 68.1 68.2 64 83 72 obl
clausal 63.5 71.6 67.3
xcomp 75.0 76.4 75.7 74 73 74
ccomp 51.2 65.6 57.5 78 64 70 comp
pcomp 69.6 66.7 68.1
aux 92.8 90.5 91.6
conj 71.7 71.0 71.4 68 62 65
ta 39.1 48.2 43.2
passive 93.6 70.6 80.5 80 83 82
adegree 89.2 72.4 79.9 81 72 76
coord form 92.3 85.7 88.9 92 93 93
num 92.2 89.8 91.0 86 87 86
number type 86.3 92.7 89.4 96 95 96
precoord form 100.0 16.7 28.6 100 50 67
pron form 92.1 91.9 92.0 88 89 89
prt form 71.1 58.7 64.3 72 65 68
subord form 60.7 48.1 53.6
macroaverage 69.0 63.4 66.1
microaverage 81.5 78.1 79.7 80 79 79
Table 1: Accuracy of our parser, and where
roughly comparable, the XLE as reported by King
et al
than this since some of the test sentences are el-
liptical or fragmentary, but in many cases are rec-
ognized as single complete constituents. Kaplan
et al report that the complete XLE system finds
rooted analyses for 79% of section 23 of the WSJ
but do not report coverage just for the test sen-
tences. The XLE parser uses several performance
optimizations which mean that processing of sub-
analyses in longer sentences can be curtailed or
preempted, so that it is not clear what proportion
of the remaining data is outside grammatical cov-
erage.
Table 1 shows accuracy results for each indi-
vidual relation and feature, starting with the GR
bilexical relations in the extended DepBank and
followed by most DepBank features reported by
Kaplan et al, and finally overall macro- and mi-
croaverages. The macroaverage is calculated by
taking the average of each measure for each indi-
vidual relation and feature; the microaverage mea-
sures are calculated from the counts for all rela-
tions and features.4 Indentation of GRs shows
degree of specificity of the relation. Thus, mod
scores are microaveraged over the counts for the
five fully specified modifier relations listed imme-
diately after it in Table 1. This allows comparison
of overall accuracy on modifiers with, for instance
overall accuracy on arguments. Figures in italics
to the right are discussed in the next section.
Kaplan et al?s microaveraged scores for
Collins? Model 3 and the cut-down and complete
versions of the XLE parser are given in Table 2,
along with the microaveraged scores for our parser
from Table 1. Our system?s accuracy results (eval-
uated on the reannotated DepBank) are better than
those for Collins and the cut-down XLE, and very
similar overall to the complete XLE (evaluated
on DepBank). Speed of processing is also very
competitive.5 These results demonstrate that a
statistical parser with roughly state-of-the-art ac-
curacy can be constructed without the need for
large in-domain treebanks. However, the perfor-
mance of the system, as measured by microrav-
eraged F1-score on GR extraction alone, has de-
clined by 2.7% over the held-out Susanne data,
so even the unlexicalized parser is by no means
domain-independent.
4.3 Evaluation Issues
The DepBank num feature on nouns is evalu-
ated by Kaplan et al on the grounds that it is
semantically-relevant for applications. There are
over 5K num features in DepBank so the overall
microaveraged scores for a system will be signifi-
cantly affected by accuracy on num. We expected
our system, which incorporates a tagger with good
empirical (97.1%) accuracy on the test data, to re-
cover this feature with 95% accuracy or better, as
it will correlate with tags NNx1 and NNx2 (where
?x? represents zero or more capitals in the CLAWS
4We did not compute the remaining DepBank features
stmt type, tense, prog or perf as these rely on information
that can only be extracted from the derivation tree rather than
the GR set.
5Processing time for our system was 61 seconds on one
2.2GHz Opteron CPU (comprising tokenization, tagging,
morphology, and parsing, including module startup over-
heads). Allowing for slightly different CPUs, this is 2.5?10
times faster than the Collins and XLE parsers, as reported by
Kaplan et al
46
System Eval corpus Precision Recall F1
Collins DepBank 78.3 71.2 74.6
Cut-down XLE DepBank 79.1 76.2 77.6
Complete XLE DepBank 79.4 79.8 79.6
Our system DepBank/GR 81.5 78.1 79.7
Table 2: Microaveraged overall scores from Kaplan et al and for our system.
tagset). However, DepBank treats the majority
of prenominal modifiers as adjectives rather than
nouns and, therefore, associates them with an ade-
gree rather than a num feature. The PoS tag se-
lected depends primarily on the relative lexical
probabilities of each tag for a given lexical item
recorded in the tagger lexicon. But, regardless
of this lexical decision, the correct GR is recov-
ered, and neither adegree(positive) or num(sg)
add anything semantically-relevant when the lex-
ical item is a nominal premodifier. A strategy
which only provided a num feature for nominal
heads would be both more semantically-relevant
and would also yield higher precision (95.2%).
However, recall (48.4%) then suffers against Dep-
Bank as noun premodifiers have a num feature.
Therefore, in the results presented in Table 1 we
have not counted cases where either DepBank or
our system assign a premodifier adegree(positive)
or num(sg).
There are similar issues with other DepBank
features and relations. For instance, the form of
a subordinator with clausal complements is anno-
tated as a relation between verb and subordina-
tor, while there is a separate comp relation be-
tween verb and complement head. The GR rep-
resentation adds the subordinator as a subtype of
ccomp recording essentially identical information
in a single relation. So evaluation scores based on
aggregated counts of correct decisions will be dou-
bled for a system which structures this informa-
tion as in DepBank. However, reproducing the ex-
act DepBank subord form relation from the GR
ccomp one is non-trivial because DepBank treats
modal auxiliaries as syntactic heads while the GR-
scheme treats the main verb as head in all ccomp
relations. We have not attempted to compensate
for any further such discrepancies other than the
one discussed in the previous paragraph. However,
we do believe that they collectively damage scores
for our system.
As King et al note, it is difficult to identify
such informational redundancies to avoid double-
counting and to eradicate all system specific bi-
ases. However, reporting precision, recall and F1-
scores for each relation and feature separately and
microaveraging these scores on the basis of a hi-
erarchy, as in our GR scheme, ameliorates many
of these problems and gives a better indication
of the strengths and weaknesses of a particular
parser, which may also be useful in a decision
about its usefulness for a specific application. Un-
fortunately, Kaplan et al do not report their re-
sults broken down by relation or feature so it is
not possible, for example, on the basis of the ar-
guments made above, to choose to compare the
performance of our system on ccomp to theirs for
comp, ignoring subord form. King et al do re-
port individual results for selected features and re-
lations from an evaluation of the complete XLE
parser on all 700 DepBank sentences with an al-
most identical overall microaveraged F1 score of
79.5%, suggesting that these results provide a rea-
sonably accurate idea of the XLE parser?s relative
performance on different features and relations.
Where we believe that the information captured
by a DepBank feature or relation is roughly com-
parable to that expressed by a GR in our extended
DepBank, we have included King et al?s scores
in the rightmost column in Table 1 for compari-
son purposes. Even if these features and relations
were drawn from the same experiment, however,
they would still not be exactly comparable. For in-
stance, as discussed in ?3 nearly half (just over 1K)
the DepBank subj relations include pro as one el-
ement, mostly double counting a corresponding
xcomp relation. On the other hand, our ta rela-
tion syntactically underspecifies many DepBank
adjunct relations. Nevertheless, it is possible to
see, for instance, that while both parsers perform
badly on second objects ours is worse, presumably
because of lack of lexical subcategorization infor-
mation.
47
5 Conclusions
We have demonstrated that an unlexicalized parser
with minimal manual modification for WSJ text ?
but no tuning of performance to optimize on this
dataset alne, and no use of PTB ? can achieve
accuracy competitive with parsers employing lex-
icalized statistical models trained on PTB.
We speculate that we achieve these results be-
cause our system is engineered to make minimal
use of lexical information both in the grammar and
in parse ranking, because the grammar has been
developed to constrain ambiguity despite this lack
of lexical information, and because we can com-
pute the full packed parse forest for all the test sen-
tences efficiently (without sacrificing speed of pro-
cessing with respect to other statistical parsers).
These advantages appear to effectively offset the
disadvantage of relying on a coarser, purely struc-
tural model for probabilistic parse selection. In fu-
ture work, we hope to improve the accuracy of the
system by adding lexical information to the statis-
tical parse selection component without exploiting
in-domain treebanks.
Clearly, more work is needed to enable more
accurate, informative, objective and wider com-
parison of extant parsers. More recent PTB-based
parsers show small improvements over Collins?
Model 3 using PARSEVAL, while Clark and Cur-
ran (2004) and Miyao and Tsujii (2005) report
84% and 86.7% F1-scores respectively for their
own relational evaluations on section 23 of WSJ.
However, it is impossible to meaningfully com-
pare these results to those reported here. The rean-
notated DepBank potentially supports evaluations
which score according to the degree of agreement
between this and the original annotation and/or de-
velopment of future consensual versions through
collaborative reannotation by the research com-
munity. We have also highlighted difficulties for
relational evaluation schemes and argued that pre-
senting individual scores for (classes of) relations
and features is both more informative and facili-
tates system comparisons.
6 References
Bikel, D.. 2004. Intricacies of Collins? parsing model, Com-
putational Linguistics, 30(4):479?512.
Briscoe, E.J.. 2006. An introduction to tag sequence gram-
mars and the RASP system parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria. 1499?1504.
Carroll, J. and E.J. Briscoe. 2002. High precision extraction
of grammatical relations. In Proceedings of the 19th Int.
Conf. on Computational Linguistics (COLING), Taipei,
Taiwan. 134?140.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser evalu-
ation: a survey and a new proposal. In Proceedings of the
1st International Conference on Language Resources and
Evaluation, Granada, Spain. 447?454.
Clark, S. and J. Curran. 2004. The importance of supertag-
ging for wide-coverage CCG parsing. In Proceedings of
the 20th International Conference on Computational Lin-
guistics (COLING-04), Geneva, Switzerland. 282?288.
Collins, M.. 1999. Head-driven Statistical Models for Nat-
ural Language Parsing. PhD Dissertation, Computer and
Information Science, University of Pennsylvania.
Gildea, D.. 2001. Corpus variation and parser performance.
In Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP?01), Pittsburgh, PA.
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga.
1997. A new formalization of probabilistic GLR parsing.
In Proceedings of the 5th International Workshop on Pars-
ing Technologies (IWPT?97), Boston, MA. 123?134.
Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasser-
man and R. Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of the
HLT Conference and the 4th Annual Meeting of the North
American Chapter of the ACL (HLT-NAACL?04), Boston,
MA.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf. 1999.
A bag of useful techniques for efficient and robust pars-
ing. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, University of
Maryland. 473?480.
King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan. 2003. The PARC700 Dependency Bank. In Pro-
ceedings of the 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), Budapest, Hungary.
Klein, D. and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, Sapporo,
Japan. 423?430.
Lin, D.. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of the Workshop at LREC?98 on The Eval-
uation of Parsing Systems, Granada, Spain.
Manning, C. and H. Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press, Cambridge,
MA.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proceedings
of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, MI. 83?90.
Nunberg, G.. 1990. The Linguistics of Punctuation. CSLI
Lecture Notes 18, Stanford, CA.
Sampson, G.. 1995. English for the Computer. Oxford Uni-
versity Press, Oxford, UK.
Watson, R.. 2006. Part-of-speech tagging models for parsing.
In Proceedings of the 9th Conference of Computational
Linguistics in the UK (CLUK?06), Open University, Mil-
ton Keynes.
Watson, R., J. Carroll and E.J. Briscoe. 2005. Efficient ex-
traction of grammatical relations. In Proceedings of the
9th Int. Workshop on Parsing Technologies (IWPT?05),
Vancouver, Ca..
48
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
           
The Second Release of the RASP System
Ted Briscoe? John Carroll? Rebecca Watson?
?Computer Laboratory, University of Cambridge, Cambridge CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
?Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We describe the new release of the RASP
(robust accurate statistical parsing) sys-
tem, designed for syntactic annotation of
free text. The new version includes a
revised and more semantically-motivated
output representation, an enhanced gram-
mar and part-of-speech tagger lexicon, and
a more flexible and semi-supervised train-
ing method for the structural parse ranking
model. We evaluate the released version
on the WSJ using a relational evaluation
scheme, and describe how the new release
allows users to enhance performance using
(in-domain) lexical information.
1 Introduction
The first public release of the RASP system
(Briscoe & Carroll, 2002) has been downloaded
by over 120 sites and used in diverse natural lan-
guage processing tasks, such as anaphora res-
olution, word sense disambiguation, identifying
rhetorical relations, resolving metonymy, detect-
ing compositionality in phrasal verbs, and diverse
applications, such as topic and sentiment classifi-
cation, text anonymisation, summarisation, infor-
mation extraction, and open domain question an-
swering. Briscoe & Carroll (2002) give further de-
tails about the first release. Briscoe (2006) pro-
vides references and more information about ex-
tant use of RASP and fully describes the modifi-
cations discussed more briefly here.
The new release, which is free for all non-
commercial use1, is designed to address several
weaknesses of the extant toolkit. Firstly, all mod-
ules have been incrementally improved to cover a
greater range of text types. Secondly, the part-of-
speech tagger lexicon has been semi-automatically
enhanced to better deal with rare or unseen be-
haviour of known words. Thirdly, better facil-
ities have been provided for user customisation.
1See http://www.informatics.susx.ac.uk/research/nlp/rasp/
for licence and download details.
?raw text
Tokeniser
?
PoS Tagger
?
Lemmatiser
?
Parser/Grammar
?
Parse Ranking Model
Figure 1: RASP Pipeline
Fourthly, the grammatical relations output has
been redesigned to better support further process-
ing. Finally, the training and tuning of the parse
ranking model has been made more flexible.
2 Components of the System
RASP is implemented as a series of modules writ-
ten in C and Common Lisp, which are pipelined,
working as a series of Unix-style filters. RASP
runs on Unix and is compatible with most C com-
pilers and Common Lisp implementations. The
public release includes Lisp and C executables for
common 32- and 64-bit architectures, shell scripts
for running and parameterising the system, docu-
mentation, and so forth. An overview of the sys-
tem is given in Figure 1.
2.1 Sentence Boundary Detection and
Tokenisation
The system is designed to take unannotated text or
transcribed (and punctuated) speech as input, and
not simply to run on pre-tokenised input such as
that typically found in corpora produced for NLP
purposes. Sentence boundary detection and to-
kenisation modules, implemented as a set of deter-
ministic finite-state rules in Flex (an open source
re-implementation of the original Unix Lex utility)
77
         
 PPPPP
QQQ
HHH
!!!!
HHH
QQQQ
!!!!!!!!      
(((((((

hhhhhhhhh
PPPPPSS

        
aaaa
XXXXXXX
hhhhhhhhhh
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj or dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 2: The GR hierarchy
and compiled into C, convert raw ASCII (or Uni-
code in UTF-8) data into a sequence of sentences
in which, for example punctuation tokens are sep-
arated from words by spaces, and so forth.
Since the first release this part of the system
has been incrementally improved to deal with a
greater variety of text types, and handle quo-
tation appropriately. Users are able to modify
the rules used and recompile the modules. All
RASP modules now accept XML mark up (with
certain hard-coded assumptions) so that data can
be pre-annotated?for example to identify named
entities?before being passed to the tokeniser, al-
lowing for more domain-dependent, potentially
multiword tokenisation and classification prior to
parsing if desired (e.g. Vlachos et al, 2006), as
well as, for example, handling of text with sen-
tence boundaries already determined.
2.2 PoS and Punctuation Tagging
The tokenised text is tagged with one of 150
part-of-speech (PoS) and punctuation labels (de-
rived from the CLAWS tagset). This is done
using a first-order (?bigram?) hidden markov
model (HMM) tagger implemented in C (Elwor-
thy, 1994) and trained on the manually-corrected
tagged versions of the Susanne, LOB and (sub-
set of) BNC corpora. The tagger has been aug-
mented with an unknown word model which per-
forms well under most circumstances. However,
known but rare words often caused problems as
tags for all realisations were rarely present. A se-
ries of manually developed rules has been semi-
automatically applied to the lexicon to amelio-
rate this problem by adding further tags with low
counts to rare words. The new tagger has an accu-
racy of just over 97% on the DepBank part of sec-
tion 23 of the Wall Street Journal, suggesting that
this modification has resulted in competitive per-
formance on out-of-domain newspaper text. The
tagger implements the Forward-Backward algo-
rithm as well as the Viterbi algorithm, so users can
opt for tag thresholding rather than forced-choice
tagging (giving >99% tag recall on DepBank, at
some cost to overall system speed). Recent exper-
iments suggest that this can lead to a small gain
in parse accuracy as well as coverage (Watson,
2006).
2.3 Morphological Analysis
The morphological analyser is also implemented
in Flex, with about 1400 finite-state rules in-
corporating a great deal of lexically exceptional
data. These rules are compiled into an efficient
C program encoding a deterministic finite state
transducer. The analyser takes a word form and
CLAWS tag and returns a lemma plus any inflec-
tional affixes. The type and token error rate of
the current system is less than 0.07% (Minnen,
Carroll and Pearce, 2001). The primary system-
internal value of morphological analysis is to en-
able later modules to use lexical information asso-
ciated with lemmas, and to facilitate further acqui-
sition of such information from lemmas in parses.
2.4 PoS and Punctuation Sequence Parsing
The manually-developed wide-coverage tag se-
quence grammar utilised in this version of the
parser consists of 689 unification-based phrase
structure rules (up from 400 in the first release).
The preterminals to this grammar are the PoS
and punctuation tags2. The terminals are featu-
ral descriptions of the preterminals, and the non-
terminals project information up the tree using
an X-bar scheme with 41 attributes with a maxi-
mum of 33 atomic values. Many of the original
2The relatively high level of detail in the tagset helps the
grammar writer to limit overgeneration and overacceptance.
78
     
rules have been replaced with multiple more spe-
cific variants to increase precision. In addition,
coverage has been extended in various ways, no-
tably to cover quotation and word order permuta-
tions associated with direct and indirect quotation,
as is common in newspaper text. All rules now
have a rule-to-rule declarative specification of the
grammatical relations they license (see ?2.6). Fi-
nally, around 20% of the rules have been manu-
ally identified as ?marked? in some way; this can
be exploited in customisation and in parse ranking.
Users can specify that certain rules should not be
used and so to some extent tune the parser to dif-
ferent genres without the need for retraining.
The current version of the grammar finds at least
one parse rooted in S for about 85% of the Susanne
corpus (used for grammar development), and most
of the remainder consists of phrasal fragments
marked as independent text sentences in passages
of dialogue. The coverage of our WSJ test data is
84%. In cases where there is no parse rooted in S,
the parser returns a connected sequence of partial
parses covering the input. The criteria are partial
parse probability and a preference for longer but
non-lexical combinations (Kiefer et al, 1999).
2.5 Generalised LR Parser
A non-deterministic LALR(1) table is constructed
automatically from a CF ?backbone? compiled
from the feature-based grammar. The parser
builds a packed parse forest using this table to
guide the actions it performs. Probabilities are as-
sociated with subanalyses in the forest via those
associated with specific actions in cells of the LR
table (Inui et al, 1997). The n-best (i.e. most
probable) parses can be efficiently extracted by
unpacking subanalyses, following pointers to con-
tained subanalyses and choosing alternatives in or-
der of probabilistic ranking. This process back-
tracks occasionally since unifications are required
during the unpacking process and they occasion-
ally fail (see Oepen and Carroll, 2000).
The probabilities of actions in the LR table
are computed using bootstrapping methods which
utilise an unlabelled bracketing of the Susanne
Treebank (Watson et al, 2006). This makes the
system more easily retrainable after changes in the
grammar and opens up the possibility of quicker
tuning to in-domain data. In addition, the struc-
tural ranking induced by the parser can be re-
ranked using (in-domain) lexical data which pro-
vides conditional probability distributions for the
SUBCATegorisation attributes of the major lexi-
cal categories. Some generic data is supplied for
common verbs, but this can be augmented by user
supplied, possibly domain specific files.
2.6 Grammatical Relations Output
The resulting set of ranked parses can be dis-
played, or passed on for further processing, in a
variety of formats which retain varying degrees of
information from the full derivations. We origi-
nally proposed transforming derivation trees into
a set of named grammatical relations (GRs), il-
lustrated as a subsumption hierarchy in Figure 2,
as a way of facilitating cross-system evaluation.
The revised GR scheme captures those aspects
of predicate-argument structure that the system is
able to recover and is the most stable and gram-
mar independent representation available. Revi-
sions include a treatment of coordination in which
the coordinator is the head in subsuming relations
to enable appropriate semantic inferences, and ad-
dition of a text adjunct (punctuation) relation to
the scheme.
Factoring rooted, directed graphs of GRs into a
set of bilexical dependencies makes it possible to
compute the transderivational support for a partic-
ular relation and thus compute a weighting which
takes account both of the probability of derivations
yielding a specific relation and of the proportion
of such derivations in the forest produced by the
parser. A weighted set of GRs from the parse for-
est is now computed efficiently using a variant of
the inside-outside algorithm (Watson et al, 2005).
3 Evaluation
The new system has been evaluated using our re-
annotation of the PARC dependency bank (Dep-
Bank; King et al, 2003)?consisting of 560 sen-
tences chosen randomly from section 23 of the
Wall Street Journal?with grammatical relations
compatible with our system. Briscoe and Carroll
(2006) discuss issues raised by this reannotation.
Relations take the following form: (relation
subtype head dependent initial) where relation
specifies the type of relationship between the head
and dependent. The remaining subtype and ini-
tial slots encode additional specifications of the re-
lation type for some relations and the initial or un-
derlying logical relation of the grammatical sub-
ject in constructions such as passive. We deter-
79
        
mine for each sentence the relations in the test set
which are correct at each level of the relational hi-
erarchy. A relation is correct if the head and de-
pendent slots are equal and if the other slots are
equal (if specified). If a relation is incorrect at
a given level in the hierarchy it may still match
for a subsuming relation (if the remaining slots all
match); for example, if a ncmod relation is mis-
labelled with xmod, it will be correct for all rela-
tions which subsume both ncmod and xmod, e.g.
mod. Similarly, the GR will be considered incor-
rect for xmod and all relations that subsume xmod
but not ncmod. Thus, the evaluation scheme cal-
culates unlabelled dependency accuracy at the de-
pendency (most general) level in the hierarchy.
The micro-averaged precision, recall and F1 score
are calculated from the counts for all relations in
the hierarchy. The macroaveraged scores are the
mean of the individual scores for each relation.
On the reannotated DepBank, the system
achieves a microaveraged F1 score of 76.3%
across all relations, using our new training method
(Watson et al, 2006). Briscoe and Carroll (2006)
show that the system has equivalent accuracy to
the PARC XLE parser when the morphosyntactic
features in the original DepBank gold standard are
taken into account. Figure 3 shows a breakdown
of the new system?s results by individual relation.
Acknowledgements
Development has been partially funded by
the EPSRC RASP project (GR/N36462 and
GR/N36493) and greatly facilitated by Anna Ko-
rhonen, Diana McCarthy, Judita Preiss and An-
dreas Vlachos. Much of the system rests on ear-
lier work on the ANLT or associated tools by Bran
Boguraev, David Elworthy, Claire Grover, Kevin
Humphries, Guido Minnen, and Larry Piano.
References
Briscoe, E.J. (2006) An Introduction to Tag Sequence Gram-
mars and the RASP System Parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll (2002) ?Robust accurate statisti-
cal annotation of general text?, Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC?02),
Las Palmas, Gran Canaria, pp. 1499?1504.
Briscoe, E.J. and J. Carroll (2006) ?Evaluating the Accu-
racy of an Unlexicalized Statistical Parser on the PARC
DepBank?, Proceedings of the COLING/ACL Conference,
Sydney, Australia.
Elworthy, D. (1994) ?Does Baum-Welch re-estimation help
taggers??, Proceedings of the 4th ACL Conference on Ap-
plied NLP, Stuttgart, Germany, pp. 53?58.
Relation Precision Recall F1 std GRs
dependent 79.76 77.49 78.61 10696
aux 93.33 91.00 92.15 400
conj 72.39 72.27 72.33 595
ta 42.61 51.37 46.58 292
det 87.73 90.48 89.09 1114
arg mod 79.18 75.47 77.28 8295
mod 74.43 67.78 70.95 3908
ncmod 75.72 69.94 72.72 3550
xmod 53.21 46.63 49.70 178
cmod 45.95 30.36 36.56 168
pmod 30.77 33.33 32.00 12
arg 77.42 76.45 76.94 4387
subj or dobj 82.36 74.51 78.24 3127
subj 78.55 66.91 72.27 1363
ncsubj 79.16 67.06 72.61 1354
xsubj 33.33 28.57 30.77 7
csubj 12.50 50.00 20.00 2
comp 75.89 79.53 77.67 3024
obj 79.49 79.42 79.46 2328
dobj 83.63 79.08 81.29 1764
obj2 23.08 30.00 26.09 20
iobj 70.77 76.10 73.34 544
clausal 60.98 74.40 67.02 672
xcomp 76.88 77.69 77.28 381
ccomp 46.44 69.42 55.55 291
pcomp 72.73 66.67 69.57 24
macroaverage 62.12 63.77 62.94
microaverage 77.66 74.98 76.29
Figure 3: Accuracy on DepBank
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga
(1997) ?A new formalization of probabilistic GLR pars-
ing?, Proceedings of the 5th International Workshop on
Parsing Technologies (IWPT?97), MIT, pp. 123?134.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf (1999) ?A
bag of useful techniques for efficient and robust parsing?,
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, University of Mary-
land, pp. 473?480.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan (2003) ?The PARC700 Dependency Bank?, Proceed-
ings of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), Budapest, Hungary.
Minnen, G., J. Carroll and D. Pearce (2001) ?Applied mor-
phological processing of English?, Natural Language En-
gineering, vol.7.3, 225?250.
Oepen, S. and J. Carroll (2000) ?Ambiguity packing in
constraint-based parsing ? practical results?, Proceedings
of the 1st Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle, WA,
pp. 162?169.
Watson, R. (2006) ?Part-of-speech tagging models for pars-
ing?, Proceedings of the 9th Annual CLUK Colloquium,
Open University, Milton Keynes, UK.
Watson, R., E.J. Briscoe and J. Carroll (2006) Semi-
supervised Training of a Statistical Parser from Unlabeled
Partially-bracketed Data, forthcoming.
Watson, R., J. Carroll and E.J. Briscoe (2005) ?Efficient ex-
traction of grammatical relations?, Proceedings of the 9th
Int. Workshop on Parsing Technologies (IWPT?05), Van-
couver, Canada.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912?919,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival
Subcategorization Frames from Corpora
Judita Preiss, Ted Briscoe, and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Judita.Preiss, Ted.Briscoe, Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper describes the first system for
large-scale acquisition of subcategorization
frames (SCFs) from English corpus data
which can be used to acquire comprehen-
sive lexicons for verbs, nouns and adjectives.
The system incorporates an extensive rule-
based classifier which identifies 168 verbal,
37 adjectival and 31 nominal frames from
grammatical relations (GRs) output by a ro-
bust parser. The system achieves state-of-
the-art performance on all three sets.
1 Introduction
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al, 1998),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) ((Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from unannotated English corpora
emerged over a decade ago (Brent, 1991; Manning,
1993). Subsequent research has yielded systems for
English (Carroll and Rooth, 1998; Briscoe and Car-
roll, 1997; Korhonen, 2002) capable of detecting
comprehensive sets of SCFs with promising accu-
racy and demonstrated success in application tasks
(e.g. (Carroll et al, 1998; Korhonen et al, 2003)).
Recently, a large publicly available subcategoriza-
tion lexicon was produced using such technology
which contains frame and frequency information for
over 6,300 English verbs ? the VALEX lexicon (Ko-
rhonen et al, 2006).
While there has been considerable work in the
area, most of it has focussed on verbs. Although
verbs are the richest words in terms of subcatego-
rization and although verb SCF distribution data is
likely to offer the greatest boost in parser perfor-
mance, accurate and comprehensive knowledge of
the many noun and adjective SCFs in English could
improve the accuracy of parsing at several levels
(from tagging to syntactic and semantic analysis).
Furthermore the selection of the correct analysis
from the set returned by a parser which does not ini-
tially utilize fine-grained lexico-syntactic informa-
tion can depend on the interaction of conditional
probabilities of lemmas of different classes occur-
912
ring with specific SCFs. For example, a) and b) be-
low indicate the most plausible analyses in which the
sentential complement attaches to the noun and verb
respectively
a) Kim (VP believes (NP the evidence (Scomp that
Sandy was present)))
b) Kim (VP persuaded (NP the judge) (Scomp that
Sandy was present))
However, both a) and b) consist of an identical
sequence of coarse-grained lexical syntactic cate-
gories, so correctly ranking them requires learn-
ing that P (NP | believe).P (Scomp | evidence) >
P (NP&Scomp | believe).P (None | evidence)
and P (NP | persuade).P (Scomp | judge) <
P (NP&Scomp | persuade).P (None | judge). If
we acquired frames and frame frequencies for all
open-class predicates taking SCFs using a single sys-
tem applied to similar data, we would have a better
chance of modeling such interactions accurately.
In this paper we present the first system for large-
scale acquisition of SCFs from English corpus data
which can be used to acquire comprehensive lexi-
cons for verbs, nouns and adjectives. The classifier
incorporates 168 verbal, 37 adjectival and 31 nomi-
nal SCF distinctions. An improved acquisition tech-
nique is used which expands on the ideas Yallop et
al. (2005) recently explored for a small experiment
on adjectival SCF acquisition. It involves identifying
SCFs on the basis of grammatical relations (GRs) in
the output of the RASP (Robust Accurate Statistical
Parsing) system (Briscoe et al, 2006).
As detailed later, the system performs better with
verbs than previous comparable state-of-art systems,
achieving 68.9 F-measure in detecting SCF types. It
achieves similarly good performance with nouns and
adjectives (62.2 and 71.9 F-measure, respectively).
Additionally, we have developed a tool for lin-
guistic annotation of SCFs in corpus data aimed at
alleviating the process of obtaining training and test
data for subcategorization acquisition. The tool in-
corporates an intuitive interface with the ability to
significantly reduce the number of frames presented
to the user for each sentence.
We introduce the new system for SCF acquisition
in section 2. Details of the experimental evaluation
are supplied in section 3. Section 4 provides discus-
sion of our results and future work, and section 5
concludes.
2 Description of the System
A common strategy in existing large-scale SCF ac-
quisition systems (e.g. (Briscoe and Carroll, 1997))
is to extract SCFs from parse trees, introducing an
unnecessary dependence on the details of a particu-
lar parser. In our approach SCFs are extracted from
GRs ? representations of head-dependent relations
which are more parser/grammar independent but at
the appropriate level of abstraction for extraction of
SCFs.
A similar approach was recently motivated and
explored by Yallop et al (2005). A decision-tree
classifier was developed for 30 adjectival SCF types
which tests for the presence of GRs in the GR out-
put of the RASP (Robust Accurate Statistical Pars-
ing) system (Briscoe and Carroll, 2002). The results
reported with 9 test adjectives were promising (68.9
F-measure in detecting SCF types).
Our acquisition process consists of four main
steps: 1) extracting GRs from corpus data, 2) feeding
the GR sets as input to a rule-based classifier which
incrementally matches them with the corresponding
SCFs, 3) building lexical entries from the classified
data, and 4) filtering those entries to obtain a more
accurate lexicon. The details of these steps are pro-
vided in the subsequent sections.
2.1 Obtaining Grammatical Relations
We obtain the GRs using the recent, second release
of the RASP toolkit (Briscoe et al, 2006). RASP is a
modular statistical parsing system which includes a
tokenizer, tagger, lemmatizer, and a wide-coverage
unification-based tag-sequence parser. We use the
standard scripts supplied with RASP to output the set
of GRs for the most probable analysis returned by the
parser or, in the case of parse failures, the GRs for
the most likely sequence of subanalyses. The GRs
are organized as a subsumption hierarchy as shown
in Figure 1.
The dependency relationships which the GRs em-
body correspond closely to the head-complement
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of easy
913
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 1: The GR hierarchy used by RASP
?
?
?
?
?
?
?
?
SUBJECT NP 1 ,
ADJ-COMPS
?
PP
[
PVAL for
NP 3
]
,
VP
?
?
?
?
MOOD to-infinitive
SUBJECT 3
OMISSION 1
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Feature structure for SCF
adj-obj-for-to-inf
(|These:1_DD2| |example+s:2_NN2| |of:3_IO|
|animal:4_JJ| |senses:5_NN2| |be+:6_VBR|
|relatively:7_RR| |easy:8_JJ| |for:9_IF|
|we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xcomp(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
Figure 3: GRs from RASP for adj-obj-for-to-inf
in the sentence: These examples of animal senses
are relatively easy for us to comprehend as they are
not too far removed from our own experience. Ac-
cording to the COMLEX classification, this is an ex-
ample of the frame adj-obj-for-to-inf, shown in
Figure 2, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence is shown in Figure 3.
Each instantiated GR in Figure 3 corresponds to
one or more parts of the feature structure in Fig-
ure 2. xcomp( be[6] easy[8]) establishes be[6]
as the head of the VP in which easy[8] occurs as
a complement. The first (PP)-complement is for us,
as indicated by ncmod(for[9] easy[8] we+[10]),
with for as PFORM and we+ (us) as NP. The sec-
ond complement is represented by xcomp(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
xcomp ?Y : pos=vb,val=be ?X : pos=adj
xcomp ?S : val=to ?Y : pos=vb,val=be ?W : pos=VV0
ncsubj ?Y : pos=vb,val=be ?Z : pos=noun
ncmod ?T : val=for ?X : pos=adj ?Y: pos=pron
ncsubj ?W : pos=VV0 ?V : pos=pron
Figure 4: Pattern for frame adj-obj-for-to-inf
NP headed by examples is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) corresponds to
the coindexation marked by 3 : the subject of the
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
2.2 SCF Classifier
SCF Frames
The SCFs recognized by the classifier were ob-
tained by manually merging the frames exempli-
fied in the COMLEX Syntax (Grishman et al, 1994),
ANLT (Boguraev et al, 1987) and/or NOMLEX
(Macleod et al, 1997) dictionaries and including
additional frames found by manual inspection of
unclassifiable examples during development of the
classifier. These consisted of e.g. some occurrences
of phrasal verbs with complex complementation and
with flexible ordering of the preposition/particle,
some non-passivizable words with a surface direct
object, and some rarer combinations of governed
preposition and complementizer combinations.
The frames were created so that they abstract
over specific lexically-governed particles and prepo-
sitions and specific predicate selectional preferences
914
but include some derived semi-predictable bounded
dependency constructions.
Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against one
or more rules which express the possible mappings
from GRs to SCFs. The rules were manually devel-
oped by examining a set of development sentences
to determine which relations were actually emitted
by the parser for each SCF.
In our rule representation, a GR pattern is a set of
partially instantiated GRs with variables in place of
heads and dependents, augmented with constraints
that restrict the possible instantiations of the vari-
ables. A match is successful if the set of GRs for
a sentence can be unified with any rule. Unifica-
tion of sentence GRs and a rule GR pattern occurs
when there is a one-to-one correspondence between
sentence elements and rule elements that includes a
consistent mapping from variables to values.
A sample pattern for matching
adj-obj-for-to-inf can be seen in Fig-
ure 4. Each element matches either an empty GR
slot ( ), a variable with possible constraints on part
of speech (pos) and word value (val), or an already
instantiated variable. Unlike in Yallop?s work (Yal-
lop et al, 2005), our rules are declarative rather than
procedural and these rules, written independently
of the acquisition system, are expanded by the
system in a number of ways prior to execution. For
example, the verb rules which contain an ncsubj
relation will not contain one inside an embedded
clause. For verbs, the basic rule set contains 248
rules but automatic expansion gives rise to 1088
classifier rules for verbs.
Numerous approaches were investigated to allow
an efficient execution of the system: for example, for
each target word in a sentence, we initially find the
number of ARGument GRs (see Figure 1) containing
it in head position, as the word must appear in ex-
actly the same set in a matching rule. This allows
us to discard all patterns which specify a different
number of GRs: for example, for verbs each group
only contains an average of 109 patterns.
For a further increase in speed, both the sentence
GRs and the GRs within the patterns are ordered (ac-
cording to frequency) and matching is performed us-
ing a backing off strategy allowing us to exploit the
relatively low number of possible GRs (compared
to the number of possible rules). The system exe-
cutes on 3500 sentences in approx. 1.5 seconds of
real time on a machine with a 3.2 GHz Intel Xenon
processor and 4GB of RAM.
Lexicon Creation and Filtering
Lexical entries are constructed for each word and
SCF combination found in the corpus data. Each lex-
ical entry includes the raw and relative frequency of
the SCF with the word in question, and includes var-
ious additional information e.g. about the syntax of
detected arguments and the argument heads in dif-
ferent argument positions1.
Finally the entries are filtered to obtain a more
accurate lexicon. A way to maximise the accu-
racy of the lexicon would be to smooth (correct) the
acquired SCF distributions with back-off estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002) (see section 4) before filtering them.
However, in this first experiment with the new sys-
tem we filtered the entries directly so that we could
evaluate the performance of the new classifier with-
out any additional modules. For the same reason, the
filtering was done by using a very simple method:
by setting empirically determined thresholds on the
relative frequencies of SCFs.
3 Experimental Evaluation
3.1 Data
In order to test the accuracy of our system, we se-
lected a set of 183 verbs, 30 nouns and 30 adjec-
tives for experimentation. The words were selected
at random, subject to the constraint that they exhib-
ited multiple complementation patterns and had a
sufficient number of corpus occurrences (> 150) for
experimentation. We took the 100M-word British
National Corpus (BNC) (Burnard, 1995), and ex-
tracted all sentences containing an occurrence of one
of the test words. The sentences were processed us-
ing the SCF acquisition system described in the pre-
vious section. The citations from which entries were
derived totaled approximately 744K for verbs and
219K for nouns and adjectives, respectively.
1The lexical entries are similar to those in the VALEX lexi-
con. See (Korhonen et al, 2006) for a sample entry.
915
3.2 Gold Standard
Our gold standard was based on a manual analysis
of some of the test corpus data, supplemented with
additional frames from the ANLT, COMLEX, and/or
NOMLEX dictionaries. The gold standard for verbs
was available, but it was extended to include addi-
tional SCFs missing from the old system. For nouns
and adjectives the gold standard was created. For
each noun and adjective, 100-300 sentences from the
BNC (an average of 267 per word) were randomly
extracted. The resulting c. 16K sentences were then
manually associated with appropriate SCFs, and the
SCF frequency counts were recorded.
To alleviate the manual analysis we developed
a tool which first uses the RASP parser with some
heuristics to reduce the number of SCF presented,
and then allows an annotator to select the preferred
choice in a window. The heuristics reduced the av-
erage number of SCFs presented alongside each sen-
tence from 52 to 7. The annotator was also presented
with an example sentence of each SCF and an intu-
itive name for the frame, such as PRED (e.g. Kim
is silly). The program includes an option to record
that particular sentences could not (initially) be clas-
sified. A screenshot of the tool is shown in Figure 5.
The manual analysis was done by two linguists;
one who did the first annotation for the whole data,
and another who re-evaluated and corrected some of
the initial frame assignments, and classified most of
the data left unclassified by the first annotator2). A
total of 27 SCF types were found for the nouns and
30 for the adjectives in the annotated data. The av-
erage number of SCFs taken by nouns was 9 (with
the average of 2 added from dictionaries to supple-
ment the manual annotation) and by adjectives 11
(3 of which were from dictionaries). The latter are
rare and may not be exemplified in the data given the
extraction system.
3.3 Evaluation Measures
We used the standard evaluation metrics to evaluate
the accuracy of the SCF lexicons: type precision (the
percentage of SCF types that the system proposes
2The process precluded measurements of inter-annotator
agreement, but this was judged less important than the enhanced
accuracy of the gold standard data.
Figure 5: Sample screen of the annotation tool
which are correct), type recall (the percentage of SCF
types in the gold standard that the system proposes)
and the F-measure which is the harmonic mean of
type precision and recall.
We also compared the similarity between the ac-
quired unfiltered3 SCF distributions and gold stan-
dard SCF distributions using various measures of
distributional similarity: the Spearman rank corre-
lation (RC), Kullback-Leibler distance (KL), Jensen-
Shannon divergence (JS), cross entropy (CE), skew
divergence (SD) and intersection (IS). The details of
these measures and their application to subcatego-
rization acquisition can be found in (Korhonen and
Krymolowski, 2002).
Finally, we recorded the total number of gold
standard SCFs unseen in the system output, i.e. the
type of false negatives which were never detected
by the classifier.
3.4 Results
Table 1 includes the average results for the 183
verbs. The first column shows the results for Briscoe
and Carroll?s (1997) (B&C) system when this sys-
tem is run with the original classifier but a more
recent version of the parser (Briscoe and Carroll,
2002) and the same filtering technique as our new
system (thresholding based on the relative frequen-
cies of SCFs). The classifier of B&C system is com-
parable to our classifier in the sense that it targets al-
most the same set of verbal SCFs (165 out of the 168;
the 3 additional ones are infrequent in language and
thus unlikely to affect the comparison). The second
column shows the results for our new system (New).
3No threshold was applied to remove the noisy SCFs from
the distributions.
916
Verbs - Method
Measures B&C New
Precision (%) 47.3 81.8
Recall (%) 40.4 59.5
F-measure 43.6 68.9
KL 3.24 1.57
JS 0.20 0.11
CE 4.85 3.10
SD 1.39 0.74
RC 0.33 0.66
IS 0.49 0.76
Unseen SCFs 28 17
Table 1: Average results for verbs
The figures show that the new system clearly per-
forms better than the B&C system. It yields 68.9 F-
measure which is a 25.3 absolute improvement over
the B&C system. The better performance can be ob-
served on all measures, but particularly on SCF type
precision (81.8% with our system vs. 47.3% with the
B&C system) and on measures of distributional sim-
ilarity. The clearly higher IS (0.76 vs. 0.49) and the
fewer gold standard SCFs unseen in the output of the
classifier (17 vs. 28) indicate that the new system is
capable of detecting a higher number of SCFs.
The main reason for better performance is the
ability of the new system to detect a number of chal-
lenging or complex SCFs which the B&C system
could not detect4. The improvement is partly at-
tributable to more accurate parses produced by the
second release of RASP and partly to the improved
SCF classifier developed here. For example, the new
system is now able to distinguish predicative PP ar-
guments, such as I sent him as a messenger from the
wider class of referential PP arguments, supporting
discrimination of several syntactically similar SCFs
with distinct semantics.
Running our system on the adjective and noun test
data yielded the results summarized in Table 2. The
F-measure is lower for nouns (62.2) than for verbs
(68.9); for adjectives it is slightly better (71.9).5
4The results reported here for the B&C system are lower
than those recently reported in (Korhonen et al, 2006) for the
same set of 183 test verbs. This is because we use an improved
gold standard. However, the results for the B&C system re-
ported using the less ambitious gold standard are still less ac-
curate (58.6 F-measure) than the ones reported here for the new
system.
5The results for different word classes are not directly com-
parable because they are affected by the total number of SCFs
evaluated for each word class, which is higher for verbs and
Measures Nouns Adjectives
Precision (%) 91.2 95.5
Recall (%) 47.2 57.6
F-measure 62.2 71.9
KL 0.91 0.69
JS 0.09 0.05
CE 2.03 2.01
SD 0.48 0.36
RC 0.70 0.77
IS 0.62 0.72
Unseen SCFs 15 7
Table 2: Average results for nouns and adjectives
The noun and adjective classifiers yield very high
precision compared to recall. The lower recall fig-
ures are mostly due to the higher number of gold
standard SCFs unseen in the classifier output (rather
than, for example, the filtering step). This is par-
ticularly evident for nouns for which 15 of the 27
frames exemplified in the gold standard are missing
in the classifier output. For adjectives only 7 of the
30 gold standard SCFs are unseen, resulting in better
recall (57.6% vs. 47.2% for nouns).
For verbs, subcategorization acquisition perfor-
mance often correlates with the size of the input
data to acquisition (the more data, the better perfor-
mance). When considering the F-measure results for
the individual words shown in Table 3 there appears
to be little such correlation for nouns and adjectives.
For example, although there are individual high fre-
quency nouns with high performance (e.g. plan,
freq. 5046, F 90.9) and low frequency nouns with
low performance (e.g. characterisation, freq. 91, F
40.0), there are also many nouns which contradict
the trend (compare e.g. answer, freq. 2510, F 50.0
with fondness, freq. 71, F 85.7).6
Although the SCF distributions for nouns and ad-
jectives appear Zipfian (i.e. the most frequent frames
are highly probable, but most frames are infre-
quent), the total number of SCFs per word is typi-
cally smaller than for verbs, resulting in better resis-
tance to sparse data problems.
There is, however, a clear correlation between
the performance and the type of gold standard SCFs
taken by individual words. Many of the gold stan-
lower for nouns and adjectives. This particularly applies to the
sensitive measures of distributional similarity.
6The frequencies here refer to the number of citations suc-
cessfully processed by the parser and the classifier.
917
Noun F Adjective F
abundance 75.0 able 66.7
acknowledgement 47.1 angry 62.5
answer 50.0 anxious 82.4
anxiety 53.3 aware 87.5
apology 50.0 certain 73.7
appearance 46.2 clear 77.8
appointment 66.7 curious 57.1
belief 76.9 desperate 83.3
call 58.8 difficult 77.8
characterisation 40.0 doubtful 63.6
communication 40.0 eager 83.3
condition 66.7 easy 66.7
danger 76.9 generous 57.1
decision 70.6 imperative 81.8
definition 42.8 important 60.9
demand 66.7 impractical 71.4
desire 71.4 improbable 54.6
doubt 66.7 insistent 80.0
evidence 66.7 kind 66.7
examination 54.6 likely 66.7
experimentation 60.0 practical 88.9
fondness 85.7 probable 80.0
message 66.7 sure 84.2
obsession 54.6 unaware 85.7
plan 90.9 uncertain 60.0
provision 70.6 unclear 63.2
reminder 63.2 unimportant 61.5
rumour 61.5 unlikely 69.6
temptation 71.4 unspecified 50.0
use 60.0 unsure 90.0
Table 3: System performance for each test noun and
adjective
dard nominal and adjectival SCFs unseen by the
classifier involve complex complementation patterns
which are challenging to extract, e.g. those exem-
plified in The argument of Jo with Kim about Fido
surfaced, Jo?s preference that Kim be sacked sur-
faced, and that Sandy came is certain. In addition,
many of these SCFs unseen in the data are also very
low in frequency, and some may even be true nega-
tives (recall that the gold standard was supplemented
with additional SCFs from dictionaries, which may
not necessarily appear in the test data).
The main problem is that the RASP parser system-
atically fails to select the correct analysis for some
SCFs with nouns and adjectives regardless of their
context of occurrence. In future work, we hope to al-
leviate this problem by using the weighted GR output
from the top n-ranked parses returned by the parser
as input to the SCF classifier.
4 Discussion
The current system needs refinement to alleviate the
bias against some SCFs introduced by the parser?s
unlexicalized parse selection model. We plan to in-
vestigate using weighted GR output with the clas-
sifier rather than just the GR set from the highest
ranked parse. Some SCF classes also need to be fur-
ther resolved mainly to differentiate control options
with predicative complementation. This requires a
lexico-semantic classification of predicate classes.
Experiments with Briscoe and Carroll?s system
have shown that it is possible to incorporate some
semantic information in the acquisition process us-
ing a technique that smooths the acquired SCF dis-
tributions using back-off (i.e. probability) estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002). The estimates help to correct the ac-
quired SCF distributions and predict SCFs which are
rare or unseen e.g. due to sparse data. They could
also form the basis for predicting control of predica-
tive complements.
We plan to modify and extend this technique for
the new system and use it to improve the perfor-
mance further. The technique has so far been applied
to verbs only, but it can also be applied to nouns
and adjectives because they can also be classified on
lexical-semantic grounds. For example, the adjec-
tive simple belongs to the class of EASY adjectives,
and this knowledge can help to predict that it takes
similar SCFs to the other class members and that
control of ?understood? arguments will pattern with
easy (e.g. easy, difficult, convenient): The problem
will be simple for John to solve, For John to solve
the problem will be simple, The problem will be sim-
ple to solve, etc.
Further research is needed before highly accurate
lexicons encoding information also about semantic
aspects of subcategorization (e.g. different predicate
senses, the mapping from syntactic arguments to
semantic representation of argument structure, se-
lectional preferences on argument heads, diathesis
alternations, etc.) can be obtained automatically.
However, with the extensions suggested above, the
system presented here is sufficiently accurate for
building an extensive SCF lexicon capable of sup-
porting various NLP application tasks. Such a lex-
icon will be built and distributed for research pur-
918
poses along with the gold standard described here.
5 Conclusion
We have described the first system for automatically
acquiring verbal, nominal and adjectival subcat-
egorization and associated frequency information
from English corpora, which can be used to build
large-scale lexicons for NLP purposes. We have
also described a new annotation tool for producing
training and test data for the task. The acquisition
system, which is capable of distinguishing 168
verbal, 37 adjectival and 31 nominal frames, clas-
sifies corpus occurrences to SCFs on the basis of
GRs produced by a robust statistical parser. The
information provided by GRs closely matches the
structure that subcategorization acquisition seeks
to recover. Our experiment shows that the system
achieves state-of-the-art performance with each
word class. The discussion suggests ways in which
we could improve the system further before using it
to build a large subcategorization lexicon capable of
supporting various NLP application tasks.
Acknowledgements
This work was supported by the Royal Society and
UK EPSRC project ?Accurate and Comprehensive
Lexical Classification for Natural Language Pro-
cessing Applications? (ACLEX). We would like to
thank Diane Nicholls for her help during this work.
References
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon
from the Longman Dictionary of Contemporary English. In
Proc. of the 25th Annual Meeting of ACL, pages 193?200,
Stanford, CA.
M. Brent. 1991. Automatic acquisition of subcategorization
frames from untagged text. In Proc. of the 29th Meeting of
ACL, pages 209?214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction of
Subcategorization from Corpora. In Proc. of the 5th ANLP,
Washington DC, USA.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd LREC, pages
1499?1504, Las Palmas, Canary Islands, May.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the rasp system. In Proc. of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney, Australia.
L. Burnard, 1995. The BNC Users Reference Guide. British
National Corpus Consortium, Oxford, May.
G. Carroll and M. Rooth. 1998. Valence induction with a head-
lexicalized pcfg. In Proc. of the 3rd Conference on EMNLP,
Granada, Spain.
J. Carroll, G. Minnen, and E. J. Briscoe. 1998. Can Subcat-
egorisation Probabilities Help a Statistical Parser? In Pro-
ceedings of the 6th ACL/SIGDAT Workshop on Very Large
Corpora, pages 118?126, Montreal, Canada.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX
Syntax: Building a Computational Lexicon. In COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness
of Entropy-Based Similarity Measures in Evaluation of Sub-
categorization Acquisition Systems. In Proc. of the Sixth
CoNLL, pages 91?97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering
Polysemic Subcategorization Frame Distributions Semanti-
cally. In Proc. of the 41st Annual Meeting of ACL, pages
64?71, Sapporo, Japan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A
large subcategorization lexicon for natural language process-
ing applications. In Proc. of the 5th LREC, Genova, Italy.
A. Korhonen. 2002. Subcategorization acquisition. Ph.D. the-
sis, University of Cambridge Computer Laboratory.
C. Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves.
1997. Designing a dictionary of derived nominals. In Proc.
of RANLP, Tzigov Chark, Bulgaria.
C. Manning. 1993. Automatic Acquisition of a Large Subcat-
egorization Dictionary from Corpora. In Proc. of the 31st
Meeting of ACL, pages 235?242.
S. Schulte im Walde and C. Brew. 2002. Inducing german se-
mantic verb classes from purely syntactic subcategorisation
information. In Proc. of the 40th Annual Meeting of ACL,
Philadephia, USA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proc. of the 41st Annual Meeting of ACL, Sapporo.
J. Yallop, A. Korhonen, and E. J. Briscoe. 2005. Auto-
matic acquisition of adjectival subcategorization from cor-
pora. In Proc. of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 614?621, Ann Arbor,
Michigan.
919
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992?999,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Weakly Supervised Learning for Hedge Classification in Scientific Literature
Ben Medlock
Computer Laboratory
University of Cambridge
Cambridge, CB3 OFD
benmedlock@cantab.net
Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, CB3 OFD
ejb@cl.cam.ac.uk
Abstract
We investigate automatic classification
of speculative language (?hedging?), in
biomedical text using weakly supervised
machine learning. Our contributions include
a precise description of the task with anno-
tation guidelines, analysis and discussion,
a probabilistic weakly supervised learning
model, and experimental evaluation of the
methods presented. We show that hedge
classification is feasible using weakly
supervised ML, and point toward avenues
for future research.
1 Introduction
The automatic processing of scientific papers using
NLP and machine learning (ML) techniques is an
increasingly important aspect of technical informat-
ics. In the quest for a deeper machine-driven ?under-
standing? of the mass of scientific literature, a fre-
quently occuring linguistic phenomenon that must
be accounted for is the use of hedging to denote
propositions of a speculative nature. Consider the
following:
1. Our results prove that XfK89 inhibits Felin-9.
2. Our results suggest that XfK89 might inhibit Felin-9.
The second example contains a hedge, signaled
by the use of suggest and might, which renders
the proposition inhibit(XfK89?Felin-9) speculative.
Such analysis would be useful in various applica-
tions; for instance, consider a system designed to
identify and extract interactions between genetic en-
tities in the biomedical domain. Case 1 above pro-
vides clear textual evidence of such an interaction
and justifies extraction of inhibit(XfK89?Felin-9),
whereas case 2 provides only weak evidence for
such an interaction.
Hedging occurs across the entire spectrum of sci-
entific literature, though it is particularly common in
the experimental natural sciences. In this study we
consider the problem of learning to automatically
classify sentences containing instances of hedging,
given only a very limited amount of annotator-
labelled ?seed? data. This falls within the weakly su-
pervised ML framework, for which a range of tech-
niques have been previously explored. The contri-
butions of our work are as follows:
1. We provide a clear description of the prob-
lem of hedge classification and offer an im-
proved and expanded set of annotation guide-
lines, which as we demonstrate experimentally
are sufficient to induce a high level of agree-
ment between independent annotators.
2. We discuss the specificities of hedge classifica-
tion as a weakly supervised ML task.
3. We derive a probabilistic weakly supervised
learning model and use it to motivate our ap-
proach.
4. We analyze our learning model experimentally
and report promising results for the task on a
new publicly-available dataset.1
2 Related Work
2.1 Hedge Classification
While there is a certain amount of literature within
the linguistics community on the use of hedging in
1available from www.cl.cam.ac.uk/?bwm23/
992
scientific text, eg. (Hyland, 1994), there is little of
direct relevance to the task of classifying speculative
language from an NLP/ML perspective.
The most clearly relevant study is Light et al
(2004) where the focus is on introducing the prob-
lem, exploring annotation issues and outlining po-
tential applications rather than on the specificities
of the ML approach, though they do present some
results using a manually crafted substring match-
ing classifier and a supervised SVM on a collection
of Medline abstracts. We will draw on this work
throughout our presentation of the task.
Hedging is sometimes classed under the umbrella
concept of subjectivity, which covers a variety of lin-
guistic phenomena used to express differing forms
of authorial opinion (Wiebe et al, 2004). Riloff et al
(2003) explore bootstrapping techniques to identify
subjective nouns and subsequently classify subjec-
tive vs. objective sentences in newswire text. Their
work bears some relation to ours; however, our do-
mains of interest differ (newswire vs. scientific text)
and they do not address the problem of hedge clas-
sification directly.
2.2 Weakly Supervised Learning
Recent years have witnessed a significant growth
of research into weakly supervised ML techniques
for NLP applications. Different approaches are of-
ten characterised as either multi- or single-view,
where the former generate multiple redundant (or
semi-redundant) ?views? of a data sample and per-
form mutual bootstrapping. This idea was for-
malised by Blum and Mitchell (1998) in their
presentation of co-training. Co-training has also
been used for named entity recognition (NER)
(Collins and Singer, 1999), coreference resolution
(Ng and Cardie, 2003), text categorization (Nigam
and Ghani, 2000) and improving gene name data
(Wellner, 2005).
Conversely, single-view learning models operate
without an explicit partition of the feature space.
Perhaps the most well known of such approaches
is expectation maximization (EM), used by Nigam
et al (2000) for text categorization and by Ng and
Cardie (2003) in combination with a meta-level fea-
ture selection procedure. Self-training is an alterna-
tive single-view algorithm in which a labelled pool
is incrementally enlarged with unlabelled samples
for which the learner is most confident. Early work
by Yarowsky (1995) falls within this framework.
Banko and Brill (2001) use ?bagging? and agree-
ment to measure confidence on unlabelled samples,
and more recently McClosky et al (2006) use self-
training for improving parse reranking.
Other relevant recent work includes (Zhang,
2004), in which random feature projection and a
committee of SVM classifiers is used in a hybrid
co/self-training strategy for weakly supervised re-
lation classification and (Chen et al, 2006) where
a graph based algorithm called label propagation is
employed to perform weakly supervised relation ex-
traction.
3 The Hedge Classification Task
Given a collection of sentences, S, the task is to
label each sentence as either speculative or non-
speculative (spec or nspec henceforth). Specifically,
S is to be partitioned into two disjoint sets, one rep-
resenting sentences that contain some form of hedg-
ing, and the other representing those that do not.
To further elucidate the nature of the task and im-
prove annotation consistency, we have developed a
new set of guidelines, building on the work of Light
et al (2004). As noted by Light et al, speculative
assertions are to be identified on the basis of judge-
ments about the author?s intended meaning, rather
than on the presence of certain designated hedge
terms.
We begin with the hedge definition given by
Light et al (item 1) and introduce a set of further
guidelines to help elucidate various ?grey areas? and
tighten the task specification. These were developed
after initial annotation by the authors, and through
discussion with colleagues. Further examples are
given in online Appendix A2.
The following are considered hedge instances:
1. An assertion relating to a result that does not
necessarily follow from work presented, but
could be extrapolated from it (Light et al).
2. Relay of hedge made in previous work.
Dl and Ser have been proposed to act redundantly in the
sensory bristle lineage.
3. Statement of knowledge paucity.
2available from www.cl.cam.ac.uk/?bwm23/
993
How endocytosis of Dl leads to the activation of N re-
mains to be elucidated.
4. Speculative question.
A second important question is whether the roX genes
have the same, overlapping or complementing functions.
5. Statement of speculative hypothesis.
To test whether the reported sea urchin sequences repre-
sent a true RAG1-like match, we repeated the BLASTP
search against all GenBank proteins.
6. Anaphoric hedge reference.
This hypothesis is supported by our finding that both pu-
pariation rate and survival are affected by EL9.
The following are not considered hedge instances:
1. Indication of experimentally observed non-
universal behaviour.
proteins with single BIR domains can also have functions
in cell cycle regulation and cytokinesis.
2. Confident assertion based on external work.
Two distinct E3 ubiquitin ligases have been shown to reg-
ulate Dl signaling in Drosophila melanogaster.
3. Statement of existence of proposed alterna-
tives.
Different models have been proposed to explain how en-
docytosis of the ligand, which removes the ligand from the
cell surface, results in N receptor activation.
4. Experimentally-supported confirmation of pre-
vious speculation.
Here we show that the hemocytes are the main regulator
of adenosine in the Drosophila larva, as was speculated
previously for mammals.
5. Negation of previous hedge.
Although the adgf-a mutation leads to larval or pupal
death, we have shown that this is not due to the adenosine
or deoxyadenosine simply blocking cellular proliferation
or survival, as the experiments in vitro would suggest.
4 Data
We used an archive of 5579 full-text papers from the
functional genomics literature relating to Drosophila
melanogaster (the fruit fly). The papers were con-
verted to XML and linguistically processed using
the RASP toolkit3. We annotated six of the pa-
pers to form a test set with a total of 380 spec sen-
tences and 1157 nspec sentences, and randomly se-
lected 300,000 sentences from the remaining papers
as training data for the weakly supervised learner. To
ensure selection of complete sentences rather than
3www.informatics.susx.ac.uk/research/nlp/rasp
Frel1 ?
Original 0.8293 0.9336
Corrected 0.9652 0.9848
Table 1: Agreement Scores
headings, captions etc., unlabelled samples were
chosen under the constraints that they must be at
least 10 words in length and contain a main verb.
5 Annotation and Agreement
Two separate annotators were commissioned to la-
bel the sentences in the test set, firstly one of the
authors and secondly a domain expert with no prior
input into the guideline development process. The
two annotators labelled the data independently us-
ing the guidelines outlined in section 3. Relative
F1 (Frel1 ) and Cohen?s Kappa (?) were then used to
quantify the level of agreement. For brevity we refer
the reader to (Artstein and Poesio, 2005) and (Hripc-
sak and Rothschild, 2004) for formulation and dis-
cussion of ? and Frel1 respectively.
The two metrics are based on different assump-
tions about the nature of the annotation task. Frel1
is founded on the premise that the task is to recog-
nise and label spec sentences from within a back-
ground population, and does not explicitly model
agreement on nspec instances. It ranges from 0 (no
agreement) to 1 (no disagreement). Conversely, ?
gives explicit credit for agreement on both spec and
nspec instances. The observed agreement is then
corrected for ?chance agreement?, yielding a metric
that ranges between ?1 and 1. Given our defini-
tion of hedge classification and assessing the manner
in which the annotation was carried out, we suggest
that the founding assumption of Frel1 fits the nature
of the task better than that of ?.
Following initial agreement calculation, the in-
stances of disagreement were examined. It turned
out that the large majority of cases of disagreement
were due to negligence on behalf of one or other of
the annotators (i.e. cases of clear hedging that were
missed), and that the cases of genuine disagreement
were actually quite rare. New labelings were then
created with the negligent disagreements corrected,
resulting in significantly higher agreement scores.
Values for the original and negligence-corrected la-
994
belings are reported in Table 1.
Annotator conferral violates the fundamental as-
sumption of annotator independence, and so the lat-
ter agreement scores do not represent the true level
of agreement; however, it is reasonable to conclude
that the actual agreement is approximately lower
bounded by the initial values and upper bounded by
the latter values. In fact even the lower bound is
well within the range usually accepted as represent-
ing ?good? agreement, and thus we are confident in
accepting human labeling as a gold-standard for the
hedge classification task. For our experiments, we
use the labeling of the genetics expert, corrected for
negligent instances.
6 Discussion
In this study we use single terms as features, based
on the intuition that many hedge cues are single
terms (suggest, likely etc.) and due to the success
of ?bag of words? representations in many classifica-
tion tasks to date. Investigating more complex sam-
ple representation strategies is an avenue for future
research.
There are a number of factors that make our for-
mulation of hedge classification both interesting and
challenging from a weakly supervised learning per-
spective. Firstly, due to the relative sparsity of hedge
cues, most samples contain large numbers of irrele-
vant features. This is in contrast to much previous
work on weakly supervised learning, where for in-
stance in the case of text categorization (Blum and
Mitchell, 1998; Nigam et al, 2000) almost all con-
tent terms are to some degree relevant, and irrel-
evant terms can often be filtered out (e.g. stop-
word removal). In the same vein, for the case of
entity/relation extraction and classification (Collins
and Singer, 1999; Zhang, 2004; Chen et al, 2006)
the context of the entity or entities in consideration
provides a highly relevant feature space.
Another interesting factor in our formulation of
hedge classification is that the nspec class is defined
on the basis of the absence of hedge cues, render-
ing it hard to model directly. This characteristic
is also problematic in terms of selecting a reliable
set of nspec seed sentences, as by definition at the
beginning of the learning cycle the learner has lit-
tle knowledge about what a hedge looks like. This
problem is addressed in section 10.3.
In this study we develop a learning model based
around the concept of iteratively predicting labels
for unlabelled training samples, the basic paradigm
for both co-training and self-training. However we
generalise by framing the task in terms of the acqui-
sition of labelled training data, from which a super-
vised classifier can subsequently be learned.
7 A Probabilistic Model for Training Data
Acquisition
In this section, we derive a simple probabilistic
model for acquiring training data for a given learn-
ing task, and use it to motivate our approach to
weakly supervised hedge classification.
Given:
? sample space X
? set of target concept classes Y = {y1 . . . yN}
? target function Y : X ? Y
? set of seed samples for each class S1 . . .SN
where Si ? X and ?x ? Si[Y (x)=yi]
? set of unlabelled samples U = {x1 . . .xK}
Aim: Infer a set of training samples Ti for each con-
cept class yi such that ?x ? Ti[Y (x) = yi]
Now, it follows that ?x?Ti[Y (x)=yi] is satisfied
in the case that ?x?Ti[P (yi|x)=1], which leads to
a model in which Ti is initialised to Si and then iter-
atively augmented with the unlabelled sample(s) for
which the posterior probability of class membership
is maximal. Formally:
At each iteration:
Ti ? xj(? U)
where j = argmax
j
[P (yi|xj)] (1)
Expansion with Bayes? Rule yields:
argmax
j
[P (yi|xj)]
= argmax
j
[
P (xj |yi) ? P (yi)
P (xj)
]
(2)
An interesting observation is the importance of
the sample prior P (xj) in the denominator, of-
ten ignored for classification purposes because of
its invariance to class. We can expand further by
995
marginalising over the classes in the denominator in
expression 2, yielding:
argmax
j
[
P (xj |yi) ? P (yi)
?N
n=1 P (yn)P (xj |yn)
]
(3)
so we are left with the class priors and class-
conditional likelihoods, which can usually be esti-
mated directly from the data, at least under limited
dependence assumptions. The class priors can be
estimated based on the relative distribution sizes de-
rived from the current training sets:
P (yi) =
|Ti|
?
k |Tk|
(4)
where |S| is the number of samples in training set S.
If we assume feature independence, which as we
will see for our task is not as gross an approximation
as it may at first seem, we can simplify the class-
conditional likelihood in the well known manner:
P (xj |yi) =
?
k
P (xjk|yi) (5)
and then estimate the likelihood for each feature:
P (xk|yi) =
?P (yi) + f(xk, Ti)
?P (yi) + |Ti|
(6)
where f(x,S) is the number of samples in training
set S in which feature x is present, and ? is a uni-
versal smoothing constant, scaled by the class prior.
This scaling is motivated by the principle that with-
out knowledge of the true distribution of a partic-
ular feature it makes sense to include knowledge
of the class distribution in the smoothing mecha-
nism. Smoothing is particularly important in the
early stages of the learning process when the amount
of training data is severely limited resulting in unre-
liable frequency estimates.
8 Hedge Classification
We will now consider how to apply this learning
model to the hedge classification task. As discussed
earlier, the speculative/non-speculative distinction
hinges on the presence or absence of a few hedge
cues within the sentence. Working on this premise,
all features are ranked according to their probability
of ?hedge cue-ness?:
P (spec|xk) =
P (xk|spec) ? P (spec)
?N
n=1 P (yn)P (xk|yn)
(7)
which can be computed directly using (4) and (6).
Themmost probable features are then selected from
each sentence to compute (5) and the rest are ig-
nored. This has the dual benefit of removing irrele-
vant features and also reducing dependence between
features, as the selected features will often be non-
local and thus not too tightly correlated.
Note that this idea differs from traditional feature
selection in two important ways:
1. Only features indicative of the spec class are
retained, or to put it another way, nspec class
membership is inferred from the absence of
strong spec features.
2. Feature selection in this context is not a prepro-
cessing step; i.e. there is no re-estimation after
selection. This has the potentially detrimental
side effect of skewing the posterior estimates
in favour of the spec class, but is admissible
for the purposes of ranking and classification
by posterior thresholding (see next section).
9 Classification
The weakly supervised learner returns a labelled
data set for each class, from which a classifier can
be trained. We can easily derive a classifier using
the estimates from our learning model by:
xj ? spec if P (spec|xj) > ? (8)
where ? is an arbitrary threshold used to control the
precision/recall balance. For comparison purposes,
we also use Joachims? SVMlight (Joachims, 1999).
10 Experimental Evaluation
10.1 Method
To examine the practical efficacy of the learning and
classification models we have presented, we use the
following experimental method:
1. Generate seed training data: Sspec and Snspec
2. Initialise: Tspec?Sspec and Tnspec?Snspec
3. Iterate:
? Order U by P (spec|xj) (expression 3)
? Tspec ? most probable batch
? Tnspec ? least probable batch
? Train classifier using Tspec and Tnspec
996
Rank ? = 0 ? = 1 ? = 5 ? = 100 ? = 500
1 interactswith suggest suggest suggest suggest
2 TAFb likely likely likely likely
3 sexta may may may may
4 CRYs might might These These
5 DsRed seems seems results results
6 Cell-Nonautonomous suggests Taken might that
7 arva probably suggests observations be
8 inter-homologue suggesting probably Taken data
9 Mohanty possibly Together findings it
10 meld suggested suggesting Our Our
11 aDNA Taken possibly seems observations
12 Deer unlikely suggested together role
13 Borel Together findings Together most
14 substripe physiology observations role these
15 Failing modulated Given that together
Table 2: Features ranked by P (spec|xk) for varying ?
? Compute spec recall/precision BEP
(break-even point) on the test data
The batch size for each iteration is set to 0.001? |U|.
After each learning iteration, we compute the preci-
sion/recall BEP for the spec class using both clas-
sifiers trained on the current labelled data. We use
BEP because it helps to mitigate against misleading
results due to discrepancies in classification thresh-
old placement. Disadvantageously, BEP does not
measure a classifier?s performance across the whole
of the recall/precision spectrum (as can be obtained,
for instance, from receiver-operating characteristic
(ROC) curves), but for our purposes it provides a
clear, abstracted overview of a classifier?s accuracy
given a particular training set.
10.2 Parameter Setting
The training and classification models we have pre-
sented require the setting of two parameters: the
smoothing parameter ? and the number of features
per sample m. Analysis of the effect of varying ?
on feature ranking reveals that when ? = 0, low fre-
quency terms with spurious class correlation dom-
inate and as ? increases, high frequency terms be-
come increasingly dominant, eventually smoothing
away genuine low-to-mid frequency correlations.
This effect is illustrated in Table 2, and from this
analysis we chose ? = 5 as an appropriate level of
smoothing. We use m=5 based on the intuition that
five is a rough upper bound on the number of hedge
cue features likely to occur in any one sentence.
We use the linear kernel for SVMlight with the
default setting for the regularization parameter C.
We construct binary valued, L2-normalised (unit
length) input vectors to represent each sentence,
as this resulted in better performance than using
frequency-based weights and concords with our
presence/absence feature estimates.
10.3 Seed Generation
The learning model we have presented requires a
set of seeds for each class. To generate seeds for
the spec class, we extracted all sentences from U
containing either (or both) of the terms suggest or
likely, as these are very good (though not perfect)
hedge cues, yielding 6423 spec seeds. Generating
seeds for nspec is much more difficult, as integrity
requires the absence of hedge cues, and this cannot
be done automatically. Thus, we used the following
procedure to obtain a set of nspec seeds:
1. Create initial Snspec by sampling randomly
from U .
2. Manually remove more ?obvious? speculative
sentences using pattern matching
3. Iterate:
? Order Snspec by P (spec|xj) using esti-
mates from Sspec and current Snspec
? Examine most probable sentences and re-
move speculative instances
We started with 8830 sentences and after a couple of
hours work reduced this down to a (still potentially
noisy) nspec seed set of 7541 sentences.
997
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0  20  40  60  80  100  120  140
BE
P
Iteration
Prob (Prob)Prob (SVM)SVM (Prob)SVM (SVM)Baseline
Prob (Prob) denotes our probabilistic learning model and classifier (?9)
Prob (SVM) denotes probabilistic learning model with SVM classifier
SVM (Prob) denotes committee-based model (?10.4) with probabilistic classifier
SVM (SVM) denotes committee-based model with SVM classifier
Baseline denotes substring matching classifier of (Light et al, 2004)
Figure 1: Learning curves
10.4 Baselines
As a baseline classifier we use the substring match-
ing technique of (Light et al, 2004), which labels
a sentence as spec if it contains one or more of the
following: suggest, potential, likely, may, at least,
in part, possibl, further investigation, unlikely, pu-
tative, insights, point toward, promise and propose.
To provide a comparison for our learning model,
we implement a more traditional self-training pro-
cedure in which at each iteration a committee of five
SVMs is trained on randomly generated overlapping
subsets of the training data and their cumulative con-
fidence is used to select items for augmenting the
labelled training data. For similar work see (Banko
and Brill, 2001; Zhang, 2004).
10.5 Results
Figure 1 plots accuracy as a function of the train-
ing iteration. After 150 iterations, all of the weakly
supervised learning models are significantly more
accurate than the baseline according to a binomial
sign test (p < 0.01), though there is clearly still
much room for improvement. The baseline classi-
fier achieves a BEP of 0.60 while both classifiers
using our learning model reach approximately 0.76
BEP with little to tell between them. Interestingly,
the combination of the SVM committee-based learn-
ing model with our classifier (denoted by ?SVM
(Prob)?), performs competitively with both of the ap-
proaches that use our probabilistic learning model
and significantly better than the SVM committee-
based learning model with an SVM classifier, ?SVM
(SVM)?, according to a binomial sign test (p<0.01)
after 150 iterations. These results suggest that per-
formance may be enhanced when the learning and
classification tasks are carried out by different mod-
els. This is an interesting possibility, which we in-
tend to explore further.
An important issue in incremental learning sce-
narios is identification of the optimum stopping
point. Various methods have been investigated to ad-
dress this problem, such as ?counter-training? (Yan-
garber, 2003) and committee agreement (Zhang,
2004); how such ideas can be adapted for this task is
one of many avenues for future research.
10.6 Error Analysis
Some errors are due to the variety of hedge forms.
For example, the learning models were unsuccess-
ful in identifying assertive statements of knowledge
paucity, eg: There is no clear evidence for cy-
tochrome c release during apoptosis in C elegans
or Drosophila. Whether it is possible to learn such
examples without additional seed information is an
open question. This example also highlights the po-
tential benefit of an enriched sample representation,
in this case one which accounts for the negation of
the phrase ?clear evidence? which otherwise might
suggest a strongly non-speculative assertion.
In many cases hedge classification is challenging
even for a human annotator. For instance, distin-
guishing between a speculative assertion and one
relating to a pattern of observed non-universal be-
haviour is often difficult. The following example
was chosen by the learner as a spec sentence on the
150th training iteration: Each component consists of
a set of subcomponents that can be localized within
a larger distributed neural system. The sentence
does not, in fact, contain a hedge but rather a state-
ment of observed non-universal behaviour. How-
ever, an almost identical variant with ?could? instead
of ?can? would be a strong speculative candidate.
This highlights the similarity between many hedge
and non-hedge instances, which makes such cases
hard to learn in a weakly supervised manner.
998
11 Conclusions and Future Work
We have shown that weakly supervised ML is ap-
plicable to the problem of hedge classification and
that a reasonable level of accuracy can be achieved.
The work presented here has application in the wider
academic community; in fact a key motivation in
this study is to incorporate hedge classification into
an interactive system for aiding curators in the con-
struction and population of gene databases. We have
presented our initial results on the task using a sim-
ple probabilistic model in the hope that this will
encourage others to investigate alternative learning
models and pursue new techniques for improving ac-
curacy. Our next aim is to explore possibilities of
introducing linguistically-motivated knowledge into
the sample representation to help the learner identify
key hedge-related sentential components, and also to
consider hedge classification at the granularity of as-
sertions rather than text sentences.
Acknowledgements
This work was partially supported by the FlySlip
project, BBSRC Grant BBS/B/16291, and we thank
Nikiforos Karamanis and Ruth Seal for thorough an-
notation and helpful discussion. The first author is
supported by an University of Cambridge Millen-
nium Scholarship.
References
Ron Artstein and Massimo Poesio. 2005. Kappa3 = al-
pha (or beta). Technical report, University of Essex
Department of Computer Science.
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Meeting of the Association for Computational Linguis-
tics, pages 26?33.
Avrim Blum and Tom Mitchell. 1998. Combining la-
belled and unlabelled data with co-training. In Pro-
ceedings of COLT? 98, pages 92?100, New York, NY,
USA. ACM Press.
Jinxiu Chen, Donghong Ji, Chew L. Tan, and Zhengyu
Niu. 2006. Relation extraction using label propaga-
tion based semi-supervised learning. In Proceedings
of ACL?06, pages 129?136.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in NLP and Very Large Corpora.
George Hripcsak and Adam Rothschild. 2004. Agree-
ment, the f-measure, and reliability in information re-
trieval. J Am Med Inform Assoc., 12(3):296?298.
K. Hyland. 1994. Hedging in academic writing and eap
textbooks. English for Specific Purposes, 13:239?256.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
M. Light, X.Y. Qiu, and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and state-
ments in between. In Proceedings of BioLink 2004
Workshop on Linking Biological Literature, Ontolo-
gies and Databases: Tools for Users, Boston, May
2004.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In HLT-
NAACL.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of NAACL ?03, pages 94?101, Morris-
town, NJ, USA.
K. Nigam and R. Ghani. 2000. Understanding the be-
havior of co-training. In Proceedings of KDD-2000
Workshop on Text Mining.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103?134.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Seventh Conference on Natural Lan-
guage Learning (CoNLL-03). ACL SIGNLL., pages
25?32.
Ben Wellner. 2005. Weakly supervised learning meth-
ods for improving the quality of gene name normal-
ization data. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases, pages 1?8, Detroit, June. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30(3):277?308.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings of ACL?03, pages
343?350, Morristown, NJ, USA.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of ACL?95, pages 189?196, Morristown, NJ,
USA. ACL.
Zhu Zhang. 2004. Weakly-supervised relation clas-
sification for information extraction. In CIKM ?04:
Proceedings of the thirteenth ACM international con-
ference on Information and knowledge management,
pages 581?588, New York, NY, USA. ACM Press.
999
Extended Lexical-Semantic Classication of English Verbs
Anna Korhonen and Ted Briscoe
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 OFD, UK
alk23@cl.cam.ac.uk, ejb@cl.cam.ac.uk
Abstract
Lexical-semantic verb classifications have
proved useful in supporting various natural lan-
guage processing (NLP) tasks. The largest and
the most widely deployed classification in En-
glish is Levin?s (1993) taxonomy of verbs and
their classes. While this resource is attrac-
tive in being extensive enough for some NLP
use, it is not comprehensive. In this paper, we
present a substantial extension to Levin?s tax-
onomy which incorporates 57 novel classes for
verbs not covered (comprehensively) by Levin.
We also introduce 106 novel diathesis alterna-
tions, created as a side product of constructing
the new classes. We demonstrate the utility of
our novel classes by using them to support au-
tomatic subcategorization acquisition and show
that the resulting extended classification has
extensive coverage over the English verb lex-
icon.
1 Introduction
Lexical-semantic classes which aim to capture the close
relationship between the syntax and semantics of verbs
have attracted considerable interest in both linguistics and
computational linguistics (e.g. (Pinker, 1989; Jackendoff,
1990; Levin, 1993; Dorr, 1997; Dang et al, 1998; Merlo
and Stevenson, 2001)). Such classes can capture general-
izations over a range of (cross-)linguistic properties, and
can therefore be used as a valuable means of reducing
redundancy in the lexicon and for filling gaps in lexical
knowledge.
Verb classes have proved useful in various (multilin-
gual) natural language processing (NLP) tasks and ap-
plications, such as computational lexicography (Kipper
et al, 2000), language generation (Stede, 1998), ma-
chine translation (Dorr, 1997), word sense disambigua-
tion (Prescher et al, 2000), document classification (Kla-
vans and Kan, 1998), and subcategorization acquisition
(Korhonen, 2002). Fundamentally, such classes define
the mapping from surface realization of arguments to
predicate-argument structure and are therefore a critical
component of any NLP system which needs to recover
predicate-argument structure. In many operational con-
texts, lexical information must be acquired from small
application- and/or domain-specific corpora. The predic-
tive power of classes can help compensate for lack of suf-
ficient data fully exemplifying the behaviour of relevant
words, through use of back-off smoothing or similar tech-
niques.
Although several classifications are now available for
English verbs (e.g. (Pinker, 1989; Jackendoff, 1990;
Levin, 1993)), they are all restricted to certain class
types and many of them have few exemplars with each
class. For example, the largest and the most widely de-
ployed classification in English, Levin?s (1993) taxon-
omy, mainly deals with verbs taking noun and preposi-
tional phrase complements, and does not provide large
numbers of exemplars of the classes. The fact that no
comprehensive classification is available limits the use-
fulness of the classes for practical NLP.
Some experiments have been reported recently which
indicate that it should be possible, in the future, to au-
tomatically supplement extant classifications with novel
verb classes and member verbs from corpus data (Brew
and Schulte im Walde, 2002; Merlo and Stevenson, 2001;
Korhonen et al, 2003). While the automatic approach
will avoid the expensive overhead of manual classifica-
tion, the very development of the technology capable of
large-scale automatic classification will require access to
a target classification and gold standard exemplification
of it more extensive than that available currently.
In this paper, we address these problems by introduc-
ing a substantial extension to Levin?s classification which
incorporates 57 novel classes for verbs not covered (com-
prehensively) by Levin. These classes, many of them
drawn initially from linguistic resources, were created
semi-automatically by looking for diathesis alternations
shared by candidate verbs. 106 new alternations not cov-
ered by Levin were identified for this work. We demon-
strate the usefulness of our novel classes by using them
to improve the performance of our extant subcategoriza-
tion acquisition system. We show that the resulting ex-
tended classification has good coverage over the English
verb lexicon. Discussion is provided on how the classifi-
cation could be further refined and extended in the future,
and integrated as part of Levin?s extant taxonomy.
We discuss Levin?s classification and its extensions in
section 2. Section 3 describes the process of creating the
new verb classes. Section 4 reports the experimental eval-
uation and section 5 discusses further work. Conclusions
are drawn in section 6.
2 Levin?s Classification
Levin?s classification (Levin, 1993) provides a summary
of the variety of theoretical research done on lexical-
semantic verb classification over the past decades. In
this classification, verbs which display the same or simi-
lar set of diathesis alternations in the realization of their
argument structure are assumed to share certain meaning
components and are organized into a semantically coher-
ent class. Although alternations are chosen as the primary
means for identifying verb classes, additional properties
related to subcategorization, morphology and extended
meanings of verbs are taken into account as well.
For instance, the Levin class of ?Break Verbs? (class
45.1), which refers to actions that bring about a change
in the material integrity of some entity, is characterized
by its participation (1-3) or non-participation (4-6) in the
following alternations and other constructions (7-8):
1. Causative/inchoative alternation:
Tony broke the window   The window broke
2. Middle alternation:
Tony broke the window   The window broke easily
3. Instrument subject alternation:
Tony broke the window with the hammer   The hammer
broke the window
4. *With/against alternation:
Tony broke the cup against the wall   *Tony broke the
wall with the cup
5. *Conative alternation:
Tony broke the window   *Tony broke at the window
6. *Body-Part possessor ascension alternation:
*Tony broke herself on the arm   Tony broke her arm
7. Unintentional interpretation available (some verbs):
Reflexive object: *Tony broke himself
Body-part object: Tony broke his finger
8. Resultative phrase:
Tony broke the piggy bank open, Tony broke the glass to
pieces
Levin?s taxonomy provides a classification of 3,024
verbs (4,186 senses) into 48 broad and 192 fine-grained
classes according to their participation in 79 alternations
involving NP and PP complements.
Some extensions have recently been proposed to
this resource. Dang et al (1998) have supplemented
the taxonomy with intersective classes: special classes
for verbs which share membership of more than one
Levin class because of regular polysemy. Bonnie Dorr
(University of Maryland) has provided a reformulated
and extended version of Levin?s classification in her LCS
database (http://www.umiacs.umd.edu/  bonnie/verbs-
English.lcs). This resource groups 4,432 verbs (11,000
senses) into 466 Levin-based and 26 novel classes.
The latter are Levin classes refined according to verbal
telicity patterns (Olsen et al, 1997), while the former
are additional classes for non-Levin verbs which do not
fall into any of the Levin classes due to their distinctive
syntactic behaviour (Dorr, 1997).
As a result of this work, the taxonomy has gained con-
siderably in depth, but not to the same extent in breadth.
Verbs taking ADJP, ADVP, ADL, particle, predicative,
control and sentential complements are still largely ex-
cluded, except where they show interesting behaviour
with respect to NP and PP complementation. As many
of these verbs are highly frequent in language, NLP ap-
plications utilizing lexical-semantic classes would bene-
fit greatly from a linguistic resource which provides ad-
equate classification of their senses. When extending
Levin?s classification with new classes, we particularly
focussed on these verbs.
3 Creating Novel Classes
Levin?s original taxonomy was created by
1. selecting a set of diathesis alternations from linguis-
tic resources,
2. classifying a large number of verbs according to
their participation in these alternations,
3. grouping the verbs into semantic classes based on
their participation in sets of alternations.
We adopted a different, faster approach. This involved
1. composing a set of diathesis alternations for verbs
not covered comprehensively by Levin,
2. selecting a set of candidate lexical-semantic classes
for these verbs from linguistic resources,
3. examining whether (sub)sets of verbs in each candi-
date class could be related to each other via alterna-
tions and thus warrant creation of a new class.
In what follows, we will describe these steps in detail.
3.1 Novel Diathesis Alternations
When constructing novel diathesis alternations, we took
as a starting point the subcategorization classification
of Briscoe (2000). This fairly comprehensive classifica-
tion incorporates 163 different subcategorization frames
(SCFs), a superset of those listed in the ANLT (Boguraev
et al, 1987) and COMLEX Syntax dictionaries (Grishman
et al, 1994). The SCFs define mappings from surface
arguments to predicate-argument structure for bounded
dependency constructions, but abstract over specific par-
ticles and prepositions, as these can be trivially instanti-
ated when the a frame is associated with a specific verb.
As most diathesis alternations are only semi-predictable
on a verb-by-verb basis, a distinct SCF is defined for every
such construction, and thus all alternations can be repre-
sented as mappings between such SCFs.
We considered possible alternations between pairs of
SCFs in this classification, focusing in particular on those
SCFs not covered by Levin. The identification of alterna-
tions was done manually, using criteria similar to Levin?s:
the SCFs alternating should preserve the sense in ques-
tion, or modify it systematically.
106 new alternations were discovered using this
method and grouped into different, partly overlapping
categories. Table 1 shows some example alternations and
their corresponding categories. The alternating patterns
are indicated using an arrow (  ). The SCFs are marked
using number codes whose detailed description can be
found in (Briscoe, 2000) (e.g. SCF 53. refers to the COM-
LEX subcategorization class NP-TO-INF-OC).
3.2 Candidate Lexical-Semantic Classes
Starting off from set of candidate classes accelerated the
work considerably as it enabled building on extant lin-
guistic research. Although a number of studies are avail-
able on verb classes not covered by Levin, many of these
assume a classification system completely different to
that of Levin?s, and/or incorporate sense distinctions too
fine-grained for easy integrations with Levin?s classifica-
tion. We therefore restricted our scope to a few classifi-
cations of a suitable style and granularity:
3.2.1 The LCS Database
The LCS database includes 26 classes for verbs which
could not be mapped into any of the Levin classes due
to their distinctive syntactic behaviour. These classes
were originally created by an automatic verb classifica-
tion algorithm described in (Dorr, 1997). Although they
appear semantically meaningful, their syntactic-semantic
properties have not been systematically studied in terms
of diathesis alternations, and therefore re-examination is
warranted.
3.2.2 Rudanko?s Classification
Rudanko (1996, 2000) provides a semantically moti-
vated classification for verbs taking various types of sen-
tential complements (including predicative and control
constructions). His relatively fine-grained classes, orga-
nized into sets of independent taxonomies, have been cre-
ated in a manner similar to Levin?s. We took 43 of Run-
danko?s verb classes for consideration.
3.2.3 Sager?s Classification
Sager (1981) presents a small classification consisting
of 13 classes, which groups verbs (mostly) on the basis
of their syntactic alternations. While semantic properties
are largely ignored, many of the classes appear distinctive
also in terms of semantics.
3.2.4 Levin?s Classification
At least 20 (broad) Levin classes involve verb senses
which take sentential complements. Because full treat-
ment of these senses requires considering sentential com-
plementation, we re-evaluated these classes using our
method.
3.3 Method for Creating Classes
Each candidate class was evaluated as follows:
1. We extracted from its class description (where one
was available) and/or from the COMLEX Syntax dic-
tionary (Grishman et al, 1994) all the SCFs taken by
its member verbs.
2. We extracted from Levin?s taxonomy and from our
novel list of 106 alternations all the alternations
where these SCFs were involved.
3. Where one or several alternations where found
which captured the sense in question, and where the
minimum of two member verbs were identified, a
new verb class was created.
Steps 1-2 were done automatically and step 3 manu-
ally. Identifying relevant alternations helped to identify
additional SCFs, which in turn often led to the discov-
ery of additional alternations. The SCFs and alternations
discovered in this way were used to create the syntactic-
semantic description of each novel class.
For those candidate classes which had an insufficient
number of member verbs, new members were searched
for in WordNet (Miller, 1990). Although WordNet clas-
sifies verbs on a purely semantic basis, the syntactic reg-
ularities studied by Levin are to some extent reflected
Category Example Alternations Alternating SCFs
Equi I advised Mary to go  I advised Mary 53  24
He helped her bake the cake  He helped bake the cake 33  142
Raising Julie strikes me as foolish  Julie strikes me as a fool 143  29
He appeared to her to be ill  It appeared to her that he was ill 99  12
Category He failed in attempting to climb  He failed in the climb 63  87
switches I promised Mary to go  I promised Mary that I will go 54  52
PP deletion Phil explained to him how to do it  Phil explained how to do it 90  17
He contracted with him for the man to go  He contracted for the man to go 88  15
P/C deletion I prefer for her to do it  I prefer her to do it 15  53
They asked about what to do  They asked what to do 73  116
Table 1: Examples of new alternations
by semantic relatedness as it is represented by Word-
Net?s particular structure (e.g. (Fellbaum, 1999)). New
member verbs were frequently found among the syn-
onyms, troponyms, hypernyms, coordinate terms and/or
antonyms of the extant member verbs.
For example, using this method, we gave the following
description to one of the candidate classes of Rudanko
(1996), which he describes syntactically with the single
SCF 63 (see the below list) and semantically by stating
that verbs in this class (e.g. succeed, manage, fail) have
approximate meaning1 ?perform the act of? or ?carry out
the activity of?:
20. SUCCEED VERBS
SCF 22: John succeeded
SCF 87: John succeeded in the climb
SCF 63: John succeeded in attempting the climb
SCF 112: John succeeded to climb
Alternating SCFs: 22  87, 87  63, 22  112
Some of the candidate classes, particularly those of
Rudanko, proved too fine-grained to be helpful for a
Levin type of classification, and were either combined
with other classes or excluded from consideration. Some
other classes, particularly the large ones in the LCS
database, proved too coarse-grained after our method was
applied, and were split down to subclasses.
For example, the LCS class of Coerce Verbs (002) was
divided into four subclasses according to the particular
syntactic-semantic properties of the subsets of its mem-
ber verbs. One of these subclasses was created for verbs
such as force, induce, and seduce, which share the ap-
1Rudanko does not assign unique labels to his classes, and
the descriptions he gives - when taken out of the context - cannot
be used to uniquely identify the meaning involved in a specific
class. For details of this class, see his description in (Rudanko,
1996) page 28.
proximate meaning of ?urge or force (a person) to an ac-
tion?. The sense gives rise to object equi SCFs and alter-
nations:
2. FORCE VERBS
SCF 24: John forced him
SCF 40: John forced him into coming
SCF 49: John forced him into it
SCF 53: John forced him to come
Alternating SCFs: 24  53, 40  49, 49  24
Another subclass was created for verbs such as order
and require, which share the approximate meaning of ?di-
rect somebody to do something?. These verbs take object
raising SCFs and alternations:
3. ORDER VERBS
SCF 57: John ordered him to be nice
SCF 104: John ordered that he should be nice
SCF 106: John ordered that he be nice
Alternating SCFs: 57  104, 104  106
New subclasses were also created for those Levin
classes which did not adequately account for the varia-
tion among their member verbs. For example, a new class
was created for those 37. Verbs of Communication which
have an approximate meaning of ?make a proposal? (e.g.
suggest, recommend, propose). These verbs take a rather
distinct set of SCFs and alternations, which differ from
those taken by other communication verbs. This class
is somewhat similar in meaning to Levin?s 37.9 Advise
Verbs. In fact, a subset of the verbs in 37.9 (e.g. ad-
vise, instruct) participate in alternations prototypical to
this class (e.g. 104  106) but not, for example, in the
ones involving PPs (e.g. 103  116).
47. SUGGEST VERBS
SCF 16: John suggested how she could do it
SCF 17: John suggested how to do it
SCF 24: John suggested it
SCF 49: John suggested it to her
SCF 89: John suggested to her how she could do it
SCF 90: John suggested to her how to do it
SCF 97: John suggested to her that she would do it
SCF 98: John suggested to her that she do it
SCF 101: John suggested to her what she could do
SCF 103: John suggested to her what to do
SCF 104: John suggested that she could do it
SCF 106: John suggested that she do it
SCF 114: John suggested what she could do
SCF 116: John suggested what to do
Alternating SCFs: 16  17, 24  49, 89  16,
90  17, 97  104, 98  106, 101  114,
103  116, 104  106
Our work resulted in accepting, rejecting, combining
and refining the 102 candidate classes and - as a by-
product - identifying 5 new classes not included in any
of the resources we used. In the end, 57 new verb classes
were formed, each associated with 2-45 member verbs.
Those Levin or Dorr classes which were examined but
found distinctive enough as they stand are not included
in this count. However, their possible subclasses are, as
well as any of the classes adapted from the resources of
Rudanko or Sager. The new classes are listed in table 2,
along with example verbs.
4 Evaluation
4.1 Task-Based Evaluation
We performed an experiment in the context of automatic
SCF acquisition to investigate whether the new classes
can be used to support an important NLP task. The task is
to associate classes to specific verbs along with an es-
timate of the conditional probability of a SCF given a
specific verb. The resulting valency or subcategorization
lexicon can be used by a (statistical) parser to recover
predicate-argument structure.
Our test data consisted of a total of 35 verbs from 12
new verb classes. The classes were chosen at random,
subject to the constraint that their member verbs were fre-
quent enough in corpus data. A minimum of 300 corpus
occurrences per verb is required to yield a reliable SCF
distribution for a polysemic verb with multiple SCFs (Ko-
rhonen, 2002). We took a sample of 20 million words of
the British National Corpus (BNC) (Leech, 1992) and ex-
tracted all sentences containing an occurrence of one of
the test verbs. After the extraction process, we retained
Class Example Verbs
1. URGE ask, persuade
2. FORCE manipulate, pressure
3. ORDER command, require
4. WANT need, want
5. TRY attempt, try
6. WISH hope, expect
7. ENFORCE impose, risk
8. ALLOW allow, permit
9. ADMIT include, welcome
10. CONSUME spend, waste
11. PAY pay, spend
12. FORBID prohibit, ban
13. REFRAIN abstain, refrain
14. RELY bet, count
15. CONVERT convert, switch
16. SHIFT resort, return
17. ALLOW allow, permit
18. HELP aid, assist
19. COOPERATE collaborate, work
20. SUCCEED fail, manage
21. NEGLECT omit, fail
22. LIMIT restrict, restrain
23. APPROVE accept, object
24. ENQUIRE ask, consult
25. CONFESS acknowledge, reveal
26. INDICATE demonstrate, imply
27. DEDICATE devote, commit
28. FREE cure, relieve
29. SUSPECT accuse, condemn
30. WITHDRAW retreat, retire
31. COPE handle, deal
32. DISCOVER hear, learn
33. MIX pair, mix
34. CORRELATE coincide, alternate
35. CONSIDER imagine, remember
36. SEE notice, feel
37. LOVE like, hate
38. FOCUS focus, concentrate
39. CARE mind, worry
40. DISCUSS debate, argue
41. BATTLE fight, communicate
42. SETTLE agree, contract
43. SHOW demonstrate, quote
44. ALLOW allow, permit
45. EXPLAIN write, read
46. LECTURE comment, remark
47. SUGGEST propose, recommend
48. OCCUR happen, occur
49. MATTER count, weight
50. AVOID miss, boycott
51. HESITATE loiter, hesitate
52. BEGIN continue, resume
53. STOP terminate, finish
54. NEGLECT overlook, neglect
55. CHARGE commit, charge
56. REACH arrive, hit
57. ADOPT assume, adopt
Table 2: New Verb Classes
1000 citations, on average, for each verb.
Our method for SCF acquisition (Korho-
nen, 2002) involves first using the system of
Briscoe and Carroll (1997) to acquire a putative SCF dis-
tribution for each test verb from corpus data. This system
employs a robust statistical parser (Briscoe and Carroll,
2002) which yields complete though shallow parses from
the PoS tagged data. The parse contexts around verbs
are passed to a comprehensive SCF classifier, which
selects one of the 163 SCFs. The SCF distribution is then
smoothed with the back-off distribution corresponding
to the semantic class of the predominant sense of a verb.
Although many of the test verbs are polysemic, we relied
on the knowledge that the majority of English verbs have
a single predominating sense in balanced corpus data
(Korhonen and Preiss, 2003).
The back-off estimates were obtained by the following
method:
(i) A few individual verbs were chosen from a new
verb class whose predominant sense according to the
WordNet frequency data belongs to this class,
(ii) SCF distributions were built for these verbs by man-
ually analysing c. 300 occurrences of each verb in
the BNC,
(iii) the resulting SCF distributions were merged.
An empirically-determined threshold was finally set on
the probability estimates from smoothing to reject noisy
SCFs caused by errors during the statistical parsing phase.
This method for SCF acquisition is highly sensitive to
the accuracy of the lexical-semantic classes. Where a
class adequately predicts the syntactic behaviour of the
predominant sense of a test verb, significant improvement
is seen in SCF acquisition, as accurate back-off estimates
help to correct the acquired SCF distribution and deal
with sparse data. Incorrect class assignments or choice
of classes can, however, degrade performance.
The SCFs were evaluated against manually analysed
corpus data. This was obtained by annotating a maximum
of 300 occurrences for each test verb in the BNC data. We
calculated type precision (the percentage of SCF types
that the system proposes which are correct), type recall
(the percentage of SCF types in the gold standard that the
system proposes) and F  -measure2. To investigate how
well the novel classes help to deal with sparse data, we
recorded the total number of SCFs missing in the distri-
butions, i.e. false negatives which did not even occur in
the unthresholded distributions and were, therefore, never
hypothesized by the parser and classifier. We also com-
pared the similarity between the acquired unthresholded
2 
	 ffProceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 160?170,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficient extraction of grammatical relations
Rebecca Watson, John Carroll
 
and Ted Briscoe
Computer Laboratory, University of Cambridge, Cambridge, CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
 
Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We present a novel approach for applying
the Inside-Outside Algorithm to a packed
parse forest produced by a unification-
based parser. The approach allows a node
in the forest to be assigned multiple inside
and outside probabilities, enabling a set of
?weighted GRs? to be computed directly
from the forest. The approach improves
on previous work which either loses effi-
ciency by unpacking the parse forest be-
fore extracting weighted GRs, or places
extra constraints on which nodes can be
packed, leading to less compact forests.
Our experiments demonstrate substantial
increases in parser accuracy and through-
put for weighted GR output.
1 Introduction
RASP is a robust statistical analysis system for
English developed by Briscoe and Carroll (2002).
It contains a syntactic parser which can output
analyses in a number of formats, including (n-
best) syntactic trees, robust minimal recursion se-
mantics (Copestake, 2003), grammatical relations
(GRs), and weighted GRs. The weighted GRs for
a sentence comprise the set of grammatical relations
in all parses licensed for that sentence, each GR is
weighted based on the probabilities of the parses
in which it occurs. This weight is normalised to
fall within the range  0,1  where  indicates that
all parses contain the GR. Therefore, high precision
GR sets can be determined by thresholding on the
GR weight (Carroll and Briscoe, 2002). Carroll and
Briscoe compute weighted GRs by first unpacking
all parses or the n-best subset from the parse forest.
Hence, this approach is either (a) inefficient (and for
some examples impracticable) if a large number of
parses are licensed by the grammar, or (b) inaccu-
rate if the number of parses unpacked is less than
the number licensed by the grammar.
In this paper, we show how to obviate the need
to trade off efficiency and accuracy by extracting
weighted GRs directly from the parse forest us-
ing a dynamic programming approach based on the
Inside-Outside algorithm (IOA) (Baker, 1979; Lari
and Young, 1990). This approach enables efficient
calculation of weighted GRs over all parses and sub-
stantially improves the throughput and memory us-
age of the parser. Since the parser is unification-
based, we also modify the parsing algorithm so that
local ambiguity packing is based on feature structure
equivalence rather than subsumption.
Similar dynamic programming techniques that
are variants of the IOA have been applied for re-
lated tasks, such as parse selection (Johnson, 2001;
Schmid and Rooth, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Kaplan et al, 2004;
Taskar et al, 2004). The approach we take is similar
to Schmid and Rooth?s (2001) adaptation of the al-
gorithm, where ?expected governors? (similar to our
?GR specifications?) are determined for each tree,
and alternative nodes in the parse forest have the
same lexical head. Initially, they create a packed
parse forest and during a second pass the parse forest
nodes are split if multiple lexical heads occur. The
IOA is applied over this split data structure. Simi-
larly, Clark and Curran (2004) alter their packing al-
gorithm so that nodes in the packed chart have the
same semantic head and ?unfilled? GRs. Our ap-
160
proach is novel in that while calculating inside prob-
abilities we allow any node in the parse forest to
have multiple semantic heads.
Clark and Curran (2004) apply Miyao and Tsu-
jii?s (2002) dynamic programming approach to de-
termine weighted GRs. They outline an alterna-
tive parse selection method based on the resulting
weighted GRs: select the (consistent) GR set with
the highest average weighted GR score. We apply
this parse selection approach and achieve 3.01% rel-
ative reduction in error. Further, the GR set output
by this approach is a consistent set whereas the high
precision GR sets outlined in (Carroll and Briscoe,
2002) are neither consistent nor coherent.
The remainder of this paper is organised as fol-
lows: Section 2 gives details of the RASP sys-
tem that are relevant to this work. Section 3 de-
scribes our test suite and experimental environment.
Changes required to the current parse forest cre-
ation algorithm are discussed in Section 4, while
Section 5 outlines our dynamic programming ap-
proach for extracting weighted GRs (EWG). Sec-
tion 6 presents experimental results showing (a) im-
proved efficiency achieved by EWG, (b) increased
upper bounds of precision and recall achieved us-
ing EWG, and (c) increased accuracy achieved by
a parse selection algorithm that would otherwise be
too inefficient to consider. Finally, Section 7 out-
lines our conclusions and future lines of research.
2 The RASP System
RASP is based on a pipelined modular architec-
ture in which text is pre-processed by a series of
components including sentence boundary detection,
tokenisation, part of speech tagging, named entity
recognition and morphological analysis, before be-
ing passed to a statistical parser1 . A brief overview
of relevant aspects of syntactic processing in RASP
is given below; for full details of system compo-
nents, see Briscoe and Carroll (1995; 2002; 2005)2.
1Processing times given in this paper do not include these
pre-processing stages, since they take negligible time compared
with parsing.
2RASP is freely available for research use; visit
http://www.informatics.susx.ac.uk/research/nlp/rasp/
2.1 The Grammar
Briscoe and Carroll (2005) describe the (manually-
written) feature-based unification grammar and the
rule-to-rule mapping from local trees to GRs. The
mapping specifies for each grammar rule the seman-
tic head(s) of the rule (henceforth, head), and one or
more GRs that should be output (optionally depend-
ing on feature values instantiated at parse time). For
example, Figure 1 shows a grammar rule analysing a
verb phrase followed by a prepositional phrase mod-
ifier. The rule identifies the first daughter (1) as the
semantic head, and specifies that one of five possi-
ble GRs is to be output, depending on the value of
the PSUBCAT syntactic feature; so, for example, if the
feature has the value NP, then the relation is ncmod
(non-clausal modifier), with slots filled by the se-
mantic heads of the first and second daughters (the 1
and 2 arguments).
Before parsing, a context free backbone is derived
automatically from the grammar, and an LALR(1)
parse table is computed from this backbone (Carroll,
1993, describes the procedure in detail). Probabili-
ties are associated with actions in the parse table,
by training on around 4K sentences from the Su-
sanne corpus (Sampson, 1995), each sentence hav-
ing been semi-automatically converted from a tree-
bank bracketing to a tree conforming to the unifica-
tion grammar (Briscoe and Carroll, 1995).
2.2 The Parse Forest
When parsing, the LALR table action probabilities
are used to assign a score to each newly derived
(sub-)analysis. Additionally, on each reduce ac-
tion (i.e. complete application of a rule), the rule?s
daughters are unified with the sequence of sub-
analyses being consumed. If unification fails then
the reduce action is aborted. Local ambiguity pack-
ing (packing, henceforth) is performed on the ba-
sis of feature structure subsumption. Thus, the
parser builds and returns a compact structure that ef-
ficiently represents all parses licensed by the gram-
mar: the parse forest. Since unification often fails
it is not possible to apply beam or best first search
strategies during construction of the parse forest;
statistically high scoring paths often end up in unifi-
cation failure. Hence, the parse forest represents all
parses licensed by the grammar.
161
V1/vp_pp : V1[MOD +] --> H1 P2[ADJ -, WH -] :
1 :
2 = [PSUBCAT NP], (ncmod _ 1 2) :
2 = [PSUBCAT NONE], (ncmod prt 1 2) :
2 = [PSUBCAT (VP, VPINF, VPING, VPPRT, AP)], (xmod _ 1 2) :
2 = [PSUBCAT (SFIN, SINF, SING)], (cmod _ 1 2) :
2 = [PSUBCAT PP], (pmod 1 2).
Figure 1: Example grammar rule, showing the rule name and syntactic specification (on the first line),
identification of daughter 1 as the semantic head (second line), and possible GR outputs depending on the
parse-time value of the PSUBCAT feature of daughter 2 (subsequent lines).
Figure 2 shows a simplified parse forest contain-
ing three parses generated for the following pre-
processed text3:
I PPIS1 see+ed VVD the AT man NN1
in II the AT park NN1
The GR specifications shown are instantiated based
on the values of syntactic features at daughter nodes,
as discussed in Section 2.1 above. For example, the
V1/vp pp sub-analysis (towards the left hand side of
the Figure) contains the instantiated GR specifica-
tion   1, (ncmod 1 2)  since its second daughter has
the value NP for its PSUBCAT feature.
Henceforth, we will use the term ?node? to refer to
data structures in our parse forest corresponding to a
rule instantiation: a sub-analysis resulting from ap-
plication of a reduce action. Back pointers are stored
in nodes, indicating which daughters were used to
create the sub-analysis. These pointers provide a
means to traverse the parse forest during subsequent
processing stages. A ?packed node? is a node rep-
resenting a sub-analysis that is subsumed by, and
hence packed into, another node. Packing is consid-
ered for nodes only if they are produced in the same
LR state and represent sub-analyses with the same
word span. A parse forest can have a number of root
nodes, each one dominating analyses spanning the
whole sentence with the specified top category.
2.3 Parser Output
From the parse forest, RASP unpacks the ?n-best?4
syntactic trees using a depth-first beam search (Car-
roll, 1993). There are a number of types of analysis
3The part of speech tagger uses a subset of the Lancaster
CLAWS2 tagset ? http://www.comp.lancs.ac.uk/computing/research/
ucrel/claws2tags.html
4This number  is specified by the user, and represents the
maximal number of parses to be unpacked.
output available, including syntactic tree, grammati-
cal relations (GRs) and robust minimal recursion se-
mantics (RMRS). Each of these is computed from
the n-best trees.
Another output possibility is weighted GRs (Car-
roll and Briscoe, 2002); this is the unique set of GRs
from the n-best GRs, each GR weighted according
to the sum of the probabilities of the parses in which
it occurs. Therefore, a number of processing stages
determine this output: unpacking the n-best syntac-
tic trees, determining the corresponding n-best GR
sets and finding the unique set of GRs and corre-
sponding weights.
The GRs for each parse are computed from the
set of GR specifications at each node, passing the
(semantic) head of each sub-analysis up to the next
higher level in the parse tree (beginning from word
nodes). GR specifications for nodes (which, if re-
quired, have been instantiated based on the features
of daughter nodes) are referred to as ?unfilled? un-
til the slots containing numbers are ?filled? with the
corresponding heads of daughter nodes. For exam-
ple, the grammar rule named NP/det n has the un-
filled GR specification   2, (det 2 1)  . Therefore, if
an NP/det n local tree has two daughters with heads
the and cat respectively, the resulting filled GR spec-
ification will be   cat, (det cat the)  , i.e. the head of
the local tree is cat and the GR output is (det cat the).
Figure 3 illustrates the n-best GRs and the
corresponding (non-normalised and normalised)
weighted GRs for the sentence I saw the man in
the park. The corresponding parse forest for this
example is shown in Figure 2. Weights on the
GRs are normalised probabilities representing the
weighted proportion of parses in which the GR
occurs. This weighting is in practice calculated
as the sum of parse probabilities for parses con-
162
T/
tx
t-
sc
1/
-
S/
np
_v
p
I_
PP
IS
1
V1
/v
_n
p_
pp
V1
/v
p_
pp
V1
/v
_n
p
se
e+
ed
_V
VD
th
e_
AT
ma
n_
NN
1
in
_I
I
th
e_
AT
pa
rk
_N
N1
N1
/n
N1
/n
NP
/d
et
_n
NP
/d
et
_n
PP
/p
1
PP
/p
1
PP
/p
1
P1
/p
_n
p
NP
/d
et
_n
N1
/n
1_
pp
1
P1
/p
_n
p
P1
/p
_n
p
in
_I
I
V1
/v
_n
p
in
_I
I
-0.
02
63
28
93
2
-4.
05
27
59
6
-0.
47
56
08
94
-3.
19
57
16
-2.
78
88
75
-3.
23
82
97
2
-4.
61
76
63
2e
-4
-0.
00
67
16
03
06
-1.
35
22
34
5e
-4
-0.
13
60
40
73 -
0.0
01
26
28
43
3
-1.
35
22
34
5e
-4
-1.
08
08
25
7
-0.
39
11
12
9
-7.
49
44
67
7
-3.
77
18
31
3
-3.
83
19
09
2
-3.
59
08
41
8
-0.
00
64
18
27
42
-3.
83
19
09
2
-3.
59
08
41
8
-1.
59
90
18
3
-0.
32
94
57
7
-3.
83
19
09
2
-3.
59
08
41
8
-2.
75
51
12
4
-3.
77
18
31
3
<1
>
<2
,(d
et 
2 1
)>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
>
<2
,(d
et 
2 1
)>
<2
,(d
et 
2 1
)>
<1
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
>
<1
>
<1
,(io
bj 
1 3
)>
 
<2
,(n
csu
bj 
2 1
)> 
<1
,(d
ob
j 1
 2)
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
,(d
ob
j 1
 2)
>
*p
ac
ke
d*
*p
ac
ke
d*
Figure 2: Simplified parse forest for I saw the man in the park. Each element in the directed acyclic graph
represents a node in the parse forest and is shown with the sub-analysis? rule name, reduce probability
(or shift probability at word nodes) and (instantiated) GR specifications. Two nodes are packed into the
V1/v np pp node, so there will be three alternative parses for the sentence. Nodes with multiple in-going
pointers on their left are shared. All thin lines indicate pointers from left to right, i.e. from mother to daughter
nodes.
163
taining the specific GR, normalised by the sum
of all parse probabilities. For example, the GR
(iobj see+ed in) is in one parse with probability
 

 	 , the non-normalised score. The sum of
all parse probabilities is        
  . Therefore,
the normalised probability (and final weight) of the
GR is  
 Proceedings of the 10th Conference on Parsing Technologies, pages 23?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Semi-supervised Training of a Statistical Parser
from Unlabeled Partially-bracketed Data
Rebecca Watson and Ted Briscoe
Computer Laboratory
University of Cambridge, UK
FirstName.LastName@cl.cam.ac.uk
John Carroll
Department of Informatics
University of Sussex, UK
J.A.Carroll@sussex.ac.uk
Abstract
We compare the accuracy of a statisti-
cal parse ranking model trained from a
fully-annotated portion of the Susanne
treebank with one trained from unla-
beled partially-bracketed sentences de-
rived from this treebank and from the
Penn Treebank. We demonstrate that
confidence-based semi-supervised tech-
niques similar to self-training outperform
expectation maximization when both are
constrained by partial bracketing. Both
methods based on partially-bracketed
training data outperform the fully su-
pervised technique, and both can, in
principle, be applied to any statistical
parser whose output is consistent with
such partial-bracketing. We also explore
tuning the model to a different domain
and the effect of in-domain data in the
semi-supervised training processes.
1 Introduction
Extant statistical parsers require extensive and
detailed treebanks, as many of their lexical and
structural parameters are estimated in a fully-
supervised fashion from treebank derivations.
Collins (1999) is a detailed exposition of one
such ongoing line of research which utilizes the
Wall Street Journal (WSJ) sections of the Penn
Treebank (PTB). However, there are disadvan-
tages to this approach. Firstly, treebanks are ex-
pensive to create manually. Secondly, the richer
the annotation required, the harder it is to adapt
the treebank to train parsers which make differ-
ent assumptions about the structure of syntac-
tic analyses. For example, Hockenmeier (2003)
trains a statistical parser based on Combinatory
Categorial Grammar (CCG) on the WSJ PTB,
but first maps the treebank to CCG derivations
semi-automatically. Thirdly, many (lexical) pa-
rameter estimates do not generalize well be-
tween domains. For instance, Gildea (2001) re-
ports that WSJ-derived bilexical parameters in
Collins? (1999) Model 1 parser contribute about
1% to parse selection accuracy when test data
is in the same domain, but yield no improve-
ment for test data selected from the Brown Cor-
pus. Tadayoshi et al (2005) adapt a statistical
parser trained on the WSJ PTB to the biomed-
ical domain by retraining on the Genia Corpus,
augmented with manually corrected derivations
in the same format. To make statistical parsing
more viable for a range of applications, we need
to make more effective and flexible use of extant
training data and minimize the cost of annota-
tion for new data created to tune a system to a
new domain.
Unsupervised methods for training parsers
have been relatively unsuccessful to date, in-
cluding expectation maximization (EM) such as
the inside-outside algorithm (IOA) over PCFGs
(Baker, 1979; Prescher, 2001). However, Pereira
and Schabes (1992) adapted the IOA to apply
over semi-supervised data (unlabeled bracket-
ings) extracted from the PTB. They constrain
the training data (parses) considered within the
IOA to those consistent with the constituent
boundaries defined by the bracketing. One ad-
vantage of this approach is that, although less
information is derived from the treebank, it gen-
23
eralizes better to parsers which make different
representational assumptions, and it is easier,
as Pereira and Schabes did, to map unlabeled
bracketings to a format more consistent with
the target grammar. Another is that the cost
of annotation with unlabeled brackets should be
lower than that of developing a representation-
ally richer treebank. More recently, both Riezler
et al (2002) and Clark and Curran (2004) have
successfully trained maximum entropy parsing
models utilizing all derivations in the model con-
sistent with the annotation of the WSJ PTB,
weighting counts by the normalized probability
of the associated derivation. In this paper, we
extend this line of investigation by utilizing only
unlabeled and partial bracketing.
We compare the performance of a statisti-
cal parsing model trained from a detailed tree-
bank with that of the same model trained with
semi-supervised techniques that require only un-
labeled partially-bracketed data. We contrast
an IOA-based EM method for training a PGLR
parser (Inui et al, 1997), similar to the method
applied by Pereira and Schabes to PCFGs, to a
range of confidence-based semi-supervised meth-
ods described below. The IOA is a generaliza-
tion of the Baum-Welch or Forward-Backward
algorithm, another instance of EM, which can be
used to train Hidden Markov Models (HMMs).
Elworthy (1994) and Merialdo (1994) demon-
strated that Baum-Welch does not necessarily
improve the performance of an HMM part-of-
speech tagger when deployed in an unsuper-
vised or semi-supervised setting. These some-
what negative results, in contrast to those of
Pereira and Schabes (1992), suggest that EM
techniques require fairly determinate training
data to yield useful models. Another motiva-
tion to explore alternative non-iterative meth-
ods is that the derivation space over partially-
bracketed data can remain large (>1K) while
the confidence-based methods we explore have a
total processing overhead equivalent to one iter-
ation of an IOA-based EM algorithm.
As we utilize an initial model to annotate ad-
ditional training data, our methods are closely
related to self-training methods described in the
literature (e.g. McClosky et al 2006, Bacchi-
ani et al 2006). However these methods have
been applied to fully-annotated training data
to create the initial model, which is then used
to annotate further training data derived from
unannotated text. Instead, we train entirely
from partially-bracketed data, starting from the
small proportion of ?unambiguous? data whereby
a single parse is consistent with the annota-
tion. Therefore, our methods are better de-
scribed as semi-supervised and the main focus
of this work is the flexible re-use of existing
treebanks to train a wider variety of statistical
parsing models. While many statistical parsers
extract a context-free grammar in parallel with
a statistical parse selection model, we demon-
strate that existing treebanks can be utilized to
train parsers that deploy grammars that make
other representational assumptions. As a result,
our methods can be applied by a range of parsers
to minimize the manual effort required to train
a parser or adapt to a new domain.
?2 gives details of the parsing system that are
relevant to this work. ?3 and ?4 describe our
data and evaluation schemes, respectively. ?5
describes our semi-supervised training methods.
?6 explores the problem of tuning a parser to a
new domain. Finally, ?7 gives conclusions and
future work.
2 The Parsing System
Sentences are automatically preprocessed in a
series of modular pipelined steps, including to-
kenization, part of speech (POS) tagging, and
morphological analysis, before being passed to
the statistical parser. The parser utilizes a man-
ually written feature-based unification grammar
over POS tag sequences.
2.1 The Parse Selection Model
A context-free ?backbone? is automatically de-
rived from the unification grammar1 and a gen-
eralized or non-deterministic LALR(1) table is
1This backbone is determined by compiling out the
values of prespecified attributes. For example, if we com-
pile out the attribute PLURAL which has 2 possible val-
ues (plural or not) we will create 2 CFG rules for each
rule with categories that contain PLURAL. Therefore,
no information is lost during this process.
24
constructed from this backbone (Tomita, 1987).
The residue of features not incorporated into
the backbone are unified on each reduce action
and if unification fails the associated derivation
paths also fail. The parser creates a packed
parse forest represented as a graph-structured
stack.2 The parse selection model ranks com-
plete derivations in the parse forest by com-
puting the product of the probabilities of the
(shift/reduce) parse actions (given LR state and
lookahead item) which created each derivation
(Inui et al, 1997).
Estimating action probabilities, consists of
a) recording an action history for the correct
derivation in the parse forest (for each sen-
tence in a treebank), b) computing the fre-
quency of each action over all action histories
and c) normalizing these frequencies to deter-
mine probability distributions over conflicting
(i.e. shift/reduce or reduce/reduce) actions.
Inui et al (1997) describe the probability
model utilized in the system where a transition
is represented by the probability of moving from
one stack state, ?i?1, (an instance of the graph
structured stack) to another, ?i. They estimate
this probability using the stack-top state si?1,
next input symbol li and next action ai. This
probability is conditioned on the type of state
si?1. Ss and Sr are mutually exclusive sets
of states which represent those states reached
after shift or reduce actions, respectively. The
probability of an action is estimated as:
P (li, ai, ?i|?i?1) ?
{
P (li, ai|si?1) si?1 ? Ss
P (ai|si?1, li) si?1 ? Sr
}
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead
for the state depending on whether the state is
a member of Ss or Sr, respectively (hereafter
the I function). In addition, Laplace estimation
can be used to ensure that all actions in the
2The parse forest is an instance of a feature forest as
defined by Miyao and Tsujii (2002). We will use the term
?node? herein to refer to an element in a derivation tree
or in the parse forest that corresponds to a (sub-)analysis
whose label is the mother?s label in the corresponding CF
?backbone? rule.
table are assigned a non-zero probability (the
IL function).
3 Training Data
The treebanks we use in this work are in one of
two possible formats. In either case, a treebank
T consists of a set of sentences. Each sentence
t is a pair (s,M), where s is the automatically
preprocessed set of POS tagged tokens (see ?2)
and M is either a fully annotated derivation, A,
or an unlabeled bracketing U . This bracketing
may be partial in the sense that it may be com-
patible with more than one derivation produced
by a given parser. Although occasionally the
bracketing is itself complete but alternative non-
terminal labeling causes indeterminacy, most of-
ten the ?flatter? bracketing available from ex-
tant treebanks is compatible with several alter-
native ?deeper? mostly binary-branching deriva-
tions output by a parser.
3.1 Derivation Consistency
Given t = (s,A), there will exist a single deriva-
tion in the parse forest that is compatible (cor-
rect). In this case, equality between the deriva-
tion tree and the treebank annotation A iden-
tifies the correct derivation. Following Pereira
and Schabes (1992) given t = (s, U), a node?s
span in the parse forest is valid if it does not
overlap with any span outlined in U , and hence,
a derivation is correct if the span of every node
in the derivation is valid in U . That is, if no
crossing brackets are present in the derivation.
Thus, given t = (s, U), there will often be more
than one derivation compatible with the partial
bracketing.
Given the correct nodes in the parse forest
or in derivations, we can then extract the cor-
responding action histories and estimate action
probabilities as described in ?2.1. In this way,
partial bracketing is used to constrain the set of
derivations considered in training to those that
are compatible with this bracketing.
3.2 The Susanne Treebank and
Baseline Training Data
The Susanne Treebank (Sampson, 1995) is uti-
lized to create fully annotated training data.
25
This treebank contains detailed syntactic deriva-
tions represented as trees, but the node label-
ing is incompatible with the system grammar.
We extracted sentences from Susanne and auto-
matically preprocessed them. A few multiwords
are retokenized, and the sentences are retagged
using the POS tagger, and the bracketing de-
terministically modified to more closely match
that of the grammar, resulting in a bracketed
corpus of 6674 sentences. We will refer to this
bracketed treebank as S, henceforth.
A fully-annotated and system compatible
treebank of 3543 sentences from S was also
created. We will refer to this annotated tree-
bank, used for fully supervised training, as B.
The system parser was applied to construct
a parse forest of analyses which are compati-
ble with the bracketing. For 1258 sentences,
the grammar writer interactively selected cor-
rect (sub)analyses within this set until a sin-
gle analysis remained. The remaining 2285 sen-
tences were automatically parsed and all consis-
tent derivations were returned. Since B contains
more than one possible derivation for roughly
two thirds of the data the 1258 sentences (paired
with a single tree) were repeated twice so that
counts from these trees were weighted more
highly. The level of reweighting was determined
experimentally using some held out data from
S. The baseline supervised model against which
we compare in this work is defined by the func-
tion IL(B) as described in ?2.1. The costs of
deriving the fully-annotated treebank are high
as interactive manual disambiguation takes an
average of ten minutes per sentence, even given
the partial bracketing derived from Susanne.
3.3 The WSJ PTB Training Data
The Wall Street Journal (WSJ) sections of the
Penn Treebank (PTB) are employed as both
training and test data by many researchers in
the field of statistical parsing. The annotated
corpus implicitly defines a grammar by provid-
ing a labeled bracketing over words annotated
with POS tags. We extracted the unlabeled
bracketing from the de facto standard training
sections (2-21 inclusive).3 We will refer to the
resulting corpus as W and the combination (con-
catenation) of the partially-bracketed corpora S
and W as SW .
3.4 The DepBank Test Data
King et al (2003) describe the development
of the PARC 700 Dependency Bank, a gold-
standard set of relational dependencies for 700
sentences (from the PTB) drawn at random
from section 23 of the WSJ (the de facto stan-
dard test set for statistical parsing). In all the
evaluations reported in this paper we test our
parser over a gold-standard set of relational de-
pendencies compatible with our parser output
derived (Briscoe and Carroll, 2006) from the
PARC 700 Dependency Bank (DepBank, hence-
forth).
The Susanne Corpus is a (balanced) subset of
the Brown Corpus which consists of 15 broad
categories of American English texts. All but
one category (reportage text) is drawn from dif-
ferent domains than the WSJ. We therefore, fol-
lowing Gildea (2001) and others, consider S, and
also the baseline training data, B, as out-of-
domain training data.
4 The Evaluation Scheme
The parser?s output is evaluated using a rela-
tional dependency evaluation scheme (Carroll,
et al, 1998; Lin, 1998) with standard measures:
precision, recall and F1. Relations are organized
into a hierarchy with the root node specifying an
unlabeled dependency. The microaveraged pre-
cision, recall and F1 scores are calculated from
the counts for all relations in the hierarchy which
subsume the parser output. The microaveraged
F1 score for the baseline system using this eval-
uation scheme is 75.61%, which ? over similar
sets of relational dependencies ? is broadly com-
parable to recent evaluation results published by
King and collaborators with their state-of-the-
art parsing system (Briscoe et al, 2006).
3The pipeline is the same as that used for creating S
though we do not automatically map the bracketing to
be more consistent with the system grammar, instead,
we simply removed unary brackets.
26
4.1 Wilcoxon Signed Ranks Test
The Wilcoxon Signed Ranks (Wilcoxon, hence-
forth) test is a non-parametric test for statistical
significance that is appropriate when there is one
data sample and several measures. For example,
to compare the accuracy of two parsers over the
same data set. As the number of samples (sen-
tences) is large we use the normal approximation
for z. Siegel and Castellan (1988) describe and
motivate this test. We use a 0.05 level of sig-
nificance, and provide z-value probabilities for
significant results reported below. These results
are computed over microaveraged F1 scores for
each sentence in DepBank.
5 Training from Unlabeled
Bracketings
We parsed all the bracketed training data us-
ing the baseline model to obtain up to 1K top-
ranked derivations and found that a significant
proportion of the sentences of the potential set
available for training had only a single deriva-
tion compatible with their unlabeled bracket-
ing. We refer to these sets as the unambiguous
training data (?) and will refer to the remaining
sentences (for which more than one derivation
was compatible with their unlabeled bracketing)
as the ambiguous training data (?). The avail-
ability of significant quantities of unambiguous
training data that can be found automatically
suggests that we may be able to dispense with
the costly reannotation step required to gener-
ate the fully supervised training corpus, B.
Table 1 illustrates the split of the corpora into
mutually exclusive sets ?, ?, ?no match? and
?timeout?. The latter two sets are not utilized
during training and refer to sentences for which
all parses were inconsistent with the bracketing
and those for which no parses were found due
to time and memory limitations (self-imposed)
on the system.4 As our grammar is different
from that implicit in the WSJ PTB there is a
high proportion of sentences where no parses
were consistent with the unmodified PTB brack-
4As there are time and memory restrictions during
parsing, the SW results are not equal to the sum of those
from S and W analysis.
Corpus | ? | | ? | No Match Timeout
S 1097 4138 1322 191
W 6334 15152 15749 1094
SW 7409 19248 16946 1475
Table 1: Corpus split for S, W and SW .
eting. However, a preliminary investigation of
no matches didn?t yield any clear patterns of
inconsistency that we could quickly ameliorate
by simple modifications of the PTB bracketing.
We leave for the future a more extensive investi-
gation of these cases which, in principle, would
allow us to make more use of this training data.
An alternative approach that we have also ex-
plored is to utilize a similar bootstrapping ap-
proach with data partially-annotated for gram-
matical relations (Watson and Briscoe, 2007).
5.1 Confidence-Based Approaches
We use ? to build an initial model. We then
utilize this initial model to derive derivations
(compatible with the unlabeled partial brack-
eting) for ? from which we select additional
training data. We employ two types of selection
methods. First, we select the top-ranked deriva-
tion only and weight actions which resulted in
this derivation equally with those of the initial
model (C1). This method is similar to ?Viterbi
training? of HMMs though we do not weight
the corresponding actions using the top parse?s
probability. Secondly, we select more than one
derivation, placing an appropriate weight on
the corresponding action histories based on the
initial model?s confidence in the derivation. We
consider three such models, in which we weight
transitions corresponding to each derivation
ranked r with probability p in the set of size n
either using 1n , 1r or p itself to weight counts.5
For example, given a treebank T with sentences
t = (s, U), function P to return the set of
parses consistent with U given t and function A
that returns the set of actions given a parse p,
then the frequency count of action ak in Cr is
5In ?2.1 we calculate action probabilities based on fre-
quency counts where we perform a weighted sum over
action histories and each history has a weight of 1. We
extend this scheme to include weights that differ between
action histories corresponding to each derivation.
27
determined as follows:
| ak |=
?|T |
i=1
?|P (ti)|
j=1,ak?A(pij)
1
j
These methods all perform normalization over
the resulting action histories using the training
function IL and will be referred to as Cn, Cr
and Cp, respectively. Cn is a ?uniform? model
which weights counts only by degree of ambi-
guity and makes no use of ranking information.
Cr weights counts by derivation rank, and Cp
is simpler than and different to one iteration of
EM as outside probabilities are not utilized. All
of the semi-supervised functions described here
take two arguments: an initial model and the
data to train over, respectively.
Models derived from unambiguous training
data, ?, alone are relatively accurate, achiev-
ing indistinguishable performance to that of the
baseline system given either W or SW as train-
ing data. We utilize these models as initial mod-
els and train over different corpora with each of
the confidence-based models. Table 2 gives re-
sults for all models. Results statistically signifi-
cant compared to the baseline system are shown
in bold print (better) or italic print (worse).
These methods show promise, often yielding sys-
tems whose performance is significantly better
than the baseline system. Method Cr achieved
the best performance in this experiment and re-
mained consistently better in those reported be-
low. Throughout the different approaches a do-
main effect can be seen, models utilizing just S
are worse, although the best performing models
benefit from the use of both S and W as training
data (i.e. SW ).
5.2 EM
Our EM model differs from that of Pereira and
Schabes as a PGLR parser adds context over
a PCFG so that a single rule can be applied
in several different states containing reduce ac-
tions. Therefore, the summation and normaliza-
tion performed for a CFG rule within IOA is in-
stead applied within such contexts. We can ap-
ply I (our PGLR normalization function with-
out Laplace smoothing) to perform the required
steps if we output the action history with the
Model Prec Rec F1 P (z)?
Baseline 77.05 74.22 75.61 -
IL(?(S)) 76.02 73.40 74.69 0.0294
C1(IL(?(S)), ?(S)) 77.05 74.22 75.61 0.4960
Cn(IL(?(S)), ?(S)) 77.51 74.80 76.13 0.0655
Cr(IL(?(S)), ?(S)) 77.73 74.98 76.33 0.0154
Cp(IL(?(S)), ?(S)) 76.45 73.91 75.16 0.2090
IL(?(W )) 77.01 74.31 75.64 0.1038
C1(IL(?(W )), ?(W )) 76.90 74.23 75.55 0.2546
Cn(IL(?(W )), ?(W )) 77.85 75.07 76.43 0.0017
Cr(IL(?(W )), ?(W )) 77.88 75.04 76.43 0.0011
Cp(IL(?(W )), ?(W )) 77.40 74.75 76.05 0.1335
IL(?(SW )) 77.09 74.35 75.70 0.1003
C1(IL(?(SW )), ?(SW )) 76.86 74.21 75.51 0.2483
Cn(IL(?(SW )), ?(SW )) 77.88 75.05 76.44 0.0048
Cr(IL(?(SW )), ?(SW )) 78.01 75.13 76.54 0.0007
Cp(IL(?(SW )), ?(SW )) 77.54 74.95 76.23 0.0618
Table 2: Performance of all models on DepBank.
?represents the statistical significance of the sys-
tem against the baseline model.
corresponding normalized inside-outside weight
for each node (Watson et al, 2005).
We perform EM starting from two initial mod-
els; either a uniform probability model, IL(), or
from models derived from unambiguous train-
ing data, ?. Figure 1 shows the cross entropy
decreasing monotonically from iteration 2 (as
guaranteed by the EM method) for different cor-
pora and initial models. Some models show an
initial increase in cross-entropy from iteration 1
to iteration 2, because the models are initial-
ized from a subset of the data which is used to
perform maximisation. Cross-entropy increases,
by definition, as we incorporate ambiguous data
with more than one consistent derivation.
Performance over DepBank can be seen in
Figures 2, 3, and 4 for each dataset S, W and
SW, respectively. Comparing the Cr and EM
lines in each of Figures 2, 3, and 4, it is evident
that Cr outperforms EM across all datasets, re-
gardless of the initial model applied. In most
cases, these results are significant, even when
we manually select the best model (iteration)
for EM.
The graphs of EM performance from itera-
tion 1 illustrate the same ?classical? and ?initial?
patterns observed by Elworthy (1994). When
EM is initialized from a relatively poor model,
such as that built from S (Figure 2), a ?classical?
28
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0 2 4 6 8 10 12 14 16
H(C,G)
Iteration Number
EM(IL(), S)
r
r
r r r r r r r r r r r r r
r
EM(IL(?(S)), S)
c
c
c c c c c c c c c c c c c
c
EM(IL(), W )
4
4 4 4 4 4 4 4 4 4 4 4 4 4 4
4
EM(IL(?(W )),W )
?
? ? ? ? ? ? ? ? ? ? ? ? ? ?
?
EM(IL(), SW )
2
2 2 2 2 2 2 2 2 2 2 2 2 2 2
2
EM(IL(?(SW )), SW )
3
3 3 3 3 3 3 3 3 3 3 3 3 3 3
3
Figure 1: Cross Entropy Convergence for vari-
ous training data and models, with EM.
pattern emerges with relatively steady improve-
ment from iteration 1 until performance asymp-
totes. However, when the starting point is better
(Figures 3 and 4), the ?initial? pattern emerges
in which the best performance is reached after a
single iteration.
6 Tuning to a New Domain
When building NLP applications we would want
to be able to tune a parser to a new domain
with minimal manual effort. To obtain training
data in a new domain, annotating a corpus with
partial-bracketing information is much cheaper
than full annotation. To investigate whether
such data would be of value, we considered W
to be the corpus over which we were tuning and
applied the best performing model trained over
S, Cr(IL(?(S)), ?(S)), as our initial model. Fig-
ure 5 illustrates the performance of Cr compared
to EM.
Tuning using Cr was not significantly differ-
ent from the model built directly from the entire
data set with Cr, achieving 76.57% as opposed
to 76.54% F1 (see Table 2). By contrast, EM
performs better given all the data from the be-
ginning rather than tuning to the new domain.
74
74.5
75
75.5
76
76.5
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(S)), ?(S))
EM(IL(), S)
r
r r
r
r
r r r r r r r
r
r r
r
EM(IL(?(S)), S)
b
b b
b
b b
b
b
b b b b b b b
b
b
Figure 2: Performance over S for Cr and EM.
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(W )), ?(W ))
EM(IL(), W )
r
r
r
r r r r
r r r r
r
r r r
r
EM(IL(?(W )),W )
b
b
b
b
b b b b b b
b b b b
b b
b
Figure 3: Performance over W for Cr and EM.
29
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
76.8
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
EM(IL(), SW )
r r
r
r r r r r r r r r r
r r
r
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
Figure 4: Performance over SW for Cr and EM.
Cr generally outperforms EM, though it is worth
noting the behavior of EM given only the tun-
ing data (W ) rather than the data from both do-
mains (SW ). In this case, the graph illustrates a
combination of Elworthy?s ?initial? and ?classical?
patterns. The steep drop in performance (down
to 69.93% F1) after the first iteration is proba-
bly due to loss of information from S. However,
this run also eventually converges to similar per-
formance, suggesting that the information in S
is effectively disregarded as it forms only a small
portion of SW , and that these runs effectively
converge to a local maximum over W .
Bacchiani et al (2006), working in a similar
framework, explore weighting the contribution
(frequency counts) of the in-domain and out-of-
domain training datasets and demonstrate that
this can have beneficial effects. Furthermore,
they also tried unsupervised tuning to the in-
domain corpus by weighting parses for it by
their normalized probability. This method is
similar to our Cp method. However, when we
tried unsupervised tuning using the WSJ and
an initial model built from S in conjunction with
our confidence-based methods, performance de-
graded significantly.
74
74.5
75
75.5
76
76.5
77
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
Cr(Cr(IL(?(S)), ?(S)), W )
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
EM(Cr(IL(?(S)), ?(S)), W )
r
r
r
r
r r
r
r r r r r
r r r
r
EM(Cr(IL(?(S)), ?(S)), SW )
c
c
c
c c c c c
c c c c c c c c
c
Figure 5: Tuning over the WSJ PTB (W ) from
Susanne Corpus (S).
7 Conclusions
We have presented several semi-supervised
confidence-based training methods which have
significantly improved performance over an ex-
tant (more supervised) method, while also re-
ducing the manual effort required to create
training or tuning data. We have shown
that given a medium-sized unlabeled partially
bracketed corpus, the confidence-based models
achieve superior results to those achieved with
EM applied to the same PGLR parse selection
model. Indeed, a bracketed corpus provides flex-
ibility as existing treebanks can be utilized de-
spite the incompatibility between the system
grammar and the underlying grammar of the
treebank. Mapping an incompatible annotated
treebank to a compatible partially-bracketed
corpus is relatively easy compared to mapping
to a compatible fully-annotated corpus.
An immediate benefit of this work is that
(re)training parsers with incrementally-modified
grammars based on different linguistic frame-
works should be much more straightforward ?
see, for example Oepen et al (2002) for a good
discussion of the problem. Furthermore, it sug-
gests that it may be possible to usefully tune
30
a parser to a new domain with less annotation
effort.
Our findings support those of Elworthy (1994)
and Merialdo (1994) for POS tagging and sug-
gest that EM is not always the most suit-
able semi-supervised training method (espe-
cially when some in-domain training data is
available). The confidence-based methods were
successful because the level of noise introduced
did not outweigh the benefit of incorporating
all derivations compatible with the bracketing
in which the derivations contained a high pro-
portion of correct constituents. These findings
may not hold if the level of bracketing available
does not adequately constrain the parses consid-
ered ? see Hwa (1999) for a related investigation
with EM.
In future work we intend to further investigate
the problem of tuning to a new domain, given
that minimal manual effort is a major prior-
ity. We hope to develop methods which required
no manual annotation, for example, high preci-
sion automatic partial bracketing using phrase
chunking and/or named entity recognition tech-
niques might yield enough information to sup-
port the training methods developed here.
Finally, further experiments on weighting the
contribution of each dataset might be beneficial.
For instance, Bacchiani et al (2006) demon-
strate imrpovements in parsing accuracy with
unsupervised adaptation from unannotated data
and explore the effect of different weighting of
counts derived from the supervised and unsu-
pervised data.
Acknowledgements
The first author is funded by the Overseas Re-
search Students Awards Scheme, and the Poyn-
ton Scholarship awarded by the Cambridge Aus-
tralia Trust in collaboration with the Cam-
bridge Commonwealth Trust. Development of
the RASP system was and is supported by the
EPSRC (grants GR/N36462, GR/N36493 and
GR/T19919).
References
Bacchiani, M., Riley, M., Roark, B. and R.
Sproat (2006) ?MAP adaptation of stochas-
tic grammars?, Computer Speech and Lan-
guage, vol.20.1, pp.41?68.
Baker, J. K. (1979) ?Trainable grammars for
speech recognition? in Klatt, D. and Wolf,
J. (eds.), Speech Communications Papers for
the 97th Meeting of the Acoustical Society of
America, MIT, Cambridge, Massachusetts,
pp. 557?550.
Briscoe, E.J., J. Carroll and R. Watson (2006)
?The Second Release of the RASP System?,
Proceedings of ACL-Coling?06, Sydney, Aus-
tralia.
Carroll, J., Briscoe, T. and Sanfilippo, A. (1998)
?Parser evaluation: a survey and a new
proposal?, Proceedings of LREC, Granada,
pp. 447?454.
Clark, S. and J. Curran (2004) ?Parsing the WSJ
Using CCG and Log-Linear Models?, Pro-
ceedings of 42nd Meeting of the Association
for Computational Linguistics, Barcelona,
pp. 103?110.
Collins, M. (1999) Head-driven Statistical Mod-
els for Natural Language Parsing, PhD Dis-
sertation, University of Pennsylvania.
Elworthy, D. (1994) ?Does Baum-Welch Re-
estimation Help Taggers??, Proceedings of
ANLP, Stuttgart, Germany, pp. 53?58.
Gildea, D. (2001) ?Corpus variation and parser
performance?, Proceedings of EMNLP, Pitts-
burgh, PA.
Hockenmaier, J. (2003) Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar, PhD Dissertation, The Univer-
sity of Edinburgh.
Hwa, R. (1999) ?Supervised grammar induction
using training data with limited constituent
information?, Proceedings of ACL, College
Park, Maryland, pp. 73?79.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) ?A new formalization
of probabilistic GLR parsing?, Proceedings
31
of IWPT, MIT, Cambridge, Massachusetts,
pp. 123?134.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple
and R. Kaplan (2003) ?The PARC700 De-
pendency Bank?, Proceedings of LINC, Bu-
dapest.
Lin, D. (1998) ?Dependency-based evaluation
of MINIPAR?, Proceedings of Workshop at
LREC?98 on The Evaluation of Parsing Sys-
tems, Granada, Spain.
McClosky, D., Charniak, E. and M. Johnson
(2006) ?Effective self-training for parsing?,
Proceedings of HLT-NAACL, New York.
Merialdo, B. (1994) ?Tagging English Text with
a Probabilistic Model?, Computational Lin-
guistics, vol.20.2, pp.155?171.
Miyao, Y. and J. Tsujii (2002) ?Maximum En-
tropy Estimation for Feature Forests?, Pro-
ceedings of HLT, San Diego, California.
Oepen, S., K. Toutanova, S. Shieber, C. Man-
ning, D. Flickinger, and T. Brants (2002)
?The LinGO Redwoods Treebank: Motiva-
tion and preliminary applications?, Proceed-
ings of COLING, Taipei, Taiwan.
Pereira, F and Y. Schabes (1992) ?Inside-
Outside Reestimation From Partially
Bracketed Corpora?, Proceedings of ACL,
Delaware.
Prescher, D. (2001) ?Inside-outside estimation
meets dynamic EM?, Proceedings of 7th
Int. Workshop on Parsing Technologies
(IWPT01), Beijing, China.
Riezler, S., T. King, R. Kaplan, R. Crouch,
J. Maxwell III and M. Johnson (2002)
?Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discrimi-
native Estimation Techniques?, Proceedings
of 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia,
pp. 271?278.
Sampson, G. (1995) English for the Computer,
Oxford University Press, Oxford, UK.
Siegel S. and N. J. Castellan (1988) Nonpara-
metric Statistics for the Behavioural Sci-
ences, 2nd edition, McGraw-Hill.
Tadayoshi, H., Y. Miyao and J. Tsujii (2005)
?Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain?,
Proceedings of IJCNLP, Jeju Island, Korea.
Tomita, M. (1987) ?An Efficient Augmented
Context-Free Parsing Algorithm?, Computa-
tional Linguistics, vol.13(1?2), pp.31?46.
Watson, R. and E.J. Briscoe (2007) ?Adapting
the RASP system for the CoNLL07 domain-
adaptation task?, Proceedings of EMNLP-
CoNLL-07, Prague.
Watson, R., J. Carroll and E.J. Briscoe (2005)
?Efficient extraction of grammatical rela-
tions?, Proceedings of 9th Int. Workshop on
Parsing Technologies (IWPT?05), Vancou-
ver, Ca..
32
Proceedings of the Workshop on BioNLP: Shared Task, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Extraction without Training Data
Andreas Vlachos, Paula Buttery, Diarmuid O? Se?aghdha, Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
av308,pjb48,do242,ejb@cl.cam.ac.uk
Abstract
We describe our system for the BioNLP 2009
event detection task. It is designed to be as
domain-independent and unsupervised as pos-
sible. Nevertheless, the precisions achieved
for single theme event classes range from 75%
to 92%, while maintaining reasonable recall.
The overall F-scores achieved were 36.44%
and 30.80% on the development and the test
sets respectively.
1 Introduction
In this paper we describe the system built for the
BioNLP 2009 event detection and characterization
task (Task 1). The approach is based on the output
of a syntactic parser and standard linguistic process-
ing, augmented by rules acquired from the develop-
ment data. The key idea is that a trigger connected
with an appropriate argument along a path through
the syntactic dependency graph forms an event.
The goal we set for our approach was to avoid
using training data explicitly annotated for the task
and to preserve domain independence. While we
acknowledge the utility of supervision (in the form
of annotated data) and domain knowledge, we be-
lieve it is valuable to explore an unsupervised ap-
proach. Firstly, manually annotated data is ex-
pensive to create and the annotation process itself
is difficult and unavoidably results in inconsisten-
cies, even in well-explored tasks such as named en-
tity recognition (NER). Secondly, unsupervised ap-
proaches, even if they fail to reach the performance
of supervised ones, are likely to be informative in
identifying useful features for the latter. Thirdly, ex-
ploring the potential of such a system may highlight
what domain knowledge is useful and its potential
contribution to performance. Finally, preserving do-
main independence allows us to develop and evalu-
ate a system that could be used for similar tasks with
minimal adaptation.
The overall architecture of the system is as fol-
lows. Initiallly, event triggers are identified and la-
belled with event types using seed terms. Based on
the dependency output of the parser the triggers are
connected with candidate arguments using patterns
identified in the development data. Anaphoric can-
didate arguments are then resolved. Finally, the trig-
gers connected with appropriate arguments are post-
processed to generate the final set of events. Each
of these stages are described in detail in subsequent
sections, followed by experiments and discussion.
2 Trigger identification
We perform trigger identification using the assump-
tion that events are triggered in text either by verbal
or nominal prdicates (Cohen et al, 2008).
To build a dictionary of verbs and their associ-
ated event classes we use the triggers annotated in
the training data. We lemmatize and stem the trig-
gers with the morphology component of the RASP
toolkit (Briscoe et al, 2006)1 and the Porter stem-
mer2 respectively. We sort the trigger stem - event
class pairs found according to their frequency in
the training data and we keep only those pairs that
appear at least 10 times. The trigger stems are
then mapped to verbs. This excludes some rela-
tively common triggers, which will reduce recall,
but, given that we rely exclusively on the parser for
1http://www.cogs.susx.ac.uk/lab/nlp/rasp/
2http://www.tartarus.org/?martin/PorterStemmer
37
argument extraction, such triggers would be difficult
to handle. For verbs with more than one event class
we keep only the most frequent one.
We consider the assumption that each verb de-
notes a single event class to be a reasonable one
given the restricted task domain. It hinders us from
dealing with triggers denoting multiple event classes
but it simplifies the task so that we do not need anno-
tated data. While we use the training data triggers to
obtain the list of verbs and their corresponding event
types, we believe that such lists could be obtained by
clustering (Korhonen et al, 2008) with editing and
labelling by domain experts. This is the only use of
the training data we make in our system.
During testing, using the tokenized text provided,
we attempt to match each token with one of the
verbs associated with an event type. We perform
this by relaxing the matching successively, using the
token lemma, then stem, and finally allowing a par-
tial match in order to deal with particles (so that e.g.
co-transfect matches transfect). This process returns
single-token candidate triggers which, while they do
not reproduce the trigger annotation, are likely to be
adequate for event extraction. We overgenerate trig-
gers, since not all occurrences denote an event, ei-
ther because they are not connected with appropriate
arguments or because they are found in a non-event
denoting context, but we expect to filter these at the
argument extraction stage.
3 Argument extraction
Given a set of candidate triggers, we attempt to con-
nect them with appropriate arguments using the de-
pendency graph provided by a parser. In our ex-
periments we use the domain-independent unlexi-
calized RASP parser, which generates parses over
the part-of-speech (PoS) tags of the tokens generated
by an HMM-based tagger trained on balanced En-
glish text. While we expect that a parser adapted to
the biomedical domain may perform better, we want
to preserve the domain-independence of the system
and explore its potential.
The only adjustment we make is to change the
PoS tags of tokens that are part of a protein name
to proper names tags. We consider such an adjust-
ment domain-independent given that NER is avail-
able in many domains (Lewin, 2007). Following
Haghighi et al(2005), in order to ameliorate pars-
ing errors, we use the top-10 parses and return a
set of bilexical head-dependent grammatical rela-
tions (GRs) weighted according to the proportion
and probability of the top parses supporting that GR.
The GRs produced by the parser define directed
graphs between tokens in the sentence, and a partial
event is formed when a path that connects a trigger
with an appropriate argument is identified. GR paths
that are likely to generate events are selected using
the development data, which does not contradict the
goals of our approach because we do not require an-
notated training data. Development data is always
needed in order to build and test a system, and such
supervision could be provided by a human expert,
albeit not as easily as for the list of trigger verbs.
The set of GR paths identified follow:
VERB-TRIGGER ?subject? ARG
NOUN-TRIGGER ?iobj? PREP ?dobj? ARG
NOUN-TRIGGER ?modifier? ARG
TRIGGER ?modifier? PREP ?obj? ARG
TRIGGER ?passive subject? ARG
The final system uses three sets of GR paths:
one for Regulation events; one for Binding events;
and one for all other events. The difference be-
tween these sets is in the lexicalization of the link-
ing prepositions. For example, in Binding events
the linking preposition required lexicalization since
binds x to/with y denotes a correct event but not
binds x by y. Binding events also required additional
GR paths to capture constructions such as binding of
x to y. For Regulation events, the path set was fur-
ther augmented to differentiate between theme and
cause. When the lexicalized GR pattern sets yielded
no events we backed-off to the unlexicalized pattern
set, which is identical for all event types. In all GR
path sets, the trigger was unlexicalized and only re-
stricted by PoS tag.
4 Anaphora resolution
The events and arguments identified in the parsed
abstracts are post-processed in context to iden-
tify protein referents for event arguments that are
anaphoric (e.g., these proteins, its phosphorylation)
or too complex to be extracted directly from the
grammatical relations (phosphorylation of cellular
proteins , notably phospholipase C gamma 1). The
38
anaphoric linking is performed by a set of heuris-
tic rules manually designed to capture a number of
common cases observed in the development dataset.
A further phenomenon dealt with by rules is coref-
erence between events, for example in The expres-
sion of LAL-mRNA is induced. This induction is de-
pendent on. . . where the Induction event described
by the first sentence is the same as the theme of the
Regulation event in the second and should be given
the same event index. The development of the post-
processing rules favoured precision over recall, but
the low frequency of each case considered means
that some overfitting to the development data may
have been unavoidable.
5 Event post-processing
At the event post-processing stage, we form com-
plete events considering the trigger-argument pairs
produced at the argument extraction stage whose ar-
guments are resolved (possibly using anaphora res-
olution) either to a protein name or to a candidate
trigger. The latter are considered only for regula-
tion event triggers. Furthermore, regulation event
trigger-argument pairs are tagged either as theme or
cause at the argument extraction stage.
For each non-regulation trigger-argument pair, we
generate a single event with the argument marked as
theme. Given that we are dealing only with Task
1, this approach is expected to deal adequately with
all event types except Binding, which can have mul-
tiple themes. Regulation events are formed in the
following way. Given that the cause argument is
optional, we generate regulation events for trigger-
argument pairs whose argument is a protein name or
a trigger that has a formed event. Since regulation
events can have other regulation events as themes,
we repeat this process until no more events can be
formed. Occasionally, the use of multiple parses re-
sults in cycles between regulation triggers which are
resolved using the weighted GR scores. Then, we at-
tach any cause arguments that share the same trigger
with a formed regulation event.
In the analysis performed for trigger identification
in Section 2, we observed that certain verbs were
consistently annotated with two events (namely
overexpress and transfect), a non-regulation event
and a regulation event with the former event as its
theme. For candidate triggers that were recognized
due to such verbs, we treat them as non-regulation
events until the post-processing stage where we gen-
erate two events.
6 Experiments - Discussion
We expected that our approach would achieve high
precision but relatively low recall. The evaluation
of our final submissions on the development and test
data (Table 1) confirmed this to a large extent. For
the non-regulation event classes excluding Binding,
the precisions achieved range from 75% to 92% in
both development and test data, with the exception
of Transcription in the test data. Our approach ex-
tracts Binding events with a single theme, more suit-
ably evaluated by the Event Decomposition evalua-
tion mode in which a similar high precision/low re-
call trend is observed, albeit with lower scores.
Of particular interest are the event classes for
which a single trigger verb was identified, namely
Transcription, Protein catabolism and Phosphoryla-
tion, which makes it easier to identify the strengths
and weaknesses of our approach. For the Phos-
phorylation class, almost all the triggers that were
annotated in the training data can be captured us-
ing the verb phosporylate and as a result, the per-
formances achieved by our system are 70.59% and
60.63% F-score on the development and test data re-
spectively. The precision was approximately 78% in
both datasets, while recall was lower due to parser
errors and unresolved anaphoric references. For the
Protein catabolism class, degrade was identified as
the only trigger verb, resulting in similar high preci-
sion but relatively lower recall due to the higher lex-
ical variation of the triggers for this class. For the
Transcription class we considered only transcribe
as a trigger verb, but while the performance on the
development data is reasonable (55%), the perfor-
mance on the test data is substantially lower (20%).
Inspecting the event triggers in the training data re-
veals that some very common triggers for this class
either cannot be mapped to a verb (e.g., mrna) or are
commonly used as triggers for other event classes.
A notable case of the latter type is the verb express,
which, while mostly a Gene Expressions trigger, is
also annotated as Transcription more than 100 times
in the training data. Assuming that this is desirable,
39
Development Test
Event Class recall precision fscore recall precision fscore
Localization 45.28 92.31 60.76 25.86 90.00 40.18
Binding 12.50 24.41 16.53 12.68 31.88 18.14
Gene expression 52.25 80.79 63.46 45.57 75.81 56.92
Transcription 42.68 77.78 55.12 12.41 56.67 20.36
Protein catabolism 42.86 81.82 56.25 35.71 83.33 50.00
Phosphorylation 63.83 78.95 70.59 49.63 77.91 60.63
Event Total 39.03 65.97 49.05 33.16 68.15 44.61
Regulation 20.12 50.75 28.81 9.28 36.49 14.79
Positive regulation 16.86 48.83 25.06 11.39 38.49 17.58
Negative regulation 11.22 36.67 17.19 6.86 36.11 11.53
Regulation Total 16.29 47.06 24.21 9.98 37.76 15.79
Total 26.55 58.09 36.44 21.12 56.90 30.80
Binding (decomposed) 26.92 66.14 38.27 18.84 54.35 27.99
Table 1: Performance analysis on development and test data using Approximate Span/Partial Recursive Matching.
a more appropriate solution would need to take con-
text into account.
Our performance on the regulation events is sub-
stantially lower in both recall and precision. This
is expected, as they rely on the extraction of non-
regulation events. The variety of lexical triggers is
not causing the drop in performance though, since
our system performed reasonably well in the Gene
Expression and Localization classes which have
similar lexical variation. Rather it is due to the com-
bination of the lexical variation with the requirement
to make the distinction between the theme and op-
tional cause argument, which cannot be handled ap-
propriately by the small set of GR paths employed.
The contribution of anaphora resolution to our
system is limited as it relies on the argument ex-
traction stage which, apart from introducing noise,
is geared towards maintaining high precision. Over-
all, it contributes 22 additional events on the de-
velopment set, of which 14 out of 16 are correct
non-regulation events. Of the remaining 6 regula-
tion events only 2 were correct. Similar trends were
observed on the test data.
7 Conclusions - Future work
We described an almost unsupervised approach for
the BioNLP09 shared task on biomedical event ex-
traction which requires only a dictionary of verbs
and a set of argument extraction rules. Ignoring trig-
ger spans, the performance of the approach is parser-
dependent and while we used a domain-independent
parser in our experiments we also want to explore
the benefits of using an adapted one.
The main weakness of our approach is the han-
dling of events with multiple arguments and the dis-
tinctions between them, which are difficult to deal
with using simple unlexicalized rules. In our fu-
ture work we intend to explore semi-supervised ap-
proaches that allow us to acquire more complex
rules efficiently.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL Interactive presentation ses-
sions, pages 77?80.
Kevin B. Cohen, Martha Palmer, and Lawrence Hunter.
2008. Nominalization and alternations in biomedical
language. PLoS ONE, 3(9).
Aria Haghighi, Kristina Toutanova, and Chris Manning.
2005. A Joint Model for Semantic Role Labeling. In
Proceedings of CoNLL-2005: Shared Task.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In Proceedings of Coling.
Ian Lewin. 2007. BaseNPs that contain gene names:
domain specificity and genericity. In Proceedings of
the ACL workshop BioNLP: Biological, translational,
and clinical language processing, pages 163?170.
40
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1740?1751, Dublin, Ireland, August 23-29 2014.
Detecting Learner Errors in the Choice of Content Words
Using Compositional Distributional Semantics
Ekaterina Kochmar
Computer Laboratory
University of Cambridge
ek358@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
We describe a novel approach to error detection in adjective?noun combinations. We present and
release a new dataset of annotated errors where the examples are extracted from learner texts and
annotated with error types. We show how compositional distributional semantic approaches can
be applied to discriminate between correct and incorrect word combinations from learner data.
Finally, we show how the output of the compositional distributional semantic models can be used
as features in a classifier yielding good precision and accuracy.
1 Introduction
The task of error detection and correction (henceforth, EDC) in non-native writing in English has been
a focus of research in recent years. However, usually research in this area focuses on EDC in the use of
function words, such as articles or prepositions (Leacock et al., 2010; Dale et al., 2012), while much less
attention has been paid to errors in the choice of content words.
Errors in function words are some of the most common error types in learner writing (Dalgish, 1985;
Leacock et al., 2010), so it is important for any EDC system to be able to deal with such errors. Certain
properties of these errors facilitate their detection and correction. As function words belong to closed
classes, the set of possible corrections is limited by the size of the function word set. Since errors in
function words are systematic and highly recurrent, in practice, each article or preposition has an even
smaller number of appropriate alternatives. We illustrate this point with the following examples on (1)
article and (2) preposition errors:
(1) I am 0*/a student.
(2) Last October, I came in*/to Tokyo.
In (1) an EDC system would consider {a, an, the} as possible corrections for the missing article. To
correct the preposition in in (2), an EDC system would consider the most frequent prepositions {on,
from, for, of, about, to, at, with, by}, among which at or to would have a higher chance to be appropriate
corrections as these are most often confused with in. Confusion sets can be learnt from learner texts, and
probabilities can be set up according to the distribution of the confusions (Rozovskaya and Roth, 2011).
EDC is usually cast as a multi-class classification task, with the number of classes equal to the number
of target corrections. Detection and correction can occur simultaneously: an error is detected when an
EDC system suggests using a word different from the one originally used by the learner, and the sug-
gested word can be used as a correction. Each occurrence of a function word is represented with a feature
vector, where features are derived from the surrounding context. This is usually highly informative for
function words: for example, a context of I am and student or a similar noun requires the use of an
indefinite article, while the only correct preposition to relate a verb of movement like come to a locative
like Tokyo is to.
In this work, however, we focus on errors in the choice of content words, which have received much
less attention in spite of being the third most frequent error type in learner writing (Leacock et al., 2010).
Errors in content words are more challenging than errors in function words, since the number of possible
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1740
confusions and corrections cannot be reduced to a finite set. For example, consider incorrect choice of
adjectives in the following sentences extracted from learner data:
(3) A big*/great damage has been made to the environment.
(4) I have tried a rock?n?roll dance and a classic*/classical dance already.
The confusion in (3) is caused by semantic similarity of the adjectives big and great, while in (4) it is
due to similarity in form between classic and classical. It is much harder to cast the EDC in content words
as multi-class classification, unless we consider the full set of English adjectives as possible classes. The
surrounding contexts are much sparser and less informative, and in addition to that, often contain further
errors. In this work, we address error detection and focus on adjective-noun combinations (ANs), which
are representative of the more general task of EDC in content word combinations and are a frequent error
type in learner text.
We have created a dataset of ANs, where the combinations are extracted from learner texts and man-
ually error-coded using a novel annotation scheme. This scheme is motivated by observations about
typical learner confusions in the choice of adjectives and nouns ? for example, semantically-related or
form-related confusions. Since errors in content words are related to semantics, we derive semantically-
motivated features through models of compositional distributional semantics and use these features for
error detection. We treat error detection as a binary classification task, following the usual convention in
EDC.
The original contributions of this paper are that we:
? present and release an error-annotated AN dataset extracted from learner data;
? show how compositional distributional semantic models can be applied to detect semantic anomalies
in this dataset;
? demonstrate that the output of these models can be used to derive features for error detection in AN
combinations.
2 Previous work
2.1 Error Detection in Content Words
Previous work on EDC for content words has either focused on correction alone assuming that errors
are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing
improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al.,
2008; Yi et al., 2008;
?
Ostling and Knutsson, 2009).
In the first case, the task is reduced to the search for the most suitable correction among the alternatives
typically composed of synonyms, homophones or L1-related paraphrases (Dahlmeier and Ng, 2011),
while the more challenging error detection step is omitted. In the second case, error detection is integrated
into suggestion of alternatives and their comparison to the originally used word combination according
to some metric of collocational strength. Such approaches aim to improve the fluency of non-native
texts by correcting erroneous idioms or collocations, where low frequency or low collocational strength
clearly signifies an error.
These approaches might be useful for correcting collocations, but they are less suitable for error detec-
tion in free word combinations. As they compare original word combinations to their alternatives using
corpus statistics, they are not applicable to unseen word combinations, while learner texts contain many
previously unseen combinations, not all of which are errors. Moreover, some word combinations may
be correct even though less fluent than some of their alternatives. For example, appropriate concern,
though it is correct, would have lower collocational strength than its alternative proper concern, and
would, according to this approach, be tagged as an error. From the educational point of view, tagging an
acceptable combination as an error is misleading for language learners and should be avoided.
We implement a baseline model inspired by such comparison-based approaches and demonstrate that it
cannot be usefully applied to error detection in content word combinations. Then we present an approach
that is capable of dealing with unseen data and does not rely on direct corpus-based comparison.
1741
2.2 Semantic Anomaly Detection
Learner errors in content words often result from a semantic mismatch between the chosen words. A
similar problem of semantic anomaly detection in content word combinations has been addressed with
compositional distributional semantic models.
These models are based on distributional representations for words which are then composed to derive
phrase representations. They rely on the assumption that a word meaning can be approximated by its
distribution across its contexts of use. Words are represented as vectors in a high-dimensional space with
each dimension encoding a word?s co-occurrence with one of its contextual elements. Distributional
models are less suitable for representing content word combinations directly since these will be very
sparse and will often remain unattested even in an extremely large corpus.
A promising solution is provided by compositional distributional semantic models, which combine
distributional vectors for the component words using some function over such vectors. Compositional
distributional semantic representations have been previously used to detect semantic anomaly in AN
combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models
of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a
set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional
semantic representations for the semantically acceptable and anomalous combinations, suggesting that
compositional distributional models can be used to detect semantic anomaly without relying directly on
corpus statistics.
Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish
between correct and incorrect ANs extracted from learner texts. Their results support the assumption
that there is a distinguishable difference between the composite vectors for the correct and incorrect
ANs, but they did not address the question of how to integrate these semantic models into an error
detection system.
Recent work by Lazaridou et al. (2013) has shown that measures used for quantifying the degree of
semantic anomaly in phrases derived from their compositional distributional semantic representations
can be used as features by a classifier to help resolve syntactic ambiguities.
Our goals are to test, using a new and larger AN dataset, whether semantic models can distinguish
between correct and incorrect AN combinations, which cannot be dealt with using simpler error detection
approaches, and to implement an error detection system using these semantically-based features.
3 Data Annotation
We present and release a dataset of AN combinations which, on the one hand, exemplify the typical
errors committed by language learners in the choice of content words within such combinations, and, on
the other hand, are challenging for an EDC system.
For that, we examined the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011), used
the error annotation (Nicholls, 2003), and analysed the typical errors in AN combinations committed by
language learners. We have compiled a list of 61 adjectives that are most problematic for learners.
Most typically, learners confuse semantically related words: for example, they are unable to distin-
guish between synonyms, near-synonyms or co-hyponyms and choose an appropriate one from the set.
Our list of adjectives contains some frequent ones that are confused with each other due to their similarity
in meaning. For example, the adjectives within the set {big, large, great} are frequently confused with
each other as in:
(5) big*/large quantity (6) big*/great importance
Another common source of error related to the high-frequency adjectives involves using them instead
of more specific ones: in such cases, learners are unable to distinguish between the more specific terms
and they choose the most frequent adjective, usually encompassing a variety of related meanings, to
represent the whole class of similar words. For example, adjectives big and large encompass a variety of
meanings including those of high, wide or broad. As learners often lack intuitions about which of these
1742
more specific adjectives should be chosen, they use the ones with more general meaning. This results in
errors like:
(7) big*/long history
(8) bigger*/wider variety
(9) greatest*/highest revenue
(10) large*/broad knowledge
The reverse of this ? an incorrect selection of a more specific term instead of the more general one ?
also leads to learner errors.
Form-related confusions represent another typical source of learner errors, and we have included pairs
of adjectives such as classic and classical, economic and economical and the like in our dataset:
(11) classic*/classical dance (12) economical*/economic crisis
Using this set of 61 adjectives, we have extracted AN combinations from the Cambridge Learner
Corpus (CLC),
1
a large corpus of texts produced by English language learners, sitting Cambridge As-
sessment?s examinations.
2
We have focused on AN combinations previously unseen in a native English
corpus, as we hypothesise that they would have a higher chance of containing an error. Such combina-
tions are more challenging for EDC algorithms since:
? these ANs cannot be effectively handled with simple comparison-based approaches like the ones
overviewed in section 2.1;
? language learners are creative in their writing, so there is a substantial number of such previously
unseen combinations;
? as no corpus could cover all possible acceptable content word combinations in language, the fact that
these combinations are not seen in the corpus cannot be used as definitive evidence of incorrectness.
To summarise, it is important for an EDC algorithm to handle such combinations, but their absence in
a native corpus of English makes it impossible to rely on simpler approaches and suggests that semantic
analysis of such combinations would be more effective. In our research, we used the British National
Corpus (BNC)
3
to select the corpus-unattested combinations.
We have compiled a set of 798 AN combinations.
4
An annotation scheme has been devised to annotate
these examples as correct or incorrect, and for the incorrect combinations, to identify the locus of error
(adjective, noun or both) and the type of confusion (incorrect synonym, form-related word, or non-related
word). The most appropriate corrections are included in the dataset.
We also distinguish between out-of-context (OOC) and in-context (IC) annotation. The motivation
behind this distinction is as follows: some combinations may appear to be correct when considered
out of their original context of use, because there might be other contexts where the same combination
would be appropriate. For example, classic dance is annotated as correct out of context because one
could imagine using it in a context where it would denote some typical dance like:
(13) They performed a classic Ceilidh dance.
However, in practice, the AN classical dance is used much more frequently, and classic dance is most
often errorful in context, as in (4) above.
Some ANs in our dataset are represented with more than one context of use, and in that case the
in-context annotation can be conditioned on each context, or used to derive the most typical annotation
for the AN. Both types of information are useful, as EDC systems which make use of the surrounding
context should rely on the annotation in each particular context of use and, for example, be able to detect
1
http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/
Cambridge-International-Corpus-Cambridge-Learner-Corpus/
2
http://www.cambridgeenglish.org
3
http://www.natcorp.ox.ac.uk/
4
This dataset is released and publicly-available at http://www.ilexir.com/
1743
Type Cor. Incor. LB UB
OOC 633 165 0.7932 0.8650
IC 394 404 0.5063 0.7467
Table 1: Distribution of correct (cor.) and incorrect (incor.) ANs in the dataset.
that classic dance is correct in one specific context, while in others it is incorrect. EDC systems that do
not make use of the context can simply rely on the most frequent in-context annotation and detect that
classic dance is typically an error in learner writing.
To create the two-level annotation, the annotators were first presented with an AN combination and
asked to tag each word as correct or incorrect depending on whether they can think of some appropriate
contexts of use for it. Next, the same combination was presented in its context of use from the CLC and
the annotators were asked to annotate it with respect to its context.
The dataset was primarily annotated by a professional linguist. To ensure that the annotation scheme
is clear and efficient, the dataset was split into 100 and 698 ANs, and the 100 ANs were first annotated
by the same professional annotator and three other annotators. We have measured the inter-annotator
agreement for the two levels of annotation using the mean values for the observed agreement within
each pair of annotators, as well as mean Cohen?s kappa value (Cohen, 1960). In Table 1 we report
the mean inter-annotator agreement for the correct versus incorrect combinations at the two annotation
levels, which represents the upper bound (UB) in our experiments. We have obtained the mean kappa
values of 0.65 and 0.49 at the two levels of annotation, which are interpreted as substantial and medium
agreement between annotators and confirm that the annotation scheme is clear.
5
Table 1 presents the
distribution of ANs and the majority class baseline which we further use as a lower bound (LB).
4 Semantic Models for Error Detection
We replicate the semantic approaches, which have previously shown promising results in detecting se-
mantic anomaly and content word errors (Vecchi et al., 2011; Kochmar and Briscoe, 2013), and test their
performance on our dataset of corpus-unattested correct and incorrect AN combinations.
4.1 Experimental Setting
We use the additive (add) and multiplicative (mult) models of Mitchell and Lapata (2008), and the
adjective-specific linear maps (alm) of Baroni and Zamparelli (2010).
The first two models derive the composite phrase vector through addition and multiplication of the
components of the word vectors. These models have a clear mathematical interpretation and require
no training. Their principal weakness is that they are symmetric, and fail to represent the difference in
grammatical function of the component words. The alm model provides a theoretically more appropriate
way of representing ANs based on this asymmetry: nouns are represented by their distributional vectors,
while attributive adjectives are functions mapping from noun meanings to a composite noun-like vector
for the ANs. Adjectives are represented as weight matrices which are learned from corpus-attested
examples of noun?AN mappings, and composition is defined by matrix-by-vector multiplication.
We use the experimental setting previously described (Vecchi et al., 2011; Kochmar and Briscoe,
2013) and populate the semantic space with the constituent nouns and adjectives from the test ANs,
frequent nouns and adjectives from the BNC and the AN combinations containing these frequent words.
We use about 8K nouns, 4K adjectives and 64K ANs following Kochmar and Briscoe (2013). The
semantic space is represented by a matrix encoding word co-occurrences, where the rows represent the
76K elements mentioned above, and the columns represent a selected set of 10K context elements.
The 10K context elements include the most frequent nouns, adjectives and verbs from the corpus. The
word co-occurrence counts are estimated using the BNC. The corpora have been lemmatized, tagged and
parsed with the RASP system (Briscoe et al., 2006; Andersen et al., 2008; Yannakoudakis et al., 2011),
and all statistics are extracted at the lemma level.
5
Further details of the annotation experiment are described in the dataset release.
1744
We transform the raw sentence-internal co-occurrence counts into Local Mutual Information
scores (Baroni and Zamparelli, 2010; Evert, 2005), and perform dimensionality reduction applying Sin-
gular Value Decomposition to the noun and adjective matrix rows, projecting the AN rows onto the same
reduced space following Baroni and Zamparelli (2010). The original 76K ? 10K matrix is reduced to a
76K ? 300 matrix. This allows us to perform training and other calculations in the semantic space more
efficiently.
The weight coefficients for the alm model are estimated with multivariate partial least squares re-
gression using the RPLS package (Mevik and Wehrens, 2007). The weight matrix is learned for each
adjective separately.
4.2 Semantic Cues
In previous work (Vecchi et al., 2011; Kochmar and Briscoe, 2013) several semantic measures for de-
tecting semantic anomaly have been introduced. We reimplement these measures (1 to 8), but also test
some additional measures (9 to 13) that we hypothesise can also help distinguish between correct and
incorrect word combinations:
1. Vector length (VLen): vectors for correct and incorrect combinations may differ with respect to
their length, and the latter are expected to be shorter;
2. Cosine to the input noun (cosN): the distance between the model-generated AN vector and the
input noun vector is expected to be greater for the incorrect combinations, as the noun meaning is
typically ?distorted?;
3. Cosine to the input adjective (cosA): analogical to cosN measure, the adjective meaning might be
?distorted? as well, especially as two of the composition functions are symmetric;
4. Density of the neighbourhood populated by 10 nearest neighbours (dens) is calculated as the
average distance from the model-generated vector to the 10 nearest neighbours in the original se-
mantic space, and is expected to be higher for the correct ANs;
5. Density among the 10 nearest neighbours (densAll) is a modification of dens, which is estimated
as an average for the 11 density values calculated for each member within the set consisting of the
AN vector and its 10 neighbours;
6. Ranked density in close proximity (Rdens) relies on the notion of close proximity, which is defined
as a neighbourhood populated by some very close neighbours (for example, within a distance of
? 0.8). It is calculated as: RDens =
?
N
i=1
rank
i
distance
i
with N being the total number of
close neighbours within close proximity, each with its rank and distance;
7. Number of neighbours within close proximity (num) is used as another measure, and is assumed
to be lower for incorrect combinations, which are expected to be more isolated in the semantic
space;
8. Overlap between the 10 nearest neighbours and constituent noun/adjective (OverAN) assumes
correct ANs should be surrounded by similar words and combinations. It is calculated as the pro-
portion of the 10 nearest neighbours containing the same constituent words as in the tested ANs;
9. Overlap between the 10 nearest neighbours and input noun (OverN) is a variant of the OverAN
with only the noun considered;
10. Overlap between the 10 nearest neighbours and input adjective (OverA) is a variant of the
OverAN with only the adjective considered;
11. Overlap between the 10 nearest neighbours for the AN and constituent noun/adjective
(NOverAN) assumes that correct ANs and their constituent words should be placed in similar neigh-
bourhoods. It is calculated as the proportion of the common neighbours among the 10 nearest
neighbours for the model-generated AN and the constituent words;
1745
Metric add mult alm
VLen 0.7589 0.7690 0.1676
cosN 0.1621 0.0248 0.0227
cosA 0.0029 0.4782 0.0921
dens 0.6731 0.1182 0.1024
densAll 0.4967 0.1026 0.1176
RDens 0.2786 0.8754 0.1970
num 0.3132 0.4673 0.3765
OverAN 0.8529 0.1622 0.2808
OverA 0.0151 0.6377 0.4886
OverN 0.0138 0.0764 0.4118
NOverAN 0.3941 0.6730 0.0858
NOverA 0.0009 0.3342 0.1575
NOverN 0.0018 0.1463 0.1497
Table 2: p values, out-of-context annotation
Metric add mult alm
VLen 0.6675 0.0027 0.0111
cosN 0.0417 0.0070 0.1845
cosA 0.00003 0.1791 0.1442
dens 0.4756 0.7120 0.1278
densAll 0.2262 0.7139 0.5310
RDens 0.8934 0.8664 0.1985
num 0.7077 0.7415 0.4259
OverAN 0.1962 0.8635 0.5669
OverA 0.00007 0.7271 0.6229
OverN 0.0017 0.9680 0.7733
NOverAN 0.0227 0.3473 0.1587
NOverA 0.000004 0.3749 0.1576
NOverN 0.0001 0.6651 0.2610
Table 3: p values, in-context annotation
12. Overlap between the 10 nearest neighbours for the AN and input noun (NOverN) is a variant
of the NOverAN with only the noun considered;
13. Overlap between the 10 nearest neighbours for the AN and input adjective (NOverA) is a
variant of the NOverAN with only the adjective considered.
4.3 Results
We evaluate the models and report the results following the procedure that has been used before in Vecchi
et al. (2011) and Kochmar and Briscoe (2013). For each model and semantic measure, we report the p
value denoting statistical significance of the difference between the groups of correct and incorrect ANs.
The statistical significance is reported at the p<0.05 level, and if a measure applied to the two groups of
ANs shows statistically significant difference we interpret that as an ability of this measure to distinguish
the correct ANs from the incorrect ones in general. The results for the out-of-context annotation are
reported in Table 2, and those for the in-context annotation in Table 3.
The results show that the difference between the vector representations for the correct and incorrect AN
combinations can be reliably detected with a number of the proposed measures. Measures which show
statistically significant results with at least one model are marked in bold. These results also suggest that
the values for the semantic measures can be used to derive discriminative features for a classifier.
5 Error Detection as Classification Task
5.1 Baseline System
We implement a simple comparison-based baseline system inspired by previous work on error detection
in content words (see section 2.1). For every AN, we create a set of possible alternatives crossing the
confusion set for the adjective with that for the noun, and compare the collocational strength of the
original combination with that for each of the alternatives. If an alternative has higher collocational
strength than the original combination, the original combination is tagged as an error and the alternative
is chosen as a correction. Since semantically related confusions are a rich source of learner errors in
content word combinations, we include adjective synonyms in the confusion set for an adjective, and
noun synonyms and hyponyms in the confusion set for a noun. All synonyms and hyponyms are retrieved
using WordNet 3.0 without word sense disambiguation.
We measure collocational strength using normalized pointwise mutual information (npmi) of the ad-
jective a and noun n, which is defined as:
1746
npmi(a, n) =
pmi(a, n)
?log[p(a, n)]
(1) pmi(a, n) = log
p(a, n)
p(a)p(n)
(2)
All probabilities are estimated from the BNC. This approach performs poorly on the unseen ANs in
our dataset, since any alternative AN seen in the BNC would be preferred by this system over the original
unseen AN. This ensures that less fluent (in this case, unseen) word combinations are substituted with
more fluent (seen) ones. As a result, even though an original AN important conversation in our dataset
is correct, it is still ?corrected? by this system to serious conversation. At the same time, some incorrect
combinations are not recognised if no appropriate alternative is found (e.g., *high shyness). It shows that
this approach lacks deeper semantic analysis and is also too dependent on the set of alternatives found
for a word combination.
We measure accuracy (acc) as the proportion of true positives (TP) and true negatives (TN) to the total
number of test items:
Acc =
TP + TN
TP + FP + TN + FN
(3)
Accuracy reflects how often an error detection system correctly identifies that an AN is correct or
incorrect. We compare the results to the lower and upper bounds set as the majority class distribution
and inter-annotator agreement, respectively (see section 3).
With this approach we get quite low accuracy of 0.3897 on the out-of-context annotation since most
of the test items are correct out of context (LB=0.7932), and the baseline system overcorrects many of
those. Accuracy of the baseline system on the in-context annotation is 0.5147, which is slightly above
the lower bound of 0.5063. These results are used as a baseline and included in Table 4.
Type Accuracy Baseline LB UB
OOC 0.8113 ? 0.0149 0.3897 0.7932 0.8650
IC 0.6535 ? 0.0189 0.5147 0.5063 0.7467
Table 4: Decision Tree classification results
Type P (correct) P (incorrect)
OOC 0.8193 0.7500
IC 0.6241 0.6850
Table 5: Classification precision
5.2 Classification
We implement a supervised classifier which uses output of the semantic models as features. We have
tested a number of classifier models but the best results so far have been obtained with the Decision
Tree classifier using NLTK (Bird et al., 2009). We assume that this classifier effectively learns the
inter-dependencies between the features within the small feature set that we use in our experiments. We
use feature binning where the whole range of feature values is divided into 10 bins according to the
distribution of values for each feature. This feature representation technique combined with the classifier
helps generalise over feature values, reducing feature space dimensionality. The order of the feature
application to the data is determined by the classifier on the basis of the information gain for the features
and their values.
We apply 5-fold cross-validation and report average accuracy over the folds. The 798 ANs are split
into 5 subsets with 80% in each of the splits used for training and 20% for testing. We keep the AN error
rate in the training and test sets, as well as for each adjective, approximately the same across the splits to
avoid any bias. Error detection is cast as a binary classification task. The output of the semantic models
is used to derive numeric features for the classifier. Most values are in the range of [0, 1], and we apply
normalisation to VLen, RDens and num which originally have a different range.
The full feature set contains 14 features, with 13 features derived from the semantic measures, and
1 feature representing adjective identity. We hypothesise that introduction of this feature might help
classifier learn that, for example, an AN containing an adjective classic has a higher chance of being
incorrect, as most of the ANs with this adjective in the learner data are incorrect and involve confusions
with classical. We also hypothesise that it facilitates learning correlations between the adjective and other
1747
feature values: it might be the case that ANs with an adjective adj
1
, on the average, have higher cosN
values than ANs with an adjective adj
2
. This feature helps the classifier establish such dependencies
between the adjective and the values of the semantic measures. For instance, in our data ANs with
the adjective true have significantly higher cosine between AN vectors and vectors for their constituent
nouns than ANs with the adjective false: this is in accordance with an intuition that, for example, true
happiness is more similar to happiness than false happiness is.
The best results in our experiments have been obtained with the mult model. We have performed
ablation tests incrementally removing features that did not improve classifier performance in order to
find an optimal feature set. The best-performing feature set we found for the mult model on the out-
of-context annotation uses adjective, cosN and RDens features, while for the in-context annotation the
best-performing feature set found uses a combination of features including adjective, VLen, densAll,
NOverA, NOverN, RDens and num features.
We note that the sets of best performing features in the classification experiments do not coincide with
the semantic measures that showed the highest statistically significant difference (Tables 2 and 3). We
conclude that although the p values reported in Tables 2 and 3 show that some semantic measures can
distinguish one group of ANs from another on the basis of the statistically significant difference between
the means of the two groups, when the measures are used as features for a classifier the results depend
on how these features interact with each other as well as on their individual discriminativeness across the
test dataset. For example, Figure 1 illustrates a small part of the decision tree constructed using the best
performing feature set on the in-context annotation:
Figure 1: Decision Tree classifier pseudocode.
Figure 1 shows how interaction of feature values for num and VLen in combination with the adjective
identity feature can help classify the two ANs containing adjective large as correct (1) or incorrect (-1).
In Table 4 we report results for the out-of-context (OOC) and in-context (IC) annotation. The accuracy
is reported with its mean ? standard deviation over the 5 data splits. We compare the Decision Tree
classifier results to those obtained with the baseline system, as well as to the lower and upper bounds set
as before (see section 3). The results show that a classifier that uses output of the semantic models as
features outperforms the comparison-based baseline system by a large margin.
6 Discussion
In the previous section, we showed that a classifier that uses output of the semantic models as features
outperforms the comparison-based baseline system and shows good accuracy. In this section, we analyse
the classifier?s performance in more detail.
We note that, from an educational point of view, it is important for an EDC system to have high
precision. For example, it has been shown that grammatical error detection systems with high preci-
sion maximize learning effect, and that systems with high precision but lower recall are more useful
in language learning than systems with high recall and lower precision (Nagata and Nakatani, 2010).
This suggests that learners might be misled and confused if they are frequently notified by a system that
something is an error when it is not.
Since precision is measured as the proportion of true positives (TP) to the sum of true positives and
false positives (FP):
1748
P =
TP
TP + FP
(4)
an EDC system that achieves precision less than 0.5 is, in fact, misleading for language learners: for
example, precision of less than 0.5 on the class of errors means that the system misidentifies correct use
as an error more frequently than it correctly detects an error.
Our classifier achieves good precision values with respect to both out-of-context and in-context anno-
tations, on correct and incorrect examples. Precision (P ) values are reported in Table 5. As precision
figures are higher than 0.5 in each case, it shows that the implemented error detection system would, on
balance, help guide a learner to text regions in need of reformulation.
With respect to the out-of-context annotation, the error detection system has good precision and recall
on correct examples (P = 0.8193, R = 0.9762). Precision on the incorrect examples is also high
(P = 0.7500). This is a very encouraging result, suggesting the system would rarely misidentify an
originally correct AN combination as an error.
For the in-context annotation, both precision and recall on correct and incorrect examples are quite
high: P = 0.6241 and R = 0.7169 on the correct examples, and P = 0.6850 and R = 0.5849 on the
incorrect examples.
Error analysis on the classifier?s output shows that the majority of the incorrect examples misclassified
as correct (missed errors) contain semantically-related confusions. It appears that the classifier relying
on semantically-motivated features misses a number of cases where the original AN and its correction
are semantically similar: for example, it misses the errors in big*/great anger, biggest*/greatest painter
and small*/short speech. Since the ANs in these pairs are semantically similar, the features based on
their semantic representations might not be discriminative enough. In contrast, the classifier is more
effective in detecting errors in cases where the original AN and its correction are only similar in form, or
not related to each other.
7 Conclusion
We have presented and released a dataset of learner errors in ANs, which has been extracted from learner
texts and annotated with error types and corrections. The dataset contains examples not seen in a native
corpus of English, and error annotation shows that a substantial number of such examples are correct.
Error detection in this dataset is a challenging task, since absence of the ANs in a corpus of English
cannot be used as definitive evidence of incorrectness. We have implemented a simple baseline system
inspired by previous work on improving content word combinations and shown that such a system would
not be effective for error detection in our dataset.
We have cast error detection as a binary classification task and implemented a supervised classifier
that uses semantically-motivated features. The features are derived from the compositional distributional
semantic representations of the AN combinations. We use a number of semantic measures that describe
and distinguish between semantic representations for correct and incorrect combinations. We have intro-
duced new semantic measures in addition to the ones used in previous work and show that they can be
effectively applied to this task.
The best results in our experiments are obtained with a Decision Tree classifier, and we show that the
resulting error detection system can identify errors with high precision and accuracy. We aim to extend
this system to perform error correction on ANs, as well as error detection and correction on other types
of content word combinations.
Acknowledgments
We are grateful to Cambridge English Language Assessment and Cambridge University Press for sup-
porting this research and for granting us access to the CLC for research purposes. We would like to
thank ?istein Andersen for providing us with the annotation tool, Diane Nicholls for undertaking the
bulk of the annotation work, and Helen Yannakoudakis and the anonymous reviewers for their valuable
comments.
1749
References
?istein Andersen, Julien Nioche, Ted Briscoe and John Carroll 2008. The BNC parsed with RASP4UIMA. In
Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC), pp. 865?869.
Marco Baroni and Roberto Zamparelli 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the EMNLP-2010, pp. 1183?1193.
Steven Bird, Ewan Klein, and Edward Loper 2009. Natural Language Processing with Python ? Analyzing Text
with the Natural Language Toolkit. O?Reilly Media.
Ted Briscoe, John Carroll and Rebecca Watson 2006. The Second Release of the RASP System. In Proceedings of
the COLING/ACL-2006 Interactive Presentation Sessions, pp. 59?68.
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen and Hsien-Chin Liou 2008. An automatic collocation writing
assistant for Taiwanese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language
Learning, 21(3), pp. 283?299.
Jacob Cohen 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1), pp. 37?46.
Robert Dale, Ilya Anisimoff and George Narroway 2012. HOO 2012: A Report on the Preposition and Determiner
Error Correction Shared Task. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building
Educational Applications, pp. 54?62.
Daniel Dahlmeier and Hwee Tou Ng 2011. Correcting Semantic Collocation Errors with L1-induced Paraphrases.
In Proceedings of the EMNLP-2011, pp. 107?117.
Gerard M. Dalgish 1985. Computer-assisted ESL research. In CALICO Journal, 2(2), pp. 32?37.
Stefan Evert 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart University.
Yoko Futagi, Paul Deane, Martin Chodorow and Joel Tetreault 2009. A computational approach to detecting
collocation errors in the writing of non-native speakers of English. Computer Assisted Language Learning,
21(4), pp. 353?367.
Ekaterina Kochmar and Ted Briscoe 2013. Capturing Anomalies in the Choice of Content Words in Composi-
tional Distributional Semantic Space. In Proceedings of the Recent Advances in Natural Language Processing
(RANLP-2013).
Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni 2013. Fish transporters and miracle homes: How com-
positional distributional semantics can help NP parsing. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pp. 1908?1913.
Claudia Leacock, Martin Chodorow, Michael Gamon and Joel Tetreault 2010. Automated Grammatical Error
Detection for Language Learners. Morgan and Claypool Publishers.
Anne Li-E Liu, David Wible and Nai-Lung Tsao 2009. Automated suggestions for miscollocations. In Proceed-
ings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications, pp. 47?50.
Bj?rn-Helge Mevik and Ron Wehrens 2007. The pls package: Principal component and partial least squares
regression in R. Journal of Statistical Software, 18(2), pp. 1?24.
Jeff Mitchell and Mirella Lapata 2008. Vector-based models of semantic composition. In Proceedings of ACL,
pp. 236?244.
Jeff Mitchell and Mirella Lapata 2010. Composition in distributional models of semantics. Cognitive Science, 34,
pp. 1388?1429.
Ryo Nagata and Kazuhide Nakatani 2010. Evaluating performance of grammatical error detection to maximize
learning effect. In Proceedings of COLING 2010, pp. 894?900.
Diane Nicholls 2003. The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT. In
Proceedings of the Corpus Linguistics conference, pp. 572?581.
Robert
?
Ostling and Ola Knutsson 2009. A corpus-based tool for helping writers with Swedish collocations. In
Proceedings of the Workshop on Extracting and Using Constructions in NLP, pp. 28?33.
1750
Taehyun Park, Edward Lank, Pascal Poupart, Michael Terry 2008. Is the sky pure today? AwkChecker: an assistive
tool for detecting and correcting collocation errors. In Proceedings of the 21st annual ACM symposium on User
interface software and technology, pp. 121?130.
Alla Rozovskaya and Dan Roth 2011. Algorithm Selection and Model Adaptation for ESL Correction Tasks.
InProceeding of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies ? Volume 1, pp. 924?933.
Chi-Chiang Shei and Helen Pain 2000. An ESL Writer?s Collocation Aid. Computer Assisted Language Learning,
13(2), pp. 167?182.
Eva Maria Vecchi, Marco Baroni and Roberto Zamparelli 2011. (Linear) maps of the impossible: Capturing
semantic anomalies in distributional space. In Proceedings of the DISCO Workshop at ACL-2011, pp. 1?9.
David Wible, Chin-Hwa Kuo, Nai-Lung Tsao, Anne Liu and H.-L. Lin 2003. Bootstrapping in a language-
learning environment. Journal of Computer Assisted Learning, 19(4), pp. 90?102.
Helen Yannakoudakis, Ted Briscoe and Ben Medlock 2011. A New Dataset and Method for Automatically Grad-
ing ESOL Texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, 1, pp. 180?189.
Xing Yi, Jianfeng Gao and William B. Dolan 2008. A Web-based English Proofing System for English as a Second
Language Users. In Proceedings of the third International Joint Conference on Natural Language Processing
(IJCNLP), pp. 619?624.
1751
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 1?4,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Camtology: Intelligent Information Access for Science
Ted Briscoe1,2, Karl Harrison5, Andrew Naish-Guzman4, Andy Parker1,
Advaith Siddharthan3, David Sinclair4, Mark Slater5 and Rebecca Watson2
1University of Cambridge 2iLexIR Ltd 4Camtology Ltd 5University of Birmingham
ejb1@cl.cam.ac.uk,
parker@hep.phy.cam.ac.uk,
rfw@ilexir.co.uk
3University of Aberdeen
advaith@abdn.ac.uk
david.sinclair@imense.co.uk,
a.naish@gmail.com
kh@hep.ph.bham.ac.uk,
mws@hep.ph.bham.ac.uk
Abstract
We describe a novel semantic search engine
for scientific literature. The Camtology sys-
tem allows for sentence-level searches of PDF
files and combines text and image searches,
thus facilitating the retrieval of information
present in tables and figures. It allows the user
to generate complex queries for search terms
that are related through particular grammati-
cal/semantic relations in an intuitive manner.
The system uses Grid processing to parallelise
the analysis of large numbers of papers.
1 Introduction
Scientific, technological, engineering and medi-
cal (STEM) research is entering the so-called 4th
Paradigm of ?data-intensive scientific discovery?, in
which advanced data mining and pattern discovery
techniques need to be applied to vast datasets in or-
der to drive further discoveries. A key component
of this process is efficient search and exploitation of
the huge repository of information that only exists in
textual or visual form within the ?bibliome?, which
itself continues to grow exponentially.
Today?s computationally driven research methods
have outgrown traditional methods of searching for
scientific data, creating a widespread and unfulfilled
need for advanced search and information extrac-
tion. Camtology combines text and image process-
ing to create a unique solution to this problem.
2 Status
Camtology has developed a search and information
extraction system which is currently undergoing us-
ability testing with the curation team for FlyBase1,
a $1m/year NIH-funded curated database covering
the functional genomics of the fruit fly. To provide
a scalable solution capable of analysing the entire
STEM bibliome of over 20m electronic journal and
1http://flybase.org/
conference papers, we have developed a robust sys-
tem that can be used with a grid of computers run-
ning distributed job management software.
This system has been deployed and tested using
a subset of the resources provided by the UK Grid
for Particle Physics (Britton et al, 2009), part of the
worldwide Grid of around 200000 CPU cores as-
sembled to allow analysis of the petabyte-scale data
volumes to be recorded each year by experiments at
the Large Hadron Collider in Geneva. Processing
of the FlyBase archive of around 15000 papers re-
quired about 8000 hours of CPU time, and has been
successfully completed in about 3 days, with up to a
few hundred jobs run in parallel. A distributed spi-
der for collecting open-source PDF documents has
also been developed. This has been run concurrently
on over 2000 cores cores, and has been used to re-
trieve over 350000 subject-specific papers, but these
are not considered in the present demo.
3 Functionality
Camtology?s search and extraction engine is the first
to integrate a full structural analysis of a scientific
paper in PDF format (identifying headings, sections,
captions and associated figures, citations and ref-
erences) with a sentence-by-sentence grammatical
analysis of the text and direct visual search over
figures. Combining these capabilities allows us to
transform paper search from keyword based paper
retrieval, where the end result is a set of putatively
relevant PDF files which need to be read, to informa-
tion extraction based on the ability to interactively
specify a rich variety of linguistic patterns which
return sentences in specific document locales, and
which combine text with image-based constraints;
for instance:
?all sentences in figure captions which contain
any gene name as the theme of the action ?ex-
press? where the figure is a picture of an eye?
1
Camtology allows the user to build up such com-
plex queries quickly though an intuitive process of
query refinement.
Figures often convey information crucial to the
understanding of the content of a paper and are typ-
ically not available to search. Camtology?s search
engine integrates text search to the figure and cap-
tion level with the ability to re-rank search returns on
the basis of visual similarity to a chosen archetype
(ambiguities in textual relevance are often resolved
by visual appearance). Figure 1 provides a compact
overview of the search functionality supported by
our current demonstrator. Interactively, constructing
and running such complex queries takes a few sec-
onds in our intuitive user interface, and allows the
user to quickly browse and then aggregate informa-
tion across the entire collection of papers indexed by
the system. For instance, saving the search result
from the example above would yield a computer-
readable list of gene names involved in eye develop-
ment (in fruit flies in our demonstrator) in a second
or so. With existing web portals and keyword based
selection of PDF files (for example, Google Scholar,
ScienceDirect, DeepDyve or PubGet), a query like
this would typically take many hours to open and
read each one, using cut and paste to extract gene
names (and excludes the possibility of ordering re-
sults on a visual basis). The only other alterna-
tive would require expensive bespoke adaptation of
a text mining system by IT professionals using li-
censed software (such as Ariadne Genomics, Temis
or Linguamatics). This option is only available to a
tiny minority of researchers working for large well-
funded corporations.
4 Summary of Technology
4.1 PDF to SciXML
The PDF format represents a document in a
manner designed to facilitate printing. In short,
it provides information on font and position for
textual and graphical units. To enable informa-
tion retrieval and extraction, we need to convert
this typographic representation into a logical one
that reflects the structure of scientific documents.
We use an XML schema called SciXML (first
introduced in Teufel et al (1999)) that we extend
to include images. We linearise the textual ele-
ments in the PDF, representing these as <div>
elements in XML and classify these divisions as
{Title|Author|Affiliation|Abstract|Footnote|Caption|
Heading|Citation| References|Text} in a constraint
satisfaction framework.
In addition, we identify all graphics in the PDF,
including lines and images. We then identify ta-
bles by looking for specific patterns of text and
lines. A bounding box is identified for a table and
an image is generated that overlays the text on the
lines. Similarly we overlay text onto images that
have been identified and identify bounding boxes
for figures. This representation allows us to re-
trieve figures and tables that consist of text and
graphics. Once bounding boxes for tables or fig-
ures have been identified, we identify a one-to-one
association between captions and boxes that min-
imises the total distance between captions and their
associated figures or tables. The image is then ref-
erenced from the caption using a ?SRC? attribute;
for example, in (abbreviated for space constraints):
<CAPTION SRC=
?FBrf0174566 fig 6 o.png?>
<b>Fig. 6. </b> Phenotypic
analysis of denticle belt fusions
during embryogenesis. (A)
The denticle belt fusion phe-
notype resulted in folds around
the surrounding fused... ...(G)
...the only cuticle phenotype
of the DN-EGFR-expressing
embryos was strong denticle
belt fusions in alternating
parasegments (<i>paired
</i>domains).</CAPTION>
Note how informative the caption is, and the value
of being able to search this caption in conjunction
with the corresponding image (also shown above).
4.2 Natural Language Processing
Every sentence, including those in abstracts, titles
and captions, is run through our named-entity recog-
niser and syntactic parser. The output of these sys-
tems is then indexed, enabling semantic search.
Named Entity Recognition
NER in the biomedical domain was implemented
as described in Vlachos (2007). Gene Mention
tagging was performed using Conditional Random
Fields and syntactic parsing, using features derived
from grammatical relations to augment the tagging.
We also use a probabilistic model for resolution of
non-pronominal anaphora in biomedical texts. The
model focuses on biomedical entities and seeks to
find the antecedents of anaphora, both coreferent
and associative ones, and also to identify discourse-
new expressions (Gasperin and Briscoe, 2008).
2
Parsing
The RASP toolkit (Briscoe et al, 2006) is used
for sentence boundary detection, tokenisation, PoS
tagging and finding grammatical relations (GR) be-
tween words in the text. GRs are triplets consisting
of a relation-type and two arguments and also en-
code morphology, word position and part-of-speech;
for example, parsing ?John likes Mary.? gives us a
subject relation and a direct object relation:
(|ncsubj| |like+s:2 VVZ| |John:1 NP1|)
(|dobj| |like+s:2 VVZ| |Mary:3 NP1|)
Representing a parse as a set of flat triplets allows
us to index on grammatical relations, thus enabling
complex relational queries.
4.3 Image Processing
We build a low-dimensional feature vector to sum-
marise the content of each extracted image. Colour
and intensity histograms are encoded in a short bit
string which describes the image globally; this is
concatenated with a description of the image derived
from a wavelet decomposition (Jacobs et al, 1995)
that captures finer-scale edge information. Efficient
similar image search is achieved by projecting these
feature vectors onto a small number of randomly-
generated hyperplanes and using the signs of the
projections as a key for locality-sensitive hashing
(Gionis et al, 1999).
4.4 Indexing and Search
We use Lucene (Goetz, 2002) for indexing and re-
trieving sentences and images. Lucene is an open
source indexing and information retrieval library
that has been shown to scale up efficiently and han-
dle large numbers of queries. We index using fields
derived from word-lemmas, grammatical relations
and named entities. At the same time, these complex
representations are hidden from the user, who, as a
first step, performs a simple keyword search; for ex-
ample ?express Vnd?. This returns all sentences that
contain the words ?express? and ?Vnd? (search is
on lemmatised words, so morphological variants of
?express? will be retrieved). Different colours rep-
resent different types of biological entities and pro-
cesses (green for a gene), and blue shows the entered
search terms in the result sentences. An example
sentence retrieved for the above query follows:
It is possible that like ac , sc and l?sc ,
vnd is expressed initially in cell clusters and
then restricted to single cells .
Next, the user can select specific words in the
returned sentences to indirectly specify a relation.
Clicking on a word will select it, indicated by un-
derlining of the word. In the example above, the
words ?vnd? and ?expressed? have been selected by
the user. This creates a new query that returns sen-
tences where ?vnd? is the subject of ?express? and
the clause is in passive voice. This retrieval is based
on a sophisticated grammatical analysis of the text,
and can retrieve sentences where the words in the
relation are far apart. An example of a sentence re-
trieved for the refined query is shown below:
First , vnd might be spatially regulated in a
manner similar to ac and sc and selectively
expressed in these clusters .
Camtology offers two other functionalities. The
user can browse the MeSH (Medical Subject Head-
ings) ontology and retrieve papers relevant to a
MeSH term. Also, for both search and MeSH brows-
ing, retrieved papers are plotted on a world map; this
is done by converting the affiliations of the authors
into geospatial coordinates. The user can then di-
rectly access papers from a particular site.
5 Script Outline
I Quick overview of existing means of searching sci-
ence (PubMed, FlyBase, Google Scholar).
II Walk through the functionality of Camtology (these
are numbered in Figure 1:
? (1) Initial query through textual search box; (2)
Retrieval of relevant sentences; (3) Query re-
finement by clicking on words; (4) Using im-
plicit grammatical relations for new search;
? Alternative to search: (5) Browse MeSH On-
tology to retrieve papers with MeSH terms.
? (6) Specifically searching for tables/figures
? (7) Viewing the affiliation of the authors of re-
trieved papers on a world map.
? (8) Image search using similarity of image.
6 Acknowledgements
This work was supported in part by a STFC miniP-
IPSS grant to the University of Cambridge and
iLexIR Ltd.
References
T. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proc. ACL 2006.
D. Britton, AJ Cass, PEL Clarke, et al 2009. GridPP:
the UK grid for particle physics. Philosophical Trans-
actions A, 367(1897):2447.
3
C. Gasperin and T. Briscoe. 2008. Statistical anaphora
resolution in biomedical texts. In Proc. COLING?08.
A. Gionis, P. Indyk, and R. Motwani. 1999. Similarity
search in high dimensions via hashing. In Proc. 25th
ACM Internat. Conf. on Very Large Data Bases.
B. Goetz. 2002. The Lucene search engine: Powerful,
flexible, and free. Javaworld http://www. javaworld.
com/javaworld/jw-09-2000/jw-0915-lucene. html.
C.E. Jacobs, A. Finkelstein, and D.H. Salesin. 1995. Fast
multiresolution image querying. In Proc. 22nd ACM
annual conference on Computer graphics and interac-
tive techniques.
S. Teufel, J. Carletta, and M. Moens. 1999. An annota-
tion scheme for discourse-level argumentation in re-
search articles. In Proc. EACL?99.
A. Vlachos. 2007. Tackling the BioCreative2 gene men-
tion task with CRFs and syntactic parsing. In Proc.
2nd BioCreative Challenge Evaluation Workshop.
F
ig
ur
e
1:
S
cr
ee
ns
ho
ts
sh
ow
in
g
fu
nc
ti
on
al
it
y
of
th
e
C
am
to
lo
gy
se
ar
ch
en
gi
ne
.
4
Proceedings of NAACL-HLT 2013, pages 391?400,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Parser lexicalisation through self-learning
Marek Rei
Computer Labratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
We describe a new self-learning framework
for parser lexicalisation that requires only a
plain-text corpus of in-domain text. The
method first creates augmented versions of de-
pendency graphs by applying a series of mod-
ifications designed to directly capture higher-
order lexical path dependencies. Scores are
assigned to each edge in the graph using statis-
tics from an automatically parsed background
corpus. As bilexical dependencies are sparse,
a novel directed distributional word similar-
ity measure is used to smooth edge score es-
timates. Edge scores are then combined into
graph scores and used for reranking the top-
n analyses found by the unlexicalised parser.
The approach achieves significant improve-
ments on WSJ and biomedical text over the
unlexicalised baseline parser, which is origi-
nally trained on a subset of the Brown corpus.
1 Introduction
Most parsers exploit supervised machine learning
methods and a syntactically annotated dataset (i.e.
treebank), incorporating a wide range of features in
the training process to deliver competitive perfor-
mance. The use of lexically-conditioned features,
such as relations between lemmas or word forms,
is often critical when choosing the correct syntac-
tic analysis in ambiguous contexts. However, util-
ising such features leads the parser to learn infor-
mation that is often specific to the domain and/or
genre of the training data. Several experiments have
demonstrated that many lexical features learnt in
one domain provide little if any benefit when pars-
ing text from different domains and genres (Sekine,
1997; Gildea, 2001). Furthermore, manual creation
of in-domain treebanks is an expensive and time-
consuming process, which can only be performed by
experts with sufficient linguistic and domain knowl-
edge.
In contrast, unlexicalised parsers avoid using lex-
ical information and select a syntactic analysis us-
ing only more general features, such as POS tags.
While they cannot be expected to achieve optimal
performance when trained and tested in a single do-
main, unlexicalised parsers can be surprisingly com-
petitive with their lexicalised counterparts (Klein
and Manning, 2003; Petrov et al, 2006). In this
work, instead of trying to adapt a lexicalised parser
to new domains, we explore how bilexical features
can be integrated effectively with any unlexicalised
parser. As our novel self-learning framework re-
quires only a large unannotated corpus, lexical fea-
tures can be easily tuned to a specific domain or
genre by selecting a suitable dataset. In addition,
we describe a graph expansion process that captures
selected bilexical relations which improve perfor-
mance but would otherwise require sparse higher-
order dependency path feature types in most ap-
proaches to dependency parsing. As many bilex-
ical features will still be sparse, we also develop
an approach to estimating confidence scores for de-
pendency relations using a directional distributional
word similarity measure. The final framework in-
tegrates easily with any unlexicalised (and therefore
potentially less domain/genre-biased) parser capable
of returning ranked dependency analyses.
391
2 Background
We hypothesise that a large corpus will often contain
examples of dependency relations in non-ambiguous
contexts, and these will mostly be correctly parsed
by an unlexicalised parser. Lexical statistics derived
from the corpus can then be used to select the cor-
rect parse in a more difficult context. For example,
consider the following sentences:
(1) a. Government projects interest researchers
b. Government raises interest rates
c. Government projects receive funding
d. Interest rates are increasing
Noun-verb ambiguities over projects and interest
might erroneously result in the unlexicalised parser
returning similar dependency graphs for both a and
b. However, sentences c and d contain less ambigu-
ous instances of the same phrases and can provide
clues to correctly parsing the first two examples. In
a large in-domain corpus we are likely to find more
cases of researchers being the object for interest and
fewer cases where it is the object of project. In con-
trast, rates is more likely to have interest as a mod-
ifier than as a head in an object relation. Exploiting
this lexical information, we can assign the correct
derivation to each of the more ambiguous sentences.
Similar intuitions have been used to motivate the
acquisition of bilexical features from background
corpora for improving parser accuracy. However,
previous work has focused on including these statis-
tics as auxiliary features during supervised training.
For example, van Noord (2007) incorporated bilex-
ical preferences as features via self-training to im-
prove the Alpino parser for Dutch. Plank and van
Noord (2008) investigated the application of aux-
iliary distributions for domain adaptation. They
incorporated information from both in-domain and
out-of-domain sources into their maximum entropy
model and found that the out-of-domain auxiliary
distributions did not contribute to parsing accuracy
in the target domain. Zhou et al (2011) extracted n-
gram counts from Google queries and a large corpus
to improve the MSTParser. In contrast to previous
work, we refer to our approach as self-learning be-
cause it differs from self-training by utilising statis-
tics found using an initial parse ranking model to
create a separate unsupervised reranking compo-
nent, without retraining the baseline unlexicalised
model.
We formulate our self-learning framework as a
reranking process that assigns new scores to the top-
n ranked analyses found by the original parser. Parse
reranking has been successfully used in previous
work as a method of including a wider range of fea-
tures to rescore a smaller selection of highly-ranked
candidate parses. Collins (2000) was one of the first
to propose supervised reranking as an additional step
to increase parser accuracy and achieved 1.55% ac-
curacy improvement for his parser. Charniak and
Johnson (2005) utilise a discriminative reranker and
show a 1.3% improvement for the Charniak parser.
McClosky et al (2006) extend their work by adding
new features and further increase the performance
by 0.3%. Ng et al (2010) implemented a dis-
criminative maximum entropy reranker for the C&C
parser and showed a 0.23% improvement over the
baseline. Bansal and Klein (2011) discriminatively
rerank derivations from the Berkeley unlexicalised
parser (Petrov et al, 2006) demonstrating that lex-
ical features derived from the Google n-gram cor-
pus improve accuracy even when used in conjunc-
tion with other reranking features. They have all
treated reranking as a supervised task and trained a
discriminative classifier using parse tree features and
annotated in-domain data. In contrast, our reranker
only uses statistics from an unlabelled source and
requires no manual annotation or training of the
reranking component. As we utilise an unlexicalised
parser, our baseline performance on WSJ text is
lower compared to some fully-lexicalised parsers.
However, an unlexicalised parser is also likely to be
less biased to domains or genres manifested in the
text used to train its original ranking model. This
may allow the reranker to adapt it to a new domain
and/or genre more effectively.
3 Reordering dependency graphs
For our experiments, we make use of the unlexi-
calised RASP parser (Briscoe et al, 2006) as the
baseline system. For every sentence s the parser
returns a list of dependency graphs Gs, ranked by
the log probability of the associated derivation in the
structural ranking model. Our goal is to reorder this
392
list to improve ranking accuracy and, most impor-
tantly, to improve the quality of the highest-ranked
dependency graph. This is done by assigning a con-
fidence score to every graph gs,r ? Gs where r is the
rank of gs for sentence s. The method treats each
sentence independently, therefore we can omit the
sentence identifiers and refer to gs,r as gr.
We first calculate confidence scores for all the in-
dividual edges and then combine them into an over-
all score for the dependency graph. In the following
sections, we describe a series of graph modifications
that incorporates selected higher-order dependency
path relations, without introducing unwanted noise
or complexity into the reranker. Next, we outline
different approaches for calculating and smoothing
the confidence scores for bilexical relations. Finally,
we describe methods for combining together these
scores and calculating an overall score for a depen-
dency graph. We make publically available all the
code developed for performing these steps in the
parse reranking system.1
3.1 Graph modifications
For every dependency graph gr the graph expan-
sion procedure creates a modified representation g?r
which contains a wider range of bilexical relations.
The motivation for this graph expansion step is sim-
ilar to that motivating the move from first-order to
higher-order dependency path feature types (e.g.,
Carreras (2007)). However, compared to using all
nth-order paths, these rules are chosen to maximise
the utility and minimise the sparsity of the result-
ing bilexical features. In addition, the cascading na-
ture of the expansion steps means in some cases the
expansion captures useful 3rd and 4th order depen-
dencies. Similar approaches to graph modifications
have been successfully used for several NLP tasks
(van Noord, 2007; Arora et al, 2010).
For any edge e we also use notation (rel, w1, w2),
referring to an edge from w1 to w2 with the label
rel. We perform the following modifications on ev-
ery dependency graph:
1. Normalising lemmas. All lemmas are converted
to lowercase. Numerical lemmas are replaced
with more generic tags to reduce sparsity.
1www.marekrei.com/projects/lexicalisation
2. Bypassing conjunctions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a conjunction, we create an additional
edge (rel1, w1, w3). This bypasses the conjunc-
tion node and creates direct edges between the
head and dependents of the conjunctive lemma.
3. Bypassing prepositions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a preposition, we create an additional
edge (rel3, w1, w3). rel3 = rel1 +? prep?, where
? prep? is added as a marker to indicate that the
relation originally contained a preposition.
4. Bypassing verbs. For every edge pair
(rel1, w1, w2) and (rel2, w1, w3) where w1 is
tagged as a verb, w2 and w3 are both tagged
as open-class lemmas, rel1 starts with a subject
relation, and rel2 starts with an object relation,
we create an additional edge (rel3, w2, w3) where
rel3 = rel1 + ?-? + rel2. This creates an additional
edge between the subject and the object, with the
new edge label containing both of the original la-
bels.
5. Duplicating nodes. For every existing node in
the graph, containing the lemma and POS for
each token (lemma pos), we create a parallel node
without the POS information (lemma). Then, for
each existing edge, we create three correspond-
ing edges, interconnecting the parallel nodes to
each other and the original graph. This allows the
reranker to exploit both specific and more generic
instantiations of each lemma.
Figure 1 illustrates the graph modification pro-
cess. It is important to note that each of these mod-
ifications gets applied in the order that they are de-
scribed above. For example, when creating edges for
bypassing verbs, the new edges for prepositions and
conjunctions have already been created and also par-
ticipate in this step. We performed ablation tests on
the development data and verified that each of these
modifications contributes positively to the final per-
formance.
3.2 Edge scoring methods
We start the scoring process by assigning individual
confidence scores to every bilexical relation in the
393
italian pm meet with cabinet member and senior official
JJ NP1 VVZ IW NN1 NN2 CC JJ NN2
ncmod ncsubj iobj
dobj
ncmod conj
conj
ncmod
ncsubj-iobj prepncsubj-iobj prep
iobj prepiobj prep
iobj prep
dobjdobj
Figure 1: Modified graph for the sentence ?Italian PM meets with Cabinet members and senior officials? after steps
1-4. Edges above the text are created by the parser, edges below the text are automatically created using the operations
described in Section 3.1. The 5th step will create 9 new nodes and 45 additional edges (not shown).
modified graph. In this section we give an overview
of some possible strategies for performing this task.
The parser returns a ranked list of graphs and this
can be used to derive an edge score without requir-
ing any additional information. We estimate that the
likelihood of a parse being the best possible parse for
a given sentence is roughly inversely proportional
to the rank that it is assigned by the parser. These
values can be summed for all graphs that contain a
specific edge, normalised to approximate a proba-
bility. We then calculate the score for edge e as the
Reciprocal Edge Score (RES) ? the probability of e
belonging to the best possible parse:
RES(e) =
?R
r=1[ 1r ? contains(g?r, e)]
?R
r=1
1
r
whereR is the total number of parses for a sentence,
and contains(g?r, e) returns 1 if graph g?r contains
edge e, and 0 otherwise. The value is normalised,
so that an edge which is found in all parses will have
a score of 1.0, but occurrences at higher ranks will
have a considerably larger contribution.
The score of an edge can also be assigned by es-
timating the probability of that edge using a parsed
reference corpus. van Noord (2007) improved over-
all parsing performance in a supervised self-training
framework using feature weights based on pointwise
mutual information:
I(e) = log P(rel, w1, w2)
P(rel, w1, ?)? P(?, ?, w2)
where P(rel, w1, w2) is the probability of seeing an
edge from w1 to w2 with label rel, P(rel, w1, ?) is
the probability of seeing an edge from w1 to any
node with label rel, and P(?, ?, w2) is the prob-
ability of seeing any type of edge linking to w2.
Plank and van Noord (2008) used the same approach
for semi-supervised domain adaptation but were not
able to achieve similar performance benefits. In our
implementation we omit the logarithm in the equa-
tion, as this improves performance and avoids prob-
lems with log(0) for unseen edges.
I(e) compares the probability of the complete
edge to the probabilities of partially specified edges,
but it assumes that w2 will have an incoming rela-
tion, and that w1 will have an outgoing relation of
type rel to some unknown node. These assumptions
may or may not be true ? given the input sentence,
we have observed w1 and w2 but do not know what
relations they are involved in. Therefore, we create
a more general version of the measure that compares
the probability of the complete edge to the individual
probabilities of the two lemmas ? the Conditional
Edge Score (CES1):
CES1(e) =
P(rel, w1, w2)
P(w1)? P(w2)
where P(w1) is the probability of seeing w1 in text,
estimated from a background corpus using maxi-
mum likelihood.
Finally, we know that w1 and w2 are in a sen-
tence together but cannot assume that there is a de-
pendency relation between them. However, we can
choose to think of each sentence as a fully connected
graph, with an edge going from every lemma to ev-
ery other lemma in the same sentence. If there exists
394
ECES1(rel, w1, w2) =
1
2 ? (
?
c1?C1
sim(c1, w1)? P(rel,c1,w2)P(c1)?P(w2)?
c1?C1
sim(c1, w1)
+
?
c2?C2
sim(c2, w2)? P(rel,w1,c2)P(w1)?P(c2)?
c2?C2
sim(c2, w2)
)
ECES2(rel, w1, w2) =
1
2 ? (
?
c1?C1
sim(c1, w1)? P(rel,c1,w2)P(?,c1,w2)?
c1?C1
sim(c1, w1)
+
?
c2?C2
sim(c2, w2)? P(rel,w1,c2)P(?,w1,c2)?
c2?C2
sim(c2, w2)
)
Figure 2: Expanded edge score calculation methods using the list of distributionally similar lemmas
no genuine relation between the lemmas, the edge is
simply considered a null edge. We can then find the
conditional probability of the relation type given the
two lemmas:
CES2(e) =
P(rel, w1, w2)
P(?, w1, w2)
where P(rel, w1, w2) is the probability of the fully-
specified relation, and P(?, w1, w2) is the probability
of there being an edge of any type fromw1 tow2, in-
cluding a null edge. Using fully connected graphs,
the latter is equivalent to the probability of w1 and
w2 appearing in a sentence together, which again can
be calculated from the background corpus.
3.3 Smoothing edge scores
Apart from RES, all the scoring methods from
the previous section rely on correctly estimat-
ing the probability of the fully-specified edge,
P(rel, w1, w2). Even in a large background corpus
these triples will be very sparse, and it can be useful
to find approximate methods for estimating the edge
scores.
Using smoothing techniques derived from work
on language modelling, we could back-off to a more
general version of the relation. For example, if
(dobj, read, publication) is not frequent enough, the
value could be approximated using the probabilities
of (dobj, read, *) and (dobj, *, publication). How-
ever, this can lead to unexpected results due to com-
positionality ? while (dobj, read, *) and (dobj, *,
rugby) can be fairly common, (dobj, read, rugby) is
an unlikely relation.
Instead, we can consider looking at other lemmas
which are similar to the rare lemmas in the relation.
If (dobj, read, publication) is infrequent in the data,
the system might predict that book is a reasonable
substitute for publication and use (dobj, read, book)
to estimate the original probability.
Given that we have a reliable way of finding likely
substitutes for a given lemma, we can create ex-
panded versions of CES1 and CES2, as shown in
Figure 2. C1 is the list of substitute lemmas for w1,
and sim(c1, w1) is a measure showing how similar
c1 is to w1. The methods iterate over the list of sub-
stitutes and calculate the CES score for each of the
modified relations. The values are then combined by
using the similarity score as a weight ? more similar
lemmas will have a higher contribution to the final
result. This is done for both the head and the depen-
dent in the original relation, and the scores are then
normalised and averaged.
Experiments with a wide variety of distributional
word similarity measures revealed that WeightedCo-
sine (Rei, 2013), a directional similarity measure
designed to better capture hyponymy relations, per-
formed best. Hyponyms are more specific versions
of a word and normally include the general proper-
ties of the hypernym, making them well-suited for
lexical substitution. The WeightedCosine measure
incorporates an additional directional weight into
the standard cosine similarity, assigning different
importance to individual features for the hyponymy
relation. We retain the 10 most distributionally simi-
lar putative hyponyms for each lemma and substitute
them in the relation. The original lemma is also in-
cluded with similarity 1.0, thereby assigning it the
highest weight. The lemma vectors are built from
the same vector space model that is used for cal-
culating edge probabilities, which includes all the
graph modifications described in Section 3.1.
3.4 Combining edge scores
While the CES and ECES measures calculate con-
fidence scores for bilexical relations using statistics
from a large background corpus, they do not include
any knowledge about grammar, syntax, or the con-
395
CMB1(e) = 3
?
RES(e) ? CES1(e) ? CES2(e) CMB2(e) = 3
?
RES(e) ? ECES1(e) ? ECES2(e)
Figure 3: Edge score combination methods
text in a specific sentence. In contrast, the RES score
implicitly includes some of this information, as it is
calculated based on the original parser ranking. In
order to take advantage of both information sources,
we combine these scores into CMB1 and CMB2, as
shown in Figure 3.
3.5 Graph scoring
Every edge in graph g?r is assigned a score indicat-
ing the reranker?s confidence in that edge belonging
to the best parse. We investigated different strate-
gies for combining these values together into a con-
fidence score for the whole graph. The simplest so-
lution is to sum together individual edge scores, but
this would lead to always preferring graphs that have
a larger number of edges. Interestingly, averaging
the edge scores does not produce good results either
because it is biased towards smaller graph fragments
containing only highly-confident edges.
We created a new scoring method which prefers
graphs that cover all the nodes, but does not create
bias for a higher number of edges. For every node
in the graph, it finds the average score of all edges
which have that node as a dependent. These scores
are then averaged again over all nodes:
NScore(n) =
?
e?Eg
EdgeScore(e)? isDep(e, n)
?
e?Eg
isDep(e, n)
GraphScore(g) =
?
n?Ng
NScore(n)
|Ng|
where g is the graph being scored, n ? Ng is a
node in graph g, e ? Eg is an edge in graph g,
isDep(e, n) is a function returning 1.0 if n is the de-
pendent in edge e, and 0.0 otherwise. NScore(n) is
set to 0 if the node does not appear as a dependent in
any edges. We found this metric performs well, as
it prefers graphs that connect together many nodes
without simply rewarding a larger number of edges.
While the score calculation is done using the
modified graph g?r, the resulting score is directly as-
signed to the corresponding original graph gr, and
the reordering of the original dependency graphs is
used for evaluation.
4 Experiments
4.1 Evaluation methods
In order to evaluate how much the reranker improves
the highest-ranked dependency graph, we calculate
the microaveraged precision, recall and F-score over
all dependencies from the top-ranking parses for
the test set. Following the official RASP evalua-
tion (Briscoe et al, 2006) we employ the hierarchi-
cal edge matching scheme which aggregates counts
up the dependency relation subsumption hierarchy
and thus rewards the parser for making more fine-
grained distinctions.2 Statistical significance of the
change in F-score is calculated by using the Approx-
imate Randomisation Test (Noreen, 1989; Cohen,
1995) with 106 iterations.
We also wish to measure how well the reranker
does at the overall task of ordering dependency
graphs. For this we make use of an oracle that cre-
ates the perfect ranking for a set of graphs by calcu-
lating their individual F-scores; this ideal ranking is
then compared to the output of our system. Spear-
man?s rank correlation coefficient between the two
rankings is calculated for each sentence and then av-
eraged over all sentences. If the scores for all of the
returned analyses are equal, this coefficient cannot
be calculated and is set to 0.
4.2 DepBank
We evaluated our self-learning framework using
the DepBank/GR reannotation (Briscoe and Carroll,
2006) of the PARC 700 Dependency Bank (King
et al, 2003). The dataset is provided with the
open-source RASP distribution3 and has been used
for evaluating different parsers, including RASP
(Briscoe and Carroll, 2006; Watson et al, 2007) and
2Slight changes in the performance of the baseline parser
compared to previous publications are due to using a more re-
cent version of the parser and minor corrections to the gold stan-
dard annotation.
3ilexir.co.uk/2012/open-source-rasp-3-1/
396
C&C (Clark and Curran, 2007). It contains 700 sen-
tences, randomly chosen from section 23 of the WSJ
Penn Treebank (Marcus et al, 1993), divided into
development (140 sentences) and test data (560 sen-
tences). We made use of the development data to
experiment with a wider selection of edge and graph
scoring methods, and report the final results on the
test data.
For reranking we collect up to 1000 top-ranked
analyses for each sentence. The actual number of
analyses that the RASP parser outputs depends on
the sentence and can be smaller. As the parser first
constructs parse trees and converts them to depen-
dency graphs, several parse trees may result in iden-
tical graphs; we remove any duplicates to obtain a
ranking of unique dependency graphs.
Our approach relies on a large unannotated corpus
of in-domain text, and for this we used the BLLIP
corpus containing 50M words of in-domain WSJ ar-
ticles. Our version of this corpus excludes texts that
are found in the Penn Treebank, thereby also exclud-
ing the section that we use for evaluation.
The baseline system is the unlexicalised RASP
parser with default settings. In order to construct
the upper bound, we use an oracle to calculate the F-
score for each dependency graph individually, and
then create the best possible ranking using these
scores.
Table 1 contains evaluation results on the Dep-
Bank/GR test set. The baseline system achieves
76.41% F-score on the test data, with 32.70% av-
erage correlation. I and RES scoring methods give
comparable results, with RES improving correlation
by 9.56%. The CES and ECES scores all make use
of corpus-based statistics and all significantly im-
prove over the baseline system, with absolute in-
creases in F-score of more than 2% for the fully-
connected edge score variants.
Finally, we combine the RES score with the
corpus-based methods and the fully-connected
CMB2 variant again delivers the best overall results.
The final F-score is 79.21%, an absolute improve-
ment of 2.8%, corresponding to 33.65% relative er-
ror reduction with respect to the upper bound. Cor-
relation is also increased by 16.32%; this means the
methods not only improve the chances of finding the
best dependency graph, but also manage to create
a better overall ranking. The F-scores for all the
corpus-based scoring methods are statistically sig-
nificant when compared to the baseline (p < 0.05).
By using our self-learning framework, we were
able to significantly improve the original unlexi-
calised parser. To put the overall result in a wider
perspective, Clark and Curran (2007) achieve an
F-score of 81.86% on the DepBank/GR test sen-
tences using the C&C lexicalised parser, trained
on 40,000 manually-treebanked sentences from the
WSJ. The unlexicalised RASP parser, using a
manually-developed grammar and a parse ranking
component trained on 4,000 partially-bracketed un-
labelled sentences from a domain/genre balanced
subset of Brown (Watson et al, 2007), achieves an
F-score of 76.41% on the same test set. The method
introduced here improves this to 79.21% F-score
without using any further manually-annotated data,
closing more than half of the gap between the perfor-
mance of a fully-supervised in-domain parser and a
more weakly-supervised more domain-neutral one.
We also performed an additional detailed analysis
of the results and found that, with the exception of
the auxiliary dependency relation, the reranking pro-
cess was able to improve the F-score of all other in-
dividual dependency types. Complements and mod-
ifiers are attached with much higher accuracy, result-
ing in 3.34% and 3.15% increase in the correspond-
ing F-scores. The non-clausal modifier relation (nc-
mod), which is the most frequent label in the dataset,
increases by 3.16%.
4.3 Genia
One advantage of our reranking framework is that
it does not rely on any domain-dependent manually
annotated resources. Therefore, we are interested in
seeing how it performs on text from a completely
different domain and genre.
The GENIA-GR dataset (Tateisi et al, 2008) is
a collection of 492 sentences taken from biomedi-
cal research papers in the GENIA corpus (Kim et
al., 2003). The sentences have been manually anno-
tated with dependency-based grammatical relations
identical to those output by the RASP parser. How-
ever, it does not contain dependencies for all tokens
and many multi-word phrases are treated as single
units. For example, the tokens ?intracellular redox
status? are annotated as one node with label intra-
cellular redox status. We retain this annotation and
397
DepBank/GR GENIA-GR
Prec Rec F ? Prec Rec F ?
Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54
Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66
I 77.77 75.00 76.36 33.32 77.18 76.21 76.69 30.23
RES 78.13 74.94 76.50 42.26 80.06 78.89 79.47 47.52
CES1 79.68 76.40 78.01 41.95 78.64 77.50 78.07 36.06
CES2 80.48 77.28 78.85 48.43 79.92 78.92 79.42 43.09
ECES1 79.96 76.68 78.29 42.41 79.09 78.11 78.60 38.02
ECES2 80.71 77.52 79.08 49.05 79.84 78.95 79.39 43.64
CMB1 80.64 77.31 78.94 48.25 80.60 79.51 80.05 44.96
CMB2 80.88 77.60 79.21 49.02 80.69 79.64 80.16 46.24
Table 1: Performance of different edge scoring methods on the test data. For each measure we report precision,
recall, F-score, and average Spearman?s correlation (?). The highest results for each measure are marked in bold. The
underlined F-scores are significantly better compared to the baseline.
allow the unlexicalised parser to treat these nodes as
atomic unseen words during POS tagging and pars-
ing. However, we use the last lemma in each multi-
word phrase for calculating the edge score statistics.
In order to initialise our parse reranking frame-
work, we also need a background corpus that closely
matches the evaluation domain. The annotated sen-
tences in GENIA-GR were chosen from abstracts
that are labelled with the MeSH term ?NF-kappa B?.
Following this method, we created our background
corpus by extracting 7,100 full-text articles (1.6M
sentences) from the PubMed Central Open Access
collection, containing any of the following terms
with any capitalisation: ?nf-kappa b?, ?nf-kappab?,
?nf kappa b?, ?nf-kappa b?, ?nf-kb?, ?nf-?b?. Since
we retain all texts from matching documents, this
keyword search acts as a broad indicator that the sen-
tences contain topics which correspond to the evalu-
ation dataset. This focussed corpus was then parsed
with the unlexicalised parser and used to create a
statistical model for the reranking system, following
the same methods as described in Sections 3 and 4.2.
Table 1 also contains the results for experiments
in the biomedical domain. The first thing to notice
is that while the upper bound for the unlexicalised
parser is similar to that for the DepBank experiments
in Section 4.2, the baseline results are considerably
higher. This is largely due to the nature of the dataset
? since many complicated multi-word phrases are
treated as single nodes, the parser is not evaluated on
edges within these nodes. In addition, treating these
nodes as unseen words eliminates many incorrect
derivations that would otherwise split the phrases.
This results in a naturally higher baseline of 79.38%,
and also makes it more difficult to further improve
the performance.
The edge scoring methods I, CES1 and ECES1
deliver F-scores lower than the baseline in this ex-
periment. RES, CES2 and ECES2 yield a modest
improvement in both F-score and Spearman?s cor-
relation. Finally, the combination methods again
give the best performance, with CMB2 delivering an
F-score of 80.16%, an absolute increase of 0.78%,
which is statistically significant (p < 0.05). The
experiment shows that our self-learning framework
works on very different domains, and it can be used
to significantly increase the accuracy of an unlexi-
calised parser without requiring any annotated data.
5 Conclusion
We developed a new self-learning framework for de-
pendency graph reranking that requires only a plain-
text corpus from a suitable domain. We automati-
cally parse this corpus and use the highest ranked
analyses to estimate maximum likelihood probabili-
ties for bilexical relations. Every dependency graph
is first modified to incorporate additional edges that
model selected higher-order dependency path rela-
tionships. Each edge in the graph is then assigned a
confidence score based on statistics from the back-
ground corpus and ranking preferences from the un-
398
lexicalised parser. We also described a novel method
for smoothing these scores using directional dis-
tributional similarity measures. Finally, the edge
scores are combined into an overall graph score by
first averaging them over individual nodes.
As the method requires no annotated data, it can
be easily adapted to different domains and genres.
Our experiments showed that the reranking process
significantly improved performance on both WSJ
and biomedical data.
References
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose?,
and Eric Nyberg. 2010. Sentiment Classification us-
ing Automatically Extracted Subgraph Features. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 693?702.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the COLING/ACL
on Main conference poster sessions, number July,
pages 41?48, Morristown, NJ, USA. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77?80, Sydney, Aus-
tralia. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics - ACL ?05,
1(June):173?180.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, vol-
ume 45, pages 248?255.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The 17th International Con-
ference on Machine Learning (ICML).
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
pages 167?202.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(1):180?182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the EACL03:
4th International Workshop on Linguistically Inter-
preted Corpora (LINC-03), pages 1?8.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, number July, pages 423?430. Association for
Computational Linguistics Morristown, NJ, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, pages 1?22.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, number June, pages 152?
159, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage CCG parser. In
Australasian Language Technology Association Work-
shop 2010, page 90.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL (ACL ?06),
pages 433?440, Morristown, NJ, USA. Association for
Computational Linguistics.
Barbara Plank and Gertjan van Noord. 2008. Explor-
ing an auxiliary distribution based approach to domain
adaptation of a syntactic disambiguation model. In
Coling 2008: Proceedings of the Workshop on Cross-
Framework and Cross- Domain Parser Evaluation,
pages 9?16, Manchester, UK. Association for Com-
putational Linguistics.
399
Marek Rei. 2013. Minimally supervised dependency-
based methods for natural language processing. Ph.D.
thesis, University of Cambridge.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proceedings of the fifth conference on Applied
natural language processing, volume 1, pages 96?102,
Morristown, NJ, USA. Association for Computational
Linguistics.
Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun?ichi
Tsujii. 2008. GENIA-GR: a Grammatical Relation
Corpus for Parser Evaluation in the Biomedical Do-
main. In Proceedings of LREC, pages 1942?1948.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the 10th International Conference on
Parsing Technologies, number June, pages 1?10, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Watson, Ted Briscoe, and John Carroll. 2007.
Semi-supervised training of a statistical parser from
unlabeled partially-bracketed data. Proceedings of the
10th International Conference on Parsing Technolo-
gies - IWPT ?07, (June):23?32.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1556?1565.
400
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180?189,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A New Dataset and Method for Automatically Grading ESOL Texts
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Ben Medlock
iLexIR Ltd
Cambridge
United Kingdom
ben@ilexir.co.uk
Abstract
We demonstrate how supervised discrimina-
tive machine learning techniques can be used
to automate the assessment of ?English as a
Second or Other Language? (ESOL) examina-
tion scripts. In particular, we use rank prefer-
ence learning to explicitly model the grade re-
lationships between scripts. A number of dif-
ferent features are extracted and ablation tests
are used to investigate their contribution to
overall performance. A comparison between
regression and rank preference models further
supports our method. Experimental results on
the first publically available dataset show that
our system can achieve levels of performance
close to the upper bound for the task, as de-
fined by the agreement between human exam-
iners on the same corpus. Finally, using a set
of ?outlier? texts, we test the validity of our
model and identify cases where the model?s
scores diverge from that of a human examiner.
1 Introduction
The task of automated assessment of free text fo-
cuses on automatically analysing and assessing the
quality of writing competence. Automated assess-
ment systems exploit textual features in order to
measure the overall quality and assign a score to a
text. The earliest systems used superficial features,
such as word and sentence length, as proxies for
understanding the text. More recent systems have
used more sophisticated automated text processing
techniques to measure grammaticality, textual co-
herence, prespecified errors, and so forth.
Deployment of automated assessment systems
gives a number of advantages, such as the reduced
workload in marking texts, especially when applied
to large-scale assessments. Additionally, automated
systems guarantee the application of the same mark-
ing criteria, thus reducing inconsistency, which may
arise when more than one human examiner is em-
ployed. Often, implementations include feedback
with respect to the writers? writing abilities, thus fa-
cilitating self-assessment and self-tutoring.
Implicitly or explicitly, previous work has mostly
treated automated assessment as a supervised text
classification task, where training texts are labelled
with a grade and unlabelled test texts are fitted to the
same grade point scale via a regression step applied
to the classifier output (see Section 6 for more de-
tails). Different techniques have been used, includ-
ing cosine similarity of vectors representing text in
various ways (Attali and Burstein, 2006), often com-
bined with dimensionality reduction techniques such
as Latent Semantic Analysis (LSA) (Landauer et al,
2003), generative machine learning models (Rudner
and Liang, 2002), domain-specific feature extraction
(Attali and Burstein, 2006), and/or modified syntac-
tic parsers (Lonsdale and Strong-Krause, 2003).
A recent review identifies twelve different auto-
mated free-text scoring systems (Williamson, 2009).
Examples include e-Rater (Attali and Burstein,
2006), Intelligent Essay Assessor (IEA) (Landauer
et al, 2003), IntelliMetric (Elliot, 2003; Rudner et
al., 2006) and Project Essay Grade (PEG) (Page,
2003). Several of these are now deployed in high-
stakes assessment of examination scripts. Although
there are many published analyses of the perfor-
180
mance of individual systems, as yet there is no pub-
lically available shared dataset for training and test-
ing such systems and comparing their performance.
As it is likely that the deployment of such systems
will increase, standardised and independent evalua-
tion methods are important. We make such a dataset
of ESOL examination scripts available1 (see Section
2 for more details), describe our novel approach to
the task, and provide results for our system on this
dataset.
We address automated assessment as a supervised
discriminative machine learning problem and par-
ticularly as a rank preference problem (Joachims,
2002). Our reasons are twofold:
Discriminative classification techniques often
outperform non-discriminative ones in the context of
text classification (Joachims, 1998). Additionally,
rank preference techniques (Joachims, 2002) allow
us to explicitly learn an optimal ranking model of
text quality. Learning a ranking directly, rather than
fitting a classifier score to a grade point scale after
training, is both a more generic approach to the task
and one which exploits the labelling information in
the training data efficiently and directly.
Techniques such as LSA (Landauer and Foltz,
1998) measure, in addition to writing competence,
the semantic relevance of a text written in response
to a given prompt. However, although our corpus
of manually-marked texts was produced by learners
of English in response to prompts eliciting free-text
answers, the marking criteria are primarily based on
the accurate use of a range of different linguistic
constructions. For this reason, we believe that an
approach which directly measures linguistic compe-
tence will be better suited to ESOL text assessment,
and will have the additional advantage that it may
not require retraining for new prompts or tasks.
As far as we know, this is the first application
of a rank preference model to automated assess-
ment (hereafter AA). In this paper, we report exper-
iments on rank preference Support Vector Machines
(SVMs) trained on a relatively small amount of data,
on identification of appropriate feature types derived
automatically from generic text processing tools, on
comparison with a regression SVM model, and on
the robustness of the best model to ?outlier? texts.
1http://www.ilexir.com/
We report a consistent, comparable and replicable
set of results based entirely on the new dataset and
on public-domain tools and data, whilst also exper-
imentally motivating some novel feature types for
the AA task, thus extending the work described in
(Briscoe et al, 2010).
In the following sections we describe in more de-
tail the dataset used for training and testing, the sys-
tem developed, the evaluation methodology, as well
as ablation experiments aimed at studying the con-
tribution of different feature types to the AA task.
We show experimentally that discriminative models
with appropriate feature types can achieve perfor-
mance close to the upper bound, as defined by the
agreement between human examiners on the same
test corpus.
2 Cambridge Learner Corpus
The Cambridge Learner Corpus2 (CLC), developed
as a collaborative project between Cambridge Uni-
versity Press and Cambridge Assessment, is a large
collection of texts produced by English language
learners from around the world, sitting Cambridge
Assessment?s English as a Second or Other Lan-
guage (ESOL) examinations3.
For the purpose of this work, we extracted scripts
produced by learners taking the First Certificate in
English (FCE) exam, which assesses English at an
upper-intermediate level. The scripts, which are
anonymised, are annotated using XML and linked
to meta-data about the question prompts, the candi-
date?s grades, native language and age. The FCE
writing component consists of two tasks asking
learners to write either a letter, a report, an article,
a composition or a short story, between 200 and 400
words. Answers to each of these tasks are anno-
tated with marks (in the range 1?40), which have
been fitted to a RASCH model (Fischer and Mole-
naar, 1995) to correct for inter-examiner inconsis-
tency and comparability. In addition, an overall
mark is assigned to both tasks, which is the one we
use in our experiments.
Each script has been also manually tagged with
information about the linguistic errors committed,
2http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/
item3646603/Cambridge-International-Corpus-Cambridge-
Learner-Corpus/?site locale=en GB
3http://www.cambridgeesol.org/
181
using a taxonomy of approximately 80 error types
(Nicholls, 2003). The following is an example error-
coded sentence:
In the morning, you are <NS type = ?TV?>
waken|woken</NS> up by a singing puppy.
In this sentence, TV denotes an incorrect tense of
verb error, where waken can be corrected to woken.
Our data consists of 1141 scripts from the year
2000 for training written by 1141 distinct learners,
and 97 scripts from the year 2001 for testing written
by 97 distinct learners. The learners? ages follow
a bimodal distribution with peaks at approximately
16?20 and 26?30 years of age.
The prompts eliciting the free text are provided
with the dataset. However, in this paper we make
no use of prompt information and do not make any
attempt to check that the text answer is appropriate
to the prompt. Our focus is on developing an accu-
rate AA system for ESOL text that does not require
prompt-specific or topic-specific training. There is
no overlap between the prompts used in 2000 and in
2001. A typical prompt taken from the 2000 training
dataset is shown below:
Your teacher has asked you to write a story for the
school?s English language magazine. The story must
begin with the following words: ?Unfortunately, Pat
wasn?t very good at keeping secrets?.
3 Approach
We treat automated assessment of ESOL text (see
Section 2) as a rank preference learning problem
(see Section 1). In the experiments reported here
we use Support Vector Machines (SVMs) (Vap-
nik, 1995) through the SVMlight package (Joachims,
1999). Using the dataset described in Section 2, a
number of linguistic features are automatically ex-
tracted and their contribution to overall performance
is investigated.
3.1 Rank preference model
SVMs have been extensively used for learning clas-
sification, regression and ranking functions. In its
basic form, a binary SVM classifier learns a linear
threshold function that discriminates data points of
two categories. By using a different loss function,
the ?-insensitive loss function (Smola, 1996), SVMs
can also perform regression. SVMs in regression
mode estimate a function that outputs a real number
based on the training data. In both cases, the model
generalises by computing a hyperplane that has the
largest (soft-)margin.
In rank preference SVMs, the goal is to learn a
ranking function which outputs a score for each data
point, from which a global ordering of the data is
constructed. This procedure requires a setR consist-
ing of training samples ~xn and their target rankings
rn:
R = {(~x1, r1), (~x2, r2), ..., (~xn, rn)} (1)
such that ~xi R ~xj when ri < rj , where
1 ? i, j ? n and i 6= j.
A rank preference model is not trained directly on
this set of data objects and their labels; rather a set of
pair-wise difference vectors is created. The goal of
a linear ranking model is to compute a weight vec-
tor ~w that maximises the number of correctly ranked
pairs:
?(~xi R ~xj) : ~w(~xi ? ~xj) > 0 (2)
This is equivalent to solving the following opti-
misation problem:
Minimise:
1
2
?~w?2 + C
?
?ij (3)
Subject to the constraints:
?(~xi R ~xj) : ~w(~xi ? ~xj) ? 1? ?ij (4)
?ij ? 0 (5)
The factor C allows a trade-off between the train-
ing error and the margin size, while ?ij are non-
negative slack variables that measure the degree of
misclassification.
The optimisation problem is equivalent to that for
the classification model on pair-wise difference vec-
tors. In this case, generalisation is achieved by max-
imising the differences between closely-ranked data
pairs.
The principal advantage of applying rank prefer-
ence learning to the AA task is that we explicitly
182
model the grade relationships between scripts and
do not need to apply a further regression step to fit
the classifier output to the scoring scheme. The re-
sults reported in this paper are obtained by learning
a linear classification function.
3.2 Feature set
We parsed the training and test data (see Section
2) using the Robust Accurate Statistical Parsing
(RASP) system with the standard tokenisation and
sentence boundary detection modules (Briscoe et al,
2006) in order to broaden the space of candidate fea-
tures suitable for the task. The features used in our
experiments are mainly motivated by the fact that
lexical and grammatical features should be highly
discriminative for the AA task. Our full feature set
is as follows:
i. Lexical ngrams
(a) Word unigrams
(b) Word bigrams
ii. Part-of-speech (PoS) ngrams
(a) PoS unigrams
(b) PoS bigrams
(c) PoS trigrams
iii. Features representing syntax
(a) Phrase structure (PS) rules
(b) Grammatical relation (GR) distance mea-
sures
iv. Other features
(a) Script length
(b) Error-rate
Word unigrams and bigrams are lower-cased and
used in their inflected forms. PoS unigrams, bigrams
and trigrams are extracted using the RASP tagger,
which uses the CLAWS4 tagset. The most proba-
ble posterior tag per word is used to construct PoS
ngram features, but we use the RASP parser?s op-
tion to analyse words assigned multiple tags when
the posterior probability of the highest ranked tag is
less than 0.9, and the next n tags have probability
greater than 150 of it.
4http://ucrel.lancs.ac.uk/claws/
Based on the most likely parse for each identified
sentence, we extract the rule names from the phrase
structure (PS) tree. RASP?s rule names are semi-
automatically generated and encode detailed infor-
mation about the grammatical constructions found
(e.g. V1/modal bse/+-, ?a VP consisting of a modal
auxiliary head followed by an (optional) adverbial
phrase, followed by a VP headed by a verb with base
inflection?). Moreover, rule names explicitly repre-
sent information about peripheral or rare construc-
tions (e.g. S/pp-ap s-r, ?a S with preposed PP with
adjectival complement, e.g. for better or worse, he
left?), as well as about fragmentary and likely extra-
grammatical sequences (e.g. T/txt-frag, ?a text unit
consisting of 2 or more subanalyses that cannot be
combined using any rule in the grammar?). There-
fore, we believe that many (longer-distance) gram-
matical constructions and errors found in texts can
be (implicitly) captured by this feature type.
In developing our AA system, a number of dif-
ferent grammatical complexity measures were ex-
tracted from parses, and their impact on the accuracy
of the system was explored. For the experiments re-
ported here, we use complexity measures represent-
ing the sum of the longest distance in word tokens
between a head and dependent in a grammatical re-
lation (GR) from the RASP GR output, calculated
for each GR graph from the top 10 parses per sen-
tence. In particular, we extract the mean and median
values of these distances per sentence and use the
maximum values per script. Intuitively, this feature
captures information about the grammatical sophis-
tication of the writer. However, it may also be con-
founded in cases where sentence boundaries are not
identified through, for example, poor punctuation.
Although the CLC contains information about the
linguistic errors committed (see Section 2), we try
to extract an error-rate in a way that doesn?t require
manually tagged data. However, we also use an
error-rate calculated from the CLC error tags to ob-
tain an upper bound for the performance of an auto-
mated error estimator (true CLC error-rate).
In order to estimate the error-rate, we build a tri-
gram language model (LM) using ukWaC (ukWaC
LM) (Ferraresi et al, 2008), a large corpus of En-
glish containing more than 2 billion tokens. Next,
we extend our language model with trigrams ex-
tracted from a subset of the texts contained in the
183
Features
Pearson?s Spearman?s
correlation correlation
word ngrams 0.601 0.598
+PoS ngrams 0.682 0.687
+script length 0.692 0.689
+PS rules 0.707 0.708
+complexity 0.714 0.712
Error-rate features
+ukWaC LM 0.735 0.758
+CLC LM 0.741 0.773
+true CLC error-rate 0.751 0.789
Table 1: Correlation between the CLC scores and the AA
system predicted values.
CLC (CLC LM). As the CLC contains texts pro-
duced by second language learners, we only extract
frequently occurring trigrams from highly ranked
scripts to avoid introducing erroneous ones to our
language model. A word trigram in test data is
counted as an error if it is not found in the language
model. We compute presence/absence efficiently us-
ing a Bloom filter encoding of the language models
(Bloom, 1970).
Feature instances of types i and ii are weighted
using the tf*idf scheme and normalised by the L2
norm. Feature type iii is weighted using frequency
counts, while iii and iv are scaled so that their final
value has approximately the same order of magni-
tude as i and ii.
The script length is based on the number of words
and is mainly added to balance the effect the length
of a script has on other features. Finally, features
whose overall frequency is lower than four are dis-
carded from the model.
4 Evaluation
In order to evaluate our AA system, we use two cor-
relation measures, Pearson?s product-moment cor-
relation coefficient and Spearman?s rank correla-
tion coefficient (hereafter Pearson?s and Spearman?s
correlation respectively). Pearson?s correlation de-
termines the degree to which two linearly depen-
dent variables are related. As Pearson?s correlation
is sensitive to the distribution of data and, due to
outliers, its value can be misleading, we also re-
port Spearman?s correlation. The latter is a non-
parametric robust measure of association which is
Ablated Pearson?s Spearman?s
feature correlation correlation
none 0.741 0.773
word ngrams 0.713 0.762
PoS ngrams 0.724 0.737
script length 0.734 0.772
PS rules 0.712 0.731
complexity 0.738 0.760
ukWaC+CLC LM 0.714 0.712
Table 2: Ablation tests showing the correlation between
the CLC and the AA system.
sensitive only to the ordinal arrangement of values.
As our data contains some tied values, we calculate
Spearman?s correlation by using Pearson?s correla-
tion on the ranks.
Table 1 presents the Pearson?s and Spearman?s
correlation between the CLC scores and the AA sys-
tem predicted values, when incrementally adding
to the model the feature types described in Sec-
tion 3.2. Each feature type improves the model?s
performance. Extending our language model with
frequent trigrams extracted from the CLC improves
Pearson?s and Spearman?s correlation by 0.006 and
0.015 respectively. The addition of the error-rate ob-
tained from the manually annotated CLC error tags
on top of all the features further improves perfor-
mance by 0.01 and 0.016. An evaluation of our best
error detection method shows a Pearson correlation
of 0.611 between the estimated and the true CLC er-
ror counts. This suggests that there is room for im-
provement in the language models we developed to
estimate the error-rate. In the experiments reported
hereafter, we use the ukWaC+CLC LM to calculate
the error-rate.
In order to assess the independent as opposed to
the order-dependent additive contribution of each
feature type to the overall performance of the sys-
tem, we run a number of ablation tests. An ablation
test consists of removing one feature of the system
at a time and re-evaluating the model on the test set.
Table 2 presents Pearson?s and Spearman?s correla-
tion between the CLC and our system, when remov-
ing one feature at a time. All features have a positive
effect on performance, while the error-rate has a big
impact, as its absence is responsible for a 0.061 de-
crease of Spearman?s correlation. In addition, the
184
Model
Pearson?s Spearman?s
correlation correlation
Regression 0.697 0.706
Rank preference 0.741 0.773
Table 3: Comparison between regression and rank pref-
erence model.
removal of either the word ngrams, the PS rules, or
the error-rate estimate contributes to a large decrease
in Pearson?s correlation.
In order to test the significance of the improved
correlations, we ran one-tailed t-tests with a = 0.05
for the difference between dependent correlations
(Williams, 1959; Steiger, 1980). The results showed
that PoS ngrams, PS rules, the complexity measures,
and the estimated error-rate contribute significantly
to the improvement of Spearman?s correlation, while
PS rules also contribute significantly to the improve-
ment of Pearson?s correlation.
One of the main approaches adopted by previ-
ous systems involves the identification of features
that measure writing skill, and then the application
of linear or stepwise regression to find optimal fea-
ture weights so that the correlation with manually
assigned scores is maximised. We trained a SVM
regression model with our full set of feature types
and compared it to the SVM rank preference model.
The results are given in Table 3. The rank preference
model improves Pearson?s and Spearman?s correla-
tion by 0.044 and 0.067 respectively, and these dif-
ferences are significant, suggesting that rank prefer-
ence is a more appropriate model for the AA task.
Four senior and experienced ESOL examiners re-
marked the 97 FCE test scripts drawn from 2001 ex-
ams, using the marking scheme from that year (see
Section 2). In order to obtain a ceiling for the perfor-
mance of our system, we calculate the average corre-
lation between the CLC and the examiners? scores,
and find an upper bound of 0.796 and 0.792 Pear-
son?s and Spearman?s correlation respectively.
In order to evaluate the overall performance of our
system, we calculate its correlation with the four se-
nior examiners in addition to the RASCH-adjusted
CLC scores. Tables 4 and 5 present the results ob-
tained.
The average correlation of the AA system with the
CLC and the examiner scores shows that it is close
CLC E1 E2 E3 E4 AA
CLC - 0.820 0.787 0.767 0.810 0.741
E1 0.820 - 0.851 0.845 0.878 0.721
E2 0.787 0.851 - 0.775 0.788 0.730
E3 0.767 0.845 0.775 - 0.779 0.747
E4 0.810 0.878 0.788 0.779 - 0.679
AA 0.741 0.721 0.730 0.747 0.679 -
Avg 0.785 0.823 0.786 0.782 0.786 0.723
Table 4: Pearson?s correlation of the AA system predicted
values with the CLC and the examiners? scores, where E1
refers to the first examiner, E2 to the second etc.
CLC E1 E2 E3 E4 AA
CLC - 0.801 0.799 0.788 0.782 0.773
E1 0.801 - 0.809 0.806 0.850 0.675
E2 0.799 0.809 - 0.744 0.787 0.724
E3 0.788 0.806 0.744 - 0.794 0.738
E4 0.782 0.850 0.787 0.794 - 0.697
AA 0.773 0.675 0.724 0.738 0.697 -
Avg 0.788 0.788 0.772 0.774 0.782 0.721
Table 5: Spearman?s correlation of the AA system pre-
dicted values with the CLC and the examiners? scores,
where E1 refers to the first examiner, E2 to the second
etc.
to the upper bound for the task. Human?machine
agreement is comparable to that of human?human
agreement, with the exception of Pearson?s correla-
tion with examiner E4 and Spearman?s correlation
with examiners E1 and E4, where the discrepancies
are higher. It is likely that a larger training set and/or
more consistent grading of the existing training data
would help to close this gap. However, our system is
not measuring some properties of the scripts, such as
discourse cohesion or relevance to the prompt elicit-
ing the text, that examiners will take into account.
5 Validity tests
The practical utility of an AA system will depend
strongly on its robustness to subversion by writers
who understand something of its workings and at-
tempt to exploit this to maximise their scores (in-
dependently of their underlying ability). Surpris-
ingly, there is very little published data on the ro-
bustness of existing systems. However, Powers et
al. (2002) invited writing experts to trick the scoring
185
capabilities of an earlier version of e-Rater (Burstein
et al, 1998). e-Rater (see Section 6 for more de-
tails) assigns a score to a text based on linguistic fea-
ture types extracted using relatively domain-specific
techniques. Participants were given a description of
these techniques as well as of the cue words that the
system uses. The results showed that it was easier
to fool the system into assigning higher than lower
scores.
Our goal here is to determine the extent to which
knowledge of the feature types deployed poses a
threat to the validity of our system, where certain
text generation strategies may give rise to large pos-
itive discrepancies. As mentioned in Section 2, the
marking criteria for FCE scripts are primarily based
on the accurate use of a range of different grammati-
cal constructions relevant to specific communicative
goals, but our system assesses this indirectly.
We extracted 6 high-scoring FCE scripts from the
CLC that do not overlap with our training and test
data. Based on the features used by our system and
without bias towards any modification, we modified
each script in one of the following ways:
i. Randomly order:
(a) word unigrams within a sentence
(b) word bigrams within a sentence
(c) word trigrams within a sentence
(d) sentences within a script
ii. Swap words that have the same PoS within a
sentence
Although the above modifications do not ex-
haust the potential challenges a deployed AA system
might face, they represent a threat to the validity of
our system since we are using a highly related fea-
ture set. In total, we create 30 such ?outlier? texts,
which were given to an ESOL examiner for mark-
ing. Using the ?outlier? scripts as well as their origi-
nal/unmodified versions, we ran our system on each
modification separately and calculated the correla-
tion between the predicted values and the examiner?s
scores. Table 6 presents the results.
The predicted values of the system have a high
correlation with the examiner?s scores when tested
on ?outlier? texts of modification types i(a), i(b) and
Modification
Pearson?s Spearman?s
correlation correlation
i(a) 0.960 0.912
i(b) 0.938 0.914
i(c) 0.801 0.867
i(d) 0.08 0.163
ii 0.634 0.761
Table 6: Correlation between the predicted values and the
examiner?s scores on ?outlier? texts.
i(c). However, as i(c) has a lower correlation com-
pared to i(a) and i(b), it is likely that a random order-
ing of ngrams with N > 3 will further decrease per-
formance. A modification of type ii, where words
with the same PoS within a sentence are swapped,
results in a Pearson and Spearman correlation of
0.634 and 0.761 respectively.
Analysis of the results showed that our system
predicted higher scores than the ones assigned by the
examiner. This can be explained by the fact that texts
produced using modification type ii contain a small
portion of correct sentences. However, the marking
criteria are based on the overall writing quality. The
final case, where correct sentences are randomly or-
dered, receives the lowest correlation. As our sys-
tem is not measuring discourse cohesion, discrepan-
cies are much higher; the system?s predicted scores
are high whilst the ones assigned by the examiner
are very low. However, for a writer to be able to
generate text of this type already requires significant
linguistic competence, whilst a number of generic
methods for assessing text and/or discourse cohe-
sion have been developed and could be deployed in
an extended version of our system.
It is also likely that highly creative ?outlier? essays
may give rise to large negative discrepancies. Recent
comments in the British media have focussed on this
issue, reporting that, for example, one deployed es-
say marking system assigned Winston Churchill?s
speech ?We Shall Fight on the Beaches? a low score
because of excessive repetition5. Our model pre-
dicted a high passing mark for this text, but not the
highest one possible, that some journalists clearly
feel it deserves.
5http://news.bbc.co.uk/1/hi/education/8356572.stm
186
6 Previous work
In this section we briefly discuss a number of the
more influential and/or better described approaches.
Pe?rez-Mar??n et al (2009), Williamson (2009), Dikli
(2006) and Valenti et al (2003) provide a more de-
tailed overview of existing AA systems.
Project Essay Grade (PEG) (Page, 2003), one of
the earliest systems, uses a number of manually-
identified mostly shallow textual features, which are
considered to be proxies for intrinsic qualities of
writing competence. Linear regression is used to as-
sign optimal feature weights that maximise the cor-
relation with the examiner?s scores. The main is-
sue with this system is that features such as word
length and script length are easy to manipulate in-
dependently of genuine writing ability, potentially
undermining the validity of the system.
In e-Rater (Attali and Burstein, 2006), texts
are represented using vectors of weighted features.
Each feature corresponds to a different property of
texts, such as an aspect of grammar, style, discourse
and topic similarity. Additional features, represent-
ing stereotypical grammatical errors for example,
are extracted using manually-coded task-specific de-
tectors based, in part, on typical marking criteria. An
unmarked text is scored based on the cosine simi-
larity between its weighted vector and the ones in
the training set. Feature weights and/or scores can
be fitted to a marking scheme by stepwise or lin-
ear regression. Unlike our approach, e-Rater mod-
els discourse structure, semantic coherence and rel-
evance to the prompt. However, the system contains
manually developed task-specific components and
requires retraining or tuning for each new prompt
and assessment task.
Intelligent Essay Assessor (IEA) (Landauer et al,
2003) uses Latent Semantic Analysis (LSA) (Lan-
dauer and Foltz, 1998) to compute the semantic sim-
ilarity between texts, at a specific grade point, and
a test text. In LSA, text is represented by a ma-
trix, where rows correspond to words and columns
to context (texts). Singular Value Decomposition
(SVD) is used to obtain a reduced dimension matrix
clustering words and contexts. The system is trained
on topic and/or prompt specific texts while test texts
are assigned a score based on the ones in the training
set that are most similar. The overall score, which is
calculated using regression techniques, is based on
the content score as well as on other properties of
texts, such as style, grammar, and so forth, though
the methods used to assess these are not described
in any detail in published work. Again, the system
requires retraining or tuning for new prompts and
assessment tasks.
Lonsdale and Strong-Krause (2003) use a mod-
ified syntactic parser to analyse and score texts.
Their method is based on a modified version of
the Link Grammar parser (Sleator and Templerley,
1995) where the overall score of a text is calculated
as the average of the scores assigned to each sen-
tence. Sentences are scored on a five-point scale
based on the parser?s cost vector, which roughly
measures the complexity and deviation of a sentence
from the parser?s grammatical model. This approach
bears some similarities to our use of grammatical
complexity and extragrammaticality features, but
grammatical features represent only one component
of our overall system, and of the task.
The Bayesian Essay Test Scoring sYstem
(BETSY) (Rudner and Liang, 2002) uses multino-
mial or Bernoulli Naive Bayes models to classify
texts into different classes (e.g. pass/fail, grades A?
F) based on content and style features such as word
unigrams and bigrams, sentence length, number of
verbs, noun?verb pairs etc. Classification is based
on the conditional probability of a class given a set
of features, which is calculated using the assumption
that each feature is independent of the other. This
system shows that treating AA as a text classifica-
tion problem is viable, but the feature types are all
fairly shallow, and the approach doesn?t make effi-
cient use of the training data as a separate classifier
is trained for each grade point.
Recently, Chen et al (2010) has proposed an un-
supervised approach to AA of texts addressing the
same topic, based on a voting algorithm. Texts are
clustered according to their grade and given an ini-
tial Z-score. A model is trained where the initial
score of a text changes iteratively based on its sim-
ilarity with the rest of the texts as well as their Z-
scores. The approach might be better described as
weakly supervised as the distribution of text grades
in the training data is used to fit the final Z-scores to
grades. The system uses a bag-of-words represen-
tation of text, so would be easy to subvert. Never-
187
theless, exploration of the trade-offs between degree
of supervision required in training and grading ac-
curacy is an important area for future research.
7 Conclusions and future work
Though many of the systems described in Section
6 have been shown to correlate well with examin-
ers? marks on test data in many experimental con-
texts, no cross-system comparisons are available be-
cause of the lack of a shared training and test dataset.
Furthermore, none of the published work of which
we are aware has systematically compared the con-
tribution of different feature types to the AA task,
and only one (Powers et al, 2002) assesses the ease
with which the system can be subverted given some
knowledge of the features deployed.
We have shown experimentally how rank prefer-
ence models can be effectively deployed for auto-
mated assessment of ESOL free-text answers. Based
on a range of feature types automatically extracted
using generic text processing techniques, our sys-
tem achieves performance close to the upper bound
for the task. Ablation tests highlight the contribu-
tion of each feature type to the overall performance,
while significance of the resulting improvements in
correlation with human scores has been calculated.
A comparison between regression and rank prefer-
ence models further supports our approach. Prelim-
inary experiments based on a set of ?outlier? texts
have shown the types of texts for which the system?s
scoring capability can be undermined.
We plan to experiment with better error detection
techniques, since the overall error-rate of a script is
one of the most discriminant features. Briscoe et
al. (2010) describe an approach to automatic off-
prompt detection which does not require retraining
for each new question prompt and which we plan
to integrate with our system. It is clear from the
?outlier? experiments reported here that our system
would benefit from features assessing discourse co-
herence, and to a lesser extent from features as-
sessing semantic (selectional) coherence over longer
bounds than those captured by ngrams. The addition
of an incoherence metric to the feature set of an AA
system has been shown to improve performance sig-
nificantly (Miltsakaki and Kukich, 2000; Miltsakaki
and Kukich, 2004).
Finally, we hope that the release of the training
and test dataset described here will facilitate further
research on the AA task for ESOL free text and, in
particular, precise comparison of different systems,
feature types, and grade fitting methods.
Acknowledgements
We would like to thank Cambridge ESOL, a division
of Cambridge Assessment, for permission to use and
distribute the examination scripts. We are also grate-
ful to Cambridge Assessment for arranging for the
test scripts to be remarked by four of their senior ex-
aminers. Finally, we would like to thank Marek Rei,
?istein Andersen and the anonymous reviewers for
their useful comments.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3):1?30.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13(7):422?426, July.
E.J. Briscoe, J. Carroll, and R Watson. 2006. The second
release of the RASP system. In ACL-Coling?06 In-
teractive Presentation Session, pages 77?80, Sydney,
Australia.
E.J. Briscoe, B. Medlock, and ?. Andersen. 2010. Au-
tomated Assessment of ESOL Free Text Examinations.
Cambridge University, Computer Laboratory, TR-790.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. Proceedings of the 36th
annual meeting on Association for Computational Lin-
guistics, pages 206?210.
YY Chen, CL Liu, TH Chang, and CH Lee. 2010.
An Unsupervised Automated Essay Scoring System.
IEEE Intelligent Systems, pages 61?67.
Semire Dikli. 2006. An overview of automated scoring
of essays. Journal of Technology, Learning, and As-
sessment, 5(1).
S. Elliot. 2003. IntelliMetric: From here to validity. In
M.D. Shermis and J.C. Burstein, editors, Automated
essay scoring: A cross-disciplinary perspective, pages
71?86.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
188
In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-
ceedings of the 4th Web as Corpus Workshop (WAC-4).
G.H. Fischer and I.W. Molenaar. 1995. Rasch models:
Foundations, recent developments, and applications.
Springer.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning, pages 137?142. Springer.
Thorsten Joachims. 1999. Making large scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Mining
(KDD), pages 133?142. ACM.
T.K. Landauer and P.W. Foltz. 1998. An introduction to
latent semantic analysis. Discourse processes, pages
259?284.
T.K. Landauer, D. Laham, and P.W. Foltz. 2003. Au-
tomated scoring and annotation of essays with the In-
telligent Essay Assessor. In M.D. Shermis and J.C.
Burstein, editors, Automated essay scoring: A cross-
disciplinary perspective, pages 87?112.
Deryle Lonsdale and D. Strong-Krause. 2003. Auto-
mated rating of ESL essays. In Proceedings of the
HLT-NAACL 2003 Workshop: Building Educational
Applications Using Natural Language Processing.
Eleni Miltsakaki and Karen Kukich. 2000. Automated
evaluation of coherence in student essays. In Proceed-
ings of LREC 2000.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(01):25?55, March.
D. Nicholls. 2003. The Cambridge Learner Corpus: Er-
ror coding and analysis for lexicography and ELT. In
Proceedings of the Corpus Linguistics 2003 confer-
ence, pages 572?581.
E.B. Page. 2003. Project essay grade: PEG. In M.D.
Shermis and J.C. Burstein, editors, Automated essay
scoring: A cross-disciplinary perspective, pages 43?
54.
D. Pe?rez-Mar??n, Ismael Pascual-Nieto, and P. Rodr??guez.
2009. Computer-assisted assessment of free-text
answers. The Knowledge Engineering Review,
24(04):353?374, December.
D.E. Powers, J.C. Burstein, M. Chodorow, M.E. Fowles,
and K. Kukich. 2002. Stumping e-rater: challenging
the validity of automated essay scoring. Computers in
Human Behavior, 18(2):103?134.
L.M. Rudner and Tahung Liang. 2002. Automated essay
scoring using Bayes? theorem. The Journal of Tech-
nology, Learning and Assessment, 1(2):3?21.
L.M. Rudner, Veronica Garcia, and Catherine Welch.
2006. An Evaluation of the IntelliMetric Essay Scor-
ing System. Journal of Technology, Learning, and As-
sessment, 4(4):1?21.
D.D.K. Sleator and D. Templerley. 1995. Parsing En-
glish with a link grammar. Proceedings of the 3rd In-
ternational Workshop on Parsing Technologies, ACL.
AJ Smola. 1996. Regression estimation with support
vector learning machines. Master?s thesis, Technische
Universita?t Munchen.
J.H. Steiger. 1980. Tests for comparing elements of a
correlation matrix. Psychological Bulletin, 87(2):245?
251.
Salvatore Valenti, Francesca Neri, and Alessandro Cuc-
chiarelli. 2003. An overview of current research
on automated essay grading. Journal of Information
Technology Education, 2:3?118.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer.
E. J. Williams. 1959. The Comparison of Regression
Variables. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 21(2):396?399.
DM Williamson. 2009. A Framework for Implement-
ing Automated Scoring. In Annual Meeting of the
American Educational Research Association and the
National Council on Measurement in Education, San
Diego, CA.
189
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Active Learning for Constrained Dirichlet Process Mixture Models
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Recent work applied Dirichlet Process
Mixture Models to the task of verb cluster-
ing, incorporating supervision in the form
of must-links and cannot-links constraints
between instances. In this work, we intro-
duce an active learning approach for con-
straint selection employing uncertainty-
based sampling. We achieve substantial
improvements over random selection on
two datasets.
1 Introduction
Bayesian non-parametric mixture models have the
attractive property that the number of components
used to model the data is not fixed in advance but
is determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel in-
formation. Recent work has applied such mod-
els to various tasks with promising results, e.g.
Teh (2006) and Cohn et al (2009).
Vlachos et al (2009) applied the basic model
of this class, the Dirichlet Process Mixture Model
(DPMM), to lexical-semantic verb clustering with
encouraging results. The task involves discov-
ering classes of verbs similar in terms of their
syntactic-semantic properties (e.g. MOTION class
for travel, walk, run, etc.). Such classes can pro-
vide important support for other tasks, such as
word sense disambiguation, parsing and seman-
tic role labeling. (Dang, 2004; Swier and Steven-
son, 2004) Although some fixed classifications are
available these are not comprehensive and are in-
adequate for specific domains.
Furthermore, Vlachos et al (2009) used a con-
strained version of the DPMM in order to guide
clustering towards some prior intuition or consid-
erations relevant to the specific task at hand. This
supervision was modelled as pairwise constraints
between instances and it informs the model of re-
lations between them that cannot be recovered by
the model on the basis of the feature representa-
tion used. Like other forms of supervision, these
constraints require manual annotation and it is im-
portant to maximize the benefits obtained from it.
Therefore it is natural to consider active learning
(Settles, 2009) in order to focus the supervision on
clusterings on which the model is uncertain.
In this work, we propose a simple yet effec-
tive active learning method employing uncertainty
based sampling. The effectiveness of the AL
method is demonstrated on two datasets, one of
which has multiple gold standards.
2 Constrained DPMMs for clustering
In DPMMs, the parameters of each component are
generated by a Dirichlet Process (DP) which can
be seen as a distribution over distributions. Each
instance, represented by its features, is generated
by the component it is assigned to. The compo-
nents discovered correspond to the clusters. The
prior probability of assigning an instance to a par-
ticular component is proportionate to the number
of instances already assigned to it, in other words,
the DPMM exhibits the ?rich get richer? prop-
erty. A popular metaphor to describe the DPMM
which exhibits an equivalent clustering property
is the Chinese Restaurant Process (CRP). Cus-
tomers (instances) arrive at a Chinese restaurant
which has an infinite number of tables (compo-
nents). Each customer sits at one of the tables that
is either occupied or vacant with popular tables at-
tracting more customers.
Following Navarro et al (2006), parameter es-
timation is performed using Gibbs sampling by
sampling the assignment zi of each instance xi
given all the others z?i and the data X:
P (zi = z|z?i, X) ?
p(zi = z|z?i)P (xi|zi = z,X?i) (1)
57
In Eq. 1 p(zi = z|z?i) is the CRP prior and
P (xi|zi = z,X?i) is the distribution that gener-
ates instance xi given it has been assigned to com-
ponent z. This sampling scheme is possible be-
cause the assignments in the model are exchange-
able, i.e. their order is not relevant.
The constrained version of the DPMM uses
pairwise constraints over instances in order to
adapt the clustering discovered. Following
Wagstaff & Cardie (2000), a pair of instances is
either linked together (must-link) or not (cannot-
link). For example, charge and run should form a
must-link if the aim is to cluster MOTION verbs
together, but they should form a cannot-link if we
are interested in BILL verbs. All links are as-
sumed to be consistent with each other. In order
to incorporate the constraints in the DPMM, the
Gibbs sampling scheme is modified so that must-
linked instances are generated by the same compo-
nent and cannot-linked instances always by differ-
ent ones. Following Vlachos et al (2009), for each
instance that does not belong to a linked-group, the
sampler is restricted to choose components that do
not contain instances cannot-linked with it. For
instances in a linked-group, their assignment is
sampled jointly, again taking into account their
cannot-links. This is performed by adding each
instance of the linked-group successively to the
same component. In terms of the CRP metaphor,
customers connected with must-links arrive at the
restaurant and choose a table jointly, respecting
their cannot-links with other customers.
3 Active Constraint Selection
In active learning, the model selects the supervi-
sion to be provided by a human expert. In the con-
text of the DPMMs, the model chooses a pair of
instances for which a must-link or a cannot-link
must be provided. To select the pair, we employ
the simple but effective idea of uncertainty based
sampling. We consider the most informative link
as that on which the model is most uncertain, more
formally the link between instances l?ij that maxi-
mizes the following entropy:
l?ij = argmaxi,j
H(zi = zj) (2)
If we consider clustering as binary classification of
links into must-links and cannot-links, it is equiv-
alent to selecting the pair with the highest label
entropy. During the sampling process used for
parameter inference, component assignments vary
between samples and the components themselves
are not identifiable, i.e. one cannot match the com-
ponents of one sample with those of another. Fur-
thermore, the conditional assignments estimated
during Gibbs sampling (Eq. 1) they do not capture
the uncertainty of the assignments z?i on which
they condition. Therefore, we resort to generating
a set of samples from the (possibly constrained)
DPMM and pick the link on which these sam-
ples maximally disagree, i.e. we approximate the
distribution in Eq. 2 with the probability that in-
stances i, j are in the same cluster or not. Thus,
in a given set of samples the most uncertain link
would be the one between two instances which are
in the same cluster in exactly half of these sam-
ples. Using multiple samples allows us to take into
account the uncertainty in the assignments of the
other instances, as well as the varying number of
components.
Compared to standard pool-based AL, when
clustering with constraints the possible links be-
tween two instances (ignoring transitivity) are
C(N, 2) = N(N ? 1)/2 (N is the size of the
dataset) and there is an equal number of candi-
date queries to be considered, as opposed to N
queries in a supervised classification task. Another
interesting difference is that the the AL process
can be initiated without any supervision, since the
DPMM is unsupervised. On the other hand, in
the standard AL scenario a (usually small) labelled
seed set is used. Therefore, we rely exclusively on
the model and the features to guide the constraint
selection process. If the model combined with the
features is not appropriate for the task then the
constraints chosen are unlikely to be useful.
4 Datasets and Evaluation
In our experiments we used two verb clustering
datasets, one from general English (Sun et al,
2008) and one from the biomedical domain (Ko-
rhonen et al, 2006). In both datasets the fea-
tures for each verb are its subcategorization frames
(SCFs) which capture the syntactic context in
which it occurs. They were acquired automati-
cally using a domain-independent statistical pars-
ing toolkit, RASP (Briscoe and Carroll, 2002), and
a classifier which identifies verbal SCFs. As a
consequence, they include some noise due to stan-
dard text processing and parsing errors and due to
the subtlety of the argument-adjunct distinction.
The general English dataset contains 204 verbs
58
belonging to 17 fine-grained classes in Levin?s
(Levin, 1993) taxonomy so that each class con-
tains 12 verbs. The biomedical dataset consists of
193 medium to high frequency verbs from a cor-
pus of 2230 full-text articles from 3 biomedical
journals. A team of linguists and biologists cre-
ated a three-level gold standard with 16, 34 and
50 classes. Both datasets were pre-processed us-
ing non-negative matrix factorization (Lin, 2007)
which decomposes a large sparse matrix into two
dense matrices (of lower dimensionality) with
non-negative values. In all experiments 35 dimen-
sions were kept. Preliminary experiments with
different number of dimensions kept did not affect
the performance substantially.
We evaluate our results using three informa-
tion theoretic measures: Variation of Informa-
tion (Meila?, 2007), V-measure (Rosenberg and
Hirschberg, 2007) and V-beta (Vlachos et al,
2009). All three assess the two desirable proper-
ties that a clustering should have with respect to
a gold standard, homogeneity and completeness.
Homogeneity reflects the degree to which each
cluster contains instances from a single class and
is defined as the conditional entropy of the class
distribution of the gold standard given the clus-
tering. Completeness reflects the degree to which
each class is contained in a single cluster and is de-
fined as the conditional entropy of clustering given
the class distribution in the gold standard. V-beta
balances these properties explicitly by taking into
account the ratio of the number of cluster discov-
ered over the number of classes in the gold stan-
dard. While an ideal clustering should have both
properties, naively improving one of them can be
harmful for the other. Compared to the more com-
monly used F-measure (Fung et al, 2003), these
measures have the advantage that they do not as-
sume a mapping between clusters and classes.
5 Experiments
We performed experiments in order to assess the
effectiveness of the AL algorithm for the con-
strained DPMM comparing it to random selection.
In each AL round, we run the Gibbs sampler for
the (constrained) DPMM five times, using 100 it-
erations for burn-in, draw 20 samples from each
run with 5 iterations lag between samples and se-
lect the most uncertain link to be labeled. Fol-
lowing Navarro et al (2006), the concentration
parameter is inferred from the data using Gibbs
sampling. The performances were averaged across
the collected samples. Random selection was re-
peated three times. The three levels of the biomed-
ical gold standard were used independently and to-
gether with the general English dataset result in
four experimental setups.
The comparison between AL and random se-
lection for each dataset is shown in graphs 1(a)-
1(d) using V-beta, noting that the observations
made hold with all evaluation metrics used. Con-
straints selected via AL improve the performance
rapidly. Indicatively, the performance reached us-
ing 1000 randomly chosen constraints is obtained
using only 110 actively selected ones in the bio-50
dataset. AL performance levels out in later stages
with performance superior to the one achieved us-
ing random selection with the same number of
constraints. The poor performance of random se-
lection is expected, since the unsupervised DPMM
predicts more than 90% of the binary links cor-
rectly. Another interesting observation is that, dur-
ing AL, homogeneity increased faster than com-
pleteness (graphs 1(g) and 1(h)). This suggests
that the features used lead the model towards finer-
grained clusters, which is further confirmed by
the fact that the highest scores on the biomedical
dataset are achieved when comparing against the
finest-grained version of the gold standard. While
it is possible to choose constraints to the model
that would increase completeness with respect to
the gold standard, we argue that this would not al-
low us to obtain obtain insights on the model and
the features used.
We also noticed that the choice of batch size
has a significant effect on the learning rate of the
model. This phenomenon occurs in varying de-
grees in many applications of AL. Manual inspec-
tion of the links chosen at each round revealed that
batches often contained links involving the same
instances. This is expected due to transitivity: if
the link between instances A and B is uncertain
but the link between instances B and C is certain,
then the link between A and C will be uncertain
too. While reducing the batch size leads to bet-
ter learning rates, it requires estimating the model
more often. In order to ameliorate this issue, af-
ter obtaining the label of the most uncertain link,
we remove the samples that disagreed with it and
re-calculate the uncertainty of the remaining links
given the remaining samples. This is repeated un-
til the intended batch size is reached. Thus, we
59
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(a) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(b) bio-34
 0.72
 0.76
 0.8
 0.84
 0.88
 0  50  100  150  200  250
V-
be
ta
links
active
random
(c) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0  50  100  150  200  250
V-
be
ta
links
active
random
(d) gen. English
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(e) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(f) bio-34
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(g) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(h) gen. English
Figure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL. (e),(f):
Batch selection comparison. (g),(h): Homogeneity and completeness curves during AL.
avoid selecting links involving the same instance,
unless their uncertainty was not reduced by the
constraints added. A consideration that arises is
that by reducing the number of samples used for
uncertainty estimation, progressively we are left
with fewer samples to rank the remaining links.
Each labeled link reduces the number of samples
approximately by half since the most uncertain
link is likely to be a must-link in half the sam-
ples and a cannot-lnk in the remaining half. As
a result, for a batch with size |B| the uncertainty
of the last link will be estimated using |S|/2|B|?1
samples. A crude solution would be to generate
enough samples for the desired batch size. How-
ever, obtaining a very large number of samples can
be computationally expensive. Therefore, we set a
threshold for the minimum number of samples to
be used to estimate the link uncertainty and when
it is reached, more samples are generated using the
constraints selected. In graphs 1(e) and 1(f) we
demonstrate the effectiveness of the batch selec-
tion method proposed (labeled ?batch?) compared
to naive batch selection (labeled ?active10?).
6 Discussion and Future Work
We presented an AL method for constrained DP-
MMs employing uncertainty based sampling. We
applied it to two different verb clustering datasets
with 4 gold standards in total and obtained very
good results compared to random selection. The
idea, while explored in the context of verb cluster-
ing with the constrained DPMM, is likely to be ap-
plicable to other models that can incorporate must-
links and cannot-links in MCMC sampling.
Most literature on AL for NLP considers super-
vised methods for classification or sequential tag-
ging. However, AL for clustering is a relatively
under-explored area. Klein et al (2002) incorpo-
rated actively selected constraints in hierarchical
agglomerative clustering. Basu et al (2006) have
applied AL to obtain must-links and cannot-links
however, the clustering framework used requires
the number of clusters to be known in advance
which restricts counter-intuitively the clustering
solutions that are discovered. Moreover, semi-
supervised clustering is a form of semi-supervised
learning and in this light, our approach is related
to the work of Zhu et al (2003).
With respect to the practical application of the
AL method suggested, it is worth noting that in all
our experiments the constraints were obtained for
the respective gold standard of the dataset at ques-
tion and consequently they are all consistent with
each other. However, this assumption might not
hold in case human experts are employed for the
same purpose. In order to use such feedback in the
framework suggested, it is necessary to filter the
constraints provided in order to obtain a consistent
subset. To this end, it would be interesting to in-
vestigate the potential of using ?soft? constraints,
i.e. constraints that are provided with relative con-
fidence.
60
References
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond J. Mooney. 2006. Probabilis-
tic semi-supervised clustering with constraints. In
O. Chapelle, B. Schoelkopf, and A. Zien, edi-
tors, Semi-Supervised Learning, pages 73?102. MIT
Press.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Benjamin C. M. Fung, Ke Wang, and Martin Ester.
2003. Hierarchical document clustering using fre-
quent itemsets. In Proceedings of SIAM Interna-
tional Conference on Data Mining, pages 59?70.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307?314.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006. Automatic classification of verbs in
biomedical texts. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 345?352.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using Dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985?992, Sydney, Australia, July.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Combining Active Learning and Semi-
Supervised Learning Using Gaussian Fields and
Harmonic Functions. In ICML workshop on The
Continuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, pages 58?65.
61
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 56?63,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Combining Manual Rules and Supervised Learning for Hedge Cue and
Scope Detection
Marek Rei
Computer Laboratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
Hedge cues were detected using a super-
vised Conditional Random Field (CRF)
classifier exploiting features from the
RASP parser. The CRF?s predictions were
filtered using known cues and unseen in-
stances were removed, increasing preci-
sion while retaining recall. Rules for scope
detection, based on the grammatical re-
lations of the sentence and the part-of-
speech tag of the cue, were manually-
developed. However, another supervised
CRF classifier was used to refine these pre-
dictions. As a final step, scopes were con-
structed from the classifier output using a
small set of post-processing rules. Devel-
opment of the system revealed a number of
issues with the annotation scheme adopted
by the organisers.
1 Introduction
Speculative or, more generally, ?hedged? language
is a way of weakening the strength of a statement.
It is usually signalled by a word or phrase, called a
hedge cue, which weakens some clauses or propo-
sitions. These weakened portions of a sentence
form the scope of the hedge cues.
Hedging is an important tool in scientific lan-
guage allowing scientists to guide research be-
yond the evidence without overstating what fol-
lows from their work. Vincze et al (2008) show
that 19.44% of all sentences in the full papers
of the BioScope corpus contain hedge cues. De-
tecting these cues is potentially valuable for tasks
such as scientific information extraction or liter-
ature curation, as typically only definite informa-
tion should be extracted and curated. Most work
so far has been done on classifying entire text
sentences as hedged or not, but this risks los-
ing valuable information in (semi-)automated sys-
tems. More recent approaches attempt to find the
specific parts of a text sentence that are hedged.
Here we describe a system that is designed to
find hedge cues and their scopes in biomedical re-
search papers. It works in three stages:
1. Detecting the cues using a token-level super-
vised classifier.
2. Finding the scopes with a combination of
manual rules and a second supervised token-
level classifier.
3. Applying postprocessing rules to convert the
token-level annotation into predictions about
scope.
Parts of the system are similar to that of Morante
and Daelemans (2009) ? both make use of ma-
chine learning to tag tokens as being in a cue or
a scope. The most important differences are the
use of manually defined rules and the inclusion of
grammatical relations from a parser as critical fea-
tures.
2 Data
A revised version of the BioScope corpus (Vincze
et al, 2008), containing annotation of cues and
scopes, was provided as training data for the
CoNLL-2010 shared task (Farkas et al, 2010).
This includes 9 full papers and 1273 abstracts
from biomedical research fields. A separate new
set of full papers was released for evaluation as
part of the task. Table 1 contains an overview of
the training corpus statistics.
(1) provides an example sentence from the cor-
pus illustrating the annotation provided for train-
ing.
(2) shows the same sentence, representing cues
with angle brackets and scopes with round brack-
ets.
56
Papers Abstracts
Documents 9 1273
Sentences 2670 11871
Cues 682 2694
Scopes 668 2659
Unique cues 100 112
Cues with multiple words 10.70% 12.25%
Scopes start with cue 72.00% 80.59%
Scopes with multiple cues 2.10% 1.28%
Table 1: Training data statistics.
(1) <sentence id=?S1.166?>We <xcope
id=?X1.166.2?><cue ref=?X1.166.2?
type=?speculation?>expect</cue> that this cluster
<xcope id=?X1.166.1?><cue ref=?X1.166.1?
type=?speculation?>may</cue> represent a novel
selenoprotein
family</xcope></xcope>.</sentence>
(2) We (<expect> that this cluster (<may> represent a
novel selenoprotein family)).
There are a number of conditions on the anno-
tation that are imposed:
? Every cue has one scope.
? Every scope has one or more cues.
? The cue must be contained within its scope.
? Cues and scopes have to be continuous.
For development of the system, before the eval-
uation data were released, we used 60% of the
available corpus for training and 40% for testing.
The results we give below measure the system per-
formance on the evaluation data while using all
of the training data to build the supervised clas-
sifiers. The manually-developed rules are based
on the 60% of the development data we originally
reserved for training.
All of the training and test data sentences
were tokenised and parsed using the RASP sys-
tem (Briscoe et al, 2006). Multiple part-of-
speech (POS) tag outputs were passed to the parser
(to compensate for the high number of unseen
words in biomedical text), retaining just the high-
est ranked directed graph of grammatical relations
(GRs). Each node in the graph represents a word
token annotated with POS, lemma, and positional
order information. In the case of parse failure the
set of unconnected graphs returned by the highest-
ranked spanning subanalyses for each sentence
were retained.
3 Speculation cues
The hedge cues are found using a Conditional
Random Field (CRF) (Lafferty et al, 2001) classi-
fier, implemented using CRF++ 1. We chose the
CRF model because we framed the task as one
of token-level sequential tagging and CRFs are
known to achieve state-of-the-art performance on
related text classification tasks. Each word token
is assigned one of the following tags: F (first word
of a cue), I (inside a cue), L (last word of a cue),
O (outside, not a cue), hereafter referred to as the
FILO scheme.
The feature types used for classification are de-
fined in terms of the grammatical relations output
provided by the RASP system. We use binary fea-
tures that indicate whether a word token is a head
or a dependent in specific types of grammatical
relation (GR). This distinguishes between differ-
ent functions of the same word (when used as a
subject, object, modifier, etc.). These features are
combined with POS and lemma of the word to dis-
tinguish between uses of different cues and cue
types. We also utilise features for the lemma and
POS of the 3 words before and after the current
word.
The list of feature types for training the classi-
fier is:
? string
? lemma
? part-of-speech
? broad part-of-speech
? incoming GRs + POS
? outgoing GRs + POS
? incoming GRs + POS + lemma
? outgoing GRs + POS + lemma
? lemma + POS + POS of next word
? lemma + POS + POS of previous word
? 3 previous lemma + POS combinations
? 3 following lemma + POS combinations.
Outgoing GRs are grammatical relations where
the current word is the head, incoming GRs where
it is the dependent.
The predictions from the classifier are com-
pared to the list of known cues extracted from the
training data; the longest possible match is marked
as a cue. For example, the classifier could output
the following tag sequence:
(3) This[O] indicates[F] that[O] these[O] two[O]
lethal[O] mutations[O] . . .
1http://crfpp.sourceforge.net
57
indicates is classified as a cue but that is not.The list of known cues contains ?indicates that?which matches this sentence, therefore the systemprediction is:
(4) This <indicates that> these two lethal mutations . . .
Experiments in section 5.1 show that our sys-
tem is not good at finding previously unseen cues.
Lemma is the most important feature type for cue
detection and when it is not available, there is not
enough evidence to make good predictions. There-
fore, we compare all system predictions to the list
of known cues and if there is no match, they are
removed. The detection of unseen hedge cues is a
potential topic for future research.
4 Speculation scopes
We find a scope for each cue predicted in the pre-
vious step. Each word token in the sentence is
tagged with either F (first word of a scope), I (in-
side a scope), L (last word of a scope) or O (out-
side, not in a scope). Using our example sentence
(2) the correct tagging is:
expect may
We O O
expect F O
that I O
this I O
cluster I O
may I F
represent I I
a I I
novel I I
selenoprotein I I
family L L
. O O
Table 2: Example of scope tagging.
If a cue contains multiple words, they are each
processed separately and the predictions are later
combined by postprocessing rules.
As the first step, manually written rules are ap-
plied that find the scope based on GRs and POS
tags. We refine these predictions using a second
CRF classifier and further feature types extracted
from the RASP system output. Finally, postpro-
cessing rules are applied to convert the tagging se-
quence into scopes. By default, the minimal scope
returned is the cue itself.
4.1 Manual rules
Manual rules were constructed based on the de-
velopment data and annotation guidelines. In the
following rules and examples:
? ?below? refers to nodes that are in the sub-
graph of GRs rooted in the current node.
? ?parent? refers to the node that is the head
of the current node in the directed, connected
GR graph.
? ?before? and ?after? refer to word positions
in the text centered on the current node.
? ?mark everything below? means mark all
nodes in the subgraph as being in the scope
(i.e. tag as F/I/L as appropriate). However,
the traversal of the graph is terminated when
a text adjunct (TA) GR boundary or a word
POS-tagged as a clause separator is found,
since they often indicate the end of the scope.
The rules for finding the scope of a cue are trig-
gered based on the generalised POS tag of the cue:
? Auxiliary ? VM
Mark everything that is below the parent and
after the cue.
If the parent verb is passive, mark everything
below its subject (i.e. the dependent of the
subj GR) before the cue.
? Verb ? VV
Mark everything that is below the cue and af-
ter the cue.
If cue is appear or seem, mark everything be-
low subject before the cue.
If cue is passive, mark everything below sub-
ject before the cue.
? Adjective ? JJ
Find parent of cue. If there is no parent, the
cue is used instead.
Mark everything that is below the parent and
after the cue.
If parent is passive, mark everything below
subject before the cue.
If cue is (un)likely and the next word is to,
mark everything below subject before the
cue.
? Adverb ? RR
Mark everything that is below the parent and
after the cue.
58
? Noun ? NN
Find parent of cue. If there is no parent, the
cue is used instead.
Mark everything that is below the parent and
after the cue.
If parent is passive, mark everything below
subject before the cue.
? Conjunction ? CC
Mark everything below the conjunction.
If the cue is or and there is another cue either
before, combine them together.
? ?Whether? as a conjunction ? CSW
Mark everything that is below the cue and af-
ter the cue.
? Default ? anything else
Mark everything that is below the parent and
after the cue.
If parent verb is passive, mark everything be-
low subject before the cue.
Either . . . or . . . is a frequent exception contain-
ing two separate cues that form a single scope. An
additional rule combines these cues when they are
found in the same sentence.
The partial GR graph for (5) is given in Figure
1 (with positional numbering suppressed for read-
ability).
(5) Lobanov et al thus developed a sensitive search
method to deal with this problem, but they also
admitted that it (<would> fail to identify highly
unusual tRNAs).
Following the rules, would is identified as a cue
word with the part-of-speech VM; this triggers the
first rule in the list. The parent of would is fail
since they are connected with a GR where fail is
the head. Everything that is below fail in the GR
graph and positioned after would is marked as be-
ing in the scope. Since fail is not passive, the sub-
ject it is left out. The final scope returned by the
rule is then would fail to identify highly unusual
tRNAs.
4.2 Machine learning
The tagging sequence from the manual rules is
used as input to a second CRF classifier, along
with other feature types from RASP. The output
of the classifier is a modified sequence of FILO
tags.
The list of features for each token, used both
alone and as sequences of 5-grams before and after
the token, is:
Figure 1: Partial GR graph for sample sentence (5)
? tag from manual rules
? lemma
? POS
? is the token also the cue
? distance from the cue
? absolute distance from the cue
? relative position to the cue
? are there brackets between the word and the
cue
? is there any other punctuation between the
word and the cue
? are there any special (clause ending) words
between the word and cue
? is the word in the GR subtree of the cue
? is the word in the GR subtree of the main verb
? is the word in the GR subject subtree of the
main verb
Features of the current word, used in combina-
tion with the POS sequence of the cue:
? POS
? distance from the cue
? absolute distance from the cue
? relative position to the cue
? is the word in the GR subtree of the cue
? is the word in the GR subtree of the main verb
? is the word in the GR subject subtree of the
main verb
59
Additional features:
? GR paths between the word and the cue: full
path plus subpaths with up to 5 nodes
? GR paths in combination with the lemma se-
quence of the cue
The scope of the hedge cue can often be found
by tracking the sequence of grammatical relations
in the GR graph of a sentence, as described by
the manual rules. To allow the classifier to learn
such regularities, we introduce the concept of a
GR path.
Given that the sentence has a full parse and con-
nected graph, we can find the shortest connected
path between any two words. We take the con-
nected path between the word and the cue and con-
vert it into a string representation to use it as a fea-
ture value in the classifier. Path sections of differ-
ent lengths allow the system to find both general
and more specific patterns. POS tags are used as
node values to abstract away from word tokens.
An example for the word unusual, using the
graph from Figure 1, is given below. Five fea-
tures representing paths with increasing lengths
plus one feature containing the full path are ex-
tracted.
(6) 1: VM
2: VM<?aux?VV0
3: VM<?aux?VV0?xcomp?>VV0
4: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2
5: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?
ncmod?>JJ
6: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?
ncmod?>JJ
Line 1 shows the POS of the cue would (VM).
On line 2, this node is connected to fail (VV0) by
an auxiliary GR type. More links are added until
we reach unusual (JJ).
The presence of potential clause ending words,
used by Morante and Daelemans (2009), is in-
cluded as a feature type with values: whereas,
but, although, nevertheless, notwithstanding, how-
ever, consequently, hence, therefore, thus, instead,
otherwise, alternatively, furthermore, moreover,
since.
4.3 Post-processing
If the cue contains multiple words, the tag se-
quences have to be combined. This is done by
overlapping the sequences and choosing the pre-
ferred tag for each word, according to the hierar-
chy F > L > I > O.
Next, scopes are constructed from tag se-
quences using the following rules:
? Scope start point is the first token tagged as
F before the cue. If none are found, the first
word of the cue is used as the start point.
? Scope end point is the last token tagged as L
after the cue. If none are found, look for tags
I and F. If none are found, the last word of the
cue is used as end point.
The scopes are further modified until none of
the rules below return any updates:
? If the last token of the scope is punctuation,
move the endpoint before the token.
? If the last token is a closing bracket, move the
scope endpoint before the opening bracket.
? If the last token is a number and it is not pre-
ceded by a capitalised word (e.g. Table 16),
move the scope endpoint before the token.
This is a heuristic rule to handle trailing ci-
tations which are frequent in the training data
and often misattached by the parser.
Finally, scopes are checked for partial overlap
and any instances are corrected. For example, the
system might return a faulty version (7) of the sen-
tence (2) in which one scope is only partially con-
tained within the other.
(7) We [<expect> that this cluster (<may> represent a
novel] selenoprotein family).
This prediction cannot be represented within the
format specified for the shared task and we were
unable to find cases where such annotation would
be needed. These scopes are modified by moving
the end of the first scope to the end of the second
scope. The example above would become:
(8) We [<expect> that this cluster (<may> represent a
novel selenoprotein family)].
5 Results
5.1 Hedge cues
In evaluation a predicted cue is correct if it con-
tains the correct substring of the sentence. Token-
level evaluation would not give accurate results
because of varying tokenisation rules. A sentence
is classified as hedged if it contains one or more
cues.
60
The results below are obtained using the scorers
implemented by the organisers of the shared task.
As our baseline system, we use simple string
matching. The list of known cues is collected from
the training data and compared to the evaluation
sentences. The longest possible match is always
marked as a cue. ML1 to ML3 are variations of
the system described in section 3. All available
data, from papers and abstracts, were used to train
the CRF classifier. ML1 uses the results of the
classifier directly. The longest sequence of tokens
tagged as being part of a cue is used to form the fi-
nal prediction. ML2 incorporates the list of known
cues, constructing a cue over the longest sequence
of matching tokens where at least one token has
been tagged as belonging to a cue. ML3 uses the
list of known cues and also removes any predicted
cues not seen in the training data.
Baseline ML1 ML2 ML3
Total cues 1047 1047 1047 1047
Predicted cues 3062 995 1006 995
Correctly
predicted cues
1018 785 810 810
Cue precision 0.332 0.789 0.805 0.814
Cue recall 0.972 0.750 0.774 0.774
Cue F-measure 0.495 0.769 0.789 0.793
Sentence
precision
0.413 0.831 0.831 0.838
Sentence recall 0.995 0.843 0.843 0.842
Sentence
F-measure
0.584 0.837 0.837 0.840
Table 3: Cue detection results.
The baseline system returns nearly all cues but
since it matches every string, it also returns many
false positives, resulting in low precision. ML1
delivers more realistic predictions and increases
precision to 0.79. This illustrates how the use of a
word as a hedge cue depends on its context and not
only on the word itself. ML2 incorporates known
cues and increases both precision and recall. ML3
removes any unseen cue predictions further im-
proving precision. This shows the system is un-
able to accurately predict cues that have not been
included in the training data.
Table 4 lists the ten most common cues in the
test data and the number of cues found by the ML3
system.
In the cases of may and suggest, which are also
the most common cues in the development data,
the system finds all the correct instances. Can
and or are not detected as accurately because they
are both common words that in most cases are
TP FP Gold
may 161 5 161
suggest 124 0 124
can 2 1 61
or 9 12 52
indicate that 49 2 50
whether 42 6 42
might 42 1 42
could 30 17 41
would 37 14 37
appear 31 14 31
Table 4: True and false positives of the ten most
common cues in the evaluation data, using ML3
system.
not functioning as hedges. For example, there are
1215 occurrences of or in the training data and
only 146 of them are hedge cues; can is a cue in 64
out of 506 instances. We have not found any ex-
tractable features that reliably distinguish between
the different uses of these words.
5.2 Hedge scopes
A scope is counted as correct if it has the correct
beginning and end points in the sentence and is
associated with the correct cues. Scope prediction
systems take cues as input, therefore we present
two separate evaluations ? one with gold standard
cues and the other with cues predicted by the ML3
system from section 4.
The baseline system looks at each cue and
marks a scope from the beginning of the cue to the
end of the sentence, excluding the full stop. The
system using manual rules applies a rule for each
cue to find its scope, as described in section 4.1.
The POS tag of the cue is used to decide which
rule should be used and the GRs determine the
scope.
The final system uses the result from the manual
rules to derive features, adds various further fea-
tures from the parser and trains a CRF classifier to
refine the predictions.
We hypothesized that the speculative sentences
in abstracts may differ from the ones in full papers
and a 10-fold cross-validation of the development
data supported this intuition. Therefore, the orig-
inal system (CRF1) only used data from the full
papers to train the scope detection classifier. We
present here also the system trained on all of the
available data (CRF2).
Post-processing rules are applied equally to all
of these systems.
The baseline system performs remarkably well.
61
Baseline Manual
rules
Manual
rules +
CRF1
Manual
rules +
CRF2
Total scopes 1033 1033 1033 1033
Predicted 1047 1035 1035 1035
Correctly
predicted
596 661 686 683
Precision 0.569 0.639 0.663 0.660
Recall 0.577 0.640 0.664 0.661
F-measure 0.573 0.639 0.663 0.661
Table 5: Scope detection results using gold stan-
dard cues.
Baseline Manual
rules
Manual
rules +
CRF1
Manual
rules +
CRF2
Total scopes 1033 1033 1033 1033
Predicted 995 994 994 994
Correctly
predicted
507 532 564 567
Precision 0.510 0.535 0.567 0.570
Recall 0.491 0.515 0.546 0.549
F-measure 0.500 0.525 0.556 0.559
Table 6: Scope detection results using predicted
cues.
It does not use any grammatical or lexical know-
ledge apart from the cue and yet it delivers an F-
score of 0.50 with predicted and 0.57 with gold
standard cues.
Manual rules are essentially a more fine-grained
version of the baseline. Instead of a single rule,
one of 8 possible rules is selected based on the
POS tag of the cue. This improves the results,
increasing the F-score to 0.53 with predicted and
0.64 with gold standard cues. The improvement
suggests that the POS tag of a cue is a good indi-
cator of how it behaves in the sentence.
Error analysis showed that 35% of faulty scopes
were due to incorrect or unconnected GR graphs
output by the parser, and 65% due to exceptions
that the rules do not cover. An example of an ex-
ception, the braces { } showing the scopes pre-
dicted by the rules, is given in (9).
(9) Contamination is {(<probably> below 1%)}, which
is {(<likely> lower than the contamination rate of the
positive dataset) as discussed in 47}.
as discussed in 47 is a modifier of the clause
which is usually included in the scope but in this
case should be left out.
Finally, the last system combines features from
the rule-based system with features from RASP to
train a second classifier and improves our results
further, reaching 0.56 with predicted cues.
Inclusion of the abstracts as training data gave
a small improvement with predicted cues but not
with gold standard cues. It is part of future work
to determine if and how the use of hedges differs
across text sources.
6 Annotation scheme
During analysis of the data, several examples were
found that could not be correctly annotated due to
the restrictions of the markup. This leads us to
believe that the current rules for annotation might
not be best suited to handle complex constructions
containing hedged text.
Most importantly, the requirement for the hedge
scope to be continuous over the surface form of
text sentence does not work for some examples
drawn from the development data. In (10) below
it is uncertain whether fat body disintegration is
independent of the AdoR. In contrast, it is stated
with certainty that fat body disintegration is pro-
moted by action of the hemocytes, yet the latter
assertion is included in the scope to keep it contin-
uous.
(10) (The block of pupariation <appears> to involve
signaling through the adenosine receptor ( AdoR )) ,
but (fat body disintegration , which is promoted by
action of the hemocytes , <seems> to be independent
of the AdoR) .
Similarly, according to the guidelines, the sub-
ject of be likely should be included in its scope,
as shown in example (11). In sentence (12), how-
ever, the subject this phenomenon is separated by
two non-speculative clauses and is therefore left
out of the scope.
(11) Some predictors make use of the observation that
(neighboring genes whose relative location is
conserved across several prokaryotic organisms are
<likely> to interact).
(12) This phenomenon, which is independent of tumour
necrosis factor, is associated with HIV replication, and
(is thus <likely> to explain at least in part the
perpetuation of HIV infection in monocytes).
In (13), arguably, there is no hedging as the sen-
tence precisely describes a statistical technique for
predicting interaction given an assumption.
(13) More recently, other groups have come up with
sophisticated statistical methods to estimate
(<putatively> interacting domain pairs), based on the
(<assumption> of domain reusability).
Ultimately, dealing effectively with these and
related examples would involve representing
62
hedge scope in terms of sets of semantic proposi-
tions recovered from a logical semantic represen-
tation of the text, in which anaphora, word sense,
and entailments had been resolved.
7 Related work
Most of the previous work has been done on classi-
fying sentences as hedged or not, rather than find-
ing the scope of the hedge.
The first linguistically and computationally mo-
tivated study of hedging in biomedical texts is
Light et al (2004). They present an analysis of the
problem based on Medline abstracts and construct
an initial experiment for automated classification.
Medlock and Briscoe (2007) propose a weakly
supervised machine learning approach to the
hedge classification problem. They construct a
classifier with single words as features and use
a small amount of seed data to bootstrap the
system, achieving the precision/recall break-even
point (BEP) of 0.76. Szarvas (2008) extends this
work by introducing bigrams and trigrams as fea-
ture types, improving feature selection and us-
ing external data sources to construct lists of cue
words, achieving a BEP of 0.85.
Kilicoglu and Bergler (2008) apply a combina-
tion of lexical and syntactic methods, improving
on previous results and showing that quantifying
the strength of a hedge can be beneficial for clas-
sification of speculative sentences.
Vincze et al (2008) created a publicly available
annotated corpus of biomedical papers, abstracts
and clinical data called BioScope, parts of which
were also used as training data for the CoNLL10
shared task, building on the dataset and annota-
tion scheme used for evaluation by Medlock and
Briscoe (2007).
Morante and Daelemans (2009) use the Bio-
Scope corpus to approach the problem of identify-
ing cues and scopes via supervised machine learn-
ing. They train a selection of classifiers to tag each
word and combine the results with a final classi-
fier, finding 65.6% of the scopes in abstracts and
35.9% of the scopes in papers.
8 Conclusions
We have shown that the GRs output by the RASP
system can be effectively used as features for de-
tecting cues in a supervised classifier and also as
the basis for manual rules and features for scope
detection. We demonstrated that a small num-
ber of manual rules can provide competitive re-
sults, but that these can be further improved using
machine learning techniques and post-processing
rules. The generally low ceiling for the scope de-
tection results demonstrates the difficulty of both
annotating and detecting the hedge scopes in terms
of surface sentential forms.
Future work could usefully be directed at im-
proving performance on unseen cue detection and
on learning rules of the same form as those de-
veloped manually from annotated training data.
However, perhaps the most pressing issue is that of
establishing the best possible annotation and con-
sequent definition of the scope detection task.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 on Interactive
Presentation Sessions.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML 2001.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and
statements in between. In Proceedings of BioLink
2004.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of ACL 2007.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the Workshop on BioNLP.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL 2008.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11.
63
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 10?18,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Entailment Detection between Dependency Graph Fragments
Marek Rei
Computer Laboratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
Entailment detection systems are generally
designed to work either on single words, re-
lations or full sentences. We propose a new
task ? detecting entailment between depen-
dency graph fragments of any type ? which
relaxes these restrictions and leads to much
wider entailment discovery. An unsupervised
framework is described that uses intrinsic sim-
ilarity, multi-level extrinsic similarity and the
detection of negation and hedged language to
assign a confidence score to entailment rela-
tions between two fragments. The final system
achieves 84.1% average precision on a data set
of entailment examples from the biomedical
domain.
1 Introduction
Understanding that two different texts are semanti-
cally similar has benefits for nearly all NLP tasks,
including IR, IE, QA and Summarisation. Similar-
ity detection is usually performed either on single
words (synonymy) or full sentences and paragraphs
(paraphrasing). A symmetric similarity relation im-
plies that both elements can be interchanged (syn-
onymy and paraphrasing), while directional similar-
ity suggests that one fragment can be substituted for
the other but not the opposite (hyponymy and entail-
ment).
All of these language phenomena can be ex-
pressed using a single entailment relation. For para-
phrases and synonyms the relation holds in both di-
rections (observe? notice), whereas entailment and
hyponymy are modelled as a unidirectional relation
(overexpress ? express). Such relations, however,
can be defined between text fragments of any size
and shape, ranging from a single word to a complete
text segment. For example (argues against? does
not support; the protein has been implicated? the
protein has been shown to be involved).
We propose a new task ? detecting entailment
relations between any kinds of text fragments. A
unified approach is not expected to perform better
when compared to systems optimised only for a spe-
cific task (e.g. recognising entailment between sen-
tences), but constructing a common theory to cover
all text fragments has important benefits. A broader
approach will allow for entailment discovery among
a much wider range of fragment types for which no
specialised systems exist. In addition, entailment re-
lations can be found between different types of frag-
ments (e.g. a predicate entailing an adjunct). Finally,
a common system is much easier to develop and in-
tegrate with potential applications compared to hav-
ing a separate system for each type of fragment.
In this paper we present a unified framework that
can be used to detect entailment relations between
fragments of various types and sizes. The system
is designed to work with anything that can be rep-
resented as a dependency graph, including single
words, constituents of various sizes, text adjuncts,
predicates, relations and full sentences. The ap-
proach is completely unsupervised and requires only
a large plain text corpus to gather information for
calculating distributional similarity. This makes it
ideal for the biomedical domain where the availabil-
ity of annotated training data is limited. We ap-
ply these methods by using a background corpus
10
of biomedical papers and evaluate on a manually
constructed dataset of entailing fragment pairs, ex-
tracted from biomedical texts.
2 Applications
Entailment detection between fragments is a vital
step towards entailment generation ? given text T ,
the system will have to generate all texts that ei-
ther entail T or are entailed by T . This is motivated
by applications in Relation Extraction, Information
Retrieval and Information Extraction. For example,
if we wish to find all genes that are synthesised in
the larval tissue, the following IE query can be con-
structed (with x and y marking unknown variables):
(1) x is synthesised in the larval tissue
Known entailment relations can be used to mod-
ify the query: (overexpression? synthesis), (larval
fat body ? larval tissue) and (the synthesis of x in
y ? x is synthesised in y). Pattern (2) entails pat-
tern (1) and would also return results matching the
information need.
(2) the overexpression of x in the larval fat body
A system for entailment detection can automati-
cally extract a database of entailing fragments from
a large corpus and use them to modify any query
given by the user. Recent studies have also inves-
tigated how complex sentence-level entailment re-
lations can be broken down into smaller consecu-
tive steps involving fragment-level entailment (Sam-
mons et al, 2010; Bentivogli et al, 2010). For ex-
ample:
(3) Text: The mitogenic effects of the B beta chain of
fibrinogen are mediated through cell surface
calreticulin.
Hypothesis: Fibrinogen beta chain interacts with
CRP55.
To recognise that the hypothesis is entailed by the
text, it can be decomposed into five separate steps
involving text fragments:
1. B beta chain of fibrinogen? Fibrinogen beta chain
2. calreticulin? CRP55
3. the mitogenic effects of x are mediated through y?
y mediates the mitogenic effects of x
4. y mediates the mitogenic effects of x ? y interacts
with x
5. y interacts with x? x interacts with y
This illustrates how entailment detection between
various smaller fragments can be used to construct
an entailment decision between more complicated
sentences. However, only the presence of these con-
structions has been investigated and, to the best of
our knowledge, no models currently exist for auto-
mated detection and composition of such entailment
relations.
3 Modelling entailment between graph
fragments
In order to cover a wide variety of language phe-
nomena, a fragment is defined in the following way:
Definition 1. A fragment is any connected subgraph
of a directed dependency graph containing one or
more words and the grammatical relations between
them.
This definition is intended to allow extraction of
a wide variety of fragments from a dependency tree
or graph representation of sentences found using any
appropriate parser capable of returning such output
(e.g. Ku?bler et al, 2009). The definition covers
single- or multi-word constituents functioning as de-
pendents (e.g. sites, putative binding sites), text ad-
juncts (in the cell wall), single- or multi-word pred-
icates (* binds to receptors in the airways) and rela-
tions (* binds and activates *) including ones with
?internal? dependent slots (* inhibits * at *), some of
which may be fixed in the fragment (* induces au-
tophosphorylation of * in * cells), and also full sen-
tences1. An example dependency graph and some
selected fragments can be seen in Figure 1.
Our aim is to detect semantically similar frag-
ments which can be substituted for each other in text,
resulting in more general or more specific versions
of the same proposition. This kind of similarity can
be thought of as an entailment relation and we define
entailment between two fragments as follows:
1The asterisks (*) are used to indicate missing dependents
in order to increase the readability of the fragment when repre-
sented textually. The actual fragments are kept in graph form
and have no need for them.
11
induce inB61recombinant
autophosphorylation of ECK
cell intactmod mod
subj iobj
iobjdobj dobjdobj
Figure 1: Dependency graph for the sentence: Recombinant B61 induces autophosphorylation of ECK in intact cells.
Some interesting fragments are marked by dotted lines.
Definition 2. Fragment A entails fragment B (A?
B) if A can be replaced by B in sentence S and the re-
sulting sentence S? can be entailed from the original
one (S? S?).
This also requires estimating entailment relations
between sentences, for which we use the definition
established by Bar-Haim et al (2006):
Definition 3. Text T entails hypothesis H (T ? H)
if, typically, a human reading T would infer that H
is most likely true.
We model the semantic similarity of fragments as
a combination of two separate directional similarity
scores:
1. Intrinsic similarity: how similar are the com-
ponents of the fragments.
2. Extrinsic similarity: how similar are the con-
texts of the fragments.
To find the overall score, these two similarity
measures are combined linearly using a weighting
parameter ?:
Score(A? B) = ?? IntSim(A? B)
+(1? ?)? ExtSim(A? B)
In this paper f(A ? B) designates an asym-
metric function between A and B. When referring
only to single words, lowercase letters (a,b) are used;
when referring to fragments of any size, including
single words, then uppercase letters are used (A,B).
Score(A? B) is the confidence score that frag-
ment A entails fragment B ? higher score indi-
cates higher confidence and 0 means no entailment.
IntSim(A? B) is the intrinsic similarity between
two fragments. It can be any function that compares
them, for example by matching the structure of one
fragment to another, and outputs a similarity score in
the range of [0, 1]. ExtSim(A ? B) is a measure
of extrinsic similarity that compares the contexts of
the two fragments. ? is set to 0.5 for an unsuper-
vised approach but the effects of tuning this param-
eter are further analysed in Section 5.
The directional similarity score is first found be-
tween words in each fragment, which are then used
to calculate the score between the two fragments.
3.1 Intrinsic similarity
IntSim(A? B) is the intrinsic similarity between
the two words or fragments. In order to best capture
entailment, the measure should be non-symmetrical.
We use the following simple formula for word-level
score calculation:
IntSim(a? b) =
length(c)
length(b)
where c is the longest common substring for a and
b. This measure will show the ratio of b that is also
contained in a. For example:
IntSim(overexpress? expression) = 0.70
IntSim(expression? overexpress) = 0.64
The intrinsic similarity function for fragments is
defined using an injective function between compo-
nents of A and components of B:
IntSim(A? B) =
Mapping(A? B)
|B|
where Mapping(A ? B) is a function that goes
through all the possible word pairs {(a, b)|a ?
A, b ? B} and at each iteration connects the one
12
with the highest entailment score, returning the sum
of those scores. Figure 2 contains pseudocode
for the mapping process. Dividing the value of
Mapping(A ? B) by the number of components
in B gives an asymmetric score that indicates how
well B is covered by A. It returns a lower score
if B contains more elements than A as some words
cannot be matched to anything. While there are ex-
ceptions, it is common that if B is larger than A,
then it cannot be entailed by A as it contains more
information.
while unused elements in A and B do
bestScore = 0
for a ? A, b ? B, a and b are unused do
if Score(a? b) > bestScore then
bestScore = Score(a? b)
end if
end for
total+ = bestScore
end while
return total
Figure 2: Pseudocode for mapping between two frag-
ments
The word-level entailment score Score(a ? b)
is directly used to estimate the entailment score be-
tween fragments, Score(A ? B). In this case we
are working with two levels ? fragments which in
turn consist of words. However, this can be extended
to a truly recursive method where fragments consist
of smaller fragments.
3.2 Extrinsic similarity
The extrinsic similarity between two fragments or
words is modelled using measures of directional dis-
tributional similarity. We define a context relation as
a tuple (a, d, r, a?) where a is the main word, a? is a
word connected to it through a dependency relation,
r is the label of that relation and d shows the direc-
tion of the relation. The tuple f : (d, r, a?) is referred
to as a feature of a.
To calculate the distributional similarity between
two fragments, we adopt an approach similar to
Weeds et al (2005). Using the previous notation,
(d, r, a?) is a feature of fragment A if (d, r, a?) is a
feature for a word which is contained inA. The gen-
eral algorithm for feature collection is as follows:
1. Find the next instance of a fragment in the
background corpus.
2. For each word in the fragment, find dependency
relations which connect to words not contained
in the fragment.
3. Count these dependency relations as distribu-
tional features for the fragment.
For example, in Figure 1 the fragment (* induces
* in *) has three features: (1, subj, B61), (1, dobj,
autophosphorylation) and (1, dobj, cell).
The BioMed Central2 corpus of full papers was
used to collect distributional similarity features for
each fragment. 1000 papers were randomly selected
and separated for constructing the test set, leaving
70821 biomedical full papers. These were tokenised
and parsed using the RASP system (Briscoe et al,
2006) in order to extract dependency relations.
We experimented with various schemes for fea-
ture weighting and found the best one to be a varia-
tion of Dice?s coefficient (Dice, 1945), described by
Curran (2003):
wA(f) =
2P (A, f)
P (A, ?) + P (?, f)
where wA(f) is the weight of feature f for fragment
A, P (?, f) is the probability of the feature appear-
ing in the corpus with any fragment, P (A, ?) is the
probability of the fragment appearing with any fea-
ture, and P (A, f) is the probability of the fragment
and the feature appearing together.
Different measures of distributional similarity,
both symmetrical and directonal, were also tested
and ClarkeDE (Clarke, 2009) was used for the fi-
nal system as it achieved the highest performance on
graph fragments:
ClarkeDE(A? B) =
?
f?FA?FB
min(wA(f), wB(f))
?
f?FA
wA(f)
where FA is the set of features for fragmentA and
wA(f) is the weight of feature f for fragment A. It
quantifies the weighted coverage of the features ofA
by the features of B by finding the sum of minimum
weights.
2http://www.biomedcentral.com/info/about/datamining/
13
The ClarkeDE similarity measure is designed to
detect whether the features of A are a proper subset
of the features of B. This works well for finding
more general versions of fragments, but not when
comparing fragments which are roughly equal para-
phrases. As a solution we constructed a new mea-
sure based on the symmetrical Lin measure (Lin,
1998).
LinD(A? B) =
?
f?FA?FB
[wA(f) + wB(f)]
?
f?FA
wA(f) +
?
f?FA?FB
wB(f)
Compared to the original, the features ofB which
are not found in A are excluded from the score
calculation, making the score non-symmetrical but
more balanced compared to ClarkeDE. We ap-
plied this for word-level distributional similarity and
achieved better results than with other common sim-
ilarity measures.
The LinD similarity is also calculated between
fragment levels to help detect possible paraphrases.
If the similarity is very high (greater than 85%), then
a symmetric relationship between the fragments is
assumed and the value of LinD is used as ExtSim.
Otherwise, the system reverts to the ClarkeDE
measure for handling unidirectional relations.
3.3 Hedging and negation
Language constructions such as hedging and nega-
tion typically invert the normal direction of an en-
tailment relation. For example, (biological discov-
ery? discovery) becomes (no biological discovery
? no discovery) and (is repressed by? is affected
by) becomes (may be repressed by? is affected by).
Such cases are handled by inverting the direction
of the score calculation if a fragment is found to
contain a special cue word that commonly indicates
hedged language or negation. In order to find the
list of indicative hedge cues, we analysed the train-
ing corpus of CoNLL 2010 Shared Task (Farkas et
al., 2010) which is annotated for speculation cues
and scopes. Any cues that occurred less than 5 times
or occurred more often as normal text than as cue
words were filtered out, resulting in the following
list:
(4) suggest, may, might, indicate that, appear,
likely, could, possible, whether, would, think,
seem, probably, assume, putative, unclear,
propose, imply, possibly
For negation cues we used the list collected by
Morante (2009):
(5) absence, absent, cannot, could not, either,
except, exclude, fail, failure, favor over,
impossible, instead of, lack, loss, miss,
negative, neither, nor, never, no, no longer,
none, not, rather than, rule out, unable, with
the exception of, without
This is a fast and basic method for estimating
the presence of hedging and negation in a fragment.
When dealing with longer texts, the exact scope of
the cue word should be detected, but for relatively
short fragments the presence of a keyword acts as a
good indication of hedging and negation.
4 Evaluation
A ?pilot? dataset was created to evaluate different
entailment detection methods between fragments3.
In order to look for valid entailment examples, 1000
biomedical papers from the BioMed Central full-text
corpus were randomly chosen and analysed. We
hypothesised that two very similar sentences orig-
inating from the same paper are likely to be more
and less general versions of the same proposition.
First, the similarities between all sentences in a sin-
gle paper were calculated using a bag-of-words ap-
proach. Then, ten of the most similar but non-
identical sentence pairs from each paper were pre-
sented for manual review and 150 fragment pairs
were created based on these sentences, 100 of which
were selected for the final set.
When applied to sentence-level entailment extrac-
tion, similar methods can suffer from high lexical
overlap as sentences need to contain many match-
ing words to pass the initial filter. However, for the
extraction of entailing fragments most of the match-
ing words are discarded and only the non-identical
fragments are stored, greatly reducing the overlap
problem. Experiments in Section 5 demonstrate
that a simple bag-of-words approach performs rather
poorly on the task, confirming that the extraction
method produces a diverse selection of fragments.
3http://www.cl.cam.ac.uk/~mr472/entailment/
14
Two annotators assigned a relation type to can-
didate pairs based on how well one fragment can
be substituted for the other in text while preserving
meaning (A ? B, A ? B, A ? B or A 6= B).
Cohen?s Kappa between the annotators was 0.88, in-
dicating very high agreement. Instances with dis-
agreement were then reviewed and replaced for the
final dataset.
Each fragment pair has two binary entailment de-
cisions (one in either direction) and the set is evenly
balanced, containing 100 entailment and 100 non-
entailment relations. An example sentence with the
first fragment is also included. Fragment sizes range
from 1 to 20 words, with the average of 2.86 words.
The system assigns a score to each entailment re-
lation, with higher values indicating higher confi-
dence in entailment. All the relations are ranked
based on their score, and average precision (AP) is
used to evaluate the performance:
AP =
1
R
N?
i=1
E(i)? CorrectUpTo(i)
i
where R is the number of correct entailment re-
lations, N is the number of possible relations in
the test set, E(i) is 1 if the i-th relation is en-
tailment in the gold standard and 0 otherwise, and
CorrectUpTo(i) is the number of correctly re-
turned entailment relations up to rank i. Average
precision assigns a higher score to systems which
rank correct entailment examples higher in the list.
As a secondary measure we also report the Break-
Even Point (BEP) which is defined as precision at
the rank where precision is equal to recall. Using
the previous annotation, this can also be calculated
as precision at rank R:
BEP =
CorrectUpTo(R)
R
BEP is a much more strict measure, treating the task
as binary classification and ignoring changes to the
ranks within the classes.
5 Results
The test set is balanced, therefore random guessing
would be expected to achieve an AP and BEP of
0.5 which can be regarded as the simplest (random)
baseline. Table 1 contains results for two more basic
approaches to the task. For the bag-of-words (BOW)
system, the score of A entailing B is the proportion
of words in B that are also contained in A.
Scorebow(A? B) =
|{b|b ? A,B}|
|{b|b ? B}|
We also tested entailment detection when using
only the directional distributional similarity between
fragments as it is commonly done for words. While
both of the systems perform better than random, the
results are much lower than those for more informed
methods. This indicates that even though there is
some lexical overlap between the fragments, it is not
enough to make good decisions about the entailment
relations.
System type AP BEP
Random baseline 0.500 0.500
BOW 0.657 0.610
Distributional similarity 0.645 0.480
Table 1: Results for basic approaches.
Table 2 contains the results for the system de-
scribed in Section 3. We start with the most basic
version and gradually add components. Using only
the intrinsic similarity, the system performs better
than any of the basic approaches, delivering 0.71 AP.
System type AP BEP
Intrinsic similarity only 0.710 0.680
+ Word ExtSim 0.754 0.710
+ Fragment ExtSim 0.801 0.710
+ Negation & hedging 0.831 0.720
+ Paraphrase check 0.841 0.720
Table 2: Results for the system described in Section 3.
Components are added incrementally.
Next, the extrinsic similarity between words is in-
cluded, raising the AP to 0.754. When the string-
level similarity fails, the added directional distri-
butional similarity helps in mapping semantically
equivalent words to each other.
The inclusion of extrinsic similarity between frag-
ments gives a further increase, with AP of 0.801.
The 4.5% increase shows that while fragments are
15
larger and occur less often in a corpus, their distribu-
tional similarity can still be used as a valuable com-
ponent to detect semantic similarity and entailment.
Checking for negation and hedge cues raises the
AP to 0.831. The performance is already high and
a 3% improvement shows that hedging and negation
affect fragment-level entailment and other compo-
nents do not manage to successfully capture this in-
formation.
Finally, applying the fragment-level check for
paraphrases with a more appropriate distributional
similarity measure, as described in Section 3.2, re-
turns an AP of 0.841. The results of this final con-
figuration are significantly different compared to the
initial system using only intrinsic similarity, accord-
ing to the Wilcoxon signed rank test at the level of
0.05.
The formula in Section 3 contains parameter ?
which can be tuned to adjust the balance of intrinsic
and extrinsic similarity. This can be done heuristi-
cally or through machine learning methods and dif-
ferent values can be used for fragments and words.
In order to investigate the effects of tuning on the
system, we tried all possible combinations of ?word
and ?fragment with values between 0 and 1 at incre-
ments of 0.05. Table 3 contains results for some of
these experiments.
?word ?fragment AP BEP
0.5 0.5 0.841 0.720
* 0.0 0.656 0.480
0.0 1.0 0.813 0.720
1.0 1.0 0.765 0.690
0.45 0.65 0.847 0.740
Table 3: Results of tuning the weights for intrinsic and
distributional similarity.
The best results were obtained with ?word = 0.45
and ?fragment = 0.65, resulting in 0.847 AP and
0.74 BEP. This shows that parameter tuning can im-
prove the results, but the 0.6% increase is modest
and a completely unsupervised approach can give
competitive results. In addition, the optimal values
of ? are close to 0.5, indicating that all four com-
ponents (intrinsic and distributional similarities be-
tween words and fragments) are all contributing to
the performance of the final system.
6 Previous work
Most work on entailment has focused on compar-
ing sentences or paragraphs. For example, Haghighi
et al (2005) represent sentences as directed depen-
dency graphs and use graph matching to measure se-
mantic overlap. This method also compares the de-
pendencies when calculating similarity, which sup-
ports incorporation of extra syntactic information.
Hickl et al (2006) combine lexico-syntactic features
and automatically acquired paraphrases to classify
entailing sentences. Lintean and Rus (2009) make
use of weighted dependencies and word semantics
to detect paraphrases. In addition to similarity they
look at dissimilarity between two sentences and use
their ratio as the confidence score for paraphrasing.
Lin and Pantel (2001) were one of the first to
extend the distributional hypothesis to dependency
paths to detect entailment between relations. Szpek-
tor et al (2004) describe the TEASE method for ex-
tracting entailing relation templates from the Web.
Szpektor and Dagan (2008) use the distributional
similarity of arguments to detect unary template en-
tailment, whilst Berant et al (2010) apply it to bi-
nary relations in focused entailment graphs.
Snow et al (2005) described a basic method of
syntactic pattern matching to automatically discover
word-level hypernym relations from text. The use of
directional distributional similarity measures to find
inference relations between single words is explored
by Kotlerman et al (2010). They propose new mea-
sures based on feature ranks and compare them with
existing ones for the tasks of lexical expansion and
text categorisation.
In contrast to current work, each of the ap-
proaches described above only focuses on detecting
entailment between specific subtypes of fragments
(either sentences, relations or words) and optimis-
ing the system for a single scenario. This means
only limited types of entailment relations are found
and they cannot be used for entailment generation
or compositional entailment detection as described
in Section 2.
MacCartney and Manning (2008) approach
sentence-level entailment detection by breaking the
problem into a sequence of atomic edits linking the
premise to the hypothesis. Entailment relations are
then predicted for each edit, propagated up through
16
a syntax tree and then used to compose the result-
ing entailment decision. However, their system fo-
cuses more on natural logic and uses a predefined set
of compositional rules to capture a subset of valid
inferences with high precision but low recall. It
also relies on a supervised classifier and information
from WordNet to reach the final entailment decision.
Androutsopoulos and Malakasiotis (2010) have
assembled a survey of different tasks and approaches
related to paraphrasing and entailment. They de-
scribe three different goals (paraphrase recogni-
tion, generation and extraction) and analyse various
methods for solving them.
7 Conclusion
Entailment detection systems are generally devel-
oped to work on specific text units ? either single
words, relations, or full sentences. While this re-
duces the complexity of the problem, it can also
lead to important information being disregarded. In
this paper we proposed a new task ? detecting en-
tailment relations between any kind of dependency
graph fragments. The definition of a fragment cov-
ers the language structures mentioned above and
also extends to others that have not been fully in-
vestigated in the context of entailment recognition
(such as multi-word constituents, predicates and ad-
juncts).
To perform entailment detection between various
types of dependency graph fragments, a new sys-
tem was built that combines the directional intrin-
sic and extrinsic similarities of two fragments to
reach a final score. Fragments which contain hedg-
ing or negation are identified and their score cal-
culation is inverted to better model the effect on
entailment. The extrinsic similarity is found with
two different distributional similarity measures, first
checking for symmetric similarity and then for di-
rectional containment of distributional features. The
system was evaluated on a manually constructed
dataset of fragment-level entailment relations from
the biomedical domain and each of the added meth-
ods improved the results.
Traditionally, the method for entailment recogni-
tion is chosen based on what appears optimal for
the task ? either structure matching or distributional
similarity. Our experiments show that the combina-
tion of both gives the best performance for all frag-
ment types. It is to be expected that single words will
benefit more from distributional measures while full
sentences get matched by their components. How-
ever, this separation is not strict and evidence from
both methods can be used to strengthen the decision.
The experiments confirmed that entailment be-
tween dependency graph fragments of various types
can be detected in a completely unsupervised set-
ting, without the need for specific resources or an-
notated training data. As our method can be applied
equally to any domain and requires only a large plain
text corpus, we believe it is a promising direction
for research in entailment detection. This can lead
to useful applications in biomedical information ex-
traction where manually annotated datasets are in
short supply.
We have shown that a unified approach can be
used to detect entailment relations between depen-
dency graph fragments. This allows for entail-
ment discovery among a wide range of fragment
types, including ones for which no specialised sys-
tems currently exist. The framework for fragment-
level entailment detection can be integrated into var-
ious applications that require identifying and rewrit-
ing semantically equivalent phrases - for example,
query expansion in IE and IR, text mining, sentence-
level entailment composition, relation extraction and
protein-protein interaction extraction.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38(7):135?187.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9. Citeseer.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10).
17
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, number Section
6, pages 1220?1229. Association for Computational
Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77?80, Sydney, Aus-
tralia. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, number March, pages 112?119. As-
sociation for Computational Linguistics.
James Richard Curran. 2003. From distributional to se-
mantic similarity. Ph.D. thesis, University of Edin-
burgh.
Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja`nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning ? Shared Task, pages 1?12. As-
sociation for Computational Linguistics.
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, Morristown, NJ, USA.
Association for Computational Linguistics.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In Proceedings of the Second PASCAL Chal-
lenges Workshop.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on Hu-
man Language Technologies, 2:1?127.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(04):343?360.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th in-
ternational conference on Computational linguistics-
Volume 2, pages 768?774. Association for Computa-
tional Linguistics.
Mihain C. Lintean and Vasile Rus. 2009. Para-
phrase Identification Using Weighted Dependencies
and Word Semantics. In Proceedings of the FLAIRS-
22, volume 22, pages 19?28.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in natu-
ral language inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, pages 521?528. Association for Computa-
tional Linguistics.
Roser Morante. 2009. Descriptive analysis of negation
cues in biomedical texts. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC10), pages 1429?1436.
Mark Sammons, V.G. Vinod Vydiswaran, and Dan Roth.
2010. Ask not what textual entailment can do for
you... In Proceedings of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
1199?1208. Association for Computational Linguis-
tics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics - COLING ?08, pages 849?856, Morristown,
NJ, USA. Association for Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, vol-
ume 4, pages 41?48.
Julie Weeds, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 7?12, Morristown,
NJ, USA. Association for Computational Linguistics.
18
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 35?43,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Automating Second Language Acquisition Research:
Integrating Information Visualisation and Machine Learning
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Theodora Alexopoulou
DTAL
University of Cambridge
United Kingdom
ta259@cam.ac.uk
Abstract
We demonstrate how data-driven ap-
proaches to learner corpora can support
Second Language Acquisition research
when integrated with visualisation tools.
We present a visual user interface support-
ing the investigation of a set of linguistic
features discriminating between pass and
fail ?English as a Second or Other Lan-
guage? exam scripts. The system displays
directed graphs to model interactions
between features and supports exploratory
search over a set of learner scripts. We
illustrate how the interface can support
the investigation of the co-occurrence
of many individual features, and discuss
how such investigations can shed light on
understanding the linguistic abilities that
characterise different levels of attainment
and, more generally, developmental aspects
of learner grammars.
1 Introduction
The Common European Framework of Reference
for Languages (CEFR)1 is an international bench-
mark of language attainment at different stages of
learning. The English Profile (EP)2 research pro-
gramme aims to enhance the learning, teaching
and assessment of English as an additional lan-
guage by creating detailed reference level descrip-
tions of the language abilities expected at each
level. As part of our research within that frame-
work, we modify and combine techniques devel-
oped for information visualisation with method-
ologies from computational linguistics to support
a novel and more empirical perspective on CEFR
1http://www.coe.int/t/dg4/linguistic/cadre en.asp
2http://www.englishprofile.org/
levels. In particular, we build a visual user in-
terface (hereafter UI) which aids the develop-
ment of hypotheses about learner grammars us-
ing graphs of linguistic features discriminating
pass/fail exam scripts for intermediate English.
Briscoe et al (2010) use supervised discrimi-
native machine learning methods to automate the
assessment of ?English as a Second or Other Lan-
guage? (ESOL) exam scripts, and in particular, the
First Certificate in English (FCE) exam, which
assesses English at an upper-intermediate level
(CEFR level B2). They use a binary discrimina-
tive classifier to learn a linear threshold function
that best discriminates passing from failing FCE
scripts, and predict whether a script can be clas-
sified as such. To facilitate learning of the clas-
sification function, the data should be represented
appropriately with the most relevant set of (lin-
guistic) features. They found a discriminative fea-
ture set includes, among other feature types, lexi-
cal and part-of-speech (POS) ngrams. We extract
the discriminative instances of these two feature
types and focus on their linguistic analysis3. Ta-
ble 1 presents a small subset ordered by discrimi-
native weight.
The investigation of discriminative features can
offer insights into assessment and into the linguis-
tic properties characterising the relevant CEFR
level. However, the amount and variety of data
potentially made available by the classifier is con-
siderable, as it typically finds hundreds of thou-
sands of discriminative feature instances. Even
if investigation is restricted to the most discrim-
inative ones, calculations of relationships be-
3Briscoe et al (2010) POS tagged and parsed the data
using the RASP toolkit (Briscoe et al, 2006). POS tags are
based on the CLAWS tagset.
35
tween features can rapidly grow and become over-
whelming. Discriminative features typically cap-
ture relatively low-level, specific and local prop-
erties of texts, so features need to be linked to the
scripts they appear in to allow investigation of the
contexts in which they occur. The scripts, in turn,
need to be searched for further linguistic prop-
erties in order to formulate and evaluate higher-
level, more general and comprehensible hypothe-
ses which can inform reference level descriptions
and understanding of learner grammars.
The appeal of information visualisation is to
gain a deeper understanding of important phe-
nomena that are represented in a database (Card et
al., 1999) by making it possible to navigate large
amounts of data for formulating and testing hy-
potheses faster, intuitively, and with relative ease.
An important challenge is to identify and assess
the usefulness of the enormous number of pro-
jections that can potentially be visualised. Explo-
ration of (large) databases can lead quickly to nu-
merous possible research directions; lack of good
tools often slows down the process of identifying
the most productive paths to pursue.
In our context, we require a tool that visu-
alises features flexibly, supports interactive inves-
tigation of scripts instantiating them, and allows
statistics about scripts, such as the co-occurrence
of features or presence of other linguistic proper-
ties, to be derived quickly. One of the advantages
of using visualisation techniques over command-
line database search tools is that Second Lan-
guage Acquisition (SLA) researchers and related
users, such as assessors and teachers, can access
scripts, associated features and annotation intu-
itively without the need to learn query language
syntax.
We modify previously-developed visualisation
techniques (Di Battista et al, 1999) and build a
visual UI supporting hypothesis formation about
learner grammars. Features are grouped in terms
of their co-occurrence in the corpus and directed
graphs are used in order to illustrate their rela-
tionships. Selection of different feature combi-
nations automatically generates queries over the
data and returns the relevant scripts as well as as-
sociations with meta-data and different types of
errors committed by the learners4. In the next sec-
4Our interface integrates a command-line Lucene search
tool (Gospodnetic and Hatcher, 2004) developed by Gram
and Buttery (2009).
Feature Example
VM RR (POS bigram: +) could clearly
, because (word bigram: ?) , because of
necessary (word unigram: +) it is necessary that
the people (word bigram: ?) *the people are clever
VV? VV? (POS bigram: ?) *we go see film
NN2 VVG (POS bigram: +) children smiling
Table 1: Subset of features ordered by discriminative
weight; + and ? show their association with either
passing or failing scripts.
tions we describe in detail the visualiser, illustrate
how it can support the investigation of individual
features, and discuss how such investigations can
shed light on the relationships between features
and developmental aspects of learner grammars.
To the best of our knowledge, this is the first
attempt to visually analyse as well as perform
a linguistic interpretation of discriminative fea-
tures that characterise learner English. We also
apply our visualiser to a set of 1,244 publically-
available FCE ESOL texts (Yannakoudakis et al,
2011) and make it available as a web service to
other researchers5.
2 Dataset
We use texts produced by candidates taking the
FCE exam, which assesses English at an upper-
intermediate level. The FCE texts, which are
part of the Cambridge Learner Corpus6, are pro-
duced by English language learners from around
the world sitting Cambridge Assessment?s ESOL
examinations7. The texts are manually tagged
with information about linguistic errors (Nicholls,
2003) and linked to meta-data about the learners
(e.g., age and native language) and the exam (e.g.,
grade).
3 The English Profile visualiser
3.1 Basic structure and front-end
The English Profile (EP) visualiser is developed
in Java and uses the Prefuse library (Heer et
al., 2005) for the visual components. Figure 1
shows its front-end. Features are represented
5Available by request: http://ilexir.co.uk/applications/ep-
visualiser/
6http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/
custom/item3646603/
7http://www.cambridgeesol.org/
36
Figure 1: Front-end of the EP visualiser.
by a labelled node and displayed in the central
panel; positive features (i.e., those associated with
passing the exam) are shaded in a light green
colour while negative ones are light red8. A field
at the bottom right supports searching for fea-
tures/nodes that start with specified characters and
highlighting them in blue. An important aspect is
the display of feature patterns, discussed in more
detail in the next section (3.2).
3.2 Feature relations
Crucial to understanding discriminative features
is finding the relationships that hold between
them. We calculate co-occurrences of features at
the sentence-level in order to extract ?meaningful?
relations and possible patterns of use. Combi-
nations of features that may be ?useful? are kept
while the rest are discarded. ?Usefulness? is mea-
sured as follows:
Consider the set of all the sentences in the cor-
pus S = {s1, s2, ..., sN} and the set of all the fea-
tures F = {f1, f2, ..., fM}. A feature fi ? F is
associated with a feature fj ? F , where i 6= j
and 1 ? i, j ? M , if their relative co-occurrence
score is within a predefined range:
score(fj , fi) =
?N
k=1 exists(fj , fi, sk)
?N
k=1 exists(fi, sk)
(1)
8Colours can be customised by the user.
where sk ? S, 1 ? k ? N , exists() is a
binary function that returns 1 if the input fea-
tures occur in sk, and 0 ? score(fj , fi) ? 1.
We group features in terms of their relative co-
occurrence within sentences in the corpus and dis-
play these co-occurrence relationships as directed
graphs. Two nodes (features) are connected by
an edge if their score, based on Equation (1), is
within a user-defined range (see example below).
Given fi and fj , the outgoing edges of fi are mod-
elled using score(fj , fi) and the incoming edges
using score(fi, fj). Feature relations are shown
via highlighting of features when the user hovers
the cursor over them, while the strength of the re-
lations is visually encoded in the edge width.
For example, one of the highest-weighted pos-
itive discriminative features is VM RR (see Ta-
ble 1), which captures sequences of a modal
auxiliary followed by an adverb as in will al-
ways (avoid) or could clearly (see). Investigat-
ing its relative co-occurrence with other features
using a score range of 0.8?1 and regardless of
directionality, we find that VM RR is related to
the following: (i) POS ngrams: RR VB? AT1,
VM RR VB?, VM RR VH?, PPH1 VM RR,
VM RR VV?, PPIS1 VM RR, PPIS2 VM RR,
RR VB?; (ii) word ngrams: will also, can only,
can also, can just. These relations show us the
37
syntactic environments of the feature (i) or its
characteristic lexicalisations (ii).
3.3 Dynamic creation of graphs via selection
criteria
Questions relating to a graph display may include
information about the most connected nodes, sep-
arate components of the graph, types of intercon-
nected features, etc. However, the functionality,
usability and tractability of graphs is severely lim-
ited when the number of nodes and edges grows
by more than a few dozen (Fry, 2007). In order
to provide adequate information, but at the same
time avoid overly complex graphs, we support dy-
namic creation and visualisation of graphs using
a variety of selection criteria. The EP visualiser
supports the flexible investigation of the top 4,000
discriminative features and their relations.
The Menu item on the top left of the UI in Fig-
ure 1 activates a panel that enables users to select
the top N features to be displayed. The user can
choose whether to display positive and/or neg-
ative features and set thresholds for, as well as
rank by discriminative weight, connectivity with
other features (i.e., the number of features it is
connected to), and frequency. For instance, a
user can choose to investigate features that have
a connectivity between 500 and 900, rank them
by frequency and display the top 100. Highly-
connected features might tell us something about
the learner grammar while infrequent features, al-
though discriminative, might not lead to useful
linguistic insights. Additionally, users can in-
vestigate feature relations and set different score
ranges according to Equation (1), which controls
the edges to be displayed.
Figure 2(a) presents the graph of the 5 most
frequent negative features, using a score range
of 0.8?1. The system displays only one edge,
while the rest of the features are isolated. How-
ever, these features might be related to other fea-
tures from the list of 4,000 (which are not dis-
played since they are not found in the top N
list of features). Blue aggregation markers in the
shape of a circle, located at the bottom right of
each node, are used to visually display that in-
formation. When a node with an aggregation
marker is selected, the system automatically ex-
pands the graph and displays the related features.
The marker shape of an expanded node changes
to a star, while a different border stroke pattern
(a) Graph of the top 5 most fre-
quent negative features using a
score range of 0.8?1.
(b) Expanded graph when the aggregation marker for the
feature VVD II is selected.
Figure 2: Dynamic graph creation.
is used to visually distinguish the revealed nodes
from the top N . Figure 2(b) presents the ex-
panded graph when the aggregation marker for the
feature VVD II is selected. If the same aggrega-
tion marker is selected twice, the graph collapses
and returns to its original form.
3.4 Feature?Error relations
The FCE texts have been manually error-coded
(Nicholls, 2003) so it is possible to find associa-
tions between discriminative features and specific
error types. The Feature?Error relations compo-
nent on the left of Figure 1 displays a list of the
features, ranked by their discriminative weight,
together with statistics on their relations with er-
rors. Feature?error relations are computed at the
sentence level by calculating the proportion of
sentences containing a feature that also contain
a specific error (similar to Equation (1)). In the
example in Figure 1, we see that 27% of the sen-
tences that contain the feature bigram the people
also have an unnecessary determiner (UD) error,
while 14% have a replace verb (RV) error9.
9In the example image we only output the top 5 errors
(can be customised by the user).
38
Figure 3: Sentences, split by grade, containing occurrences of how to and RGQ TO VV?. The list on the left
gives error frequencies for the matching scripts, including the frequencies of lemmata and POSs inside an error.
3.5 Searching the data
In order to allow the user to explore how fea-
tures are related to the data, the EP visualiser
supports browsing operations. Selecting multiple
features ? highlighted in yellow ? and clicking
on the button get scripts returns relevant scripts.
The right panel of the front-end in Figure 1 dis-
plays a number of search and output options.
Users can choose to output the original/error-
coded/POS-tagged text and/or the grammatical
relations found by the RASP parser (Briscoe et
al., 2006), while different colours are used in or-
der to help readability. Data can be retrieved at
the sentence or script level and separated accord-
ing to grade. Additionally, Boolean queries can be
executed in order to examine occurrences of (se-
lected features and) specific errors only10. Also,
users can investigate scripts based on meta-data
information such as learner age.
Figure 3 shows the display of the system when
the features how to and RGQ TO VV? (how to
followed by a verb in base form) are selected. The
text area in the centre displays sentences instanti-
ating them. A search box at the top supports nav-
10For example, users can activate the Scripts with errors:
option and type ?R OR W?. This will return sentences con-
taining replace or word order errors.
igation, highlighting search terms in red, while
a small text area underneath displays the current
search query, the size of the database and the num-
ber of matching scripts or sentences. The Errors
by decreasing frequency pane on the left shows
a list of the errors found in the matching scripts,
ordered by decreasing frequency. Three different
tabs (lemma, POS and lemma POS) provide in-
formation about and allow extraction of counts of
lemmata and POSs inside an error tag.
3.6 Learner native language
Research on SLA highlights the possible effect of
a native language (L1) on the learning process.
Using the Menu item on the top left corner of
Figure 1, users can select the language of inter-
est while the system displays a new window with
an identical front-end and functionality. Feature?
error statistics are now displayed per L1, while
selecting multiple features returns scripts written
by learners speaking the chosen L1.
4 Interpreting discriminative features: a
case study
We now illustrate in greater depth how the EP vi-
sualiser can support interpretation of discrimina-
tive features: the POS trigram RG JJ NN1 (?) is
39
the 18th most discriminative (negative) feature. It
corresponds to a sequence of a degree adverb fol-
lowed by an adjective and a singular noun as in
very good boy. The question is why such a fea-
ture is negative since the string is not ungrammat-
ical. Visualisation of this feature using the ?dy-
namic graph creation? component of the visualiser
allows us to see the features it is related to. This
offers an intuitive and manageable way of inves-
tigating the large number of underlying discrimi-
native features.
We find that RG JJ NN1 is related to its dis-
criminative lexicalisation, very good (?), which
is the 513th most discriminative feature. Also,
it is related to JJ NN1 II (?) (e.g., difficult sport
at), ranked 2,700th, which suggests a particular
context for RG JJ NN1 when the noun is fol-
lowed by a preposition. Searching for this con-
junction of features in scripts, we get production
examples like 1a,b,c. Perhaps more interestingly,
RG JJ NN1 is related to VBZ RG (?) (ranked
243rd): is followed by a degree adverb. This
relation suggests a link with predicative struc-
tures since putting the two ngrams together yields
strings VBZ RG JJ NN1 corresponding to exam-
ples like 1c,d; if we also add II we get examples
like 1c.
1a It might seem to be very difficult sport at the
beginning.
1b We know a lot about very difficult situation
in your country.
1c I think it?s very good idea to spending vaca-
tion together.
1d Unix is very powerful system but there is one
thing against it.
The associations between features already give
an idea of the source of the problem. In the se-
quences including the verb be the indefinite ar-
ticle is omitted. So the next thing to investigate
is if indeed RG JJ NN1 is associated with ar-
ticle omission, not only in predicative contexts,
but more generally. The Feature?Error relations
component of the UI reveals an association with
MD (missing determiner) errors: 23% of sen-
tences that contain RG JJ NN1 also have a MD
error. The same holds for very good, JJ NN1 II
and VBZ RG with percentages 12%, 14% and
Language f1 f2 f3 f4
all 0.26 0.40 0.02 0.03
Turkish 0.29 0.48 0.04 0.03
Japanese 0.17 0.39 0.02 0.02
Korean 0.30 0.58 0.06 0.03
Russian 0.35 0.52 0.03 0.03
Chinese 0.25 0.56 0.02 0.03
French 0.21 0.41 0.00 0.03
German 0.19 0.41 0.00 0.02
Spanish 0.27 0.32 0.00 0.03
Greek 0.30 0.35 0.02 0.02
Table 2: f1/2/3/4:doc ratios for different L1s.
15% respectively. We then compared the num-
ber of MD errors per script across different types
of scripts. Across all scripts the ratio MD:doc
is 2.18, that is, approximately 2 MD errors per
script; in RG JJ NN1 scripts this ratio goes up
to 2.75, so that each script has roughly 3 MD
errors. VBZ RG follows with 2.68, JJ NN1 II
with 2.48, and very good with 2.32. In scripts
containing all features the ratio goes up to 4.02
(3.68 without very good), and in scripts contain-
ing VBZ RG JJ the ratio goes up to 2.73. Also,
in most of these scripts the error involves the in-
definite article. The emerging picture then is that
there is a link between these richer nominal struc-
tures that include more than one modifier and the
omission of the article. Two questions arise: (i)
why these richer nominals should associate with
article omission and (ii) why only singular nouns
are implicated in this feature.
Article omission errors are typical of learn-
ers coming from L1s lacking an article sys-
tem (Robertson, 2000; Ionin and Montrul, 2010;
Hawkins and Buttery, 2010). Trenkic (2008) pro-
poses that such learners analyse articles as adjecti-
val modifiers rather than as a separate category of
determiners or articles. When no adjective is in-
volved, learners may be aware that bare nominals
are ungrammatical in English and provide the ar-
ticle. However, with complex adjectival phrases,
learners may omit the article because of the pres-
ence of a degree adverb. In order to evaluate this
hypothesis further we need to investigate if arti-
cle omission is indeed more pronounced in our
data with more complex adjectival phrases e.g.,
very difficult situation than with simpler ones e.g.,
nice boy and whether this is primarily the case for
40
learners from L1s lacking articles.
Again, using the Errors by decreasing fre-
quency pane we found that the MD:doc ratio in
scripts containing the bigram JJ NN1 is 2.20. Ad-
ditionally, in scripts containing JJ NN1 and not
RG JJ NN1 it goes down to 2.04. These results
are much lower compared to the MD:doc ratio
in scripts containing RG JJ NN1 and/or the fea-
tures with which it is related (see above), fur-
ther supporting our hypothesis. We also found
the ratio of RG JJ NN1 (f1) occurrences per doc-
ument across different L1s, as well as the ratio
of VBZ RG JJ (f2), VBZ RG JJ NN1 (f3) and
RG JJ NN1 II (f4). As shown in Table 2 there
is no correlation between these features and the
L1, with the exception of f1 and f2 which are
more pronounced in Korean and Russian speak-
ers, and of f3 which seems completely absent
from French, German and Spanish which all have
articles. The exception is Greek which has articles
but uses bare nominals in predicative structures.
However, a more systematic pattern is revealed
when relations with MD errors are considered (us-
ing the Feature?Error relations and Errors by de-
creasing frequency components for different L1s).
As shown in Table 3, there is a sharp contrast be-
tween L1s with articles (French, German, Spanish
and Greek) and those without (Turkish, Japanese,
Korean, Russian, Chinese), which further sup-
ports our hypothesis. A further question is why
only the singular article is implicated in this fea-
ture. The association with predicative contexts
may provide a clue. Such contexts select nomi-
nals which require the indefinite article only in the
singular case; compare Unix is (a) very powerful
system with Macs are very elegant machines.
In sum, navigating the UI, we formed some
initial interpretations for why a particular feature
is negatively discriminative. In particular, nomi-
nals with complex adjectival phrases appear par-
ticularly susceptible to article omission errors by
learners of English with L1s lacking articles. The
example illustrates not just the usefulness of visu-
alisation techniques for navigating and interpret-
ing large amounts of data, but, more generally
the relevance of features weighted by discrimina-
tive classifiers. Despite being superficial in their
structure, POS ngrams can pick up syntactic envi-
ronments linked to particular phenomena. In this
case, the features do not just identify a high rate of
article omission errors, but, importantly, a partic-
sentences% MD:doc
Language f1 f2 f1 f2
all 23.0 15.6 2.75 2.73
Turkish 45.2 29.0 5.81 5.82
Japanese 44.4 22.3 4.48 3.98
Korean 46.7 35.0 5.48 5.31
Russian 46.7 23.4 5.42 4.59
Chinese 23.4 13.5 3.58 3.25
French 6.9 6.7 1.32 1.49
German 2.1 3.0 0.91 0.92
Spanish 10.0 9.6 1.18 1.35
Greek 15.5 12.9 1.60 1.70
Table 3: f1/2 relations with MD errors for different
L1s, where sentences% shows the proportion of sen-
tences containing f1/2 that also contain a MD.
ular syntactic environment triggering higher rates
of such errors.
5 Previous work
To the best of our knowledge, this is the first at-
tempt to visually analyse as well as perform a
linguistic interpretation of discriminative features
that characterise learner English.
Collins (2010) in his dissertation addresses vi-
sualisation for NLP research. The Bubble Sets vi-
sualisation draws secondary set relations around
arbitrary collections of items, such as a linguis-
tic parse tree. VisLink provides a general plat-
form within which multiple visualisations of lan-
guage (e.g., a force-directed graph and a radial
graph) can be connected, cross-queried and com-
pared. Moreover, he explores the space of content
analysis. DocuBurst is an interactive visualisation
of document content, which spatially organizes
words using an expert-created ontology (e.g.,
WordNet). Parallel Tag Clouds combine keyword
extraction and coordinated visualisations to pro-
vide comparative overviews across subsets of a
faceted text corpus. Recently, Rohrdantz et al
(2011) proposed a new approach to detecting and
investigating changes in word senses by visually
modelling and plotting aggregated views about
the diachronic development in word contexts.
Visualisation techniques have been success-
fully used in other areas including the humanities
(e.g., Plaisant et al (2006) and Don et al (2007)),
as well as genomics (e.g., Meyer et al (2010a)
and Meyer et al (2010b)). For example, Meyer
41
et al (2010a) present a system that supports the
inspection and curation of data sets showing gene
expression over time, in conjunction with the spa-
tial location of the cells where the genes are ex-
pressed.
Graph layouts have been effectively used in
the analysis of domains such as social networks
(e.g., terrorism network) to allow for a system-
atic exploration of a variety of Social Network
Analysis measures (e.g., Gao et al (2009) and
Perer and Shneiderman (2006)). Heer and Boyd
(2005) have implemented Vizster, a visualisation
system for the exploration of on-line social net-
works (e.g., facebook) designed to facilitate the
discovery of people, promote awareness of com-
munity structure etc. Van Ham et al (2009) intro-
duce Phrase Net, a system that analyses unstruc-
tured text by taking as input a predefined pattern
and displaying a graph whose nodes are words
and whose edges link the words that are found as
matches.
We believe our integration of highly-weighted
discriminative features identified by a supervised
classifier into a graph-based visualiser to support
linguistic SLA research is, however, novel.
6 Conclusions
We have demonstrated how a data-driven ap-
proach to learner corpora can support SLA re-
search when guided by discriminative features
and augmented with visualisation tools. We de-
scribed a visual UI which supports exploratory
search over a corpus of learner texts using di-
rected graphs of features, and presented a case
study of how the system allows SLA researchers
to investigate the data and form hypotheses about
intermediate level learners. Although the use-
fulness of the EP visualiser should be con-
firmed through more rigorous evaluation tech-
niques, such as longitudinal case studies (Shnei-
derman and Plaisant, 2006; Munzner, 2009) with
a broad field of experts, these initial explorations
are encouraging. One of the main advantages of
using visualisation techniques over command-line
database search tools is that SLA researchers can
start developing and testing hypotheses without
the need to learn a query syntax first.
We would also like to point out that we adopted
a user-driven development of the visualiser based
on the needs of the third author, an SLA re-
searcher who acted as a design partner during
the development of the tool and was eager to use
and test it. There were dozens of meetings over
a period of seven months, and the feedback on
early interfaces was incorporated in the version
described here. After the prototype reached a sat-
isfactory level of stability, the final version overall
felt enjoyable and inviting, as well as allowed her
to form hypotheses and draw on different types of
evidence in order to substantiate it (Alexopoulou
et al, 2012). Future work will include the devel-
opment, testing and evaluation of the UI with a
wider range of users, as well as be directed to-
wards investigation and evaluation of different vi-
sualisation techniques of machine learned or ex-
tracted features that support hypothesis formation
about learner grammars.
Acknowledgments
We are grateful to Cambridge ESOL for support-
ing this research. We would like to thank Marek
Rei, ?istein Andersen, Paula Buttery and Ange-
liki Salamoura for fruitful discussions and feed-
back, Tim Parish for making the tool available on
the web, as well as the anonymous reviewers for
their valuable comments and suggestions.
References
Theodora Alexopoulou, Helen Yannakoudakis, and
Angeliki Salamoura. 2012. Classifying interme-
diate Learner English: a data-driven approach to
learner corpora. to appear.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL, volume 6.
Ted Briscoe, Ben Medlock, and ?istein Andersen.
2010. Automated Assessment of ESOL Free Text
Examinations. University of Cambridge, Computer
Laboratory, TR-790.
Stuart K. Card, Jock D. Mackinlay, and Ben Shneider-
man. 1999. Readings in information visualization:
using vision to think. Morgan Kaufmann.
Christopher M. Collins. 2010. Interactive Visualiza-
tions of natural language. Ph.D. thesis, University
of Toronto.
Giuseppe Di Battista, Peter Eades, Roberto Tamassia,
and Ioannis G. Tollis. 1999. Graph Drawing: Al-
gorithms for the Visualization of Graphs. Prentice
Hall Press.
Anthony Don, Elena Zheleva, Machon Gregory,
Sureyya Tarkan, Loretta Auvil, Tanya Clement, Ben
Shneiderman, and Catherine Plaisant. 2007. Dis-
covering interesting usage patterns in text collec-
tions: integrating text mining with visualization. In
42
Proceedings of the sixteenth ACM conference on in-
formation and knowledge management, pages 213?
222. ACM.
Ben Fry. 2007. Visualizing Data: Exploring and
Explaining Data with the Processing Environment.
O?Reilly Media.
Jie Gao, Kazuo Misue, and Jiro Tanaka. 2009. A
Multiple-Aspects Visualization Tool for Exploring
Social Networks. Human Interface and the Man-
agement of Information, pages 277?286.
Otis Gospodnetic and Erik Hatcher. 2004. Lucene in
Action. Manning Publications.
Lu Gram and Paula Buttery. 2009. A tutorial intro-
duction to iLexIR Search. unpublished.
John Hawkins and Paula Buttery. 2010. Criterial fea-
tures in Learner Corpora: theory and illustrations.
English Profile Journal, 1(1):1?23.
Jeffrey Heer and Danah Boyd. 2005. Vizster: visual-
izing online social networks. IEEE Symposium on
Information Visualization (INFOVIS), pages 32?39.
Jeffrey Heer, Stuart K. Card, and James A. Landay.
2005. Prefuse: a toolkit for interactive informa-
tion visualization. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 421?430, New York, USA. ACM.
Tania Ionin and Silvina Montrul. 2010. The role
of l1 transfer in the interpretation of articles with
definite plurals in l2 english. Language Learning,
60(4):877?925.
Miriah Meyer, Tamara Munzner, Angela DePace, and
Hanspeter Pfister. 2010a. MulteeSum: a tool for
comparative spatial and temporal gene expression
data. IEEE transactions on visualization and com-
puter graphics, 16(6):908?17.
Miriah Meyer, Bang Wong, Mark Styczynski, Tamara
Munzner, and Hanspeter Pfister. 2010b. Pathline:
A tool for comparative functional genomics. Com-
puter Graphics, 29(3).
Tamara Munzner. 2009. A Nested Model for Visual-
ization Design and Validation. IEEE Transactions
on Visualization and Computer Graphics, 15(6).
Diane Nicholls. 2003. The Cambridge Learner
Corpus-error coding and analysis for lexicography
and ELT. In Proceedings of the Corpus Linguistics
2003 conference, pages 572?581.
Adam Perer and Ben Shneiderman. 2006. Balanc-
ing Systematic and Flexible Exploration of Social
Networks. IEEE Transactions on Visualization and
Computer Graphics, 12(5):693?700.
Catherine Plaisant, James Rose, Bei Yu, Loretta Auvil,
Matthew G. Kirschenbaum, Martha N. Smith,
Tanya Clement, and Greg Lord. 2006. Exploring
erotics in Emily Dickinson?s correspondence with
text mining and visual interfaces. In Proceedings of
the 6th ACM/IEEE-CS joint conference on Digital
libraries, pages 141?150. ACM.
Daniel Robertson. 2000. Variability in the use of the
English article system by Chinese learners of En-
glish. Second Language Research, 2:135?172.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
and Miriam Butt. 2011. Towards tracking seman-
tic change by visual analytics. Proceedings of the
49th Meeting of the Association for Computational
Linguistics, pages 305?310.
Ben Shneiderman and Catherine Plaisant. 2006.
Strategies for evaluating information visualization
tools: multi-dimensional in-depth long-term case
studies. In Proceedings of the 2006 AVI workshop
on BEyond time and errors: novel evaluation meth-
ods for information visualization. ACM.
Danijela Trenkic. 2008. The representation of English
articles in second language grammars: Determiners
or adjectives? Bilingualism: Language and Cogni-
tion, 11(01):1?18.
Frank Van Ham, Martin Wattenberg, and Fernanda B.
Vie?gas. 2009. Mapping text with phrase nets.
IEEE Transactions on Visualization and Computer
Graphics, 15(6):1169?76.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
43
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33?43,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Modeling coherence in ESOL learner texts
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
To date, few attempts have been made to de-
velop new methods and validate existing ones
for automatic evaluation of discourse coher-
ence in the noisy domain of learner texts.
We present the first systematic analysis of
several methods for assessing coherence un-
der the framework of automated assessment
(AA) of learner free-text responses. We ex-
amine the predictive power of different coher-
ence models by measuring the effect on per-
formance when combined with an AA system
that achieves competitive results, but does not
use discourse coherence features, which are
also strong indicators of a learner?s level of at-
tainment. Additionally, we identify new tech-
niques that outperform previously developed
ones and improve on the best published result
for AA on a publically-available dataset of En-
glish learner free-text examination scripts.
1 Introduction
Automated assessment (hereafter AA) systems of
English learner text assign grades based on textual
features which attempt to balance evidence of writ-
ing competence against evidence of performance er-
rors. Previous work has mostly treated AA as a
supervised text classification or regression task. A
number of techniques have been investigated, in-
cluding cosine similarity of feature vectors (Attali
and Burstein, 2006), often combined with dimen-
sionality reduction techniques such as Latent Se-
mantic Analysis (LSA) (Landauer et al, 2003), and
generative machine learning models (Rudner and
Liang, 2002) as well as discriminative ones (Yan-
nakoudakis et al, 2011). As multiple factors influ-
ence the linguistic quality of texts, such systems ex-
ploit features that correspond to different properties
of texts, such as grammar, style, vocabulary usage,
topic similarity, and discourse coherence and cohe-
sion.
Cohesion refers to the use of explicit linguistic
cohesive devices (e.g., anaphora, lexical semantic
relatedness, discourse markers, etc.) within a text
that can signal primarily suprasentential discourse
relations between textual units (Halliday and Hasan,
1976). Cohesion is not the only mechanism of dis-
course coherence, which may also be inferred from
meaning without presence of explicit linguistic cues.
Coherence can be assessed locally in terms of tran-
sitions between adjacent clauses, parentheticals, and
other textual units capable of standing in discourse
relations, or more globally in terms of the overall
topical coherence of text passages.
There is a large body of work that has investi-
gated a number of different coherence models on
news texts (e.g., Lin et al (2011), Elsner and Char-
niak (2008), and Soricut and Marcu (2006)). Re-
cently, Pitler et al (2010) presented a detailed survey
of current techniques in coherence analysis of ex-
tractive summaries. To date, however, few attempts
have been made to develop new methods and vali-
date existing ones for automatic evaluation of dis-
course coherence and cohesion in the noisy domain
of learner texts, where spelling and grammatical er-
rors are common.
Coherence quality is typically present in marking
criteria for evaluating learner texts, and it is iden-
33
tified by examiners as a determinant of the overall
score. Thus we expect that adding a coherence met-
ric to the feature set of an AA system would better
reflect the evaluation performed by examiners and
improve performance. The goal of the experiments
presented in this paper is to measure the effect a
number of (previously-developed and new) coher-
ence models have on performance when combined
with an AA system that achieves competitive results,
but does not use discourse coherence features.
Our contribution is threefold: 1) we present the
first systematic analysis of several methods for as-
sessing discourse coherence in the framework of
AA of learner free-text responses, 2) we identify
new discourse features that serve as proxies for the
level of (in)coherence in texts and outperform pre-
viously developed techniques, and 3) we improve
the best results reported by Yannakoudakis et al
(2011) on the publically available ?English as a Sec-
ond or Other Language? (ESOL) corpus of learner
texts (to date, this is the only public-domain corpus
that contains grades). Finally, we explore the utility
of our best model for assessing the incoherent ?out-
lier? texts used in Yannakoudakis et al (2011).
2 Experimental Design & Background
We examine the predictive power of a number of
different coherence models by measuring the effect
on performance when combined with an AA system
that achieves state-of-the-art results, but does not
use discourse coherence features. Specifically, we
describe a number of different experiments improv-
ing on the AA system presented in Yannakoudakis
et al (2011); AA is treated as a rank preference
supervised learning problem and ranking Support
Vector Machines (SVMs) (Joachims, 2002) are used
to explicitly model the grade relationships between
scripts. This system uses a number of different lin-
guistic features that achieve good performance on
the AA task. However, these features only focus on
lexical and grammatical properties, as well as errors
within individual sentences, ignoring discourse co-
herence, which is also present in marking criteria for
evaluating learner texts, as well as a strong indicator
of a writer?s understanding of a language.
Also, in Yannakoudakis et al (2011), experiments
are presented that test the validity of the system
using a number of automatically-created ?outlier?
texts. The results showed that the model is vulner-
able to input where individually high-scoring sen-
tences are randomly ordered within a text. Failing to
identify such pathological cases makes AA systems
vulnerable to subversion by writers who understand
something of its workings, thus posing a threat to
their validity. For example, an examinee might learn
by rote a set of well-formed sentences and repro-
duce these in an exam in the knowledge that an AA
system is not checking for prompt relevance or co-
herence1.
3 Dataset & Experimental Setup
We use the First Certificate in English (FCE) ESOL
examination scripts2 (upper-intermediate level as-
sessment) described in detail in Yannakoudakis et al
(2011), extracted from the Cambridge Learner Cor-
pus3 (CLC). The dataset consists of 1,238 texts be-
tween 200 and 400 words produced by 1,238 distinct
learners in response to two different prompts. An
overall mark has been assigned in the range 1?40.
For all experiments, we use a series of 5-fold
cross-validation runs on 1,141 texts from the exami-
nation year 2000 to evaluate performance as well as
generalization of numerous models. Moreover, we
identify the best model on year 2000 and we also test
it on 97 texts from the examination year 2001, previ-
ously used in Yannakoudakis et al (2011) to report
the best published results. Validating the results on
a different examination year tests generalization to
some prompts not used in 2000, and also allows us to
test correlation between examiners and the AA sys-
tem. Again, we treat AA as a rank preference learn-
ing problem and use SVMs, utilizing the SVMlight
package (Joachims, 2002), to facilitate comparison
with Yannakoudakis et al (2011).
4 Discourse Coherence
We focus on the development and evaluation of (au-
tomated) methods for assessing coherence in learner
1Powers et al (2002) report the results of a related exper-
iment with the AA system e-Rater, in which experts tried to
subvert the system by submitting essays they believed would be
inaccurately scored.
2http://ilexir.co.uk/applications/clc-fce-dataset/
3http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom
/item3646603/
34
texts under the framework of AA. Most of the meth-
ods we investigate require syntactic analysis. As in
Yannakoudakis et al (2011), we analyze all texts us-
ing the RASP toolkit (Briscoe et al, 2006)4.
4.1 ?Superficial? Proxies
In this section we introduce diverse classes of ?su-
perficial? cohesive features that serve as proxies for
coherence. Surface text properties have been as-
sessed in the framework of automatic summary eval-
uation (Pitler et al, 2010), and have been shown to
significantly correlate with the fluency of machine-
translated sentences (Chae and Nenkova, 2009).
4.1.1 Part-of-Speech (POS) Distribution
The AA system described in Yannakoudakis et
al. (2011) exploited features based on POS tag se-
quences, but did not consider the distribution of POS
types across grades. In coherent texts, constituent
clauses and sentences are related and depend on each
other for their interpretation. Anaphors such as pro-
nouns link the current sentence to those where the
entities were previously mentioned. Pronouns can
be directly related to (lack of) coherence and make
intuitive sense as cohesive devices. We compute the
number of pronouns in a text and use it as a shallow
feature for capturing coherence.
4.1.2 Discourse Connectives
Discourse connectives (such as but or because) re-
late propositions expressed by different clauses or
sentences. The presence of such items in a text
should be indicative of (better) coherence. We thus
compute a number of shallow cohesive features as
proxies for coherence, based on fixed lists of words
belonging to the following categories: (a) Addition
(e.g., additionally), (b) Comparison (e.g., likewise),
(c) Contrast (e.g., whereas) and (d) Conclusion (e.g.,
therefore), and use the frequencies of these four cat-
egories as features.
4.1.3 Word Length
The previous AA system treated script length as
a normalizing feature, but otherwise avoided such
?superficial? proxies of text quality. However, many
cohesive words are longer than average, especially
for the closed-class functional component of English
4http://ilexir.co.uk/applications/rasp/
vocabulary. We thus assess the minimum, maximum
and average word length as a superficial proxy for
coherence.
4.2 Semantic Similarity
We explore the utility of inter-sentential feature
types for assessing discourse coherence. Among the
features used in Yannakoudakis et al (2011), none
explicitly captures coherence and none models inter-
sentential relationships. Incremental Semantic anal-
ysis (ISA) (Baroni et al, 2007) is a word-level dis-
tributional model that induces a semantic space from
input texts. ISA is a fully-incremental variation of
Random Indexing (RI) (Sahlgren, 2005), which can
efficiently capture second-order effects in common
with other dimensionality-reduction methods based
on singular value decomposition, but does not rely
on stoplists or global statistics for weighting pur-
poses.
Utilizing the S-Space package (Jurgens and
Stevens, 2010), we trained an ISA model5 using a
subset of ukWaC (Ferraresi et al, 2008), a large cor-
pus of English containing more than 2 billion tokens.
We used the POS tagger lexicon provided with the
RASP system to discard documents whose propor-
tion of valid English words to total words is less than
0.4; 78,000 documents were extracted in total and
were then preprocessed replacing URLs, email ad-
dresses, IP addresses, numbers and emoticons with
special markers. To measure local coherence we de-
fine the similarity between two sentences si and si+1
as the maximum cosine similarity between the his-
tory vectors of the words they contain. The overall
coherence of a text T is then measured by taking the
mean of all sentence-pair scores:
coherence(T ) =
?n?1
i=1 maxk,j sim(s
k
i , s
j
i+1)
n? 1
(1)
where sim(ski , s
j
i+1) is the cosine similarity between
the history vectors of the kth word in si and the
jth word in si+1, and n is the total number of
sentences6. We investigate the efficacy of ISA by
adding this coherence score, as well as the maximum
5The parameters of our ISA model are fairly standard: 1800
dimensions, a context window of 3 words, impact rate i =
0.0003 and decay rate km = 50.
6We exclude articles, conjunctions, prepositions and auxil-
iary verbs from the calculation of sentence similarity.
35
sim value found over the entire text, to the vectors
of features associated with a text. The hypothesis
is that the degree of semantic relatedness between
adjoining sentences serves as a proxy for local dis-
course coherence; that is, coherent text units contain
semantically-related words.
Higgins et al (2004) and Higgins and Burstein
(2007) use RI to determine the semantic similarity
between sentences of same/different discourse seg-
ments (e.g., from the essay thesis and conclusion, or
between sentences and the essay prompt), and assess
the percentage of sentences that are correctly clas-
sified as related or unrelated. The main differences
from our approach are that we assess the utility of se-
mantic space models for predicting the overall grade
for a text, in contrast to binary classification at the
sentence-level, and we use ISA rather than RI7.
4.3 Entity-based Coherence
The entity-based coherence model, proposed by
Barzilay and Lapata (2008), is one of the most pop-
ular statistical models of inter-sentential coherence,
and learns coherence properties similar to those em-
ployed by Centering Theory (Grosz et al, 1995).
Local coherence is modeled on the basis of se-
quences of entity mentions that are labeled with
their syntactic roles (e.g., subject, object). We con-
struct the entity grids using the Brown Coherence
Toolkit8,9 (Elsner and Charniak, 2011b), and use as
features the probabilities of different entity transi-
tion types, defined in terms of their role in adja-
cent sentences10. Burstein et al (2010) show how
the entity-grid can be used to discriminate high-
coherence from low-coherence learner texts. The
main difference with our approach is that we eval-
uate the entity-grid model in the context of AA text
grading, rather than binary classification.
7We also used RI in addition to ISA, and found that it did
not yield significantly different results. In particular, we trained
a RI model with 2,000 dimensions and a context window of 3
on the same ukWaC data. Below we only report results for the
fully-incremental ISA model.
8https://bitbucket.org/melsner/browncoherence
9The tool does not perform full coreference resolution; in-
stead, coreference is approximated by linking entities that share
a head noun.
10We represent entities with specified roles (Subject, Object,
Neither, Absent), use transition probabilities of length 2, 3 and
4, and a salience option of 2.
4.4 Pronoun Coreference Model
Pronominal anaphora is another important aspect
of coherence. Charniak and Elsner (2009) present
an unsupervised generative model of pronominal
anaphora for coherence modeling. In their imple-
mentation, they model each pronoun as generated by
an antecedent somewhere in the previous two sen-
tences. If a ?good? antecedent is found, the probabil-
ity of a pronoun will be high; otherwise, the proba-
bility will be low. The overall probability of a text
is then calculated as the probability of the result-
ing sequence of pronoun assignments. In our ex-
periments, we use the pre-trained model distributed
by Charniak and Elsner (2009) for news text to esti-
mate the probability of a text and include it as a fea-
ture. However, this model is trained on high-quality
texts, so performance may deteriorate when applied
to learner texts. It is not obvious how to train such
a model on learner texts and we leave this for future
research.
4.5 Discourse-new Model
Elsner and Charniak (2008) apply a discourse-new
classifier to model coherence. Their classifier dis-
tinguishes NPs whose referents have not been pre-
viously mentioned in the discourse from those that
have been already introduced, using a number of
syntactic and lexical features. To model coher-
ence, they assign each NP in a text a label Lnp ?
{new, old}11, and calculate the probability of a text
as ?np:NPsP (Lnp|np). Again, we use the pre-
trained model distributed by Charniak and Elsner
(2009) for news text to find the probability of a text
following Elsner and Charniak (2008) and include it
as a feature.
4.6 IBM Coherence Model
Soricut and Marcu (2006) adapted the IBM model
1 (Brown et al, 1994) used in machine translation
(MT) to model local discourse coherence. The intu-
ition behind the IBM model in MT is that the use of
certain words in a source language is likely to trig-
ger the use of certain words in a target language.
Instead, they hypothesized that the use of certain
words in a sentence tends to trigger the use of cer-
tain words in an adjoining sentence. In contrast to
11NPs with the same head are considered to be coreferent.
36
semantic space models such as ISA or RI (discussed
above), this method models the intuition that local
coherence is signaled by the identification of word
co-occurrence patterns across adjacent sentences.
We compute two features introduced by Soricut
and Marcu (2006): the forward likelihood and the
backward likelihood. The first refers to the likeli-
hood of observing the words in sentence si+1 condi-
tioned on si, and the latter to the likelihood of ob-
serving the words in si conditioned on si+1. We
extract 3 million adjacent sentences from ukWaC12,
and use the GIZA++ (Och and Ney, 2000) imple-
mentation of IBM model 1 to obtain the probabili-
ties of recurring patterns. The forward and backward
probabilities are calculated over the entire text, and
their values are used as features in our feature vec-
tors13. We further extend the above model and incor-
porate syntactic aspects of text coherence by train-
ing on POS tags instead of lexical items. We try to
model the intuition that local coherence is signaled
by the identification of POS co-occurrence patterns
across adjacent sentences, where the use of certain
POS tags in a sentence tends to trigger the use of
other POS tags in an adjacent sentence. We analyze
3 million adjacent sentences using the RASP POS
tagger and train the same IBM model to obtain the
probabilities of recurring POS patterns.
4.7 Lemma/POS Cosine Similarity
A simple method of incorporating (syntactic) as-
pects of text coherence is to use cosine similarity
between vectors of lemma and/or POS-tag counts in
adjacent sentences. We experiment with both: each
sentence is represented by a vector whose dimen-
sion depends on the total number of lemmas/POS-
types. The sentence vectors are weighted using
lemma/POS frequency, and the cosine similarity be-
tween adjacent sentences is calculated. The coher-
ence of a text T is then calculated as the average
value of cosine similarity over the entire text14:
coherence(T ) =
?n?1
i=1 sim(si, si+1)
n? 1
(2)
12We use the same subset of documents as the ones used to
train our ISA model in Section 4.2.
13Pitler et al (2010) have also investigated the IBM model to
measure text quality in automatically-generated texts.
14Pitler et al (2010) use POS cosine similarity to measure
continuity in automatically-generated texts.
4.8 Locally-Weighted Bag-of-Words
The popular bag-of-words (BOW) assumption rep-
resents a text as a histogram of word occurrences.
While computationally efficient, such a represen-
tation is unable to maintain any sequential infor-
mation. The locally-weighted bag-of-words (LOW-
BOW) framework, introduced by Lebanon et al
(2007), is a sequentially-sensitive alternative to
BOW. In BOW, we represent a text as a histogram
over the vocabulary used to generate that text. In
LOWBOW, a text is represented by a set of lo-
cal histograms computed across the whole text, but
smoothed by kernels centered on different locations.
More specifically, a smoothed characterization
of the local histogram is obtained by integrating a
length-normalized document with respect to a non-
uniform measure that is concentrated around a par-
ticular location ? ? [0, 1]. In accordance with the
statistical literature on non-parametric smoothing,
we refer to such a measure as a smoothing kernel.
The kernel parameters ? and ? specify the local his-
togram?s position in the text (i.e., where it is cen-
tered) and its scale (i.e., to what extent it is smoothed
over the surrounding region) respectively. In con-
trast to BOW or n-grams, which keep track of fre-
quently occurring patterns independent of their po-
sitions, this representation is able to robustly capture
medium and long range sequential trends in a text by
keeping track of changes in the histograms from its
beginning to end.
Geometrically, LOWBOW uses local smoothing
to embed texts as smooth curves in the multinomial
simplex. These curves summarize the progression
of semantic and/or statistical trends through the text.
By varying the amount of smoothing we obtain a
family of sequential representations possessing dif-
ferent sequential resolutions or scales. Low resolu-
tion representations capture topic trends and shifts
while ignoring finer details. High resolution repre-
sentations capture fine sequential details but make it
difficult to grasp the general trends within the text15.
Since coherence involves both cohesive lexical
devices and sequential progression within a text, we
believe that LOWBOW can be used to assess the se-
quential content and the global structure and coher-
15For more details regarding LOWBOW and its geometric
properties see Lebanon et al (2007).
37
ence of texts. We use a publically-available LOW-
BOW implementation16 to create local histograms
over word unigrams. For the LOWBOW kernel
smoothing function (see above), we use the Gaus-
sian probability density function restricted to [0, 1]
and re-normalized, and a smoothing ? value of 0.02.
Additionally, we consider a total number of 9 local
histograms (discourse segments). We further extend
the above model and incorporate syntactic aspects of
text coherence by using local histograms over POS
unigrams. This representation is able to capture se-
quential trends abstracted into POS tags. We try
to model the hypothesis that coherence is signaled
by sequential, mostly inter-sentential progression of
POS types.
Since each text is represented by a set of local
histrograms/vectors, and standard SVM kernels can-
not work with such input spaces, we use instead a
kernel defined over sets of vectors: the diffusion
kernel (Lafferty and Lebanon, 2005) compares lo-
cal histograms in a one-to-one fashion (i.e., his-
tograms at the same locations are compared to each
other), and has proven to be useful for related tasks
(Lebanon et al, 2007; Escalante et al, 2011). To the
best of our knowledge, LOWBOW representations
have not been investigated for coherence evaluation
(under the AA framework). So far, they have been
applied to discourse segmentation (AMIDA, 2007),
text categorization (Lebanon et al, 2007), and au-
thorship attribution (Escalante et al, 2011).
5 Evaluation
We examine the predictive power of each of the co-
herence models/features described in Section 4 by
measuring the effect on performance when com-
bined with an AA system that achieves state-of-the-
art results on the FCE dataset, but does not use dis-
course coherence features. In particular, we use the
system described in Yannakoudakis et al (2011) as
our baseline AA system. Discourse coherence is a
strong indicator of thorough knowledge of a second
language and thus we expect coherence features to
further improve performance of AA systems.
We evaluate the grade predictions of our mod-
els against the gold standard grades in the dataset
using Pearson?s product-moment correlation coeffi-
16http://goo.gl/yQ0Q0
cient (r) and Spearman?s rank correlation coefficient
(?) as is standard in AA research (Briscoe et al,
2010). Table 1 gives results obtained by augmenting
the baseline model with each of the coherence fea-
tures described above. In each of these experiments,
we perform 5-fold cross-validation17 using all 1,141
texts from the exam year 2000 (see Section 3).
Most of the resulting models have minimal ef-
fect on performance18. However, word length, ISA,
LOWBOWlex, and the IBM modelPOSf derived mod-
els all improve performance, while larger differ-
ences are observed in r. The highest performance
? 0.675 and 0.678 ? is obtained with ISA, while the
second best feature is word length. The entity-grid,
the pronoun model and the discourse-new model do
not improve on the baseline. Although these mod-
els have been successfully used as components in
state-of-the-art systems for discriminating coherent
from incoherent news documents (Elsner and Char-
niak, 2011b), and the entity-grid model has also
been successfully applied to learner text (Burstein
et al, 2010), they seem to have minimal impact
on performance, while the discourse-new model de-
creases ? by?0.01. On the other hand, LOWBOWlex
and LOWBOWPOS give an increase in performance,
which confirms our hypothesis that local histograms
are useful. Also, the former seems to perform
slightly better than the latter.
Our adapted version of the IBM model ? IBM
modelPOS ? performs better than its lexicalized ver-
sion, which does not have an impact on perfor-
mance, while larger differences are observed in r.
Additionally, the increase in performance is larger
than the one obtained with the entity-grid, pro-
noun or discourse-new model. The forward ver-
sion of IBM modelPOS seems to perform slightly
better than the backward one, while the results are
comparable to LOWBOWPOS and outperformed by
LOWBOWlex. The rest of the models do not perform
as well; the number of pronouns or discourse con-
nectives gives low results, while lemma and POS co-
sine similarity between adjacent sentences are also
17We compute mean values of correlation coefficients by first
applying the r-to-Z Fisher transformation, and then using the
Fisher weighted mean correlation coefficient (Faller, 1981).
18Significance tests in averaged correlations are omitted as
variable estimates are produced, whose variance is hard to be
estimated unbiasedly.
38
r ?
0 Baseline 0.651 0.670
1 POS distr. 0.653 0.670
2 Disc. connectives 0.648 0.668
3 Word length 0.667 0.676
4 ISA 0.675 0.678
5 EGrid 0.650 0.668
6 Pronoun 0.650 0.668
7 Disc-new 0.646 0.662
8 LOWBOWlex 0.663 0.677
9 LOWBOWPOS 0.659 0.674
10 IBM modellexf 0.649 0.668
11 IBM modellexb 0.649 0.667
12 IBM modelPOSf 0.661 0.672
13 IBM modelPOSb 0.658 0.669
14 Lemma cosine 0.651 0.667
15 POS cosine 0.650 0.665
16 5+6+7+10+11 0.648 0.665
17 All 0.677 0.671
Table 1: 5-fold cross-validation performance on texts
from year 2000 when adding different coherence features
on top of the baseline AA system.
among the weakest predictors.
Elsner and Charniak (2011b) have shown that
combining the entity-grid with the pronoun,
discourse-new and lexicalized IBM models gives
state-of-the-art results for discriminating news docu-
ments and their random permutations. We also com-
bine these models and assess their performance un-
der the AA framework. Row 16 of Table 1 shows
that the combination does not give an improvement
over the individual models. Moreover, combining
all feature classes together in row 17 does not yield
higher results than those obtained with ISA, while ?
is no better than the baseline.
In the following experiments, we evaluate the best
model identified on year 2000 on a set of 97 texts
from the exam year 2001, previously used in Yan-
nakoudakis et al (2011) to report results of the fi-
nal best system. Validating the model on a different
exam year also shows us the extent to which it gen-
eralizes between years. Table 2 presents the results.
The published correlations on this dataset are 0.741
and 0.773 r and ? respectively. Adding ISA on top
of the previous system significantly improves19 the
19Calculated using one-tailed tests for the difference between
r ?
Baseline 0.741 0.773
ISA 0.749 0.790?
Table 2: Performance on the exam scripts drawn from the
examination year 2001. ? indicates a significant improve-
ment at ? = 0.05.
published results on the 2001 texts, getting closer to
the upper-bound. The upper-bound on this dataset20
is 0.796 and 0.792 r and ? respectively, calculated
by taking the average correlation between the FCE
grades and the ones provided by 4 senior ESOL ex-
aminers21. Table 3 also presents the average corre-
lation between our extended AA system?s predicted
grades and the 4 examiners? grades, in addition to
the original FCE grades from the dataset. Again,
our extended model improves over the baseline.
Finally, we explore the utility of our best model
for assessing the publically available ?outlier? texts
used in Yannakoudakis et al (2011). The previous
AA system is unable to downgrade appropriately
?outlier? scripts containing individually high-scoring
sentences with poor overall coherence, created by
randomly ordering a set of highly-marked texts. To
test our best system, we train an SVM rank prefer-
ence model with the ISA-derived coherence feature,
which can explicitly capture such sequential trends.
A generic model for flagging putative ?outlier? texts
? whose predicted score is lower than a predefined
threshold ? for manual checking might be used as
the first stage of a deployed AA system. The ISA
model improves r and ? by 0.320 and 0.463 respec-
tively for predicting a score on this type of ?outlier?
texts and their original version (Table 4).
6 Analysis & Discussion
In the previous section, we evaluated various co-
hesion and coherence features on learner data, and
found different patterns of performance compared to
those previously reported on news texts (see Section
7 for more details). Although most of the models ex-
amined gave a minimal effect on AA performance,
ISA, LOWBOWlex, IBM modelPOSf and word length
dependent correlations (Williams, 1959; Steiger, 1980).
20See Yannakoudakis et al (2011) for details.
21The examiners? scores are also distributed with the FCE
dataset.
39
r ?
Baseline 0.723 0.721
ISA 0.727 0.736
Table 3: Average correlation between the AA model, the
FCE dataset grades, and 4 examiners on the exam scripts
from year 2000.
r ?
Baseline 0.08 0.163
ISA 0.400 0.626
Table 4: Performance of the ISA AA model on outliers.
gave a clear improvement in correlation, with larger
differences in r. Our results indicate that coherence
metrics further improve the performance of a com-
petitive AA system. More specifically, we found the
ISA-derived feature to be the most effective contrib-
utor to the prediction of text quality. This suggests
that incoherence in FCE texts might be due to topic
discontinuities. Also, the improvement obtained by
LOWBOW suggests that patterns of sequential pro-
gression within a text can be useful: coherent texts
appear to use similar token distributions at similar
locations across different documents.
The word length feature was successfully used as
a proxy for coherence, perhaps because many cohe-
sive words are longer than average. However, such
a feature can also capture further aspects of texts,
such as lexical complexity, so further investigation
is needed to identify the extent to which it measures
different properties. On the other hand, the minimal
effect of the entity-grid, pronoun and discourse-new
model suggests that infelicitous use of pronominal
forms or sequences of entities may not be an issue
in FCE texts. Preliminary investigation of the scripts
showed that learners tend to repeat the same entity
names or descriptions rather than use pronouns or
shorter descriptions.
A possible explanation for the difference in per-
formance between the lexicalized and POS IBM
model is that the latter abstracts away from lexi-
cal information and thus avoids misspellings and
reduces sparsity. Also, our discourse connective
classes do not seem to have a predictive power. This
may be because our manually-built word lists do not
have sufficient coverage.
7 Previous Work
Comparatively few metrics have been investigated
for evaluating coherence in (ESOL) learner texts.
Miltsakaki and Kukich (2004) employ e-Rater (At-
tali and Burstein, 2006), an essay scoring system,
and show that Centering Theory?s Rough-Shift tran-
sitions (Grosz et al, 1995) contribute significantly to
the assessment of learner texts. Higgins et al (2004)
and Higgins and Burstein (2007) use RI to deter-
mine the semantic similarity between sentences of
same/different discourse segments. Their model is
based on a number of different semantic similarity
scores and assesses the percentage of sentences that
are correctly classified as (un)related. Among their
results, they found that it is hard to beat the baseline
(as 98.1% of the sentences were annotated as ?highly
related?) and identify sentences which are not related
to other ones in the same discourse segment. We
demonstrate that the related fully-incremental ISA
model can be used to improve AA grading accuracy
on the FCE dataset, as opposed to classifying the
(non-)relatedness of sentences.
Burstein et al (2010) show how the entity-grid
can be used to discriminate high-coherence from
low-coherence learner texts. They augment this
model with additional features related to writing
quality and word usage, and show a positive effect
in performance for automated coherence prediction
of student essays of different populations. On the
FCE dataset used here, entity-grids do not improve
AA grading accuracy. This may be because the texts
are shorter or because grading is a more difficult task
than binary classification. Application of their aug-
mented entity-grid model to FCE texts would be an
interesting avenue for future research.
Foltz et al (1998) examine local coherence in
textbooks and articles using Latent Semantic Anal-
ysis (LSA) (Landauer et al, 2003). They assess se-
mantic relatedness using vector-based similarity be-
tween adjacent sentences. They argue that LSA may
be more appropriate for comparing the relative qual-
ity of texts; for determining the overall text coher-
ence it may be difficult to set a criterion for the co-
herence value since it depends on a variety of dif-
ferent factors, such as the size of the text units to be
compared. Nevertheless, our results show that ISA,
a similar distributional semantic model with dimen-
40
sionality reduction, improves FCE grading accuracy.
Barzilay and Lee (2004) implement lexicalized
content models that represent global text proper-
ties on news articles and narratives using Hidden
Markov Models (HMMs). In the HMM, states cor-
respond to distinct topics, and transitions between
states represent the probability of moving from one
topic to another. This approach has the advantage
of capturing the order in which different topics ap-
pear in texts; however, the HMMs are highly domain
specific and would probably need retraining for each
distinct essay prompt.
Soricut and Marcu (2006) use a log-linear model
that combines local and global models of coher-
ence and show that it outperforms each of the in-
dividual ones on news articles and accident reports.
Their global model is based on the document con-
tent model proposed by Barzilay and Lee (2004).
Their local model of discourse coherence is based
on the entity-grid (Barzilay and Lapata, 2008), as
well as on the lexicalized IBM model (see Section
4.6 above); we have experimented with both, and
showed that they have a minimal effect on grading
performance with the FCE dataset.
Elsner and Charniak (2008;2011a) apply a
discourse-new classifier and a pronoun coreference
system to model coherence (see Section 4) on dia-
logue and news texts. They found that combining
these models with the entity-grid achieves state-of-
the-art performance. We found that such a combina-
tion, as well as the individual models do not perform
well for grading the FCE texts.
Recently, Elsner and Charniak (2011a) proposed a
variation of the entity-grid intended to integrate top-
ical information. They use Latent Dirichlet Alloca-
tion (Blei et al, 2003) to learn topic-to-word distri-
butions, and model coherence by generalizing the bi-
nary history features of the entity-grid and comput-
ing a real-valued feature which represents the simi-
larity between an entity and the subject(s) of the pre-
vious sentence. Also, Lin et al (2011) proposed a
model that assesses the coherence of a text based on
discourse relation transitions. The underlying idea
is that coherent texts exhibit measurable preferences
for specific intra- and inter-discourse relation order-
ing. They found their model to be complementary to
the entity-grid, as it encodes the notion of preferen-
tial ordering of discourse relations, and thus tackles
local coherence from a different perspective. Apply-
ing the above models to AA on learner texts would
also be an interesting avenue for future work.
8 Conclusion
We presented the first systematic analysis of a wide
variety of models for assessing discourse coherence
on learner data, and evaluated their individual per-
formance as well as their combinations for the AA
grading task. We adapted the LOWBOW model for
assessing sequential content in texts, and showed
evidence supporting our hypothesis that local his-
tograms are useful. We also successfully adapted
ISA, an efficient and incremental variant distribu-
tional semantic model, to this task. ISA, LOWBOW,
the POS IBM model and word length are the best in-
dividual features for assessing coherence.
A significant improvement over the AA system
presented in Yannakoudakis et al (2011) and the
best published result on the FCE dataset was ob-
tained by augmenting the system with an ISA-based
local coherence feature. However, it is quite likely
that further experimentation with LOWBOW fea-
tures, given the large range of possible parameter
settings, would yield better results too.
We also explored the robustness of the ISA model
of local coherence on ?outlier? texts and achieved
much better correlations with the examiner?s grades
for these texts in the FCE dataset. This should facil-
itate development of an automated system to detect
essays consisting of high-quality but incoherent se-
quences of sentences.
All our results are specific to ESOL FCE texts and
may not generalize to other genres or ESOL attain-
ment levels. Future work should also investigate a
wider range of (learner) texts and further coherence
models, such as that of Elsner and Charniak (2011a)
and Lin et al (2011).
Acknowledgments
We are grateful to Cambridge ESOL, a division
of Cambridge Assessment, for supporting this re-
search. We would like to thank Marek Rei and ?is-
tein Andersen for their valuable comments and sug-
gestions, Yi Mao for giving us access to her code,
as well as the anonymous reviewers for their useful
feedback.
41
References
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from www. amidapro-
ject.org/, AMIDA Report.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3):1?30.
Marco Baroni, Alessandro Lenci, and Luca Onnis. 2007.
ISA meets Lara: An incremental word space model for
cognitively plausible simulations of semantic learning.
In Proceedings of the Association for Computational
Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
Local Coherence: An Entity-Based Approach. Com-
putational Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL, volume 6.
Ted Briscoe, Ben Medlock, and ?istein Andersen. 2010.
Automated assessment of ESOL free text examina-
tions. Technical Report UCAM-CL-TR-790, Univer-
sity of Cambridge, Computer Laboratory, November.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1994. The mathe-
matic of statistical machine translation: Parameter es-
timation. Computational linguistics, 19(2):263?311.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 681?684.
Jieun Chae and Ani Nenkova. 2009. Predicting the flu-
ency of text with shallow structural features: case stud-
ies of machine translation and human-written text. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 139?147.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 148?156.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies,
pages 41?44.
Micha Elsner and Eugene Charniak. 2011a. Disentan-
gling chat with local coherence models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1179?1189.
Micha Elsner and Eugene Charniak. 2011b. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 125?129.
Hugo J. Escalante, Thamar Solorio, and Manuel Montes-
y Go?mez. 2011. Local Histograms of Character N-
grams for Authorship Attribution. In Proceedings of
the 49th Annual Meeting on Association for Computa-
tional Linguistics, pages 288?298.
Alan J. Faller. 1981. An Average Correlation Coeffi-
cient. Journal of Applied Meteorology.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-
ceedings of the 4th Web as Corpus Workshop.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse processes,
25(2):285?308.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational linguistics,
21(2):203?225.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English . Longman Pub Group.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceedings
of the 7th International Workshop on Computational
Semantics, pages 1?12.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing, pages 133?142.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In Proceedings of the Association for Computa-
tional Linguistics 2010 System Demonstrations, pages
30?35.
42
John Lafferty and Guy Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129?163.
Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.
2003. Automated scoring and annotation of essays
with the Intelligent Essay Assessor. In M.D. Shermis
and J.C. Burstein, editors, Automated essay scoring: A
cross-disciplinary perspective, pages 87?112.
Guy Lebanon, Yi Mao, and Joshua Dillon. 2007. The
locally weighted bag-of-words framework for docu-
ment representation. Journal of Machine Learning Re-
search, 8(10):2405?2441.
Ziheng Lin, Hwee T. Ng, and Min-Yen Kan. 2011. Auto-
matically Evaluating Text Coherence Using Discourse
Relations. In Proceedings of the 49th Annual Meeting
on Association for Computational Linguistics.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(01):25?55.
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 440?447.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554.
Donald E. Powers, Jill C. Burstein, Martin Chodorow,
Mary E. Fowles, and Karen Kukich. 2002. Stump-
ing e-rater: challenging the validity of automated essay
scoring. Computers in Human Behavior, 18(2):103?
134.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes? theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3?
21.
Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Methods and Applications of Semantic In-
dexing Workshop at the 7th International Conference
on Terminology and Knowledge Engineering, pages 1?
9. Citeseer.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 803?810.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bulletin,
87(2):245?251.
Evan J. Williams. 1959. The Comparison of Regression
Variables. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 21(2):396?399.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
43
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 242?250,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HOO 2012 Error Recognition and Correction Shared Task:
Cambridge University Submission Report
Ekaterina Kochmar
Computer Laboratory
University of Cambridge
ek358@cl.cam.ac.uk
?istein Andersen
iLexIR Ltd
Cambridge
and@ilexir.co.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Previous work on automated error recognition
and correction of texts written by learners of
English as a Second Language has demon-
strated experimentally that training classifiers
on error-annotated ESL text generally outper-
forms training on native text alone and that
adaptation of error correction models to the
native language (L1) of the writer improves
performance. Nevertheless, most extant mod-
els have poor precision, particularly when at-
tempting error correction, and this limits their
usefulness in practical applications requiring
feedback.
We experiment with various feature types,
varying quantities of error-corrected data, and
generic versus L1-specific adaptation to typi-
cal errors using Na??ve Bayes (NB) classifiers
and develop one model which maximizes pre-
cision. We report and discuss the results for
8 models, 5 trained on the HOO data and
3 (partly) on the full error-coded Cambridge
Learner Corpus, from which the HOO data is
drawn.
1 Introduction
The task of detecting and correcting writing errors
made by learners of English as a Second Language
(ESL) has recently become a focus of research.
The majority of previous papers in this area
have presented machine learning methods with mod-
els being trained on well-formed native English
text (Eeg-Olofsson and Knutsson, 2003; De Felice
and Pulman, 2008; Gamon et al, 2008; Han et al,
2006; Izumi et al, 2003; Tetreault and Chodorow,
2008; Tetreault et al, 2010). However, some recent
approaches have explored ways of using annotated
non-native text either by incorporating error-tagged
data into the training process (Gamon, 2010; Han
et al, 2010), or by using native language-specific
error statistics (Rozovskaya and Roth, 2010b; Ro-
zovskaya and Roth, 2010c; Rozovskaya and Roth,
2011). Both approaches show improvements over
the models trained solely on well-formed native text.
Training a model on error-tagged non-native
text is expensive, as it requires large amounts of
manually-annotated data, not currently publically
available. In contrast, using native language-specific
error statistics to adapt a model to a writer?s first or
native language (L1) is less restricted by the amount
of training data.
Rozovskaya and Roth (2010b; 2010c) show that
adapting error corrections to the writer?s L1 and in-
corporating artificial errors, in a way that mimics
the typical error rates and confusion patterns of non-
native text, improves both precision and recall com-
pared to classifiers trained on native data only. The
approach proposed in Rozovskaya and Roth (2011)
uses L1-specific error correction patterns as a dis-
tribution on priors over the corrections, incorporat-
ing the appropriate priors into a generic Na??ve Bayes
(NB) model. This approach is both cheaper to im-
plement, since it does not require a separate classi-
fier to be trained for every L1, and more effective,
since the priors condition on the writer?s L1 as well
as on the possible confusion sets.
Some extant approaches have achieved good re-
sults on error detection. However, error correction
is much harder and on this task precision remains
242
low. This is a disadvantage for applications such
as self-tutoring or writing assistance, which require
feedback to the user. A high proportion of error-
ful suggestions is likely to further confuse learners
and/or non-native writers rather than improve their
writing or assist learning. Instead a system which
maximizes precision over recall returning accurate
suggestions for a small proportion of errors is likely
to be more helpful (Nagata and Nakatani, 2010).
In section 2 we describe the data used for train-
ing and testing the systems we developed. In sec-
tion 3 we describe the preprocessing of the ESL text
undertaken to provide a source of features for the
classifiers. We also discuss the feature types that
we exploit in our classifiers. In section 4 we de-
scribe and report results for a high precision system
which makes no attempt to generalize from train-
ing data. In section 5 we describe our approach to
adapting multiclass NB classifiers to characteristic
errors and L1s. We also report the performance of
some of these NB classifiers on the training and test
data. In section 6 we report the official results of
all our submitted runs on the test data and also on
the HOO training data, cross-validated where appro-
priate. Finally, we briefly discuss our main results,
further work, and lessons learnt.
2 Cambridge Learner Corpus
The Cambridge Learner Corpus1 (CLC) is a large
corpus of learner English. It has been developed
by Cambridge University Press in collaboration with
Cambridge Assessment, and contains examination
scripts written by learners of English from 86 L1
backgrounds. The scripts have been produced by
language learners taking Cambridge Assessment?s
ESL examinations.2
The linguistic errors committed by the learners
have been manually annotated using a taxonomy of
86 error types (Nicholls, 2003). Each error has been
manually identified and tagged with an appropriate
code, specifying the error type, and a suggested cor-
rection. Additionally, the scripts are linked to meta-
data about examination and learner. This includes
the year of examination, the question prompts, the
1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/
custom/item3646603/Cambridge-International-Corpus-
Cambridge-Learner-Corpus
2http://www.cambridgeesol.org/
learner?s L1, as well as the grades obtained. The cur-
rent version of the CLC contains about 20M words
of error-annotated scripts from a wide variety of ex-
aminations.
The HOO training and test datasets are drawn
from the CLC. The training dataset is a reformatted
1000-script subset of a publically-available subset of
CLC scripts produced by learners sitting the First
Certficate in English (FCE) examination.3 This ex-
amination assesses English at an upper-intermediate
level, so many learners sitting this exam still man-
ifest a number of errors motivated by the conven-
tions of their L1s. The CLC-FCE subcorpus was ex-
tracted, anonymized, and made available as a set of
XML files by Yannakoudakis et al (2011).4
The HOO training dataset contains scripts from
FCE examinations undertaken in the years 2000 and
2001 written by speakers of 16 L1s. These scripts
can be divided into two broad L1 typological groups,
Asian (Chinese, Thai, Korean, Japanese) and Euro-
pean (French, Spanish, Italian, Portuguese, Catalan,
Greek, Russian, Polish). The latter can be further
subdivided into Slavic (Russian, Polish) and Ro-
mance. In turn, the Romance languages differ in ty-
pological relatedness with, for example, Portuguese
and Spanish being closer than Spanish and French.
Error coding which is not relevant to preposition or
determiner errors has been removed from the train-
ing data so that only six error type annotations are
retained for training: incorrect, missing or unnec-
essary determiners (RD, MD, UD) and prepositions
(RT, MT, UT).
One consequence of this reformatting is that the
contexts of these errors often contain further errors
of different types that are no longer coded. The idea
is that errors should be considered in their natural
habitat, and that correcting and copy-editing the sur-
rounding text would create an artificial task. On the
other hand, not correcting anything makes it difficult
in some cases and nigh impossible in others to de-
termine whether a given determiner or preposition is
correct or not. The error-coding in the CLC in such
cases (provided the writer?s intent is deemed recov-
erable) depends not only on the original text, but also
on the correction of nearby errors.
3http://www.cambridgeesol.org/exams/general-english/fce.
html
4http://ilexir.co.uk/applications/clc- fce-dataset/
243
Certain errors even appear as a direct result of
correcting others: for instance, the phrase to sleep
in tents has been corrected to to sleep in a tent in
the CLC; this ends up as a ?correction? to to sleep
in a tents in the HOO dataset. This issue is diffi-
cult to avoid given that the potential solutions are all
labour-intensive (explicit indication of dependencies
between error annotations, completely separate error
annotation for different types of errors, or manual re-
moval of spurious errors after extraction of the types
of error under consideration), and we mention it here
mainly to explain the origin of some surprising an-
notations in the dataset.
A more HOO-specific problem is the ?[removal
of] elements [from] some of [the] files [...] to
dispose of nested edits and other phenomena that
caused difficulties in the preprocessing of the data?
(Dale et al, 2012). This approach unfortunately
leads to mutilated sentences such as I think if we
wear thistoevery wherespace ships. This mean. re-
placing the original I think if we wear this clothes we
will travel to every where easier than we use cars,
ships, planes and space ships. This mean the engi-
neering will find the way to useless petrol for it, so it
must useful in the future.
The HOO test set consists of 100 responses to
individual prompts from FCE examinations set be-
tween 1993 and 2009, also drawn from the CLC.
As a side effect of removing the test data from the
full CLC, we have discovered that the distribution of
L1s, examination years and exam prompts is differ-
ent from the training data. There are 27 L1s exem-
plified, a superset of the 16 seen in the HOO train-
ing data; about half are Romance, and the rest are
widely distributed with Asian and Slavic languages
less well represented than in the training data.
In the experiments reported below, we make use
of both the HOO training data and the full 20M
words of error-annotated CLC, but with the HOO
test data removed, to train our systems. Whenever
we use the larger training set we refer to this as the
full CLC below.
3 Data Preprocessing
We parsed the training and test data (see Section
2) using the Robust Accurate Statistical Parsing
(RASP) system with the standard tokenization and
My friend was (MD: a) good student
Grammatical Relations (GRs):
(ncsubj be+ed:3 VBDZ friend:2 NN1 )
(xcomp be+ed:3 VBDZ student:6 NN1)
(ncmod student:6 NN1 good:5 JJ)
(det friend:2 NN1 My:1 APP$)
*(det student:6 NN1 a:4 AT1)
Figure 1: RASP GR output
sentence boundary detection modules and the unlex-
icalized version of the parser (Briscoe et al, 2006)
in order to broaden the space of candidate fea-
tures types. The features used in our experiments
are mainly motivated by the fact that lexical and
grammatical features have been shown in previous
work to be effective for error detection and correc-
tion. We believe RASP is an appropriate tool to
use with ESL text because the PoS tagger deploys
a well-developed unknown word handling mecha-
nism, which makes it relatively robust to noisy in-
put such as misspellings, and because the parser de-
ploys a hand-coded grammar which indicates un-
grammaticality of sentences and markedness of con-
structions and is encoded entirely in terms of PoS
tag sequences. We utilize the open-source version
of RASP embedded in an XML-handling pipeline
that allows XML-encoded metadata in the CLC and
HOO training data to be preserved in the output,
but ensures that unannotated text is passed to RASP
(Andersen et al, 2008).
Relevant output of the system is shown in Fig-
ure 1 for a typical errorful example. The grammati-
cal relations (GRs) form a connected, directed graph
of typed bilexical head-dependent relations (where a
non-fragmentary analysis is found). Nodes are lem-
matized word tokens with associated PoS tag and
sentence position number. Directed arcs are labelled
with GR types. In the factored representation shown
here, each line represents a GR type, the head node,
the dependent node, and optional subtype informa-
tion either after the GR type or after the dependent.
In this example, the asterisked GR would be missing
in the errorful version of the sentence. We extract the
most likely analysis for each sentence based on the
most probable tag sequence found by the tagger.
Extraction of the lexical and grammatical infor-
244
mation from the parser output is easier when a deter-
miner or preposition is present than when it is miss-
ing. During training, for all nouns, we checked for a
det relation to a determiner, and whenever no det
GR is present, we checked whether the noun is pre-
ceded by an MD annotation in the XML file. For
missing prepositions, we have only extracted cases
where a noun is governed by a verb with a dobj
relation, and cases where a noun is governed by an-
other noun with an ncmod (non-clausal modifier)
relation. For example, in It?s been a long time since
I last wrote you, in absence of the preposition to the
parser would ?recognize? a dobj relation between
you and wrote, and this case would be used as a
training example for a missing preposition, while I
trusted him with the same dobj relation between
trusted and him would be used as a training exam-
ple to correct unwanted use of a preposition as in I
trusted *to him.
3.1 Feature Types
In all the experiments and system configurations
described below, we used a similar set of features
based on the following feature templates.
For determiner errors:
? Noun lemma: lemma of the noun that gov-
erns the determiner
? Noun PoS: PoS tag of the noun
? Distance from Noun: distance in num-
ber of words to the governed determiner
? Head lemma: head lemma in the shortest
grammatical relation in which the noun is de-
pendent
? Head PoS: as defined above, but with PoS tag
rather than lemma
? Distance from Head: distance in num-
ber of words to the determiner from head, as
defined above (for Head lemma)
? GR type to Noun: a GR between Head
and Noun.
For instance for the example shown in Figure 1, the
noun lemma is student, the noun PoS is NN1, the
distance from the noun is 2, the head lemma is be,
the head PoS is VBDZ, and the distance from the
head is 1, while the GR type to the noun is xcomp.
For preposition errors:
? Preposition (P): target preposition
? Head lemma (H): head lemma of the GR in
which the preposition is dependent
? Dependent lemma (D): dependent
lemma of the GR in which the preposition is
head.
For instance, in I am looking forward to your reply,
P is to, H is look and D is reply.
In contrast to work by Rozovskaya and Roth,
amongst others, we have not used word context fea-
tures, but instead focused on grammatical context in-
formation for detecting and correcting errors. We
also experimented with some other feature types,
such as n-grams consisting of the head, preposition
and dependent lemmas, but these did not improve
performance on the cross-validated HOO training
data, perhaps because they are sparser and the train-
ing set is small. However, there are many other po-
tential feature types, such as PoS n-grams or syn-
tactic rule types, and so forth that we don?t explore
here, despite their probable utility. Our main focus
in these experiments is not on optimal feature engi-
neering but rather on the issues of classifier adaption
to errors and high precision error correction.
4 A Simple High Precision Correction
System
We have experimented with a number of approaches
to maximizing precision and have not outperformed
a simple model that doesn?t generalize from the
training data using machine learning techniques. We
leverage the large amount of error-corrected text in
the full CLC to learn reliable contexts in which er-
rors occur and their associated corrections. For the
HOO shared task, we tested variants of this approach
for missing determiner (MD) and incorrect prepo-
sition (RT) errors. Better performing features and
thresholds used to define contexts were found by
testing variants on the HOO training data. The fea-
ture types from section 3.1 deployed for the MD
system submitted for the official run were Noun
245
lemma, Noun PoS, GR types to Noun and
GR types from Noun (set of GRs which has
the noun as head). For the RT system, all three P, H,
and D features were used to define contexts. A con-
text is considered reliable if it occurs at least twice
in the full CLC and more than 75% of the time it
occurs with an error.
The performance of this system on the training
data was very similar to performance on the test data
(in contrast to our other runs). We also explored L1-
specific and L1-group variants of these systems; for
instance, we split the CLC data into Asian and Eu-
ropean languages, trained separate systems on each,
and then applied them according to the L1 meta-
data supplied with the HOO training data. However,
all these systems performed worse than the best un-
adapted system.
The results for the generic, unadapted MD and RT
systems appear as run 0 in Tables 4?9 below. These
figures are artefactually low as we don?t attempt to
detect or correct UD, UT, RD or MT errors. The
actual results computed from the official runs solely
for MD errors are for detection, recognition and cor-
rection: 83.33 precision and 7.63 recall, which gives
an F-measure of 13.99; the RT system performed at
66.67 precision, 8.05 recall and 14.37 F-measure on
the detection, recognition and correction tasks. De-
spite the low recall, this was our best submitted sys-
tem in terms of official correction F-score.
5 Na??ve Bayes (NB) (Un)Adapted
Multiclass Classifiers
Rozovskaya and Roth (2011) demonstrate on a
different dataset that Na??ve Bayes (NB) can out-
perform discriminative classifiers on preposition
error detection and correction if the prior is adapted
to L1-specific estimates of error-correction pairs.
They compare the performance of an unadapted
NB multiclass classifier, in which the prior for a
preposition is defined as the relative probability
of seeing a specific preposition compared to a
predefined subset of the overall PoS class (which
they call the Conf(usion) Set):
prior(p) =
C(p)
?
q?ConfSet C(q)
,
to the performance of the same NB classfier with
an adapted prior which calculates the probability of
a correct preposition as:
prior(c, p,L1) =
CL1(p, c)
CL1(p)
,
where CL1(p) is the number of times preposition
p is seen in texts written by learners with L1 as
their native language, and CL1(p, c) is the number
of times c is the correct preposition when p is used.
We applied Rozovskaya and Roth?s approach to
determiners as well as prepositions, and experi-
mented with priors calculated in the same way for
L1 groups as well as specific L1s. We also com-
pared L1-adaptation to generic adaption to correc-
tions, calculated as:
prior(c, p) =
C(p, c)
C(p)
,
We have limited the set of determiners and prepo-
sitions that our classifiers aim to detect and correct,
if necessary. Our confusions sets contain:
? Determiners: no determiner, the, a, an;
? Prepositions: no preposition, in, of, for,
to, at, with, on, about, from, by, after.
Therefore, for determiners, our systems were only
aimed at detecting and correcting errors in the use of
articles, and we have not taken into account any er-
rors in the use of possessive pronouns (my, our, etc.),
demonstratives (this, those, etc.), and other types of
determiners (any, some, etc.). For prepositions, it is
well known that a set of about 10 of the most fre-
quent prepositions account for more than 80% of all
prepositional usage (Gamon, 2010).
We have calculated the upper bounds for the train-
ing and test sets when the determiner and preposi-
tion confusion sets are limited this way. The upper
bound recall for recognition (i.e., ability of the clas-
sifier to recognize that there is an error, dependent on
the fact that only the chosen determiners and prepo-
sitions are considered) is calculated as the propor-
tion of cases where the incorrect, missing or unnec-
essary determiner or preposition is contained in our
confusion set. For the training set, it is estimated at
91.95, and for the test at 93.20. Since for correction,
246
the determiner or preposition suggested by the sys-
tem should also be contained in our confusion set,
upper bound recall for correction is slightly lower
than that for recognition, and is estimated at 86.24
for the training set, and at 86.39 for the test set.
These figures show that the chosen candidates dis-
tribute similarly in both datasets, and that a system
aimed at recognition and correction of only these
function words can obtain good performance on the
full task.
The 1000 training scripts were divided into 5 por-
tions pseudo-randomly to ensure that each portion
contained approximately the same number of L1-
specific scripts in order not to introduce any L1-
related bias. The results on the training set pre-
sented below were averaged across 5 runs, where in
each run 4 portions (about 800 scripts) were used
for training, and one portion (about 200 scripts) was
used for testing.
We treated the task as multi-class classification,
where the number of classes equates to the size of
our confusion set, and when the classifier?s decision
is different from the input, it is considered to be er-
rorful. For determiners, we used the full set of fea-
tures described in section 3.1, whereas for preposi-
tions, we have tried two different feature sets: only
head lemma (H), or H with the dependent lemma (D).
We ran the unadapted and L1-adapted NB classi-
fiers on determiners and prepositions using the fea-
tures defined above. The results of these preliminary
experiments are presented below.
5.1 Unadapted and L1-adapted NB classifiers
Tables 1 to 3 below present results averaged over
the 5 runs for the unadapted classifiers. We report
the results in terms of recall, precision and F-score
for detection, recognition and correction of errors as
defined for the HOO shared task.5
We have experimented with two types of L1-
specific classification: classifier1 below is a
combination of 16 separate multiclass NB classi-
fiers, each trained on a specific L1 and applied to
the corresponding parts of the data. Classifier2
is a replication of the classifier presented in Ro-
zovskaya and Roth (2011), which uses the priors
5For precise definitions of these measures see
www.correcttext.org/hoo2012
adapted to the writer?s L1 and to the chosen deter-
miner or preposition at decision time. The priors
used for these runs were estimated from the HOO
training data.
We present only the results of the systems that use
H+D features for prepositions, since these systems
outperform systems using H only. Tables 1, 2 and
3 below show the comparative results of the three
classifiers averaged over 5 runs, with all errors, de-
terminer errors only, and preposition errors only, re-
spectively.
Detection Recognition Correction
R P F R P F R P F
U 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.85
C1 64.51 16.17 25.85 50.25 12.56 20.10 30.95 7.74 12.39
C2 33.74 16.51 22.15 28.50 13.96 18.72 16.51 8.10 10.85
Table 1: All errors included. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
Detection Recognition Correction
R P F R P F R P F
U 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.83
C1 61.19 20.25 30.42 52.20 17.27 25.94 40.57 13.43 20.17
C2 40.56 15.88 22.81 37.24 14.58 20.94 23.20 9.08 13.04
Table 2: Determiner errors. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
Detection Recognition Correction
R P F R P F R P F
U 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.84
C1 66.96 13.86 22.97 48.51 10.05 16.65 22.70 4.70 7.79
C2 27.45 17.06 21.00 21.00 13.07 16.09 10.79 6.73 8.27
Table 3: Preposition errors. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
The results show some improvement with a com-
bination of classifiers trained on L1-subsets in terms
of recall for detection and recognition of errors, and
a slight improvement in precision using L1-specific
priors for preposition errors. However, in general,
unadapted classifiers outperform L1-adapted classi-
fiers with identical feature types. Therefore, we have
not included L1-specific classifiers in the submitted
set of runs.
5.2 Submitted systems
For the official runs, we trained various versions of
the unadapted and generic adapted NB classifiers.
247
We trained all the adapted priors on the full CLC
dataset in the expectation that this would yield more
accurate estimates. We trained the unadapted priors
and the NB features as before on the HOO training
dataset. We also trained the NB features on the full
CLC dataset and tested the impact of the preposi-
tion feature D (dependent lemma of the GR from the
preposition, i.e., the head of the preposition comple-
ment) with the different training set sizes. For all
runs we used the full set of determiner features de-
scribed in section 3.1.
The full set of multiclass NB classifiers submitted
is described below:
? Run1: unadapted, trained on the HOO data. H
feature for prepositions;
? Run2: unadapted, trained on the HOO data. H
and D features for prepositions;
? Run3: a combination of the NB classifiers
trained for each of the used candidate words
separately. H and D features are used for prepo-
sitions;
? Run4: generic adapted, trained on HOO data.
H feature for prepositions;
? Run5: generic adapted, trained on HOO data.
H and D features for prepositions;
? Run6: unadapted, trained on the full CLC. H
feature for prepositions;
? Run7: unadapted, trained on the full CLC. H
and D features for prepositions.
The classifiers used for runs 1 and 2 differ from
the ones used for runs 6 and 7 only in the amount
of training data. None of these classifiers involve
any adaptation. The classifiers used for runs 4 and
5 involve prior adaptation to the input determiner
or preposition, adjusted at decision time. In run
3, a combination of classifiers trained on the input
determiner- or preposition-specific partitions of the
HOO training data are used. At test time, the appro-
priate classifier from this set is applied depending on
the preposition or determiner chosen by the learner.
To limit the number of classes for the classifiers
used in runs 1?3 and 6?7, we have combined the
training cases for determiners a and an in one class
a/an; after classification one of the variants is chosen
depending on the first letter of the next word. How-
ever, for the classifiers used in runs 4?5, we used
priors including confusions between a and an.
The results for these runs on the training data are
shown in Tables 4 to 6 below.
Detection Recognition Correction
R P F R P F R P F
0 5.54 81.08 10.37 5.32 77.95 9.97 4.90 71.70 9.17
1 60.14 18.57 28.37 48.21 14.88 22.74 32.71 10.09 15.43
2 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.85
3 50.09 27.54 35.52 45.99 25.23 32.57 28.78 15.80 20.39
4 25.39 25.48 25.39 22.10 22.23 22.13 12.23 12.33 12.26
5 31.17 22.33 25.94 26.28 18.88 21.90 14.50 10.46 12.11
6 62.41 10.73 18.31 49.95 8.57 14.63 32.66 5.60 9.57
7 62.92 11.60 19.59 52.29 9.61 16.24 34.32 6.31 10.66
Table 4: Training set results, all errors
Detection Recognition Correction
R P F R P F R P F
0 5.02 82.98 9.46 5.02 82.98 9.46 4.81 79.57 9.07
1?2 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.83
3 58.50 62.22 60.22 57.41 61.07 59.11 46.33 49.25 47.68
4?5 34.93 31.09 32.68 33.66 30.01 31.52 19.74 17.66 18.51
6?7 58.65 8.11 14.24 53.90 7.43 13.06 40.61 5.60 9.84
Table 5: Training set results, determiner errors
Detection Recognition Correction
R P F R P F R P F
0 5.87 78.30 10.93 5.59 74.49 10.40 4.97 66.28 9.25
1 64.71 14.04 23.06 46.54 10.11 16.61 25.86 5.61 9.22
2 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.84
3 42.63 16.53 23.81 36.18 14.04 20.22 13.74 5.35 7.70
4 16.85 19.27 17.97 12.24 14.03 13.06 5.81 6.67 6.21
5 27.49 16.89 20.88 19.96 12.30 15.19 10.03 6.20 7.65
6 64.69 14.03 23.06 46.51 10.10 16.60 25.83 5.61 9.22
7 65.68 16.89 26.87 50.87 13.09 20.82 28.92 7.44 11.84
Table 6: Training set results, preposition errors
The results on the training data show that use
of the D feature improves the performance of all
the preposition classifiers. Use of the full CLC for
training improves recall, but does not improve pre-
cision for prepositions, while for determiners pre-
cision of the classifiers trained on the full CLC
is much worse. Adaptation of the classifiers with
determiner/preposition-specific priors slightly im-
proves precision on prepositions, but is damaging
for recall. Therefore, in terms of F-score, unadapted
classifiers outperform adapted ones. The over-
all best-performing system on the cross-validated
training data is Run3, which is trained on the
determiner/preposition-specific data subsets and ap-
248
plies an input-specific classifier to test data. How-
ever, the result is due to improved performance on
determiners, not prepositions.
6 Official Evaluation Results
The results presented below are calculated using the
evaluation tool provided by the organizers, imple-
menting the scheme specified in the HOO shared
task. The results on the test set, presented in Ta-
bles 7?9 are from the final official run after correc-
tion of errors in the annotation and score calculation
scripts.
Detection Recognition Correction
R P F R P F R P F
0 4.86 76.67 9.15 4.65 73.33 8.75 4.65 73.33 8.75
1 34.46 13.04 18.92 22.83 8.64 12.54 13.53 5.12 7.43
2 35.73 14.04 20.16 23.47 9.22 13.24 12.26 4.82 6.92
3 19.24 12.10 14.86 14.59 9.18 11.27 5.71 3.59 4.41
4 9.51 14.95 11.63 7.19 11.30 8.79 5.29 8.31 6.46
5 15.43 14.31 14.85 10.78 10.00 10.38 6.77 6.28 6.51
6 55.60 11.15 18.58 41.86 8.40 13.99 28.54 5.73 9.54
7 56.66 11.59 19.24 42.49 8.69 14.43 27.27 5.58 9.26
Table 7: Test set results, all errors
Detection Recognition Correction
R P F R P F R P F
0 4.37 83.33 8.30 4.37 83.33 8.30 4.37 83.33 8.30
1?2 8.73 7.61 8.13 4.80 4.18 4.47 4.37 3.80 4.07
3 6.11 11.29 7.93 5.24 9.68 6.80 5.24 9.68 6.80
4?5 6.11 9.72 7.51 4.80 7.64 5.90 4.80 7.64 5.90
6?7 51.09 8.53 14.63 44.10 7.37 12.63 35.37 5.91 10.13
Table 8: Test set results, determiner errors
Detection Recognition Correction
R P F R P F R P F
0 5.33 72.22 9.92 4.92 66.67 9.16 4.92 66.67 9.16
1 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.77
2 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.10
3 29.10 11.31 16.28 23.36 9.08 13.07 6.15 2.39 3.44
4 12.71 19.75 15.46 9.43 14.65 11.47 5.74 8.92 6.98
5 24.18 16.12 19.34 16.39 10.93 13.12 8.61 5.74 6.89
6 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.77
7 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.10
Table 9: Test set results, preposition errors
The test set results for NB classifiers (Runs 1?
7) are significantly worse than our preliminary re-
sults obtained on the training data partitions, espe-
cially for determiners. Use of additional training
data (Runs 6 and 7) improves recall, but does not im-
prove precision. Adaptation to the input preposition
improves precision as compared to the unadapted
classifier for prepositions (Run 4), whereas training
on the determiner-specific subsets improves preci-
sion for determiners (Run 3). However, generally
these results are worse than the results of the similar
classifiers on the training data subsets.
We calculated the upper bound recall for our clas-
sifiers on the test data. The upper bound recall on
the test data is 93.20 for recognition, and 86.39 for
correction, given our confusion sets for both deter-
miners and prepositions. However, the actual upper
bound recall is 71.82, with upper bound recall on
determiners at 71.74 and on prepositions at 71.90,
because 65 out of 230 determiner errors, and 68 out
of 243 preposition errors are not considered by our
classifiers, primarily because when the parser fails to
find a full analysis, the grammatical context is often
not recovered accurately enough to identify missing
input positions or relevant GRs. This is an inher-
ent weakness of using only parser-extracted features
from noisy and often ungrammatical input. Taking
this into account, some models (Runs 1, 2, 6 and 7)
achieved quite high recall.
We suspect the considerable drop in precision is
explained by the differences in the training and test
data. The training set contains answers from learners
of a smaller group of L1s from one examination year
to a much more restricted set of prompts. The well-
known weaknesses of generative NB classifiers may
prevent effective exploitation of the additional infor-
mation in the full CLC over the HOO training data.
Experimentation with count weighting schemes and
optimized interpolation of adapted priors may well
be beneficial (Rennie et al, 2003).
Acknowledgements
We thank Cambridge ESOL, a division of Cam-
bridge Assessment for a partial grant to the first au-
thor and a research contract with iLexIR Ltd. We
also thank them and Cambridge University Press for
granting us access to the CLC for research purposes.
References
?istein Andersen. 2011 Semi-automatic ESOL error
annotation. English Profile Journal, vol2:e1. DOI:
10.1017/S2041536211000018, Cambridge University
Press.
?istein Andersen, Julian Nioche, Ted Briscoe, and
John Carroll. 2008 The BNC parsed with
249
RASP4UIMA. In 6th Int. Conf. on Language Re-
sources and Evaluation (LREC), Marrakech, Moroc-
cco
Ted Briscoe, John A. Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL, vol 6.
Robert Dale, Ilya Anisimoff and George Narroway 2012
HOO 2012: A Report on the Preposition and Deter-
miner Error Correction Shared Task. In Proceedings
of the Seventh Workshop on Innovative Use of NLP for
Building Educational Applications Montreal, Canada,
June.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), pages 169?176,
Manchester, UK, -August.
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing: A Meta-Classifier
Approach. In Proceedings of NAACL 2010, pages
163?171, Los Angeles, USA, June.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by non-
native speakers. Journal of Natural Language Engi-
neering, 12(2):115?129.
Na-Rae Han, Joel R. Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an Error-Annotated Learner
Corpus to Develop an ESL/EFL Error Correction
System. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC-10), Valletta, Malta, May.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Automatic
error detection in the Japanese learners? English spo-
ken data. In The Companion Volume to the Proceed-
ings of 41st Annual Meeting of the Association for
Computational Linguistics, pages 145?148, Sapporo,
Japan, July.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Language by Error Analysis University of
Cambridge, MPhil Dissertation.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
John Lee and Stephanie Seneff. 2008. An analysis of
grammatical errors in non-native speech in English. In
Proceedings of the 2008 Spoken Language Technology
Workshop.
Ryo Nagata and Kazuhide Nakatani. 2010 Evaluating
performance of grammatical error detection to max-
imize learning effect. In Proceedings of Int. Conf.
on Computational Linguistics (Coling-10), Poster Ses-
sion, pages 894?900, Beijing, China.
Diane Nicholls. 2003. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics conference,
pages 572?581.
Jason Rennie, Lawrence Shih, Jaime Teevan, and
David Karger. 2003 Tackling the Poor Assumtions
of Naive Bayes Text Classifiers. 20th Int. Conference
on Machine Learning (ICML-2003) Washington, DC
Alla Rozovskaya and Dan Roth. 2010a. Annotating ESL
Errors: Challenges and Rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
Alla Rozovskaya and Dan Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Alla Rozovskaya and Dan Roth. 2010c. Training
Paradigms for Correcting Errors in Grammar and Us-
age. In Proceedings of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL).
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the Annual Meeting of the Associa-
tion of Computational Linguistics (ACL).
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 865?872, Manchester, UK, August.
Joel R. Tetreault, Jennifer Foster, Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In ACL.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
250
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68?77,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Looking for Hyponyms in Vector Space
Marek Rei
SwiftKey
95 Southwark Bridge Rd
London, UK
marek@swiftkey.net
Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
ted.briscoe@cl.cam.ac.uk
Abstract
The task of detecting and generating hy-
ponyms is at the core of semantic under-
standing of language, and has numerous
practical applications. We investigate how
neural network embeddings perform on
this task, compared to dependency-based
vector space models, and evaluate a range
of similarity measures on hyponym gener-
ation. A new asymmetric similarity mea-
sure and a combination approach are de-
scribed, both of which significantly im-
prove precision. We release three new
datasets of lexical vector representations
trained on the BNC and our evaluation
dataset for hyponym generation.
1 Introduction
Hyponymy is a relation between two word senses,
indicating that the meaning of one word is also
contained in the other. It can be thought of as a
type-of relation; for example car, ship and train
are all hyponyms of vehicle. We denote a hy-
ponymy relation between words a and b as (a? b),
showing that a is a hyponym of b, and b is a hyper-
nym of a. Hyponymy relations are closely related
to the concept of entailment, and this notation is
consistent with indicating the direction of infer-
ence ? if a is true, b must be true as well.
Automatic detection and generation of hy-
ponyms has many practical applications in nearly
all natural language processing tasks. Information
retrieval, information extraction and question an-
swering can be improved by performing appropri-
ate query expansions. For example, a user search-
ing for arthritis treatment is most likely also inter-
ested in results containing the hyponyms of treat-
ment, such as arthritis therapy, arthritis medica-
tion, and arthritis rehabilitation. Summarisation
systems can increase coherence and reduce repe-
tition by correctly handling hyponymous words in
the input text. Entailment and inference systems
can improve sentence-level entailment resolution
by detecting the presence and direction of word-
level hyponymy relations. Distributionally simi-
lar words have been used for smoothing language
models and word co-occurrence probabilities (Da-
gan et al., 1999; Weeds and Weir, 2005), and hy-
ponyms can be more suitable for this application.
We distinguish between three different tasks
related to hyponyms. Given a directional word
pair, the goal of hyponym detection is to deter-
mine whether one word is a hyponym of the other
(Zhitomirsky-Geffet and Dagan, 2009; Kotlerman
et al., 2010; Baroni and Lenci, 2011). In con-
trast, hyponym acquisition is the task of extract-
ing all possible hyponym relations from a given
text (Hearst, 1992; Caraballo, 1999; Pantel and
Ravichandran, 2004; Snow et al., 2005). Such sys-
tems often make use of heuristic rules and patterns
for extracting relations from surface text, and pop-
ulate a database with hyponymous word pairs. Fi-
nally, the task of hyponym generation is to re-
turn a list of all possible hyponyms, given only
a single word as input. This is most relevant to
practical applications, as many systems require a
set of appropriate substitutes for a specific term.
Automated ontology creation (Biemann, 2005) is
a related field that also makes use of distributional
similarity measures. However, it is mostly focused
on building prototype-based ontologies through
clustering (Ushioda, 1996; Bisson et al., 2000;
Wagner, 2000; Paa? et al., 2004; Cimiano and
Staab, 2005), and is not directly applicable to hy-
ponym generation.
While most work has been done on hyponym
detection (and the related task of lexical substitu-
tion), barely any evaluation has been done for hy-
ponym generation. We have found that systems for
hyponym detection often perform poorly on hy-
ponym generation, as the latter requires returning
results from a much less restricted candidate set,
68
and therefore a task-specific evaluation is required.
In this paper we focus on hyponym generation
and approach it by scoring a very large candidate
set of potential hyponyms. Distributional similar-
ity methods are especially interesting for this task,
as they can be easily applied to different domains,
genres and languages without requiring annotated
training data or manual pattern construction. We
perform a systematic comparison of different vec-
tor space models and similarity measures, in order
to better understand the properties of a successful
method for hyponym generation.
The main contributions of this paper are:
1. Systematic evaluation of different vector
space models and similarity measures on the
task of hyponym generation.
2. Proposal of new properties for modelling the
directional hyponymy relation.
3. Release of three lexical vector datasets,
trained using neural network, window-based,
and dependency-based features.
2 Vector space models
In order to use similarity measures for hyponym
detection, every word needs to be mapped to a
point in vector space. The method of choosing
appropriate features for these vectors is crucial to
achieving the optimal performance. We compare
five different approaches:
Window: As a simple baseline, we created vec-
tors by counting word co-occurrences in a fixed
context window. Every word that occurs within a
window of three words before or after is counted
as a feature for the target word. Pointwise mutual
information is then used for weighting.
CW: Collobert and Weston (2008) constructed
a neural network language model that is trained to
predict the next word in the sequence, and simul-
taneously learns vector representations for each
word. The vectors for context words are concate-
nated and used as input for the neural network,
which uses a sample of possible outputs for gra-
dient calculation to speed up the training process.
Turian et al. (2010) recreated their experiments
and made the vectors available online.
1
HLBL: Mnih and Hinton (2007) created word
representations using the hierarchical log-bilinear
1
http://metaoptimize.com/projects/wordreprs/
model ? a neural network that takes the concate-
nated vectors of context words as input, and is
trained to predict the vector representation of the
next word, which is then transformed into a prob-
ability distribution over possible words. To speed
up training and testing, they use a hierarchical data
structure for filtering down the list of candidates.
Both CW and HLBL vectors were trained using
37M words from RCV1.
Word2vec: We created word representations
using the word2vec
2
toolkit. The tool is based
on a feedforward neural network language model,
with modifications to make representation learn-
ing more efficient (Mikolov et al., 2013a). We
make use of the skip-gram model, which takes
each word in a sequence as an input to a log-linear
classifier with a continuous projection layer, and
predicts words within a certain range before and
after the input word. The window size was set to
5 and vectors were trained with both 100 and 500
dimensions.
Dependencies: Finally, we created vector rep-
resentations for words by using dependency rela-
tions from a parser as features. Every incoming
and outgoing dependency relation is counted as a
feature, together with the connected term. For ex-
ample, given the dependency relation (play, dobj,
guitar), the tuple (>dobj, guitar) is extracted as a
feature for play, and (<dobj, play) as a feature for
guitar. We use only features that occur more than
once in the dataset, and weight them using point-
wise mutual information to construct feature vec-
tors for every term. Features with negative weights
were retained, as they proved to be beneficial for
some similarity measures.
The window-based, dependency-based and
word2vec vector sets were all trained on 112M
words from the British National Corpus, with pre-
processing steps for lowercasing and lemmatis-
ing. Any numbers were grouped and substituted
by more generic tokens. For constructing the
dependency-based vector representations, we used
the parsed version of the BNC created by Ander-
sen et al. (2008) with the RASP toolkit (Briscoe
et al., 2006). When saved as plain text, the 500-
dimensional word2vec vectors and dependency-
based vectors are comparable in size (602MB and
549MB), whereas the window-based vectors are
twice as large (1,004MB). We make these vector
2
https://code.google.com/p/word2vec/
69
sets publically available for download.
3
Recently, Mikolov et al. (2013b) published in-
teresting results about linguistic regularities in
vector space models. They proposed that the rela-
tionship between two words can be characterised
by their vector offset, for example, we could find
the vector for word ?queen? by performing the op-
eration ?king - man + woman? on corresponding
vectors. They also applied this approach to hy-
ponym relations such as (shirt ? clothing) and
(bowl ? dish). We evaluate how well this method
applies to hyponym generation with each of the
vector space models mentioned above. Using the
training data, we learn a vector for the hyponymy
relation by averaging over all the offset vectors
for hyponym-hypernym pairs. This vector is then
added to the hypernym during query time, and
the result is compared to hyponym candidates us-
ing cosine similarity. For sparse high-dimensional
vector space models it was not feasible to use the
full offset vector during experiments, therefore we
retain only the top 1,000 highest-weighted fea-
tures.
3 Similarity measures
We compare the performance of a range of simi-
larity measures, both directional and symmetrical,
on the task of hyponym generation.
Cosine similarity is defined as the angle be-
tween two feature vectors and has become a stan-
dard measure of similarity between weighted vec-
tors in information retrieval (IR).
Lin similarity, created by Lin (1998), uses the
ratio of shared feature weights compared to all fea-
ture weights. It measures the weighted proportion
of features that are shared by both words.
DiceGen2 is one possible method for generalis-
ing the Dice measure to real-valued weights (Cur-
ran, 2003; Grefenstette, 1994). The dot product of
the weight vectors is normalised by the total sum
of all weights. The same formula can also be con-
sidered as a possible generalisation for the Jaccard
measure.
WeedsPrec and WeedsRec were proposed by
Weeds et al. (2004) who suggested using precision
and recall as directional measures of word simi-
larity. In this framework, the features are treated
similarly to retrieved documents in information re-
trieval ? the vector of the broader term b is used as
the gold standard, and the vector of the narrower
3
http://www.marekrei.com/projects/vectorsets/
term a is in the role of retrieval results. Precision
is then calculated by comparing the intersection
(items correctly returned) to the values of the nar-
rower term only (all items returned). In contrast,
WeedsRec quantifies how well the features of the
breader term are covered by the narrower term.
Balprec is a measure created by Szpektor and
Dagan (2008). They proposed combining Weed-
sPrec together with the Lin measure by taking
their geometric average. This aims to balance the
WeedsPrec score, as the Lin measure will penalise
cases where one vector contains very few features.
ClarkeDE, proposed by Clarke (2009), is an
asymmetric degree of entailment measure, based
on the concept of distributional generality (Weeds
et al., 2004). It quantifies the weighted coverage of
the features of the narrower term a by the features
of the broader term b.
BalAPInc, a measure described by Kotlerman
et al. (2010), combines the APInc score with Lin
similarity by taking their geometric average. The
APInc measure finds the proportion of shared fea-
tures relative to the features for the narrower term,
but this can lead to unreliable results when the
number of features is very small. The motivation
behind combining these measures is that the sym-
metric Lin measure will decrease the final score
for such word pairs, thereby balancing the results.
4 Properties of a directional measure
Finding similar words in a vector space, given
a symmetric similarity measure, is a relatively
straightforward task. However finding hyponyms
is arguably more difficult, as the relation is asym-
metric, and looking at the distance or angle be-
tween the two words may not be enough.
Kotlerman et al. (2010) investigate the related
problem of detecting directional lexical entail-
ment, and they propose three desirable properties
that a directional distributional similarity measure
should capture:
1. The relevance of the shared features to the
narrower term.
2. The relevance of the shared features to the
broader term.
3. That relevance is less reliable if the num-
ber of features of either the narrower or the
broader term is small.
70
Given a term pair (a ? b) we refer to a as the
narrower term and b as the broader term. The fea-
tures of a that are also found in b (have non-zero
weights for both a and b) are referred to as shared
features.
They show that existing measures which cor-
respond to these criteria perform better and con-
struct the BalAPInc measure based on the princi-
ples. However, it is interesting to note that these
properties do not explicitly specify any directional
aspects of the measure, and symmetric similarity
scores can also fulfil the requirements.
Based on investigating hyponym distributions
in our training data, we suggest two additions to
this list of desired properties, one of which specif-
ically targets the asymmetric properties of the de-
sired similarity measures:
4. The shared features are more important to
the directional score calculation, compared to
non-shared features.
5. Highly weighted features of the broader term
are more important to the score calculation,
compared to features of the narrower term.
Most existing directional similarity scores mea-
sure how many features of the narrower term are
present for the broader term. If a entails b, then
it is assumed that the possible contexts of a are a
subset of contexts for b, but b occurs in a wider
range of contexts compared to a. This intuition is
used by directional measures such as ClarkeDE,
WeedsPrec and BalAPInc. In contrast, we found
that many features of the narrower term are often
highly specific to that term and do not generalise
even to hypernyms. Since these features have a
very high weight for the narrower term, their ab-
sence with the broader term will have a big nega-
tive impact on the similarity score.
We hypothesise that many terms have certain
individual features that are common to them but
not to other related words. Since most weighting
schemes reward high relative co-occurrence, these
features are also likely to receive high weights.
Therefore, we suggest that features which are not
found for both terms should have a decreased im-
pact on the score calculation, as many of them are
not expected to be shared between hyponyms and
hypernyms. However, removing them completely
is also not advisable, as they allow the measure
to estimate the overall relative importance of the
shared features to the specific term.
We also propose that among the shared features,
those ranked higher for the broader term are more
important to the directional measure. In the hy-
ponymy relation (a ? b), the term b is more gen-
eral and covers a wider range of semantic con-
cepts. This also means it is more likely to be
used in contexts that apply to different hyponyms
of b. For example, some of the high-ranking fea-
tures for food are blandly-flavoured, high-calorie
and uneaten. These are properties that co-occur
often with the term food, but can also be applied
to most hyponyms of food. Therefore, we hypoth-
esise that the presence of these features for the nar-
rower term is a good indication of a hyponymy re-
lation. This is somewhat in contrast to most previ-
ous work, where the weights of the narrower term
have been used as the main guideline for similarity
calculation.
5 Weighted cosine
We now aim to construct a similarity measure that
follows all five of the properties mentioned above.
Cosine similarity is one of the symmetric similar-
ity measures which corresponds to the first three
desired properties, and our experiments showed
that it performs remarkably well at the task of hy-
ponym generation. Therefore, we decided to mod-
ify cosine similarity to also reflect the final two
properties and produce a more appropriate asym-
metric score.
The standard feature vectors for each word con-
tain weights indicating how important this feature
is to the word. We specify additional weights that
measure how important the feature is to that spe-
cific directional relation between the two terms.
Weighted cosine similarity, shown in Table 1, can
then be used to calculate a modified similarity
score. F
a
denotes the set of weighted features for
word a, w
a
(f) is the weight of feature f for word
a, and z(f) is the additional weight for feature f ,
given the directional word pair (a, b).
Based on the new desired properties we want
to downweight the importance of features that are
not present for both terms. For this, we choose
the simple solution of scaling them with a small
constant C ? [0, 1]. Next, we also want to assign
higher z(f) values to the shared features that have
high weights for the broader term b. We use the
relative rank of feature f in F
b
, r
b
(f), as the indi-
cator of its importance and scale this value to the
range from C to 1. This results in the importance
71
WeightedCosine(F
a
, F
b
) =
?
f?F
a
?F
b
(z(f)?w
a
(f))?(z(f)?w
b
(f))
??
f?F
a
(z(f)?w
a
(f))
2
?
??
f?F
b
(z(f)?w
b
(f))
2
z(f) =
{
(1?
r
b
(f)
|F
b
|+1
)? (1? C) + C if f ? F
a
? F
b
C otherwise
Table 1: Weighted cosine similarity measure
function decreasing linearly as the rank number
increases, but the weights for the shared features
always remain higher compared to the non-shared
features. Tied feature values are handled by as-
signing them the average rank value. Adding 1
to the denominator of the relative rank calculation
avoids exceptions with empty vectors, and also en-
sures that the value will always be strictly greater
than C. While the basic function is still the sym-
metric cosine, the z(f) values will be different de-
pending on the order of the arguments.
The parameter C controls the relative impor-
tance of the ?unimportant? features to the direc-
tional relation. Setting it to 0 will ignore these
features completely, while setting it to 1 will result
in the traditional cosine measure. Experiments on
the development data showed that the exact value
of this parameter is not very important, as long as
it is not too close to the extreme values of 0 or 1.
We use the value C = 0.5 for reporting our results,
meaning that the non-shared features are half as
important, compared to the shared features.
6 Dataset
As WordNet (Miller, 1995) contains numerous
manually annotated hyponymy relations, we can
use it to construct suitable datasets for evaluat-
ing hyponym generation. While WordNet terms
are annotated with only the closest hyponyms, we
are considering all indirect/inherited hyponyms
to be relevant ? for example, given relations
(genomics ? genetics) and (genetics ? biology),
then genomics is also regarded as a hyponym of
biology. WordNet relations are defined between
synsets, but we refrain from the task of word sense
disambiguation and count word a as a valid hy-
ponym for word b if it is valid for any sense of b.
Synonymy can be thought of as a symmetric is-
a relation, and most real-world applications would
require synonyms to also be returned, together
with hyponyms. Therefore, in our dataset we con-
sider synonyms as hyponyms in both directions.
We also performed experiments without synonyms
and found that this had limited effect on the re-
sults ? while the accuracy of all similarity mea-
sures slightly decreased (due to fewer numbers of
correct answers), the relative ranking remained the
same. As shown in the next section, the number of
synonyms is typically small compared to the num-
ber of all inherited hyponyms.
To construct the dataset, we first found all
single-word nouns in WordNet that are contained
at least 10 times in the British National Corpus
(BNC). Next, we retained only words that have
at least 10 hyponyms, such that they occur 10 or
more times in the BNC. This selection process
aims to discard WordNet hypernyms that are very
rare in practical use, and would not have enough
examples for learning informative vector represen-
tations. The final dataset contains the remaining
terms, together with all of their hyponyms, includ-
ing the rare/unseen hyponyms. As expected, some
general terms, such as group or location, have a
large number of inherited hyponyms. On average,
each hypernym in the dataset has 233 hyponyms,
but the distribution is roughly exponential, and the
median is only 36.
In order to better facilitate future experiments
with supervised methods, such as described by Ba-
roni et al. (2012), we randomly separated the data
into training (1230 hypernyms), validation (922),
and test (922) sets, and we make these datasets
publically available online.
4
7 Experiments
We evaluate how well different vector space mod-
els and similarity measures perform on the task of
hyponym generation. Given a single word as in-
put, the system needs to return a ranked list of
words with correct hyponyms at the top. As the
list of candidates for scoring we use all words in
the BNC that occur at least 10 times (a total of
86,496 words). All the experiments are performed
using tokenised and lemmatised words.
As the main evaluation measure, we report
4
http://www.marekrei.com/projects/hypgen/
72
Cosine Cosine+offset
MAP P@1 P@5 MAP P@1 P@5
Window 2.18 19.76 12.20 2.19 19.76 12.25
CW-100 0.66 3.80 3.21 0.59 3.91 2.89
HLBL-100 1.01 10.31 6.04 1.01 10.31 6.06
Word2vec-100 1.78 15.96 10.12 1.50 12.38 8.71
Word2vec-500 2.06 19.76 11.92 1.77 17.05 10.71
Dependencies 2.73 25.41 14.90 2.73 25.52 14.92
Table 2: Experiments using different vector space models for hyponym generation on the test set. We
report results using regular cosine similarity and the vector offset method described in Section 2.
Mean Average Precision (MAP), which averages
precision values at various recall points in the re-
turned list. It combines both precision and recall,
as well as the quality of the ranking, into a sin-
gle measure, and is therefore well-suited for com-
paring different methods. The reported MAP val-
ues are very low ? this is due to many rare Word-
Net hyponyms not occurring in the candidate set,
for which all systems are automatically penalised.
However, this allows us to evaluate recall, making
the results comparable between different systems
and background datasets. We also report precision
at top-1 and top-5 returned hyponyms.
As a baseline we report the results of a tra-
ditional hyponym acquisition system. For this,
we implemented the pattern-based matching pro-
cess described by Hearst (1992), and also used by
Snow et al. (2005). These patterns look for ex-
plicit examples of hyponym relations mentioned
in the text, for example:
X such as {Y
1
, Y
2
, ... , (and|or)} Y
n
where X will be extracted as the hypernym, and Y
1
to Y
n
as hyponyms. We ran the patterns over the
BNC and extracted 21,704 hyponym pairs, which
were then ranked according to the number of times
they were found.
7.1 Evaluation of vector spaces
Table 2 contains experiments with different vector
space models. We report here results using cosine,
as it is an established measure and a competitive
baseline. For our task, the HLBL vectors perform
better than CW vectors, even though they were
trained on the same data. Both of them are out-
performed by word2vec-100 vectors, which have
the same dimensionality but are trained on much
more text. Increasing the dimensionality with
word2vec-500 gives a further improvement. In-
terestingly, the simple window-based vectors per-
form just as well as the ones trained with neural
networks. However, the advantage of word2vec-
500 is that the representations are more compact
and require only about half the space. Finally,
the dependency-based vectors outperform all other
vector types, giving 2.73% MAP and 25.41% pre-
cision at the top-ranked result. While the other
models are built by using neighbouring words as
context, this model looks at dependency relations,
thereby taking both semantic and syntactic roles
into account. The results indicate that word2vec
and window-based models are more suitable when
the general topic of words needs to be captured,
whereas dependency-based vectors are preferred
when the task requires both topical and functional
similarity between words. Our experiments also
included the evaluation of other similarity mea-
sures on different vector space models, and we we
found these results to be representative.
Contrary to previous work, the vector offset
method, described in Section 2, did not pro-
vide substantial improvements on the hyponym
generation task. For the neural network-based
vectors this approach generally decreased perfor-
mance, compared to using direct cosine similar-
ity. There are some marginal improvements for
window and dependency-based models. Unfortu-
nately, the original work did not include baseline
performance using cosine similarity, without ap-
plying vector modifications. It is possible that this
method does not generalise to all word relations
equally well. As part of future work, it is worth
exploring if a hypernym-specific strategy of se-
lecting training examples could improve the per-
formance.
73
Validation Test
MAP P@1 P@5 MAP P@1 P@5
Pattern-based 0.53 7.06 4.58 0.51 8.14 4.45
Cosine 2.48 21.06 12.96 2.73 25.41 14.90
Lin 1.87 16.50 10.75 2.01 21.17 12.23
DiceGen2 2.27 18.57 12.62 2.44 21.82 14.55
WeedsPrec 0.13 0.00 0.09 0.12 0.11 0.04
WeedsRec 0.72 0.33 2.45 0.69 0.54 2.41
BalPrec 1.78 15.31 10.55 1.88 17.48 11.34
ClarkeDE 0.23 0.00 0.02 0.24 0.00 0.09
BalAPInc 1.64 14.22 9.12 1.68 15.85 9.66
WeightedCosine 2.59 21.39 13.59 2.85 25.84 15.46
Combined 3.27 23.02 16.09 3.51 27.69 18.02
Table 3: Evaluation of different vector similarity measures on the validation and test set of hyponym
generation. We report Mean Average Precision (MAP), precision at rank 1 (P@1), and precision at rank
5 (P@5).
7.2 Evaluation of similarity measures
Table 3 contains experiments with different sim-
ilarity measures, using the dependency-based
model, and Table 4 contains sample output from
the best system. The results show that the pattern-
based baseline does rather poorly on this task.
MAP is low due to the system having very lim-
ited recall, but higher precision at top ranks would
have been expected. Analysis showed that this
system was unable to find any hyponyms for more
than half (513/922) of the hypernyms in the vali-
dation set, leading to such poor recall that it also
affects Precision@1. While the pattern-based sys-
tem did extract a relatively large number of hy-
ponyms from the corpus (21,704 pairs), these are
largely concentrated on a small number of hyper-
nyms (e.g., area, company, material, country) that
are more likely to be mentioned in matching con-
texts.
Cosine, DiceGen2 and Lin ? all symmetric
similarity measures ? perform relatively well on
this task, whereas established directional measures
perform unexpectedly poorly. This can perhaps be
explained by considering the distribution of hy-
ponyms. Given a word, the most likely candi-
dates for a high cosine similarity are synonyms,
antonyms, hypernyms and hyponyms of that word
? these are words that are likely to be used in simi-
lar topics, contexts, and syntactic roles. By def-
inition, there are an equal number of hyponym
and hypernym relations in WordNet, but this ra-
tio changes rapidly as we remove lower-frequency
words. Figure 1 shows the number of relations ex-
tracted from WordNet, as we restrict the minimum
frequency of the main word. It can be seen that the
number of hyponyms increases much faster com-
pared to the other three relations. This also applies
to real-world data ? when averaging over word in-
stances found in the BNC, hyponyms cover 85% of
these relations. Therefore, the high performance
of cosine can be explained by distributionally sim-
ilar words having a relatively high likelihood of
being hyponyms.
0 10 20 30 40 50 60 70 80 90 100
0
20
40
60
80
100
hyponyms hypernyms
synonyms antonyms
min freq
avg
 re
late
d w
ord
s
Figure 1: Average number of different relations
per word in WordNet, as we restrict the minimum
word frequency.
One possible reason for the poor performance
of directional measures is that most of them quan-
tify how well the features of the narrower term are
included in the broader term. In contrast, we found
that for hyponym generation it is more important
to measure how well the features of the broader
term are included in the narrower term. This
74
scientist researcher, biologist, psychologist, economist, observer, physicist, sociologist
sport football, golf, club, tennis, athletics, rugby, cricket, game, recreation, entertainment
treatment therapy, medication, patient, procedure, surgery, remedy, regimen, medicine
Table 4: Examples of top results using the combined system. WordNet hyponyms are marked in bold.
is supported by WeedsRec outperforming Weed-
sPrec, although the opposite was intended by their
design.
Another explanation for the low performance
is that these directional measures are often devel-
oped in an artificial context. For example, Kotler-
man et al. (2010) evaluated lexical entailment de-
tection on a dataset where the symmetric Lin sim-
ilarity measure was used to select word pairs for
manual annotation. This creates a different task,
as correct terms that do not have a high symmetric
similarity will be excluded from evaluation. The
BalAPInc measure performed best in that setting,
but does not do as well for hyponym generation,
where candidates are filtered only based on mini-
mum frequency.
The weighted cosine measure, proposed in Sec-
tion 5, outperformed all other similarity measures
on both hyponym generation datasets. The im-
provement over cosine is relatively small; how-
ever, it is consistent and the improvement in MAP
is statistically significant on both datasets (p <
0.05), using the Approximate Randomisation Test
(Noreen, 1989; Cohen, 1995) with 10
6
iterations.
This further supports the properties of a directional
similarity measure described in Section 4.
Finally, we created a new system by combining
together two separate approaches: the weighted
cosine measure using the dependency-based vec-
tor space, and the normal cosine similarity using
word2vec-500 vectors. We found that the former
is good at modelling the grammatical roles and di-
rectional containment, whereas the latter can pro-
vide useful information about the topic and seman-
tics of the word. Turney (2012) also demonstrated
the importance of both topical (domain) and func-
tional vector space models when working with se-
mantic relations. We combined these approaches
by calculating both scores for each word pair and
taking their geometric average, or 0 if it could not
be calculated. This final system gives considerable
improvements across all evaluation metrics, and is
significantly (p < 0.05) better compared to cosine
or weighted cosine methods individually. Table 4
contains some example output from this system.
8 Conclusion
Hyponym generation has a wide range of pos-
sible applications in NLP, such as query expan-
sion, entailment detection, and language model
smoothing. Pattern-based hyponym acquisition
can be used to find relevant hyponyms, but these
approaches rely on both words being mentioned
together in a specific context, leading to very low
recall. Vector similarity methods are interesting
for this task, as they can be easily applied to differ-
ent domains and languages without any supervised
learning or manual pattern construction. We cre-
ated a dataset for evaluating hyponym generation
systems and experimented with a range of vector
space models and similarity measures.
Our results show that choosing an appropriate
vector space model is equally important to using a
suitable similarity measure. We achieved the high-
est performance using dependency-based vector
representations, which outperformed neural net-
work and window-based models. Symmetric sim-
ilarity measures, especially cosine similarity, per-
formed surprisingly well on this task. This can
be attributed to an unbalanced distribution of hy-
ponyms, compared to other high-similarity words.
The choice of vector space can be highly depen-
dent on the specific task, and we have made avail-
able our vector datasets created from the same
source using three different methods.
We proposed two new properties for detecting
hyponyms, and used them to construct a new di-
rectional similarity measure. This weighted co-
sine measure significantly outperformed all others,
showing that a theoretically-motivated directional
measure is still the most accurate method for mod-
elling hyponymy relations. Finally, we combined
together two different methods, achieving further
substantial improvements on all evaluation met-
rics.
References
?istein E. Andersen, Julien Nioche, Edward J. Briscoe,
and John Carroll. 2008. The BNC parsed with
RASP4UIMA. In Proceedings of the Sixth Interna-
75
tional Language Resources and Evaluation Confer-
ence (LREC08), Marrakech, Morocco.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
Edinburgh.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 23?32.
Chris Biemann. 2005. Ontology learning from text: A
survey of methods. LDV Forum, 20(2002):75?93.
Gilles Bisson, Claire N?edellec, and Dolores Ca?namero.
2000. Designing clustering methods for ontology
building-The Mo?K workbench. In ECAI Ontology
Learning Workshop.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, number July, pages 77?80, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. Pro-
ceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 120?126.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided hierar-
chical clustering algorithm. In ICML-Workshop on
Learning and Extending Lexical Ontologies by using
Machine Learning Methods.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, number March, pages 112?
119. Association for Computational Linguistics.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. Proceed-
ings of the 25th international conference on Ma-
chine learning.
James R. Curran. 2003. From distributional to seman-
tic similarity. Ph.D. thesis, University of Edinburgh.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 31:1?31.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguistics
(COLING ?92), number July, page 539, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(04):359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768?774. Association for Compu-
tational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. ICLR Workshop, pages
1?12.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. (June):746?751.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
Proceedings of the 24th international conference on
Machine learning - ICML ?07, pages 641?648.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley,
New York.
Gerhard Paa?, J?org Kindermann, and Edda Leopold.
2004. Learning prototype ontologies by hierachical
latent semantic analysis.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT/NAACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING ?08), pages 849?856,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
76
Joseph Turian, Lev Ratinov, and Y Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Akira Ushioda. 1996. Hierarchical clustering of words
and application to NLP tasks. In Fourth Workshop
on Very Large Corpora, pages 28?41.
Andreas Wagner. 2000. Enriching a lexical semantic
net with selectional preferences by means of statisti-
cal corpus analysis. In ECAI Workshop on Ontology
Learning.
Julie Weeds and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical distribu-
tional similarity. Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. Proceedings of the 20th international
conference on Computational Linguistics - COLING
?04.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435?461, Septem-
ber.
77
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?14,
Baltimore, Maryland, 26-27 July 2014.
c
?2014 Association for Computational Linguistics
The CoNLL-2014 Shared Task on Grammatical Error Correction
Hwee Tou Ng
1
Siew Mei Wu
2
Ted Briscoe
3
Christian Hadiwinoto
1
Raymond Hendy Susanto
1
Christopher Bryant
1
1
Department of Computer Science, National University of Singapore
{nght,chrhad,raymondhs,bryant}@comp.nus.edu.sg
2
Centre for English Language Communication, National University of Singapore
elcwusm@nus.edu.sg
3
Computer Laboratory, University of Cambridge
Ted.Briscoe@cl.cam.ac.uk
Abstract
The CoNLL-2014 shared task was devoted
to grammatical error correction of all error
types. In this paper, we give the task defi-
nition, present the data sets, and describe
the evaluation metric and scorer used in
the shared task. We also give an overview
of the various approaches adopted by the
participating teams, and present the eval-
uation results. Compared to the CoNLL-
2013 shared task, we have introduced the
following changes in CoNLL-2014: (1)
A participating system is expected to de-
tect and correct grammatical errors of all
types, instead of just the five error types
in CoNLL-2013; (2) The evaluation metric
was changed from F
1
to F
0.5
, to empha-
size precision over recall; and (3) We have
two human annotators who independently
annotated the test essays, compared to just
one human annotator in CoNLL-2013.
1 Introduction
Grammatical error correction is the shared task of
the Eighteenth Conference on Computational Nat-
ural Language Learning in 2014 (CoNLL-2014).
In this task, given an English essay written by a
learner of English as a second language, the goal
is to detect and correct the grammatical errors of
all error types present in the essay, and return the
corrected essay.
This task has attracted much recent research in-
terest, with two shared tasks Helping Our Own
(HOO) organized in 2011 and 2012 (Dale and Kil-
garriff, 2011; Dale et al., 2012), and a CoNLL
shared task on grammatical error correction orga-
nized in 2013 (Ng et al., 2013). In contrast to
previous CoNLL shared tasks which focused on
particular subtasks of natural language process-
ing, such as named entity recognition, semantic
role labeling, dependency parsing, or coreference
resolution, grammatical error correction aims at
building a complete end-to-end application. This
task is challenging since for many error types,
current grammatical error correction systems do
not achieve high performance and much research
is still needed. Also, tackling this task has far-
reaching impact, since it is estimated that hun-
dreds of millions of people worldwide are learn-
ing English and they benefit directly from an auto-
mated grammar checker.
The CoNLL-2014 shared task provides a forum
for participating teams to work on the same gram-
matical error correction task, with evaluation on
the same blind test set using the same evaluation
metric and scorer. This overview paper contains a
detailed description of the shared task, and is orga-
nized as follows. Section 2 provides the task def-
inition. Section 3 describes the annotated training
data provided and the blind test data. Section 4 de-
scribes the evaluation metric and the scorer. Sec-
tion 5 lists the participating teams and outlines the
approaches to grammatical error correction used
by the teams. Section 6 presents the results of the
shared task, including a discussion on cross anno-
tator comparison. Section 7 concludes the paper.
2 Task Definition
The goal of the CoNLL-2014 shared task is to
evaluate algorithms and systems for automati-
cally detecting and correcting grammatical errors
1
present in English essays written by second lan-
guage learners of English. Each participating
team is given training data manually annotated
with corrections of grammatical errors. The test
data consists of new, blind test essays. Prepro-
cessed test essays, which have been sentence-
segmented and tokenized, are also made available
to the participating teams. Each team is to submit
its system output consisting of the automatically
corrected essays, in sentence-segmented and tok-
enized form.
Grammatical errors consist of many different
types, including articles or determiners, preposi-
tions, noun form, verb form, subject-verb agree-
ment, pronouns, word choice, sentence structure,
punctuation, capitalization, etc. However, most
prior published research on grammatical error cor-
rection only focuses on a small number of fre-
quently occurring error types, such as article and
preposition errors (Han et al., 2006; Gamon, 2010;
Rozovskaya and Roth, 2010; Tetreault et al., 2010;
Dahlmeier and Ng, 2011b). Article and preposi-
tion errors were also the only error types featured
in the HOO 2012 shared task. Likewise, although
all error types were included in the HOO 2011
shared task, almost all participating teams dealt
with article and preposition errors only (besides
spelling and punctuation errors). In the CoNLL-
2013 shared task, the error types were extended
to include five error types, comprising article or
determiner, preposition, noun number, verb form,
and subject-verb agreement. Other error types
such as word choice errors (Dahlmeier and Ng,
2011a) were not dealt with.
In the CoNLL-2014 shared task, it was felt that
the community is now ready to deal with all er-
ror types. Table 1 shows examples of the 28 error
types in the CoNLL-2014 shared task.
Since there are 28 error types in our shared task
compared to two in HOO 2012 and five in CoNLL-
2013, there is a greater chance of encountering
multiple, interacting errors in a sentence in our
shared task. This increases the complexity of our
shared task. To illustrate, consider the following
sentence:
Social network plays a role in providing
and also filtering information.
The noun number error networks needs to be cor-
rected (network ? networks). This necessitates
the correction of a subject-verb agreement error
(plays ? play). A pipeline system in which cor-
rections for subject-verb agreement errors occur
strictly before corrections for noun number errors
would not be able to arrive at a fully corrected
sentence for this example. The ability to correct
multiple, interacting errors is thus necessary in our
shared task. The recent work of Dahlmeier and Ng
(2012a) and Wu and Ng (2013), for example, is
designed to deal with multiple, interacting errors.
3 Data
This section describes the training and test data
released to each participating team in our shared
task.
3.1 Training Data
The training data provided in our shared task is
the NUCLE corpus, the NUS Corpus of Learner
English (Dahlmeier et al., 2013). As noted by
(Leacock et al., 2010), the lack of a manually an-
notated and corrected corpus of English learner
texts has been an impediment to progress in gram-
matical error correction, since it prevents com-
parative evaluations on a common benchmark test
data set. NUCLE was created precisely to fill this
void. It is a collection of 1,414 essays written
by students at the National University of Singa-
pore (NUS) who are non-native speakers of En-
glish. The essays were written in response to some
prompts, and they cover a wide range of topics,
such as environmental pollution, health care, etc.
The grammatical errors in these essays have been
hand-corrected by professional English instructors
at NUS. For each grammatical error instance, the
start and end character offsets of the erroneous text
span are marked, and the error type and the cor-
rection string are provided. Manual annotation is
carried out using a graphical user interface specif-
ically built for this purpose. The error annotations
are saved as stand-off annotations, in SGML for-
mat.
To illustrate, consider the following sentence at
the start of the sixth paragraph of an essay:
Nothing is absolute right or wrong.
There is a word form error (absolute? absolutely)
in this sentence. The error annotation, also called
correction or edit, in SGML format is shown in
Figure 1. start par (end par) denotes the
paragraph ID of the start (end) of the erroneous
2
Type Description Example
Vt Verb tense Medical technology during that time [is ? was] not advanced enough to
cure him.
Vm Verb modal Although the problem [would ? may] not be serious, people [would ?
might] still be afraid.
V0 Missing verb However, there are also a great number of people [who? who are] against
this technology.
Vform Verb form A study in 2010 [shown? showed] that patients recover faster when sur-
rounded by family members.
SVA Subject-verb agreement The benefits of disclosing genetic risk information [outweighs ? out-
weigh] the costs.
ArtOrDet Article or determiner It is obvious to see that [internet? the internet] saves people time and also
connects people globally.
Nn Noun number A carrier may consider not having any [child ? children] after getting
married.
Npos Noun possessive Someone should tell the [carriers? carrier?s] relatives about the genetic
problem.
Pform Pronoun form A couple should run a few tests to see if [their? they] have any genetic
diseases beforehand.
Pref Pronoun reference It is everyone?s duty to ensure that [he or she ? they] undergo regular
health checks.
Prep Preposition This essay will [discuss about? discuss] whether a carrier should tell his
relatives or not.
Wci Wrong collocation/idiom Early examination is [healthy? advisable] and will cast away unwanted
doubts.
Wa Acronyms After [WOWII ? World War II], the population of China decreased
rapidly.
Wform Word form The sense of [guilty? guilt] can be more than expected.
Wtone Tone (formal/informal) [It?s? It is] our family and relatives that bring us up.
Srun Run-on sentences,
comma splices
The issue is highly [debatable, a? debatable. A] genetic risk could come
from either side of the family.
Smod Dangling modifiers [Undeniable,? It is undeniable that] it becomes addictive when we spend
more time socializing virtually.
Spar Parallelism We must pay attention to this information and [assisting ? assist] those
who are at risk.
Sfrag Sentence fragment However, from the ethical point of view.
Ssub Subordinate clause This is an issue [needs? that needs] to be addressed.
WOinc Incorrect word order [Someone having what kind of disease?What kind of disease someone
has] is a matter of their own privacy.
WOadv Incorrect adjective/
adverb order
In conclusion, [personally I? I personally] feel that it is important to tell
one?s family members.
Trans Linking words/phrases It is sometimes hard to find [out? out if] one has this disease.
Mec Spelling, punctuation,
capitalization, etc.
This knowledge [maybe relavant? may be relevant] to them.
Rloc? Redundancy It is up to the [patient?s own choice? patient] to disclose information.
Cit Citation Poor citation practice.
Others Other errors An error that does not fit into any other category but can still be corrected.
Um Unclear meaning Genetic disease has a close relationship with the born gene. (i.e., no cor-
rection possible without further clarification.)
Table 1: The 28 error types in the shared task.
3
text span (paragraph ID starts from 0 by conven-
tion). start off (end off) denotes the char-
acter offset of the start (end) of the erroneous text
span (again, character offset starts from 0 by con-
vention). The error tag is Wform, and the correc-
tion string is absolutely.
The NUCLE corpus was first used in
(Dahlmeier and Ng, 2011b), and has been
publicly available for research purposes since
June 2011
1
. All instances of grammatical errors
are annotated in NUCLE.
To help participating teams in their prepara-
tion for the shared task, we also performed au-
tomatic preprocessing of the NUCLE corpus and
released the preprocessed form of NUCLE. The
preprocessing operations performed on the NU-
CLE essays include sentence segmentation and
word tokenization using the NLTK toolkit (Bird
et al., 2009), and part-of-speech (POS) tagging,
constituency and dependency tree parsing using
the Stanford parser (Klein and Manning, 2003;
de Marneffe et al., 2006). The error annotations,
which are originally at the character level, are
then mapped to error annotations at the word to-
ken level. Error annotations at the word token
level also facilitate scoring, as we will see in Sec-
tion 4, since our scorer operates by matching to-
kens. Note that although we released our own
preprocessed version of NUCLE, the participating
teams were however free to perform their own pre-
processing if they so preferred.
NUCLE release version 3.2 was used in the
CoNLL-2014 shared task. In this version, 17 es-
says were removed from the first release of NU-
CLE since these essays were duplicates with mul-
tiple annotations. In addition, in order to facilitate
the detection and correction of article/determiner
errors and preposition errors, we performed some
automatic mapping of error types in the original
NUCLE corpus to arrive at release version 3.2. Ng
et al. (2013) gives more details of how the map-
ping was carried out.
The statistics of the NUCLE corpus (release 3.2
version) are shown in Table 2. The distribution of
errors among all error types is shown in Table 3.
While the NUCLE corpus is provided in our
shared task, participating teams are free to not use
NUCLE, or to use additional resources and tools
in building their grammatical error correction sys-
tems, as long as these resources and tools are pub-
1
http://www.comp.nus.edu.sg/?nlp/corpora.html
Training data Test data
(NUCLE)
# essays 1,397 50
# sentences 57,151 1,312
# word tokens 1,161,567 30,144
Table 2: Statistics of training and test data.
licly available and not proprietary. For example,
participating teams are free to use the Cambridge
FCE corpus (Yannakoudakis et al., 2011; Nicholls,
2003) (the training data provided in HOO 2012
(Dale et al., 2012)) as additional training data.
3.2 Test Data
Similar to CoNLL-2013, 25 NUS students, who
are non-native speakers of English, were recruited
to write new essays to be used as blind test data
in the shared task. Each student wrote two essays
in response to the two prompts shown in Table 4,
one essay per prompt. The first prompt was also
used in the NUCLE training data, but the second
prompt is entirely new and not used previously. As
a result, 50 new test essays were collected. The
statistics of the test essays are also shown in Ta-
ble 2.
Error annotation on the test essays was carried
out independently by two native speakers of En-
glish. One of them is a lecturer at the NUS Cen-
tre for English Language Communication, and the
other is a freelance English linguist with exten-
sive prior experience in error annotation of English
learners? essays. The distribution of errors in the
test essays among the error types is shown in Ta-
ble 3. The test essays were then preprocessed in
the same manner as the NUCLE corpus. The pre-
processed test essays were released to the partic-
ipating teams. Similar to CoNLL-2013, the test
essays and their error annotations in the CoNLL-
2014 shared task will be made freely available af-
ter the shared task.
4 Evaluation Metric and Scorer
A grammatical error correction system is evalu-
ated by how well its proposed corrections or edits
match the gold-standard edits. An essay is first
sentence-segmented and tokenized before evalua-
tion is carried out on the essay. To illustrate, con-
sider the following tokenized sentence S written
by an English learner:
4
<MISTAKE start par="5" start off="11" end par="5" end off="19">
<TYPE>Wform</TYPE>
<CORRECTION>absolutely</CORRECTION>
</MISTAKE>
Figure 1: An example error annotation.
Error type Training % Test % Test %
data data data
(NUCLE) (Annotator 1) (Annotator 2)
Vt 3,204 7.1% 133 5.5% 150 4.5%
Vm 431 1.0% 49 2.0% 37 1.1%
V0 414 0.9% 31 1.3% 37 1.1%
Vform 1,443 3.2% 132 5.5% 91 2.7%
SVA 1,524 3.4% 105 4.4% 154 4.6%
ArtOrDet 6,640 14.8% 332 13.9% 444 13.3%
Nn 3,768 8.4% 215 9.0% 228 6.8%
Npos 239 0.5% 19 0.8% 15 0.5%
Pform 186 0.4% 47 2.0% 18 0.5%
Pref 927 2.1% 96 4.0% 153 4.6%
Prep 2,413 5.4% 211 8.8% 390 11.7%
Wci 5,305 11.8% 340 14.2% 479 14.4%
Wa 50 0.1% 0 0.0% 1 0.0%
Wform 2,161 4.8% 77 3.2% 103 3.1%
Wtone 593 1.3% 9 0.4% 15 0.5%
Srun 873 1.9% 7 0.3% 26 0.8%
Smod 51 0.1% 0 0.0% 5 0.2%
Spar 519 1.2% 3 0.1% 24 0.7%
Sfrag 250 0.6% 13 0.5% 5 0.2%
Ssub 362 0.8% 68 2.8% 10 0.3%
WOinc 698 1.6% 22 0.9% 54 1.6%
WOadv 347 0.8% 12 0.5% 27 0.8%
Trans 1,377 3.1% 94 3.9% 79 2.4%
Mec 3,145 7.0% 231 9.6% 496 14.9%
Rloc? 4,703 10.5% 95 4.0% 199 6.0%
Cit 658 1.5% 0 0.0% 0 0.0%
Others 1,467 3.3% 44 1.8% 49 1.5%
Um 1,164 2.6% 12 0.5% 42 1.3%
All types 44,912 100.0% 2,397 100.0% 3,331 100.0%
Table 3: Error type distribution of the training and test data. The test data were annotated independently
by two annotators.
5
ID Prompt
1 ?The decision to undergo genetic testing can only be made by the individual at risk for a disor-
der. Once a test has been conducted and the results are known, however, a new, family-related
ethical dilemma is born: Should a carrier of a known genetic risk be obligated to tell his or her
relatives?? Respond to the question above, supporting your argument with concrete examples.
2 While social media sites such as Twitter and Facebook can connect us closely to people in
many parts of the world, some argue that the reduction in face-to-face human contact affects
interpersonal skills. Explain the advantages and disadvantages of using social media in your
daily life/society.
Table 4: The two prompts used for the test essays.
There is no a doubt , tracking system
has brought many benefits in this infor-
mation age .
The set of gold-standard edits of a human annota-
tor is g = {a doubt ? doubt, system ? systems,
has ? have}. Suppose the tokenized output sen-
tence H of a grammatical error correction system
given the above sentence is:
There is no doubt , tracking system has
brought many benefits in this informa-
tion age .
That is, the set of system edits is e = {a doubt
? doubt}. The performance of the grammatical
error correction system is measured by how well
the two sets g and e match, in the form of recall
R, precision P , and F
0.5
measure: R = 1/3, P =
1/1, F
0.5
= (1 + 0.5
2
)?RP/(R + 0.5
2
? P ) =
5/7.
More generally, given a set of n sentences,
where g
i
is the set of gold-standard edits for sen-
tence i, and e
i
is the set of system edits for sen-
tence i, recall, precision, and F
0.5
are defined as
follows:
R =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|g
i
|
(1)
P =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|e
i
|
(2)
F
0.5
=
(1 + 0.5
2
)?R? P
R + 0.5
2
? P
(3)
where the intersection between g
i
and e
i
for sen-
tence i is defined as
g
i
? e
i
= {e ? e
i
|?g ? g
i
,match(g, e)} (4)
Note that we have adopted F
0.5
as the evaluation
metric in the CoNLL-2014 shared task instead of
the standard F
1
used in CoNLL-2013. F
0.5
em-
phasizes precision twice as much as recall, while
F
1
weighs precision and recall equally. When a
grammar checker is put into actual use, it is im-
portant that its proposed corrections are highly ac-
curate in order to gain user acceptance. Neglecting
to propose a correction is not as bad as proposing
an erroneous correction.
Similar to CoNLL-2013, we use the MaxMatch
(M
2
) scorer
2
(Dahlmeier and Ng, 2012b) as the of-
ficial scorer in CoNLL-2014. The M
2
scorer
3
effi-
ciently searches for a set of system edits that max-
imally matches the set of gold-standard edits spec-
ified by an annotator. It overcomes a limitation of
the scorer used in HOO shared tasks, which can
return an erroneous score since the system edits
are computed deterministically by the HOO scorer
without regard to the gold-standard edits.
5 Approaches
45 teams registered to participate in the shared
task, out of which 13 teams submitted the out-
put of their grammatical error correction systems.
These teams are listed in Table 5. Each team is as-
signed a 3 to 4-letter team ID. In the remainder of
this paper, we will use the assigned team ID to re-
fer to a participating team. Every team submitted
a system description paper (the only exception is
the NARA team). Four of the 13 teams submitted
their system output only after the deadline (they
were given up to one week of extension). These
four teams (IITB, IPN, PKU, and UFC) have an
asterisk affixed after their team names in Table 5.
Each participating team in the CoNLL-2014
shared task tackled the error correction problem
in a different way. A full list summarizing each
2
http://www.comp.nus.edu.sg/?nlp/software.html
3
A few minor bugs were fixed in the M
2
scorer before it
was used in the CoNLL-2014 shared task.
6
Team ID Affiliation
AMU Adam Mickiewicz University
CAMB University of Cambridge
CUUI Columbia University and the University of Illinois at Urbana-Champaign
IITB
?
Indian Institute of Technology, Bombay
IPN
?
Instituto Polit?ecnico Nacional
NARA Nara Institute of Science and Technology
NTHU National Tsing Hua University
PKU
?
Peking University
POST Pohang University of Science and Technology
RAC Research Institute for Artificial Intelligence, Romanian Academy
SJTU Shanghai Jiao Tong University
UFC
?
University of Franche-Comt?e
UMC University of Macau
Table 5: The list of 13 participating teams. The teams that submitted their system output after the
deadline have an asterisk affixed after their team names. NARA did not submit any system description
paper.
team?s approach can be found in Table 6. While
machine-learnt classifiers for specific error types
proved popular in last year?s CoNLL-2013 shared
task, since this year?s task required the correction
of all 28 error types, teams tended to prefer meth-
ods that could deal with all error types simultane-
ously. In fact, most teams built hybrid systems that
made use of a combination of different approaches
to identify and correct errors.
One of the most popular approaches to non-
specific error type correction, incorporated to var-
ious extents in many teams? systems, was the Lan-
guage Model (LM) based approach. Specifically,
the probability of a learner n-gram is compared
with the probability of a candidate corrected n-
gram, and if the difference is greater than some
threshold, an error was perceived to have been de-
tected and a higher scoring replacement n-gram
could be suggested. Some teams used this ap-
proach only to detect errors, e.g., IPN (Hernandez
and Calvo, 2014), which could then be corrected
by other methods, whilst other teams used other
methods to detect errors first, and then made cor-
rections based on the alternative highest n-gram
probability score, e.g., RAC (Boros? et al., 2014).
No single team used a uniquely LM-based solution
and the LM approach was always a component in
a hybrid system.
An alternative solution to correcting all er-
rors was to use a phrase-based statistical machine
translation (MT) system to ?translate? learner En-
glish into correct English. Teams that followed the
MT approach mainly differed in terms of their at-
titude toward tuning; CAMB (Felice et al., 2014)
performed no tuning at all, IITB (Kunchukut-
tan et al., 2014) and UMC (Wang et al., 2014b)
tuned F
0.5
using MERT, while AMU (Junczys-
Dowmunt and Grundkiewicz, 2014) explored a va-
riety of tuning options, ultimately tuning F
0.5
us-
ing a combination of kb-MIRA and MERT. No
team used a syntax-based translation model, al-
though UMC did include POS tags and morphol-
ogy in a factored translation model.
With regard to correcting single error types,
rule-based (RB) approaches were also common in
most teams? systems. A possible reason for this
is that some error types are more regular than oth-
ers, and so in order to boost accuracy, simple rules
can be written to make sure that, for example, the
number of a subject agrees with the number of
a verb. In contrast, it is a lot harder to write a
rule to consistently correct Wci (wrong colloca-
tion/idiom) errors. As such, RB methods were of-
ten, but not always, used as a preliminary or sup-
plementary stage in a larger hybrid system.
Finally, although there were fewer machine-
learnt classifier (ML) approaches than last year,
some teams still used various classifiers to correct
specific error types. In fact, CUUI (Rozovskaya
et al., 2014) only built classifiers for specific er-
ror types and did not attempt to tackle the whole
range of errors. SJTU (Wang et al., 2014a) also
preprocessed the training data into more precise
error categories using rules (e.g., verb tense (Vt)
7
errors might be subcategorized into present, past,
or future tense etc.) and then built a single max-
imum entropy classifier to correct all error types.
See Table 6 to find out which teams tackled which
error types.
While every effort has been made to make clear
which team used which approach to correct which
set of error types, as there were more error types
than last year, it was sometimes impractical to fit
all this information into Table 6. For more infor-
mation on the specific methods used to correct a
specific error type, we must refer the reader to that
team?s CoNLL-2014 system description paper.
Table 6 also shows the linguistic features used
by the participating teams, which include lexical
features (i.e., words, collocations, n-grams), parts-
of-speech (POS), constituency parses, and depen-
dency parses.
While all teams in the shared task used the NU-
CLE corpus, they were also allowed to use addi-
tional external resources (both corpora and tools)
so long as they were publicly available and not
proprietary. Three teams also used last year?s
CoNLL-2013 test set as a development set in this
year?s CoNLL-2014 shared task. The external re-
sources used by the teams are also listed in Ta-
ble 6.
6 Results
All submitted system output was evaluated using
the M
2
scorer, based on the error annotations pro-
vided by our annotators. The recall (R), pre-
cision (P ), and F
0.5
measure of all teams are
shown in Table 7. The performance of the teams
varies greatly, from little more than five per cent to
37.33% for the top team.
The nature of grammatical error correction is
such that multiple, different corrections are of-
ten acceptable. In order to allow the participating
teams to raise their disagreement with the origi-
nal gold-standard annotations provided by the an-
notators, and not understate the performance of
the teams, we allow the teams to submit their
proposed alternative answers. This was also the
practice adopted in HOO 2011, HOO 2012, and
CoNLL-2013. Specifically, after the teams sub-
mitted their system output and the error annota-
tions on the test essays were released, we allowed
the teams to propose alternative answers (gold-
standard edits), to be submitted within four days
after the initial error annotations were released.
Team ID Precision Recall F
0.5
CAMB 39.71 30.10 37.33
CUUI 41.78 24.88 36.79
AMU 41.62 21.40 35.01
POST 34.51 21.73 30.88
NTHU 35.08 18.85 29.92
RAC 33.14 14.99 26.68
UMC 31.27 14.46 25.37
PKU
?
32.21 13.65 25.32
NARA 21.57 29.38 22.78
SJTU 30.11 5.10 15.19
UFC
?
70.00 1.72 7.84
IPN
?
11.28 2.85 7.09
IITB
?
30.77 1.39 5.90
Table 7: Scores (in %) without alternative an-
swers. The teams that submitted their system out-
put after the deadline have an asterisk affixed after
their team names.
The same annotators who provided the error an-
notations on the test essays also judged the alter-
native answers proposed by the teams, to ensure
consistency. In all, three teams (CAMB, CUUI,
UMC) submitted alternative answers.
The same submitted system output was then
evaluated using the M
2
scorer, with the original
annotations augmented with the alternative an-
swers. Table 8 shows the recall (R), precision (P ),
and F
0.5
measure of all teams under this new eval-
uation setting.
The F
0.5
measure of every team improves when
evaluated with alternative answers. Not surpris-
ingly, the teams which submitted alternative an-
swers tend to show the greatest improvements in
their F
0.5
measure. Overall, the CUUI team (Ro-
zovskaya et al., 2014) achieves the best F
0.5
mea-
sure when evaluated with alternative answers, and
the CAMB team (Felice et al., 2014) achieves the
best F
0.5
measure when evaluated without alterna-
tive answers.
For future research which uses the test data of
the CoNLL-2014 shared task, we recommend that
evaluation be carried out in the setting that does
not use alternative answers, to ensure a fairer eval-
uation. This is because the scores of the teams
which submitted alternative answers tend to be
higher in a biased way when evaluated with alter-
native answers.
We are also interested in the analysis of the
system performance for each of the error types.
8
T
e
a
m
E
r
r
o
r
A
p
p
r
o
a
c
h
D
e
s
c
r
i
p
t
i
o
n
o
f
A
p
p
r
o
a
c
h
L
i
n
g
u
i
s
t
i
c
F
e
a
t
u
r
e
s
E
x
t
e
r
n
a
l
R
e
s
o
u
r
c
e
s
A
M
U
A
l
l
M
T
P
h
r
a
s
e
-
b
a
s
e
d
t
r
a
n
s
l
a
t
i
o
n
o
p
t
i
m
i
z
e
d
f
o
r
F
-
s
c
o
r
e
u
s
i
n
g
a
c
o
m
b
i
n
a
t
i
o
n
o
f
k
b
-
M
I
R
A
a
n
d
M
E
R
T
w
i
t
h
a
u
g
m
e
n
t
e
d
l
a
n
g
u
a
g
e
m
o
d
e
l
s
a
n
d
t
a
s
k
-
s
p
e
c
i
fi
c
f
e
a
t
u
r
e
s
.
L
e
x
i
c
a
l
W
i
k
i
p
e
d
i
a
,
C
o
m
m
o
n
C
r
a
w
l
,
L
a
n
g
-
8
C
A
M
B
A
l
l
R
B
/
L
M
/
M
T
P
i
p
e
l
i
n
e
:
R
u
l
e
-
b
a
s
e
d
?
L
M
r
a
n
k
i
n
g
?
U
n
t
u
n
e
d
S
M
T
?
L
M
r
a
n
k
i
n
g
?
T
y
p
e
fi
l
t
e
r
i
n
g
L
e
x
i
c
a
l
,
P
O
S
C
a
m
b
r
i
d
g
e
?
W
r
i
t
e
a
n
d
I
m
p
r
o
v
e
?
S
A
T
s
y
s
t
e
m
,
C
a
m
b
r
i
d
g
e
L
e
a
r
n
e
r
C
o
r
p
u
s
,
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
F
i
r
s
t
C
e
r
t
i
fi
c
a
t
e
i
n
E
n
g
l
i
s
h
c
o
r
p
u
s
,
E
n
g
l
i
s
h
V
o
c
a
b
u
l
a
r
y
P
r
o
fi
l
e
c
o
r
p
u
s
,
M
i
c
r
o
s
o
f
t
W
e
b
L
M
C
U
U
I
A
r
t
O
r
D
e
t
,
M
e
c
,
N
n
,
P
r
e
p
,
S
V
A
,
V
f
o
r
m
,
V
t
,
W
f
o
r
m
,
W
t
o
n
e
M
L
D
i
f
f
e
r
e
n
t
c
o
m
b
i
n
a
t
i
o
n
s
o
f
a
v
e
r
a
g
e
d
p
e
r
c
e
p
t
r
o
n
,
n
a
?
?
v
e
B
a
y
e
s
,
a
n
d
p
a
t
t
e
r
n
-
b
a
s
e
d
l
e
a
r
n
i
n
g
t
r
a
i
n
e
d
o
n
d
i
f
f
e
r
e
n
t
d
a
t
a
s
e
t
s
f
o
r
d
i
f
f
e
r
e
n
t
e
r
r
o
r
t
y
p
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
s
h
a
l
l
o
w
p
a
r
s
e
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
G
o
o
g
l
e
W
e
b
1
T
I
I
T
B
A
l
l
M
T
/
M
L
P
h
r
a
s
e
-
b
a
s
e
d
t
r
a
n
s
l
a
t
i
o
n
o
p
t
i
m
i
z
e
d
f
o
r
F
-
s
c
o
r
e
u
s
i
n
g
M
E
R
T
a
n
d
s
u
p
p
l
e
m
e
n
t
e
d
w
i
t
h
a
d
d
i
t
i
o
n
a
l
R
B
m
o
d
u
l
e
s
f
o
r
S
V
A
e
r
r
o
r
s
a
n
d
M
L
m
o
d
u
l
e
s
f
o
r
N
n
a
n
d
A
r
t
O
r
D
e
t
.
L
e
x
i
c
a
l
,
s
h
a
l
l
o
w
p
a
r
s
e
N
o
n
e
I
P
N
A
l
l
e
x
c
e
p
t
P
r
e
p
L
M
/
R
B
L
o
w
L
M
s
c
o
r
e
t
r
i
g
r
a
m
s
a
r
e
i
d
e
n
t
i
fi
e
d
a
s
e
r
r
o
r
s
w
h
i
c
h
a
r
e
s
u
b
s
e
q
u
e
n
t
l
y
c
o
r
r
e
c
t
e
d
b
y
r
u
l
e
s
.
L
e
x
i
c
a
l
,
l
e
m
m
a
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
W
i
k
i
p
e
d
i
a
N
T
H
U
A
r
t
O
r
D
e
t
,
N
n
,
P
r
e
p
,
?
P
r
e
p
+
V
e
r
b
?
,
S
p
e
l
l
i
n
g
a
n
d
C
o
m
m
a
s
,
S
V
A
,
W
f
o
r
m
R
B
/
L
M
/
M
T
E
x
t
e
r
n
a
l
r
e
s
o
u
r
c
e
s
c
o
r
r
e
c
t
s
p
e
l
l
i
n
g
e
r
r
o
r
s
w
h
i
l
e
a
c
o
n
d
i
-
t
i
o
n
a
l
r
a
n
d
o
m
fi
e
l
d
m
o
d
e
l
c
o
r
r
e
c
t
s
c
o
m
m
a
e
r
r
o
r
s
.
S
V
A
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
u
s
i
n
g
a
R
B
a
p
p
r
o
a
c
h
.
A
l
l
o
t
h
e
r
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
b
y
m
e
a
n
s
o
f
a
l
a
n
g
u
a
g
e
m
o
d
e
l
.
I
n
t
e
r
a
c
t
i
n
g
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
u
s
i
n
g
a
n
M
T
s
y
s
t
e
m
.
L
e
x
i
c
a
l
,
P
O
S
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
A
s
p
e
l
l
,
G
i
n
g
e
r
I
t
,
A
c
a
d
e
m
i
c
W
o
r
d
L
i
s
t
,
B
r
i
t
i
s
h
N
a
t
i
o
n
a
l
C
o
r
p
u
s
,
G
o
o
g
l
e
W
e
b
1
T
,
G
o
o
g
l
e
B
o
o
k
s
S
y
n
t
a
c
t
i
c
N
-
G
r
a
m
s
,
E
n
g
l
i
s
h
G
i
g
a
w
o
r
d
P
K
U
A
l
l
L
M
/
M
L
A
L
M
i
s
u
s
e
d
t
o
fi
n
d
t
h
e
h
i
g
h
e
s
t
s
c
o
r
i
n
g
v
a
r
i
a
n
t
o
f
a
w
o
r
d
w
i
t
h
a
c
o
m
m
o
n
s
t
e
m
w
h
i
l
e
m
a
x
i
m
u
m
e
n
t
r
o
p
y
c
l
a
s
s
i
fi
e
r
s
d
e
a
l
w
i
t
h
a
r
t
i
c
l
e
s
a
n
d
p
r
e
p
o
s
i
t
i
o
n
s
.
L
e
x
i
c
a
l
,
P
O
S
,
s
t
e
m
G
i
g
a
w
o
r
d
,
A
p
a
c
h
e
L
u
c
e
n
e
S
p
e
l
l
c
h
e
c
k
e
r
P
O
S
T
A
l
l
L
M
/
R
B
N
-
g
r
a
m
-
b
a
s
e
d
a
p
p
r
o
a
c
h
fi
n
d
s
u
n
l
i
k
e
l
y
n
-
g
r
a
m
?
f
r
a
m
e
s
?
w
h
i
c
h
a
r
e
t
h
e
n
c
o
r
r
e
c
t
e
d
v
i
a
h
i
g
h
s
c
o
r
i
n
g
L
M
a
l
t
e
r
n
a
-
t
i
v
e
s
.
R
u
l
e
-
b
a
s
e
d
m
e
t
h
o
d
s
t
h
e
n
i
m
p
r
o
v
e
t
h
e
r
e
s
u
l
t
s
f
o
r
c
e
r
t
a
i
n
e
r
r
o
r
t
y
p
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
d
e
-
p
e
n
d
e
n
c
y
p
a
r
s
e
,
c
o
n
s
t
i
t
u
e
n
c
y
p
a
r
s
e
G
o
o
g
l
e
W
e
b
1
T
,
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
P
y
E
n
c
h
a
n
t
S
p
e
l
l
c
h
e
c
k
i
n
g
L
i
b
r
a
r
y
R
A
C
S
e
e
F
o
o
t
n
o
t
e
a
R
B
/
L
M
R
u
l
e
-
b
a
s
e
d
m
e
t
h
o
d
s
a
r
e
u
s
e
d
t
o
d
e
t
e
c
t
e
r
r
o
r
s
w
h
i
c
h
c
a
n
t
h
e
n
b
e
c
o
r
r
e
c
t
e
d
b
a
s
e
d
o
n
L
M
s
c
o
r
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
s
h
a
l
l
o
w
p
a
r
s
e
G
o
o
g
l
e
W
e
b
1
T
,
N
e
w
s
C
R
A
W
L
(
2
0
0
7
?
2
0
1
2
)
,
E
u
r
o
p
a
r
l
,
U
N
F
r
e
n
c
h
-
E
n
g
l
i
s
h
C
o
r
p
u
s
,
N
e
w
s
C
o
m
m
e
n
t
a
r
y
,
W
i
k
i
p
e
d
i
a
,
L
a
n
g
u
a
g
e
T
o
o
l
.
o
r
g
S
J
T
U
A
l
l
R
B
/
M
L
R
u
l
e
-
b
a
s
e
d
s
y
s
t
e
m
g
e
n
e
r
a
t
e
s
m
o
r
e
d
e
t
a
i
l
e
d
e
r
r
o
r
c
a
t
e
-
g
o
r
i
e
s
w
h
i
c
h
a
r
e
t
h
e
n
u
s
e
d
t
o
t
r
a
i
n
a
s
i
n
g
l
e
m
a
x
i
m
u
m
e
n
t
r
o
p
y
m
o
d
e
l
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
d
e
p
e
n
d
e
n
c
y
p
a
r
s
e
N
o
n
e
U
F
C
S
V
A
,
V
f
o
r
m
,
W
f
o
r
m
R
B
M
i
s
m
a
t
c
h
e
d
P
O
S
t
a
g
s
g
e
n
e
r
a
t
e
d
b
y
t
w
o
d
i
f
f
e
r
e
n
t
t
a
g
-
g
e
r
s
a
r
e
t
r
e
a
t
e
d
a
s
e
r
r
o
r
s
w
h
i
c
h
a
r
e
t
h
e
n
c
o
r
r
e
c
t
e
d
b
y
r
u
l
e
s
.
P
O
S
N
o
d
e
b
o
x
E
n
g
l
i
s
h
L
i
n
g
u
i
s
t
i
c
s
L
i
b
r
a
r
y
U
M
C
A
l
l
M
T
F
a
c
t
o
r
e
d
t
r
a
n
s
l
a
t
i
o
n
m
o
d
e
l
u
s
i
n
g
m
o
d
i
fi
e
d
P
O
S
t
a
g
s
a
n
d
m
o
r
p
h
o
l
o
g
y
a
s
f
e
a
t
u
r
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
p
r
e
fi
x
,
s
u
f
fi
x
,
s
t
e
m
W
M
T
2
0
1
4
M
o
n
o
l
i
n
g
u
a
l
D
a
t
a
T
a
b
l
e
6
:
P
r
o
fi
l
e
o
f
t
h
e
p
a
r
t
i
c
i
p
a
t
i
n
g
t
e
a
m
s
.
T
h
e
E
r
r
o
r
c
o
l
u
m
n
l
i
s
t
s
t
h
e
e
r
r
o
r
t
y
p
e
s
t
a
c
k
l
e
d
b
y
a
t
e
a
m
i
f
n
o
t
a
l
l
w
e
r
e
c
o
r
r
e
c
t
e
d
.
T
h
e
A
p
p
r
o
a
c
h
c
o
l
u
m
n
l
i
s
t
s
t
h
e
t
y
p
e
o
f
a
p
p
r
o
a
c
h
u
s
e
d
,
w
h
e
r
e
L
M
d
e
n
o
t
e
s
a
L
a
n
g
u
a
g
e
M
o
d
e
l
i
n
g
b
a
s
e
d
a
p
p
r
o
a
c
h
,
M
L
a
M
a
c
h
i
n
e
L
e
a
r
n
i
n
g
c
l
a
s
s
i
fi
e
r
b
a
s
e
d
a
p
p
r
o
a
c
h
,
M
T
a
s
t
a
t
i
s
t
i
c
a
l
M
a
c
h
i
n
e
T
r
a
n
s
l
a
t
i
o
n
a
p
p
r
o
a
c
h
,
a
n
d
R
B
a
R
u
l
e
-
B
a
s
e
d
a
p
p
r
o
a
c
h
.
a
T
h
e
R
A
C
t
e
a
m
u
s
e
s
r
u
l
e
s
t
o
c
o
r
r
e
c
t
e
r
r
o
r
t
y
p
e
s
t
h
a
t
d
i
f
f
e
r
f
r
o
m
t
h
e
2
8
o
f
fi
c
i
a
l
e
r
r
o
r
t
y
p
e
s
.
T
h
e
y
i
n
c
l
u
d
e
:
?
t
h
e
c
o
r
r
e
c
t
i
o
n
o
f
t
h
e
v
e
r
b
t
e
n
s
e
e
s
p
e
c
i
a
l
l
y
i
n
t
i
m
e
c
l
a
u
s
e
s
,
t
h
e
u
s
e
o
f
t
h
e
s
h
o
r
t
i
n
fi
n
i
t
i
v
e
a
f
t
e
r
m
o
d
a
l
s
,
t
h
e
p
o
s
i
t
i
o
n
o
f
f
r
e
q
u
e
n
c
y
a
d
v
e
r
b
s
i
n
a
s
e
n
t
e
n
c
e
,
s
u
b
j
e
c
t
-
v
e
r
b
a
g
r
e
e
m
e
n
t
,
w
o
r
d
o
r
d
e
r
i
n
i
n
t
e
r
r
o
g
a
t
i
v
e
s
e
n
t
e
n
c
e
s
,
p
u
n
c
t
u
a
t
i
o
n
a
c
c
o
m
p
a
n
y
i
n
g
c
e
r
t
a
i
n
l
e
x
i
c
a
l
e
l
e
m
e
n
t
s
,
t
h
e
u
s
e
o
f
a
r
t
i
c
l
e
s
,
o
f
c
o
r
r
e
l
a
t
i
v
e
s
,
e
t
c
.
?
9
T
y
p
e
A
M
U
C
A
M
B
C
U
U
I
I
I
T
B
I
P
N
N
A
R
A
N
T
H
U
P
K
U
P
O
S
T
R
A
C
S
J
T
U
U
F
C
U
M
C
V
t
1
0
.
6
6
1
9
.
1
2
3
.
7
9
1
.
7
4
0
.
8
8
1
4
.
1
8
1
0
.
6
1
1
2
.
3
0
3
.
7
6
2
6
.
1
9
4
.
1
7
0
.
0
0
1
4
.
8
4
V
m
1
0
.
8
1
2
2
.
5
8
0
.
0
0
0
.
0
0
0
.
0
0
2
9
.
0
3
0
.
0
0
3
.
2
3
0
.
0
0
3
5
.
9
0
0
.
0
0
0
.
0
0
6
.
4
5
V
0
1
7
.
8
6
2
5
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
6
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
9
3
V
f
o
r
m
2
2
.
7
6
2
4
.
3
7
2
1
.
4
3
1
.
8
5
4
.
6
3
2
7
.
6
2
2
4
.
3
0
2
5
.
6
4
1
.
8
9
2
7
.
3
5
3
.
6
7
0
.
9
5
1
4
.
6
8
S
V
A
2
4
.
3
0
3
1
.
3
6
7
0
.
3
4
1
.
0
6
1
4
.
1
4
2
7
.
5
0
6
2
.
6
7
1
7
.
3
1
2
0
.
5
6
3
0
.
3
6
1
4
.
8
5
2
8
.
7
0
1
4
.
4
1
A
r
t
O
r
D
e
t
1
5
.
5
2
4
9
.
4
8
5
8
.
8
5
0
.
6
8
0
.
3
3
5
0
.
8
9
3
3
.
6
3
8
.
2
0
5
4
.
4
5
0
.
6
3
1
2
.
5
4
0
.
0
0
2
4
.
0
5
N
n
5
8
.
7
4
5
4
.
1
1
5
6
.
1
0
4
.
4
9
1
0
.
3
6
5
7
.
3
2
4
6
.
7
6
4
1
.
7
8
5
5
.
6
0
3
6
.
4
5
1
0
.
1
1
0
.
0
0
1
7
.
0
3
N
p
o
s
1
4
.
2
9
7
.
6
9
4
.
7
6
0
.
0
0
0
.
0
0
2
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
4
.
7
6
4
.
5
5
0
.
0
0
5
.
2
6
P
f
o
r
m
2
2
.
2
2
2
2
.
5
8
7
.
1
4
0
.
0
0
0
.
0
0
1
4
.
8
1
1
6
.
1
3
1
2
.
0
0
0
.
0
0
3
.
7
0
0
.
0
0
0
.
0
0
1
7
.
2
4
P
r
e
f
9
.
3
3
1
9
.
3
5
1
.
3
2
0
.
0
0
0
.
0
0
1
0
.
0
0
1
.
2
0
1
.
3
5
1
.
3
2
0
.
0
0
0
.
0
0
0
.
0
0
1
2
.
0
5
P
r
e
p
1
8
.
4
1
3
8
.
2
6
1
5
.
4
5
2
.
1
2
0
.
0
0
2
9
.
7
2
1
9
.
4
2
0
.
0
0
2
.
2
8
0
.
0
0
7
.
9
2
0
.
0
0
1
4
.
5
5
W
c
i
1
2
.
0
0
9
.
1
7
0
.
9
4
0
.
3
6
0
.
3
5
7
.
5
5
0
.
6
3
1
.
6
5
1
.
2
7
0
.
3
4
0
.
0
0
0
.
0
0
3
.
2
3
W
a
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
W
f
o
r
m
4
5
.
5
6
4
5
.
0
5
1
7
.
2
4
4
.
0
5
2
.
6
0
3
9
.
0
8
1
4
.
8
1
2
5
.
8
8
6
.
4
9
1
1
.
2
5
1
.
3
9
1
.
3
0
1
6
.
4
6
W
t
o
n
e
8
1
.
8
2
3
6
.
3
6
3
6
.
3
6
0
.
0
0
0
.
0
0
1
4
.
2
9
0
.
0
0
0
.
0
0
2
8
.
5
7
0
.
0
0
1
6
.
6
7
0
.
0
0
4
4
.
4
4
S
r
u
n
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
m
o
d
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
p
a
r
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
f
r
a
g
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
0
0
0
.
0
0
2
5
.
0
0
S
s
u
b
7
.
8
9
1
4
.
6
3
0
.
0
0
0
.
0
0
2
.
2
7
1
5
.
3
8
0
.
0
0
0
.
0
0
9
.
5
2
2
.
3
8
2
.
2
7
0
.
0
0
6
.
9
8
W
O
i
n
c
0
.
0
0
3
.
0
3
0
.
0
0
3
.
5
7
0
.
0
0
3
.
0
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
6
.
6
7
W
O
a
d
v
0
.
0
0
4
7
.
6
2
0
.
0
0
1
2
.
5
0
0
.
0
0
4
3
.
7
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
4
4
.
4
4
T
r
a
n
s
1
3
.
4
3
2
1
.
4
3
2
.
8
6
1
.
4
3
0
.
0
0
1
1
.
2
5
1
.
4
1
1
.
5
2
2
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
1
2
.
1
6
M
e
c
2
9
.
3
5
2
8
.
7
5
1
5
.
7
9
1
.
0
2
4
.
3
3
3
6
.
6
9
6
.
6
7
3
0
.
2
8
3
6
.
6
1
4
3
.
5
1
0
.
5
1
0
.
0
0
1
6
.
8
0
R
l
o
c
?
5
.
4
1
2
0
.
1
6
7
.
7
6
0
.
0
0
5
.
5
6
1
8
.
6
4
9
.
6
8
1
0
.
4
8
9
.
2
6
9
.
0
9
2
.
5
0
0
.
0
0
1
5
.
8
4
C
i
t
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
O
t
h
e
r
s
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
.
1
2
0
.
0
0
0
.
0
0
0
.
0
0
U
m
7
.
6
9
9
.
0
9
0
.
0
0
0
.
0
0
0
.
0
0
4
.
0
0
0
.
0
0
1
5
.
7
9
8
.
7
0
8
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
T
a
b
l
e
9
:
R
e
c
a
l
l
(
i
n
%
)
f
o
r
e
a
c
h
e
r
r
o
r
t
y
p
e
w
i
t
h
o
u
t
a
l
t
e
r
n
a
t
i
v
e
a
n
s
w
e
r
s
,
i
n
d
i
c
a
t
i
n
g
h
o
w
w
e
l
l
e
a
c
h
t
e
a
m
p
e
r
f
o
r
m
s
a
g
a
i
n
s
t
a
p
a
r
t
i
c
u
l
a
r
e
r
r
o
r
t
y
p
e
.
10
T
y
p
e
A
M
U
C
A
M
B
C
U
U
I
I
I
T
B
I
P
N
N
A
R
A
N
T
H
U
P
K
U
P
O
S
T
R
A
C
S
J
T
U
U
F
C
U
M
C
V
t
1
1
.
6
1
2
0
.
0
0
5
.
7
9
1
.
9
0
0
.
9
8
1
6
.
1
8
1
2
.
9
0
1
4
.
1
6
3
.
3
1
2
9
.
1
7
4
.
5
9
0
.
0
0
1
7
.
6
0
V
m
1
1
.
1
1
2
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
2
9
.
0
3
0
.
0
0
3
.
3
3
0
.
0
0
3
9
.
4
7
0
.
0
0
0
.
0
0
7
.
6
9
V
0
1
9
.
2
3
2
9
.
6
3
0
.
0
0
0
.
0
0
0
.
0
0
3
8
.
7
1
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
0
.
7
7
V
f
o
r
m
2
3
.
9
3
2
7
.
4
2
2
1
.
0
5
1
.
9
2
4
.
8
5
2
9
.
0
9
2
4
.
0
7
2
6
.
7
9
2
.
8
3
2
6
.
9
6
3
.
7
7
0
.
9
8
1
5
.
3
2
S
V
A
2
5
.
0
0
3
3
.
9
0
7
2
.
4
1
1
.
1
1
1
4
.
7
4
2
8
.
5
7
6
3
.
7
6
1
7
.
8
2
2
2
.
8
6
3
2
.
4
3
1
5
.
4
6
3
0
.
0
9
1
4
.
9
5
A
r
t
O
r
D
e
t
1
8
.
7
5
5
4
.
7
4
6
7
.
3
8
1
.
8
1
0
.
3
6
5
4
.
4
2
3
7
.
9
6
9
.
6
5
5
9
.
4
1
0
.
6
6
1
4
.
6
3
0
.
0
0
3
3
.
4
2
N
n
6
2
.
1
4
6
2
.
0
3
6
5
.
5
3
4
.
9
1
1
2
.
2
9
6
2
.
6
9
5
2
.
8
9
5
1
.
0
1
6
4
.
1
4
4
2
.
6
7
1
1
.
9
3
0
.
0
0
2
2
.
2
2
N
p
o
s
2
3
.
3
3
4
0
.
0
0
4
.
3
5
0
.
0
0
0
.
0
0
2
9
.
1
7
0
.
0
0
0
.
0
0
0
.
0
0
9
.
5
2
4
.
5
5
0
.
0
0
1
3
.
6
4
P
f
o
r
m
2
2
.
2
2
2
3
.
3
3
7
.
6
9
0
.
0
0
0
.
0
0
1
4
.
8
1
1
7
.
8
6
1
2
.
5
0
0
.
0
0
4
.
0
0
0
.
0
0
0
.
0
0
2
2
.
2
2
P
r
e
f
9
.
5
9
1
8
.
5
6
1
.
3
2
0
.
0
0
0
.
0
0
9
.
8
0
1
.
2
5
1
.
3
3
1
.
3
7
0
.
0
0
0
.
0
0
0
.
0
0
1
1
.
1
1
P
r
e
p
1
8
.
4
1
3
8
.
6
3
1
8
.
2
2
2
.
2
1
0
.
0
0
3
0
.
2
8
2
0
.
4
2
0
.
0
0
2
.
2
5
0
.
0
0
8
.
9
5
0
.
0
0
1
6
.
9
8
W
c
i
1
5
.
2
6
1
5
.
1
8
0
.
9
6
0
.
7
9
0
.
3
8
8
.
0
5
1
.
3
3
3
.
1
7
1
.
9
4
0
.
3
6
0
.
0
0
0
.
0
0
9
.
5
7
W
a
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
W
f
o
r
m
4
5
.
4
5
4
6
.
5
9
2
1
.
1
1
2
.
9
0
2
.
6
7
4
0
.
9
1
1
5
.
5
8
2
7
.
3
8
6
.
4
9
1
2
.
5
0
1
.
4
7
1
.
3
7
1
7
.
1
1
W
t
o
n
e
8
8
.
2
4
3
8
.
4
6
5
2
.
6
3
0
.
0
0
0
.
0
0
1
2
.
5
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
3
3
.
3
3
0
.
0
0
5
5
.
5
6
S
r
u
n
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
m
o
d
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
p
a
r
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
f
r
a
g
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
1
6
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
0
0
0
.
0
0
2
0
.
0
0
S
s
u
b
7
.
8
9
1
4
.
2
9
0
.
0
0
0
.
0
0
2
.
3
3
1
5
.
3
8
0
.
0
0
0
.
0
0
9
.
7
6
2
.
4
4
2
.
3
3
0
.
0
0
6
.
9
8
W
O
i
n
c
0
.
0
0
3
.
4
5
0
.
0
0
4
.
0
0
0
.
0
0
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
7
.
1
4
W
O
a
d
v
0
.
0
0
5
0
.
0
0
0
.
0
0
1
6
.
6
7
0
.
0
0
4
4
.
4
4
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
T
r
a
n
s
1
4
.
5
2
2
2
.
3
9
3
.
0
8
1
.
6
7
0
.
0
0
1
1
.
8
4
1
.
5
6
1
.
6
4
2
.
8
2
0
.
0
0
0
.
0
0
0
.
0
0
2
0
.
7
8
M
e
c
3
1
.
5
6
3
0
.
6
7
1
7
.
4
7
1
.
1
3
4
.
7
9
3
7
.
2
8
7
.
1
7
3
1
.
6
9
3
7
.
8
8
4
5
.
8
2
1
.
1
0
0
.
0
0
2
2
.
3
1
R
l
o
c
?
5
.
4
5
2
6
.
4
7
7
.
3
8
0
.
0
0
5
.
6
2
2
1
.
4
3
1
1
.
3
4
1
2
.
3
8
1
1
.
8
2
1
0
.
0
0
3
.
6
6
0
.
0
0
2
9
.
6
6
C
i
t
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
O
t
h
e
r
s
0
.
0
0
3
.
0
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
U
m
7
.
6
9
9
.
0
9
0
.
0
0
0
.
0
0
0
.
0
0
4
.
3
5
0
.
0
0
1
5
.
0
0
4
.
5
5
8
.
7
0
0
.
0
0
0
.
0
0
0
.
0
0
T
a
b
l
e
1
0
:
R
e
c
a
l
l
(
i
n
%
)
f
o
r
e
a
c
h
e
r
r
o
r
t
y
p
e
w
i
t
h
a
l
t
e
r
n
a
t
i
v
e
a
n
s
w
e
r
s
,
i
n
d
i
c
a
t
i
n
g
h
o
w
w
e
l
l
e
a
c
h
t
e
a
m
p
e
r
f
o
r
m
s
a
g
a
i
n
s
t
a
p
a
r
t
i
c
u
l
a
r
e
r
r
o
r
t
y
p
e
.
11
Team ID Precision Recall F
0.5
CUUI 52.44 29.89 45.57
CAMB 46.70 34.30 43.55
AMU 45.68 23.78 38.58
POST 41.28 25.59 36.77
UMC 43.17 19.72 34.88
NTHU 38.34 21.12 32.97
PKU
?
36.64 15.96 29.10
RAC 35.63 16.73 29.06
NARA 23.83 31.95 25.11
SJTU 32.95 5.95 17.28
UFC
?
72.00 1.90 8.60
IPN
?
11.66 3.17 7.59
IITB
?
34.07 1.66 6.94
Table 8: Scores (in %) with alternative answers.
The teams that submitted their system output af-
ter the deadline have an asterisk affixed after their
team names.
Computing the recall of an error type is straight-
forward as the error type of each gold-standard
edit is provided. Conversely, computing the pre-
cision of each of the 28 error types is difficult as
the error type of each system edit is not available
since the submitted system output only contains
corrected sentences with no indication of the er-
ror type of the system edits. Predicting the error
type out of the 28 types for a particular system
edit not found in gold-standard annotation can be
tricky and error-prone. Therefore, we decided to
compute the per-type performance based on recall.
The recall scores when distinguished by error type
are shown in Tables 9 and 10.
6.1 Cross Annotator Comparison
To measure the agreement between our two an-
notators, we computed Cohen?s Kappa coefficient
(Cohen, 1960) for identification, which measures
the extent to which annotators agreed which words
needed correction and which did not, regardless
of the error type or correction. We obtained a
Kappa coefficient value of 0.43, indicating mod-
erate agreement (since it falls between 0.40 and
0.60). While this may seem low, it is worth point-
ing out that the Kappa coefficient does not take
into account the fact that there is often more than
one valid way to correct a sentence.
In addition to computing the performance of
each team against the gold standard annotations of
both annotators with and without alternative anno-
tations, we also had an opportunity to compare the
performance of each team?s system against each
annotator individually.
A recent concern is that there can be a high
degree of variability between individual annota-
tors which can dramatically affect a system?s out-
put score. For example, in a much simplified er-
ror correction task concerning only the correction
of prepositions, Tetreault and Chodorow (2008)
showed an actual difference of 10% precision and
5% recall between two annotators. Table 11 hence
shows the precision (P ), recall (R), and F
0.5
scores for all error types against the gold standard
annotations of each CoNLL-2014 annotator indi-
vidually.
The results show that there can indeed be a high
amount of disagreement between two annotators,
the most noticeable being precision in the UFC
system: precision was 70% for Annotator 2 but
only 28% for Annotator 1. This 42% difference is,
however, likely to be an extreme case, and most
teams show little more than 10% variation in pre-
cision and 5% variation in F
0.5
. Recall remained
fairly constant between annotators. 10% is still
a large margin however, and these results rein-
force the idea that error correction systems should
be judged against the gold-standard annotations of
multiple annotators.
Table 12 additionally shows how each annotator
compares against each other; i.e., what score An-
notator 1 gets if Annotator 2 was the gold standard
(part (a) of Table 12) and vice versa (part (b)).
The low F
0.5
scores of 45.36% and 38.54% rep-
resent an upper bound for system performance on
this data set and again emphasize the difficulty of
the task. The low human F
0.5
scores imply that
there are many ways to correct a sentence.
7 Conclusions
The CoNLL-2014 shared task saw the participa-
tion of 13 teams worldwide to evaluate their gram-
matical error correction systems on a common test
set, using a common evaluation metric and scorer.
The best systems in the shared task achieve an
F
0.5
score of 37.33% when it is scored without
alternative answers, and 45.57% with alternative
answers. There is still much room for improve-
ment in the accuracy of grammatical error correc-
tion systems. The evaluation data sets and scorer
used in our shared task serve as a benchmark for
12
Team ID Annotator 1 Annotator 2
P R F
0.5
P R F
0.5
AMU 27.30 13.55 22.69 35.49 12.90 26.29
CAMB 24.96 19.62 23.67 35.22 20.29 30.70
CUUI 26.05 15.60 22.97 36.91 16.37 29.51
IITB 23.33 0.88 3.82 24.18 0.66 2.99
IPN 5.80 1.25 3.36 9.62 1.51 4.63
NARA 13.54 19.20 14.38 18.74 19.69 18.92
NTHU 22.19 11.38 18.64 31.48 11.79 23.60
PKU 21.53 8.36 16.37 27.47 7.72 18.17
POST 22.39 13.89 19.94 29.53 13.42 23.81
RAC 19.68 8.28 15.43 28.52 8.80 19.70
SJTU 21.08 3.09 9.75 24.64 2.59 9.12
UFC 28.00 0.59 2.70 70.00 1.06 4.98
UMC 20.41 8.78 16.14 26.63 8.38 18.55
Table 11: Performance (in %) for each team?s output scored against the annotations of a single annotator.
P R F
0.5
50.47 32.29 45.36
(a)
P R F
0.5
37.14 45.38 38.54
(b)
Table 12: Performance (in %) for output of one gold standard annotation scored against the other gold
standard annotation: (a) The score of Annotator 1 if Annotator 2 was the gold standard, (b) The score of
Annotator 2 if Annotator 1 was the gold standard.
future research on grammatical error correction
4
.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
We thank our two annotators Mark Brooke and Di-
ane Nicholls who provided the gold-standard an-
notations.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Tiberiu Boros?, Stefan Daniel Dumitrescu, Adrian
Zafiu, Dan Tufis?, Verginica Mititelu Barbu, and
Paul Ionut? V?aduva. 2014. RACAI GEC ? a hybrid
approach to grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
4
http://www.comp.nus.edu.sg/?nlp/conll14st.html
Daniel Dahlmeier and Hwee Tou Ng. 2011a. Cor-
recting semantic collocation errors with L1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117.
Daniel Dahlmeier and Hwee Tou Ng. 2011b. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
13
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation, pages 242?249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54?62.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth Conference on Language
Resources and Evaluation, pages 449?454.
Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-
len Yannakoudakis, and Ekaterina Kochmar. 2014.
Grammatical error correction using hybrid systems
and type filtering. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
S. David Hernandez and Hiram Calvo. 2014. CoNLL
2014 shared task: Grammatical error correction with
a syntactic n-gram language model from a big cor-
pora. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task.
Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2014. The AMU system in the CoNLL-2014
shared task: Grammatical error correction by data-
intensive and feature-rich statistical machine trans-
lation. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Anoop Kunchukuttan, Sriram Chaudhury, and Pushpak
Bhattacharyya. 2014. Tuning a grammar correction
system for increased precision. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Proceedings of the Corpus Linguistics 2003
Conference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
961?970.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
Dan Roth, and Nizar Habash. 2014. The Illinois-
Columbia system in the CoNLL-2014 shared task.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning: Shared
Task.
Joel R. Tetreault and Martin Chodorow. 2008. Na-
tive judgments of non-native usage: Experiments in
preposition error detection. In COLING Workshop
on Human Judgments in Computational Linguistics,
Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358.
Peilu Wang, Zhongye Jia, and Hai Zhao. 2014a.
Grammatical error detection and correction using a
single maximum entropy model. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task.
Yiming Wang, Longyue Wang, Derek F. Wong,
Lidia S. Chao, Xiaodong Zeng, and Yi Lu. 2014b.
Factored statistical machine translation for gram-
matical error correction. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Yuanbin Wu and Hwee Tou Ng. 2013. Grammat-
ical error correction using integer linear program-
ming. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1456?1465.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189.
14
