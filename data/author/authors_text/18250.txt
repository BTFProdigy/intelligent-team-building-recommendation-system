Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447?1452,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Mined Coreference Chains as a Resource for a Semantic Task
Heike Adel and Hinrich Sch?utze
Center for Information and Language Processing
University of Munich
Germany
heike.adel@cis.lmu.de
Abstract
We propose to use coreference chains ex-
tracted from a large corpus as a resource
for semantic tasks. We extract three mil-
lion coreference chains and train word
embeddings on them. Then, we com-
pare these embeddings to word vectors de-
rived from raw text data and show that
coreference-based word embeddings im-
prove F
1
on the task of antonym classifi-
cation by up to .09.
1 Introduction
After more than a decade of work on coreference
resolution, coreference resolution systems have
reached a certain level of maturity (e.g., Recasens
et al. (2010)). While accuracy is far from perfect
and many phenomena such as bridging still pose
difficult research problems, the quality of the out-
put of these systems is high enough to be useful
for many applications.
In this paper, we propose to run coreference res-
olution systems on large corpora, to collect the
coreference chains found and to use them as a re-
source for solving semantic tasks. This amounts
to using mined coreference chains as an automat-
ically compiled resource similar to the way cooc-
currence statistics, dependency pairs and aligned
parallel corpora are used in many applications in
NLP. Coreference chains have interesting comple-
mentary properties compared to these other re-
sources. For example, it is difficult to distinguish
true semantic similarity (e.g., ?cows? ? ?cattle?)
from mere associational relatedness (e.g., ?cows?
? ?milk?) based on cooccurrence statistics. In con-
trast, coreference chains should be able to make
that distinction since only ?cows? and ?cattle? can
occur in the same coreference chain, not ?cows?
and ?milk?.
As a proof of concept we compile a resource
of mined coreference chains from the Gigaword
corpus and apply it to the task of identifying
antonyms. We induce distributed representations
for words based on (i) cooccurrence statistics and
(ii) mined coreference chains and show that a com-
bination of both outperforms cooccurrence statis-
tics on antonym identification.
In summary, we make two contributions. First,
we propose to use coreference chains mined from
large corpora as a resource in NLP and publish the
first such resource. Second, in a proof of concept
study, we show that they can be used to solve a se-
mantic task ? antonym identification ? better than
is possible with existing resources.
We focus on the task of finding antonyms in this
paper since antonyms usually are distributionally
similar but semantically dissimilar words. Hence,
it is often not possible to distinguish them from
synonyms with distributional models only. In con-
trast, we expect that the coreference-based repre-
sentations can provide useful complementary in-
formation to this task. In general, coreference-
based similarity can however be used as an addi-
tional feature for any task that distributional simi-
larity is useful for. Thus, our coreference resource
can be applied to a variety of NLP tasks, e.g. find-
ing alternative names for entities (in a way similar
to Wikipedia anchors) for tasks in the context of
knowledge base population.
The remainder of the paper is organized as fol-
lows. In Section 2, we describe how we create
word embeddings and how our antonym classi-
fier works. The word embeddings are then eval-
uated qualitatively, quantitatively and for the task
of antonym detection (Section 3). Section 4 dis-
cusses related work and Section 5 concludes.
2 System description
2.1 Coreference-based embeddings
Standard word embeddings derived from text data
may not be able to distinguish between semantic
1447
text-based coref.-based
his my, their, her, your, our he, him, himself, zechariah, ancestor
woman man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia
Table 1: Nearest neighbors of ?his? / ?woman? for text-based & coreference-based embeddings
association and true synonymy. As a result, syn-
onyms and antonyms may be mapped to similar
word vectors (Yih et al., 2012). For many NLP
tasks, however, information about true synonymy
or antonymy may be important.
In this paper, we develop two different word
embeddings: embeddings calculated on raw text
data and embeddings derived from automatically
extracted coreference chains. For the calcula-
tion of the vector representations, the word2vec
toolkit
1
by Mikolov et al. (2013) is applied. We
use the skip-gram model for our experiments be-
cause its results for semantic similarity are better
according to Mikolov et al. (2013). We train a
first model on a subset of English Gigaword data.
2
In the following sections, we call the resulting
embeddings text-based. To improve the seman-
tic similarities of the vectors, we prepare another
training text consisting of coreference chains. We
use CoreNLP (Lee et al., 2011) to extract coref-
erence chains from the Gigaword corpus. Then
we build a skip-gram model on these coreference
chains. The extracted coreference chains are pro-
vided as an additional resource to this paper
3
. Al-
though they have been developed using only a
publicly available toolkit, we expect this resource
to be helpful for other researchers since the pro-
cess to extract the coreference chains of such a
large text corpus takes several weeks on multi-core
machines. In total, we extracted 3.1M coreference
chains. 2.7M of them consist of at least two differ-
ent markables. The median (mean) length of the
chains is 3 (4.0) and the median (mean) length of
a markable is 1 (2.7). To train word embeddings,
the markables of each coreference chain are con-
catenated to one text line. These lines are used as
input sentences for word2vec. We refer to the re-
sulting embeddings as coreference-based.
2.2 Antonym detection
In the following experiments, we use word em-
beddings to discriminate antonyms from non-
antonyms. We formalize this as a supervised clas-
1
https://code.google.com/p/word2vec
2
LDC2012T21, Agence France-Presse 2010
3
https://code.google.com/p/cistern
sification task and apply SVMs (Chang and Lin,
2011).
The following features are used to represent a
pair of two words w and v:
1. cosine similarity of the text-based embed-
dings of w and v;
2. inverse rank of v in the nearest text-based
neighbors of w;
3. cosine similarity of the coreference-based
embeddings of w and v;
4. inverse rank of v in the nearest coreference-
based neighbors of w;
5. difference of (1) and (3);
6. difference of (2) and (4).
We experiment with three different subsets of
these features: text-based (1 and 2), coreference-
based (3 and 4) and all features.
3 Experiments and results
3.1 Qualitative analysis of word vectors
Table 1 lists the five nearest neighbors based on
cosine similarity of text-based and coreference-
based word vectors for ?his? and ?woman?.
We see that the two types of embeddings cap-
ture different notions of similarity. Unlike the text-
based neighbors, the coreference-based neighbors
have the same gender. The text-based neighbors
are mutually substitutable words, but substitution
seems to change the meaning more than for the
coreference-based neighbors.
In Figure 1, we illustrate the vectors for some
antonyms (connected by lines).
For reducing the dimensionality of the vector
space to 2D, we applied the t-SNE toolkit
4
. It uses
stochastic neighbor embedding with a Student?s
t-distribution to map high dimensional vectors
into a lower dimensional space (Van der Maaten
and Hinton, 2008). The Figure shows that the
coreference-based word embeddings are able to
4
http://homepage.tudelft.nl/19j49/t-SNE.html
1448
1.5 1.0 0.5 0.0 0.5 1.01.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
willingness
innocence
guiltliteracy
unwillingness
toughness
illiteracy
frailty
1.5 1.0 0.5 0.0 0.5 1.01.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
willingness
innocence
guilt
unwillingness
frailty
toughness literacy
illiteracy
Figure 1: 2D-positions of words in the text-based (top) and coreference-based embeddings (bottom)
enlarge the distance between antonyms (especially
for guilt vs. innocence and toughness vs. frailty)
compared to text-based word vectors.
3.2 Quantitative analysis of word vectors
To verify that coreference-based embeddings bet-
ter represent semantic components relevant to
coreference, we split our coreference resource into
two parts (about 85% and 15% of the data), trained
embeddings on the first part and computed the co-
sine similarity ? both text-based and coreference-
based ? for each pair of words occurring in the
same coreference chain in the second part. The
statistics in Table 2 confirm that coreference-based
vectors have higher similarity within chains than
text-based vectors.
3.3 Experimental setup
We formalize antonym detection as a binary classi-
fication task. Given a target word w and one of its
nearest neighbors v, the classifier decides whether
v is an antonym of w. Our data set is a set of pairs,
each consisting of a target word w and a candi-
date v. For all word types of our vocabulary, we
search for antonyms using the online dictionary
Merriam Webster.
5
The resulting list is provided
as an additional resource
6
. It contains 6225 words
with antonyms. Positive training examples are col-
lected by checking if the 500 nearest text-based
neighbors of w contain one of the antonyms listed
by Webster. Negative training examples are cre-
ated by replacing the antonym with a random word
from the 500 nearest neighbors that is not listed as
5
http://www.merriam-webster.com
6
https://code.google.com/p/cistern
an antonym. By selecting both the positive and
the negative examples from the nearest neighbors
of the word vectors, we intend to develop a task
which is hard to solve: The classifier has to find
the small portion of semantically dissimilar words
(i.e., antonyms) among distributionally very simi-
lar words. The total number of positive and nega-
tive examples is 2337 each. The data are split into
training (80%), development (10%) and test (10%)
sets.
In initial experiments, we found only a small
difference in antonym classification performance
between text-based and coreference-based fea-
tures. When analyzing the errors, we realized that
our rationale for using coreference-based embed-
dings only applies to nouns, not to other parts of
speech. This will be discussed in detail below. We
therefore run our experiments in two modes: all
word classification (all pairs are considered) and
noun classification (only pairs are considered for
which the target word is a noun). We use the Stan-
ford part-of-speech tagger (Toutanova et al., 2003)
to determine whether a word is a noun or not.
Our classifier is a radial basis function (rbf) sup-
port vector machine (SVM). The rbf kernel per-
formed better than a linear kernel in initial exper-
iments. The SVM parameters C and ? are opti-
mized on the development set. The representation
of target-candidate pairs consists of the features
described in Section 2.
3.4 Experimental results and discussion
We perform the experiments with the three differ-
ent feature sets described in Section 2: text-based,
coreference-based and all features. Table 3 shows
1449
all word classification noun classification
development set test set development set test set
feature set P R F
1
P R F
1
P R F
1
P R F
1
text-based .83 .66 .74 .74 .55 .63 .91 .61 .73 .74 .51 .60
coreference-based .67 .42 .51 .65 .43 .52 .86 .47 .61 .77 .45 .57
text+coref .79 .65 .72 .75 .58 .66 .88 .70 .78 .79 .61 .69
Table 3: Results for different feature sets. Best result in each column in bold.
minimum maximum median
text-based vectors -0.350 0.998 0.156
coref.-based vectors -0.318 0.999 0.161
Table 2: Cosine similarity of words in the same
coreference chain
results for development and test sets.
For all word classification, coreference-based
features do not improve performance on the de-
velopment set (e.g., F
1
is .74 for text-based vs .72
for text+coref). On the test set, however, the com-
bination of all features (text+coref) has better per-
formance than text-based alone: .66 vs .63.
For noun classification, using coreference-
based features in addition to text-based features
improves results on development set (F
1
is .78 vs
.73) and test set (.69 vs .60).
These results show that mined coreference
chains are a useful resource and provide infor-
mation that is complementary to other methods.
Even though adding coreference-based embed-
dings improves performance on antonym classi-
fication, the experiments also show that using
only coreference-based embeddings is almost al-
ways worse than using only text-based embed-
dings. This is not surprising given that the amount
of training data for the word embeddings is differ-
ent in the two cases. Coreference chains provide
only a small subset of the word-word relations that
are given to the word2vec skip-gram model when
applied to raw text. If the sizes of the training data
sets were similar in the two cases, we would ex-
pect performance to be comparable.
In the beginning, our hypothesis was that coref-
erence information should be helpful for antonym
classification in general. When we performed an
error analysis for our initial results, we realized
that this hypothesis only holds for nouns. Other
types of words cooccurring in coreference chains
are not more likely to be synonyms than words
cooccurring in text windows. Two contexts that
illustrate this point are ?bright sides, but also dif-
ficult and dark ones? and ?a series of black and
white shots? (elements of coreference chains in
italics). Thus, adjectives with opposite meanings
can cooccur in coreference chains just as they can
cooccur in window-based contexts. For nouns, it
is much less likely that the same coreference chain
will contain both a noun and its antonym since ?
by definition ? markables in a coreference chain
refer to the same identical entity.
4 Related work
Traditionally, words have been represented by
vectors of the size of the vocabulary with a one at
the word index and zeros otherwise (one-hot vec-
tors). However, this approach cannot handle un-
known words (Turian et al., 2010) and similari-
ties among words cannot be represented (Mikolov
et al., 2013). Therefore, distributed word repre-
sentations (embeddings) become more and more
popular. They are low-dimensional, real-valued
vectors. Mikolov et al. (2013) have published
word2vec, a toolkit that provides different possi-
bilities to estimate word embeddings (cbow model
and skip-gram model). They show that the re-
sulting word vectors capture semantic and syntac-
tic relationships of words. Baroni et al. (2014)
show that word embeddings are able to outper-
form count based word vectors on a variety of
NLP tasks. Recently, Levy and Goldberg (2014)
have generalized the skip-gram model to include
not only linear but arbitrary contexts like contexts
derived from dependency parse trees. Andreas and
Klein (2014) investigate the amount of additional
information continuous word embeddings could
add to a constituency parser and find that most
of their information is redundant to what can be
learned from labeled parse trees. In (Yih et al.,
2012), the vector space representation of words is
modified so that high positive similarities are as-
signed to synonyms and high negative similarities
to antonyms. For this, latent semantic analysis is
applied to a matrix of thesaurus entries. The val-
1450
ues representing antonyms are negated.
There has been a great deal of work on apply-
ing the vector space model and cosine similarity
to find synonyms or antonyms. Hagiwara et al.
(2006) represent each word as a vector with cooc-
currence frequencies of words and contexts as el-
ements, normalized by the inverse document fre-
quency. The authors investigate three types of con-
textual information (dependency, sentence cooc-
currence and proximity) and find that a combi-
nation of them leads to the most stable results.
Schulte im Walde and K?oper (2013) build a vector
space model on lexico-syntactic patterns and ap-
ply a Rocchio classifier to distinguish synonyms
from antonyms, among other tasks. Van der Plas
and Tiedemann (2006) use automatically aligned
translations of the same text in different languages
to build context vectors. Based on these vectors,
they detect synonyms.
In contrast, there are also studies using linguis-
tic knowledge from external resources: Senellart
and Blondel (2008) propose a method for syn-
onym detection based on graph similarity in a
graph generated using the definitions of a mono-
lingual dictionary. Harabagiu et al. (2006) rec-
ognize antonymy by generating antonymy chains
based on WordNet relations. Mohammad et al.
(2008) look for the word with the highest degree of
antonymy to a given target word among five candi-
dates. For this task, they use thesaurus information
and the similarity of the contexts of two contrast-
ing words. Lin et al. (2003) use Hearst patterns
to distiguish synonyms from antonyms. Work by
Turney (2008) is similar except that the patterns
are learned.
Except for the publicly available coreference
resolution system, our approach does not need ex-
ternal resources such as dictionaries or bilingual
corpora and no human labor is required. Thus,
it can be easily applied to any corpus in any lan-
guage as long as there exists a coreference resolu-
tion system in this language. The pattern-based
approach (Lin et al., 2003; Turney, 2008) dis-
cussed above also needs few resources. In contrast
to our work, it relies on patterns and might there-
fore restrict the number of recognizable synonyms
and antonyms to those appearing in the context of
the pre-defined patterns. On the other hand, pat-
terns could explicitely distinguish contexts typical
for synonyms from contexts for antonyms. Hence,
we plan to combine our coreference-based method
with pattern-based methods in the future.
5 Conclusion
In this paper, we showed that mined corefer-
ence chains can be used for creating word em-
beddings that capture a type of semantic sim-
ilarity that is different from the one captured
by standard text-based embeddings. We showed
that coreference-based embeddings improve per-
formance of antonym classification by .09 F
1
compared to using only text-based embeddings.
We achieved precision values of up to .79, recall
values of up to .61 and F
1
scores of up to .69.
Acknowledgments
This work was supported by DFG (grant SCHU
2246/4-2).
References
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In ACL,
pages 822?827.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238?247.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2006. Selection of effective contextual
information for automatic synonym acquisition. In
COLING/ACL, pages 353?360.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In AAAI, volume 6, pages 755?762.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In CoNLL: Shared Task, pages 28?34.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL, pages 302?308.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In IJCAI, pages 1492?1493.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Workshop at ICLR.
1451
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982?991.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In 5th International Workshop
on Semantic Evaluation, pages 1?8.
Sabine Schulte im Walde and Maximilian K?oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web, pages 184?
198. Springer.
Pierre Senellart and Vincent D Blondel. 2008. Auto-
matic discovery of similar words. In Survey of Text
Mining II, pages 25?44. Springer.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT, pages 252?259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL, pages 384?
394.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
COLING, pages 905?912.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(11):2579?2605.
Lonneke Van der Plas and J?org Tiedemann. 2006.
Finding synonyms using automatic word alignment
and measures of distributional similarity. In COL-
ING/ACL, pages 866?873.
Wen-tau Yih, Geoffrey Zweig, and John C Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP/CoNLL, pages 1212?1222.
1452
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206?211,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combination of Recurrent Neural Networks and Factored Language
Models for Code-Switching Language Modeling
Heike Adel
heike.adel@student.kit.edu
Ngoc Thang Vu
Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)
thang.vu@kit.edu
Tanja Schultz
tanja.schultz@kit.edu
Abstract
In this paper, we investigate the appli-
cation of recurrent neural network lan-
guage models (RNNLM) and factored
language models (FLM) to the task of
language modeling for Code-Switching
speech. We present a way to integrate part-
of-speech tags (POS) and language in-
formation (LID) into these models which
leads to significant improvements in terms
of perplexity. Furthermore, a comparison
between RNNLMs and FLMs and a de-
tailed analysis of perplexities on the dif-
ferent backoff levels are performed. Fi-
nally, we show that recurrent neural net-
works and factored language models can
be combined using linear interpolation to
achieve the best performance. The final
combined language model provides 37.8%
relative improvement in terms of perplex-
ity on the SEAME development set and
a relative improvement of 32.7% on the
evaluation set compared to the traditional
n-gram language model.
Index Terms: multilingual speech processing,
code switching, language modeling, recurrent
neural networks, factored language models
1 Introduction
Code-Switching (CS) speech is defined as speech
that contains more than one language (?code?). It
is a common phenomenon in multilingual com-
munities (Auer, 1999a). For the automated pro-
cessing of spoken communication in these sce-
narios, a speech recognition system must be able
to handle code switches. However, the compo-
nents of speech recognition systems are usually
trained on monolingual data. Furthermore, there
is a lack of bilingual training data. While there
have been promising research results in the area
of acoustic modeling, only few approaches so far
address Code-Switching in the language model.
Recently, it has been shown that recurrent neu-
ral network language models (RNNLMs) can im-
prove perplexity and error rates in speech recogni-
tion systems in comparison to traditional n-gram
approaches (Mikolov et al, 2010; Mikolov et al,
2011). One reason for that is their ability to han-
dle longer contexts. Furthermore, the integration
of additional features as input is rather straight-
forward due to their structure. On the other hand,
factored language models (FLMs) have been used
successfully for languages with rich morphology
due to their ability to process syntactical features,
such as word stems or part-of-speech tags (Bilmes
and Kirchhoff, 2003; El-Desoky et al, 2010).
The main contribution of this paper is the appli-
cation of RNNLMs and FLMs to the challenging
task of Code-Switching. Furthermore, the two dif-
ferent models are combined using linear interpo-
lation. In addition, a comparison between them is
provided including a detailed analysis to explain
their results.
2 Related Work
For this work, three different topics are investi-
gated and combined: linguistic investigation of
Code-Switching, recurrent neural network lan-
guage modeling and factored language models.
In (Muysken, 2000; Poplack, 1978; Bokamba,
1989), it is observed that code switches occur at
positions in an utterance where they do not violate
the syntactical rules of the involved languages. On
the one hand, Code-Switching can be regarded as
a speaker dependent phenomenon (Auer, 1999b;
Vu, Adel et al, 2013). On the other hand, par-
ticular Code-Switching patterns are shared across
speakers (Poplack, 1980). It can be observed that
part-of-speech tags may predict Code-Switching
points more reliable than words themselves. The
206
authors of (Solorio et al, 2008a) predict Code-
Switching points using several linguistic features,
such as word form, language ID, part-of-speech
tags or the position of the word relative to the
phrase (BIO). The best result is obtained by com-
bining those features. In (Chan et.al., 2006), four
different kinds of n-gram language models are
compared to predict Code-Switching. It is dis-
covered that clustering all foreign words into their
part-of-speech classes leads to the best perfor-
mance.
In the last years, neural networks have been used
for a variety of tasks, including language model-
ing (Mikolov et al, 2010). Recurrent neural net-
works are able to handle long-term contexts since
the input vector does not only contain the cur-
rent word but also the previous hidden layer. It
is shown that these networks outperform tradi-
tional language models, such as n-grams which
only contain very limited histories. In (Mikolov
et al, 2011), the network is extended by factoriz-
ing the output layer into classes to accelerate the
training and testing processes. The input layer
can be augmented to model features, such as part-
of-speech tags (Shi et al, 2011; Adel, Vu et al,
2013). In (Adel, Vu et al, 2013), recurrent neural
networks are applied to Code-Switching speech. It
is shown that the integration of POS tags into the
neural network, which predicts the next language
as well as the next word, leads to significant per-
plexity reductions.
A factored language model refers to a word as a
vector of features, such as the word itself, morpho-
logical classes, POS tags or word stems. Hence, it
provides another possibility to integrate syntacti-
cal features into the language modeling process.
In (Bilmes and Kirchhoff, 2003), it is shown that
factored language models are able to outperform
standard n-gram techniques in terms of perplexity.
In the same paper, generalized parallel backoff is
introduced. This technique can be used to general-
ize traditional backoff methods and to improve the
performance of factored language models. Due to
the integration of various features, it is possible to
handle rich morphology in languages like Arabic
or Turkish (Duh and Kirchhoff, 2004; El-Desoky
et al, 2010).
3 Code-Switching Language Modeling
3.1 Motivation
Since there is a lack of Code-Switching data, lan-
guage modeling is a challenging task. Traditional
n-gram approaches may not provide reliable esti-
mates. Hence, more general features than words
should be integrated into the language models.
Therefore, we apply recurrent neural networks and
factored language models. As features, we use
part-of-speech tags and language identifiers.
3.2 Using Recurrent Neural Networks As
Language Model
This section describes the structure of the recur-
rent neural network (RNNLM) that we use as
Code-Switching language model. It has been pro-
posed in (Adel, Vu et al, 2013) and is illustrated
in figure 1.
w(t)
 f(t)
s(t)
 y(t)
 c(t)
U1  V
WU2
Figure 1: RNNLM for Code-Switching
(based upon a figure in (Mikolov et al, 2011))
Vectorw(t), which represents the current word us-
ing 1-of-N coding, forms the input of the recur-
rent neural network. Thus, its dimension equals
the size of the vocabulary. Vector s(t) con-
tains the state of the network and is called ?hid-
den layer?. The network is trained using back-
propagation through time (BPTT), an extension of
the back-propagation algorithm for recurrent neu-
ral networks. With BPTT, the error is propagated
through recurrent connections back in time for a
specific number of time steps t. Hence, the net-
work is able to remember information for several
time steps. The matrices U1, U2, V , and W con-
tain the weights for the connections between the
layers. These weights are learned during the train-
ing phase. Moreover, the output layer is factorized
207
into classes which provide language information.
In this work, four classes are used: English, Man-
darin, other languages and particles. Vector c(t)
contains the probabilities for each class and vector
y(t) provides the probabilities for each word given
its class. Hence, the probability P (wi|history) is
computed as shown in equation 1.
P (wi|history) = P (ci|s(t))P (wi|ci, s(t)) (1)
It is intended to not only predict the next word but
also the next language. Hence according to equa-
tion 1, the probability of the next language is com-
puted first and then the probability of each word
given the language. Furthermore, a vector f(t)
is added to the input layer. It provides features
(in this work part-of-speech tags) corresponding
to the current word. Thus, not only the current
word is activated but also its features. Since the
POS tags are integrated into the input layer, they
are also propagated into the hidden layer and back-
propagated into its history s(t). Hence, not only
the previous feature is stored in the history but also
features from several time steps in the past.
3.3 Using Factored Language Models
Factored language models (FLM) are another ap-
proach to integrate syntactical features, such as
part-of-speech tags or language identifiers into the
language modeling process. Each word is re-
garded as a sequence of features which are used
for the computation of the n-gram probabilities.
If a particular sequence of features has not been
detected in the training data, backoff techniques
will be applied. For our task of Code-Switching,
we develop two different models: One model with
only part-of-speech tags as features and one model
including also language information tags. Un-
fortunately, the number of possible parameters is
rather high: Different feature combinations from
different time steps can be used to predict the
next word (conditioning factors), different back-
off paths and different smoothing methods may
be applied. To detect useful parameters, the ge-
netic algorithm described in (Duh and Kirchhoff,
2004) is used. It is an evolution-inspired technique
that encodes the parameters of an FLM as binary
strings (genes). First, an initializing set of genes is
generated. Then, a loop follows that evaluates the
fitness of the genes and mutates them until their
average fitness is not improved any more. As fit-
ness value, the inverse perplexity of the FLM cor-
responding to the gene on the development set is
Wt-1        Pt-1     Pt-2
Wt-1        Pt-2 Wt-1        Pt-1
Pt-2 Wt-1 Pt-1
unigram
Figure 2: Backoff graph of the FLM
used. Hence, parameter solutions with lower per-
plexities are preferred in the selection of the genes
for the following iteration. In (Duh and Kirch-
hoff, 2004), it is shown that this genetic method
outperforms both knowledge-based and random-
ized choices. For the case of part-of-speech tags
as features, the method results in three condition-
ing factors: the previous word Wt?1 and the two
previous POS tags Pt?1 and Pt?2. The backoff
graph obtained by the algorithm is illustrated in
figure 2. According to the result of the genetic al-
gorithm, different smoothing methods are used at
different backoff levels: For the backoff from three
factors to two factors, Kneser-Ney discounting is
applied. If the probabilities for the factor combi-
nation Wt?1Pt?2 could not be estimated reliably,
absolute discounting is used. In all other cases,
Witten-Bell discounting is applied. An overview
of the different smoothing methods can be found
in (Rosenfeld, 2000).
4 Experiments and Results
4.1 Data Corpus
SEAME (South East Asia Mandarin-English) is a
conversational Mandarin-English Code-Switching
speech corpus recorded from Singaporean and
Malaysian speakers (D.C. Lyu et al, 2011). It
was used for the research project ?Code-Switch?
jointly performed by Nanyang Technological Uni-
versity (NTU) and Karlsruhe Institute of Technol-
ogy (KIT). The recordings consist of spontanously
spoken interviews and conversations of about 63
hours of audio data. For this task, we deleted all
hesitations and divided the transcribed words into
four categories: English words, Mandarin words,
particles (Singaporean and Malaysian discourse
particles) and others (other languages). These cat-
egories are used as language information in the
language models. The average number of Code-
Switching points between Mandarin and English
208
is 2.6 per utterance and the duration of monolin-
gual segments is quite short: The average dura-
tion of English and Mandarin segments is only
0.67 seconds and 0.81 seconds respectively. In to-
tal, the corpus contains 9,210 unique English and
7,471 unique Mandarin vocabulary words. We di-
vided the corpus into three disjoint sets (training,
development and test set) and assigned the data
based on several criteria (gender, speaking style,
ratio of Singaporean and Malaysian speakers, ra-
tio of the four categories, and the duration in each
set). Table 1 lists the statistics of the corpus in
these sets.
Train set Dev set Eval set
# Speakers 139 8 8
Duration(hrs) 59.2 2.1 1.5
# Utterances 48,040 1,943 1,018
# Token 525,168 23,776 11,294
Table 1: Statistics of the SEAME corpus
4.2 POS Tagger for Code-Switching Speech
To be able to assign part-of-speech tags to our
bilingual text corpus, we apply the POS tagger
described in (Schultz et al, 2010) and (Adel, Vu
et al, 2013). It consists of two different mono-
lingual (Stanford log-linear) taggers (Toutanova
et al, 2003; Toutanova et al, 2000) and a com-
bination of their results. While (Solorio et al,
2008b) passes the whole Code-Switching text to
both monolingual taggers and combines their re-
sults using different heuristics, in this work, the
text is splitted into different languages first. The
tagging process is illustrated in figure 3.
Mandarin is determined as matrix language (the
main language of an utterance) and English as em-
bedded language. If three or more words of the
embedded language are detected, they are passed
to the English tagger. The rest of the text is passed
to the Mandarin tagger, even if it contains foreign
words. The idea is to provide the tagger as much
context as possible. Since most English words in
the Mandarin segments are falsely tagged as nouns
by the Mandarin tagger, a postprocessing step is
applied. It passes all foreign words of the Man-
darin segments to the English tagger in order to
replace the wrong tags with the correct ones.
Wt-1 P2un-igr-gamu?ut-i? PiW???a?n-igr-gamu?u?gnP?
??1a21
?igr-gauP?-i???u?a?a?a????? ??PiPig1a21
??1-gga u? ut-i?- Pi??1-gga u? u?gnP?
?r1?1 ?1?1?gnP??g?i1? Pi a?PiPig1a21
????a?Pig?
Figure 3: Tagging of Code-Switching speech
4.3 Evaluation
For evaluation, we compute the perplexity of each
language model on the SEAME development and
evaluation set und perform an analysis of the dif-
ferent back-off levels to understand in detail the
behavior of each language model. A traditional 3-
gram LM trained with the SEAME transcriptions
serves as baseline.
4.3.1 LM Performance
The language models are evaluated in terms of per-
plexity. Table 2 presents the results on the devel-
opment and test set.
Model dev set test set
Baseline 3-gram 285.87 285.25
FLM (pos) 263.57 271.57
FLM (pos + lid) 263.84 276.99
RNNLM (pos) 233.50 268.05
RNNLM (pos + lid) 219.85 239.21
Table 2: Perplexity results
It can be noticed that both the RNNLM and the
FLM model outperform the traditional 3-gram
model. Hence, adding syntactical features im-
proves the word prediction. For the FLM, it leads
to no improvement to add the language identifier
as feature. In contrast, clustering the words into
their languages on the output layer of the RNNLM
leads to lower perplexities.
209
4.3.2 Backoff Level Analysis
To understand the different results of the RNNLM
and the FLM, an analysis similar to the one de-
scribed in (Oparin et al, 2012) is performed. For
each word, the backoff-level of the n-gram model
is observed. Then, a level-dependent perplexity is
computed for each model as shown in equation 2.
PPLk = 10
? 1Nk
?
wk
log10P (wk|hk) (2)
In the equation, k denotes the backoff-level, Nk
the number of words on this level, wk the current
word and hk its history. Table 3 shows how often
each backoff-level is used and presents the level-
dependent perplexities of each model on the de-
velopment set.
1-gram 2-gram 3-gram
# occurences 6894 11628 6226
Baseline 3-gram 5,786.24 165.82 28.28
FLM (pos) 4,950.31 147.70 30.99
RNNLM 3,231.02 151.67 21.24
Table 3: Backoff-level-dependent PPLs
In case of backoff to the 2-gram, the FLM pro-
vides the best perplexity, while for the 3-gram and
backoff to the 1-gram, the RNNLM performs best.
This may be correlated with the better over-all per-
plexity of the RNNLM in comparison to the FLM.
Nevertheless, the backoff to the 2-gram is used
about twice as often as the backoff to the 1-gram
or the 3-gram.
4.4 LM Interpolation
The different results of RNNLM and FLM show
that they provide different estimates of the next
word. Thus, a combination of them may reduce
the perplexities of table 2. Hence, we apply lin-
ear interpolation to the probabilities of each two
models as shown in equation 3.
P (w|h) = ??PM1(w|h)+(1??)?PM2(w|h) (3)
The equation shows the computation of the pob-
ability for word w given its history h. PM1 de-
notes the probability provided by the first model
and PM2 the probability from the second model.
Table 4 shows the results of this experiment. The
weights are optimized on the development set.
The interpolation of RNNLM and FLM leads to
the best results. This may be caused by the supe-
rior backoff-level-dependent PPLs in comparison
PPL PPL
Model weight on dev on eval
FLM + 3-gram 0.7, 0.3 211.13 227.57
RNNLM + 3-gram 0.8, 0.2 206.49 227.08
RNNLM + FLM 0.6, 0.4 177.79 192.08
Table 4: Perplexities after interpolation
to the 3-gram model. While the RNNLM performs
better for the 3-gram and for the backoff to the 1-
gram, the FLM performs the best in case of back-
off to the 2-gram which is used more often than
the other levels (table 3).
5 Conclusions
In this paper, we presented two different methods
for language modeling of Code-Switching speech:
Recurrent neural networks and factored language
models. We integrated part-of-speech tags and
language information to improve the performance
of the language models. In addition, we ana-
lyzed their behavior on the different backoff lev-
els. While the FLM performed better in case of
backoff to the 2-gram, the RNNLM led to a bet-
ter over-all performance. Finally, the models were
combined using linear interpolation. The com-
bined language model provided 37.8% relative im-
provement in terms of perplexity on the SEAME
development set and a relative improvement of
32.7% on the evaluation set compared to the tra-
ditional n-gram LM.
References
H. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz.
2013 Recurrent Neural Network Language Model-
ing for Code Switching Conversational Speech In:
Proceedings of ICASSP 2013.
P. Auer 1999 Code-Switching in Conversation , Rout-
ledge.
P. Auer 1999 From codeswitching via language mixing
to fused lects toward a dynamic typology of bilin-
gual speech In: International Journal of Bilingual-
ism, vol. 3, no. 4, pp. 309-332.
J.A. Bilmes and K. Kirchhoff. 2003 Factored Lan-
guage Models and Generalized Parallel Backoff In:
Proceedings of NAACL, 2003.
E.G. Bokamba 1989 Are there syntactic constraints on
code-mixing? In: World Englishes, vol. 8, no. 3, pp.
277-292.
J.Y.C. Chan, PC Ching, T. Lee, and H. Cao 2006
Automatic speech recognition of Cantonese-English
210
code-mixing utterances In: Proceeding of Inter-
speech 2006.
K. Duh and K. Kirchhoff. 2004. Automatic Learning
of Language Model Structure, pg 148. In: Proceed-
ings of the 20th international conference on Compu-
tational Linguistics.
A. El-Desoky, R. Schlu?ter, H.Ney 2010 AHybrid Mor-
phologically Decomposed Factored Language Mod-
els for Arabic LVCSR In: NAACL 2010.
D.C. Lyu, T.P. Tan, E.S. Cheng, H. Li 2011 An Anal-
ysis of Mandarin-English Code-Switching Speech
Corpus: SEAME In: Proceedings of Interspeech
2011.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993 Building a large annotated corpus of english:
The penn treebank In: Computational Linguistics,
vol. 19, no. 2, pp. 313330.
T. Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.
Khudanpur. 2010 Recurrent Neural Network based
Language Model In: Proceedings of Interspeech
2010.
T. Mikolov, S. Kombrink, L. Burget, J. Jernocky and
S. Khudanpur. 2011 Extensions of Recurrent Neu-
ral Network Language Model In: Proceedings of
ICASSP 2011.
P. Muysken 2000 Bilingual speech: A typology of
code-mixing In: Cambridge University Press, vol.
11.
I. Oparin, M. Sundermeyer, H. Ney, J.-L. Gauvain
2012 Performance analysis of Neural Networks
in combination with n-gram language models In:
ICASSP, 2012.
S. Poplack 1978 Syntactic structure and social func-
tion of code-switching , Centro de Estudios Puertor-
riquenos, City University of New York.
S. Poplack 1980 Sometimes ill start a sentence in
spanish y termino en espanol: toward a typology of
code-switching In: Linguistics, vol. 18, no. 7-8, pp.
581-618.
R. Rosenfeld 2000 Two decades of statistical language
modeling: Where do we go from here? In: Proceed-
ings of the IEEE 88.8 (2000): 1270-1278.
T. Schultz, P. Fung, and C. Burgmer, 2010 Detecting
code-switch events based on textual features.
Y. Shi, P. Wiggers, M. Jonker 2011 Towards Recurrent
Neural Network Language Model with Linguistics
and Contextual Features In: Proceedings of Inter-
speech 2011.
T. Solorio, Y. Liu 2008 Part-of-speech tagging for
English-Spanish code-switched text In: Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, 2008.
T. Solorio, Y. Liu 2008 Learning to predict code-
switching points In: Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, 2008.
K. Toutanova, C.D. Manning 2000 Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger In: Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natu-
ral language processing and very large corpora: held
in conjunction with the 38th Annual Meeting of the
Association for Computational Linguistics, vol. 13.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer
2003 Feature-rich part-of-speech tagging with a
cyclic dependency network In: Proceedings of
NAACL 2003.
N.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F.
Blaicher, E.S. Chng, T. Schultz, H. Li 2012 A First
Speech Recognition System For Mandarin-English
Code-Switch Conversational Speech In: Proceed-
ings of Interspeech 2012.
N.T. Vu, H. Adel, T. Schultz 2013 An Investigation of
Code-Switching Attitude Dependent Language Mod-
eling In: In Statistical Language and Speech Pro-
cessing, First International Conference, 2013.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer 2005 The
penn chinese treebank: Phrase structure annotation
of a large corpusk In: Natural Language Engineer-
ing, vol. 11, no. 2, pp. 207.
211
