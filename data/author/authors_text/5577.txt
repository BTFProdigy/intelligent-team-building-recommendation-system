Semantic Case Role Detection for Information Extraction 
 
Rik DE BUSSER and Roxana ANGHELUTA and Marie-Francine MOENS 
Interdisciplinary Centre for Law and IT 
Katholieke Universiteit Leuven 
Tiensestraat 41 
B-3000 Leuven, Belgium 
rik.debusser, roxana.angheluta, marie-france.moens@law.kuleuven.ac.be 
 
Abstract 
If information extraction wants to make its 
results more accurate, it will have to resort 
increasingly to a coherent implementation of 
natural language semantics. In this paper, we 
will focus on the extraction of semantic case 
roles from texts. After setting the essential 
theoretical framework, we will argue that it 
is possible to detect case roles on the basis 
of morphosyntactic and lexical surface 
phenomena. We will give a concise 
overview of our methodology and of a 
preliminary test that seems to confirm our 
hypotheses. 
Introduction 
Information extraction (IE) from texts currently 
receives a large research interest. Traditionally, 
it has been associated with the ? often verbatim 
? extraction of domain-specific information 
from free text (Riloff & Lorenzen 1999). Input 
documents are scanned for very specific relevant 
information elements on a particular topic, 
which are used to fill out empty slots in a 
predefined frame. Other types of systems try to 
acquire this knowledge automatically by 
detecting reoccurring lexical and syntactic 
information from manually annotated example 
texts (e.g. Soderland 1999). 
Most of these techniques are inherently limited 
because they exclude natural language semantics 
as much as possible. This is understandable for 
reasons of efficiency and genericity but it 
restricts the algorithms' possibilities and it 
disregards the fact that ? at least in free text ? IE 
has much to do with identifying semantic roles. 
In most of these systems, case role detection as a 
goal in itself has been treated in a rather trivial 
way. Our research will try to provide a 
systematic approach to case role detection as an 
independent extraction task. Using notions from 
systemic-functional grammar and presupposing 
a possible mapping between morphosyntactic 
properties and functional role patterns, we will 
develop a general model for case role extraction. 
The idea is to learn domain-independent case 
role patterns from a tagged corpus, which are 
then (automatically) specialized to particular 
domain-dependent case role sets and which can 
be reassigned to previously unseen text. In this 
paper, we will focus on the first part of this task.  
For IE, an accurate and speedy detection of 
functional case roles is of major importance, 
since they describe events (or states) and 
participants to these events and thus allow for 
identifying real-world entities, their properties 
and interactions between them. 
1 Theoretical setting 
One of the earliest and most notable accounts on 
case roles is without any doubt Charles 
Fillmore's groundbreaking article (Fillmore 
1968). His most fundamental argument is that 
the notion of case is not so much connected to 
morphosyntactic surface realisation as to 
syntactico-semantic categories in the deep 
structure of a language. Particular constellations 
of case roles determine distinctive functional 
patterns, a considerable part of which (according 
to Fillmore) is likely to be universally valid. 
This deep-structure case system can be realized 
in the surface structure by means of a set of 
language-dependent transformation rules (see 
Fillmore 1968). As a consequence there has to 
be a regular mapping between the case system 
and its surface realization ? which includes case 
markers, word order, grammatical roles, etc.  For 
our research, we will disregard the 
transformational dimension in Fillmore's theory 
but we will nevertheless assume that there is at 
least some degree of correspondence between 
the case role system underlying a language and 
its (1) morphosyntax, (2) relative word order and 
(3) lexicon. 
In Halliday's systemic-functional grammar 
(Halliday 1994; Halliday & Matthiessen 1999), 
functional patterns that are part of the language's 
deep structure are organized as figures, i.e. 
configurations of case roles which consist of: 
1. A nuclear process, which is typically realized 
by a verb phrase. Processes express an event 
or state as it is distinctly perceived by the 
language user. 
2. A limited number of participants, which are 
inherent to the process and are typically 
realized by noun phrases. They represent 
entities or abstractions that participate in the 
process. 
3. An in theory unlimited number of 
circumstantial elements. Circumstances are 
in most cases optional and are typically 
realized by prepositional or adverbial 
phrases. They allocate the process and its 
participants in a temporal, spatial, causal, ? 
context.  
Processes are classified into types and subtypes, 
each having its particular participant 
combinations. We discern four main process 
types: Material, Mental, Verbal and Relational 
(Halliday 1994). Figure 1 is an example of a 
verbal process, the Sayer being the participant 
'doing' the process and the Receiver the one to 
whom the (implicit) verbal message is directed. 
 
Invesco in merger talks with AIM Management 
Sayer Verbal Process Receiver 
Figure 1 ? Example of a verbal process 
 
Since these main types (and some secondary 
ones) correspond to universal experiential modi, 
it is to be expected that they will have a certain 
universal validity, i.e. that they are in some way 
or another present in all languages of the world. 
For our preliminary experiments, we use a 
reduced version of the case role model proposed 
by Halliday (1994, p. 106-175), as it is a 
consistent, well-developed and relatively simple 
system, which makes it very suitable for testing 
the validity of our assumptions. For actual 
applications, we will replace it by a more 
elaborate variant, most likely Bateman's 
Generalized Upper Model (Bateman 1990; 
Bateman et al in progress). Bateman's model is 
finer-grained than Halliday's; it is to a large 
extent language-independent; and it has been 
specifically developed for implementation into 
NLP systems (see Bateman et al in progress).    
2 Our approach 
Given the framework outlined above, we 
consider case role detection to be a standard 
classification task. In pattern classification one 
attempts to learn certain patterns or rules from 
classified examples and to use them for 
classifying previously unseen instances (Hand 
1997). In our case, a class is a concatenation of 
case roles that constitute one particular process 
(i.e. the deep structure figure) and the pattern 
itself is to be derived from the morphosyntactic 
and lexical properties corresponding to that 
process (its surface realisation). 
Taking that point-of-view, individual 
realisations of figures ? roughly corresponding 
to stripped-down clauses ? are translated into 
fixed-length sets of lexical and morphosyntactic 
features (word order is implicitly encoded) and a 
functional classification is manually assigned to 
them. For each verb the classification algorithm 
then attempts to match all functional patterns to 
one or a few relevant sets of distinctive features. 
The latter are translated into patterns that can be 
used to match an occurrence in a text to a 
particular constellation of case roles. 
The entire learning process consists of five main 
steps: 
1. Preprocessing 
2. Annotation 
3. Feature selection 
4. Training of the classifier 
5. Translation into rules 
In the preprocessing phase, the input text is 
tagged, lemmatised and chunked. The output is 
standardized and passed to the annotation tool, 
in which the user is asked to assign case role 
patterns to individual clauses. For now, we will 
only take into account processes, participants 
and circumstantial elements of Extent and 
Location. 
In a next step, individual training examples ? 
each example corresponding to one figure ? are 
converted to a fixed-length feature vector. For 
each phrase, the lexical and morphosyntactic 
features of the head and of the left and right 
context boundaries (i.e. the first and the last 
token of the strings pre- and postmodifying the 
head) are automatically extracted from the 
tagged text and added to the vector. This enables 
us to align corresponding features quite 
accurately without having to resort to any 
complex form of phrasal analysis. Although this 
reduction of the context of the head word may 
seem to be counter-intuitive from a grammatical 
point-of-view, our initial tests indicate that it 
does capture most constructions that are relevant 
to the extraction task. 
Feature selection is necessary for two main 
reasons. Firstly, it is impossible to take into 
account all lexical and morphosyntactic features, 
since that would boost the time-complexity, 
incorporate many irrelevant features and bring 
down accuracy when a limited set of training 
examples is available. Secondly, natural 
language utterances have the uncanny habit of 
being of variable length. The latter aspect is 
problematic not only because classification 
algorithms usually expect a clearly delineated 
set of features, but also because it is crucial to 
align examples in order to compare 
correspondent features. 
In our test setting, we will constrain the maximal 
number of case roles per figure to four. Since 
each case role is transformed into a set of 10 
features, a figure will be translated into a 40-
dimensional feature vector (see Figure 2). 
As a result, a particular constellation of case 
roles is treated as one pattern in which each role 
and each of its relevant features has a fixed 
position. We expect this vector representation to 
be relevant in most languages apart from free 
word order languages. Currently, our model 
focuses on English. 
In the fourth step, the classifier is trained to 
discriminate features that are distinctive for each 
process type associated with a particular verb. 
These features are again translated into rules that 
can be used for reassigning case roles that have 
been learned to previously unseen text. 
This is necessary because the variable length of 
figures and ? within figures ? of phrases is 
bound to cause difficulties when applying the 
patterns that were learned to new sentences. 
Rules have the advantage over feature vectors in 
that they allow us to use head-centred 
stretching: when figures are assigned to 
previously unseen sentences and no pattern can 
immediately be matched, the nearest equivalent 
according to the head of the figure will be 
assigned; the rest of the pattern will be allocated 
by shifting the left and right context of the head 
towards the left and right sentence boundaries. A 
similar approach will be used for matching 
individual roles to phrases. 
3 An experiment 
Before engaging in the laboursome task of 
building a set of tools and tagging an entire 
corpus, we decided to test the practical validity 
of our ideas on a small scale on the verb be. We 
manually constructed a limited set of training 
examples (76 occurrences) from the new Reuters 
corpus (CD-rom, Reuters Corpus. Volume 1: 
English Language, 1996-08-20 to 1997-08-19) 
and processed it with the C4.5 classification 
algorithm (Quinlan 1993). 
Figure 3 gives an overview of the process. The 
tagged text1 (step 1) is translated into a set of 
                                                      
1 For our first experiment we used TnT 
(http://www.coli.uni-sb.de/~thorsten/tnt/).  In our 
 
Figure 2 ? The feature set 
 
features (step 2). A functional pattern is used as 
the class corresponding to the feature set (the 
last entry in step 2). The classifier extracts one 
or more distinctive features (step 3), which are 
in turn transposed into a rule (step 4) that is used 
in case role assignment. 
 
 
Figure 3 ? Schematic illustration of the experiment 
 
Initial results are encouraging. The evaluation 
component of C4.5 revealed an error rate of 
9.2% when reapplying its rule extractions on the 
training data. Given the limited amount of data, 
these results are reasonable. Manual application 
of the rules (from step 4) to new text confirmed 
their natural look-and-feel. We are currently 
testing the approach with larger amounts of 
training and testing data. Most of the current 
errors are caused by the limited amount of 
training data in our experiment: in a number of 
cases there was only one instance of a particular 
figure. 
4 Discussion and future improvements 
Although most shortcomings that arose in our 
present set-up can be settled relatively easily, a 
number of issues still remains to be resolved. 
From a theoretical angle, the most urgent 
problem is the underspecification of the material 
domain (or the disagreement on exactly how 
material processes ought to be subclassified). 
Unfortunately, most verb meanings are material 
                                                                                
present experiment, it is replaced by LT POS 
(http://www.ltg.ed.ac.uk/~mikheev/software.html). 
We manually lemmatised the tokens, but we are 
currently using a lemmatizer based on WordNet. 
and distinctions in the material domain tend to 
be rather crucial in most IE applications. 
Two major implementational difficulties are 
related to circumstantial elements. Since 
circumstances are normally not inherent to the 
process, they do not tend to have a fixed position 
in a figure. In addition, no formal parameters 
exist to distinguish obligatory circumstances 
from optional ones. Since it would be absurd to 
encode all variation in separate patterns, it is 
tempting just to add empty slots at the most 
predictable positions where circumstances could 
appear, but that would still not tell apart optional 
and obligatory circumstances and it would be a 
rather ad hoc solution. We are currently 
investigating whether both problems might be 
dealt with by encoding the relative position of 
case roles explicitly. 
In our current information society, it will 
become increasingly important to extract 
information on well-specified events or entities 
from documents. Case role detection will 
provide a way to do this by integrating 'real' 
semantics into the systems without 
overburdening the algorithms. For instance, in 
our example analysis (Figure 1) we can 
immediately identify two entities involved in a 
communicative action, one that does the talking 
('Invesco') and one that is being talked to ('AIM 
management'). An immediate application of case 
role detection is straightforward IE, which 
typically attempts to extract specific information 
from a text. However, the algorithm could also 
be used for optimising information retrieval 
applications, in the construction of knowledge 
bases, in questioning-answering systems or in 
case-based reasoning. Actually, for real natural 
language understanding a highly accurate model 
for interpreting case roles in some form will be 
unavoidable. 
A major advantage of our approach is that the 
pattern base resulting from it will contain 
semantic information and yet be fully domain-
independent. In a next stage of our research, we 
will try to specialize the generic case roles 
automatically to domain-dependent ones. At first 
sight, this two-step approach might appear 
cumbersome, but it will enable us to easily 
expand the pattern base while reusing the hard-
won patterns. 
5 Related research 
Historically, case role detection has its roots in 
frame-based approaches to IE (e.g. Schank & 
Abelson 1977). The main problem here is that to 
build case frames one needs prior knowledge on 
which information exactly one wants to extract. 
In recent years, different solutions have been 
offered to automatically generate those frames 
from annotated examples (e.g. Riloff & 
Schmelzenbach 1998, Soderland 1999) or by 
using added knowledge (e.g. Harabagiu & 
Maiorano 2000). Many of those approaches 
were very successful but most of them have a 
tendency to blend syntactic and semantic 
concepts and they still have to be trained on 
individual domains. 
Some very interesting research on case frame 
detection has been done by Gildea (Gildea 2000, 
Gildea 2001). He uses statistical methods to 
learn case frames from parsed examples from 
FrameNet (Johnson et al 2001). 
Conclusion 
There is a definite need for case role analysis in 
IE and in natural language processing in general. 
In this article, we have tried to argue that generic 
case role detection is possible by using shallow 
text analysis methods. We outlined our 
functional framework and presented a model 
that considers case role pattern extraction to be a 
standard classification task. Our main focus for 
the near future will be on automating as many 
aspects of the annotation process as possible and 
on the construction of the case role assignment 
algorithm. In these tasks, the emphasis will be 
on genericity and reusability. 
Acknowledgements 
We would like to thank the Institute for the 
Promotion of Innovation by Science and 
Technology in Flanders (IWT-Flanders) for the 
research funding. 
References  
Bateman J. (1990) Upper Modelling: a general 
organization of knowledge for natural language 
processing. In "Proceedings of the International 
Language Generation Workshop", Pittsburgh, June 
1990. 
Bateman, J. (in progress) The Generalized Upper 
Model 2.0. http://www.darmstadt.gmd.de/publish/ 
komet/gen-um/newUM.html. Checked 15 February 
2002. 
Fillmore Ch. (1968) The case for case. In "Universals 
in Linguistic Theory", E. Bach & R.T. Harms, ed., 
Holt, Rinehart and Winston, New York, pp. 1-88. 
Gildea D. (2000) Automatic labeling of semantic 
roles. Qualifying exam proposal, University of 
California, January 2000, 21 p. 
Gildea D. (2001) Statistical Language Understanding 
Using Frame Semantics. PhD. dissertation, 
University of California at Berkeley, 2001, 109 p. 
Halliday M.A.K. (1994) An introduction to functional 
grammar. Arnold, London, 434 p. 
Halliday M.A.K. and Matthiessen C. (1999) 
Construing Experience Through Meaning. A 
Language-Based Approach to Cognition. Cassell, 
London, 657 p. 
Hand D. (1997) Construction and Assessment of 
Classification Rules. Chichester: John Wiley & 
Sons, Chichester, 214 p. 
Harabagiu S. and Maiorano S. (2000) Acquisition of 
linguistic patterns for knowledge-based 
information extraction. In "Proceedings of LREC-
2000", Athens, June 2000. 
Johnson C. et al (2001). The FrameNet Project: 
Tools for Lexicon Building. http://www.icsi. 
berkeley.edu/~framenet/book.pdf. Checked 15 
February 2002. 
Quinlan J. (1993) C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, 302 p. 
Riloff E. and Lorenzen J. (1999) Extraction-based 
text categorization: generating domain-specific 
role relationships automatically. In "Natural 
Language Information Retrieval", T. Strzalkowski, 
ed., Kluwer Academic Publishers, Dordrecht, pp. 
167-195. 
Riloff E. and Schelzenbach M. (1998) An empirical 
approach to conceptual case frame acquisition. In 
"Proceedings of the Sixth Workshop on Very large 
Corpora", Montreal, Canada, August 1998. 
Schank R. and Abelson R. (1977) Scripts, Plans, 
Goals and Understanding. An Inquiry into Human 
Knowledge Structures. Erlbaum, Hillsdale, NJ, 
248p. 
Soderland S. (1999) Learning information extraction 
rules for semi-structured and free text. In Machine 
Learning 34, 1/3, pp. 233-272.  
 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21?29,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Semantic Role Labeling
Using the Latent Words Language Model
Koen Deschacht
Department of computer science
K.U.Leuven, Belgium
koen.deshahts.kuleuven.be
Marie-Francine Moens
Department of computer science
K.U.Leuven, Belgium
sien.moenss.kuleuven.be
Abstract
Semantic Role Labeling (SRL) has proved
to be a valuable tool for performing auto-
matic analysis of natural language texts.
Currently however, most systems rely on
a large training set, which is manually an-
notated, an effort that needs to be repeated
whenever different languages or a differ-
ent set of semantic roles is used in a cer-
tain application. A possible solution for
this problem is semi-supervised learning,
where a small set of training examples
is automatically expanded using unlabeled
texts. We present the Latent Words Lan-
guage Model, which is a language model
that learns word similarities from unla-
beled texts. We use these similarities for
different semi-supervised SRL methods as
additional features or to automatically ex-
pand a small training set. We evaluate the
methods on the PropBank dataset and find
that for small training sizes our best per-
forming system achieves an error reduc-
tion of 33.27% F1-measure compared to
a state-of-the-art supervised baseline.
1 Introduction
Automatic analysis of natural language is still a
very hard task to perform for a computer. Al-
though some successful applications have been de-
veloped (see for instance (Chinchor, 1998)), im-
plementing an automatic text analysis system is
still a labour and time intensive task. Many ap-
plications would benefit from an intermediate rep-
resentation of texts, where an automatic analysis
is already performed which is sufficiently general
to be useful in a wide range of applications.
Syntactic analysis of texts (such as Part-Of-
Speech tagging and syntactic parsing) is an ex-
ample of such a generic analysis, and has proved
useful in applications ranging from machine trans-
lation (Marcu et al, 2006) to text mining in the
bio-medical domain (Cohen and Hersh, 2005). A
syntactic parse is however a representation that is
very closely tied with the surface-form of natural
language, in contrast to Semantic Role Labeling
(SRL) which adds a layer of predicate-argument
information that generalizes across different syn-
tactic alternations (Palmer et al, 2005). SRL has
received a lot of attention in the research commu-
nity, and many systems have been developed (see
section 2). Most of these systems rely on a large
dataset for training that is manually annotated. In
this paper we investigate whether we can develop a
system that achieves state-of-the-art semantic role
labeling without relying on a large number of la-
beled examples. We aim to do so by employing the
Latent Words Language Model that learns latent
words from a large unlabeled corpus. Latent words
are words that (unlike observed words) did not oc-
cur at a particular position in a text, but given se-
mantic and syntactic constraints from the context
could have occurred at that particular position.
In section 2 we revise existing work on SRL and
on semi-supervised learning. Section 3 outlines
our supervised classifier for SRL and section 4 dis-
cusses the Latent Words Language Model. In sec-
tion 5 we will combine the two models for semi-
supervised role labeling. We will test the model
on the standard PropBank dataset and compare it
with state-of-the-art semi-supervised SRL systems
in section 6 and finally in section 7 we draw con-
clusions and outline future work.
2 Related work
Gildea and Jurafsky (2002) were the first to de-
scribe a statistical system trained on the data from
the FrameNet project to automatically assign se-
mantic roles. This approach was soon followed
by other researchers (Surdeanu et al, 2003; Prad-
han et al, 2004; Xue and Palmer, 2004), focus-
21
ing on improved sets of features, improved ma-
chine learning methods or both, and SRL became
a shared task at the CoNLL 2004, 2005 and 2008
conferences1 . The best system (Johansson and
Nugues, 2008) in CoNLL 2008 achieved an F1-
measure of 81.65% on the workshop?s evaluation
corpus.
Semi-supervised learning has been suggested
by many researchers as a solution to the annota-
tion bottleneck (see (Chapelle et al, 2006; Zhu,
2005) for an overview), and has been applied suc-
cessfully on a number of natural language pro-
cessing tasks. Mann and McCallum (2007) ap-
ply Expectation Regularization to Named Entity
Recognition and Part-Of-Speech tagging, achiev-
ing improved performance when compared to su-
pervised methods, especially on small numbers of
training data. Koo et al (2008) present an algo-
rithm for dependency parsing that uses clusters of
semantically related words, which were learned
in an unsupervised manner. There has been lit-
tle research on semi-supervised learning for SRL.
We refer to He and Gildea (2006) who tested ac-
tive learning and co-training methods, but found
little or no gain from semi-supervised learning,
and to Swier and Stevenson (2004), who achieved
good results using semi-supervised methods, but
tested their methods on a small number of Verb-
Net roles, which have not been used by other SRL
systems. To the best of our knowledge no sys-
tem was able to reproduce the successful results
of (Swier and Stevenson, 2004) on the PropBank
roleset. Our approach most closely resembles the
work of F?rstenau and Lapata (2009) who auto-
matically expand a small training set using an au-
tomatic dependency alignment of unlabeled sen-
tences. This method was tested on the FrameNet
corpus and improved results when compared to a
fully-supervised classifier. We will discuss their
method in detail in section 5.
3 Semantic role labeling
Fillmore (1968) introduced semantic structures
called semantic frames, describing abstract ac-
tions or common situations (frames) with common
roles and themes (semantic roles). Inspired by this
idea different resources were constructed, includ-
ing FrameNet (Baker et al, 1998) and PropBank
(Palmer et al, 2005). An alternative approach to
semantic role labeling is the framework developed
1See http://www.cnts.ua.ac.be/conll/ for an overview.
by Halliday (1994) and implemented by Mehay
et al (2005). PropBank has thus far received the
most attention of the research community, and is
used in our work.
3.1 PropBank
The goal of the PropBank project is to add seman-
tic information to the syntactic nodes in the En-
glish Penn Treebank. The main motivation for this
annotation is the preservation of semantic roles
across different syntactic realizations. Take for in-
stance the sentences
1. The window broke.
2. John broke the window.
In both sentences the constituent ?the window? is
broken, although it occurs at different syntactic
positions. The PropBank project defines for a
large collection of verbs (excluding auxiliary
verbs such as ?will?, ?can?, ...) a set of senses,
that reflect the different meanings and syntactic
alternations of this verb. Every sense has a
number of expected roles, numbered from Arg0
to Arg5. A small number of arguments are shared
among all senses of all verbs, such as temporals
(Arg-TMP), locatives (Arg-LOC) and directionals
(Arg-DIR). Additional to the frame definitions,
PropBank has annotated a large training corpus
containing approximately 113.000 annotated
verbs. An example of an annotated sentence is
[John Arg0][broke BREAK.01] [the window Arg1].
Here BREAK.01 is the first sense of the ?break?
verb. Note that (1) although roles are defined for
every frame separately, in reality roles with iden-
tical names are identical or very similar for all
frames, a fact that is exploited to train accurate role
classifiers and (2) semantic role labeling systems
typically assume that a frame is fully expressed in
a single sentence and thus do not try to instanti-
ate roles across sentence boundaries. Although the
original PropBank corpus assigned semantic roles
to syntactic phrases (such as noun phrases), we use
the CoNLL dataset, where the PropBank corpus
was converted to a dependency representation, as-
signing semantic roles to single (head) words.
3.2 Features
In this section we discuss the features used in the
semantic role labeling system. All features but the
22
Split path feature are taken from existing seman-
tic role labeling systems, see for example (Gildea
and Jurafsky, 2002; Lim et al, 2004; Thompson
et al, 2006). The number in brackets denotes the
number of unique features for that type.
Word We split every sentence in (unigram) word
tokens, including punctuation. (37079)
Stem We reduce the word tokens to their stem,
e.g. ?walks? -> ?walk?. (28690)
POS The part-of-speech tag for every word, e.g.
?NNP? (for a singular proper noun). (77)
Neighbor POS?s The concatenated part-of-
speech tags of the word before and the word
just after the current word, e.g. ?RBS_JJR?.
(1787)
Path This important feature describes the path
through the dependency tree from the current
word to the position of the predicate, e.g.
?coord?obj?adv?root?dep?nmod?pmod?,
where ??? indicates going up a constituent
and ??? going down one constituent.
(829642)
Split Path Because of the nature of the path fea-
ture, an explosion of unique features is found
in a given data set. We reduce this by split-
ting the path in different parts and using every
part as a distinct feature. We split, for exam-
ple, the previous path in 6 different features:
?coord?, ??obj?, ??adv?, ??root?, ??dep?,
??nmod?, ??pmod?. Note that the split path
feature includes the POS feature, since the
first component of the path is the POS tag for
the current word. This feature has not been
used previously for semantic role detection.
(155)
For every word wi in the training and test set we
construct the feature vector f(wi), where at every
position in this vector 1 indicates the presence for
the corresponding feature and 0 the absence of that
feature.
3.3 Discriminative model
Discriminative models have been found to outper-
form generative models for many different tasks
including SRL (Lim et al, 2004). For this reason
we also employ discriminative models here. The
structure of the model was inspired by a similar
Figure 1: Discriminative model for SRL. Grey
circles represent observed variables, white circles
hidden variables and arrows directed dependen-
cies. s ranges over all sentences in the corpus and
j over the n words in the sentence.
(although generative) model in (Thompson et al,
2006) where it was used for semantic frame clas-
sification. The model (fig. 1) assumes that the role
label ri j for the word wi is conditioned on the fea-
tures fi and on the role label ri?1 j of the previous
word and that the predicate label p j for word w j is
conditioned on the role labels R j and on the fea-
tures f j. This model can be seen as an extension
of the standard Maximum Entropy Markov Model
(MEMM, see (Ratnaparkhi, 1996)) with an extra
dependency on the predicate label, we will hence-
forth refer to this model as MEMM+pred.
To estimate the parameters of the MEMM+pred
model we turn to the successful Maximum En-
tropy (Berger et al, 1996) parameter estimation
method. The Maximum Entropy principle states
that the best model given the training data is the
model such that the conditional distribution de-
fined by the model has maximum entropy subject
to the constraints represented by the training ex-
amples. There is no closed form solution to find
this maximum and we thus turn to an iterative
method. In this work we use Generalized Itera-
tive Scaling2, but other methods such as (quasi-)
Newton optimization could also have been used.
4 Latent Words Language Model
4.1 Rationale
As discussed in sections 1 and 3 most SRL sys-
tems are trained today on a large set of manually
annotated examples. PropBank for example con-
tains approximately 50000 sentences. This man-
ual annotation is both time and labour-intensive,
and needs to be repeated for new languages or
2We use the maxent package available on
http://maxent.sourceforge.net/
23
for new domains requiring a different set of roles.
One approach that can help to solve this problem
is semi-supervised learning, where a small set of
annotated examples is used together with a large
set of unlabeled examples when training a SRL
model.
Manual inspection of the results of the super-
vised model discussed in the previous section
showed that the main source of errors was in-
correct labeling of a word because the word to-
ken did not occur, or occurred only a small num-
ber of times in the training set. We hypothesize
that knowledge of semantic similar words could
overcome this problem by associating words that
occurred infrequently in the training set to sim-
ilar words that occurred more frequently. Fur-
thermore, we would like to learn these similar-
ities automatically, to be independent of knowl-
edge sources that might not be available for all
languages or domains.
The Distributional Hypothesis, supported by
theoretical linguists such as Harris (1954), states
that words that occur in the same contexts tend
to have similar meanings. This suggests that one
can learn the similarity between two words auto-
matically by comparing their relative contexts in
a large unlabeled corpus, which was confirmed by
different researchers (e.g. (Lin, 1998; McDonald
and Ramscar, 2001; Grefenstette, 1994)). Differ-
ent methods for computing word similarities have
been proposed, differing between methods to rep-
resent the context (using dependency relationship
or a window of words) and between methods that,
given a set of contexts, compute the similarity be-
tween different words (ranging from cosine simi-
larity to more complex metrics such as the Jaccard
index). We refer to (Lin, 1998) for a comparison
of the different similarity metrics.
In the next section we propose a novel method
to learn word similarities, the Latent Words Lan-
guage Model (LWLM) (Deschacht and Moens,
2009). This model learns similar words and learns
the a distribution over the contexts in which cer-
tain types of words occur typically.
4.2 Definition
The LWLM introduces for a text T = w1...wN of
length N for every observed word wi at position i
a hidden variable hi. The model is a generative
model for natural language, in which the latent
variable hi is generated by its context C(hi) and the
observed word wi is generated by the latent vari-
able hi. In the current model we assume that the
context is C(hi) = hi?1i?2h
i+2
i+1 where h
i?1
i?2 = hi?2hi?1
is the two previous words and hi+2i+1 = hi+1hi+2 is
the two next words. The observed wi has a value
from the vocabulary V , while the hidden variable
hi is unknown, and is modeled as a probability
distribution over all words of V . We will see in
the next section how this distribution is estimated
from a large unlabeled training corpus. The aim
of this model is to estimate, at every position i,
a distribution for hi, assigning high probabilities
to words that are similar to wi, given the context
of this word C(hi), and low probabilities to words
that are not similar to wi in this context.
A possible interpretation of this model states
that every hidden variable hi models the ?mean-
ing? for a particular word in a particular context.
In this probabilistic model, when generating a sen-
tence, we generate the meaning of a word (which
is an unobserved representation) with a certain
probability, and then we generate a certain obser-
vation by writing down one of the possible words
that express this meaning.
Creating a representation that models the mean-
ing of a word is an interesting (and controversial)
topic in its own right, but in this work we make
the assumption that the meaning of a particular
word can be modeled using other words. Model-
ing the meaning of a word with other words is not
an unreasonable one, since it is already employed
in practice by humans (e.g. by using dictionar-
ies and thesauri) and machines (e.g. relying on a
lexical resource such as WordNet) in word sense
disambiguation tasks.
4.3 Parameter estimation
As we will further see the LWLM model has three
probability distributions: P(wi|hi), the probability
of the observed word w j given the latent variable
h j, P(hi|hi?1i?2), the probability of the hidden word
h j given the previous variables h j?2 and h j?1, and
P(hi|hi+2i+1), the probability of the hidden word h j
given the next variables h j+1 and h j+2. These dis-
tributions need to be learned from a training text
Ttrain =< w0...wz > of length Z.
4.3.1 The Baum-Welch algorithm
The attentive reader will have noticed the sim-
ilarity between the proposed model and a stan-
dard second-order Hidden Markov Model (HMM)
where the hidden state is dependent on the two
24
previous states. However, we are not able to use
the standard Baum-Welch (or forward-backward)
algorithm, because the hidden variable hi is mod-
eled as a probability distribution over all words
in the vocabulary V . The Baum-Welch algorithm
would result in an execution time of O(|V |3NG)
where |V | is the size of the vocabulary, N is the
length of the training text and G is the number of
iterations needed to converge. Since in our dataset
the vocabulary size is more than 30K words (see
section 3.2), using this algorithm is not possible.
Instead we use techniques of approximate infer-
ence, i.e. Gibbs sampling.
4.3.2 Initialization
Gibbs sampling starts from a random initializa-
tion for the hidden variables and then improves
the estimates in subsequent iterations. In prelimi-
nary experiments it was found that a pure random
initialization results in a very long burn-in-period
and a poor performance of the final model. For
this reason we initially set the distributions for the
hidden words equal to the distribution of words as
given by a standard language model3.
4.3.3 Gibbs sampling
We store the initial estimate of the hidden vari-
ables in M0train =< h0...hZ >, where hi generates
wi at every position i. Gibbs sampling is a Markov
Chain Monte Carlo method that updates the esti-
mates of the hidden variables in a number of it-
erations. M?train denotes the estimate of the hid-
den variables in iteration ? . In every iteration a
new estimate M?+1train is generated from the previ-
ous estimate M?train by selecting a random posi-
tion j and updating the value of the hidden vari-
able at that position. The probability distributions
P?(w j|h j), P?(h j|h j?1j?2) and P?(h j|h
j+2
j+1) are con-
structed by collecting the counts from all positions
i 6= j. The hidden variable h j is dependent on h j?2,
h j?1, h j+1, h j+2 and w j and we can compute the
distribution of possible values for the variable h j
as
P?(h j|w j,h j?10 ,h
Z
j+1) =
P?(w j|h j)P?(h j|h j?1j?2h
j+2
j+1)
?hi P?(wi|hi)P?(h j|h j?1j?2h j+2j+1)
We set P(h j|h j?1j?2h
j+2
j+1) = P(h j|h
j?1
j?2) ?P(h j|h
j+2
j+1)
which can be easily computed given the above dis-
3We used the interpolated Kneser-Ney model as described
in (Goodman, 2001).
tributions. We select a new value for the hidden
variable according to P?(h j|w j,h j?10 ,hZj+1) and
place it at position j in M?+1train. The current esti-
mate for all other unobserved words remains the
same. After performing this iteration a large num-
ber of times (|V | ?10 in this experiment), the dis-
tribution approaches the true maximum likelihood
distribution. Gibbs sampling however samples this
distribution, and thus will never reach it exactly. A
number of iterations (|V | ?100) is then performed
in which Gibbs sampling oscillates around the cor-
rect distribution. We collect independent samples
of this distribution every |V | ?10 iterations, which
are then used to construct the final model.
4.4 Evaluation of the Language Model
A first evaluation of the quality of the automat-
ically learned latent words is by translation of
this model into a sequential language model and
by measuring its perplexity on previously unseen
texts. In (Deschacht and Moens, 2009) we per-
form a number of experiments, comparing differ-
ent corpora (news texts from Reuters and from
Associated Press, and articles from Wikipedia)
and n-gram sizes (3-gram and 4-gram). We also
compared the proposed model with two state-of-
the-art language models, Interpolated Kneser-Ney
smoothing and fullibmpredict (Goodman, 2001),
and found that LWLM outperformed both models
on all corpora, with a perplexity reduction ranging
between 12.40% and 5.87%. These results show
that the estimated distributions over latent words
are of a high quality and lead us to believe they
could be used to improve automatic text analysis,
like SRL.
5 Role labeling using latent words
The previous section discussed how the LWLM
learns similar words and how these similarities im-
proved the perplexity on an unseen text of the lan-
guage model derived from this model. In this sec-
tion we will see how we integrate the latent words
model in two novel semi-supervised SRL models
and compare these with two state-of-the-art semi-
supervised models for SRL and dependency pars-
ing.
Latent words as additional features
In a first approach we estimate the distribution of
latent words for every word for both the training
and test set. We then use the latent words at every
25
position as additional probabilistic features for the
discriminative model. More specifically, we ap-
pend |V | extra values to the feature vector f(w j),
containing the probability distribution over the |V |
possible words for the hidden variable hi4. We call
this the LWFeatures method.
This method has the advantage that it is simple
to implement and that many existing SRL systems
can be easily extended by adding additional fea-
tures. We also expect that this method can be em-
ployed almost effortless in other information ex-
traction tasks, such as Named Entity Recognition
or Part-Of-Speech labeling.
We compare this approach to the semi-
supervised method in Koo et al (2008) who em-
ploy clusters of related words constructed by the
Brown clustering algorithm (Brown et al, 1992)
for syntactic processing of texts. Interestingly,
this clustering algorithm has a similar objective as
LWLM since it tries to optimize a class-based lan-
guage model in terms of perplexity on an unseen
test text. We employ a slightly different clustering
method here, the fullibmpredict method discussed
in (Goodman, 2001). This method was shown
to outperform the class based model proposed in
(Brown et al, 1992) and can thus be expected to
discover better clusters of words. We append the
feature vector f(w j) with c extra values (where c is
the number of clusters), respectively set to 1 if the
word wi belongs to the corresponding cluster or to
0 otherwise. We call this method the ClusterFea-
tures method.
Automatic expansion of the training set using
predicate argument alignment
We compare our approach with a method proposed
by F?rstenau and Lapata (2009). This approach is
more tailored to the specific case of SRL and is
summarized here.
Given a set of labeled seed verbs with annotated
semantic roles, for every annotated verb a number
of occurrences of this verb is found in unlabeled
texts where the context is similar to the context of
the annotated example. The context is defined here
as all words in the sentence that are direct depen-
dents of this verb, given the syntactic dependency
tree. The similarity between two occurrences of a
particular verb is measured by finding all different
alignments ? : M? ? {1...n} (M? ? {1, ...,m})
4Probabilities smaller than 1e10?4 were set to 0 for effi-
ciency reasons.
between the m dependents of the first occurrence
and the n dependents of the second occurrence.
Every alignment ? is assigned a score given by
?
i?M?
(
A ? syn(gi,g?(i))+ sem(wi,w?(i))?B
)
where syn(gi,g?(i)) denotes the syntactic simi-
larity between grammatical role5 gi of word wi
and grammatical role g?(i) of word w?(i), and
sem(wi,w?(i)) measures the semantic similarity
between words wi and w?(i). A is a constant
weighting the importance of the syntactic simi-
larity compared to semantic similarity, and B can
be interpreted as the lowest similarity value for
which an alignment between two arguments is
possible. The syntactic similarity syn(gi,g?(i)) is
defined as 1 if the dependency relations are iden-
tical, 0 < a < 1 if the relations are of the same
type but of a different subtype6 and 0 otherwise.
The semantic similarity sem(wi,w?(i)) is automat-
ically estimated as the cosine similarity between
the contexts of wi and w?(i) in a large text cor-
pus. For details we refer to (F?rstenau and Lapata,
2009).
For every verb in the annotated training set we
find the k occurrences of that verb in the unlabeled
texts where the contexts are most similar given the
best alignment. We then expand the training set
with these examples, automatically generating an
annotation using the discovered alignments. The
variable k controls the trade-off between anno-
tation confidence and expansion size. The final
model is then learned by running the supervised
training method on the expanded training set. We
call this method AutomaticExpansionCOS7 . The
values for k, a, A and B are optimized automati-
cally in every experiment on a held-out set (dis-
joint from both training and test set).
We adapt this approach by employing a different
method for measuring semantic similarity. Given
two words wi and w?(i) we estimate the distri-
bution of latent words, respectively L(hi) and
5Note that this is a syntactic role, not a semantic role as
the ones discussed in this article.
6Subtypes are fine-grained distinctions made by the parser
such as the underlying grammatical roles in passive construc-
tions.
7The only major differences with (F?rstenau and Lap-
ata, 2009) are the dependency parser which was used (the
MALT parser (Nivre et al, 2006) instead of the RASP parser
(Briscoe et al, 2006)) and the corpus employed to learn se-
mantic similarities (the Reuters corpus instead of the British
National Corpus). We expect that these differences will only
influence the results minimally.
26
5% 20% 50% 100%
Supervised 40.49% 67.23% 74.93% 78.65%
LWFeatures 60.29% 72.88% 76.42% 80.98%
ClusterFeatures 59.51% 66.70% 70.15% 72.62%
AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52%
AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66%
Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing
the supervised method (Supervised) with the semi-supervised methods LWFeatures, ClusterFeatures,
AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods.
Best results are in bold.
L(h?(i)). We then compute the semantic similarity
measure as the Jensen-Shannon (Lin, 1997) diver-
gence
JS(L(hi)||L(h?(i))) =
1
2
[
D(L(hi)||avg)+D
(
L(h?(i))||avg
)]
where avg = (L(hi) + L(h?(i)))/2 is the average
between the two distributions and D(L(hi)||avg)
is the Kullback?Leiber divergence (Cover and
Thomas, 2006).
Although this change might appear only a slight
deviation from the original model discussed in
(F?rstenau and Lapata, 2009) it is potentially an
important one, since an accurate semantic similar-
ity measure will greatly influence the accuracy of
the alignments, and thus of the accuracy of the au-
tomatic expansion. We call this method Automat-
icExpansionLW.
6 Experiments
We perform a number of experiments where we
compare the fully supervised model with the semi-
supervised models proposed in the previous sec-
tion. We first train the LWLM model on an unla-
beled 5 million word Reuters corpus8.
We perform different experiments for the super-
vised and the four different semi-supervised meth-
ods (see previous section). Table 1 shows the re-
sults of the different methods on the test set of the
CoNLL 2008 shared task. We experimented with
different sizes for the training set, ranging from
5% to 100%. When using a subset of the full train-
ing set, we run 10 different experiments with ran-
dom subsets and average the results.
We see that the LWFeatures method performs
better than the other methods across all train-
ing sizes. Furthermore, these improvements are
8See http://www.daviddlewis.com/resources
larger for smaller training sets, showing that the
approach can be applied successfully in a setting
where only a small number of training examples
is available.
When comparing the LWFeatures method with
the ClusterFeatures method we see that, although
the ClusterFeatures method has a similar perfor-
mance for small training sizes, this performance
drops for larger training sizes. A possible expla-
nation for this result is the use of the clusters em-
ployed in the ClusterFeatures method. By defini-
tion the clusters merge many words into one clus-
ter, which might lead to good generalization (more
important for small training sizes) but can poten-
tially hurt precision (more important for larger
training sizes).
A third observation that can be made from table
1 is that, although both automatic expansion meth-
ods (AutomaticExpansionCOS and AutomaticEx-
pansionCOS) outperform the supervised method
for the smallest training size, for other sizes of the
training set they perform relatively poorly. An in-
formal inspection showed that for some examples
in the training set, little or no correct similar occur-
rences were found in the unlabeled text. The algo-
rithm described in section 5 adds the most similar
k occurrences to the training set for every anno-
tated example, also for these examples where lit-
tle or no similar occurrences were found. Often
the automatic alignment fails to generate correct
labels for these occurrences and introduces errors
in the training set. In the future we would like to
perform experiments that determine dynamically
(for instance based on the similarity measure be-
tween occurrences) for every annotated example
how many training examples to add.
27
7 Conclusions and future work
We have presented the Latent Words Language
Model and showed how it learns, from unla-
beled texts, latent words that capture the mean-
ing of a certain word, depending on the con-
text. We then experimented with different meth-
ods to incorporate the latent words for Semantic
Role Labeling, and tested different methods on the
PropBank dataset. Our best performing method
showed a significant improvement over the su-
pervised model and over methods previously pro-
posed in the literature. On the full training set
the best method performed 2.33% better than the
fully supervised model, which is a 10.91% error
reduction. Using only 5% of the training data the
best semi-supervised model still achieved 60.29%,
compared to 40.49% by the supervised model,
which is an error reduction of 33.27%. These re-
sults demonstrate that the latent words learned by
the LWLM help for this complex information ex-
traction task. Furthermore we have shown that the
latent words are simple to incorporate in an ex-
isting classifier by adding additional features. We
would like to perform experiments on employing
this model in other information extraction tasks,
such as Word Sense Disambiguation or Named
Entity Recognition. The current model uses the
context in a very straightforward way, i.e. the two
words left and right of the current word, but in
the future we would like to explore more advanced
methods to improve the similarity estimates. Lin
(1998) for example discusses a method where a
syntactic parse of the text is performed and the
context of a word is modeled using dependency
triples.
The other semi-supervised methods proposed
here were less successful, although all improved
on the supervised model for small training sizes.
In the future we would like to improve the de-
scribed automatic expansion methods, since we
feel that their full potential has not yet been
reached. More specifically we plan to experiment
with more advanced methods to decide whether
some automatically generated examples should be
added to the training set.
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978) and the IWT-SBO project AMASS++
(IWT-SBO-060051). We thank the anonymous re-
viewers for their helpful comments and Dennis N.
Mehay for his help on clarifying the linguistic mo-
tivation of our models.
References
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, volume 98.
Montreal, Canada.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational linguistics,
22(1):39?71.
T. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL, vol-
ume 6.
P.F. Brown, R.L. Mercer, V.J. Della Pietra, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
O. Chapelle, B. Sch?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
N.A. Chinchor. 1998. Overview of MUC-7/MET-2. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7), volume 1.
A.M. Cohen and W.R. Hersh. 2005. A survey of cur-
rent work in biomedical text mining. Briefings in
Bioinformatics, 6(1):57?71.
T.M. Cover and J.A. Thomas. 2006. Elements of In-
formation Theory. Wiley-Interscience.
Koen Deschacht and Marie-Francine Moens. 2009.
The Latent Words Language Model. In Proceed-
ings of the 18th Annual Belgian-Dutch Conference
on Machine Learning.
C. J. Fillmore. 1968. The case for case. In E. Bach and
R. Harms, editors, Universals in Linguistic Theory.
Rinehart & Winston.
Hagen F?rstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 220?228, Athens, Greece.
Association for Computational Linguistics.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical re-
port, Microsoft Research.
28
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
M.A.K. Halliday. 1994. An Introduction to Functional
Grammar (second edition). Edward Arnold, Lon-
don.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
S. He and D. Gildea. 2006. Self-training and Co-
training for Semantic Role Labeling: Primary Re-
port. Technical report. TR 891.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183?187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 595?603.
J.-H. Lim, Y.-S. Hwang, S.-Y. Park, and H.-C. Rim.
2004. Semantic role labeling using maximum en-
tropy model. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 122?125, Boston, Massachusetts, USA. ACL.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, volume 35,
pages 64?71. ACL.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational Linguistics,
pages 768?774. Association for Computational Lin-
guistics Morristown, NJ, USA.
G.S. Mann and A. McCallum. 2007. Simple, ro-
bust, scalable semi-supervised learning via expecta-
tion regularization. In Proceedings of the 24th In-
ternational Conference on Machine Learning, pages
593?600. ACM Press New York, USA.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proceedings of the
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 44?52.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the 23rd Annual Conference of the Cognitive Sci-
ence Society, pages 611?616.
Dennis Mehay, Rik De Busser, and Marie-Francine
Moens. 2005. Labeling generic semantic roles. In
Proceedings of the Sixth International Workshop on
Computational Semantics.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A
datadriven parser-generator for dependency parsing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation, pages
2216?2219.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of the Hu-
man Language Technology Conference/North Amer-
ican chapter of the Association of Computational
Linguistics, Boston, MA.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142. Association for Com-
putational Linguistics.
M. Surdeanu, S. Harabagiu, J. Williams, and
P. Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 8?15.
R.S. Swier and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95?102.
C. Thompson, R. Levy, and C. Manning. 2006. A gen-
erative model for FrameNet semantic role labeling .
In Proceedings of the 14th European Conference on
Machine Learning, Cavtat-Dubrovnik, Croatia.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, volume 4.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
29
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1000?1007,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Text Analysis for Automatic Image Annotation
Koen Deschacht and Marie-Francine Moens
Interdisciplinary Centre for Law & IT
Department of Computer Science
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
{koen.deschacht,marie-france.moens}@law.kuleuven.ac.be
Abstract
We present a novel approach to automati-
cally annotate images using associated text.
We detect and classify all entities (persons
and objects) in the text after which we de-
termine the salience (the importance of an
entity in a text) and visualness (the extent to
which an entity can be perceived visually)
of these entities. We combine these mea-
sures to compute the probability that an en-
tity is present in the image. The suitability
of our approach was successfully tested on
100 image-text pairs of Yahoo! News.
1 Introduction
Our society deals with a growing bulk of un-
structured information such as text, images and
video, a situation witnessed in many domains (news,
biomedical information, intelligence information,
business documents, etc.). This growth comes along
with the demand for more effective tools to search
and summarize this information. Moreover, there is
the need to mine information from texts and images
when they contribute to decision making by gov-
ernments, businesses and other institutions. The
capability to accurately recognize content in these
sources would largely contribute to improved index-
ing, classification, filtering, mining and interroga-
tion.
Algorithms and techniques for the disclosure of
information from the different media have been de-
veloped for every medium independently during the
last decennium, but only recently the interplay be-
tween these different media has become a topic of
interest. One of the possible applications is to help
analysis in one medium by employing information
from another medium. In this paper we study text
that is associated with an image, such as for instance
image captions, video transcripts or surrounding text
in a web page. We develop techniques that extract
information from these texts to help with the diffi-
cult task of accurate object recognition in images.
Although images and associated texts never contain
precisely the same information, in many situations
the associated text offers valuable information that
helps to interpret the image.
The central objective of the CLASS project1 is to
develop advanced learning methods that allow ima-
ges, video and associated text to be automatically
analyzed and structured. In this paper we test the
feasibility of automatically annotating images by us-
ing textual information in near-parallel image-text
pairs, in which most of the content of the image
corresponds to content of the text and vice versa.
We will focus on entities such as persons and ob-
jects. We will hereby take into account the text?s dis-
course structure and semantics, which allow a more
fine-grained identification of what content might be
present in the image, and will enrich our model with
world knowledge that is not present in the text.
We will first discuss the corpus on which we ap-
ply and test our techniques in section 2, after which
we outline what techniques we have developed: we
start with a baseline system to annotate images with
person names (section 3) and improve this by com-
puting the importance of the persons in the text (sec-
tion 4). We will then extend the model to include all
1http://class.inrialpes.fr/
1000
Hiram Myers, of Edmond, Okla., walks across the
fence, attempting to deliver what he called a ?people?s
indictment? of Halliburton CEO David Lesar, outside the
site of the annual Halliburton shareholders meeting in
Duncan, Okla., leading to his arrest, Wednesday, May 17,
2006.
Figure 1: Image-text pair with entity ?Hiram Myers?
appearing both in the text and in the image.
types of objects (section 5) and improve it by defin-
ing and computing the visualness measure (section
6). Finally we will combine these different tech-
niques in one probabilistic model in section 7.
2 The parallel corpus
We have created a parallel corpus consisting of 1700
image-text pairs, retrieved from the Yahoo! News
website2. Every image has an accompanying text
which describes the content of the image. This text
will in general discuss one or more persons in the
image, possibly one or more other objects, the loca-
tion and the event for which the picture was taken.
An example of an image-text pair is given in fig. 1.
Not all persons or objects who are pictured in the
images are necessarily described in the texts. The
inverse is also true, i.e. content mentioned in the
text may not be present in the image.
We have randomly selected 100 text-pairs from
the corpus, and one annotator has labeled every
image-text pair with the entities (i.e. persons and
2http://news.yahoo.com/
other objects) that appear both in the image and in
the text. For example, the image-text pair shown in
fig. 1 is annotated with one entity, ?Hiram Myers?,
since this is the only entity that appears both in the
text and in the image. On average these texts contain
15.04 entities, of which 2.58 appear in the image.
To build the appearance model of the text, we
have combined different tools. We will evaluate
every tool separately on 100 image-text pairs. This
way we have a detailed view on the nature of the
errors in the final model.
3 Automatically annotating person names
Given a text that is associated with an image, we
want to compute a probabilistic appearance model,
i.e. a collection of entities that are visible in the
image. We will start with a model that holds the
names of the persons that appear in the image, such
as was done by (Satoh et al, 1999; Berg et al, 2004),
and extend this model in section 5 to include all
other objects.
3.1 Named Entity Recognition
A logical first step to detect person names is Named
Entity Recognition (NER). We use the OpenNLP
package3, which detects noun phrase chunks in the
sentences that represent persons, locations, organi-
zations and dates. To improve the recognition of
person names, we use a dictionary of names, which
we have extracted from the Wikipedia4 website. We
have manually evaluated performance of NER on
our test corpus and found that performance was sa-
tisfying: we obtained a precision of 93.37% and a re-
call of 97.69%. Precision is the percentage of iden-
tified person names by the system that corresponds
to correct person names, and recall is the percentage
of person names in the text that have been correctly
identified by the system.
The texts contain a small number of noun phrase
coreferents that are in the form of pronouns, we have
resolved these using the LingPipe5 package.
3.2 Baseline system
We want to annotate an image using the associated
text. We try to find the names of persons which are
3http://opennlp.sourceforge.net/
4http://en.wikipedia.org/
5http://www.alias-i.com/lingpipe/
1001
both described in the text and visible in the image,
and we want to do so by relying only on an analysis
of the text. In some cases, such as the following
example, the text states explicitly whether a person
is (not) visible in the image:
President Bush [...] with Danish Prime
Minister Anders Fogh Rasmussen, not
pictured, at Camp David [...].
Developing a system that could extract this informa-
tion is not trivial, and even if we could do so, only a
very small percentage of the texts in our corpus con-
tain this kind of information. In the next section we
will look into a method that is applicable to a wide
range of (descriptive) texts and that does not rely on
specific information within the text.
To evaluate the performance of this system, we
will compare it with a simple baseline system. The
baseline system assumes that all persons in the text
are visible in the image, which results in a precision
of 71.27% and a recall of 95.56%. The (low) preci-
sion can be explained by the fact that the texts often
discuss people which are not present in the image.
4 Detection of the salience of a person
Not all persons discussed in a text are equally im-
portant. We would like to discover what persons
are in the focus of a text and what persons are only
mentioned briefly, because we presume that more
important persons in the text have a larger proba-
bility of appearing in the image than less important
persons. Because of the short lengths of the docu-
ments in our corpus, an analysis of lexical cohesion
between terms in the text will not be sufficient for
distinguishing between important and less important
entities. We define a measure, salience, which is a
number between 0 and 1 that represents the impor-
tance of an entity in a text. We present here a method
for computing this score based on an in depth ana-
lysis of the discourse of the text and of the syntactic
structure of the individual sentences.
4.1 Discourse segmentation
The discourse segmentation module, which we de-
veloped in earlier research, hierarchically and se-
quentially segments the discourse in different topics
and subtopics resulting in a table of contents of a
text (Moens, 2006). The table shows the main en-
tities and the related subtopic entities in a tree-like
structure that also indicates the segments (by means
of character pointers) to which an entity applies. The
algorithm detects patterns of thematic progression in
texts and can thus recognize the main topic of a sen-
tence (i.e., about whom or what the sentence speaks)
and the hierarchical and sequential relationships be-
tween individual topics. A mixture model, taking
into account different discourse features, is trained
with the Expectation Maximization algorithm on an
annotated DUC-2003 corpus. We use the resulting
discourse segmentation to define the salience of in-
dividual entities that are recognized as topics of a
sentence. We compute for each noun entity er in the
discourse its salience (Sal1) in the discourse tree,
which is proportional with the depth of the entity in
the discourse tree -hereby assuming that deeper in
this tree more detailed topics of a text are described-
and normalize this value to be between zero and one.
When an entity occurs in different subtrees, its max-
imum score is chosen.
4.2 Refinement with sentence parse
information
Because not all entities of the text are captured in the
discourse tree, we implement an additional refine-
ment of the computation of the salience of an entity
which is inspired by (Moens et al, 2006). The seg-
mentation module already determines the main topic
of a sentence. Since the syntactic structure is often
indicative of the information distribution in a sen-
tence, we can determine the relative importance of
the other entities in a sentence by relying on the re-
lationships between entities as signaled by the parse
tree. When determining the salience of an entity, we
take into account the level of the entity mention in
the parse tree (Sal2), and the number of children for
the entity in this structure (Sal3), where the normal-
ized score is respectively inversely proportional with
the depth of the parse tree where the entity occurs,
and proportional with the number of children.
We combine the three salience values (Sal1,
Sal2 and Sal3) by using a linear weighting. We
have experimentally determined reasonable coeffi-
cients for these three values, which are respectively
0.8, 0.1 and 0.1. Eventually, we could learn these
coefficients from a training corpus (e.g., with the
1002
Precision Recall F-measure
NER 71.27% 95.56% 81.65%
NER+DYN 97.66% 92.59% 95.06%
Table 1: Comparison of methods to predict what per-
sons described in the text will appear in the image,
using Named Entity Recognition (NER), and the
salience measure with dynamic cut-off (DYN).
Expectation Maximization algorithm).
We do not separately evaluate our technology
for salience detection as this technology was
already extensively evaluated in the past (Moens,
2006).
4.3 Evaluating the improved system
The salience measure defines a ranking of all the
persons in a text. We will use this ranking to improve
our baseline system. We assume that it is possible
to automatically determine the number of faces that
are recognized in the image, which gives us an indi-
cation of a suitable cut-off value. This approach is
reasonable since face detection (determine whether a
face is present in the image) is significant easier than
face recognition (determine which person is present
in the image). In the improved model we assume
that persons which are ranked higher than, or equal
to, the cut-off value appear in the image. For ex-
ample, if 4 faces appear in the image, we assume
that only the 4 persons of which the names in the
text have been assigned the highest salience appear
in the image. We see from table 1 that the precision
(97.66%) has improved drastically, while the recall
remained high (92.59%). This confirms the hypoth-
esis that determining the focus of a text helps in de-
termining the persons that appear in the image.
5 Automatically annotating persons and
objects
After having developed a reasonable successful sys-
tem to detect what persons will appear in the image,
we turn to a more difficult case : Detecting persons
and all other objects that are described in the text.
5.1 Entity detection
We will first detect what words in the text refer to an
entity. For this, we perform part-of-speech tagging
(i.e., detecting the syntactic word class such as noun,
verb, etc.). We take that every noun in the text rep-
resents an entity. We have used LTPOS (Mikheev,
1997), which performed the task almost errorless
(precision of 98.144% and recall of 97.36% on the
nouns in the test corpus). Person names which were
segmented using the NER package are also marked
as entities.
5.2 Baseline system
We want to detect the objects and the names of per-
sons which are both visible in the image and de-
scribed in the text. We start with a simple baseline
system, in which we assume that every entity in the
text appears in the image. As can be expected, this
results in a high recall (91.08%), and a very low pre-
cision (15.62%). We see that the problem here is
far more difficult compared to detecting only per-
son names. This can be explained by the fact that
many entities (such as for example August, idea and
history) will never (or only indirectly) appear in an
image. In the next section we will try to determine
what types of entities are more likely to appear in
the image.
6 Detection of the visualness of an entity
The assumption that every entity in the text appears
in the image is rather crude. We will enrich our
model with external world knowledge to find enti-
ties which are not likely to appear in an image. We
define a measure called visualness, which is defined
as the extent to which an entity can be perceived vi-
sually.
6.1 Entity classification
After we have performed entity detection, we want
to classify every entity according to a certain seman-
tic database. We use the WordNet (Fellbaum, 1998)
database, which organizes English nouns, verbs, ad-
jectives and adverbs in synsets. A synset is a col-
lection of words that have a close meaning and that
represent an underlying concept. An example of
such a synset is ?person, individual, someone, some-
body, mortal, soul?. All these words refer to a hu-
1003
man being. In order to correctly assign a noun in
a text to its synset, i.e., to disambiguate the sense
of this word, we use an efficient Word Sense Dis-
ambiguation (WSD) system that was developed by
the authors and which is described in (Deschacht
and Moens, 2006). Proper names are labeled by
the Named Entity Recognizer, which recognizes per-
sons, locations and organizations. These labels in
turn allow us to assign the corresponding WordNet
synset.
The combination of the WSD system and the
NER package achieved a 75.97% accuracy in classi-
fying the entities. Apart from errors that resulted
from erroneous entity detection (32.32%), errors
were mainly due to the WSD system (60.56%) and
in a smaller amount to the NER package (8.12%).
6.2 WordNet similarity
We determine the visualness for every synset us-
ing a method that was inspired by Kamps and Marx
(2002). Kamps and Marx use a distance measure
defined on the adjectives of the WordNet database
together with two seed adjectives to determine the
emotive or affective meaning of any given adjective.
They compute the relative distance of the adjective
to the seed synsets ?good? and ?bad? and use this
distance to define a measure of affective meaning.
We take a similar approach to determine the visu-
alness of a given synset. We first define a similarity
measure between synsets in the WordNet database.
Then we select a set of seed synsets, i.e. synsets
with a predefined visualness, and use the similarity
of a given synset to the seed synsets to determine the
visualness.
6.3 Distance measure
The WordNet database defines different relations be-
tween its synsets. An important relation for nouns is
the hypernym/hyponym relation. A noun X is a hy-
pernym of a noun Y if Y is a subtype or instance of
X. For example, ?bird? is a hypernym of ?penguin?
(and ?penguin? is a hyponym of ?bird?). A synset
in WordNet can have one or more hypernyms. This
relation organizes the synsets in a hierarchical tree
(Hayes, 1999).
The similarity measure defined by Lin (1998) uses
the hypernym/hyponym relation to compute a se-
mantic similarity between two WordNet synsets S1
and S2. First it finds the most specific (lowest in the
tree) synset Sp that is a parent of both S1 and S2.
Then it computes the similarity of S1 and S2 as
sim(S1, S2) =
2logP (Sp)
logP (S1) + logP (S2)
Here the probability P (Si) is the probability of
labeling any word in a text with synset Si or with
one of the descendants of Si in the WordNet hier-
archy. We estimate these probabilities by counting
the number of occurrences of a synset in the Sem-
cor corpus (Fellbaum, 1998; Landes et al, 1998),
where all noun chunks are labeled with their Word-
Net synset. The probability P (Si) is computed as
P (Si) =
C(Si)
?N
n=1 C(Sn)
+ ?Kk=1 P (Sk)
where C(Si) is the number of occurrences of Si,
N is the total number of synsets in WordNet and
K is the number of children of Si. The Word-
Net::Similarity package (Pedersen et al, 2004) im-
plements this distance measure and was used by the
authors.
6.4 Seed synsets
We have manually selected 25 seed synsets in Word-
Net, where we tried to cover the wide range of topics
we were likely to encounter in the test corpus. We
have set the visualness of these seed synsets to either
1 (visual) or 0 (not visual). We determine the visu-
alness of all other synsets using these seed synsets.
A synset that is close to a visual seed synset gets a
high visualness and vice versa. We choose a linear
weighting:
vis(s) =
?
i
vis(si)
sim(s, si)
C(s)
where vis(s) returns a number between 0 and 1 de-
noting the visualness of a synset s, si are the seed
synsets, sim(s, t) returns a number between 0 and 1
denoting the similarity between synsets s and t and
C(s) is constant given a synset s:
C(s) =
?
i
sim(s, si)
1004
6.5 Evaluation of the visualness computation
To determine the visualness, we first assign the cor-
rect WordNet synset to every entity, after which we
compute a visualness score for these synsets. Since
these scores are floating point numbers, they are
hard to evaluate manually. During evaluation, we
make the simplifying assumption that all entities
with a visualness below a certain threshold are not
visual, and all entities above this threshold are vi-
sual. We choose this threshold to be 0.5. This re-
sults in an accuracy of 79.56%. Errors are mainly
caused by erroneous entity detection and classifica-
tion (63.10%) but also because of an incorrect as-
signment of the visualness (36.90%) by the method
described above.
7 Creating an appearance model using
salience and visualness
In the previous section we have created a method to
calculate a visualness score for every entity, because
we stated that removing the entities which can never
be perceived visually will improve the performance
of our baseline system. An experiment proves that
this is exactly the case. If we assume that only the
entities that have a visualness above a 0.5 thresh-
old are visible and will appear in the image, we get
a precision of 48.81% and a recall of 87.98%. We
see from table 2 that this is already a significant im-
provement over the baseline system.
In section 4 we have seen that the salience mea-
sure helps in determining what persons are visible in
the image. We have used the fact that face detection
in images is relatively easily and can thus supply a
cut-off value for the ranked person names. In the
present state-of-the-art, we are not able to exploit a
similar fact when detecting all types of entities. We
will thus use the salience measure in a different way.
We compute the salience of every entity, and we
assume that only the entities with a salience score
above a threshold of 0.5 will appear in the image.
We see that this method drastically improves preci-
sion to 66.03%, but also lowers recall until 54.26%.
We now create a last model where we combine
both the visualness and the salience measures. We
want to calculate the probability of the occurrence of
an entity eim in the image, given a text t, P (eim|t).
We assume that this probability is proportional with
Precision Recall F-measure
Ent 15.62% 91.08% 26.66%
Ent+Vis 48.81% 87.98% 62.78%
Ent+Sal 66.03% 54.26% 59.56%
Ent+Vis+Sal 70.56% 67.82% 69.39%
Table 2: Comparison of methods to predict the en-
tities that appear in the image, using entity detec-
tion (Ent), and the visualness (Vis) and salience (Sal)
measures.
the degree of visualness and salience of eim in t. In
our framework, P (eim|t) is computed as the product
of the salience of the entity eim and its visualness
score, as we assume both scores to be independent.
Again, for evaluation sake, we choose a threshold
of 0.4 to transform this continuous ranking into a
binary classification. This results in a precision of
70.56% and a recall of 67.82%. This model is the
best of the 4 models for entity annotation which have
been evaluated.
8 Related Research
Using text that accompanies the image for annotat-
ing images and for training image recognition is not
new. The earliest work (only on person names) is
by Satoh (1999) and this research can be considered
as the closest to our work. The authors make a dis-
tinction between proper names, common nouns and
other words, and detect entities based on a thesaurus
list of persons, social groups and other words, thus
exploiting already simple semantics. Also a rudi-
mentary approach to discourse analysis is followed
by taking into account the position of words in a
text. The results were not satisfactory: 752 words
were extracted from video as candidates for being in
the accompanying images, but only 94 were correct
where 658 were false positives. Mori et al (2000)
learn textual descriptions of images from surround-
ing texts. These authors filter nouns and adjectives
from the surrounding texts when they occur above
a certain frequency and obtain a maximum hit rate
of top 3 words that is situated between 30% and
40%. Other approaches consider both the textual
and image features when building a content model
of the image. For instance, some content is selected
from the text (such as person names) and from the
1005
image (such as faces) and both contribute in describ-
ing the content of a document. This approach was
followed by Barnard (2003).
Westerveld (2000) combines image features and
words from collateral text into one semantic space.
This author uses Latent Semantic Indexing for rep-
resenting the image/text pair content. Ayache et al
(2005) classify video data into different topical con-
cepts. The results of these approaches are often dis-
appointing. The methods here represent the text as a
bag of words possibly augmented with a tf (term fre-
quency) x idf (inverse document frequency) weight
of the words (Amir et al, 2005). In exceptional
cases, the hierarchical XML structure of a text doc-
ument (which was manually annotated) is taken into
account (Westerveld et al, 2005). The most inter-
esting work here to mention is the work of Berg
et al (2004) who also process the nearly parallel
image-text pairs found in the Yahoo! news corpus.
They link faces in the image with names in the text
(recognized with named entity recognition), but do
not consider other objects. They consider pairs of
person names (text) and faces (image) and use clus-
tering with the Expectation Maximization algorithm
to find all faces belonging to a certain person. In
their model they consider the probability that an en-
tity is pictured given the textual context (i.e., the
part-of-speech tags immediately prior and after the
name, the location of the name in the text and the
distance to particular symbols such as ?(R)?), which
is learned with a probabilistic classifier in each step
of the EM iteration. They obtained an accuracy of
84% on person face recognition.
In the CLASS project we work together with
groups specialized in image recognition. In future
work we will combine face and object recognition
with text analysis techniques. We expect the recog-
nition and disambiguation of faces to improve if
many image-text pairs that treat the same person are
used. On the other hand our approach is also valu-
able when there are few image-text pairs that picture
a certain person or object. The approach of Berg
et al could be augmented with the typical features
that we use, namely salience and visualness. In De-
schacht et al (2007) we have evaluated the ranking
of persons and objects by the method we have de-
scribed here and we have shown that this ranking
correlates with the importance of persons and ob-
jects in the picture.
None of the above state-of-the-art approaches
consider salience and visualness as discriminating
factors in the entity recognition, although these as-
pects could advance the state-of-the-art.
9 Conclusion
Our society in the 21st century produces gigantic
amounts of data, which are a mixture of different
media. Our repositories contain texts interwoven
with images, audio and video and we need auto-
mated ways to automatically index these data and
to automatically find interrelationships between the
various media contents. This is not an easy task.
However, if we succeed in recognizing and aligning
content in near-parallel image-text pairs, we might
be able to use this acquired knowledge in index-
ing comparable image-text pairs (e.g., in video) by
aligning content in these media.
In the experiment described above, we analyze
the discourse and semantics of texts of near-parallel
image-text pairs in order to compute the probability
that an entity mentioned in the text is also present in
the accompanying image. First, we have developed
an approach for computing the salience of each en-
tity mentioned in the text. Secondly, we have used
the WordNet classification in order to detect the vi-
sualness of an entity, which is translated into a vi-
sualness probability. The combined salience and vi-
sualness provide a score that signals the probability
that the entity is present in the accompanying image.
We extensively evaluated all the different modules
of our system, pinpointing weak points that could be
improved and exposing the potential of our work in
cross-media exploitation of content.
We were able to detect the persons in the text
that are also present in the image with a (evenly
weighted) F-measure of more than 95%, and in addi-
tion were able to detect the entities that are present
in the image with a F-measure of more than 69%.
These results have been obtained by relying only on
an analysis of the text and were substantially better
than the baseline approach. Even if we can not re-
solve all ambiguity, keeping the most confident hy-
potheses generated by our textual hypotheses will
greatly assist in analyzing images.
In the future we hope to extrinsically evaluate
1006
the proposed technologies, e.g., by testing whether
the recognized content in the text, improves image
recognition, retrieval of multimedia sources, mining
of these sources, and cross-media retrieval. In addi-
tion, we will investigate how we can build more re-
fined appearance models that incorporate attributes
and actions of entities.
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978). We acknowledge the CLASS consortium
partners for their valuable comments and we are es-
pecially grateful to Yves Gufflet from the INRIA
research team (Grenoble, France) for collecting the
Yahoo! News dataset.
References
Arnon Amir, Janne Argillander, Murray Campbell,
Alexander Haubold, Giridharan Iyengar, Shahram
Ebadollahi, Feng Kang, Milind R. Naphade, Apostol
Natsev, John R. Smith, Jelena Tes?io?, and Timo Volk-
mer. 2005. IBM Research TRECVID-2005 Video
Retrieval System. In Proceedings of TRECVID 2005,
Gaithersburg, MD.
Ste?phane Ayache, Gearges M. Qunot, Jrme Gensel, and
Shin?Ichi Satoh. 2005. CLIPS-LRS-NII Experiments
at TRECVID 2005. In Proceedings of TRECVID
2005, Gaithersburg, MD.
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
Matching Words and Pictures. Journal of Machine
Learning Research, 3(6):1107?1135.
Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and
D.A. Forsyth. 2004. Who?s in the Picture? In Neural
Information Processing Systems, pages 137?144.
Koen Deschacht and Marie-Francine Moens. 2006. Ef-
ficient Hierarchical Entity Classification Using Con-
ditional Random Fields. In Proceedings of the
2nd Workshop on Ontology Learning and Population,
pages 33?40, Sydney, July.
Koen Deschacht, Marie-Francine Moens, and
W Robeyns. 2007. Cross-media entity recogni-
tion in nearly parallel visual and textual documents.
In Proceedings of the 8th RIAO Conference on Large-
Scale Semantic Access to Content (Text, Image, Video
and Sound). Cmu. (in press).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Brian Hayes. 1999. The Web of Words. American Sci-
entist, 87(2):108?112, March-April.
Jaap Kamps and Maarten Marx. 2002. Words with Atti-
tude. In Proceedings of the 1st International Confer-
ence on Global WordNet, pages 332?341, India.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building Semantic Concordances. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic Lex-
ical Database. The MIT Press.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proc. 15th International Conf. on Ma-
chine Learning.
Andrei Mikheev. 1997. Automatic Rule Induction for
Unknown-Word Guessing. Computational Linguis-
tics, 23(3):405?423.
Marie-Francine Moens, Patrick Jeuniaux, Roxana
Angheluta, and Rudradeb Mitra. 2006. Measur-
ing Aboutness of an Entity in a Text. In Proceed-
ings of HLT-NAACL 2006 TextGraphs: Graph-based
Algorithms for Natural Language Processing, East
Stroudsburg. ACL.
Marie-Francine Moens. 2006. Using Patterns of The-
matic Progression for Building a Table of Content of
a Text. Journal of Natural Language Engineering,
12(3):1?28.
Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka.
2000. Automatic Word Assignment to Images Based
on Image Division and Vector Quantization. In RIAO-
2000 Content-Based Multimedia Information Access,
Paris, April 12-14.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In The Proceedings of Fifth An-
nual Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-04),
Boston, May.
Shin?ichi Satoh, Yuichi Nakamura, and Takeo Kanade.
1999. Name-It: Naming and Detecting Faces in News
Videos. IEEE MultiMedia, 6(1):22?35, January-
March.
Thijs Westerveld, Jan C. van Gemert, Roberto Cornac-
chia, Djoerd Hiemstra, and Arjen de Vries. 2005. An
Integrated Approach to Text and Image Retrieval. In
Proceedings of TRECVID 2005, Gaithersburg, MD.
Thijs Westerveld. 2000. Image Retrieval: Content versus
Context. In Content-Based Multimedia Information
Access, RIAO 2000 Conference Proceedings, pages
276?284, April.
1007
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Hierarchical Entity Classifier Using Conditional Random Fields
Koen Deschacht
Interdisciplinary Centre for Law & IT
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
koen.deschacht@law.kuleuven.ac.be
Marie-Francine Moens
Interdisciplinary Centre for Law & IT
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
marie-france.moens@law.kuleuven.be
Abstract
In this paper we develop an automatic
classifier for a very large set of labels, the
WordNet synsets. We employ Conditional
Random Fields (CRFs) because of their
flexibility to include a wide variety of non-
independent features. Training CRFs on a
big number of labels proved a problem be-
cause of the large training cost. By tak-
ing into account the hypernym/hyponym
relation between synsets in WordNet, we
reduced the complexity of training from
O(TM2NG) to O(T (logM)2NG) with
only a limited loss in accuracy.
1 Introduction
The work described in this paper was carried out
during the CLASS project1. The central objec-
tive of this project is to develop advanced learning
methods that allow images, video and associated
text to be analyzed and structured automatically.
One of the goals of the project is the alignment of
visual and textual information. We will, for exam-
ple, learn the correspondence between faces in an
image and persons described in surrounding text.
The role of the authors in the CLASS project is
mainly on information extraction from text.
In the first phase of the project we build a clas-
sifier for automatic identification and categoriza-
tion of entities in texts which we report here. This
classifier extracts entities from text, and assigns a
label to these entities chosen from an inventory
of possible labels. This task is closely related to
both named entity recognition (NER), which tra-
ditionally assigns nouns to a small number of cate-
gories and word sense disambiguation (Agirre and
1http://class.inrialpes.fr/
Rigau, 1996; Yarowsky, 1995), where the sense
for a word is chosen from a much larger inventory
of word senses.
We will employ a probabilistic model that?s
been used successfully in NER (Conditional Ran-
dom Fields) and use this with an extensive inven-
tory of word senses (the WordNet lexical database)
to perform entity detection.
In section 2 we describe WordNet and it?s use
for entity categorization. Section 3 gives an
overview of Conditional Random Fields and sec-
tion 4 explains how the parameters of this model
are estimated during training. We will drastically
reduce the computational complexity of training in
section 5. Section 6 describes the implementation
of this method, section 7 the obtained results and
finally section 8 future work.
2 WordNet
WordNet (Fellbaum et al, 1998) is a lexical
database whose design is inspired by psycholin-
guistic theories of human lexical memory. English
nouns, verbs, adjectives and adverbs are organized
in synsets. A synset is a collection of words that
have a close meaning and that represent an under-
lying concept. An example of such a synset is
?person, individual, someone, somebody, mortal,
soul?. All these words refer to a human being.
WordNet (v2.1) contains 155.327 words, which
are organized in 117.597 synsets. WordNet de-
fines a number of relations between synsets. For
nouns the most important relation is the hyper-
nym/hyponym relation. A noun X is a hypernym
of a noun Y if Y is a subtype or instance of X. For
example, ?bird? is a hypernym of ?penguin? (and
?penguin? is a hyponym of ?bird?). This relation
organizes the synsets in a hierarchical tree (Hayes,
1999), of which a fragment is pictured in fig. 1.
33
Figure 1: Fragment of the hypernym/hyponym
tree
This tree has a depth of 18 levels and maximum
width of 17837 synsets (fig. 2).
We will build a classifier using CRFs that tags
noun phrases in a text with their WordNet synset.
This will enable us to recognize entities, and to
classify the entities in certain groups. Moreover,
it allows learning the context pattern of a certain
meaning of a word. Take for example the sentence
?The ambulance took the remains of the bomber
to the morgue.? Having every noun phrase tagged
with it?s WordNet synset reveals that in this sen-
tence, ?bomber? is ?a person who plants bombs?
(and not ?a military aircraft that drops bombs dur-
ing flight?). Using the hypernym/hyponym rela-
tions from WordNet, we can also easily find out
that ?ambulance? is a kind of ?car?, which in turn
is a kind of ?conveyance, transport? which in turn
is a ?physical object?.
3 Conditional Random Fields
Conditional random fields (CRFs) (Lafferty et al,
2001; Jordan, 1999; Wallach, 2004) is a statistical
method based on undirected graphical models. Let
X be a random variable over data sequences to be
labeled and Y a random variable over correspond-
ing label sequences. All components Yi of Y are
assumed to range over a finite label alphabet K.
In this paper X will range over the sentences of
a text, tagged with POS-labels and Y ranges over
the synsets to be recognized in these sentences.
We define G = (V,E) to be an undirected
graph such that there is a node v ? V correspond-
ing to each of the random variables representing an
element Yv of Y . If each random variable Yv obeys
the Markov property with respect to G (e.g., in a
first order model the transition probability depends
only on the neighboring state), then the model
(Y,X) is a Conditional Random Field. Although
the structure of the graph G may be arbitrary, we
limit the discussion here to graph structures in
Figure 2: Number of synsets per level in WordNet
which the nodes corresponding to elements of Y
form a simple first-order Markov chain.
A CRF defines a conditional probability distri-
bution p(Y |X) of label sequences given input se-
quences. We assume that the random variable se-
quences X and Y have the same length and use
x = (x1, ..., xT ) and y = (y1, ..., yT ) for an input
sequence and label sequence respectively. Instead
of defining a joint distribution over both label and
observation sequences, the model defines a condi-
tional probability over labeled sequences. A novel
observation sequence x is labeled with y, so that
the conditional probability p(y|x) is maximized.
We define a set of K binary-valued features or
feature functions fk(yt?1, yt,x) that each express
some characteristic of the empirical distribution of
the training data that should also hold in the model
distribution. An example of such a feature is
fk(yt?1, yt,x) =
?
?
?
?
?
1 if x has POS ?NN? andyt is concept ?entity?
0 otherwise
(1)
Feature functions can depend on the previous
(yt?1) and the current (yt) state. Considering K
feature functions, the conditional probability dis-
tribution defined by the CRF is
p(y|x) = 1Z(x)exp
{ T
?
t=1
K
?
k=1
?kfk(yt?1, yt,x)
}
(2)
where ?j is a parameter to model the observed
statistics and Z(x) is a normalizing constant com-
puted as
Z(x) =
?
y?Y
exp
{ T
?
t=1
K
?
k=1
?kfk(yt?1, yt,x)
}
This method can be thought of a generalization
of both the Maximum Entropy Markov model
(MEMM) and the Hidden Markov model (HMM).
34
It brings together the best of discriminative mod-
els and generative models: (1) It can accommo-
date many statistically correlated features of the
inputs, contrasting with generative models, which
often require conditional independent assumptions
in order to make the computations tractable and (2)
it has the possibility of context-dependent learning
by trading off decisions at different sequence posi-
tions to obtain a global optimal labeling. Because
CRFs adhere to the maximum entropy principle,
they offer a valid solution when learning from in-
complete information. Given that in information
extraction tasks, we often lack an annotated train-
ing set that covers all possible extraction patterns,
this is a valuable asset.
Lafferty et al (Lafferty et al, 2001) have shown
that CRFs outperform both MEMM and HMM
on synthetic data and on a part-of-speech tagging
task. Furthermore, CRFs have been used success-
fully in information extraction (Peng and McCal-
lum, 2004), named entity recognition (Li and Mc-
Callum, 2003; McCallum and Li, 2003) and sen-
tence parsing (Sha and Pereira, 2003).
4 Parameter estimation
In this section we?ll explain to some detail how to
derive the parameters ? = {?k}, given the train-
ing data. The problem can be considered as a con-
strained optimization problem, where we have to
find a set of parameters which maximizes the log
likelihood of the conditional distribution (McCal-
lum, 2003). We are confronted with the problem
of efficiently calculating the expectation of each
feature function with respect to the CRF model
distribution for every observation sequence x in
the training data. Formally, we are given a set
of training examples D =
{
x(i),y(i)
}N
i=1
where
each x(i) =
{
x(i)1 , x
(i)
2 , ..., x
(i)
T
}
is a sequence
of inputs and y(i) =
{
y(i)1 , y
(i)
2 , ..., y
(i)
T
}
is a se-
quence of the desired labels. We will estimate the
parameters by penalized maximum likelihood, op-
timizing the function:
l(?) =
N
?
i=1
log p(y(i)|x(i)) (3)
After substituting the CRF model (2) in the like-
lihood (3), we get the following expression:
l(?) =
N
?
i=1
T
?
t=1
K
?
k=1
?kfk(y(i)t?1, y
(i)
t ,x(i))
?
N
?
i=1
log Z(x(i))
The function l(?) cannot be maximized in closed
form, so numerical optimization is used. The par-
tial derivates are:
?l(?)
??k =
N
?
i=1
T
?
t=1
fk(y(i)t , y
(i)
t?1,x(i))
?
N
?
i=1
T
?
t=1
?
y,y?
fk(y?, y,x(i)) p(y?, y|x(i))
(4)
Using these derivates, we can iteratively adjust
the parameters ? (with Limited-Memory BFGS
(Byrd et al, 1994)) until l(?) has reached an opti-
mum. During each iteration we have to calculate
p(y?, y|x(i)). This can be done, as for the Hid-
den Markov Model, using the forward-backward
algorithm (Baum and Petrie, 1966; Forney, 1996).
This algorithm has a computational complexity of
O(TM2) (where T is the length of the sequence
and M the number of the labels). We have to exe-
cute the forward-backward algorithm once for ev-
ery training instance during every iteration. The
total cost of training a linear-chained CRFs is thus:
O(TM2NG)
where N is the number of training examples and G
the number of iterations. We?ve experienced that
this complexity is an important delimiting factor
when learning a big collection of labels. Employ-
ing CRFs to learn the 95076 WordNet synsets with
20133 training examples was not feasible on cur-
rent hardware. In the next section we?ll describe
the method we?ve implemented to drastically re-
duce this complexity.
5 Reducing complexity
In this section we?ll see how we create groups of
features for every label that enable an important
reduction in complexity of both labeling and train-
ing. We?ll first discuss how these groups of fea-
tures are created (section 5.1) and then how both
labeling (section 5.2) and training (section 5.3) are
performed using these groups.
35
Figure 3: Fragment of the tree used for labeling
5.1 Hierarchical feature selection
To reduce the complexity of CRFs, we assign a
selection of features to every node in the hierar-
chical tree. As discussed in section 2 WordNet de-
fines a relation between synsets which organises
the synsets in a tree. In its current form this tree
does not meet our needs: we need a tree where
every label used for labeling corresponds to ex-
actly one leaf-node, and no label corresponds to
a non-leaf node. We therefor modify the existing
tree. We create a new top node (?top?) and add the
original tree as defined by WordNet as a subtree to
this top-node. We add leaf-nodes corresponding
to the labels ?NONE?, ?ADJ?, ?ADV?, ?VERB?
to the top-node and for the other labels (the noun
synsets) we add a leaf-node to the node represent-
ing the corresponding synset. For example, we
add a node corresponding to the label ?ENTITY?
to the node ?entity?. Fig. 3 pictures a fraction of
this tree. Nodes corresponding to a label have an
uppercase name, nodes not corresponding to a la-
bel have a lowercase name.
We use v to denote nodes of the tree. We call
the top concept vtop and the concept v+ the parent
of v, which is the parent of v?. We call Av the
collection of ancestors of a concept v, including v
itself.
We will now show how we transform a regular
CRF in a CRF that uses hierarchical feature selec-
tion. We first notice that we can rewrite eq. 2 as
p(y|x) = 1Z(x)
T
?
t=1
G(yt?1, yt,x)
with G(yt?1, yt,x) = exp(
K
?
k=1
?kfk(yt?1, yt,x))
We rewrite this equation because it will enable
us to reduce the complexity of CRFs and it has
the property that p(yt|yt?1,x) ? G(yt?1, yt,x)
which we will use in section 5.3.
We now define a collection of features Fv for
every node v. If v is leaf-node, we define Fv as the
collection of features fk(yt?1, yt,x) for which it is
possible to find a node vt?1 and input x for which
fk(vt?1, v,x) 6= 0. If v is a non-leaf node, we de-
fine Fv as the collection of features fk(yt?1, yt,x)
(1) which are elements of Fv? for every child node
v? of v and (2) for every v?1 and v?2 , children of
v, it is valid that for every previous label vt?1 and
input x fk(vt?1, v?1 ,x) =fk(vt?1, v?2 ,x).
Informally, Fv is the collection of features
which are useful to evaluate for a certain node. For
the leaf-nodes, this is the collection of features that
can possibly return a non-zero value. For non-leaf
nodes, it?s useful to evaluate features belonging to
Fv when they have the same value for all the de-
scendants of that node (which we can put to good
use, see further).
We define F ?v = Fv\Fv+ where v+ is the parent
of label v. For the top node vtop we define F ?vtop =
Fvtop . We also set
G?(yt?1, yt,x) = exp
?
?
?
?
fk?F ?yt
?kfk(yt?1, yt,x)
?
?
?
We?ve now organised the collection of features in
such a way that we can use the hierarchical rela-
tions defined by WordNet when determining the
probability of a certain labeling y. We first see
that
G(yt?1, yt,x) = exp
?
?
?
?
fk?Fyt
?kfk(yt?1, yt,x)
?
?
?
= G(yt?1, y+t , x)G?(yt?1, yt, x)
= ...
=
?
v?Ayt
G?(yt?1, v, x)
we can now determine the probability of a labeling
y, given input x
p(y|x) = 1Z(x)
T
?
t=1
?
v?Ayt
G?(yt?1, v,x) (5)
This formula has exactly the same result as eq. 2.
Because we assigned a collection of features to ev-
ery node, we can discard parts of the search space
when searching for possible labelings, obtaining
an important reduction in complexity. We elab-
orate this idea in the following sections for both
labeling and training.
36
5.2 Labeling
The standard method to label a sentence with
CRFs is by using the Viterbi algorithm (Forney,
1973; Viterbi, 1967) which has a computational
complexity of O(TM2). The basic idea to reduce
this computational complexity is to select the best
labeling in a number of iterations. In the first itera-
tion, we label every word in a sentence with a label
chosen from the top-level labels. After choosing
the best labeling, we refine our choice (choose a
child label of the previous chosen label) in subse-
quent iterations until we arrive at a synset which
has no children. In every iteration we only have
to choose from a very small number of labels, thus
breaking down the problem of selecting the correct
label from a large number of labels in a number of
smaller problems.
Formally, when labeling a sentence we find the
label sequence y such that y has the maximum
probability of all labelings. We will estimate the
best labeling in an iterative way: we start with
the best labeling ytop?1 = {ytop?11 , ..., y
top?1
T }
choosing only from the children ytop?1t of the top
node. The probability of this labeling ytop?1 is
p(ytop?1|x) = 1Z ?(x)
T
?
t=1
G?(yt?1, ytop?1t ,x)
where Z ?(x) is an appropriate normalizing con-
stant. We now select a labeling ytop?2 so that on
every position t node ytop?2t is a child of y
top?1
t .
The probabilty of this labeling is (following eq. 5)
p(ytop?2|x) = 1Z ?(x)
T
?
t=1
?
v?A
ytop?2t
G?(yt?1, v,x)
After selecting a labeling ytop?2 with maximum
probability, we proceed by selecting a labeling
ytop?3 with maximum probability etc.. We pro-
ceed using this method until we reach a labeling
in which every yt is a node which has no children
and return this labeling as the final labeling.
The assumption we make here is that if a node
v is selected at position t of the most probable la-
beling ytop?s the children v? have a larger prob-
ability of being selected at position t in the most
probable labeling ytop?s?1. We reduce the num-
ber of labels we take into consideration by stating
that for every concept v for which v 6= ytop?st , we
set G?(yt?1, v?t ,x) = 0 for every child v? of v.
This reduces the space of possible labelings dras-
tically, reducing the computational complexity of
Figure 4: Nodes that need to be taken into account
during the forward-backward algorithm
the Viterbi algorithm. If q is the average number
of children of a concept, the depth of the tree is
logq(M). On every level we have to execute the
Viterbi algorithm for q labels, thus resulting in a
total complexity of
O(T logq(M)q2) (6)
5.3 Training
We will now discuss how we reduce the compu-
tational complexity of training. As explained in
section 4 we have to estimate the parameters ?k
that optimize the function l(?). We will show here
how we can reduce the computational complex-
ity of the calculation of the partial derivates ?l(?)??k(eq. 4). The predominant factor with regard to
the computational complexity in the evaluation of
this equation is the calculation of p(yt?1, y|x(i)).
Recall we do this with the forward-backward al-
gorithm, which has a computational complexity
of O(TM2). We reduce the number of labels to
improve performance. We will do this by mak-
ing the same assumption as in the previous sec-
tion: for every concept v at level s, for which
v 6= ytop?st , we set G?(yt?1, v?t ,x) = 0 for
every child v? of v. Since (as noted in sect.
5.2) p(vt|yt?1,x) ? G(yt?1, vt,x), this has the
consequence that p(vt|yt?1,x) = 0 and that
p(vt, yt?1|x) = 0. Fig. 4 gives a graphical repre-
sentation of this reduction of the search space. The
correct label here is ?LABEL1? , the grey nodes
have a non-zero p(vt, yt?1|x) and the white nodes
have a zero p(vt, yt?1|x).
In the forward backward algorithm we only
have to account every node v that has a non-zero
p(v, yt?1|x). As can be easily seen from fig. 4,
the number of nodes is qlogqM , where q is the
average number of children of a concept. The to-
tal complexity of running the forward-backward
algorithm is O(T (q logqM)2). Since we have to
run this algorithm once for every gradient compu-
37
Figure 5: Time needed for one training cycle
tation for every training instance we find the total
training cost
O(T (q logqM)2NG) (7)
6 Implementation
To implement the described method we need two
components: an interface to the WordNet database
and an implementation of CRFs using a hierar-
chical model. JWordNet is a Java interface to
WordNet developed by Oliver Steele (which can
be found on http://jwn.sourceforge.
net/). We used this interface to extract the Word-
Net hierarchy.
An implementation of CRFs using the hierar-
chical model was obtained by adapting the Mallet2
package. The Mallet package (McCallum, 2002)
is an integrated collection of Java code useful for
statistical natural language processing, document
classification, clustering, and information extrac-
tion. It also offers an efficient implementation of
CRFs. We?ve adapted this implementation so it
creates hierarchical selections of features which
are then used for training and labeling.
We used the Semcor corpus (Fellbaum et al,
1998; Landes et al, 1998) for training. This cor-
pus, which was created by the Princeton Univer-
sity, is a subset of the English Brown corpus con-
taining almost 700,000 words. Every sentence in
the corpus is noun phrase chunked. The chunks
are tagged by POS and both noun and verb phrases
are tagged with their WordNet sense. Since we do
not want to learn a classification for verb synsets,
we replace the tags of the verbs with one tag
?VERB?.
2http://mallet.cs.umass.edu/
Figure 6: Time needed for labeling
7 Results
The major goal of this paper was to build a clas-
sifier that could learn all the WordNet synsets in a
reasonable amount of time. We will first discuss
the improvement in time needed for training and
labeling and then discuss accuracy.
We want to test the influence of the number of
labels on the time needed for training. Therefor,
we created different training sets, all of which had
the same input (246 sentences tagged with POS la-
bels), but a different number of labels. The first
training set only had 5 labels (?ADJ?, ?ADV?,
?VERB?, ?entity? and ?NONE?). The second had
the same labels except we replaced the label ?en-
tity? with either ?physical entity?, ?abstract entity?
or ?thing?. We continued this procedure, replac-
ing parent nouns labels with their children (i.e.
hyponyms) for subsequent training sets. We then
trained both a CRF using a hierarchical feature se-
lection and a standard CRF on these training sets.
Fig. 5 shows the time needed for one iteration
of training with different numbers of labels. We
can see how the time needed for training slowly
increases for the CRF using hierarchical feature
selection but increases fast when using a standard
CRF. This is conform to eq. 7.
Fig. 6 shows the average time needed for la-
beling a sentence. Here again the time increases
slowly for a CRF using hierarchical feature selec-
tion, but increases fast for a standard CRF, con-
form to eq. 6.
Finally, fig 7 shows the error rate (on the train-
ing data) after each training cycle. We see that a
standard CRF and a CRF using hierarchical fea-
ture selection perform comparable. Note that fig
7 gives the error rate on the training data but this
38
can differ considerable from the error rate on un-
seen data.
After these tests on a small section of the Sem-
cor corpus, we trained a CRF using hierarchi-
cal feature selection on 7/8 of the full corpus.
We trained for 23 iterations, which took approx-
imately 102 hours. Testing the model on the re-
maining 1/8 of the corpus resulted in an accuracy
of 77.82%. As reported in (McCarthy et al, 2004),
a baseline approach that ignors context but simply
assigns the most likely sense to a given word ob-
tains a accuracy of 67%. We did not have the pos-
sibility to compare the accuracy of this model with
a standard CRF, since as already stated, training
such a CRF takes impractically long, but we can
compare our systems with existing WSD-systems.
Mihalcea and Moldovan (Mihalcea and Moldovan,
1999) use the semantic density between words to
determine the word sense. They achieve an ac-
curacy of 86.5% (testing on the first two tagged
files of the Semcor corpus). Wilks and Stevenson
(Wilks and Stevenson, 1998) use a combination
of knowledge sources and achieve an accuracy of
92%3. Note that both these methods use additional
knowledge apart from the WordNet hierarchy.
The sentences in the training and testing sets
were already (perfectly) POS-tagged and noun
chunked, and that in a real-life situation addi-
tional preprocessing by a POS-tagger (such as the
LT-POS-tagger4) and noun chunker (such as de-
scribed in (Ramshaw and Marcus, 1995)) which
will introduce additional errors.
8 Future work
In this section we?ll discuss some of the work we
plan to do in the future. First of all we wish to
evaluate our algorithm on standard test sets, such
as the data of the Senseval conference5 , which
tests performance on word sense disambiguation,
and the data of the CoNLL 2003 shared task6, on
named entity recognition.
An important weakness of our algorithm is the
fact that, to label a sentence, we have to traverse
the hierarchy tree and choose the correct synsets
at every level. An error at a certain level can not
be recovered. Therefor, we would like to perform
3This method was tested on the Semcore corpus, but use
the word senses of the Longman Dictionary of Contemporary
English
4http://www.ltg.ed.ac.uk/software/
5http://www.senseval.org/
6http://www.cnts.ua.ac.be/conll2003/
Figure 7: Error rate during training
some a of beam-search (Bisiani, 1992), keeping
a number of best labelings at every level. We
strongly suspect this will have a positive impact
on the accuracy of our algorithm.
As already mentioned, this work is carried out
during the CLASS project. In the second phase
of this project we will discover classes and at-
tributes of entities in texts. To accomplish this
we will not only need to label nouns with their
synset, but we also need to label verbs, adjec-
tives and adverbs. This can become problem-
atic as WordNet has no hypernym/hyponym rela-
tion (or equivalent) for the synsets of adjectives
and adverbs. WordNet has an equivalent relation
for verbs (hypernym/troponym), but this structures
the verb synsets in a big number of loosely struc-
tured trees, which is less suitable for the described
method. VerbNet (Kipper et al, 2000) seems a
more promising resource to use when classify-
ing verbs, and we will also investigate the use
of other lexical databases, such as ThoughtTrea-
sure (Mueller, 1998), Cyc (Lenat, 1995), Open-
mind Commonsense (Stork, 1999) and FrameNet
(Baker et al, 1998).
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978).
References
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In Pro-
ceedings of the 16th International Conference on
39
Computational Linguistics (Coling?96), pages 16?
22, Copenhagen, Denmark.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley Framenet project. In Proceedings of the
COLING-ACL.
L. E. Baum and T. Petrie. 1966. Statistical in-
ference for probabilistic functions of finite state
markov chains. Annals of Mathematical Statistics,,
37:1554?1563.
R. Bisiani. 1992. Beam search. In S. C. Shapiro,
editor, Encyclopedia of Artificial Intelligence, New
York. Wiley-Interscience.
Richard H. Byrd, Jorge Nocedal, and Robert B. Schn-
abel. 1994. Representations of quasi-newton matri-
ces and their use in limited memory methods. Math.
Program., 63(2):129?156.
C. Fellbaum, J. Grabowski, and S. Landes. 1998. Per-
formance and confidence in a semantic annotation
task. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
G. D. Forney. 1973. The viterbi algorithm. In Pro-
ceeding of the IEEE, pages 268 ? 278.
G. D. Forney. 1996. The forward-backward algo-
rithm. In Proceedings of the 34th Allerton Confer-
ence on Communications, Control and Computing,
pages 432?446.
Brian Hayes. 1999. The web of words. American
Scientist, 87(2):108?112, March-April.
Michael I. Jordan, editor. 1999. Learning in Graphical
Models. The MIT Press, Cambridge.
K. Kipper, H.T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. Proceedings
of the Seventh National Conference on Artificial In-
telligence (AAAI-2000).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning.
S. Landes, C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11):32?38.
Wei Li and Andrew McCallum. 2003. Rapid develop-
ment of hindi named entity recognition using con-
ditional random fields and feature induction. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 2(3):290?294.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL-2003, pages 188?191.
Edmonton, Canada.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of the
Nineteenth Conference on Uncertainty in Artificial
Intelligence.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Using automatically acquired predominant
senses for word sense disambiguation. In Proceed-
ings of the ACL SENSEVAL-3 workshop, pages 151?
154, Barcelona, Spain.
R. Mihalcea and D.I. Moldovan. 1999. A method
for word sense disambiguation of unrestricted text.
In Proceedings of the 37th conference on Associa-
tion for Computational Linguistics, pages 152?158.
Association for Computational Linguistics Morris-
town, NJ, USA.
Erik T. Mueller. 1998. Natural language processing
with ThoughtTreasure. Signiform, New York.
F. Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using con-
ditional random fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 329?336.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceed-
ings of the Third ACL Workshop on Very Large Cor-
pora, pages 82?94. Cambridge MA, USA.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of Human
Language Technology, HLT-NAACL.
D. Stork. 1999. The openmind initiative. IEEE Intelli-
gent Systems & their applications, 14(3):19?20.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding algo-
rithm. IEEE Trans. Informat. Theory, 13:260?269.
Hanna M. Wallach. 2004. Conditional random fields:
An introduction. Technical Report MS-CIS-04-21.,
University of Pennsylvania CIS.
Y. Wilks and M. Stevenson. 1998. Word sense disam-
biguation using optimised combinations of knowl-
edge sources. Proceedings of COLING/ACL, 98.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196.
40
Workshop on TextGraphs, at HLT-NAACL 2006, pages 25?28,
New York City, June 2006. c?2006 Association for Computational Linguistics
Measuring Aboutness of an Entity in a Text 
 
Marie-Francine Moens Patrick Jeuniaux 
Legal Informatics and Information Retrieval Department. of Psychology 
Katholieke Universiteit Leuven, Belgium University of Memphis, USA 
marie-france.moens@law.kuleuven.be pjeuniaux@mail.psyc.memphis.edu 
Roxana Angheluta Rudradeb Mitra 
Legal Informatics and Information Retrieval Mission Critical, IT 
Katholieke Universiteit Leuven, Belgium Brussels, Belgium 
anghelutar@yahoo.com rdm@missioncriticalit.com  
  
Abstract 
In many information retrieval and selec-
tion tasks it is valuable to score how much 
a text is about a certain entity and to com-
pute how much the text discusses the en-
tity with respect to a certain viewpoint. In 
this paper we are interested in giving an 
aboutness score to a text, when the input 
query is a person name and we want to 
measure the aboutness with respect to the 
biographical data of that person. We pre-
sent a graph-based algorithm and compare 
its results with other approaches. 
1 Introduction 
In many information processing tasks one is inter-
ested in measuring how much a text or passage is 
about a certain entity. This is called aboutness or 
topical relevance (Beghtol 1986; Soergel 1994). 
Simple word counts of the entity term often give 
only a rough estimation of aboutness. The true fre-
quency of the entity might be hidden by corefer-
ents. Two entities are considered as coreferents 
when they both refer to the same entity in the situa-
tion described in the text (e.g., in the sentences: 
"Dan Quayle met his wife in college. The Indiana 
senator married her shortly after he finished his 
studies": "his", "Indiana senator" and "he" all core-
fer to "Dan Quayle"). If we want to score the 
aboutness of an entity with respect to a certain 
viewpoint, the aboutness is also obfuscated by the 
referents that refer to the chosen viewpoint and in 
which context the entity is mentioned. In the ex-
ample ?Dan Quayle ran for presidency?, ?presi-
dency? can be considered as a referent for ?Dan 
Quayle?. Because, coreferents and referents can be 
depicted in a graphical representation of the dis-
course content, it seems interesting to exploit this 
graph structure in order to compute aboutness. This 
approach is inspired by studies in cognitive science 
on text comprehension (van Dijk and Kintsch, 
1983). When humans read a text, they make many 
inferences about and link information that is found 
in the text, a behavior that influences aboutness 
assessment. Automated aboutness computation has 
many applications such as text indexing, summari-
zation, and text linking.  
We focus on estimating the aboutness score of a 
text given an input query in the form of a person 
proper name. The score should reflect how much 
the text deals with biographical information about 
the person. We present an algorithm based on ei-
genvector analysis of the link matrix of the dis-
course graph built by the noun phrase coreferents 
and referents. We test the approach with a small set 
of documents, which we rank by decreasing about-
ness of the input entity. We compare the results 
with results obtained by traditional approaches 
such as a normalized term frequency (possibly cor-
rected by coreference resolution and augmented 
with other referent information). Although the re-
sults on a small test set do not pretend to give firm 
evidence on the validity of our approach, our con-
tribution lies in the reflection of using graph based 
document representations of discourse content and 
exploiting this structure in content recognition.  
2 Methods  
Our approach involves the detection of entities and 
their noun phrase coreferents, the generation of 
terms that are correlated with biographical infor-
25
mation, the detection of references between enti-
ties, and the computation of the aboutness score. 
As linguistic resources we used the LT-POS tagger 
developed at the University of Edinburgh and the 
Charniak parser developed at Brown University.  
2.1 Noun Phrase Coreference Resolution 
Coreference resolution focuses on detecting ?iden-
tity'' relationships between noun phrases (i.e. not 
on is-a or whole/part links). It is natural to view 
coreferencing as a partitioning or clustering of the 
set of entities. The idea is to group coreferents into 
the same cluster, which is accomplished in two 
steps: 1) detection of the entities and extraction of 
their features set; 2) clustering of the entities. For 
the first subtask we use the same set of features as 
in Cardie and Wagstaff (1999). For the second step 
we used the progressive fuzzy clustering algorithm 
described in Angheluta et al (2004).  
2.2 Learning Biographical Terms 
We learn a term?s biographical value as the corre-
lation of the term with texts of biographical nature. 
There are different ways of learning associations 
present in corpora (e.g., use of the mutual informa-
tion statistic, use of the chi-square statistic). We 
use the likelihood ratio for a binomial distribution 
(Dunning 1993), which tests the hypothesis 
whether the term occurs independently in texts of 
biographical nature given a large corpus of bio-
graphical and non-biographical texts. For consider-
ing a term as biography-related, we set a likelihood 
ratio threshold such that the hypothesis can be re-
jected with a certain significance level.  
2.3 Reference Detection between Entities  
We assume that the syntactic relationships between 
entities (proper or common nouns) in a text give us 
information on their semantic reference status. In 
our simple experiment, we consider reference rela-
tionships found within a single sentence, and more 
specifically we take into account relationships be-
tween two noun phrase entities. The analysis re-
quires that the sentences are syntactically analyzed 
or parsed. The following syntactic relationships are 
detected in the parse tree of each sentence:   
1) Subject-object: An object refers to the subject 
(e.g., in the sentence He eats an apple, an apple 
refers to He). This relationship type also covers 
prepositional phrases that are the argument of a 
verb (e.g., in the sentence He goes to Hollywood, 
Hollywood refers to He). The relationship holds 
between the heads of the respective noun phrases 
in case other nouns modify them.    
2) NP-PP{NP}: A noun phrase is modified by a 
prepositional noun phrase: the head of the preposi-
tional noun phrase refers to the head of the domi-
nant noun phrase (e.g., in the chunk The nominee 
for presidency, presidency refers to The nominee). 
3) NP-NP: A noun phrase modifies another noun 
phrase: the head of the modifying noun phrase re-
fers to the head of the dominant noun phrase (e.g., 
in the chunk Dan Quayle's sister, Dan Quayle re-
fers to sister, in the chunk sugar factory, sugar 
refers to factory). 
 When a sentence is composed of different sub-
clauses and when one of the components of the 
first two relationships has the form of a subclause, 
the first noun phrase of the subclause is consid-
ered. When computing a reference relation with an 
entity term, we only consider biographical terms 
found as described in (2.2).  
2.4 Computing the Aboutness Score  
The aboutness of a document text D for the input 
entity E is computed as follows:  
 
aboutness(D,E) = entity _ score(E)
entity _ score(F)
F?distinctentities of D
?  
 
entity_score is zero when E does not occur in D. 
Otherwise we compute the entity score as follows. 
We represent D as a graph, where nodes represent 
the entities as mentioned in the text and the 
weights of the connections represent the reference 
score (in our experiments set to 1 when the entities 
are coreferents, 0.5 when the entities are other ref-
erents). The values 1 and 0.5 were selected ad hoc. 
Future fine-tuning of the weights of the edges of 
the discourse graph based on discourse features 
could be explored (cf. Giv?n 2001). The edge val-
ues are stored in a link matrix A. The authority of 
an entity is computed by considering the values of 
the principal eigenvector of ATA. (cf. Kleinberg 
1998) (in the results below this approach is re-
ferred to as LM). In this way we compute the au-
thority of each entity in a text.  
26
 We implemented four other entity scores: the 
term frequency (TF), the term frequency aug-
mented with noun phrase coreference information 
(TFCOREF), the term frequency augmented with 
reference information (weighted by 0.5) (TFREF) 
and the term frequency augmented with corefer-
ence and reference information (TFCOREFREF). 
The purpose is not that the 4 scoring functions are 
mutually comparable, but that the ranking of the 
documents that is produced by each of them can be 
compared against an ideal ranking built by hu-
mans.  
3 Experiments and Results 
For learning person related words we used a train-
ing corpus consisting of biographical texts of per-
sons obtained from the Web (from 
http://www.biography.com) and biographical and 
non-biographical texts from DUC-2002 and DUC-
2003. For considering a term as biography-related, 
we set a likelihood ratio threshold such that the 
hypothesis of independence can be rejected with a 
significance level of less than 0.0025, assuring that 
the selected terms are really biography-related.  
 In order to evaluate the aboutness computation, 
we considered five input queries consisting of a 
proper person name phrase ("Dan Quayle" (D), 
"Hillary Clinton" (H), "Napoleon" (N), "Sadam 
Hussein" (S) and "Sharon Stone" (ST)) and 
downloaded for each of the queries 5 texts from 
the Web (each text contains minimally once an 
exact match with the input query). Two persons 
were asked to rank the texts according to rele-
vancy, if they were searching biographical infor-
mation on the input person (100% agreement was 
obtained). Two aspects are important in determin-
ing relevancy: a text should really and almost ex-
clusively contain biographical information of the 
input person in order not to lose time with other 
information. For each query, at least one of the 
texts is a biographical text and one of the texts only 
marginally mentions the person in question. All 
texts except for the biography texts speak about 
other persons, and pronouns are abundantly used. 
The "Hillary Clinton" texts do not contain many 
other persons except for Hillary, in contrast with 
the "Dan Quayle", "Napoleon" and "Sadam Hus-
sein" texts. The "Hillary Clinton" texts are in gen-
eral quite relevant for this first lady. For 
"Napoleon" there is one biographical text on Napo-
leon's surgeon that mentions Napoleon only mar-
ginally. The ?Dan Quayle? texts contain a lot of 
direct speech. For "Sharon Stone" 4 out of the 5 
texts described a movie in which this actress 
played a role, thus being only marginally relevant 
for a demand of biographical data of the actress.  
 Then we ranked the texts based on the TF, 
TFCOREF, TFREF, TFCOREFREF and LM 
scores and computed the congruence of each rank-
ing (Rx) with the manual ranking (Rm). We used the 
following measure of similarity of the rankings:  
 
sim(Rx, Rm) =1?
rx, i? rm, i
i
?
floor
n2
2
*100
 
where n is the number of items in the 2 rankings 
and rx,i and rm,i denote the position of the ith item in 
Rx and Rm. respectively. Table 1 shows the results.  
4 Discussion of the Results and Related 
Research 
From our limited experiments we can draw the 
following findings. It is logical that erroneous 
coreference resolution worsens the results com-
pared to the TF baseline. In one of the "Napoleon? 
texts, one mention of Napoleon and one mention of 
the name of his surgeon entail that a large number 
of pronouns in the text are wrongly resolved. They 
all refer to the surgeon, but the system considers 
them as referring to Napoleon, making that the 
ranking of this text is completely inversed com-
pared to the ideal one. Adding other reference in-
formation gives some mixed results. The ranking 
based on the principal eigenvector computation of 
the link matrix of the text that represents reference 
relationships between entities provides a natural 
way of computing a ranking of the texts with re-
gard to the person entity. This can be explained as 
follows. Decomposition into eigenvectors breaks 
down the original relationships into linear inde-
pendent components. Sorting them according to 
their corresponding eigenvalues sorts the compo-
nents from the most important information to the 
less important one. When keeping the principal 
eigenvector, we keep the most important informa-
tion which best distinguishes it from other infor-
mation while ignoring marginal information. In 
this way we hope to smooth some noise that is 
generated when building the links. On the other 
hand, when relationships that are wrongly detected 
27
are dominant, they will be reinforced (as is the case 
in the ?Napoleon? text). Although an aboutness 
score is normalized by the sum of a text?s entity 
scores, the effect of this normalization and the be-
havior of eigenvectors in case of texts of different 
length should be studied.  
 The work is inspired by link analysis algorithms 
such as HITS, which uses theories of spectral parti-
tioning of a graph for detecting authoritative pages 
in a graph of hyperlinked pages (Kleinberg 1998). 
Analogically, Zha (2002) detects terms and sen-
tences with a high salience in a text and uses these 
for summarization. The graph here is made of 
linked term and sentence nodes. Other work on 
text summarization computes centrality on graphs 
(Erkan and Radev 2004; Mihalcea and Tarau 
2004). We use a linguistic motivation for linking 
terms in texts founded in reference relationships 
such as coreference and reference by biographical 
terms in certain syntactical constructs. Intuitively, 
an important entity is linked to many referents; the 
more important the referents are, the more impor-
tant the entity is. Latent semantic indexing (LSI) is 
also used to detect main topics in a set of docu-
ments/sentences, it will not explicitly model the 
weights of the edges between entities.  
 Our implementation aims at measuring the 
aboutness of an entity from a biographical view-
point. One can easily focus upon other viewpoints 
when determining the terms that enter into a refer-
ence relationship with the input entity (e.g., com-
puting the aboutness of an input animal name with 
regard to its reproductive activities). 
5 Conclusion 
In this paper we considered the problem of ranking 
texts when the input query is in the form of a per-
son proper name and when we are interested in 
biographical information. The ranking based on the 
computation of the principal eigenvector of the 
link matrix that represents coreferent and other 
referent relationships between noun phrase entities 
offers novel directions for future research. 
6 Acknowledgements 
The research was sponsored by the IWT-grant Nr. 
ADV/000135/KUL).  
 
Table 1. Similarity of the system made rankings com-
pared to the ideal ranking for the methods used with 
regard to the input queries.  
 
 TF TFCOREF TFREF TFCOREFREF LM 
D 0.33 0.00 0.33 0.00 0.50 
H 0.33 0.50 0.33 0.33 0.66 
N 0.66 0.33 0.66 0.66 0.33 
S 0.83 0.66 0.66 0.66 1.00 
ST 0.00 0.33 0.16 0.50 0.83 
 
7 References 
Angheluta, R., Jeuniaux, P., Mitra, R. and Moens, M.-F. 
(2004). Clustering algorithms for noun phrase 
coreference resolution. In Proceedings JADT - 2004. 
7?mes Journ?es internationales d'Analyse statistique 
des Donn?es Textuelles.  Louvain-La-Neuve, Bel-
gium. 
Beghtol, C. (1986). Bibliographic classification theory 
and text linguistics: Aboutness analysis, intertextual-
ity and the cognitive act of classifying documents. 
Journal of Documentation, 42(2): 84-113.   
Cardie C. and Wagstaff K. (1999). Noun phrase co-
reference as clustering. In Proceedings of the Joint 
Conference on Empirical Methods in NLP and Very 
Large Corpora.  
Dunning, T. (1993). Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics, 19: 61-74.  
Erkan, G. and Radev, D.R. (2004). LexRank: Graph-
based lexical centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22: 
457-479.  
Giv?n, T. (2001). Syntax. An Introduction. Amsterdam: 
John Benjamins.  
Kleinberg, J.M. (1998). Authoritative sources in a hy-
perlinked environment. In Proceedings 9th ACM-
SIAM Symposium on Discrete Algorithms (pp. 668-
677).  
Mihalcea, R. and Tarau, P. (2004). TextRank : Bringing 
order into texts. In Proceedings of EMNLP (pp. 404-
411).  
Soergel, D. (1994). Indexing and retrieval performance: 
The logical evidence. Journal of the American Soci-
ety for Information Science, 45 (8): 589-599.    
Van Dijk, T. A. and Kintsch, W. (1983). Strategies of 
Discourse Comprehension. New York: Academic 
Press. 
Zha, H. (2002). Generic summarization and keyphrase 
extraction using mutual reinforcement principle and 
sentence clustering. In Proceedings of the 25th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval 
(pp. 113-120). New York : ACM. 
28
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 52?57,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Meeting TempEval-2: Shallow Approach for Temporal Tagger  
 
 
Oleksandr Kolomiyets 
Katholieke Universiteit Leuven 
Department of Computer Science 
Celestijnenlaan 200A, Heverlee, Belgium 
oleksandr.kolomiyets 
@cs.kuleuven.be 
 
 
Marie-Francine Moens 
Katholieke Universiteit Leuven 
Department of Computer Science 
Celestijnenlaan 200A, Heverlee, Belgium 
sien.moens@cs.kuleuven.be 
 
 
 
 
 
 
Abstract 
Temporal expressions are one of the important 
structures in natural language. In order to un-
derstand text, temporal expressions have to be 
identified and normalized by providing ISO-
based values. In this paper we present a shal-
low approach for automatic recognition of 
temporal expressions based on a supervised 
machine learning approach trained on an an-
notated corpus for temporal information, 
namely TimeBank. Our experiments demon-
strate a performance level comparable to a 
rule-based implementation and achieve the 
scores of 0.872, 0.836 and 0.852 for precision, 
recall and F1-measure for the detection task 
respectively, and 0.866, 0.796, 0.828 when an 
exact match is required.   
1 Introduction 
The task of recognizing temporal expressions 
(sometimes also referred as time expressions or 
simply TIMEX) was first introduced in the Mes-
sage Understanding Conference (MUC) in 1995. 
Temporal expressions were treated as a part of the 
Named Entity Recognition (NER) task, in which 
capitalized tokens in text were labeled with one of 
the predefined semantic labels, such as Date, Time, 
Person, Organization, Location, Percentage, and 
Money. As the types of temporal entities identified 
in this way were too restricted and provided little 
further information, the Automated Content Ex-
traction (ACE) launched a competition campaign 
for Temporal Expression Recognition and Norma-
lization (TERN 2004). The tasks were to identify 
temporal expressions in free text and normalize 
them providing an ISO-based date-time value. Lat-
er evaluations of ACE in 2005, 2006 and 2007 un-
fortunately did not set new challenges for temporal 
expression recognition and thus the participation 
interest in this particular task decreased.  
TempEval-2 is a successor of TempEval-2007 
and will take place in 2010. The new evaluation 
initiative sets new challenges for temporal text 
analysis. While TempEval-2007 was solely fo-
cused on recognition of temporal links, the     
TempEval-2 tasks aim at an all-around temporal 
processing with separate evaluations for recogni-
tion of temporal expressions and events, for the 
estimation of temporal relations between events 
and times in the same sentence, between events 
and document creation time, between two events in 
consecutive sentences and between two events, 
where one of them syntactically dominates the oth-
er (Pustejovsky et al, 2009). These evaluations 
became possible with a new freely available corpus 
with annotated temporal information, TimeBank 
(Pustejovsky et al, 2003a), and an annotation 
schema, called TimeML (Pustejovsky et al, 
2003b).  
For us all the tasks of TempEval-2 seem to be 
interesting. In this paper we make the first step 
towards a comprehensive temporal analysis and 
address the problem of temporal expression recog-
nition as it is set in TempEval-2. Despite a number 
of previous implementations mainly done in the 
context of the ACE TERN competition, very few, 
52
and exclusively rule-based methods were reported 
for temporal taggers on TimeBank developed by 
using the TimeML annotation scheme. As a main 
result of the deep analysis of relevant work (Sec-
tion 2), we decided to employ a machine learning 
approach for constituent-based classifications with 
generic syntactic and lexical features. 
    The remainder of the paper is organized as fol-
lows: in Section 2 we provide the details of rele-
vant work done in this field along with corpora and 
annotations schemes used; Section 3 describes the 
approach; experimental setup, results and error 
analysis are provided in Section 4. Finally, Section 
5 gives an outlook for further improvements and 
research.  
2 Related Work 
For better understanding of the performance levels 
provided in the paper we first describe evaluation 
metrics defined for the temporal expression recog-
nition task and then the methods and datasets used 
in previous research.   
2.1 Evaluation metrics 
With the start of the ACE TERN competition in 
2004, two major evaluation conditions were pro-
posed: Recognition+Normalization (full task) and 
Recognition only (TERN, 2004). 
Detection (Recognition): Detection is a prelimi-
nary task towards the full TERN task, in which 
temporally relevant expressions have to be found. 
The scoring is very generous and implies a minim-
al overlap in the extent of the reference and the 
system output tags. As long as there is at least one 
overlapping character, the tags will be aligned. 
Any alignment of the system output tags are scored 
as a correct detection. 
Sloopy span: Spans usually refer to strict match of 
both boundaries (the extent) of a temporal expres-
sion (see Exact Match). ?Sloopy? admits recog-
nized temporal expressions as long as their right 
boundary is the same as in the corresponding 
TimeBank?s extents (Boguraev and Ando, 2005). 
The motivation was to assess the correctness of 
temporal expressions recognized in TimeBank, 
which was reported as inconsistent with respect to 
some left boundary items, such as determiners and 
pre-determiners.   
Exact Match (Bracketing or Extent Recogni-
tion): Exact match measures the ability to correct-
ly identify the extent of the TIMEX. The extent of 
the reference and the system output tags must 
match exactly the system output tag to be scored as 
correct.  
2.2 Datasets  
To date, there are two annotated corpora used for 
temporal evaluations, the ACE TERN corpus and 
TimeBank (Pustejovsky et al, 2003a). In this sec-
tion we provide a brief description of the temporal 
corpora and annotation standards, which can sub-
stantially influence recognition results.  
Most of the implementations referred as the 
state-of-the-art were developed in the scope of the 
ACE TERN 2004. For evaluations, a training cor-
pus of 862 documents with about 306 thousand 
words was provided. Each document represents a 
news article formatted in XML, in which TIMEX2 
tags denote temporal expressions. The total num-
ber of temporal expressions for training is 8047 
TIMEX2 tags with an average of 10.5 per docu-
ment. The test set comprises 192 documents with 
1828 TIMEX2 tags (Ferro, 2004).  
The annotation of temporal expressions in the 
ACE corpus was done with respect to the TIDES 
annotation guidelines (Ferro et al, 2003). The 
TIDES standard specifies so-called markable ex-
pressions, whose syntactic head must be an appro-
priate lexical trigger, e.g. ?minute?, ?afternoon?, 
?Monday?, ?8:00?, ?future? etc. When tagged, the 
full extent of the tag must correspond to one of the 
grammatical categories: nouns (NN, NNP), noun 
phrases (NP), adjectives (JJ), adjective phrases 
(ADJP), adverbs (RB) and adverb phrases 
(ADVP). According to this, all pre- and postmo-
difiers as well as dependent clauses are also in-
cluded to the TIMEX2 extent, e.g. ?five days after 
he came back?, ?nearly four decades of expe-
rience?. Such a broad extent for annotations is of 
course necessary for correct normalization, but on 
the other hand, introduces difficulties for exact 
match. Another important characteristic of the 
TIDES standard are the nested temporal expres-
sions as for example: 
 
<TIMEX2>The<TIMEX2 VAL = "1994">1994 
</TIMEX2> baseball season </TIMEX2> 
 
53
The most recent annotation language for tem-
poral expressions, TimeML (Pustejovsky et al, 
2003b), with an underlying corpus TimeBank 
(Pustejovsky et al, 2003a), opens up new possibili-
ties for processing temporal information in text. 
Besides the specification for temporal expressions, 
i.e. TIMEX3, which is to a large extent inherited 
from TIDES, TimeML provides a means to capture 
temporal semantics by annotations with suitably 
defined attributes for fine-grained specification of 
analytical detail (Boguraev et al, 2007). The anno-
tation schema establishes new entity and relation 
marking tags along with numerous attributes for 
them. This advancement influenced the extent for 
event-based temporal expression, in which depen-
dent clauses are no longer included into TIMEX3 
tags. The TimeBank corpus includes 186 docu-
ments with 68.5 thousand words and 1423 
TIMEX3 tags.       
2.3 Approaches for temporal processing  
As for any recognition problem, there are two ma-
jor ways to solve it. Historically, rule-based sys-
tems were first implemented. Such systems are 
characterized by a great human effort in data anal-
ysis and rule writing. With a high precision such 
systems can be successfully employed for recogni-
tion of temporal expressions, whereas the recall 
reflects the effort put into the rule development. By 
contrast, machine learning methods require an an-
notated training set, and with a decent feature de-
sign and a minimal human effort can provide 
comparable or even better results than rule-based 
implementations. As the temporal expression rec-
ognition is not only about to detect them but also to 
provide an exact match, machine learning ap-
proaches can be divided into token-by-token classi-
fication following B(egin)-I(nside)-O(utside) 
encoding and binary constituent-based classifica-
tion, in which an entire chunk-phrase is under con-
sideration to be classified as a temporal expression 
or not. In this case, exact segmentation is the re-
sponsibility of the chunker or the parser used.  
Rule-based systems: One of the first well-known 
implementations of temporal taggers was presented 
in (Many and Wilson, 2000). The approach relies 
on a set of hand-crafted and machine-discovered 
rules, which are based upon shallow lexical fea-
tures. On average the system achieved a value of 
83.2% for F1-measure against hand-annotated da-
ta. The dataset used comprised a set of 22 New 
York Times articles and 199 transcripts of Voice of 
America taken from the TDT2 collection (Graff et 
al., 1999). It should be noted that the reported per-
formance was provided in terms of an exact match. 
Another example of rule-based temporal taggers is 
Chronos described in (Negri and Marseglia, 2004), 
which achieved the highest scores (F1-measure) in 
the TERN 2004 of 0.926 and 0.878 for recognition 
and exact match.  
Recognition of temporal expressions using 
TimeBank as an annotated corpus, is reported in 
(Boguraev and Ando, 2005) based on a cascaded 
finite-state grammar (500 stages and 16000 transi-
tions). A complex approach achieved an F1-
measure value of 0.817 for exact match and 0.896 
for detecting ?sloopy? spans.  Another known im-
plementation for TimeBank is an adaptation of 
(Mani and Wilson, 2000) from TIMEX2 to 
TIMEX3 with no reported performance level. 
Machine learning recognition systems: Success-
ful machine learning TIMEX recognition systems 
are described in (Ahn et al, 2005; Hacioglu et al, 
2005; Poveda et al, 2007). Proposed approaches 
made use of a token-by-token classification for 
temporal expressions represented by B-I-O encod-
ing with a set of lexical and syntactic features, e.g., 
token itself, part-of-speech tag, label in the chunk 
phrase and the same features for each token in the 
context window. The performance levels are pre-
sented in Table 1. All the results were obtained on 
the ACE TERN dataset.  
Approach F1 (detection) F1 (exact match) 
Ahn et al, 2005 0.914 0.798 
Hacioglu et al, 
2005 0.935 0.878 
Poveda et al, 
2007 0.986 0.757 
 
Table 1. Performance of Machine Learning Ap-
proaches with B-I-O Encoding 
 
Constituent-based classification approach for 
temporal expression recognition was presented in 
(Ahn et al, 2007). By comparing to the previous 
work (Ahn et al, 2005) on the same ACE TERN 
dataset, the method demonstrates a slight decrease 
in detection with F1-measure of 0.844 and a nearly 
equivalent F1-measure value for exact match of 
0.787.   
54
The major characteristic of machine learning 
approaches was a simple system design with a mi-
nimal human effort. Machine-learning based rec-
ognition systems have proven to have a 
comparable recognition performance level to state-
of-the-art rule-based detectors.  
3 Approach 
The approach we describe in this section employs a 
machine-learning technique and more specifically 
a binary constituent based classification. In this 
case the entire phrase is under consideration to be 
labeled as a TIMEX or not. We restrict the classifi-
cation for the following phrase types and grammat-
ical categories: NN, NNP, CD, NP, JJ, ADJP, RB, 
ADVP and PP. In order to make it possible, for 
each sentence we parse the initial input line with a 
Maximum Entropy parser (Ratnaparkhi, 1998) and 
extract all phrase candidates with respect the types 
defined above. Each phrase candidate is examined 
against the manual annotations for temporal ex-
pressions found in the sentence. Those phrases, 
which correspond to the temporal expressions in 
the sentence are taken as positive examples, while 
the rest are considered as negative ones. Only one 
sub-tree from a parse is marked as positive for a 
distinct TIMEX at once.  After that, for each can-
didate we produce a feature vector, which includes 
the following features: head phrase, head word, 
part-of-speech for head word, character type and 
character type pattern for head word as well as for 
the entire phrase. Character type and character type 
pattern1 features are implemented following Ahn et 
al. (2005). The patterns are defined by using the 
symbols X, x and 9. X and x are used for character 
type as well as for character type patterns for 
representing capital and lower-case letters for a 
token. 9 is used for representing numeric tokens. 
Once the character types are computed, the corres-
ponding character patterns are produced. A pattern 
consists of the same symbols as character types, 
and contains no sequential redundant occurrences 
of the same symbol. For example, the constituent 
?January 30th? has character type ?Xxxxxxx 
99xx? and pattern ?X(x) (9)(x)?.  
On this basis, we employ a classifier that im-
plements a Maximum Entropy model2 and per-
                                                          
1
 In literature such patterns are also known as shorttypes. 
2
 http://maxent.sourceforge.net/ 
forms categorization of constituent-phrases ex-
tracted from the input.  
4 Experiments, Results and Error Analy-
sis 
After processing the TimeBank corpus of 183 
documents we had 2612 parsed sentences with 
1224 temporal expressions in them. 2612 sentences 
resulted in 49656 phrase candidates. We separated 
the data in order to perform 10-fold cross valida-
tion, train the classifier and test it on an unseen 
dataset. The evaluations were conducted with re-
spect to the TERN 2004 evaluation plan (TERN, 
2004) and described in Section 2.1.    
After running experiments the classifier demon-
strated the performance in detection of TIMEX3 
tags with a minimal overlap of one character with 
precision, recall and F1-measure at 0.872, 0.836 
and 0.852 respectively. Since the candidate phrases 
provided by the parser do not always exactly align 
annotated temporal expressions, the results for the 
exact match experiments are constrained by an es-
timated upper-bound recall of 0.919. The experi-
ments on exact match demonstrated a small decline 
of performance level and received scores of 0.866, 
0.796 and 0.828 for precision, recall and F1-
measure respectively.  
Putting the received figures in context, we can 
say that with a very few shallow features and a 
standard machine learning algorithm the recogniz-
er of temporal expressions performed at a compa-
rable operational level to the rule-based approach 
of (Boguraev and Ando, 2005) and outperformed it 
in exact match. A comparative performance sum-
mary is presented in Table 2.  
Sometimes it is very hard even for humans to 
identify the use of obvious temporal triggers in a 
specific context. As a result, many occurrences of 
such triggers remained unannotated for which 
TIMEX3 identification could not be properly car-
ried out.   Apart of obvious incorrect parses, in-
exact alignment between temporal expressions and 
candidate phrases was caused by annotations that 
occurred at the middle of a phrase, for example 
?eight-years-long?, ?overnight?, ?yesterday?s?. In 
total there are 99 TIMEX3 tags (or 8.1%) misa-
ligned with the parser output, which resulted in 53 
(or 4.3%) undetected TIMEX3s. 
 
55
 P R F1 
Detection 
Our approach 0.872 0.836 0.852 
Sloopy Span 
(Boguraev and 
Ando, 2005) 0.852 0.952 0.896 
Exact Match 
Our approach 0.866 0.796 0.828 
(Boguraev and 
Ando, 2005) 0.776 0.861 0.817 
 
Table 2. Comparative Performance Summary 
 
Definite and indefinite articles are unsystemati-
cally left out or included into TIMEX3 extent, 
which may introduce an additional bias in classifi-
cation.    
5 Conclusion and Future Work 
In this paper we presented a machine learning 
approach for detecting temporal expression using a 
recent annotated corpus for temporal information, 
TimeBank. Employing shallow syntactic and lexi-
cal features, the performance level of the method 
achieved comparable results to a rule-based ap-
proach of Boguraev and Ando (2005) and for the 
exact match task even outperforms it. Although a 
direct comparison with other state-of-the-art sys-
tems is not possible, due to different evaluation 
corpora, annotation standards and size in particu-
lar, our experiments disclose a very important cha-
racteristic. While the recognition systems in the 
TERN 2004 reported a substantial drop of F1-
measure between detection and exact match results 
(6.5 ? 11.6%), our phrase-based detector demon-
strates a light decrease in F1-measure (2.4%), whe-
reas the precision declines only by 0.6%. This 
important finding leads us to the conclusion that 
most of TIMEX3s in TimeBank can be detected at 
a phrase-based level with a reasonably high per-
formance.  
Despite a good recognition performance level 
there is, of course, room for improvement. Many 
implementations in the TERN 2004 employ a set 
of apparent temporal tokens as one of the features. 
In our implementation, the classifier has difficul-
ties with very simple temporal expressions such as 
?now?, ?future?, ?current?, ?currently?, ?recent?, 
?recently?. A direct employment of vocabularies 
with temporal tokens may substantially increase 
the F1-measure of the method, however, it yet has 
to be proven. As reported in (Ahn et al, 2007) a 
precise recognition of temporal expressions is a 
prerequisite for accurate normalization.  
With our detector and a future normalizer we 
are able make the first step towards solving the 
TempEval-2 tasks, which introduce new challenges 
in temporal information processing: identification 
of events, identification of temporal expressions 
and identification of temporal relations (Puste-
jovsky et al, 2009). Our future work will be fo-
cused on improving current results by a new 
feature design, finalizing the normalization task 
and identification of temporal relations. All these 
components will result in a solid system infrastruc-
ture for all-around temporal analysis.  
Acknowledgments 
This work has been partly funded by the Flemish 
government (through IWT) and by Space Applica-
tions Services NV as part of the ITEA2 project  
LINDO (ITEA2-06011). 
References 
 
Ahn, D., Adafre, S. F., and de Rijke, M. 2005. Extract-
ing Temporal Information from Open Domain Text: 
A Comparative Exploration. Digital Information 
Management, 3(1):14-20, 2005.  
Ahn, D., van Rantwijk, J., and de Rijke, M. 2007. A 
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings NAACL-
HLT 2007. 
Boguraev, B., and Ando, R. K. 2005. TimeBank-Driven 
TimeML Analysis. In Annotating, Extracting and 
Reasoning about Time and Events. Dagstuhl Seminar 
Proceedings. Dagstuhl, Germany 
Boguraev, B., Pustejovsky, J., Ando, R., and Verhagen, 
M. 2007. TimeBank Evolution as a Community Re-
source for TimeML Parsing. Language Resource and 
Evaluation, 41(1): 91?115. 
Ferro, L., Gerber, L., Mani, I., Sundheim, B., and Wil-
son, G. 2003. TIDES 2003 Standard for the Annota-
tion of Temporal Expressions. Sept. 2003. 
timex2.mitre.org. 
Ferro, L. 2004. TERN Evaluation Task Overview and 
Corpus, 
<http://fofoca.mitre.org/tern_2004/ferro1_TERN200
4_task_corpus.pdf> (accessed: 5.03.2009) 
56
Graff, D., Cieri, C., Strassel, S., and Martey, N. 1999. 
The TDT-2 Text and Speech Corpus. In Proceedings 
of DARPA Broadcast News Workshop, pp. 57-60. 
Hacioglu, K., Chen, Y., and Douglas, B. 2005. Auto-
matic Time Expression Labeling for English and 
Chinese Text. In Proceedings of CICLing-2005, pp. 
348-359; Springer-Verlag, Lecture Notes in Comput-
er Science, vol. 3406. 
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics (Hong Kong, October 03 - 06, 2000). Annual 
Meeting of the ACL. Association for Computational 
Linguistics, Morristown, NJ, pp. 69-76. 
Negri, M. and Marseglia, L. 2004. Recognition and 
Normalization of Time Expressions: ITC-irst at 
TERN 2004. Technical Report, ITC-irst, Trento. 
Poveda, J., Surdeanu, M., and Turmo, J. 2007. A Com-
parison of Statistical and Rule-Induction Learners for 
Automatic Tagging of Time Expressions in English. 
In Proceedings of the International Symposium on 
Temporal Representation and Reasoning, pp. 141-
149.  
Pustejovsky, J., Hanks, P., Saur?, R., See, A., Day, D., 
Ferro, L., Gaizauskas, R., Lazo, M., Setzer, A., and 
Sundheim, B. 2003a. The TimeBank Corpus. In Pro-
ceedings of Corpus Linguistics 2003, pp. 647-656. 
Pustejovsky, J., Casta?o, J., Ingria, R., Saur?, R., Gai-
zauskas, R., Setzer, A., and Katz, G. 2003b. Time-
ML: Robust Specification of Event and Temporal 
Expressions in Text. In Proceedings of IWCS-5, Fifth 
International Workshop on Computational Seman-
tics. 
Pustejovsky, J., Verhagen, M., Nianwen, X., Gai-
zauskas, R., Hepple, M., Schilder, F., Katz, G., Saur?, 
R., Saquete, E., Caselli, T., Calzolari, N., Lee, K., 
and Im, S. 2009. TempEval2: Evaluating Events, 
Time Expressions and Temporal Relations. 
<http://www.timeml.org/tempeval2/tempeval2-
proposal.pdf> (accessed: 5.03.2009) 
Ratnaparkhi, A. 1999. Learning to Parse Natural Lan-
guage with Maximum Entropy Models. Machine 
Learning, 34(1): 151-175. 
TERN 2004 Evaluation Plan, 2004, 
<http://fofoca.mitre.org/tern_2004/tern_evalplan-
2004.29apr04.pdf> (accessed: 5.03.2009) 
 
57
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613?1624,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data
(and Nothing Else)
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
We present a new language pair agnostic ap-
proach to inducing bilingual vector spaces
from non-parallel data without any other re-
source in a bootstrapping fashion. The pa-
per systematically introduces and describes all
key elements of the bootstrapping procedure:
(1) starting point or seed lexicon, (2) the confi-
dence estimation and selection of new dimen-
sions of the space, and (3) convergence. We
test the quality of the induced bilingual vec-
tor spaces, and analyze the influence of the
different components of the bootstrapping ap-
proach in the task of bilingual lexicon extrac-
tion (BLE) for two language pairs. Results re-
veal that, contrary to conclusions from prior
work, the seeding of the bootstrapping pro-
cess has a heavy impact on the quality of the
learned lexicons. We also show that our ap-
proach outperforms the best performing fully
corpus-based BLE methods on these test sets.
1 Introduction
Bilingual lexicons serve as an indispensable source
of knowledge for various cross-lingual tasks such
as cross-lingual information retrieval (Lavrenko et
al., 2002; Levow et al, 2005) or statistical machine
translation (Och and Ney, 2003). Additionally, they
are a crucial component in cross-lingual knowledge
transfer, where the knowledge about utterances in
one language may be transferred to another. The
utility of the transfer or annotation projection by
means of bilingual lexicons has already been proven
in various tasks such as semantic role labeling (Pado?
and Lapata, 2009; van der Plas et al, 2011), parsing
(Zhao et al, 2009; Durrett et al, 2012; Ta?ckstro?m et
al., 2013b), POS tagging (Yarowsky and Ngai, 2001;
Das and Petrov, 2011; Ta?ckstro?m et al, 2013a), etc.
Techniques for automatic bilingual lexicon ex-
traction (BLE) from parallel corpora on the basis
of word alignment models are well established (Och
and Ney, 2003). However, due to a relative scarce-
ness of parallel data for many language pairs and
domains, alternative approaches that rely on compa-
rable corpora have also gained much interest (e.g.,
Fung and Yee (1998); Rapp (1999)).
The models that rely on non-parallel data typ-
ically represent each word by a high-dimensional
vector in a feature vector space, where the dimen-
sions of the vector are its context features. The con-
text features are typically words co-occurring with
the word in a predefined context.1 The similar-
ity of two words, wS1 given in the source language
LS with vocabulary V S and wT2 in the target lan-
guage LT with vocabulary V T is then computed as
sim(wS1 , wT2 ) = SF (cv(wS1 ), cv(wT2 )). cv(wS1 ) =
[scS1 (c1), . . . , scS1 (cN )] is a context vector for wS1
with N context features ck, where scS1 (ck) denotes
the score for wS1 associated with context feature ck
(similar for wT2 ). SF is a similarity function (e.g.,
cosine, the Kullback-Leibler divergence, the Jaccard
index) operating on the context vectors (Lee, 1999).
When operating with 2 languages, the context fea-
tures cannot be compared directly. Therefore, in
order to compare the feature vectors cv(wS1 ) and
cv(wT2 ), the context features need to span a shared
1The context may be a document, a paragraph, a window of
predefined size around each occurrence of wSi in CS , etc. For
an overview, see, e.g., (Tamura et al, 2012).
1613
bilingual vector space. The standard way of build-
ing a bilingual vector space is to use bilingual lex-
icon entries (Rapp, 1999; Fung and Cheung, 2004;
Gaussier et al, 2004) as dimensions of the space.
However, there seems to be an apparent flaw in
logic, since the methods assume that there exist
readily available bilingual lexicons that are then
used to induce bilingual lexicons! Therefore, the fo-
cus of the researchers has turned to designing BLE
methods that do not rely on any external translation
resources such as machine-readable bilingual lex-
icons and parallel corpora (Haghighi et al, 2008;
Vulic? et al, 2011).
In order to circumvent this issue, one line of re-
cent work aims to bootstrap high-quality bilingual
vector spaces from a small initial seed lexicon. The
seed lexicon is constructed by harvesting identical
or similarly spelled words across languages (Koehn
and Knight, 2002; Peirsman and Pado?, 2010), and it
spans the initial bilingual vector space. The space is
then gradually enriched with new dimensions/axes
during the bootstrapping procedure. The bootstrap-
ping process has already proven its validity in induc-
ing bilingual lexicons for closely similar languages
such as Spanish-Portuguese or Croatian-Slovene
(Fis?er and Ljubes?ic?, 2011), but it still lacks further
generalization to more distant language pairs.
The main goal of this paper is to shed new light
on the bootstrapping approaches to bilingual lexicon
extraction, and to construct a language pair agnos-
tic bootstrapping method that is able to build high-
quality bilingual vector spaces that consequently
lead to high-quality bilingual lexicons for more dis-
tant language pairs where orthographic similarity is
not sufficient to seed bilingual vector spaces. We
aim to answer the following key questions:
? How to seed bilingual vector spaces besides us-
ing only orthographically similar words?
? Is it better to seed bilingual spaces with trans-
lation pairs/dimensions that are frequent in the
corpus, and does the frequency matter at all?
Does the size of the initial seed lexicon matter?
? How to enrich bilingual vector spaces with only
highly reliable dimensions in order to prevent
semantic drift?
With respect to these questions, the main contribu-
tions of this article are:
? We present a complete overview of the frame-
work of bootstrapping bilingual vector spaces
from non-parallel data without any additional
resources. We dissect the bootstrapping pro-
cess and describe all its key components.
? We introduce a new way of seeding the boot-
strapping procedure that does not rely on any
orthographic clues and that yields bilingual
vector spaces of higher quality. We analyze the
impact of different seed lexicons on the quality
of induced bilingual vector spaces.
? We show that in the setting without any ex-
ternal translation resources, our bootstrapping
approach yields lexicons that outperform the
best performing corpus-based BLE methods on
standard test datasets for 2 language pairs.
2 Boostrapping Bilingual Vector Spaces: A
General Overview
This section presents the complete bootstrapping
procedure that starts with an initial seed lexicon
which spans the initial bilingual vector space, while
as the output in each iteration of the procedure it pro-
duces an updated bilingual vector space that can be
used to extract a bilingual lexicon.
2.1 General Framework
We assume that we are solely in possession of a
(non-parallel) bilingual corpus C that is composed
of a sub-corpus CS given in the source language LS ,
and a sub-corpus CT in the target language LT . All
word types that occur in CS constitute a set V S . All
word types in CT constitute a set V T . The goal is to
build a bilingual vector space using only corpus C.
Assumption 1. Dimensions of the bilingual vector
space are one-to-one word translation pairs. For in-
stance, dimensions of a Spanish-English space are
pairs like (perro, dog), (ciencia, science), etc. The
one-to-one constraint (Melamed, 2000), although
not valid in general, simplifies the construction of
the bootstrapping procedure. Z denotes the set of
translation pairs that are the dimensions of the space.
Computing cross-lingual word similarity in a
bilingual vector space. Now, assume that our bilin-
gual vector space consists of N one-to-one word
translation pairs ck = (cSk , cTk ), k = 1, . . . , N . For
each word wSi ? V S , we compute the similarity of
1614
that word with each word wTj ? V T by computing
the similarity between their context vectors cv(wSi )
and cv(wTj ), which are actually their representations
in the N -dimensional bilingual vector space.
The cross-lingual similarity is computed follow-
ing the standard procedure (Gaussier et al, 2004):
(1) For each source word wSi ? V S , build its N -
dimensional context vector cv(wSi ) that consists of
association scores scSk (cSk ), that is, we compute the
strength of association with the ?source? part of each
dimension ck that constitutes the N -dimensional
bilingual space. The association is dependent on the
co-occurrence of wSi and cSk in a predefined context.
Various functions such as the log-likelihood ratio
(LLR) (Rapp, 1999; Ismail and Manandhar, 2010),
TF-IDF (Fung and Yee, 1998), or pointwise mu-
tual information (PMI) (Bullinaria and Levy, 2007;
Shezaf and Rappoport, 2010) are typically used as
weighting functions to quantify the strength of the
association.
(2) Repeat step (1) for each target word wTj ? V T
and build context vectors cv(wTj ) that consist of
scores scTk (cTk ).
(3) Since cSk and cTk address the same dimension
ck in the bilingual vector space for each k =
1, . . . , N , we are able to compute the similarity be-
tween cv(wSi ) and cv(wTj ) using any similarity mea-
sure such as the Jaccard index, the Kullback-Leibler
or the Jensen-Shannon divergence, the cosine mea-
sure, or others (Lee, 1999; Cha, 2007).
The similarity score for two words wSi and wTj
is sim(wSi , wTj ). For each source word wSi , we can
build a ranked listRL(wSi ) that consists of all words
wTj ? V T ranked according to their respective sim-
ilarity scores sim(wSi , wTj ). In the similar fashion,
we can build a ranked list RL(wTj ), for each target
word wTj . We call the top scoring target word wTj
for some source word wSi its translation candidate,
and write TC(wSi ) = wTj . Additionally, we label
the ranked list RL(wSi ) that is pruned at position M
as RLM (wSi ).
Bootstrapping. The key idea of the bootstrapping
approach relies on an insight that highly reliable
translation pairs (wS1 , wT2 ) that are encountered us-
ing the N -dimensional bilingual vector space might
be added as new dimensions of the space. By adding
these new dimensions, it might be possible to extract
more highly reliable translation pairs that were pre-
viously not used as dimensions of the space, and the
iterative procedure repeats until no new dimensions
are found. The induced bilingual vector space may
then be observed as a bilingual lexicon per se, but it
may also be used to find translation equivalents for
other words which are not used to span the space.
Algorithm 1: Bootstrapping a bilingual vector space
Input : Bilingual corpus C = CS ? CT
Initialize: (1) Obtain a one-to-one seed lexicon. The
entries from the lexicon are initial dimensions of the
space: Z0; (2) s = 0;
Bootstrap:
repeat
1: For each wSi ? V
S : compute RL(wSi ) using Zs ;
2: For each wTj ? V
T : compute RL(wTj ) using Zs ;
3: For each wSi ? V
S and wTj ? V
T : score each
translation pair (wSi , TC(w
S
i )) and (TC(w
T
j ), w
T
j )
and add them to a pool of candidate dimensions ;
4: Choose the best candidates from the pool and add
them as new dimensions: Zs+1 ? Zs ? {best} ;
5: Resolve collisions in Zs+1;
6: s? s + 1 ;
until no new dimensions are found (convergence) ;
Output: One-to-one translation pairs? Dimensions of a
bilingual vector space Zfinal
The overview of the procedure as given by alg. 1
reveals these crucial points in the procedure: (Q1)
how to provide initial dimensions of the space? (the
initialization step), (Q2) how to score each trans-
lation pair, estimate their confidence, and how to
choose the best candidates from the pool of candi-
dates? (steps 3 and 4), and (Q3) how to resolve
potential collisions that violate the one-to-one con-
straint? (step 5). We will discuss (Q1) and (Q2) in
more detail later, while we resolve (Q3) following a
simple heuristic as follows:
Assumption 2. In case of collision, dimen-
sions/pairs that are found at later stages of the boot-
strapping process overwrite previous dimensions.
The intuition here is that we expect for the quality of
the space to increase at each stage of the bootstrap-
ping process, and newer translation pairs should be
more confident than the older ones. For instance, if 2
out of N dimensions of a Spanish-English bilingual
space are pairs (piedra,wall) and (tapia,stone), but
then if during the bootstrapping process we extract a
new candidate pair (piedra,stone), we will delete the
former two dimensions and add the latter.
1615
2.2 Initializing Bilingual Vector Spaces
Seeding or initializing a bootstrapping procedure is
often a critical step regardless of the actual task
(McIntosh and Curran, 2009; Kozareva and Hovy,
2010), and it decides whether the complete process
will end as a success or a failure. However, Peirsman
and Pado? (2011) argue that the initialization step is
not crucial when dealing with bootstrapping bilin-
gual vector spaces. Here, we present two different
strategies of initializing the bilingual vector space.
Identical words and cognates. Previous work re-
lies exclusively on identical and similarly spelled
words to build the initial set of dimensions Z0
(Koehn and Knight, 2002; Peirsman and Pado?, 2010;
Fis?er and Ljubes?ic?, 2011). This strategy yields
promising results for closely similar language pairs,
but is of limited use for other language pairs.
High-frequency seeds. Another problem with us-
ing only identical words and cognates as seeds lies in
the fact that many of them might be infrequent in the
corpus, and as a consequence the expressiveness of a
bilingual vector space might be limited. On the other
hand, high-frequency words offer a lot of evidence
in the corpus that could be exploited in the boot-
strapping approach. In order to induce initial trans-
lation pairs, we rely on the framework of multilin-
gual probabilistic topic modeling (MuPTM) (Boyd-
Graber and Blei, 2009; De Smet and Moens, 2009;
Mimno et al, 2009; Zhang et al, 2010), that does
not require a bilingual lexicon, it operates with non-
parallel data, and is able to produce highly confident
translation pairs for high-frequency words (Mimno
et al, 2009; Vulic? and Moens, 2013).2 Therefore,
we can construct the initial seed lexicon as follows:
(1) Train a multilingual topic model on the corpus.
(2) Obtain one-to-one translation pairs using any of
the MuPTM-based models of cross-lingual similar-
ity, e.g., (Vulic? et al, 2011; Vulic? and Moens, 2013).
(3) Retain only symmetric translation pairs. This
step ensures that only highly confident pairs are used
as seed translation pairs.
(4) Rank translation pairs according to their fre-
quency in the corpus and use a subset of the most
2One can also use other models that are similar to MuPTM
such as (Haghighi et al, 2008; Daume? III and Jagarlamudi,
2011) to produce the initial seed lexicon, but that analysis is
beyond the scope of this work.
frequent symmetric pairs as seeds.
2.3 Estimating Confidence of New Dimensions
Another crucial step in the bootstrapping proce-
dure is the estimation of confidence in a translation
pair/candidate dimension. Errors in the early stages
of the procedure may negatively affect the learning
process and even cause semantic drift (Riloff and
Shepherd, 1999; McIntosh and Curran, 2009). We
therefore impose the constraint which requires trans-
lation pairs to be symmetric in order to qualify as po-
tential new dimensions of the space. In other words,
given the current set of dimensions Zs, a transla-
tion pair (wSi , wTj ) has a possibility to be chosen as
a new dimension from the pool of candidate dimen-
sions if and only if it holds: TC(wSi ) = wTj and
TC(wTj ) = wSi . This symmetry constraint should
ensure a relative reliability of translation pairs.
In each iteration of the bootstrapping process, we
may add all symmetric pairs from the pool of candi-
dates as new dimensions, or we could impose addi-
tional selection criteria that quantify the degree of
confidence in translation pairs. We are then able
to rank the symmetric candidate translation pairs in
the pool of candidates according to their confidence
scores (step 3 of alg. 1), and choose only the best
B candidates from the pool in each iteration (step 4)
as done in (Thelen and Riloff, 2002; McIntosh and
Curran, 2009; Huang and Riloff, 2012). By picking
only a subset of the B most confident candidates in
each iteration, we hope to further prevent a possibil-
ity of semantic drift, i.e., ?poisoning? the bootstrap-
ping process that might happen if we include incor-
rect translation pairs as dimensions of the space.
In this paper, we investigate 3 different confidence
estimation functions:3
(1) Absolute similarity score. Confidence of a
translation pair CF (wSi , TC(wSi )) is simply the ab-
solute similarity value sim(wSi , TC(wSi ))
(2) M-Best confidence function. It contrasts the
score of the translation candidate with the average
score over the first M most similar words in the
ranked list. The larger the difference, the more con-
fidence we have in the translation candidate. Given
a word wSi ? V S and a ranked list RLM (wSi ), the
3A symmetrized version of the confidence functions is com-
puted as the geometric mean of source-to-target and target-to-
source confidence scores.
1616
average score of the best M words is computed as:
simM (wSi ) =
1
M
?
wTj ?RLM (w
S
i )
sim(wSi , wTj )
The final confidence score is then:
CF (wSi , TC(wSi )) = sim(wSi , TC(wSi ))? simM (wSi )
(3) Entropy-based confidence function. We adapt
the well-known entropy-based confidence (Smith
and Eisner, 2007; Tu and Honavar, 2012) to this par-
ticular task. First, we need to define a distribution:
p(wTj |wSi ) =
esim(wSi ,wTj )
?
wTl ?V
T esim(wSi ,wTl )
The confidence function is then minus the entropy
of the probability distribution p:
CF (wSi , TC(wSi )) =
?
wTl ?V T
p(wTl |wSi ) log p(wTl |wSi )
3 Experimental Setup
Data collections. We investigate our bootstrapping
approach on the BLE task for 2 language pairs:
Spanish-English (ES-EN) and Italian-English (IT-
EN), and work with the following corpora previ-
ously used by Vulic? and Moens (2013): (i) a col-
lection of 13, 696 Spanish-English Wikipedia arti-
cle pairs (Wiki-ES-EN), (ii) 18, 898 Italian-English
Wikipedia article pairs (Wiki-IT-EN).4
Following (Koehn and Knight, 2002; Haghighi et
al., 2008; Prochasson and Fung, 2011; Vulic? and
Moens, 2013), we use TreeTagger (Schmid, 1994)
for POS-tagging and lemmatization of the corpora,
and then retain only nouns that occur at least 5 times
in the corpus. We record the lemmatized form when
available, and the original form otherwise. Our fi-
nal vocabularies consist of 9, 439 Spanish nouns and
4Vulic? and Moens (2013) also worked with Dutch-English
(NL-EN), but we have decided to leave out the results obtained
for that language pair due to space constraints, high similarity
between the two languages, and the fact that the results obtained
for that language pair are qualitatively similar to the results we
report for ES-EN and IT-EN. Hence including the results for
NL-EN would not contribute to the paper with any new impor-
tant insight and conclusion.
12, 945 nouns for ES-EN, and 7, 160 Italian nouns
and 9, 116 English nouns for IT-EN.
Ground truth. The goal of the BLE task is to ex-
tract a bilingual lexicon of one-to-one translations.
In order to test the quality of bilingual vector spaces
induced by our bootstrapping approach, we evaluate
it on standard 1000 ground truth one-to-one trans-
lation pairs built for the Wiki-ES-EN and Wiki-IT-
EN datasets (Vulic? and Moens, 2013). Note that
we do not explicitly test the bilingual vector space
as a bilingual lexicon, but rather its ability to find
semantically similar words and translations also for
words that are not used as dimensions of the space
(see sect. 2.1).
Evaluation metrics. We measure the performance
on the BLE task using a standard Top M accuracy
(AccM ) metric. It denotes the number of source
words wSi from ground truth translation pairs whose
list RLM (wSi ) contains the correct translation ac-
cording to our ground truth over the total number
of ground truth translation pairs (=1000) (Gaussier
et al, 2004; Tamura et al, 2012).5 Additionally,
we report the mean reciprocal rank (MRR) scores
(Voorhees, 1999) for some experimental runs.
Multilingual topic model. We utilize a straightfor-
ward multilingual extension of the standard Blei et
al.?s LDA model (Blei et al, 2003) called bilingual
LDA (Mimno et al, 2009; Ni et al, 2009; De Smet
and Moens, 2009). BiLDA training follows the pro-
cedure from (Vulic? and Moens, 2013), that is, the
training method is Gibbs sampling with the number
of topics set to K = 2000. Hyperparameters of the
model are set to standard values (Steyvers and Grif-
fiths, 2007): ? = 50/K and ? = 0.01.
Building initial seed lexicons. To produce the lists
of one-to-one translation pairs that are used as seeds
for the bootstrapping approach (see sect. 2.2), we
experiment with the TopicBC and the ResponseBC
methods from (Vulic? and Moens, 2013), which are
the MuPTM-based models of cross-lingual seman-
tic similarity that obtain the best results in the BLE
task on these datasets. In short, the TopicBC method
computes the similarity of two words according to
the similarity of their conditional topic distributions
(Griffiths et al, 2007; Vulic? et al, 2011) using
5We can build a one-to-one bilingual lexicon by harvesting
one-to-one translation pairs (wSi , TC(w
S
i )), and the quality of
that lexicon is best reflected in the Acc1 score.
1617
the Bhattacharyya coefficient (BC) (Kazama et al,
2010) as the similarity function. ResponseBC is a
second-order similarity method. It first computes
initial similarity scores between all words cross-
lingually and monolingually using the cross-lingual
topical space and, in the second step, it computes the
similarity between 2 words as the similarity between
their word vectors that now contain the initial word-
to-word similarity scores with all source and target
words. The similarity function is again BC.
We use these models of similarity as a black box
to acquire seeds for the bootstrapping approach, but
we encourage the interested reader to find more de-
tails about the methods in the relevant literature.
These two models also serve as our baseline models,
and our goal is to test whether we are able to obtain
bilingual lexicons of higher quality using bootstrap-
ping that starts from the output of these models.
Weighting and similarity functions. We have
experimented with different families of weighting
(e.g., PMI, LLR, TF-IDF, chi-square) and similar-
ity functions (e.g., cosine, Dice, Kullback-Leibler,
Jensen-Shannon) (Lee, 1999; Turney and Pantel,
2010). In this paper, we present results obtained
by positive pointwise mutual information (PPMI)
(Niwa and Nitta, 1994) as a weighting function,
which is a standard choice in vector space seman-
tics (Turney and Pantel, 2010), and (combined with
cosine) yields the best results over a group of seman-
tic tasks according to (Bullinaria and Levy, 2007).
We use a smoothed version of PPMI as presented
in (Pantel and Lin, 2002; Turney and Pantel, 2010).
Again, based on the results reported in the relevant
literature (Bullinaria and Levy, 2007; Laroche and
Langlais, 2010; Turney and Pantel, 2010), we opt
for the cosine similarity as a standard choice for SF .
We have also experimented with different window
sizes ranging from 3 to 15 in both directions around
the pivot word, but we have not detected any major
qualitative difference in the results and their inter-
pretation. Therefore, all results reported in the paper
are obtained by setting the window size to 6.
4 Results and Discussion
4.1 Are Seeds Important?
In recent work, Peirsman and Pado? (2010; 2011)
report that ?the size and quality of the (seed) lex-
icon are not of primary importance given that the
bootstrapping procedure effectively helped filter out
incorrect translation pairs and added more newly
identified mutual nearest neighbors.? According to
their findings, (1) noisy translation pairs are cor-
rected in later stages of the bootstrapping process,
since the quality of bilingual vector spaces gradu-
ally increases, (2) the size of the seed lexicon does
not matter since the bootstrapping approach is able
to learn translation pairs that were previously not
present in the seed lexicon. Additionally, they do not
provide any insight whether the frequency of seeds
in the corpus influences the quality of induced bilin-
gual vector spaces. In this paper, we question these
claims with a series of BLE experiments.
All experiments conducted in this section do not
rely on any extra confidence estimation except for
the symmetry constraint, that is, in each step we en-
rich the bilingual vector space with all new symmet-
ric translation pairs (see alg. 1 and sect. 2.3).
Exp. I: Same size, different seedings? The goal
of this experiment is to test whether the quality of
seeds plays an important role in the bootstrapping
approach. We experiment with 3 different seed lex-
icons: (1) Following (Peirsman and Pado?, 2010;
Fis?er and Ljubes?ic?, 2011), we harvest identically
spelled words across 2 languages and treat them
as one-to-one translations. This procedure results
in 459 seed translation pairs for ES-EN, and 431
pairs for IT-EN (SEED-ID), (2) We obtain symmet-
ric translation pairs using the TopicBC method (see
sect. 3) and use 459 pairs that have the highest fre-
quency in the Wiki-ES-EN corpus as seeds for ES-
EN (similarly 431 pairs for IT-EN) (SEED-TB), (3)
As in (2), but we now use the ResponseBC method to
acquire seeds (SEED-RB). The frequency of a one-
to-one translation pair is simply computed as the ge-
ometric mean of the frequencies of words that con-
stitute the translation pair.
Fig. 1(a) and 1(b) display the progress of the same
bootstrapping procedure using the 3 different seed
lexicons. We derive several interesting conclusions:
(i) Regardless of the actual choice of the seeding
method, the bootstrapping process proves its valid-
ity and utility since we observe that the quality of
induced bilingual vector spaces increases over time
for all 3 seeding methods. The bootstrapping proce-
dure converges quickly. The increase is especially
1618
0.2
0.4
0.6
0.8
Acc M
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
Acc1 (SEED-ID)Acc1 (SEED-TB)Acc1 (SEED-RB)Acc10 (SEED-ID)Acc10 (SEED-TB)Acc10 (SEED-RB)
(a) Spanish to English
0.2
0.4
0.6
0.8
Acc M
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
Acc1 (SEED-ID)Acc1 (SEED-TB)Acc1 (SEED-RB)Acc10 (SEED-ID)Acc10 (SEED-TB)Acc10 (SEED-RB)
(b) Italian to English
500
750
1000
1250
1500
1750
Num
ber
ofd
imen
sion
s
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
SEED-ID (ES-EN)SEED-TB (ES-EN)SEED-RB (ES-EN)SEED-ID (IT-EN)SEED-TB (IT-EN)SEED-RB (IT-EN)
(c) # Dimensions over iterations
Figure 1: Results with 3 different seeding methods as starting points of the bootstrapping process: (i) identical words
only (SEED-ID), (ii) the TopicBC method (SEED-TB), (iii) the ResponseBC method (SEED-RB). (a)AccM scores for
ES-EN; (b) AccM scores for IT-EN; (c) the number of dimensions in the space with the 3 different seeding methods
in each iteration for ES-EN and IT-EN. The bootstrapping procedure typically converges after a few iterations.
0
0.2
0.4
0.6
0.8
1
Acc
M
0 1 2 3 4 5 6 7 8 9 10
Iteration
Acc1 (HF-SEED)Acc1 (MF-SEED)Acc1 (LF-SEED)Acc10 (HF-SEED)Acc10 (MF-SEED)Acc10 (LF-SEED)
(a) Spanish to English
0
0.2
0.4
0.6
0.8
1
Acc
M
0 1 2 3 4 5 6 7 8 9 10
Iteration
Acc1 (HF-SEED)Acc1 (MF-SEED)Acc1 (LF-SEED)Acc10 (HF-SEED)Acc10 (MF-SEED)Acc10 (LF-SEED)
(b) Italian to English
Figure 2: Results on the BLE task with SEED-RB when using seed translation pairs of different frequency: (i) high-
frequency (HF-SEED), (ii) medium-frequency (MF-SEED), (iii) low-frequency (LF-SEED).
prominent in the first few iterations, when the ap-
proach learns more new dimensions (see fig. 1(c)).
(ii) The seeding method is important. A bootstrap-
ping approach that starts with a better seed lexicon
is able to extract bilingual lexicons of higher quality
as reflected in Acc1 scores. Although the bootstrap-
ping approach seems more beneficial when dealing
with noisier seed lexicons (226% increase in terms
of Acc1 for ES-EN and 177% increase for IT-EN
when starting with SEED-ID, compared to 35% in-
crease for ES-EN, and 15% for IT-EN with SEED-
RB), when starting from a noisy seed lexicon such
as SEED-ID the method is unable to reach the same
level of performance. Starting with SEED-ID, the
approach is able to recover noisy dimensions from
an initial bilingual vector space, but it is still unable
to match the results that are obtained when starting
from a better initial space (e.g., SEED-RB).
(iii) SEED-RB produces slightly better results than
SEED-TB (e.g., the final Acc1 of 0.649 for SEED-
RB compared to 0.626 for SEED-TB for IT-EN, and
0.572 compared to 0.553 for ES-EN). This finding is
in line with the results reported in (Vulic? and Moens,
2013) where ResponseBC proved to be a more ro-
bust and a more effective method when applied to
the BLE task directly. In all further experiments we
use ResponseBC to acquire seed pairs, i.e., the seed-
ing method is SEED-RB.
Exp. II: Does the frequency of seeds matter? In
the next experiment, we test whether the frequency
of seeds in the corpus plays an important role in
the bootstrapping process. The intuition is that by
using highly frequent and highly confident transla-
tion pairs the bootstrapping method has more reli-
able clues that help extract new dimensions in sub-
sequent iterations. On the other hand, low-frequency
1619
pairs (although potentially correct one-to-one trans-
lations) do not occur in the corpus and in the con-
texts of other words frequently enough, and are
therefore not sufficient to extract reliable new di-
mensions of the space.
To test the hypothesis, we again obtain all sym-
metric translation pairs using ResponseBC and then
sort them in descending order based on their fre-
quency in the corpus. In total, we retrieve a sorted
list of 2031 symmetric translation pairs for ES-EN,
and 1689 pairs for IT-EN. Following that, we split
the list in 3 parts of equal size: (i) the top third com-
prises translation pairs with the highest frequency in
the corpus (HF-SEED), (ii) the middle third com-
prises pairs of ?medium? frequency (MF-SEED),
(iii) the bottom third are low-frequency pairs (LF-
SEED). We then use these 3 different seed lexicons
of equal size to seed the bootstrapping approach.
Fig. 2(a) and 2(b) show the progress of the boot-
strapping process using these 3 seed lexicons. We
again observe several interesting phenomena:
(i) High-frequency seed translation pairs are better
seeds, and that finding is in line with our hypothesis.
Although the bootstrapping approach again displays
a positive trend regardless of the actual choice of
seeds (we observe an increase even when using LF-
SEED), high-frequency seeds lead to better overall
results in the BLE task. Besides its high presence in
contexts of other words, another advantage of high-
frequency seed pairs is the fact that an initial sim-
ilarity method will typically acquire more reliable
translation candidates for such words (Pekar et al,
2006). For instance, 89.5% of ES-EN pairs in HF-
SEED are correct one-to-one translations, compared
to 65.1% in MF-SEED, and 44.3% in LF-SEED.
(ii) The difference in results between HF-SEED and
MF-SEED is more visible in Acc1 scores. Although
both seed lexicons for all test words provide ranked
lists which contain words that exhibit some semantic
relation to the given word, the reliability and the fre-
quency of translation pairs are especially important
for detecting the relation of cross-lingual word syn-
onymy, that is, the translational equivalence that is
exploited in building one-to-one bilingual lexicons.
Exp. III: Does size matter? The following exper-
iment investigates whether bilingual vector spaces
may be effectively bootstrapped from small high-
quality seed lexicons, and if larger seed lexicons
necessarily lead to bilingual vector spaces of higher
quality as reflected in BLE results. We again retrieve
a sorted list of symmetric translation pairs as in Exp.
II. Following that, we build seed lexicons of vari-
ous sizes by retaining only the first N pairs from
the list, where we vary N from 200 to 1400 in steps
of 200. We also use the entire sorted list as a seed
lexicon (All), and compare the results on the BLE
task with the results obtained by applying the Re-
sponseBC and TopicBC methods directly (Vulic? and
Moens, 2013). The results are summarized in tables
1 and 2. We observe the following:
(i) If we provide a seed lexicon with sufficient en-
tries, the bootstrapping procedure provides compa-
rable results regardless of the seed lexicon size, al-
though results tend to be higher for larger seed lex-
icons (e.g., compare results when starting with 600
and 1200 lexicon entries). When starting with the
size of 600, the bootstrapping approach is able to
find dimensions that were already in the seed lexi-
con of size 1200. The consequence is that, although
bootstrapping with a smaller seed lexicon displays a
slower start (see the difference in results at iteration
0), the performances level after convergence.
(ii) Regardless of the seed lexicon size, the boot-
strapping approach is valuable. It consistently im-
proves the quality of the induced bilingual vector
space, and consequently, the quality of bilingual lex-
icons extracted using that vector space. The positive
impact is more prominent for smaller seed lexicons,
i.e., we observe an increase of 78% for ES-EN when
starting with only 200 seed pairs, compared to an
increase of 15% when starting with 800 seed pairs,
and 10% when starting with 1400 seed pairs.
(iii) The bootstrapping approach outperforms Re-
sponseBC and TopicBC in terms of Acc1 and MRR
scores for both language pairs when the seed lexi-
con provides a sufficient number of entries. How-
ever, in terms of Acc10, TopicBC and ResponseBC
still exhibit comparable (for IT-EN) or even better
(ES-EN) results. Both TopicBC and ResponseBC
are MuPTM-based methods that, due to MuPTM
properties, model the similarity of two words at the
level of documents as contexts, while the bootstrap-
ping approach is a window-based approach that nar-
rows down the context to a local neighborhood of a
word. The MuPTM-based models are better suited
to detect a general topical similarity of words, and
1620
Iteration: 0 2 5 10
Seed lexicon Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
200(?1617) 0.274 0.352 0.525 0.446 0.534 0.713 0.481 0.569 0.753 0.488 0.576 0.752
400(?1563) 0.416 0.499 0.663 0.518 0.602 0.774 0.542 0.620 0.787 0.545 0.625 0.788
600(?1554) 0.459 0.539 0.707 0.550 0.630 0.787 0.573 0.650 0.803 0.578 0.654 0.802
800(?1582) 0.494 0.572 0.728 0.548 0.631 0.799 0.563 0.644 0.802 0.567 0.646 0.806
1000(?1636) 0.516 0.591 0.744 0.563 0.644 0.805 0.578 0.656 0.813 0.581 0.658 0.817
1200(?1740) 0.536 0.613 0.764 0.586 0.661 0.804 0.588 0.664 0.812 0.591 0.667 0.814
1400(?1888) 0.536 0.620 0.776 0.583 0.659 0.808 0.589 0.666 0.815 0.588 0.666 0.818
All-2031(?2437) 0.543 0.625 0.785 0.589 0.667 0.813 0.597 0.675 0.818 0.599 0.677 0.820
TopicBC 0.433 0.576 0.843 ? ? ? ? ? ? ? ? ?
ResponseBC 0.517 0.635 0.891 ? ? ? ? ? ? ? ? ?
Table 1: ES-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number
of dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB.
Iteration: 0 2 5 10
Seed lexicon Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
200(?1255) 0.394 0.469 0.703 0.515 0.595 0.757 0.548 0.621 0.782 0.555 0.628 0.787
400(?1265) 0.546 0.618 0.757 0.623 0.690 0.831 0.639 0.704 0.840 0.644 0.709 0.844
600(?1309) 0.585 0.657 0.798 0.653 0.718 0.856 0.664 0.726 0.859 0.672 0.734 0.862
800(?1365) 0.602 0.672 0.813 0.657 0.723 0.857 0.663 0.726 0.865 0.665 0.730 0.867
1000(?1416) 0.616 0.688 0.828 0.629 0.706 0.853 0.636 0.709 0.857 0.642 0.714 0.861
1200(?1581) 0.628 0.700 0.840 0.655 0.724 0.869 0.664 0.733 0.877 0.668 0.736 0.883
1400(?1749) 0.626 0.701 0.851 0.654 0.727 0.867 0.656 0.728 0.867 0.661 0.733 0.874
All-1689(?2008) 0.616 0.695 0.850 0.643 0.716 0.860 0.653 0.724 0.862 0.654 0.726 0.866
TopicBC 0.578 0.667 0.834 ? ? ? ? ? ? ? ? ?
ResponseBC 0.622 0.729 0.882 ? ? ? ? ? ? ? ? ?
Table 2: IT-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number of
dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB.
are therefore not always able to push the real cross-
lingual synonyms higher in the ranked list of seman-
tically similar words, while the window-based boot-
strapping approach is better tailored to model the
relation of cross-lingual synonymy, i.e., to extract
one-to-one translation pairs (as reflected in Acc1
scores). A similar conclusion for monolingual set-
tings is drawn by Baroni and Lenci (2010).
(iv) Since our bootstrapping approach utilizes Re-
sponseBC or TopicBC as a preprocessing step, it is
obvious that the approach leads to an increased com-
plexity. On top of the initial complexity of Respon-
seBC and TopicBC, the bootstrapping method re-
quires |V S ||V T | comparisons at each iteration, but
given the fact that each wSi ? V S may be processed
independently of any other wSj ? V S in each itera-
tion, the bootstrapping method is trivially paralleliz-
able. That makes the method computationally fea-
sible even for vocabularies larger than the ones re-
ported in the paper.
4.2 Is Confidence Estimation Important?
According to the results from tables 1 and 2, re-
gardless of the seed lexicon size, the bootstrapping
approach does not suffer from semantic drift, i.e.,
if we seed the process with high-quality symmetric
translation pairs, it is able to recover more pairs and
add them as new dimensions of the bilingual vector
space. However, we also study the influence of ap-
plying different confidence estimation functions on
top of the symmetry constraint (see sect 2.3), but we
do not observe any improvement in the BLE results,
regardless of the actual choice of a confidence esti-
mation function. The only observed phenomenon,
as illustrated by fig. 3, is the slower convergence
rate when setting the parameter B to lower values.
The symmetry constraint alone seems to be sufficient
to prevent semantic drift, but it might also be a too
strong and a too conservative assumption, since only
a small portion of all possible translation pairs is
used to span the bilingual vector space (for instance,
1621
0.4
0.45
0.5
0.55
0.6
Ac
c 1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Iteration
B = 30B = 50B = 100B = 150B = 200B = All
Figure 3: The effect of learning rate B on bootstrapping.
Language pair: ES-EN, seed lexicon: SEED-RB with
600 pairs, confidence function: symmetrized M-Best.
when starting with 600 entries for ES-EN, the final
bilingual vector space consists of only 1554 pairs,
while the total number of ES nouns is 9439). One
line of future work will address the construction of
bootstrapping algorithms that also enable the usage
of highly reliable asymmetric pairs as dimensions,
and the confidence estimation functions might have
a more important role in that setting.
5 Conclusion
We have presented a new bootstrapping approach to
inducing bilingual vector spaces from non-parallel
data, and have shown the utility of the induced space
in the BLE task. The approach is fully corpus-based
and, unlike previous work, it does not rely on the
availability of machine-readable translation dictio-
naries or predefined concept categories. We have
systematically described, analyzed and evaluated all
key components of the bootstrapping approach. Re-
sults reveal that, contrary to conclusions from prior
work, the initialization of the bilingual vector space
is especially important. We have presented a novel
approach to initializing the bootstrapping procedure,
and have shown that better results in the BLE task
are obtained by starting from seed lexicons that com-
prise highly reliable high-frequent translation pairs.
The bootstrapping framework presented in the pa-
per is completely language pair independent, which
makes it effectively applicable to any language pair.
In future work, we will investigate other models
of similarity besides TopicBC and ResponseBC (e.g,
the method from (Haghighi et al, 2008)) that could
be used as preliminary models for constructing an
initial bilingual vector space. Furthermore, we plan
to study other confidence functions and explore if
asymmetric translation candidates could also con-
tribute to the bootstrapping method. Finally, we also
plan to test the robustness of our fully corpus-based
bootstrapping approach by porting it to more lan-
guage pairs.
Acknowledgments
We would like to thank the anonymous reviewers for
their useful suggestions. This research has been car-
ried out in the framework of the TermWise Knowl-
edge Platform (IOF-KP/09/001) funded by the In-
dustrial Research Fund, KU Leuven, Belgium.
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?
721.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of UAI, pages 75?82.
John A. Bullinaria and Joseph P. Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39(3):510?526.
Sung-Hyuk Cha. 2007. Comprehensive survey on
distance/similarity measures between probability den-
sity functions. International Journal of Mathematical
Models and Methods in Applied Sciences, 1(4):300?
307.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, pages 600?609.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of ACL-HLT, pages 407?
412.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the Web using in-
terlingual topic modeling. In Proceedings of the CIKM
2009 Workshop on Social Web Search and Mining,
pages 57?64.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings
of EMNLP-CoNLL, pages 1?11.
1622
Darja Fis?er and Nikola Ljubes?ic?. 2011. Bilingual lexicon
extraction from comparable corpora for closely related
languages. In Proceedings of RANLP, pages 125?131.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING, pages 414?420.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of ACL, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proceedings
of EACL, pages 286?295.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora using
in-domain terms. In Proceedings of COLING, pages
481?489.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of ACL, pages 247?256.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL Workshop on Unsupervised Lexi-
cal Acquisition, pages 9?16.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Proceedings of NAACL-HLT, pages 618?
626.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
COLING, pages 617?625.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceedings
of SIGIR, pages 175?182.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of ACL, pages 25?32.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management, 41(3):523?547.
Tara McIntosh and James R. Curran. 2009. Reducing se-
mantic drift with bagging and distributional similarity.
In Proceedings of ACL, pages 396?404.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of EMNLP,
pages 880?889.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of WWW, pages 1155?1156.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-occurrence
vectors from corpora vs. distance vectors from dictio-
naries. In Proceedings of COLING, pages 304?309.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD, pages 613?
619.
Yves Peirsman and Sebastian Pado?. 2010. Cross-
lingual induction of selectional preferences with bilin-
gual vector spaces. In Proceedings of NAACL-HLT,
pages 921?929.
Yves Peirsman and Sebastian Pado?. 2011. Semantic re-
lations in bilingual lexicons. ACM Transactions on
Speech and Language Processing, 8(2):article 3.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247?266.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of ACL, pages 1327?1335.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Ellen Riloff and Jessica Shepherd. 1999. A corpus-based
bootstrapping algorithm for semi-automated semantic
lexicon construction. Natural Language Engineering,
5(2):147?156.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual lex-
icon generation using non-aligned signatures. In Pro-
ceedings of ACL, pages 98?107.
1623
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors.
In Proceedings of EMNLP-CoNLL, pages 667?677.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013a. Token and type
constraints for cross-lingual part-of-speech tagging.
Transactions of ACL, 1:1?12.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.
2013b. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of NAACL-HLT,
pages 1061?1071.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of
EMNLP-CoNLL, pages 24?36.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of EMNLP,
pages 214?221.
Kewei Tu and Vasant Honavar. 2012. Unambiguity reg-
ularization for unsupervised learning of probabilistic
grammars. In Proceedings of EMNLP-CoNLL, pages
1324?1334.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
Journal of Artifical Intelligence Research, 37(1):141?
188.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In Proceedings of ACL-HLT, pages
299?304.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC, pages 77?
82.
Ivan Vulic? and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
NAACL-HLT, pages 106?116.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from comparable
corpora using latent topic models. In Proceedings of
ACL-HLT, pages 479?484.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL, pages 200?207.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceedings
of ACL, pages 1128?1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of ACL, pages 55?
63.
1624
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 349?362,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Probabilistic Models of Cross-Lingual Semantic Similarity in Context
Based on Latent Cross-Lingual Concepts Induced from Comparable Data
Ivan Vuli
?
c and Marie-Francine Moens
Department of Computer Science
KU Leuven, Belgium
{ivan.vulic|marie-francine.moens}@cs.kuleuven.be
Abstract
We propose the first probabilistic approach
to modeling cross-lingual semantic sim-
ilarity (CLSS) in context which requires
only comparable data. The approach re-
lies on an idea of projecting words and
sets of words into a shared latent semantic
space spanned by language-pair indepen-
dent latent semantic concepts (e.g., cross-
lingual topics obtained by a multilingual
topic model). These latent cross-lingual
concepts are induced from a comparable
corpus without any additional lexical re-
sources. Word meaning is represented as
a probability distribution over the latent
concepts, and a change in meaning is rep-
resented as a change in the distribution
over these latent concepts. We present new
models that modulate the isolated out-of-
context word representations with contex-
tual knowledge. Results on the task of
suggesting word translations in context for
3 language pairs reveal the utility of the
proposed contextualized models of cross-
lingual semantic similarity.
1 Introduction
Cross-lingual semantic similarity (CLSS) is a met-
ric that measures to which extent words (or more
generally, text units) describe similar semantic
concepts and convey similar meanings across lan-
guages. Models of cross-lingual similarity are typ-
ically used to automatically induce bilingual lexi-
cons and have found numerous applications in in-
formation retrieval (IR), statistical machine trans-
lation (SMT) and other natural language process-
ing (NLP) tasks. Within the IR framework, the
output of the CLSS models is a key resource in
the models of dictionary-based cross-lingual in-
formation retrieval (Ballesteros and Croft, 1997;
Lavrenko et al., 2002; Levow et al., 2005; Wang
and Oard, 2006) or may be utilized in query ex-
pansion in cross-lingual IR models (Adriani and
van Rijsbergen, 1999; Vuli?c et al., 2013). These
CLSS models may also be utilized as an addi-
tional source of knowledge in SMT systems (Och
and Ney, 2003; Wu et al., 2008). Additionally,
the models are a crucial component in the cross-
lingual tasks involving a sort of cross-lingual
knowledge transfer, where the knowledge about
utterances in one language may be transferred to
another. The utility of the transfer or annotation
projection by means of bilingual lexicons obtained
from the CLSS models has already been proven
in various tasks such as semantic role labeling
(Pad?o and Lapata, 2009; van der Plas et al., 2011),
parsing (Zhao et al., 2009; Durrett et al., 2012;
T?ackstr?om et al., 2013b), POS tagging (Yarowsky
and Ngai, 2001; Das and Petrov, 2011; T?ackstr?om
et al., 2013a; Ganchev and Das, 2013), verb clas-
sification (Merlo et al., 2002), inducing selectional
preferences (Peirsman and Pad?o, 2010), named
entity recognition (Kim et al., 2012), named en-
tity segmentation (Ganchev and Das, 2013), etc.
The models of cross-lingual semantic similar-
ity from parallel corpora rely on word alignment
models (Brown et al., 1993; Och and Ney, 2003),
but due to a relative scarceness of parallel texts for
many language pairs and domains, the models of
cross-lingual similarity from comparable corpora
have gained much attention recently.
All these models from parallel and compara-
ble corpora provide ranked lists of semantically
similar words in the target language in isolation
or invariably, that is, they do not explicitly iden-
349
tify and encode different senses of words. In
practice, it means that, given the sentence ?The
coach of his team was not satisfied with the game
yesterday.?, these context-insensitive models of
similarity are not able to detect that the Spanish
word entrenador is more similar to the polyse-
mous word coach in the context of this sentence
than the Spanish word autocar, although auto-
car is listed as the most semantically similar word
to coach globally/invariably without any observed
context. In another example, while Spanish words
partido, encuentro, cerilla or correspondencia are
all highly similar to the ambiguous English word
match when observed in isolation, given the Span-
ish sentence ?She was unable to find a match in
her pocket to light up a cigarette.?, it is clear that
the strength of semantic similarity should change
in context as only cerilla exhibits a strong seman-
tic similarity to match within this particular sen-
tential context.
Following this intuition, in this paper we inves-
tigate models of cross-lingual semantic similarity
in context. The context-sensitive models of sim-
ilarity target to re-rank the lists of semantically
similar words based on the co-occurring contexts
of words. Unlike prior work (e.g., (Ng et al., 2003;
Prior et al., 2011; Apidianaki, 2011)), we explore
these models in a particularly difficult and min-
imalist setting that builds only on co-occurrence
counts and latent cross-lingual semantic concepts
induced directly from comparable corpora, and
which does not rely on any other resource (e.g.,
machine-readable dictionaries, parallel corpora,
explicit ontology and category knowledge). In
that respect, the work reported in this paper ex-
tends the current research on purely statistical
data-driven distributional models of cross-lingual
semantic similarity that are built upon the idea
of latent cross-lingual concepts (Haghighi et al.,
2008; Daum?e III and Jagarlamudi, 2011; Vuli?c et
al., 2011; Vuli?c and Moens, 2013) induced from
non-parallel data. While all the previous mod-
els in this framework are context-insensitive mod-
els of semantic similarity, we demonstrate how to
build context-aware models of semantic similarity
within the same probabilistic framework which re-
lies on the same shared set of latent concepts.
The main contributions of this paper are:
? We present a new probabilistic approach to
modeling cross-lingual semantic similarity in
context based on latent cross-lingual seman-
tic concepts induced from non-parallel data.
? We show how to use the models of cross-
lingual semantic similarity in the task of sug-
gesting word translations in context.
? We provide results for three language
pairs which demonstrate that contextualized
models of similarity significantly outscore
context-insensitive models.
2 Towards Cross-Lingual Semantic
Similarity in Context
Latent Cross-Lingual Concepts. Latent cross-
lingual concepts/senses may be interpreted as
language-independent semantic concepts present
in a multilingual corpus (e.g., document-aligned
Wikipedia articles in English, Spanish and Dutch)
that have their language-specific representations in
different languages. For instance, having a multi-
lingual collection in English, Spanish and Dutch,
and then discovering a latent semantic concept
on Soccer, that concept would be represented by
words (actually probabilities over words P (w|z
k
),
where w denotes a word, and z
k
denotes k-th
latent concept): {player, goal, coach, . . .} in
English, bal?on (ball), futbolista (soccer player),
equipo (team), . . . } in Spanish, and {wedstrijd
(match), elftal (soccer team), doelpunt (goal), . . .}
in Dutch. Given a multilingual corpus C, the goal
is to learn and extract a set Z of K latent cross-
lingual concepts {z
1
, . . . , z
K
} that optimally de-
scribe the observed data, that is, the multilingual
corpus C. Extracting cross-lingual concepts ac-
tually implies learning per-document concept dis-
tributions for each document in the corpus, and
discovering language-specific representations of
these concepts given by per-concept word distri-
butions in each language.
Z = {z
1
, . . . , z
K
} represents the set of K la-
tent cross-lingual concepts present in the multilin-
gual corpus. These K semantic concepts actually
span a latent cross-lingual semantic space. Each
word w, irrespective of its actual language, may
be represented in that latent semantic space as a
K-dimensional vector, where each vector compo-
nent is a conditional concept score P (z
k
|w).
A number of models may be employed to in-
duce the latent concepts. For instance, one could
use cross-lingual Latent Semantic Indexing (Du-
mais et al., 1996), probabilistic Principal Compo-
nent Analysis (Tipping and Bishop, 1999), or a
probabilistic interpretation of non-negative matrix
350
factorization (Lee and Seung, 1999; Gaussier and
Goutte, 2005; Ding et al., 2008) on concatenated
documents in aligned document pairs. Other more
recent models include matching canonical correla-
tion analysis (Haghighi et al., 2008; Daum?e III and
Jagarlamudi, 2011) and multilingual probabilistic
topic models (Ni et al., 2009; De Smet and Moens,
2009; Mimno et al., 2009; Boyd-Graber and Blei,
2009; Zhang et al., 2010; Fukumasu et al., 2012).
Due to its inherent language pair indepen-
dent nature and state-of-the-art performance in the
tasks such as bilingual lexicon extraction (Vuli?c et
al., 2011) and cross-lingual information retrieval
(Vuli?c et al., 2013), the description in this pa-
per relies on the multilingual probabilistic topic
modeling (MuPTM) framework. We draw a di-
rect parallel between latent cross-lingual concepts
and latent cross-lingual topics, and we present
the framework from the MuPTM perspective, but
the proposed framework is generic and allows the
usage of all other models that are able to com-
pute probability scores P (z
k
|w). These scores in
MuPTM are induced from their output language-
specific per-topic word distributions. The mul-
tilingual probabilistic topic models output prob-
ability scores P (w
S
i
|z
k
) and P (w
T
j
|z
k
) for each
w
S
i
? V
S
and w
T
j
? V
T
and each z
k
?
Z , and it holds
?
w
S
i
?V
S
P (w
S
i
|z
k
) = 1 and
?
w
T
j
?V
T
P (w
T
j
|z
k
) = 1. The scores are then
used to compute scores P (z
k
|w
S
i
) and P (z
k
|w
T
j
)
in order to represent words from the two different
languages in the same latent semantic space in a
uniform way.
Context-Insensitive Models of Similarity. With-
out observing any context, the standard models of
semantic word similarity that rely on the seman-
tic space spanned by latent cross-lingual concepts
in both monolingual (Dinu and Lapata, 2010a;
Dinu and Lapata, 2010b) and multilingual set-
tings (Vuli?c et al., 2011) typically proceed in the
following manner. Latent language-independent
concepts (e.g., cross-lingual topics or latent word
senses) are estimated on a large corpus. The
K-dimensional vector representation of the word
w
S
1
? V
S
is:
vec(w
S
1
) = [P (z
1
|w
S
1
), . . . , P (z
K
|w
S
1
)] (1)
Similarly, we are able to represent any target lan-
guage word w
T
2
in the same latent semantic space
by aK-dimensional vector with scores P (z
k
|w
T
2
).
Each word regardless of its language is repre-
sented as a distribution over K latent concepts.
The similarity between w
S
1
and some word w
T
2
?
V
T
is then computed as the similarity between
their K-dimensional vector representations using
some of the standard similarity measures (e.g.,
the Kullback-Leibler or the Jensen-Shannon diver-
gence, the cosine measure). These methods use
only global co-occurrence statistics from the train-
ing set and do not take into account any contex-
tual information. They provide only out-of-context
word representations and are therefore able to de-
liver only context-insensitive models of similarity.
Defining Context. Given an occurrence of a
word w
S
1
, we build its context set Con(w
S
1
) =
{cw
S
1
, . . . , cw
S
r
} that comprises r words from V
S
that co-occur with w
S
1
in a defined contextual
scope or granularity. In this work we do not in-
vestigate the influence of the context scope (e.g.,
document-based, paragraph-based, window-based
contexts). Following the recent work from Huang
et al. (2012) in the monolingual setting, we
limit the contextual scope to the sentential context.
However, we emphasize that the proposed models
are designed to be fully functional regardless of
the actual chosen context granularity. e.g., when
operating in the sentential context, Con(w
S
1
) con-
sists of words occurring in the same sentence with
the particular instance of w
S
1
. Following Mitchell
and Lapata (2008), for the sake of simplicity, we
impose the bag-of-words assumption, and do not
take into account the order of words in the context
set as well as context words? dependency relations
to w
S
1
. Investigating different context types (e.g.,
dependency-based) is a subject of future work.
By using all words occurring with w
S
1
in a con-
text set (e.g., a sentence) to build the setCon(w
S
1
),
we do not make any distinction between ?infor-
mative and ?uninformative? context words. How-
ever, some context words bear more contextual in-
formation about the observed word w
S
1
and are
stronger indicators of the correct word meaning in
that particular context. For instance, in the sen-
tence ?The coach of his team was not satisfied
with the game yesterday?, words game and team
are strong clues that coach should be translated
as entrenador while the context word yesterday
does not bring any extra contextual information
that could resolve the ambiguity.
Therefore, in the final context set Con(w
S
1
) it
is useful to retain only the context words that re-
351
ally bring extra semantic information. We achieve
that by exploiting the same latent semantic space
to provide the similarity score between the ob-
served word w
S
1
and each word cw
S
i
, i = 1, . . . , r
from its context set Con(w
S
1
). Each word cw
S
i
may be represented by its vector vec(cw
S
i
) (see eq.
(1)) in the same latent semantic space, and there
we can compute the similarity between its vec-
tor and vec(w
S
1
). We can then sort the similarity
scores for each cw
S
i
and retain only the top scoring
M context words in the final set Con(w
S
1
). The
procedure of context sorting and pruning should
improve the semantic cohesion between w
S
1
and
its context since only informative context features
are now present in Con(w
S
1
), and we reduce the
noise coming from uninformative contextual fea-
tures that are not semantically related tow
S
1
. Other
options for the context sorting and pruning are
possible, but the main goal in this paper is to il-
lustrate the core utility of the procedure.
3 Cross-Lingual Semantic Similarity in
Context via Latent Concepts
Representing Context. The probabilistic frame-
work that is supported by latent cross-lingual con-
cepts allows for having the K-dimensional vector
representations in the same latent semantic space
spanned by cross-lingual topics for: (1) Single
words regardless of their actual language, and (2)
Sets that comprise multiple words. Therefore, we
are able to project the observed source word, all
target words, and the context set of the observed
source word to the same latent semantic space
spanned by latent cross-lingual concepts.
Eq. (1) shows how to represent single words in
the latent semantic space. Now, we present a way
to address compositionality, that is, we show how
to build the same representations in the same latent
semantic space beyond the word level. We need to
compute a conditional concept distribution for the
context set Con(w
S
1
), that is, we have to compute
the probability scores P (z
k
|Con(w
S
1
)) for each
z
k
? Z . Remember that the context Con(w
S
1
)
is actually a set of r (or M after pruning) words
Con(w
S
1
) = {cw
S
1
, . . . , cw
S
r
}. Under the single-
topic assumption (Griffiths et al., 2007) and fol-
lowing Bayes? rule, it holds:
P (z
k
|Con(w
S
1
)) =
P (Con(w
S
1
)|z
k
)P (z
k
)
P (Con(w
S
1
))
=
P (cw
S
1
, . . . , cw
S
r
|z
k
)P (z
k
)
?
K
l=1
P (cw
S
1
, . . . , cw
S
r
|z
l
)P (z
l
)
(2)
=
?
r
j=1
P (cw
S
j
|z
k
)P (z
k
)
?
K
l=1
?
r
j=1
P (cw
S
j
|z
l
)P (z
l
)
(3)
Note that here we use a simplification where we
assume that all cw
S
j
? Con(w
S
1
) are condition-
ally independent given z
k
. The assumption of the
conditional independence of unigrams is a stan-
dard heuristic applied in bag-of-words model in
NLP and IR (e.g., one may observe a direct anal-
ogy to probabilistic language models for IR where
the assumption of independence of query words
is imposed (Ponte and Croft, 1998; Hiemstra,
1998; Lavrenko and Croft, 2001)), but we have
to forewarn the reader that in general the equa-
tion P (cw
S
1
, . . . , cw
S
r
|z
k
) =
?
r
j=1
P (cw
S
j
|z
k
) is
not exact. However, by adopting the conditional
independence assumption, in case of the uniform
topic prior P (z
k
) (i.e., we assume that we do not
posses any prior knowledge about the importance
of latent cross-lingual concepts in a multilingual
corpus), eq. (3) may be further simplified:
P (z
k
|Con(w
S
1
)) ?
?
r
j=1
P (cw
S
j
|z
k
)
?
K
l=1
?
r
j=1
P (cw
S
j
|z
l
)
(4)
The representation of the context set in the latent
semantic space is then:
vec(Con(w
S
1
)) = [P (z
1
|Con(w
S
1
)), . . . , P (z
K
|Con(w
S
1
))]
We can then compute the similarity between
words and sets of words given in the same latent
semantic space in a uniform way, irrespective of
their actual language. We use all these properties
when building our context-sensitive CLSS mod-
els.
One remark: As a by-product of our modeling
approach, by this procedure for computing repre-
sentations for sets of words, we have in fact paved
the way towards compositional cross-lingual mod-
els of similarity which rely on latent cross-lingual
concepts. Similar to compositional models in
monolingual settings (Mitchell and Lapata, 2010;
Rudolph and Giesbrecht, 2010; Baroni and Zam-
parelli, 2010; Socher et al., 2011; Grefenstette
and Sadrzadeh, 2011; Blacoe and Lapata, 2012;
Clarke, 2012; Socher et al., 2012) and multilingual
settings (Hermann and Blunsom, 2014; Ko?cisk?y
et al., 2014), the representation of a set of words
(e.g., a phrase or a sentence) is exactly the same
as the representation of a single word; it is simply
a K-dimensional real-valued vector. Our work on
inducing structured representations of words and
352
text units beyond words is similar to (Klemen-
tiev et al., 2012; Hermann and Blunsom, 2014;
Ko?cisk?y et al., 2014), but unlike them, we do not
need high-quality sentence-aligned parallel data to
induce bilingual text representations. Moreover,
this work on compositionality in multilingual set-
tings is only preliminary (e.g., we treat phrases and
sentences as bags-of-words), and in future work
we will aim to include syntactic information in the
composition models as already done in monolin-
gual settings (Socher et al., 2012; Hermann and
Blunsom, 2013).
Intuition behind the Approach. Going back to
our novel CLSS models in context, these models
rely on the representations of words and their con-
texts in the same latent semantic space spanned by
latent cross-lingual concepts/topics. The models
differ in the way the contextual knowledge is fused
with the out-of-context word representations.
The key idea behind these models is to repre-
sent a word w
S
1
in the latent semantic space as a
distribution over the latent cross-lingual concepts,
but now with an additional modulation of the rep-
resentation after taking its local context into ac-
count. The modulated word representation in the
semantic space spanned by K latent cross-lingual
concepts is then:
vec(w
S
1
, Con(w
S
1
)) = [P
?
(z
1
|w
S
1
), . . . , P
?
(z
K
|w
S
1
)] (5)
where P
?
(z
K
|w
S
1
) denotes the recalculated (or
modulated) probability score for the conditional
concept/topic distribution ofw
S
1
after observing its
context Con(w
S
1
). For an illustration of the key
idea, see fig. 1. The intuition is that the context
helps to disambiguate the true meaning of the oc-
currence of the word w
S
1
. In other words, after
observing the context of the word w
S
1
, fewer latent
cross-lingual concepts will share most of the prob-
ability mass in the modulated context-aware word
representation.
Model I: Direct-Fusion. The first approach
makes the conditional distribution over latent se-
mantic concepts directly dependent on both word
w
S
1
and its context Con(w
S
1
). The probability
score P
?
(z
k
|w
S
1
) from eq. (5) for each z
k
? Z is
then given as P
?
(z
k
|w
S
1
) = P (z
k
|w
S
1
, Con(w
S
1
)).
We have to estimate the probability
P (z
k
|w
S
1
, Con(w
S
1
)), that is, the probability that
word w
S
1
is assigned to the latent concept/topic z
k
given its context Con(w
S
1
):
P (z
k
|w
S
1
, Con(w
S
1
)) =
P (z
k
, w
S
1
)P (Con(w
S
1
)|z
k
)
?
K
l=1
P (z
l
, w
S
1
)P (Con(w
S
1
)|z
l
)
(6)
Since P (z
k
, w
S
1
) = P (w
S
1
|z
k
)P (z
k
), if we closely
follow the derivation from eq. (3) which shows
how to project context into the latent semantic
space (and again assume the uniform topic prior
P (z
k
)), we finally obtain the following formula:
P
?
(z
k
|w
S
1
) ?
P (w
S
1
|z
k
)
?
r
j=1
P (cw
S
j
|z
k
)
?
K
l=1
P (w
S
1
|z
l
)
?
r
j=1
P (cw
S
j
|z
l
)
(7)
The ranking of all words w
T
2
? V
T
according to
their similarity to w
S
1
may be computed by detect-
ing the similarity score between their representa-
tion in the K-dimensional latent semantic space
and the modulated source word representation as
given by eq. (5) and eq. (7) using any of the ex-
isting similarity functions (Lee, 1999; Cha, 2007).
The similarity score Sim(w
S
1
, w
T
2
, Con(w
S
1
)) be-
tween some w
T
2
? V
T
represented by its vector
vec(w
T
2
) and the observed word w
S
1
given its con-
text Con(w
S
1
) is computed as:
sim(w
S
1
, w
T
2
, Con(w
S
1
))
= SF
(
vec
(
w
S
1
, Con(w
S
1
)
)
, vec
(
w
T
2
))
(8)
where SF denotes a similarity function. Words
are then ranked according to their respective sim-
ilarity scores and the best scoring candidate may
be selected as the best translation of an oc-
currence of the word w
S
1
given its local con-
text. Since the contextual knowledge is inte-
grated directly into the estimation of probability
P (z
k
|w
S
1
, Con(w
S
1
)), we name this context-aware
CLSS model the Direct-Fusion model.
Model II: Smoothed-Fusion. The next model
follows the modeling paradigm established within
the framework of language modeling (LM), where
the idea is to ?back off? to a lower order N-
gram in case we do not possess any evidence
about a higher-order N-gram (Jurafsky and Mar-
tin, 2000). The idea now is to smooth the repre-
sentation of a word in the latent semantic space
induced only by the words in its local context
with the out-of-context type-based representation
of that word induced directly from a large training
corpus. In other words, the modulated probability
score P
?
(z
k
|w
S
1
) from eq. (5) is calculated as:
P
?
(z
k
|w
S
1
) = ?
1
P (z
k
|Con(w
S
1
)) + (1? ?
1
)P (z
k
|w
S
1
) (9)
where ?
1
is the interpolation parameter, P (z
k
|w
S
1
)
is the out-of-context conditional concept probabil-
ity score as in eq. (1), and P (z
k
|Con(w
S
1
)) is
given by eq. (3). This model compromises be-
tween the pure contextual word representation and
353
z3
z
2
z
1
coach
(in isolation)
entrenador
autocar
z
3
z
2
z
1coach
(contextualized)
entrenador
autocarThe coach of his team was not
satisfied with the game yesterday.
K
coach
K
coach
CONTEXT-INSENSITIVE CONTEXT-SENSITIVE
Figure 1: An illustrative toy example of the main intuitions in our probabilistic framework for building
context sensitive models with only three latent cross-lingual concepts (axes z
1
, z
2
and z
3
): A change
in meaning is reflected as a change in a probability distribution over latent cross-lingual concepts that
span a shared latent semantic space. A change in the probability distribution may then actually steer an
English word coach towards its correct (Spanish) meaning in context.
the out-of-context word representation. In cases
when the local context of word w
S
1
is informa-
tive enough, the factor P (z
k
|Con(w
S
1
)) is suffi-
cient to provide the ranking of terms in V
T
, that
is, to detect words that are semantically similar to
w
S
1
based on its context. However, if the context is
not reliable, we have to smooth the pure context-
based representation with the out-of-context word
representation (the factor P (z
k
|w
S
1
)). We call this
model the Smoothed-Fusion model.
The ranking of words w
T
2
? V
T
then finally
proceeds in the same manner as in Direct-Fusion
following eq. (8), but now using eq. (9) for the
modulated probability scores P
?
(z
k
|w
S
1
).
Model III: Late-Fusion. The last model is con-
ceptually similar to Smoothed-Fusion, but it per-
forms smoothing at a later stage. It proceeds in
two steps: (1) Given a target word w
T
2
? V
T
, the
model computes similarity scores separately be-
tween (i) the context set Con(w
S
1
) and w
T
2
, and
(ii) the word w
S
1
in isolation and w
T
2
(again, on the
type level); (2) It linearly combines the obtained
similarity scores. More formally, we may write:
Sim(w
S
1
, w
T
2
, Con(w
S
1
))
= ?
2
SF
(
vec
(
Con(w
S
1
)
)
, vec
(
w
T
2
))
+ (1? ?
2
)SF
(
vec
(
w
S
1
)
, vec
(
w
T
2
))
(10)
where ?
2
is the interpolation parameter. Since
this model computes the similarity with each tar-
get word separately for the source word in isola-
tion and its local context, and combines the ob-
tained similarity scores after the computations,
this model is called Late-Fusion.
4 Experimental Setup
Evaluation Task: Suggesting Word Transla-
tions in Context. Given an occurrence of a pol-
ysemous word w
S
1
? V
S
in the source language
L
S
with vocabulary V
S
, the task is to choose the
correct translation in the target language L
T
of
that particular occurrence of w
S
1
from the given
set T = {t
T
1
, . . . , t
T
q
}, T ? V
T
, of its q possible
translations/meanings (i.e., its translation or sense
inventory). The task of suggesting a word trans-
lation in context may be interpreted as ranking the
q translations with respect to the observed local
context Con(w
S
1
) of the occurrence of the word
w
S
1
. The best scoring translation candidate in the
ranked list is then the suggested correct translation
for that particular occurrence of w
S
1
after observ-
ing its local context Con(w
S
1
).
Training Data. We use the following corpora for
inducing latent cross-lingual concepts/topics, i.e.,
for training our multilingual topic model: (i) a col-
lection of 13, 696 Spanish-English Wikipedia arti-
cle pairs (Wiki-ES-EN), (ii) a collection of 18, 898
Italian-English Wikipedia article pairs, (iii) a col-
lection of 7, 612 Dutch-English Wikipedia arti-
cle pairs (Wiki-NL-EN), and (iv) the Wiki-NL-
EN corpus augmented with 6,206 Dutch-English
document pairs from Europarl (Koehn, 2005)
(Wiki+EP-NL-EN). The corpora were previously
used in (Vuli?c and Moens, 2013). No explicit use
is made of sentence-level alignments in Europarl.
354
Sentence in Italian Correct Translation (EN)
1. I primi calci furono prodotti in legno ma recentemente... stock
2. In caso di osteoporosi si verifica un eccesso di rilascio di calcio dallo scheletro... calcium
3. La crescita del calcio femminile professionistico ha visto il lancio di competizioni... football
4. Il calcio di questa pistola (Beretta Modello 21a, calibro .25) ha le guancette in materiale... stock
Table 1: Example sentences from our IT evaluation dataset with corresponding correct translations.
Spanish Italian Dutch
Ambiguous word Ambiguous word Ambiguous word
(Possible senses/translations) (Possible senses/translations) (Possible senses/translations)
1. estaci?on 1. raggio 1. toren
(station; season) (ray; radius; spoke) (rook; tower)
2. ensayo 2. accordo 2. beeld
(essay; rehearsal; trial) (chord; agreement) (image; statue)
3. n?ucleo 3. moto 3. blade
(core; kernel; nucleus) (motion; motorcycle) (blade; leaf; magazine)
4. vela 4. calcio 4.fusie
(sail; candle) (calcium; football; stock) (fusion; merger)
5. escudo 5. terra 5. stam
(escudo; escutcheon; shield) (earth; land) (stem; trunk; tribe)
6. papa 6. tavola 6. koper
(Pope; potato) (board; panel; table) (copper; buyer)
7. cola 7. campione 7. bloem
(glue; coke; tail; queue) (champion; sample) (flower; flour)
8. cometa 8. carta 8. spanning
(comet; kite) (card; paper; map) (voltage; tension; stress)
9. disco 9. piano 9. noot
(disco; discus; disk) (floor; plane; plan; piano) (note; nut)
10. banda 10. disco 10. akkoord
(band; gang; strip) (disco; discus; disk) (chord; agreement)
11. cinta 11. istruzione 11. munt
(ribbon; tape) (education; instruction) (coin; currency; mint)
12. banco 12. gabinetto 12. pool
(bank; bench; shoal) (cabinet; office; toilet) (pole; pool)
13. frente 13. torre 13. band
(forehead; front) (rook; tower) (band; tyre; tape)
14. fuga 14. campo 14. kern
(escape; fugue; leak) (camp; field) (core; kernel; nucleus)
15. gota 15. gomma 15. kop
(gout; drop) (rubber; gum; tyre) (cup; head)
Table 2: Sets of 15 ambiguous words in Spanish, Italian and Dutch from our test set accompanied by the
sets of their respective possible senses/translations in English.
All corpora are theme-aligned comparable cor-
pora, i.e, the aligned document pairs discuss sim-
ilar themes, but are in general not direct trans-
lations (except for Europarl). By training on
Wiki+EP-NL-EN we want to test how the training
corpus of higher quality affects the estimation of
latent cross-lingual concepts that span the shared
latent semantic space and, consequently, the over-
all results in the task of suggesting word transla-
tions in context. Following prior work (Koehn and
Knight, 2002; Haghighi et al., 2008; Prochasson
and Fung, 2011; Vuli?c and Moens, 2013), we re-
tain only nouns that occur at least 5 times in the
corpus. We record lemmatized word forms when
available, and original forms otherwise. We use
TreeTagger (Schmid, 1994) for POS tagging and
lemmatization.
Test Data. We have constructed test datasets in
Spanish (ES), Italian (IT) and Dutch (NL), where
the aim is to find their correct translation in En-
glish (EN) given the sentential context. We have
selected 15 polysemous nouns (see tab. 2 for
the list of nouns along with their possible transla-
tions) in each of the 3 languages, and have man-
ually extracted 24 sentences (not present in the
training data) for each noun that capture different
meanings of the noun from Wikipedia. In order
to construct datasets that are balanced across dif-
ferent possible translations of a noun, in case of
q different translation candidates in T for some
word w
S
1
, the dataset contains exactly 24/q sen-
tences for each translation from T . In total, we
have designed 360 sentences for each language
355
pair (ES/IT/NL-EN), 1080 sentences in total.
1
. We
have used 5 extra nouns with 20 sentences each as
a development set to tune the parameters of our
models. As a by-product, we have built an initial
repository of ES/IT/NL ambiguous words. Tab.
1 presents a small sample from the IT evaluation
dataset, and illustrates the task of suggesting word
translations in context.
Evaluation Procedure. Our task is to present
the system a list of possible translations and let
the system decide a single most likely translation
given the word and its sentential context. Ground
truth thus contains one word, that is, one correct
translation for each sentence from the evaluation
dataset. We have manually annotated the correct
translation for the ground truth
1
by inspecting the
discourse in Wikipedia articles and the interlingual
Wikipedia links. We measure the performance of
all models as Top 1 accuracy (Acc
1
) (Gaussier et
al., 2004; Tamura et al., 2012). It denotes the num-
ber of word instances from the evaluation dataset
whose top proposed candidate in the ranked list of
translation candidates from T is exactly the cor-
rect translation for that word instance as given by
ground truth over the total number of test word in-
stances (360 in each test dataset).
Parameters. We have tuned ?
1
and ?
2
on the de-
velopment sets. We set ?
1
= ?
2
= 0.9 for all
language pairs. We use sorted context sets (see
sect. 2) and perform a cut-off at M = 3 most de-
scriptive context words in the sorted context sets
for all models. In the following section we discuss
the utility of this context sorting and pruning, as
well as its influence on the overall results.
Inducing Latent Cross-Lingual Concepts. Our
context-aware models are generic and allow ex-
perimentations with different models that induce
latent cross-lingual semantic concepts. However,
in this particular work we present results obtained
by a multilingual probabilistic topic model called
bilingual LDA (Mimno et al., 2009; Ni et al.,
2009; De Smet and Moens, 2009). The BiLDA
model is a straightforward multilingual extension
of the standard LDA model (Blei et al., 2003).
For the details regarding the modeling, generative
story and training of the bilingual LDA model, we
refer the interested reader to the aforementioned
relevant literature.
We have used the Gibbs sampling procedure
1
Available at http://people.cs.kuleuven.be/
?ivan.vulic/software/
(Geman and Geman, 1984) tailored for BiLDA
in particular for training and have experimented
with different number of topics K in the interval
300? 2500. Here, we present only the results ob-
tained withK = 2000 for all language pairs which
also yielded the best or near-optimal performance
in (Dinu and Lapata, 2010b; Vuli?c et al., 2011).
Other parameters of the model are set to the typical
values according to Steyvers and Griffiths (2007):
? = 50/K and ? = 0.01.
2
Models in Comparison. We test the performance
of our Direct-Fusion, Smoothed-Fusion and Late-
Fusion models, and compare their results with
the context-insensitive CLSS models described in
sect. 2 (No-Context). We provide results with
two different similarity functions: (1) We have
tested different SF-s (e.g., the Kullback-Leibler
and the Jensen-Shannon divergence, the cosine
measure) on the K-dimensional vector represen-
tations, and have detected that in general the best
scores are obtained with the Bhattacharyya coef-
ficient (BC) (Cha, 2007; Kazama et al., 2010),
(2) Another similarity method we use is the so-
called Cue method (Griffiths et al., 2007; Vuli?c
et al., 2011), which models the probability that
a target word t
T
i
will be generated as an as-
sociation response given some cue source word
w
S
1
. In short, the method computes the score
P (t
T
i
|w
S
1
) = P (t
T
i
|z
k
)P (z
k
|w
S
1
). We can use
the scores P (t
T
i
|w
S
1
) obtained by inputting out-of-
context probability scores P (z
k
|w
S
1
) or modulated
probability scores P
?
(z
k
|w
S
1
) to produce the rank-
ing of translation candidates.
5 Results and Discussion
The performance of all the models in comparison
is displayed in tab. 3. These results lead us to
several conclusions:
(i) All proposed context-sensitive CLSS models
suggesting word translations in context signifi-
cantly outperform context-insensitive CLSS mod-
els, which are able to produce only word trans-
lations in isolation. The improvements in re-
sults when taking context into account are ob-
2
We are well aware that different hyper-parameter set-
tings (Asuncion et al., 2009; Lu et al., 2011), might have
influence on the quality of learned latent cross-lingual con-
cepts/topics and, consequently, the quality of latent semantic
space, but that analysis is not the focus of this work. Addi-
tionally, we perform semantic space pruning (Reisinger and
Mooney, 2010; Vuli?c and Moens, 2013). All computations
are performed over the best scoring 100 cross-lingual topics
according to their respective scores P (z
k
|w
S
i
) similarly to
(Vuli?c and Moens, 2013).
356
Direction: ES?EN IT?EN NL?EN (Wiki) NL?EN (Wiki+EP)
Model
Acc
1
Acc
1
Acc
1
Acc
1
Acc
1
Acc
1
Acc
1
Acc
1
(SF=BC) (SF=Cue) (SF=BC) (SF=Cue) (SF=BC) (SF=Cue) (SF=BC) (SF=Cue)
No-Context .406 .406 .408 .408 .433 .433 .433 .433
Direct-Fusion .617 .575 .714 .697 .603 .592 .606 .636
Smoothed-Fusion .664 .703* .731 .789* .669 .712* .692 .761*
Late-Fusion .675 .667 .742 .728 .667 .644 .683 .722
Table 3: Results on the 3 evaluation datasets. Translation direction is ES/IT/NL?EN. The improvements
of all contextualized models over non-contextualized models are statistically significant according to a
chi-square statistical significance test (p<0.05). The asterisk (*) denotes significant improvements of
Smoothed-Fusion over Late-Fusion using the same significance test.
served for all 3 language pairs. The large im-
provements in the results (i.e., we observe an aver-
age relative increase of 51.6% for the BC+Direct-
Fusion combination, 64.3% for BC+Smoothed-
Fusion, 64.9% for BC+Late-Fusion, 49.1% for
Cue+Direct-Fusion, 76.7% for Cue+Smoothed-
Fusion, and 64.5% for Cue+Late-Fusion) confirm
that the local context of a word is essential in ac-
quiring correct word translations for polysemous
words, as isolated non-contextualized word repre-
sentations are not sufficient.
(ii) The choice of a similarity function influences
the results. On average, the Cue method as SF out-
performs other standard similarity functions (e.g.,
Kullback-Leibler, Jensen-Shannon, cosine, BC) in
this evaluation task. However, it is again impor-
tant to state that regardless of the actual choice
of SF, context-aware models that modulate out-of-
context word representations using the knowledge
of local context outscore context-insensitive mod-
els that utilize non-modulated out-of-context rep-
resentations (with all other parameters equal).
(iii) The Direct-Fusion model, conceptually sim-
ilar to a model of word similarity in context in
monolingual settings (Dinu and Lapata, 2010a),
is outperformed by the other two context-sensitive
models. In Direct-Fusion, the observed word and
its context are modeled in the same fashion, that is,
the model does not distinguish between the word
and its surrounding context when it computes the
modulated probability scores P
?
(z
k
|w
S
1
) (see eq.
(7)). Unlike Direct-Fusion, the modeling assump-
tions of Smoothed-Fusion and Late-Fusion pro-
vide a clear distinction between the observed word
w
S
1
and its context Con(w
S
1
) and combine the out-
of-context representation of w
S
1
and its contextual
knowledge into a smoothed LM-inspired proba-
bilistic model. As the results reveal, that strategy
leads to better overall scores. The best scores in
general are obtained by Smoothed-Fusion, but it
is also outperformed by Late-Fusion in several ex-
perimental runs where BC was used as SF. How-
ever, the difference in results between Smoothed-
Fusion and Late-Fusion in these experimental runs
is not statistically significant according to a chi-
squared significance test (p < 0.05).
(iv) The results for Dutch-English are influenced
by the quality of training data. The performance
of our models of similarity is higher for models
that rely on latent-cross lingual topics estimated
from the data of higher quality (i.e., compare the
results when trained on Wiki and Wiki+EP in tab.
3). The overall quality of our models of similarity
is of course dependent on the quality of the latent
cross-lingual topics estimated from training data,
and the quality of these latent cross-lingual con-
cepts is further dependent on the quality of multi-
lingual training data. This finding is in line with
a similar finding reported for the task of bilingual
lexicon extraction (Vuli?c and Moens, 2013).
(v) Although Dutch is regarded as more similar
to English than Italian or Spanish, we do not ob-
serve any major increase in the results on both
test datasets for the English-Dutch language pair
compared to English-Spanish/Italian. That phe-
nomenon may be attributed to the difference in
size and quality of our training Wikipedia datasets.
Moreover, while the probabilistic framework pro-
posed in this chapter is completely language pair
agnostic as it does not make any language pair
dependent modeling assumptions, we acknowl-
edge the fact that all three language pairs com-
prise languages coming from the same phylum,
that is, the Indo-European language family. Future
extensions of our probabilistic modeling frame-
work also include porting the framework to other
more distant language pairs that do not share the
same roots nor the same alphabet (e.g., English-
Chinese/Hindi).
Analysis of Context Sorting and Pruning. We
357
0.55
0.6
0.65
0.7
0.75
0.8
Acc
1
1 2 3 4 5 6 7 8 9 10 11 All
Size of the ranked context
ES-EN
IT-EN
NL-EN (Wiki)
NL-EN (Wiki+EP)
Figure 2: The influence of the size of sorted con-
text on the accuracy of word translation in context.
The model is Cue+Smoothed-Fusion.
also investigate the utility of context sorting and
pruning, and its influence on the overall results
in our evaluation task. Therefore, we have con-
ducted experiments with sorted context sets that
were pruned at different positions, ranging from 1
(only the most similar word to w
S
1
in a sentence is
included in the context set Con(w
S
1
)) to All (all
words occurring in a same sentence with w
S
1
are
included in Con(w
S
1
)). The monolingual similar-
ity between w
S
1
and each potential context word in
a sentence has been computed using BC on their
out-of-context representations in the latent seman-
tic space spanned by cross-lingual topics. Fig. 2
shows how the size of the sorted context influences
the overall results. The presented results have been
obtained by the Cue+Smoothed-Fusion combina-
tion, but a similar behavior is observed when em-
ploying other combinations.
Fig. 2 clearly indicates the importance of con-
text sorting and pruning. The procedure ensures
that only the most semantically similar words in a
given scope (e.g., a sentence) influence the choice
of a correct meaning. In other words, closely
semantically similar words in the same sentence
are more reliable indicators for the most probable
word meaning. They are more informative in mod-
ulating the out-of-context word representations in
context-sensitive similarity models. We observe
large improvements in scores when we retain only
the top M semantically similar words in the con-
text set (e.g., when M=5, the scores are 0.694,
0.758, 0.717, and 0.767 for ES-EN, IT-EN, NL-
EN (Wiki) and NL-EN (Wiki+EP), respectively;
while the same scores are 0.572, 0.703, 0.639 and
0.672 when M=All).
6 Conclusions and Future Work
We have proposed a new probabilistic approach to
modeling cross-lingual semantic similarity in con-
text, which relies only on co-occurrence counts
and latent cross-lingual concepts which can be es-
timated using only comparable data. The approach
is purely statistical and it does not make any ad-
ditional language-pair dependent assumptions; it
does not rely on a bilingual lexicon, orthographic
clues or predefined ontology/category knowledge,
and it does not require parallel data.
The key idea in the approach is to represent
words, regardless of their actual language, as dis-
tributions over the latent concepts, and both out-
of-context and contextualized word representa-
tions are then presented in the same latent space
spanned by the latent semantic concepts. A
change in word meaning after observing its con-
text is reflected in a change of its distribution
over the latent concepts. Results for three lan-
guage pairs have clearly shown the importance
of the newly developed modulated or ?contextual-
ized? word representations in the task of suggest-
ing word translations in context.
We believe that the proposed framework is only
a start, as it ignites a series of new research ques-
tions and perspectives. One may further exam-
ine the influence of context scope (e.g., document-
based vs. sentence-based vs. window-based con-
texts), as well as context selection and aggregation
(see sect. 2) on the contextualized models. For
instance, similar to the model from
?
O S?eaghdha
and Korhonen (2011) in the monolingual setting,
one may try to introduce dependency-based con-
texts (Pad?o and Lapata, 2007) and incorporate
the syntax-based knowledge in the context-aware
CLSS modeling. It is also worth studying other
models that induce latent semantic concepts from
multilingual data (see sect. 2) within this frame-
work of context-sensitive CLSS modeling. One
may also investigate a similar approach to context-
sensitive CLSS modeling that could operate with
explicitly defined concept categories (Gabrilovich
and Markovitch, 2007; Cimiano et al., 2009; Has-
san and Mihalcea, 2009; Hassan and Mihalcea,
2011; McCrae et al., 2013).
Acknowledgments
We would like to thank the anonymous review-
ers for their comments and suggestions. This re-
search has been carried out in the framework of the
Smart Computer-Aided Translation Environment
(SCATE) project (IWT-SBO 130041).
358
References
Mirna Adriani and C. J. van Rijsbergen. 1999. Term
similarity-based query expansion for cross-language
information retrieval. In Proceedings of the 3rd Eu-
ropean Conference on Research and Advanced Tech-
nology for Digital Libraries (ECDL), pages 311?
322.
Marianna Apidianaki. 2011. Unsupervised cross-
lingual lexical substitution. In Proceedings of the 1st
Workshop on Unsupervised Learning in NLP, pages
13?23.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of the 25th Confer-
ence on Uncertainty in Artificial Intelligence (UAI),
pages 27?34.
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In Proceed-
ings of the 20th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 84?91.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1183?1193.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 546?556.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Mul-
tilingual topic models for unaligned text. In Pro-
ceedings of the 25th Conference on Uncertainty in
Artificial Intelligence (UAI), pages 75?82.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. International Journal of Mathe-
matical Models and Methods in Applied Sciences,
1(4):300?307.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information
retrieval. In Proceedings of the 21st International
Joint Conference on Artifical Intelligence (IJCAI),
pages 1513?1518.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 600?609.
Hal Daum?e III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 407?412.
Wim De Smet and Marie-Francine Moens. 2009.
Cross-language linking of news stories on the Web
using interlingual topic modeling. In Proceedings of
the CIKM 2009 Workshop on Social Web Search and
Mining (SWSM@CIKM), pages 57?64.
Chris H. Q. Ding, Tao Li, and Wei Peng. 2008. On
the equivalence between non-negative matrix fac-
torization and probabilistic latent semantic index-
ing. Computational Statistics & Data Analysis,
52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1162?1172.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING), pages 250?
258.
Susan T. Dumais, Thomas K. Landauer, and Michael
Littman. 1996. Automatic cross-linguistic infor-
mation retrieval using Latent Semantic Indexing.
In Proceedings of the SIGIR Workshop on Cross-
Linguistic Information Retrieval, pages 16?23.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11.
Kosuke Fukumasu, Koji Eguchi, and Eric P. Xing.
2012. Symmetric correspondence topic models for
multilingual text analysis. In Procedings of the 25th
Annual Conference on Advances in Neural Informa-
tion Processing Systems (NIPS), pages 1295?1303.
359
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1606?1611.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1996?2006.
?
Eric Gaussier and Cyril Goutte. 2005. Relation be-
tween PLSA and NMF and implications. In Pro-
ceedings of the 28th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 601?602.
?
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv?e D?ejean. 2004. A geomet-
ric view on bilingual lexicon extraction from com-
parable corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 526?533.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6(6):721?741.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1394?1404.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211?244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-HLT), pages 771?779.
Samer Hassan and Rada Mihalcea. 2009. Cross-
lingual semantic relatedness using encyclopedic
knowledge. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1192?1201.
Samer Hassan and Rada Mihalcea. 2011. Semantic
relatedness using salient semantic analysis. In Pro-
ceedings of the 25th AAAI Conference on Artificial
Intelligence (AAAI), pages 884?889.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 894?904.
Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual models for compositional distributed se-
mantics. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 58?68.
Djoerd Hiemstra. 1998. A linguistically motivated
probabilistic model of information retrieval. In Pro-
ceedings of the 2nd European Conference on Re-
search and Advanced Technology for Digital Li-
braries (ECDL), pages 569?584.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 873?882.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall PTR.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
Bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 247?256.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from Wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
694?702.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. In Proceedings of the 24th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 1459?1474.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL Workshop on Unsupervised
Lexical Acquisition (ULA), pages 9?16.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit (MT SUMMIT),
pages 79?86.
Tom?a?s Ko?cisk?y, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 224?229.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120?127.
360
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 175?182.
Daniel D. Lee and H. Sebastian Seung. 1999. Al-
gorithms for non-negative matrix factorization. In
Proceedings of the 12th Conference on Advances
in Neural Information Processing Systems (NIPS),
pages 556?562.
Lillian Lee. 1999. Measures of distributional sim-
ilarity. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 25?32.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management, 41(3):523?547.
Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: An empirical study of PLSA and LDA. In-
formation Retrieval, 14(2):178?203.
John Philip McCrae, Philipp Cimiano, and Roman
Klinger. 2013. Orthonormal explicit topic analysis
for cross-lingual document matching. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1732?1740.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 207?214.
David Mimno, Hanna Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 880?889.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 455?462.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In Proceedings of the 18th International World Wide
Web Conference (WWW), pages 1155?1156.
Diarmuid
?
O S?eaghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1047?1057.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Proceedings of the 11th
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 921?929.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 275?281.
Anat Prior, Shuly Wintner, Brian MacWhinney, and
Alon Lavie. 2011. Translation ambiguity in and
out of context. Applied Psycholinguistics, 32(1):93?
111.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned compara-
ble documents. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 1327?1335.
Joseph Reisinger and Raymond J. Mooney. 2010.
A mixture model with sharing for lexical seman-
tics. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1173?1182.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
907?916.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
the 24th Annual Conference on Advances in Neural
361
Information Processing Systems (NIPS), pages 801?
809.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1201?1211.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analy-
sis, 427(7):424?440.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013a. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of ACL, 1:1?12.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013b. Target language adaptation of discriminative
transfer parsers. In Proceedings of the 14th Meeting
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1061?1071.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 24?36.
Michael E. Tipping and Christopher M. Bishop. 1999.
Mixtures of probabilistic principal component anal-
ysers. Neural Computation, 11(2):443?482.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL-HLT), pages 299?304.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 14th Meeting of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT), pages
106?116.
Ivan Vuli?c, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 479?484.
Ivan Vuli?c, Wim De Smet, and Marie-Francine Moens.
2013. Cross-language information retrieval models
based on latent topic models trained with document-
aligned comparable corpora. Information Retrieval,
16(3):331?368.
Jianqiang Wang and Douglas W. Oard. 2006. Com-
bining bidirectional translation and synonymy for
cross-language information retrieval. In Proceed-
ings of the 29th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 202?209.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING),
pages 993?1000.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pages 200?207.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.
2010. Cross-lingual latent topic extraction. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1128?1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 55?63.
362
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336?344,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Skip N-grams and Ranking Functions for Predicting Script Events
Bram Jans
KU Leuven
Leuven, Belgium
bram.jans@gmail.com
Steven Bethard
University of Colorado Boulder
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ivan Vulic?
KU Leuven
Leuven, Belgium
ivan.vulic@cs.kuleuven.be
Marie Francine Moens
KU Leuven
Leuven, Belgium
sien.moens@cs.kuleuven.be
Abstract
In this paper, we extend current state-of-the-
art research on unsupervised acquisition of
scripts, that is, stereotypical and frequently
observed sequences of events. We design,
evaluate and compare different methods for
constructing models for script event predic-
tion: given a partial chain of events in a
script, predict other events that are likely
to belong to the script. Our work aims
to answer key questions about how best
to (1) identify representative event chains
from a source text, (2) gather statistics from
the event chains, and (3) choose ranking
functions for predicting new script events.
We make several contributions, introducing
skip-grams for collecting event statistics, de-
signing improved methods for ranking event
predictions, defining a more reliable evalu-
ation metric for measuring predictiveness,
and providing a systematic analysis of the
various event prediction models.
1 Introduction
There has been recent interest in automatically ac-
quiring world knowledge in the form of scripts
(Schank and Abelson, 1977), that is, frequently
recurring situations that have a stereotypical se-
quence of events, such as a visit to a restaurant.
All of the techniques so far proposed for this task
share a common sub-task: given an event or partial
chain of events, predict other events that belong
to the same script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Chambers and Ju-
rafsky, 2011; Manshadi et al 2008; McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010; Regneri
et al 2010). Such a model can then serve as input
to a system that identifies the order of the events
within that script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009) or that generates
a story using the selected events (McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010).
In this article, we analyze and compare tech-
niques for constructing models that, given a partial
chain of events, predict other events that belong to
the script. In particular, we consider the following
questions:
? How should representative chains of events
be selected from the source text?
? Given an event chain, how should statistics
be gathered from it?
? Given event n-gram statistics, which ranking
function best predicts the events for a script?
In the process of answering these questions, this
article makes several contributions to the field of
script and narrative event chain understanding:
? We explore for the first time the use of skip-
grams for collecting narrative event statistics,
and show that this approach performs better
than classic n-gram statistics.
? We propose a new method for ranking events
given a partial script, and show that it per-
forms substantially better than ranking meth-
ods from prior work.
? We propose a new evaluation procedure (us-
ing Recall@N) for the cloze test, and advo-
cate its usage instead of average rank used
previously in the literature.
? We provide a systematic analysis of the in-
teractions between the choices made when
constructing an event prediction model.
336
Section 2 gives an overview of the prior work
related to this task. Section 3 lists and briefly de-
scribes different approaches that try to provide
answers to the three questions posed in this intro-
duction, while Section 4 presents the results of our
experiments and reports on our findings. Finally,
Section 5 provides a conclusive discussion along
with ideas for future work.
2 Prior Work
Our work is primarily inspired by the work of
Chambers and Jurafsky, which combined a depen-
dency parser with coreference resolution to col-
lect event script statistics and predict script events
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). For each document in their training
corpus, they used coreference resolution to iden-
tify all the entities, and a dependency parser to
identify all verbs that had an entity as either a sub-
ject or object. They defined an event as a verb plus
a dependency type (either subject or object), and
collected for each entity, the chain of events that
it participated in. They then calculated pointwise
mutual information (PMI) statistics over all the
pairs of events that occurred in the event chains in
their corpus. To predict a new script event given
a partial chain of events, they selected the event
with the highest sum of PMIs with all the events
in the partial chain.
The work of McIntyre and Lapata followed in
this same paradigm, (McIntyre and Lapata, 2009;
McIntyre and Lapata, 2010), collecting chains of
events by looking at entities and the sequence of
verbs for which they were a subject or object. They
also calculated statistics over the collected event
chains, though they considered both event bigram
and event trigram counts. Rather than predicting
an event for a script however, they used these sim-
ple counts to predict the next event that should be
generated for a children?s story.
Manshadi and colleagues were concerned about
the scalability of running parsers and coreference
over a large collection of story blogs, and so used
a simplified version of event chains ? just the main
verb of each sentence (Manshadi et al 2008).
Rather than rely on an ad-hoc summation of PMIs,
they apply language modeling techniques (specifi-
cally, a smoothed 5-gram model) over the sequence
of events in the collected chains. However, they
only tested these language models on sequencing
tasks (e.g. is the real sequence better than a ran-
dom sequence?) rather than on prediction tasks
(e.g. which event should follow these events?).
In the current article, we attempt to shed some
light on these previous works by comparing differ-
ent ways of collecting and using event chains.
3 Methods
Models that predict script events typically have
three stages. First, a large corpus is processed to
find event chains in each of the documents. Next,
statistics over these event chains are gathered and
stored. Finally, the gathered statistics are used to
create a model that takes as input a partial script
and produces as output a ranked list of events for
that script. The following sections give more de-
tails about each of these stages and identify the
decisions that must be made in each step, and an
overview of the whole process with an example
source text is displayed in Figure 1.
3.1 Identifying Event Chains
Event chains are typically defined as a sequence
of actions performed by some actor. Formally, an
event chain C for some actor a, is a partially or-
dered set of events (v, d) where each v is a verb
that has the actor a as its dependency d. Following
prior work (Chambers and Jurafsky, 2008; Cham-
bers and Jurafsky, 2009; McIntyre and Lapata,
2009; McIntyre and Lapata, 2010), these event
chains are identified by running a coreference sys-
tem and a dependency parser. Then for each en-
tity identified by the coreference system, all verbs
that have a mention of that entity as one of their
dependencies are collected1. The event chain is
then the sequence of (verb, dependency-type) tu-
ples. For example, given the sentence A Crow
was sitting on a branch of a tree when a Fox ob-
served her, the event chain for the Crow would be
(sitting, SUBJECT), (observed, OBJECT).
Once event chains have been identified, the most
appropriate event chains for training the model
must be selected. The goal of this process is to
select the subset of the event chains identified by
the coreference system and the dependency parser
that look to be the most reliable. Both the coref-
erence system and the dependency parser make
some errors, so not all event chains are necessarily
useful for training a model. The three strategies
we consider for this selection process are:
1Also following prior work, we consider only the depen-
dencies subject and object.
337
John woke up. He opened his eyes and yawned. Then he crossed the room and walked to the door.There he saw Mary. Mary smiled and kissed him. Then they both blushed.JOHN(woke, SUBJ)(opened, SUBJ)(yawned, SUBJ)(crossed, SUBJ)(walked, SUBJ)(saw, SUBJ)(kissed, OBJ)(blushed, SUBJ) MARY(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)(blushed, SUBJ)all chains, long chains,the longest chain all chains 1. Identifying event chains... [(saw, OBJ), (smiled, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(smiled, SUBJ), (blushed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(saw, OBJ), (blushed, SUBJ)]...[(kissed, SUBJ), (blushed, SUBJ)]regular bigrams 2-skip bigrams1-skip bigrams 2. Gathering event chain statistics(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)_________ (missing event)constructing a partial script (cloze test)1. (looked, OBJ)2. (gave, SUBJ)3. (saw, SUBJ)... 1. (kissed, OBJ)2. (looked, OBJ)3. (waited, SUBJ)... 1. (blushed, SUBJ)2. (kissed, OBJ)3. (smiled, SUBJ)C&J PMIOrdered PMIBigram prob. 3. Predicting script events
Figure 1: An overview of the whole linear work flow showing the three key steps ? identifying event chains,
collecting statistics out of the chains and predicting a missing event in a script. The figure also displays how a
partial script for evaluation (Section 4.3) is constructed. We show the whole process for Mary?s event chain only,
but the same steps are followed for John?s event chain.
? Select all event chains, that is, all sequences
of two or more events linked by common
actors. This strategy will produce the largest
number of event chains to train a model from,
but it may produce noisier training data as
the very short chains included by this strategy
may be less likely to represent real scripts.
? Select all long event chains consisting of 5
or more events. This strategy will produce a
smaller number of event chains, but as they
are longer, they may be more likely to repre-
sent scripts.
? Select only the longest event chain. This
strategy will produce the smallest number of
event chains from a corpus. However, they
may be of higher quality, since this strategy
looks for the key actor in each story, and only
uses the events that are tied together by that
key actor. Since this is the single actor that
played the largest role in the story, its actions
may be the most likely to represent a real
script.
3.2 Gathering Event Chain Statistics
Once event chains have been collected from the
corpus, the statistics necessary for constructing
the event prediction model must be gathered. Fol-
lowing prior work (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Manshadi et al
2008; McIntyre and Lapata, 2009; McIntyre and
Lapata, 2010), we focus on gathering statistics
about the n-grams of events that occur in the
collected event chains. Specifically, we look at
strategies for collecting bigram statistics, the most
common type of statistics gathered in prior work.
We consider three strategies for collecting bigram
statistics:
? Regular bigrams. We find all pairs of
events that are adjacent in an event chain
and collect the number of times each event
pair was observed. For example, given the
chain of events (saw, SUBJ), (kissed, OBJ),
(blushed, SUBJ), we would extract the two
event bigrams: ((saw, SUBJ), (kissed, OBJ))
338
and ((kissed, OBJ), (blushed, SUBJ)). In addi-
tion to the event pair counts, we also collect
the number of times each event was observed
individually, to allow for various conditional
probability calculations. This strategy fol-
lows the classic approach for most language
models.
? 1-skip bigrams. We collect pairs of events
that occur with 0 or 1 events intervening be-
tween them. For example, given the chain
(saw, SUBJ), (kissed, OBJ), (blushed, SUBJ),
we would extract three bigrams: the two regu-
lar bigrams ((saw, SUBJ), (kissed, OBJ)) and
((kissed, OBJ), (blushed, SUBJ)), plus the 1-
skip-bigram, ((saw, SUBJ), (blushed, SUBJ)).
This approach to collecting n-gram statistics
is sometimes called skip-gram modeling, and
it can reduce data sparsity by extracting more
event pairs per chain (Guthrie et al 2006).
It has not previously been applied in the task
of predicting script events, but it may be
quite appropriate to this task because in most
scripts it is possible to skip some events in
the sequence.
? 2-skip bigrams. We collect pairs of events
that occur with 0, 1 or 2 intervening events,
similar to what was done in the 1-skip bi-
grams strategy. This will extract even more
pairs of events from each chain, but it is pos-
sible the statistics over these pairs of events
will be noisier.
3.3 Predicting Script Events
Once statistics over event chains have been col-
lected, it is possible to construct the model for
predicting script events. The input of this model
will be a partial script c of n events, where c =
c1c2 . . . cn = (v1, d1), (v2, d2), . . . , (vn, dn), and
the output of this model will be a ranked list of
events where the highest ranked events are the ones
most likely to belong to the event sequence in the
script. Thus, the key issue for this model is to de-
fine the function f for ranking events. We consider
three such ranking functions:
? Chambers & Jurafsky PMI. Chambers and
Jurafsky (2008) define their event ranking
function based on pointwise mutual infor-
mation. Given a partial script c as defined
above, they consider each event e = (v?, d?)
collected from their corpus, and score it as
the sum of the pointwise mutual informations
between the event e and each of the events in
the script:
f(e, c) =
n?
i
log
P (ci, e)
P (ci)P (e)
Chambers and Jurafsky?s description of this
score suggests that it is unordered, such that
P (a, b) = P (b, a). Thus the probabilities
must be defined as:
P (e1, e2) =
C(e1, e2) + C(e2, e1)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
where C(e1, e2) is the number of times that
the ordered event pair (e1, e2) was counted in
the training data, and C(e) is the number of
times that the event e was counted.
? Ordered PMI. A variation on the approach
of Chambers and Jurafsky is to have a score
that takes the order of the events in the chain
into account. In this scenario, we assume that
in addition to the partial script of events, we
are given an insertion point, m, where the
new event should be added. The score is then
defined as:
f(e, c) =
m?
k=1
log
P (ck, e)
P (ck)P (e)
+
n?
k=m+1
log
P (e, ck)
P (e)P (ck)
where the probabilities are defined as:
P (e1, e2) =
C(e1, e2)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
This approach uses pointwise mutual infor-
mation but also models the event chain in the
order it was observed.
? Bigram probabilities. Finally, a natural
ranking function, which has not been applied
to the script event prediction task (but has
339
been applied to related tasks (Manshadi et
al., 2008)) is to use the bigram probabilities
of language modeling rather than pointwise
mutual information scores. Again, given an
insertion point m for the event in the script,
we define the score as:
f(e, c) =
m?
k=1
logP (e|ck) +
n?
k=m+1
logP (ck|e)
where the conditional probability is defined
as2:
P (e1|e2) =
C(e1, e2)
C(e2)
This approach scores an event based on the
probability that it was observed following all
the events before it in the chain and preceding
all the events after it in the chain. This ap-
proach most directly models the event chain
in the order it was observed.
4 Experiments
Our experiments aimed to answer three questions:
Which event chains are worth keeping? How
should event bigram counts be collected? And
which ranking method is best for predicting script
events? To answer these questions we use two
corpora, the Reuters Corpus and the Andrew Lang
Fairy Tale Corpus, to evaluate our three differ-
ent chain selection methods, {all chains, long
chains, the longest chain}, our three different bi-
gram counting methods, {regular bigrams, 1-skip
bigrams, 2-skip bigrams}, and our three different
ranking methods, {Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}.
4.1 Corpora
We consider two corpora for evaluation:
? Reuters Corpus, Volume 1 3 (Lewis et
al., 2004) ? a large collection of 806, 791
news stories written in English concerning
a number of different topics such as politics,
2Note that predicted bigram probabilities are calculated
in this way for both classic language modeling and skip-gram
modeling. In skip-gram modeling, skips in the n-grams are
only used to increase the size of the training data; prediction
is performed exactly as in classic language modeling.
3http://trec.nist.gov/data/reuters/reuters.html
economics, sports, etc., strongly varying in
length, topics and narrative structure.
? Andrew Lang Fairy Tale Corpus 4 ? a
small collection of 437 children stories with
an average length of 125 sentences, and used
previously for story generation by McIntyre
and Lapata (2009).
In general, the Reuters Corpus is much larger and
allows us to see how well script events can be
predicted when a lot of data is available, while the
Andrew Lang Fairy Tale Corpus is much smaller,
but has a more straightforward narrative structure
that may make identifying scripts simpler.
4.2 Corpus Processing
Constructing a model for predicting script events
requires a corpus that has been parsed with a de-
pendency parser, and whose entities have been
identified via a coreference system. We there-
fore processed our corpora by (1) filtering out
non-narrative articles, (2) applying a dependency
parser, (3) applying a coreference resolution sys-
tem and (4) identifying event chains via entities
and dependencies.
First, articles that had no narrative content were
removed from the corpora. In the Reuters Corpus,
we removed all files solely listing stock exchange
values, interest rates, etc., as well as all articles
that were simply summaries of headlines from dif-
ferent countries or cities. After removing these
files, the Reuters corpus was reduced to 788, 245
files. Removing files from the Fairy Tale corpus
was not necessary ? all 437 stories were retained.
We then applied the Stanford Parser (Klein and
Manning, 2003) to identify the dependency struc-
ture of each sentence in each article in the corpus.
This parser produces a constitutent-based syntactic
parse tree for each sentence, and then converts this
tree to a collapsed dependency structure via a set
of tree patterns.
Next we applied the OpenNLP coreference en-
gine5 to identify the entities in each article, and the
noun phrases that were mentions of each entity.
Finally, to identify the event chains, we took
each of the entities proposed by the coreference
system, walked through each of the noun phrases
associated with that entity, retrieved any subject
4http://www.mythfolklore.net/andrewlang/
5http://incubator.apache.org/opennlp/
340
or object dependencies that linked a verb to that
noun phrase, and created an event chain from the
sequence of (verb, dependency-type) tuples in the
order that they appeared in the text.
4.3 Evaluation Metrics
We follow the approach of Chambers and Jurafsky
(2008), evaluating our models for predicting script
events in a narrative cloze task. The narrative
cloze task is inspired by the classic psychological
cloze task in which subjects are given a sentence
with a word missing and asked to fill in the blank
(Taylor, 1953). Similarly, in the narrative cloze
task, the system is given a sequence of events from
a script where one event is missing, and asked
to predict the missing event. The difficulty of a
cloze task depends a lot on the context around
the missing item ? in some cases it may be quite
predictable, but in many cases there is no single
correct answer, though some answers are more
probable than others. Thus, performing well on a
cloze task is more about ranking the missing event
highly, and not about proposing a single ?correct?
event.
In this way, narrative cloze is like perplexity
in a language model. However, where perplexity
measures how good the model is at predicting a
script event given the previous events in the script,
narrative cloze measures how good the model is
at predicting what is missing between events in
the script. Thus narrative cloze is somewhat more
appropriate to our task, and at the same time sim-
plifies comparisons to prior work.
Rather than manually constructing a set of
scripts on which to run the cloze test, we follow
Chambers and Jurafsky in reserving a section of
our parsed corpora for testing, and then using the
event chains from that section as the scripts for
which the system must predict events. Given an
event chain of length n, we run n cloze tests, with
a different one of the n events removed each time
to create a partial script from the remaining n? 1
events (see Figure 1). Given a partial script as
input, an accurate event prediction model should
rank the missing event highly in the guess list that
it generates as output.
We consider two approaches to evaluating the
guess lists produced in response to narrative cloze
tests. Both are defined in terms of a test collection
C, consisting of |C| partial scripts, where for each
partial script c with missing event e, ranksys(c) is
the rank of e in the system?s guess list for c.
? Average rank. The average rank of the miss-
ing event across all of the partial scripts:
1
|C|
?
c?C
ranksys(c)
This is the evaluation metric used by Cham-
bers and Jurafsky (2008).
? Recall@N. The fraction of partial scripts
where the missing event is ranked N or less6
in the guess list.
1
|C|
|{c : c ? C ? ranksys(c) ? N}|
In our experiments we use N = 50, but re-
sults are roughly similar for lower and higher
values of N .
Recall@N has not been used before for evaluat-
ing models that predict script events, however we
suggest that it is a more reliable metric than Av-
erage rank. When calculating the average rank,
the length of the guess lists will have a significant
influence on results. For instance, if a small model
is trained with only a small vocabulary of events,
its guess lists will usually be shorter than a larger
model, but if both models predict the missing event
at the bottom of the list, the larger model will get
penalized more. Recall@N does not have this is-
sue ? it is not influenced by length of the guess
lists.
An alternative evaluation metric would have
been mean average precision (MAP), a metric
commonly used to evaluate information retrieval.
Mean average precision reduces to mean recipro-
cal rank (MRR) when there?s only a single answer
as in the case of narrative cloze, and would have
scored the ranked lists as:
1
|C|
?
c?C
1
ranksys(c)
Note that mean reciprocal rank has the same issues
with guess list length that average rank does. Thus,
since it does not aid us in comparing to prior work,
and it has the same deficiencies as average rank,
we do not report MRR in this article.
6Rank 1 is the event that the system predicts is most prob-
able, so we want the missing event to have the smallest rank
possible.
341
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 502 0.5179
long chains 549 0.4951
the longest chain 546 0.4984
Table 1: Chain selection methods for the Reuters corpus
- comparison of average ranks and Recall@50.
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 1650 0.3376
long chains 452 0.3461
the longest chain 1534 0.3376
Table 2: Chain selection methods for the Fairy Tale
corpus - comparison of average ranks and Recall@50.
4.4 Results
We considered all 27 combinations of our chain
selection methods, bigram counting methods, and
ranking methods: {all chains, long chains, the
longest chain}x{regular bigrams, 1-skip bigrams,
2-skip bigrams}x{Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}. The best among
these 27 combinations for the Reuters corpus was
{all chains}x{2-skip bigrams}x{bigram probabil-
ities} achieving an average rank of 502 and a Re-
call@50 of 0.5179.
Since viewing all the combinations at once
would be confusing, instead the following sec-
tions investigate each decision (selection, counting,
ranking) one at a time. While one decision is var-
ied across its three choices, the other decisions are
held to their values in the best model above.
4.4.1 Identifying Event Chains
We first try to answer the question: How should
representative chains of events be selected from
the source text? Tables 1 and 2 show perfor-
mance when we vary the strategy for selecting
event chains, while fixing the counting method to
2-skip bigrams, and fixing the ranking method to
bigram probabilities.
For the Reuters collection, we see that using all
chains gives a lower average rank and a higher
Recall@50 than either of the strategies that select
a subset of the event chains. The explanation is
probably simple: using all chains produces more
than 700,000 bigrams from the Reuters corpus,
while using only the long chains produces only
around 300,000. So more data is better data for
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 789 0.4886
1-skip bigrams 630 0.4951
2-skip bigrams 502 0.5179
Table 3: Event bigram selection methods for the
Reuters corpus - comparison of average ranks and Re-
call@50.
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 2363 0.3227
1-skip bigrams 1690 0.3418
2-skip bigrams 1650 0.3376
Table 4: Event bigram selection methods for the Fairy
Tales corpus - comparison of average ranks and Re-
call@50.
predicting script events.
For the Fairy Tale collection, long chains gives
the lowest average rank and highest Recall@50. In
this collection, there is apparently some benefit to
filtering the shorter event chains, probably because
the collection is small enough that the noise in-
troduced from dependency and coreference errors
plays a larger role.
4.4.2 Gathering Event Chain Statistics
We next try to answer the question: Given an
event chain, how should statistics be gathered from
it? Tables 3 and 4 show performance when we vary
the strategy for counting event pairs, while fixing
the selecting method to all chains, and fixing the
ranking method to bigram probabilities.
For the Reuters corpus, 2-skip bigrams achieves
the lowest average rank and the highest Recall@50.
For the Fairy Tale corpus, 1-skip bigrams and 2-
skip bigrams perform similarly, and both have
lower average rank and higher Recall@50 than
regular bigrams.
Skip-grams probably outperform regular n-
grams on both of these corpora because the skip-
grams provide many more event pairs over which
to calculate statistics: in the Reuters corpus, regu-
lar bigrams extracts 737,103 bigrams, while 2-skip
bigrams extracts 1,201,185 bigrams. Though skip-
grams have not been applied to predicting script
events before, it seems that they are a good fit,
and better capture statistics about narrative event
chains than regular n-grams do.
342
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 2052 0.1954
ordered PMI 3584 0.1694
bigram prob. 502 0.5179
Table 5: Ranking methods for the Reuters corpus -
comparison of average ranks and Recall@50.
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 1455 0.1975
ordered PMI 2460 0.0467
bigram prob. 1650 0.3376
Table 6: Ranking methods for the Fairy Tale corpus -
comparison of average ranks and Recall@50.
4.4.3 Predicting Script Events
Finally, we try to answer the question: Given
event n-gram statistics, which ranking function
best predicts the events for a script? Tables 5 and
6 show performance when we vary the strategy for
ranking event predictions, while fixing the selec-
tion method to all chains, and fixing the counting
method to 2-skip bigrams.
For both Reuters and the Fairy Tale corpus, Re-
call@50 identifies bigram probabilities as the best
ranking function by far. On the Reuters corpus
the Chambers & Jurafsky PMI ranking method
achieves Recall@50 of only 0.1954, while bigram
probabilities ranking method achieves 0.5179. The
gap is also quite large on the Fairy Tales corpus:
0.1975 vs. 0.3376.
On the Reuters corpus, average rank also identi-
fies bigram probabilities as the best ranking func-
tion, yet for the Fairy Tales corpus, Chambers &
Jurafsky PMI and bigram probabilities have simi-
lar average ranks. This inconsistency is probably
due to the flaws in the average rank evaluation
measure that were discussed in Section 4.3 ? the
measure is overly sensitive to the length of the
guess list, particularly when the missing event is
ranked lower, as it is likely to be when training on
a smaller corpus like the Fairy Tales corpus.
5 Discussion
Our experiments have led us to several important
conclusions. First, we have introduced skip-grams
and proved their utility for acquiring script knowl-
edge ? our models that employ skip bigrams score
consistently higher on event prediction. By follow-
ing the intuition that events do not have to appear
strictly one after another to be closely semantically
related, skip-grams decrease data sparsity and in-
crease the size of the training data.
Second, our novel bigram probabilities ranking
function outperforms the other ranking methods.
In particular, it outperforms the state-of-the-art
pointwise mutual information method introduced
by Chambers and Jurafsky (2008), and it does so
by a large margin, more than doubling the Re-
call@50 on the Reuters corpus. The key insight
here is that, when modeling events in a script, a
language-model-like approach better fits the task
than a mutual information approach.
Third, we have discussed why Recall@N is a
better and more consistent evaluation metric than
Average rank. However, both evaluation metrics
suffer from the strictness of the narrative cloze test,
which accepts only one event being the correct
event, while it is sometimes very difficult, even
for humans, to predict the missing events, and
sometimes more solutions are possible and equally
correct. In future research, our goal is to design
a better evaluation framework which is more suit-
able for this task, where credit can be given for
proposed script events that are appropriate but not
identical to the ones observed in a text.
Fourth, we have observed some differences in
results between the Reuters and the Fairy Tale
corpora. The results for Reuters are consistently
better (higher Recall@50, lower average rank), al-
though fairy tales contain a plainer narrative struc-
ture, which should be more appropriate to our task.
This again leads us to the conclusion that more
data (even with more noise as in Reuters) leads to
a greater coverage of events, better overall models
and, consequently, to more accurate predictions.
Still, the Reuters corpus seems to be far from a
perfect corpus for research in the automatic acqui-
sition of scripts, since only a small portion of the
corpus contains true narratives. Future work must
therefore gather a large corpus of true narratives,
like fairy tales and children?s stories, whose sim-
ple plot structures should provide better learning
material, both for models predicting script events,
and for related tasks like automatic storytelling
(McIntyre and Lapata, 2009).
One of the limitations of the work presented
here is that it takes a fairly linear, n-gram-based ap-
proach to characterizing story structure. We think
such an approach is useful because it forms a natu-
343
ral baseline for the task (as it does in many other
tasks such as named entity tagging and language
modeling). However, story structure is seldom
strictly linear, and future work should consider
models based on grammatical or discourse links
that can capture the more complex nature of script
events and story structure.
Acknowledgments
We would like to thank the anonymous reviewers
for their constructive comments. This research
was carried out as a master thesis in the frame-
work of the TERENCE European project (EU FP7-
257410).
References
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 602?610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 976?986.
David Guthrie, Ben Allison, W. Liu, Louise Guthrie,
and Yorick Wilks. 2006. A closer look at skip-gram
modelling. In Proceedings of the Fifth international
Conference on Language Resources and Evaluation
(LREC), pages 1222?1225.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for
text categorization research. Journal of Machine
Learning Research, 5:361?397.
Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proceed-
ings of the Twenty-First International Florida Artifi-
cial Intelligence Research Society Conference.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 217?225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1562?1572.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979?988.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: an inquiry into
human knowledge structures. Lawrence Erlbaum
Associates.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readibility. Journalism Quarterly,
30:415?433.
344
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449?459,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Detecting Highly Confident Word Translations from Comparable
Corpora without Any Prior Knowledge
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
In this paper, we extend the work on using
latent cross-language topic models for iden-
tifying word translations across compara-
ble corpora. We present a novel precision-
oriented algorithm that relies on per-topic
word distributions obtained by the bilin-
gual LDA (BiLDA) latent topic model.
The algorithm aims at harvesting only the
most probable word translations across lan-
guages in a greedy fashion, without any
prior knowledge about the language pair,
relying on a symmetrization process and
the one-to-one constraint. We report our re-
sults for Italian-English and Dutch-English
language pairs that outperform the current
state-of-the-art results by a significant mar-
gin. In addition, we show how to use the al-
gorithm for the construction of high-quality
initial seed lexicons of translations.
1 Introduction
Bilingual lexicons serve as an invaluable resource
of knowledge in various natural language pro-
cessing tasks, such as dictionary-based cross-
language information retrieval (Carbonell et al
1997; Levow et al 2005) and statistical machine
translation (SMT) (Och and Ney, 2003). In or-
der to construct high quality bilingual lexicons for
different domains, one usually needs to possess
parallel corpora or build such lexicons by hand.
Compiling such lexicons manually is often an ex-
pensive and time-consuming task, whereas the
methods for mining the lexicons from parallel cor-
pora are not applicable for language pairs and do-
mains where such corpora is unavailable or miss-
ing. Therefore the focus of researchers turned to
comparable corpora, which consist of documents
with partially overlapping content, usually avail-
able in abundance. Thus, it is much easier to build
a high-volume comparable corpus. A representa-
tive example of such a comparable text collection
is Wikipedia, where one may observe articles dis-
cussing the similar topic, but strongly varying in
style, length and vocabulary, while still sharing a
certain amount of main concepts (or topics).
Over the years, several approaches for min-
ing translations from non-parallel corpora have
emerged (Rapp, 1995; Fung and Yee, 1998; Rapp,
1999; Diab and Finch, 2000; De?jean et al 2002;
Chiao and Zweigenbaum, 2002; Gaussier et al
2004; Fung and Cheung, 2004; Morin et al 2007;
Haghighi et al 2008; Shezaf and Rappoport,
2010; Laroche and Langlais, 2010), all sharing
the same Firthian assumption, often called the
distributionial hypothesis (Harris, 1954), which
states that words with a similar meaning are likely
to appear in similar contexts across languages.
All these methods have examined different rep-
resentations of word contexts and different meth-
ods for matching words across languages, but they
all have in common a need for a seed lexicon of
translations to efficiently bridge the gap between
languages. That seed lexicon is usually crawled
from the Web or obtained from parallel corpora.
Recently, Li et al(2011) have proposed an ap-
proach that improves precision of the existing
methods for bilingual lexicon extraction, based
on improving the comparability of the corpus un-
der consideration, prior to extracting actual bilin-
gual lexicons. Other methods such as (Koehn and
Knight, 2002) try to design a bootstrapping algo-
rithm based on an initial seed lexicon of transla-
tions and various lexical evidences. However, the
quality of their initial seed lexicon is disputable,
449
since the construction of their lexicon is language-
pair biased and cannot be completely employed
on distant languages. It solely relies on unsatis-
factory language-pair independent cross-language
clues such as words shared across languages.
Recent work from Vulic? et al2011) utilized
the distributional hypothesis in a different direc-
tion. It attempts to abrogate the need of a seed lex-
icon as a prerequisite for bilingual lexicon extrac-
tion. They train a cross-language topic model on
document-aligned comparable corpora and intro-
duce different methods for identifying word trans-
lations across languages, underpinned by per-
topic word distributions from the trained topic
model. Due to the fact that they deal with compa-
rable Wikipedia data, their translation model con-
tains a lot of noise, and some words are poorly
translated simply because there are not enough
occurrences in the corpus. The goal of this work is
to design an algorithm which will learn to harvest
only the most probable translations from the per-
word topic distributions. The translations learned
by the algorithm then might serve as a highly ac-
curate, precision-based initial seed lexicon, which
can then be used as a tool for translating source
word vectors into the target language. The key ad-
vantage of such a lexicon lies in the fact that there
is no language-pair dependent prior knowledge
involved in its construction (e.g., orthographic
features). Hence, it is completely applicable to
any language pair for which there exist sufficient
comparable data for training of the topic model.
Since comparable corpora often construct a
very noisy environment, it is of the utmost impor-
tance for a precision-oriented algorithm to learn
when to stop the process of matching words, and
which candidate pairs are surely not translations
of each other. The method described in this paper
follows this intuition: while extracting a bilingual
lexicon, we try to rematch words, keeping only
the most confident candidate pairs and disregard-
ing all the others. After that step, the most con-
fident candidate pairs might be used with some
of the existing context-based techniques to find
translations for the words discarded in the pre-
vious step. The algorithm is based on: (1) the
assumption of symmetry, and (2) the one-to-one
constraint. The idea of symmetrization has been
borrowed from the symmetrization heuristics in-
troduced for word alignments in SMT (Och and
Ney, 2003), where the intersection heuristics is
employed for a precision-oriented algorithm. In
our setting, it basically means that we keep a
translation pair (wSi , wTj ) if and only if, after the
symmetrization process, the top translation candi-
date for the source word wSi is the target word wTi
and vice versa. The one-to-one constraint aims
at matching the most confident candidates during
the early stages of the algorithm, and then exclud-
ing them from further search. The utility of the
constraint for parallel corpora has already been
evaluated by Melamed (2000).
The remainder of the paper is structured as
follows. Section 2 gives a brief overview of
the methods, relying on per-topic word distribu-
tions, which serve as the tool for computing cross-
language similarity between words. In Section
3, we motivate the main assumptions of the al-
gorithm and describe the full algorithm. Sec-
tion 4 justifies the underlying assumptions of
the algorithm by providing comparisons with a
current-state-of-the-art system for Italian-English
and Dutch-English language pairs. It also con-
tains another set of experiments which inves-
tigates the potential of the algorithm in build-
ing a language-pair unbiased seed lexicon, and
compares the lexicon with other seed lexicons.
Finally, Section 5 lists conclusion and possible
paths of future work.
2 Calculating Initial Cross-Language
Word Similarity
This section gives a quick overview of the Cue
method, the TI method, and their combination,
described by Vulic? et al2011), which proved to
be the most efficient and accurate for identify-
ing potential word translations once the cross-
language BiLDA topic model is trained and the
associated per-topic distributions are obtained for
both source and target corpora. The BiLDA
model we use is a natural extension of the stan-
dard LDA model and, along with the definition of
per-topic word distributions, has been presented
in (Ni et al 2009; De Smet and Moens, 2009;
Mimno et al 2009). BiLDA takes advantage of
the document alignment by using a single variable
that contains the topic distribution ?. This vari-
able is language-independent, because it is shared
by each of the paired bilingual comparable doc-
uments. Topics for each document are sampled
from ?, from which the words are then sampled
in conjugation with the vocabulary distribution ?
450
zSji wSji
? ?
zTji wTji
?
?
?
MS
MT
D
Figure 1: Plate model for bilingual Latent Dirichlet Allocation
1
Figure 1: The bilingual LDA (BiLDA) model
(for language S) and ? (for language T).
2.1 Cue Method
A straightforward approach to express similarity
between words tries to emphasize the associative
relation in a natural way - modeling the proba-
bility P (wT2 |wS1 ), i.e. the probability that a tar-
get word wT2 will be generated as a response to a
cue source word wS1 , where the link between the
words is established via the shared topic space:
P (wT2 |wS1 ) =
?K
k=1 P (wT2 |zk)P (zk|wS1 ), where
K denotes the number of cross-language topics.
2.2 TI Method
This approach constructs word vectors over a
shared space of cross-language topics, where val-
ues within vectors are the TF-ITF scores (term
frequency - inverse topic frequency), computed
in a completely analogical manner as the TF-
IDF scores for the original word-document space
(Manning and Schu?tze, 1999). Term frequency,
given a source word wSi and a topic zk, measures
the importance of the word wSi within the particu-
lar topic zk, while inverse topical frequency (ITF)
of the word wSi measures the general importance
of the source word wSi across all topics. The fi-
nal TF-ITF score for the source word wSi and the
topic zk is given by TF ?ITFi,k = TFi,k ?ITFi.
The TF-ITF scores for target words associated
with target topics are calculated in an analogical
manner and the standard cosine similarity is then
used to find the most similar target word vectors
for a given source word vector.
2.3 Combining the Methods
Topic models have the ability to build clusters of
words which might not always co-occur together
in the same textual units and therefore add ex-
tra information of potential relatedness. These
two methods for automatic bilingual lexicon ex-
traction interpret and exploit underlying per-topic
word distributions in different ways, so combin-
ing the two should lead to even better results. The
two methods are linearly combined, with the over-
all score given by:
SimTI+Cue(wS1 , wT2 ) = ?SimTI(wS1 , wT2 )
+ (1? ?)SimCue(wS1 , wT2 ) (1)
Both methods posses several desirable proper-
ties. According to Griffiths et al(2007), the con-
ditioning for the Cue method automatically com-
promises between word frequency and semantic
relatedness since higher frequency words tend to
have higher probability across all topics, but the
distribution over topics P (zk|wS1 ) ensures that se-
mantically related topics dominate the sum. The
similar phenomenon is captured by the TI method
by the usage of TF, which rewards high frequency
words, and ITF, which assigns a higher impor-
tance for words semantically more related to a
specific topic. These properties are incorporated
in the combination of the methods. As the final
result, the combined method provides, for each
source word, a ranked list of target words with as-
sociated scores that measure the strength of cross-
language similarity. The higher the score, the
more confident a translation pair is. We will use
this observation in the next section during the al-
gorithm construction.
The lexicon constructed by solely applying the
combination of these methods without any addi-
tional assumptions will serve as a baseline in the
results section.
3 Constructing the Algorithm
This section explains the underlying assumptions
of the algorithm: the assumption of symmetry
and the one-to-one assumption. Finally, it pro-
vides the complete outline of the algorithm.
3.1 Assumption of Symmetry
First, we start with the intuition that the assump-
tion of symmetry strengthens the confidence of a
translation pair. In other words, if the most prob-
able translation candidate for a source word wS1 is
a target word wT2 and, vice versa, the most prob-
able translation candidate of the target word wT2
451
is the source word wS1 , and their TI+Cue scores
are above a certain threshold, we can claim that
the words wS1 and wT2 are a translation pair. The
definition of the symmetric relation can also be
relaxed. Instead of observing only one top can-
didate from the lists, we can observe top N can-
didates from both sides and include them in the
search space, and then re-rank the potential candi-
dates taking into account their associated TI+Cue
scores and their respective positions in the list.
We will call N the search space depth. Here is
the outline of the re-ranking method if the search
space consists of the top N candidates on both
sides:
1. Given is a source word wSs , for which we ac-
tually want to find the most probable trans-
lation candidate. Initialize an empty list
Finals = {} in which target language
candidates with their recalculated associated
scores will be stored.
2. Obtain TI+Cue scores for all target words.
Keep only N best scoring target candidates:
{wTs,1, . . . , wTs,N} along with their respective
scores.
3. For each target candidate from
{wTs,1, . . . , wTs,N} acquire TI+Cue scores
over the entire source vocabulary. Keep only
N best scoring source language candidates.
Each word wTs,i ? {wTs,1, . . . , wTs,N} now
has a list of N source language candidates
associated with it: {wSi,1, wSi,2 . . . , wSi,N}.
4. For each target candidate word wTs,i ?
{wTs,1, . . . , wTs,N}, do as follows:
(a) If one of the words from the associated
list is the given source word wSs , re-
member: (1) the position m, denoting
how high in the list the word wSs was
found, and (2) the associated TI+Cue
score SimTI+Cue(wTs,i, wSi,m = wSs ).
Calculate:
(i) G1,i = SimTI+Cue(wSs , wTs,i)/i
(ii) G2,i = SimTI+Cue(wTs,i, wSi,m)/m
Following that, calculate GMi, the ge-
ometric mean of the values G1,i and
G2,i1: GMi =
?
G1,i ?G2,i. Add a tu-
1Scores G1,i and G2,i are structured in such a way to
balance between positions in the ranked lists and the TI+Cue
scores, since they reward candidate words which have high
TI+Cue scores associated with them, and penalize words if
they are found lower in the list of potential candidates.
ple (wTs,i, GMi) to the list Finals.
(b) If we have reached the end of the list
for the target candidate word wTs,i with-
out finding the given source word wSs ,
and i < N , continue with the next word
wTs,i+1. Do not add any tuple to Finals
in this step.
5. If the list Finals is not empty, sort the tuples
in the list in descending order according to
their GMi scores. The first element of the
sorted list contains a word wTs,high, the final
translation candidate of the source word wSs .
If the list Finals is not empty, the final re-
sult of this process will be the cross-language
word translation pair (wSs , wTs,high).
We will call this symmetrization process the
symmetrizing re-ranking. It attempts at push-
ing the correct cross-language synonym to the top
of the candidates list, taking into account both
the strength of similarities defined through the
TI+Cue scores in both directions, and positions
in ranked lists. A blatant example depicting how
this process helps boost precision is presented in
Figure 2. We can also design a thresholded variant
of this procedure by imposing an extra constraint.
When calculating target language candidates for
the source word wSs in Step 2, we proceed fur-
ther only if the first target candidate scores above
a certain threshold P and, additionally, in Step 3,
we keep lists of N source language candidates
for only those target words for which the first
source language candidate in their respective list
scored above the same threshold P . We will call
this procedure the thresholded symmetrizing re-
ranking, and this version will be employed in the
final algorithm.
3.2 One-to-one Assumption
Melamed (2000) has already established that most
source words in parallel corpora tend to translate
to only one target word. That tendency is modeled
by the one-to-one assumption, which constrains
each source word to have at most one translation
on the target side. Melamed?s paper reports that
this bias leads to a significant positive impact on
precision and recall of bilingual lexicon extraction
from parallel corpora. This assumption should
also be reasonable for many types of comparable
corpora such as Wikipedia or news corpora, which
are topically aligned or cover similar themes. We
452
abdij monasterymonkabbey kloostermonnikbenedictijnkloostermonnikabdijabdijmonnikklooster0.22370.15860.1155 0.30490.17400.13380.22660.14940.11310.25490.14960.1288
Figure 2: An example where the assumption of symmetry and the one-to-one assumption clearly help boost
precision. If we keep top Nc = 3 candidates from both sides, the algorithm is able to detect that the correct
Dutch-English translation pair is (abdij, abbey). The TI+Cue method without any assumptions would result with
an indirect association (abdij, monastery). If only the one-to-one assumption was present, the algorithm would
greedily learn the correct direct association (monastery, klooster), remove those words from their respective
vocabularies and then again result with another indirect association (abdij, monk). By additionally employing
the assumption of symmetry with the re-ranking method from Subsection 3.1, the algorithm correctly learns
the translation pair (abdij, abbey). Correct translation pairs (klooster, monastery) and (monnik, monk) are also
obtained. Again here, the pair (monnik, monk) would not be obtained without the one-to-one assumption.
will prove that the assumption leads to better pre-
cision scores even for bilingual lexicon extraction
from such comparable data. The intuition be-
hind introducing this constraint is fairly simple.
Without the assumption, the similarity scores be-
tween source and target words are calculated in-
dependently of each other. We will illustrate the
problem arising from the independence assump-
tion with an example.
Suppose we have an Italian word arcipelago,
and we would like to detect its correct English
translation (archipelago). However, after the
TI+Cue method is employed, and even after the
symmetrizing re-ranking process from the previ-
ous step is used, we still acquire a wrong transla-
tion candidate pair (arcipelago, island). Why is
that so? The word (arcipelago) (or its translation)
and the acquired translation (island) are semanti-
cally very close, and therefore have similar distri-
butions over cross-language topics, but island is a
much more frequent term. The TI+Cue method
concludes that two words are potential trans-
lations whenever their distributions over cross-
language topics are much more similar than ex-
pected by chance. Moreover, it gives a preference
to more frequent candidates, so it will eventually
end up learning an indirect association2 between
words arcipelago and island. The one-to-one as-
sumption should mitigate the problem of such in-
direct associations if we design our algorithm in
such a way that it learns the most confident direct
associations2 first:
2A direct association, as defined in (Melamed, 2000), is
an association between two words (in this setting found by
the TI+Cue method) where the two words are indeed mutual
translations. Otherwise, it is an indirect association.
453
1. Learn the correct direct association pair
(isola, island).
2. Remove the words isola and island from
their respective vocabularies.
3. Since island is not in the vocabulary, the
indirect association between arcipelago and
island is not present any more. The algo-
rithm learns the correct direct association
(arcipelago, archipelago).
3.3 The Algorithm
3.3.1 One-Vocabulary-Pass
First, we will provide a version of the algorithm
with a fixed threshold P which completes only
one pass through the source vocabulary. Let V S
denote a given source vocabulary, and let V T de-
note a given target vocabulary. We need to define
several parameters of the algorithm. Let N0 be
the initial maximum search space depth for the
thresholded symmetrizing re-ranking procedure.
In Figure 2, the current depth Nc is 3, while the
maximum depth might be set to a value higher
than 3. The algorithm with the fixed threshold P
proceeds as follows:
1. Initialize the maximum search space depth
NM = N0. Initialize an empty lexicon L.
2. For each source word wSs ? V S do:
(a) Set the current search space depthNc =
1.3
(b) Perform the thresholded symmetrizing
re-ranking procedure with the current
search space set toNc and the threshold
P . If a translation pair (wSs , wTs,high) is
found, go to the Sub-step 2(d).
(c) If a translation pair is not found, and
Nc < NM , increment the current
search space Nc = Nc+1 and return to
the previous Sub-step 2(b). If a trans-
lation pair is not found and Nc = NM ,
return to Step 2 and proceed with the
next word.
(d) For the found translation pair
(wSs , wTs,high), remove words wSs
and wTs,high from their respective
3The intuition here is simple ? we are trying to detect
a direct association as high as possible in the list. In other
words, if the first translation candidate for the source word
isola is the target word island, and, vice versa, the first
translation candidate for the target word island is isola, we
do not need to expand our search depth, because these two
words are the most likely translations.
vocabularies: V S = V S ? {wSs } and
V T = V T ? {wTs,high} to satisfy the
one-to-one constraint. Add the pair
(wSs , wTs,high) to the lexicon L.
We will name this procedure the one-
vocabulary-pass and employ it later in an iter-
ative algorithm with a varying threshold and a
varying maximum search space depth.
3.3.2 The Final Algorithm
Let us now define P0 as the initial threshold, let
Pf be the threshold at which we stop decreas-
ing the value for threshold and start expanding
our maximum search space depth for the thresh-
olded symmetrizing re-ranking, and let decp be a
value for which we decrease the current threshold
in each step. Finally, let Nf be the limit for the
maximum search space depth, andNM denote the
current maximum search space depth. The final
algorithm is given by:
1. Initialize the maximum search space depth
NM = N0 and the starting threshold P =
P0. Initialize an empty lexicon Lfinal.
2. Check the stopping criterion: If NM > Nf ,
go to Step 5, otherwise continue with Step 3.
3. Perform the one-vocabulary-pass with the
current values of P and NM . Whenever a
translation pair is found, it is added to the
lexicon Lfinal. Additionally, we can also
save the threshold and the depth at which that
pair was found.
4. Decrease P : P = P ? decp, and check
if P < Pf . If still not P < Pf , go to
Step 3 and perform the one-vocabulary-pass
again. Otherwise, if P < Pf and there are
still unmatched words in the source vocab-
ulary, reset P : P = P0, increment NM :
NM = NM + 1 and go to Step 2.
5. Return Lfinal as the final output of the algo-
rithm.
The parameters of the algorithm model its be-
havior. Typically, we would like to setP0 to a high
value, and N0 to a low value, which makes our
constraints strict and narrows our search space,
and consequently, extracts less translation pairs
in the first steps of the algorithm, but the set
of those translation pairs should be highly accu-
rate. Once it is not possible to extract any more
pairs with such strict constraints, the algorithm re-
454
laxes them by lowering the threshold and expand-
ing the search space by incrementing the max-
imum search space depth. The algorithm may
leave some of the source words unmatched, which
is also dependent on the parameters of the algo-
rithm, but, due to the one-to-one assumption, that
scenario also occurs whenever a target vocabulary
contains more words than a source vocabulary.
The number of operations of the algorithm also
depends on the parameters, but it mostly depends
on the sizes of the given vocabularies. The com-
plexity isO(|V S ||V T |), but the algorithm is com-
putationally feasible even for large vocabularies.
4 Results and Discussion
4.1 Training Collections
The data used for training of the models is col-
lected from various sources and varies strongly in
theme, style, length and its comparableness. In
order to reduce data sparsity, we keep only lem-
matized non-proper noun forms.
For Italian-English language pair, we use
18, 898 Wikipedia article pairs to train BiLDA,
covering different themes with different scopes
and subtopics being addressed. Document align-
ment is established via interlingual links from the
Wikipedia metadata. Our vocabularies consist of
7, 160 Italian nouns and 9, 116 English nouns.
For Dutch-English language pair, we use 7, 602
Wikipedia article pairs, and 6, 206 Europarl doc-
ument pairs, and combine them for training.4 Our
final vocabularies consist of 15, 284 Dutch nouns
and 12, 715 English nouns.
Unlike, for instance, Wikipedia articles, where
document alignment is established via interlin-
gual links, in some cases it is necessary to perform
document alignment as the initial step. Since our
work focuses on Wikipedia data, we will not get
into detail with algorithms for document align-
ment. An IR-based method for document align-
ment is given in (Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005), and a feature-based
method can be found in (Vu et al 2009).
4.2 Experimental Setup
All our experiments rely on BiLDA training
with comparable data. Corpora and software for
4In case of Europarl, we use only the evidence of docu-
ment alignment during the training and do not benefit from
the parallelness of the sentences in the corpus.
BiLDA training are obtained from Vulic? et al
(2011). We train the BiLDA model with 2000
topics using Gibbs sampling, since that number
of topics displays the best performance in their
paper. The linear interpolation parameter for the
combined TI+Cue method is set to ? = 0.1.
The parameters of the algorithm, adjusted on a
set of 500 randomly sampled Italian words, are set
to the following values in all experiments, except
where noted different: P0 = 0.20, Pf = 0.00,
decp = 0.01, N0 = 3, and Nf = 10.
The initial ground truth for our source vocab-
ularies has been constructed by the freely avail-
able Google Translate tool. The final ground truth
for our test sets has been established after we
have manually revised the list of pairs obtained by
Google Translate, deleting incorrect entries and
adding additional correct entries. All translation
candidates are evaluated against this benchmark
lexicon.
4.3 Experiment I: Do Our Assumptions Help
Lexicon Extraction?
With this set of experiments, we wanted to test
whether both the assumption of symmetry and
the one-to-one assumption are useful in improv-
ing precision of the initial TI+Cue lexicon extrac-
tion method. We compare three different lexicon
extraction algorithms: (1) the basic TI+Cue ex-
traction algorithm (LALG-BASIC) which serves
as the baseline algorithm5, (2) the algorithm from
Section 3, but without the one-to-one assump-
tion (LALG-SYM), meaning that if we find a
translation pair, we still keep words from the
translation pair in their respective vocabularies,
and (3) the complete algorithm from Section 3
(LALG-ALL). In order to evaluate these lexicon
extraction algorithms for both Italian-English and
Dutch-English, we have constructed a test set of
650 Italian nouns, and a test set of 1000 Dutch
nouns of high and medium frequency. Precision
scores for both language pairs and for all lexicon
extraction algorithms are provided in Table 1.
Based on these results, it is clearly visible that
both assumptions our algorithm makes are valid
5We have also tested whether LALG-BASIC outperforms
a method modeling direct co-occurrence, that uses cosine
to detect similarity between word vectors consisting of TF-
IDF scores in the shared document space (Cimiano et al
2009). Precision using that method is significantly lower,
e.g. 0.5538 vs. 0.6708 of LALG-BASIC for Italian-English.
455
LEX Algorithm Italian-English Dutch-English
LALG-BASIC 0.6708 0.6560
LALG-SYM 0.6862 0.6780
LALG-ALL 0.7215 0.7170
Table 1: Precision scores on our test sets for the 3 dif-
ferent lexicon extraction algorithms.
and contribute to better overall scores. Therefore
in all further experiments we will use the LALG-
ALL extraction algorithm.
4.4 Experiment II: How Does Thresholding
Affect Precision?
The next set of experiments aims at exploring how
precision scores change while we gradually de-
crease threshold values. The main goal of these
experiments is to detect when to stop with the ex-
traction of translation candidates in order to pre-
serve a lexicon of only highly accurate transla-
tions. We have fixed the maximum search space
depth N0 = Nf = 3. We used the same test sets
from Experiment I. Figure 3 displays the change
of precision in relation to different threshold val-
ues, where we start harvesting translations from
the threshold P0 = 0.2 down to Pf = 0.0. Since
our goal is to extract as many correct translation
pairs as possible, but without decreasing the pre-
cision scores, we have also examined what impact
this gradual decrease of threshold also has on the
number of extracted translations. We have opted
for the F? measure (van Rijsbergen, 1979):
F? = (1 + ?2)
Precision ?Recall
?2 ? Precision+Recall (2)
Since our task is precision-oriented, we have set
? = 0.5. F0.5 measure values precision as twice
as important as recall. The F0.5 scores are also
provided in Figure 3.
4.5 Experiment III: Building a Seed Lexicon
Finally, we wanted to test how many accurate
translation pairs our best scoring LALG-ALL al-
gorithm is able to acquire from the entire source
vocabulary, with very high precision still remain-
ing paramount. The obtained highly-precise seed
lexicon then might be employed for an additional
bootstrapping procedure similar to (Koehn and
Knight, 2002; Fung and Cheung, 2004) or sim-
ply for translating context vectors as in (Gaussier
et al 2004).
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Pre
cisi
on/
F-s
cor
e
00.050.10.150.2
Threshold
IT-EN Precision
IT-EN F-score
NL-EN Precision
NL-EN F-score
Pre
cisi
on/
F-s
cor
e
Pre
cisi
on/
F-s
cor
e
Pre
cisi
on/
F-s
cor
e
Figure 3: Precision and F0.5 scores in relation to
threshold values. We can observe that the algorithm
retrieves only highly accurate translations for both lan-
guage pairs while the threshold goes down from value
0.2 to 0.1, while precision starts to drop significantly
after the threshold of 0.1. F0.5 scores also reach their
peaks within that threshold region.
If we do not know anything about a given lan-
guage pair, we can only use words shared across
languages as lexical clues for the construction of
a seed lexicon. It often leads to a low precision
lexicon, since many false friends are detected.
For Italian-English, we have found 431 nouns
shared between the two languages, of which 350
were correct translations, leading to a precision
of 0.8121. As an illustration, if we take the
first 431 translation pairs retrieved by LALG-
ALL, there are 427 correct translation pairs, lead-
ing to a precision of 0.9907. Some pairs do
not share any orthographic similarities: (uccello,
bird), (tastiera, keyboard), (salute, health), (terre-
moto, earthquake) etc.
Following Koehn and Knight (2002), we have
also employed simple transformation rules for the
adoption of words from one language to another.
The rules specific to the Italian-English transla-
tion process that have been employed are: (R1) if
an Italian noun ends in?ione, but not in?zione,
strip the final e to obtain the corresponding En-
glish noun. Otherwise, strip the suffix ?zione,
and append ?tion; (R2) if a noun ends in ?ia,
but not in ?zia or ?fia, replace the suffix ?ia
with ?y. If a noun ends in ?zia, replace the suf-
fix with ?cy and if a noun ends in ?fia, replace
456
Italian-English Dutch-English
Lexicon # Correct Precision F0.5 # Correct Precision F0.5
LEX-1 350 0.8121 0.1876 898 0.8618 0.2308
LEX-2 766 0.8938 0.3473 1376 0.9011 0.3216
LEX-LALG 782 0.8958 0.3524 1106 0.9559 0.2778
LEX-1+LEX-LALG 1070 0.8785 0.4290 1860 0.9082 0.3961
LEX-R+LEX-LALG 1141 0.9239 0.4548 1507 0.9642 0.3500
LEX-2+LEX-LALG 1429 0.8926 0.5102 2261 0.9217 0.4505
Table 2: A comparison of different lexicons. For lexicons employing our LALG-ALL algorithm, only translation
candidates that scored above the threshold P = 0.11 have been kept.
it with ?phy. Similar rules have been introduced
for Dutch-English: the suffix ?tie is replaced by
?tion, ?sie by ?sion, and ?teit by ?ty.
Finally, we have compared the results of the
following constructed lexicons:
? A lexicon containing only words shared
across languages (LEX-1).
? A lexicon containing shared words and trans-
lation pairs found by applying the language-
specific transformation rules (LEX-2).
? A lexicon containing only translation pairs
obtained by the LALG-ALL algorithm that
score above a certain threshold P (LEX-
LALG).
? A combination of the lexicons LEX-1 and
LEX-LALG (LEX-1+LEX-LALG). Non-
matching duplicates are resolved by taking
the translation pair from LEX-LALG as the
correct one. Note that this lexicon is com-
pletely language-pair independent.
? A lexicon combining only translation pairs
found by applying the language-specific
transformation rules and LEX-LALG (LEX-
R+LEX-LALG).
? A combination of the lexicons LEX-2 and
LEX-LALG, where non-matching dupli-
cates are resolved by taking the translation
pair from LEX-LALG if it is present in
LEX-1, and from LEX-2 otherwise (LEX-
2+LEX-LALG).
According to the results from Table 2, we can
conclude that adding translation pairs extracted
by our LALG-ALL algorithm has a major posi-
tive impact on both precision and coverage. Ob-
taining results for two different language pairs
proves that the approach is generic and appli-
cable to any other language pairs. The previ-
ous approach relying on work from Koehn and
Knight (2002) has been outperformed in terms of
precision and coverage. Additionally, we have
shown that adding simple translation rules for lan-
guages sharing same roots might lead to even bet-
ter scores (LEX-2+LEX-LALG). However, it is
not always possible to rely on such knowledge,
and the usefulness of the designed LALG-ALL
algorithm really comes to the fore when the algo-
rithm is applied on distant language pairs which
do not share many words and cognates, and word
translation rules cannot be easily established. In
such cases, without any prior knowledge about the
languages involved in a translation process, one is
left with the linguistically unbiased LEX-1+LEX-
LALG lexicon, which also displays a promising
performance.
5 Conclusions and Future Work
We have designed an algorithm that focuses on ac-
quiring and keeping only highly confident trans-
lation candidates from multilingual comparable
corpora. By employing the algorithm we have
improved precision scores of the methods rely-
ing on per-topic word distributions from a cross-
language topic model. We have shown that the al-
gorithm is able to produce a highly reliable bilin-
gual seed lexicon even when all other lexical clues
are absent, thus making our algorithm suitable
even for unrelated language pairs. In future work,
we plan to further improve the algorithm and use
it as a source of translational evidence for differ-
ent alignment tasks in the setting of non-parallel
corpora.
Acknowledgments
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research
Fund K.U. Leuven, Belgium.
457
References
Jaime G. Carbonell, Jaime G. Yang, Robert E. Fred-
erking, Ralf D. Brown, Yibing Geng, Danny Lee,
Yiming Frederking, Robert E, Ralf D. Geng, and
Yiming Yang. 1997. Translingual information re-
trieval: A comparative evaluation. In Proceedings
of the 15th International Joint Conference on Arti-
ficial Intelligence, pages 708?714.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics, pages 1?5.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus
latent concept models for cross-language informa-
tion retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artifical Intelligence,
pages 1513?1518.
Wim De Smet and Marie-Francine Moens. 2009.
Cross-language linking of news stories on the Web
using interlingual topic modeling. In Proceedings
of the CIKM 2009 Workshop on Social Web Search
and Mining, pages 57?64.
Herve? De?jean, E?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics, pages 1?7.
Mona T. Diab and Steve Finch. 2000. A statis-
tical translation model using comparable corpora.
In Proceedings of the 6th Triennial Conference on
Recherche d?Information Assiste?e par Ordinateur
(RIAO), pages 1500?1508.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 414?420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geomet-
ric view on bilingual lexicon extraction from com-
parable corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic represen-
tation. Psychological Review, 114(2):211?244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 771?779.
Zellig S. Harris. 1954. Distributional structure. Word
10, (23):146?162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 617?625.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management, 41:523?547.
Bo Li, Eric Gaussier, and Akiko Aizawa. 2011. Clus-
tering comparable corpora for bilingual lexicon ex-
traction. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 473?478.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, MA, USA.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26:221?249.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880?889.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 664?671.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics, 31:477?504.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
Wikipedia. In Proceedings of the 18th International
World Wide Web Conference, pages 1155?1156.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 320?322.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
458
Meeting of the Association for Computational Lin-
guistics, pages 519?526.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 98?
107.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 72?79.
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworth.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2009. Feature-
based method for document alignment in compara-
ble news corpora. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 843?851.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 479?484.
459
Proceedings of NAACL-HLT 2013, pages 106?116,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Semantic Similarity of Words as the Similarity of Their
Semantic Word Responses
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
We propose a new approach to identifying
semantically similar words across languages.
The approach is based on an idea that two
words in different languages are similar if they
are likely to generate similar words (which in-
cludes both source and target language words)
as their top semantic word responses. Se-
mantic word responding is a concept from
cognitive science which addresses detecting
most likely words that humans output as free
word associations given some cue word. The
method consists of two main steps: (1) it uti-
lizes a probabilistic multilingual topic model
trained on comparable data to learn and quan-
tify the semantic word responses, (2) it pro-
vides ranked lists of similar words accord-
ing to the similarity of their semantic word
response vectors. We evaluate our approach
in the task of bilingual lexicon extraction
(BLE) for a variety of language pairs. We
show that in the cross-lingual settings without
any language pair dependent knowledge the
response-based method of similarity is more
robust and outperforms current state-of-the art
methods that directly operate in the semantic
space of latent cross-lingual concepts/topics.
1 Introduction
Cross-lingual semantic word similarity addresses
the task of detecting words that refer to similar se-
mantic concepts and convey similar meanings across
languages. It ultimately boils down to the automatic
identification of translation pairs, that is, bilingual
lexicon extraction (BLE). Such lexicons and seman-
tically similar words serve as important resources
in cross-lingual knowledge induction (e.g., Zhao et
al. (2009)), statistical machine translation (Och and
Ney, 2003) and cross-lingual information retrieval
(Ballesteros and Croft, 1997; Levow et al, 2005).
From parallel corpora, semantically similar words
and bilingual lexicons are induced on the basis of
word alignment models (Brown et al, 1993; Och
and Ney, 2003). However, due to a relative scarce-
ness of parallel texts for many language pairs and
domains, there has been a recent growing interest in
mining semantically similar words across languages
on the basis of comparable data readily available on
the Web (e.g., Wikipedia, news stories) (Haghighi et
al., 2008; Hassan and Mihalcea, 2009; Vulic? et al,
2011; Prochasson and Fung, 2011).
Approaches to detecting semantic word similarity
from comparable corpora are most commonly based
on an idea known as the distributional hypothesis
(Harris, 1954), which states that words with sim-
ilar meanings are likely to appear in similar con-
texts. Each word is typically represented by a high-
dimensional vector in a feature vector space or a so-
called semantic space, where the dimensions of the
vector are its context features. The semantic similar-
ity of two words, wS1 given in the source language
LS with vocabulary V S and wT2 in the target lan-
guage LT with vocabulary V T is then:
Sim(wS1 , w
T
2 ) = SF (cv(w
S
1 ), cv(w
T
2 )) (1)
cv(wS1 ) = [sc
S
1 (c1), . . . , sc
S
1 (cN )] denotes a context
vector for wS1 with N context features ck, where
scS1 (ck) denotes the score for w
S
1 associated with
context feature ck (similar for wT2 ). SF is a sim-
ilarity function (e.g., cosine, the Kullback-Leibler
106
divergence, the Jaccard index) operating on the con-
text vectors (Lee, 1999; Cha, 2007).
In order to compute cross-lingual semantic word
similarity, one needs to design the context features
of words given in two different languages that span
a shared cross-lingual semantic space. Such cross-
lingual semantic spaces are typically spanned by:
(1) bilingual lexicon entries (Rapp, 1999; Gaussier
et al, 2004; Laroche and Langlais, 2010; Tamura
et al, 2012), or (2) latent language-independent se-
mantic concepts/axes (e.g., latent cross-lingual top-
ics) induced by an algebraic model (Dumais et al,
1996), or more recently by a generative probabilis-
tic model (Haghighi et al, 2008; Daume? III and Ja-
garlamudi, 2011; Vulic? et al, 2011). Context vec-
tors cv(wS1 ) and cv(w
T
2 ) for both source and target
words are then compared in the semantic space in-
dependently of their respective languages.
In this work, we propose a new approach to con-
structing the shared cross-lingual semantic space
that relies on a paradigm of semantic word respond-
ing or free word association. We borrow that con-
cept from the psychology/cognitive science litera-
ture. Semantic word responding addresses a task
that requires participants to produce first words that
come to their mind that are related to a presented cue
word (Nelson et al, 2000; Steyvers et al, 2004).
The new cross-lingual semantic space is spanned
by all vocabulary words in the source and the target
language. Each axis in the space denotes a semantic
word response. The similarity between two words is
then computed as the similarity between the vectors
comprising their semantic word responses using any
of existing SF -s. Two words are considered seman-
tically similar if they are likely to generate similar
semantic word responses and assign similar impor-
tance to them.
We utilize a shared semantic space of latent cross-
lingual topics learned by a multilingual probabilistic
topic model to obtain semantic word responses and
quantify the strength of association between any cue
word and its responses monolingually and across
languages, and, consequently, to build semantic re-
sponse vectors. That effectively translates the task
of word similarity from the semantic space spanned
by latent cross-lingual topics to the semantic space
spanned by all vocabulary words in both languages.
The main contributions of this article are:
? We propose a new approach to modeling cross-
lingual semantic similarity of words based on
the similarity of their semantic word responses.
? We present how to estimate and quantify se-
mantic word responses by means of a multilin-
gual probabilistic topic model.
? We demonstrate how to employ our novel
paradigm that relies on semantic word respond-
ing in the task of bilingual lexicon extraction
(BLE) from comparable data.
? We show that the response-based model of sim-
ilarity is more robust and obtains better results
for BLE than the models that operate in the se-
mantic space spanned by latent semantic con-
cepts, i.e., cross-lingual topics directly.
The following sections first review relevant prior
work and provide a very short introduction to multi-
lingual probabilistic topic modeling, then describe
our response-based approach to modeling cross-
lingual semantic word similarity, and finally present
our evaluation and results on the BLE task for a va-
riety of language pairs.
2 Related Work
When dealing with the cross-lingual semantic word
similarity, the focus of the researchers is typically
on BLE, since usually the most similar words across
languages are direct translations of each other. Nu-
merous approaches emerged over the years that try
to induce bilingual word lexicons on the basis of
distributional information. Especially challenging
is the task of mining semantically similar words
from comparable data without any external knowl-
edge source such as machine-readable seed bilin-
gual lexicons used in (Fung and Yee, 1998; Rapp,
1999; Fung and Cheung, 2004; Gaussier et al, 2004;
Morin et al, 2007; Andrade et al, 2010; Tamura
et al, 2012), predefined explicit ontology or cate-
gory knowledge used in (De?jean et al, 2002; Hassan
and Mihalcea, 2009; Agirre et al, 2009), or ortho-
graphic clues as used in (Koehn and Knight, 2002;
Haghighi et al, 2008; Daume? III and Jagarlamudi,
2011). This work addresses that particularly difficult
setting which does not assume any language pair de-
pendent background knowledge. It makes methods
107
developed in such a setting applicable even on dis-
tant language pairs with scarce resources.
Recently, Griffiths et al (2007), and Steyvers and
Griffiths (2007) proposed models of free word asso-
ciation and semantic word similarity in the monolin-
gual settings based on per-topic word distributions
from probabilistic topic models such as pLSA (Hof-
mann, 1999) and LDA (Blei et al, 2003). Addition-
ally, Vulic? et al (2011) constructed several models
that utilize a shared cross-lingual topical space ob-
tained by a multilingual topic model (Mimno et al,
2009; De Smet and Moens, 2009; Boyd-Graber and
Blei, 2009; Ni et al, 2009; Jagarlamudi and Daume?
III, 2010; Zhang et al, 2010) to identify potential
translation candidates in the cross-lingual settings
without any background knowledge. In this paper,
we show that a transition from their semantic space
spanned by cross-lingual topics to a semantic space
spanned by all vocabulary words yields more robust
models of cross-lingual semantic word similarity.
3 Modeling Word Similarity as the
Similarity of Semantic Word Responses
This section contains a detailed description of our
semantic word similarity method that relies on se-
mantic word responses. Since the method utilizes
the concept of multilingual probabilistic topic mod-
eling, we first provide a very short overview of that
concept, then present the intuition behind the ap-
proach, and finally describe our method in detail.
3.1 Multilingual Probabilistic Topic Modeling
Assume that we are given a multilingual corpus
C of l languages, and C is a set of text collec-
tions {C1, . . . , Cl} in those languages. A multi-
lingual probabilistic topic model (Mimno et al,
2009; De Smet and Moens, 2009; Boyd-Graber
and Blei, 2009; Ni et al, 2009; Jagarlamudi and
Daume? III, 2010; Zhang et al, 2010) of a mul-
tilingual corpus C is defined as a set of semanti-
cally coherent multinomial distributions of words
with values Pj(w
j
i |zk), j = 1, . . . , l, for each vo-
cabulary V 1, . . . , V j , . . . , V l associated with text
collections C1, . . . , Cj , . . . , Cl ? C given in lan-
guages L1, . . . , Lj , . . . , Ll. Pj(w
j
i |zk) is calculated
for eachwji ? V
j . The probability scores Pj(w
j
i |zk)
build per-topic word distributions, and they consti-
tute a language-specific representation (e.g., a prob-
ability value is assigned only for words from V j)
of a language-independent cross-lingual latent con-
cept, that is, latent cross-lingual topic zk ? Z .
Z = {z1, . . . , zK} represents the set of all K la-
tent cross-lingual topics present in the multilingual
corpus. Each document in the multilingual corpus
is thus considered a mixture of K cross-lingual top-
ics from the set Z . That mixture for some docu-
ment dji ? Cj is modeled by the probability scores
Pj(zk|d
j
i ) that altogether build per-document topic
distributions.
Each cross-lingual topic from the set Z can be
observed as a latent language-independent concept
present in the multilingual corpus, but each lan-
guage in the corpus uses only words from its own
vocabulary to describe the content of that concept.
For instance, having a multilingual collection in En-
glish, Spanish and Dutch and discovering a topic
on Soccer, that cross-lingual topic would be repre-
sented by words (actually probabilities over words)
{player, goal, coach, . . .} in English, {balo?n (ball),
futbolista (soccer player), goleador (scorer), . . .}
in Spanish, and {wedstrijd (match), elftal (soccer
team), doelpunt (goal), . . .} in Dutch. We have
?
wji?V
j Pj(w
j
i |zk) = 1, for each vocabulary V
j
representing language Lj , and for each topic zk ?
Z . Therefore, the latent cross-lingual topics also
span a shared cross-lingual semantic space.
3.2 The Intuition Behind the Approach
Imagine the following thought experiment. A group
of human subjects who have been raised bilingually
and thus are native speakers of two languages LS
and LT , is playing a game of word associations.
The game consists of possibly an infinite number of
iterations, and each iteration consists of 4 rounds.
In the first round (the S-S round), given a word in
the language LS , the subject has to generate a list
of words in the same language LS that first occur
to her/him as semantic word responses to the given
word. The list is in descending order, with more
prominent word responses occurring higher in the
list. In the second round (the S-T round), the sub-
ject repeats the procedure, and generates the list of
word responses to the same word from LS , but now
in the other language LT . The third (the T-T round)
108
and the fourth round (the T-S round) are similar to
the first and the second round, but now a list of word
responses in both LS and LT has to be generated for
some cue word from LT . The process of generating
the lists of semantic responses then continues with
other cue words and other human subjects.
As the final result, for each word in the source
language LS , and each word in the target language
LT , we obtain a single list of semantic word re-
sponses comprising words in both languages. All
lists are sorted in descending order, based on some
association score that takes into account both the
number of times a word has occurred as an asso-
ciative response, as well as the position in the list
in each round. We can now measure the similarity
of any two words, regardless of their corresponding
languages, according to the similarity of their cor-
responding lists that contain their word responses.
Words that are equally likely to trigger the same as-
sociative responses in the human brain, and more-
over assign equal importance to those responses, as
provided in the lists of associative responses, are
very likely to be closely semantically similar. Addi-
tionally, for a given word wS1 in the source language
LS , some word wT2 in LT that has the highest simi-
larity score among all words inLT should be a direct
word-to-word translation of wS1 .
3.3 Modeling Semantic Word Responses via
Cross-Lingual Topics
Cross-lingual topics provide a sound framework to
construct a probabilistic model of the aforemen-
tioned experiment. To model semantic word re-
sponses via the shared space of cross-lingual top-
ics, we have to set a probabilistic mass that quan-
tifies the degree of association. Given two words
w1, w2 ? V S ? V T , a natural way of expressing the
asymmetric semantic association is by modeling the
probability P (w2|w1) (Griffiths et al, 2007), that is,
the probability to generate word w2 as a response
given word w1. After the training of a multilin-
gual topic model on a multilingual corpus, we obtain
per-topic word distributions with scores PS(wSi |zk)
and PT (wTi |zk) (see Sect. 3.1).
1 The probability
1A remark on notation throughout the paper: Since the
shared space of cross-lingual topics allows us to construct a
uniform representation for all words regardless of a vocabulary
they belong to, due to simplicity and to stress the uniformity,
P (w2|w1) is then decomposed as follows:
Resp(w1, w2) = P (w2|w1) =
K?
k=1
P (w2|zk)P (zk|w1) (2)
The probability scores P (w2|zk) select words that
are highly descriptive for each particular topic. The
probability scores P (zk|w1) ensure that topics zk
that are semantically relevant to the given word
w1 dominate the sum, so the overall high score
Resp(w1, w2) of the semantic word response is as-
signed only to highly descriptive words of the se-
mantically related topics. Using the shared space
of cross-lingual topics, semantic response scores can
be derived for any two words w1, w2 ? V S ? V T .1
The generative model closely resembles the ac-
tual process in the human brain - when we gener-
ate semantic word responses, we first tend to as-
sociate that word with a related semantic/cognitive
concept, in this case a cross-lingual topic (the factor
P (zk|w1)), and then, after establishing the concept,
we output a list of words that we consider the most
prominent/descriptive for that concept (words with
high scores in the factor P (w2|zk)) (Nelson et al,
2000; Steyvers et al, 2004). Due to such modeling
properties, this model of semantic word responding
tends to assign higher association scores for high
frequency words. It eventually leads to asymmet-
ric associations/responses. We have detected that
phenomenon both monolingually and across lan-
guages. For instance, the first response to Span-
ish word mutacio?n (mutation) is English word gene.
Other examples include caldera (boiler)-steam, de-
portista (sportsman)-sport, horario (schedule)-hour
or pescador (fisherman)-fish. In the other associa-
tion direction, we have detected top responses such
as merchant-comercio (trade) or neologism-palabra
(word). In the monolingual setting, we acquire
English pairs such as songwriter-music, discipline-
sport, or Spanish pairs gripe (flu)-enfermedad (dis-
ease), cuenca (basin)-r??o (river), etc.
3.4 Response-Based Model of Similarity
Eq. (2) provides a way to measure the strength of
semantic word responses. In order to establish the
we sometimes use notation P (wi|zk) and P (zk|wi) instead of
PS(wi|zk) or PS(zk|wi) (similar for subscript T ). However,
the reader must be aware that, for instance, P (wi|zk) actually
means PS(wi|zk) if wi ? V S , and PT (wi|zk) if wi ? V T .
109
Semantic responses Response-based similarity
dramaturgo (playwright) play playwright dramaturgo
obra (play) .101 play .142 play .122 playwright
escritor (writer) .083 obra (play) .111 escritor (writer) .087 dramatist
play .066 player .033 obra (play) .073 tragedy
writer .050 escena (scene) .031 writer .060 play
poet .047 jugador (player) .026 poeta (poet) .055 essayist
autor (author) .041 adaptation .025 poet .053 novelist
poeta (poet) .039 stage .024 autor (author) .046 drama
teatro (theatre) .030 game .022 teatro (theatre) .043 tragedian
drama .026 juego (game) .021 tragedy .031 satirist
contribution .025 teatro (theatre) .019 drama .026 writer
Table 1: An example of top 10 semantic word responses and the final response-based similarity for some Spanish and
English words. The responses are estimated from Spanish-English Wikipedia data by bilingual LDA. We can observe
several interesting phenomena: (1) High-frequency words tend to appear higher in the lists of semantic responses
(e.g., play and obra for all 3 words), (2) Due to the modeling properties that give preference to high-frequency words
(Sect. 3.3), a word might not generate itself as the top semantic response (e.g., playwright-play), (3) Both source
and target language words occur as the top responses in the lists, (4) Although play is the top semantic response in
English for both dramaturgo and playwright, its list of top semantic responses is less similar to the lists of those two
words, (5) Although the English word playwright does not appear in the top 10 semantic responses to dramaturgo,
and dramaturgo does not appear in the top 10 responses to playwright, the more robust response-based similarity
method detects that the two words are actually very similar based on their lists of responses, (6) dramaturgo and
playwright have very similar lists of semantic responses which ultimately leads to detecting that playwright is the
most semantically similar word to dramaturgo across the two languages (the last column), i.e., they are direct one-to-
one translations of each other, (7) Another English word dramatist very similar to Spanish dramaturgo is also pushed
higher in the final list, although it is not found in the list of top semantic responses to dramaturgo.
final similarity between two words, we have to com-
pare their semantic response vectors, that is, their
semantic response scores over all words in both
vocabularies. The final model of word similarity
closely mimics our thought experiment. First, for
each word wSi ? V
S , we generate probability scores
P (wSj |w
S
i ) for all words w
S
j ? V
S (the S-S rounds).
Note that P (wSi |w
S
i ) is also defined by Eq. (2).
Following that, for each word wSi ? V
S , we gen-
erate probability scores P (wTj |w
S
i ), for all words
wTj ? V
T (the S-T rounds). Similarly, we calcu-
late probability scores P (wTj |w
T
i ) and P (w
S
j |w
T
i ),
for each wTi , w
T
j ? V
T , and for each wSj ? V
S (the
T-T and T-S rounds).
Now, each word wi ? V S ? V T may be repre-
sented by a (|V S |+ |V T |)-dimensional context vec-
tor cv(wi) as follows:2
[P (wS1 |wi), . . . , P (w
S
|V S ||wi), . . . , P (w
T
|V T ||wi)].
We have created a language-independent cross-
2We assume that the two sets V S and V T are disjunct. It
means that, for instance, Spanish word pie (foot) from V S and
English word pie from V T are treated as two different word
types. In that case, it holds |V S ? V T | = |V S |+ |V T |.
lingual semantic space spanned by all vocabulary
words in both languages. Each feature corresponds
to one word from vocabularies V S and V T , while
the exact score for each feature in the context
vector cv(wi) is precisely the probability that this
word/feature will be generated as a word response
given word wi. The degree of similarity between
two words is then computed on the basis of similar-
ity between their feature vectors using some of the
standard similarity functions (Cha, 2007).
The novel response-based approach of similarity
removes the effect of high-frequency words that tend
to appear higher in the lists of semantic word re-
sponses. Therefore, the real synonyms and trans-
lations should occur as top candidates in the lists
of similar words obtained by the response-based
method. That property may be exploited to identify
one-to-one translations across languages and build a
bilingual lexicon (see Table 1).
4 Experimental Setup
4.1 Data Collections
We work with the following corpora:
110
? IT-EN-W: A collection of 18, 898 Italian-
English Wikipedia article pairs previously used
by Vulic? et al (2011).
? ES-EN-W: A collection of 13, 696 Spanish-
English Wikipedia article pairs.
? NL-EN-W: A collection of 7, 612 Dutch-
English Wikipedia article pairs.
? NL-EN-W+EP: The NL-EN-W corpus aug-
mented with 6,206 Dutch-English document
pairs from Europarl (Koehn, 2005). Although
Europarl is a parallel corpus, no explicit use is
made of sentence-level alignments.
All corpora are theme-aligned, that is, the aligned
document pairs discuss similar subjects, but are
in general not direct translations (except the Eu-
roparl document pairs). NL-EN-W+EP serves to test
whether better semantic responses could be learned
from data of higher quality, and to measure how it
affects the response-based similarity method and the
quality of induced lexicons. Following (Koehn and
Knight, 2002; Haghighi et al, 2008; Prochasson and
Fung, 2011), we consider only noun word types. We
retain only nouns that occur at least 5 times in the
corpus. We record the lemmatized form when avail-
able, and the original form otherwise. Again follow-
ing their setup, we use TreeTagger (Schmid, 1994)
for POS tagging and lemmatization.
4.2 Multilingual Topic Model
The multilingual probabilistic topic model we use
is a straightforward multilingual extension of the
standard Blei et al?s LDA model (Blei et al, 2003)
called bilingual LDA (Mimno et al, 2009; Ni et
al., 2009; De Smet and Moens, 2009). For the de-
tails regarding the modeling assumptions, generative
story, training and inference procedure of the bilin-
gual LDA model, we refer the interested reader to
the aforementioned relevant literature. The poten-
tial of the model in the task of bilingual lexicon ex-
traction was investigated before (Mimno et al, 2009;
Vulic? et al, 2011), and it was also utilized in other
cross-lingual tasks (e.g., Platt et al (2010); Ni et
al. (2011)). We use Gibbs sampling for training.
In a typical setting for mining semantically similar
words using latent topic models in both monolingual
(Griffiths et al, 2007; Dinu and Lapata, 2010) and
cross-lingual setting (Vulic? et al, 2011), the best re-
sults are obtained with the number of topics set to
a few thousands (? 2000). Therefore, our bilingual
LDA model on all corpora is trained with the number
of topics K = 2000. Other parameters of the model
are set to the standard values according to Steyvers
and Griffiths (2007): ? = 50/K and ? = 0.01.
We are aware that different hyper-parameter settings
(Asuncion et al, 2009; Lu et al, 2011), might have
influence on the quality of learned cross-lingual top-
ics, but that analysis is out of the scope of this paper.
4.3 Compared Methods
We evaluate and compare the following word simi-
larity approaches in all our experiments:
1) The method that regards the lists of semantic
word responses across languages obtained by Eq.
(2) directly as the lists of semantically similar words
(Direct-SWR).
2) The state-of-the-art method that employs a simi-
larity function (SF) on theK-dimensional word vec-
tors cv(wi) in the semantic space of latent cross-
lingual topics. The dimensions of the vectors are
conditional topic distribution scores P (zk|wi) that
are obtained by the multilingual topic model directly
(Steyvers and Griffiths, 2007; Vulic? et al, 2011). We
have tested different SF-s (e.g., the Kullback-Leibler
and the Jensen-Shannon divergence, the cosine mea-
sure), and have detected that in general the best
scores are obtained when using the Bhattacharyya
coefficient (BC) (Bhattacharyya, 1943; Kazama et
al., 2010) (Topic-BC).
3) The best scoring similarity method from Vulic?
et al (2011) named TI+Cue. This state-of-the-art
method also operates in the semantic space of latent
cross-lingual concepts/topics.
4) The response-based similarity described in Sect.
3. As for Topic-BC, we again use BC as the simi-
larity function, but now on |V S ? V T |-dimensional
context vectors in the semantic space spanned by
all words in both vocabularies that represent seman-
tic word responses (Response-BC). Given two N -
dimensional word vectors cv(wS1 ) and cv(w
T
2 ), the
BC or the fidelity measure (Cha, 2007) is defined as:
BC(cv(wS1 ), cv(w
T
2 )) =
N?
n=1
?
scS1 (cn) ? sc
T
2 (cn) (3)
111
Corpus: IT-EN-W ES-EN-W NL-EN-W NL-EN-W+EP
Method Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
Direct-SWR .501 .576 .740 .332 .437 .675 .186 .254 .423 .344 .450 .652
Topic-BC .578 .667 .834 .433 .576 .843 .237 .314 .489 .534 .630 .836
TI+Cue .597 .702 .897 .429 .569 .828 .225 .296 .459 .446 .569 .808
Response-BC .622 .729 .882 .517 .635 .891 .236 .320 .511 .574 .653 .864
Table 2: BLE performance of all the methods for Italian-English, Spanish-English and Dutch-English (with 2 different
corpora utilized for the training of bilingual LDA and the estimation of semantic word responses for Dutch-English).
For the Topic-BC method N = K, while N =
|V S ? V T | for Response-BC. Additionally, since
P (zk|wi) > 0 and P (wk|wi) > 0 for each zk ? Z
and each wk ? V S ? V T , a lot of probability mass
is assigned to topics and semantic responses that
are completely irrelevant to the given word. Re-
ducing the dimensionality of the semantic repre-
sentation a posteriori to only a smaller number of
most important semantic axes in the semantic spaces
should decrease the effects of that statistical noise,
and even more firmly emphasize the latent corre-
lation among words. The utility of such semantic
space truncating or feature pruning in monolingual
settings (Reisinger and Mooney, 2010) was also de-
tected previously for LSA and LDA-based models
(Landauer and Dumais, 1997; Griffiths et al, 2007).
Therefore, unless noted otherwise, we perform all
our calculations over the best scoring 200 cross-
lingual topics and the best scoring 2000 semantic
word responses.3
4.4 Evaluation
Ground truth translation pairs.4 Since our task
is bilingual lexicon extraction, we designed a set
of ground truth one-to-one translation pairs for all
3 language pairs as follows. For Dutch-English
and Spanish-English, we randomly sampled a set
of Dutch (Spanish) nouns from our Wikipedia cor-
pora. Following that, we used the Google Trans-
late tool plus an additional annotator to translate
those words to English. The annotator manually
revised the lists and retained only words that have
3The values are set empirically. Calculating similarity
Sim(wS1 , w
T
2 ) may be interpreted as: ?Given word w
S
1 detect
how similar word wT2 is to the word w
S
1 .? Therefore, when
calculating Sim(wS1 , w
T
2 ), even when dealing with symmetric
similarity functions such as BC, we always consider only the
scores P (?|wS1 ) for truncating.
4Available online: http://people.cs.kuleuven.be
/?ivan.vulic/software/
their corresponding translation in the English vo-
cabulary. Additionally, only one possible translation
was annotated as correct. When more than 1 trans-
lation is possible, the annotator marked as correct
the translation that occurs more frequently in the En-
glish Wikipedia data. Finally, we built a set of 1000
one-to-one translation pairs for Dutch-English and
Spanish-English. The same procedure was followed
for Italian-English, but there we obtained the ground
truth one-to-one translation pairs for 1000 most fre-
quent Italian nouns in order to test the effect of word
frequency on the quality of semantic word responses
and the overall lexicon quality.
Evaluation metrics. All the methods under con-
sideration actually retrieve ranked lists of semanti-
cally similar words that could be observed as poten-
tial translation candidates. We measure the perfor-
mance on BLE as Top M accuracy (AccM ). It de-
notes the number of source words from ground truth
translation pairs whose top M semantically simi-
lar words contain the correct translation according
to our ground truth over the total number of ground
truth translation pairs (=1000) (Tamura et al, 2012).
Additionally, we compute the mean reciprocal rank
(MRR) scores (Voorhees, 1999).
5 Results and Discussion
Table 2 displays the performance of each compared
method on the BLE task. It shows the difference in
results for different language pairs and different cor-
pora used to extract latent cross-lingual topics and
estimate the lists of semantic word responses. Ex-
ample lists of semantically similar words over all 3
language pairs are shown in Table 3. Based on these
results, we are able to derive several conclusions:
(i) Response-BC performs consistently better than
the other 3 methods over all corpora and all language
pairs. It is more robust and is able to find some
cross-lingual similarities omitted by the other meth-
112
Italian-English (IT-EN) Spanish-English (ES-EN) Dutch-English (NL-EN)
(1) affresco (2) spigolo (3) coppa (1) caza (2) discurso (3) comprador (1) behoud (2) schroef (3) spar
(fresco) (edge) (cup) (hunting) (speech) (buyer) (conservation) (screw) (fir)
fresco polyhedron club hunting rhetoric purchase conservation socket conifer
mural polygon competition hunt oration seller preservation wire pine
nave vertices final hunter speech tariff heritage wrap firewood
wall diagonal champion hound discourse market diversity wrench seedling
testimonial edge football safari dialectic bidding emphasis screw weevil
apse vertex trophy huntsman rhetorician auction consequence pin chestnut
rediscovery binomial team wildlife oratory bid danger fastener acorn
draughtsman solid relegation animal wisdom microeconomics contribution torque girth
ceiling graph tournament ungulate oration trade decline pipe lumber
palace modifier soccer chase persuasion listing framework routing bark
Table 3: Example lists of top 10 semantically similar words across all 3 language pairs according to our Response-BC
similarity method, where the correct translation word is: (col. 1) found as the most similar word, (2) contained lower
in the list, and (3) not found in the top 10 words.
IT-EN ES-EN NL-EN
direttore-director flauta-flute kustlijn-coastline
radice-root eficacia-efficacy begrafenis-funeral
sintomo-symptom empleo-employment mengsel-mixture
perdita-loss descubierta-discovery lijm-glue
danno-damage desalojo-eviction kijker-viewer
battaglione-battalion miedo-fear oppervlak-surface
Table 4: Example translations found by the Response-BC
method, but missed by the other 3 methods.
ods (see Table 4). The overall quality of the cross-
lingual word similarities and lexicons extracted by
the method is dependent on the quality of estimated
semantic response vectors. The quality of these
vectors is of course further dependent on the qual-
ity of multilingual training data. For instance, for
Dutch-English, we may observe a rather spectacular
increase in overall scores (the tests are performed
over the same set of 1000 words) when we aug-
ment Wikipedia data with Europarl data (compare
the scores for NL-EN-W and NL-EN-W+EP).
(ii) A transition from a semantic space spanned by
cross-lingual topics (Topic-BC) to a semantic space
spanned by vocabulary words (Response-BC) leads
to better results over all corpora and language pairs.
The difference is less visible when using training
data of lesser quality (the scores for NL-EN-W).
Moreover, since the shared space of cross-lingual
topics is used to obtain and quantify semantic word
responses, the quality of learned cross-lingual topics
influences the quality of semantic word responses.
If the semantic coherence of the cross-lingual top-
ical space is unsatisfying, the method is unable to
generate good semantic response vectors, and ul-
timately unable to correctly identify semantically
similar words across languages.
(iii) Due to its modeling properties that assign more
importance to high-frequency words, Direct-SWR
produces reasonable results in the BLE task only for
high-frequency words (see results for IT-EN-W). Al-
though Eq. (2) models the concept of semantic word
responding in a sound way (Griffiths et al, 2007),
using the semantic word responses directly is not
suitable for the actual BLE task.
(iv) The effect of word frequency is clearly visi-
ble when comparing the results obtained on IT-EN-
W with the results obtained on the other Wikipedia
corpora. High-frequency words produce more re-
dundancies in training data that are captured by sta-
tistical models such as latent topic models. High-
frequency words then obtain better estimates of their
semantic response vectors which consequently leads
to better overall scores. The effect of word fre-
quency on statistical methods in the BLE task was
investigated before (Pekar et al, 2006; Prochasson
and Fung, 2011; Tamura et al, 2012), and we also
confirm their findings.
(v) Unlike (Koehn and Knight, 2002; Haghighi et
al., 2008), our response-based method does not rely
on any orthographic features such as cognates or
words shared across languages. It is a pure statis-
tical method that only relies on word distributions
over a multilingual corpus. Based on these distribu-
tions, it performs the initial shallow semantic analy-
sis of the corpus by means of a multilingual prob-
abilistic model. The method then builds, via the
concept of semantic word responding, a language-
113
independent semantic space spanned by all vocabu-
lary words/responses in both languages. That makes
the method portable to distant language pairs. How-
ever, for similar languages, including more evidence
such as orthographic clues might lead to further in-
crease in scores, but we leave that for future work.
6 Conclusion
We have proposed a new statistical approach to iden-
tifying semantically similar words across languages
that relies on the paradigm of semantic word re-
sponding previously defined in cognitive science.
The proposed approach is robust and does not make
any additional language-pair dependent assumptions
(e.g., it does not rely on a seed lexicon, orthographic
clues or predefined concept categories). That effec-
tively makes it applicable to any language pair. Our
experiments on the task of bilingual lexicon extrac-
tion for a variety of language pairs have proved that
the response-based approach is more robust and out-
performs the methods that operate in the semantic
space of latent concepts (e.g., cross-lingual topics)
directly.
Acknowledgments
We would like to thank Steven Bethard and the
anonymous reviewers for their useful suggestions.
This research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund,
KU Leuven, Belgium.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
NAACL-HLT, pages 19?27.
Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsujii.
2010. Robust measurement and comparison of context
similarity for finding translation pairs. In Proceedings
of COLING, pages 19?27.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In Proceedings of UAI, pages 27?34.
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of SI-
GIR, pages 84?91.
A. Bhattacharyya. 1943. On a measure of divergence be-
tween two statistical populations defined by their prob-
ability distributions. Bulletin of the Calcutta Mathe-
matical Society, 35:199?209.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of UAI, pages 75?82.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Sung-Hyuk Cha. 2007. Comprehensive survey on
distance/similarity measures between probability den-
sity functions. International Journal of Mathematical
Models and Methods in Applied Sciences, 1(4):300?
307.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of ACL, pages 407?412.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the Web using in-
terlingual topic modeling. In CIKM Workshop on So-
cial Web Search and Mining (SWSM), pages 57?64.
Herve? De?jean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of COLING, pages 1?7.
Georgiana Dinu and Mirella Lapata. 2010. Topic models
for meaning similarity in context. In Proceedings of
COLING, pages 250?258.
Susan T. Dumais, Thomas K. Landauer, and Michael
Littman. 1996. Automatic cross-linguistic informa-
tion retrieval using Latent Semantic Indexing. In Pro-
ceedings of the SIGIR Workshop on Cross-Linguistic
Information Retrieval, pages 16?23.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING, pages 414?420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of ACL, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
114
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proceedings of EMNLP, pages 1192?1201.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proceedings of SIGIR, pages 50?57.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Proceedings of ECIR, pages 444?456.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of ACL, pages 247?256.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition, pages
9?16.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit, pages 79?86.
Thomas K. Landauer and Susan T. Dumais. 1997. Solu-
tions to Plato?s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
COLING, pages 617?625.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of ACL, pages 25?32.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management, 41:523?547.
Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14(2):178?203.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of EMNLP,
pages 880?889.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of ACL, pages 664?671.
Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-
nis. 2000. What is free association and what does it
measure? Memory and Cognition, 28:887?899.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of WWW, pages 1155?1156.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2011. Cross lingual text classification by mining mul-
tilingual topics from Wikipedia. In Proceedings of
WSDM, pages 375?384.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247?266.
John C. Platt, Kristina Toutanova, and Wen-Tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of EMNLP,
pages 251?261.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of ACL, pages 1327?1335.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Joseph Reisinger and Raymond J. Mooney. 2010. A
mixture model with sharing for lexical semantics. In
Proceedings of EMNLP, pages 1173?1182.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nel-
son. 2004. Word association spaces for predicting se-
mantic similarity effects in episodic memory. In Ex-
perimental Cognitive Psychology and Its Applications,
pages 237?249.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of
EMNLP, pages 24?36.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC, pages 77?
82.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from comparable
corpora using latent topic models. In Proceedings of
ACL, pages 479?484.
115
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceedings
of ACL, pages 1128?1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of ACL, pages 55?
63.
116
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271?276,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Model-Portability Experiments for Textual Temporal Analysis 
Oleksandr Kolomiyets, Steven Bethard and Marie-Francine Moens Department of Computer Science Katholieke Universiteit Leuven Celestijnenlaan 200A, Heverlee, 3001, Belgium {oleksandr.kolomiyets, steven.bethard, sien.moens}@cs.kuleuven.be  
 
 
Abstract 
We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substan-tial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alne never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.  
1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a cru-cial first step for applications like question answer-ing that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time 
normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al, 2010). Many researchers com-peted in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al, 2005; Ahn et al, 2007; Poveda et al, 2007; Str?tgen and Gertz 2010; Llorens et al, 2010), and achieving F1 measures as high as 0.86 for recog-nizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire.  Thus we know little about how well time expression recognition systems generalize to other sorts of text. We there-fore take a state-of-the-art time recognizer and eva-luate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expres-sions than are available explicitly in the newswire training data. We therefore introduce a semi-supervised approach for expanding the training data, where we take words from temporal expres-sions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via Word-Net, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised mod-el on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 
271
2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Sur-deanu et al, 2006). The most relevant research to our work here is that of (Poveda et al, 2009), which investigated a semi-supervised approach to time expression rec-ognition. They begin by selecting 100 time expres-sions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding to-kens, parts-of-speech, syntactic chunks etc.) and then search for new seeds given their patterns. The patterns resulting from this iterative process achieve F1 scores of up to 0.604 on the test half of the Automatic Content Extraction corpus. Our approach is quite different from that of (Po-veda et al, 2009) ? we use our training corpus for learning a supervised model rather than for se-lecting high precision seeds, we generate addi-tional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010).  3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: ? The head word of the phrase ? The part-of-speech tag of the head word ? All tokens and part-of-speech tags in the phrase as a bag of words 
? The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30  ? The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 ? The concatenated string of the syntactic types of the children of the phrase in the parse tree ? The depth in the parse tree  3.2 Lexical Resources for Bootstrapping Sparsity of annotated corpora is the biggest chal-lenge for any supervised machine learning tech-nique and especially for porting the trained models onto other domains. To overcome this problem we hypothesize that knowledge of semantically similar words, like temporal triggers, could be found by associating words that do not occur in the training set to similar words that do occur in the training set. Furthermore, we would like to learn these similarities automatically to be independent of knowledge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) ? a language model that learns from an unlabeled corpus how to pro-vide a weighted set of synonyms for words in con-text. The LWLM model is trained on the Reuters news article corpus of 80 million words.  WordNet (Miller, 1995) is another resource for synonyms widely used in research and applications of natural language processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense. For each synset in WordNet there is a collection of other ?sister? syn-sets, called coordinate terms, which are topologi-cally located under the same hypernym.  3.3 Bootstrapping Strategies Having a list of synonyms for each token in the sentence, we can replace one of the original tokens by its synonym while still mostly preserving the sentence semantics. We choose to replace just the headword, under the assumption that since tempo-ral trigger words usually occur at the headword position, adding alternative synonyms for the headword should allow our model to learn tempo-ral triggers that did not appear in the training data.  
272
We designed the following bootstrapping strate-gies for generating new temporal expressions: ? LWLM: the phrasal head is replaced by one of the LWLM synonyms. ? WordNet 1st Sense: Synonyms and coordinate terms for the most common sense of the phrasal head are selected and used for generat-ing new examples of time expressions. ? WordNet Pseudo-Lesk: The synset for the phrasal head is selected as having the largest intersection between the synset?s words and the LWLM synonyms. Then, synonyms and coordinate terms are used for generating new examples of time expressions. ? LWLM+WordNet: The intersection of the LWLM synonyms and the WordNet synset found by pseudo-Lesk are used. In this way for every annotated time expression we generate n new examples (n?[1,10]) and use them for training bootstrapped classification models.  4 Experimental Setup The tested model is trained on the official Tem-pEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens. For testing the portability of the model to other domains we anno-tated two small target domain document collec-tions with TIMEX3 tags. The first corpus is 12 Reuters news articles from the Reuters corpus 
(Lewis et al, 2004), containing 2960 total tokens and 240 annotated TIMEX3 tokens (inter-annotator agreement 0.909 F1-score). The second corpus is the Wikipedia article for Barak Obama (http://en.wikipedia.org/wiki/Obama), containing 7029 total tokens and 512 annotated TIMEX3 to-kens (inter-annotator agreement 0.901 F1-score). The basic TempEval model is evaluated on the source domain (TempEval 2010 evaluation set ? 9599 tokens in total and 269 TIMEX3 annotated tokens) and target domain data (Reuters and Wikipedia) using the TempEval 2010 evaluation metrics. Since porting the model onto other do-mains usually causes a performance drop, our ex-periments are focused on improving the results by employing different bootstrapping strategies1. 5 Results The recognition performance of the model is re-ported in Table 1 (column ?Basic TempEval Mod-el?) for the source and the target domains. The basic TempEval model itself achieves F1-score of 0.834 on the official TempEval 2010 evaluation corpus and has a potential rank 8 among 15 par-ticipated systems. The top seven TempEval-2 sys-tems achieved F1-score between 0.83 and 0.86.                                                            1 The annotated datasets are available at http://www.cs.kuleuven.be/groups/liir/software.php 
Bootstrapped Models   Basic TempEval Model LWLM WordNet 1st Sense WordNet Pseudo-Lesk LWLM+ WordNet # Syn 0 1 1 1 2 P 0.916 0.865 0.881 0.894 0.857 R 0.770 0.807 0.773 0.781 0.830 TempEval 2010 F1 0.834 0.835 0.824 0.833 0.829 # Syn 0 5 7 6 4 P 0.896 0.841 0.820 0.839 0.860 R 0.679 0.812 0.721 0.717 0.742 Reuters F1 0.773 0.826 0.767 0.773 0.796 # Syn 0 3 1 6 5 P 0.959 0.924 0.922 0.909 0.913 R 0.770 0.830 0.781 0.820 0.844 Wikipedia F1 0.859 0.874 0.858 0.862 0.877 Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters and Wikipedia) domains. Bootstrapped models were asked to generate between one and ten additional train-ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem-pEval Model are marked in bold. 
273
However, this model does not port well to the Reuters corpus (0.773 vs. 0.834 F1-score). For the Wikipedia-based corpus, the basic TempEval mod-el actually performs a little better than on the source domain (0.859 vs. 0.834 F1-score). Four bootstrapping strategies were proposed and evaluated. Table 1 shows the maximum F1 score achieved by each of these strategies, along with the number of generated synonyms (between one and ten) at which this maximum was achieved. None of the bootstrapped models outperformed the basic TempEval model on the TempEval 2010 evalua-tion data, and the WordNet 1st Sense strategy and the WordNet Pseudo-Lesk strategy never outper-formed the basic TempEval model on any corpus. However, for the Reuters and Wikipedia cor-pora, the LWLM and LWLM+WordNet bootstrap-ping strategies outperformed the basic TempEval model. The LWLM strategy gives a large boost to model performance on the Reuters corpus from 0.773 up to 0.826 (a 23.3% error reduction) when using the first 5 synonyms. This puts performance on Reuters near performance on the TempEval domain from which the model was trained (0.834). This suggests that the (Reuters-trained) LWLM is finding exactly the right kinds of synonyms: those that were not originally present in the TempEval data but are present in the Reuters test data. On the Wikipedia corpus, the LWLM bootstrapping strat-egy results in a moderate boost, from 0.859 up to 0.874 (a 10.6% error reduction) when using the first three synonyms. Figure 1 shows that using more synonyms with this strategy drops perform-
ance on the Wikipedia corpus back down to the level of the basic TempEval model. The LWLM+WordNet strategy gives a moderate boost on the Reuters corpus from 0.773 up to 0.796 (a 10.1% error reduction) when four synonyms are used. Figure 2 shows that using six or more syno-nyms drops this performance back to just above the basic TempEval model. On the Wikipedia corpus, the LWLM+WordNet strategy results in a moder-ate boost, from 0.859 up to 0.877 (a 12.8% error reduction), with five synonyms. Using additional synonyms results in a small decline in perform-ance, though even with ten synonyms, the per-formance is better than the basic TempEval model. In general, the LWLM strategy gives the best performance, while the LWLM+WordNet strategy is less sensitive to the exact number of synonyms used when expanding the training data. 6 TempEval Error Analysis We were curious why synonym-based boot-strapping did not improve performance on the source-domain TempEval 2010 data. An error analysis suggested that some time expressions might have been left unannotated by the human annotators. Two of the authors re-annotated the TempEval evaluation data, finding inter-annotator agreement of 0.912 F1-score with each other, but only 0.868 and 0.887 F1-score with the TempEval annotators, primarily due to unannotated time ex-pressions such as 23-year, a few days and third-quarter. 
 Figure 1: F1 score of the LWLM bootstrapping strat-egy, generating from zero to ten additional training examples per instance. 
 Figure 2: F1 score of the LWLM+WordNet bootstrap-ping strategy, generating from zero to ten additional training examples per instance. 
274
Using this re-annotated TempEval 2010 data2, we re-evaluated the proposed bootstrapping tech-niques. Figure 3 and Figure 4 compare perform-ance on the original TempEval data to performance on the re-annotated version. We now see the same trends for the TempEval data as were observed for the Reuters and Wikipedia corpora: using a small number of synonyms from the LWLM to generate new training examples leads to performance gains. The LWLM bootstrapping model using the first synonym achieves 0.861 F1 score, a 22.8% error reduction over the baseline of 0.820 F1 score. 7 Discussion and Conclusions We have presented model-portability experiments on time expression recognition with a number of bootstrapping strategies. These bootstrapping strat-egies generate additional training examples by substituting temporal expression words with poten-tial synonyms from two sources: WordNet and the Latent Word Language Model (LWLM). Bootstrapping with LWLM synonyms provides a large boost for Reuters data and TempEval data and a decent boost for Wikipedia data when the top few synonyms are used. Additional synonyms do not help, probably because they are too newswire-specific: both the contexts from the TempEval training data and the synonyms from the Reuters-trained LWLM come from newswire text, so the 
                                                           2 Available at http://www.cs.kuleuven.be/groups/liir/software.php 
lower synonyms are probably more domain-specific. Intersecting the synonyms generated by the LWLM and by WordNet moderates the LWLM, making the bootstrapping strategy less sensitive to the exact number of synonyms used. However, while the intersected model performs as well as the LWLM model on Wikipedia, the gains over the non-bootstrapped model on Reuters and TempEval data are smaller. Overall, our results show that when porting time expression recognition models to other domains, a performance drop can be avoided by synonym-based bootstrapping. Future work will focus on using synonym-based expansion in the contexts (not just the time expressions headwords), and on incorporating contextual information and syntactic transformations. Acknowledgments This work has been funded by the Flemish gov-ernment as a part of the project AMASS++ (Ad-vanced Multimedia Alignment and Structured Summarization) (Grant: IWT-SBO-060051). References David Ahn, Joris van Rantwijk, and Maarten de Rijke. 2007. A Cascaded Machine Learning Approach to Interpreting Temporal Expressions. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Lin-guistics (NAACL-HLT 2007). 
 Figure 3: F1 score of the LWLM bootstrapping strat-egy, comparing performance on the original TempEval data to the re-annotated version. 
 Figure 4: F1 score of the LWLM+WordNet bootstrap-ping strategy, comparing performance on the original TempEval data to the re-annotated version. 
275
Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceed-ings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 100?110, College Park, MD. ACL. Koen Deschacht and Marie-Francine Moens. 2009. Us-ing the Latent Words Language Model for Semi-Supervised Semantic Role Labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In Proceedings of the 16th Conference on Computa-tional Linguistics, pp. 466?471. Kadri Hacioglu, Ying Chen, and Benjamin Douglas 2005. Automatic Time Expression Labeling for Eng-lish and Chinese Text. In Gelbukh, A. (ed.) CICLing 2005. LNCS, vol. 3406, pp. 548?559. Springer, Hei-delberg. Oleksandr Kolomiyets, Marie-Francine Moens. 2010. KUL: Recognition and Normalization of Temporal Expressions. In Proceedings of SemEval-2 5th Work-shop on Semantic Evaluation. pp. 325-328. Uppsala, Sweden. ACL. David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. Machine Learning Re-search. 5: 361-397 Inderjeet Mani, and George Wilson. 2000. Robust Tem-poral Processing of News. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pp. 69-76, Morristown, NJ. ACL. George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41. Matteo Negri, and Luca Marseglia. 2004. Recognition and Normalization of Time Expressions: ITC-irst at TERN 2004. Technical Report, ITC-irst, Trento. Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval 2. In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, pp. 284?291, Uppsala, Sweden. ACL. Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2007. A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English. In Proceedings of the International Sym-posium on Temporal Representation and Reasoning, pp. 141-149. 
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2009. An Analysis of Bootstrapping for the Recognition of Temporal Expressions. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pp. 49-57, Stroudsburg, PA, USA. ACL. Jannik Str?tgen and Michael Gertz. 2010. HeidelTime: High Quality Rule-Based Extraction and Normaliza-tion of Temporal Expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 321?324, Uppsala, Sweden. ACL.  Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Informa-tion Extraction Patterns. In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006). ACL. Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 57?62, Upp-sala, Sweden. ACL. 	 ?David Yarowsky. 1995. Unsupervised word sense dis-ambiguation rivaling supervised methods. In Pro-ceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189?196, Cambridge, MA. ACL.   
276
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479?484,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Word Translations from Comparable Corpora Using Latent
Topic Models
Ivan Vulic?, Wim De Smet and Marie-Francine Moens
Department of Computer Science
K.U. Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,wim.desmet,sien.moens}@cs.kuleuven.be
Abstract
A topic model outputs a set of multinomial
distributions over words for each topic. In
this paper, we investigate the value of bilin-
gual topic models, i.e., a bilingual Latent
Dirichlet Allocation model for finding trans-
lations of terms in comparable corpora with-
out using any linguistic resources. Experi-
ments on a document-aligned English-Italian
Wikipedia corpus confirm that the developed
methods which only use knowledge from
word-topic distributions outperform methods
based on similarity measures in the original
word-document space. The best results, ob-
tained by combining knowledge from word-
topic distributions with similarity measures in
the original space, are also reported.
1 Introduction
Generative models for documents such as Latent
Dirichlet Allocation (LDA) (Blei et al, 2003) are
based upon the idea that latent variables exist which
determine how words in documents might be gener-
ated. Fitting a generative model means finding the
best set of those latent variables in order to explain
the observed data. Within that setting, documents
are observed as mixtures of latent topics, where top-
ics are probability distributions over words.
Our goal is to model and test the capability of
probabilistic topic models to identify potential trans-
lations from document-aligned text collections. A
representative example of such a comparable text
collection is Wikipedia, where one may observe arti-
cles discussing the same topic, but strongly varying
in style, length and even vocabulary, while still shar-
ing a certain amount of main concepts (or topics).
We try to establish a connection between such latent
topics and an idea known as the distributional hy-
pothesis (Harris, 1954) - words with a similar mean-
ing are often used in similar contexts.
Besides the obvious context of direct co-
occurrence, we believe that topic models are an ad-
ditional source of knowledge which might be used
to improve results in the quest for translation can-
didates extracted without the availability of a trans-
lation dictionary and linguistic knowledge. We de-
signed several methods, all derived from the core
idea of using word distributions over topics as an
extra source of contextual knowledge. Two words
are potential translation candidates if they are often
present in the same cross-lingual topics and not ob-
served in other cross-lingual topics. In other words,
a wordw2 from a target language is a potential trans-
lation candidate for a word w1 from a source lan-
guage, if the distribution of w2 over the target lan-
guage topics is similar to the distribution of w1 over
the source language topics.
The remainder of this paper is structured as fol-
lows. Section 2 describes related work, focusing on
previous attempts to use topic models to recognize
potential translations. Section 3 provides a short
summary of the BiLDA model used in the experi-
ments, presents all main ideas behind our work and
gives an overview and a theoretical background of
the methods. Section 4 evaluates and discusses ini-
tial results. Finally, section 5 proposes several ex-
tensions and gives a summary of the current work.
479
2 Related Work
The idea to acquire translation candidates based
on comparable and unrelated corpora comes from
(Rapp, 1995). Similar approaches are described in
(Diab and Finch, 2000), (Koehn and Knight, 2002)
and (Gaussier et al, 2004). These methods need
an initial lexicon of translations, cognates or simi-
lar words which are then used to acquire additional
translations of the context words. In contrast, our
method does not bootstrap on language pairs that
share morphology, cognates or similar words.
Some attempts of obtaining translations using
cross-lingual topic models have been made in the
last few years, but they are model-dependent and do
not provide a general environment to adapt and ap-
ply other topic models for the task of finding trans-
lation correspondences. (Ni et al, 2009) have de-
signed a probabilistic topic model that fits Wikipedia
data, but they did not use their models to obtain po-
tential translations. (Mimno et al, 2009) retrieve
a list of potential translations simply by selecting
a small number N of the most probable words in
both languages and then add the Cartesian product
of these sets for every topic to a set of candidate
translations. This approach is straightforward, but it
does not catch the structure of the latent topic space
completely.
Another model proposed in (Boyd-Graber and
Blei, 2009) builds topics as distributions over bilin-
gual matchings where matching priors may come
from different initial evidences such as a machine
readable dictionary, edit distance, or the Point-
wise Mutual Information (PMI) statistic scores from
available parallel corpora. The main shortcoming is
that it introduces external knowledge for matching
priors, suffers from overfitting and uses a restricted
vocabulary.
3 Methodology
In this section we present the topic model we used
in our experiments and outline the formal framework
within which three different approaches for acquir-
ing potential word translations were built.
3.1 Bilingual LDA
The topic model we use is a bilingual extension
of a standard LDA model, called bilingual LDA
(BiLDA), which has been presented in (Ni et al,
2009; Mimno et al, 2009; De Smet and Moens,
2009). As the name suggests, it is an extension
of the basic LDA model, taking into account bilin-
guality and designed for parallel document pairs.
We test its performance on a collection of compara-
ble texts which are document-aligned and therefore
share their topics. BiLDA takes advantage of the
document alignment by using a single variable that
contains the topic distribution ?, that is language-
independent by assumption and shared by the paired
bilingual comparable documents. Topics for each
document are sampled from ?, from which the words
are sampled in conjugation with the vocabulary dis-
tribution ? (for language S) and ? (for language
T). Algorithm 3.1 summarizes the generative story,
while figure 1 shows the plate model.
Algorithm
3.1: GENERATIVE STORY FOR BILDA()
for each document pair dj
do
?
???????
???????
for each word position i ? djS
do
{
sample zSji ?Mult(?)
sample wSji ?Mult(?, zSji)
for each word position i ? djT
do
{
sample zTji ?Mult(?)
sample wTji ?Mult(?, zTji)
D
N M
?
?
?
?
?
zSji zTji
wSji wTji
Figure 1: The standard bilingual LDA model
Having one common ? for both of the related doc-
uments implies parallelism between the texts. This
observation does not completely hold for compara-
ble corpora with topically aligned texts. To train the
480
model we use Gibbs sampling, similar to the sam-
pling method for monolingual LDA, with param-
eters ? and ? set to 50/K and 0.01 respectively,
where K denotes the number of topics. After the
training we end up with a set of ? and ? word-topic
probability distributions that are used for the calcu-
lations of the word associations.
If we are given a source vocabulary WS , then the
distribution ? of sampling a new token as word wi ?
WS from a topic zk can be obtained as follows:
P (wi|zk) = ?k,i =
n(wi)k + ?
?|WS |
j=1 n
(wj)
k +WS?
(1)
where, for a word wi and a topic zk, n(wi)k denotes
the total number of times that the topic zk is assigned
to the word wi from the vocabulary WS , ? is a sym-
metric Dirichlet prior,
?|WS |
j=1 n
(wj)
k is the total num-
ber of words assigned to the topic zk, and |WS | is
the total number of distinct words in the vocabulary.
The formula for a set of ? word-topic probability
distributions for the target side of a corpus is com-
puted in an analogical manner.
3.2 Main Framework
Once we derive a shared set of topics along with
language-specific distributions of words over topics,
it is possible to use them for the computation of the
similarity between words in different languages.
3.2.1 KL Method
The similarity between a source word w1 and a tar-
get word w2 is measured by the extent to which
they share the same topics, i.e., by the extent that
their conditional topic distributions are similar. One
way of expressing similarity is the Kullback-Leibler
(KL) divergence, already used in a monolingual set-
ting in (Steyvers and Griffiths, 2007). The simi-
larity between two words is based on the similar-
ity between ?(1) and ?(2), the similarity of con-
ditional topic distributions for words w1 and w2,
where ?(1) = P (Z|w1)1 and ?(2) = P (Z|w2). We
have to calculate the probabilities P (zj |wi), which
describe a probability that a given word is assigned
to a particular topic. If we apply Bayes? rule, we
get P (Z|w) = P (w|Z)P (Z)P (w) , where P (Z) and P (w)
1P (Z|w1) refers to a set of all conditional topic distributions
P (zj |w1)
are prior distributions for topics and words respec-
tively. P (Z) is a uniform distribution for the BiLDA
model, whereas this assumption clearly does not
hold for topic models with a non-uniform topic prior.
P (w) is given by P (w) = P (w|Z)P (Z). If the
assumption of uniformity for P (Z) holds, we can
write:
P (zj |wi) ?
P (wi|zj)
Norm?
= ?j,iNorm?
(2)
for an English word wi, and:
P (zj |wi) ?
P (wi|zj)
Norm?
= ?j,iNorm?
(3)
for a French word wi, where Norm? denotes the
normalization factor
?K
j=1 P (wi|zj), i.e., the sum
of all probabilities ? (or probabilities ? forNorm?)
for the currently observed word wi.
We can then calculate the KL divergence as fol-
lows:
KL(?(1), ?(2)) ?
K?
j=1
?j,1
Norm?
log ?j,1/Norm??j,2/Norm?
(4)
3.2.2 Cue Method
An alternative, more straightforward approach
(called the Cue method) tries to express similarity
between two words emphasizing the associative re-
lation between two words in a more natural way. It
models the probability P (w2|w1), i.e., the probabil-
ity that a target word w2 will be generated as a re-
sponse to a cue source word w1. For the BiLDA
model we can write:
P (w2|w1) =
K?
j=1
P (w2|zj)P (zj |w1)
=
K?
j=1
?j,2
?j,1
Norm?
(5)
This conditioning automatically compromises be-
tween word frequency and semantic relatedness
(Griffiths et al, 2007), since higher frequency words
tend to have higher probabilities across all topics,
but the distribution over topics P (zj |w1) ensures
that semantically related topics dominate the sum.
481
3.2.3 TI Method
The last approach borrows an idea from information
retrieval and constructs word vectors over a shared
latent topic space. Values within vectors are the
TF-ITF (term frequency - inverse topic frequency)
scores which are calculated in a completely ana-
logical manner as the TF-IDF scores for the orig-
inal word-document space (Manning and Schu?tze,
1999). If we are given a source word wi, n(wi)k,S de-
notes the number of times the word wi is associated
with a source topic zk. Term frequency (TF) of the
source word wi for the source topic zk is given as:
TFi,k =
n(wi)k,S
?
wj?WS
n(wj)k,S
(6)
Inverse topical frequency (ITF) measures the gen-
eral importance of the source word wi across all
source topics. Rare words are given a higher im-
portance and thus they tend to be more descriptive
for a specific topic. The inverse topical frequency
for the source word wi is calculated as2:
ITFi = log
K
1 + |k : n(wi)k,S > 0|
(7)
The final TF-ITF score for the source wordwi and
the topic zk is given by TF?ITFi,k = TFi,k ?ITFi.
We calculate the TF-ITF scores for target words as-
sociated with target topics in an analogical man-
ner. Source and target words share the same K-
dimensional topical space, where K-dimensional
vectors consisting of the TF-ITF scores are built
for all words. The standard cosine similarity met-
ric is then used to find the most similar word vectors
from the target vocabulary for a source word vec-
tor. We name this method the TI method. For in-
stance, given a source word w1 represented by a K-
dimensional vector S1 and a target word w2 repre-
sented by a K-dimensional vector T 2, the similarity
between the two words is calculated as follows:
2Stronger association with a topic is modeled by setting a
higher threshold value in n(wi)k,S > threshold, where we have
chosen 0.
cos(w1, w2) =
?K
k=1 S1k ? T 2k?
?K
k=1 (S1k)
2 ?
?
?K
k=1 (T 2k )
2
(8)
4 Results and Discussion
As our training corpus, we use the English-Italian
Wikipedia corpus of 18, 898 document pairs, where
each aligned pair discusses the same subject. In or-
der to reduce data sparsity, we keep only lemmatized
noun forms for further analysis. Our Italian vocabu-
lary consists of 7, 160 nouns, while our English vo-
cabulary contains 9, 166 nouns. The subset of the
650 most frequent terms was used for testing. We
have used the Google Translate tool for evaluations.
As our baseline system, we use the cosine similar-
ity between Italian word vectors and English word
vectors with TF-IDF scores in the original word-
document space (Cos), with aligned documents.
Table 1 shows the Precision@1 scores (the per-
centage of words where the first word from the list
of translations is the correct one) for all three ap-
proaches (KL, Cue and TI), for different number
of topics K. Although KL is designed specifically
to measure the similarity of two distributions, its re-
sults are significantly below those of the Cue and TI,
whose performances are comparable. Whereas the
latter two methods yield the highest results around
the 2, 000 topics mark, the performance of KL in-
creases linearly with the number of topics. This is
an undesirable result as good results are computa-
tionally hard to get.
We have also detected that we are able to boost
overall scores if we combine two methods. We have
opted for the two best methods (TI+Cue), where
overall score is calculated by Score =??ScoreCue+
ScoreTI .3 We also provide the results obtained by
linearly combining (with equal weights) the cosine
similarity between TF-ITF vectors with that between
TF-IDF vector (TI+Cos).
In a more lenient evaluation setting we employ the
mean reciprocal rank (MRR) (Voorhees, 1999). For
a source word w, rankw denotes the rank of its cor-
rect translation within the retrieved list of potential
translations. MRR is then defined as follows:
3The value of ? is empirically set to 10
482
K KL Cue TI TI+Cue TI+Cos
200 0.3015 0.1800 0.3169 0.2862 0.5369
500 0.2846 0.3338 0.3754 0.4000 0.5308
800 0.2969 0.4215 0.4523 0.4877 0.5631
1200 0.3246 0.5138 0.4969 0.5708 0.5985
1500 0.3323 0.5123 0.4938 0.5723 0.5908
1800 0.3569 0.5246 0.5154 0.5985 0.6123
2000 0.3954 0.5246 0.5385 0.6077 0.6046
2200 0.4185 0.5323 0.5169 0.5908 0.6015
2600 0.4292 0.4938 0.5185 0.5662 0.5907
3000 0.4354 0.4554 0.4923 0.5631 0.5953
3500 0.4585 0.4492 0.4785 0.5738 0.5785
Table 1: Precision@1 scores for the test subset of the IT-
EN Wikipedia corpus (baseline precision score: 0.5031)
MRR = 1
|V |
?
w?V
1
rankw
(9)
where V denotes the set of words used for evalu-
ation. We kept only the top 20 candidates from the
ranked list. Table 2 shows the MRR scores for the
same set of experiments.
K KL Cue TI TI+Cue TI+Cos
200 0.3569 0.2990 0.3868 0.4189 0.5899
500 0.3349 0.4331 0.4431 0.4965 0.5808
800 0.3490 0.5093 0.5215 0.5733 0.6173
1200 0.3773 0.5751 0.5618 0.6372 0.6514
1500 0.3865 0.5756 0.5562 0.6320 0.6435
1800 0.4169 0.5858 0.5802 0.6581 0.6583
2000 0.4561 0.5841 0.5914 0.6616 0.6548
2200 0.4686 0.5898 0.5753 0.6471 0.6523
2600 0.4763 0.5550 0.5710 0.6268 0.6416
3000 0.4848 0.5272 0.5572 0.6257 0.6465
3500 0.5022 0.5199 0.5450 0.6238 0.6310
Table 2: MRR scores for the test subset of the IT-EN
Wikipedia corpus (baseline MRR score: 0.5890)
Topic models have the ability to build clusters of
words which might not always co-occur together in
the same textual units and therefore add extra infor-
mation of potential relatedness. Although we have
presented results for a document-aligned corpus, the
framework is completely generic and applicable to
other topically related corpora.
Again, the KL method has the weakest perfor-
mance among the three methods based on the word-
topic distributions, while the other two methods
seem very useful when combined together or when
combined with the similarity measure used in the
original word-document space. We believe that the
results are in reality even higher than presented in
the paper, due to errors in the evaluation tool (e.g.,
the Italian word raggio is correctly translated as ray,
but Google Translate returns radius as the first trans-
lation candidate).
All proposed methods retrieve lists of semanti-
cally related words, where synonymy is not the only
semantic relation observed. Such lists provide com-
prehensible and useful contextual information in the
target language for the source word, even when the
correct translation candidate is missing, as might be
seen in table 3.
(1) romanzo (2) paesaggio (3) cavallo
(novel) (landscape) (horse)
writer tourist horse
novella painting stud
novellette landscape horseback
humorist local hoof
novelist visitor breed
essayist hut stamina
penchant draftsman luggage
formative tourism mare
foreword attraction riding
author vegetation pony
Table 3: Lists of the top 10 translation candidates, where
the correct translation is not found (column 1), lies hidden
lower in the list (2), and is retrieved as the first candidate
(3); K=2000; TI+Cue.
5 Conclusion
We have presented a generic, language-independent
framework for mining translations of words from
latent topic models. We have proven that topical
knowledge is useful and improves the quality of
word translations. The quality of translations de-
pends only on the quality of a topic model and its
ability to find latent relations between words. Our
next steps involve experiments with other topic mod-
els and other corpora, and combining this unsuper-
vised approach with other tools for lexicon extrac-
tion and synonymy detection from unrelated and
comparable corpora.
Acknowledgements
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund
K.U. Leuven, Belgium, and the Flemish SBO-IWT
project AMASS++ (SBO-IWT 0060051).
483
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ?09, pages 75?82.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the web using
interlingual topic modelling. In Proceedings of the
CIKM 2009 Workshop on Social Web Search and Min-
ing, pages 57?64.
Mona T. Diab and Steve Finch. 2000. A statistical trans-
lation model using comparable corpora. In Proceed-
ings of the 2000 Conference on Content-Based Multi-
media Information Access (RIAO), pages 1500?1508.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, pages
526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Zellig S. Harris. 1954. Distributional structure. In Word
10 (23), pages 146?162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition - Volume 9, ULA ?02, pages 9?16.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA, USA.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880?889.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of the 18th International World Wide Web
Conference, pages 1155?1156.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?95, pages 320?322.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of the Eighth TExt
Retrieval Conference (TREC-8).
484
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 88?97,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting Narrative Timelines as Temporal Dependency Structures
Oleksandr Kolomiyets
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Oleksandr.Kolomiyets@
cs.kuleuven.be
Steven Bethard
University of Colorado
Campus Box 594
Boulder, CO 80309, USA
Steven.Bethard@
colorado.edu
Marie-Francine Moens
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Sien.Moens@
cs.kuleuven.be
Abstract
We propose a new approach to characterizing
the timeline of a text: temporal dependency
structures, where all the events of a narrative
are linked via partial ordering relations like BE-
FORE, AFTER, OVERLAP and IDENTITY. We
annotate a corpus of children?s stories with tem-
poral dependency trees, achieving agreement
(Krippendorff?s Alpha) of 0.856 on the event
words, 0.822 on the links between events, and
of 0.700 on the ordering relation labels. We
compare two parsing models for temporal de-
pendency structures, and show that a determin-
istic non-projective dependency parser outper-
forms a graph-based maximum spanning tree
parser, achieving labeled attachment accuracy
of 0.647 and labeled tree edit distance of 0.596.
Our analysis of the dependency parser errors
gives some insights into future research direc-
tions.
1 Introduction
There has been much recent interest in identifying
events, times and their relations along the timeline,
from event and time ordering problems in the Temp-
Eval shared tasks (Verhagen et al, 2007; Verhagen
et al, 2010), to identifying time arguments of event
structures in the Automated Content Extraction pro-
gram (Linguistic Data Consortium, 2005; Gupta and
Ji, 2009), to timestamping event intervals in the
Knowledge Base Population shared task (Artiles et
al., 2011; Amigo? et al, 2011).
However, to date, this research has produced frag-
mented document timelines, because only specific
types of temporal relations in specific contexts have
been targeted. For example, the TempEval tasks only
looked at relations between events in the same or ad-
jacent sentences (Verhagen et al, 2007; Verhagen et
al., 2010), and the Automated Content Extraction pro-
gram only looked at time arguments for specific types
of events, like being born or transferring money.
In this article, we propose an approach to temporal
information extraction that identifies a single con-
nected timeline for a text. The temporal language
in a text often fails to specify a total ordering over
all the events, so we annotate the timelines as tem-
poral dependency structures, where each event is a
node in the dependency tree, and each edge between
nodes represents a temporal ordering relation such
as BEFORE, AFTER, OVERLAP or IDENTITY. We
construct an evaluation corpus by annotating such
temporal dependency trees over a set of children?s
stories. We then demonstrate how to train a time-
line extraction system based on dependency parsing
techniques instead of the pair-wise classification ap-
proaches typical of prior work.
The main contributions of this article are:
? We propose a new approach to characterizing
temporal structure via dependency trees.
? We produce an annotated corpus of temporal
dependency trees in children?s stories.
? We design a non-projective dependency parser
for inferring timelines from text.
The following sections first review some relevant
prior work, then describe the corpus annotation and
the dependency parsing algorithm, and finally present
our evaluation results.
88
2 Related Work
Much prior work on the annotation of temporal in-
formation has constructed corpora with incomplete
timelines. The TimeBank (Pustejovsky et al, 2003b;
Pustejovsky et al, 2003a) provided a corpus anno-
tated for all events and times, but temporal relations
were only annotated when the relation was judged to
be salient by the annotator. In the TempEval compe-
titions (Verhagen et al, 2007; Verhagen et al, 2010),
annotated texts were provided for a few different
event and time configurations, for example, an event
and a time in the same sentence, or two main-clause
events from adjacent sentences. Bethard et al (2007)
proposed to annotate temporal relations one syntactic
construction at a time, producing an initial corpus of
only verbal events linked to events in subordinated
clauses. One notable exception to this pattern of
incomplete timelines is the work of Bramsen et al
(2006) where temporal structures were annotated as
directed acyclic graphs. However they worked on a
much coarser granularity, annotating not the order-
ing between individual events, but between multi-
sentence segments of text.
In part because of the structure of the available
training corpora, most existing temporal informa-
tion extraction models formulate temporal linking
as a pair-wise classification task, where each pair
of events and/or times is examined and classified as
having a temporal relation or not. Early work on the
TimeBank took this approach (Boguraev and Ando,
2005), classifying relations between all events and
times within 64 tokens of each other. Most of the top-
performing systems in the TempEval competitions
also took this pair-wise classification approach for
both event-time and event-event temporal relations
(Bethard and Martin, 2007; Cheng et al, 2007; UzZa-
man and Allen, 2010; Llorens et al, 2010). Systems
have also tried to take advantage of more global in-
formation to ensure that the pair-wise classifications
satisfy temporal logic transitivity constraints, using
frameworks such as integer linear programming and
Markov logic networks (Bramsen et al, 2006; Cham-
bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz-
Zaman and Allen, 2010). Yet the basic approach is
still centered around pair-wise classifications, not the
complete temporal structure of a document.
Our work builds upon this prior research, both
improving the annotation approach to generate the
fully connected timeline of a story, and improving
the models for timeline extraction using dependency
parsing techniques. We use the annotation scheme
introduced in more detail in Bethard et. al. (2012),
which proposes to annotate temporal relations as de-
pendency links between head events and dependent
events. This annotation scheme addresses the issues
of incoherent and incomplete annotations by guaran-
teeing that all events in a plot are connected along
a single timeline. These connected timelines allow
us to design new models for timeline extraction in
which we jointly infer the temporal structure of the
text and the labeled temporal relations. We employ
methods from syntactic dependency parsing, adapt-
ing them to our task by including features typical of
temporal relation labeling models.
3 Corpus Annotation
The corpus of stories for children was drawn from the
fables collection of (McIntyre and Lapata, 2009)1 and
annotated as described in (Bethard et al, 2012). In
this section we illustrate the main annotation princi-
ples for coherent temporal annotation. As an example
story, consider:
Two Travellers were on the road together,
when a Bear suddenly appeared on the
scene. Before he observed them, one made
for a tree at the side of the road, and
climbed up into the branches and hid there.
The other was not so nimble as his compan-
ion; and, as he could not escape, he threw
himself on the ground and pretended to be
dead. . . [37.txt]
Figure 1 shows the temporal dependency structure
that we expect our annotators to identify in this story.
The annotators were provided with guidelines both
for which kinds of words should be identified as
events, and for which kinds of events should be
linked by temporal relations. For identifying event
words, the standard TimeML guidelines for anno-
tating events (Pustejovsky et al, 2003a) were aug-
mented with two additional guidelines:
1Data available at http://homepages.inf.ed.ac.
uk/s0233364/McIntyreLapata09/
89
Figure 1: Event timeline for the story of the Travellers and the Bear. Nodes are events and edges are temporal relations.
Edges denote temporal relations signaled by linguistic cues in the text. Temporal relations that can be inferred via
transitivity are not shown.
? Skip negated, modal or hypothetical events (e.g.
could not escape, dead in pretended to be dead).
? For phrasal events, select the single word that
best paraphrases the meaning (e.g. in used to
snap the event should be snap, in kept perfectly
still the event should be still).
For identifying the temporal dependencies (i.e. the
ordering relations between event words), the anno-
tators were instructed to link each event in the story
to a single nearby event, similar to what has been
observed in reading comprehension studies (Johnson-
Laird, 1980; Brewer and Lichtenstein, 1982). When
there were several reasonable nearby events to choose
from, the annotators were instructed to choose the
temporal relation that was easiest to infer from the
text (e.g. preferring relations with explicit cue words
like before). A set of six temporal relations was used:
BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDEN-
TITY or OVERLAP.
Two annotators annotated temporal dependency
structures in the first 100 fables of the McIntyre-
Lapata collection and measured inter-annotator agree-
ment by Krippendorff?s Alpha for nominal data (Krip-
pendorff, 2004; Hayes and Krippendorff, 2007). For
the resulting annotated corpus annotators achieved
Alpha of 0.856 on the event words, 0.822 on the links
between events, and of 0.700 on the ordering rela-
tion labels. Thus, we concluded that the temporal
dependency annotation paradigm was reliable, and
the resulting corpus of 100 fables2 could be used to
2Available from http://www.bethard.info/data/
fables-100-temporal-dependency.xml
train a temporal dependency parsing model.
4 Parsing Models
We consider two different approaches to learning a
temporal dependency parser: a shift-reduce model
(Nivre, 2008) and a graph-based model (McDonald
et al, 2005). Both models take as input a sequence
of event words and produce as output a tree structure
where the events are linked via temporal relations.
Formally, a parsing model is a function (W ? ?)
where W = w1w2 . . . wn is a sequence of event
words, and pi ? ? is a dependency tree pi = (V,E)
where:
? V = W ? {Root}, that is, the vertex set of the
graph is the set of words in W plus an artificial
root node.
? E = {(wh, r, wd) : wh ? V,wd ? V, r ? R =
{BEFORE, AFTER, INCLUDES, IS INCLUDED,
IDENTITY, OVERLAP}}, that is, in the edge set
of the graph, each edge is a link between a de-
pendent word and its head word, labeled with a
temporal relation.
? (wh, r, wd) ? E =? wd 6= Root, that is, the
artificial root node has no head.
? (wh, r, wd) ? E =? ((w?h, r
?, wd) ? E =?
wh = w?h? r = r
?), that is, for every node there
is at most one head and one relation label.
? E contains no (non-empty) subset of arcs
(wh, ri, wi), (wi, rj , wj), . . . , (wk, rl, wh), that
is, there are no cycles in the graph.
90
SHIFT Move all of L2 and the head of Q onto L1
([a1 . . . ai], [b1 . . . bj ], [wkwk+1 . . .], E) ? ([a1 . . . aib1 . . . bjwk], [], [wk+1 . . .], E)
NO-ARC Move the head of L1 to the head of L2
([a1 . . . aiai+1], [b1 . . . bj ], Q,E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], Q,E)
LEFT-ARC Create a relation where the head of L1 depends on the head of Q
Not applicable if ai+1 is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (wk, r, ai+1)
RIGHT-ARC Create a relation where the head of Q depends on the head of L1
Not applicable if wk is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (ai+1, r, wk)
Table 1: Transition system for Covington-style shift-reduce dependency parsers.
4.1 Shift-Reduce Parsing Model
Shift-reduce dependency parsers start with an input
queue of unlinked words, and link them into a tree
by repeatedly choosing and performing actions like
shifting a node to a stack, or popping two nodes from
the stack and linking them. Shift-reduce parsers are
typically defined in terms of configurations and a tran-
sition system, where the configurations describe the
current internal state of the parser, and the transition
system describes how to get from one state to another.
Formally, a deterministic shift-reduce dependency
parser is defined as (C, T,CF , INIT, TREE) where:
? C is the set of possible parser configurations ci
? T ? (C ? C) is the set of transitions ti from
one configuration cj to another cj+1 allowed by
the parser
? INIT ? (W ? C) is a function from the input
words to an initial parser configuration
? CF ? C are the set of final parser configura-
tions cF where the parser is allowed to terminate
? TREE ? (CF ? ?) is a function that extracts a
dependency tree pi from a final parser state cF
Given this formalism and an oracle o ? (C ? T ),
which can choose a transition given the current con-
figuration of the parser, dependency parsing can be
accomplished by Algorithm 1. For temporal depen-
dency parsing, we adopt the Covington set of transi-
tions (Covington, 2001) as it allows for parsing the
non-projective trees, which may also contain ?cross-
ing? edges, that occasionally occur in our annotated
corpus. Our parser is therefore defined as:
Algorithm 1 Deterministic parsing with an oracle.
c? INIT(W )
while c /? CF do
t? o(c)
c? t(c)
end while
return TREE(c)
? c = (L1, L2, Q,E) is a parser configuration,
where L1 and L2 are lists for temporary storage,
Q is the queue of input words, and E is the set
of identified edges of the dependency tree.
? T = {SHIFT,NO-ARC,LEFT-ARC,RIGHT-ARC}
is the set of transitions described in Table 1.
? INIT(W ) = ([Root], [], [w1, w2, . . . , wn], ?)
puts all input words on the queue and the ar-
tificial root on L1.
? CF = {(L1, L2, Q,E) ? C : L1 = {W ?
{Root}}, L2 = Q = ?} accepts final states
where the input words have been moved off of
the queue and lists and into the edges in E.
? TREE((L1, L2, Q,E)) = (W ?{Root}, E) ex-
tracts the final dependency tree.
The oracle o is typically defined as a machine learn-
ing classifier, which characterizes a parser configu-
ration c in terms of a set of features. For temporal
dependency parsing, we learn a Support Vector Ma-
chine classifier (Yamada and Matsumoto, 2003) using
the features described in Section 5.
4.2 Graph-Based Parsing Model
One shortcoming of the shift-reduce dependency
parsing approach is that each transition decision
91
Figure 2: A setting for the graph-based parsing model: an initial dense graph G (left) with edge scores SCORE(e). The
resulting dependency tree as a spanning tree with the highest score over the edges (right).
made by the model is final, and cannot be revisited to
search for more globally optimal trees. Graph-based
models are an alternative dependency parsing model,
which assembles a graph with weighted edges be-
tween all pairs of words, and selects the tree-shaped
subset of this graph that gives the highest total score
(Fig. 2). Formally, a graph-based parser follows
Algorithm 2, where:
? W ? = W ? {Root}
? SCORE ? ((W ??R?W ) ? <) is a function
for scoring edges
? SPANNINGTREE is a function for selecting a
subset of edges that is a tree that spans over all
the nodes of the graph.
Algorithm 2 Graph-based dependency parsing
E ? {(e, SCORE(e)) : e ? (W ??R?W ))}
G? (W ?, E)
return SPANNINGTREE(G)
The SPANNINGTREE function is usually defined
using one of the efficient search techniques for find-
ing a maximum spanning tree. For temporal depen-
dency parsing, we use the Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967) which
solves this problem by iteratively selecting the edge
with the highest weight and removing edges that
would create cycles. The result is the globally op-
timal maximum spanning tree for the graph (Geor-
giadis, 2003).
The SCORE function is typically defined as a ma-
chine learning model that scores an edge based on a
set of features. For temporal dependency parsing, we
learn a model to predict edge scores via the Margin
Infused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003; Crammer et al, 2006) using the set of
features defined in Section 5.
5 Feature Design
The proposed parsing algorithms both rely on ma-
chine learning methods. The shift-reduce parser
(SRP) trains a machine learning classifier as the or-
acle o ? (C ? T ) to predict a transition t from a
parser configuration c = (L1, L2, Q,E), using node
features such as the heads of L1, L2 and Q, and
edge features from the already predicted temporal
relations in E. The graph-based maximum spanning
tree (MST) parser trains a machine learning model
to predict SCORE(e) for an edge e = (wi, rj , wk),
using features of the nodes wi and wk. The full set
of features proposed for both parsing models, de-
rived from the state-of-the-art systems for temporal
relation labeling, is presented in Table 2. Note that
both models share features that look at the nodes,
while only the shift-reduce parser has features for
previously classified edges.
6 Evaluations
Evaluations were performed using 10-fold cross-
validation on the fables annotated in Section 3. The
corpus contains 100 fables, a total of 14,279 tokens
and a total of 1136 annotated temporal relations. As
92
Feature SRP MST
Word
?? ??
Lemma
?? ??
Part of speech (POS) tag
?? ??
Suffixes
?? ??
Syntactically governing verb
?? ??
Governing verb lemma
?? ??
Governing verb POS tag
?? ??
Governing verb POS suffixes
?? ??
Prepositional phrase occurrence
?? ??
Dominated by auxiliary verb?
?? ??
Dominated by modal verb?
?? ??
Temporal signal word is nearby?
?? ??
Head word lemma
?? ??
Temporal relation labels of ai and its
leftmost and rightmost dependents
?
Temporal relation labels of ai?1?s
leftmost and rightmost dependents
?
Temporal relation labels of b1 and its
leftmost and rightmost dependents
?
Table 2: Features for the shift-reduce parser (SRP) and the
graph-based maximum spanning tree (MST) parser. The?? features are extracted from the heads of L1, L2 and Q
for SRP and from each node of the edge for MST.
only 40 instances of OVERLAP relations were an-
notated when neither INCLUDES nor IS INCLUDED
label matched, for evaluation purposes all instances
of these relations were merged into the temporally
coarse OVERLAP relation. Thus, the total number of
OVERLAP relations in the corpus grew from 40 to
258 annotations in total.
To evaluate the parsing models (SRP and MST)
we proposed two baselines. Both are based on the
assumption of linear temporal structures of narratives
as the temporal ordering process that was evidenced
by studies in human text rewriting (Hickmann, 2003).
The proposed baselines are:
? LinearSeq: A model that assumes all events
occur in the order they are written, adding links
between each pair of adjacent events, and label-
ing all links with the relation BEFORE.
? ClassifySeq: A model that links each pair of
adjacent events, but trains a pair-wise classifier
to predict the relation label for each pair. The
classifier is a support vector machine trained us-
ing the same features as the MST parser. This is
an approximation of prior work, where the pairs
of events to classify with a temporal relation
were given as an input to the system. (Note that
Section 6.2 will show that for our corpus, apply-
ing the model only to adjacent pairs of events
is quite competitive for just getting the basic
unlabeled link structure right.)
The Shift-Reduce parser (SRP; Section 4.1) and the
graph-based, maximum spanning tree parser (MST;
Section 4.2) are compared to these baselines.
6.1 Evaluation Criteria and Metrics
Model performance was evaluated using standard
evaluation criteria for parser evaluations:
Unlabeled Attachment Score (UAS) The fraction
of events whose head events were correctly predicted.
This measures whether the correct pairs of events
were linked, but not if they were linked by the correct
relations.
Labeled Attachment Score (LAS) The fraction
of events whose head events were correctly pre-
dicted with the correct relations. This measures both
whether the correct pairs of events were linked and
whether their temporal ordering is correct.
Tree Edit Distance In addition to the UAS and
LAS the tree edit distance score has been recently in-
troduced for evaluating dependency structures (Tsar-
faty et al, 2011). The tree edit distance score
for a tree pi is based on the following operations
? ? ? : ? = {DELETE, INSERT, RELABEL}:
? ? =DELETE delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
? ? =INSERT insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
? ? =RELABEL change the label of node v in pi
Any two trees pi1 and pi2 can be turned one into an-
other by a sequence of edit operations {?1, ..., ?n}.
93
UAS LAS UTEDS LTEDS
LinearSeq 0.830 0.581 0.689 0.549
ClassifySeq 0.830 0.581 0.689 0.549
MST 0.837 0.614? 0.710 0.571
SRP 0.830 0.647?? 0.712 0.596?
Table 3: Performance levels of temporal structure pars-
ing methods. A ? indicates that the model outperforms
LinearSeq and ClassifiedSeq at p < 0.01 and a ? indicates
that the model outperforms MST at p < 0.05.
Taking the shortest such sequence, the tree edit dis-
tance is calculated as the sum of the edit operation
costs divided by the size of the tree (i.e. the number
of words in the sentence). For temporal dependency
trees, we assume each operation costs 1.0. The fi-
nal score subtracts the edit distance from 1 so that
a perfect tree has score 1.0. The labeled tree edit
distance score (LTEDS) calculates sequences over
the tree with all its labeled temporal relations, while
the unlabeled tree edit distance score (UTEDS) treats
all edges as if they had the same label.
6.2 Results
Table 3 shows the results of the evaluation. The
unlabeled attachment score for the LinearSeq base-
line was 0.830, suggesting that annotators were most
often linking adjacent events. At the same time,
the labeled attachment score was 0.581, indicating
that even in fables, the stories are not simply linear,
that is, there are many relations other than BEFORE.
The ClassifySeq baseline performs identically to the
LinearSeq baseline, which shows that the simple pair-
wise classifier was unable to learn anything beyond
predicting all relations as BEFORE.
In terms of labeled attachment score, both de-
pendency parsing models outperformed the base-
line models ? the maximum spanning tree parser
achieved 0.614 LAS, and the shift-reduce parser
achieved 0.647 LAS. The shift-reduce parser also
outperformed the baseline models in terms of labeled
tree edit distance, achieving 0.596 LTEDS vs. the
baseline 0.549 LTEDS. These results indicate that de-
pendency parsing models are a good fit to our whole-
story timeline extraction task.
Finally, in comparing the two different depen-
dency parsing models, we observe that the shift-
reduce parser outperforms the maximum spanning
Error Type Num. %
OVERLAP? BEFORE 24 43.7
Attach to further head 18 32.7
Attach to nearer head 6 11.0
Other types of errors 7 12.6
Total 55 100
Table 4: Error distribution from the analysis of 55 errors
of the Shift-Reduce parsing model.
tree parser in terms of labeled attachment score
(0.647 vs. 0.614). It has been argued that graph-
based models like the maximum spanning tree parser
should be able to produce more globally consistent
and correct dependency trees, yet we do not observe
that here. A likely explanation for this phenomenon
is that the shift-reduce parsing model allows for fea-
tures describing previous parse decisions (similar to
the incremental nature of human parse decisions),
while the joint nature of the maximum spanning tree
parser does not.
6.3 Error Analysis
To better understand the errors our model is still mak-
ing, we examined two folds (55 errors in total in
20% of the evaluation data) and identified the major
categories of errors:
? OVERLAP? BEFORE: The model predicts the
correct head, but predicts its label as BEFORE,
while the correct label is OVERLAP.
? Attach to further head: The model predicts
the wrong head, and predicts as the head an
event that is further away than the true head.
? Attach to nearer head: The model predicts the
wrong head, and predicts as the head an event
that is closer than the true head.
Table 4 shows the distribution of the errors over these
categories. The two most common types of errors,
OVERLAP ? BEFORE and Attach to further head,
account for 76.4% of all the errors.
The most common type of error is predicting
a BEFORE relation when the correct answer is an
OVERLAP relation. Figure 3 shows an example of
such an error, where the model predicts that the
Spendthrift stood before he saw, while the anno-
tator indicates that the seeing happened during the
94
Figure 3: An OVERLAP ? BEFORE parser error. True
links are solid lines; the parser error is the dotted line.
Figure 4: Parser errors attaching to further away heads.
True links are solid lines; parser errors are dotted lines.
time in which he was standing. An analysis of these
OVERLAP? BEFORE errors suggests that they occur
in scenarios like this one, where the duration of one
event is significantly longer than the duration of an-
other, but there are no direct cues for these duration
differences. We also observe these types of errors
when one event has many sub-events, and therefore
the duration of the main event typically includes the
durations of all the sub-events. It might be possible
to address these kinds of errors by incorporating auto-
matically extracted event duration information (Pan
et al, 2006; Gusev et al, 2011).
The second most common error type of the model
is the prediction of a head event that is further away
than the head identified by the annotators. Figure 4
gives an example of such an error, where the model
predicts that the gathering includes the smarting, in-
stead of that the gathering includes the stung. The
second error in the figure is also of the same type.
In 65% of the cases where this type of error occurs,
it occurs after the parser had already made a label
classification error such as BEFORE ? OVERLAP.
So these errors may be in part due to the sequen-
tial nature of shift-reduce parsing, where early errors
propagate and cause later errors.
7 Discussion and Conclusions
In this article, we have presented an approach to tem-
poral information extraction that represents the time-
line of a story as a temporal dependency tree. We
have constructed an evaluation corpus where such
temporal dependencies have been annotated over a
set of 100 children?s stories. We have introduced two
dependency parsing techniques for extracting story
timelines and have shown that both outperform a rule-
based baseline and a prior-work-inspired pair-wise
classification baseline. Comparing the two depen-
dency parsing models, we have found that a shift-
reduce parser, which more closely mirrors the incre-
mental processing of our human annotators, outper-
forms a graph-based maximum spanning tree parser.
Our error analysis of the shift-reduce parser revealed
that being able to estimate differences in event dura-
tions may play a key role in improving parse quality.
We have focused on children?s stories in this study,
in part because they typically have simpler temporal
structures (though not so simple that our rule-based
baseline could parse them accurately). In most of our
fables, there were only one or two characters with at
most one or two simultaneous sequences of actions.
In other domains, the timeline of a text is likely to
be more complex. For example, in clinical records,
descriptions of patients may jump back and forth
between the patient history, the current examination,
and procedures that have not yet happened.
In future work, we plan to investigate how to best
apply the dependency structure approach to such
domains. One approach might be to first group
events into their narrative containers (Pustejovsky
and Stubbs, 2011), for example, grouping together all
events linked to the time of a patient?s examination.
Then within each narrative container, our dependency
parsing approach could be applied. Another approach
might be to join the individual timeline trees into a
document-wide tree via discourse relations or rela-
tions to the document creation time. Work on how
humans incrementally process such timelines in text
may help to decide which of these approaches holds
the most promise.
Acknowledgements
We would like to thank the anonymous reviewers
for their constructive comments. This research was
partially funded by the TERENCE project (EU FP7-
257410) and the PARIS project (IWT SBO 110067).
95
References
[Amigo? et al2011] Enrique Amigo?, Javier Artiles, Qi Li,
and Heng Ji. 2011. An evaluation framework for aggre-
gated temporal information extraction. In SIGIR-2011
Workshop on Entity-Oriented Search.
[Artiles et al2011] Javier Artiles, Qi Li, Taylor Cas-
sidy, Suzanne Tamang, and Heng Ji. 2011.
CUNY BLENDER TAC-KBP2011 temporal slot fill-
ing system description. In Text Analytics Conference
(TAC2011).
[Bethard and Martin2007] Steven Bethard and James H.
Martin. 2007. CU-TMP: Temporal relation classifica-
tion using syntactic and semantic features. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 129?132, Prague,
Czech Republic, June. ACL.
[Bethard et al2007] Steven Bethard, James H. Martin, and
Sara Klingenstein. 2007. Finding temporal structure in
text: Machine learning of syntactic temporal relations.
International Journal of Semantic Computing (IJSC),
1(4):441?458, 12.
[Bethard et al2012] Steven Bethard, Oleksandr
Kolomiyets, and Marie-Francine Moens. 2012.
Annotating narrative timelines as temporal dependency
structures. In Proceedings of the International
Conference on Linguistic Resources and Evaluation,
Istanbul, Turkey, May. ELRA.
[Boguraev and Ando2005] Branimir Boguraev and
Rie Kubota Ando. 2005. TimeBank-driven TimeML
analysis. In Annotating, Extracting and Reasoning
about Time and Events. Springer.
[Bramsen et al2006] P. Bramsen, P. Deshpande, Y.K. Lee,
and R. Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 189?
198. ACL.
[Brewer and Lichtenstein1982] William F. Brewer and Ed-
ward H. Lichtenstein. 1982. Stories are to entertain: A
structural-affect theory of stories. Journal of Pragmat-
ics, 6(5-6):473 ? 486.
[Chambers and Jurafsky2008] N. Chambers and D. Juraf-
sky. 2008. Jointly combining implicit constraints im-
proves temporal ordering. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706. ACL.
[Cheng et al2007] Yuchang Cheng, Masayuki Asahara,
and Yuji Matsumoto. 2007. NAIST.Japan: Tempo-
ral relation identification using dependency parsed tree.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 245?248,
Prague, Czech Republic, June. ACL.
[Chu and Liu1965] Y. J. Chu and T.H. Liu. 1965. On
the shortest arborescence of a directed graph. Science
Sinica, pages 1396?1400.
[Covington2001] M.A. Covington. 2001. A fundamental
algorithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
[Crammer and Singer2003] K. Crammer and Y. Singer.
2003. Ultraconservative online algorithms for multi-
class problems. Journal of Machine Learning Research,
3:951?991.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
[Edmonds1967] J. Edmonds. 1967. Optimum branchings.
Journal of Research of the National Bureau of Stan-
dards, pages 233?240.
[Georgiadis2003] L. Georgiadis. 2003. Arborescence op-
timization problems solvable by Edmonds? algorithm.
Theoretical Computer Science, 301(1-3):427?437.
[Gupta and Ji2009] Prashant Gupta and Heng Ji. 2009.
Predicting unknown time arguments based on cross-
event propagation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ?09, pages
369?372, Stroudsburg, PA, USA. ACL.
[Gusev et al2011] Andrey Gusev, Nathanael Chambers,
Divye Raj Khilnani, Pranav Khaitan, Steven Bethard,
and Dan Jurafsky. 2011. Using query patterns to learn
the duration of events. In Proceedings of the Interna-
tional Conference on Computational Semantics, pages
145?154.
[Hayes and Krippendorff2007] A.F. Hayes and K. Krip-
pendorff. 2007. Answering the call for a standard
reliability measure for coding data. Communication
Methods and Measures, 1(1):77?89.
[Hickmann2003] Maya Hickmann. 2003. Children?s Dis-
course: Person, Space and Time Across Languages.
Cambridge University Press, Cambridge, UK.
[Johnson-Laird1980] P.N. Johnson-Laird. 1980. Men-
tal models in cognitive science. Cognitive Science,
4(1):71?115.
[Krippendorff2004] K. Krippendorff. 2004. Content anal-
ysis: An introduction to its methodology. Sage Publica-
tions, Inc.
[Linguistic Data Consortium2005] Linguistic Data Con-
sortium. 2005. ACE (Automatic Content Extraction)
English annotation guidelines for events version 5.4.3
2005.07.01.
[Llorens et al2010] Hector Llorens, Estela Saquete, and
Borja Navarro. 2010. TIPSem (English and Spanish):
Evaluating CRFs and semantic roles in TempEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291, Uppsala, Sweden,
July. ACL.
96
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
[McIntyre and Lapata2009] N. McIntyre and M. Lapata.
2009. Learning to tell tales: A data-driven approach to
story generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
217?225. ACL.
[Nivre2008] J. Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computational
Linguistics, 34(4):513?553.
[Pan et al2006] Feng Pan, Rutu Mulkar, and Jerry R.
Hobbs. 2006. Learning event durations from event
descriptions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 393?400, Sydney, Australia, July. ACL.
[Pustejovsky and Stubbs2011] J. Pustejovsky and
A. Stubbs. 2011. Increasing informativeness in
temporal annotation. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 152?160. ACL.
[Pustejovsky et al2003a] James Pustejovsky, Jose?
Castan?o, Robert Ingria, Roser Saury?, Robert
Gaizauskas, Andrea Setzer, and Graham Katz. 2003a.
TimeML: Robust specification of event and temporal
expressions in text. In Proceedings of the Fifth
International Workshop on Computational Semantics
(IWCS-5), Tilburg.
[Pustejovsky et al2003b] James Pustejovsky, Patrick
Hanks, Roser Saury?, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth Sundheim,
David Day, Lisa Ferro, and Marcia Lazo. 2003b.
The TimeBank corpus. In Proceedings of Corpus
Linguistics, pages 647?656.
[Tsarfaty et al2011] R. Tsarfaty, J. Nivre, and E. Ander-
sson. 2011. Evaluating dependency parsing: Robust
and heuristics-free cross-annotation evaluation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 385?396. ACL.
[UzZaman and Allen2010] Naushad UzZaman and James
Allen. 2010. TRIPS and TRIOS system for TempEval-
2: Extracting temporal information from text. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 276?283, Uppsala, Sweden, July.
ACL.
[Verhagen et al2007] Marc Verhagen, Robert Gaizauskas,
Frank Schilder, Graham Katz, and James Pustejovsky.
2007. SemEval2007 Task 15: TempEval temporal rela-
tion identification. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.
[Verhagen et al2010] Marc Verhagen, Roser Saur??, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 Task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 57?62, Stroudsburg, PA, USA. ACL.
[Yamada and Matsumoto2003] H. Yamada and Y. Mat-
sumoto. 2003. Statistical dependency analysis with
support vector machines. In Proceedings of IWPT.
[Yoshikawa et al2009] K. Yoshikawa, S. Riedel, M. Asa-
hara, and Y. Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
405?413. ACL.
97
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 325?328,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
KUL: Recognition and Normalization of Temporal Expressions 
 
 
Oleksandr Kolomiyets, Marie-Francine Moens 
Department of Computer Science 
Katholieke Universiteit Leuven 
{oleksandr.kolomiyets, sien.moens}@cs.kuleuven.be 
 
 
  
 
Abstract 
 
In this paper we describe a system for the 
recognition and normalization of temporal 
expressions (Task 13: TempEval-2, Task 
A). The recognition task is approached as 
a classification problem of sentence con-
stituents and the normalization is imple-
mented in a rule-based manner. One of the 
system features is extending positive an-
notations in the corpus by semantically 
similar words automatically obtained from 
a large unannotated textual corpus. The 
best results obtained by the system are 
0.85 and 0.84 for precision and recall re-
spectively for recognition of temporal ex-
pressions; the accuracy values of 0.91 and 
0.55 were obtained for the feature values 
TYPE and VAL respectively. 
1 Introduction 
Recognition of temporal expressions1 is a task of 
proper identification of phrases with temporal 
semantics in running text. After several evalua-
tion campaigns targeted at temporal processing 
of text, such as MUC, ACE TERN and TempEv-
al-1 (Verhagen et al, 2007), the recognition and 
normalization task has been again newly reintro-
duced in TempEval-2 (Pustejovsky & Verhagen, 
2009). The task is defined as follows: determine 
the extent of the time expressions; in addition, 
determine the value of the features TYPE for the 
type of the temporal expression and its temporal 
value VAL. In this paper we describe the KUL 
system that has participated in this task.  
                                                 
1
 Temporal expressions are sometimes referenced as time 
expressions and timexes.  
Architecturally, the system employs a pipe-
lined information processing chain and imple-
ments a number of machine learning classifiers 
for extracting the necessary information for the 
temporal value estimation. The normalization 
step employs a number of hand-crafted vocabula-
ries for tagging single elements of a temporal 
expression and a rule-based system for estimat-
ing the temporal value. The performance of the 
system obtained the values of 0.85 and 0.84 for 
precision and recall respectively for the recogni-
tion of temporal expressions. The accuracy for 
the type and value is 0.91 and 0.55 respectively. 
 The remainder of the paper is organized as 
follows: Section 2 reports on the architecture of 
the system with single modules and describes 
theirs functions. Section 3 presents the results 
and error analysis; the conclusions are provided 
in Section 4. 
2 System Architecture 
The system is implemented in Java and follows a 
pipelined method for information processing. 
Regarding the problems it solves, it can be split 
in two sub-systems: recognition and normaliza-
tion.  
2.1 Recognition of Temporal Expressions  
This sub-system is employed for finding tempor-
al expressions in the text. It takes a sentence as 
input and looks for temporal expressions in it.   
Pre-processing: At this step the input text un-
dergoes syntactic analysis. Sentence detection, 
tokenization, part-of-speech tagging and parsing 
are applied2.  
Candidate selection: Since only certain lexi-
cal categories can be temporal expressions and 
they are defined in the TIDES standard (Ferro et 
                                                 
2
 For preprocessing we use the OpenNLP package 
(http://opennlp.sourceforge.net).  
325
al., 2003), in our implementation we consider the 
following chunk-phrases as candidates for tem-
poral expressions: nouns (week, day), proper 
names (Tuesday, May), noun phrases (last Tues-
day), adjectives (current), adjective phrases (then 
current), adverbs (currently), adverbial phrases 
(a year ago), and numbers (2000). As input it 
takes the sentences with provided syntactic in-
formation and marks phrases in the parse tree 
belonging to the above types for temporal ex-
pressions.   
Annotation alignment: If the system is used 
for training classifiers, all the candidates in a 
sentence are examined against the available an-
notations. The candidates, whose parse and anno-
tation extents aligned, are taken as positive ex-
amples and the rest is considered as negative.  
Feature Design: To produce a feature-vector 
we use most valuable features extracted for 
phrase-candidate. After a number of experiments 
the following features were selected:  
? Last token in the phrase, most probable 
token to be a temporal trigger; 
? Lemma of the last phrasal token; 
? Part-of-speech of the last phrasal token; 
? Character pattern of the last phrasal to-
ken as introduced in (Ahn et al, 2007); 
? Neighbor POS?s. The concatenated part-
of-speech tags of the last phrasal token 
and its preceding token;  
? Character pattern of the entire phrase; 
? Phrase surface. A concatenated string of 
sub-parse  types for the phrase; 
? A Boolean feature indicating nested 
complex phrasal parses, such as noun 
verb, adverbial, adjective or preposition-
al phrase; 
? Depth of the phrase. The number of the 
nested sub-parses to the deepest pre-
terminal sub-parse.  
All the features are considered as Boolean. 
Classification: Once the classifiers are trained 
they can be used for recognition of temporal ex-
pressions on test sentences. A preprocessed sen-
tence is taken as input and starting from its 
parse-tree root the candidate-phrases are classi-
fied. The most probable class will be assigned to 
the candidate under consideration. Once the 
phrase is classified as temporal expression no 
further classification of nested phrases is per-
formed, since no embedded timexes are allowed 
in the corpus. After a series of experiments with 
different machine learning techniques on the 
training data the maximum entropy classifier was 
chosen. 
Extending positive instances: Sparseness of 
annotated corpora is the biggest challenge for 
any supervised machine learning technique. To 
overcome this problem we hypothesize that 
knowledge of semantic similar words could be 
found by associating words that do not occur in 
the training set to similar words that did occur in 
the training set. Furthermore, we would like to 
learn these similarities automatically in order to 
be as much as possible independent of know-
ledge sources that might not be available for all 
languages or domains. For example, there is in 
TimeBank a temporal expression ?last summer? 
with the temporal trigger summer, but there is no 
annotation of temporal expressions built around 
the temporal trigger winter, and this means that 
no temporal expression with the trigger winter 
can be recognized. Something similar usually 
happens to any annotated corpus and we want to 
find a way how to find other temporal expres-
sions outside the available data, which can be 
used for training. On the other hand, we want to 
avoid a na?ve selection of words as, for example, 
from a gazetteer with temporal triggers, which 
may contradict with grammatical rules and the 
lexical context of a timex in text, e.g.: 
 
on Tuesday said.... 
 
But grammatically wrong by na?ve replacement 
from a gazetteer:  
? on week said*? 
? on day said*? 
? on month said* ? 
 
In order to find these words, which are legiti-
mate at a certain position in a certain context we 
use the latent word language model (LWLM) 
(Deschacht & Moens, 2009) with a Hidden Mar-
kov Model approach for estimating the latent 
word parameters.  
Complementary, we use WordNet (Miller, 
1995) as a source that can provide a most com-
plete set of words similar to the given one. One 
should note that the use of WordNet is not 
straight-forward. Due to the polysemy, the word 
sense disambiguation (WSD) problem has to be 
solved. Our system uses latent words obtained by 
the LWLM and chooses the synset with the high-
326
est overlap between WordNet synonyms and 
coordinate terms, and the latent words. The over-
lap value is calculated as the sum of LWLM 
probabilities for matching words. 
Having these two sets of synonyms and after a 
series of preliminary tests we found the setting, 
at which the system produces the highest results 
and submitted several runs with different strate-
gies:  
? Baseline (no expansion) (KUL Run 1) 
? 3 LWLM words with highest probabili-
ties (KUL Run 2) 
? 3 WordNet coordinate terms; WSD is 
solved by means of LWLM3 (KUL Run 
3)  
For each available annotation in the corpus a 
positive instance is generated. After that, the to-
ken at the most probable position for a temporal 
trigger is replaced by a synonym from the syn-
onym set found to the available token.  
2.2 Normalization of Temporal Expressions 
Normalization of temporal expressions is a 
process of estimating standardized temporal val-
ues and types. For example, the temporal expres-
sion ?summer 1990? has to be resolved to its 
value of 1990-SU and the type of DATE. In 
contrast, for the expression ?last year? the value 
cannot be estimated directly, rather it gets a mod-
ified value of another time expression.  
Due to a large variance of expressions denot-
ing the same date and vagueness in language, 
rule-based systems have been proven to perform 
better than machine-learning ones for the norma-
lization task. The current implementation follows 
a rule-based approach and takes a pre-processed 
document with recognized temporal expressions 
(as it is described in Section 2.1) and estimates a 
standardized ISO-based date/time value. In the 
following sections we provide implementation 
details of the system. 
Before the temporal value is estimated, we 
employ a classifier, which uses the same feature 
sets and classify the temporal expression among 
type classes DATE, TIME, DURATION and 
SET.  
Labeling: Labeling text is a process of provid-
ing tags to tokens of chunk-phrases from a de-
                                                 
3
 Preliminary experiments, when the most common sense in 
WordNet is chosen for increasing the number of positive 
examples, showed a low performance level and thus has not 
been proposed for evaluations. 
fined set of tags. We carefully examined availa-
ble annotated temporal expressions and annota-
tion standards to determine categories of words 
participating in temporal expressions. The fol-
lowing set of categories with labels based on se-
mantics of temporally relevant information and 
simple syntax was defined: ordinal numbers 
(first, 30th etc.), cardinal numbers (one, two, 10 
etc.), month names (Jan., January etc.), week 
day names (Mo., Monday etc.), season names 
(summer, winter etc.), parts of day (morning, 
afternoon etc.), temporal directions (ago, later, 
earlier etc.), quantifiers (several, few etc.), mod-
ifiers (recent, last etc.), approximators (almost, 
nearly etc.), temporal co-references (time, period 
etc.), fixed single token timexes (tomorrow, to-
day etc.), holidays (Christmas, Easter etc.) and 
temporal units (days, months, years etc.). Also 
fine-grained categories are introduced: day num-
ber, month number and year number. For each 
category we manually construct a vocabulary, in 
which each entry specifies a value of a temporal 
field or a final date/time value, or a method with 
parameters to apply.  
As input, the normalization takes a recognized 
temporal expression and its properties, such as 
the temporal type and the discourse type4. During 
labeling each token in a temporal expression is 
tagged with one or multiple labels corresponding 
to the categories defined above. For each of the 
categories a custom detector is implemented. The 
detector declares the method to run and the ex-
pected type of the result. The rules that imple-
ment the logics for the detector are inherited 
from an abstract class for this specific detector, 
so that if a new rule needs to be implemented its 
realization is limited to the development of one 
class, all the rest the detector does automatically. 
Besides, the order, in which detectors have to be 
run, can be specified (as for example, in case of 
fine-grained detectors). As output, the module 
provides labels of the categories to the tokens in 
the temporal expression. If there is no entry in 
the vocabulary for a token, its part-of-speech tag 
is used as the label.  
 Value estimation: Value estimation is 
implemented in the way of aggregating the 
values defined for entries in the vocabulary 
and/or executing instructions or methods 
specified. Also a set of predefined resolution 
                                                 
4
 Since in TempEval-2 the reference to the timex with re-
spect to which the value estimated is given, the normaliza-
tion module considers all timexes as deictic.   
327
rules is provided and can be extended with new 
implementations of resolution strategies.  
For resolution of complex relative temporal 
expressions, the value for which cannot be esti-
mated directly, we need to rely on additional in-
formation found at the recognition step. This in-
cludes the semantic type of the timex, discourse 
type and contextual temporal information 
(speech or document creation time, or previously 
mentioned timexes). Let?s consider the following 
temporal expression as an example: 10 days ago.  
In this example the temporal expression receives 
a modified value of another timex, namely the 
value of the document creation time. The tem-
poral expression is recognized and classified as a 
date (SEM TYPE: DATE), which refers to 
another timex (DISCOURSE TYPE: DEIC-
TIC). It takes the value of the referenced timex 
and modifies it with respect to the number (10), 
magnitude (days) and temporal direction (ago). 
Thus, the final value is calculated by subtracting 
a number of days for the value of the referenced 
timex.  
3 Results and Error Analysis  
In the Table 1 the results of the best-performing 
runs are presented.  
Table 1. Results of different runs of the system. 
As we can see the best results were obtained 
by extending available annotations with maxi-
mum 3 additional instances, which are extracted 
as coordinate terms in WordNet, whereas the 
WSD problem was solved as the greatest overlap 
between coordinate terms and latent words ob-
tained by the LWLM.  
Most of the errors at the recognition step were 
caused by misaligned parses and annotations.  
For normalization we acknowledge the signi-
ficance of estimating a proper temporal value 
with a correct link to the temporal expression 
with its value. In the TempEval-2 training data 
the links to the temporal expressions indicating 
how the value is calculated were not provided, 
and thus, the use of machine learning tools for 
training and automatic disambiguation was not 
possible. We choose a fixed strategy and all rela-
tive temporal expressions were resolved with 
respect to the document creation time, which 
caused errors with wrong temporal values and a 
low performance level.  
4 Conclusions 
For TempEval-2 we proposed a system for the 
recognition and normalization of temporal ex-
pressions. Multiple runs were submitted, among 
which the best results were obtained with auto-
matically expanded positive instances by words 
derived as coordinate terms from WordNet for 
which the proper sense was found as the greatest 
overlap between coordinate terms and latent 
words found by the LWLM.  
Acknowledgements 
This work has been funded by the Flemish gov-
ernment as a part of the project AMASS++ 
(Grant: IWT-60051) and by Space Applications 
Services NV as part of the ITEA2 project LIN-
DO (ITEA2-06011, IWT-70043). 
References  
Ahn, D., van Rantwijk, J., and de Rijke, M. 2007. A 
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings of 
NAACL-HLT 2007. 
Deschacht, K., and Moens M.-F. 2009. Using the La-
tent Words Language Model for Semi-Supervised 
Semantic Role Labeling. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing.   
Ferro, L., Gerber, L., Mani, I., Sundheim, B., and 
Wilson, G. 2003. TIDES 2003 Standard for the 
Annotation of Temporal Expressions. 
Miller, G. A. 1995. WordNet: A Lexical Database for 
English. Communications of the ACM, 38(11): 39-
41. 
Pustejovsky, J. and Verhagen, M. 2009. SemEval-
2010 Task 13: Evaluating Events, Time Expres-
sions, and Temporal Relations (TempEval-2). In 
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions. 
Verhagen, M., Gaizauskas, R.,  Schilder, F., Hepple, 
M.,  and Pustejovsky, J. 2007. Semeval-2007 Task 
15: Tempeval Temporal Relation Identification. In 
SemEval-2007: 4th International Workshop on 
Semantic Evaluations. 
Run Recognition Normalization 
 P R F1 TYPE Acc. 
VAL 
Acc. 
1 0.78 0.82 0.8 0.91 0.55 
2 0.75 0.85 0.797 0.91 0.51 
3 0.85 0.84 0.845 0.91 0.55 
328
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 365?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 3: Spatial Role Labeling
Parisa Kordjamshidi
Katholieke Universiteit Leuven
parisa.kordjamshidi@
cs.kuleuven.be
Steven Bethard
University of Colorado
steven.bethard@
colorado.edu
Marie-Francine Moens
Katholieke Universiteit Leuven
sien.moens@
cs.kuleuven.be
Abstract
This SemEval2012 shared task is based on a
recently introduced spatial annotation scheme
called Spatial Role Labeling. The Spatial Role
Labeling task concerns the extraction of main
components of the spatial semantics from nat-
ural language: trajectors, landmarks and spa-
tial indicators. In addition to these major
components, the links between them and the
general-type of spatial relationships includ-
ing region, direction and distance are targeted.
The annotated dataset contains about 1213
sentences which describe 612 images of the
CLEF IAPR TC-12 Image Benchmark. We
have one participant system with two runs.
The participant?s runs are compared to the sys-
tem in (Kordjamshidi et al, 2011c) which is
provided by task organizers.
1 Introduction
One of the essential functions of natural language is
to talk about spatial relationships between objects.
The sentence ?Give me the book on AI on the big
table behind the wall.? expresses information about
the spatial configuration of the objects (book, table,
wall) in some space. Particularly, it explains the re-
gion occupied by the book with respect to the table
and the direction (orientation) of the table with re-
spect to the wall. Understanding such spatial utter-
ances is a problem in many areas, including robotics,
navigation, traffic management, and query answer-
ing systems (Tappan, 2004).
Linguistic constructs can express highly complex,
relational structures of objects, spatial relations be-
tween them, and patterns of motion through space
relative to some reference point. Compared to nat-
ural language, formal spatial models focus on one
particular spatial aspect such as orientation, topol-
ogy or distance and specify its underlying spatial
logic in detail (Hois and Kutz, 2008). These for-
mal models enable spatial reasoning that is difficult
to perform on natural language expressions.
Learning how to map natural language spatial in-
formation onto a formal representation is a challeng-
ing problem. The complexity of spatial semantics
from the cognitive-linguistic point of view on the
one hand, the diversity of formal spatial represen-
tation models in different applications on the other
hand and the gap between the specification level of
the two sides has led to the present situation that no
well-defined framework for automatic spatial infor-
mation extraction exists that can handle all of these
aspects.
In a previous paper (Kordjamshidi et al, 2010b),
we introduced the task of spatial role labeling
(SpRL) and proposed an annotation scheme that is
language-independent and practically facilitates the
application of machine learning techniques. Our
framework consists of a set of spatial roles based
on the theory of holistic spatial semantics (Zlat-
evl, 2007) with the intent of covering the main as-
pects of spatial concepts at a course level, includ-
ing both static and dynamic spatial semantics. This
shared task is defined on the basis of that annota-
tion scheme. Since this is the first shared task on the
spatial information and this particular data, we pro-
posed a simplified version of the original scheme.
The intention of this simplification was to make this
practice feasible in the given timeframe. However,
365
the current task is very challenging particularly for
learning the spatial links and relations.
The core problem of SpRL is: i) the identification
of the words that play a role in describing spatial
concepts, and ii) the classification of the relational
role that these words play in the spatial configura-
tion.
For example, consider again the sentence ?Give
me the book on AI on the big table behind the wall.?.
The phrase headed by the token book is referring
to a trajector object. The trajector (TR) is an en-
tity whose location is described in the sentence. The
phrase headed by the token table is referring to the
role of a landmark (LM). The landmark is a refer-
ence object for describing the location of a trajector.
These two spatial entities are related by the spatial
expression on denoted as spatial indicator (SP). The
spatial indicator (often a preposition in English, but
sometimes a verb, noun, adjective, or adverb) indi-
cates the existence of spatial information in the sen-
tence and establishes the type of a spatial relation.
The spatial relations that can be extracted from the
whole sentence are <onSP bookTR tableLM> and
<behindSP tableTR wallLM>. One could also use
spatial reasoning to infer that the statement <behind
book wall> holds, however, such inferred relations
are not considered in this task. Although the spa-
tial indicators are mostly prepositions, the reverse
may not hold- for example, the first preposition
on only states the topic of the book, so <on book
AI> is not a spatial relation. For each of the true
spatial relations, a general type is assigned. The
<onSP bookTR tableLM> relation expresses a kind
of topological relationship between the two objects
and we assign it a general type named region. The
<behindSP tableTR wallLM> relation expresses di-
rectional information and we assign it a general type
named direction.
In general we assume two main abstraction layers
for the extraction of spatial information (Bateman,
2010; Kordjamshidi et al, 2010a; Kordjamshidi et
al., 2011a): (a) a linguistic layer, corresponding to
the annotation scheme described above, which starts
with unrestricted natural language and predicts the
existence of spatial information at the sentence level
by identifying the words that play a particular spa-
tial role as well as their spatial relationship; (b) a
formal layer, in which the spatial roles are mapped
onto a spatial calculus model (Galton, 2009). For
example, the linguistic layer recognizes that the spa-
tial relation (on) holds between book and table, and
the formal layer maps this to a specific, formal spa-
tial representation, e.g., a logical representation like
AboveExternallyConnected(book, table) or a
formal qualitative spatial representation like EC (ex-
ternally connected) in the RCC model (Regional
Connection Calculus) (Cohn and Renz, 2008).
In this shared task we focus on the first (linguistic)
level which is a necessary step for mapping natural
language to any formal spatial calculus. The main
roles that are considered here are trajector, land-
mark, spatial indicator, their links and the general
type of their spatial relation. The general type of a
relation can be direction, region or distance.
2 Motivation and related work
Spatial role labeling is a key task for applications
that are required to answer questions or reason about
spatial relationships between entities. Examples in-
clude systems that perform text-to-scene conversion,
generation of textual descriptions from visual data,
robot navigation tasks, giving directional instruc-
tions, and geographical information systems (GIS).
Recent research trends (Ross et al, 2010; Hois et
al., 2011; Tellex et al, 2011) indicate an increasing
interest in the area of extracting spatial information
from language and mapping it to a formal spatial
representation. Although cognitive-linguistic stud-
ies have investigated this problem extensively, the
computational aspect of making this bridge between
language and formal spatial representation (Hois
and Kutz, 2008) is still in its elementary stages. The
possession of a practical and appropriate annotation
scheme along with data is the first requirement. To
obtain this one has to investigate and schematize
both linguistic and spatial ontologies. This process
needs to cover the necessary information and seman-
tics on the one hand, and to maintain the practical
feasibility of the automatic annotation of unobserved
data on the other hand.
In recent research on spatial information and nat-
ural language, several annotation schemes have been
proposed such as ACE, GUM, GML, KML, TRML
which are briefly described and compared to Spa-
tialML scheme in (MITRE Corporation, 2010). But
366
to our knowledge, the main obstacles for employing
machine learning in this context and the very limited
usage of this effective approach have been (a) the
lack of an agreement on a unique semantic model
for spatial information; (b) the diversity of formal
spatial relations; and consequently (c) the lack of
annotated data on which machine learning can be
employed to learn and extract the spatial relations.
The most systematic work in this area includes the
SpatialML (Mani et al, 2008) scheme which fo-
cuses on geographical information, and the work of
(Pustejovsky and Moszkowicz, 2009) in which the
pivot of the spatial information is the spatial verb.
The most recent and active work is the ISO-Space
scheme (Pustejovsky et al, 2011) which is based
on the above two schemes. The ideas behind ISO-
Space are closely related to our annotation scheme
in (Kordjamshidi et al, 2010b), however it consid-
ers more detailed and fine-grained spatial and lin-
guistic elements which makes the preparation of the
data for machine learning more difficult.
Spatial information is directly related to the part
of the language that can be visualized. Thus, the
extraction of spatial information is useful for mul-
timodal environments. One advantage of our pro-
posed scheme is that it considers this dimension. Be-
cause it abstracts the spatial elements that could be
aligned with the objects in images/videos and used
for annotation of audio-visual descriptions (Butko et
al., 2011). This is useful in the multimodal environ-
ments where, for example, natural language instruc-
tions are given to a robot for finding the way or ob-
jects.
Not much work exists on using annotations for
learning models to extract spatial information. Our
previous work (Kordjamshidi et al, 2011c) is a first
step in this direction and provides a domain indepen-
dent linguistic and spatial analysis to this problem.
This shared task invites interested research groups
for a similar effort. The idea behind this task is
firstly to motivate the application of different ma-
chine learning approaches, secondly to investigate
effective features for this task, and thirdly to reveal
the practical problems in the annotation schemes and
the annotated concepts. This will help to enrich the
data and the annotation in parallel with the machine
learning practice.
3 Annotation scheme
As mentioned in the introduction, the annotation of
the data set is according to the general spatial role
labeling scheme (Kordjamshidi et al, 2010b). The
below example presents the annotated elements in
this scheme.
A womanTR and a childTR are
walkingMOTION overSP the squareLM .
General-type: region
Specific type: RCC
Spatial value: PP (proper part)
Dynamic
Path: middle
Frame of reference: ?
According to this scheme the main spatial roles are,
Trajector (TR). The entity, i.e., person, object or
event whose location is described, which can
be static or dynamic; (also called: local/figure
object, locatum). In the above example woman
and child are two trajectors.
Landmark (LM). The reference entity in relation
to which the location or the motion of the tra-
jector is specified. (also called: reference ob-
ject or relatum). square is the landmark in the
above example.
Spatial indicator (SP). The element that defines
constraints on spatial properties such as the lo-
cation of the trajector with respect to the land-
mark. The spatial indicator determines the type
of spatial relation. The preposition over is an-
notated as the spatial indicator in the current
example.
Moreover, the links between the three roles are an-
notated as a spatial Relation. Since each spatial
relation is defined with three arguments we call
it a spatial triplet. Each triplet indicates a re-
lation between the three above mentioned spatial
roles. The sentence contains two spatial relations
of <overSP womanTR squareLM> and <overSP
childTR squareLM>, with the same spatial at-
tributes listed below the example. In spatial infor-
mation theory the relations and properties are usu-
ally grouped into the domains of topological, direc-
tional, and distance relations and also shape (Stock,
367
1997). Accordingly, we propose a mapping between
the extracted spatial triplets to the coarse-grained
type of spatial relationships including region, direc-
tion or distance. We call these types as general-
type of the spatial relations and briefly describe
these below:
Region. refers to a region of space which is always
defined in relation to a landmark, e.g. the inte-
rior or exterior, e.g. ?the flower is in the vase?.
Direction. denotes a direction along the axes pro-
vided by the different frames of reference, in
case the trajector of motion is not characterized
in terms of its relation to the region of a land-
mark, e.g. ?the vase is on the left?.
Distance. states information about the spatial dis-
tance of the objects and could be a qualitative
expression such as close, far or quantitative
such as 12 km, e.g. ?the kids are close to the
blackboard?.
The general-type of the relation in the example is
annotated as region.
After extraction of these relations a next fine-
grained step will be to map each general spatial re-
lationship to an appropriate spatial calculi represen-
tation. This step is not intended for this task and
the additional tags in the scheme will be consid-
ered in the future shared tasks. For example Re-
gion Connection Calculus RCC-8 (Cohn and Renz,
2008) representation reflects region-based topolog-
ical relations. Topological or region-based spatial
information has been researched in depth in the area
of qualitative spatial representation and reasoning.
We assume that the trajectors and landmarks can of-
ten be interpreted as spatial regions and, as a conse-
quence, their relation can be annotated with a spe-
cific RCC-8 relation. The RCC type in the above
example is specifically annotated as the PP (proper
part). Similarly, the direction and distance relations
are mapped to more specific formal representations.
Two additional annotations are about motion
verbs and dynamism. Dynamic spatial information
are associated with spatial movements and spatial
changes. In dynamic spatial relations mostly mo-
tion verbs are involved. Motion verbs carry spatial
information and influence the spatial semantics. In
the above example the spatial indicator over is re-
lated to a motion verb walking. Hence the spatial
relation is dynamic and walking is annotated as the
motion. In contrast to the dynamic spatial relations,
the static ones explain a static spatial configuration
such as the example of the previous section <onSP
bookTR tableLM> .
In the case of dynamic spatial information a path
is associated with the location of the trajector. In our
scheme the path is characterized by the three values
of beginning, middle, end and zero. The frame of
reference can be intrinsic, relative or absolute and is
typically relevant for directional relations. For more
details about the scheme, see (Kordjamshidi et al,
2010b).
4 Tasks
The SemEval-2012 shared task is defined in three
parts.
? The first part considers labeling the spatial
indicators and trajector(s) / landmark(s). In
other words at this step we consider the
extraction of the individual roles that are
tagged with TRAJECTOR, LANDMARK and
SPATIAL INDICATOR.
? The second part is a kind of relation prediction
task and the goal is to extract triples contain-
ing (spatial-indicator, trajector, landmark). The
prediction of the tag of RELATION with its three
arguments of SP, TR, LM at the same time is
considered.
? The third part concerns the classification of the
type of the spatial relation. At the most coarse-
grained level this includes labeling the spatial
relations i.e. the triplets of (spatial indicator,
trajector, landmark) with region, direction, and
distance labels. This means the general-type
of the RELATION should be predicted. The
general-type is an attribute of the RELATION
tag, see the example represented in XML for-
mat in section 5.1.
5 Preparation of the dataset
The annotated corpus that we used for this shared
task is a subset of IAPR TC-12 image Bench-
mark (Grubinger et al, 2006). It contains 613 text
368
files that include 1213 sentences in total. This is an
extension of the dataset used in (Kordjamshidi et
al., 2011c). The original corpus was available free
of charge and without copyright restrictions. The
corpus contains images taken by tourists with de-
scriptions in different languages. The texts describe
objects, and their absolute and relative positions in
the image. This makes the corpus a rich resource for
spatial information. However the descriptions are
not always limited to spatial information. Therefore
they are less domain-specific and contain free expla-
nations about the images. Table 1 shows the detailed
statistics of this data. The average length of the sen-
tences in this data is about 15 words including punc-
tuation marks with a standard deviation of 8.
The spatial roles are assigned both to phrases and
their headwords, but only the headwords are eval-
uated for this task. The spatial relations indicate a
triplet of these roles. The general-type is assigned to
each triplet of spatial indicator, trajector and land-
mark.
At the starting point two annotators including one
task-organizer and another non-expert annotator, an-
notated 325 sentences for the spatial roles and rela-
tions. The purpose was to realize the disagreement
points and prepare a set of instructions in a way to
achieve highest-possible agreement. From the first
effort an inter-annotator agreement (Carletta, 1996)
of 0.89 for Cohen?s kappa was obtained. We contin-
ued with the a third annotator for the remaining 888
sentences. The annotator had an explanatory session
and received a set of instructions and annotated ex-
amples to decrease the ambiguity in the annotations.
To avoid complexity only the relations that are di-
rectly expressed in the sentence are annotated and
spatial reasoning was avoided during the annota-
tions. Sometimes the trajectors and landmarks or
both are implicit, meaning that there is no word in
the sentence to represent them. For example in the
sentence Come over here, the trajector you is only
implicitly present. To be consistent with the number
of arguments in spatial relations, in these cases we
use the term undefined for the implicit roles. There-
fore, the spatial relation in the above example is
<overSP undefinedTR hereLM>.
5.1 Data format
The data is released in XML format. The original
textual files are split into sentences. Each sentence
is placed in a <SENTENCE/> tag and assigned an
identifier. This tag contains all the other tags which
describe the content and spatial relations of one sen-
tence.
The content of the sentence is placed in the
<CONTENT/> tag. The words in each sentence
are assigned identifiers depending on their specific
roles. Trajectors, landmarks and spatial indicators
are identified by <TRAJECTOR/>, <LANDMARK/>
and <SPATIAL INDICATOR/> tags, respectively.
Each of these XML elements has an ?ID? attribute
that identifies a related word by its index. The ?ID?
prefixed by either ?TW?, ?LW? or ?SW?, respec-
tively for the mentioned roles. For example, a tra-
jector with ID=?TW2? corresponds to the word at
index 2 in the sentence. Indexes start at 0. Com-
mas, parentheses and apostrophes are also counted
as tokens.
Spatial relations are assigned identifiers too, and
relate the role-playing words to each other. Spa-
tial relations are identified by the <RELATION/>
tag. The spatial indicator, trajector and land-
mark for the relation are identified by the ?SP?,
?TR? and ?LM? attributes, respectively. The val-
ues of these attributes correspond to the ?ID? at-
tributes in the <TRAJECTOR/>, <LANDMARK/>
and <SPATIAL INDICATOR/> elements. If a tra-
jector or landmark is implicit, then the index of
?TR? or ?LM? attribute will be set to a dummy
index. This dummy index is equal to the in-
dex of the last word in the sentence plus one.
In this case, the value of TRAJECTOR or LAND-
MARK is set to ?undefined?. The coarse-grained
spatial type of the relation is indicated by the
?GENERAL TYPE? attribute and gets one value
in {REGION, DIRECTION, DISTANCE}. In the
original data set there are cases annotated with
multiple spatial types. This is due to the ambi-
guity and/or under-specificity of natural language
compared to formal spatial representations (Kord-
jamshidi et al, 2010a). In this task the general-
type with a higher priority by the annotator is pro-
vided. Here, by the high priority type, we mean the
general type which has been the most informative
369
Spatial Roles Relations General Types
Sentences TR LM SP Spatial triplets Region Direction Distance
1213 1593 1408 1464 1715 1036 644 35
Table 1: Number of annotated components in the data set.
and relevant type for a relation, from the annotator?s
point of view. This task considers labeling words
rather than phrases for all spatial roles. However, in
the XML file for spatial indicators often the whole
phrase is tagged. In these cases, the index of the
indicator refers to one word which is typically the
spatial preposition of the phrase. For evaluation only
the indexed words are compared and should be pre-
dicted correctly.
Below is one example copied from the data. For
more examples and details about the general anno-
tation scheme see (Kordjamshidi et al, 2010b).
<SENTENCE ID=?S11?>
<CONTENT >
there are red umbrellas in a park on the right .
</CONTENT>
<TRAJECTOR ID=?TW3?>
umbrellas
</TRAJECTOR>
<LANDMARK ID=?LW6?>
park
</LANDMARK>
<SPATIAL INDICATOR ID=?SW4?>
in
</SPATIAL INDICATOR>
<RELATION ID=?R0? SP=?SW4? TR=?TW3?
LM=?LW6? GENERAL TYPE=?REGION?/>
<SPATIAL INDICATOR ID=?SW7?>
on the right
</SPATIAL INDICATOR>
<RELATION ID=?R1? SP=?SW7? TR=?TW3?
LM=?LW6? GENERAL TYPE=?DIRECTION?/>
</SENTENCE>
The dataset, both train and test, also the 10-fold
splits are made available in the LIIR research group
webpage of KU Leuven.1
6 Evaluation methodology
According to the usual setting of the shared tasks
our evaluation setting was based on splitting the data
set into a training and a testing set. Each set con-
tained about 50% of the whole data. The test set re-
1http://www.cs.kuleuven.be/groups/liir/software/
SpRL Data/
leased without the ground-truth labels. However, af-
ter the systems submission deadline the ground-truth
test was released. Hence the participant group per-
formed an additional 10-fold cross validation eval-
uation too. We report the results of both evaluation
settings.
Prediction of each component including TRAJEC-
TORs, LANDMARKs and SPATIAL-INDICATORs is
evaluated on the test set using their individual spatial
element XML tags. The evaluation metrics of pre-
cision, recall and F1-measure are used, which are
defined as:
recall = TPTP+FN (1)
precision = TPTP+FP (2)
F1 = 2?recall?precision(recall+precision) , (3)
where:
TP = the number of system-produced
XML tags that match an annotated XML
tag,
FP = the number of system-produced
XML tags that do not match an annotated
tag,
FN = the number of annotated XML tags
that do not match a system-produced tag.
For the roles evaluation two XML tags match
when they have exactly same identifier. In fact,
when the identifiers are the same then the role and
the word index are the same. In addition, systems
are evaluated on how well they are able to retrieve
triplets of (trajector, spatial-indicator, landmark), in
terms of precision, recall and F1-measure. The TP,
FP, FN are counted in a similar way but two RELA-
TION tags match if the combination of their TR, LM
and SP is exactly the same. In other words a true pre-
diction requires all the three elements are correctly
predicted at the same time.
The last evaluation is on how well the systems are
able to retrieve the relations and their general type
370
i.e {region, direction, distance} at the same time.
To evaluate the GENERAL-TYPE similarly the RELA-
TION tag is checked. For a true prediction, an exact
match between the ground-truth and all the elements
of the predicted RELATION tag including TR, LM,SP
and GENERAL-TYPE is required.
7 Systems and results
One system with two runs was submitted from the
University of Texas Dallas. The two runs (Roberts
and Harabagiu, 2012), UTDSPRL-SUPERVISED1
and UTDSPRL-SUPERVISED2 are based on the
joint classification of the spatial triplets in a bi-
nary classification setting. To produce the candi-
date (indicator, trajector, landmark) triples, in the
first stage heuristic rules targeting a high recall are
used. Then a binary support vector machine clas-
sifier is employed to predict whether a triple is a
spatial relation or not. Both runs start with a large
number of manually engineered features, and use
floating forward feature selection to select the most
important ones. The difference between the two
runs of UTDSPRL-SUPERVISED1 and UTDSPRL-
SUPERVISED2 is their feature set. Particularly, in
UTDSPRL-SUPERVISED1 a joint feature based on
the conjunctions (e.g. and, but) is considered before
running feature selection but this feature is removed
in UTDSPRL-SUPERVISED2.
The submitted runs are compared to a previous
system from the task organizers (Kordjamshidi et
al., 2011c) which is evaluated on the current data
with the same settings. This system, KUL-SKIP-
CHAIN-CRF, uses a skip chain conditional random
field (CRF) model (Sutton and MacCallum, 2006)
to annotate the sentence as a sequence. It considers
the long distance dependencies between the prepo-
sitions and nouns in the sentence.
The type and structure of the features used in the
UTD and KUL systems are different. In the UTD
system, the classifier works on triples and the fea-
tures are of two main types: (a) argument-specific
features about the trajector, landmark, or indicator
e.g., the landmark?s hypernyms, or the indicator?s
first token; and (b) joint features that consider two
or more of the arguments, e.g. the dependency path
between indicator and landmark. For more detail,
see (Roberts and Harabagiu, 2012). In the KUL sys-
Label Precsion Recall F1
TRAJECTOR 0.731 0.621 0.672
LANDMARK 0.871 0.645 0.741
SPATIAL-INDICATOR 0.928 0.712 0.806
RELATION 0.567 0.500 0.531
GENERAL-TYPE 0.561 0.494 0.526
Table 2: UTDSPRL-SUPERVISED1: The University
of Texas-Dallas system with a larger number of fea-
tures,test/train one split.
Label Precsion Recall F1
TRAJECTOR 0.782 0.646 0.707
LANDMARK 0.894 0.680 0.772
SPATIAL-INDICATOR 0.940 0.732 0.823
RELATION 0.610 0.540 0.573
GENERAL-TYPE 0.603 0.534 0.566
Table 3: UTDSPRL-SUPERVISED2: The University of
Texas-Dallas system with a smaller number of features,
test/train one split.
Label Precsion Recall F1
TRAJECTOR 0.697 0.603 0.646
LANDMARK 0.773 0.740 0.756
SPATIAL-INDICATOR 0.913 0.887 0.900
RELATION 0.487 0.512 0.500
Table 4: KUL-SKIP-CHAIN-CRF: The organizers? sys-
tem (Kordjamshidi et al, 2011c)- test/train one split.
tem, the classifier works on all tokens in a sentence,
and a number of linguistically motivated local and
pairwise features over candidate words and preposi-
tions are used. To consider long distance dependen-
cies a template, called a preposition template, is used
in the general CRF framework. Loopy belief prop-
agation is used for inference. Mallet2 and GRMM:3
implementations are employed there.
Tables 2, 3 and 4 show the results of the three
runs in the standard setting of the shared task us-
ing the train/test split. In this evaluation setting the
UTDSPRL-SUPERVISED2 run achieves the highest
performance on the test set, with F1 of 0.573 for
the full triplet identification task, and an F1 of 0.566
for additionally classifying the triplet?s general-type
2http://mallet.cs.umass.edu/download.php
3http://mallet.cs.umass.edu/grmm/index.php
371
System Precsion Recall F1
KUL-SKIP-CHAIN-CRF 0.745 0.773 0.758
UTDSPRL-SUPERVISED2 0.773 0.679 0.723
Table 5: The RELATION extraction of KUL-SKIP-CHAIN-CRF (Kordjamshidi et al, 2011c) vs. UTDSPRL-
SUPERVISED2 evaluated with 10-fold cross validation
correctly. It also consistently outperforms both the
UTDSPRL-SUPERVISED1 run and the KUL-SKIP-
CHAIN-CRF system on each of the individual trajec-
tor, landmark and spatial-indicator extraction.
The dataset was relatively small, so we released
the test data and the two systems were addition-
ally evaluated using 10-fold cross validation. The
results of this cross-validation are shown in Ta-
ble 5. The UTDSPRL-SUPERVISED2 run achieves
a higher precision, while the KUL-SKIP-CHAIN-
CRF system achieves a higher recall. It should be
mentioned the 10-fold splits used by KUL and UTD
are not the same. This implies that the results with
exactly the same cross-folds may vary slightly from
these reported in Table 5.
Using 10-fold cross validation, we also evaluated
the classification of the general-type of a relation
given the manually annotated positive triplets. The
UTDSPRL-SUPERVISED2 system achieved F1=
0.974, and similar experiments using SMO-SVM in
(Kordjamshidi et al, 2011b; Kordjamshidi et al,
2011a) achieved F1= 0.973. Thus it appears that
identifying the general-type of a relation is a rela-
tively easy task on this data.
Discussion. Since the feature sets of the two sys-
tems are different and given the evaluation results
in the two evaluation settings, it is difficult to assert
which model is better in general. Obviously using
joint features potentially inputs richer information to
the model. However, it can increase the sparsity in
one hand and overfitting on the training data on the
other hand. Another problem is that finding heuris-
tics for high recall that are sufficiently general to be
used in every domain is not an easy task. By increas-
ing the number of candidates the dataset imbalance
will increase dramatically. This can cause a lower
performance of a joint model based on a binary clas-
sification setting when applied on different data sets.
It seems that this task might require a more elabo-
rated structured output prediction model which can
consider the joint features and alleviate the problem
of huge negatives in that framework while consider-
ing the correlations between the output components.
8 Conclusion
The SemEval-2012 spatial role labeling task is a
starting point to formally consider the extraction of
spatial semantics from the language. The aim is
to consider this task as a standalone linguistic task
which is important for many applications. Our first
practice on this task and the current submitted sys-
tem to SemEval 2012 clarify the type of the features
and the machine learning approaches appropriate for
it. The proposed features and models help to per-
form this task automatically in a reasonable accu-
racy. Although the spatial scheme is domain inde-
pendent, the achieved accuracy is dependent on the
domain of the used data for training a model. Our
future plan is to extend the data for the next work-
shops and to cover more semantic aspects of spatial
information particularly for mapping to formal spa-
tial representation models and spatial calculus.
Acknowledgments
Special thanks to Martijn Van Otterlo for his great
cooperation from the initiation phase and in the
growth of this task. Many thanks to Sabine
Drebusch for her kind and open cooperation in an-
notating the very first dataset. Thanks to Tigist Kas-
sahun for her help in annotating the current dataset.
Thanks the participant team of the University of
Texas Dallas and their useful feedback on the an-
notated data.
References
J. A. Bateman. 2010. Language and space: a two-level
semantic approach based on principles of ontological
engineering. International Journal of Speech Technol-
ogy, 13(1):29?48.
372
T. Butko, C. Nadeu, and A. Moreno. 2011. A multi-
lingual corpus for rich audio-visual scenedescription
in a meeting-room environment. In ICMI workshop
on multimodal corpora for machine learning: Taking
Stock and Roadmapping the Future.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
A. G. Cohn and J. Renz. 2008. Qualitative spatial repre-
sentation and reasoning. In Handbook of Knowledge
Representation, volume 3 of Foundations of Artificial
Intelligence, pages 551 ? 596. Elsevier.
A. Galton. 2009. Spatial and temporal knowledge rep-
resentation. Journal of Earth Science Informatics,
2(3):169?187.
M. Grubinger, P. Clough, Henning Mu?ller, and Thomas
Deselaers. 2006. The IAPR benchmark: A new evalu-
ation resource for visual information systems. In In-
ternational Conference on Language Resources and
Evaluation (LREC).
J. Hois and O. Kutz. 2008. Natural language meets spa-
tial calculi. In Christian Freksa, Nora S. Newcombe,
Peter Ga?rdenfors, and Stefan Wo?lfl, editors, Spatial
Cognition, volume 5248 of Lecture Notes in Computer
Science, pages 266?282. Springer.
J. Hois, R. J. Ross, J. D. Kelleher, and J. A. Bateman.
2011. Computational models of spatial language in-
terpretation and generation. In COSLI-2011.
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010a. From language towards formal spatial calculi.
In Workshop on Computational Models of Spatial Lan-
guage Interpretation (CoSLI 2010, at Spatial Cogni-
tion 2010).
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010b. Spatial role labeling: Task definition and anno-
tation scheme. In Proceedings of the Seventh confer-
ence on International Language Resources and Eval-
uation (LREC?10).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.-F.
Moens. 2011a. Machine learning for interpretation of
spatial natural language in terms of qsr. Poster Presen-
tation at the 10th International Conference on Spatial
Information Theory (COSIT?11).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.F.
Moens. 2011b. Learning to interpret spatial natural
language in terms of qualitative spatial relations. Rep-
resenting space in cognition: Interrelations of behav-
ior, language, and formal models. Series Explorations
in Language and Space, Oxford University Press, sub-
mitted.
P. Kordjamshidi, M. Van Otterlo, and M.F. Moens.
2011c. Spatial role labeling: Towards extraction of
spatial relations from natural language. ACM Trans.
Speech Lang. Process., 8:1?36, December.
I. Mani, J. Hitzeman, J. Richer, D. Harris, R. Quimby, and
B. Wellner. 2008. SpatialML: Annotation scheme,
corpora, and tools. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08). European Language Re-
sources Association (ELRA).
MITRE Corporation. 2010. SpatialML: Annotation
scheme for marking spatial expression in natural lan-
guage. Technical Report Version 3.0.1, The MITRE
Corporation.
J. Pustejovsky and J.L. Moszkowicz. 2009. Integrat-
ing motion predicate classes with spatial and tempo-
ral annotations. In CoLing 2008: Companion volume
Posters and Demonstrations, pages 95?98.
J. Pustejovsky, J. Moszkowicz, and M. Verhagen. 2011.
Iso-space: The annotation of spatial information in
language. In Proceedings of ISA-6: ACL-ISO Inter-
national Workshop on Semantic Annotation.
K. Roberts and S.M. Harabagiu. 2012. Utd-sprl: A joint
approach to spatial role labeling. In Submitted to this
workshop of SemEval-2012.
R. Ross, J. Hois, and J. Kelleher. 2010. Computational
models of spatial language interpretation. In COSLI-
2010.
O. Stock, editor. 1997. Spatial and Temporal Reasoning.
Kluwer.
C. Sutton and A. MacCallum. 2006. Introduction to con-
ditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statis-
tical Relational Learning. MIT Press.
D. A. Tappan. 2004. Knowledge-Based Spatial Rea-
soning for Automated Scene Generation from Text De-
scriptions. Ph.D. thesis.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, and
N. Roy A. G. Banerjee, S. Teller. 2011. Understand-
ing natural language commands for robotic naviga-
tion and mobile manipulation. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
San Francisco, CA.
J. Zlatevl. 2007. Spatial semantics. In Hubert Cuyck-
ens and Dirk Geeraerts (eds.) The Oxford Handbook
of Cognitive Linguistics, Chapter 13, pages 318?350.
373
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 83?87, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
KUL: A Data-driven Approach to Temporal Parsing of Documents
Oleksandr Kolomiyets
KU Leuven
Celestijnenlaan 200A
Heverlee 3001, Belgium
Department of Computer Science
oleksandr.kolomiyets
@cs.kuleuven.be
Marie-Francine Moens
KU Leuven
Celestijnenlaan 200A
Heverlee 3001, Belgium
Department of Computer Science
sien.moens@cs.kuleuven.be
Abstract
This paper describes a system for temporal
processing of text, which participated in the
Temporal Evaluations 2013 campaign. The
system employs a number of machine learning
classifiers to perform the core tasks of: identi-
fication of time expressions and events, recog-
nition of their attributes, and estimation of
temporal links between recognized events and
times. The central feature of the proposed sys-
tem is temporal parsing ? an approach which
identifies temporal relation arguments (event-
event and event-timex pairs) and the semantic
label of the relation as a single decision.
1 Introduction
Temporal Evaluations 2013 (TempEval-3) is
the third iteration of temporal evaluations (after
TempEval-1 (Verhagen et al, 2007) and TempEval-
2 (Verhagen et al, 2010)) which addresses the
task of temporal information processing of text. In
contrast to the previous evaluation campaigns where
the temporal relation recognition task was simpli-
fied by restricting grammatical context (events in
adjacent sentences, events and times in the same
sentences) and proposed relation pairs, TempEval-3
does not set any context in which temporal re-
lations have to be identified. Thus, for temporal
relation recognition the challenges consist of: first,
detecting a pair of events, or an event and a time
that constitutes a temporal relation; and, second,
determining what semantic label to assign to the
proposed pair. Moreover, TempEval-3 proposes the
task of end-to-end temporal processing in which
events and times, their attributes and relations have
to be identified from a raw text input.
In this paper we present a data-driven approach
to all-around temporal processing of text. A num-
ber of machine-learning detectors were designed to
recognize temporal ?markables? (events and times)
and their attributes. The key feature of our approach
is that argument pairs, as well as relations between
them, are jointly estimated without specifying in ad-
vance the context in which these pairs have to occur.
2 Our Approach
2.1 Timex Processing
2.1.1 Timex Recognition and Normalization
The proposed method for timex recognition im-
plements a supervised machine learning approach
that processes each chunk-phrase derived from the
parse tree. Time expressions are detected by the
model as phrasal chunks in the parse with their cor-
responding spans. In addition, the model is boot-
strapped by substitutions of temporal triggers with
their synonyms learned by the Latent Words Lan-
guage Model (Deschacht et al, 2012) as described in
(Kolomiyets et al, 2011). We implemented a logis-
tic regression model that makes use of the following
features:
? the head word of the phrase and its POS tag;
? all tokens and POS tags in the phrase as a bag
of words;
? the word-shape representation of the head word
and the entire phrase, e.g. Xxxxx 99 for the
expression April 30;
83
? the condensed word-shape representation for
the head word and the entire phrase, e.g. X(x)
(9) for the expression April 30;
? the concatenated string of the syntactic types of
the children of the phrase in the parse tree;
? the depth in the parse tree.
In addition, we considered a special label for sin-
gle tokens of time expressions. In this way, we
detect parts of temporal expressions if they cannot
be found in the chunk-based fashion. In detail, if
a token is recognized as part of a timex and satis-
fies the pre-condition on its POS tag, we employ a
?look-behind? rule for the phrasal chunk to match
the begin token of the temporal expression. The le-
gitimate start POS tags are determiners, adjectives,
and cardinals. Another set of rules specifies unsuit-
able timexes, such as single cardinals with values
outside predefined ranges of day-of-month, month-
of-year and year numbers.
Normalization of temporal expressions is a pro-
cess of estimating standard temporal values and
types for temporal expressions. Due to a large vari-
ance of expressions denoting the same date and
vagueness in language, rule-based approaches are
usually employed for the normalization task, and our
implementation is a rule-based system. The nor-
malization procedure is the same as described in
(Kolomiyets and Moens, 2010), which participated
in TempEval-2.
2.2 Event Processing
The proposed method to event recognition imple-
ments a supervised machine learning approach that
classifies every single token in the input sentence as
an event instance of a specific semantic type. We im-
plemented a logistic regression model with features
largely derived from the work of Bethard and Martin
(2006):
? the token, its lemma, coarse and fine-grained
POS tags, token?s suffixes and affixes;
? token?s hypernyms and derivations in Word-
Net;
? the grammatical class of the chunk, in which
the token occurs;
? the lemma of the governing verb of the token;
? phrasal chunks in the contextual window;
? the light verb feature for the governing verb;
? the polarity of the token?s context;
? the determiner of the token and the sentence?s
subject;
In addition, we classify the tense attribute for the
detected event by applying a set of thirteen hand-
crafted rules.
2.3 Temporal Relation Processing
Temporal relation recognition is the most difficult
task of temporal information processing, as it re-
quires recognitions of argument pairs, and subse-
quent classifications of relation types. Our ap-
proach employs a shift-reduce parsing technique,
which treats each document as a dependency struc-
ture of annotations labeled with temporal relations
(Kolomiyets et al, 2012). On the one hand, the ad-
vantage of the model is that the relation arguments
and the relation between them are extracted as a sin-
gle decision of a statistical classification model. On
the other hand, such a decision is local and might
not lead to the optimal global solution1. The follow-
ing features for deterministic shift-reduce temporal
parsing are employed:
? the token, its lemma, suffixes, coarse and fine-
grained POS tags;
? the governing verb, its POS tag and suffixes;
? the sentence?s root verb, its lemma and POS
tag;
? features for a prepositional phrase occurrence,
and domination by an auxiliary or modal verb;
? features for the presence of a temporal signal in
the chunk and co-occurrence in the same sen-
tence;
? a feature indicating if the sentence root verb
lemmas of the arguments are the same;
? the temporal relation between the argument and
the document creation time (DCT) (see below);
? a feature indicating if one argument is labeled
as a semantic role of the other;
? timex value generation pattern (e.g. YYYY-MM
for 2013-02, or PXY for P5Y) and timex
granularity (e.g. DAY-OF-MONTH for Friday,
MONTH-OF-YEAR for February etc.);
1For further details on the deterministic temporal parsing
model we refer the reader to (Kolomiyets et al, 2012).
84
Training Test P R F1
TimeBank
TimeBank
10-fold
0.907 0.99 0.947
AQUAINT 0.755 0.972 0.850
Silver 0.736 0.963 0.834
AQUAINT
TimeBank 0.918 0.986 0.951
AQUAINT
10-fold
0.795 0.970 0.874
Silver 0.746 0.959 0.851
Silver
TimeBank 0.941 0.976 0.958
AQUAINT 0.822 0.955 0.883
Silver 10-fold 0.798 0.944 0.865
Table 1: Results for timex detection in different corpora.
As one of the features above provides information
about the temporal relation between the argument
and the DCT, we employ an interval-based algebra
to classify relations between timexes and the DCT.
In case the argument is an event, we use a simple
logistic regression classifier with the following fea-
tures:
? the event token, its lemma, coarse and fine-
grained POS tags;
? tense, polarity, modality and aspect attributes;
? the token?s suffixes;
? the governing verb, its POS tag, tense and the
grammatical class of the chunk, in which the
event occurs;
? preceding tokens of the chunk;
3 Results
3.1 Pre-Evaluation Results
The following results are obtained by 10-fold cross-
validations and corpus cross-validations with re-
spect to the evaluation criteria and metrics used in
TempEval-2. Tables 1 and 2 present the results for
the timex recognition and normalization tasks (Task
A), and, Tables 3 and 4 present the results for the
event recognition task (Task B).
As can be seen from the pre-evaluation results, the
most accurate classification of timexes on all cor-
pora in terms of F1 score is achieved for the model
trained on the Silver corpus. As for timex normaliza-
tion, the performances on TimeBank and the Silver
Test Corpus Type Acc. Value Acc.
TimeBank 0.847 0.742
AQUAINT 0.852 0.714
Silver 0.853 0.739
Table 2: Results for normalization in different corpora.
Training Test P R F1
TimeBank
TimeBank
10-fold
0.82 0.641 0.72
AQUAINT 0.864 0.649 0.741
Silver 0.888 0.734 0.804
AQUAINT
TimeBank 0.766 0.575 0.657
AQUAINT
10-fold
0.900 0.776 0.836
Silver 0.869 0.755 0.808
Silver
TimeBank 0.827 0.717 0.768
AQUAINT 0.906 0.807 0.854
Silver 10-fold 0.916 0.888 0.902
Table 3: Results for event detection in different corpora.
Training Test Class Acc.
TimeBank
TimeBank 10-fold 0.691
AQUAINT 0.717
Silver 0.804
AQUAINT
TimeBank 0.620
AQUAINT 10-fold 0.830
Silver 0.794
Silver
TimeBank 0.724
AQUAINT 0.829
Silver 10-fold 0.900
Table 4: Results for event classification in different cor-
pora.
corpus are not very different for type and value accu-
racies. Similarly, we observe the tendency for a bet-
ter performance on larger datasets with an exception
for 10-fold cross-validation using the AQUAINT
corpus.
3.2 Evaluation Results
For the official evaluations we submitted three runs
of the system, one of which addresses Tasks A
and B (timex and event recognition)2, one (KUL-
2During the official evaluation period, this run was re-
submitted with no changes in the output together with KUL-
TE3RunABC, which led to duplicate evaluation results known
85
Run Relaxed Evaluation
P R F1 Rank
KULRun-1 0.929 0.769 0.836 21/23
KUL-
TE3RunABC
0.921 0.754 0.829 22/23
Run Strict Evaluation
P R F1 Rank
KULRun-1 0.77 0.63 0.693 22/23
KUL-
TE3RunABC
0.814 0.667 0.733 15/23
Table 5: Results for the timex detection task.
TE3RunABC) provides a full temporal informa-
tion processing pipeline (Task ABC), and the one
for Task C only (KUL-TaskC). For KULRun-1 we
employed the recognition models described above,
all trained on the aggregated corpus comprising
all three available training corpora in the evalua-
tions. For KUL-TE3RunABC we also trained the
markable recognition models on the aggregated cor-
pus, but the event recognition output was slightly
changed in order to merge multiple consequent
events of the same semantic class into a single multi-
token event. The temporal dependency parsing
model was trained on the TimeBank and AQUAINT
corpora only, with a reduced set of relation labels.
This decision was motivated by the time constraints
and the training time needed. The final relation la-
bel set contains the following temporal relation la-
bels: BEFORE, AFTER, DURING, DURING INV,
INCLUDES and IS INCLUDED. Below we present
the obtained results for each task separately. The re-
sults for Task A are presented in Tables 5 and 6, for
Task B in Tables 7 and 8, and, for Task ABC and
Task-C-only in Table 9. It is worth mentioning that
for Task B the aspect value was provided as NONE,
thus this evaluation criterion is not representative for
our system.
4 Conclusion
For TempEval-3 we proposed a number of statisti-
cal and rule-based approaches. For Task A we em-
ployed a logistic regression classifier whose output
as KULRun-1 and KULRun-2. Further in the paper, we refer to
this run as simply to KULRun-1.
Run Rank
KULRun-1
F1
Value Type
18/23
0.629 0.741
Accuracy
Value Type
14/23
0.752 0.886
KUL-
TE3RunABC
F1
Value Type
19/23
0.621 0.733
Accuracy
Value Type
15/23
0.750 0.885
Table 6: Results for the timex normalization task.
Run P R F1 Rank
KULRun-1 0.807 0.779 0.792 5/15
KUL-
TE3RunABC
0.776 0.765 0.77 12/15
Table 7: Results for the event detection task.
Run Rank
KULRun-1
F1
Class Tense Aspect
3/15
0.701 n.a. n.a.
Accuracy
Class Tense Aspect
3/15
0.884 n.a. n.a.
KUL-
TE3RunABC
F1
Class Tense Aspect
5/15
0.687 0.497 0.632
Accuracy
Class Tense Aspect
1/15
0.891 0.644 0.82
Table 8: Results for the event attribute recognition task.
Run P R F1 Rank
KUL-
TE3RunABC
0.18 0.202 0.191 8/8
KUL-TaskC 0.234 0.265 0.248 10/13
Table 9: Results for Tasks ABC (end-to-end processing)
and C (gold entities are given).
was augmented by a small number of hand-crafted
rules to increase the recall. For the temporal ex-
86
pression normalization subtask we employed a rule-
based system which estimates the attribute values for
the recognized timexes. For Task B we proposed
a logistic regression classifier which processes in-
put tokens and classifies them as event instances of
particular semantic classes. The optional tense at-
tribute was estimated by a number of manually de-
signed rules. For the most difficult tasks, Task ABC
and Task C, we proposed a dependency parsing tech-
nique that jointly learns from data what arguments
constitute a temporal relation and what the temporal
relation label is. Due to evaluation time constraints
and the time needed to model training, we reduced
the set of relation labels and trained the model on
two small annotated corpora.
The evaluations evidenced that the use of larger
annotated data sets did not improve the timex recog-
nition performance as it was expected from the pre-
evaluations. Interestingly, we did not observe the ex-
pected improvement in terms of recall, as it was the
case in the pre-evaluations. Yet, the timex normal-
ization performance levels in the official evaluations
were slightly higher than in the pre-evaluations. In
contrast to timex recognition, the use of a large an-
notated corpus improved the results for event recog-
nition. The pilot implementation of a temporal
parser for newswire articles showed the lowest per-
formance in the evaluations for Task ABC, but still
provided decent results for Task C. One of the ad-
vantages of the proposed temporal parser is that the
parser selects arguments for a temporal relation and
classifies it at the same time. The decision is drawn
by a statistical model trained on the annotated data,
that is, the parser does not consider any particular
predefined grammatical context in which the relation
arguments have to be found. Another weak point of
the parser is that it requires a large volume of high-
quality annotations and long training times. The last
two facts made it impossible to fully evaluate the
proposed temporal parsing model, and we will fur-
ther investigate the effectiveness of the model.
Acknowledgments
The presented research was supporter by the TER-
ENCE (EU FP7-257410) and MUSE (EU FP7-
296703) projects.
References
Steven Bethard and James H Martin. 2006. Identification
of Event Mentions and their Semantic Class. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 146?154.
Association for Computational Linguistics.
Koen Deschacht, Jan De Belder, and Marie-Francine
Moens. 2012. The Latent Words Language Model.
Computer Speech & Language.
Oleksandr Kolomiyets and Marie-Francine Moens. 2010.
Kul: Recognition and Normalization of Temporal Ex-
pressions. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 325?328.
Association for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 271?276.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting Narrative Time-
lines as Temporal Dependency Structures. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 88?97. Association
for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval Temporal
Relation Identification. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, pages
75?80. Association for Computational Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62. As-
sociation for Computational Linguistics.
87
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 255?262, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 3: Spatial Role Labeling
Oleksandr Kolomiyets?, Parisa Kordjamshidi?,
Steven Bethard? and Marie-Francine Moens?
?KU Leuven, Celestijnenlaan 200A, Heverlee 3001, Belgium
?University of Colorado, Campus Box 594 Boulder, Colorado, USA
Abstract
Many NLP applications require information
about locations of objects referenced in text,
or relations between them in space. For ex-
ample, the phrase a book on the desk contains
information about the location of the object
book, as trajector, with respect to another ob-
ject desk, as landmark. Spatial Role Label-
ing (SpRL) is an evaluation task in the infor-
mation extraction domain which sets a goal
to automatically process text and identify ob-
jects of spatial scenes and relations between
them. This paper describes the task in Se-
mantic Evaluations 2013, annotation schema,
corpora, participants, methods and results ob-
tained by the participants.
1 Introduction
Spatial Role Labeling at SemEval-2013 is the sec-
ond iteration of the task, which was initially in-
troduced at SemEval-2012 (Kordjamshidi et al,
2012a). The second iteration extends the previous
work with an additional training corpus, which con-
tains besides ?static? spatial relations, annotated mo-
tions. Motion detection is a novel task for annotating
trajectors (objects, which are moving), landmarks
(spatial context in which the motion is performed),
motion indicators (lexical triggers which signals tra-
jector?s motion), paths (a path along which the mo-
tion is performed), directions (absolute or relative
directions of trajector?s motion) and distances (a
distance as a product of motion). For annotating
motions the existing annotation scheme has been
adapted with additional markables which are, all to-
gether, described below.
2 Spatial Annotation Schema
In this Section we describe the annotation format of
spatial markables in text, and annotation guidelines
for the annotators.
2.1 Spatial Annotation Format
Building upon the previous work, we used the no-
tions of trajectors, landmarks and spatial indicators
as introduced by Kordjamshidi et al (2010). In ad-
dition, we further expanded the set of spatial roles
labels with motion indicators, paths, directions and
distances to capture fine-grained spatial semantics of
static spatial relations (as the ones which do not in-
volve motions), and to accommodate dynamic spa-
tial relations (the ones which do involve motions).
2.1.1 Static Spatial Relations and their Roles
Static spatial relations are defined as relations be-
tween still objects, whereas one object plays a cen-
tral role in the spatial scene, which is called tra-
jector, and the second one plays a secondary role,
and it is called landmark. In language, a spatial re-
lation between two objects is usually implemented
by a preposition (in, on, at, etc.) or a prepositional
phrase (on top of, inside of, etc.).
A static spatial relation is defined as a tuple that
contains a trajector, a landmark and a spatial indica-
tor. In the annotation schema, these annotations are
defined as follows:
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase that denotes a central
object of a spatial scene. For example:
? [Trajector a lake] in the forest
255
? [Trajector a flag] on top of the building
Landmark: Landmark is a spatial role label as-
signed to a word or a phrase that denotes a secondary
object of a spatial scene, to which a possible spatial
relation (as between two objects in space) can be es-
tablished. For example:
? a lake in [Landmark the forest]
? a flag on top of [Landmark the building]
Spatial Indicator: Spatial Indicator is a spatial
role label assigned to a word or a phrase that sig-
nals a spatial relation between objects (trajectors and
landmarks) of a spatial scene. For example:
? a lake [Sp indicator in] the forest
? a flag] [Sp indicator on top of ] the building
Spatial Relation: Spatial Relation is a relation
that holds between spatial markables in text as, e.g.,
between a trajector and a landmark and triggered by
a spatial indicator. In spatial information theory the
relations and properties are usually grouped into the
domains of topological, directional, and distance re-
lations and also shape (Stock, 1998). Three semantic
classes for spatial relations were proposed:
? Region. This type refers to a region of space
which is always defined in relation to a land-
mark, e.g., the interior or exterior. For exam-
ple:
a lake in the forest =? ?Region, [Sp indicator
in], [Trajector a lake], [Landmark the forest]?
? Direction. This relation type denotes a direc-
tion along the axes provided by the different
frames of reference, in case the trajector of mo-
tion is not characterized in terms of its relation
to the region of a landmark. For example:
a flag on top of the building =? ?Direction,
[Sp indicator on top of ], [Trajector a flag],
[Landmark the building]?
? Distance. Type Distance states information
about the spatial distance of the objects and
could be a qualitative expression, such as close,
far or quantitative, such as 12 km. For example:
the kids are close to the blackboard =?
?Distance, [Distance close], [Trajector the kids],
[Landmark the blackboard]?
2.1.2 Dynamic Spatial Relations
In addition to static spatial relations and their
roles, SpRL-2013 introduces new spatial roles to
capture dynamic spatial relations which involve
motions. Let us demonstrate this with the following
example:
(1) In Brazil coming from the North-East I
stepped into the small forest and followed down a
dried creek.
The text above describes a motion, and the reader
can identify a number of concepts which are pecu-
liar for motions: there is an object whose location
is changing, the motion is performed in a specific
spatial context, with a specific direction, and with a
number of locations related to the object?s motion.
There has been an enormous effort in formalizing
and annotating motions in natural language. While
annotating motions was out of scope for the previ-
ous SpRL task and SpatialML (Mani et al, 2010),
the most recent work on the Dynamic Interval Tem-
poral Logic (DITL) (Pustejovsky and Moszkowicz,
2011) presents a framework for modeling motions
as a change of state, which adapts linguistic back-
ground considering path constructions and manner-
of-motion constructions. On this basis the Spa-
tiotemporal Markup Language (STML) has been in-
troduced for annotating motions in natural language.
In STML, a motion is treated as a change of location
over time, while differentiating between a number
of spatial configurations along the path. Being well-
defined for the formal representations of motion and
reasoning, in which representations either take ex-
plicit reference to temporal frames or reify a spatial
object for a path, all the previous work seems to be
difficult to apply in practice when annotating mo-
tions in natural language. It can be attributed to pos-
sible vague descriptions of path in natural language
when neither clear temporal event ordering, nor dis-
tinction between the start, end or intermediate path
point can be made.
In SpRL-2013, we simplify the previously intro-
duced notion of path in order to provide practical
motion annotations. For dynamic spatial relations
we introduce the following roles:
256
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase which denotes an object
which moves, starts, interrupts, resumes a motion, or
is forcibly involved in a motion. For example:
? ... coming from the North-East [Trajector I]
stepped into ...
Motion Indicator: Motion indicator is a spatial
role label assigned to a word or a phrase which sig-
nals a motion of the trajector along a path. In Exam-
ple (1), a number of motion indicators can be identi-
fied:
? ... [Motion coming] from the North-East I
[Motion stepped into] ... and [Motion followed
down] ...
Path: Path is a spatial role label assigned to a word
or phrase that denotes the path of the motion as the
trajector is moving along, starting in, arriving in or
traversing it. In SpRL-2013, as opposite to STML,
the notion of path does not have the temporal dimen-
sion, thus whenever the motion is performed along a
path, for which either a start, an intermediate, an end
path point, or an entire path can be identified in text,
they are labeled as path. In Example (1), a number
of path labels can be identified:
? ... coming [Path from the North-East] I stepped
into [Path the small forest] and followed down
[Path a dried creek].
Landmark: The notion of path should not be con-
fused with landmarks. For spatial annotations, land-
mark has been introduced as a spatial role label for
a secondary object of the spatial scene. Being of
great importance for static spatial relations, in dy-
namic spatial relations, landmarks are used to cap-
ture a spatial context of a motion as for example:
? In [Landmark Brazil] coming from the North-
East ...
Distance: In contrast to the previous SpRL anno-
tation standard, in which distances and directions
have been uniformly treated as signals, in SpRL-
2013 if the motion is performed for a certain dis-
tance, and such a distance is mentioned in text, the
corresponding textual span is labeled as distance.
Distance is a spatial role label assigned to a word
or a phrase that denotes an absolute or relative dis-
tance of motion, or the distance between a trajector
and a landmark in case of a static spatial scene. For
example:
? [Distance 25 km]
? [Distance about 100 m]
? [Distance not far away]
? [Distance 25 min by car]
Direction: Additionally, if the motion is per-
formed in a certain (absolute or relative) direction,
and such a direction is mentioned in text, the corre-
sponding textual span is annotated as direction. Di-
rection is a spatial role label assigned to a word or
a phrase that denotes an absolute or relative direc-
tion of motion, or a spatial arrangement between a
trajector and a landmark. For example:
? [Direction the North-West]
? [Direction northwards]
? [Direction west]
? [Direction the left-hand side]
Spatial Relation: Similarly to static spatial rela-
tions, dynamic spatial relations are annotated by re-
lations that hold between a number of spatial roles.
The major difference to static spatial relations is the
mandatory motion indicator1. For example:
? In Brazil coming from the North-East I ...
=? ?Direction, [Sp indicator In], [Trajector I],
[Landmark Brazil], [Motion coming],[Path from
the North-East]?
? ... I stepped into the small forest and ...
=? ?Direction, [Trajector I], [Motion stepped
into],[Path the small forest]?
? ... I [...] and followed down a dried creek.
=? ?Direction, [Trajector I], [Motion followed
down],[Path a dried creek]?
1All dynamic spatial relations were annotated with type Di-
rection.
257
Corpus Files Sent. TR LM SI MI Path Dir Dis Relation
IAPR TC-12
Training 1 600 716 661 670 - - - - 765
Evaluation 1 613 872 743 796 - - - - 940
Confluence
Project
Training 95 1422 1701 1037 879 1039 945 223 307 2105
Evaluation 22 367 497 316 247 305 240 37 87 598
Table 1: Corpus statistics for SpRL-2013 with respect to annotated spatial roles (trajectors (TR), landmarks (LM),
spatial indicators (SI), motion indicators (MI), paths (Path), directions (Dir) and distances (Dis)) and spatial relations.
3 Corpora
The data for the shared task comprises two different
corpora.
3.1 IAPR TC-12 Image Benchmark Corpus
The first corpus is a subset of the IAPR TC-12 image
benchmark corpus (Grubinger et al, 2006). It con-
tains 613 text files that include 1213 sentences in to-
tal, and represents an extension of the dataset previ-
ously used in (Kordjamshidi et al, 2011). The orig-
inal corpus was available free of charge and without
copyright restrictions. The corpus contains images
taken by tourists with descriptions in different lan-
guages. The texts describe objects, and their abso-
lute and relative positions in the image. This makes
the corpus a rich resource for spatial information,
however, the descriptions are not always limited to
spatial information. Therefore, they are less domain-
specific and contain free explanations about the im-
ages. For training we released 600 sentences (about
50% of the corpus), and used remaining 613 sen-
tences for evaluations.
3.2 Confluence Project Corpus
The second corpus comes from the Confluence
project that targets the description of locations sit-
uated at each of the latitude and longitude inte-
ger degree intersection in the world. This corpus
contains user-generated content produced by, some-
times, non-native English speakers. We gathered the
content by keeping the original orthography and for-
mating. In addition, we stored the URLs of the de-
scriptions and extracted the coordinates of the de-
scribed confluence point, which might be interest-
ing for further research. In total, the entire corpus
contains 117 files with 1789 sentences (about 40,000
tokens). For training we released 95 annotated files
with 1422 sentences, 2105 annotated relations in to-
tal. For evaluation we used 22 annotated files with
367 sentences. The statistics on both corpora are
provided in Table 1.
3.3 Data Format
One important change to the data was made in
SpRL-2013. In contrast to SpRL-2012, where spa-
tial roles were annotated over ?head words? whose
indexes were part of unique identifiers, in SpRL-
2013 we switched to span-based annotations. More-
over, in order to provide a single data format for
the task, we transformed SpRL-2012 data into span-
based annotations, in course of which, we identified
a number of annotation errors and made further im-
provements for about 50 annotations.
For annotating the Confluence Project corpus we
used a freely available annotation tool MAE created
by Amber Stubbs (Stubbs, 2011). The resulting data
format uses the same annotation tags as in SpRL-
2012, but each role annotation refers to a character
offset in the original text2. Spatial relations are com-
posed of references to annotations by their unique
identifiers. Similarly to SpRL-2012, we allowed
annotators to provide non-consuming annotations,
where entity mentions, for which spatial roles can
be identified, are omitted in text but necessary for a
spatial relation triggered by either a spatial indicator
or a motion indicator. Two spatial roles are eligible
for non-consuming annotations: trajectors and land-
marks.
4 Tasks Descriptions
For the sake of consistency with SpRL-2012, in
SpRL-2013 we proposed the following tasks:
2Due to paper length constraints we omit the BNF specifica-
tions for spatial roles and relations. For further data format in-
formation we refer the reader to the task description web page:
www.cs.york.ac.uk/semeval-2013/task3/
258
? Task A: Identification of markable spans for
three types of spatial annotations such as tra-
jector, landmark and spatial indicator.
? Task B: Identification of tuples (triplets) that
connect trajectors, landmarks and spatial indi-
cators identified in Task A into spatial relations.
That is, identification of spatial relations with
three markables connected, and without se-
mantic relation classification.
? Task C: Identification of markable spans for all
spatial annotations such as trajector, landmark,
spatial indicator, motion indicator, path, direc-
tion and distance.
? Task D: Identification of n-tuples that connect
spatial markables identified in Task C into spa-
tial relations. That is, identification of spatial
relations with as many participating mark-
ables as possible, and without semantic rela-
tion classification.
? Task E: Semantic classification of spatial rela-
tions identified in Task D.
5 Evaluation Criteria and Metrics
System outputs were evaluated against the gold
annotations, which had to conform to the role?s
Backus-Naur form. For Tasks A and C, the system
annotations are spatial roles: spans of text associated
with spatial role types. A system annotation of a
role is considered correct if it has a minimal overlap
of one character with a gold annotation and matches
the role type of the gold annotation. For Tasks B and
D, the system annotations are spatial relation tuples
(of length 3 in task B, of length 3 to 5 in Task D) of
references to markable annotations. A system anno-
tation of a spatial relation tuple is considered correct
if it is of the same length as the gold annotation, and
if each spatial role in the system tuple matches each
role in the gold tuple. A spatial role estimated by a
system is considered correct if it matches a gold ref-
erence when having the same character offsets and
markable types (strict evaluation settings). In ad-
dition we introduced relaxed evaluation settings, in
which a minimal overlap of one character between
a system and a gold markable references is required
for a positive match under condition that the roles
match. For Task E, the system annotations are spa-
tial relation tuples of length 3 to 5, along with re-
lation type labels. A system annotation of a spatial
relation is considered correct if the spatial relation
tuple is correct under the evaluation of Task D and
the relation type of the system relation is the same
as the relation type of the gold relation.
Systems were evaluated for each of the tasks in
terms of precision (P), recall (R) and F1-score which
are defined as follows:
Precision =
tp
tp + fp
(1)
Recall =
tp
tp + fn
(2)
where tp is the number of true positives (the num-
ber of instances that are correctly found), fp is the
number of false positives (number of instances that
are predicted by the system but not a true instance),
and fn is the number of false negatives (missing re-
sults).
F1 = 2 ?
Precision ?Recall
Precision + Recall
(3)
6 System Description and Evaluation
Results
UNITOR. The UNITOR-HMM-TK system ad-
dressed Tasks A,B and C (Bastianelli et al, 2013).
In Tasks A and C, roles are labeled by a sequence-
based classifier: each word in a sentence is classi-
fied with respect to the possible spatial roles. An
approach based on the SVM-HMM learning algo-
rithm, formulated in (Tsochantaridis et al, 2006),
was used. It is in line with other methods based
on sequence-based classifier for Spatial Role La-
beling, such as Conditional Random Fields (Kord-
jamshidi et al, 2011), and the same SVM-HMM
learning algorithm (Kordjamshidi et al, 2012b).
UNITOR?s labeling approach has been inspired by
the work in (Croce et al, 2012), where an SVM-
HMM learning algorithm has been applied to the
classical FrameNet-based Semantic Role Labeling.
The main contribution of the proposed approach is
the adoption of shallow grammatical features instead
of the full syntax of the sentence, in order to avoid
over-fitting on the training data. Moreover, lexical
information has been generalized through the use
259
Run Task Evaluation Label P R F1-score
UNITOR.Run1.1
Task A relaxed
TR 0.684 0.681 0.682
LM 0.741 0.835 0.785
SI 0.967 0.889 0.926
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run1.2
Task A relaxed
TR 0.682 0.493 0.572
LM 0.801 0.560 0.659
SI 0.968 0.585 0.729
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run2.1
Task A relaxed
TR 0.565 0.317 0.406
LM 0.661 0.476 0.554
SI 0.612 0.481 0.538
Task C relaxed
TR 0.565 0.317 0.406
LM 0.662 0.476 0.554
SI 0.609 0.479 0.536
MI 0.892 0.294 0.443
Path 0.775 0.295 0.427
Dir 0.312 0.229 0.264
Dis 0.946 0.331 0.490
Table 2: Results of UNITOR for SpRL-2013 tasks (Task A, B and C).
of Word Space ? a Distributional Model of Lexi-
cal Semantics derived from the unsupevised anal-
ysis of an unlabeled large-scale corpus (Sahlgren,
2006). Similarly to the approaches demonstrated
in SpRL-2012, the proposed approach first classi-
fies spatial and motion indicators, then, using these
outcomes further spatial roles are determined. For
classifying indicators, the classifier makes use of
lexical and grammatical features like lemmas, part-
of-speech tags and lexical context representations.
The remaining spatial roles are estimated by another
classifier additionally employing the lemma of the
indicator, distance and relative position to the indi-
cator, and the number of tokens composing the indi-
cator as features.
In Task B, all roles found in a sentence for Task A
are combined to generate candidate relations, which
are verified by a Support Vector Machine (SVM)
classifier. As the entire sentence is informative
to determine the proper conjunction of all roles, a
Smoothed Partial Tree Kernel (SPTK) within the
classifier that enhances both syntactic and lexical in-
formation of the examples was applied (Croce et al,
2011). This is a convolution kernel that measures the
similarity between syntactic structures, which are
partially similar and whose nodes can be different,
but are, nevertheless, semantically related. Each ex-
ample is represented as a tree-structure which is di-
rectly derived from the sentence dependency parse,
and thus allows for avoiding manual feature engi-
neering as in contrast to the work of Roberts and
Harabagiu (2012). In the end, the similarity score
between lexical nodes is measured by the Word
Space model.
UNITOR submitted two runs for the IAPR TC-
12 Image benchmark corpus (we refer to them
as to UNITOR.Run1.1 and UNITOR.Run1.2) and
one run for the Confluence Project corpus (UN-
ITOR.Run2.1), based on the models individually
trained on the different corpora. The difference
between UNITOR.Run1.1 and UNITOR.Run1.2 is
that for UNITOR.Run1.1 the results are obtained for
all spatial roles (also the ones that have no spatial
relation), and UNITOR.Run1.2 only provided the
roles for which also spatial relations were identified.
The results are presented in Table 2.
260
Although, not directly comparable to the results in
SpRL-2012, one may observe some common trends.
First, similarly to the previous findings, the perfor-
mance for recognition of landmarks and spatial in-
dicators (Task A) on the IAPR TC-12 Image bench-
mark corpus is better than trajectors (F1-scores of
0.785, 0.926 and 0.682 respectively), and spatial in-
dicators is the ?easiest? spatial role to recognize (F1-
score of 0.926).
In contrast, spatial role labeling on the Confluence
Project corpus performs worse than on the IAPR
TC-12 Image benchmark corpus (with F1-scores of
0.406, 0.538 and 0.554 for trajectors, spatial indica-
tors and landmarks respectively). Interestingly, the
performance for landmarks is generally higher than
for trajectors, which is in line with previous findings
in SpRL-2012. The performance drop on the new
corpus can be attributed to more complex text and
descriptions, whereas multiple roles can be identi-
fied for the same span (for example, a path which
spans over trajectors, landmarks and spatial indica-
tors). For the new spatial roles of motion indicators,
paths, directions and distances, the performance lev-
els are overall higher than for trajectors with an ex-
ception of directions. Yet, the precision levels for
new roles is much higher than the recall (0.892 vs.
0.294 for motion indicators, 0.775 vs. 0.295 for
paths and 0.946 vs. 0.331 for distances). Directions
turned out to be the most difficult role to classify
(0.312, 0.229 and 0.264 for P , R and F1-score re-
spectively).
7 Conclusion
In this paper we described an evaluation task on Spa-
tial Role Labeling in the context of Semantic Evalu-
ations 2013. The task sets a goal to automatically
process text and identify objects of spatial scenes
and relations between them. Building largely upon
the previous evaluation campaign, SpRL-2012, in
SpRL-2013 we introduced additional spatial roles
and relations for capturing motions in text. In ad-
dition, a new annotated corpus for spatial roles (in-
cluding annotated motions) was produced and re-
leased to the participants. It comprises a set of 117
files with about 40,000 tokens in total.
With the registered number of 10 participants and
the final number of submissions (only one) we can
conclude that spatial role labeling is an interesting
task within the research community, however some-
times underestimated in its complexity. Our further
steps in promoting spatial role labeling will be a de-
tailed description of the annotation scheme and an-
notation guidelines, analysis of the corpora and ob-
tained results.
Acknowledgments
The presented research was supporter by the PARIS
project (IWT - SBO 110067), TERENCE (EU FP7?
257410) and MUSE (EU FP7?296703).
References
Emanuele Bastianelli, Danilo Croce, Roberto Basili, and
Daniele Nardi. 2013. UNITOR-HMM-TK: Struc-
tured Kernel-based learning for Spatial Role Labeling.
In Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013). Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-
tianelli. 2012. Structured Learning for Semantic Role
Labeling. Intelligenza Artificiale, 6(2):163?176.
Michael Grubinger, Paul Clough, Henning Mu?ller, and
Thomas Deselaers. 2006. The IAPR TC-12 Bench-
mark: A New Evaluation Resource for Visual Informa-
tion Systems. In International Workshop OntoImage,
pages 13?23.
Parisa Kordjamshidi, Marie-Francine Moens, and Mar-
tijn van Otterlo. 2010. Spatial Role Labeling: Task
Definition and Annotation Scheme. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation (LREC?10), pages 413?420.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial Role Labeling: To-
wards Extraction of Spatial Relations from Natural
Language. ACM Transactions on Speech and Lan-
guage Processing (TSLP), 8(3):4.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012a. Semeval-2012 Task 3: Spa-
tial Role Labeling. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, pages
365?373. Association for Computational Linguistics.
Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
261
2012b. Relational Learning for Spatial Relation Ex-
traction from Natural Language. In Inductive Logic
Programming, pages 204?220. Springer.
Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-
man, Rob Quimby, Justin Richer, Ben Wellner, Scott
Mardis, and Seamus Clancy. 2010. SpatialML: Anno-
tation Scheme, Resources, and Evaluation. Language
Resources and Evaluation, 44(3):263?280.
James Pustejovsky and Jessica L Moszkowicz. 2011.
The Qualitative Spatial Dynamics of Motion in Lan-
guage. Spatial Cognition & Computation, 11(1):15?
44.
Kirk Roberts and Sanda M Harabagiu. 2012. UTD-
SpRL: A Joint Approach to Spatial Role Labeling. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 419?424. Association for
Computational Linguistics.
Magnus Sahlgren. 2006. The Word-space Model. Ph.D.
thesis, Stockholm University.
Oliviero Stock. 1998. Spatial and Temporal Reasoning.
Springer-Verlag New York Incorporated.
Amber Stubbs. 2011. MAE and MAI: Lightweight An-
notation and Adjudication Tools. In Proceedings of
the 5th Linguistic Annotation Workshop, LAW V ?11,
pages 129?133, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, Yasemin Altun, and Yoram Singer. 2006. Large
Margin Methods for Structured and Interdependent
Output Variables. Journal of Machine Learning Re-
search, 6(2):1453.
262
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 263?271,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
KU Leuven at HOO-2012: A Hybrid Approach to Detection and Correction
of Determiner and Preposition Errors in Non-native English Text
Li Quan, Oleksandr Kolomiyets, Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
3001 Heverlee, Belgium
li.quan@student.kuleuven.be
{oleksandr.kolomiyets, sien.moens}@cs.kuleuven.be
Abstract
In this paper we describe the technical im-
plementation of our system that participated
in the Helping Our Own 2012 Shared Task
(HOO-2012). The system employs a num-
ber of preprocessing steps and machine learn-
ing classifiers for correction of determiner and
preposition errors in non-native English texts.
We use maximum entropy classifiers trained
on the provided HOO-2012 development data
and a large high-quality English text collec-
tion. The system proposes a number of highly-
probable corrections, which are evaluated by a
language model and compared with the origi-
nal text. A number of deterministic rules are
used to increase the precision and recall of the
system. Our system is ranked among the three
best performing HOO-2012 systems with a
precision of 31.15%, recall of 22.08% and F1-
score of 25.84% for correction of determiner
and preposition errors combined.
1 Introduction
The Helping Our Own Challenge (Dale and Kilgar-
riff, 2010) is a shared task that was proposed to ad-
dress automated error correction of non-native En-
glish texts. In particular, the Helping Our Own 2012
Shared Task (HOO-2012) (Dale et al, 2012) focuses
on determiners and prepositions as they are well-
known sources for errors produced by non-native
English writers. For instance, Bitchener et al (2005)
reported error rates of respectively 20% and 29%.
Determiners are in particular challenging because
they depend on a large discourse context and world
knowledge, and moreover, they simply do not exist
in many languages, such as Slavic and South-East
Asian languages (Ghomeshi et al, 2009). The use
of prepositions in English is idiomatic and thus very
difficult for learners of English. On the one hand,
prepositions connect noun phrases to other words in
a sentence (e.g. . . . by bus), on the other hand, they
can also be part of phrasal verbs such as carry on,
hold on, etc.
In this paper we describe our system implemen-
tation and results in HOO-2012. The paper is struc-
tured as follows. Section 2 gives the task definition,
errors addressed, data resources and evaluation cri-
teria and metrics. Section 3 shows some background
and related work. Section 4 gives the full system de-
scription, while Section 5 reports and discusses the
results of the experiments. Section 6 concludes with
an error analysis and possible further improvements.
2 HOO-2012 Tasks and Resources
2.1 Tasks
In the scope of HOO-2012 the following six possible
error types1 are targeted:
? Replace determiner (RD):
Have the nice day. ? Have a nice day.
? Missing determiner (MD):
That is great idea. ? That is a great idea.
? Unnecessary determiner (UD):
I like the pop music. ? I like pop music.
1The set of error tags is based on the Cambridge University
Press Error Coding System, fully described in (Nicholls, 2003).
263
? Replace preposition (RT):
In the other hand. . . ? On the other hand. . .
? Missing preposition (MT):
She woke up 6 o?clock. ? She woke up at 6
o?clock.
? Unnecessary preposition (UT):
He must go to home. ? He must go home.
2.2 Data
The HOO development dataset consists of 1000
exam scripts drawn from a subset of the CLC FCE
Dataset (Yannakoudakis et al, 2011). This corpus
contains texts written by students who attended the
Cambridge ESOL First Certificate in English exam-
ination in 2000 and 2001. The entire development
dataset comprises 374680 words, with an average
of 375 words per file. The test data consists of a
further 100 files provided by Cambridge University
Press (CUP), with 18013 words, and an average of
180 words per file.
Type # Dev # Test A # Test B
RD 609 38 37
MD 2230 125 131
UD 1048 53 62
Det 3887 217 230
RT 2618 136 148
MT 1104 57 56
UT 822 43 39
Prep 4545 236 243
Total 8432 453 473
Words/Error 44.18 39.77 38.08
Table 1: Data error statistics.
Counts of the different error types are provided in
Table 1. The table shows counts for the development
dataset (?Dev?) and two versions of the gold stan-
dard test data: the original version as derived from
the CUP-provided dataset (?Test A?), and a revised
version (?Test B?) which was compiled in response
to requests for corrections from participating teams.
The datasets and the revision process are further ex-
plained in (Dale et al, 2012).
2.3 Evaluation Criteria and Metrics
For evaluation in the HOO framework, a distinction
is made between scores and measures. The com-
plete evaluation mechanism is described in detail in
(Dale and Narroway, 2012) and on the HOO-2012
website.2
Scores Three different scores are used:
1. Detection: does the system determine that an
edit of the specified type is required at some
point in the text?
2. Recognition: does the system correctly deter-
mine the extent of the source text that requires
editing?
3. Correction: does the system offer a correction
that is identical to that provided in the gold
standard?
Measures For each score, three measures are cal-
culated: precision (1), recall (2) and F -score (3).
precision =
tp
tp+ fp
(1)
recall =
tp
tp+ fn
(2)
where tp is the number of true positives (the num-
ber of instances that are correctly found by the sys-
tem), fp the number of false positives (the number
of instances that are incorrectly found), and fn the
number of false negatives (missing results).
F? = (1 + ?
2)
precision ? recall
?2 ? precision+ recall
(3)
where ? is used as a weight factor regulating the
trade-off between recall and precision. We use the
balanced F -score, i.e. ? = 1, such that recall and
precision are equally weighted.
Combined We provide results on prepositions and
determiners combined, and for each of these two
subcategories separately. We also report on each of
the different error types separately.
2See http://www.correcttext.org/hoo2012.
264
3 Related Work
HOO-2012 follows on from the HOO-2011 Shared
Task Pilot Round (Dale and Kilgarriff, 2011). That
task targeted a broader range of error types, and used
a much smaller dataset.
Most work on models for determiner and preposi-
tion generation has been developed in the context of
machine translation output (e.g. (Knight and Chan-
der, 1994), (Minnen et al, 2000), (De Felice and
Pulman, 2007) and (Toutanova and Suzuki, 2007)).
Some of these methods depend on full parsing of
text, which is not reliable in the context of noisy
non-native English texts.
Only more recently, models for automated error
detection and correction of non-native texts have
been explicitly developed and studied. Most of these
methods use large corpora of well-formed native En-
glish text to train statistical models, e.g. (Han et al,
2004), (Gamon et al, 2008) and (De Felice and Pul-
man, 2008). Yi et al (2008) used web counts to de-
termine correct article usage, while Han et al (2010)
trained a classifier solely on a large error-tagged
learner corpus for preposition error correction.
4 System Description
4.1 Global System Workflow
The system utilizes a hybrid approach that combines
statistical machine learning classifiers and a rule-
based system. The global system architecture is pre-
sented in Figure 1. This section describes the global
system workflow. The subsequent sections elabo-
rate on the machine learning classifiers and heuris-
tics implemented in the system.
The system workflow is divided in the following
processing steps:
1. Text Preprocessing: The system performs a
preliminary text analysis by automated spelling
correction and subsequent syntactic analysis,
such as tokenization and part-of-speech (POS)
tagging.
2. Error Detection, Recognition and Correction:
The system identifies if a correction is needed,
and the type and extent of that correction. Two
families of error correction tasks that separately
address determiners and prepositions are per-
formed in parallel.
3. Correction validation: Once a correction has
been proposed, it is validated by a language
model derived from a large corpus of high-
quality English text.
4.1.1 Text Preprocessing
In HOO-2012, texts submitted for automated cor-
rections are written by learners of English. Besides
the error types that are addressed in HOO-2012, mis-
spellings are another type of highly-frequent errors.
For example, one student writes the following: In my
point of vue, Internet is the most important discover
of the 2000 centery.
When using automated natural language process-
ing tools, incorrect spelling (and grammar) can in-
troduce an additional bias. To reduce the bias propa-
gated from the preprocessing steps, the text is first
automatically corrected by the open-source spell
checker GNU Aspell.3
At the next step, the text undergoes a shallow syn-
tactic analysis that includes sentence boundary de-
tection, tokenization, part-of-speech tagging, chunk-
ing, lemmatization, relation finding and preposi-
tional phrase attachment. These tasks are performed
by MBSP (De Smedt et al, 2010).4
4.1.2 Error Detection, Recognition and
Correction
In general, the task of automated error correction
is addressed by a number of subtasks of finding the
position in text, recognizing the type of error, and
the proposal for a correction. In our implementation
we approach these tasks in a two-step approach as
proposed in (Gamon et al, 2008). With two families
of errors, the system therefore employs four classi-
fiers in total.
For determiner error corrections, a classifier (C1
in Figure 1) first predicts whether a determiner is
required in the observed context. If it is required,
another classifier (C2 in Figure 1) estimates which
one. The same approach is employed for the prepo-
sition error correction task (classifiers C3 and C4 in
Figure 1). The details on how the classifiers were
implemented are highlighted in Section 4.2.
3http://aspell.net/
4MBSP is a text analysis system based on the TiMBL and
MBT memory based learning applications developed at CLiPS
and ILK (Daelemans and van den Bosch, 2005).
265
Figure 1: System architecture.
4.1.3 Correction Validation
Our error correction system implements a correc-
tion validation mechanism as proposed in (Gamon et
al., 2008). The validation mechanism makes use of
a language model that is derived from a large corpus
of English. We use a trigram language model trained
on the English Gigaword corpus with a 64K-word
vocabulary (using interpolated Kneser-Ney smooth-
ing with a bigram cutoff of 3 and trigram cutoff of
5).
The language model serves to increase the pre-
cision at the cost of recall as false positives can be
confusing for learners for English. The original sen-
tence and the error-corrected version are passed to
the language model. Only if the difference in proba-
bility of being generated by the language model ex-
ceeds a heuristic threshold (estimated using a tuning
set) is the correction finally accepted.
4.2 Machine Learning Classifiers
As already mentioned, the system employs four ma-
chine learning classifiers in total (C1?C4 ? two for
each family of errors). Classifiers C1 and C3 re-
spectively estimate the presence of determiners and
prepositions in the observed context. If one is ex-
pected, the second set of classifiers estimates which
one is the most likely.
For the determiner choice classifier (C2), we re-
strict the determiner choice class values to the indef-
inite and definite articles: a/an and the. The prepo-
sition choice class values for the preposition choice
classifier (C4) are restricted to set of the following
10 common prepositions: on, in, at, for, of, about,
from, to, by, with and (other).
All the classifiers are implemented by discrimina-
tive maximum entropy classification models (ME)
(Ratnaparkhi, 1998). Such models have been proven
effective for a number of natural language process-
ing tasks by combining heterogeneous forms of evi-
dence (Ratnaparkhi, 2010).
Training Classifiers and Inference As training
instances we consider each noun phrase (NP) in ev-
ery sentence of the training data. For the binary clas-
sifiers (C1 and C3), a positive example is a noun
phrase that follows a determiner/preposition, and a
negative example is one that does not. The multi-
class classifiers (C2 and C4) are trained respectively
to distinguish specific instances of determiners (defi-
nite and indefinite for C2) and the set of prepositions
mentioned above. For each classifier, a training in-
stance is represented by the following features:
? Tokens in NP.
? Tokens? POS tags in NP.
? Tokens? lemmas in NP.
? Tokens in a contextual window of 3 tokens to
the left and to the right from the potential cor-
rection position.
? Tokens? POS tags in a contextual window of 3
tokens from the potential correction position.
? Tokens? lemmas in a contextual window of 3
tokens from the potential correction position.
? Trigrams of concatenated tokens before and af-
ter NP.
266
? Trigrams of concatenated tokens? POS tags be-
fore and after NP.
? Trigrams of concatenated tokens? lemmas be-
fore and after NP.
? Head noun in NP.
? POS tag of head noun in NP.
? Lemma of head noun in NP.
Once the classification models have been derived,
the classifiers are ready to be employed in the sys-
tem. For the text correction task, each sentence
undergoes the same preprocessing analysis as de-
scribed in Section 4.1.1. Then, for each noun phrase
in the input sentence, we extract the feature con-
text, and use the models to predict the need for
the presence of a determiner or preposition, and if
so, which one. Our system only accepts classifier
predictions if they are obtained with a high confi-
dence. The confidence thresholds were empirically
estimated from pre-evaluation experiments with a
tuning dataset (Section 5.1).
4.3 Rule-based Modules
Our system also has a number of rule-based mod-
ules. The first rule-based module is in charge of
making the choice between a and an if the deter-
miner type classifier (C2) predicts the presence of
an indefinite determiner. The choice is determined
by a lookup in the CMU pronouncing dictionary5
(a/an CMU Dictionary in Figure 1). In this dictio-
nary each word entry is mapped to one or a number
of pronunciations in the phonetic transcription code
system Arpabet. If the pronunciation of the word
that follows the estimated correction position starts
with a consonant, a is used; if it starts with a vowel,
an is selected.
The second rule-based module corrects confusion
errors of determiner-noun agreement, e.g. this/these
and that/those (Definite Determiner in Figure 1). It
is implemented by introducing rules with patterns
based on whether the noun was tagged as singular
or plural.
The third rule-based module is used to filter out
unnecessary corrections proposed by the classifiers
5http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
(C1-C4) and augmented by the already described
rule-based modules. Each correction is examined
against the input text and if it yields a different text
than the original input text, such a correction is con-
sidered as a necessary correction.
However, sometimes automatically proposed cor-
rections have to be rejected because they are out of
scope of the addressed errors. We do not replace
possessive determiners such as my, your, his, our,
their by the definite article the. Similarly, some
prepositions can be grouped in opposite pairs, for
example from and to, for which we do not propose
any correction as it requires a deep semantic analysis
of text.
5 Experiments and Results
In this section we describe the pre-evaluation exper-
iments and the results of the final evaluation on the
HOO-2012 test set. Table 2 shows the characteris-
tics of the datasets used in the experiments.
Dataset Sentences Tokens
HOO training 21925 340693
HOO tuning 2560 40966
HOO held-out 2749 42325
Reuters 207083 5487021
Wikipedia 53370 1430428
HOO test 1376 20606
Table 2: Datasets used.
5.1 Pre-Evaluation Experiments
In the course of system development, we split the
files in the HOO development dataset into a train-
ing set (80%), a tuning set (10%) and a held-out test
set (10%). From the beginning it was clear that the
provided development dataset alne was too small to
address the automated error correction tasks by em-
ploying machine learning classification techniques.
Additionally to that dataset, we used a set of Reuters
news data and the Wikipedia corpus for training the
classifiers.
Once the classification models had been derived,
the system was evaluated on the tuning data and ad-
justed in order to increase the overall performance.
267
After that, the system was evaluated on the held-out
test set for which the results are shown in Table 3.
Type Precision Recall F1-score
Det 64.11 14.89 24.17
Prep 52.32 16.38 25.32
All 60.19 15.38 24.50
Table 3: Correction results on held-out test set.
5.2 Final System Configuration and Evaluation
Results
For the final evaluation, we retrained the models us-
ing the complete HOO development data (again, in
addition to the Reuters and Wikipedia corpus men-
tioned above). The number of training instances are
shown in Table 4.
Classifier # Training instances
C1 1746128
C2 530885
C3 1763784
C4 706775
Table 4: Number of training instances used for the
ME models.
In the HOO framework, precision and recall are
weighted equally. However, in the domain of error
correction for non-native writers, precision is prob-
ably more important because false positives can be
very confusing and demotivating for learners of En-
glish. For this reason, we submitted two different
runs which also gave us insights into the impact of
the language model. ?Run 0? denotes the system ex-
cluding the language model and using lower thresh-
olds, such that neither precision nor recall is favored
in particular, while ?Run 1? focuses on precision
by using the language model as a filter, and having
higher thresholds. Thus, we present the results for
two different runs on the final HOO test set, both
before and after manual revision (see Section 2.2).
Table 5 presents the results for recognition and Ta-
ble 6 those for correction.
The difficulty of the HOO 2012 Shared Task is
reflected by rather low system performance levels
(Dale et al, 2012). Nonetheless, we observed some
interesting patterns. In terms of the overall system
performance, our system achieved better results for
determiner errors than for preposition errors.
With respect to determiners, missing determiners
are handled best by our system, while unnecessary
determiners and replacement errors are more diffi-
cult. Concerning prepositions, missing prepositions
are found to be the most challenging. This confirms
the difficulty of choosing the right preposition due to
the large number of possible alternatives, and their
sometimes subtle differences in usage and meaning.
While ?Run 1? achieved a higher precision (at the
cost of recall), ?Run 0? performed better in terms of
overall performance (F1-score). This result can be
explained by the relative small size and limited tun-
ing of the language model. Moreover, it also shows
that the use of the F1-score might not be the most
informative evaluation metric in this context.
6 Conclusions
Determiners and prepositions present real chal-
lenges for non-native English writers. For auto-
mated determiner and preposition error correction
in HOO-2012, we implemented a hybrid system
that combines statistical machine learning classifiers
and a rule-based system. By employing a language
model for correction validation, the system achieved
a precision of 42.16%, recall of 9.49% and F1-score
of 15.50%. Without the language model, a preci-
sion of 31.15%, recall of 22.08% and F1-score of
25.84% were reached, and our system was ranked
third in terms of F1-score.
Three major bottlenecks were identified in the im-
plementation: (i) spelling errors should first be cor-
rected due to the noisy input texts; (ii) classifier
thresholds must be carefully adjusted to minimize
false positives; and (iii) overall, preposition errors
are handled worse than determiner errors, although
there is also a large difference among the various er-
ror types.
For future work, we will focus on models that ex-
plicitly utilize the writer?s background. Also, a full
evaluation of the system should include a thorough
user-centric study with evaluation criteria and met-
rics beyond the traditional precision, recall and F -
score.
268
Type Precision Recall F1-score
RD 17.95 17.95 17.95
MD 60.76 38.40 47.06
UD 22.67 32.08 26.56
Det 37.31 33.18 35.12
RT 55.88 13.97 22.35
MT 50.00 5.26 9.52
UT 14.77 30.23 19.85
Prep 27.34 14.83 19.23
All 33.33 23.62 27.65
(a) Run 0 (before revision)
Type Precision Recall F1-score
RD 19.44 17.95 18.67
MD 65.82 39.69 49.52
UD 26.67 32.26 29.20
Det 40.93 34.50 37.44
RT 61.76 14.09 22.95
MT 50.00 5.36 9.68
UT 15.91 35.90 22.05
Prep 29.69 15.57 20.43
All 29.47 24.74 29.47
(b) Run 0 (after revision).
Type Precision Recall F1-score
RD 37.50 7.69 12.77
MD 66.67 12.80 21.48
UD 16.67 1.89 3.39
Det 52.63 9.22 15.69
RT 51.61 11.76 19.16
MT 40.00 3.51 6.45
UT 32.14 20.93 25.35
Prep 42.19 11.44 18.00
All 46.08 10.38 16.94
(c) Run 1 (before revision).
Type Precision Recall F1-score
RD 37.50 8.33 13.64
MD 79.17 14.50 24.52
UD 33.33 3.23 5.88
Det 63.16 10.48 17.98
RT 54.84 11.41 18.89
MT 40.00 3.57 6.56
UT 35.71 25.64 29.85
Prep 45.31 11.89 18.83
All 51.96 11.21 18.43
(d) Run 1 (after revision).
Table 5: Recognition results of the runs on the test set.
269
Type Precision Recall F1-score
RD 17.95 17.95 17.95
MD 54.43 34.40 42.16
UD 22.67 32.08 26.56
Det 34.72 30.88 32.68
RT 50.00 12.50 20.00
MT 50.00 5.26 9.52
UT 14.77 30.23 19.85
Prep 25.78 13.98 18.13
All 31.15 22.08 25.84
(a) Run 0 (before revision)
Type Precision Recall F1-score
RD 17.95 19.44 18.67
MD 59.49 35.88 44.76
UD 26.67 32.26 29.20
Det 38.34 32.31 35.07
RT 55.88 12.75 20.77
MT 50.00 5.36 9.68
UT 15.91 35.90 22.05
Prep 28.13 14.81 19.41
All 34.27 23.26 27.71
(b) Run 0 (after revision).
Type Precision Recall F1-score
RD 37.50 7.69 12.77
MD 62.50 12.00 20.13
UD 16.67 1.89 3.39
Det 50.00 8.76 14.90
RT 41.94 9.56 15.57
MT 40.00 3.51 6.45
UT 32.14 20.93 25.35
Prep 37.50 10.17 16.00
All 42.16 9.49 15.50
(c) Run 1 (before revision).
Type Precision Recall F1-score
RD 37.50 8.33 13.64
MD 75.00 13.74 23.23
UD 33.33 3.23 5.88
Det 60.05 10.04 17.23
RT 45.16 9.40 15.56
MT 40.00 3.57 6.56
UT 35.71 25.64 29.85
Prep 40.63 10.66 16.88
All 48.04 10.36 17.04
(d) Run 1 (after revision).
Table 6: Correction results of the runs on the test set.
270
References
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on ESL student writing. Journal of Second Lan-
guage Writing, 14:191?205.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Dublin, Ireland, 7?9 July 2010.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, pages 242?249, Nancy, France,
28?30 September 2011.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation, Istanbul, Turkey, 21?27 May
2012.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and de-
terminer error correction shared task. In Proceedings
of the Seventh Workshop on Innovative Use of NLP for
Building Educational Applications, Montreal, Canada,
3?8 June 2012.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 45?50, Prague, Czech Republic,
28 June 2007.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 169?176, Manchester, United
Kingdom, 18?22 August 2008.
Tom De Smedt, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based shallow parser for
Python. CLiPS Technical Report Series (CTRS), 2.
Michael Gamon, Lucy Vanderwende, Jianfeng Gao,
Chris Brockett, Alexandre Klementiev, William B.
Dolan, and Dmitriy Belenko. 2008. Using contex-
tual speller techniques and language modeling for ESL
error correction. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 449?456, Hyderabad, India, 7?12 January 2008.
Jila Ghomeshi, Paul Ileana, and Martina Wiltschko.
2009. Determiners: Universals and Variation. Lin-
guistik Aktuell/Linguistics Today. John Benjamins
Publishing Company.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting errors in English article usage with
a maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
Lisbon, Portugal, 26?28 May 2004.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an ESL/EFL error correction sys-
tem. In Proceedings of the Seventh International Con-
ference on Language Resources and Evaluation, Val-
letta, Malta, 19?21 May 2010.
Kevin Knight and Ishwar Chander. 1994. Automatic
postediting of documents. In Proceedings of the 12th
National Conference on Artificial Intelligence, pages
779?784, Seattle, Washington, USA, 31 July?4 Au-
gust 1994.
Guido Minnen, Francis Bond, and Ann Copestake. 2000.
Memory-based learning for article generation. In Pro-
ceedings of the 4th Conference on Computational Nat-
ural Language Learning and the Second Learning
Language in Logic Workshop, pages 43?48, Lisbon,
Portugal, 13?14 September 2000.
Diane Nicholls. 2003. The Cambridge Learner
Corpus?error coding and analysis for lexicography
and ELT. In Proceedings of the Corpus Linguis-
tics 2003 Conference, pages 572?581, Lancaster, UK,
29 March?2 April 2003.
Adwait Ratnaparkhi. 1998. Maximum entropy models
for natural language ambiguity resolution. Ph.D. the-
sis, Philadelphia, PA, USA. AAI9840230.
Adwait Ratnaparkhi. 2010. Maximum entropy models
for natural language processing. In Encyclopedia of
Machine Learning, pages 647?651.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Human
Language Technology Conference of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics, pages 49?56, Rochester, New York, USA,
22?27 April 2007.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, 19?24 June 2011.
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A
web-based English proofing system for English as a
second language users. In Proceedings of the Third
International Join Conference on Natural Language
Processing, Hyderabad, India, 7?12 January 2008.
271
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 135?138,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Detecting Relations in the Gene Regulation Network
Thomas Provoost
Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A, 3000 Leuven, Belgium
{thomas.provoost, sien.moens}@cs.kuleuven.be
Abstract
The BioNLP Shared Task 2013 is organ-
ised to further advance the field of in-
formation extraction in biomedical texts.
This paper describes our entry in the Gene
Regulation Network in Bacteria (GRN)
part, for which our system finished in sec-
ond place (out of five). To tackle this re-
lation extraction task, we employ a basic
Support Vector Machine framework. We
discuss our findings in constructing local
and contextual features, that augment our
precision with as much as 7.5%. We touch
upon the interaction type hierarchy inher-
ent in the problem, and the importance of
the evaluation procedure to encourage ex-
ploration of that structure.
1 Introduction
The increasing number of results in the biomedical
knowledge field has been responsible for attract-
ing attention and research efforts towards meth-
ods of automated information extraction. Of par-
ticular interest is the recognition of information
from sources that are formulated in natural lan-
guage, since a great part of our knowledge is still
in this format. Naturally, the correct detection of
biomedical events and relations in texts is a matter
which continues to challenge the scientific com-
munity. Thanks to the BioNLP Shared tasks, al-
ready in the third instalment, researchers are given
data sets and evaluation methods to further ad-
vance this field.
We participated in the Gene Regulation Network
(GRN) Task (Bossy et al, 2013), which is an
extension of the Bacteria Gene Interactions Task
from 2011 (Jourde et al, 2011). In this task, ef-
forts are made to automatically extract gene inter-
actions for sporulation, a specific cellular function
of the bacterium bacillus subtilis for which a sta-
ble reference regulatory network exists. An exam-
ple sentence can be seen below. Note that all en-
tities (except for event triggers, i.e. action entities
like transcription in figure 1) are given as input in
both training and test phases. Therefore, this task
makes abstraction of the entity recognition issue,
putting complete focus on the subproblem of rela-
tion detection.
sspG transcription also requires the DNA binding protein GerE .
Event
Interaction: Requirement
Agent
Target
Figure 1: Example sentence: there is an Interac-
tion:Requirement relation defined between entities
GerE and sspG, through the action event of tran-
scription. Full-line entities are given in the test
phase, while dashed-lined ones are not.
As this is our first participation in this task, we
have built a simple, yet adaptable framework. Our
contributions lie therefore more in the domain of
feature definition and exploration, rather than in
designing novel machine learning models.
Predictions could be given in two ways. Either
all events and relations could be predicted, from
which the regulation network would then be infer-
enced (cfr. figure 1, detect all dashed-lined enti-
ties, and the relations between them). Or, a speci-
fication of the regulation network itself is directly
predicted (in the example, this amounts to finding
GerE ? sspG, and the type (Requirement)). We
chose to implement the latter method. In section
2 we will lay out the framework we constructed,
and the tools we used. In that section, we will also
look at some of the design choices for our feature
construction. Finally we discuss our results in sec-
tion 3, and touch upon opportunities to exploit the
available interaction hierarchy in this data.
135
2 Implementation
Basic Framework For this interaction detec-
tion task, we implement a Support Vector Ma-
chine (SVM) (Vapnik, 1995), with the use of the
SVMLight (Joachims, 1999) implementation in
the Shogun Machine Learning Toolbox. Per given
sentence, we construct our data points to be all
pairs of genic entities in that sentence, i.e., all pos-
sible interaction agent/target pairs. Note that since
the regulation network is a directed graph, the or-
der of the nodes matters; each such pair therefore
occurs twice in the data. It is obvious from this
construction that this leads to a great imbalance:
there are a lot more negatively labelled data points
than positive ones. To respond to this, we tried
applying differential weighing (as seen in (Shawe-
Taylor and Cristianini, 1999) and (Veropoulos et
al., 1999)). This amounts to appointing a big-
ger regularisation parameter C to the positive data
points when training the SVM, thus tightening the
boundary constraint on the margin for these points.
The results of this were unconvincing however, so
we decided not to implement it.
For each interaction type (there are 6 of them),
we then train a separate binary (local, hence one-
versus-all) SVM classifier1, with a Gaussian Ra-
dial Basis Function (RBF) kernel as in (Ai?zerman
et al, 1964) and (Scho?lkopf et al, 1997). We eval-
uated several types of kernels (linear, polynomial,
Gaussian) in a 25-fold cross-validation over the
union of training and validation set, and the RBF-
kernel consistently gave better results.
Feature Construction and Selection Consider
our data points (i.e., the agent/target pairs) xijk =
(eij , eik), j 6= k, where eij denotes the jth entity
of sentence i. For each such point, the basic (real-
valued) feature set-up is this:
f(xijk) = fent(eij ) fent(eik) fextra(eij , eik),
a concatenation (the operation) of the respective
feature vectors fent defined separately on the pro-
vided entities. To that we add fextra, which con-
tains the Stanford parse tree (Klein and Manning,
2003) distance of the two entities, and the location
and count (if any) of Promoter entities: these are
necessary elements for transcription without being
part of the gene itself. For any entity, we then con-
1There is a lot of scope for leveraging the hierarchy in the
interaction types; we touch upon this in the conclusion.
struct the feature vector as:
fent(eij ) =
1
Nij
?
w?eij
fbase(w) fcontext(w, i),
whereNij is the number of words in eij . This is an
average over all words w that make up entity eij
2,
with the choice of averaging as a normalisation
mechanism, to prevent a consistent assignment of
relatively higher values to multi-word entities. In-
side the sum is the concatenation of the local fea-
ture function on the words (fbase) with fcontext,
which will later be seen as encoding the sentence
context.
The base feature function on a word is a vector
containing the following dimensions:
? The entity type, as values ? {0, 1};
? Vocabulary features: for each word in the dic-
tionary (consisting of all words encountered),
a similarity score ? [0, 1] is assigned that
measures how much of the beginning of the
word is shared3. In using a similarity scor-
ing instead of a binary-valued indicator func-
tion, we want to respond to the feature spar-
sity, aggravated by the low amount of data
(134 sentences in training + validation set).
While this introduces some additional noise
in the feature space, this is greatly offset by
a better alignment of dimensions that are ef-
fectively related in nature. Also note that,
due to the nature of the English language,
this approach of scoring similarities based on
a shared beginning, is more or less equiva-
lent to stemming (albeit with a bias towards
more commonly occurring stems). For our
cross-validations, utilisation of these similar-
ity scores attributed to an increase in F-score
of 7.6% (mainly due to an increase in re-
call of 7.0%, without compromising preci-
sion) when compared to the standard binary
vocabulary features.
? Part-of-speech information, using the
Penn-Treebank (maximum entropy) tagger,
through the NLTK Python library (Bird et
al., 2009). These are constructed in the same
fashion as the vocabulary features;
2Note that one entity can consist of multiple words.
3To not overemphasise small similarities (e.g. one or two
initial letters in common), we take this as a convex function
of the proportion of common letters.
136
? Location of the word in its sentence (nor-
malised to be ? [0, 1]). Note that next to
being of potential importance in determining
an entity to be either target or agent, the sub-
space of the two location dimensions of the
respective entities in the data point xijk =
(eij , eik) also encodes the word distance be-
tween these.
? Depth in the parse tree (normalised to be ?
[0, 1]).
Adding contextual features On top of these ba-
sic features, we add some more information about
the context in which the entities reside. To this ef-
fect, we concatenate to the basic word features the
tree context: a weighted average of all other words
in the sentence:
fcontext(w, i) =
1
Z
?
wj?sentencei
?di(w,wj)fbase(wj)
with fbase the basic word features described
above, and weights given by ? ? 14 and di(w,wj)
the parse tree distance from w to wj . The normal-
isation factor we use is
Z =
?
wj?sentencei
?di(w,wj)
i.e., the value we would get if a feature would be
consistently equal to 1 for all words. This nor-
malisation makes sure longer sentences are not
overweighted. For the inner parse tree nodes we
then construct a similar vector (using only part-
of-speech and phrasal category information), and
append it to the word context vector.
Note that the above definition of fcontext also al-
lows us to define di(w,wj) to be the word distance
in the sentence, leaving out any grammatical (tree)
notion. We designate this by the term sentence
context.
3 Results and Conclusion
Cross-validation performance on training data
Because we have no direct access to the final test
data, we explore the model performance by con-
sidering results from a 25-fold cross-validation on
the combined training and validation set. Table 1
4We optimised ? to be 0.4, by tuning on a 25-fold cross-
validation, only using training and validation set.
shows the numbers of three different implementa-
tions5: one with respectively no fcontext concate-
nated, and the tree context (the official submission
method) and sentence context versions. We see
that a model based uniquely on information from
the agent and target entities already performs quite
well; a reason for this could be the limited amount
of entities and/or interactions that come into play
in the biological process of sporulation, augment-
ing the possibility that a pair can already be ob-
served in the training data. Adding context infor-
mation increases the F-score by 2%, mainly due to
a substantial increase in precision, as high as 7.5%
for the sentence context. Recall performs better in
the tree variant however, pointing to the fact that
grammatical structure can play a role in identify-
ing relations.
Note that we only considered the sentence alter-
ation after the submission deadline, so the better
results seen here could no longer implore us to use
this version of the context features.
Context SER Prec. Recall F1
None 0.827 0.668 0.266 0.380
Tree 0.794 0.709 0.285 0.406
Sentence 0.787 0.743 0.278 0.405
Table 1: Results of the cross-validation for several
implementations of context features. (C = 5, ? =
8.75)
We can identify some key focus points to fur-
ther improve our performance. Generally, as can
be seen in the additional results of table 1, a low
recall is the main weakness in our system. These
low numbers can in part be explained by the lack
of great variation in the features, mainly due to
the low amount of data we have. Interesting to
note here, is the great diversity of performance of
the local classifiers separately: the SVM for Tran-
scription attains a recall of 42.0%, in part because
this type is the most frequent in our data. However,
the worst performers, Requirement and Regulation
(with a recall of 0.0% and 3.7% respectively) are
not per se the least frequent; in fact, Regulation
is the second most occurring. Considerable effort
should be put into addressing the general recall is-
sue, and gaining further insight into the reasons
behind the displayed variability.
5For simplicity, we keep all other parameters (C, and the
RBF kernel parameter ?) identical across the different entries
of the table. While in theory a separate parameter optimisa-
tion on each model could affect the comparison, this showed
to be of little qualitative influence on the results.
137
Final results on test data On submission of the
output from the test data, our system achieved a
Slot Error Rate (SER) of 0.830 (precision: 0.500,
recall: 0.227, F1: 0.313), coming in second place
after the University of Ljubljana (Zitnik et al,
2013) who scored a SER of 0.727 (precision:
0.682, recall: 0.341, F1: 0.455).
Exploring structure One of the main issues of
interest for future research is the inherent hierar-
chical structure in the interactions under consid-
eration. These are not independent of each other,
since there are the following inclusions:
Regulation
Inhibition Activation
Requirement
Binding
Transcription
So for example, each interaction of type Tran-
scription is also of type Binding, and Regula-
tion. This structure implicates additional knowl-
edge about the output space, and we can use this
to our benefit when constructing our classifier.
In our initial framework, we make use of local
classifiers, and hence do not leverage this addi-
tional knowledge about type structure. We have
already started exploring the design of techniques
that can exploit this structure, and preliminary re-
sults are promising.
One thing we wish to underline in this process
is the need for an evaluation procedure that is as
aware of the present structures as the classifier. For
instance, a system that predicts a Binding interac-
tion to be of type Regulation, is more precise than
a system that identifies it as an Inhibition. Both
for internal as external performance comparison,
we feel this differentiation could broaden the fo-
cus towards a more knowledge-driven approach of
evaluating.
Acknowledgements
We would like to thank the Research Foundation
Flanders (FWO) for funding this research (grant
G.0356.12).
References
Mark A. Ai?zerman, E. M. Braverman, and Lev I. Rozo-
noer. 1964. Theoretical foundations of the potential
function method in pattern recognition learning. Au-
tomation and Remote Control, 25:821?837.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. OReilly
Media Inc.
Robert Bossy, Philippe Bessir`es, and Claire Ne?dellec.
2013. BioNLP shared task 2013 - an overview of
the genic regulation network task. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, chapter 11, pages 41?56. MIT Press.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP shared task 2011
- bacteria gene interactions and renaming. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 56?64.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems, number 15, pages 3?10.
MIT Press.
Bernhard Scho?lkopf, Kah-Kay Sung, Christopher J. C.
Burges, Federico Girosi, Partha Niyogi, Tomaso
Poggio, and Vladimir N. Vapnik. 1997. Compar-
ing support vector machines with Gaussian kernels
to radial basis function classifiers. IEEE Transac-
tions on Signal Processing, 45(11):2758?2765.
John Shawe-Taylor and Nello Cristianini. 1999. Fur-
ther results on the margin distribution. In Proceed-
ings of the Twelfth Annual Conference on Computa-
tional Learning Theory, COLT ?99, pages 278?285,
New York, NY, USA. ACM.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Konstantinos Veropoulos, Colin Campbell, and Nello
Cristianini. 1999. Controlling the sensitivity of sup-
port vector machines. In Proceedings of the Inter-
national Joint Conference on AI, pages 55?60.
Slavko Zitnik, Marinka itnik, Bla Zupan, and Marko
Bajec. 2013. Extracting gene regulation networks
using linear-chain conditional random fields and
rules. In Proceedings of BioNLP Shared Task 2013
Workshop, Sofia, Bulgaria, August. Association for
Computational Linguistics.
138
Proceedings of the 25th International Conference on Computational Linguistics, pages 46?53,
Dublin, Ireland, August 23-29 2014.
Key Event Detection in Video using ASR and Visual Data
Niraj Shrestha Aparna N. Venkitasubramanian
KU Leuven, Belgium
{niraj.shrestha, Aparna.NuraniVenkitasubramanian,
Marie-Francine.Moens}@cs.kuleuven.be
Marie-Francine Moens
Abstract
Multimedia data grow day by day which makes it necessary to index them automatically and effi-
ciently for fast retrieval, and more precisely to automatically index them with key events. In this
paper, we present preliminary work on key event detection in British royal wedding videos using
automatic speech recognition (ASR) and visual data. The system first automatically acquires
key events of royal weddings from an external corpus such as Wikipedia, and then identifies
those events in the ASR data. The system also models name and face alignment to identify the
persons involved in the wedding events. We compare the results obtained with the ASR output
with results obtained with subtitles. The error is only slightly higher when using ASR output in
the detection of key events and their participants in the wedding videos compared to the results
obtained with subtitles.
1 Introduction
With the increase of multimedia data widely available on the Web and in social media, it becomes
necessary to automatically index the multimedia resources with key events for information search and
mining. For instance, it is not possible to manually index all the frames of a video. Automatically
indexing multimedia data with key events makes the retrieval and mining effective and efficient.
Event detection is an important and current research problem in the field of multimedia information
retrieval. Most of the event detection in video is done by analyzing the visual features using manually
transcribed data. In this paper, we propose key event detection in British royal wedding videos using
automatic speech recognition (ASR) data and where possible also to recognize the actors involved in the
recognized events using visual and textual data. An event is something that happens at a certain moment
in time and at a certain location possibly involving different actors. Events can be quite specific as in this
case the key events are the typical events that make up a royal wedding scenario. For example, events like
?design of cake/dress/bouquet?, ?couple heading to Buckingham palace?, ?appearing on balcony? etc. are
key events in British royal wedding video. Figure 1 shows an example of a frame containing an event with
its actors, together with the associated subtitle and ASR output. While most works in this domain have
focussed on clean textual content such as manual transcripts or subtitles, which are difficult to acquire, we
use the output of an ASR system. While the event detection and name-face alignment problem by itself
is already quite difficult, the nature of the ASR text adds an additional complexity. ASR data is noisy
and inaccurate, it does not contain some parts of the actual spoken text, and does not contain sentence
boundaries. Figure 2 illustrates this problem. For the key events, the system first acquires the necessary
knowledge from external corpora - in our case Wikipedia articles associated with royal weddings. Then
the system identifies the key events in the ASR data. The system also models name and face alignment
to identify the persons involved in the wedding events. We perform named entity recognition in the text
associated with a window of frames to first generate a noisy label for the faces occurring in the frames
and this rough alignment is refined using an Expectation-Maximization (EM) algorithm. We compare
the results obtained with the ASR output with results obtained with subtitles. The error is only slightly
This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
46
Sub-title: ?Outside, fully 150,000 people with unbounded enthusiasm acclaimed Princess Margaret and
her husband when they appeared on the balcony.. . . ?
ASR: ?outside only a hundred and 50 people on TV and using it as a . . . ?
Figure 1: An example of a frame containing an event with associated subtitle and ASR output
higher when using ASR output in the detection of key events and their participants in the wedding videos
compared to the results obtained with subtitles. The methodology that we propose can be applied for the
detection of many different types of video events.
2 Related work
Event detection has some relationship with Topic Detection and Tracking (TDT) and with concept de-
tection. TDT regards the detection and tracking over time of the main event of a news story and is a
challenging problem in the field of text and visual analysis. Although widely studied in text (Allan,
2002), (Allan et al., 2005), (Mei and Zhai, 2005), (Wang et al., 2007), (Zhao et al., 2007), topic detection
in video is still not well studied. An event in this context is usually broader in scope than the events
we want to recognize in wedding videos in this paper. In the multimedia research community, most of
the works focus on concept detection like in (Liu et al., 2008), (Yang et al., 2007), (Snoek et al., 2006)
rather than event detection. A concept detection task is different from event detection as a concept can
be defined as any object or specific configuration of objects. Any frame then can be labelled with some
concept descriptor (e.g., church, cake, etc.). While in an event, there is a start and end time in between
which something happens, and in video, an event is represented by a sequence of frames.
Event detection is a challenging problem which is not well studied. Only few event detection systems
that process video exist. They recognize events such as goal, run, tackle in a soccer game, or recognize
specific actions in news video (e.g., meeting of two well-known people) or in a surveillance video (e.g.,
unusual event). Event detection in video is often related to sports like basketball (Saur et al., 1997),
soccer (Yow et al., 1995) and baseball (Kawashima et al., 1998) (Rui et al., 2000). (Wang et al., 2008)
developed a model based on a multi-resolution, multi-source and multi-modal bootstrapping framework
that exploits knowledge of sub-domains for concept detection in news video. (Adam et al., 2008) de-
veloped an algorithm based on multiple local monitors which collect low-level statistics to detect certain
types of unusual events in surveillance video.
Most of these works rely only on visual analysis (e.g., detection of certain motion patterns) to identify
events in video and the event detection is performed with a supervised learning method, where a model
is trained on manually annotated examples of known events. In this paper, we propose a novel idea in
which the system learns events from an external corpus like Wikipedia and identifies those events in the
ASR or subtitle data of the video. In addition, we identify the persons involved in an event based on the
analysis of visual and textual data.
47
Figure 2: An example showing sub-title vs. ASR data
3 Methodology
The main objective of this work is to identify and index key events in videos using ASR data along with
key actors involved in the event. We start by identifying key events related to a certain domain, using
external corpora. In addition, the proposed method involves pre-processing of the textual and visual data.
At 11.30, Elizabeth entered the abbey on her father?s arm, but they did not head straight down the aisle
as expected.
Figure 3: Sequence of frames showing the anchor talking about an event of the wedding, but there is no
visual appearance of the event.
3.1 Acquiring background knowledge
Our approach for identifying key events in weddings exploits external text corpora. We use two corpora:
1. A genre-specific corpus: a set of pages specific to the topic, for example, from Wikipedia - to
identify events associated with the topic.
48
2. A generic corpus, used to weigh the events identified in the genre-specific corpus.
The process is as follows. We first collect content from Wikipedia articles relevant for Britain?s royal
weddings
1
in the form of nine documents. These articles include both pages related to weddings, such as
these of Diana and Charles, that were mentioned in our test videos as well as pages about other British
royal weddings not shown in the videos, such as the wedding of Kate and William. This set of articles
formed our wedding corpus for learning typical wedding events. The generic corpus is formed by all
English Wikipedia articles.
From each document of this corpus we extract events together with their arguments (subject and object
arguments) using a state-of-the-art event annotator
2
. This tool uses linguistic features such as the results
of a dependency parse of a sentence, to detect the events and their arguments in a text. Next, we use
a data mining technique to find frequent word patterns that signal the event and its arguments in the
wedding articles, we keep each event that has sufficient support in the wedding articles and weigh it by
a factor that is inversely proportional to the frequency of this event in the more general corpus. We keep
the N highest weighted events from the obtained ranked list, where N is determined by whether we want
to keep the most common wedding events or include also more rare events. The list obtained has items
such as ?to announce engagement?, ?to make dress?, ?to make cake? etc, which are typical for weddings.
We report here on preliminary work and acknowledge that the methodology can be largely refined.
3.2 Detecting person names
In royal wedding videos, there are many persons who appear in the video like anchor, interviewee, the
persons married or to be married, the dress designer, the bouquet designer, the cake maker, the friends
etc. As in this preliminary work we are only interested in the brides and bridegrooms (which are also the
most important persons when indexing the video) we use a gazetteer with their names for recognizing
the names in the texts.
3.3 Detecting the faces of persons
In the video key frames are extracted at the rate of 1 frame per second using (ffmpeg, 2012), which
ensures that no faces appearing in the video are omitted. To detect the faces in the video, a face detector
tool from (Bradski, 2000) is used. Next, we extract the features from the faces detected in the video.
Although there are several dedicated facial feature extraction methods such as (Finkel et al., 2005),(Strehl
and Ghosh, 2003), in this implementation, we use a simple bag-of-visual-words model (Csurka et al.,
2004).
Once feature vectors are built, clustering of the bounding boxes of the detected faces is performed.
Each object is, then, compared to the cluster centers obtained and is replaced with the closest center.
The clustering is done using Elkan?s k-means algorithm (Jain and Obermayer, 2010) which produces the
same results as the regular k-means algorithm, but is computationally more efficient. This accelerated
algorithm eliminates some distance calculations by applying the triangle inequality and by keeping track
of lower and upper bounds for distances between points and centers. This algorithm, however, needs the
number k of clusters present in the data. Since we are primarily interested in the brides and bridegrooms
and since there are seven weddings shown in the video, we experiment with values of k equal to 7*2 = 14.
Although this approach very likely introduces errors in the clustering as we do not know beforehand how
many persons apart from the couple appear in the chosen key frames, it showed to be a better strategy
than trying to align all persons mentioned in the texts. The clustering is performed using an Euclidean
distance metric.
3.4 Name and face alignment
If a key frame contains a face, then we identify the corresponding ASR or subtitle data that co-occur in a
fixed time window with this frame. Further, the names occurring in the textual data are listed as possible
names for the frame. As a result, it is possible that an entity mentioned in the text is suggested for several
1
http://en.wikipedia.org/wiki/Category:British royal weddings
2
http://ariadne.cs.kuleuven.be/TERENCEStoryService/
49
Table 1: Names and faces alignment results on subtitle vs. ASR data on events
Subtitle ASR
P R F
1
P R F
1
Textual 38.095 21.622 27.586 36.585 17.857 24
EM 41.304 25.676 31.667 40.426 22.619 29.008
Table 2: WinDiff score on event identification on subtitle vs. ASR data on the union setting
Subtitle ASR
11.06 13.80
key frames. However, when there is no corresponding text, or when the text does not contain person
entities, no name is suggested for the key frame.
Name and face alignment in royal wedding video is difficult and complicated since the video contains
many other faces of persons mentioned above. Sometimes the anchor or designer talks about the couple
involved in the wedding, but there is no appearance of this couple in the corresponding video key frame
as shown in figure 3.
We minimize this problem of name and face alignment by using the EM algorithm cited in (Pham et
al., 2010). Alignment is the process of mapping the faces in the video to the names mentioned in the
textual data. For each frame, the most probable alignment scheme has to be chosen from all possible
schemes. The EM algorithm has an initialization step followed by the iterative E- and M-steps. The
E-step estimates the likelihood of each alignment scheme for a frame, while the M-step updates the
probability distribution based on the estimated alignments over all key frames of the video.
3.5 Event identification in subtitle and ASR data with person involvement (if any)
Once the system has learned the events from the Wikipedia data, it identifies the events from the subtitles.
The process is as follow: the system scans each subtitle for the key words from the event list. If the key
word appears in the subtitle data, then it is treated as the occurrence of the event and stores the set of
frames that co-occur with that subtitle. The name and face alignment module already might have yielded
a list of names present in this subtitle if there is any person involved. If that is the case, then the names
are assigned to the events identified.
The same process is repeated using ASR data.
4 Experimental setup
In this section, we describe the dataset, experimental setup and the metrics used for evaluation.
4.1 Datasets and ground truth annotations
The data used in our experiments is the DVD on Britain?s Royal Weddings published by the BBC. The
duration of this video is 116 minutes at a frame rate of 25 frames per second, and the frame resolution
is 720x576 pixels. Frames are extracted at the rate of one frame per second using the ffmpeg tool
(ffmpeg, 2012). Faces in the frames are annotated manually using the Picasa tool for building the ground
truth for evaluation. This tool is very handy and user-friendly to tag the faces. We have found that
there are 69 persons including British wedding couples in the video. The subtitles came along with the
DVD which are already split into segments of around 3 seconds. We use the (FBK, 2013) system to
obtain the ASR data of the videos. Since the (FBK, 2013) system takes only sound (.mp3 file) as input,
we have converted the video into a mp3 file using (ffmpeg, 2012). The obtained ASR data is then in
XML format without any sentence boundaries so we have converted the ASR data into segments in the
range of three seconds, which is standard when presenting subtitles in video. It is clear that the ASR
transcription contains many words that are incorrectly transcribed. It is also visible that the ASR system
does not recognize or misspells many words from the actual speech. As mentioned above, we have built
50
a gazetteer of the couples? names. A set of events are recognized by our system as being important in
the context of weddings. To evaluate the quality of these events, the events in the video were annotated
by two annotators independently. This annotation includes the actual event, and the start and end times
of the event. These two sets with annotations form the groundtruth. To be able to compare the system
generated events with the ground truth events, we adopt a two-step approach. First, we combine the
corresponding ground truth entries from different annotators into one sequence of frames. Suppose one
entry in a ground truth file (GT (a)) by one annotator contains the following start (x
a
) and end (y
a
)
time range: GT (a) : [x
a
, y
a
], and the corresponding entry in the other ground truth file (GT (b)) (by the
second annotator) contains the following start (x
b
) and end (y
b
) time range: GT (b) : [x
b
, y
b
]. Merging
of the ground truth event ranges can be done in different ways, but we report here on the union of the
two ranges.
GT (a) ?GT (b) = [min(x
a
, x
b
),max(y
a
, y
b
)] (1)
4.2 Evaluation Metrics
Let FL be the final list of name and face alignment retrieved by our system for all the faces detected in
all frames, and GL the complete ground truth list. To evaluate the name and face alignment task, we use
standard precision (P ), recall (R) and F
1
scores for evaluation:
P =
|FL ?GL|
|FL|
R =
|FL ?GL|
|GL|
F
1
= 2 ?
P ?R
P +R
To evaluate correctness of event segment boundaries, precision and recall are too strict since they
penalize boundaries placed very close to the ground truth boundaries. We use the WindowDiff (Pevzner
and Hearst, 2002) metric that measures the difference between the ground truth segment GT and the
segment SE found by the machine originally designed for text segmentation. For our scenario, this
metric is defined as follows:
WD(GT, SE) =
1
M ? k
M?k
?
i=1
(|b(GT
i
, GT
i+k
)? b(SE
i
, SE
i+k
)| > 0) (2)
where M = 7102, is the number of frames extracted, k = 1, is the window size and b(i, j) represents
the number of boundaries between frame indices i and j.
5 Results
Figure 4: Events learned from the Wikipedia data and their identification in the subtitles and ASR by the
system
51
5.1 Evaluation of the extraction of wedding events from Wikipedia
Figure 4 shows which key events typical for royal weddings the system has learned from Wikipedia data
and how it found these events in the subtitles and the ASR data. It is seen from figure 4 that the system
could not learn many events that are important to wedding events, but the system recognized the events
that it has learned quite accurately in the subtitles and ASR data.
5.2 Evaluation of the event segmentation and recognition
Table 2 shows the results of WinDiff score obtained on subtitles versus ASR data on the union setting
discussed in 4.1. Though the error rate is more or less the same, it degrades in ASR data which is
obviously due to the different ASR errors. The error rate is increased by 2.74% in ASR data using a
window size of 1. Here a window size 1 is equivalent to one second so it corresponds to one frame.
In this case the system tries to find the event boundaries in each frame and evaluates these against the
ground truth event boundaries.
5.3 Evaluation of the name-face alignments
Table 1 shows the result of the name and face alignment given the detected events. Though the result
is not quite satisfactory even after applying the EM algorithm, there are many bottlenecks that need to
be tackled. Many parts of the video contain interviews. Interviewees and anchors mostly talk about the
couples that are married or are to be married, but the couples are not shown which might cause errors in
the name and face alignment.
6 Conclusion and future work
In this paper, we have presented ongoing research on event detection in video using ASR and visual
data. To some extent the system is able to learn key events from relevant external corpora. The event
identification process is quite satisfactory as the system learns from external corpora. If the system would
have learnt the events from external corpora good enough, it might identify events very well from subtitle
or ASR data. We are interested in improving the learning process from external corpora in further work.
Finding event boundaries in the frame sequence corresponding to a subtitle or ASR data where the event
is mentioned is still a challenging problem because an event key word might be identified in a subtitle
segment or in a sentence which actually may not correspond to what is shown the aligned frames. We
have also tried to implement name and face alignment techniques to identify persons involved in the
event. As a further improvement of our system, we need to find how to deal with the many interviews in
this type of videos which might improve the alignment of names and faces.
References
A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. 2008. Robust real-time unusual event detection using multiple
fixed-location monitors. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(3):555?560,
March.
James Allan, Stephen Harding, David Fisher, Alvaro Bolivar, Sergio Guzman-Lara, and Peter Amstutz. 2005.
Taking topic detection from evaluation to practice. In Proceedings of the 38th Annual Hawaii International
Conference on System Sciences (HICSS?05) - Track 4 - Volume 04, HICSS ?05, pages 101.1?, Washington, DC,
USA. IEEE Computer Society.
James Allan, editor. 2002. Topic Detection and Tracking: Event-based Information Organization. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
G. Bradski. 2000. Opencv face detector tool. Dr. Dobb?s Journal of Software Tools. Available at http:
//opencv.org/downloads.html.
Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, and Cdric Bray. 2004. Visual categoriza-
tion with bags of keypoints. In Workshop on Statistical Learning in Computer Vision, ECCV, pages 1?22.
FBK. 2013. FBK ASR transcription. Available at https://hlt-tools.fbk.eu/tosca/publish/
ASR/transcribe.
52
ffmpeg. 2012. ffmpeg audio/video tool. Available at http://www.ffmpeg.org.
Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In Proceedings of ACL, pages 363?370.
Brijnesh J. Jain and Klaus Obermayer. 2010. Elkan?s k-means algorithm for graphs. In Proceedings of the 9th
Mexican International Conference on Artificial Intelligence: Conference on Advances in Soft Computing: Part
II, MICAI?10, pages 22?32, Berlin, Heidelberg. Springer-Verlag.
Toshio Kawashima, Kouichi Tateyama, Toshimasa Iijima, and Yoshinao Aoki. 1998. Indexing of baseball telecast
for content-based video retrieval. In ICIP (1), pages 871?874.
Ken-Hao Liu, Ming-Fang Weng, Chi-Yao Tseng, Yung-Yu Chuang, and Ming-Syan Chen. 2008. Association and
temporal rule mining for post-filtering of semantic concept detection in video. Trans. Multi., 10(2):240?251,
February.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering evolutionary theme patterns from text: An exploration of
temporal text mining. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge
Discovery in Data Mining, KDD ?05, pages 198?207, New York, NY, USA. ACM.
Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation.
Computational Linguistics, 28(1):19?36.
Phi The Pham, M. F. Moens, and T. Tuytelaars. 2010. Cross-media alignment of names and faces. IEEE Transac-
tions on Multimedia, 12(1):13?27, January.
Yong Rui, Anoop Gupta, and Alex Acero. 2000. Automatically extracting highlights for tv baseball programs. In
Proceedings of the Eighth ACM International Conference on Multimedia, MULTIMEDIA ?00, pages 105?115.
Drew D. Saur, Yap-Peng Tan, Sanjeev R. Kulkarni, and Peter J. Ramadge. 1997. Automated analysis and annota-
tion of basketball video. In Storage and Retrieval for Image and Video Databases (SPIE), pages 176?187.
Cees G. M. Snoek, Marcel Worring, Jan C. van Gemert, Jan-Mark Geusebroek, and Arnold W. M. Smeulders.
2006. The challenge problem for automated detection of 101 semantic concepts in multimedia. In Proceedings
of the 14th Annual ACM International Conference on Multimedia, MULTIMEDIA ?06, pages 421?430, New
York, NY, USA.
Alexander Strehl and Joydeep Ghosh. 2003. Cluster ensembles ? a knowledge reuse framework for combining
multiple partitions. J. Mach. Learn. Res., 3:583?617, March.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard Sproat. 2007. Mining correlated bursty topic patterns
from coordinated text streams. In Proceedings of the 13th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?07, pages 784?793.
Gang Wang, Tat-Seng Chua, and Ming Zhao. 2008. Exploring knowledge of sub-domain in a multi-resolution
bootstrapping framework for concept detection in news video. In Proceedings of the 16th ACM International
Conference on Multimedia, MM ?08, pages 249?258, New York, NY, USA. ACM.
Jun Yang, Rong Yan, and Alexander G. Hauptmann. 2007. Cross-domain video concept detection using adaptive
SVMs. In Proceedings of the 15th International Conference on Multimedia, MULTIMEDIA ?07, pages 188?
197, New York, NY, USA. ACM.
Dennis Yow, Boon lock Yeo, Minerva Yeung, and Bede Liu. 1995. Analysis and presentation of soccer highlights
from digital video. pages 499?503.
Qiankun Zhao, Prasenjit Mitra, and Bi Chen. 2007. Temporal and information flow based event detection from
social text streams. In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2,
AAAI?07, pages 1501?1506.
53
