  
An Empirical Study of Speech Recognition Errors  
in a Task-oriented Dialogue System 
 
Marc CAVAZZA 
School of Computing and Mathematics, 
University of Teeside,  
TS1 3BA, Middlesbrough, United Kingdom, 
m.o.cavazza@tees.ac.uk    
 
 
Abstract  
The development of spoken dialogue 
systems is often limited by the performance 
of their speech recognition component. The 
impact of speech recognition errors on 
dialogue systems is often studied at the 
global level of task completion. In this 
paper, we carry an empirical study on the 
consequences of speech recognition errors 
on a fully-implemented dialogue prototype, 
based on a speech acts formalisms. We 
report the impact of speech recognition 
errors on speech act identification and 
discuss how standard control mechanisms 
can participate to robustness by assisting the 
user in repairing the consequences of speech 
recognition errors.  
Introduction 
The development of spoken dialogue systems is 
faced with limitations in speech recognition 
technologies that make recognition errors a 
recurring problem for any dialogue system. 
Several studies have shown little correlation 
between speech recognition scores and user 
satisfaction, or the ability to complete the tasks 
underlying spoken dialogue [Yankelovich et al, 
1995] [Dybkjaer et al, 1997], suggesting that a 
certain level of errors should not prevent spoken 
dialogue systems from being successful.  
However, most of the studies on speech 
recognition errors have concentrated either on 
parsing incomplete utterances or on global 
dialogue robustness, i.e. at task completion level 
[Allen et al, 1996] [Stromback and Jonsson, 
1998] [Brandt-Pook et al, 1996]. 
In this paper, we investigate the impact of 
speech recognition errors on a fully-
implemented prototype for a task-oriented 
dialogue system. This system supports a 
conversational character for Interactive 
Television and is based on a speech acts 
formalism. We report a first empirical study on 
the consequences of speech recognition errors 
on the identification of speech acts, and the 
conditions under which the system can be robust 
to those errors. 
1 System Overview 
The VIP (?Virtual Interactive Presenter?) system 
is a dialogue-based interface to an Electronic 
Programme Guide (EPG). One main advantage 
of human-computer dialogue is that it breaks 
down the information exchange into elementary 
units that correspond to the actual criteria on the 
basis of which TV programmes are selected, i.e. 
individual features such as the cast, the movie 
genre, its rating, etc. It assists the user in 
progressively refining the programme 
description without requiring explicit knowledge 
of the editorial categories used to index the EPG.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The System Interface. 
 
Related applications, i.e. dialogue systems 
involving the choice of a film or TV programme 
  
have been also described in [Hagen, 2000] 
[Ludwig et al, 2000]. Our system is a mixed-
initiative conversational interface organised 
around a human-like character with which the 
user communicates through speech recognition 
[Nagao and Takeuchi, 1994] [Beskow and 
McGlashan, 1997]. The interface is based on the 
Microsoft Agent? system with a set of 
animated bitmaps acquired from a real human 
subject (Figure 1). 
An example dialogue illustrating the system 
capabilities is presented below (this example has 
been obtained with keyboard input only). After 
greetings by the system, the user opens the 
dialogue with a first request (U1). Even though 
the system?s goal is to refine the selection, it 
offers potential choices even at early stages of 
the dialogue, e.g. after only the programme 
genre has been specified (S2). As the dialogue 
progresses, it gives a count of programmes 
matching the current criteria and proposes the 
best one. The system acknowledges the most 
specific selections only (S10, S14). Also, the 
system takes initiative whenever necessary 
(S12). It repairs non-productive dialogue, i.e. 
when several utterances have not altered the 
programme description (?is this programme all 
right, then??). Whenever high-level categories 
are rejected by the user, leaaving the system 
without sufficient criteria to filter the EPG 
contents, it re-starts the dialogue (?what would 
you like then??, S12). 
 
U1: Do you have any sports programmes 
S2: I have found 5 programmes for this choice. 
Would you like to watch ?Keegan?s greatest 
games?? 
U3: Can I have some cricket instead? 
S4: I can suggest this programme with cricket: 
?Sportstalk? 
U5: Who?s conducting? 
S6: ?Sportstalk? features Donald 
U7: Anything else? 
S8: I have no other programmes for that 
selection 
U9: Is there anything funny, then? 
S10: I can recommend the following comedy: 
?Starting Out? 
U11: I do not want a sitcom 
S12: What would you like to see, then? 
U13: A funny movie 
S14: I would suggest this programme with 
comedy: ?Analyze This? 
U15: Can I have another one? 
S16 There are 1 other programmes for this 
choice. What about the following programme: 
?Casablanca?? 
U17: I want a western instead 
S18: I would suggest the following western: 
?Unforgiven? 
 
The software architecture is a pipeline 
comprising speech recognition, parsing and 
dialogue. In the next sections, we describe each 
of these components from the perspective of 
speech recognition errors. Finally, we discuss 
the impact of speech recognition errors on 
example dialogues and the mechanisms that 
contribute to dialogue robustness.  
 
 
 
 
 
 
 
 
 
 
Figure 2. The ABBOT Interface. 
2 The Speech Recognition Component 
Speech recognition is based on the ABBOT 
system [Robinson et al, 1996]. A specific 
ABBOT version has been developed for the VIP 
prototype, VIP-ABBOT, with a test vocabulary 
of 300+ words (Figure 2). This version is based 
on a trigram model, trained on a small corpus of 
200 user questions and replies, using data from 
six speakers (average recording time is twelve 
minutes). Though the size of the corpus is in 
principle too small to obtain an accurate 
language model, the VIP-ABBOT system 
achieves a satisfactory performance. Global 
speech recognition accuracy has been tested as 
part of the development of the VIP-ABBOT 
version. The recognition accuracy varied across 
tests from 65 to a maximum 80 % (at this stage 
only laboratory conditions with non-noisy 
environments and good quality microphones 
have been considered). The system outputs the 
1-best recognised utterance, which is passed to 
the dialogue system via a datagram socket. 
  
We have assembled an evaluation corpus of 500 
utterances, collected from five speakers 
including one non-native speaker. Including a 
non-native speaker was an empirical way of 
increasing the error rate. Other researchers have 
suggested varying parameters of the speech 
recognition system, such as the beam width 
[Boros et al, 1996], as a method to increase 
word error rate, in order to collect error corpora. 
However, they have not documented whether the 
kind of errors induced in this way actually 
reproduce (in terms of distribution) those 
obtained during the actual use of the system. On 
the other hand, recognition errors obtained with 
native and non-native speakers appear similar in 
our experience, the overall error rate just being 
higher in the latter.  
For the whole corpus, approximately 50% of 
recognised utterances contain at least one speech 
recognition error. 
3 Integrated Parsing of User Utterances 
Strictly speaking, a significant proportion 
(around 50%) of the recognition hypotheses 
produced by VIP-ABBOT are ungrammatical. 
For obvious reasons, and since the early stages 
of system development, we have abandoned the 
idea of producing a complete parse for the 
speech input, not so much because user 
expressions themselves could be ungrammatical 
but rather because recognised utterances were 
most certain to be, considering the error rate.  
One of the key questions for parsing, especially 
in the case of dialogue, where the average 
utterance length is 5-7 words, is whether 
complete parsing is at all necessary [Lewin et 
al., 1999]. We have implemented a simplified 
parser based on a variant of Tree-Adjoining 
Grammars [Cavazza, 1998], This syntactic 
formalism being lexicalised has interesting 
properties in terms of syntax-semantics 
integration. This lexicalised formalism, 
combined with a simple bottom-up parser, is 
well adapted to the partial parsing of 
ungrammatical utterances (Figure 3). 
The main goal of parsing is to produce a 
semantic structure from which speech acts can 
be identified. Semantic features are aggregated 
as the parsing progresses following the syntactic 
operations. As a result, the parser produces a 
feature structure whose semantic elements can 
be mapped to the descriptors indexing the 
programmes in the EPG, such as genre (e.g. 
?movie?, ?news?, ?documentary?), sub-genre 
(e.g., ?comedy?, ?lifestyle?), cast (e.g., ?Jeremy 
Clarkson?), channel (?BBC one?), rating  (e.g., 
?caution?, ?family?), etc.  
Whenever the parser fails to produce a single 
parse, the semantic structures obtained from 
partial parses are merged on a content basis. For 
instance, descriptors such as ?cast? or ?channel? 
are attached to programme descriptions, etc. 
This process confers a good level of robustness 
and tolerance to ungrammaticality. This kind of 
approach, where dialogue strategy is privileged 
over parsing was inspired from early versions of 
the AGS system [Sadek, 1999]. These semantic 
structures are used to generate search filters on 
the EPG database, which correspond to semantic 
descriptions of the user choice. They are also 
used for content-based speech act identification, 
by comparing the semantic contents of 
successive utterances [Cavazza, 2000]. 
 
 
 
 
 
 
Figure 3. Parsing of an Utterance 
4 The Dialogue Process 
T e dialogue strategy has been determined 
m
p
u
a
H
c
?
(
t
w
i
W
i
T
p
s
[
r
s
c
c
S
 V
is there
N0 ?
 *N N
anything funny
 N
 I
pro
S
   V
can-?
N0 ? V0 ?
V 
watch
S1 F2
S3
S4h
ainly from the task model. As the task is to 
rogressively refine a programme description by 
sing elementary dialogue acts, we have adopted 
 speech acts based approach [Traum and 
inkelmann, 1992]. Each speech act 
orresponds to a specific construction operation 
 it is possible to map communicative operations 
rejection, implicit rejection, specification, etc.) 
o the updating of the programme description, 
hich is a filter through which the EPG database 
s searched. 
e are using a content-based approach to the 
dentification of speech acts [Cavazza, 2000]. 
his method has similarities with the one 
reviously described by Maier [1996]. Another 
ource of inspiration was the work of Walker 
1996], though it was restricted to the 
ecognition of acceptance rather than a complete 
et of speech acts. Figure 4 shows the 
onstruction of search filters from the semantic 
ontents of user utterances. Once a new 
  
utterance is analysed, its semantic contents are 
compared with the active search filter, which has 
been constructed from previous user utterances, 
and this comparison determines speech act 
recognition. For instance, when the last 
utterance contains semantic information for a 
programme sub-genre, the speech act is a 
specification. Explicit rejections are signalled by 
markers of negation, while implicit rejection 
speech acts are recognised when the semantic 
contents of the latest utterance overwrite the 
descriptors of the current filter (this is the case 
when, for instance, when the current filter 
contains the comedy sub-genre and the user asks 
?can I have a western??). 
In this context, speech acts provide a unified and 
consistent way to determine the most 
appropriate answer to the user as well as the way 
in which the search filter should be updated at 
each dialogue turn. In the next section, we 
propose a first empirical categorisation of 
speech recognition errors according to their 
impact on the dialogue process. 
5 From Speech Recognition Errors to Speech 
Acts Recognition Errors 
Traditional error metrics used in speech 
recognition such as ?word accuracy? are not 
reliable to measure the global consequences of 
speech recognition errors on the dialogue 
process. This is why it has been proposed that a 
?concept accuracy? be used in place of a word 
accuracy. These two metrics appear however to 
be linearly correlated [Boros et al, 1996].  
Word errors result in semantic errors, which in 
turn result in speech act recognition errors. It is 
the latter phenomenon that we are investigating, 
though through a largely empirical approach. 
We have developed the following experimental 
approach to assess the consequences of speech 
recognition errors. Within a set of possible ?test? 
dialogues, we have been running each user reply 
on several users to collect an error corpus. Then 
the impact of errors at critical points of dialogue 
can be tested on the system using textual input, 
entering the utterances comprising speech 
recognition errors. This also makes possible to 
explore the consequences of a set of errors at 
various stages of the dialogue process (see 
below).  
In doing so, we are essentially exploring system 
behaviour in a glass box approach: this does not 
constitute an evaluation method for dialogue 
performance [Walker et al, 1997]. The only 
global metric that can be used in our case is 
dialogue length, as a measure of the extra 
dialogue turns required to repair the 
consequences of a speech recognition error. 
5.1 ?Fatal? Speech Recognition Errors 
Obviously, the worst errors are those which 
terminate the dialogue by selecting a programme 
that the user intended to reject. This happens 
when the semantic structure produced from the 
mis-recognised utterance can be interpreted as 
an acceptance speech act. In the following short 
dialogues, the user implicitly rejects the 
selection offered by the system. He does so by 
specifying either the parental rating (U21) or the 
sub-genre (U25). However, due to speech 
recognition errors, specific semantic information 
disappears in both cases, while the insertion of 
the word ?good? results in the system producing 
a semantic structure that is recognised as an 
acceptance speech act.  
 
U19: Can I have a movie? 
S20: I have found 22 programmes for that 
selection. I would recommend: ?Midnight Fear? 
U21: I want a movie my good can watch 
(REF: I want a movie my kids can watch) 
S22: Here is ?Midnight Fear?, then. Goodbye! 
 
U23: Do you have any movies? 
S24: I found 22 programmes for your request. I 
would suggest: ?Twelve Monkeys? 
U25: Good if I mean anything for long (REF: 
could you find me anything funny)  
S26: Here is ?Twelve Monkeys?, then. 
Goodbye! 
 
5.2 User Correction of Mis-recognition 
Some speech recognition errors result in an 
incorrect speech act being recognised, but can be 
easily corrected by the user at the expense of a 
few additional dialogue turns, as shown by the 
following example (especially U31). 
 
U27: I would like to watch a movie 
S28: I found 22 programmes for this choice. I 
can recommend: ?The Day of the Jackal?   
U29: Can I have worse him? (REF: can I have 
a western?) 
S30: What about the following programme: 
?Midnight Fear??  
  
U31: No, I want a western 
S32: I can recommend this programme which is 
a western: ?Unforgiven?  
 
The next dialogue illustrates an important aspect 
of speech recognition errors. Their impact 
actually varies according to the dialogue stage at 
which they take place. The same kind of error 
that resulted in premature acceptance has less 
dramatic consequences when it occurs in the 
opening request. It results in a partial 
understanding, where the EPG is searched for 
the right genre (?movie?) but without the right 
parental rating. This additional criterion can thus 
be subsequently added by the user, resulting in a 
successful search at the price of a longer 
dialogue. 
 
U33: Is there a movie my good can watch? 
(REF: is there a movie my kids can watch?) 
S34: I have 22 programmes for your request. 
Would you like to watch: ?Midnight Fear?  
U35: What is its rating? 
S36: Its rating is caution 
U37: No, I want a movie my kids can watch 
S38: I would recommend this programme 
corresponding to that ?family? rating: ?Rio 
Bravo? 
 
5.3 System Detection of Mis-recognition 
Some dialogue control mechanisms originally 
developed to monitor dialogue progression can 
also assist in recovering from speech recognition 
errors. The system can detect dead ends in 
dialogue, for instance when not enough 
information is available for programme 
selection. The speech recognition error in U39 
results in a semantic structure rejecting sports 
programmes as ?boring?. In the absence of any 
category selected, the system returns to the user 
(S40), who can then correct the selection: an 
explicit request for the sports category over-
writes its previous rejection. 
 
U39:  What kind of sports boring you have? 
(REF: what kind of sports programmes do you 
have) 
S40: What would you like to see, then?  
U41: A sports programme 
S42: I could find 5 programmes for this choice. I 
can recommend: ?Row Z?  
 
Also, the system reacts to non-productive 
dialogue, i.e. utterances that simply repeat 
categories already represented in the programme 
descriptors. This has an interesting side effect, in 
that it can detect deletion of negation markers, as 
in the example below.  
 
S43: What about the following programme: 
?Countryfile? 
U44: Again want a documentary (REF: I don?t 
want a documentary) 
S45: Is this programme all right, then? 
 
5.4 ?Semantic? Robustness 
The basis for robustness can be seen as a 
combination of shallow processing and natural 
redundancy of information in some of the user 
replies. For instance, in U52, even though the 
entry ?role? as been deleted from the recognised 
utterance, there are sufficient semantic cues to 
activate a ?cast? descriptor. 
 
U46: I would like to watch a movie 
S47: I found 22 programmes for this choice. I 
would recommend: ?Midnight Fear?   
U48: Good if I mean anything funny (REF: 
could you find me anything funny) 
S49: I can recommend this programme which is 
a comedy: ?Casablanca?  
 
U50: what kind of movies do you have 
S51: I have found 22 programmes for this 
choice. I would suggest: ?Twelve Monkeys?   
U52: Who is playing the title all? (REF: who is 
playing the title role?) 
S53: ?Twelve Monkeys? features Bruce Willis   
 
U54: Do you have any documentaries 
S55: I have 2 programmes for your request. My 
first selection is: ?Ground Force? 
U56: An old gardening is not a little A   (REF: 
no, gardening is not OK) 
S57: I can recommend this programme, which is 
not a gardening: ?Fangs? 
 
Another element of robustness lies in the surface 
form of some linguistic expressions for 
acknowledgement, acceptance and rejection, 
which often contains redundant information: 
?yes, this is fine?, ?no, I don?t like that one?, 
?no, gardening is not OK?, etc. 
 
 
  
User:            IS THERE ANYTHING FUNNY I CAN WATCH 
Recognised: THE IS THERE ANYTHING FUNNY I CAN WATCH   
Semantics: (((QUESTION) (EXIST) (PROGRAMME ((CHOICE+))) (DET))
((GOOD) (SUB_GENRE COMEDY)) ((AUDIENCE USER)) ((QUESTION))
((VIEW)))  
Filter: ((SUB_GENRE COMEDY)) 
Speech Act: (INITIAL (SUB_GENRE COMEDY) SEARCH)  
 
System: I found 4 programmes corresponding to that selection. What about: ?Starting Out??   
 
User:              NO I DON?T WANT A SITCOM 
Recognised:   AN OLD I DON'T WANT A SITCOM   
Semantics: ((CHOICE+) (NEGATIVE) (AUDIENCE ((AUDIENCE USER)))
(PROGRAMME
((SUB_GENRE COMEDY) (GENRE ENTERTAINMENT) (INDET))))  
Filter: ((GENRE ENTERTAINMENT) (SUB_GENRE COMEDY) (NEGATIVE)) 
Speech Act: (REJECT GENRE NIL NO_SEARCH)  
 
System: What would you like to see, then?  
 
User:            A COMEDY MOVIE 
Recognised: A COMEDY MOVIE   
Semantics: (((SUB_GENRE COMEDY) (INDET)) ((GENRE MOVIES)))  
Filter: ((GENRE MOVIES) (SUB_GENRE COMEDY))  
Speech Act: (SPECIFY SUB_GENRE COMEDY SEARCH)  
 
System: My first selection is this programme with comedy: ?Casablanca?  
 
User:            YES THIS IS FINE 
Recognised: YES THE IS FIND THAT   
Semantics: (((GOOD) (POSITIVE)) ((DET)) ((ATTRIBUTE)))  
Speech Act: (SIMPLE-ACCEPT) 
 
System: Here is ?Casablanca?, then. Goodbye!  
 
Figure 4. A Sample Dialogue with Internal Representations 
 
 
5.5  ?Accidental? Robustness 
Another form of robustness appears to be rather 
accidental. It happens when a incorrect speech 
act is recognised, but its effects in terms of 
system response are a subset of those of the 
correct speech act. In the following example, the 
user intends to reject the proposal (?Salvador?) 
by specifying a parental rating. This is actually 
interpreted as a simple rejection of the 
programme instance (Other speech act), 
preserving the current ?movie? category 
selected. The new proposal happens to match the 
criteria by chance (i.e., the parental rating is not 
grounded in the system reply, see e.g. S38).  
 
U58: What movies do you have? 
S59: I found 22 programmes for this choice. I 
can suggest: ?Salvador? 
U60: Other one like is this to there it is (REF: 
I don?t want my kids to see this) 
S61: I have 21 other programmes for that 
selection. What about the following programme: 
?Casablanca? 
Conclusion 
We have explored the consequences of speech 
recognition errors in terms of content-based 
speech acts recognition. There appears to be a 
number of factors that support the robustness of 
the system to speech recognition errors, among 
which the fact that dialogue control mechanisms 
triggered by speech act recognition can 
contribute to repairing the consequences of 
speech recognition errors. Some improvement is 
possible in the treatment of errors involving 
mismatches between categories and 
  
connotations (such as ?funny motoring?), by 
including semantic consistency checks. On the 
other hand, errors involving wrongful 
acceptance and dialogue termination appear 
difficult to deal with. 
Finally, Fischer and Batliner [2000] have 
investigated which system replies are most 
likely to upset the user. These replies cannot 
always be always avoided, though, precisely 
because they are used to repair incorrect 
understanding or inconsistent one. It is thus 
important to investigate whether speech 
recognition errors increase the occurrence of 
these upsetting replies (apart from the 
unavoidable and necessary repairs). Obviously, 
in our context the most upsetting cases are the 
selection of a programme explicitly rejected by 
the user. However, It would also be necessary to 
explore whether the repair mechanisms 
described above are well accepted by the users.   
Acknowledgements 
James Christie at Cambridge University 
Engineering Department developed the VIP-
ABBOT version and produced some of the 
global speech recognition results. Tony 
Robinson (now at Softsound Ltd) is thanked for 
his assistance in using the ABBOT system. 
Steve Francis developed the user interface and 
the character. EPG data has been provided by 
the BBC: David Kirby, Matthew Marks and 
Adam Hume are thanked for their support. This 
work has been funded in part by the DTI, under 
the DTI/EPSRC ?LINK Broadcast? Programme. 
References  
James F. Allen, Brad Miller, Eric Ringger, and 
Teresa Sikorski (1996). Robust Understanding in a 
Dialogue System. Proceedings of the 34th Annual 
Meeting of the Association for Computational 
Linguistics, San Francisco, pp. 62-70. 
Jonas Beskow, and Scott McGlashan (1997). Olga: A 
Conversational Agent with Gestures. In: 
Proceedings of the IJCAI'97 workshop on 
Animated Interface Agents - Making them 
Intelligent, Nagoya, Japan, August 1997. 
Manuela Boros, W. Eckert, F. Gallwitz, G. Gorz, G. 
Hanrieder, and H. Niemann (1996). Towards 
Understanding Spontaneous Speech: Word 
Accuracy Vs. Concept Accuracy.  Proceedings of 
the Int. Conf. on Spoken Language Processing 
(ICSLP?96), Philadelphia, pp. 1009-1012. 
Hans Brandt-Pook, Gernot A. Fink, Bernd 
Hildebrandt, Franz Kummert, and Gerhard Sagerer. 
(1996). A Robust Dialogue System for Making an 
Appointment. Proceedings of the Int. Conf. on 
Spoken Language Processing (ICSLP?96), 
Philadelphia, pp. 693-696. 
Marc Cavazza, (1998). An Integrated TFG Parser 
with Explicit Tree Typing. In: Proceedings of the 
fourth TAG+ workshop, IRCS, University of 
Pennsylvania, pp. 34-37. 
Marc Cavazza (2000). From Speech Acts to Search 
Acts: a Semantic Approach to Speech Act 
Recognition. Proceedings of GOTALOG 2000, 
Gothenburg, Sweden, pp. 187-190, June 2000. 
Kerstin Fischer and Anton Batliner (2000). What 
Makes Speakers Angry in Human-Computer 
Conversation. Proceedings of the Third Human-
Computer Conversation Workshop (HCCW), 
Bellagio, Italy, pp. 62-67. 
 Eli Hagen (2000). A Flexible Spoken Dialogue 
Manager. Proceedings of the Third Human-
Computer Conversation Workshop (HCCW), 
Bellagio, Italy, pp.68-73.  
Ian Lewin, Ralph Becket, Johan Boye, David Carter, 
Manny Rayner, and Mats Wiren (1999). Language 
processing for spoken dialogue systems: is shallow 
parsing enough? Accessing Information in Spoken 
Audio: Proceedings of ESCA ETRW Workshop, 
Cambridge, pp. 37--42. 
Bernard Ludwig, Martin Klarner, Heinrich Niemann 
and Gunther Goerz (2000). Context and Content in 
Dialogue Systems. Proceedings of the Third 
Human-Computer Conversation Workshop 
(HCCW), Bellagio, Italy, pp. 105-111. 
Elisabeth Maier (1996). Context Construction as 
Subtask of Dialogue Processing: the VERBMOBIL 
Case. Proceedings of the Eleventh Twente 
Workshop on Language Technologies (TWLT-11), 
Dialogue Management in Natural Language 
Systems, University of Twente, The Netherlands, 
pp. 113-122. 
Katashi Nagao and Akikazu Takeuchi,(1994). Speech 
Dialogue with Facial Displays: Multimodal 
Human-Computer Conversation. In: Proceedings 
of the 32nd Annual Meeting of the Association for 
Computational Linguistics (ACL'94), pp. 102-109. 
Tony Robinson, Mike Hochberg and Steve Renals 
(1996). The use of recurrent neural networks in 
continuous speech recognition. In: C. H. Lee, K. K. 
Paliwal and F. K. Soong (Eds.), Automatic Speech 
and Speaker Recognition ? Advanced Topics, 
Kluwer. 
Lena Stromback and Arne Jonsson. Robust 
interpretation for spoken dialogue systems. (1998). 
Proceedings of ICSLP'98, Sydney, Australia. 
  
David Traum and Elisabeth A. Hinkelman (1992). 
Conversation Acts in Task-Oriented Spoken 
Dialogue. Computational Intelligence, vol. 8, n. 3. 
Marilyn A. Walker (1996). Inferring Acceptance and 
Rejection in Dialogue by Default Rules of 
Inference. Language and Speech, 39-2. 
Marilyn A. Walker, Diane J. Litman, Candace A. 
Kamm and Alicia Abella (1997). PARADISE: A 
Framework for Evaluating Spoken Dialogue 
Agents. Proceedings of the 35th Annual Meeting of 
the Association for Computational Linguistics, pp. 
271-280. 
Nicole Yankelovich, Gina-Anne Levow and Matt 
Marx (1995). Designing Speech Acts: Issues in 
Speech User Interfaces. Procedings of CHI'95, 
Denver. 
 
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 37?42,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
?How was your day??
S. G. Pulman, J. Boye
University of Oxford
sgp@clg.ox.ac.uk
M. Cavazza, C. Smith
Teesside University
m.o.cavazza@tees.ac.uk
R. S. de la Ca?mara
Telefonica I+D
e.rsai@tid.es
Abstract
We describe a ?How was your day??
(HWYD) Companion whose purpose is to
establish a comforting and supportive rela-
tionship with a user via a conversation on
a variety of work-related topics. The sys-
tem has several fairly novel features aimed
at increasing the naturalness of the interac-
tion: a rapid ?short loop? response primed
by the results of acoustic emotion anal-
ysis, and an ?interruption manager?, en-
abling the user to interrupt lengthy or ap-
parently inappropriate system responses,
prompting a replanning of behaviour on
the part of the system. The ?long loop?
also takes into account the emotional state
of the user, but using more conventional
dialogue management and planning tech-
niques. We describe the architecture and
components of the implemented prototype
HWYD system.
1 Introduction
As the existence of this workshop shows, there is a
good deal of interest in a type of spoken language
dialogue system distinct from the traditional task-
based models used for booking airline tickets and
the like. The purpose of these ?social agent? sys-
tems is to be found in the relationship they can
establish with human users, rather than on the as-
sistance the agent can provide in giving informa-
tion or solving a problem. Designing such agents
provides many significant technical challenges, re-
quiring progress in the integration of linguistic
communication and non-verbal behaviour for af-
fective dialogue (Andre? et al 2004). In this pa-
per, we present the implementation of a Compan-
ion Embodied Conversational Agent (ECA) which
integrates emotion and sentiment detection with
more traditional dialogue components.
2 From Dialogue to Conversation
Most spoken language dialogue systems are ?task-
based?: they aim at getting from the user values for
a fixed number of slots in some template. When
enough values have been found, the filled tem-
plate is sent off to some back-end system so that
the task in question - ordering a pizza, booking a
ticket etc. - can be carried out. However, a so-
cial Companion agent assumes a kind of conver-
sation not necessarily connected to any immediate
task, and which may not follow the conventions
associated with task-driven dialogues, for exam-
ple, the relatively strict turn-taking of task-based
dialogue. In everyday life, many interhuman con-
versations see one of the participants producing
lengthy descriptions of events, without this corre-
sponding to any specific request or overall con-
versational purpose. Our objective was to sup-
port such free conversation, whilst still obtaining
meaningful answers from the agent, in the form of
advice appropriate both to the affective and infor-
mational content of the conversation. In order to
balance the constraints of free conversation with
those of tractability, we have deliberately opted
for a single-domain conversation, in contrast with
both small talk (Bickmore and Cassell, 1999) and
?chatterbot? approaches. Our HWYD domain in-
volves typical events and topics of conversation in
the workplace, ranging from the relatively mun-
dane - meeting colleagues, getting delayed by traf-
fic, project deadlines - to rather more important -
promotions, firings, arguments, office politics - de-
signed to evoke stronger emotions and hence more
affective dialogues.
However, our HWYD Companions retains
some features of a typical task based system, in
that each of these subtopics can be thought of as a
task or information extraction template. Unfilled
slots will drive the dialogue manager to question
the user for possible values. When enough slots
37
are filled, the initiative will be passed to an ?affec-
tive strategy? module, which will generate a longer
response designed to empathise appropriately with
the user over that particular topic.
3 System Overview and Architecture
The HWYD Companion integrates 15 different
software components, covering at least to some
degree all the necessary aspects of multimodal af-
fective input and output: including speech recog-
nition (ASR, using Dragon Naturally Speaking),
emotional speech recognition (AA: the EmoVoice
system (Vogt et al 2008)), turn detection (ATT),
Dialogue Act segmentation and tagging (DAT),
Emotional modelling (EM), Sentiment Analy-
sis (SA) (Moilanen et al 2007), Natural Lan-
guage Understanding (NLU), Dialogue Manage-
ment (DM), user modelling and a knowledge
base (KB/UM), an ?Affective Strategy Module?
(ASM) generating complex system replies, Natu-
ral Language Generation (NLG), Speech Synthe-
sis (TTS), an avatar (ECA), and Multimodal con-
trol of the ECA persona (MFM): gesture and fa-
cial expression, supported by the Haptek anima-
tion toolkit. Clearly the use of Naturally Speaking
imposes on us speaker dependence, since the sys-
tem needs training: in the scenario we have chosen
this is in fact not too unrealistic an assumption, but
this is merely a practical decision - we are not do-
ing research on speech recognition as such in this
project and so want to get as good a recognition
rate as possible.
The software architecture of the prototype re-
lies on the Inamode Framework developed by
Telefnica I+D. Communication between modules
follows a blackboard-like paradigm, in which cen-
tral hubs broadcast any incoming message from
any module to all of the other modules that are
connected to it. Figure 1 below shows the system
architecture, and Figure 2 shows one version of
what is on the screen when the system is running.
4 Emotional Feedback Loops
Recognising and responding appropriately to dif-
ferent emotions is an important aspect of a social
agent. In our HWYD Companion, emotion and
sentiment are used in two ways: firstly, to pro-
vide immediate feedback to a user utterance (given
that there will inevitably be some delay in the re-
sponse from natural language and dialogue pro-
cessing modules) and secondly to inform the more
extended responses given by the system when it
has learned enough about the current sub-topic.
There are two feedback loops: the ?short loop?
(response time < 700 ms) provides an immedi-
ate backchannel, and its main purpose is to main-
tain contact and keep the communication alive and
realistic. This is achieved by matching the non-
verbal response (gesture, facial expression) of the
avatar to the emotional speech parameters detected
by EmoVoice prior to affective fusion (where the
emotion detected from speech and the sentiment
value detected from the corresponding text are
merged: see below), and occasionally including
an appropriate verbal acknowledgement, on a ran-
dom basis to avoid acknowledging all user utter-
ances. The short loop essentially aligns the ECA
response to the user?s attitude, thus showing empa-
thy. (We should also use SA for this, but currently
processing speed is not fast enough).
The ?major loop? (response time < 3000 ms) in-
volves the ECA?s full response to the user utter-
ance in terms of both verbal and non-verbal be-
haviour. There are effectively two sources of sys-
tem output: the dialogue manager engages with
the user to find out what happened during their
work day, and will ask questions, or drop into
clarificatory sub-dialogues, gradually building up
a complex event description along with an assess-
ment of the prevailing emotions of the speaker.
When sufficient information has been gathered,
control is passed to an ?affective strategy module?
which will produce a longer output, typically ad-
vice or warning in response to the user?s recollec-
tion of his daily events.
The system also includes an interruption man-
ager which detects interruption and barge-in by the
user, resulting in the immediate suspension of the
current system utterance, triggering the process-
ing of any content specific to the interrupting ut-
terance, and consequent replanning on the part of
other modules to produce an appropriate response.
Such an interruption is illustrated in Figure 1. The
design of such an interruption manager in a sys-
tem with so many separate modules is quite chal-
lenging, in fact: the system is described further in
Crook et al (2010).
The ECA listens sympathetically to the user?s
account of work difficulties, whilst also reacting to
apparent discrepancies between perceived mood
and the affective content of the recognised events.
In the following example from a real conversation,
38
Figure 1: Overview of the System Architecture illustrating the information flow between modules.
Figure 2: The HWYD Companion in operation: the central portion of the screen traces the dialogue, whilst the right-hand
one displays which current software module is activated.
39
the user announces a company restructuring, but in
a rather cheerful tone. Notice that some of the En-
glish is a little strange: the generator produces a
mixture of genuinely generated output along with
some canned text.
User: I have just learnt that my department is going to be
merged with the Barcelona one (EMOTION FUSION: POS-
ITIVE ACTIVE)
System: You seem a bit too ecstatic! I?m doubtful that your
workload could be maintained.
Every time the Companion has gathered suffi-
cient information it will try to respond more glob-
ally to the situation at hand. In the following ex-
ample (see also Figure 2), after the user has ex-
pressed different sorts of concerns, the Companion
picks on the latest (positive) information to sum-
marise the few positive elements it picked from the
conversation:
User: yes but one good thing anyway we had a new
coffee machine arrived today (EMOTION FUSION: POSI-
TIVE ACTIVE)
System: it?s always reassuring to hear good news! It?s right
to feel good about such a good situation. I am quite fond of
getting new stuff. I?m happy that your mood improves. It?s
worth remembering you should take time to enjoy the good
things when they happen.
5 Emotional Modelling and Fusion
Two different modules analyse the emotional con-
tent of user utterances: an emotional speech recog-
nition system (EmoVoice (Vogt et al 2008) which
returns information indicating both the arousal and
valence of the acoustic properties of the user?s
speech as negative passive, negative-active, neu-
tral, positive-active or positive-passive, and a text-
based Sentiment Analysis module which operates
on the utterance transcript after its recognition by
the ASR module. The SA module operates in
a compositional way and is able to classify lin-
guistic units of any syntactic type: noun phrases,
clauses, sentences etc. It is also able to assign
a ?strength? of the sentiment expressed. In the
current implementation it simply classifies clauses
as either negative, neutral or positive. These two
emotional inputs are then merged by a fusion pro-
cedure, whose purpose is to provide an aggregate
emotional category to be attached to the event de-
scription template produced by the NLU and DM
module. Essentially, the mechanism for affective
fusion consists in overriding the valence category
of EmoVoice with the one obtained by SA every
time the confidence score attached to EmoVoice
is below a preset value (depending on the com-
peting valence categories). Fusion is currently an
underdeveloped module: for example, detecting
mismatches between speech and language emo-
tion and sentiment values could lead to the recog-
nition of irony, sarcasm etc. (Tepperman et al
2006). Saying an intrinsically negative thing in a
positive and cheerful way, or the other way round,
suggests that the speaker is trying for some special
effect.
6 Natural Language Understanding and
Dialogue Management
The task of the NLU module is to recognise a spe-
cific set of events reported by the user within ut-
terances which can be of significant length (> 50
words) and which can be difficult to parse due to
speech recognition errors. This led us to follow an
Information Extraction (IE) approach to dialogue
analysis (see Jo?nsson et al 2004), using shallow
syntactic and semantic processing to find instan-
tiations of event templates. The NLU component
of the HWYD Companion demonstration system
takes the 1-best output from the speech recogniser
(currently: work in progress will take n-best),
which has already been segmented into dialogue-
act sized utterances (by the DAT module which si-
multaneously segments and labels the recogniser
output: see Figure 1). So, for example, a sequence
like ?It was okay there are not many projects at the
moment so it is very quiet would be segmented
into three separate dialogue acts. The utterances
are then part-of-speech tagged and chunked into
Noun Phrase (NP) and Verb Group (VG) units.
VGs consist of a main verb and any auxiliary verbs
or semantically important adverbs. Both of these
stages are carried out by a Hidden Markov Model
trained on the Penn Treebank, although some cus-
tomisation has been carried out for this applica-
tion: relevant vocabulary added and some proba-
bilities re-estimated to reflect properties of the ap-
plication. NP and VG chunks are then classified
into ?Named Entity? classes, some of which are
the usual person, organisation, time etc. but oth-
ers of which are specific to the scenario, as is tradi-
tional in IE: e.g. salient work events, expressions
of emotion, organisational structure etc. Named
Entity classification, in the absence of domain spe-
cific training data, is carried out via hand-written
pattern matching rules and gazetteers. Each chunk
40
is further annotated with features encoding the
head word, stem form, polarity, agreement fea-
tures, relevant modifiers, etc. for later syntac-
tic and semantic processing. The NPs and VGs
are represented as unification grammar categories
containing information about the internal structure
of the constituents.
The next stage applies unification based syn-
tax rules which combine NP and VG chunks into
larger constituents. These rules are of two types:
most are syntactically motivated and are attempt-
ing to build a parse tree from which main gram-
matical relations (subject, object, etc.) can be
recognised. These have coverage of the main syn-
tactic constructs of English. But within the same
formalism we add domain specific Information
Extraction type patterns, looking out for particular
constellations of entities and events relevant to the
HWYD scenario, for example ?argument at work
between X and Y?, or ?meeting with X about Y?.
Processing is non-deterministic and so sentences
will get many analyses. We use a ?shortest path
through the chart heuristic to select an interpre-
tation. This is far from perfect, and we are cur-
rently working on a separate more motivated dis-
ambiguation module.
The final stage of processing before the Dia-
logue Manager takes over is to perform reference
resolution for pronouns and definite NPs. This
module is based partly on the system described
by Kennedy and Boguraev 1996, with the various
weighting factors based on theirs, but designed so
that the weights can be trained given appropriate
data. Currently we are collecting such data and
the present set of weights are taken from Kennedy
and Boguraev but with additional salience given
to the domain-specific named entity classes. Each
referring NP gives rise to a discourse referent, and
these are grouped into coreference classes based
on grammatical, semantic, and salience properties.
The DMmaintains an information state contain-
ing all objects mentioned during the conversation,
and uses this information to decide whether the
objects referred to in the utterance are salient or
not. The DM also uses type information to inter-
pret elliptical answers to questions (System: ?Who
was at the meeting?? User: ?Nigel.?). After the
user?s utterance has been interpreted in its dia-
logue context and the information state has been
updated, the dialogue manager decides on the ap-
propriate response. If a new object has been intro-
duced by the user, the DM adds a goal to its agenda
to talk about that object. For instance, if a new per-
son is mentioned, the DM will ask questions about
the user?s relation to that person, etc.
For each turn of the dialogue, the DM chooses
which topic to pursue next by considering all the
currently un-satisfied goals on the agenda and
heuristically rating them for importance. The
heuristics employed use factors such as recency in
the dialogue history, general importance, and emo-
tional value associated with the goal. We are cur-
rently exploring the use of reinforcement learning
with a reward function based on the results of SA
on the users input to choose goals in a more natural
way. The DM also has the option of invoking the
ASM (described below) to generate an appropri-
ate answer, in the cases where the user says some-
thing highly emotive. Again, this is a decision that
could involve reinforcement learning, and we are
exploring this in our current work.
The joint operation of the NLU and the DM
hence supports a kind of IE or task-specific
template-filling: the content of the user?s utter-
ances, prompted by questions from the DM, pro-
vides the information necessary to fill a template
to the point where the ASM can take over. The
number of templates for domain events is signifi-
cantly higher than in traditional IE or task-based
dialogue systems, however, since the HWYD
Companion currently instantiates more than 30
templates, and will eventually cover around 50.
7 Affective Dialogue Strategies
Once the NLU and DM have a sufficiently in-
stantiated template, which also records emotional
value, it is passed to the ASM. This controls the
generation of longer ECA narrative replies which
aim at influencing the user by providing advice or
reassurance. Our overall framework for influence
is inspired by the work of Bremond 1973. The
narrative is constituted by a set of argumentative
statements which can be based on emotional op-
erators (e.g. show-empathy) or specific commu-
nicative operators. The ASM is based on a Hier-
archical Task Network (HTN) planner (Nau et al
2004), which works through recursive decompo-
sition of a high level task into sub-tasks until we
reach a plan of sub-tasks that can be directly ex-
ecuted. The operators constituting the plan gen-
erated by the HTN implement Bremond?s the-
ory of influence by emphasising the determinants
41
of the event reported by the user. For instance,
various operators can emphasise or play down
the event consequences (emphasise-outcome-
importance, emphasise-outcome-justification,
emphasise-outcome-warning) or comment on
additional factors that may affect the course
of events (commend-enabler, reassure-helper).
The planner uses a set of 25 operators, each of
which can be in addition instantiated to incorpo-
rate specific elements of the event. Overall this
supports the generation of hundreds of signifi-
cantly different influencing strategies.
8 Results and Conclusions
We have described an initial, fully-implemented
prototype of a Companion ECA supporting free
conversation, including affective aspects, over a
variety of everyday work-related topics. The sys-
tem has been demonstrated extensively outside of
its development group and was regularly able to
sustain consistent dialogues with an average du-
ration exceeding 20 minutes. The Companion
ECA recently won the best demonstration prize
at AAMAS 2010,the 9th International Conference
on Autonomous Agents and Multiagent Systems,
Toronto, which is some subjective indication at
least that its behaviour is of some interest outside
of the project which developed it.
However, we have not yet systematically evalu-
ated the ECA, although this task has begun (Webb
et al 2010). The question of evaluation for sys-
tems like this is in fact a rather difficult one, since
unlike task-based systems there is no simple mea-
sure of success. In our current work we aim to
conduct extensive trials with real users and via
interview and questionnaires to get some useful
measure of how natural and ?companionable? the
system is perceived to be.
In other current work we are, as mentioned
above, experimenting with reinforcement learning
where the reward function is based on the emo-
tion and sentiment detected in the user?s input. We
are collecting data via Amazon?s Mechanical Turk
and hope to be able to show how the ECA can de-
velop different ?personalities? depending on how
this reward function is defined. For example, we
could imagine using simulated dialogues to pro-
duce a Companion that was relentlessly cheerful,
producing positive outputs whatever the input. Al-
ternatively, we could produce a ?mirror? Compan-
ion which simply reflected the mood of the user.
We could even produce a ?misery loves company?
Companion which, instead of trying to cheer the
user up when recognising negative sentiment or
emotion, could reply in an equally negative man-
ner.
Acknowledgements
This work was funded by the Companions project
(http://www.companions-project.org) sponsored by the Euro-
pean Commission as part of the Information Society Tech-
nologies (IST) programme under EC grant number IST-FP6-
034434. The EmoVoice system has been used courtesy of
the Multimedia Concepts and Applications Group of the Uni-
versity of Augsburg. Other contributors to the prototype de-
scribed in this paper are Karo Moilanen, and from the COM-
PANIONS consortium: David Benyon, Jay Bradley, Daniel
Charlton, WeiWei Cheng, Morena Danieli, Simon Dobnik,
Carlos Sanchez Fernandez, Debora Field, Mari Carmen Ro-
driguez Gancedo, Jose Relano Gil, Ramon Granell, Jaakko
Hakulinen, Preben Hansen, Sue Harding, Topi Hurtig, Oli
Mival, Roger Moore, Olov Stahl, Markku Turunen, Enrico
Zovato.
References
Andre?, E., Dybkjr, L., Minker, W., and Heisterkamp, P.
(Eds.), 2004, Affective Dialogue Systems Lecture Notes in
Computer Science 3068, Springer.
Bickmore, T., and Cassell, J., 1999. Small Talk and Con-
versational Storytelling in Embodied Interface Agents. Pro-
ceedings of the AAAI Fall Symposium on Narrative Intelli-
gence, pp. 87-92. November 5-7, Cape Cod, MA.
Bremond, C., 1973, Logique du Re?cit, Paris: Editions du
Seuil.
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. And Andre?,
E. 2009, Emotional input for character-based interactive sto-
rytelling. International Joint Conference on Autonomous
Agents and Multi-Agents Systems 2009, pp. 313-320.
Nigel Crook, Cameron Smith, Marc Cavazza, Stephen
Pulman, Roger Moore, Johan Boye, 2010, Handling User In-
terruptions in an Embodied Conversational Agent Proceed-
ings of International Workshop on Interacting with ECAs as
Virtual Characters, AAMAS 2010.
Jo?nsson, A., Ande?n, F., Degerstedt, L., Flycht-Eriksson,
A., Merkel, M., and Norberg, S., 2004, Experiences from
combining dialogue system development with information ex-
traction techniques, in: Mark T. Maybury (Ed), New Direc-
tions in Question Answering, AAAI/MIT Press.
Kennedy and B. Boguraev, 1996, Anaphora for everyone:
Pronominal anaphora resolution without a parser. Proceed-
ings of the 16th International Conference on Computational
Linguistics, Copenhagen, ACL, pp 113-118.
Moilanen, K. and Pulman, S. G. , 2007, Sentiment Compo-
sition, Proceedings of the Recent Advances in Natural Lan-
guage Processing International Conference (RANLP-2007),
pp 378?382.
Nau, D., Ghallab, M., Traverso, P., 2004,Automated Plan-
ning: Theory and Practice, Morgan Kaufmann Publishers
Inc., San Francisco, CA.
J Tepperman, D Traum, and S Narayanan, 2006, ?Yeah
right?: Sarcasm recognition for spoken dialogue systems, In-
terspeech 2006, Pittsburgh, PA, 2006.
Vogt, T., Andre?, E. and Bee, N., 2008 EmoVoice - A frame-
work for online recognition of emotions from voice. In: Pro-
ceedings of Workshop on Perception and Interactive Tech-
nologies for Speech-Based Systems, Springer, Kloster Irsee,
Germany, (June 2008).
Webb, N., D. Benyon, P. Hansen and O. Mival, 2010,
Evaluating Human-Machine Conversation for Appropriate-
ness, in proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC2010), Valletta,
Malta.
42
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277?280,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An affective companion ECA prototype 
 
Marc Cavazza 
School of Computing 
Teesside University 
Middlesbrough TS1 3BA 
m.o.cavazza@tees.ac.uk 
Ra?l Santos de la C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
e.rsai@tid.es 
Markku Turunen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
mturunen@cs.uta.fi 
 
 
Jos? Rela?o Gil 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
joserg@tid.es 
Jaakko Hakulinen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
jh@cs.uta.fi 
Nigel Crook 
Oxford University 
Computing Laboratory 
Oxford OX1 3QD 
nigc@comlab.ox.
ac.uk 
Debora Field 
Computer Science 
Sheffield University 
Sheffield S1 4DP 
d.field@shef. 
ac.uk 
 
Abstract 
This paper presents a dialogue system in 
the form of an ECA that acts as a socia-
ble and emotionally intelligent compan-
ion for the user. The system dialogue is 
not task-driven but is social conversation 
in which the user talks about his/her day 
at the office. During conversations the 
system monitors the emotional state of 
the user and uses that information to in-
form its dialogue turns. The system is 
able to respond to spoken interruptions 
by the user, for example, the user can in-
terrupt to correct the system. The system 
is already fully implemented and aspects 
of actual output will be used to illustrate. 
1 Introduction 
Historically, Embodied Conversational Agents 
(ECAs) have been used in research and industry 
make information and complex tasks more ac-
cessible to customers and users. With the rise of 
new technologies in affective dialogue systems, 
we are beginning to see a future in which ECA 
dialogues are not all task-driven, but some will 
be focused on the social aspects of conversation. 
We envisage the development of ECAs that en-
hance the social quality of life of the user, and 
that build deep relationships with their users over 
years of use. Our affective companion ECA is 
our first contribution to this emerging technol-
ogy.  
Our emotionally intelligent dialogue system 
requires a high level of understanding of the 
meaning of the user turns, and so is quite unlike 
a ?chatbot?. Affective aspects of the situation are 
also key in influencing the course of the conver-
sation. The system attempts to balance both se-
mantic and affective understanding in order to 
create an engaging and enjoyable companion for 
social conversation. 
2 Scenario description 
The system conversation is free-ranging and fol-
lows the user?s initiative, rather than being task-
driven. We have for the time being restricted the 
range of conversation topics to things that hap-
pen in the life of someone who works in an of-
fice (the ?How was your day?? (HWYD) sce-
nario). The user ?offloads? about his/her day at 
work by chatting to the ECA, and the ECA tries 
to empathise with and advise the user. The ECA 
asks questions to elicit information from the user, 
makes empathetic and sympathetic comments, 
and offers advice.  
The system is able to process long user turns, 
as well as to generate long system turns (around 
50 words). If the user perhaps dislikes what 
he/she hears, he/she can interrupt the system by 
barging in, and the ECA will respond in a hu-
man-like way. The conversation style is therefore 
considerably different from traditional task-based 
dialogues that consist of short dialogue turns and 
in which the system takes all the initiative.  
The dialogue in Table 1 (actual output from a 
user session) illustrates some of the significant 
features of the system, including long system 
277
turns, user interruptions and emotional analysis 
of user input.  
 
User1 Worst day ever. I arrived late be-
cause of the traffic. 
System1 You have my sympathy. What 
happened next? 
User2 I missed the meeting on project 
responsibilities; got more work 
dumped on me. Just what I don?t 
need. I?m already likely to miss 
the deadline. [ECA looks worried 
when user?s negative tone is de-
tected] 
System2 Sorry to hear that. Who was at the 
meeting? 
User3 Only my team. They gave me a 
set of PowerPoint slides from the 
meeting. I spent the rest of the day 
reading them but my computer 
kept crashing because of  updates. 
System3 Oh dear! It is understandable how 
you must be feeling. It?s right to 
be unhappy about such a bad situ-
ation. Try not to worry about 
missing the deadline because of 
the technical problems. [At this 
point user interrupts ECA by 
beginning turn User4] It?s sad that 
your motivation could be affected 
by them. [System stops speaking] 
User4 No, I?m not worried about the 
technical problems, I?m worried 
about the amount of extra work on 
this project and the deadline. 
System4 What is the project about? 
 
Table 1: Dialogue example 
 
A video demonstration of the system in action 
is available at: 
http://www.youtube.com/watch?v=
BmDMNguQUmM 
3 Architecture 
Figure 1 shows a screen shot taken at run-time of 
actual system output. The ECA is represented on 
a screen as a woman (waist up) who displays 
natural, human-like movements and performs a 
wide range of complex facial expressions, bodily 
movements, and hand and arm gestures. 
The screen also displays a transcript of the 
user and system turns. The user turns shown con-
stitute the output of the Automatic Speech Rec-
ogniser (ASR). The system?s analysis of the 
user?s emotional state is also shown. 
The right-most panel of the screen shows 
graphics which convey real-time information 
about how the dialogue is being processed. It 
presents a streamlined view of the software 
modules that comprise the system. Module activ-
ity is visually represented at run-time by flashing 
colours. This ?glass-box? approach enables de-
tailed observation and analysis of system 
procedure at run-time. 
The system comprises a number of distinct 
modules that are connected using Inamode, a 
hub-based message-passing framework using 
XML formatted messages over plain text sock-
ets. 
The system?s ASR is the NuanceTM dictation 
engine. This is run in parallel with our own a-
coustic analysis pipeline which extracts low level 
(pitch, tone) speech features and also high-level 
features such as emotional characteristics. 
Analysis of the emotions is currently carried out 
 
Figure 1: Screenshot of the prototype interface 
 
278
by EmoVoice (Vogt et al (2008)). The ASR 
output strings are analysed for sentiment by the 
AFFECTiS system (Moilanen and Pulman (2007, 
2009)) and classed as positive, neutral, or nega-
tive. This output is fused with the output from 
EmoVoice to generate a value that represents the 
user?s current emotional state, which is ex-
pressed as a valence+arousal pairing (with five 
possible values). 
The ASR output goes to our own Natural Lan-
guage Understanding (NLU) module which per-
forms syntactic and semantic analysis of user 
utterances and derives noun phrases and verb 
groups and associated arguments. Events rele-
vant to the scenario (e.g., promotions, redundan-
cies, meetings, arguments, etc.) are recognised 
by the NLU and are used to populate an ontology 
(a model of the conversation content).  The sys-
tem is currently able to recognize and respond to 
more than 30 event types.  
The events recognised in a user turn are 
labelled with the output of the Emotion Module 
for that turn; the result is a representation of both 
the semantic and affective information that the 
user might be trying to convey. 
Our own rule-based Dialogue Manager (DM) 
takes the affect-annotated semantic output of the 
NLU, and from that and its model of the conver-
sation content determines the next system turn. It 
will either ask a question about the events that 
occurred in the user?s day, express an opinion on 
the events already described, or make empathetic 
comments. Whenever the system has gained suf-
ficient understanding of a key event in the user?s 
day, it generates a complex long turn that encap-
sulates comfort, opinion, warnings and advice to 
the user. 
These long system turns are generated by our 
own plan-based Affective Strategy Module that 
makes an appraisal of the user?s situation and  
generates an appropriate emotional strategy 
(Cavazza et al (2010)). This strategy?expressed 
as an abstract, conceptual representation?is han-
ded to our own Natural Language Generator 
(NLG) that maps it into a series of linguistic sur-
face forms (usually 4 or 5 sentences). We use a 
style-controllable system using Tree-Furcating 
Grammars (an extension of the Tree-Adjoining 
Grammars formalism (Joshi et al (1997)). This 
ensures the generation of a large set of different 
surface forms from the same semantic input. 
The output of the NLG is passed to a module 
that adds this information to its system turn 
instructions for the ECA. The ECA has been de-
veloped around the HaptekTM toolkit and is con-
trolled using an FML-like language (after 
Hern?ndez et al (2008)). This 2-D embodiment 
produces gestures, facial expressions, and body 
movements that convey the emotional state of 
the ECA. Its movements and expressions enable 
it to visually display interest and enjoyment in 
talking to the user, and to display empathy with 
the user. The speech synthesis module is our own 
emotion-focused extension of the LoquendoTM 
TTS system. It includes paralinguistic elements 
such as exclamations and laughter, and emo-
tional prosody generation for negative and posi-
tive utterances. 
4 Special procedural features 
A significant processing design feature of the 
system is that there are two main processing 
loops from user input to system output; a ?long 
loop? which passes through all the components 
of the system; and a ?short loop? or ?feedback 
loop? which will now be discussed (the proce-
dure already described in Section 3 is the long 
loop procedure). 
4.1 Feedback loop 
The feedback loop (?short loop?) bypasses many 
linguistic components and generates immediate 
reactions to user activity. The main function of 
the short loop is maintain user engagement by 
preventing unnaturally long gaps of ECA inactiv-
ity. The feedback loop engages the acoustic 
analysis components, the TTS, and the ECA. It is 
responsible for the generation of real-time (< 500 
ms) reactions in the ECA in response to the emo-
tional state of the user. It attempts to align  both 
verbal behaviour (backchannelling) and non-
verbal behaviour (facial expressions, gestures, 
and general body language) to the emotions de-
tected during most recent user turn. In order to 
achieve a reasonable level of realism, these sys-
tem reactions to the perceived emotional state of 
the user need to be perceptibly instantaneous. 
Using this short feedback loop that bypasses 
many of the linguistic components ensures this. 
The feedback loop is also occasionally used to 
make sympathetic comments immediately after 
the user stops speaking. These act as acknowl-
edgements of the emotion expressed by the user. 
An example can be seen in the System2 turn of 
the example dialogue in Table 1: 
1.?Sorry to hear that. Who was at the meeting?? 
Here, the first utterance was spoken by the sys-
tem within a few tenths of a second after the end 
279
of the previous user turn (User2). The system 
tried to identify the user?s emotion in the previ-
ous turn and then to behave linguistically and 
visually in an empathetic way. The actual sympa-
thetic utterance was randomly chosen from a set 
of ?negative emotion utterances? (there are also 
?positive? and ?neutral? sets).  
The second half of the system turn in (1) was 
derived by the system?s ?long loop?. It is a ques-
tion which refers to a meeting that the user men-
tioned in the previous turn. This ?meeting? event 
has been heard by the ASR, understood by the 
NLU system, remembered by the DM, and is 
now referred to by an appropriate definite noun 
phrase in the output of the NLG.   
The feedback and main loops run in parallel. 
However, the feedback loop generates its speech 
output almost immediately, giving time for the 
main dialogue loop to complete its more detailed 
analysis of the user?s utterance.  
4.2 Handling user interruptions 
This system has a complex strategy for handling 
situations in which the user interrupts long 
system turns.  The system?s response to ?barge-
in? user interruptions is overseen by the Interrup-
tion Manager (IM), which is alerted by the 
acoustic input modules whenever a genuine user 
interruption (as opposed to, say, a backchannel) 
is detected during a long system utterance. When 
alerted, the IM instructs the ECA to stop speak-
ing when it reaches a natural stopping point in its 
current turn (usually the end of the current 
phrase). The user?s interruption utterance is 
processed by the long loop. Its progress is 
tracked and controlled by the IM, for example, it 
makes sure that the linguistic modules know that 
the current utterance is an interruption, whic 
means it requires special treatment. The DM has 
a range of strategies for system recoveries from 
user interruptions, including different ways of 
continuing, replanning, and aborting. An exam-
ple of a user interruption is shown in Table 1. 
The user interrupts the long system utterance in 
the System3 turn. The system?s response to the 
interruption is to stop the speech output from the 
ECA, abort the long system turn altogether, and 
instead to ask for more details about the project 
that the user has just mentioned during the inter-
ruption. (See (Crook et al (2010))  for a more 
detailed description of the IM.) 
 
 
Acknowledgements 
This work was funded by Companions, a Eu-
ropean Commission Sixth Framework Pro-
gramme Information Society Technologies Inte-
grated Project (IST-34434).  
We would also like to thank the following 
people for their valuable contributions to the 
work presented here: Stephen Pulman, Ramon 
Granell, and Simon Dobnick (Oxford Univer-
sity), Johan Boye (KTH Stockholm), Cameron 
Smith and Daniel Charlton (Teesside Univer-
sity), Roger Moore, WeiWei Cheng and Lei Ye 
(University of Sheffield), Morena Danieli and 
Enrico Zovato (Loquendo). 
References 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. of the 5th Int. Conf. on Per-
suasive Technology (Persuasive 2010), to appear 
2010. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent In 
proc. of AAMAS 2010. 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
(2008) Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language.  
Joshi, A.K. & Schabes, Y. (1997) Tree-adjoining 
Grammars. Handbook of formal languages, vol. 3: 
Beyond Words, Springer-Verlag New York, Inc., 
New York, NY, 1997. 
Moilanen, K. and Pulman. S. (2009). Multi-entity 
Sentiment Scoring. Proc. Recent Advances in 
Natural Language Processing (RANLP 2009). 
September 14-16, Borovets, Bulgaria. pp. 258--
263.  
Moilanen, K. and Pulman. S. (2007). Sentiment Com-
position. Proc. Recent Advances in Natural Lan-
guage Processing (RANLP 2007). September 27-
29, Borovets, Bulgaria. pp. 378--382. 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions 
from voice. Proc. Workshop on Perception and 
Interactive Technologies for Speech-Based Sys-
tems, Springer, Kloster Irsee, Germany, (June 
2008). 
280
