Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1501?1510, Dublin, Ireland, August 23-29 2014.
Annotating Argument Components and Relations in Persuasive Essays
Christian Stab
?
and Iryna Gurevych
??
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA),
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF),
German Institute for Educational Research
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we present a novel approach to model arguments, their components and relations
in persuasive essays in English. We propose an annotation scheme that includes the annotation
of claims and premises as well as support and attack relations for capturing the structure of argu-
mentative discourse. We further conduct a manual annotation study with three annotators on 90
persuasive essays. The obtained inter-rater agreement of ?
U
= 0.72 for argument components
and ? = 0.81 for argumentative relations indicates that the proposed annotation scheme success-
fully guides annotators to substantial agreement. The final corpus and the annotation guidelines
are freely available to encourage future research in argument recognition.
1 Introduction
The ability of formulating persuasive arguments is a crucial aspect in writing skills acquisition. On the
one hand, well-defined arguments are the foundation for convincing an audience of novel ideas. On the
other hand, good argumentation skills are essential for analyzing different stances in general decision
making. By automatically recognizing arguments in text documents, students will be able to inspect
their texts for plausibility as well as revise the discourse structure for improving argumentation quality.
This assumption is supported by recent findings in psychology, which confirm that even general tutorials
effectively improve the quality of written arguments (Butler and Britt, 2011). In addition, argumentative
writing support systems will enable tailored feedback by incorporating argument recognition. Therefore,
it could be expected that they provide appropriate guidance for improving argumentation quality as well
as the student?s writing skills.
An argument consists of several components (i.e. claims and premises) and exhibits a certain structure
constituted by argumentative relations between components (Peldszus and Stede, 2013). Hence, rec-
ognizing arguments in textual documents includes several subtasks: (1) separating argumentative from
non-argumentative text units, (2) identifying claims and premises, and (3) identifying relations between
argument components.
There exist a great demand for reliably annotated corpora including argument components as well as
argumentative relations (Reed et al., 2008; Feng and Hirst, 2011) since they are required for supervised
machine learning approaches for extracting arguments. Previous argument annotated corpora are limited
to specific domains including legal documents (Mochales-Palau andMoens, 2008), newspapers and court
cases (Reed et al., 2008), product reviews (Villalba and Saint-Dizier, 2012) and online debates (Cabrio
and Villata, 2012). To the best of our knowledge, no work has been carried out to annotate argument
components and argumentative relations in persuasive essays (section 2). In addition, the reliability of
the corpora is unknown, since only few of these works provide holistic inter-rater agreement scores and
none a detailed analysis and discussion of inter-rater agreement.
In this work, we introduce a new argument annotation scheme and a corpus of persuasive essays
annotated with argument components and argumentative relations. Our primary motivation is to create
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1501
a corpus for argumentative writing support and to achieve a better understanding of how arguments are
represented in texts. In particular, the contributions of this paper are the following: First, we introduce
a novel annotation scheme for modeling arguments in texts. Second, we present the findings of a pre-
study and show how the findings influenced the definition of the annotation guidelines. Third, we show
that the proposed annotation scheme and guidelines lead to substantial agreement in an annotation study
with three annotators. Fourth, we provide the annotated corpus as freely available resource to encourage
future research.
1
2 Related Work
2.1 Previous Argument Annotated Corpora
Currently, there exist only a few corpora that include argument annotations. The work most similar to
ours with respect to the annotation scheme is Araucaria (Reed et al., 2008) since it also includes structural
information of arguments. It is based on the Argumentation Markup Language (AML) that models argu-
ment components in a XML-based tree structure. Thus, it is possible to derive argumentative relations
between components though they are not explicitly included. In contrast to our work, the corpus con-
sists of several text genres including newspaper editorials, parliamentary records, judicial summaries and
discussion boards. In addition, the reliability of the annotations is unknown. Nevertheless, researchers
use the corpus for different computational tasks, e.g. separating argumentative from non-argumentative
sentences (Mochales-Palau and Moens, 2011), identifying argument components (Rooney et al., 2012)
and classifying argumentation schemes (Feng and Hirst, 2011).
Mochales-Palau and Moens (2008) conduct an argument annotation study in legal cases of the Euro-
pean Court of Human Rights (ECHR). They experiment with a small corpus of 10 documents and obtain
an inter-rater agreemnt of ? = 0.58. In a subsequent study, they elaborated their guidelines and obtain
an inter-rater agreement of ? = 0.75 on a corpus of 47 documents (Mochales-Palau and Moens, 2011).
Unfortunately, the annotation scheme is not described in detail, but it can be seen from the examples that
it includes annotations for claims and supporting or refuting premises. Unlike our work, the annotation
scheme does not include argumentative relations.
Cabrio and Villata (2012) annotate argumentative relations in debates gathered from Debatepedia. In-
stead of identifying argument components, they are interested in relations between arguments to identify
which are the ones accepted by the community. They apply textual entailment for identifying support and
attack relations between arguments and utilize the resulting structure for identifying accepted arguments.
Therefore, they annotate a pair of arguments as either entailment or not. In contrast to our work, the ap-
proach models relationships between pairs of arguments and does not consider components of individual
arguments. In addition, the work does not include an evaluation of the annotation?s reliability.
Villalba and Saint-Dizier (2012) study argumentation annotation in a corpus of French and English
product reviews. Their goal is to identify arguments related to opinion expressions for recognizing
reasons of customer opinions. Their annotation scheme is limited to eight types of support (e.g. jus-
tification, elaboration, contrast). Compared to our annotation scheme, the work distinguishes between
different premise types. However, the approach is tailored to product reviews, and the work does not
provide an inter-rater agreement study.
In contrast to previous work, our annotation scheme includes argument components and argumentative
relations. Both are crucial for argument recognition (Sergeant, 2013) and argumentative writing support.
First, argumentative relations are essential for evaluating the quality of claims, since it is not possible
to examine how well a claim is justified without knowing which premises belong to a claim (Sampson
and Clark, 2006). Second, methods that recognize if a statement supports or attacks a claim enable
the collection of additional evidence from other resources to recommend argument improvement. In
addition, we provide a detailed analysis of the inter-rater agreement and an analysis of disagreements.
1
http://www.ukp.tu-darmstadt.de/data/argumentation-mining
1502
2.2 Persuasive Essays
Persuasive essays are extensively studied in the context of automated essay grading (Shermis and
Burstein, 2013), which aims at automatically assigning a grade to a student?s essay by means of sev-
eral features. Since the argument structure is crucial for evaluating essay quality, Burstein et al. (1998)
propose an approach for identifying the argumentative discourse structure by means of discourse mark-
ing. They utilize a surface cue word and phrase lexicon to identify the boundaries of arguments at the
sentence level in order to evaluate the content of individual arguments and to enrich their feature set
for determining precise grades. Although the identification of argument boundaries is important for ar-
gument recognition, our work allows a more fine-grained analysis of arguments since it also includes
argument components and argumentative relations.
Madnani et al. (2012) studied persuasive essays for separating organizational elements from content.
They argue that the detection of organizational elements is a step towards argument recognition and
inferring the structure of persuasive discourse. Further, they refer to organizational elements as claim
and premise indicating word sequences which they call shell expressions. They annotate 200 essays and
estimate an inter-rater agreement of ? = 0.699 and F
1
= 0.726 on a subset of 50 essays annotated by
two annotators. However, their annotation scheme is limited to shell expressions and compared to our
work it does not include argument components or argumentative relations.
Additional annotation studies on persuasive essays focus on identifying style criteria (Burstein and
Wolska, 2003), factual information (Beigman Klebanov and Higgins, 2012), holistic scores for argu-
mentation quality (Attali et al., 2013) or metaphors (Beigman Klebanov and Flor, 2013). We are not
aware of an annotation study including argument components and argumentative relations in persuasive
essays.
3 Annotation Scheme
The goal of our proposed annotation scheme is to model argument components as well as argumenta-
tive relations that constitute the argumentative discourse structure in persuasive essays. We propose an
annotation scheme including three argument components and two argumentative relations (figure 1).
Figure 1: Argument annotation scheme including argument components and argumentative relations
indicated by arrows below the components.
3.1 Argument Components
Persuasive essays exhibit a common structure. Usually, the introduction includes a major claim that
expresses the author?s stance with respect to the topic. The major claim is supported or attacked by
arguments covering certain aspects in subsequent paragraphs. Sentences (1?3) illustrate three examples
of major claims (the major claim is in bold face).
2
(1) ?I believe that we should attach more importance to cooperation during education.?
(2) ?From my viewpoint, people should perceive the value of museums in enhancing their
own knowledge.?
(3) ?Whatever the definition is, camping is an experience that should be tried by everyone.?
In the first example, the author explicitly states her stance towards cooperation during education.
The major claims in the second and third example are taken from essays about museums and camping
2
We use examples from our corpus (5.1) without correcting grammatical or spelling errors.
1503
respectively. In (1) and (2) a stance indicating expression (?I believe? and ?Frommy viewpoint?) denotes
the presence of the major claim. Although, these indicators are frequent in persuasive essays, not every
essay contains an expression that denotes the major claim. In those cases, the annotators are asked to
select the expression that is most representative with respect to the topic and author?s stance (cf. (3)).
The paragraphs between introduction and conclusion of persuasive essays contain the actual arguments
which either support or attack the major claim.
3
Since argumentation has been a subject in philosophy
and logic for a long time, there is a vast amount of argumentation theories which provide detailed defini-
tions of argument components (Toulmin, 1958; Walton et al., 2008; Freemen, 2011).
4
All these theories
generally agree that an argument consists of several components and that it includes a claim that is sup-
ported or attacked by at least one premise. Examples (4) and (5) illustrate two arguments containing a
claim (in bold face) and a premise (underlined).
(4) ?It is more convenient to learn about historical or art items online. With Internet, people
do not need to travel long distance to have a real look at a painting or a sculpture, which
probably takes a lot of time and travel fee.?
(5) ?Locker checks should be made mandatory and done frequently because they assure se-
curity in schools, makes students healthy, and will make students obey school policies.?
The claim is the central component of an argument. It is a controversial statement that is either true
or false and should not be accepted by readers without additional support. The premise underpins the
validity of the claim. It is a reason given by an author for persuading readers of the claim. For instance, in
(4) the author underpins his claim that Internet usage is convenient for exploring cultural items because
of time and travel fee savings. In this example, both components cover a complete sentence. However,
a sentence can also contain several argument components like in example (5). Therefore, we do not
predefine the boundaries of the expression to be annotated (markable) in advance and annotate each
argument as a statement, which is a sequence of words that constitutes a grammatically correct sentence.
To indicate if an argument supports or attacks a major claim, we add a stance attribute to the claim
that denotes the polarity of an argument with respect to the author?s stance. This attribute can take the
values for or against. For example, the argument given in (4) refutes the major claim in example (2).
Thus, the stance attribute of the claim in (4) is set to against in this example.
3.2 Argumentative Relations
Argumentative relations model the discourse structure of arguments in persuasive essays. They indicate
which premises belong to a claim and constitute the structure of arguments. We follow the approach
proposed by Peldszus and Stede (2013) and define two directed relations between argument components:
support and attack.
5
Both relations can hold between a premise and another premise, a premise and a
(major-) claim, or a claim and a major claim (figure 1). For instance, in example (4) the premise in the
second sentence is a reason or justification for the claim in the first sentence and the claim in (4) attacks
the major claim of example (2). Thus, an argumentative relation between two components indicates
that the source component is a reason or a refutation for the target component. The following example
illustrates a more complex argument including one claim and three premises.
(6) ?Living and studying overseas is an irreplaceable experience when it comes to learn
standing on your own feet. One who is living overseas will of course struggle with loneliness,
living away from family and friends
1
but those difficulties will turn into valuable experiences
in the following steps of life
2
. Moreover, the one will learn living without depending on anyone
else
3
.?
Figure 2 illustrates the structure of this argument. The claim is attacked by premise
1
, whereas premise
2
is a refutation of premise
1
. The third premise is another reason that underpins the claim in this paragraph.
3
In some cases, the introduction or conclusion contains arguments as well, those are also annotated in the annotation study.
4
A review of argumentation theory is beyond the scope of this paper but a survey can be found in (Bentahar et al., 2010)
5
Peldszus and Stede also define a counter-attacking relation that is omitted in our scheme, since it can also be represented
as a chain of attacking premises.
1504
This shows that it is not necessary to explicitly distinguish between supporting and attacking premises,
since the relational structure and the type of argumentative relations implicitly denote the role of argu-
ment components. Additionally, argumentative relations enable the modeling of relationships between
pairs of arguments on the macro level, e.g., by linking claims to the major claim.
Figure 2: Argumentation structure of example (6)
4 Pre-Study
We conduct a preliminary study to define the annotation guidelines on a corpus of 14 short text snip-
pets (1?2 sentences) that are either gathered from example essays or written by one of the authors. We
ask five non-trained annotators to classify each text as argumentative or non-argumentative. If a text is
classified as argumentative, the annotators are asked to identify the claim and the premise. In the first
task, we obtain an inter-rater agreement of 58.6% and multi-? = 0.171 (Fleiss, 1971)
6
. We identified the
markables for measuring the inter-rater agreement of the second task by manually determining the state-
ments in each of the 14 text snippets. In total, we determined 32 statements and obtained an inter-rater
agreement of 55.9% and multi-? = 0.291. These results indicate a low reliability of the annotations. In
addition, they emphasize the demand for a precisely defined argument annotation strategy. In subsequent
discussions, we discovered that the primary source of uncertainty is due to the missing context. Since
the text snippets are provided without any information about the topic, the annotators found it difficult
to decide if a snippet includes an argument or not. In addition, the annotators report that the author?s
stance might facilitate the separation of argumentative from non-argumentative text and to determine the
components of arguments.
According to these findings, we define a new top-down annotation process starting with the major
claim and drill-down to the claims and premises. Therefore, the annotators are aware of the author?s
stance after identifying the major claim. In addition, we ask the annotators to read the entire essay in
order to identify the topic before starting with the actual annotation task. Although, this approach is
more time-consuming than a direct identification of argument components, we show in our annotation
study (section 5) that it yields reliably annotated data. In particular, the annotation guidelines consist of
the following steps:
1. Topic and stance identification: Before starting with the annotation process, annotators identify the
topic and the author?s stance by reading the entire essay.
2. Annotation of argument components: In this step, the major claim is identified either in the intro-
duction or in the conclusion of an essay. Subsequently, annotators identify the claims and premises
in each paragraph. We instruct the annotators to annotate each argument component as a state-
ment covering an entire sentence or less. We consolidate the annotations of all annotators before
continuing with the next step (section 5.4).
3. Annotation of argumentative relations: Finally, the claims and premises are linked within each
paragraph, and the claims are linked to the major claim either with a support or attack relation.
6
Although the coefficient was introduced by Fleiss as a generalization of Cohen?s ? (Cohen, 1960), it is actually a gener-
alization of Scott?s ? (Scott, 1955), since it assumes a cumulative distribution of annotations by all annotators (Artstein and
Poesio, 2008). We follow the naming proposed by Artstein and Poesio and refer to the measure as multi-?.
1505
5 Annotation Study
Three annotators participate in the study and annotate the essays independently using our described an-
notation scheme. We conduct several training sessions after each annotator has read the annotation guide-
lines. In these sessions, annotators collaboratively annotate 8 example essays for resolving disagreements
and obtaining a common understanding of the annotation guidelines. For the actual annotation task, we
used the brat annotation tool that is freely available.
7
It allows the annotation of text units with arbitrary
boundaries as well as the linking of annotations for modeling argumentative discourse structures.
5.1 Data
Our corpus consists of 90 persuasive essays in English, which we selected from essayforum
8
. This
forum is an active community that provides writing feedback for different kinds of texts. For instance,
students post their essays for retrieving feedback about their writing skills while preparing themselves
for standardized tests. We randomly selected the essays from the writing feedback section of the forum
and manually reviewed each essay. Due to the non-argumentative writing style and significant language
flaws, we replaced 4 of them during a manual revision of the corpus. The final corpus includes 1,673
sentences with 34,917 tokens. On average, each essay has 19 sentences and 388 tokens.
5.2 Inter-rater Agreement
We evaluate the reliability of the argument component annotations using two strategies. Since there
are no predefined markables in our study, annotators have to identify the boundaries of argument com-
ponents. We evaluate the annotations using Krippendorff?s ?
U
(Krippendorff, 2004). It considers the
differences in the markable boundaries of several annotators and thus allows for assessing the reliability
of our annotated corpus. In addition, we evaluate if a sentence contains an argument component of a par-
ticular category using percentage agreement and two chance-corrected measures: multi-? (Fleiss, 1971)
and Krippendorff?s ? (Krippendorff, 1980). Since only 5.6% of the sentences contain several annota-
tions of different argument components, evaluating the reliability at the sentence-level provides a good
approximation of the inter-rater agreement. In addition, it enables comparability with future argument
annotation studies that are conducted at the sentence-level. The annotations yield the following class
distribution at the token-level: 3.5% major claim, 18.2% claim, 48.1% premise and 30.2% are not an-
notated. At the sentence-level 5.4% contain a major claim, 26.4% a claim, 61.1% a premise and 19.3%
none annotation. Thus, 12.2% of the sentences contain several annotations.
% ? ? ?
U
MajorClaim .9827 .8334 .8365 .7726
Claim .8690 .6590 .6655 .6033
Premise .8618 .7075 .7131 .7594
Table 1: Inter-rater agreement of argument component annotations
We obtain the highest inter-rater agreement for the annotations of the major claim (table 1). The inter-
rater agreement of 98% and multi-? = 0.833 indicates that the major claim can be reliably annotated in
persuasive essays. In addition, there are few differences regarding the boundaries of major claims (?
U
=
0.773). Thus, annotators identify the sentence containing the major claim as well as the boundaries
reliably. We obtain an inter-rater agreement of multi-? = 0.708 for premise annotations and multi-? =
0.66 for claims. This is only slightly below the ?tentative conclusion boundary? proposed by Carletta
(1996) and Krippendorff (1980). The unitized ? of the major claim and the claim are lower than the
sentence-level agreements (table 1). Only the unitized ? of the premise annotations is higher compared
to the sentence-level agreement. Thus, the boundaries of premises are more precisely identified. The joint
unitized measure for all categories is ?
U
= 0.724. Hence, we tentatively conclude that the annotation of
argument components in persuasive essays is reliably possible.
7
http://brat.nlplab.org
8
http://www.essayforum.com
1506
The agreement of the stance attribute is computed for each sentence. We follow the same methodology
as for the computation of the argument component agreement, but treat each sentence containing a claim
as either for or against according to the stance attribute (sentences not containing a claim are treated as
not annotated, but are included in the markables). Thus, the upper boundary for the stance agreement
constitutes the agreement of the claim annotations. The agreement of the stance attribute is only slightly
below the agreement of the claim (86%; multi-? = 0.643; ? = 0.65). Hence, the identification of either
attacking or rebutting claims is feasible with high agreement.
We determine the markables for evaluating the reliability of argumentative relations as the set of all
pairs between argument components according to our annotation scheme. So, the markables correspond
to all relations that were possible during the annotation task. In total, the markables include 5,137 pairs
of which 25.5% are annotated as support relation and 3.1% as attack relations. We obtain an inter-rater
agreement above 0.8 for both support and attack relations (table 2) that is considered by Krippendorff
(1980) as good reliability. Therefore, we conclude that argumentative relations can be reliably annotated
in persuasive essays.
% ? ?
support .9267 .8105 .8120
attack .9883 .8052 .8066
Table 2: Inter-rater agreement of argumentative relation annotations
5.3 Error Analysis
To study the disagreements encountered during the annotation study, we created confusion probability
matrices (CPM) (Cinkova? et al., 2012) for argument components and argumentative relations. A CPM
contains the conditional probabilities that an annotator assigns a certain category (column) given that an-
other annotator has chosen the category in the row for a specific item. In contrast to traditional confusion
matrices, a CPM also enables the evaluation of confusions if more than two annotators are involved in
an annotation study.
Major Claim Claim Premise None
Major Claim .675 .132 .148 .045
Claim .025 .552 .338 .086
Premise .014 .163 .754 .069
None .012 .123 .204 .660
Table 3: Confusion probability matrix for argument component annotations (Category ?None? indicates
argument components that are not identified by an annotator.)
The major disagreement is between claims and premises (table 3). This could be expected since a
claim can also serve as premise for another claim, and it is difficult to distinguish these two concepts in
the presence of reasoning chains. For instance, examples (7?9) constitute a reasoning chain in which (7)
is supported by (8) and (8) is supported by (9):
(7) ?Random locker checks should be made obligatory.?
(8) ?Locker checks help students stay both physically and mentally healthy.?
(9) ?It discourages students from bringing firearms and especially drugs.?
Considering this structure, (7) can be classified as claim. However, if (7) is omitted, (8) becomes a
claim that is supported by (9). Thus, the distinction between claims and premises depends not only on the
context and the intention of the author but also on the structure of a specific argument. Interestingly, the
distinction between major claims and claims is less critical. Apparently, the identification of the major
claim is easier since it is directly related to the author?s stance in contrast to more general claims that
cover a certain aspect with respect to the overall topic of the essay.
The CPM for relations (table 4) reveals that the highest confusion is between support/attack relations
and none classified relations. This could be due to the fact that it is difficult to identify the correct target of
a relation, especially in the presence of multiple claims or reasoning chains in a paragraph. For instance,
1507
support attack none
support .750 .013 .238
attack .104 .691 .205
none .092 .001 .898
Table 4: Confusion probability matrix for argumentative relation annotations
in the previous example an annotator could also link (9) directly to (7) or even to (7) and (8). In both
cases, the argument would be still meaningful. The distinction between support and attack relations does
not reveal high disagreements. To sum up, the error analysis reveals that the annotation of argumentative
relations yields more reliable results than that of argument components. This could be due to the fact
that in our studies, argument components are known before annotating the relations and thus the task is
easier. Nevertheless, it could be interesting to annotate relations before classifying the types of argument
components and to investigate if it positively influences the reliability of annotations.
5.4 Creation of the Final Corpus
The creation of the final corpus consists of two independent tasks. First, we consolidate the argument
components before the annotation of argumentative relations. So each annotator works on the same
argumentative components when annotating the relations. Second, we consolidate the argumentative
relations to obtain the final corpus. We follow a majority voting in both steps. Thus, an annotation is
adopted in the final corpus if at least two annotators agree on the category as well as on the boundaries. In
applying this strategy, we observed seven cases for argument components and ten cases for argumentative
relations that could not be solved by majority voting. Those cases were discussed in the group of all
annotators to reach an agreement. Table 5 shows an overview of the final corpus. It includes 90 major
ALL avg. per essay standard deviation
Sentence 1,673 19 7
Tokens 34,917 388 124
MajorClaim 90 1 0
Claim 429 5 2
Claim (for) 365 4 2
Claim (against) 64 1 1
Premises 1,033 11 6
support 1,312 15 7
attack 161 2 2
Table 5: Statistics of the final corpus
claims (each essay contains exactly one), 429 claims and 1,033 premises. This proportion between
claims and premises is common in argumentation and confirms the findings of Mochales-Palau and
Moens (2011, p. 10) that claims are usually supported by several premises for ?ensuring a complete and
stable standpoint?.
6 Conclusion & Future Work
We presented an annotation study of argument components and argumentative relations in persuasive
essays. Previous argument annotation studies suffer from several limitations: Either they do not follow a
systematic methodology and do not provide detailed inter-rater agreement studies or they do not include
annotations of argumentative relations. Our annotation study is the first step towards computational
argument analysis in educational applications that provides both annotations of argumentative relations
and a comprehensive evaluation of the inter-rater agreement. The results of our study indicate that the
annotation guidelines yield substantial agreement. The resulting corpus and the annotation guidelines
are freely available to encourage future research in argument recognition.
In future work, we plan to utilize the created corpus as training data for supervised machine learning
methods in order to automatically identify argument components as well as argumentative relations. In
addition, there is a demand to scale the proposed annotation scheme to other genres e.g. scientific articles
or newspapers and to create larger corpora.
1508
Acknowledgements
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806. We thank Piyush Paliwal and Krish Perumal for their valuable contri-
butions and we thank the anonymous reviewers for their helpful comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Yigal Attali, Will Lewis, and Michael Steier. 2013. Scoring with the computer: Alternative procedures for
improving the reliability of holistic essay scoring. Language Testing, 30(1):125?141.
Beata Beigman Klebanov and Michael Flor. 2013. Argumentation-Relevant Metaphors in Test-Taker Essays. In
Proceedings of the First Workshop on Metaphor in NLP, pages 11?20, Atlanta, GA, USA.
Beata Beigman Klebanov and Derrick Higgins. 2012. Measuring the use of factual information in test-taker
essays. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 63?
72, Montreal, Quebec, Canada.
Jamal Bentahar, Bernard Moulin, and Micheline Be?langer. 2010. A taxonomy of argumentation models used for
knowledge representation. Artificial Intelligence Review, 33(3):211?259.
Jill Burstein and Magdalena Wolska. 2003. Toward evaluation of writing style: finding overly repetitive word use
in student essays. In Proceedings of the tenth conference of European chapter of the Association for Computa-
tional Linguistics, EACL ?03, pages 35?42, Budapest, Hungary.
Jill Burstein, Karen Kukich, Susanne Wolff, Ji Lu, and Martin Chodorow. 1998. Enriching Automated Essay
Scoring Using Discourse Marking. In Proceedings of the Workshop on Discourse Relations and Discourse
Markers, pages 15?21, Montreal, Quebec, Canada.
Jodie A. Butler and M. Anne Britt. 2011. Investigating Instruction for Improving Revision of Argumentative
Essays. Written Communication, 28(1):70?96.
Elena Cabrio and Serena Villata. 2012. Natural language arguments: A combined approach. In Proceedings of
the 20th European Conference on Artificial Intelligence, ECAI ?12, pages 205?210, Montpellier, France.
Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
Silvie Cinkova?, Martin Holub, and Vincent Kr??z?. 2012. Managing uncertainty in semantic tagging. In Proceedings
of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ?12,
pages 840?850, Avignon, France.
Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT
?11, pages 987?996, Portland, OR, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin,
76(5):378?382.
James B. Freemen. 2011. Argument Structure: Representation and Theory, volume 18 of Argumentation Library.
Springer.
Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Sage.
Klaus Krippendorff. 2004. Measuring the Reliability of Qualitative Text Analysis Data. Quality & Quantity,
38(6):787?800.
Nitin Madnani, Michael Heilman, Joel Tetrault, and Martin Chodorow. 2012. Identifying High-Level Organiza-
tional Elements in Argumentative Discourse. In Proceedings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ?12,
pages 20?28, Montreal, Quebec, Canada.
1509
Raquel Mochales-Palau and Marie-Francine Moens. 2008. Study on the Structure of Argumentation in Case
Law. In JURIX the twenty-first annual conference on legal knowledge and information systems, pages 11?20,
Florence, Italy.
Raquel Mochales-Palau and Marie-Francine Moens. 2011. Argumentation mining. Artificial Intelligence and
Law, 19(1):1?22.
Andreas Peldszus and Manfred Stede. 2013. From Argument Diagrams to Argumentation Mining in Texts: A
Survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1?31.
Chris Reed, Raquel Mochales-Palau, Glenn Rowe, and Marie-Francine Moens. 2008. Language resources for
studying argument. In Proceedings of the Sixth International Conference on Language Resources and Evalua-
tion, LREC ?08, pages 2613?2618, Marrakech, Morocco.
Niall Rooney, Hui Wang, and Fiona Browne. 2012. Applying kernel methods to argumentation mining. In Pro-
ceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference, FLAIRS
?12, pages 272?275, Marco Island, FL, USA.
Victor D. Sampson and Douglas B. Clark. 2006. Assessment of argument in science education: A critical review
of the literature. In Proceedings of the 7th International Conference on Learning Sciences, ICLS ?06, pages
655?661, Bloomington, IN, USA.
William A. Scott. 1955. Reliability of Content Analysis: The Case of Nominal Scale Coding. Public Opinion
Quarterly, 19(3):321?325.
Alan Sergeant. 2013. Automatic argumentation extraction. In Proceedings of the 10th European Semantic Web
Conference, ESWC ?13, pages 656?660, Montpellier, France.
Mark D. Shermis and Jill Burstein. 2013. Handbook of Automated Essay Evaluation: Current Applications and
New Directions. Routledge Chapman & Hall.
Stephen E. Toulmin. 1958. The uses of Argument. Cambridge University Press.
Maria Paz Garcia Villalba and Patrick Saint-Dizier. 2012. Some facets of argumentmining for opinion analysis. In
Proceeding of the 2012 conference on ComputationalModels of Argument, COMMA ?12, pages 23?34, Vienna,
Austria.
DouglasWalton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation Schemes. Cambridge University Press.
1510
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 105?109, Dublin, Ireland, August 23-29 2014.
DKPro Agreement: An Open-Source Java Library for
Measuring Inter-Rater Agreement
Christian M. Meyer,
?
Margot Mieskes,
??
Christian Stab,
?
and Iryna Gurevych
??
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Computer Science Department, Technische Universit?at Darmstadt
?
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
?
Information Center for Education
German Institute for Educational Research (DIPF)
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we introduce a novel Java implementation of multiple inter-rater agreement mea-
sures, which we make available as open-source software. Besides assessing the reliability of
coding tasks using S, pi, ?, ?, etc., we particularly support unitizing tasks by measuring ?
U
as
the agreement of the boundaries of the identified annotation units. We provide a unified interface
and data model for both tasks as well as multiple diagnostic devices for analyzing the results.
1 Introduction
Reliability is a necessary precondition for obtaining high-quality datasets and thus for drawing valid con-
clusions from an annotation study. Assessing the reliability by means of inter-rater agreement measures
has been an established scientific practice in psychology (e.g., Cohen, 1960), medicine (Cicchetti et al.,
1978), and content analysis (Krippendorff, 1980) for decades. In the computational linguistics and natu-
ral language processing community, reliability discussions have long been limited or completely ignored.
It was not until Carletta?s (1996) appeal that researchers started measuring the inter-rater agreement at a
larger scale. However, there are still numerous papers published every year that lack a proper discussion
of data quality. The reliability of the utilized datasets is, for example, not discussed at all in six out of the
thirteen task description papers of SemEval-2013, and it remains shallow in another four papers, which
do not provide a suitable inter-rater agreement figure.
1
A major reason for this is the limited availability
of software components, which support the standard measures and are well-integrated with existing sys-
tems. In fact, many researchers currently rely on manual calculations, hasty implementations of single
coefficients, or free online calculators that often lack documentation of the implementation details.
In this work, we present the novel Java-based software library DKPro Agreement for computing mul-
tiple inter-rater agreement measures using a shared interface and data model. For the first time, we
provide a unified model for analyzing coding (i.e., assigning categories to fixed items) and unitizing
studies (i.e., segmenting the data into codable units). By supporting these two fundamental annotation
setups, our software can be used for analyzing many different annotation tasks including syntactic (e.g.,
part-of-speech tagging), semantic (e.g., word sense assignment, keyphrase identification), and discourse
annotation tasks (e.g., dialogue act tagging). We particularly provide diagnostic devices for analyzing
systematic disagreement, which is often overlooked in reliability discussions. DKPro Agreement is avail-
able as open-source software, thoroughly tested on a wide range of examples, and well-documented for
getting started quickly.
2
Our implementation is targeted at scientists and software developers working
in Java, including the large communities around the major Java-based toolkits OpenNLP, DKPro, and
GATE.
3
The software integrates more easily with existing Java applications than statistics software such
as Octave, R, or SPSS, and its usage requires a less pronounced statistics background.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
All SemEval task descriptions (http://www.cs.york.ac.uk/semeval-2013) have been discussed among the authors.
2
DKPro Agreement is part of the DKPro Statistics library. The project is available from https://code.google.com/p/
dkpro-statistics/ and licensed under the Apache License 2.0.
3http://opennlp.apache.org, http://code.google.com/p/dkpro-core-asl, http://gate.ac.uk
105
2 Related Work
Reliability is the subject of an extensive body of literature. Artstein and Poesio (2008) and Krippen-
dorff (1980) give a general introduction to this topic. Implementations of inter-rater agreement mea-
sures exist both as stand-alone and as online tools (e.g., http://uni-leipzig.de/?jenderek/tool/
tool.htm, http://terpconnect.umd.edu/?dchoy/thesis/Kappa). Most of them focus on one
type of agreement measure (e.g., http://vassarstats.net/kappa.html and Randolph (2005) for ? as
well as http://ron.artstein.org/software.html for ?). Others are limited to a few different mea-
sures (e.g., https://mlnl.net/jg/software/ira and http://dfreelon.org/utils/recalfront/recal3).
Frameworks offering a wider range of measures are either commercial with a high price tag (e.g.,
http://www.medcalc.org/manual/kappa.php) or limited to specific platforms (e.g., http://www.
agreestat.com/agreestat.html). The situation is even worse for unitizing studies, for which we are
only aware of Perry and Krippendorff?s (2013) implementation available at http://www.gabriela.
trindade.nom.br/2013/02/calculating-alpha-d-and-alpha-u/. To the best of our knowledge,
there is no software library providing a large variety of agreement measures and diagnostic devices,
which covers both coding and unitizing studies and which integrates well with existing systems.
3 Implementation
The standard workflow for using DKPro Agreement is (1) representing the annotated data using our
unified data model, (2) measuring the agreement among the individual raters, and (3) analyzing the
results using diagnostic devices and visualizations.
Data model. We provide the Java interface IAnnotationStudy as the basic representation of the
annotated data. An annotation study consists of a set of raters, a set of codable units, and a set of
categories, which may be used by the raters to code a unit. The categories do not have a specific type,
such that any Java object (including integers and enums) may be used without any extensions.
Following Krippendorff?s (1980) terminology, we distinguish two basic annotation setups: In cod-
ing studies, the raters receive a set of annotation items with fixed boundaries (e.g., full articles from a
newspaper), which each of them should code (?annotate?) with one of the categories (e.g., politics, eco-
nomics). We consider each rater?s annotation of an item a single annotation unit. In unitizing studies,
the raters are asked to identify the annotation units themselves by marking their boundaries (e.g., high-
lighting key phrases). Depending on the task definition, the identified units may be coded with one of
multiple categories or just distinguish identified segments from so-called gaps between these segments.
Figure 1 illustrates this data model for both setups. The framed boxes show the annotation units and
the categories assigned to them. In coding studies, the units with identical index (i.e., the columns) yield
the annotation items. In unitizing studies, the units may be positioned arbitrarily within the continuum.
We represent missing annotations as empty boxes (coding studies) and as horizontal lines between the
identified units (unitizing studies).
Software developers can either instantiate our default implementation (e.g., by reading data from flat
files or databases) or implement the provided Java interfaces in order to reuse their own data model.
For coding studies, there is an addItem method with a varargs parameter (i.e., a method taking an
arbitrary number of parameters) for specifying the annotation of each rater for a certain item. The line
study.addItem("B", "C", null, "B") indicates that four raters coded an item with the categories B,
C, null, and B. We use null to represent missing annotations. Similarly, unitizing studies provide an
addUnit method, which takes the boundaries and the category assigned to the unit by a certain rater.
The line study.addUnit(10, 4, 2, "A") indicates, for instance, a unit of length 4, which starts at position
10 and which has been annotated as category A by rater 2.
Coding measures. Table 1 shows an overview of the inter-rater agreement measures currently avail-
able in DKPro Agreement. Artstein and Poesio (2008) give an overview of these measures. While the
percentage agreement simply divides the number of agreements by the item count, all other measures
perform a chance correction. A major difference is the assumed probability distribution for the expected
agreement, which is considered different for each study and rater (i.e., rater-specific), the same for all
106
Measure Type Raters Chance correction Weighted
Percentage agreement coding ? 2 ? ?
Bennett et al.?s S (1954) coding 2 uniform ?
Scott?s pi (1955) coding 2 study-specific ?
Cohen?s ? (1960) coding 2 rater-specific ?
Randolph?s ? (2005) [multi-S] coding ? 2 uniform ?
Fleiss?s ? (1971) [multi-pi] coding ? 2 study-specific ?
Hubert?s ? (1977) [multi-?] coding ? 2 rater-specific ?
Krippendorff?s ? (1980) coding ? 2 study-specific X
Cohen?s weighted ?
w
(1968) coding ? 2 rater-specific X
Krippendorff?s ?
u
(1995) unitizing ? 2 study-specific ?
Table 1: Implemented inter-rater agreement measures
Coding studies
items: 1 2 3 4 5 6
...
rater 1 A A B A A B
rater 2 A B A C
.
.
.
Unitizing studies
continuum:
...
rater 1
{
A A A
B B
rater 2
{
A A A
B B
.
.
.
Figure 1: Data model
raters (study-specific), or the same for all studies and raters (uniform). As all of these measures are used
in the literature, we need implementations for each of them to be able to compare different results. This
is particularly important for ? measures, because many different definitions exist. Cohen?s ? (1960) is,
for instance, often compared to Fleiss?s ? (1971), although both measures assume a different probabil-
ity distribution. Since all measures implement a standardized interface, different results can be easily
compared using our software. The line new PercentageAgreement(study).calculateAgreement()
returns, for instance, the percentage agreement of the given study, while the line new FleissKappa-
Agreement(study).calculateAgreement() returns Fleiss?s ? (1971) for the very same study.
Most early measures consider an agreement if, and only if, the categories assigned to an item are
identical. Weighted measures allow for defining a distance function that expresses the similarity of two
categories. We provide distance functions for nominal, ordinal, interval, and ratio scales (Krippendorff,
1980), as well as the MASI distance function for set-valued data (Passonneau, 2006). Set annotations
are an example for a more complex type of category, as they facilitate assigning multiple categories to a
given unit. Additionally, researchers can easily define new study-specific distance functions.
Unitizing measures. Although unitizing has long been identified as a major issue of reliability analy-
sis (cf. Auld and Whitea, 1956), there are so far only few formalizations. The most elaborate measure
is Krippendorff?s ?
U
(1995), which is based on a distance function for comparing units with identical,
overlapping, or disjoint boundaries. As a model for expected disagreement, ?
U
considers all possible
unitizations for the given continuum and raters. Thereby, ?
U
becomes fully compatible with Krippen-
dorff?s ? (1980) for coding tasks. Due to the lack of implementations and the complex statistics, ?
U
is
almost never reported in the literature. While our implementation follows the original definition, it does
not require to explicitly encode the gaps, because we generate them automatically. This simplifies the
usage of this measure substantially, since, for example, UIMA
4
annotations can be directly used to rep-
resent the annotation units. While the original definition focuses on only one category, we additionally
support Krippendorff?s later definition of an aggregating ?
U
over all categories (Krippendorff, 2004).
Analysis. The raw inter-rater agreement scores are useful for comparing multiple annotation studies
with each other, but they are of limited help for diagnosing disagreement and potential systematic issues
with certain categories, items, or raters. This is why we provide interfaces for measuring the agreement of
a specific category, item, or rater. Fleiss (1971) defines, for instance, a category-specific ?
c
that we realize
in our software. The same holds for the unitizing measure ?
U
, which also provides a category-specific
agreement score. Besides measuring a rater-specific agreement score, DKPro Agreement facilitates the
computation of pairwise inter-rater agreement in order to identify the pair of raters with the highest and
lowest agreement. Finally, each measure can return some of its intermediate results, for example, the
expected agreement P
e
of Scott?s pi (1955).
Another means of analysis is to display the annotation units and the disagreement among the raters.
Coding studies can be displayed as a coincidence table or as a reliability matrix (Krippendorff, 1980).
For studies with two raters, our software also allows printing a contingency table. In addition to that,
4
Unstructured Information Management Architecture, http://uima.apache.org
107
we provide a formatter for the weighing matrix of a distance function and a basic visualization of the
continuum of annotation units in unitizing studies ? similar to the representation in Figure 1.
Code example. Consider the coding study described by Krippendorff (1980, p. 139) consisting of nine
annotation items that have been categorized as category 1, 2, 3, or 4 by three raters. We can (re-)analyze
this study with DKPro Agreement using the following Java code:
CodingAnnotationStudy study = new CodingAnnotationStudy(3); ?
study.addItem(1, 1, 1); study.addItem(1, 2, 2); study.addItem(2, 2, 2); ?
study.addItem(4, 4, 4); study.addItem(1, 4, 4); study.addItem(2, 2, 2);
study.addItem(1, 2, 3); study.addItem(3, 3, 3); study.addItem(2, 2, 2);
PercentageAgreement pa = new PercentageAgreement(study); ?
System.out.println(pa.calculateAgreement());
KrippendorffAlphaAgreement alpha = new KrippendorffAlphaAgreement(
study, new NominalDistanceFunction());
System.out.println(alpha.calculateObservedDisagreement());
System.out.println(alpha.calculateExpectedDisagreement());
System.out.println(alpha.calculateAgreement());
System.out.println(alpha.calculateCategoryAgreement(1)); ?
System.out.println(alpha.calculateCategoryAgreement(2));
new CoincidenceMatrixPrinter().print(System.out, study); ?
The first step is ? the instanciation of an IAnnotationStudy for the three human raters. Then, ? we
add all 3 ? 9 = 27 annotation units to the study by providing the category chosen by each rater to code
the nine annotation items. Note that most reliability analyses would read the raters?s decisions from a
file, database, or other data structure rather than typing them in manually like in this example. Once the
data model is complete, ? we calculate the inter-rater agreement. Following the original publication, we
calculate the raw agreement and Krippendorff?s ? (1980) and thus print 0.740 (percentage agreement),
0.259 (observed disagreement D
O
), 0.724 (expected disagreement D
E
), and 0.642 (? coefficient). Fi-
nally, ? we analyze the agreement by calculating the category-specific ? for the categories 1 and 2
yielding a system output of 0.381 and 0.711, and ? we print a coincidence matrix on the system console.
4 Evaluation and Publication
Automatic tests. Even though the agreement measures are clearly defined in the literature, their im-
plementation is error-prone due to the varying notations and the required changes to take efficiency and
numerical stability into account. A single confused index (e.g., p
ij
instead of p
ji
) could easily yield
invalid conclusions for many studies. This is why we provide 61 unit tests to evaluate the correctness of
our implementation. Besides manually devised examples, we use 46 examples from the literature (e.g.,
from Krippendorff, 1980). All examples contain references to their original documentation.
Numerical stability. We especially test the analysis of larger annotation studies, which raise issues of
numerical stability. When scaling the example by Artstein and Poesio (2008, p. 558) with factor 500, an
implementation potentially returns a Cohen?s ? of 0.82, although it is only 0.35. This phenomenon is due
to arithmetic overflows, instable division operations, and rounding errors. Krippendorff?s ?
U
depends,
for instance, on the cubic factor 2l
3
hj
, which can raise such issues even for rather small studies. Where
necessary, we use logarithms or Java?s BigDecimal type to ensure accurate results.
Open-source software. We publish our implementation as open-source software under the Apache
License. By providing access to our source code, researchers can learn how the measures work and how
the software is used. Moreover, they can easily contribute in order to extend the library and evaluate its
correctness in a peer review, which is an essential step to establish the credibility of the software. Finally,
our choice of a free license should ease (re-)using the software in many projects and thus facilitate the
reproduction and comparison of annotation results, which often falls short in our community.
Documentation. DKPro Agreement is fully documented using Javadoc comments. In addition to that,
we provide a general introduction and a getting-started tutorial on the project page, which also points the
users to our numerous usage examples in the form of test cases.
108
5 Conclusion and Future Work
We have presented an open-source Java software for measuring inter-rater agreement of coding and
unitizing studies. In future work, we plan to provide additional diagnostic devices and elaborated visual-
izations (such as Hinton diagrams) and to integrate our measures with existing annotation workbenches.
An early software version has, for example, already been used in the CSniper (Eckart de Castilho et al.,
2012) and WebAnno (Yimam et al., 2013) systems. We plan to extent this to other systems and also make
use of the newly introduced unitizing setup.
Acknowledgments
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806.
References
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational
Linguistics, 34(4):555?596.
Frank Auld, Jr and Alice M. Whitea. 1956. Rules for Dividing Interviews Into Sentences. The Journal of
Psychology: Interdisciplinary and Applied, 42(2):273?281.
Edward M. Bennett, R. Alpert, and A. C. Goldstein. 1954. Communications Through Limited Response Ques-
tioning. Public Opinion Quarterly, 18(3):303?308.
Jean Carletta. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguis-
tics, 22(2):249?254.
Domenic V. Cicchetti, Chinyu Lee, Alan F. Fontana, and Barbara Noel Dowds. 1978. A Computer Program for
Assessing Specific Category Rater Agreement for Qualitative Data. Educational and Psychological Measure-
ment, 38(3):805?813.
Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70(4):213?220.
Richard Eckart de Castilho, Sabine Bartsch, and Iryna Gurevych. 2012. CSniper ? Annotation-by-query for
non-canonical constructions in large corpora. In Proceedings of the 50th Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages 85?90, Jeju Island, Korea.
Joseph L. Fleiss. 1971. Measuring Nominal Scale Agreement among many Raters. Psychological Bulletin,
76(5):378?382.
Lawrence Hubert. 1977. Kappa revisited. Psychological Bulletin, 84(2):289?297.
Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Beverly Hills, CA: Sage
Publications.
Klaus Krippendorff. 1995. On the reliability of unitizing contiguous data. Sociological Methodology, 25:47?76.
Published for American Sociological Association.
Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage
Publications, 2nd edition.
Rebecca J. Passonneau. 2006. Measuring agreement on set-valued items (MASI) for semantic and pragmatic
annotation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages
831?836, Genoa, Italy.
Gabriela Trindade Perry and Klaus Krippendorff. 2013. On the reliability of identifying design moves in protocol
analysis. Design Studies, 34(5):612?635.
Justus J. Randolph. 2005. Free-marginal multirater kappa (multirater ?
free
): An alternative to Fleiss? fixed-
marginal multirater kappa. In Proceedings of the 5th Joensuu University Learning and Instruction Symposium,
Joensuu, Finland.
William A. Scott. 1955. Reliability of Content Analysis: The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de Castilho, and Chris Biemann. 2013. WebAnno: A
Flexible, Web-based and Visually Supported System for Distributed Annotations. In Proceedings of the 51st
Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6, Sofia,
Bulgaria.
109
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46?56,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Identifying Argumentative Discourse Structures in Persuasive Essays
Christian Stab
?
and Iryna Gurevych
??
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit?at Darmstadt
?
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
www.ukp.tu-darmstadt.de
Abstract
In this paper, we present a novel ap-
proach for identifying argumentative dis-
course structures in persuasive essays. The
structure of argumentation consists of sev-
eral components (i.e. claims and premises)
that are connected with argumentative re-
lations. We consider this task in two
consecutive steps. First, we identify the
components of arguments using multiclass
classification. Second, we classify a pair
of argument components as either support
or non-support for identifying the struc-
ture of argumentative discourse. For both
tasks, we evaluate several classifiers and
propose novel feature sets including struc-
tural, lexical, syntactic and contextual fea-
tures. In our experiments, we obtain a
macro F1-score of 0.726 for identifying
argument components and 0.722 for argu-
mentative relations.
1 Introduction
Argumentation is a crucial aspect of writing skills
acquisition. The ability of formulating persuasive
arguments is not only the foundation for convinc-
ing an audience of novel ideas but also plays a ma-
jor role in general decision making and analyzing
different stances. However, current writing sup-
port is limited to feedback about spelling, gram-
mar, or stylistic properties and there is currently no
system that provides feedback about written argu-
mentation. By integrating argumentation mining
in writing environments, students will be able to
inspect their texts for plausibility and to improve
the quality of their argumentation.
An argument consists of several components. It
includes a claim that is supported or attacked by at
least one premise. The claim is the central compo-
nent of an argument. It is a controversial statement
that should not be accepted by the reader without
additional support.
1
The premise underpins the
validity of the claim. It is a reason given by an
author for persuading readers of the claim. Argu-
mentative relations model the discourse structure
of arguments. They indicate which argument com-
ponents are related and constitute the structure of
argumentative discourse. For example, the argu-
ment in the following paragraph contains four ar-
gument components: one claim (in bold face) and
three premises (underlined).
?(1) Museums and art galleries provide
a better understanding about arts than
Internet. (2) In most museums and art
galleries, detailed descriptions in terms
of the background, history and author
are provided. (3) Seeing an artwork on-
line is not the same as watching it with
our own eyes, as (4) the picture online
does not show the texture or three-di-
mensional structure of the art, which is
important to study.?
In this example, the premises (2) and (3) sup-
port the claim (1) whereas premise (4) is a support
for premise (3). Thus, this example includes three
argumentative support relations holding between
the components (2,1), (3,1) and (4,3) signaling that
the source component is a justification of the target
component. This illustrates two important proper-
ties of argumentative discourse structures. First,
argumentative relations are often implicit (not in-
dicated by discourse markers; e.g. the relation
holding between (2) and (1)). Indeed, Marcu and
Echihabi (2002) found that only 26% of the ev-
idence relations in the RST Discourse Treebank
(Carlson et al., 2001) include discourse markers.
1
We use the term claim synonymously to conclusion.
In our definition the differentiation between claims and
premises does not indicate the validity of the statements but
signals which components include the gist of an argument
and which are given by the author as justification.
46
Second, in contrast to Rhetorical Structure Theory
(RST) (Mann and Thompson, 1987), argumenta-
tive relations also hold between non-adjacent sen-
tences/clauses. For instance, in the corpus com-
piled by Stab and Gurevych (2014) only 37% of
the premises appear adjacent to a claim. There-
fore, existing approaches of discourse analysis,
e.g. based on RST, do not meet the require-
ments of argumentative discourse structure iden-
tification, since they only consider discourse re-
lations between adjacent sentences/clauses (Peld-
szus and Stede, 2013). In addition, there are no
distinct argumentative relations included in com-
mon approaches like RST or the Penn Discourse
Treebank (PDTB) (Prasad et al., 2008), since they
are focused on identifying general discourse struc-
tures (cp. section 2.2).
Most of the existing argumentation mining
methods focus solely on the identification of ar-
gument components. However, identifying argu-
mentative discourse structures is an important task
(Sergeant, 2013) in particular for providing feed-
back about argumentation. First, argumentative
discourse structures are essential for evaluating the
quality of an argument, since it is not possible
to examine how well a claim is justified without
knowing which premises belong to it. Second,
methods that recognize if a statement supports a
given claim enable the collection of additional ev-
idence from other sources. Third, the structure of
argumentation is needed for recommending better
arrangements of argument components and mean-
ingful usage of discourse markers. Both foster ar-
gument comprehension and recall (Britt and Lar-
son, 2003) and thus increase the argumentation
quality. To the best of our knowledge, there is
currently only one approach that aims at identi-
fying argumentative discourse structures proposed
by Mochales-Palau and Moens (2009). However,
it relies on a manually created context-free gram-
mar (CFG) and is tailored to the legal domain,
which follows a standardized argumentation style.
Therefore, it is likely that it will not achieve ac-
ceptable accuracy when applied to more general
texts in which discourse markers are missing or
even misleadingly used (e.g. student texts).
In this work, we present a novel approach
for identifying argumentative discourse structures
which includes two consecutive steps. In the first
step, we focus on the identification of argument
components using a multiclass classification ap-
proach. In the second step, we identify argumen-
tative relations by classifying a pair of argument
components as either support or non-support. In
particular, the contributions of this work are the
following: First, we introduce a novel approach
for identifying argumentative discourse structures.
Contrary to previous approaches, our approach
is capable of identifying argumentative discourse
structures even if discourse markers are missing or
misleadingly used. Second, we present two novel
feature sets for identifying argument components
as well as argumentative relations. Third, we eval-
uate several classifiers and feature groups for iden-
tifying the best system for both tasks.
2 Related Work
2.1 Argumentation Mining
Previous research on argumentation mining spans
several subtasks, including (1) the separation of
argumentative from non-argumentative text units
(Moens et al., 2007; Florou et al., 2013), (2)
the classification of argument components or
argumentation schemes (Rooney et al., 2012;
Mochales-Palau and Moens, 2009; Teufel, 1999;
Feng and Hirst, 2011), and (3) the identification
of argumentation structures (Mochales-Palau and
Moens, 2009; Wyner et al., 2010).
The separation of argumentative from non-
argumentative text units is usually considered as
a binary classification task and constitutes one of
the first steps in an argumentation mining pipeline.
Moens et al. (2007) propose an approach for iden-
tifying argumentative sentences in the Araucaria
corpus (Reed et al., 2008). The argument an-
notations in Araucaria are based on a domain-
independent argumentation theory proposed by
Walton (1996). In their experiments, they ob-
tain the best accuracy (73.75%) using a combi-
nation of word pairs, text statistics, verbs, and a
list of keywords indicative for argumentative dis-
course. Florou et al. (2013) report a similar ap-
proach. They classify text segments crawled with
a focused crawler as either containing an argu-
ment or not. Their approach is based on several
discourse markers and features extracted from the
tense and mood of verbs. They report an F1-score
of 0.764 for their best performing system.
One of the first approaches focusing on the
identification of argument components is Argu-
mentative Zoning proposed by Teufel (1999). The
underlying assumption of this work is that argu-
47
ment components extracted from a scientific arti-
cle provide a good summary of its content. Each
sentence is classified as one of seven rhetorical
roles including claim, result or purpose. The ap-
proach obtained an F1-score of 0.46
2
using struc-
tural, lexical and syntactic features. Rooney et
al. (2013) also focus on the identification of ar-
gument components but in contrast to the work of
Teufel (1999) their scheme is not tailored to a par-
ticular genre. In their experiments, they identify
claims, premises and non-argumentative text units
in the Araucaria corpus and report an overall ac-
curacy of 65%. Feng and Hirst (2011) also use
the Araucaria corpus for their experiments but fo-
cus on the identification of argumentation schemes
(Walton, 1996), which are templates for forms of
arguments (e.g. argument from example or argu-
ment from consequence). Since their approach is
based on features extracted from mutual informa-
tion of claims and premises, it requires that the ar-
gument components are reliably identified in ad-
vance. In their experiments, they achieve an accu-
racy between 62.9% and 97.9% depending on the
particular scheme and the classification setup.
In contrast to all approaches mentioned above,
the work presented in this paper focuses be-
sides the separation of argumentative from non-
argumentative text units and the classification of
argument components on the extraction of the ar-
gumentative discourse structure to identify which
components of the argument belong together for
achieving a more fine-grained and detailed analy-
sis of argumentation. We are only aware of one ap-
proach (Mochales-Palau and Moens, 2009; Wyner
et al., 2010) that also focuses on the identifica-
tion of argumentative discourse structures. How-
ever, this approach is based on a manually created
CFG that is tailored to documents from the legal
domain, which follow a standardized argumenta-
tion style. Therefore, it does not accommodate ill-
formatted arguments (Wyner et al., 2010), which
are likely in argumentative writing support. In ad-
dition, the approach relies on discourse markers
and is therefore not applicable for identifying im-
plicit argumentative discourse structures.
2.2 Discourse Relations
Identifying argumentative discourse structures is
closely related to discourse analysis. As illustrated
2
Calculated from the precision and recall scores provided
for individual rhetorical roles in (Teufel, 1999, p. 225).
in the initial example, the identification of argu-
mentative relations postulates the identification of
implicit as well as non-adjacent discourse rela-
tions. Marcu and Echihabi (2002) present the first
approach focused on identifying implicit discourse
relations. They exploit several discourse mark-
ers (e.g. ?because? or ?but?) for collecting large
amounts of training data. For their experiments
they remove the discourse markers and discover
that word pair features are indicative for implicit
discourse relations. Depending on the utilized cor-
pus, they obtain accuracies between 64% and 75%
for identifying a cause-explanation-evidence rela-
tion (the most similar relation of their work com-
pared to argumentative relations).
With the release of the PDTB, the identifica-
tion of discourse relations gained a lot of interest
in the research community. The PDTB includes
implicit as well as explicit discourse relations of
different types, and there are multiple approaches
aiming at automatically identifying implicit rela-
tions. Pitler et al. (2009) experiment with polarity
tags, verb classes, length of verb phrases, modal-
ity, context and lexical features and found that
word pairs with non-zero Information Gain yield
best results. Lin et al. (2009) show that beside
lexical features, production rules collected from
parse trees yield good results, whereas Louis et
al. (2010) found that features based on named-
entities do not perform as well as lexical features.
However, current approaches to discourse analy-
sis like the RST or the PDTB are designed to ana-
lyze general discourse structures, and thus include
a large set of generic discourse relations, whereas
only a subset of those relations is relevant for ar-
gumentative discourse analysis. For instance, the
argumentation scheme proposed by Peldszus and
Stede (2013) includes three argumentative rela-
tions (support, attack and counter-attack), whereas
Stab and Gurevych (2014) propose a scheme in-
cluding only two relations (support and attack).
The difference between argumentative relations
and those included in general tagsets like RST and
PDTB is best illustrated by the work of Biran and
Rambow (2011), which is to the best of our knowl-
edge the only work that focuses on the identifica-
tion of argumentative relations. They argue that
existing definitions of discourse relations are only
relevant as a building block for identifying argu-
mentative discourse and that existing approaches
do not contain a single relation that corresponds to
48
a distinct argumentative relation. Therefore, they
consider a set of 12 discourse relations from the
RST Discourse Treebank (Carlson et al., 2001) as
a single argumentative relation in order to identify
justifications for a given claim. They first extract
a set of lexical indicators for each relation from
the RST Discourse Treebank and create a word
pair resource using the English Wikipedia. In their
experiments, they use the extracted word pairs as
features and obtain an F1-score of up to 0.51 using
two different corpora. Although the approach con-
siders non-adjacent relations, it is limited to the
identification of relations between premises and
claims and requires that claims are known in ad-
vance. In addition, the combination of several
general relations to a single argumentative relation
might lead to consistency problems and to noisy
corpora (e.g. not each instance of a contrast rela-
tion is relevant for argumentative discourse).
3 Data
For our experiments, we use a corpus of per-
suasive essays compiled by Stab and Gurevych
(2014). This corpus contains annotations of ar-
gument components at the clause-level as well
as argumentative relations. In particular, it in-
cludes annotations of major claims, claims and
premises, which are connected with argumentative
support and attack relations. Argumentative rela-
tions are directed (there is a specified source and
target component of each relation) and can hold
between a premise and another premise, a premise
and a (major-) claim, or a claim and a major claim.
Except for the last one, an argumentative relation
does not cross paragraph boundaries.
Three raters annotated the corpus with an inter-
annotator agreement of ?
U
= 0.72 (Krippendorff,
2004) for argument components and ? = 0.81 for
argumentative relations. In total, the corpus com-
prises 90 essays including 1,673 sentences. Since
it only contains a low number of attack relations,
we focus in this work solely on the identification
of argument components and argumentative sup-
port relations. However, the proposed approach
can also be applied to identify attack relations in
future work.
4 Identifying Argument Components
We consider the identification of argument com-
ponents as a multiclass classification task. Each
clause in the corpus is either classified as major
claim, claim, premise or non-argumentative. So
this task includes besides the classification of ar-
gument components also the separation of argu-
mentative and non-argumentative text units. We
label each sentence that does not contain an ar-
gument component as class ?none?. Since many
argument components cover an entire sentence
(30%), this is not an exclusive feature of this class.
In total, the corpus contains 1,879 instances.
Table 1 shows the class distribution among the
instances. The corpus includes 90 major claims
(each essay contains exactly one), 429 claims and
1,033 premises. This proportion between claims
and premises is common in argumentation since
claims are usually supported by several premises
for establishing a stable standpoint.
MajorClaim Claim Premise None
90 (4.8%) 429 (22.8%) 1,033 (55%) 327 (17.4%)
Table 1: Class distribution among the instances.
The corpus contains 1552 argument components
and 327 non-argumentative instances.
For our experiments, we randomly split the data
into a 80% training set and a 20% test set with
the same class distribution and determine the best
performing system using 10-fold cross-validation
on the training set only. In our experiments, we
use several classifiers (see section 4.2) from the
Weka data mining software (Hall et al., 2009).
For preprocessing the corpus, we use the Stanford
POS-Tagger (Toutanova et al., 2003) and Parser
(Klein and Manning, 2003) included in the DKPro
Framework (Gurevych et al., 2007). After these
steps, we use the DKPro-TC text classification
framework (Daxenberger et al., 2014) for extract-
ing the features described in the following section.
4.1 Features
Structural features: We define structural features
based on token statistics, the location and punc-
tuations of the argument component and its cov-
ering sentence. Since Biran and Rambow (2011)
found that premises are longer on the average than
other sentences, we add the number of tokens of
the argument component and its covering sentence
to our feature set. In addition, we define the num-
ber of tokens preceding and following an argument
component in the covering sentence, the token ra-
tio between covering sentence and argument com-
ponent, and a Boolean feature that indicates if the
49
argument component covers all tokens of its cov-
ering sentence as token statistics features.
For exploiting the structural properties of per-
suasive essays, we define a set of location-based
features. First, we define four Boolean features
that indicate if the argument component is present
in the introduction or conclusion of an essay and
if it is present in the first or the last sentence of
a paragraph. Second, we add the position of the
covering sentence in the essay as a numeric fea-
ture. Since major claims are always present in the
introduction or conclusion of an essay and para-
graphs frequently begin or conclude with a claim,
we expect that these features are good indicators
for classifying (major-) claims.
Further, we define structural features based on
the punctuation: the number of punctuation marks
of the covering sentence and the argument compo-
nent, the punctuation marks preceding and follow-
ing an argument component in its covering sen-
tence and a Boolean feature that indicates if the
sentence closes with a question mark.
Lexical features: We define n-grams, verbs,
adverbs and modals as lexical features. We con-
sider all n-grams of length 1-3 as a Boolean feature
and extract them from the argument component in-
cluding preceding tokens in the sentence that are
not covered by another argument component. So,
the n-gram features include discourse markers that
indicate certain argument components but which
are not included in the actual annotation of argu-
ment components.
Verbs and adverbs play an important role for
identifying argument components. For instance,
certain verbs like ?believe?, ?think? or ?agree? of-
ten signal stance expressions which indicate the
presence of a major claim and adverbs like ?also?,
?often? or ?really? emphasize the importance of a
premise. We model both verbs and adverbs as
Boolean features.
Modal verbs like ?should? and ?could? are fre-
quently used in argumentative discourse to signal
the degree of certainty when expressing a claim.
We use the POS tags generated during preprocess-
ing to identify modals and define a Boolean fea-
ture which indicates if an argument component
contains a modal verb.
Syntactic features: To capture syntactic prop-
erties of argument components, we define features
extracted from parse trees. We adopt two features
proposed by (Mochales-Palau and Moens, 2009):
the number of sub-clauses included in the covering
sentence and the depth of the parse tree. In addi-
tion, we extract production rules from the parse
tree as proposed by Lin et al. (2009) to capture
syntactic characteristics of an argument compo-
nent. The production rules are collected for each
function tag (e.g. VP, NN, S, etc.) in the sub-
tree of an argument component. The feature set
includes e.g. rules like V P ? V BG,NP or
PP ? IN,NP . We model each production rule
as a Boolean feature and set it to true if it appears
in the subtree of an argument component.
Since premises often refer to previous events
and claims are usually in present tense, we capture
the tense of the main verb of an argument compo-
nent as proposed by Mochales-Palau and Moens
(2009) and define a feature that indicates if an ar-
gument component is in the past or present tense.
Indicators: Discourse markers often indicate
the components of an argument. For example,
claims are frequently introduced with ?therefore?,
?thus? or ?consequently?, whereas premises con-
tain markers like ?because?, ?reason? or ?further-
more?. We collected a list of discourse markers
from the Penn Discourse Treebank 2.0 Annotation
Manual (Prasad et al., 2007) and removed markers
that do not indicate argumentative discourse (e.g.
markers which indicate temporal discourse). In to-
tal, we collected 55 discourse markers and model
each as a Boolean feature set to true if the particu-
lar marker precedes the argumentative component.
In addition, we define five Boolean features
which denote a reference to the first person in the
covering sentence of an argument component: ?I?,
?me?, ?my?, ?mine?, and ?myself?. An additional
Boolean feature indicates if one of them is present
in the covering sentence. We expect that those fea-
tures are good indicators of the major claim, since
it is often introduced with expressions referring to
the personal stance of the author.
Contextual features: The context plays a ma-
jor role for identifying argument components. For
instance, a premise can only be classified as such,
if there is a corresponding claim. Therefore, we
define the following features each extracted from
the sentence preceding and following the covering
sentence of an argument component: the number
of punctuations, the number of tokens, the number
of sub-clauses and a Boolean feature indicating the
presence of modal verbs.
50
4.2 Results and Analysis
For identifying the best performing system, we
conducted several experiments on the training set
using stratified 10-fold cross-validation. We de-
termine the evaluation scores by accumulating the
confusion matrices of each fold into one confusion
matrix, since it is the less biased method for evalu-
ating cross-validation studies (Forman and Scholz,
2010). In a comparison of several classifiers (Sup-
port Vector Machine, Na??ve Bayes, C4.5 Decision
Tree and Random Forest), we found that each of
the classifiers significantly outperforms a majority
baseline (McNemar Test (McNemar, 1947) with
p = 0.05) and that a Support Vector Machine
(SVM) achieves the best results using 100 top fea-
tures ranked by Information Gain.
3
It achieves an
accuracy of 77.3% on the test set and outperforms
the majority baseline with respect to overall accu-
racy as well as F1-score (table 2).
Baseline Human SVM
Accuracy 0.55 0.877 0.773
Macro F1 0.177 0.871 0.726
Macro Precision 0.137 0.864 0.773
Macro Recall 0.25 0.879 0.684
F1 MajorClaim 0 0.916 0.625
F1 Claim 0 0.841 0.538
F1 Premise 0.709 0.911 0.826
F1 None 0 0.812 0.884
Table 2: Results of an SVM for argument com-
ponent classification on the test set compared to a
majority baseline and human performance.
The upper bound for this task constitutes the
human performance which we determine by com-
paring each annotator to the gold standard. Since
the boundaries of an argument component in the
gold standard can differ from the boundaries iden-
tified by a human annotator (the annotation task
included the identification of argument component
boundaries), we label each argument component
of the gold standard with the class of the maximum
overlapping annotation of a human annotator for
determining the human performance. We obtain a
challenging upper bound of 87.7% (accuracy) by
averaging the scores of all three annotators on the
test set (table 2). So, our system achieves 88.1%
of human performance (accuracy).
Feature influence: In subsequent experiments,
we evaluate each of the defined feature groups on
the entire data set using 10-fold cross-validation to
3
Although the Na??ve Bayes classifier achieves lowest ac-
curacy, it exhibits a slightly higher recall compared to SVM.
find out which features perform best for identify-
ing argument components. As assumed, structural
features perform well for distinguishing claims
and premises in persuasive essays. They also yield
high results for separating argumentative from
non-argumentative text units (table 3).
Feature group MajorClaim Claim Premise None
Structural 0.477 0.419 0.781 0.897
Lexical 0.317 0.401 0.753 0.275
Syntactic 0.094 0.292 0.654 0.427
Indicators 0.286 0.265 0.730 0
Contextual 0 0 0.709 0
Table 3: F1-scores for individual feature groups
and classes (SVM with 10-fold cross-validation on
the entire data set)
Interestingly, the defined indicators are not
useful for separating argumentative from non-
argumentative text units though they are helpful
for classifying argument components. A reason
for this could be that not each occurrence of an
indicator distinctly signals argument components,
since their sense is often ambiguous (Prasad et
al., 2008). For example ?since? indicates temporal
properties as well as justifications, whereas ?be-
cause? also indicates causal links. Syntactic fea-
tures also contribute to the identification of argu-
ment components. They achieve an F1-score of
0.292 for claims and 0.654 for premises and also
contribute to the separation of argumentative from
non-argumentative text units. Contextual features
do not perform well. However, they increase the
accuracy by 0.7% in combination with other fea-
tures. Nevertheless, this difference is not signifi-
cant (p = 0.05).
Error analysis: The system performs well for
separating argumentative and non-argumentative
text units as well as for identifying premises.
However, the identification of claims and major
claims yields lower performance. The confusion
matrix (table 4) reveals that the most common er-
ror is between claims and premises. In total, 193
claims are incorrectly classified as premise. In
a manual assessment, we observed that many of
these errors occur if the claim is present in the first
paragraph sentence and exhibits preceding indica-
tors like ?first(ly)? or ?second(ly)? which are also
frequently used to enumerate premises. In these
cases, the author introduces the claim of the argu-
ment as support for the major claim and thus its
characteristic is similar to a premise. To prevent
51
this type of error, it might help to define features
representing the location of indicators or to disam-
biguate the function of indicators.
Predicted
A
c
t
u
a
l
MC Cl Pr No
MC 38 34 18 0
Cl 19 210 193 7
Pr 6 104 904 19
No 0 12 23 292
Table 4: Confusion matrix (SVM) for argument
component classification (MC = Major Claim; Cl
= Claim; Pr = Premise; No = None)
We also observed, that some of the misclassified
claims cover an entire sentence and don?t include
indicators. For example, it is even difficult for hu-
mans to classify the sentences ?Competition helps
in improvement and evolution? as a claim without
knowing the intention of the author. For prevent-
ing these errors, it might help to include more so-
phisticated contextual features.
5 Identifying Argumentative Relations
We consider the identification of argumentative re-
lations as a binary classification task of argument
component pairs and classify each pair as either
support or non-support. For identifying argumen-
tative relations, all possible combinations of argu-
ment components have to be tested. Since this re-
sults in a heavily skewed class distribution, we ex-
tract all possible combinations of argument com-
ponents from each paragraph of an essay.
4
So, we
omit argumentative relations between claims and
major claims which are the only relations in the
corpus that cross paragraph boundaries, but ob-
tain a better distribution between true (support)
and false (non-support) instances. In total, we ob-
tain 6,330 pairs, of which 15.6% are support and
84.4% are non-support relations (table 5).
Support Non-support
989 (15.6%) 5341 (84.4%)
Table 5: Class distribution of argument component
pairs
Equivalent to the identification of argument
components, we randomly split the data in a 80%
training and a 20% test set and determine the best
performing system using 10-fold cross-validation
4
Only 4.6% of 28,434 possible pairs are true instances
(support), if all combinations are considered.
on the training set. We use the same preprocessing
pipeline as described in section 4 and DKPro-TC
for extracting the features described below.
5.1 Features
Structural features: We define structural fea-
tures for each pair based on the source and tar-
get components, and on the mutual information of
both. Three numeric features are based on token
statistics. Two features represent the number of
tokens of the source and target components and
the third one represents the absolute difference in
the number of tokens. Three additional numeric
features count the number of punctuation marks
of the source and target components as well as
the absolute difference between both. We extract
both types of features solely from the clause an-
notated as argument component and do not con-
sider the covering sentence. In addition, we de-
fine nine structural features based on the position
of both argument components: two of them repre-
sent the position of the covering sentences in the
essay, four Boolean features indicate if the argu-
ment components are present in the first or last
sentence of a paragraph, one Boolean feature for
representing if the target component occurs before
the source component, the sentence distance be-
tween the covering sentences, and a Boolean fea-
ture which indicates if both argument components
are in the same sentence.
Lexical features: We define lexical features
based on word pairs, first words and modals. It
has been shown in previous work that word pairs
are effective for identifying implicit discourse re-
lations (Marcu and Echihabi, 2002). We define
each pair of words between the source and target
components as a Boolean feature and investigate
word pairs containing stop words as well as stop
word filtered word pairs.
In addition, we adopt the first word features
proposed by Pitler et al. (2009). We extract the
first word either from the argument component
or from non-annotated tokens preceding the ar-
gument component in the covering sentence if
present. So, the first word of an argument com-
ponent is either the first word of the sentence con-
taining the argument component, the first word
following a preceding argument component in the
same sentence or the first word of the actual ar-
gument component if it commences the sentence
or directly follows another argument component.
52
So, we ensure that the first word of an argument
component includes important discourse markers
which are not included in the annotation. We de-
fine each first word of the source and target com-
ponents as a Boolean feature and also add the pairs
of first words to our feature set.
Further, we define a Boolean feature for the
source as well as for the target component that
indicates if they contain a modal verb and a nu-
merical feature that counts the number of common
terms of the two argument components.
Syntactic features: For capturing syntactic
properties, we extract production rules from the
source and target components. Equivalent to the
features extracted for the argument component
classification (section 4.1), we model each rule as
a Boolean feature which is true if the correspond-
ing argument component includes the rule.
Indicators: We use the same list of discourse
markers introduced above (section 4.1) as indi-
cator features. For each indicator we define a
Boolean feature for the source as well as for the
target component of the pair and set it to true if
it is present in the argument component or in its
preceding tokens.
Predicted type: The argumentative type (major
claim, claim or premise) of the source and target
components is a strong indicator for identifying ar-
gumentative relations. For example, there are no
argumentative relations from claims to premises.
Thus, if the type of the argument component is
reliably identified many potential pairs can be ex-
cluded. Therefore, we define two features that rep-
resent the argumentative type of the source and tar-
get components identified in the first experiment.
5.2 Results and Analysis
The comparison of several classifiers reveals that
an SVM achieves the best results. In our exper-
iments, all classifiers except the C4.5 Decision
Tree significantly outperform a majority baseline
which classifies all pairs as non-support (p =
0.05). We also conducted several experiments
using word pair features only and found in con-
trast to Pitler et al. (2009) that limiting the num-
ber of word pairs decreases the performance. In
particular, we compared the top 100, 250, 500,
1000, 2500, 5000 word pairs ranked by Informa-
tion Gain, non-zero Information Gain word pairs
and non-filtered word pairs. The results show
that non-filtered word pairs perform best (macro
F1-score of 0.68). Our experiments also reveal
that filtering stop words containing word pairs de-
creases the macro F1-score to 0.60. We obtain the
best results using an SVM without any feature se-
lection method. Due to the class imbalance, the
SVM only slightly outperforms the accuracy of a
majority baseline on the test set (table 6). How-
ever, the macro F1-score is more appropriate for
evaluating the performance if the data is imbal-
anced since it assigns equal weight to the classes
and not to the instances. The SVM achieves a
macro F1-score of 0.722 and also outperforms the
baseline with respect to the majority class.
Baseline Human SVM
Accuracy 0.843 0.954 0.863
Macro F1 0.458 0.908 0.722
Macro Precision 0.422 0.937 0.739
Macro Recall 0.5 0.881 0.705
F1 Support 0 0.838 0.519
F1 Non-Support 0.915 0.973 0.92
Table 6: Results of an SVM for classifying argu-
mentative relations on the test set compared to a
majority baseline and human performance.
We determined the upper bound constituted by
the human performance by comparing the annota-
tions of all three annotators to the gold standard.
The scores in table 6 are the average scores of all
three annotators. Our system achieves 90.5% of
human performance (accuracy).
Feature influence: A comparison of the de-
fined feature groups using 10-fold cross-validation
on the entire data set shows that lexical features
perform best. They achieve an F1-score of 0.427
for support and 0.911 for non-support pairs (ta-
ble 7). The syntactic features also perform well
followed by the indicators. It turned out that struc-
tural features are not effective for identifying argu-
mentative relations though they are the most effec-
tive features for identifying argument components
(cp. section 4.2). However, when omitted from
the entire feature set the performance significantly
decreases by 0.018 macro F1-score (p = 0.05).
Interestingly, the predicted types from our first
experiment are not effective at all. Although the
argumentative type of the target component ex-
hibits the highest Information Gain in each fold
compared to all other features, the predicted type
does not yield a significant difference when com-
bined with all other features (p = 0.05). It only
improves the macro F1-score by 0.001 when in-
53
cluded in the entire feature set.
Feature group Support Non-Support
Structural 0 0.915
Lexical 0.427 0.911
Syntactic 0.305 0.911
Indicators 0.159 0.916
Predicted types 0 0.915
Table 7: F1-scores for individual feature groups
using an SVM and the entire data set
Error analysis: For identifying frequent er-
ror patterns, we manually investigated the mis-
takes of the classifier. Although our system identi-
fies 97.5% of the non-support pairs from claim to
premise correctly, there are still some false posi-
tives that could be prevented if the argument com-
ponents had been classified more accurately. For
instance, there are 18 non-support relations from
claim to another claim, 32 from claim to premise,
5 from major claim to premise and 4 from major
claim to claim among the false positives. How-
ever, the larger amount of errors is due to not iden-
tified support relations (false negatives). We found
that some errors might be related to missing con-
textual information and unresolved coreferences.
For instance, it might help to replace ?It? with ?Ex-
ercising? for classifying the pair ?It helps relieve
tension and stress? ? ?Exercising improves self-
esteem and confidence? as support relation or to in-
clude contextual information for the premise ?This
can have detrimental effects on health? support-
ing the claim ?There are some serious problems
springing from modern technology?.
6 Discussion
In our experiments, we have investigated the clas-
sification of argument components as well as the
identification of argumentative relations for recog-
nizing argumentative discourse structures in per-
suasive essays. Both tasks are closely related and
we assume that sharing mutual information be-
tween both tasks might be a promising direction
for future research. On the one hand, knowing the
type of argument components is a strong indica-
tor for identifying argumentative relations and on
the other hand, it is likely that information about
the argumentative structure facilitates the identi-
fication of argument components. However, our
experiments revealed that the current accuracy for
identifying argument components is not sufficient
for increasing the performance of argumentative
relation identification. Nevertheless, we obtain
almost human performance when including the
types of argument components of the gold stan-
dard (macro F1-score >0.85) in our argument re-
lation identification experiment and when includ-
ing the number of incoming and outgoing support
relations for each argument component in our first
experiment (macro F1-score >0.9). Therefore, it
can be assumed, that if the identification of argu-
ment components can be improved, the identifica-
tion of argumentative relations will achieve better
results and vice versa.
The results also show that the distinction be-
tween claims and premises is the major challenge
for identifying argument components. It turned
out that structural features are the most effective
ones for this task. However, some of those features
are unique to persuasive essays, and it is an open
question if there are general structural properties
of arguments which can be exploited for separat-
ing claims from premises.
Our experiments show that discourse markers
yield only low accuracies. Using only our defined
indicator features, we obtain an F1-score of 0.265
for identifying claims, whereas Mochales-Palau
and Moens (2009) achieve 0.673 for the same task
in legal documents using a CFG. This confirms our
initial assumption that approaches relying on dis-
course markers are not applicable for identifying
argumentative discourse structures in documents
which do not follow a standardized form. In ad-
dition, it shows that discourse markers are either
frequently missing or misleadingly used in student
texts and that there is a need for argumentative
writing support systems that assist students in em-
ploying discourse markers correctly.
7 Conclusion and Future Work
We presented a novel approach for identifying ar-
gumentative discourse structures in persuasive es-
says. Previous approaches on argument recog-
nition suffer from several limitations: Existing
approaches focus either solely on the identifica-
tion of argument components or rely on manu-
ally created rules which are not able to identify
implicit argumentative discourse structures. Our
approach is the first step towards computational
argument analysis in the educational domain and
enables the identification of implicit argumenta-
tive discourse structures. The presented approach
achieves 88.1% of human performance for identi-
54
fying argument components and 90.5% for identi-
fying argumentative relations.
For future work, we plan to extend our stud-
ies to larger corpora, to integrate our classifiers in
writing environments, and to investigate their ef-
fectiveness for supporting students.
Acknowledgements
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We thank Krish Perumal and Piyush Paliwal for
their valuable contributions and we thank the
anonymous reviewers for their helpful comments.
References
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 05(04):363?381.
M. Anne Britt and Aaron A. Larson. 2003. Construct-
ing representations of arguments. Journal of Mem-
ory and Language, 48(4):794 ? 810.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Aalborg, Denmark.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based framework for supervised learning
experiments on textual data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
pages 61?66, Baltimore, MD, USA.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 987?996, Portland,
OR, USA.
Eirini Florou, Stasinos Konstantopoulos, Antonis
Koukourikos, and Pythagoras Karampiperis. 2013.
Argument extraction for supporting public policy
formulation. In Proceedings of the 7th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 49?54, Sofia,
Bulgaria.
George Forman and Martin Scholz. 2010. Apples-to-
apples in cross-validation studies: Pitfalls in clas-
sifier performance measurement. SIGKDD Explor.
Newsl., 12(1):49?57.
Iryna Gurevych, Max M?uhlh?auser, Christof Mueller,
Juergen Steimle, Markus Weimer, and Torsten
Zesch. 2007. Darmstadt Knowledge Processing
Repository based on UIMA. In Proceedings of the
First Workshop on Unstructured Information Man-
agement Architecture at Biannual Conference of
the Society for Computational Linguistics and Lan-
guage Technology, Tuebingen, Germany.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10?18.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Sapporo, Japan.
Klaus Krippendorff. 2004. Measuring the Reliability
of Qualitative Text Analysis Data. Quality & Quan-
tity, 38(6):787?800.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1, EMNLP ?09, pages
343?351, Stroudsburg, PA, USA.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify
implicit discourse relations. In Proceedings of the
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, SIGDIAL ?10, pages
59?62, Stroudsburg, PA, USA.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, Informa-
tion Sciences Institute.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 368?375.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Raquel Mochales-Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
fication and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ?09, pages 98?
107, New York, NY, USA. ACM.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ?07, pages 225?230, Stan-
ford, California.
55
Andreas Peldszus and Manfred Stede. 2013. From
Argument Diagrams to Argumentation Mining in
Texts: A Survey. International Journal of Cogni-
tive Informatics and Natural Intelligence (IJCINI),
7(1):1?31.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, ACL ?09, pages
683?691, Suntec, Singapore. Association for Com-
putational Linguistics.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L.
Webber. 2007. The Penn Discourse Treebank 2.0
annotation manual. Technical report, Institute for
Research in Cognitive Science, University of Penn-
sylvania.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Chris Reed, Raquel Mochales-Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation, LREC ?08, pages 2613?2618, Mar-
rakech, Morocco.
Niall Rooney, Hui Wang, and Fiona Browne. 2012.
Applying kernel methods to argumentation min-
ing. In Proceedings of the Twenty-Fifth Interna-
tional Florida Artificial Intelligence Research So-
ciety Conference, FLAIRS ?12, pages 272?275,
Marco Island, FL, USA.
Alan Sergeant. 2013. Automatic argumentation ex-
traction. In Proceedings of the 10th European Se-
mantic Web Conference, ESWC ?13, pages 656?660,
Montpellier, France.
Christian Stab and Iryna Gurevych. 2014. Annotat-
ing argument components and relations in persua-
sive essays. In Proceedings of the 25th International
Conference on Computational Linguistics (COLING
2014), pages 1501?1510, Dublin, Ireland, August.
Simone Teufel. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
University of Edinburgh.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
NAACL ?03, pages 173?180, Edmonton, Canada.
Douglas N Walton. 1996. Argumentation schemes for
presumptive reasoning. Routledge.
Adam Wyner, Raquel Mochales Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Semantic
Processing of Legal Texts, volume 6036 of Lecture
Notes in Computer Science, pages 60?79.
56
