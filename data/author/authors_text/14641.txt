Coling 2010: Poster Volume, pages 1382?1390,
Beijing, August 2010
Chasing the ghost: recovering empty categories in the Chinese Treebank
Yaqin Yang
Computer Science Department
Brandeis University
yaqin@cs.brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
xuen@cs.brandeis.edu
Abstract
Empty categories represent an impor-
tant source of information in syntactic
parses annotated in the generative linguis-
tic tradition, but empty category recovery
has only started to receive serious atten-
tion until very recently, after substantial
progress in statistical parsing. This paper
describes a unified framework in recover-
ing empty categories in the Chinese Tree-
bank. Our results show that given skele-
tal gold standard parses, the empty cate-
gories can be detected with very high ac-
curacy. We report very promising results
for empty category recovery for automatic
parses as well.
1 Introduction
The use of empty categories to represent the syn-
tactic structure of a sentence is the hallmark of the
generative linguistics and they represent an im-
portant source of information in treebanks anno-
tated in this linguistic tradition. The use of empty
categories in the annotation of treebanks started
with the Penn Treebank (Marcus et al, 1993), and
this practice is continued in the Chinese Treebank
(CTB) (Xue et al, 2005) and the Arabic Tree-
bank, the Penn series of treebanks. Empty cat-
egories come in a few different varieties, serv-
ing different purposes. One use of empty cate-
gories is to mark the extraction site of an dislo-
cated phrase, thus effectively reconstructing the
canonical structure of a sentence, allowing easy
extraction of its predicate-argument structure. For
example, in Figure 1, the empty category *T*-
1 is coindexed with the dislocated topic NP ?
? (?Ningbo?), indicating that the canonical po-
sition of this NP is next to the verb ? (?come?).
The empty category effectively localizes the syn-
tactic dependency between the verb and this NP,
making it easier to detect and extract this relation.
Marking the extraction site of a dislocated item
is not the only use of empty categories. For lan-
guages like Chinese, empty categories are also
used to represent dropped pronouns. Chinese is
a pro-drop language (Huang, 1989) and subject
pronouns are routinely dropped. Recovering these
elliptical elements is important to many natural
language applications. When translated into an-
other language, for example, these dropped pro-
nouns may have to be made explicit and replaced
with overt pronouns or noun phrases if the target
language does not allow dropped pronouns.
Although empty categories have been an inte-
gral part of the syntactic representation of a sen-
tence ever since the Penn Treebank was first con-
structed, it is only recently that they are starting
to receive the attention they deserve. Works on
automatic detection of empty categories started
to emerge (Johnson, 2002; Dienes and Dubey,
2003; Campbell, 2004; Gabbard et al, 2006) af-
ter substantial progress has been made in statis-
tical syntactic parsing. This progress has been
achieved after over a decade of intensive research
on syntactic parsing that has essentially left the
empty categories behind (Collins, 1999; Char-
niak, 2000). Empty categories were and still are
routinely pruned out in parser evaluations (Black
et al, 1991). They have been excluded from the
parser development and evaluation cycle not so
much because their importance was not under-
stood, but because researchers haven?t figured out
1382
IP
NP-PN-TPC-1 NP-SBJ VP
NR PN VC VP
??
ningbo
Ningbo
?
wo
I
?
shi
be
QP-ADV VP
OD CLP VV NP-OBJ
??
disan
third
M ?
lai
come
-NONE-
?
ci
*T*-1
?Ningbo, this is the third time I came here.?
Figure 1: A CTB tree with empty categories
a way to incorporate the empty category detection
in the parsing process. In fact, the detection of
empty categories relies heavily on the other com-
ponents of the syntactic representation, and as a
result, empty category recovery is often formu-
lated as postprocessing problem after the skeletal
structure of a syntactic parse has been determined.
As work on English has demonstrated, empty cat-
egory detection can be performed with high accu-
racy given high-quality skeletal syntactic parses as
input.
Because Chinese allows dropped pronouns and
thus has more varieties of empty categories than
languages like English, it can be argued that there
is added importance in Chinese empty category
detection. However, to our knowledge, there has
been little work in this area, and the work we
report here represents the first effort in Chinese
empty category detection. Our results are promis-
ing, but they also show that Chinese empty cat-
egory detection is a very challenging problem
mostly because Chinese syntactic parsing is dif-
ficult and still lags significantly behind the state
of the art in English parsing. We show that given
skeletal gold-standard parses (with empty cate-
gories pruned out), the empty detection can be
performed with a fairly high accuracy of almost
89%. The performance drops significantly, to
63%, when the output of an automatic parser is
used.
The rest of the paper is organized as follows.
In Section 2, we formulate the empty category de-
tection as a binary classification problem where
each word is labeled as either having a empty cat-
egory before it or not. This makes it possible to
use any standard machine learning technique to
solve this problem. The key is to find the appro-
priate set of features. Section 3 describes the fea-
tures we use in our experiments. We present our
experimental results in Section 4. There are two
experimental conditions, one with gold standard
treebank parses (stripped of empty categories) as
input and the other with automatic parses. Section
5 describes related work and Section 6 conclude
our paper.
2 Formulating the empty category
detection as a tagging problem
In the CTB, empty categories are marked in a
parse tree which represents the hierarchical struc-
ture of a sentence, as illustrated in Figure 1.
There are eight types of empty categories anno-
tated in the CTB, and they are listed in Table 1.
Among them, *pro* and *PRO* are used to rep-
resent nominal empty categories, *T* and *NP*
are used to represent traces of dislocated items,
*OP* is used to represent empty relative pronouns
in relative clauses, and *RNR* is used to repre-
sent pseudo attachment. The reader is referred to
the CTB bracketing manual (Xue and Xia, 2000)
for detailed descriptions and examples. As can
be seen from Table 1, the distribution of these
empty categories is very uneven, and many of
these empty categories do not occur very often.
1383
EC Type count Description
*pro* 2024 small pro
*PRO* 2856 big pro
*T* 4486 trace for extraction
*RNR* 217 right node raising
*OP* 879 operator
* 132 trace for raising
Table 1: Empty categories in CTB.
As a first step of learning an empty category
model, we treat all the empty categories as a uni-
fied type, and for each word in the sentence, we
only try to decide if there is an empty category
before it. This amounts to an empty category de-
tection task, and the objective is to first locate the
empty categories without attempting to determine
the specific empty category type. Instead of pre-
dicting the locations of the empty categories in a
parse tree and having a separate classifier for each
syntactic construction where an empty category is
likely to occur, we adopt a linear view of the parse
tree and treat empty categories, along with overt
word tokens, as leaves in the tree. This allows us
to identify the location of the empty categories in
relation to overt word tokens in the same sentence,
as illustrated in Example (1):
(1) ?? ? ? ?? ? ? *T*?
In this representation, the position of the empty
category can be defined either in relation to the
previous or the next word, or both. To make
this even more amenable to machine learning ap-
proaches, we further reformulate the problem as a
tagging problem so that each overt word is labeled
either with EC, indicating there is an empty cate-
gory before this word, or NEC, indicating there is
no empty category. This reformulated representa-
tion is illustrated in Example (2):
(2) ? ?/NEC ?/NEC ?/NEC ? ?/NEC
?/NEC?/NEC?/EC
In (2), the EC label attached to the final period
indicates that there is an empty category before
this punctuation mark. There is a small price to
pay with this representation: when there is more
than one empty category before a word, it is indis-
tinguishable from cases where there is only one
empty category. What we have gained is a sim-
ple unified representation for all empty categories
that lend itself naturally to machine learning ap-
proaches. Another advantage is that for natural
language applications that do not need the full
parse trees but only need the empty categories,
this representation provides an easy-to-use repre-
sentation for those applications. Since this linear-
lized representation is still aligned with its parse
tree, we still have easy access to the full hierar-
chical structure of this tree from which useful fea-
tures can be extracted.
3 Features
Having modeled empty category detection as a
machine learning task, feature selection is crucial
to successfully finding a solution to this problem.
The machine learning algorithm scans the words
in a sentence from left to right one by one and
determine if there is an empty category before it.
When the sentence is paired with its parse tree,
the feature space is all the surrounding words of
the target word as well as the syntactic parse for
the sentence. The machine learning algorithm also
has access to the empty category labels (EC or
NEC) of all the words before the current word.
Figure 2 illustrates the feature space for the last
word (a period) in the sentence.
NP VP
NR PN VC
VP
QP
OD
CLP
M
VP
VV PU
NP
IP
!" # $% & ' !
NEC NEC NEC NEC NEC NEC EC
Ningbo I be third time come .
(
"Ningbo, this is the third time I came here."
Figure 2: Feature space of empty category detec-
tion
For purposes of presentation, we divide our
features into lexical and syntactic features. The
1384
lexical features are different combinations of the
words and their parts of speech (POS), while syn-
tactic features are the structural information gath-
ered from the nonterminal phrasal labels and their
syntactic relations.
3.1 Lexical features
The lexical features are collected from a narrow
window of five words and their POS tags. If the
target word is a verb, the lexical features also in-
clude transitivity information of this verb, which
is gathered from the CTB. A transitivity lexicon is
induced from the CTB by checking whether a verb
has a right NP or IP sibling. Each time a verb is
used as a transitive verb (having a right NP or IP
sibling), its transitive count is incremented by one.
Conversely, each time a verb is used as an intran-
sitive verb (not having a right NP or IP sibling), its
intransitive use is incremented by one. The result-
ing transitivity lexicon after running through the
entire Chinese Treebank consists of a list of verbs
with frequencies of their transitive and intransitive
uses. A verb is considered to be transitive if its in-
transitive count in this lexicon is zero or if its tran-
sitive use is more than three times as frequent as
its intransitive use. Similarly, a verb is considered
to be intransitive if its transitive count is zero or
if its intransitive use is at least three times as fre-
quent as its transitive use. The full list of lexical
features is presented in Table 2.
3.2 Syntactic features
Syntactic features are gathered from the CTB
parses stripped of function tags and empty cate-
gories when the gold standard trees are used as
input. The automatic parses used as input to our
system are produced by the Berkeley parser. Like
most parsers, the Berkeley parser does not repro-
duce the function tags and empty categories in the
original trees in the CTB. Syntactic features cap-
ture the syntactic context of the target word, and
as we shall show in Section 4, the syntactic fea-
tures are crucial to the success of empty category
detection. The list of syntactic features we use in
our system include:
1. 1st-IP-child: True if the current word is the
first word in the lowest IP dominating this
word.
Feature Names Description
word(0) Current word
word(-1) Previous word
pos(0) POS of current word
pos(-1,0) POS of previous and cur-
rent word
pos(0, 1) POS of current and next
word
pos(0, 1, 2) POS of current & next
word, & word 2 after
pos(-2, -1) POS of previous word &
word 2 before
word(-1), pos(0) Previous word & POS of
current word
pos(-1),word(0) POS of previous word&
current word
trans(0) current word is transitive
or intransitive verb
prep(0) true if POS of current
word is a preposition
Table 2: Feature set.
2. 1st-word-in-subjectless-IP: True if the cur-
rent word starts an IP with no subject. Sub-
ject is detected heuristically by looking at left
sisters of a VP node. Figure 3 illustrates this
feature for the first word in a sentence where
the subject is a dropped pronoun.
3. 1st-word-in-subjectless-IP+POS: POS of
the current word if it starts an IP with no sub-
ject.
4. 1st-VP-child-after-PU: True if the current
word is the first terminal child of a VP fol-
lowing a punctuation mark.
5. NT-in-IP: True if POS of current word is NT,
and it heads an NP that does not have a sub-
ject NP as its right sister.
6. verb-in-NP/VP: True if the current word is a
verb in an NP/VP.
7. parent-label: Phrasal label of the parent of
the current node, with the current node al-
ways corresponding to a terminal node in the
parse tree.
8. has-no-object: True If the previous word is
a transitive verb and this verb does not take
an object.
1385
!
"#
$%
&'
("#
)*+,-%.*/
 0
("#
.12
3 4
("#
*)/.*2%
56
("#
*77898)*,:;.)%-
<=
("#
*>>/?;.
@ABCD
("#
EFG
H
("#
I
LCP
PP
AD AD
VV
CD MLCNT
P QP
VPADVPADVP
VP
IP
By the end of  last year, (Shanghai) has approved 216 ...
Figure 3: First word in a subject-less IP
Empty categories generally occur in clausal or
phrasal boundaries, and most of the features are
designed to capture such information. For exam-
ple, the five feature types, 1st-IP-child, 1st-word-
in-subjectless-IP, 1st-word-in-subjectless-IP, 1st-
VP-child-after-PU and NT-in-IP all represent the
left edge of a clause (IP) with some level of gran-
ularity. parent label and verb-in-NP/VP represent
phrases within which empty categories typically
occur do not occur. The has-no-object feature is
intended to capture transitive uses of a verb when
the object is missing.
4 Experiments
Given that our approach is independent of specific
machine learning techniques, many standard ma-
chine learning algorithms can be applied to this
task. For our experiment we built a Maximum En-
tropy classifier with the Mallet toolkit1.
4.1 Data
In our experiments, we use a subset of the CTB
6.0. This subset is further divided into train-
ing (files chtb 0081 thorough chtb 0900), devel-
opment (files chtb 0041 through chtb 0080) and
test sets (files chtb 0001 through chtb 0040, files
chtb 0901 through chtb 0931). The reason for not
using the entire Chinese Treebank is that the data
in the CTB is from a variety of different sources
and the automatic parsing accuracy is very uneven
across these different sources.
1http://mallet.cs.umass.edu
4.2 Experimental conditions
Two different kinds of data sets were used in the
evaluation of our method: 1) gold standard parse
trees from the CTB; and 2) automatic parses pro-
duced by the Berkeley parser2 .
4.2.1 Gold standard parses
There are two experimental conditions. In our
first experiment, we use the gold standard parse
trees from the CTB as input to our classifier. The
version of the parse tree that we use as input to
our classifier is stripped of the empty category
information. What our system effectively does
is to restore the empty categories given a skele-
tal syntactic parse. The purpose of this experi-
ment is to establish a topline and see how accu-
rately the empty categories can be restored given
a?correct?parse.
4.2.2 Automatic parses
To be used in realistic scenarios, the parse trees
need to be produced automatically from raw text
using an automatic parser. In our experiments we
use the Berkeley Parser as a representative of the
state-of-the-art automatic parsers. The input to the
Berkeley parser is words that have already been
segmented in the CTB. Obviously, to achieve fully
automatic parsing, the raw text should be auto-
matically segmented as well. The Berkeley parser
comes with a fully trained model, and to make
sure that none of our test and development data is
included in the training data in the original model,
we retrained the parser with our training set and
used the resulting model to parse the documents
in the development and test sets.
When training our empty category model using
automatic parses, it is important that the quality
of the parses match between the training and test
sets. So the automatic parses in the training set
are acquired by first training the parser with 4/5
of the data and using the resulting model to parse
the remaining 1/5 of the data that has been held
out. Measured by the ParsEval metric (Black et
al., 1991), the parser accuracy stands at 80.3% (F-
score), with a precision of 81.8% and a recall of
78.8% (recall).
2http://code.google.com/p/berkeleyparser
1386
4.3 Evaluation metrics
We use precision, recall and F-measure as our
evaluation metrics for empty category detection.
Precision is defined as the number of correctly
identified Empty Categories (ECs) divided by the
total number of ECs that our system produced.
Recall is defined as the number of correctly iden-
tified ECs divided by the total number of EC la-
bels in the CTB gold standard data. F-measure
is defined as the geometric mean of precision and
recall.
R = # of correctly detected EC
# of EC tagged in corpus (1)
P = # of correctly detected EC
# of EC reported by the system (2)
F = 21/R + 1/P (3)
4.4 Overall EC detection performance
We report our best result for the gold standard
trees and the automatic parses produced by the
Berkeley parser in Table 3. These results are
achieved by using all lexical and syntactic features
presented in Section 3.
Data Prec.(%) Rec.(%) F(%)
Gold 95.9 (75.3) 83.0 (70.5) 89.0 (72.8)
Auto 80.3 (57.9) 52.1 (50.2) 63.2 (53.8)
Table 3: Best results on the gold tree.
As shown in Table 3, our feature set works
well for the gold standard trees. Not surprisingly,
the accuracy when using the automatic parses is
lower, with the performance gap between using
the gold standard trees and the Berkeley parser
at 25.8% (F-score). When the automatic parser
is used, although the precision is 80.3%, the re-
call is only 52.1%. As there is no similar work in
Chinese empty category detection using the same
data set, for comparison purposes we established
a baseline using a rule-based approach. The rule-
based algorithm captures two most frequent loca-
tions of empty categories: the subject and the ob-
ject positions. Our algorithm labels the first word
within a VP with EC if the VP does not have a
subject NP. Similarly, it assigns the EC label to the
word immediately following a transitive verb if it
does not have an NP or IP object. Since the miss-
ing subjects and objects account for most of the
empty categories in Chinese, this baseline covers
most of the empty categories. The baseline results
are also presented in Table 3 (in brackets). The
baseline results using the gold standard trees are
75.3% (precision), 70.5% (recall), and 72.8% (F-
score). Using the automatic parses, the results are
57.9% (precision), 50.2% (recall), and 53.8% (F-
score) respectively. It is clear from our results that
our machine learning model beats the rule-based
baseline by a comfortable margin in both exper-
imental conditions. Table 4 breaks down our re-
sults by empty category types. Notice that we did
not attempt to predict the specific empty category
type. This only shows the percentage of empty
categories our model is able to recover (recall) for
each type. As our model does not predict the spe-
cific empty category type, only whether there is an
empty category before a particular word, we can-
not compute the precision for each empty category
type. Nevertheless, this breakdown gives us a
sense of which empty category is easier to recover.
For both experimental conditions, the empty cate-
gory that can be recovered with the highest accu-
racy is *PRO*, an empty category often used in
subject/object control constructions. *pro* seems
to be the category that is most affected by parsing
accuracy. It has the widest gap between the two
experimental conditions, at more than 50%.
EC Type Total Correct Recall(%)
*pro* 290 274/125 94.5/43.1
*PRO* 299 298/196 99.7/65.6
*T* 578 466/338 80.6/58.5
*RNR* 32 22/20 68.8/62.5
*OP* 134 53/20 40.0/14.9
* 19 9/5 47.4/26.3
Table 4: Results of different types of empty cate-
gories.
4.5 Comparison of feature types
To investigate the relative importance of lexical
and syntactic features, we experimented with us-
ing just the lexical or syntactic features under
both experimental conditions. The results are pre-
1387
sented in Table 5. Our results show that when
using only the lexical features, the drop in accu-
racy is small when automatic parses are used in
place of gold standard trees. However, when us-
ing only the syntactic features, the drop in accu-
racy is much more dramatic. In both experimental
conditions, however, syntactic features are more
effective than the lexical features, indicating the
crucial importance of high-quality parses to suc-
cessful empty category detection. This makes in-
tuitive sense, given that all empty categories oc-
cupy clausal and phrasal boundaries that can only
defined in syntactic terms.
Data Prec.(%) Rec.(%) F(%)
Lexical 79.7/77.3 47.6/39.9 59.6/52.7
Syntactic 95.9/78.0 70.0/44.5 81.0/56.7
Table 5: Comparison of lexical and syntactic fea-
tures.
4.6 Comparison of individual features
Given the importance of syntactic features, we
conducted an experiment trying to evaluate the
impact of each individual syntactic feature on the
overall empty category detection performance. In
this experiment, we kept the lexical feature set
constant, and switched off the syntactic features
one at a time. The performance of the different
syntactic features is shown in Table 6. The re-
sults here assume that automatic parses are used.
The first row is the result of using all features
(both syntactic and lexical) while the last row is
the result of using only the lexical features. It
can be seen that syntactic features contribute more
than 10% to the overall accuracy. The results also
show that features (e.g., 1st-IP-child) that capture
clause boundary information tend to be more dis-
criminative and they occupy the first few rows of
a table that sorted based on feature performance.
5 Related work
The problem of empty category detection has been
studied both in the context of reference resolution
and syntactic parsing. In the reference resolution
literature, empty category detection manifests it-
self in the form of zero anaphora (or zero pronoun)
Feature Name Prec.(%) Rec.(%) F(%)
all 80.3 52.1 63.2
1st-IP-child 79.8 49.2 60.8
1st-VP-child-
after-PU
79.7 50.5 61.8
NT-in-IP 79.4 50.8 61.9
1st-word-in-
subjectless-
IP+Pos
79.5 51.1 62.2
has-no-object 80.0 51.1 62.4
1st-word-in-
subjectless-IP
79.4 51.5 62.5
verb-in-NP/VP 79.9 52.0 63.0
parent-label 79.4 52.4 63.1
only lexical 77.3 39.9 52.7
Table 6: Performance for individual syntactic fea-
tures with automatic parses.
detection and resolution. Zero anaphora resolu-
tion has been studied as a computational prob-
lem for many different languages. For example,
(Ferra?ndez and Peral, 2000) describes an algo-
rithm for detecting and resolving zero pronouns
in Spanish texts. (Seki et al, 2002) and (Lida et
al., 2007) reported work on zero pronoun detec-
tion and resolution in Japanese.
Zero anaphora detection and resolution for
Chinese has been studied as well. Converse
(2006) studied Chinese pronominal anaphora res-
olution, including zero anaphora resolution, al-
though there is no attempt to automatically de-
tect the zero anaphors in text. Her work only
deals with anaphora resolution, assuming the zero
anaphors have already been detected. Chinese
zero anaphora identification and resolution have
been studied in a machine learning framework-
ing in (Zhao and Ng, 2007) and (Peng and Araki,
2007).
The present work studies empty category re-
covery as part of the effort to fully parse natural
language text and as such our work is not lim-
ited to just recovering zero anaphors. We are
also interested in other types of empty categories
such as traces. Our work is thus more closely re-
lated to the work of (Johnson, 2002), (Dienes and
Dubey, 2003), (Campbell, 2004) and (Gabbard et
1388
al., 2006).
Johnson (2002) describes a pattern-matching
algorithm for recovering empty nodes from phrase
structure trees. The idea was to extract minimal
connected tree fragments that contain an empty
node and its antecedent(s), and to match the ex-
tracted fragments against an input tree. He eval-
uated his approach both on Penn Treebank gold
standard trees stripped of the empty categories and
on the output of the Charniak parser (Charniak,
2000).
(Dienes and Dubey, 2003) describes an empty
detection method that is similar to ours in that it
treats empty detection as a tagging problem. The
difference is that the tagging is done without ac-
cess to any syntactic information so that the iden-
tified empty categories along with word tokens in
the sentence can then be fed into a parser. The suc-
cess of this approach depends on strong local cues
such as infinitive markers and participles, which
are non-existent in Chinese. Not surprisingly, our
model yields low accuracy if only lexical features
are used.
Cambell (2004) proposes an algorithm that uses
linguistic principles in empty category recovery.
He argues that a rule-based approach might per-
form well for this problem because the locations
of the empty categories, at least in English, are in-
serted by annotators who follow explicit linguistic
principles.
Yuqing(2007) extends (Cahill et al, 2004) ?s
approach for recovering English non-local depen-
dencies and applies it to Chinese. This paper pro-
poses a method based on the Lexical-Functional
Grammar f-structures, which differs from our ap-
proach. Based on parser output trees including
610 files from the CTB, the authors of this pa-
per claimed they have achieved 64.71% f-score for
trace insertion and 54.71% for antecedent recov-
ery.
(Gabbard et al, 2006) describes a more recent
effort to fully parse the Penn Treebank, recovering
both the function tags and the empty categories.
Their approach is similar to ours in that they treat
empty category recovery as a post-processing pro-
cess and use a machine learning algorithm that
has access to the skeletal information in the parse
tree. Their approach is different from ours in that
they have different classifiers for different types of
empty categories.
Although generally higher accuracies are re-
ported in works on English empty category re-
covery, cross-linguistic comparison is difficult be-
cause both the types of empty categories and
the linguistic cues that are accessible to machine
learning algorithms are different. For example,
there are no empty complementizers annotated in
the CTB while English does not allow dropped
pronouns.
6 Conclusion and future work
We describe a unified framework to recover empty
categories for Chinese given skeletal parse trees as
input. In this framework, empty detection is for-
mulated as a tagging problem where each word
in the sentence receives a tag indicating whether
there is an empty category before it. This ad-
vantage of this approach is that it is amenable to
learning-based approaches and can be addressed
with a variety of machine learning algorithms.
Our results based on a Maximum Entropy model
show that given skeletal gold standard parses,
empty categories can be recovered with very high
accuracy (close to 90%). We also report promis-
ing results (over 63%). when automatic parses
produced by an off-the-shelf parser is used as in-
put.
Detecting empty categories is only the first step
towards fully reproducing the syntactic represen-
tation in the CTB, and the obvious next step is to
also classify these empty categories into different
types and wherever applicable, link the empty cat-
egories to their antecedent. This is the line of re-
search we intend to pursue in our future work.
Acknowledgment
This work is supported by the National Sci-
ence Foundation via Grant No. 0910532 enti-
tled ?Richer Representations for Machine Trans-
lation?. All views expressed in this paper are
those of the authors and do not necessarily repre-
sent the view of the National Science Foundation.
1389
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitively comparing the syntactic cov-
erage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
pages 306?311.
Cahill, Aoife, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2004. Long-
Distance Dependency Resolution in Automatically
Acquired Wide-Coverage PCFG-Based LFG Ap-
proximations. In In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics.
Campbell, Richard. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Annual Meeting on Association For Computa-
tional Linguistics.
Charniak, E. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL-2000, pages 132?
139, Seattle, Washington.
Collins, Michael. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Converse, Susan. 2006. Pronominal anaphora resolu-
tion for Chinese. Ph.D. thesis.
Dienes, Pe?ter and Amit Dubey. 2003. Deep syntac-
tic processing by combining shallow methods. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, volume 1.
Ferra?ndez, Antonio and Jesu?s Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associ-
ation For Computational Linguistics.
Gabbard, Ryan, Seth Kulick, and Mitchell Marcus.
2006. Fully parsing the penn treebank. In Proceed-
ings of HLT-NAACL 2006, pages 184?191, New
York City.
Guo, Yuqing, Haifeng Wang, and Josef van Genabith.
2007. Recovering Non-Local Dependencies for
Chinese. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Huang, James C.-T. 1989. Pro drop in Chinese, a
generalized control approach. In O, Jaeggli and
K. Safir, editors, The Null Subject Parameter. D.
Reidel Dordrecht.
Johnson, Mark. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Lida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, pages 1?22.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Peng, Jing and Kenji Araki. 2007. Zero-anaphora res-
olution in chinese using maximum entropy. IEICE
- Trans. Inf. Syst., E90-D(7):1092?1102.
Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing
Japanese anaphora integrating zero pronoun detec-
tion and resolution. In Proceedings of the 19th in-
ternational Conference on Computational Linguis-
tics, volume 1.
Xue, Nianwen and Fei Xia. 2000. The Bracket-
ing Guidelines for Penn Chinese Treebank Project.
Technical Report IRCS 00-08, University of Penn-
sylvania.
Xue, Nianwen, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207?238.
Zhao, Shanheng and Hwee Tou Ng. 2007. Identifi-
cation and Resolution of Chinese Zero Pronouns:
A Machine Learning Approach. In Proceedings of
EMNLP-CoNLL Joint Conference, Prague, Czech
Republic.
1390
Proceedings of NAACL-HLT 2013, pages 1051?1060,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dependency-based empty category detection via phrase structure trees
Nianwen Xue
Brandeis University
Waltham, MA, USA
xuen@brandeis.edu
Yaqin Yang
Brandeis University
Waltham, MA, USA
yaqin@brandeis.edu
Abstract
We describe a novel approach to detecting
empty categories (EC) as represented in de-
pendency trees as well as a new metric for
measuring EC detection accuracy. The new
metric takes into account not only the position
and type of an EC, but also the head it is a
dependent of in a dependency tree. We also
introduce a variety of new features that are
more suited for this approach. Tested on a sub-
set of the Chinese Treebank, our system im-
proved significantly over the best previously
reported results even when evaluated with this
more stringent metric.
1 Introduction
In modern theoretical linguistics, empty categories
(ECs) are an important piece of machinery in repre-
senting the syntactic structure of a sentence and they
are used to represent phonologically null elements
such as dropped pronouns and traces of dislocated
elements. They have also found their way into large-
scale treebanks which have played an important role
in advancing the state of the art in syntactic parsing.
In phrase-structure treebanks, ECs have been used to
indicate long-distance dependencies, discontinuous
constituents, and certain dropped elements (Marcus
et al, 1993; Xue et al, 2005). Together with la-
beled brackets and function tags, they make up the
full syntactic representation of a sentence.
The use of ECs captures some cross-linguistic
commonalities and differences. For example, while
both the Penn English TreeBank (PTB) (Marcus et
al., 1993) and the Chinese TreeBank (CTB) (Xue
et al, 2005) use traces to represent the extraction
site of a dislocated element, dropped pronouns (rep-
resented as *pro*s) are much more widespread in
the CTB. This is because Chinese is a pro-drop lan-
guage (Huang, 1984) that allows the subject to be
dropped in more contexts than English does. While
detecting and resolving traces is important to the in-
terpretation of the syntactic structure of a sentence in
both English and Chinese, the prevalence of dropped
nouns in Chinese text gives EC detection added sig-
nificance and urgency. They are not only an impor-
tant component of the syntactic parse of a sentence,
but are also essential to a wide range of NLP appli-
cations. For example, any meaningful tracking of
entities and events in natural language text would
have to include those represented by dropped pro-
nouns. If Chinese is translated into a different lan-
guage, it is also necessary to render these dropped
pronouns explicit if the target language does not al-
low pro-drop. In fact, Chung and Gildea (2010) re-
ported preliminary work that has shown a positive
impact of automatic EC detection on statistical ma-
chine translation.
Some ECs can be resolved to an overt element in
the same text while others only have a generic ref-
erence that cannot be linked to any specific entity.
Still others have a plausible antecedent in the text,
but are not annotated due to annotation limitations.
A common practice is to resolve ECs in two separate
stages (Johnson, 2002; Dienes and Dubey, 2003b;
Dienes and Dubey, 2003a; Campbell, 2004; Gab-
bard et al, 2006; Schmid, 2006; Cai et al, 2011).
The first stage is EC detection, where empty cate-
gories are first located and typed. The second stage
1051
is EC resolution, where empty categories are linked
to an overt element if possible.
In this paper we describe a novel approach to de-
tecting empty categories in Chinese, using the CTB
as training and test data. More concretely, EC de-
tection involves (i) identifying the position of the
EC, relative to some overt word tokens in the same
sentence, and (ii) determining the type of EC, e.g.,
whether it is a dropped pronoun or a trace. We fo-
cus on EC detection here because most of the ECs
in the Chinese Treebank are either not resolved to
an overt element or linked to another EC. For ex-
ample, dropped pronouns (*pro*) are not resolved,
and traces (*T*) in relative clauses are linked to an
empty relative pronoun (*OP*).
In previous work, ECs are either represented lin-
early, where ECs are indexed to the following word
(Yang and Xue, 2010) or attached to nodes in a
phrase structure tree (Johnson, 2002; Dienes and
Dubey, 2003b; Gabbard et al, 2006). In a linear
representation where ECs are indexed to the follow-
ing word, it is difficult to represent consecutive ECs
because that will mean more than one EC will be
indexed to the same word (making the classification
task more complicated). While in English consecu-
tive ECs are relatively rare, in Chinese this is very
common. For example, it is often the case that an
empty relative pronoun (*OP*) is followed imme-
diately by a trace (*T*). Another issue with the lin-
ear representation of ECs is that it leaves unspecified
where the EC should be attached, and crucial depen-
dencies between ECs and other elements in the syn-
tactic structure are not represented, thus limiting the
utility of this task.
In a phrase structure representation, ECs are at-
tached to a hierarchical structure and the problem
of multiple ECs indexed to the same word token can
be avoided because linearly consecutive ECs may be
attached to different non-terminal nodes in a phrase
structure tree. In a phrase structure framework, ECs
are evaluated based on their linear position as well
as on their contribution to the overall accuracy of
the syntactic parse (Cai et al, 2011).
In the present work, we propose to look at EC
detection in a dependency structure representation,
where we define EC detection as (i) determining its
linear position relative to the following word token,
(ii) determining its head it is a dependent of, and (iii)
determining the type of EC. Framing EC detection
this way also requires a new evaluation metric. An
EC is considered to be correctly detected if its linear
position, its head, and its type are all correctly de-
termined. We report experimental results that show
even using this more stringent measure, our EC de-
tection system achieved performance that improved
significantly over the state-of-the-art results.
The rest of the paper is organized as follows. In
Section 2, we will describe how to represent ECs
in a dependency structure in detail and present our
approach to EC detection. In Section 3, we describe
how linguistic information is encoded as features.
In Section 4, we discuss our experimental setup and
present our results. In Section 5, we describe related
work. Section 6 concludes the paper.
2 Approach
In order to detect ECs anchored in a dependency
tree, we first convert the phrase structure trees in the
CTB into dependency trees. After the conversion,
each word token in a dependency tree, including the
ECs, will have one and only one head (or parent).
We then train a classifier to predict the position and
type of ECs in the dependency tree. Let W be a se-
quence of word tokens in a sentence, and T is syn-
tactic parse tree for W , our task is to predict whether
there is a tuple (h, t, e), such that h and t are word to-
kens in W , e is an EC, h is the head of e, and t imme-
diately follows e. When EC detection is formulated
as a classification task, each classification instance
is thus a tuple (h, t). The input to our classifier is
T , which can either be a phrase structure tree or a
dependency tree. We choose to use a phrase struc-
ture tree because phrase structure parsers trained on
the Chinese Treebank are readily available, and we
also hypothesize that phrase structure trees have a
richer hierarchical structure that can be exploited as
features for EC detection.
2.1 Empty categories in the Chinese Treebank
According to the CTB bracketing guidelines (Xue
and Xia, 2000), there are seven different types of
ECs in the CTB. Below is a brief description of the
empty categories:
1. *pro*: small pro, used to represent dropped
pronouns.
1052
2. *PRO*: big PRO, used to represent shared el-
ements in control structures or elements that
have generic references.
3. *OP*: null operator, used to represent empty
relative pronouns.
4. *T*: trace left by movement such as topical-
ization and relativization.
5. *RNR*: right node raising.
6. *: trace left by passivization and raising.
7. *?*: missing elements of unknown category.
An example parse tree with ECs is shown in
Figure 1. In the example, there are two ECs, an
empty relative pronoun (*OP*) and a trace (*T*), a
common syntactic pattern for relative clauses in the
CTB.


Shanghai

Pudong

recently

issue
*OP*

 involve
NN

DEC

document
NR NR
AD
VV
VV
NN
DEC
NP
ADVP
NP
NP
NP
WHNP
VP
IP
CP
CP
NP
VP
VP
IP
"Shanghai Pudong recently enacted 71 regulatory documents involving
the enconomic field."
ASP

AS
T*
NN
QP
CD
M


CLP
ADJP
JJ
	
regulatory
 
economic
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 631?635,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Chinese sentence segmentation as comma classification
Nianwen Xue and Yaqin Yang
Brandeis University, Computer Science Department
Waltham, MA, 02453
{xuen,yaqin}@brandeis.edu
Abstract
We describe a method for disambiguating Chi-
nese commas that is central to Chinese sen-
tence segmentation. Chinese sentence seg-
mentation is viewed as the detection of loosely
coordinated clauses separated by commas.
Trained and tested on data derived from the
Chinese Treebank, our model achieves a clas-
sification accuracy of close to 90% overall,
which translates to an F1 score of 70% for
detecting commas that signal sentence bound-
aries.
1 Introduction
Sentence segmentation, or the detection of sentence
boundaries, is very much a solved problem for En-
glish. Sentence boundaries can be determined by
looking for periods, exclamation marks and ques-
tion marks. Although the symbol (dot) that is used to
represent period is ambiguous because it is also used
as the decimal point or in abbreviations, its resolu-
tion only requires local context. It can be resolved
fairly easily with rules in the form of regular expres-
sions or in a machine-learning framework (Reynar
and Ratnaparkhi, 1997).
Chinese also uses periods (albeit with a different
symbol), question marks, and exclamation marks to
indicate sentence boundaries. Where these punctua-
tion marks exist, sentence boundaries can be unam-
biguously detected. The difference is that the Chi-
nese comma also functions similarly as the English
period in some context and signals the boundary of a
sentence. As a result, if the commas are not disam-
biguated, Chinese would have these ?run-on? sen-
tences that can only be plausibly translated into mul-
tiple English sentences. An example is given in (1),
where one Chinese sentence is plausibly translated
into three English sentences.
(1) ?
this
?
period
??
time
??
AS
?
AS
??
pay attention to
?
this
?
CL
nano
Nano
3
3
?
,
[1]?
even
??
in person
?
visit
?
AS
?
a few
?
AS
??
computer
??
market
,
,
[2]???
comparatively
??
speaking
,
,
[3]??
Zhuoyue
?
?s
??
price
?
relatively
?
low
?
DE
,
,
[4]??
and
?
can
??
guarantee
?
be
??
genuine
?[5]
,
???
therefore
?
place
?
[AS]
?
order
?
.
?I have been paying attention to this Nano 3 re-
cently, [1] and I even visited a few computer
stores in person. [2] Comparatively speaking,
[3] Zhuoyue?s prices are relatively low, [4]
and they can also guarantee that their products
are genuine. [5] Therefore I placed the order.?
In this paper, we formulate Chinese sentence seg-
mentation as a comma disambiguation problem. The
problem is basically one of separating commas that
mark sentence boundaries (such as [2] and [5] in (1))
from those that do not (such as [1], [3] and [4]).
Sentences that can be split on commas are gener-
ally loosely coordinated structures that are syntacti-
cally and semantically complete on their own, and
they do not have a close syntactic relation with one
another. We believe that a sentence boundary detec-
tion task that disambiguates commas, if successfully
631
solved, simplifies downstream tasks such as parsing
and Machine Translation.
The rest of the paper is organized as follows. In
Section 2, we describe our procedure for deriving
training and test data from the Chinese Treebank
(Xue et al, 2005). In Section 3, we present our
learning procedure. In Section 4 we report our re-
sults. Section 5 discusses related work. Section 6
concludes our paper.
2 Obtaining data
To our knowledge, there is no data in the public
domain with commas explicitly annotated based on
whether they mark sentence boundaries. One could
imagine using parallel data where a Chinese sen-
tence is word-aligned with multiple English sen-
tences, but such data is generally noisy and com-
mas are not disambiguated based on a uniform stan-
dard. We instead pursued a different path and de-
rived our training and test data from the Chinese
Treebank (CTB). The CTB does not disambiguate
commas explicitly, and just like the Penn English
Treebank (Marcus et al, 1993), the sentence bound-
aries in the CTB are identified by periods, exclama-
tion and question marks. However, there are clear
syntactic patterns that can be used to disambiguate
the two types of commas. Commas that mark sen-
tence boundaries delimit loosely coordinated top-
level IPs, as illustrated in Figure 1, and commas that
don?t cover all other cases. One such example is
Figure 2, where a PP is separated from the rest of
the sentence with a comma. We devised a heuristic
algorithm to detect loosely coordinated structures in
the Chinese Treebank, and labeled each comma with
either EOS (end of a sentence) or Non-EOS (not the
end of a sentence).
3 Learning
After the commas are labeled, we have basically
turned comma disambiguation into a binary classi-
fication problem. The syntactic structures are an
obvious source of information for this classification
task, so we parsed the entire CTB 6.0 in a round-
robin fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). The labels for the commas are derived
????
?
?? ??
?
??
?
??
??
??
?
?? ????
?
IP PU IP PU IP PU
IP
NP VP
??
NP
VP
VV
NP
NP
VP
VV
IP
NP VP
VV NP
*pro*
ADVP
VP
??
??? ??
ADVP
VP
VV
Figure 1: Sentence-boundary denoting comma
IP
PP PU NP NP VP PU
?
P NP DNP NP
NP
DEG
VV
??
?
? ?? ? ?? ?
?? ?? ? ??? ?? ??
??
??
?
Figure 2: Non-sentence boundary denoting comma
from the gold-standard parses using the heuristics
described in Section 2, as they obviously should be.
We first established a baseline by applying the same
heuristic algorithm to the automatic parses. This will
give us a sense of how accurately commas can be
disambiguated given imperfect parses. The research
question we?re trying to address here basically is:
can we improve on the baseline accuracy with a ma-
chine learning model?
We conducted our experiments with a Maximum
Entropy classifier trained with the Mallet package
(McCallum, 2002). The following are the features
we used to train our classifier. All features are de-
scribed relative to the comma being classified and
the context is the sentence that the comma is in. The
actual feature values for the first comma in Figure 1
are given as examples:
1. Part-of-speech tag of the previous word, and
the string representation of the previous word
if it has a frequency of greater than 20 in the
training corpus, e.g., f1=VV, f2=??.
2. Part-of-speech of the following word and the
632
string representation of the following word if it
has a frequency of greater than 20 in the train-
ing corpus, e.g., f3=JJ, f4=??
3. The string representation of the following word
if it occurs more than 12,000 times in sentence-
initial positions in a large corpus external to our
training and test data.1
4. The phrase label of the left sibling and the
phrase label of their right sibling in the syntac-
tic parse tree, as well as their conjunction, e.g,
f6=IP, f7=IP, f8=IP+IP
5. The conjunction of the ancestors, the phrase la-
bel of the left sibling, and the phrase label of
the right sibling. The ancestor is defined as the
path from the parent of the comma to the root
node of the parse tree, e.g., f9=IP+IP+IP.
6. Whether there is a subordinating conjunction
(e.g., ?if?, ?because?) to the left of the comma.
The search starts at the comma and stops at the
previous punctuation mark or the beginning of
the sentence, e.g., f10=noCS.
7. Whether the parent of the comma is a coordi-
nating IP construction. A coordinating IP con-
struction is an IP that dominates a list of coor-
dinated IPs, e.g., f11=CoordIP.
8. Whether the comma is a top-level child, defined
as the child of the root node of the syntactic
tree, e.g., f12=top.
9. Whether the parent of the comma is a
top-level coordinating IP construction, e.g.,
f13=top+coordIP.
10. The punctuation mark template for this sen-
tence, e.g., f14=,+,+?
11. whether the length difference between the left
and right segments of the comma is smaller
than 7. The left (right) segment spans from the
previous (next) punctuation mark or the begin-
ning (end) of the sentence to the comma, e.g.,
f15=>7
4 Results and discussion
Our comma disambiguation models are trained and
evaluated on a subset of the Chinese TreeBank
(CTB) 6.0, released by the LDC. The unused por-
tion of CTB 6.0 consists of broadcast news data that
1This feature is not instantiated here because the following
word in this example does not occur with sufficient accuracy.
contains disfluencies, different from the rest of the
CTB 6.0. We used the training/test data split rec-
ommended in the Chinese Treebank documentation.
The CTB file IDs used in our experiments are listed
in Table 1. The automatic parses in each test set
are produced by retraining the Berkeley parser on
its corresponding training set, plus the unused por-
tion of the CTB 6.0. Measured by the ParsEval met-
ric (Black et al, 1991), the parsing accuracy on the
CTB test set stands at 83.63% (F-score), with a pre-
cision of 85.66% and a recall of 81.69%.
Data Train Test
CTB
41-325, 400-454, 500-554 1-40
590-596, 600-885, 900 901-931
1001-1078, 1100-1151
Table 1: Data set division.
There are 1,510 commas in the test set, and our
heuristic baseline algorithm is able to correctly label
1,321 or 87.5% of the commas. Among these, 250
or 16.6% of them are EOS commas that mark sen-
tence boundaries and 1,260 of them are Non-EOS
commas. The results of our experiments are pre-
sented in Table 2. The baseline precision and recall
for the EOS commas are 59.1% and 79.6% respec-
tively with an F1 score of 67.8% . For Non-EOS
commas, the baseline precision and recall are 95.7%
and 89.0% respectively, amounting to an F1 score of
70.1%. The learned maximum classifier achieved a
modest improvement over the baseline. The over-
all accuracy of the learned model is 89.2%, just shy
of 90%. The precision and recall for EOS commas
are 64.7% and 76.4% respectively and the combined
F1 score is 70.1%. For Non-EOS commas, the pre-
cision and recall are 95.1% and 91.7% respectively,
with the F1 score being 93.4%. Other than a list
of most frequent words that start a sentence, all the
features are extracted from the sentence the comma
occurs in. Given that the heuristic algorithm and the
learned model use essentially the same source of in-
formation, we attribute the improvement to the use
of lexical features that the heuristic algorithm cannot
easily take advantage of.
Table 3 shows the contribution of individual fea-
ture groups. The numbers reflect the accuracy when
each feature group is taken out of the model. While
all the features have made a contribution to the over-
633
Baseline Learning
(%) p r f1 p r f1
Overall 87.5 89.2
EOS 59.1 79.6 67.8 64.7 76.4 70.1
Non-
EOS
95.7 89.0 92.2 95.1 91.7 93.4
Table 2: Accuracy for the baseline heuristic algorithm
and the learned model
all accuracy on the development set, some of the
features (3 and 8) actually hurt the overall perfor-
mance slightly on the test set. What?s interesting is
while the heuristic algorithm that is based entirely
on syntactic structure produced a strong baseline,
when formulated as features they are not at all effec-
tive. In particular, feature groups 7, 8, 9 are explicit
reformulations of the heuristic algorithm, but they
all contributed very little to or even slightly hurt the
overall performance. The more effective features are
the lexical features (1, 2, 10, 11) probably because
they are more robust. What this suggests is that we
can get reasonable sentence segmentation accuracy
without having to parse the sentence (or rather, the
multi-sentence group) first. The sentence segmenta-
tion can thus come before parsing in the processing
pipeline even in a language like Chinese where sen-
tences are not unambiguously marked.
overall f1 (EOS) f1 (non-EOS)
all 89.2 70.1 93.4
- (1,2) 87.5 67.7 92.3
-10 87.8 67.5 92.5
-11 88.6 68.6 93.1
-4 89.0 69.6 93.3
-5 89.1 69.5 93.3
-6 89.1 69.9 93.4
-7 89.1 70.1 93.4
-9 89.1 69.7 93.3
-8 89.2 70.5 93.4
- 3 89.4 70.5 93.5
Table 3: Feature effectiveness
5 Related work
There has been a fair amount of research on punctua-
tion prediction or generation in the context of spoken
language processing (Lu and Ng, 2010; Guo et al,
2010). The task presented here is different in that the
punctuation marks are already present in the text and
we are only concerned with punctuation marks that
are semantically ambiguous. Our specific focus is
on the Chinese comma, which sometimes signals a
sentence boundary and sometimes doesn?t. The Chi-
nese comma has also been studied in the context of
syntactic parsing for long sentences (Jin et al, 2004;
Li et al, 2005), where the study of comma is seen as
part of a ?divide-and-conquer? strategy to syntactic
parsing. Long sentences are split into shorter sen-
tence segments on commas before they are parsed,
and the syntactic parses for the shorter sentence seg-
ments are then assembled into the syntactic parse for
the original sentence. We study comma disambigua-
tion in its own right aimed at helping a wide range of
NLP applications that include parsing and Machine
Translation.
6 Conclusion
The main goal of this short paper is to bring to
the attention of the field a problem that has largely
been taken for granted. We show that while sen-
tence boundary detection in Chinese is a relatively
easy task if formulated based on purely orthographic
grounds, the problem becomes much more challeng-
ing if we delve deeper and consider the semantic and
possibly the discourse basis on which sentences are
segmented. Seen in this light, the central problem
to Chinese sentence segmentation is comma disam-
biguation. We trained a statistical model using data
derived from the Chinese Treebank and reported
promising preliminary results. Much remains to be
done regarding how sentences in Chinese should be
segmented and how this problem should be modeled
in a statistical learning framework.
Acknowledgments
This work is supported by the National Science
Foundation via Grant No. 0910532 entitled ?Richer
Representations for Machine Translation?. All
views expressed in this paper are those of the au-
thors and do not necessarily represent the view of
the National Science Foundation.
634
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311.
Yuqing Guo, Haifeng Wang, and Josef Van Genabith.
2010. A Linguistically Inspired Statistical Model for
Chinese Punctuation Generation. ACM Transactions
on Asian Language Processing, 9(2).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
We Lu and Hwee Tou Ng. 2010. Better Punctuation
Prediction with Dynamic Conditional Random Fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, MIT, Mas-
sachusetts.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Slav Petrov and Dan Klein. 2007. Improved Inferencing
for Unlexicalized Parsing. In Proc of HLT-NAACL.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
Maximum Entropy Approach to Identifying Sentence
Boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing (ANLP), Wash-
ington, D.C.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
635
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 786?794,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Chinese Comma Disambiguation for Discourse Analysis
Yaqin Yang
Brandeis University
415 South Street
Waltham, MA 02453, USA
yaqin@brandeis.edu
Nianwen Xue
Brandeis University
415 South Street
Waltham, MA 02453, USA
xuen@brandeis.edu
Abstract
The Chinese comma signals the boundary of
discourse units and also anchors discourse
relations between adjacent text spans. In
this work, we propose a discourse structure-
oriented classification of the comma that can
be automatically extracted from the Chinese
Treebank based on syntactic patterns. We
then experimented with two supervised learn-
ing methods that automatically disambiguate
the Chinese comma based on this classifica-
tion. The first method integrates comma clas-
sification into parsing, and the second method
adopts a ?post-processing? approach that ex-
tracts features from automatic parses to train
a classifier. The experimental results show
that the second approach compares favorably
against the first approach.
1 Introduction
The Chinese comma, which looks graphically very
similar to its English counterpart, is functionally
quite different. It has attracted a significant amount
of research that studied the problem from the view-
point of natural language processing. For exam-
ple, Jin et al( 2004) and Li et al( 2005) view
the disambiguation of the Chinese comma as a way
of breaking up long Chinese sentences into shorter
ones to facilitate parsing. The idea is to split a
long sentence into multiple comma-separated seg-
ments, parse them individually, and reconstruct the
syntactic parse for the original sentence. Although
both studies show a positive impact of this approach,
comma disambiguation is viewed merely as a con-
venient tool to help achieve a more important goal.
Xue and Yang ( 2011) point out that the very rea-
son for the existence of these long Chinese sentences
is because the Chinese comma is ambiguous and in
some context, it identifies the boundary of a sentence
just as a period, a question mark, or an exclamation
mark does. The disambiguation of comma is viewed
as a necessary step to detect sentence boundaries in
Chinese and it can benefit a whole range of down-
stream NLP applications such as syntactic parsing
and Machine Translation. In Machine Translation,
for example, it is very typical for ?one? Chinese
sentence to be translated into multiple English sen-
tences, with each comma-separated segment corre-
sponding to one English sentence. In the present
work, we expand this view and propose to look at
the Chinese comma in the context of discourse anal-
ysis. The Chinese comma is viewed as a delimiter
of elementary discourse units (EDUs), in the sense
of the Rhetorical Structure Theory (Carlson et al,
2002; Mann et al, 1988). It is also considered to
be the anchor of discourse relations, in the sense of
the Penn Discourse Treebank (PDT) (Prasad et al,
2008). Disambiguating the comma is thus necessary
for the purpose of discourse segmentation, the iden-
tification of EDUs, a first step in building up the dis-
course structure of a Chinese text.
Developing a supervised or semi-supervised
model of discourse segmentation would require
ground truth annotated based on a well-established
representation scheme, but as of right now no such
annotation exists for Chinese to the best of our
knowledge. However, syntactically annotated tree-
banks often contain important clues that can be used
to infer discourse-level information. We present
786
a method of automatically deriving a preliminary
form of discourse structure anchored by the Chinese
comma from the Penn Chinese Treebank (CTB)
(Xue et al, 2005), and using this information to
train and test supervised models. This discourse
information is formalized as a classification of the
Chinese comma, with each class representing the
boundary of an elementary discourse unit as well
as the anchor of a coarse-grained discourse rela-
tion between the two discourse units that it delimits.
We then develop two comma classification methods.
In the first method, we replace the part-of-speech
(POS) tag of each comma in the CTB with a de-
rived discourse category and retrain a state-of-the-
art Chinese parser on the relabeled data. We then
evaluate how accurately the commas are classified
in the parsing process. In the second method, we
parse these sentences and extract lexical and syn-
tactic information as features to predict these new
discourse categories. The second approach gives us
more control over what features to extract and our
results show that it compares favorably against the
first approach.
The rest of the paper is organized as follows. In
Section 2, we present our approach to automati-
cally extract discourse information from a syntac-
tically annotated treebank and present our classifi-
cation scheme. In Section 3, we describe our su-
pervised learning methods and the features we ex-
tracted. Section 4 presents our experiment setup and
experimental results. Related work is reviewed in
Section 5. We conclude in Section 6.
2 Chinese comma classification
There are many ways to conceptualize the discourse
structure of a text (Mann et al, 1988; Prasad et
al., 2008), but there is more of a consensus among
researchers about the fundamental building blocks
of the discourse structure. For the Rhetorical Dis-
course Theory, the building blocks are Elementary
Discourse Units (EDUs). For the PDT, the build-
ing blocks are abstract objects such as propositions,
facts. Although they are phrased in different ways,
syntactically these discourse units are generally re-
alized as clauses or built on top of clauses. So the
first step in building the discourse structure of a text
is to identify these discourse units.
In Chinese, these elementary discourse units are
generally delimited by the comma, but not all com-
mas mark the boundaries of a discourse unit. In (1),
for example, Comma [1] marks the boundary of a
discourse unit while Comma [2] does not. This is
reflected in its English translation: while the first
comma corresponds to an English comma, the sec-
ond comma is not translated at all, as it marks the
boundary between a subject and its predicate, where
no comma is needed in English. Disambiguating
these two types of commas is thus an important first
step in identifying elementary discourse units and
building up the discourse structure of a text.
(1) ??
Wang Xiang
?
although
?
age
?
over
??
50
?[1]
,
?
but
?
his
??
abundant
?
DE
??
energy
?
and
??
quick
?
DE
??
thinking
?[2]
,
?
give
?
people
?
one
?
CL
???
challenger
?
DE
??
impression
?
.
?Although Wang Xiang is over 50 years old, his
abundant energy and quick thinking leave peo-
ple the impression of a challenger.?
Although to the best of our knowledge, no such
discourse segmented data for Chinese exists in the
public domain, this information can be extracted
from the syntactic annotation of the CTB. In the
syntactic annotation of the sentence, illustrated in
(a), it is clear that while the first comma in the sen-
tence marks the boundary of a clause, the second
one marks the demarcation between the subject NP
and the predicate VP and thus is not an indicator of
a discourse boundary.
(a)
IP
IP-CND
, 1
ADVP NP , 2 VP
In addition to a binary distinction of whether a
comma marks the boundary of a discourse unit,
the CTB annotation also allows the extraction of a
more elaborate classification of commas based on
coordination and subordination relations of comma-
separated clauses. This classification of the Chinese
787
comma can be viewed as a first approximation of the
discourse relations anchored by the comma that can
be refined later via a manual annotation process.
Based on the syntactic annotation in the CTB, we
classify the Chinese comma into seven hierarchi-
cally organized categories, as illustrated in Figure
1. The first distinction is made between commas
that indicate a discourse boundary (RELATION)
and those that do not (OTHER). Commas that in-
dicate discourse boundaries are further divided into
commas that separate coordinated discourse units
(COORD) vs commas that separate discourse units
in a subordination relation (SUBORD). Based on
the levels of embedding and the syntactic category
of the coordinated structures, we define three dif-
ferent types of coordination (SB, IP COORD and
VP COORD). We also define three types of subordi-
nation relations (ADJ, COMP, Sent SBJ), based on
the syntactic structure. As we will show below, each
of the six relations has a clear syntactic pattern that
can be exploited for their automatic detection.
ALL
OTHER
RELATION
SB COORD_IP COORD_VP ADJ COMP Sent_SBJ
COORD SUBORD
Figure 1: Comma classification
Sentence Boundary (SB): Following (Xue and
Yang, 2011), we consider the loosely coordinated
IPs that are the immediate children of the root IP to
be independent sentences, and the commas separat-
ing them to be delimiters of sentence boundary. This
is illustrated in (2), where a Chinese sentence can be
split into two independent shorter sentences at the
comma. We view this comma to be a marker of the
sentence boundary and it serves the same function as
the unambiguous sentence boundary delimitors (pe-
riods, question marks, exclamation marks) in Chi-
nese. The syntactic pattern that is used to infer this
relation is illustrated in (b).
(2) ???
Guangdong province
??
establish
?
ASP
??
natural
??
science
??
foundation
?[3]
,
??
every year
??
investment
?
at
??
one hundred millioin
?
yuan
??
above
?
.
?Natural Science Foundation is established in
Guangdong Province. More than one hundred
million yuan is invested every year.?
(b) IP-Root
IP
Clause
, IP
Clause
IP Coordination (IP COORD): Coordinated IPs
that are not the immediate children of the root IP are
also considered to be discourse units and the com-
mas linking them are labeled IP COORD. Different
from the sentence boundary cases, these coordinated
IPs are often embedded in a larger structure. An ex-
ample is given in (3) and its typical syntactic pattern
is illustrated in (c).
(3) ?
According to
???
Lu Renfa
??
presentation
?[4]
,
??
the whole country
??
revenue
??
goal
?
already
??
exceeding quota
??
complete
?[5]
,
??
overall
??
situation
??
fairly
??
good .
?According to Lu Renfa, the national revenue
goal is met and exceeded, and the overall situa-
tion is fairly good.?
(c) IP
PP
Modifier
, IP
IP
Conjunct
, IP
Conjunct
VP Coordination (VP COORD): Coordinated
VPs, when separated by the comma, are not seman-
tically different from coordinated IPs. The only dif-
ference is that in the latter case, the coordinated VPs
788
share a subject, while coordinated IPs tend to have
different subjects. Maintaining this distinction allow
us to model subject (dis)continuity, which helps re-
cover a subject when it is dropped, a prevalent phe-
nomenon in Chinese. As shown in (4), the VPs in the
text spans separated by Comma [6] have the same
subject, thus the subject in the second VP is dropped.
The syntactic pattern that allows us to extract this
structure is given in (d).
(4) ??
China
??
Bank
?
is
??
four major
??
state-owned
??
commercial
??
bank
??
one of these
?[6]
,
?
also
?
is
??
China
?
DE
??
major
??
foreign exchange
??
bank
?
.
?Bank of China is one of the four major state-
owned commercial banks, and it is also China?s
major foreign exchange bank.?
(d) IP
NP
Subject
VP
VP
Conjunct
, VP
Conjunct
Adjunction (ADJ): Adjunction is one of three
types of subordination relations we define. It holds
between a subordinate clause and its main clause.
The subordinate clause is normally introduced by a
subordinating conjunction and it typically provides
the cause, purpose, manner, or condition for the
main clause. In the PDT terms, these subordinate
conjunctions are discourse connectives that anchor
a discourse relation between the subordinate clause
and the main clause. In Chinese, with few excep-
tions, the subordinate clause comes before the main
clause. (5) is an example of this relation.
(5) ?
if
??
project
??
happen
??
insurance
??
liability
??
scope
?
inside
?
DE
??
natural
??
disaster
?[7]
,
??
China Insurance
??
property
??
insurance
??
company
?
will
?
according to
??
provision
??
excecute
??
compensation
?
.
?If natural disasters within the scope of the in-
surance liability happen in the project, PICC
Property Insurance Company will provide
compensations according to the provisions.?
(e) IP
CP/IP-CND
Subordinate Clause
,
Main Clause
(e) shows how (5) is represented in the syntac-
tic structure in the CTB. Extracting this relation re-
quires more than just the syntactic configuration be-
tween these two clauses. We also take advantage
of the functional (dash) tags provided in the tree-
bank. The functional tags are attached to the sub-
ordinate clause and they include CND (conditional),
PRP (purpose or reason), MNR (manner), or ADV
(other types of subordinate clauses that are adjuncts
to the main clause).
Complementation (COMP): When a comma
separates a verb governor and its complement
clause, this verb and its subject generally describe
the attribution of the complement clause. Attribu-
tion is an important notion in discourse analysis in
both the RST framework and in the PDT. An exam-
ple of this is given in (6), and the syntactic pattern
used to extract this relation is illustrated in (f).
(6) ?
The
??
company
??
present
?[8]
,
?
at
??
future
?
DE
??
five year
?
within
??
they
?
will
??
additionally
??
invest
???
ninety million
??
U.S. dollars
?[9]
,
??
estimate
???
annual output
?
will
?
reach
??
three hundred million
??
U.S. dollars
?
.
?According to the the company?s presentation,
they will invest an additional ninety million
789
U.S. dollars in the next five years, and the esti-
mated annual output will reach $ 300 million.?
(f) IP
....
VP
VV , IP
......
Sentential Subject (SBJ): This category is for
commas that separate a sentential subject from its
predicate VP. An example is given in (7) and the
syntactic pattern used to extract this relation is il-
lustrated in (g).
(7) ??
export
??
rapid
??
grow
?[10]
,
??
become
??
promote
??
economy
??
growth
?
DE
??
important
??
force
?
.
?The rapid growth of export becomes an impor-
tant force in promoting economic growth.?
(g) IP
IP-SBJ
Sentential Subject
, VP
......
Others (OTHER): The remaining cases of
comma receive the OTHER label, indicating they do
not mark the boundary of a discourse segment.
Our proposed comma classification scheme
serves the dual purpose of identifying elementary
discourse units and at the same time detecting
coarse-grained discourse relations anchored by the
comma. The discourse relations identified in this
manner by no means constitute the full discourse
analysis of a text, they are, however, a good first
approximation. The advantage of our approach is
that we do not require manual discourse annotations,
and all the information we need is automatically ex-
tracted from the syntactic annotation of the CTB
and attached to instances of the comma in the cor-
pus. This makes it possible for us to train supervised
models to automatically classify the commas in any
Chinese text.
3 Two comma classification methods
Given the gold standard parses, based on the syntac-
tic patterns described in Section 2, we can map the
POS tag of each comma instance in the CTB to one
of the seven classes described in Section 2. Using
this relabeled data as training data, we experimented
with two automatic comma disambiguation meth-
ods. In the first method, we simply retrained the
Berkeley parser (Petrov and Klein, 2007) on the re-
labeled data and computed how accurately the com-
mas are labeled in a held-out test set. In the second
method, we trained a Maximum Entropy classifier
with the Mallet (McCallum et al, 2002) machine
learning package to classify the commas. The fea-
tures are extracted from the CTB data automatically
parsed with the Berkeley parser. We implemented
features described in (Xue and Yang, 2011), and
also experimented with a set of new features as fol-
lows. In general, these new features are extracted
from the two text spans surrounding the comma.
Given a comma, we define the preceding text span as
i span and the following text span as j span. We also
collected a number of subject-predicate pairs from a
large corpus that doesn?t overlap with the CTB. We
refer to this corpus as the auxiliary corpus.
Subject and Predicate features: We explored
various combinations of the subject (sbj), predicate
(pred) and object (obj) of the two spans. The sub-
ject of i span is represented as sbji, etc.
1. The existence of sbji, sbjj , both, or neither.
2. The lemma of predi, the lemma of predj , the
conjunction of sbji and predj , the conjunction
of predi and sbjj
3. whether the conjunction of sbji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject.
4. whether the conjunction of obji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject
5. Whether the conjunction of predi and sbjj oc-
curs more than 2 times in the auxiliary corpus
when i does not have a subject.
Mutual Information features: Mutual informa-
tion is intended to capture the association strength
between the subject of a previous span and the predi-
cate of the current span. We use Mutual Information
790
(Church and Hanks, 1989) as shown in Equation
(1) and the frequency count computed based on the
auxiliary corpus to measure such constraints.
MI = log2
# co-occur of S and P * corpus size
# S occur * # P occur
(1)
1. The conjunction of sbji and predj when j does
not have a subject if their MIvalue is greater
than -8.0, an empirically established threshold.
2. Whether obji and predj has an MI value
greater than 5.0 if j does not have a subject.
3. Whether the MI value of sbji and predj is
greater than 0.0, and they occur 2 times in the
auxiliary corpus when j doesn?t have a subject.
4. Whether the MI value of obji and predj is
greater than 0.0 and they occur 2 times in the
auxiliary corpus when j doesn?t have a subject.
5. Whether the MI value of predi and sbjj is
greater than 0.0 and they occur more than 2
times in the auxiliary corpus when i does not
have a subject.
Span features: We used span features to cap-
ture syntactic information, e.g. the comma separated
spans are constituents in Tree (b) but not in Tree (d).
1. Whether i forms a single constituent, whether
j forms a single constituent.
2. The conjunction and hierarchical relation of all
constituent labels in i/j, if i/j does not form
a single constituent. The conjunction of all
constituent labels in both spans, if neither span
form a single constituent.
Lexical features:
1. The first word in i if it is an adverb, the first
word in j if it is an adverb.
2. The first word in i span if it is a coordinating
conjunction, the first word in j if it is a coordi-
nating conjunction.
4 Experiments
4.1 Datasets
We use the CTB 6.0 in our experiments and divide
it into training, development and test sets using the
data split recommended in the CTB 6.0 documenta-
tion, as shown in Table 1. There are 5436 commas
in the test set, including 1327 commas that are sen-
tence boundaries (SB), 539 commas that connect co-
ordinated IPs (IP COORD), 1173 commas that join
coordinated VPs (VP COORD), 379 commas that
delimits a subordinate clause and its main clause
(ADJ), 314 commas that anchor complementation
relations (COMP), and 1625 commas that belong to
the OTHER category.
4.2 Results
As mentioned in Section 3, we experimented with
two comma classification methods. In the first
method, we replace the part-of-speech (POS) tags of
the commas with the seven classes defined in Sec-
tion 2. We then retrain the Berkeley parser (Petrov
and Klein, 2007) using the training set as presented
in Table 1, parse the test set, and evaluate the comma
classification accuracy.
In the second method, we use the relabeled com-
mas as the gold-standard data to train a supervised
classifier to automatically classify the commas. As
shown in the previous section, syntactic structures
are an important source of information for our clas-
sifier. For feature extraction purposes, the entire
CTB6.0 is automatically parsed in a round-robin
fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). Measured by the ParsEval metric
(Black et al, 1991), the parsing accuracy on the
CTB test set stands at 83.29% (F-score), with a pre-
cision of 85.18% and a recall of 81.49%.
The results are presented in Table 2, which shows
the overall accuracy of the two methods as well as
the results for each individual category. As should
be clear from Table 2, the results for the two meth-
ods are very comparable, with the second method
performing modestly better than the first method.
4.2.1 Subject continuity
One of the goals for this classification scheme is
to model subject continuity, which answers the ques-
tion of how accurately we can predict whether two
comma-separated text spans have the same subject
or different subjects. When the two spans share
the same subject, the comma belongs to the cate-
gory VP COORD. When they have different sub-
jects, they belong to the categories IP COORD or
791
Data Train Dev Test
CTB-6.0
81-325, 400-454, 500-554 41-80 (1-40,901-931 newswire)
590-596, 600-885, 900 1120-1129 (1018, 1020, 1036, 1044
1001-1017, 1019, 1021-1035 2140-2159 1060-1061,
1037-1043, 1045-1059,1062-1071 2280-2294 1072, 1118-1119, 1132
1073-1078, 1100-1117, 1130-1131 2550-2569 1141-1142, 1148 magazine)
1133-1140, 1143-1147, 1149-1151 2775-2799 (2165-2180, 2295-2310
2000-2139, 2160-2164, 2181-2279 3080-3109 2570-2602, 2800-2819
2311-2549, 2603-2774, 2820-3079 3110-3145 broadcast news)
Table 1: CTB 6.0 data set division.
SB. When this question is meaningless, e.g., when
one of the span does not even have a subject, the
comma belongs to other categories. To evaluate the
performance of our model on this problem, we re-
computed the results by putting IP COORD and SB
in one category, putting VP COORD in another cat-
egory and the rest of the labels in a third category.
The results are presented in Table 3.
4.2.2 The effect of genre
CTB 6.0 consists of data from three different gen-
res, including newswire, magazine and broadcast
news. Data genres may have very different char-
acteristics. To evaluate how our model works on
different genres, we train a model using training
and development sets, and test the model on differ-
ent genres as described in Table 1. The results on
these three genres are presented in Table 4, and they
shows a significant fluctuation across genres. Our
model works the best on newswire, but not as good
on broadcast news and magazine articles.
4.2.3 Comparison with prior work
(Xue and Yang, 2011) presented results on a
binary classification of whether or not a comma
marks a sentence boundary, while the present work
addresses a multi-category classification problem
aimed at identifying discourse segments and prelim-
inary discourse relations anchored by the comma.
However, since we also have a SB category, com-
parison is possible. For comparison purposes, we
retrained our model on their data sets, and computed
the results of SB vs other categories. The results are
shown in Table 5. Our results are very comparable
with (Xue and Yang, 2011) despite that we are per-
forming a multicategory classification.
4.3 Error analysis
Even though our feature-based approach can the-
oretically ?correct? parsing errors, meaning that a
comma can in theory be classified correctly even if a
sentence is incorrectly parsed, when examining the
system output, errors in automatic parses often lead
to errors in comma classification. A common pars-
ing error is the confusion between Structures (h) and
(i). If the subject of the text span after a comma is
dropped as shown in (h), the parser often produces
a VP coordination structure as shown in (i) and vice
versa. This kind of parsing errors would lead to er-
rors in our syntactic features and thus directly affect
the accuracy of our model.
(h) IP
IP
NP VP
, IP
VP
(i) IP
NP VP
VP , VP
5 Related Work
There is a large body of work on discourse analysis
in the field of Natural Language Processing. Most of
the work, however, are on English. An unsupervised
approach was proposed to recognize discourse rela-
tions in (Marcu and Echihabi, 2002), which extracts
discourse relations that hold between arbitrary spans
of text making use of cue phrases. Like the present
work, a lot of research on discourse analysis is car-
ried out at the sentence level. (Soricut and Marcu,
2003; Sporleder and Lapata, 2005; Polanyi et al,
2004). (Soricut and Marcu, 2003) and (Polanyi et
al., 2004) implement models to perform discourse
parsing, while (Sporleder and Lapata, 2005) intro-
duces discourse chunking as an alternative to full-
792
Class Metric Method 1 Method 2
all acc. (%) 71.5 72.9
SB
Prec. (%) 65.6 66.2
Rec. (%) 71.7 73.1
F. (%) 68.5 69.5
IP COORD
Prec. (%) 53.3 56.0
Rec. (%) 50.5 48.6
F. (%) 52.0 52.0
VP Coord
Prec. (%) 65.6 68.3
Rec. (%) 76.3 78.2
F. (%) 70.5 72.9
ADJ
Prec. (%) 66.9 66.8
Rec. (%) 29.3 37.7
F. (%) 40.8 48.2
Comp
Prec. (%) 88.3 91.2
Rec. (%) 93.9 92.4
F. (%) 91.0 91.8
SentSBJ
Prec. (%) 25.0 31.8
Rec. (%) 6 10
F. (%) 9.7 15.6
Other
Prec. (%) 86.9 85.6
Rec. (%) 83.4 84.1
F. (%) 85.1 84.8
Table 2: Overall accuracy of the two methods as well as
the results for each individual category.
scale discourse parsing.
The emergence of linguistic corpora annotated
with discourse structure such as the RST Discourse
Treebank (Carlson et al, 2002) and PDT (Miltsakaki
et al, 2004; Prasad et al, 2008) have changed the
landscape of discourse analysis. More robust, data-
driven models are starting to emerge.
Compared with English, much less work has
been done in Chinese discourse analysis, presum-
ably due to the lack of discourse resources in Chi-
nese. (Huang and Chen, 2011) constructs a small
corpus following the PDT annotation scheme and
Prec. (%) Rec. (%) F. (%)
VP COORD 68.3 78.2 72.9
IP COORD+SB 76.0 78.7 77.3
Other 89.0 80.2 84.4
Table 3: Subject continuity results based on Maximum
Entropy model
Genre NW BN MZ
Accuracy. (%) 79.1 73.6 67.7
Table 4: Results on different genres based on Maximum
Entropy model
Xue and Yang our model
(%) p r f1 p r f1
Overall 89.2 88.7
EOS 64.7 76.4 70.1 63.0 77.9 69.7
NEOS 95.1 91.7 93.4 95.3 90.8 93.0
Table 5: Comparison of (Xue and Yang, 2011) and the
present work based on Maximum Entropy model
trains a statistical classifier to recognize discourse
relations. Their work, however, is only concerned
with discourse relations between adjacent sentences,
thus side-stepping the hard problem of disambiguat-
ing the Chinese comma and analyzing intra-sentence
discourse relations. To the best of our knowledge,
our work is the first in attempting to disambiguating
the Chinese comma as the first step in performing
Chinese discourse analysis.
6 Conclusions and future work
We proposed a approach to disambiguate the Chi-
nese comma as a first step toward discourse analy-
sis. Training and testing data are automatically de-
rived from a syntactically annotated corpus. We pre-
sented two automatic comma disambiguation meth-
ods that perform comparably. In the first method,
comma disambiguation is integrated into the parsing
process while in the second method we train a super-
vised classifier to classify the Chinese comma, us-
ing features extracted from automatic parses. Much
needs to be done in the area, but we believe our work
provides insight into the intricacy and complexity of
discourse analysis in Chinese.
Acknowledgment
This work is supported by the IIS Division of Na-
tional Science Foundation via Grant No. 0910532
entitled ?Richer Representations for Machine
Translation?. All views expressed in this paper are
those of the authors and do not necessarily represent
the view of the National Science Foundation.
793
References
L Carlson, D Marcu, M E Okurowski. 2002. RST Dis-
course Treebank. Linguistic Data Consortium 2002.
Caroline Sporleder, Mirella Lapata. 2005. Discourse
chunking and its application to sentence compression.
In Proceedings of HLT/EMNLP 2005.
Livia Polanyi, Chris Culy, Martin Van Den Berg, Gian
Lorenzo Thione and David Ahn. 2004. Sentential
structure and discourse parsing. In Proceeedings of
the ACL 2004 Workshop on Discourse Annotation
2004.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese
Discourse Relation Recognition. In Proceedings of
the 5th International Joint Conference on Natural Lan-
guage Processing 2011,pages 1442-1446.
Daniel Marcu and Abdessamad Echihabi. 2002. An Un-
supervised Approach to Recognizing Discourse Rela-
tions. In Proceedings of the ACL, July 6-12, 2002,
Philadelphia, PA, USA.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In Proceedings of the ACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi andBon-
nie Webber. 2004. The Penn Discourse Treebank. In
Proceedings of LREC 2004.
Nianwen Xue and Yaqin Yang. 2011. Chinese sentence
segmentation as comma classification. In Proceedings
of ACL 2011.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207-238.
Slav Petrov and Dan Klein. 2007. Improved Inferenc-
ing for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini, and T. Strzalkowski. 1991. A procedure
for quantitively comparing the syntactic coverage of
English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306-
311.
Mann, William C. and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text 8 (3): 243-281.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0..
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Church, K., and Hanks, P. 1989. Word Association
Norms, Mutual Information and Lexicography. As-
sociation for Computational Linguistics, Vancouver ,
Canada
794
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 117?121,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Machine Learning-Based Coreference Detection System For OntoNotes
Yaqin Yang
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
yaqin@brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
xuen@brandeis.edu
Peter Anick
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
peter anick@yahoo.com
Abstract
In this paper, we describe the algorithms and
experimental results of Brandeis University
in the participation of the CoNLL Task 2011
closed track. We report the features used in
our system, and describe a novel cluster-based
chaining algorithm to improve performance of
coreference identification. We evaluate the
system using the OntoNotes data set and de-
scribe our results.
1 Introduction
This paper describes the algorithms designed and
experiments finished in the participation of the
CoNLL Task 2011. The goal of the Task is to design
efficient algorithms for detecting entity candidates
and identifying coreferences. Coreference identifi-
cation is an important technical problem. Its impor-
tance in NLP applications has been observed in pre-
vious work, such as that of Raghunathan et al, Prad-
han et al, Bergsma et al, Haghighi et al, and Ng et
al.. While most of the existing work has evaluated
their systems using the ACE data set, in this work
we present our experimental results based on the
OntoNotes data set used in the CoNLL 2011 Shared
Task. We detail a number of linguistic features
that are used during the experiments, and highlight
their contribution in improving coreference identi-
fication performance over the OntoNotes data set.
We also describe a cluster-based approach to multi-
entity chaining. Finally, we report experimental re-
sults and summarize our work.
2 Data Preparation
We divide the CoNLL Task into three steps. First,
we detect entities from both the training data and
the development data. Second, we group related en-
tities into entity-pairs. Finally, we use the gener-
ated entity-pairs in the machine learning-based clas-
sifier to identify coreferences. In this section, we
describe how we extract the entities and group them
into pairs.
2.1 Generating Entity Candidates
We use the syntactic parse tree to extract four types
of entities, including noun phrase, pronoun, pre-
modifier and verb (Pradhan et al, 2007). This
method achieves 94.0% (Recall) of detection accu-
racy for gold standard trees in the development data.
When using the automatic parses, not surprisingly,
the detection accuracy becomes lower, with a per-
formance drop of 5.3% (Recall) compared with that
of using the gold standard trees. Nevertheless, this
method can still cover 88.7% of all entities existing
in the development data, thus we used it in our algo-
rithm.
2.2 Generating Entity-Pairs From Individual
Entities
In the annotated training documents, an entity has
been marked in a coreference chain that includes all
coreferential entities. In our algorithm, we only de-
tect the closest antecedent for each entity, instead
of all coreferences, of each entity. Specifically, we
define each training and testing instance as a pair
of entities. During the training process, for each
entity encountered by the system, we create a pos-
itive instance by pairing an entity with its closest
antecedent (Soon et al, 2001). In addition, a set
of negative instances are also created by pairing the
entity with any preceding entities that exist between
its closest antecedent and the entity itself (note that
the antecedent must be a coreference of the current
entity, whereas preceding entities may not be coref-
erential). For example, in the entity sequence ?A,
B, C, D, E?, let us assume that ?A? is the closest
antecedent of ?D?. Then, for entity ?D?, ?A-D? is
considered a positive instance, whereas ?B-D? and
?C-D? are two negative instances.
To generate testing data, every entity-pair within
the same sentence is considered to form positive or
negative instances, which are then used to form test-
ing data. Since occasionally the distance between an
entity and its closest antecedent can be far apart, we
handle considerably distant coreferences by consid-
117
ering each entity-pair that exists within the adjacent
N sentences. During our experiments, we observed
that the distance between an entity and its closest an-
tecedent could be as far as 23 sentences. Therefore,
in the classification process, we empirically set N as
23.
3 Machine Learning-Based Classification
After labeling entity pairs, we formalize the corefer-
ence identification problem as a binary classification
problem. We derive a number of linguistic features
based on each entity-pair, i and j, where i is the po-
tential antecedent and j the anaphor in the pair (Soon
et al, 2001). Generally, we select a set of features
that have been proved to be useful for the corefer-
ence classification tasks in previous work, includ-
ing gender, number, distance between the antecedent
and the anaphor, and WordNet (WordNet, 2010). In
addition, we design additional features that could
be obtained from the OntoNotes data, such as the
speaker or author information that is mainly avail-
able in Broadcast Conversation and Web Log data
(Pradhan et al, 2007). Moreover, we extract appo-
sition and copular structures and used them as fea-
tures. The features we used in the system are de-
tailed below.
? Independent feature: 1) if a noun phrase is defi-
nite; 2) if a noun phrase is demonstrative; 3) gender
information of each entity; 4) number information
of each entity; 5) the entity type of a noun phrase;
6) if an entity is a subject; 7) if an entity is an object;
8) if an noun phrase is a coordination, the number
of entities it has; 9) if a pronoun is preceded by a
preposition; 10) if a pronoun is ?you? or ?me?; 11)
if a pronoun is ?you? and it is followed by the word
?know?.
? Name entity feature: 1) i-j-same-entity-type-
etype=True, if i and j have the same entity type;
2) i-j-same-etype-subphrase=True, if i and j have
the same entity type and one is the subphrase of the
other.
? Syntactic feature: 1) i-j-both-subject=True, if i
and j are both subjects; 2) if i and j are in the same
sentence, record the syntactic path between i and j,
e.g. i-j-syn-path=PRP?NP!PRP; 3) i-j-same-sent-
diff-clause=True, if i and j are in the same sentence
but in different clauses.
? Gender and number feature: 1) i-j-same-
gender=True/False, by comparing if i and j have the
same gender; 2) i-j-same-num=True/False, by com-
paring if i and j have the same number; 3) i-j-same-
num-modifier=True/False, by comparing if i and j
have the same number modifier, e.g. ?two coun-
tries? and ?they both? have the same number mod-
ifier; 4) i-j-same-family=True/False, we designed
seven different families for pronouns, e.g. ?it?, ?its?
and ?itself? are in one family while ?he?, ?him?,
?his? and ?himself? are in another one.
? Distance feature: 1) i-j-sent-dist, if the sentence
distance between i and j is smaller than three, use
their sentence distance as a feature; 2) i-j-sent-
dist=medium/far: if the sentence distance is larger
than or equal to three, set the value of i-j-sent-dist
to ?medium?, otherwise set it to ?far? combined
with the part-of-speech of the head word in j.
? String and head word match feature: 1) i-j-
same-string=True, if i and j have the same string;
2) i-j-same-string-prp=True, if i and j are the
same string and they are both pronouns; 3) i-j-sub-
string=True, if one is the sub string of the other,
and neither is a pronoun; 4) i-j-same-head=True,
if i and j have the same head word; 5) i-j-prefix-
head=True, if the head word of i or j is the pre-
fix of the head word of the other; 6) i-j-loose-head,
the same as i-j-prefix-head, but comparing only the
first four letters of the head word.
? Apposition and copular feature: for each noun
phrase, if it has an apposition or is followed by
a copular verb, then the apposition or the subject
complement is used as an attribute of that noun
phrase. We also built up a dictionary where the
key is the noun phrase and the value is its apposi-
tion or the subject?s complement to define features.
1) i-appo-j-same-head=True, if i?s apposition and
j have the same head word; 2) i-j-appo-same-
head=True, if j?s apposition has the same head word
as i; we define the similar head match features for
the noun phrase and its complement; Also, if an i
or j is a key in the defined dictionary, we get the
head word of the corresponding value for that key
and compare it to the head word of the other entity.
? Alias feature: i-j-alias=True, if one entity is a
proper noun, then we extract the first letter of each
word in the other entity. ( The extraction process
skips the first word if it?s a determiner and also skips
the last one if it is a possessive case). If the proper
noun is the same as the first-letter string, it is the
alias of the other entity.
? Wordnet feature: for each entity, we used Wordnet
to generate all synsets for its head word, and for
each synset, we get al hypernyms and hyponyms.
1) if i is a hypernym of j, then i-hyper-j=True; 2)
if i is a hyponym of j, then i-hypo-j=True.
? Speaker information features: In a conversation,
a speaker usually uses ?I? to refer to himself/herself,
and most likely uses ?you? to refer to the next
speaker. Since speaker or author name informa-
tion is given in Broadcast Conversation and Web
Log data, we use such information to design fea-
tures that represent relations between pronouns and
118
speakers. 1) i-PRP1-j-PRP2-same-speaker=True,
if both i and j are pronouns, and they have the same
speaker; 2) i-I-j-I-same-speaker=True, if both i and
j are ?I?, and they have the same speaker; 3) i-I-j-
you-same-speaker=True, if i is ?I? and j is ?you?,
and they have the same speaker; 4) if i is ?I?, j
is ?you? and the speaker of j is right after that of
i, then we have feature i-I-j-you&itarget=jspeaker;
5) if i is ?you?, j is ?I? and the speaker of j is
right after that of i, then we have feature i-you-
j-I-itarget=jspeaker; 6) if both i and j are ?you?,
and they followed by the same speaker, we consider
?you? as a general term, and this information is used
as a negative feature.
? Other feature: i-j-both-prp=True, if both i and j
are pronouns.
4 Chaining by Using Clusters
After the classifier detects coreferential entities,
coreference detection systems usually need to chain
multiple coreferential entity-pairs together, forming
a coreference chain. A conventional approach is
to chain all entities in multiple coreferential entity-
pairs if they share the same entities. For example, if
?A-B?, ?B-C?, and ?C-D? are coreferential entity-
pairs, then A, B, C, and D would be chained to-
gether, forming a coreference chain ?A-B-C-D?.
One significant disadvantage of this approach is
that it is likely to put different coreference chains to-
gether in the case of erroneous classifications. For
example, suppose in the previous case, ?B-C? is ac-
tually a wrong coreference detection, then the coref-
erence chain created above will cause A and D to be
mistakenly linked together. This error can propagate
as coreference chains become larger.
To mitigate this issue, we design a cluster-based
chaining approach. This approach is based on the
observation that some linguistic rules are capable of
detecting coreferential entities with high detection
precision. This allows us to leverage these rules to
double-check the coreference identifications, and re-
ject chaining entities that are incompatible with rule-
based results.
To be specific, we design two lightweight yet ef-
ficient rules to cluster entities.
? Rule One. For the first noun phrase (NP) encoun-
tered by the system, if 1) this NP has a name entity
on its head word position or 2) it has a name en-
tity inside and the span of this entity includes the
head word position, a cluster is created for this NP.
The name entity of this NP is also recorded. For
each following NP with a name entity on its head
word position, if there is a cluster that has the same
name entity, this NP is considered as a coreference
to other NPs in that cluster, and is put into that clus-
ter. If the system cannot find such a cluster, a new
cluster is created for the current NP.
? Rule Two. In Broadcast Conversation or Web Log
data, a speaker or author would most likely use ?I?
to refer to himself/herself. Therefore, we used it
as the other rule to cluster all ?I? pronouns and the
same speaker information together.
Given the labeled entity pairs, we then link them in
different coreference chains by using the cluster in-
formation. As the Maximum Entropy classifier not
only labels each entity-pair but also returns a con-
fidence score of that label, we sort all positive pairs
using their possibilities. For each positive entity-pair
in the sorted list, if the two entities are in different
clusters, we consider this to be a conflict, and with-
draw this positive entity-pair; if one entity belongs to
one cluster whereas the other does not belong to any
cluster, the two entities will be both included in that
cluster. This process is repeated until no more enti-
ties can be included in a cluster. Finally, we chain
the rest of entity pairs together.
5 Results and Discussion
To evaluate the features and the chaining approach
described in this paper, we design experiments de-
scribed as follows. Since there are five different
data types in the provided OntoNotes coreference
data set, we create five different classifiers to pro-
cess each of the data types. We used the features
described in Section 3 to train the classifiers, and
did the experiments using a Maximum Entropy clas-
sifier trained with the Mallet package (McCallum,
2002). We use the gold-standard data in the training
set to train the five classifiers and test the classifiers
on both gold and automatically-parsed data in the
development data set. The MUC metric provided by
the Task is used to evaluate the results.
5.1 Performance without Clustering
First, we evaluate the system by turning the clus-
tering technique off during the process of creating
coreference chains. For entity detection, we ob-
serve that for all five data types, i.e. Broadcast
(BC), Broad news (BN), Newswire (NW), Magazine
(MZ), and Web blog (WB), the NW and WB data
types achieve relatively lower F1-scores, whereas
the BC, BN, and MZ data types achieve higher per-
119
BC BN NW MZ WB
Without Clustering
Gold 57.40 (64.92/51.44) 59.45 (63.53/55.86) 52.01 (59.71/46.07) 55.59 (62.90/49.80) 49.53 (61.16/41.62)
Auto 54.00 (61.28/48.26) 55.40 (59.05/52.17) 48.44 (55.32/43.09) 52.21 (59.78/46.33) 47.02 (58.33/39.39)
With Clustering
Gold 57.44 (64.12/52.03) 56.56 (58.10/55.09) 51.37 (56.64/46.99) 54.26 (60.07/49.47) 49.00 (60.09/41.36)
Auto 54.19 (60.82/48.87) 52.69 (54.07/51.37) 48.01 (52.74/44.05) 50.82 (56.76/46.01) 46.86 (57.49/39.55)
Table 1: Performance comparison of coreference identification between using and without using the clustering tech-
nique in chaining. Note that the results are listed in sequence of F1-scores (Recalls/Precisions). The results shown are
based on MUC.
formance. Due to limited space, the performance
table of entity detection is not included in this paper.
For coreference identification, as shown in Ta-
ble 1, we observe pretty similar performance gaps
among different data types. The NW and WB data
types achieve the lowest F1-scores (i.e. 52.01%
and 49.53% for gold standard data, and 48.44% and
47.02% for automatically-parsed data) among all the
five data types. This can be explained by seeing that
the entity detection performance of these two data
types are also relatively low. The other three types
achieves more than 55% and 52% F1-scores for gold
and auto data, respectively.
These experiments that are done without using
clustering techniques tend to indicate that the perfor-
mance of entity detection has a positive correlation
with that of coreference identification. Therefore, in
the other set of experiments, we enable the cluster-
ing technique to improve coreference identification
performance by increasing entity detection accuracy.
Metric Recall Precision F1
MUC 59.94 45.38 51.65
BCUBED 72.07 53.65 61.51
CEAF (M) 45.67 45.67 45.67
CEAF (E) 29.43 42.54 34.79
BLANC 70.86 60.55 63.37
Table 2: Official results of our system in the CoNLL Task
2011. Official score is 49.32. ((MUC + BCUBED +
CEAF (E))/3)
5.2 Performance with Clustering
After enabling the clustering technique, we observe
an improvement in entity detection performance.
This improvement occurs mainly in the cases of the
NW and WB data types, which show low entity
detection performance when not using the cluster-
ing technique. To be specific, the performance of
the NW type on both the gold standard and auto-
matic data improves by about 0.5%, and the perfor-
mance of the WB type on the automatic data im-
proves about 0.1%. In addition, the performance of
the BC type on both the gold standard and automatic
data also increases about 0.2% to 0.6%.
Although the clustering technique succeeds in im-
proving entity detection performance for multiple
data types, there is no obvious improvement gained
with respect to coreference identification. This is
quite incompatible with our observation in the ex-
periments that do not utilize the clustering tech-
nique. Currently, we attribute this issue to the low
accuracy rates of the clustering operation. For ex-
ample, ?H. D. Ye.? and ?Ye? can be estimated cor-
rectly to be coreferential by the Maxtent classifier,
but the clustering algorithm puts them into different
clusters since ?H. D. Ye.? is a PERSON type name
entity while ?Ye? is a ORG type name entity. There-
fore, the system erroneously considers them to be a
conflict and rejects them. We plan to investigate this
issue further in our future work.
The official results of our system in the CoNLL
Task 2011 are summarized in Table 2.
6 Conclusion
In this paper, we described the algorithm design and
experimental results of Brandeis University in the
CoNLL Task 2011. We show that several linguistic
features perform well in the OntoNotes data set.
References
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
120
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. International Conference on Semantic
Computing (ICSC 2007), pages 446?453, September.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
WordNet. 2010. Princeton University ?About
WordNet.? WordNet. Princeton University. 2010.
http://wordnet.princeton.edu.
121
